<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph   Regularization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f4acd61c8d7272201da7c7f3b421176a')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    57 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-10-æ›´æ–°"><a href="#2025-10-10-æ›´æ–°" class="headerlink" title="2025-10-10 æ›´æ–°"></a>2025-10-10 æ›´æ–°</h1><h2 id="Continual-Action-Quality-Assessment-via-Adaptive-Manifold-Aligned-Graph-Regularization"><a href="#Continual-Action-Quality-Assessment-via-Adaptive-Manifold-Aligned-Graph-Regularization" class="headerlink" title="Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph   Regularization"></a>Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph   Regularization</h2><p><strong>Authors:Kanglei Zhou, Qingyi Pan, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang, Liyuan Wang</strong></p>
<p>Action Quality Assessment (AQA) quantifies human actions in videos, supporting applications in sports scoring, rehabilitation, and skill evaluation. A major challenge lies in the non-stationary nature of quality distributions in real-world scenarios, which limits the generalization ability of conventional methods. We introduce Continual AQA (CAQA), which equips AQA with Continual Learning (CL) capabilities to handle evolving distributions while mitigating catastrophic forgetting. Although parameter-efficient fine-tuning of pretrained models has shown promise in CL for image classification, we find it insufficient for CAQA. Our empirical and theoretical analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is necessary for effective representation learning; yet (ii) uncontrolled FPFT induces overfitting and feature manifold shift, thereby aggravating forgetting. To address this, we propose Adaptive Manifold-Aligned Graph Regularization (MAGR++), which couples backbone fine-tuning that stabilizes shallow layers while adapting deeper ones with a two-step feature rectification pipeline: a manifold projector to translate deviated historical features into the current representation space, and a graph regularizer to align local and global distributions. We construct four CAQA benchmarks from three datasets with tailored evaluation protocols and strong baselines, enabling systematic cross-dataset comparison. Extensive experiments show that MAGR++ achieves state-of-the-art performance, with average correlation gains of 3.6% offline and 12.2% online over the strongest baseline, confirming its robustness and effectiveness. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhouKanglei/MAGRPP">https://github.com/ZhouKanglei/MAGRPP</a>. </p>
<blockquote>
<p>åŠ¨ä½œè´¨é‡è¯„ä¼°ï¼ˆAQAï¼‰èƒ½å¤Ÿå¯¹è§†é¢‘ä¸­çš„äººç±»è¡Œä¸ºè¿›è¡Œé‡åŒ–ï¼Œæ”¯æŒä½“è‚²è¯„åˆ†ã€åº·å¤å’ŒæŠ€èƒ½è¯„ä¼°ç­‰åº”ç”¨ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºç°å®åœºæ™¯ä¸­è´¨é‡åˆ†å¸ƒçš„éå¹³ç¨³æ€§ï¼Œè¿™é™åˆ¶äº†ä¼ ç»Ÿæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†æŒç»­åŠ¨ä½œè´¨é‡è¯„ä¼°ï¼ˆCAQAï¼‰ï¼Œä¸ºAQAé…å¤‡äº†æŒç»­å­¦ä¹ ï¼ˆCLï¼‰çš„èƒ½åŠ›ï¼Œä»¥å¤„ç†ä¸æ–­å˜åŒ–çš„åˆ†å¸ƒï¼ŒåŒæ—¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚å°½ç®¡é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°æœ‰æ•ˆå¾®è°ƒåœ¨CLå›¾åƒåˆ†ç±»ä¸­æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒå¯¹äºCAQAæ¥è¯´æ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬çš„ç»éªŒå’Œç†è®ºåˆ†ææ­ç¤ºäº†ä¸¤ä¸ªè§è§£ï¼šï¼ˆiï¼‰å…¨å‚æ•°å¾®è°ƒï¼ˆFPFTï¼‰å¯¹äºæœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ æ˜¯å¿…è¦çš„ï¼›ç„¶è€Œï¼ˆiiï¼‰ä¸å—æ§åˆ¶çš„FPFTä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œç‰¹å¾æµå½¢åç§»ï¼Œä»è€ŒåŠ å‰§é—å¿˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æµå½¢å¯¹é½å›¾æ­£åˆ™åŒ–ï¼ˆMAGR++ï¼‰ï¼Œå®ƒå°†ä¸»å¹²å¾®è°ƒä¸é€‚åº”æ·±å±‚ç‰¹å¾çš„ä¸¤æ­¥ç‰¹å¾æ ¡æ­£ç®¡é“ç›¸ç»“åˆï¼šæµå½¢æŠ•å½±ä»ªå°†åç¦»çš„å†å²ç‰¹å¾ç¿»è¯‘åˆ°å½“å‰è¡¨ç¤ºç©ºé—´ï¼Œå›¾æ­£åˆ™åŒ–å™¨å¯¹é½å±€éƒ¨å’Œå…¨å±€åˆ†å¸ƒã€‚æˆ‘ä»¬æ„å»ºäº†å››ä¸ªCAQAåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ•°æ®é›†ã€å®šåˆ¶è¯„ä¼°åè®®å’Œå¼ºå¤§çš„åŸºçº¿ï¼Œä»¥å®ç°è·¨æ•°æ®é›†çš„ç³»ç»Ÿæ¯”è¾ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMAGR++è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”ï¼Œç¦»çº¿å¹³å‡ç›¸å…³æ€§æé«˜äº†3.6%ï¼Œåœ¨çº¿æé«˜äº†12.2%ï¼Œè¯å®äº†å…¶ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhouKanglei/MAGRPP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhouKanglei/MAGRPPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06842v1">PDF</a> Extended Version of MAGR (ECCV 2024 Oral Presentation)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘åŠ¨ä½œè´¨é‡è¯„ä¼°ï¼ˆAQAï¼‰é¢ä¸´çš„æŒ‘æˆ˜åœ¨äºçœŸå®åœºæ™¯ä¸­çš„è´¨é‡åˆ†å¸ƒéç¨³å®šæ€§ï¼Œè¿™é™åˆ¶äº†ä¼ ç»Ÿæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡å¼•å…¥æŒç»­AQAï¼ˆCAQAï¼‰ï¼Œé…å¤‡æŒç»­å­¦ä¹ ï¼ˆCLï¼‰èƒ½åŠ›æ¥å¤„ç†ä¸æ–­å˜åŒ–çš„åˆ†å¸ƒï¼ŒåŒæ—¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚é€šè¿‡å®è¯å’Œç†è®ºåˆ†æï¼Œå‘ç°å…¨å‚æ•°å¾®è°ƒï¼ˆFPFTï¼‰å¯¹äºæœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ æ˜¯å¿…è¦çš„ï¼Œä½†ä¸å—æ§åˆ¶çš„FPFTä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œç‰¹å¾æµå½¢åç§»ï¼Œä»è€ŒåŠ å‰§é—å¿˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºè‡ªé€‚åº”æµå½¢å¯¹é½å›¾æ­£åˆ™åŒ–ï¼ˆMAGR++ï¼‰ï¼Œé€šè¿‡å¾®è°ƒä¸»å¹²ç½‘ç»œç¨³å®šæµ…å±‚ç½‘ç»œï¼ŒåŒæ—¶é€‚åº”æ·±å±‚ç½‘ç»œçš„ä¸¤æ­¥ç‰¹å¾æ ¡æ­£ç®¡é“ï¼šæµå½¢æŠ•å½±å™¨å°†åç¦»çš„å†å²ç‰¹å¾ç¿»è¯‘åˆ°å½“å‰è¡¨ç¤ºç©ºé—´ï¼Œå›¾æ­£åˆ™åŒ–å™¨å¯¹é½å±€éƒ¨å’Œå…¨å±€åˆ†å¸ƒã€‚æœ¬æ–‡æ„å»ºäº†å››ä¸ªCAQAåŸºå‡†æµ‹è¯•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºMAGR++å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡ç›¸å…³æ€§å¢ç›Šæ¯”æœ€å¼ºåŸºçº¿é«˜å‡º3.6%ï¼ˆç¦»çº¿ï¼‰å’Œ12.2%ï¼ˆåœ¨çº¿ï¼‰ï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AQAï¼ˆåŠ¨ä½œè´¨é‡è¯„ä¼°ï¼‰é¢ä¸´çœŸå®åœºæ™¯ä¸­è´¨é‡åˆ†å¸ƒéç¨³å®šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>CAQAï¼ˆæŒç»­åŠ¨ä½œè´¨é‡è¯„ä¼°ï¼‰é€šè¿‡é…å¤‡æŒç»­å­¦ä¹ ï¼ˆCLï¼‰èƒ½åŠ›æ¥å¤„ç†ä¸æ–­å˜åŒ–çš„åˆ†å¸ƒã€‚</li>
<li>å…¨å‚æ•°å¾®è°ƒï¼ˆFPFTï¼‰å¯¹äºæœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ æ˜¯å¿…è¦çš„ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œç‰¹å¾æµå½¢åç§»ã€‚</li>
<li>MAGR++é€šè¿‡å¾®è°ƒä¸»å¹²ç½‘ç»œç¨³å®šæµ…å±‚ç½‘ç»œï¼Œå¹¶é€‚åº”æ·±å±‚ç½‘ç»œï¼ŒåŒ…æ‹¬æµå½¢æŠ•å½±å’Œå›¾æ­£åˆ™åŒ–ã€‚</li>
<li>æ„å»ºäº†å››ä¸ªCAQAåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä»ä¸‰ä¸ªæ•°æ®é›†æ„å»ºçš„åŸºå‡†è¯„ä¼°åè®®ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºMAGR++æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad2922f1a35d060753c4d51dae3b7fa5" align="middle">
<img src="https://picx.zhimg.com/v2-f748a32f026f6bb10517a42424cb96f7" align="middle">
<img src="https://picx.zhimg.com/v2-61ed5ccbb82e7bdc2680359151841788" align="middle">
<img src="https://picx.zhimg.com/v2-296027cb882ea5fb2e7d165a778f8743" align="middle">
<img src="https://picx.zhimg.com/v2-3e0bca79593259a4b9114dd27b609be3" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Benchmarking-AI-evolved-cosmological-structure-formation"><a href="#Benchmarking-AI-evolved-cosmological-structure-formation" class="headerlink" title="Benchmarking AI-evolved cosmological structure formation"></a>Benchmarking AI-evolved cosmological structure formation</h2><p><strong>Authors:Xiaofeng Dong, Nesar Ramachandra, Salman Habib, Katrin Heitmann</strong></p>
<p>The potential of deep learning-based image-to-image translations has recently attracted significant attention. One possible application of such a framework is as a fast, approximate alternative to cosmological simulations, which would be particularly useful in various contexts, including covariance studies, investigations of systematics, and cosmological parameter inference. To investigate different aspects of learning-based cosmological mappings, we choose two approaches for generating suitable cosmological matter fields as datasets: a simple analytical prescription provided by the Zelâ€™dovich approximation, and a numerical N-body method using the Particle-Mesh approach. The evolution of structure formation is modeled using U-Net, a widely employed convolutional image translation framework. Because of the lack of a controlled methodology, validation of these learned mappings requires multiple benchmarks beyond simple visual comparisons and summary statistics. A comprehensive list of metrics is considered, including higher-order correlation functions, conservation laws, topological indicators, and statistical independence of density fields. We find that the U-Net approach performs well only for some of these physical metrics, and accuracy is worse at increasingly smaller scales, where the dynamic range in density is large. By introducing a custom density-weighted loss function during training, we demonstrate a significant improvement in the U-Net results at smaller scales. This study provides an example of how a family of physically motivated benchmarks can, in turn, be used to fine-tune optimization schemes â€“ such as the density-weighted loss used here â€“ to significantly enhance the accuracy of scientific machine learning approaches by focusing attention on relevant features. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„æ½œåŠ›æœ€è¿‘å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è¯¥æ¡†æ¶çš„ä¸€ä¸ªå¯èƒ½åº”ç”¨æ˜¯ä½œä¸ºå®‡å®™å­¦æ¨¡æ‹Ÿçš„å¿«é€Ÿè¿‘ä¼¼æ›¿ä»£æ–¹æ¡ˆï¼Œè¿™åœ¨åæ–¹å·®ç ”ç©¶ã€ç³»ç»Ÿè°ƒæŸ¥ä»¥åŠå®‡å®™å­¦å‚æ•°æ¨æ–­ç­‰å„ä¸ªåœºæ™¯ä¸­éƒ½å°†ç‰¹åˆ«æœ‰ç”¨ã€‚ä¸ºäº†ç ”ç©¶åŸºäºå­¦ä¹ çš„å®‡å®™å­¦æ˜ å°„çš„ä¸åŒæ–¹é¢ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸¤ç§ç”Ÿæˆåˆé€‚å®‡å®™ç‰©è´¨åœºä½œä¸ºæ•°æ®é›†çš„æ–¹æ³•ï¼šç”±æ³½å°”å¤šç»´å¥‡è¿‘ä¼¼æä¾›çš„ç®€å•åˆ†æå¤„æ–¹ï¼Œä»¥åŠä½¿ç”¨ç²’å­ç½‘æ ¼æ–¹æ³•çš„æ•°å€¼Nä½“æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨å¹¿å—æ¬¢è¿çš„å·ç§¯å›¾åƒç¿»è¯‘æ¡†æ¶U-Netå¯¹ç»“æ„å½¢æˆçš„æ¼”åŒ–è¿›è¡Œå»ºæ¨¡ã€‚ç”±äºç¼ºä¹å—æ§çš„æ–¹æ³•è®ºï¼Œå¯¹è¿™äº›å­¦ä¹ åˆ°çš„æ˜ å°„çš„éªŒè¯éœ€è¦è¶…è¶Šç®€å•è§†è§‰æ¯”è¾ƒå’Œæ€»ç»“ç»Ÿè®¡çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è€ƒè™‘äº†åŒ…æ‹¬é«˜é˜¶ç›¸å…³å‡½æ•°ã€å®ˆæ’å®šå¾‹ã€æ‹“æ‰‘æŒ‡æ ‡ä»¥åŠå¯†åº¦åœºçš„ç»Ÿè®¡ç‹¬ç«‹æ€§ç­‰åœ¨å†…çš„ç»¼åˆæŒ‡æ ‡ã€‚æˆ‘ä»¬å‘ç°U-Netæ–¹æ³•åœ¨è¿™äº›ç‰©ç†æŒ‡æ ‡ä¸­ä»…è¡¨ç°è‰¯å¥½ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨è¶Šæ¥è¶Šå°çš„å°ºåº¦ä¸Šå‡†ç¡®æ€§è¾ƒå·®ï¼Œå…¶ä¸­å¯†åº¦åŠ¨æ€èŒƒå›´è¾ƒå¤§ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥è‡ªå®šä¹‰çš„å¯†åº¦åŠ æƒæŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬åœ¨è¾ƒå°çš„å°ºåº¦ä¸Šæ˜¾è‘—æé«˜äº†U-Netçš„ç»“æœã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜å¦‚ä½•åè¿‡æ¥ä½¿ç”¨ä¸€ç³»åˆ—ç‰©ç†æ¿€åŠ±åŸºå‡†æµ‹è¯•æ¥å¾®è°ƒä¼˜åŒ–æ–¹æ¡ˆâ€”â€”å¦‚æœ¬æ–‡ä¸­ä½¿ç”¨çš„å¯†åº¦åŠ æƒæŸå¤±â€”â€”é€šè¿‡å…³æ³¨ç›¸å…³ç‰¹å¾æ¥æ˜¾è‘—æé«˜ç§‘å­¦æœºå™¨å­¦ä¹ æ–¹æ³•å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06731v1">PDF</a> Expanded and thoroughly revised version of our prior NeurIPS   submission (arXiv:2112.05681; which has no DOI), with new sections,   experiments, and analyses</p>
<p><strong>Summary</strong>ï¼šåŸºäºæ·±åº¦å­¦ä¹ å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„æ½œåŠ›æœ€è¿‘å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å…¶å¯èƒ½çš„åº”ç”¨ä¹‹ä¸€æ˜¯ä½œä¸ºä¸€ç§å¿«é€Ÿè¿‘ä¼¼æ›¿ä»£å®‡å®™å­¦æ¨¡æ‹Ÿï¼Œåœ¨å„ç§æƒ…å†µä¸‹å‡ç‰¹åˆ«æœ‰ç”¨ï¼ŒåŒ…æ‹¬åæ–¹å·®ç ”ç©¶ã€ç³»ç»Ÿè°ƒæŸ¥ä»¥åŠå®‡å®™å­¦å‚æ•°æ¨æ–­ã€‚æˆ‘ä»¬ä½¿ç”¨U-Netå·ç§¯å›¾åƒç¿»è¯‘æ¡†æ¶æ¨¡æ‹Ÿç»“æ„å½¢æˆæ¼”åŒ–è¿‡ç¨‹ï¼Œå¹¶é€‰æ‹©ä¸¤ç§ç”Ÿæˆé€‚å½“å®‡å®™å­¦ç‰©è´¨åœºæ•°æ®é›†çš„æ–¹æ³•ï¼šç”±æ³½å°”å¤šç»´å¥‡è¿‘ä¼¼æä¾›çš„ç®€å•è§£æå…¬å¼å’Œé‡‡ç”¨ç²’å­ç½‘æ ¼æ–¹æ³•çš„æ•°å€¼Nä½“æ–¹æ³•ã€‚ç”±äºç¼ºä¹æ§åˆ¶æ–¹æ³•ï¼Œè¿™äº›å­¦ä¹ æ˜ å°„çš„éªŒè¯éœ€è¦è¶…è¶Šç®€å•è§†è§‰æ¯”è¾ƒå’Œæ€»ç»“ç»Ÿè®¡æŒ‡æ ‡çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶è€ƒè™‘äº†é«˜é˜¶ç›¸å…³å‡½æ•°ã€å®ˆæ’å®šå¾‹ã€æ‹“æ‰‘æŒ‡æ ‡å’Œå¯†åº¦åœºçš„ç»Ÿè®¡ç‹¬ç«‹æ€§ç­‰ç»¼åˆæŒ‡æ ‡æ¸…å•ã€‚æˆ‘ä»¬å‘ç°U-Netæ–¹æ³•å¯¹äºè¿™äº›ç‰©ç†æŒ‡æ ‡çš„æŸäº›éƒ¨åˆ†è¡¨ç°è‰¯å¥½ï¼Œä½†ç²¾åº¦åœ¨è¾ƒå°çš„å°ºåº¦ä¸Šæ›´å·®ï¼Œå¯†åº¦åŠ¨æ€èŒƒå›´è¾ƒå¤§ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥è‡ªå®šä¹‰å¯†åº¦åŠ æƒæŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬åœ¨è¾ƒå°çš„å°ºåº¦ä¸Šæ˜¾è‘—æé«˜äº†U-Netçš„ç»“æœã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä¸€ç³»åˆ—ç‰©ç†åŸºå‡†æµ‹è¯•å¯ä»¥åè¿‡æ¥ç”¨äºè°ƒæ•´ä¼˜åŒ–æ–¹æ¡ˆï¼Œä¾‹å¦‚æ­¤å¤„ä½¿ç”¨çš„å¯†åº¦åŠ æƒæŸå¤±ï¼Œä»¥å…³æ³¨ç›¸å…³ç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—æé«˜ç§‘å­¦æœºå™¨å­¦ä¹ çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ·±åº¦å­¦ä¹ å›¾åƒåˆ°å›¾åƒè½¬æ¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ‹Ÿå®‡å®™å­¦æ¨¡æ‹Ÿæ–¹é¢è¡¨ç°å‡ºå¿«é€Ÿè¿‘ä¼¼æ›¿ä»£çš„ä¼˜åŠ¿ã€‚</li>
<li>ä½¿ç”¨U-Netæ¡†æ¶æ¨¡æ‹Ÿç»“æ„å½¢æˆæ¼”åŒ–è¿‡ç¨‹ï¼Œå¹¶é€‰æ‹©ä¸¤ç§ç”Ÿæˆå®‡å®™å­¦ç‰©è´¨åœºæ•°æ®é›†çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å¯¹äºæ­¤ç±»å­¦ä¹ æ˜ å°„çš„éªŒè¯éœ€è¦è¶…è¶Šç®€å•è§†è§‰æ¯”è¾ƒçš„å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é«˜é˜¶ç›¸å…³å‡½æ•°ç­‰ç»¼åˆè€ƒè™‘çš„æŒ‡æ ‡æ¸…å•ã€‚</li>
<li>U-Netåœ¨æŸäº›ç‰©ç†æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¸ä¸€ï¼Œåœ¨å°å°ºåº¦ä¸Šç²¾åº¦è¾ƒå·®ï¼Œå› æ­¤éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å’Œè°ƒæ•´æ–¹æ³•ä»¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥è‡ªå®šä¹‰å¯†åº¦åŠ æƒæŸå¤±å‡½æ•°ï¼Œå¯ä»¥åœ¨è¾ƒå°çš„å°ºåº¦ä¸Šæ˜¾è‘—æé«˜U-Netçš„ç»“æœã€‚</li>
<li>ç‰©ç†åŸºå‡†æµ‹è¯•å¯ä»¥ç”¨äºè°ƒæ•´å’Œä¼˜åŒ–æœºå™¨å­¦ä¹ ç®—æ³•çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»¥å…³æ³¨ç›¸å…³ç‰¹å¾å¹¶å¢å¼ºå‡†ç¡®æ€§ã€‚è¿™å¯¹äºç§‘å­¦æœºå™¨å­¦ä¹ çš„å‘å±•è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f923d94afc35d60485e7cf304bf7e197" align="middle">
<img src="https://picx.zhimg.com/v2-dd501b03ff2ba0cea5403fd25544bb8e" align="middle">
<img src="https://picx.zhimg.com/v2-46e19434ee3f8a8220b28f30ca8dcdc8" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Personalizing-Retrieval-using-Joint-Embeddings-or-â€œthe-Return-of-Fluffyâ€"><a href="#Personalizing-Retrieval-using-Joint-Embeddings-or-â€œthe-Return-of-Fluffyâ€" class="headerlink" title="Personalizing Retrieval using Joint Embeddings or â€œthe Return of Fluffyâ€"></a>Personalizing Retrieval using Joint Embeddings or â€œthe Return of Fluffyâ€</h2><p><strong>Authors:Bruno Korbar, Andrew Zisserman</strong></p>
<p>The goal of this paper is to be able to retrieve images using a compound query that combines object instance information from an image, with a natural text description of what that object is doing or where it is. For example, to retrieve an image of â€œFluffy the unicorn (specified by an image) on someoneâ€™s headâ€. To achieve this we design a mapping network that can â€œtranslateâ€ from a local image embedding (of the object instance) to a text token, such that the combination of the token and a natural language query is suitable for CLIP style text encoding, and image retrieval. Generating a text token in this manner involves a simple training procedure, that only needs to be performed once for each object instance. We show that our approach of using a trainable mapping network, termed pi-map, together with frozen CLIP text and image encoders, improves the state of the art on two benchmarks designed to assess personalized retrieval. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯é€šè¿‡ç»“åˆå›¾åƒä¸­çš„å¯¹è±¡å®ä¾‹ä¿¡æ¯ä»¥åŠè¯¥å¯¹è±¡æ­£åœ¨åšä»€ä¹ˆæˆ–åœ¨å“ªé‡Œçš„è‡ªç„¶æ–‡æœ¬æè¿°ï¼Œæ¥æ„å»ºä¸€ä¸ªå¤åˆæŸ¥è¯¢ï¼Œä»è€Œèƒ½å¤Ÿæ£€ç´¢å›¾åƒã€‚ä¾‹å¦‚ï¼Œæ£€ç´¢ä¸€å¼ â€œç‹¬è§’å…½Fluffyï¼ˆç”±å›¾åƒæŒ‡å®šï¼‰åœ¨æŸäººçš„å¤´ä¸Šâ€çš„å›¾åƒã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ˜ å°„ç½‘ç»œï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿå°†å±€éƒ¨å›¾åƒåµŒå…¥ï¼ˆå¯¹è±¡å®ä¾‹ï¼‰è½¬åŒ–ä¸ºä¸€ä¸ªæ–‡æœ¬ä»¤ç‰Œï¼Œä½¿è¯¥ä»¤ç‰Œä¸è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç›¸ç»“åˆï¼Œé€‚åˆCLIPé£æ ¼çš„æ–‡æœ¬ç¼–ç å’Œå›¾åƒæ£€ç´¢ã€‚é€šè¿‡è¿™ç§æ–¹å¼ç”Ÿæˆæ–‡æœ¬ä»¤ç‰Œæ¶‰åŠåˆ°ä¸€ä¸ªç®€å•çš„è®­ç»ƒè¿‡ç¨‹ï¼Œåªéœ€è¦é’ˆå¯¹æ¯ä¸ªå¯¹è±¡å®ä¾‹æ‰§è¡Œä¸€æ¬¡ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨å¯è®­ç»ƒçš„æ˜ å°„ç½‘ç»œï¼ˆç§°ä¸ºpi-mapï¼‰ä¸å†»ç»“çš„CLIPæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œåœ¨ä¸¤ç§ç”¨äºè¯„ä¼°ä¸ªæ€§åŒ–æ£€ç´¢çš„åŸºå‡†æµ‹è¯•ä¸Šå‡æ”¹è¿›äº†ç°æœ‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05411v1">PDF</a> Published as an oral in CBMI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç»“åˆå›¾åƒä¸­çš„å¯¹è±¡å®ä¾‹ä¿¡æ¯ä¸å¯¹è±¡è¡Œä¸ºæˆ–ä½ç½®çš„æ–‡æœ¬æè¿°ï¼Œå®ç°ä½¿ç”¨å¤åˆæŸ¥è¯¢æ£€ç´¢å›¾åƒã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ˜ å°„ç½‘ç»œï¼Œèƒ½å°†å±€éƒ¨å›¾åƒåµŒå…¥è½¬æ¢ä¸ºæ–‡æœ¬ä»¤ç‰Œï¼Œä½¿è¯¥ä»¤ç‰Œä¸è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„ç»“åˆé€‚ç”¨äºCLIPé£æ ¼çš„æ–‡æœ¬ç¼–ç å’Œå›¾åƒæ£€ç´¢ã€‚é‡‡ç”¨åä¸ºpi-mapçš„å¯è®­ç»ƒæ˜ å°„ç½‘ç»œï¼Œç»“åˆé¢„è®­ç»ƒçš„CLIPæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ï¼Œæé«˜äº†ä¸ªæ€§åŒ–æ£€ç´¢çš„ä¸¤ä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç»“åˆå›¾åƒä¸­çš„å¯¹è±¡å®ä¾‹ä¿¡æ¯å’Œæ–‡æœ¬æè¿°å®ç°å¤åˆæŸ¥è¯¢çš„å›¾åƒæ£€ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºpi-mapçš„æ˜ å°„ç½‘ç»œè®¾è®¡ï¼Œå°†å›¾åƒåµŒå…¥è½¬æ¢ä¸ºæ–‡æœ¬ä»¤ç‰Œã€‚</li>
<li>è¿™ç§è½¬æ¢æ–¹æ³•é€‚ç”¨äºCLIPé£æ ¼çš„æ–‡æœ¬ç¼–ç å’Œå›¾åƒæ£€ç´¢ã€‚</li>
<li>æ–‡ä¸­ä½¿ç”¨çš„è®­ç»ƒç¨‹åºä»…éœ€é’ˆå¯¹æ¯ä¸ªå¯¹è±¡å®ä¾‹æ‰§è¡Œä¸€æ¬¡å³å¯ç”Ÿæˆæ–‡æœ¬ä»¤ç‰Œã€‚</li>
<li>pi-mapæ˜ å°„ç½‘ç»œä¸é¢„è®­ç»ƒçš„CLIPæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ç›¸ç»“åˆï¼Œæé«˜äº†ä¸ªæ€§åŒ–æ£€ç´¢çš„æ€§èƒ½ã€‚</li>
<li>æ–‡ä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æŠ€æœ¯ã€‚</li>
<li>æ­¤æ–¹æ³•æä¾›äº†ä¸€ç§æ›´ç²¾å‡†çš„å›¾åƒæ£€ç´¢é€”å¾„ï¼Œèƒ½å¤Ÿæ ¹æ®å›¾åƒä¸­çš„ç‰¹å®šå¯¹è±¡åŠå…¶ä¸Šä¸‹æ–‡ï¼ˆå¦‚å¯¹è±¡çš„è¡Œä¸ºæˆ–ä½ç½®ï¼‰è¿›è¡Œæ£€ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46190353569a3358246c525f5a48ad04" align="middle">
<img src="https://picx.zhimg.com/v2-48fd04dd6e45530ee7f8691db8e5bc89" align="middle">
<img src="https://picx.zhimg.com/v2-8be70d339a35bef98d51a546efda35e8" align="middle">
<img src="https://picx.zhimg.com/v2-d0565096e3e725dae88129b664889c44" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bridging-Clinical-Narratives-and-ACR-Appropriateness-Guidelines-A-Multi-Agent-RAG-System-for-Medical-Imaging-Decisions"><a href="#Bridging-Clinical-Narratives-and-ACR-Appropriateness-Guidelines-A-Multi-Agent-RAG-System-for-Medical-Imaging-Decisions" class="headerlink" title="Bridging Clinical Narratives and ACR Appropriateness Guidelines: A   Multi-Agent RAG System for Medical Imaging Decisions"></a>Bridging Clinical Narratives and ACR Appropriateness Guidelines: A   Multi-Agent RAG System for Medical Imaging Decisions</h2><p><strong>Authors:Satrio Pambudi, Filippo Menolascina</strong></p>
<p>The selection of appropriate medical imaging procedures is a critical and complex clinical decision, guided by extensive evidence-based standards such as the ACR Appropriateness Criteria (ACR-AC). However, the underutilization of these guidelines, stemming from the difficulty of mapping unstructured patient narratives to structured criteria, contributes to suboptimal patient outcomes and increased healthcare costs. To bridge this gap, we introduce a multi-agent cognitive architecture that automates the translation of free-text clinical scenarios into specific, guideline-adherent imaging recommendations. Our system leverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on a synthetically generated dataset of 8,840 clinical scenario-recommendation pairs to achieve highly accurate information retrieval from the ACR-AC knowledge base. This retriever identifies candidate guidelines with a 93.9% top-10 recall, which are then processed by a sequence of LLM-based agents for selection and evidence-based synthesis. We evaluate our architecture using GPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match accuracy of 81%, meaning that in 81% of test cases the predicted procedure set was identical to the guidelineâ€™s reference set, and an F1-score of 0.879. This represents a 67-percentage-point absolute improvement in accuracy over a strong standalone GPT-4.1 baseline, underscoring the contribution that our architecture makes to a frontier model. These results were obtained on a challenging test set with substantial lexical divergence from the source guidelines. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/demo-iclr-B567/">https://anonymous.4open.science/r/demo-iclr-B567/</a> </p>
<blockquote>
<p>é€‰æ‹©åˆé€‚çš„åŒ»å­¦å½±åƒç¨‹åºæ˜¯ä¸€é¡¹é‡è¦è€Œå¤æ‚çš„ä¸´åºŠå†³ç­–ï¼Œç”±å¦‚ACRé€‚å®œæ€§æ ‡å‡†ï¼ˆACR-ACï¼‰ç­‰å¹¿æ³›çš„å¾ªè¯æ ‡å‡†æ‰€æŒ‡å¯¼ã€‚ç„¶è€Œï¼Œç”±äºå¯¹å°†éç»“æ„åŒ–æ‚£è€…å™äº‹æ˜ å°„åˆ°ç»“æ„åŒ–æ ‡å‡†çš„éš¾åº¦è¾ƒé«˜ï¼Œè¿™äº›æŒ‡å—çš„ä½¿ç”¨ä¸è¶³å¯¼è‡´æ‚£è€…ç»“æœä¸ç†æƒ³å’ŒåŒ»ç–—ä¿å¥æˆæœ¬å¢åŠ ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ™ºèƒ½ä½“è®¤çŸ¥æ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥è‡ªåŠ¨å°†è‡ªç”±æ–‡æœ¬ä¸´åºŠæƒ…æ™¯è½¬åŒ–ä¸ºå…·ä½“ã€ç¬¦åˆæŒ‡å—çš„å½±åƒå»ºè®®ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé‡‡ç”¨äº†ä¸€ç§æ–°é¢–ã€é€‚åº”é¢†åŸŸçš„å¯†é›†æ£€ç´¢æ¨¡å‹ColBERTï¼Œå®ƒåœ¨8840ä¸ªä¸´åºŠæƒ…æ™¯å»ºè®®å¯¹ä¸Šåˆæˆçš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥ä»ACR-ACçŸ¥è¯†åº“ä¸­å®ç°é«˜åº¦å‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢ã€‚è¯¥æ£€ç´¢å™¨ä»¥93.9%çš„å‰åå¬å›ç‡è¯†åˆ«å‡ºå€™é€‰æŒ‡å—ï¼Œè¿™äº›æŒ‡å—éšåè¢«ä¸€ç³»åˆ—åŸºäºLLMçš„æ™ºèƒ½ä½“å¤„ç†å’Œå¾ªè¯ç»¼åˆè¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨GPT 4.1å’ŒMedGemmaæ™ºèƒ½ä½“è¯„ä¼°æˆ‘ä»¬çš„æ¶æ„ï¼Œè¡¨ç°å‡ºæœ€å…ˆè¿›çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä¸º81%ï¼Œè¿™æ„å‘³ç€åœ¨81%çš„æµ‹è¯•æ¡ˆä¾‹ä¸­ï¼Œé¢„æµ‹çš„ç¨‹åºé›†ä¸æŒ‡å—å‚è€ƒé›†ç›¸åŒï¼Œå¹¶ä¸”F1åˆ†æ•°ä¸º0.879ã€‚è¿™ä»£è¡¨ç€ç›¸å¯¹äºå¼ºå¤§çš„ç‹¬ç«‹GPT 4.1åŸºå‡†çº¿ï¼Œå‡†ç¡®åº¦æé«˜äº†67ä¸ªç™¾åˆ†ç‚¹ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ¶æ„æ‰€å¸¦æ¥çš„è´¡çŒ®ã€‚è¿™äº›ç»“æœæ˜¯åœ¨å…·æœ‰ä¸æºæŒ‡å—æ˜¾è‘—è¯æ±‡å·®å¼‚çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•é›†ä¸Šè·å¾—çš„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/demo-iclr-B567/]%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://anonymous.4open.science/r/demo-iclr-B567/]è¿›è¡Œè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04969v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é‡‡ç”¨å¤šæ™ºèƒ½ä½“è®¤çŸ¥æ¶æ„ï¼Œç»“åˆæ–°å‹å¯†é›†æ£€ç´¢æ¨¡å‹ColBERTï¼Œå®ç°è‡ªç”±æ–‡æœ¬ä¸´åºŠæƒ…æ™¯å‘ç‰¹å®šã€ç¬¦åˆæŒ‡å—çš„æˆåƒå»ºè®®çš„è‡ªåŠ¨è½¬åŒ–ã€‚é€šè¿‡åˆæˆæ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°å¯¹ACR-ACçŸ¥è¯†åº“çš„é«˜åº¦å‡†ç¡®ä¿¡æ¯æ£€ç´¢ï¼Œå…·å¤‡æ™ºèƒ½æŒ‡å—é€‰æ‹©åŠŸèƒ½ã€‚ç ”ç©¶å‡†ç¡®ç‡é«˜ï¼Œæœ‰åŠ©äºå‡å°‘åŒ»æ‚£å†³ç­–å¤æ‚æ€§å’Œé™ä½è¯Šç–—æˆæœ¬ã€‚ç ”ç©¶æˆæœæå‡å†³ç­–æ¨¡å‹èƒ½åŠ›è¾¾åˆ°ä¸šå†…é¢†å…ˆæ°´å¹³ã€‚è¯¦ç»†æˆæœåŠä»£ç è§å…¬å¼€é“¾æ¥ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—æˆåƒç¨‹åºçš„é€‰æ‹©è‡³å…³é‡è¦ä¸”å¤æ‚ï¼Œéœ€ä¾èµ–è¯æ®åŸºç¡€çš„å‡†åˆ™å¦‚ACRé€‚å®œæ€§æ ‡å‡†ï¼ˆACR-ACï¼‰ã€‚</li>
<li>å­˜åœ¨å¯¹æŒ‡å—çš„åˆ©ç”¨ä¸è¶³é—®é¢˜ï¼Œä¸»è¦ç”±äºæ‚£è€…å™äº‹ç»“æ„åŒ–ä¸æ˜ å°„çš„éš¾åº¦å¢åŠ å¯¼è‡´ã€‚è¿™ä¸€é—®é¢˜å¯¼è‡´äº†ä¸è‰¯æ‚£è€…ç»“å±€å’ŒåŒ»ç–—æˆæœ¬å¢åŠ ã€‚</li>
<li>æå‡ºä¸€ç§å¤šæ™ºèƒ½ä½“è®¤çŸ¥æ¶æ„ï¼Œå®ç°è‡ªç”±æ–‡æœ¬ä¸´åºŠæƒ…æ™¯å‘æŒ‡å—æ”¯æŒçš„æˆåƒå»ºè®®è½¬åŒ–ã€‚è¯¥æ¶æ„åŒ…æ‹¬ä¸€ä¸ªæ–°å‹å¯†é›†æ£€ç´¢æ¨¡å‹ColBERTå’Œä¸€ä¸ªç²¾ç»†è°ƒæ•´çš„åˆæˆæ•°æ®é›†ã€‚</li>
<li>ColBERTæ¨¡å‹èƒ½å¤Ÿä»ACR-ACçŸ¥è¯†åº“ä¸­å‡†ç¡®æ£€ç´¢ä¿¡æ¯ï¼Œåœ¨å€™é€‰æŒ‡å—çš„å¬å›ç‡ä¸Šè¡¨ç°å‡ºæé«˜çš„æ€§èƒ½ã€‚ </li>
<li>é‡‡ç”¨ä¸€ç³»åˆ—åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“è¿›è¡Œé€‰æ‹©å’Œè¯æ®ç»¼åˆï¼Œè¯„ä¼°æ¶æ„ä½¿ç”¨GPT-4.1å’ŒMedGemmaæ™ºèƒ½ä½“ã€‚ </li>
<li>è¯¥æ¶æ„å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œé¢„æµ‹ç¨‹åºé›†ä¸æŒ‡å—å‚è€ƒé›†ä¹‹é—´çš„ç²¾ç¡®åŒ¹é…ç‡é«˜è¾¾81%ï¼Œå¹¶ä¸”ç›¸å¯¹äºå•ä¸€çš„GPT-4.1åŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90bf4784b060fe2b3e222d44a79c821e" align="middle">
<img src="https://picx.zhimg.com/v2-e0d2c7931fc2e93dcae5d32ea0a7ab35" align="middle">
<img src="https://picx.zhimg.com/v2-5d4976ba0f383e316f026851486f161b" align="middle">
<img src="https://picx.zhimg.com/v2-6cb359ed2cabc3ac8ad76c24c750a05b" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bidirectional-Mammogram-View-Translation-with-Column-Aware-and-Implicit-3D-Conditional-Diffusion"><a href="#Bidirectional-Mammogram-View-Translation-with-Column-Aware-and-Implicit-3D-Conditional-Diffusion" class="headerlink" title="Bidirectional Mammogram View Translation with Column-Aware and Implicit   3D Conditional Diffusion"></a>Bidirectional Mammogram View Translation with Column-Aware and Implicit   3D Conditional Diffusion</h2><p><strong>Authors:Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang</strong></p>
<p>Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics. </p>
<blockquote>
<p>åŒè§†è§’ä¹³è…ºæ‘„å½±ï¼ˆåŒ…æ‹¬é¢…å°¾å‘ï¼ˆCCï¼‰å’Œå†…å¤–æ–œä½ï¼ˆMLOï¼‰æŠ•å½±ï¼‰ä¸ºä¹³è…ºç™Œè¯Šæ–­æä¾›äº†é‡è¦çš„äº’è¡¥è§£å‰–è§†è§’ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œç”±äºé‡‡é›†é”™è¯¯æˆ–å‹ç¼©ä¼ªå½±ç­‰åŸå› ï¼Œå¯èƒ½ä¼šå¯¼è‡´æŸä¸€è§†è§’ç¼ºå¤±ã€æŸåæˆ–é€€åŒ–ï¼Œä»è€Œé™åˆ¶ä¸‹æ¸¸åˆ†æçš„æœ‰æ•ˆæ€§ã€‚è§†è§’åˆ°è§†è§’çš„ç¿»è¯‘å¯ä»¥å¸®åŠ©æ¢å¤ç¼ºå¤±çš„è§†è§’å¹¶æé«˜ç—…å˜å¯¹é½ã€‚ç„¶è€Œï¼Œä¸å¤©ç„¶å›¾åƒç›¸æ¯”ï¼Œè¿™é¡¹ä»»åŠ¡åœ¨ä¹³è…ºæ‘„å½±ä¸­æ˜¯æå…·æŒ‘æˆ˜æ€§çš„ï¼ŒåŸå› åœ¨äºXå°„çº¿æŠ•å½±ä¸­å­˜åœ¨è¾ƒå¤§çš„éåˆšæ€§å˜å½¢å’Œä¸¥é‡çš„ç»„ç»‡é‡å ï¼Œè¿™æ©ç›–äº†åƒç´ çº§åˆ«çš„å¯¹åº”å…³ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ—æ„ŸçŸ¥å’Œéšå¼3Dæ‰©æ•£ï¼ˆCA3D-Diffï¼‰æ–°å‹åŒå‘ä¹³è…ºæ‘„å½±è§†è§’ç¿»è¯‘æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è·¨è§†è§’ç»“æ„ä¸å¯¹é½çš„é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨å‡ ä½•å±æ€§ï¼Œå³è§£å‰–å¯¹åº”çš„åŒºåŸŸå€¾å‘äºä½äºä¸åŒè§†è§’çš„ç›¸ä¼¼åˆ—ä½ç½®ã€‚åº”ç”¨é«˜æ–¯è¡°å‡åå·®æ¥å¼ºè°ƒå±€éƒ¨åˆ—ç›¸å…³æ€§ï¼ŒåŒæ—¶æŠ‘åˆ¶è¿œè·ç¦»ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéšå¼3Dç»“æ„é‡å»ºæ¨¡å—ï¼Œè¯¥æ¨¡å—å°†å™ªå£°çš„2Dæ½œåœ¨å˜é‡æŠ•å½±å›åŸºäºä¹³è…ºè§†å›¾æŠ•å½±å‡ ä½•çš„ç²—ç³™3Dç‰¹å¾ä½“ç§¯ã€‚é‡å»ºçš„3Dç»“æ„ç»è¿‡ç»†åŒ–å¹¶æ³¨å…¥å»å™ªUNetä¸­ï¼Œä»¥æŒ‡å¯¼å…·æœ‰å¢å¼ºè§£å‰–æ„è¯†çš„è·¨è§†è§’ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCA3D-Diffåœ¨åŒå‘ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆæˆçš„è§†è§’æœ‰æ•ˆåœ°æé«˜äº†å•è§†è§’æ¶æ€§åˆ†ç±»åœ¨ç­›æŸ¥ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œè¯Šæ–­ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04947v1">PDF</a> BIBM2025 accept, 8 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒè§†è§’ä¹³è…ºæ‘„å½±ï¼ˆåŒ…æ‹¬é¢…å°¾ï¼ˆCCï¼‰å’Œä¾§æ–œï¼ˆMLOï¼‰æŠ•å½±ï¼‰ä¸ºä¹³è…ºç™Œè¯Šæ–­æä¾›äº†äº’è¡¥çš„è§£å‰–è§†å›¾ï¼Œä½†åœ¨ç°å®çš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œä¸€ä¸ªè§†å›¾å¯èƒ½ä¼šç¼ºå¤±ã€æŸåæˆ–é€€åŒ–ã€‚å› æ­¤ï¼Œè§†å›¾åˆ°è§†å›¾çš„è½¬æ¢å¯ä»¥å¸®åŠ©æ¢å¤ç¼ºå¤±çš„è§†å›¾å¹¶æé«˜ç—…å˜å¯¹é½çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åŒå‘ä¹³è…ºæ‘„å½±è§†å›¾è½¬æ¢æ¡†æ¶â€”â€”åˆ—æ„ŸçŸ¥ä¸éšå¼ä¸‰ç»´æ‰©æ•£ï¼ˆCA3D-Diffï¼‰ã€‚ä¸ºè§£å†³è·¨è§†å›¾ç»“æ„é”™ä½é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨å‡ ä½•å±æ€§åœ¨è§£å‰–ä¸Šå¯¹åº”çš„åŒºåŸŸå€¾å‘äºä½äºç›¸ä¼¼åˆ—ä½ç½®çš„ç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªéšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—ï¼Œå°†å™ªå£°äºŒç»´æ½œåœ¨å˜é‡åæŠ•å½±åˆ°åŸºäºä¹³è…ºè§†å›¾æŠ•å½±å‡ ä½•çš„ç²—ç³™ä¸‰ç»´ç‰¹å¾ä½“ç§¯ä¸Šã€‚å®éªŒè¡¨æ˜ï¼ŒCA3D-Diffåœ¨åŒå‘ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆæˆçš„è§†å›¾æœ‰æ•ˆåœ°æé«˜äº†å•è§†å›¾æ¶æ€§åˆ†ç±»åœ¨ç­›æŸ¥ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œæ˜¾ç¤ºäº†è¯¥æ–¹æ³•åœ¨ç°å®è¯Šæ–­ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ul>
<li>åŒè§†è§’ä¹³è…ºæ‘„å½±å¯¹ä¹³è…ºç™Œè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†åœ¨å®é™…ä¸´åºŠä¸­å¯èƒ½å‡ºç°è§†å›¾ç¼ºå¤±ã€æŸåæˆ–é€€åŒ–çš„é—®é¢˜ã€‚</li>
<li>è§†å›¾åˆ°è§†å›¾çš„è½¬æ¢èƒ½æœ‰æ•ˆæ¢å¤ç¼ºå¤±çš„è§†å›¾å¹¶æé«˜ç—…å˜å¯¹é½çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„CA3D-Diffæ¡†æ¶ç»“åˆäº†åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œéšå¼ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œå¤„ç†è·¨è§†å›¾çš„ç»“æ„é”™ä½é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼ºè°ƒå±€éƒ¨åˆ—ç›¸å…³æ€§å¹¶æŠ‘åˆ¶è¿œè·ç¦»ä¸åŒ¹é…ï¼Œè®¾è®¡äº†ä¸€ç§åˆ—æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>éšå¼ä¸‰ç»´ç»“æ„é‡å»ºæ¨¡å—å°†å™ªå£°äºŒç»´æ½œåœ¨å˜é‡åæŠ•å½±åˆ°åŸºäºä¹³è…ºè§†å›¾æŠ•å½±å‡ ä½•çš„ä¸‰ç»´ç‰¹å¾ä½“ç§¯ä¸Šï¼Œæé«˜äº†è§£å‰–æ„è¯†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCA3D-Diffåœ¨è§†è§‰ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œåˆæˆçš„è§†å›¾èƒ½æœ‰æ•ˆæé«˜å•è§†å›¾æ¶æ€§åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a75e458e6fc579d5c492765f859b03b6" align="middle">
<img src="https://picx.zhimg.com/v2-93a208d897fe77db372e7cdabe294d5b" align="middle">
<img src="https://picx.zhimg.com/v2-9671e24e77be6b552fd67430d0cc95ff" align="middle">
<img src="https://picx.zhimg.com/v2-965f1a926fc511d961f8b05cad77b4b7" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-best-performance-in-the-CARE-2025-â€“-Liver-Task-LiSeg-Contrast-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation"><a href="#The-best-performance-in-the-CARE-2025-â€“-Liver-Task-LiSeg-Contrast-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation" class="headerlink" title="The best performance in the CARE 2025 â€“ Liver Task (LiSeg-Contrast):   Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and   Test-Time Adaptation"></a>The best performance in the CARE 2025 â€“ Liver Task (LiSeg-Contrast):   Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and   Test-Time Adaptation</h2><p><strong>Authors:Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang</strong></p>
<p>Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions. </p>
<blockquote>
<p>ç²¾ç¡®çš„è‚è„åˆ†æ®µåœ¨å¢å¼ºå‹æ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­å¯¹è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨æ•°æ®æœ‰é™ã€å¢å¼ºåè®®å¼‚è´¨åŒ–ä»¥åŠæ‰«æä»ªå’Œæœºæ„é—´çš„é¢†åŸŸå·®å¼‚æ˜¾è‘—ï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆ°å›¾åƒè½¬æ¢æ¡†æ¶åœ¨é¢†åŸŸé€šç”¨åŒ–æ–¹é¢å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†å…¶åº”ç”¨å¹¶ä¸ç›´æ¥ã€‚ä¾‹å¦‚ï¼ŒPix2Pixéœ€è¦è¿›è¡Œå›¾åƒæ³¨å†Œï¼Œè€Œå¾ªç¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcycle-GANï¼‰æ— æ³•æ— ç¼é›†æˆåˆ°åˆ†æ®µç®¡é“ä¸­ã€‚åŒæ—¶ï¼Œè¿™äº›æ–¹æ³•æœ€åˆç”¨äºå¤„ç†è·¨æ¨¡æ€åœºæ™¯ï¼Œå¾€å¾€ä¼šå¼•å…¥ç»“æ„å¤±çœŸå¹¶é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¿™å¯èƒ½åœ¨æˆ‘ä»¬çš„å•æ¨¡æ€åœºæ™¯ä¸­æ„æˆç¼ºç‚¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoSSeg-TTAï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘çš„åˆ†æ®µæ¡†æ¶ï¼Œç”¨äºå¤„ç†Gd-EOB-DTPAå¢å¼ºçš„è‚èƒ†æœŸMRIæ¨¡æ€çš„GEDæ•°æ®ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨nnU-Netv2çš„åŸºç¡€ä¸Šï¼Œå¹¶å€ŸåŠ©åŠç›‘ç£å¹³å‡æ•™å¸ˆæ–¹æ¡ˆå¼€å‘å¤§é‡æœªæ ‡è®°ä½“ç§¯æ•°æ®ã€‚é¢†åŸŸé€‚åº”æ¨¡å—ç»“åˆäº†åŸºäºéšæœºç›´æ–¹å›¾çš„é£æ ¼å¤–è§‚è½¬æ¢å‡½æ•°å’Œå¯è®­ç»ƒçš„å¯¹æ¯”æ„ŸçŸ¥ç½‘ç»œï¼Œä¸°å¯Œäº†é¢†åŸŸå¤šæ ·æ€§å¹¶å‡è½»äº†è·¨ä¸­å¿ƒå·®å¼‚æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¿ç»­æµ‹è¯•æ—¶é—´é€‚åº”ç­–ç•¥ï¼Œä»¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºnnU-Netv2åŸºçº¿ï¼Œåœ¨è¾¾åˆ°è¾ƒé«˜çš„Diceåˆ†æ•°å’ŒHausdorffè·ç¦»çš„åŒæ—¶ï¼Œåœ¨ä½æ³¨é‡Šæ¡ä»¶ä¸‹å¯¹æœªè§é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04243v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹GED4æ¨¡æ€çš„ç´§å‡‘åˆ†å‰²æ¡†æ¶CoSSeg-TTAï¼Œç”¨äºä»å¢å¼ºMRIä¸­å‡†ç¡®åˆ†å‰²è‚è„ã€‚è¯¥æ¡†æ¶åŸºäºnnU-Netv2ï¼Œé‡‡ç”¨åŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°ä½“ç§¯æ•°æ®ã€‚é€šè¿‡åŸŸé€‚åº”æ¨¡å—å’ŒæŒç»­æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥ï¼Œæé«˜æ¡†æ¶çš„åŸŸå¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé™ä½è·¨ä¸­å¿ƒå˜å¼‚æ€§çš„å½±å“ï¼Œå®ç°æ›´å¥½çš„è¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoSSeg-TTAæ¡†æ¶è¢«æå‡ºï¼Œç”¨äºä»å¢å¼ºMRIä¸­å‡†ç¡®åˆ†å‰²è‚è„ï¼Œç‰¹åˆ«é€‚ç”¨äºGED4æ¨¡æ€ã€‚</li>
<li>æ¡†æ¶åŸºäºnnU-Netv2ï¼Œç»“åˆåŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡è®°ä½“ç§¯æ•°æ®ã€‚</li>
<li>åŸŸé€‚åº”æ¨¡å—é€šè¿‡éšæœºç›´æ–¹å›¾æ ·å¼è½¬æ¢å‡½æ•°å’Œå¯è®­ç»ƒå¯¹æ¯”ç½‘ç»œï¼Œæé«˜åŸŸå¤šæ ·æ€§å¹¶å‡è½»è·¨ä¸­å¿ƒå˜å¼‚æ€§ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨æŒç»­æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥ï¼Œæé«˜æ¨ç†é˜¶æ®µçš„ç¨³å¥æ€§ã€‚</li>
<li>ä¸nnU-Netv2åŸºçº¿ç›¸æ¯”ï¼ŒCoSSeg-TTAæ¡†æ¶åœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»ä¸Šè¡¨ç°æ›´ä¼˜è¶Šã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä½æ ‡æ³¨æ¡ä»¶ä¸‹å¯¹æœªè§é¢†åŸŸå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3a3d7e77135245aaacf48d35f7b8cdf" align="middle">
<img src="https://picx.zhimg.com/v2-fd93b6d3ead1a4ebf9ee116bf4377edf" align="middle">
<img src="https://picx.zhimg.com/v2-4759b31fe7fc13d067ccf4222b0acdc1" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="BLADE-Bias-Linked-Adaptive-DEbiasing"><a href="#BLADE-Bias-Linked-Adaptive-DEbiasing" class="headerlink" title="BLADE: Bias-Linked Adaptive DEbiasing"></a>BLADE: Bias-Linked Adaptive DEbiasing</h2><p><strong>Authors:Piyush Arora, Navlika Singh, Vasubhya Diwan, Pratik Mazumder</strong></p>
<p>Neural networks have revolutionized numerous fields, yet they remain vulnerable to a critical flaw: the tendency to learn implicit biases, spurious correlations between certain attributes and target labels in training data. These biases are often more prevalent and easier to learn, causing models to rely on superficial patterns rather than task-relevant features necessary for generalization. Existing methods typically rely on strong assumptions, such as prior knowledge of these biases or access to bias-conflicting samples, i.e., samples that contradict spurious correlations and counterbalance bias-aligned samples, samples that conform to these spurious correlations. However, such assumptions are often impractical in real-world settings. We propose BLADE ({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that requires no prior knowledge of bias or bias-conflicting samples. BLADE first trains a generative model to translate images across bias domains while preserving task-relevant features. Then, it adaptively refines each image with its synthetic counterpart based on the imageâ€™s susceptibility to bias. To encourage robust representations, BLADE aligns an image with its bias-translated synthetic counterpart that shares task-relevant features but differs in bias, while misaligning it with samples sharing the same bias. We evaluate BLADE on multiple benchmark datasets and show that it significantly outperforms state-of-the-art methods. Notably, it exceeds the closest baseline by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the worst group setting, establishing a new benchmark in bias mitigation and demonstrating its potential for developing more robust deep learning models without explicit supervision. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œå·²ç»é¢ è¦†äº†è®¸å¤šé¢†åŸŸï¼Œä½†å®ƒä»¬ä»å­˜åœ¨ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šå€¾å‘äºå­¦ä¹ éšå«åè§å’Œè®­ç»ƒæ•°æ®ä¸­æŸäº›å±æ€§ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„è™šå‡å…³è”ã€‚è¿™äº›åè§é€šå¸¸æ›´ä¸ºæ™®éä¸”æ›´å®¹æ˜“å­¦ä¹ ï¼Œå¯¼è‡´æ¨¡å‹ä¾èµ–è¡¨é¢æ¨¡å¼ï¼Œè€Œéæ¨å¹¿æ‰€éœ€çš„ä¸ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºå¤§çš„å‡è®¾ï¼Œä¾‹å¦‚å¯¹è¿™äº›åè§æœ‰å…ˆéªŒçŸ¥è¯†æˆ–èƒ½æ¥è§¦åˆ°ä¸åè§ç›¸å†²çªçš„æ ·æœ¬ï¼ˆå³ä¸è™šå‡å…³è”ç›¸çŸ›ç›¾çš„æ ·æœ¬ï¼Œå¯å¹³è¡¡ä¸åè§ç›¸ç¬¦çš„æ ·æœ¬ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›å‡è®¾åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­å¾€å¾€ä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬æå‡ºBLADEï¼ˆåè§é“¾æ¥è‡ªé€‚åº”å»åæ¡†æ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¼å»åæ¡†æ¶ï¼Œæ— éœ€å¯¹åè§æˆ–åè§å†²çªæ ·æœ¬æœ‰å…ˆéªŒçŸ¥è¯†ã€‚BLADEé¦–å…ˆè®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œä»¥åœ¨ä¿æŒä»»åŠ¡ç›¸å…³ç‰¹å¾çš„åŒæ—¶ï¼Œè·¨åè§é¢†åŸŸè½¬æ¢å›¾åƒã€‚ç„¶åï¼Œå®ƒæ ¹æ®å›¾åƒå¯¹åè§çš„æ•æ„Ÿæ€§ï¼Œè‡ªé€‚åº”åœ°å¯¹å…¶ä¸åˆæˆå›¾åƒè¿›è¡Œç»†åŒ–ã€‚ä¸ºäº†é¼“åŠ±ç¨³å¥çš„è¡¨ç¤ºï¼ŒBLADEå°†ä¸ä»»åŠ¡ç›¸å…³ä½†åè§ä¸åŒçš„å›¾åƒä¸å…¶ç»è¿‡åè§è½¬æ¢çš„åˆæˆå¯¹åº”ç‰©å¯¹é½ï¼ŒåŒæ—¶ä¸å…±äº«ç›¸åŒåè§çš„æ ·æœ¬é”™å¼€å¯¹é½ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°BLADEï¼Œç»“æœè¡¨æ˜å®ƒæ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æœ€ç³Ÿç³•çš„å›¢é˜Ÿè®¾ç½®ä¸‹ï¼Œå®ƒåœ¨è¢«è…èš€çš„CIFAR-10æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€æ¥è¿‘çš„åŸºçº¿çº¦18%ï¼Œåœ¨åè§ç¼“è§£æ–¹é¢å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å¼€å‘æ— éœ€æ˜ç¡®ç›‘ç£çš„æ›´ç¨³å¥æ·±åº¦å­¦ä¹ æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04174v1">PDF</a> The authors have contributed equally</p>
<p><strong>Summary</strong><br>     ç¥ç»ç½‘ç»œè™½å·²åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œä½†ä»å­˜åœ¨é‡å¤§ç¼ºé™·ï¼šæ˜“å­¦ä¹ è®­ç»ƒæ•°æ®ä¸­çš„éšå«åè§å’Œç‰¹å®šå±æ€§ä¹‹é—´çš„è™šå‡å…³è”ã€‚BLADEæ¡†æ¶é€šè¿‡å›¾åƒè·¨åè§åŸŸçš„è½¬æ¢ï¼Œè‡ªé€‚åº”è°ƒæ•´å›¾åƒä»¥å‡è½»åè§å½±å“ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æˆæœæå‡ã€‚å®ƒåœ¨å¤„ç†éšå«åè§çš„é—®é¢˜ä¸Šå…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè™½ç„¶å¯¹å¤šä¸ªé¢†åŸŸæœ‰é‡å¤§å˜é©ï¼Œä½†æ˜“å­¦ä¹ è®­ç»ƒæ•°æ®ä¸­çš„éšå«åè§å’Œè™šå‡å…³è”æ˜¯å…¶ä¸€å¤§ç¼ºé™·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºå‡è®¾ï¼Œå¦‚äº‹å…ˆçŸ¥é“åè§æˆ–èƒ½è·å–ä¸åè§ç›¸å†²çªçš„æ ·æœ¬ã€‚</li>
<li>BLADEæ¡†æ¶ä¸éœ€è¦äº‹å…ˆäº†è§£åè§æˆ–è·å–åè§ç›¸å†²çªçš„æ ·æœ¬ï¼Œé€šè¿‡è®­ç»ƒç”Ÿæˆæ¨¡å‹å®ç°å›¾åƒè·¨åè§åŸŸçš„è½¬æ¢ã€‚</li>
<li>BLADEé€šè¿‡è‡ªé€‚åº”è°ƒæ•´å›¾åƒæ¥å‡è½»åè§å½±å“ï¼Œé¼“åŠ±å½¢æˆç¨³å¥çš„è¡¨ç°ã€‚</li>
<li>BLADEå¯¹é½ä¸åè§è½¬æ¢çš„åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒï¼ŒäºŒè€…å…±äº«ä»»åŠ¡ç›¸å…³ç‰¹å¾ä½†åè§ä¸åŒï¼ŒåŒæ—¶ä¸å…±äº«ç›¸åŒåè§çš„æ ·æœ¬é”™å¼€å¯¹é½ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒBLADEæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨CIFAR-10æ•°æ®é›†çš„æœ€åç»„è®¾ç½®ä¸­ï¼Œè¶…è¿‡æœ€æ¥è¿‘çš„åŸºçº¿çº¦18%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a4d5852fc6648e9639d5a7e84264e87" align="middle">
<img src="https://picx.zhimg.com/v2-552a6ad4579fc64158754f21f7ab9bba" align="middle">
<img src="https://picx.zhimg.com/v2-f4acd61c8d7272201da7c7f3b421176a" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-OCR-for-Sino-Vietnamese-Language-Processing-via-Fine-tuned-PaddleOCRv5"><a href="#Enhancing-OCR-for-Sino-Vietnamese-Language-Processing-via-Fine-tuned-PaddleOCRv5" class="headerlink" title="Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned   PaddleOCRv5"></a>Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned   PaddleOCRv5</h2><p><strong>Authors:Minh Hoang Nguyen, Su Nguyen Thiet</strong></p>
<p>Recognizing and processing Classical Chinese (Han-Nom) texts play a vital role in digitizing Vietnamese historical documents and enabling cross-lingual semantic research. However, existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations common in ancient sources. In this work, we propose a fine-tuning approach for PaddleOCRv5 to improve character recognition on Han-Nom texts. We retrain the text recognition module using a curated subset of ancient Vietnamese Chinese manuscripts, supported by a full training pipeline covering preprocessing, LMDB conversion, evaluation, and visualization. Experimental results show a significant improvement over the base model, with exact accuracy increasing from 37.5 percent to 50.0 percent, particularly under noisy image conditions. Furthermore, we develop an interactive demo that visually compares pre- and post-fine-tuning recognition results, facilitating downstream applications such as Han-Vietnamese semantic alignment, machine translation, and historical linguistics research. The demo is available at <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5">https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5</a>. </p>
<blockquote>
<p>è¯†åˆ«å’Œå¤„ç†å¤å…¸ä¸­æ–‡ï¼ˆæ±‰æ–‡ï¼‰æ–‡æœ¬å¯¹äºæ•°å­—åŒ–è¶Šå—å†å²æ–‡ä»¶å’Œä¿ƒè¿›è·¨è¯­è¨€è¯­ä¹‰ç ”ç©¶èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„OCRç³»ç»Ÿå¯¹äºæ‰«æè´¨é‡å·®çš„æ–‡ä»¶ã€éæ ‡å‡†å­—å½¢ä»¥åŠå¤æ–‡çŒ®ä¸­å¸¸è§çš„ä¹¦å†™å·®å¼‚å­˜åœ¨å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹PaddleOCRv5çš„ç²¾è°ƒæ–¹æ³•ï¼Œä»¥æé«˜æ±‰æ–‡æ–‡æœ¬çš„å­—ç¬¦è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ç²¾é€‰çš„å¤ä»£è¶Šå—ä¸­æ–‡æ‰‹ç¨¿å­é›†é‡æ–°è®­ç»ƒæ–‡æœ¬è¯†åˆ«æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”±æ¶µç›–é¢„å¤„ç†ã€LMDBè½¬æ¢ã€è¯„ä¼°å’Œå¯è§†åŒ–çš„å®Œæ•´è®­ç»ƒç®¡é“æä¾›æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç²¾ç¡®ç‡ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œä»åŸæœ¬çš„37.5%æå‡è‡³50.0%ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå™ªå£°è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºç¨‹åºï¼Œå¯ä»¥ç›´è§‚åœ°æ¯”è¾ƒè°ƒæ•´å‰åçš„è¯†åˆ«ç»“æœï¼Œæœ‰åŠ©äºä¸‹æ¸¸åº”ç”¨å¦‚æ±‰è¶Šè¯­ä¹‰å¯¹é½ã€æœºå™¨ç¿»è¯‘å’Œå†å²è¯­è¨€å­¦ç ”ç©¶ç­‰ã€‚è¯¥æ¼”ç¤ºç¨‹åºå¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04003v1">PDF</a> 5 pages, 6 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨æ•°å­—åŒ–è¶Šå—å†å²æ–‡æ¡£å’Œä¿ƒè¿›è·¨è¯­è¨€è¯­ä¹‰ç ”ç©¶æ–¹é¢ï¼Œè¯†åˆ«å’Œå¤„ç†å¤å…¸ä¸­æ–‡ï¼ˆæ±‰æ–‡ï¼‰æ–‡æœ¬æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œç°æœ‰çš„OCRç³»ç»Ÿåœ¨å¤„ç†è€åŒ–çš„æ‰«æä»¶ã€éæ ‡å‡†å­—å½¢å’Œæ‰‹å†™å­—ä½“å˜åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§é’ˆå¯¹PaddleOCRv5çš„å¾®è°ƒæ–¹æ³•ï¼Œä»¥æé«˜æ±‰æ–‡æ–‡æœ¬çš„å­—ç¬¦è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬é‡æ–°è®­ç»ƒæ–‡æœ¬è¯†åˆ«æ¨¡å—ï¼Œä½¿ç”¨ç²¾é€‰çš„å¤ä»£è¶Šå—ä¸­æ–‡æ‰‹ç¨¿å­é›†ï¼Œå¹¶ç”±å®Œæ•´çš„è®­ç»ƒç®¡é“æ”¯æŒï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€LMDBè½¬æ¢ã€è¯„ä¼°å’Œå¯è§†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œä»37.5%æå‡è‡³50.0%ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå™ªå£°è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºï¼Œå¯è§†åŒ–æ¯”è¾ƒå¾®è°ƒå‰åçš„è¯†åˆ«ç»“æœï¼Œæœ‰åŠ©äºä¸‹æ¸¸åº”ç”¨å¦‚æ±‰è¶Šè¯­ä¹‰å¯¹é½ã€æœºå™¨ç¿»è¯‘å’Œå†å²è¯­è¨€å­¦ç ”ç©¶ã€‚æ¼”ç¤ºç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤å…¸ä¸­æ–‡ï¼ˆæ±‰æ–‡ï¼‰æ–‡æœ¬åœ¨è¶Šå—å†å²æ–‡æ¡£æ•°å­—åŒ–å’Œè·¨è¯­è¨€è¯­ä¹‰ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰OCRç³»ç»Ÿåœ¨å¤„ç†å¤å…¸ä¸­æ–‡æ–‡æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è€åŒ–çš„æ‰«æä»¶å’Œéæ ‡å‡†å­—å½¢ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒPaddleOCRv5æé«˜äº†æ±‰æ–‡æ–‡æœ¬çš„å­—ç¬¦è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>é‡æ–°è®­ç»ƒæ–‡æœ¬è¯†åˆ«æ¨¡å—ä½¿ç”¨ç²¾é€‰çš„å¤ä»£è¶Šå—ä¸­æ–‡æ‰‹ç¨¿å­é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥è·å¾—æ›´å¥½çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹å‡†ç¡®ç‡ä»37.5%æå‡è‡³50.0%ï¼Œå°¤å…¶åœ¨å›¾åƒå™ªå£°è¾ƒå¤§çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä¸ºæ˜¾è‘—ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºæ¥å¯è§†åŒ–æ¯”è¾ƒå¾®è°ƒå‰åçš„è¯†åˆ«ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0468e024d5b3cffb5086c4a8c1a48c45" align="middle">
<img src="https://picx.zhimg.com/v2-ef2362ebc7454616f29ca3a389c5defd" align="middle">
<img src="https://picx.zhimg.com/v2-dfa580d28b78e24bfa82ce9d3ffba194" align="middle">
<img src="https://picx.zhimg.com/v2-06380b8d4b3b3482b42f6d0be647d1b1" align="middle">
<img src="https://picx.zhimg.com/v2-5137ae31fad28e72255b17871571a02f" align="middle">
<img src="https://picx.zhimg.com/v2-e147b893e42f186a6dec2d2b53c22c8b" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Contrastive-SDE-Guiding-Stochastic-Differential-Equations-with-Contrastive-Learning-for-Unpaired-Image-to-Image-Translation"><a href="#Contrastive-SDE-Guiding-Stochastic-Differential-Equations-with-Contrastive-Learning-for-Unpaired-Image-to-Image-Translation" class="headerlink" title="Contrastive-SDE: Guiding Stochastic Differential Equations with   Contrastive Learning for Unpaired Image-to-Image Translation"></a>Contrastive-SDE: Guiding Stochastic Differential Equations with   Contrastive Learning for Unpaired Image-to-Image Translation</h2><p><strong>Authors:Venkata Narendra Kotyada, Revanth Eranki, Nagesh Bhattu Sristy</strong></p>
<p>Unpaired image-to-image translation involves learning mappings between source domain and target domain in the absence of aligned or corresponding samples. Score based diffusion models have demonstrated state-of-the-art performance in generative tasks. Their ability to approximate complex data distributions through stochastic differential equations (SDEs) enables them to generate high-fidelity and diverse outputs, making them particularly well-suited for unpaired I2I settings. In parallel, contrastive learning provides a powerful framework for learning semantic similarities without the need for explicit supervision or paired data. By pulling together representations of semantically similar samples and pushing apart dissimilar ones, contrastive methods are inherently aligned with the objectives of unpaired translation. Its ability to selectively enforce semantic consistency at the feature level makes contrastive learning particularly effective for guiding generation in unpaired scenarios. In this work, we propose a time-dependent contrastive learning approach where a model is trained with SimCLR by considering an image and its domain invarient feature as a positive pair, enabling the preservation of domain-invariant features and the discarding of domain-specific ones. The learned contrastive model then guides the inference of a pretrained SDE for the I2I translation task. We empirically compare Contrastive-SDE with several baselines across three common unpaired I2I tasks, using four metrics for evaluation. Constrastive-SDE achieves comparable results to the state-of-the-art on several metrics. Furthermore, we observe that our model converges significantly faster and requires no label supervision or classifier training, making it a more efficient alternative for this task. </p>
<blockquote>
<p>éé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¶‰åŠåœ¨æ²¡æœ‰ä»»ä½•å¯¹é½æˆ–å¯¹åº”æ ·æœ¬çš„æƒ…å†µä¸‹å­¦ä¹ æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„æ˜ å°„ã€‚åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å®ƒä»¬é€šè¿‡éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰æ¥é€¼è¿‘å¤æ‚æ•°æ®åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå’Œå¤šæ ·åŒ–çš„è¾“å‡ºï¼Œç‰¹åˆ«é€‚ç”¨äºéé…å¯¹çš„I2Iè®¾ç½®ã€‚åŒæ—¶ï¼Œå¯¹æ¯”å­¦ä¹ æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ç›‘ç£æˆ–é…å¯¹æ•°æ®ã€‚é€šè¿‡å°†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„æ ·æœ¬è¡¨ç¤ºæ‹‰åœ¨ä¸€èµ·ï¼Œå¹¶å°†ä¸ç›¸ä¼¼çš„æ ·æœ¬æ¨å¼€ï¼Œå¯¹æ¯”æ–¹æ³•ä¸æ— é…å¯¹ç¿»è¯‘çš„ç›®æ ‡æœ¬è´¨ä¸Šæ˜¯ä¸€è‡´çš„ã€‚å…¶åœ¨ç‰¹å¾å±‚é¢é€‰æ‹©æ€§å®æ–½è¯­ä¹‰ä¸€è‡´æ€§çš„èƒ½åŠ›ï¼Œä½¿å¯¹æ¯”å­¦ä¹ åœ¨æ— é…å¯¹åœºæ™¯ä¸­çš„ç”ŸæˆæŒ‡å¯¼ç‰¹åˆ«æœ‰æ•ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´ä¾èµ–å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨SimCLRè®­ç»ƒæ¨¡å‹ï¼Œå°†å›¾åƒåŠå…¶åŸŸä¸å˜ç‰¹å¾è§†ä¸ºæ­£æ ·æœ¬å¯¹ï¼Œä»è€Œä¿ç•™åŸŸä¸å˜ç‰¹å¾å¹¶ä¸¢å¼ƒåŸŸç‰¹å®šç‰¹å¾ã€‚ç„¶åï¼Œå­¦ä¹ å¾—åˆ°çš„å¯¹æ¯”æ¨¡å‹æŒ‡å¯¼I2Iç¿»è¯‘ä»»åŠ¡çš„é¢„è®­ç»ƒSDEçš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªå¸¸è§çš„éé…å¯¹I2Iä»»åŠ¡å¯¹Contrastive-SDEä¸å‡ ä¸ªåŸºçº¿è¿›è¡Œäº†å®è¯æ¯”è¾ƒï¼Œå¹¶ä½¿ç”¨å››ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚Contrastive-SDEåœ¨å‡ ä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æˆ‘ä»¬çš„æ¨¡å‹æ”¶æ•›å¾—æ›´å¿«ï¼Œä¸”ä¸éœ€è¦æ ‡ç­¾ç›‘ç£æˆ–åˆ†ç±»å™¨è®­ç»ƒï¼Œä½¿å…¶æˆä¸ºæ­¤ä»»åŠ¡çš„ä¸€ä¸ªæ›´æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03821v1">PDF</a> 9 pages, 3 figures</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ—¶é—´ä¾èµ–å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºæ— é…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ— é…å¯¹æ ·æœ¬çš„æƒ…å†µä¸‹å­¦ä¹ æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿé€‰æ‹©æ€§åœ°åœ¨ç‰¹å¾å±‚é¢ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç¿»è¯‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ— é…å¯¹å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œæ— éœ€æ ‡ç­¾ç›‘ç£å’Œåˆ†ç±»å™¨è®­ç»ƒï¼Œæ›´å…·æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´ä¾èµ–å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œé€‚ç”¨äºæ— é…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ èƒ½å¤Ÿåœ¨ç‰¹å¾å±‚é¢ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ï¼Œé€‚ç”¨äºæ— é…å¯¹åœºæ™¯ä¸‹çš„ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡SDEsè¿‘ä¼¼å¤æ‚æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è¾“å‡ºã€‚</li>
<li>æå‡ºçš„Contrastive-SDEæ–¹æ³•åœ¨ä¸‰ä¸ªå¸¸è§çš„æ— é…å¯¹I2Iä»»åŠ¡ä¸Šä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”è¡¨ç°ç›¸å½“ã€‚</li>
<li>Contrastive-SDEæ–¹æ³•æ”¶æ•›é€Ÿåº¦å¿«ï¼Œæ— éœ€æ ‡ç­¾ç›‘ç£å’Œåˆ†ç±»å™¨è®­ç»ƒï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>é€šè¿‡è€ƒè™‘å›¾åƒå’Œå…¶åŸŸä¸å˜ç‰¹å¾ä½œä¸ºæ­£æ ·æœ¬å¯¹ï¼Œæ¨¡å‹èƒ½å¤Ÿä¿ç•™åŸŸä¸å˜ç‰¹å¾å¹¶ä¸¢å¼ƒåŸŸç‰¹å®šç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02c589eb9ba273d167f65a34a3b80a8a" align="middle">
<img src="https://picx.zhimg.com/v2-3b478ac6f89b0c8a7e4af4d843c68c44" align="middle">
<img src="https://picx.zhimg.com/v2-ae39a70cc7de7ff2aa438086a1d53c9c" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ML2B-Multi-Lingual-ML-Benchmark-For-AutoML"><a href="#ML2B-Multi-Lingual-ML-Benchmark-For-AutoML" class="headerlink" title="ML2B: Multi-Lingual ML Benchmark For AutoML"></a>ML2B: Multi-Lingual ML Benchmark For AutoML</h2><p><strong>Authors:Ekaterina Trofimova, Zosia Shamina, Maria Selifanova, Artem Zaitsev, Remi Savchuk, Maxim Minets, Daria Ozerova, Emil Sataev, Denis Zuenko, Andrey E. Ustyuzhanin</strong></p>
<p>Large language models (LLMs) have recently demonstrated strong capabilities in generating machine learning (ML) code, enabling end-to-end pipeline construction from natural language instructions. However, existing benchmarks for ML code generation are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice. To address this gap, we present ML2B, the first benchmark for evaluating multilingual ML code generation. ML2B consists of 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, with structured metadata and validated human-reviewed translations. For evaluation, we employ AIDE, an automated framework for end-to-end assessment of data science pipelines, and provide insights into cross-lingual model performance. Our results reveal substantial 15-45% performance degradation on non-English tasks, highlighting critical challenges in multilingual representation learning for code generation. The benchmark, evaluation framework, and comprehensive results are made available through our GitHub repository to facilitate future research in multilingual ML code generation: <a target="_blank" rel="noopener" href="https://github.com/enaix/ml2b">https://github.com/enaix/ml2b</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»£ç çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­æ„å»ºç«¯åˆ°ç«¯çš„ç®¡é“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºè‹±è¯­ï¼Œå¿½ç•¥äº†MLç ”ç©¶å’Œå®è·µçš„å…¨çƒå’Œå¤šè¯­ç§æ€§è´¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ML2Bï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°å¤šè¯­ç§MLä»£ç ç”Ÿæˆèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ML2BåŒ…å«30ä¸ªç¿»è¯‘æˆ13ç§è‡ªç„¶è¯­è¨€çš„Kaggleç«èµ›ï¼Œæ¶µç›–è¡¨æ ¼ã€æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç±»å‹ï¼Œå…·å¤‡ç»“æ„åŒ–å…ƒæ•°æ®å’Œç»è¿‡éªŒè¯çš„äººå·¥å®¡æŸ¥ç¿»è¯‘ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†AIDEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ•°æ®ç§‘å­¦ç®¡é“ç«¯åˆ°ç«¯è¯„ä¼°çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼ŒåŒæ—¶æä¾›äº†è·¨è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ´å¯Ÿã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œåœ¨éè‹±è¯­ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½ä¸‹é™äº†15-45%ï¼Œè¿™çªæ˜¾äº†ä»£ç ç”Ÿæˆå¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ æ‰€é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•ã€è¯„ä¼°æ¡†æ¶å’Œå…¨é¢ç»“æœå·²é€šè¿‡æˆ‘ä»¬çš„GitHubä»“åº“æä¾›ï¼Œä»¥ä¾¿ä¿ƒè¿›æœªæ¥åœ¨å¤šè¯­ç§MLä»£ç ç”Ÿæˆæ–¹é¢çš„ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/enaix/ml2b%E3%80%82">https://github.com/enaix/ml2bã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22768v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæœºå™¨å­¦ä¹ ä»£ç æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°ä»è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ°ç«¯åˆ°ç«¯ç®¡é“æ„å»ºçš„å…¨ç¨‹è‡ªåŠ¨åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æœºå™¨å­¦ä¹ ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºè‹±è¯­ï¼Œå¿½ç•¥äº†æœºå™¨å­¦ä¹ ç ”ç©¶å’Œå®è·µçš„å…¨çƒå’Œå¤šè¯­è¨€ç‰¹æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ML2Bï¼Œé¦–ä¸ªå¤šè¯­è¨€æœºå™¨å­¦ä¹ ä»£ç ç”Ÿæˆè¯„ä¼°åŸºå‡†ã€‚ML2BåŒ…å«ç¿»è¯‘ä¸º13ç§è‡ªç„¶è¯­è¨€çš„30ä¸ªKaggleç«èµ›ï¼Œæ¶µç›–è¡¨æ ¼ã€æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç±»å‹ï¼Œå…·æœ‰ç»“æ„åŒ–å…ƒæ•°æ®å’Œç»è¿‡éªŒè¯çš„äººå·¥å®¡æŸ¥ç¿»è¯‘ã€‚æˆ‘ä»¬ä½¿ç”¨AIDEï¼ˆä¸€ç§ç”¨äºæ•°æ®ç§‘å­¦ç®¡é“ç«¯åˆ°ç«¯è¯„ä¼°çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼‰è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†è·¨è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨éè‹±è¯­ä»»åŠ¡ä¸Šæ€§èƒ½ä¸‹é™15-45%ï¼Œçªæ˜¾å‡ºä»£ç ç”Ÿæˆå¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„å…³é”®æŒ‘æˆ˜ã€‚åŸºå‡†æµ‹è¯•ã€è¯„ä¼°æ¡†æ¶å’Œå…¨é¢ç»“æœå·²é€šè¿‡GitHubä»“åº“å…¬å¼€ï¼Œä»¥ä¾¿æœªæ¥è¿›è¡Œå¤šè¯­è¨€æœºå™¨å­¦ä¹ ä»£ç ç”Ÿæˆç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½è‡ªåŠ¨ç”Ÿæˆæœºå™¨å­¦ä¹ ä»£ç ï¼Œå®ç°ä»è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ°ç«¯åˆ°ç«¯ç®¡é“æ„å»ºã€‚</li>
<li>ç°æœ‰æœºå™¨å­¦ä¹ ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºè‹±è¯­ï¼Œå­˜åœ¨å¤šè¯­è¨€ç‰¹æ€§å¿½è§†çš„é—®é¢˜ã€‚</li>
<li>ML2Bæ˜¯é¦–ä¸ªå¤šè¯­è¨€æœºå™¨å­¦ä¹ ä»£ç ç”Ÿæˆè¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«30ä¸ªKaggleç«èµ›ï¼Œç¿»è¯‘ä¸º13ç§è‡ªç„¶è¯­è¨€ã€‚</li>
<li>ML2Bæ¶µç›–è¡¨æ ¼ã€æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç±»å‹ï¼Œå…·æœ‰ç»“æ„åŒ–å…ƒæ•°æ®å’Œç»è¿‡éªŒè¯çš„ç¿»è¯‘ã€‚</li>
<li>é‡‡ç”¨AIDEè‡ªåŠ¨åŒ–æ¡†æ¶è¿›è¡Œè¯„ä¼°ï¼Œæ·±å…¥æ¢ç©¶äº†è·¨è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨éè‹±è¯­ä»»åŠ¡ä¸Šï¼Œæ¨¡å‹æ€§èƒ½ä¸‹é™15-45%ï¼Œçªæ˜¾å‡ºå¤šè¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00280cb5e445264c3353032d51361575" align="middle">
<img src="https://picx.zhimg.com/v2-0fd7441bbd16bb80fd8242b28f126963" align="middle">
<img src="https://picx.zhimg.com/v2-b10905b2de2ff1235bc0c25878129d6a" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GIIFT-Graph-guided-Inductive-Image-free-Multimodal-Machine-Translation"><a href="#GIIFT-Graph-guided-Inductive-Image-free-Multimodal-Machine-Translation" class="headerlink" title="GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation"></a>GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation</h2><p><strong>Authors:Jiafeng Xiong, Yuting Zhao</strong></p>
<p>Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference. </p>
<blockquote>
<p>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰å·²ç»è¯æ˜äº†è§†è§‰ä¿¡æ¯åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„å·¨å¤§å¸®åŠ©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MMTæ–¹æ³•åœ¨åˆ©ç”¨æ¨¡æ€å·®è·æ–¹é¢é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå®ƒä»¬é€šè¿‡å¼ºåˆ¶å®æ–½ä¸¥æ ¼çš„è§†è§‰è¯­è¨€å¯¹é½ï¼ŒåŒæ—¶å—é™äºå…¶è®­ç»ƒçš„å¤šæ¨¡æ€åŸŸå†…çš„æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°å‹çš„å¤šæ¨¡æ€åœºæ™¯å›¾ï¼Œä»¥ä¿ç•™å’Œæ•´åˆç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†GIIFTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å›¾å½¢å¼•å¯¼å½’çº³æ— å›¾å¤šæ¨¡æ€ç¿»è¯‘æ¡†æ¶ï¼Œå®ƒä½¿ç”¨è·¨æ¨¡æ€å›¾æ³¨æ„åŠ›ç½‘ç»œé€‚é…å™¨ï¼Œåœ¨ç»Ÿä¸€èåˆç©ºé—´å­¦ä¹ å¤šæ¨¡æ€çŸ¥è¯†ï¼Œå¹¶å½’çº³æ¨å¹¿è‡³æ›´å¹¿æ³›çš„æ— å›¾ç¿»è¯‘é¢†åŸŸã€‚åœ¨Multi30Kè‹±è¯­åˆ°æ³•è¯­å’Œè‹±è¯­åˆ°å¾·è¯­ä»»åŠ¡çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GIIFTè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå³ä½¿åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨å›¾åƒã€‚åœ¨WMTåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¾ƒæ— å›¾ç¿»è¯‘åŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œè¯æ˜äº†GIIFTåœ¨æ— å›¾å½’çº³æ¨ç†æ–¹é¢çš„å®åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18562v2">PDF</a> Accepted as an oral presentation at the EMNLP 2025 Workshop on   Machine Translation (WMT)</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶é€šè¿‡æ„å»ºæ–°å‹çš„å¤šæ¨¡æ€åœºæ™¯å›¾æ¥ä¿å­˜å’Œæ•´åˆæ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œå¹¶å¼•å…¥GIIFTæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å›¾åƒå¼•å¯¼å‹æ— å›¾åƒå¤šæ¨¡æ€ç¿»è¯‘æ¡†æ¶ã€‚å®ƒä½¿ç”¨è·¨æ¨¡æ€å›¾æ³¨æ„åŠ›ç½‘ç»œé€‚é…å™¨åœ¨ç»Ÿä¸€èåˆç©ºé—´ä¸­å­¦ä¹ å¤šæ¨¡æ€çŸ¥è¯†ï¼Œå¹¶å°†å…¶å½’çº³æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„æ— éœ€å›¾åƒç¿»è¯‘é¢†åŸŸã€‚åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„è‹±è¯­åˆ°æ³•è¯­å’Œè‹±è¯­åˆ°å¾·è¯­ä»»åŠ¡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGIIFTè¶…è¶Šäº†ç°æœ‰æ–¹æ³•å¹¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€ä½¿ç”¨å›¾åƒã€‚åœ¨WMTåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœä¹Ÿæ˜¾è‘—ä¼˜äºæ— å›¾åƒç¿»è¯‘åŸºçº¿ï¼Œè¯æ˜äº†GIIFTåœ¨å½’çº³æ— å›¾åƒæ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰å·²ç»è¯æ˜è§†è§‰ä¿¡æ¯å¯¹æœºå™¨ç¿»è¯‘çš„æ˜¾è‘—å¸®åŠ©ã€‚</li>
<li>ç°æœ‰MMTæ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦åœ¨äºæ¨¡æ€å·®è·å’Œåˆ©ç”¨è§†è§‰ä¸è¯­è¨€çš„åˆšæ€§å¯¹é½ä¹‹é—´çš„çŸ›ç›¾ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡æ„å»ºå¤šæ¨¡æ€åœºæ™¯å›¾æ¥ä¿å­˜å’Œæ•´åˆæ¨¡æ€ç‰¹å®šä¿¡æ¯ã€‚</li>
<li>å¼•å…¥GIIFTæ¡†æ¶ï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„å›¾åƒå¼•å¯¼å‹æ— å›¾åƒMMTæ¡†æ¶ã€‚</li>
<li>ä½¿ç”¨è·¨æ¨¡æ€å›¾æ³¨æ„åŠ›ç½‘ç»œé€‚é…å™¨åœ¨ç»Ÿä¸€èåˆç©ºé—´ä¸­å­¦ä¹ å¤šæ¨¡æ€çŸ¥è¯†ã€‚</li>
<li>GIIFTæ¡†æ¶å®ç°äº†æ— éœ€å›¾åƒçš„ç¿»è¯‘æ¨å¹¿è‡³æ›´å¹¿æ³›çš„é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-385f0824ef13a8e3260f0f3cbea07ad2" align="middle">
<img src="https://picx.zhimg.com/v2-4c60c2c4f3cd87deef60e1261aac6891" align="middle">
<img src="https://picx.zhimg.com/v2-5393ff4fcf7f670cd67b9c3c01497ddc" align="middle">
<img src="https://picx.zhimg.com/v2-7cddefcf76707ad928cb190dfe6fe32d" align="middle">
<img src="https://picx.zhimg.com/v2-c847d5b1dd97a51c096bd79df71ca723" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RoboSwap-A-GAN-driven-Video-Diffusion-Framework-For-Unsupervised-Robot-Arm-Swapping"><a href="#RoboSwap-A-GAN-driven-Video-Diffusion-Framework-For-Unsupervised-Robot-Arm-Swapping" class="headerlink" title="RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot   Arm Swapping"></a>RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot   Arm Swapping</h2><p><strong>Authors:Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Dong Chen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok</strong></p>
<p>Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„å‘å±•åœ¨è§†é¢‘åˆæˆå’Œç¼–è¾‘æ–¹é¢å¼•èµ·äº†é©å‘½æ€§çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œå¤šæ ·ä¸”é«˜è´¨é‡æ•°æ®é›†çš„åŒ®ä¹ä»ç„¶é˜»ç¢äº†è§†é¢‘æ¡ä»¶ä¸‹çš„æœºå™¨äººå­¦ä¹ ï¼Œå¹¶é™åˆ¶äº†è·¨å¹³å°çš„æ³›åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨ä¸€ä¸ªè§†é¢‘ä¸­äº¤æ¢ä¸€ä¸ªæœºæ¢°è‡‚ä¸å¦ä¸€ä¸ªæœºæ¢°è‡‚çš„æŒ‘æˆ˜ï¼šè¿™æ˜¯è·¨ä½“æ€å­¦ä¹ çš„å…³é”®æ­¥éª¤ã€‚ä¸åŒäºä»¥å‰ä¾èµ–äºç›¸åŒç¯å¢ƒè®¾ç½®ä¸­çš„é…å¯¹è§†é¢‘æ¼”ç¤ºçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºçš„RoboSwapæ¡†æ¶èƒ½å¤Ÿä»ä¸åŒçš„ç¯å¢ƒä¸­å¤„ç†æœªé…å¯¹çš„æ•°æ®ï¼Œä»è€Œå‡è½»äº†æ•°æ®æ”¶é›†çš„éœ€æ±‚ã€‚RoboSwapå¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘ç¼–è¾‘ç®¡é“ï¼Œé›†æˆäº†GANså’Œæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†å®ƒä»¬çš„ç‹¬ç«‹ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»èƒŒæ™¯ä¸­åˆ†å‰²å‡ºæœºæ¢°è‡‚ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªæœªé…å¯¹çš„GANæ¨¡å‹æ¥å°†ä¸€ä¸ªæœºæ¢°è‡‚ç¿»è¯‘åˆ°å¦ä¸€ä¸ªæœºæ¢°è‡‚ã€‚ç¿»è¯‘çš„æœºæ¢°è‡‚è¢«æ··åˆåˆ°åŸå§‹è§†é¢‘èƒŒæ™¯ä¸­ï¼Œå¹¶ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œç»†åŒ–ï¼Œä»¥æé«˜è¿è´¯æ€§ã€è¿åŠ¨ç°å®æ„Ÿå’Œç‰©ä½“äº¤äº’æ€§ã€‚GANå’Œæ‰©æ•£é˜¶æ®µæ˜¯ç‹¬ç«‹è®­ç»ƒçš„ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRoboSwapåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘å’Œå›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œåœ¨ç»“æ„è¿è´¯æ€§å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œä»è€Œä¸ºæœºå™¨äººå­¦ä¹ ä¸­çš„å¯é ã€è·¨ä½“æ€æ•°æ®ç”Ÿæˆæä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08632v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RoboSwapæ¡†æ¶åœ¨è§†é¢‘åˆæˆå’Œç¼–è¾‘é¢†åŸŸçš„åˆ›æ–°åº”ç”¨ï¼Œè¯¥æ¡†æ¶è§£å†³äº†åœ¨è·¨å¹³å°æœºå™¨äººå­¦ä¹ ä¸­å› ç¼ºä¹å¤šæ ·åŒ–å’Œé«˜è´¨é‡æ•°æ®é›†è€Œå¯¼è‡´çš„é—®é¢˜ã€‚ä¸åŒäºä¾èµ–äºç›¸åŒç¯å¢ƒè®¾ç½®çš„é…å¯¹è§†é¢‘æ¼”ç¤ºçš„å…ˆå‰æ–¹æ³•ï¼ŒRoboSwapèƒ½å¤Ÿå¯¹æ¥è‡ªä¸åŒç¯å¢ƒçš„æœªé…å¯¹æ•°æ®è¿›è¡Œæ“ä½œï¼Œå‡è½»äº†æ•°æ®æ”¶é›†çš„éœ€æ±‚ã€‚å®ƒç»“åˆäº†GANså’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åˆ†å‰²æœºå™¨äººæ‰‹è‡‚å¹¶å°†å…¶èƒŒæ™¯è¿›è¡Œç¿»è¯‘è½¬æ¢ï¼Œå†å°†è½¬æ¢åçš„æ‰‹è‡‚ä¸åŸå§‹è§†é¢‘èƒŒæ™¯èåˆï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¢å¼ºè¿è´¯æ€§å’Œè¿åŠ¨ç°å®æ„Ÿä»¥åŠç‰©ä½“äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼ŒRoboSwapåœ¨ç»“æ„è¿è´¯æ€§å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„è§†é¢‘å’Œå›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œä¸ºæœºå™¨äººå­¦ä¹ ä¸­ç”Ÿæˆå¯é ã€è·¨å®ä½“çš„æ•°æ®æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RoboSwapæ¡†æ¶è§£å†³äº†è§†é¢‘åˆæˆå’Œç¼–è¾‘é¢†åŸŸä¸­çš„æœºå™¨äººæ‰‹è‡‚æ›¿æ¢é—®é¢˜ï¼Œæ˜¯è·¨å¹³å°æœºå™¨äººå­¦ä¹ ä¸­çš„å…³é”®æ­¥éª¤ã€‚</li>
<li>ä¸ä¾èµ–é…å¯¹è§†é¢‘æ¼”ç¤ºçš„æ–¹æ³•ä¸åŒï¼ŒRoboSwapå¤„ç†æœªé…å¯¹æ•°æ®ï¼Œå¹¶èƒ½åœ¨ä¸åŒç¯å¢ƒä¸­æ“ä½œï¼Œå‡è½»äº†æ•°æ®æ”¶é›†çš„è´Ÿæ‹…ã€‚</li>
<li>RoboSwapç»“åˆäº†GANså’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°é¢–çš„è§†é¢‘ç¼–è¾‘æµç¨‹ã€‚</li>
<li>é€šè¿‡åˆ†å‰²æœºå™¨äººæ‰‹è‡‚å¹¶å°†å…¶èƒŒæ™¯è¿›è¡Œç¿»è¯‘è½¬æ¢ï¼ŒRoboSwapå®ç°äº†æœºå™¨äººæ‰‹è‡‚çš„æ›¿æ¢ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºå¢å¼ºè§†é¢‘è¿è´¯æ€§å’Œè¿åŠ¨ç°å®æ„Ÿï¼Œä»¥åŠç‰©ä½“äº¤äº’æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºRoboSwapåœ¨ç»“æ„è¿è´¯æ€§å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c6719c933c8edc2516b1fd45a500113" align="middle">
<img src="https://picx.zhimg.com/v2-ab664779a71525b354b1b2dbe15003ed" align="middle">
<img src="https://picx.zhimg.com/v2-2f722841c9a6fd8b74963f9305b7f289" align="middle">
<img src="https://picx.zhimg.com/v2-06e40d79b25479dd4de6d39f7a7b4472" align="middle">
<img src="https://picx.zhimg.com/v2-e665a1b30dd34211e4ffe0954362bb05" align="middle">
<img src="https://picx.zhimg.com/v2-9d3f93730aa68aa5f59be2548b0fbb8a" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Fully-Spiking-Neural-Networks-for-Unified-Frame-Event-Object-Tracking"><a href="#Fully-Spiking-Neural-Networks-for-Unified-Frame-Event-Object-Tracking" class="headerlink" title="Fully Spiking Neural Networks for Unified Frame-Event Object Tracking"></a>Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</h2><p><strong>Authors:Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu</strong></p>
<p>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency. </p>
<blockquote>
<p>å°†å›¾åƒå’Œäº‹ä»¶æµçš„é›†æˆä¸ºå®ç°å¤æ‚ç¯å¢ƒä¸­ç¨³å¥è§†è§‰å¯¹è±¡è·Ÿè¸ªçš„ä¸€ç§å‰æ™¯å¹¿é˜”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„èåˆæ–¹æ³•ä»¥å¤§é‡çš„è®¡ç®—å¼€é”€ä¸ºä»£ä»·å®ç°äº†é«˜æ€§èƒ½ï¼Œå¹¶ä¸”éš¾ä»¥æœ‰æ•ˆåœ°ä»äº‹ä»¶æµä¸­æå–ç¨€ç–ã€å¼‚æ­¥ä¿¡æ¯ï¼Œæœªèƒ½åˆ©ç”¨äº‹ä»¶é©±åŠ¨è„‰å†²èŒƒå¼çš„èŠ‚èƒ½ä¼˜åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºè„‰å†²çš„å¸§äº‹ä»¶è·Ÿè¸ªæ¡†æ¶ï¼Œåä¸ºSpikeFETã€‚è¯¥ç½‘ç»œåœ¨è„‰å†²èŒƒå¼å†…å®ç°äº†å·ç§¯å±€éƒ¨ç‰¹å¾æå–å’ŒåŸºäºTransformerçš„å…¨å±€å»ºæ¨¡çš„ååŒèåˆï¼Œæœ‰æ•ˆåœ°èåˆäº†å¸§å’Œäº‹ä»¶æ•°æ®ã€‚ä¸ºäº†è§£å†³å› å·ç§¯å¡«å……è€Œå¯¼è‡´çš„ç¿»è¯‘ä¸å˜æ€§é€€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†éšæœºæ‹¼è´´æ¨¡å—ï¼ˆRPMï¼‰ï¼Œè¯¥æ¨¡å—é€šè¿‡éšæœºç©ºé—´é‡ç»„å’Œå¯å­¦ä¹ ç±»å‹ç¼–ç æ¥æ¶ˆé™¤ä½ç½®åè§ï¼ŒåŒæ—¶ä¿ç•™æ®‹å·®ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶ç©ºæ­£åˆ™åŒ–ï¼ˆSTRï¼‰ç­–ç•¥ï¼Œé€šè¿‡å¼ºåˆ¶æ½œåœ¨ç©ºé—´ä¸­æ—¶é—´æ¨¡æ¿ç‰¹å¾çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå…‹æœç”±ä¸å¯¹ç§°ç‰¹å¾å¼•èµ·çš„ç›¸ä¼¼åº¦åº¦é‡é€€åŒ–ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å®ç°äº†å“è¶Šçš„è·Ÿè¸ªç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†åŠŸè€—ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20834v2">PDF</a> Accepted by NeurIPS2025</p>
<p><strong>Summary</strong></p>
<p>å·ç§¯å±€éƒ¨ç‰¹å¾æå–ä¸åŸºäºTransformerçš„å…¨å±€å»ºæ¨¡åœ¨è„‰å†²é©±åŠ¨èŒƒå¼ä¸‹çš„ååŒèåˆï¼Œå®ç°äº†å›¾åƒå’Œäº‹ä»¶æµçš„é›†æˆï¼Œæœ‰æ•ˆèåˆäº†å¸§å’Œäº‹ä»¶æ•°æ®ã€‚ä¸ºè§£å†³ç¿»è¯‘ä¸å˜æ€§å› å·ç§¯å¡«å……å¼•èµ·çš„é€€åŒ–é—®é¢˜ï¼Œå¼•å…¥Random Patchwork Moduleï¼ˆRPMï¼‰æ¨¡å—ï¼Œé€šè¿‡éšæœºç©ºé—´é‡ç»„å’Œå­¦ä¹ ç±»å‹ç¼–ç ä¿ç•™æ®‹å·®ç»“æ„ï¼ŒåŒæ—¶æ¶ˆé™¤ä½ç½®åè§ã€‚æ­¤å¤–ï¼Œæå‡ºçš„Spatial-Temporal Regularizationï¼ˆSTRï¼‰ç­–ç•¥é€šè¿‡å¯¹æ½œåœ¨ç©ºé—´ä¸­çš„æ—¶é—´æ¨¡æ¿ç‰¹å¾å¼ºåˆ¶æ‰§è¡Œæ—¶ç©ºä¸€è‡´æ€§ï¼Œå…‹æœäº†ä¸å¯¹ç§°ç‰¹å¾çš„ç›¸ä¼¼æ€§åº¦é‡é€€åŒ–é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¼˜è¶Šçš„è·Ÿè¸ªç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½åŠŸè€—ï¼Œå®ç°äº†æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„ä¼˜åŒ–å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒå’Œäº‹ä»¶æµçš„é›†æˆæ˜¯å®ç°å¤æ‚ç¯å¢ƒä¸­ç¨³å¥è§†è§‰å¯¹è±¡è·Ÿè¸ªçš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>å½“å‰èåˆæ–¹æ³•åœ¨è®¡ç®—å¼€é”€æ–¹é¢è¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œä½†éš¾ä»¥ä»äº‹ä»¶æµä¸­æå–ç¨€ç–ã€å¼‚æ­¥ä¿¡æ¯ã€‚</li>
<li>SpikeFETæ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºè„‰å†²çš„å¸§-äº‹ä»¶è·Ÿè¸ªæ¡†æ¶ï¼Œå®ç°äº†å¸§å’Œäº‹ä»¶æ•°æ®çš„æœ‰æ•ˆèåˆã€‚</li>
<li>RPMæ¨¡å—é€šè¿‡éšæœºç©ºé—´é‡ç»„å’Œå­¦ä¹ ç±»å‹ç¼–ç ï¼Œè§£å†³äº†å› å·ç§¯å¡«å……å¯¼è‡´çš„ç¿»è¯‘ä¸å˜æ€§é€€åŒ–é—®é¢˜ã€‚</li>
<li>STRç­–ç•¥é€šè¿‡å¼ºåˆ¶æ‰§è¡Œæ—¶ç©ºä¸€è‡´æ€§ï¼Œå…‹æœäº†ä¸å¯¹ç§°ç‰¹å¾çš„ç›¸ä¼¼æ€§åº¦é‡é€€åŒ–é—®é¢˜ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¼˜è¶Šçš„è·Ÿè¸ªç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdf6b0af6d8c8769bf8e56cce878dfb5" align="middle">
<img src="https://picx.zhimg.com/v2-cd4a2f890f75dadbdda55f33afc97191" align="middle">
<img src="https://picx.zhimg.com/v2-cd7c976ad66f94d5061b22c349e264f6" align="middle">
<img src="https://picx.zhimg.com/v2-5a6ff0680de9e1ace7b6d1832e9891fa" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d831a58042b2e65afc4b0f12b7f137c1" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Flow4Agent Long-form Video Understanding via Motion Prior from Optical   Flow
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c7356733d2d17e36a80ff2f1f8d36a27" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
