<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Vibe Checker Aligning Code Evaluation with Human Preference">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-5d30dc1b2c174d4bb19a9f51df938ba9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082923&auth_key=1760082923-0-0-161031898c3a921cfcc49d31605667a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-10-æ›´æ–°"><a href="#2025-10-10-æ›´æ–°" class="headerlink" title="2025-10-10 æ›´æ–°"></a>2025-10-10 æ›´æ–°</h1><h2 id="Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference"><a href="#Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference" class="headerlink" title="Vibe Checker: Aligning Code Evaluation with Human Preference"></a>Vibe Checker: Aligning Code Evaluation with Human Preference</h2><p><strong>Authors:Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings, Jiao Sun</strong></p>
<p>Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify modelsâ€™ code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ¨åŠ¨äº†æ°›å›´ç¼–ç çš„å‘å±•ï¼Œç”¨æˆ·åˆ©ç”¨LLMç”Ÿæˆä»£ç å¹¶å€ŸåŠ©è‡ªç„¶è¯­è¨€äº¤äº’è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œç›´åˆ°é€šè¿‡ä»–ä»¬çš„æ°›å›´æ£€æŸ¥ã€‚æ°›å›´æ£€æŸ¥ä¸çœŸå®ä¸–ç•Œçš„ç”¨æˆ·åå¥½æœ‰å…³ï¼Œå¹¶ä¸”è¶…è¶Šäº†åŠŸèƒ½èŒƒå›´ï¼šè§£å†³æ–¹æ¡ˆä¸ä»…è¦èƒ½å¤Ÿæ­£ç¡®è¿è¡Œï¼Œè€Œä¸”è¦ç»“æ„æ¸…æ™°ã€ä¿æŒæ„å›¾å¹¶ä¸”å§‹ç»ˆæ­£ç¡®ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä»£ç è¯„ä¼°ä»ç„¶ä¾§é‡äºå‡†ç¡®æŠ“ä½ç”¨æˆ·æ„å›¾çš„å‰kä¸ªé€‰é¡¹ï¼Œä»…å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¿½è§†äº†ç”¨æˆ·ç»å¸¸åº”ç”¨çš„éåŠŸèƒ½æ€§æŒ‡ä»¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æŒ‡ä»¤éµå¾ªæ˜¯æ°›å›´æ£€æŸ¥ä¸­ç¼ºå¤±çš„éƒ¨åˆ†ï¼Œé™¤äº†åŠŸèƒ½æ­£ç¡®æ€§å¤–ï¼Œå®ƒä»£è¡¨äº†äººç±»åœ¨ç¼–ç è¿‡ç¨‹ä¸­çš„åå¥½ã€‚ä¸ºäº†é‡åŒ–æ¨¡å‹éµå¾ªä»£ç æŒ‡ä»¤çš„èƒ½åŠ›å¹¶ä½¿ç”¨å¯æµ‹é‡çš„ä¿¡å·è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†VeriCodeï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«30ä¸ªå¯éªŒè¯ä»£ç æŒ‡ä»¤çš„ç›®å½•ï¼Œå¹¶é…å¤‡äº†ç›¸åº”çš„ç¡®å®šæ€§éªŒè¯å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨ç›®å½•æ¥å¢å¼ºç°æœ‰çš„è¯„ä¼°å¥—ä»¶ï¼Œä»è€Œå½¢æˆäº†Vibe Checkerï¼Œä¸€ä¸ªå¯ä»¥è¯„ä¼°ä»£ç æŒ‡ä»¤éµå¾ªå’ŒåŠŸèƒ½æ­£ç¡®æ€§çš„æµ‹è¯•å¹³å°ã€‚åœ¨å¯¹31ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°åï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ¨¡å‹ä¹Ÿå¾ˆéš¾éµå¾ªå¤šä¸ªæŒ‡ä»¤ï¼Œå¹¶ä¸”ä¼šå‡ºç°æ˜æ˜¾çš„åŠŸèƒ½å›å½’ã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªçš„ç»„åˆåˆ†æ•°ä¸äººç±»çš„åå¥½å…³è”åº¦æœ€é«˜ï¼Œåè€…åœ¨å®é™…ç¼–ç¨‹ä»»åŠ¡ä¸­æˆä¸ºä¸»è¦åŒºåˆ«å› ç´ ã€‚æˆ‘ä»¬çš„å·¥ä½œç¡®å®šäº†æ°›å›´æ£€æŸ¥çš„æ ¸å¿ƒå› ç´ ï¼Œä¸ºåŸºå‡†æµ‹è¯•å’Œç ”å‘æ›´ç¬¦åˆç”¨æˆ·ç¼–ç åå¥½çš„æ¨¡å‹æä¾›äº†å…·ä½“è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07315v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨åŠ¨äº†â€œæ°›å›´ç¼–ç â€ï¼Œç”¨æˆ·åˆ©ç”¨LLMç”Ÿæˆå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è¿­ä»£å®Œå–„ä»£ç ï¼Œç›´è‡³æ»¡è¶³ä»–ä»¬çš„æ°›å›´æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç è¯„ä¼°ä¸»è¦å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¿½ç•¥äº†ç”¨æˆ·çš„éåŠŸèƒ½æŒ‡ä»¤ã€‚æœ¬æ–‡æå‡ºæŒ‡ä»¤éµå¾ªæ˜¯æ°›å›´æ„ŸçŸ¥ä¸­ç¼ºå¤±çš„é‡è¦éƒ¨åˆ†ï¼Œå¹¶ä»£è¡¨äº†äººç±»ç¼–ç ä¸­çš„åå¥½ã€‚ä¸ºäº†é‡åŒ–æ¨¡å‹éµå¾ªä»£ç æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†VeriCodeï¼ŒåŒ…å«30ç§å¯éªŒè¯çš„ä»£ç æŒ‡ä»¤åŠå…¶ç›¸åº”çš„ç¡®å®šæ€§éªŒè¯å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤åˆ†ç±»æ¥å¢å¼ºç°æœ‰çš„è¯„ä¼°å¥—ä»¶ï¼Œä»è€Œå»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°ä»£ç æŒ‡ä»¤éµå¾ªå’ŒåŠŸèƒ½æ­£ç¡®æ€§çš„æµ‹è¯•å¹³å°Vibe Checkerã€‚è¯„ä¼°é¢†å…ˆçš„LLMæ—¶å‘ç°ï¼Œå³ä½¿åœ¨å¼ºå¤§çš„æ¨¡å‹ä¹Ÿéš¾ä»¥éµå¾ªå¤šä¸ªæŒ‡ä»¤ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„åŠŸèƒ½å›å½’ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¸äººç±»åå¥½æœ€ç›¸å…³çš„æ˜¯åŠŸèƒ½æ­£ç¡®æ€§å’ŒæŒ‡ä»¤éµå¾ªçš„å¤åˆåˆ†æ•°ï¼Œå…¶ä¸­åè€…åœ¨ç°å®ç¼–ç¨‹ä»»åŠ¡ä¸­æˆä¸ºä¸»è¦åŒºåˆ«ã€‚æœ¬æ–‡çš„ç ”ç©¶ä¸ºæ°›å›´æ„ŸçŸ¥æä¾›äº†æ ¸å¿ƒå› ç´ çš„å…·ä½“è·¯å¾„ï¼Œä¸ºæ›´ç¬¦åˆç”¨æˆ·ç¼–ç åå¥½çš„æ¨¡å‹è¯„ä¼°å’Œç ”å‘æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨åŠ¨äº†æ°›å›´ç¼–ç ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’ç”Ÿæˆå’Œè¿­ä»£å®Œå–„ä»£ç ã€‚</li>
<li>å½“å‰çš„ä»£ç è¯„ä¼°ä¸»è¦å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¿½ç•¥äº†ç”¨æˆ·çš„éåŠŸèƒ½æŒ‡ä»¤ã€‚</li>
<li>æŒ‡ä»¤éµå¾ªæ˜¯æ°›å›´æ„ŸçŸ¥ç¼–ç ä¸­é‡è¦çš„éƒ¨åˆ†ï¼Œä»£è¡¨äººç±»ç¼–ç ä¸­çš„åå¥½ã€‚</li>
<li>æå‡ºäº†VeriCodeåˆ†ç±»ï¼ŒåŒ…å«30ç§å¯éªŒè¯çš„ä»£ç æŒ‡ä»¤åŠå…¶éªŒè¯å™¨ã€‚</li>
<li>é€šè¿‡å»ºç«‹Vibe Checkeræµ‹è¯•å¹³å°ï¼Œå¯ä»¥è¯„ä¼°ä»£ç æŒ‡ä»¤éµå¾ªå’ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>è¯„ä¼°å‘ç°ï¼Œå³ä½¿æ˜¯å¼ºå¤§çš„LLMä¹Ÿéš¾ä»¥å®Œå…¨éµå¾ªå¤šä¸ªæŒ‡ä»¤ï¼Œå­˜åœ¨åŠŸèƒ½å›å½’ç°è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7a5581bd66b3b5f63ccf5be1157bd3fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082930&auth_key=1760082930-0-0-23dc53906448752fd64407908ae4a066&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ecd893a686c8fca91e01687151efa5bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082938&auth_key=1760082938-0-0-0670dd5cfa4a87a7d962daeab1829fc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce20eece145704c33016a42b614cdc98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082945&auth_key=1760082945-0-0-6735917d4ac9b8a32b5b126ad6e33066&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57e86c8476b803b2ffdf5b1932087d62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082952&auth_key=1760082952-0-0-18cad3d6ac544d9a35afca10743cb8b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prompt-Synthesize-Fine-Tune-A-Secure-Code-Generation-Recipe"><a href="#Prompt-Synthesize-Fine-Tune-A-Secure-Code-Generation-Recipe" class="headerlink" title="Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe"></a>Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe</h2><p><strong>Authors:Junjie Li, Fazle Rabbi, Bo Yang, Song Wang, Jinqiu Yang</strong></p>
<p>Although Large Language Models (LLMs) show promising solutions to automated code generation, they often produce insecure code that threatens software security. Current approaches (e.g., SafeCoder) to improve secure code generation suffer from limited and imbalanced datasets, reducing their effectiveness and generalizability. In this work, we present Secure-Instruct, a novel framework that automatically synthesizes high-quality vulnerable and secure code examples, generates fine-tuning instructions, and instruction-tunes LLMs to align task description and secure code generation abilities. We evaluate Secure-Instruct on four representative LLMs using two benchmarks: our own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44 CWEs, all without overlap with Secure-Instructâ€™s synthetic instruction-tuning dataset, while CWEval covers 31 CWEs with 119 manually verified security-critical tasks. We find that Secure-Instruct improves not only the security but also the functional correctness of the generated code. On CWEBench, Secure-Instruct substantially improves secure code generation, giving a 14.3% average increase in secure ratio over the pretrained models and outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14% increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained models, and surpasses SafeCoder by 15.8% and 6.8% respectively. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿå¨èƒè½¯ä»¶å®‰å…¨çš„ä¸å®‰å…¨ä»£ç ã€‚å½“å‰æ”¹è¿›å®‰å…¨ä»£ç ç”Ÿæˆçš„æ–¹æ³•ï¼ˆä¾‹å¦‚SafeCoderï¼‰å—é™äºæ•°æ®é›†çš„å¤§å°å’Œä¸å¹³è¡¡ï¼Œé™ä½äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Secure-Instructï¼Œä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨åˆæˆé«˜è´¨é‡è„†å¼±å’Œå®‰å…¨ä»£ç ç¤ºä¾‹ã€ç”Ÿæˆå¾®è°ƒæŒ‡ä»¤ã€å¹¶æ ¹æ®æŒ‡ä»¤è°ƒæ•´LLMä»¥åŒ¹é…ä»»åŠ¡æè¿°å’Œå®‰å…¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä»£è¡¨æ€§LLMä¸Šè¯„ä¼°äº†Secure-Instructï¼Œä½¿ç”¨äº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼šæˆ‘ä»¬è‡ªå·±çš„CWEBenchå’Œç°æœ‰çš„CWEvalã€‚CWEBenchåŒ…å«44ä¸ªCWEçš„93ä¸ªåœºæ™¯ï¼Œæ‰€æœ‰åœºæ™¯ä¸Secure-Instructçš„åˆæˆæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†æ²¡æœ‰é‡å ï¼Œè€ŒCWEvalæ¶µç›–31ä¸ªCWEçš„119ä¸ªç»è¿‡æ‰‹åŠ¨éªŒè¯çš„å®‰å…¨å…³é”®ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°Secure-Instructä¸ä»…æé«˜äº†ç”Ÿæˆçš„ä»£ç çš„å®‰å…¨æ€§ï¼Œè¿˜æé«˜äº†å…¶åŠŸèƒ½æ€§æ­£ç¡®æ€§ã€‚åœ¨CWEBenchä¸Šï¼ŒSecure-Instructæ˜¾è‘—æé«˜äº†å®‰å…¨ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å®‰å…¨æ¯”ç‡è¾ƒé¢„è®­ç»ƒæ¨¡å‹å¹³å‡æé«˜äº†14.3%ï¼Œå¹¶ä¼˜äºSafeCoder 7.6%ã€‚åœ¨CWEvalä¸Šï¼ŒSecure-Instructåœ¨Func-Sec@1æŒ‡æ ‡ä¸Šè¾ƒé¢„è®­ç»ƒæ¨¡å‹æé«˜äº†CodeLlama-7Bçš„14%å’ŒMistral-7Bçš„5.8%ï¼Œå¹¶åˆ†åˆ«è¶…è¿‡äº†SafeCoderçš„15.8%å’Œ6.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07189v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¸¸å¸¸ç”Ÿæˆå¨èƒè½¯ä»¶å®‰å…¨çš„ä»£ç ã€‚å½“å‰å¦‚SafeCoderç­‰æ”¹å–„å®‰å…¨ä»£ç ç”Ÿæˆçš„æ–¹æ³•å—é™äºæ•°æ®é›†ï¼Œå½±å“æ•ˆæœå’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºSecure-Instructæ¡†æ¶ï¼Œå¯è‡ªåŠ¨åˆæˆé«˜è´¨é‡çš„å®‰å…¨å’Œæ¼æ´ä»£ç ç¤ºä¾‹ï¼Œç”Ÿæˆå¾®è°ƒæŒ‡ä»¤å¹¶å¯¹LLMè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥æå‡ä»»åŠ¡æè¿°ä¸å®‰å…¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„å¯¹é½ã€‚åœ¨è‡ªå®šä¹‰çš„CWEBenchå’Œç°æœ‰çš„CWEvalä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°æ˜¾ç¤ºï¼ŒSecure-Instructä¸ä»…æé«˜äº†ä»£ç çš„å®‰å…¨æ€§ï¼Œè¿˜æå‡äº†åŠŸèƒ½æ­£ç¡®æ€§ã€‚ç›¸è¾ƒäºé¢„è®­ç»ƒæ¨¡å‹ï¼ŒSecure-Instructåœ¨CWEBenchä¸Šçš„å®‰å…¨æ¯”ç‡å¹³å‡æé«˜äº†14.3%ï¼Œå¹¶ä¼˜äºSafeCoderçš„7.6%ã€‚åœ¨CWEvalä¸Šï¼ŒSecure-Instructå¯¹CodeLlama-7Bå’ŒMistral-7Bçš„Func-Sec@1åˆ†åˆ«æé«˜äº†14%å’Œ5.8%ï¼Œå¹¶ä¸”ä¼˜äºSafeCoderã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ—¶æ˜“äº§ç”Ÿå¨èƒè½¯ä»¶å®‰å…¨çš„ä»£ç ã€‚</li>
<li>å½“å‰æ”¹å–„å®‰å…¨ä»£ç ç”Ÿæˆçš„æ–¹æ³•å¦‚SafeCoderå—é™äºæ•°æ®é›†ï¼Œå½±å“æ•ˆæœå’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Secure-Instructæ¡†æ¶é€šè¿‡è‡ªåŠ¨åˆæˆé«˜è´¨é‡çš„å®‰å…¨å’Œæ¼æ´ä»£ç ç¤ºä¾‹ï¼Œå¹¶ç”Ÿæˆå¾®è°ƒæŒ‡ä»¤æ¥æå‡LLMçš„æ€§èƒ½ã€‚</li>
<li>Secure-Instructæé«˜äº†ä»£ç çš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>åœ¨CWEBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSecure-Instructç›¸è¾ƒäºé¢„è®­ç»ƒæ¨¡å‹å®‰å…¨æ¯”ç‡å¹³å‡æé«˜äº†14.3%ï¼Œå¹¶ä¼˜äºSafeCoderã€‚</li>
<li>åœ¨CWEvalåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSecure-Instructå¯¹CodeLlama-7Bå’ŒMistral-7Bçš„Func-Sec@1æœ‰æ‰€æé«˜ã€‚</li>
<li>Secure-Instructæ¡†æ¶çš„æå‡ºä¸ºæ”¹å–„LLMåœ¨å®‰å…¨ä»£ç ç”Ÿæˆæ–¹é¢çš„ä¸è¶³æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-397bde8b1075cbf7f5f054794a64df67~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082959&auth_key=1760082959-0-0-62d32986315006ad15002bcaeb8c1a4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63775f37e07fc3114fbf848892771d4d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082968&auth_key=1760082968-0-0-e238dc00a110539e7c327b29a44e0772&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a29498bdcee89500a6bafc54fa3d7c72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082974&auth_key=1760082974-0-0-449da3cbc39b71d19a065a59e380abe4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TRIM-Token-wise-Attention-Derived-Saliency-for-Data-Efficient-Instruction-Tuning"><a href="#TRIM-Token-wise-Attention-Derived-Saliency-for-Data-Efficient-Instruction-Tuning" class="headerlink" title="TRIM: Token-wise Attention-Derived Saliency for Data-Efficient   Instruction Tuning"></a>TRIM: Token-wise Attention-Derived Saliency for Data-Efficient   Instruction Tuning</h2><p><strong>Authors:Manish Nagaraj, Sakshi Choudhary, Utkarsh Saxena, Deepak Ravikumar, Kaushik Roy</strong></p>
<p>Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though curating them remains challenging. Existing methods often rely on coarse, sample-level signals like gradients, an approach that is computationally expensive and overlooks fine-grained features. To address this, we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a forward-only, token-centric framework. Instead of using gradients, TRIM operates by matching underlying representational patterns identified via attention-based â€œfingerprintsâ€ from a handful of target samples. Such an approach makes TRIM highly efficient and uniquely sensitive to the structural features that define a task. Coresets selected by our method consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. By avoiding expensive backward passes, TRIM achieves this at a fraction of the computational cost. These findings establish TRIM as a scalable and efficient alternative for building high-quality instruction-tuning datasets. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä¸‹æ¸¸ä»»åŠ¡å¯¹é½è‡³å…³é‡è¦ï¼Œé€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„è¯­æ–™åº“ã€‚ç„¶è€Œï¼Œå°å‹é«˜è´¨é‡å­é›†ï¼Œä¹Ÿç§°ä¸ºæ ¸å¿ƒé›†ï¼Œå¯ä»¥äº§ç”Ÿç›¸å½“æˆ–æ›´å¥½çš„ç»“æœï¼Œå°½ç®¡å¯¹å…¶è¿›è¡Œç­–åˆ’ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç²—ç•¥çš„æ ·æœ¬çº§ä¿¡å·ï¼Œå¦‚æ¢¯åº¦ï¼Œè¿™ç§æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œå¹¶ä¸”å¿½ç•¥äº†ç»†ç²’åº¦ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TRIMï¼ˆé€šè¿‡å¯è§£é‡Šçš„å¤šå±‚æ³¨æ„åŠ›è·å–ä»¤ç‰Œç›¸å…³æ€§ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»…å‰å‘ã€ä»¥ä»¤ç‰Œä¸ºä¸­å¿ƒçš„æ¡†æ¶ã€‚TRIMä¸é€šè¿‡æ¢¯åº¦å·¥ä½œï¼Œè€Œæ˜¯é€šè¿‡åŒ¹é…å°‘é‡ç›®æ ‡æ ·æœ¬ä¸­é€šè¿‡æ³¨æ„åŠ›â€œæŒ‡çº¹â€è¯†åˆ«å‡ºçš„æ½œåœ¨è¡¨ç¤ºæ¨¡å¼æ¥è¿è¡Œã€‚è¿™ç§æ–¹æ³•ä½¿TRIMå…·æœ‰é«˜æ•ˆç‡ï¼Œå¹¶ä¸”å¯¹å®šä¹‰ä»»åŠ¡çš„ç»“æ„ç‰¹å¾å…·æœ‰ç‹¬ç‰¹æ•æ„Ÿæ€§ã€‚é€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•é€‰æ‹©çš„æ ¸å¿ƒé›†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå§‹ç»ˆè¶…è¿‡æœ€æ–°åŸºçº¿é«˜è¾¾9%ï¼Œå¹¶åœ¨æŸäº›è®¾ç½®ä¸‹ç”šè‡³è¶…è¿‡å…¨æ•°æ®ç²¾ç»†è°ƒæ•´çš„æ€§èƒ½ã€‚é€šè¿‡é¿å…æ˜‚è´µçš„åå‘ä¼ é€’ï¼ŒTRIMä»¥è¾ƒå°çš„è®¡ç®—æˆæœ¬å®ç°äº†è¿™ä¸€ç‚¹ã€‚è¿™äº›å‘ç°è¯æ˜TRIMæ˜¯ä¸€ç§å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ„å»ºé«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07118v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤è°ƒæ•´å¯¹äºä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œé€šå¸¸ä¾èµ–äºå¤§é‡ã€å¤šæ ·çš„è¯­æ–™åº“ã€‚ç„¶è€Œï¼Œå°çš„ã€é«˜è´¨é‡çš„å­é›†ï¼ˆç§°ä¸ºæ ¸å¿ƒé›†ï¼‰å¯ä»¥äº§ç”Ÿç›¸å½“æˆ–æ›´å¥½çš„ç»“æœï¼Œä½†æŒ‘é€‰å®ƒä»¬ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç²—ç³™çš„æ ·æœ¬çº§ä¿¡å·ï¼Œå¦‚æ¢¯åº¦ï¼Œè¿™ç§æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œå¿½ç•¥äº†ç»†å¾®çš„ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºTRIMï¼ˆé€šè¿‡å¯è§£é‡Šçš„å¤šå±‚æ³¨æ„åŠ›è·å–ä»¤ç‰Œç›¸å…³æ€§ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»…å‰å‘ä¼ æ’­çš„ã€ä»¥ä»¤ç‰Œä¸ºä¸­å¿ƒçš„æ¡†æ¶ã€‚TRIMä¸é€šè¿‡æ¢¯åº¦å·¥ä½œï¼Œè€Œæ˜¯åŒ¹é…æ¥è‡ªå°‘é‡ç›®æ ‡æ ·æœ¬çš„åŸºäºæ³¨æ„åŠ›çš„â€œæŒ‡çº¹â€æ¥è¯†åˆ«æ½œåœ¨çš„æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•ä½¿TRIMæ—¢é«˜æ•ˆåˆç‹¬ç‰¹åœ°å…³æ³¨å®šä¹‰ä»»åŠ¡çš„ç»“æ„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‰æ‹©çš„æ ¸å¿ƒé›†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿é«˜è¾¾9%ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†å…¨æ•°æ®å¾®è°ƒçš„æ€§èƒ½ã€‚é€šè¿‡é¿å…æ˜‚è´µçš„åå‘ä¼ æ’­ï¼ŒTRIMä»¥æä½çš„è®¡ç®—æˆæœ¬å®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚è¿™äº›å‘ç°è¯æ˜äº†TRIMä½œä¸ºä¸€ç§å¯æ‰©å±•å’Œé«˜æ•ˆçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†æ„å»ºæ–¹æ³•çš„åœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤è°ƒæ•´å¯¹äºLLMçš„ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œéœ€è¦ç”¨åˆ°å¤§é‡ã€å¤šæ ·çš„è¯­æ–™åº“è¿›è¡Œæ”¯æŒã€‚</li>
<li>æ ¸å¿ƒé›†çš„é€‰æ‹©æ˜¯æé«˜æŒ‡ä»¤è°ƒæ•´æ•ˆç‡çš„å…³é”®ï¼Œä½†å…¶æŒ‘é€‰è¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–æ ·æœ¬çº§ä¿¡å·å¦‚æ¢¯åº¦è¿›è¡Œæ ¸å¿ƒé›†é€‰æ‹©ï¼Œè®¡ç®—é‡å¤§ä¸”å¿½ç•¥ç»†å¾®ç‰¹å¾ã€‚</li>
<li>TRIMæ¡†æ¶é€šè¿‡åŒ¹é…åŸºäºæ³¨æ„åŠ›â€œæŒ‡çº¹â€çš„æ½œåœ¨æ¨¡å¼è¿›è¡Œæ ¸å¿ƒé›†é€‰æ‹©ï¼Œæ— éœ€ä¾èµ–æ¢¯åº¦ä¿¡æ¯ã€‚</li>
<li>TRIMæ–¹æ³•é€‰æ‹©çš„æ ¸å¿ƒé›†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºæœ€æ–°åŸºçº¿æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>TRIMæ–¹æ³•é¿å…äº†æ˜‚è´µçš„åå‘ä¼ æ’­è¿‡ç¨‹ï¼Œè®¡ç®—æ•ˆç‡æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cae524edf5bd9a184fff3d22b02bb973~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099167&auth_key=1760099167-0-0-d0a13dcaa22b5e662399a7d1eb00dd96&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89d6699e4d8e4ae7b13c407c975edcf3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083046&auth_key=1760083046-0-0-4055c8ded77a99b4aaaad320899101c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0449b7ae9671b4cf72636f913854e246~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083053&auth_key=1760083053-0-0-1962fb6141e783af7e84d28a33cdeaac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27c6f538feaf13f618ee1128f6c1ee59~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083060&auth_key=1760083060-0-0-793d40fbdc9b97cf07a6ec13738a2d7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4945dd839fc7ef83c0a87db41e18fca9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083066&auth_key=1760083066-0-0-1f532c9ab424366cf524a52ed27c8b34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GPT-5-Model-Corrected-GPT-4Vâ€™s-Chart-Reading-Errors-Not-Prompting"><a href="#GPT-5-Model-Corrected-GPT-4Vâ€™s-Chart-Reading-Errors-Not-Prompting" class="headerlink" title="GPT-5 Model Corrected GPT-4Vâ€™s Chart Reading Errors, Not Prompting"></a>GPT-5 Model Corrected GPT-4Vâ€™s Chart Reading Errors, Not Prompting</h2><p><strong>Authors:Kaichun Yang, Jian Chen</strong></p>
<p>We present a quantitative evaluation to understand the effect of zero-shot large-language model (LLMs) and prompting uses on chart reading tasks. We asked LLMs to answer 107 visualization questions to compare inference accuracies between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances, where GPT-4V failed to produce correct answers. Our results show that model architecture dominates the inference accuracy: GPT5 largely improved accuracy, while prompt variants yielded only small effects. Pre-registration of this work is available here: <a target="_blank" rel="noopener" href="https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3">https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3</a>; the Google Drive materials are here:<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view">https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾è¡¨é˜…è¯»ä»»åŠ¡ä¸­çš„å½±å“è¿›è¡Œäº†å®šé‡è¯„ä¼°ï¼Œå¹¶æ¢è®¨äº†æç¤ºçš„ä½¿ç”¨æ•ˆæœã€‚æˆ‘ä»¬è¦æ±‚LLMå›ç­”107ä¸ªå¯è§†åŒ–é—®é¢˜ï¼Œä»¥æ¯”è¾ƒGPT-5å’ŒGPT-4Våœ¨å¤šæ¨¡æ€å›°éš¾å›¾åƒå®ä¾‹ä¸Šçš„æ¨ç†å‡†ç¡®æ€§ï¼Œå…¶ä¸­GPT-4Væœªèƒ½ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹æ¶æ„å¯¹æ¨ç†ç²¾åº¦èµ·ä¸»å¯¼ä½œç”¨ï¼šGPT-5å¤§å¤§æé«˜äº†ç²¾åº¦ï¼Œè€Œæç¤ºå˜ä½“åªäº§ç”Ÿäº†å¾®å°å½±å“ã€‚æœ¬å·¥ä½œçš„é¢„æ³¨å†Œä¿¡æ¯å¯åœ¨æ­¤å¤„æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3">é“¾æ¥</a>ï¼›è°·æ­Œé©±åŠ¨ææ–™å¯åœ¨æ­¤å¤„æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦è¯„ä¼°äº†é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å›¾è¡¨é˜…è¯»ä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœï¼Œå¯¹æ¯”äº†GPT-5å’ŒGPT-4Våœ¨è§£å†³å¯è§†åŒ–é—®é¢˜ä¸Šçš„æ¨ç†å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹æ¶æ„å¯¹æ¨ç†å‡†ç¡®æ€§èµ·ä¸»å¯¼ä½œç”¨ï¼ŒGPT-5æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œæç¤ºå˜ä½“åªäº§ç”Ÿäº†è¾ƒå°çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å¯¹é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å›¾è¡¨é˜…è¯»ä»»åŠ¡ä¸­çš„æ•ˆæœè¿›è¡Œäº†å®šé‡è¯„ä¼°ã€‚</li>
<li>å¯¹æ¯”äº†GPT-5å’ŒGPT-4Våœ¨è§£å†³å¯è§†åŒ–é—®é¢˜ä¸Šçš„æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹æ¶æ„å¯¹æ¨ç†å‡†ç¡®æ€§æœ‰ä¸»å¯¼ä½œç”¨ï¼ŒGPT-5è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>æç¤ºå˜ä½“å¯¹æ¨ç†å‡†ç¡®æ€§çš„å½±å“è¾ƒå°ã€‚</li>
<li>æä¾›äº†ç ”ç©¶çš„å‰æ³¨å†Œé“¾æ¥å’ŒGoogle Driveææ–™ä¾›è¯»è€…å‚è€ƒã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è§£å†³å›°éš¾å›¾åƒå®ä¾‹æ—¶ï¼Œé€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„æ˜¯å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bc18414005d2eed9c057dcb285a52a8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099175&auth_key=1760099175-0-0-c979514984b77d73dc7f9cc3cc5e5f3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27559b03e3bc20b4de39b5916f325146~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099183&auth_key=1760099183-0-0-c73dab33fc5b34689f0fd5abde2c0033&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03222eb75a296162d926d6e9c1648e78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099190&auth_key=1760099190-0-0-266545a4d0531291b1bf1d66f220225b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1359bf837cdd653cb1fbc640aec07eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099196&auth_key=1760099196-0-0-3565e2b99c8368866d62d09044812638&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-caa6ea8c1350e7bb1eae0e73a7ad9598~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099203&auth_key=1760099203-0-0-e6668f4674c587b26dca77b5ab0235fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeFormer-Transformer-with-Attention-Modulation-Empowered-by-Temporal-Characteristics-for-Time-Series-Forecasting"><a href="#TimeFormer-Transformer-with-Attention-Modulation-Empowered-by-Temporal-Characteristics-for-Time-Series-Forecasting" class="headerlink" title="TimeFormer: Transformer with Attention Modulation Empowered by Temporal   Characteristics for Time Series Forecasting"></a>TimeFormer: Transformer with Attention Modulation Empowered by Temporal   Characteristics for Time Series Forecasting</h2><p><strong>Authors:Zhipeng Liu, Peibo Duan, Xuan Tang, Baixin Li, Yongsheng Huang, Mingyang Geng, Changsheng Zhang, Bin Zhang, Binwu Wang</strong></p>
<p>Although Transformers excel in natural language processing, their extension to time series forecasting remains challenging due to insufficient consideration of the differences between textual and temporal modalities. In this paper, we develop a novel Transformer architecture designed for time series data, aiming to maximize its representational capacity. We identify two key but often overlooked characteristics of time series: (1) unidirectional influence from the past to the future, and (2) the phenomenon of decaying influence over time. These characteristics are introduced to enhance the attention mechanism of Transformers. We propose TimeFormer, whose core innovation is a self-attention mechanism with two modulation terms (MoSA), designed to capture these temporal priors of time series under the constraints of the Hawkes process and causal masking. Additionally, TimeFormer introduces a framework based on multi-scale and subsequence analysis to capture semantic dependencies at different temporal scales, enriching the temporal dependencies. Extensive experiments conducted on multiple real-world datasets show that TimeFormer significantly outperforms state-of-the-art methods, achieving up to a 7.45% reduction in MSE compared to the best baseline and setting new benchmarks on 94.04% of evaluation metrics. Moreover, we demonstrate that the MoSA mechanism can be broadly applied to enhance the performance of other Transformer-based models. </p>
<blockquote>
<p>å°½ç®¡Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºæœªèƒ½å……åˆ†è€ƒè™‘åˆ°æ–‡æœ¬å’Œæ—¶é—´æ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œå°†å…¶åº”ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„æ–°å‹Transformeræ¶æ„ï¼Œæ—¨åœ¨æœ€å¤§é™åº¦åœ°æé«˜å…¶è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬ç¡®å®šäº†æ—¶é—´åºåˆ—çš„ä¸¤ä¸ªå…³é”®ä½†å¸¸è¢«å¿½è§†çš„ç‰¹å¾ï¼šï¼ˆ1ï¼‰è¿‡å»å¯¹æœªæ¥å•å‘å½±å“ï¼›ï¼ˆ2ï¼‰éšæ—¶é—´æµé€å½±å“è¡°å‡çš„ç°è±¡ã€‚è¿™äº›ç‰¹æ€§è¢«å¼•å…¥åˆ°å¢å¼ºTransformerçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†TimeFormerï¼Œå…¶æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä¸€ç§å¸¦æœ‰ä¸¤ä¸ªè°ƒåˆ¶é¡¹ï¼ˆMoSAï¼‰çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨åœ¨æ»¡è¶³éœå…‹æ–¯è¿‡ç¨‹å’Œå› æœæ©ç çº¦æŸçš„æ¡ä»¶ä¸‹ï¼Œæ•æ‰è¿™äº›æ—¶é—´åºåˆ—çš„æ—¶é—´å…ˆéªŒä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒTimeFormerè¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤šå°ºåº¦å’Œå­åºåˆ—åˆ†æçš„æ¡†æ¶ï¼Œä»¥æ•è·ä¸åŒæ—¶é—´å°ºåº¦ä¸Šçš„è¯­ä¹‰ä¾èµ–å…³ç³»ï¼Œä¸°å¯Œæ—¶é—´ä¾èµ–æ€§ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTimeFormeræ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ–¹é¢ç›¸æ¯”æœ€ä½³åŸºçº¿é™ä½äº†é«˜è¾¾7.45%ï¼Œåœ¨è¯„ä»·æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†94.04%çš„æ–°åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†MoSAæœºåˆ¶å¯ä»¥å¹¿æ³›åº”ç”¨äºæé«˜å…¶ä»–åŸºäºTransformerçš„æ¨¡å‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06680v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è™½ç„¶Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæœªèƒ½å……åˆ†è€ƒè™‘æ–‡æœ¬å’Œæ—¶é—´æ¨¡æ€ä¹‹é—´çš„å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„æ–°å‹Transformeræ¶æ„ï¼Œæ—¨åœ¨æé«˜å…¶è¡¨ç¤ºèƒ½åŠ›ã€‚æ–‡ç« å¼ºè°ƒäº†æ—¶é—´åºåˆ—çš„ä¸¤ä¸ªå…³é”®ä½†å¸¸è¢«å¿½è§†çš„ç‰¹æ€§ï¼šè¿‡å»å¯¹æœªæ¥å•å‘å½±å“ä»¥åŠå½±å“éšæ—¶é—´è¡°å‡çš„ç°è±¡ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†TimeFormerï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºè®¾è®¡äº†ä¸€ç§å¸¦æœ‰ä¸¤ä¸ªè°ƒåˆ¶é¡¹çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMoSAï¼‰ï¼Œèƒ½å¤Ÿåœ¨éœå…‹æ–¯è¿‡ç¨‹å’Œå› æœæ©ç çš„çº¦æŸä¸‹æ•æ‰è¿™äº›æ—¶é—´å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒTimeFormerè¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤šå°ºåº¦å’Œå­åºåˆ—åˆ†æçš„æ¡†æ¶ï¼Œä»¥åœ¨ä¸åŒçš„æ—¶é—´å°ºåº¦ä¸Šæ•æ‰è¯­ä¹‰ä¾èµ–å…³ç³»ï¼Œä¸°å¯Œæ—¶é—´ä¾èµ–æ€§ã€‚åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTimeFormeræ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨å‡æ–¹è¯¯å·®ä¸Šæœ€å¤šå‡å°‘7.45%ï¼Œå¹¶åœ¨94.04%çš„è¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°æ–°çš„åŸºå‡†æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†MoSAæœºåˆ¶å¯ä»¥å¹¿æ³›åº”ç”¨äºæé«˜å…¶ä»–åŸºäºTransformerçš„æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è™½ç„¶Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹çš„åº”ç”¨ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ–‡ç« æå‡ºäº†é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„æ–°å‹Transformeræ¶æ„â€”â€”TimeFormerã€‚</li>
<li>TimeFormerè€ƒè™‘äº†æ—¶é—´åºåˆ—çš„ä¸¤ä¸ªå…³é”®ç‰¹æ€§ï¼šè¿‡å»å¯¹æœªæ¥çš„å•å‘å½±å“ä»¥åŠå½±å“éšæ—¶é—´è¡°å‡çš„ç°è±¡ã€‚</li>
<li>TimeFormerå¼•å…¥äº†å¸¦æœ‰ä¸¤ä¸ªè°ƒåˆ¶é¡¹çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMoSAï¼‰ï¼Œä»¥æ•æ‰æ—¶é—´å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>TimeFormeråœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸æ¯”æœ€ä½³åŸºçº¿æ–¹æ³•é™ä½äº†MSEã€‚</li>
<li>MoSAæœºåˆ¶å¯å¹¿æ³›åº”ç”¨äºå¢å¼ºå…¶ä»–åŸºäºTransformerçš„æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2306c7363eb2084a41e0aa5d79edf664~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099210&auth_key=1760099210-0-0-9c91a1ccfa7ed3a6c35fea60eb169aed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-538643174db22a58d97784abe22eda6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102466&auth_key=1760102466-0-0-94f43dc656404954d2d6f740bf43435c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b2c9837752d35767aba318f445f6fd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099281&auth_key=1760099281-0-0-fa301e7e1e9af23c7cb80026b67e5d5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reproducibility-Study-of-â€œXRec-Large-Language-Models-for-Explainable-Recommendationâ€"><a href="#Reproducibility-Study-of-â€œXRec-Large-Language-Models-for-Explainable-Recommendationâ€" class="headerlink" title="Reproducibility Study of â€œXRec: Large Language Models for Explainable   Recommendationâ€"></a>Reproducibility Study of â€œXRec: Large Language Models for Explainable   Recommendationâ€</h2><p><strong>Authors:Ranjan Mishra, Julian I. Bibo, Quinten van Engelen, Henk Schaapman</strong></p>
<p>In this study, we reproduced the work done in the paper â€œXRec: Large Language Models for Explainable Recommendationâ€ by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRecâ€™s Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at <a target="_blank" rel="noopener" href="https://github.com/julianbibo/xrec-reproducibility">https://github.com/julianbibo/xrec-reproducibility</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®ç°äº†Maç­‰äººäº2024å¹´å‘è¡¨çš„è®ºæ–‡â€œXRecï¼šç”¨äºå¯è§£é‡Šæ¨èçš„å¤§å‹è¯­è¨€æ¨¡å‹â€ä¸­çš„å·¥ä½œã€‚åŸå§‹ä½œè€…ä»‹ç»äº†XRecï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³çš„ååŒæŒ‡ä»¤å¾®è°ƒæ¡†æ¶ï¼Œå®ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿä¸ºç”¨æˆ·ç”Ÿæˆæ¨èæä¾›å…¨é¢çš„è§£é‡Šã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¤åˆ¶åŸå§‹è®ºæ–‡çš„ç»“æœï¼Œä½†ä½¿ç”¨Llama 3ä½œä¸ºè¯„ä¼°çš„LLMï¼Œè€Œä¸æ˜¯GPT-3.5-turboã€‚æˆ‘ä»¬åŸºäºMaç­‰äººæä¾›çš„æºä»£ç ï¼ˆ2024å¹´ï¼‰æ¥å®ç°æˆ‘ä»¬çš„ç›®æ ‡ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡ä¿®æ”¹XRecçš„Mixture of Expertsæ¨¡å—çš„è¾“å…¥åµŒå…¥æˆ–åˆ é™¤å…¶è¾“å‡ºåµŒå…¥æ¥æ‰©å±•åŸå§‹è®ºæ–‡ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»“æœï¼ŒXRecæœ‰æ•ˆåœ°ç”Ÿæˆäº†ä¸ªæ€§åŒ–è§£é‡Šï¼Œé€šè¿‡å¼•å…¥ååŒä¿¡æ¯æé«˜äº†å…¶ç¨³å®šæ€§ã€‚ç„¶è€Œï¼ŒXRecå¹¶æœªåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†Mixture of ExpertsåµŒå…¥åœ¨å¡‘é€ è§£é‡Šç»“æ„ä¸­çš„é‡è¦æ€§ï¼Œå±•ç¤ºäº†ååŒä¿¡å·å¦‚ä½•ä¸è¯­è¨€å»ºæ¨¡è¿›è¡Œäº¤äº’ã€‚é€šè¿‡æˆ‘ä»¬çš„å·¥ä½œï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€æºè¯„ä¼°å®ç°ï¼Œå¢å¼ºäº†ç ”ç©¶è€…å’Œå®è·µè€…çš„å¯è®¿é—®æ€§ã€‚æˆ‘ä»¬çš„å®Œæ•´ä»£ç ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/julianbibo/xrec-reproducibility%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/julianbibo/xrec-reproducibilityæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶ä¸­ï¼ŒæˆåŠŸå¤åˆ¶äº†Maç­‰äººï¼ˆäºæŠ¥å‘Šæˆªè‡³æ—¶é—´æ¬¡å¹´ï¼‰ã€ŠåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§£é‡Šæ€§æ¨èç ”ç©¶ã€‹ï¼ˆç®€ç§°â€œXRecâ€ï¼‰çš„æˆæœï¼Œæ—¨åœ¨éªŒè¯è¯¥æ¨èæ¨¡å‹çš„æ³›åŒ–æ€§ï¼Œå¹¶å°†å¼€æºå·¥å…·æ‰©å±•ä»¥é€‚åº”è§£é‡Šçš„äº¤äº’æ¨¡å¼ã€‚ä»¥åŸºäºè¯­è¨€æ¨¡å‹çš„ååŒè°ƒæ•´æŒ‡ä»¤å¯¹åŸä»£ç è¿›è¡Œæ”¹è¿›ï¼Œå¹¶å‘ç°XRecåœ¨ç”Ÿæˆä¸ªæ€§åŒ–è§£é‡Šæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å¹¶éåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶äº®ç‚¹åœ¨äºæ­ç¤ºäº†ä¸“å®¶æ··åˆåµŒå…¥åœ¨è§£é‡Šç»“æ„å½¢æˆä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸”æœ‰åŠ©äºå±•ç¤ºååŒä¿¡å·ä¸è¯­è¨€æ¨¡å‹çš„ç›¸äº’ä½œç”¨æœºåˆ¶ã€‚æ›´å¤šè¯¦ç»†ç ”ç©¶èµ„æ–™å¯è®¿é—®ç›¸å…³ä»£ç åº“ï¼ˆé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/julianbibo/xrec-reproducibility%EF%BC%89%E3%80%82">https://github.com/julianbibo/xrec-reproducibilityï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ ¹æ®æä¾›çš„æ–‡æœ¬æç‚¼å‡ºçš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a7f0d32406138763b110c379c4e8708d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099288&auth_key=1760099288-0-0-6a2fc9f0f767b4e73fe44cd141049dbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4e2439cc819cc7708c6f93f065fd533~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099296&auth_key=1760099296-0-0-241ccd7ce01934afb674ce7d0ee4da2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Knowledge-Graph-Guided-Multi-Agent-Distillation-for-Reliable-Industrial-Question-Answering-with-Datasets"><a href="#Knowledge-Graph-Guided-Multi-Agent-Distillation-for-Reliable-Industrial-Question-Answering-with-Datasets" class="headerlink" title="Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial   Question Answering with Datasets"></a>Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial   Question Answering with Datasets</h2><p><strong>Authors:Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang</strong></p>
<p>Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/erwinmsmith/KG-MAD/">https://github.com/erwinmsmith/KG-MAD/</a>. </p>
<blockquote>
<p>äº§ä¸šé—®ç­”ç³»ç»Ÿï¼ˆQAï¼‰å¯¹å®‰å…¨æ€§å’Œå¯é æ€§çš„è¦æ±‚é«˜äºé€šç”¨å¯¹è¯æ¨¡å‹ï¼Œå› ä¸ºåœ¨è®¾å¤‡æ•…éšœè¯Šæ–­ç­‰é«˜é£é™©åœºæ™¯ä¸­å‡ºé”™ä¼šäº§ç”Ÿä¸¥é‡åæœã€‚è™½ç„¶å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºäº†æ¨ç†æ·±åº¦ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸å¯æ§è¿­ä»£å’Œä¸å¯éªŒè¯è¾“å‡ºçš„é—®é¢˜ï¼Œä¼ ç»Ÿè’¸é¦æ–¹æ³•éš¾ä»¥å°†åä½œæ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è½»é‡çº§ã€å¯éƒ¨ç½²çš„å­¦ç”Ÿæ¨¡å‹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å›¾è°±å¼•å¯¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè’¸é¦ï¼ˆKG-MASDï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è’¸é¦å…¬å¼åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶èåˆçŸ¥è¯†å›¾è°±ä½œä¸ºå¯éªŒè¯çš„ç»“æ„åŒ–å…ˆéªŒæ¥ä¸°å¯ŒçŠ¶æ€è¡¨ç¤ºå¹¶ç¡®ä¿æ”¶æ•›ã€‚é€šè¿‡ç»“åˆåä½œæ¨ç†å’ŒçŸ¥è¯†æ¥åœ°ï¼ŒKG-MASDç”Ÿæˆé«˜ç½®ä¿¡åº¦æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œå¹¶å°†æ¨ç†æ·±åº¦å’Œå¯éªŒè¯æ€§è”åˆè’¸é¦æˆé€‚ç”¨äºè¾¹ç¼˜éƒ¨ç½²çš„ç´§å‡‘å­¦ç”Ÿæ¨¡å‹ã€‚åœ¨å·¥ä¸šQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒKG-MASDçš„å‡†ç¡®ç‡æé«˜äº†2.4%è‡³20.1%ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å¯é æ€§ï¼Œå¯åœ¨å®‰å…¨å…³é”®å·¥ä¸šåœºæ™¯ä¸­å®ç°å¯ä¿¡AIéƒ¨ç½²ã€‚ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/erwinmsmith/KG-MAD/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/erwinmsmith/KG-MAD/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06240v1">PDF</a> 41 pages, 12 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å·¥ä¸šé—®ç­”ç³»ç»Ÿç›¸è¾ƒäºé€šç”¨å¯¹è¯æ¨¡å‹éœ€è¦æ›´é«˜çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå› ä¸ºé«˜é£é™©çš„åœºæ™¯ï¼ˆå¦‚è®¾å¤‡æ•…éšœè¯Šæ–­ï¼‰ä¸­çš„é”™è¯¯å¯èƒ½å¯¼è‡´ä¸¥é‡åæœã€‚ä¸ºè§£å†³å¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹å¸¦æ¥çš„æ¨ç†æ·±åº¦å¢å¼ºä½†è¿­ä»£ä¸å¯æ§ã€è¾“å‡ºä¸å¯éªŒè¯çš„é—®é¢˜ï¼Œä»¥åŠä¼ ç»Ÿè’¸é¦æ–¹æ³•éš¾ä»¥å°†åä½œæ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è½»é‡çº§ã€å¯éƒ¨ç½²çš„å­¦ç”Ÿæ¨¡å‹çš„é—®é¢˜ï¼Œæå‡ºäº†çŸ¥è¯†å›¾è°±å¼•å¯¼çš„å¤šä»£ç†ç³»ç»Ÿè’¸é¦ï¼ˆKG-MASDï¼‰ã€‚è¯¥æ–¹æ³•å°†è’¸é¦è¿‡ç¨‹åˆ¶å®šä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥çŸ¥è¯†å›¾è°±ä½œä¸ºå¯éªŒè¯çš„ç»“æ„åŒ–å…ˆéªŒçŸ¥è¯†æ¥ä¸°å¯ŒçŠ¶æ€è¡¨ç¤ºå¹¶ä¿éšœæ”¶æ•›æ€§ã€‚é€šè¿‡å°†åä½œæ¨ç†ä¸çŸ¥è¯†å®šä½ç›¸ç»“åˆï¼ŒKG-MASDç”Ÿæˆäº†é«˜ç½®ä¿¡åº¦çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œå¹¶å°†æ¨ç†æ·±åº¦å’Œå¯éªŒè¯æ€§è”åˆè’¸é¦æˆé€‚åˆè¾¹ç¼˜éƒ¨ç½²çš„ç´§å‡‘å­¦ç”Ÿæ¨¡å‹ã€‚åœ¨å·¥ä¸šé—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒKG-MASDç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æé«˜äº†2.4%è‡³20.1%çš„å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†å¯é æ€§ï¼Œå®ç°äº†å®‰å…¨å…³é”®å·¥ä¸šåœºæ™¯ä¸­å¯ä¿¡äººå·¥æ™ºèƒ½çš„éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä¸šé—®ç­”ç³»ç»Ÿéœ€è¦é«˜å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå› ä¸ºé«˜é£é™©çš„é”™è¯¯å¯èƒ½å¯¼è‡´ä¸¥é‡åæœã€‚</li>
<li>å¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹èƒ½æé«˜æ¨ç†æ·±åº¦ä½†å­˜åœ¨è¿­ä»£ä¸å¯æ§å’Œè¾“å‡ºä¸å¯éªŒè¯çš„é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿè’¸é¦æ–¹æ³•éš¾ä»¥å°†åä½œæ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è½»é‡çº§ã€å¯éƒ¨ç½²çš„å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>KG-MASDæ–¹æ³•ç»“åˆçŸ¥è¯†å›¾è°±å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†å›¾è°±ä½œä¸ºç»“æ„åŒ–å…ˆéªŒçŸ¥è¯†ï¼Œä¸°å¯ŒçŠ¶æ€è¡¨ç¤ºå¹¶ä¿éšœæ”¶æ•›æ€§ã€‚</li>
<li>KG-MASDé€šè¿‡ç»“åˆåä½œæ¨ç†å’ŒçŸ¥è¯†å®šä½ï¼Œç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ã€‚</li>
<li>KG-MASDæé«˜äº†åœ¨å·¥ä¸šé—®ç­”æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯é æ€§ï¼Œé€‚ç”¨äºå®‰å…¨å…³é”®çš„å·¥ä¸šåœºæ™¯éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0781b0bb3bdfa667d300507ae681ffad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099303&auth_key=1760099303-0-0-17c0a9cc81423637178c827b85af0ebb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9905c54f5e6c89da8cbe2c5552924a02~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099311&auth_key=1760099311-0-0-69513e1a523dec45912fa7f4c68de331&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AWARE-Beyond-Sentence-Boundaries-A-Contextual-Transformer-Framework-for-Identifying-Cultural-Capital-in-STEM-Narratives"><a href="#AWARE-Beyond-Sentence-Boundaries-A-Contextual-Transformer-Framework-for-Identifying-Cultural-Capital-in-STEM-Narratives" class="headerlink" title="AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework   for Identifying Cultural Capital in STEM Narratives"></a>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework   for Identifying Cultural Capital in STEM Narratives</h2><p><strong>Authors:Khalid Mehtab Khan, Anagha Kulkarni</strong></p>
<p>Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer modelâ€™s awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the modelâ€™s vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative. </p>
<blockquote>
<p>åœ¨å­¦ç”Ÿçš„åæ€ä¸­è¯†åˆ«æ–‡åŒ–èµ„æœ¬ï¼ˆCCï¼‰ä¸»é¢˜ï¼Œå¯ä»¥æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œæœ‰åŠ©äºåŸ¹å…»è¯¾å ‚ä¸­çš„å…¬å¹³å­¦ä¹ ç¯å¢ƒã€‚ç„¶è€Œï¼Œè¯¸å¦‚å¿—å‘ç›®æ ‡æˆ–å®¶åº­æ”¯æŒç­‰ä¸»é¢˜é€šå¸¸è¢«ç¼–ç»‡æˆå™äº‹ï¼Œè€Œéä»¥å…³é”®è¯çš„å½¢å¼ç›´æ¥å‡ºç°ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥è¢«æ ‡å‡†çš„NLPæ¨¡å‹æ£€æµ‹ï¼Œè¿™äº›æ¨¡å‹å­¤ç«‹åœ°å¤„ç†å¥å­ã€‚æ ¸å¿ƒæŒ‘æˆ˜æºäºç¼ºä¹æ„è¯†ï¼Œå› ä¸ºæ ‡å‡†æ¨¡å‹æ˜¯åœ¨ä¸€èˆ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯¹æ•°æ®çš„ç‰¹å®šé¢†åŸŸè¯­è¨€å’Œå™äº‹ä¸Šä¸‹æ–‡ä¸€æ— æ‰€çŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AWAREæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°å°è¯•æé«˜è½¬æ¢å™¨æ¨¡å‹å¯¹æ­¤ç±»ç»†å¾®ä»»åŠ¡çš„æ„è¯†ã€‚AWAREæœ‰ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰é¢†åŸŸæ„è¯†ï¼Œä½¿æ¨¡å‹çš„è¯æ±‡é€‚åº”å­¦ç”Ÿåæ€çš„è¯­è¨€é£æ ¼ï¼›2ï¼‰è¯­å¢ƒæ„è¯†ï¼Œç”Ÿæˆå¥å­åµŒå…¥ï¼Œæ„è¯†åˆ°æ•´ç¯‡æ–‡ç« çš„è¯­å¢ƒï¼›3ï¼‰ç±»åˆ«é‡å æ„è¯†ï¼Œé‡‡ç”¨å¤šæ ‡ç­¾ç­–ç•¥æ¥è¯†åˆ«å•ä¸ªå¥å­ä¸­å…±å­˜çš„ä¸»é¢˜ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä½¿æ¨¡å‹æ˜ç¡®æ„è¯†åˆ°è¾“å…¥çš„ç‰¹æ€§ï¼ŒAWAREåœ¨Macro-F1ä¸Šè¶…å‡ºäº†å¼ºåŸºçº¿2.1ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨æ‰€æœ‰ä¸»é¢˜ä¸Šéƒ½æ˜¾ç¤ºå‡ºå¯è§‚çš„æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œä¸ºä»»ä½•ä¾èµ–äºå™äº‹ä¸Šä¸‹æ–‡çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04983v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å­¦ç”Ÿçš„åæ€ä¸­è¯†åˆ«æ–‡åŒ–èµ„æœ¬ä¸»é¢˜çš„é‡è¦æ€§ï¼Œè¿™æœ‰åŠ©äºä¿ƒè¿›è¯¾å ‚ä¸­çš„å…¬å¹³å­¦ä¹ ç¯å¢ƒã€‚æ–‡ç« æŒ‡å‡ºï¼Œä¸»é¢˜å¦‚å¿—å‘ç›®æ ‡æˆ–å®¶åº­æ”¯æŒé€šå¸¸èå…¥å™äº‹ä¹‹ä¸­ï¼Œè€Œéä»¥å…³é”®è¯çš„å½¢å¼å‡ºç°ï¼Œè¿™ä½¿å¾—æ ‡å‡†NLPæ¨¡å‹éš¾ä»¥å­¤ç«‹åœ°å¤„ç†å¥å­æ¥æ£€æµ‹è¿™äº›ä¸»é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AWAREæ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æé«˜transformeræ¨¡å‹å¯¹æ­¤ç±»ä»»åŠ¡çš„æ•é”åº¦ï¼šé¢†åŸŸæ•é”åº¦ã€è¯­å¢ƒæ•é”åº¦å’Œç±»åˆ«é‡å æ•é”åº¦ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ä½¿æ¨¡å‹æ˜ç¡®æ„è¯†åˆ°è¾“å…¥çš„ç‰¹æ€§ï¼ŒAWAREåœ¨å®è§‚F1åˆ†æ•°ä¸Šè¶…è¶Šäº†å¼ºåŸºçº¿2.1ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨æ‰€æœ‰ä¸»é¢˜ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚è¿™ä¸ºä»»ä½•ä¾èµ–äºå™äº‹è¯­å¢ƒçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ–¹æ³•è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯†åˆ«å­¦ç”Ÿåæ€ä¸­çš„æ–‡åŒ–èµ„æœ¬ä¸»é¢˜æœ‰åŠ©äºä¿ƒè¿›å…¬å¹³çš„å­¦ä¹ ç¯å¢ƒã€‚</li>
<li>ä¸»é¢˜å¸¸èå…¥å™äº‹ä¸­ï¼Œè€Œéä»¥å…³é”®è¯å½¢å¼å‡ºç°ï¼Œè¿™ä½¿å¾—æ ‡å‡†NLPæ¨¡å‹éš¾ä»¥æ£€æµ‹ã€‚</li>
<li>AWAREæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¢†åŸŸæ•é”åº¦ã€è¯­å¢ƒæ•é”åº¦å’Œç±»åˆ«é‡å æ•é”åº¦ã€‚</li>
<li>AWAREæ¡†æ¶é€šè¿‡æé«˜æ¨¡å‹çš„æ•é”åº¦ï¼Œåœ¨å®è§‚F1åˆ†æ•°ä¸Šè¶…è¶Šäº†å¼ºåŸºçº¿ã€‚</li>
<li>AWAREæ¡†æ¶åœ¨æ‰€æœ‰ä¸»é¢˜ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>æ­¤æ–¹æ³•é€‚ç”¨äºä»»ä½•ä¾èµ–äºå™äº‹è¯­å¢ƒçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-763c3416f387290d115a32cd32a0cfc7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099318&auth_key=1760099318-0-0-be616adb31c428ad1b00a21ef6e29d6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7858f8afd2033d970909f41d3f0d2ca1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099325&auth_key=1760099325-0-0-2f1aa6fea3d81814f0161530db801d60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-302d3f041026412a9fcb80960ce42fec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099332&auth_key=1760099332-0-0-d89dc480f3dd60f587997d48a2972d41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d4e126271fb53402545a3603211f988~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099338&auth_key=1760099338-0-0-1c3605f1c4fe91e86f09c7c88ea35b92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-659bb768de122e4210eb2d43acbcf2c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099345&auth_key=1760099345-0-0-be994f9a9932cc981eee600e11912ead&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9d075576ac5224fac7e934e32b7299a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099351&auth_key=1760099351-0-0-c0b4bff513490a08b748370c19d77637&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Language-Model-Based-Text-to-Audio-Generation-Anti-Causally-Aligned-Collaborative-Residual-Transformers"><a href="#Language-Model-Based-Text-to-Audio-Generation-Anti-Causally-Aligned-Collaborative-Residual-Transformers" class="headerlink" title="Language Model Based Text-to-Audio Generation: Anti-Causally Aligned   Collaborative Residual Transformers"></a>Language Model Based Text-to-Audio Generation: Anti-Causally Aligned   Collaborative Residual Transformers</h2><p><strong>Authors:Juncheng Wang, Chao Xu, Cheng Yu, Zhe Hu, Haoyu Xie, Guoqi Yu, Lei Shang, Shujun Wang</strong></p>
<p>While language models (LMs) paired with residual vector quantization (RVQ) tokenizers have shown promise in text-to-audio (T2A) generation, they still lag behind diffusion-based models by a non-trivial margin. We identify a critical dilemma underpinning this gap: incorporating more RVQ layers improves audio reconstruction fidelity but exceeds the generation capacity of conventional LMs. To address this, we first analyze RVQ dynamics and uncover two key limitations: 1) orthogonality of features across RVQ layers hinders effective LMs training, and 2) descending semantic richness in tokens from deeper RVQ layers exacerbates exposure bias during autoregressive decoding. Based on these insights, we propose Siren, a novel LM-based framework that employs multiple isolated transformers with causal conditioning and anti-causal alignment via reinforcement learning. Extensive experiments demonstrate that Siren outperforms both existing LM-based and diffusion-based T2A systems, achieving state-of-the-art results. By bridging the representational strengths of LMs with the fidelity demands of audio synthesis, our approach repositions LMs as competitive contenders against diffusion models in T2A tasks. Moreover, by aligning audio representations with linguistic structures, Siren facilitates a promising pathway toward unified multi-modal generation frameworks. </p>
<blockquote>
<p>è™½ç„¶è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰åˆ†è¯å™¨çš„ç»“åˆåœ¨æ–‡æœ¬åˆ°éŸ³é¢‘ï¼ˆT2Aï¼‰ç”Ÿæˆä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶è½åäºåŸºäºæ‰©æ•£çš„æ¨¡å‹ç›¸å½“å¤§çš„è·ç¦»ã€‚æˆ‘ä»¬å‘ç°äº†é€ æˆè¿™ä¸€å·®è·çš„å…³é”®å›°å¢ƒï¼šå¢åŠ RVQå±‚æ•°å¯ä»¥æé«˜éŸ³é¢‘é‡å»ºçš„ä¿çœŸåº¦ï¼Œä½†è¶…å‡ºäº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æäº†RVQçš„åŠ¨æ€ç‰¹æ€§ï¼Œå¹¶å‘ç°äº†ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼š1ï¼‰RVQå±‚ä¹‹é—´çš„ç‰¹å¾æ­£äº¤æ€§é˜»ç¢äº†æœ‰æ•ˆçš„è¯­è¨€æ¨¡å‹è®­ç»ƒï¼›2ï¼‰æ¥è‡ªæ›´æ·±RVQå±‚çš„ä»¤ç‰Œè¯­ä¹‰ä¸°å¯Œæ€§çš„ä¸‹é™åŠ å‰§äº†è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­çš„æš´éœ²åå·®ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Sirenï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨å¤šä¸ªç‹¬ç«‹çš„å˜å‹å™¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°å› æœæ¡ä»¶å’Œéå› æœå¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSirenåœ¨åŸºäºè¯­è¨€æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„T2Aç³»ç»Ÿä¸­è¡¨ç°å‡ºè‰²ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚é€šè¿‡å¼¥åˆäº†è¯­è¨€æ¨¡å‹çš„è¡¨å¾å¼ºåº¦ä¸éŸ³é¢‘åˆæˆçš„ä¿çœŸåº¦éœ€æ±‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿è¯­è¨€æ¨¡å‹åœ¨T2Aä»»åŠ¡ä¸­æˆä¸ºä¸æ‰©æ•£æ¨¡å‹ç«äº‰çš„ç«äº‰è€…ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†éŸ³é¢‘è¡¨ç¤ºä¸è¯­è¨€ç»“æ„å¯¹é½ï¼ŒSirenä¸ºç»Ÿä¸€çš„å¤šæ¨¡å¼ç”Ÿæˆæ¡†æ¶æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04577v1">PDF</a> Accepted to EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰æ ‡è®°å™¨åœ¨æ–‡æœ¬è½¬éŸ³é¢‘ï¼ˆT2Aï¼‰ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€”â€”Sirenã€‚åˆ†æè¡¨æ˜ï¼Œå¼•å…¥æ›´å¤šRVQå±‚å¯ä»¥æé«˜éŸ³é¢‘é‡å»ºçš„ä¿çœŸåº¦ï¼Œä½†è¶…å‡ºäº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶åˆ†æäº†RVQçš„åŠ¨åŠ›å­¦ï¼Œå¹¶æ­ç¤ºäº†ä¸¤ä¸ªå…³é”®å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºé‡‡ç”¨å¤šä¸ªç‹¬ç«‹çš„Transformeræ¶æ„ä¸å› æœæ¡ä»¶ã€é€†å‘å­¦ä¹ ç›¸ç»“åˆçš„ç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœè¯å®ï¼ŒSirenä¸ä»…åœ¨åŸºäºè¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿä¸­è¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”è¶…è¶Šäº†è®¸å¤šåŸºäºæ‰©æ•£çš„ç³»ç»Ÿï¼Œæˆä¸ºæœ€æ–°çš„æŠ€æœ¯æˆæœã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†è¯­è¨€æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæ»¡è¶³äº†éŸ³é¢‘åˆæˆçš„ä¿çœŸåº¦è¦æ±‚ï¼Œè¿˜é‡æ–°ç¡®ç«‹äº†è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è½¬éŸ³é¢‘ä»»åŠ¡ä¸­çš„ç«äº‰åŠ›åœ°ä½ã€‚æ­¤å¤–ï¼Œé€šè¿‡éŸ³é¢‘è¡¨ç¤ºä¸è¯­è¨€ç»“æ„çš„å¯¹é½ï¼ŒSirenä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶æä¾›äº†å¯è¡Œçš„è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹ä¸æ®‹å·®å‘é‡é‡åŒ–åœ¨æ–‡æœ¬è½¬éŸ³é¢‘ç”Ÿæˆé¢†åŸŸæœ‰åº”ç”¨å‰æ™¯ï¼Œä½†ä»è½åäºæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å¢åŠ RVQå±‚å¯æé«˜éŸ³é¢‘é‡å»ºçš„ä¿çœŸåº¦ï¼Œä½†è¶…å‡ºè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>RVQåŠ¨åŠ›å­¦å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šç‰¹å¾æ­£äº¤æ€§å½±å“è¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œæ·±å±‚RVQå±‚ä¸­çš„è¯­ä¹‰ä¸°å¯Œåº¦ä¸‹é™åŠ å‰§äº†è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­çš„æš´éœ²åå·®ã€‚</li>
<li>æå‡ºSirenæ¡†æ¶ï¼Œé‡‡ç”¨å¤šä¸ªç‹¬ç«‹Transformeræ¶æ„ç»“åˆå› æœæ¡ä»¶å’Œé€†å‘å­¦ä¹ æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>Sirenåœ¨æ–‡æœ¬è½¬éŸ³é¢‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Sirené‡æ–°ç¡®ç«‹äº†è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è½¬éŸ³é¢‘ä»»åŠ¡ä¸­çš„ç«äº‰åŠ›åœ°ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e821e1bc76dbd4873466fb0d871ecbbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099359&auth_key=1760099359-0-0-03f127749ef696e9558d4cf1c4c18309&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98bc30e4f5aefda1eabe571454685dcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099366&auth_key=1760099366-0-0-75cf9f65fee7241f518d20ce601d2198&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d10667c1cc9d0b64ad18ecb67dd3db9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099373&auth_key=1760099373-0-0-dde77eff8c36a93bdd4600c445031f65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5693fdeef6e61281cef3b723b356fe15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099380&auth_key=1760099380-0-0-f8bfc82cab146f33f3f565f83771d27d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling"><a href="#CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling" class="headerlink" title="CALM Before the STORM: Unlocking Native Reasoning for Optimization   Modeling"></a>CALM Before the STORM: Unlocking Native Reasoning for Optimization   Modeling</h2><p><strong>Authors:Zhengyang Tang, Zihan Ye, Chenyu Huang, Xuhan Huang, Chengpeng Li, Sihang Li, Guanhua Chen, Ming Yan, Zizhuo Wang, Hongyuan Zha, Dayiheng Liu, Benyou Wang</strong></p>
<p>Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs â€“ In particular, we show that direct fine-tuning on traditional \textit{non-reflective} datasets leads to limited gains. To fully leverage LRMsâ€™ inherent reasoning abilities, we propose \textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚å¤šæ­¥æ¨ç†ä¸­å±•ç¤ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºæ¨¡å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸŸé€‚åº”æ–¹æ³•ï¼Œæœ€åˆæ˜¯ä¸ºæ—©æœŸçš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹è®¾è®¡çš„ï¼Œå¾€å¾€æ— æ³•åˆ©ç”¨ç°ä»£LRMsçš„å…ˆè¿›æ¨ç†æ¨¡å¼ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜ç›´æ¥åœ¨ä¼ ç»Ÿçš„\textit{éåæ€}æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä¼šå¯¼è‡´æœ‰é™çš„æ”¶ç›Šã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨LRMsçš„å›ºæœ‰æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{CALMï¼ˆä¿®æ­£é€‚åº”ä¸è½»é‡çº§ä¿®æ”¹ï¼‰}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒé€æ­¥åœ¨ä¼˜åŒ–å»ºæ¨¡ä»»åŠ¡ä¸­ç»†åŒ–LRMsçš„å›ºæœ‰æ¨ç†æ¨¡å¼ã€‚åœ¨CALMä¸­ï¼Œä¸“å®¶å¹²é¢„è€…è¯†åˆ«æ¨ç†ç¼ºé™·å¹¶æä¾›ç®€æ´çš„çº æ­£æç¤ºï¼ŒLRMä¼šå°†å…¶çº³å…¥ä»¥äº§ç”Ÿæ”¹è¿›çš„æ¨ç†è½¨è¿¹ã€‚è¿™äº›å¹²é¢„åªä¿®æ”¹äº†ä¸åˆ°2.6%çš„ç”Ÿæˆä»¤ç‰Œï¼Œä½†é€šè¿‡ç›‘ç£å¾®è°ƒå®ç°äº†é«˜è´¨é‡æ•°æ®çš„è½¯é€‚åº”ã€‚é€‚åº”åçš„æ¨¡å‹å†é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æ”¹è¿›ã€‚åŸºäºCALMï¼Œæˆ‘ä»¬å¼€å‘äº†\textbf{STORMï¼ˆæ™ºèƒ½æ€è€ƒä¼˜åŒ–æ¨ç†æ¨¡å‹ï¼‰}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰4Bå‚æ•°çš„LRMï¼Œåœ¨äº”ä¸ªæµè¡Œçš„ä¼˜åŒ–å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¹³å‡å‡†ç¡®ç‡68.9%çš„æ–°æ°´å¹³ï¼Œä¸671B LRMçš„æ€§èƒ½ç›¸åŒ¹é…ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€ã€åŸºäºæç¤ºçš„æ•°æ®åˆæˆæ—¢ä¿ç•™äº†ç°ä»£LRMsçš„å›ºæœ‰æ¨ç†æ¨¡å¼åˆæ”¾å¤§äº†å…¶ä¼˜åŠ¿ï¼Œä¸ºåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼˜åŒ–å»ºæ¨¡ä»»åŠ¡ä¸Šå®ç°ä¸“å®¶çº§æ€§èƒ½æä¾›äº†æ›´æœ‰æ•ˆã€æ›´å¯æ‰©å±•çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04204v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚å¤šæ­¥æ¨ç†æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºæ¨¡å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ç°ä»£LRMsçš„å…ˆè¿›æ¨ç†æ¨¡å¼ã€‚ä¸ºäº†å……åˆ†å‘æŒ¥LRMsçš„å›ºæœ‰æ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†CALMï¼ˆCorrective Adaptation with Lightweight Modificationï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¸è¿›å¼åœ°åœ¨åŸç”Ÿæ¨ç†æ¨¡å¼ä¸‹ä¼˜åŒ–å»ºæ¨¡ä»»åŠ¡æ¥å®Œå–„LRMsã€‚ä¸“å®¶å¹²é¢„è€…è¯†åˆ«æ¨ç†ç¼ºé™·å¹¶æä¾›ç®€æ´çš„çº æ­£æç¤ºï¼ŒLRMsç»“åˆè¿™äº›æç¤ºæ”¹è¿›æ¨ç†è½¨è¿¹ã€‚è¿™äº›å¹²é¢„ä»…ä¿®æ”¹ä¸åˆ°2.6%çš„ç”Ÿæˆä»¤ç‰Œï¼Œä½†é€šè¿‡ç›‘ç£å¾®è°ƒå®ç°äº†é«˜è´¨é‡æ•°æ®çš„è½¯é€‚åº”ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼€å‘äº†STORMï¼ˆSmart Thinking Optimization Reasoning Modelï¼‰ï¼Œåœ¨äº”ä¸ªæµè¡Œçš„ä¼˜åŒ–å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡å‡†ç¡®ç‡68.9%ï¼Œä¸671Bçš„LRMæ€§èƒ½ç›¸åŒ¹é…ã€‚è¿™è¡¨æ˜åŸºäºåŠ¨æ€æç¤ºçš„æ•°æ®åˆæˆæ—¢ä¿ç•™äº†åˆæ”¾å¤§äº†ç°ä»£LRMsçš„å›ºæœ‰æ¨ç†æ¨¡å¼ï¼Œä¸ºåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼˜åŒ–å»ºæ¨¡ä»»åŠ¡ä¸Šå®ç°ä¸“å®¶çº§æ€§èƒ½æä¾›äº†æ›´æœ‰æ•ˆã€æ›´å¯æ‰©å±•çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMsåœ¨å¤æ‚å¤šæ­¥æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºæ¨¡å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚</li>
<li>ä¼ ç»Ÿé¢†åŸŸè‡ªé€‚åº”æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ç°ä»£LRMsçš„å…ˆè¿›æ¨ç†æ¨¡å¼ã€‚</li>
<li>CALMæ¡†æ¶é€šè¿‡æ¸è¿›å¼å®Œå–„LRMsçš„æ¨ç†èƒ½åŠ›ï¼Œç»“åˆä¸“å®¶å¹²é¢„è€…çš„çº æ­£æç¤ºæ”¹è¿›æ¨ç†è½¨è¿¹ã€‚</li>
<li>CALMæ¡†æ¶ä¸­çš„å¹²é¢„ä»…ä¿®æ”¹å°‘é‡ç”Ÿæˆä»¤ç‰Œï¼Œå®ç°é«˜è´¨é‡æ•°æ®çš„è½¯é€‚åº”ã€‚</li>
<li>åŸºäºCALMæ¡†æ¶å¼€å‘çš„STORMæ¨¡å‹åœ¨å¤šä¸ªä¼˜åŒ–å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°çŠ¶æ€å¹³å‡å‡†ç¡®ç‡68.9%ã€‚</li>
<li>åŠ¨æ€ã€åŸºäºæç¤ºçš„æ•°æ®åˆæˆæ–¹æ³•æ—¢ä¿ç•™äº†åˆæ”¾å¤§äº†ç°ä»£LRMsçš„å›ºæœ‰æ¨ç†æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e19556561ea77dc86afa2f9f96b583a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099387&auth_key=1760099387-0-0-441a56d89227763e2f74a45faadb23b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea0015ba8f4f0741f48f653d8a232a34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099394&auth_key=1760099394-0-0-74338d9ab43b622f2cf7fc2d5792a98b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46c1e89315262497649602883fb73195~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102473&auth_key=1760102473-0-0-0ebabf2db78864a2fefc237305965673&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-faedeaff81fb108777b18cc84b35c6da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102480&auth_key=1760102480-0-0-17da6d88c359d755353cc03bb4e7c1d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50ceb5b6335f247d27bc0b883ad7c0e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102487&auth_key=1760102487-0-0-747514130c078864c7ba5634d71144da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Rare-Text-Semantics-Were-Always-There-in-Your-Diffusion-Transformer"><a href="#Rare-Text-Semantics-Were-Always-There-in-Your-Diffusion-Transformer" class="headerlink" title="Rare Text Semantics Were Always There in Your Diffusion Transformer"></a>Rare Text Semantics Were Always There in Your Diffusion Transformer</h2><p><strong>Authors:Seil Kang, Woojung Han, Dayun Ju, Seong Jae Hwang</strong></p>
<p>Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiTâ€™s outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface. </p>
<blockquote>
<p>ä»åŸºäºæµå’Œæ‰©æ•£çš„è½¬æ¢å™¨å¼€å§‹ï¼Œå¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTsï¼‰å·²ç»æ”¹å˜äº†æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆçš„æ–¹å¼ï¼Œå› å…¶å‡ºè‰²çš„è§†è§‰é€¼çœŸåº¦è€Œå—åˆ°èµèª‰ã€‚éšç€è¿™äº›æ¨¡å‹çš„è¿›æ­¥ï¼Œç”¨æˆ·ä¸æ–­ç”¨å¯Œæœ‰æƒ³è±¡åŠ›æˆ–ç½•è§çš„æç¤ºæ¥æŒ‘æˆ˜ç•Œé™ï¼Œä½†åœ¨ä½¿ç”¨è¿™äº›å…ˆè¿›æ¨¡å‹ç”Ÿæˆå†…å®¹æ—¶ä»ç„¶ä¼šå‡ºç°çŸ­æ¿ï¼Œå› ä¸ºé‚£äº›ç½•è§çš„æç¤ºæ‰€è¡¨è¾¾çš„æ¦‚å¿µåœ¨é¢„è®­ç»ƒé˜¶æ®µç•™ä¸‹çš„å°è®°é€šå¸¸ä¸å¤Ÿæ·±åˆ»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„å¹²é¢„æªæ–½ï¼Œå¯ä»¥åœ¨MM-DiTså†…éƒ¨æå–ç½•è§çš„è¯­ä¹‰å†…å®¹ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ­¥éª¤ã€æ•°æ®ã€å»å™ªæ—¶é—´ä¼˜åŒ–æˆ–ä¾èµ–å¤–éƒ¨æ¨¡å—ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ã€‚ç‰¹åˆ«æ˜¯MM-DiTå†…åœ¨çš„è”åˆæ³¨æ„æœºåˆ¶åœ¨å˜å‹å™¨å—å†…é¡ºåºåœ°æ›´æ–°æ–‡æœ¬åµŒå…¥å’Œå›¾åƒåµŒå…¥ã€‚æˆ‘ä»¬å‘ç°é€šè¿‡è”åˆæ³¨æ„å—ä¹‹å‰åœ¨æ–‡æœ¬ä»¤ç‰ŒåµŒå…¥å‘¨å›´æ•°å­¦æ‰©å±•è¡¨ç¤ºæ€§ç›†åœ°ï¼ˆé€šè¿‡æ–¹å·®è§„æ¨¡æ‰©å¤§ï¼‰ï¼Œç½•è§çš„è¯­ä¹‰å†…å®¹ä¼šæ¸…æ™°åœ°å‡ºç°åœ¨MM-DiTçš„è¾“å‡ºä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœåœ¨æ–‡æœ¬åˆ°è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œæ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬çš„å·¥ä½œé‚€è¯·ç”Ÿæˆæ¨¡å‹æ­ç¤ºç”¨æˆ·æ„å›¾çš„è¯­ä¹‰å†…å®¹ï¼Œè¿™äº›è¯­ä¹‰å†…å®¹è™½ç„¶æ›¾ç»éšè—ä½†å·²å‡†å¤‡å¥½æµ®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03886v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰åŸºäºæµå’Œæ‰©æ•£æŠ€æœ¯ï¼Œé‡å¡‘äº†æ–‡æœ¬åˆ°è§†è§‰çš„ç”Ÿæˆï¼Œä»¥å…¶å‡ºè‰²çš„è§†è§‰é€¼çœŸåº¦è€Œå—åˆ°èµèª‰ã€‚ç„¶è€Œï¼Œå¯¹äºå…ˆè¿›æ¨¡å‹è€Œè¨€ï¼Œç”¨æˆ·æä¾›çš„å¯Œæœ‰æƒ³è±¡åŠ›çš„æˆ–ç½•è§çš„æç¤ºä»ç„¶ä¼šåœ¨ç”Ÿæˆæ—¶é‡åˆ°å›°éš¾ï¼Œå› ä¸ºè¿™äº›ç½•è§æ¦‚å¿µåœ¨é¢„è®­ç»ƒé˜¶æ®µç•™ä¸‹çš„å°è®°é€šå¸¸è¾ƒä¸ºå¾®å¼±ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„å¹²é¢„æªæ–½ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ­¥éª¤ã€æ•°æ®ã€å»å™ªæ—¶é—´ä¼˜åŒ–æˆ–ä¾èµ–å¤–éƒ¨æ¨¡å—ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ï¼Œå³å¯åœ¨MM-DiTå†…éƒ¨å‘ˆç°ç½•è§çš„è¯­ä¹‰ã€‚é€šè¿‡æ•°å­¦ä¸Šæ‰©å¤§æ–‡æœ¬ä»¤ç‰ŒåµŒå…¥å‘¨å›´çš„è¡¨ç¤ºç›†åœ°ï¼Œå³é€šè¿‡åœ¨è”åˆæ³¨æ„åŠ›å—ä¹‹å‰æ‰©å¤§æ–¹å·®æ¯”ä¾‹ï¼Œæˆ‘ä»¬å‘ç°ç½•è§çš„è¯­ä¹‰åœ¨MM-DiTçš„è¾“å‡ºä¸­æ¸…æ™°å‡ºç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœæœ‰æ•ˆåœ°è·¨æ–‡æœ¬åˆ°è§†è§‰ä»»åŠ¡è¿›è¡Œæ¨å¹¿ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘å’Œæ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘ã€‚æœ¬ç ”ç©¶é‚€è¯·ç”Ÿæˆæ¨¡å‹æ­ç¤ºç”¨æˆ·æ„å›¾ä¸­éšè—çš„è¯­ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰å·²é‡å¡‘æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆé¢†åŸŸï¼Œå±•ç°é«˜è§†è§‰é€¼çœŸåº¦ã€‚</li>
<li>ç”¨æˆ·æä¾›çš„ç½•è§æˆ–å¯Œæœ‰æƒ³è±¡åŠ›çš„æç¤ºå¯¹å…ˆè¿›æ¨¡å‹ä»å…·æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µéš¾ä»¥æ•æ‰è¿™äº›ç½•è§æ¦‚å¿µã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹²é¢„æªæ–½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€æ•°æ®æˆ–ä¾èµ–å¤–éƒ¨æ¨¡å—ï¼Œå³å¯åœ¨MM-DiTä¸­å‘ˆç°ç½•è§è¯­ä¹‰ã€‚</li>
<li>é€šè¿‡æ•°å­¦ä¸Šæ‰©å¤§æ–‡æœ¬åµŒå…¥çš„è¡¨ç¤ºèŒƒå›´ï¼ˆå³æ–¹å·®æ¯”ä¾‹æ‰©å¤§ï¼‰ï¼ŒMM-DiTèƒ½å¤Ÿæ¸…æ™°è¾“å‡ºç½•è§è¯­ä¹‰ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æ•ˆè·¨å¤šç§æ–‡æœ¬åˆ°è§†è§‰ä»»åŠ¡æ¨å¹¿ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ä»¥åŠæ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘ã€‚</li>
<li>è¯¥ç ”ç©¶æœ‰åŠ©äºç”Ÿæˆæ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œæ­ç¤ºç”¨æˆ·æ„å›¾ä¸­çš„éšè—è¯­ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6d455ea1d58e8f4b99df1b95970229de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102494&auth_key=1760102494-0-0-c78821dd2be16ee950621feb0cddf3f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d30dc1b2c174d4bb19a9f51df938ba9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102501&auth_key=1760102501-0-0-1ddc9e10ad8825bd366c0fa2853ba27a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e09e3037928d8661bbfefd2af6cb4b98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102508&auth_key=1760102508-0-0-bbadda14674dacc5ff8bd7af1d6de628&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a4fbfa9159baf938a5d8be010be45d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102515&auth_key=1760102515-0-0-3d7b60337908a498ca85a404f87dca1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8aee1867d3ee54755098b412d1379af0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102522&auth_key=1760102522-0-0-6004022fd1eaa0d2c9618ab995f60351&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DHQA-4D-Perceptual-Quality-Assessment-of-Dynamic-4D-Digital-Human"><a href="#DHQA-4D-Perceptual-Quality-Assessment-of-Dynamic-4D-Digital-Human" class="headerlink" title="DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human"></a>DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human</h2><p><strong>Authors:Yunhao Li, Sijing Wu, Yucheng Zhu, Huiyu Duan, Zicheng Zhang, Guangtao Zhai</strong></p>
<p>With the rapid development of 3D scanning and reconstruction technologies, dynamic digital human avatars based on 4D meshes have become increasingly popular. A high-precision dynamic digital human avatar can be applied to various fields such as game production, animation generation, and remote immersive communication. However, these 4D human avatar meshes are prone to being degraded by various types of noise during the processes of collection, compression, and transmission, thereby affecting the viewing experience of users. In light of this fact, quality assessment of dynamic 4D digital humans becomes increasingly important. In this paper, we first propose a large-scale dynamic digital human quality assessment dataset, DHQA-4D, which contains 32 high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D human meshes degraded by 11 textured distortions, as well as their corresponding textured and non-textured mean opinion scores (MOSs). Equipped with DHQA-4D dataset, we analyze the influence of different types of distortion on human perception for textured dynamic 4D meshes and non-textured dynamic 4D meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model (LMM) based approach that is able to assess both textured 4D meshes and non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts multi-dimensional features, including visual features from a projected 2D video, motion features from cropped video clips, and geometry features from the 4D human mesh to provide comprehensive quality-related information. Then we utilize a LMM model to integrate the multi-dimensional features and conduct a LoRA-based instruction tuning technique to teach the LMM model to predict the quality scores. Extensive experimental results on the DHQA-4D dataset demonstrate the superiority of our DynaMesh-Rater method over previous quality assessment methods. </p>
<blockquote>
<p>éšç€3Dæ‰«æå’Œé‡å»ºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäº4Dç½‘æ ¼çš„åŠ¨æ€æ•°å­—äººç±»åŒ–èº«å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚é«˜ç²¾åº¦åŠ¨æ€æ•°å­—äººç±»åŒ–èº«å¯åº”ç”¨äºæ¸¸æˆåˆ¶ä½œã€åŠ¨ç”»ç”Ÿæˆå’Œè¿œç¨‹æ²‰æµ¸å¼é€šä¿¡ç­‰å„ç§é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›4Däººç±»åŒ–èº«ç½‘æ ¼åœ¨æ”¶é›†ã€å‹ç¼©å’Œä¼ è¾“è¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°å„ç§å™ªå£°çš„å¹²æ‰°ï¼Œä»è€Œå½±å“ç”¨æˆ·çš„ä½¿ç”¨ä½“éªŒã€‚é‰´äºæ­¤ï¼Œå¯¹åŠ¨æ€4Dæ•°å­—äººç±»çš„è´¨é‡è¯„ä¼°å˜å¾—æ—¥ç›Šé‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†å¤§è§„æ¨¡åŠ¨æ€æ•°å­—äººç±»è´¨é‡è¯„ä¼°æ•°æ®é›†DHQA-4Dï¼Œå…¶ä¸­åŒ…å«32ä¸ªé«˜è´¨é‡çœŸå®æ‰«æçš„4Däººç±»ç½‘æ ¼åºåˆ—ã€1920ä¸ªç”±11ç§çº¹ç†å¤±çœŸé™çº§çš„å¤±çœŸçº¹ç†4Däººç±»ç½‘æ ¼ï¼Œä»¥åŠå®ƒä»¬ç›¸åº”çš„çº¹ç†å’Œéçº¹ç†å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSsï¼‰ã€‚é…å¤‡DHQA-4Dæ•°æ®é›†ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒç±»å‹å¤±çœŸå¯¹çº¹ç†åŠ¨æ€4Dç½‘æ ¼å’Œéçº¹ç†åŠ¨æ€4Dç½‘æ ¼çš„äººç±»æ„ŸçŸ¥çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†DynaMesh-Raterï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿè¯„ä¼°çº¹ç†4Dç½‘æ ¼å’Œéçº¹ç†4Dç½‘æ ¼ã€‚å…·ä½“æ¥è¯´ï¼ŒDynaMesh-Raterç²¾å¿ƒæå–äº†å¤šç»´ç‰¹å¾ï¼ŒåŒ…æ‹¬ä»æŠ•å½±çš„2Dè§†é¢‘ä¸­æå–çš„è§†è§‰ç‰¹å¾ã€ä»è£å‰ªçš„è§†é¢‘ç‰‡æ®µä¸­æå–çš„è¿åŠ¨ç‰¹å¾ä»¥åŠä»4Däººç±»ç½‘æ ¼ä¸­æå–çš„å‡ ä½•ç‰¹å¾ï¼Œä»¥æä¾›å…¨é¢çš„è´¨é‡ç›¸å…³ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨LMMæ¨¡å‹æ¥æ•´åˆè¿™äº›å¤šç»´ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨åŸºäºLoRAçš„æŒ‡ä»¤å¾®è°ƒæŠ€æœ¯æ¥æ•™å¯¼LMMæ¨¡å‹é¢„æµ‹è´¨é‡åˆ†æ•°ã€‚åœ¨DHQA-4Dæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„DynaMesh-Rateræ–¹æ³•ç›¸è¾ƒäºä¹‹å‰çš„è´¨é‡è¯„ä¼°æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03874v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€3Dæ‰«æå’Œé‡å»ºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäº4Dç½‘æ ¼çš„åŠ¨æ€æ•°å­—äººç±»åŒ–èº«è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚å¯åº”ç”¨äºæ¸¸æˆåˆ¶ä½œã€åŠ¨ç”»åˆ¶ä½œå’Œè¿œç¨‹æ²‰æµ¸å¼é€šä¿¡ç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›4Däººç±»åŒ–èº«ç½‘æ ¼åœ¨æ”¶é›†ã€å‹ç¼©å’Œä¼ è¾“è¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°å„ç§å™ªå£°çš„å½±å“ï¼Œä»è€Œå½±å“ç”¨æˆ·ä½“éªŒã€‚å› æ­¤ï¼Œå¯¹åŠ¨æ€4Dæ•°å­—äººçš„è´¨é‡è¯„ä¼°å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡é¦–å…ˆæå‡ºå¤§è§„æ¨¡åŠ¨æ€æ•°å­—äººç±»è´¨é‡è¯„ä¼°æ•°æ®é›†DHQA-4Dï¼ŒåŒ…å«32ä¸ªé«˜è´¨é‡çœŸå®æ‰«æçš„4Däººç±»ç½‘æ ¼åºåˆ—ã€1920ä¸ªç”±11ç§çº¹ç†å¤±çœŸäº§ç”Ÿçš„å¤±çœŸçº¹ç†4Däººç±»ç½‘æ ¼åŠå…¶ç›¸åº”çš„çº¹ç†å’Œéçº¹ç†å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSsï¼‰ã€‚é…å¤‡DHQA-4Dæ•°æ®é›†ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒç±»å‹å¤±çœŸå¯¹çº¹ç†åŠ¨æ€4Dç½‘æ ¼å’Œéçº¹ç†åŠ¨æ€4Dç½‘æ ¼çš„äººç±»æ„ŸçŸ¥çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŠ¨æ€ç½‘æ ¼è¯„åˆ†å™¨ï¼ˆDynaMesh-Raterï¼‰çš„æ–°å‹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿè¯„ä¼°çº¹ç†å’Œéçº¹ç†çš„4Dç½‘æ ¼ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å¤šç»´ç‰¹å¾ï¼ŒåŒ…æ‹¬æ¥è‡ªæŠ•å½±çš„äºŒç»´è§†é¢‘çš„è§†è§‰ç‰¹å¾ã€æ¥è‡ªè£å‰ªçš„è§†é¢‘å‰ªè¾‘çš„è¿åŠ¨ç‰¹å¾ä»¥åŠæ¥è‡ª4Däººç±»ç½‘æ ¼çš„å‡ ä½•ç‰¹å¾ï¼Œæä¾›å…¨é¢çš„è´¨é‡ç›¸å…³ä¿¡æ¯ã€‚ç„¶ååˆ©ç”¨LMMæ¨¡å‹æ•´åˆå¤šç»´ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨LoRAæŠ€æœ¯è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œæ•™å¯¼æ¨¡å‹é¢„æµ‹è´¨é‡åˆ†æ•°ã€‚åœ¨DHQA-4Dæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DynaMesh-Rateræ–¹æ³•ä¼˜äºå…ˆå‰çš„è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€æ•°å­—äººç±»åŒ–èº«ä½¿ç”¨åŸºäº4Dç½‘æ ¼çš„æŠ€æœ¯å˜å¾—è¶Šæ¥è¶Šæµè¡Œï¼Œå¯åº”ç”¨äºæ¸¸æˆåˆ¶ä½œã€åŠ¨ç”»åˆ¶ä½œå’Œè¿œç¨‹æ²‰æµ¸å¼é€šä¿¡ã€‚</li>
<li>åŠ¨æ€4Dæ•°å­—äººçš„è´¨é‡è¯„ä¼°å˜å¾—é‡è¦ï¼Œå› ä¸ºè¿™äº›ç½‘æ ¼åœ¨æ”¶é›†ã€å‹ç¼©å’Œä¼ è¾“è¿‡ç¨‹ä¸­å®¹æ˜“å› å™ªå£°è€Œè´¨é‡ä¸‹é™ã€‚</li>
<li>DHQA-4Dæ•°æ®é›†åŒ…å«å„ç§é«˜è´¨é‡å’Œå¤±çœŸçº¹ç†çš„4Däººç±»ç½‘æ ¼åºåˆ—åŠå…¶å¯¹åº”çš„å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSsï¼‰ï¼Œç”¨äºåˆ†æä¸åŒç±»å‹å¤±çœŸå¯¹æ„ŸçŸ¥çš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ¨¡å‹æ–¹æ³•â€”â€”åŠ¨æ€ç½‘æ ¼è¯„åˆ†å™¨ï¼ˆDynaMesh-Raterï¼‰ï¼Œèƒ½å¤Ÿè¯„ä¼°çº¹ç†å’Œéçº¹ç†çš„4Dç½‘æ ¼çš„è´¨é‡ã€‚</li>
<li>DynaMesh-Rateré€šè¿‡æå–å¤šç»´ç‰¹å¾æ¥æä¾›å…¨é¢çš„è´¨é‡ç›¸å…³ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>DHQA-4Dæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–è´¨é‡è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼ŒDynaMesh-Raterå…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d9861ea4f44acf7b269d58615d0bcd26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102531&auth_key=1760102531-0-0-7d3cb5e77c665e690ad7c9ff1317e5cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4abf6cecfdaa7491fe820c0da4d26db1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102538&auth_key=1760102538-0-0-54944e08a45d713287801a73667a574e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cdffdee00aa479849ae0af9bddb7983~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102545&auth_key=1760102545-0-0-2773ee624d36f0b310452f708968facd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f98977ebcc1a2d5457db64ee164c5940~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102552&auth_key=1760102552-0-0-85aa6dc3f0c1804da1224ce814c5f779&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ef2a8c99be1df644063d71e290e0aa2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102560&auth_key=1760102560-0-0-a12edd471e56a040db3963460b434509&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-053e974ef63cab08476b30cb74d0c0ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102566&auth_key=1760102566-0-0-06476018d4928baec6ac6cbc1c3beb8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rainbow-Padding-Mitigating-Early-Termination-in-Instruction-Tuned-Diffusion-LLMs"><a href="#Rainbow-Padding-Mitigating-Early-Termination-in-Instruction-Tuned-Diffusion-LLMs" class="headerlink" title="Rainbow Padding: Mitigating Early Termination in Instruction-Tuned   Diffusion LLMs"></a>Rainbow Padding: Mitigating Early Termination in Instruction-Tuned   Diffusion LLMs</h2><p><strong>Authors:Bumjun Kim, Dongjae Jeon, Dueun Kim, Wonje Jeung, Albert No</strong></p>
<p>Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \texttt{<eos>} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \texttt{<eos>} as both termination and padding, which concentrates probability mass on \texttt{<eos>} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \texttt{<eos>} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/quasar529/rainbow-padding">https://github.com/quasar529/rainbow-padding</a>. </p>
<blockquote>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªå›å½’æ¨¡å‹çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œå®ƒæä¾›äº†çµæ´»çš„ç”Ÿæˆé¡ºåºå’Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å¼ºå¤§æ€§èƒ½ã€‚ç„¶è€Œï¼Œç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„dLLMsè¡¨ç°å‡ºæˆ‘ä»¬æ‰€è°“çš„â€œ<eos>æº¢å‡ºâ€çš„å…³é”®æ¼æ´ï¼šéšç€åˆ†é…åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œå“åº”å´çŸ›ç›¾åœ°å˜å¾—æ›´çŸ­ï¼Œé™·å…¥æå‰ç»ˆæ­¢æˆ–é€€åŒ–ä¸ºä¸€è¿ä¸²çš„<eos>æ ‡è®°ã€‚è™½ç„¶åœ¨å®è·µä¸­å·²ç»æ³¨æ„åˆ°è¿™ä¸ªé—®é¢˜ï¼Œä½†å°šæœªå¯¹å…¶è¿›è¡Œç³»ç»Ÿåˆ†æã€‚æˆ‘ä»¬è¿½æº¯å…¶æ ¹æºåˆ°<eos>çš„åŒé‡è§’è‰²ï¼Œæ—¢æ˜¯ç»ˆæ­¢ç¬¦åˆæ˜¯å¡«å……ç¬¦ï¼Œè¿™å°†åœ¨åé¢çš„ä½ç½®é›†ä¸­æ¦‚ç‡è´¨é‡ï¼Œå¹¶å‘åä¼ æ’­ä»¥è§¦å‘æå‰ç»ˆæ­¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Rainbow Paddingï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„è¡¥æ•‘æªæ–½ï¼Œç”¨é‡å¤çš„å¡«å……ä»¤ç‰Œå¾ªç¯æ›¿æ¢é‡å¤çš„<eos>å ä½ç¬¦ï¼Œåˆ†æ•£æ¦‚ç‡è´¨é‡å¹¶æ‰“ç ´<eos>çš„ä¸»å¯¼åœ°ä½ã€‚å®éªŒè¡¨æ˜ï¼ŒRainbow Paddingæå¤§åœ°æé«˜äº†é•¿åº¦ç¨³å¥æ€§å’Œè¾“å‡ºè´¨é‡ï¼Œåªéœ€ä¸ƒä¸ªå¡«å……ä»¤ç‰Œå°±è¶³ä»¥é˜²æ­¢æå‰ç»ˆæ­¢ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é›†æˆåˆ°ç°æœ‰çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸­ï¼šåªéœ€å¯¹å°‘é‡æ•°æ®è¿›è¡ŒLoRAå¾®è°ƒä¸€ä¸ªå‘¨æœŸå³å¯å–å¾—æ˜¾è‘—æ”¹è¿›ï¼Œè¿™ä½¿å¾—æ­¤è§£å†³æ–¹æ¡ˆéå¸¸å®ç”¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/quasar529/rainbow-padding%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/quasar529/rainbow-paddingå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03680v1">PDF</a> 25 pages. Project page available   at~\url{<a target="_blank" rel="noopener" href="https://ai-isl.github.io/rainbow-padding%7D">https://ai-isl.github.io/rainbow-padding}</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºå¯¹è‡ªå›å½’æ¨¡å‹çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œå…·æœ‰çµæ´»çš„ç”Ÿæˆé¡ºåºå’Œå¼ºå¤§çš„å¤æ‚æ¨ç†ä»»åŠ¡æ€§èƒ½ã€‚ç„¶è€Œï¼ŒæŒ‡ä»¤è°ƒä¼˜çš„dLLMså­˜åœ¨ä¸€ä¸ªè¢«ç§°ä¸ºâ€œæº¢å‡ºâ€çš„å…³é”®æ¼æ´ï¼šéšç€åˆ†é…åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œå“åº”å´å˜å¾—è¾ƒçŸ­ï¼Œå¯¼è‡´è¿‡æ—©ç»ˆæ­¢æˆ–é€€åŒ–ä¸ºä¸€ä¸²çš„â€œ<eos>â€æ ‡è®°ã€‚æœ¬æ–‡è¿½è¸ªäº†é—®é¢˜çš„æ ¹æºï¼Œå¹¶æå‡ºäº†å½©è™¹å¡«å……æ³•ï¼ˆRainbow Paddingï¼‰ä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ›¿æ¢é‡å¤çš„â€œ<eos>â€å ä½ç¬¦ä¸ºä¸åŒçš„å¡«å……ä»¤ç‰Œï¼Œä»è€Œæ‰“ç ´â€œ<eos>â€çš„ä¸»å¯¼åœ°ä½ã€‚å®éªŒè¡¨æ˜ï¼ŒRainbow Paddingæ˜¾è‘—æé«˜äº†é•¿åº¦ç¨³å¥æ€§å’Œè¾“å‡ºè´¨é‡ï¼Œä»…ä½¿ç”¨ä¸ƒä¸ªå¡«å……ä»¤ç‰Œå°±è¶³ä»¥é˜²æ­¢è¿‡æ—©ç»ˆæ­¢ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯é«˜æ•ˆé›†æˆåˆ°ç°æœ‰æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸­ï¼Œé€šè¿‡LoRAå¾®è°ƒå°‘é‡æ•°æ®å³å¯å®Œæˆæ˜¾è‘—æ”¹è¿›ï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>dLLMsä½œä¸ºä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œå…·æœ‰çµæ´»çš„ç”Ÿæˆé¡ºåºå’Œå¼ºå¤§çš„å¤æ‚æ¨ç†ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æŒ‡ä»¤è°ƒä¼˜çš„dLLMså­˜åœ¨â€œæº¢å‡ºâ€é—®é¢˜ï¼Œå³åœ¨åºåˆ—é•¿åº¦å¢åŠ æ—¶å“åº”åè€Œå˜çŸ­ã€‚</li>
<li>â€œæº¢å‡ºâ€é—®é¢˜çš„æ ¹æºåœ¨äºâ€œ<eos>â€æ ‡è®°çš„åŒé‡è§’è‰²ï¼ˆç»ˆæ­¢å’Œå¡«å……ï¼‰ã€‚</li>
<li>Rainbow Paddingæ–¹æ³•é€šè¿‡æ›¿æ¢é‡å¤çš„â€œ<eos>â€æ ‡è®°ä¸ºä¸åŒçš„å¡«å……ä»¤ç‰Œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜Rainbow Paddingèƒ½æ˜¾è‘—æé«˜é•¿åº¦ç¨³å¥æ€§å’Œè¾“å‡ºè´¨é‡ã€‚</li>
<li>ä»…ä½¿ç”¨å°‘é‡å¡«å……ä»¤ç‰Œï¼ˆå¦‚ä¸ƒä¸ªï¼‰å³å¯é˜²æ­¢è¿‡æ—©ç»ˆæ­¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ff120074cae7aa14033574bb72d5295f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102573&auth_key=1760102573-0-0-77f35a1f51d49fe3e1293710872580ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78feead996d7929521ea3030ae00ad17~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102581&auth_key=1760102581-0-0-838363a5152c9de84763cf4875384e6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af67d3347678c7f1bc31ab2371828c9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102588&auth_key=1760102588-0-0-87b9da575b03cbf64b27a5f438ee7ede&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98a3ddad08ff6be1636c3838624e0684~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102595&auth_key=1760102595-0-0-5bed9a31457858b87aba60c386514d55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f014ba0c296f83cefc370a8a9994a2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102602&auth_key=1760102602-0-0-25cc1c1a752e4b41862e40296bd4783c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e3462769be733a94fd458c680fb79be0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102609&auth_key=1760102609-0-0-40f0d45292c79e3a244a627e8edd23b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers"><a href="#Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers" class="headerlink" title="Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers"></a>Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers</h2><p><strong>Authors:Omar Naim, Swarnadeep Bhar, JÃ©rÃ´me Bolte, Nicholas Asher</strong></p>
<p>While Large Language modelsâ€™ abilities for in-context learning (ICL) have drawn much attention, we examine some of its limitations on semantic tasks involving quantifiers like â€œallâ€ and â€œsomeâ€, as well as on tasks with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these limitations. We propose scaled signed averaging (SSA), a novel alternative to Softmax to mitigate these problems. We show that SSA significantly improves performance on our ICL tasks. In addition, SSA outperforms transformer models with Softmax on several early learning NLP benchmarks and linguistic probing tasks on zero and few-shot settings. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„èƒ½åŠ›ä¸Šå¾—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œä½†æˆ‘ä»¬å¯¹ä¸€äº›è¯­ä¹‰ä»»åŠ¡çš„å±€é™æ€§è¿›è¡Œäº†æ¢ç©¶ï¼Œè¿™äº›ä»»åŠ¡æ¶‰åŠé‡è¯â€œæ‰€æœ‰â€ã€â€œä¸€äº›â€ï¼Œä»¥åŠæ¶‰åŠçº¿æ€§å‡½æ•°çš„ä»»åŠ¡ã€‚æˆ‘ä»¬ç¡®å®šäº†æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¯„åˆ†å‡½æ•°Softmaxæ˜¯å¯¼è‡´è¿™äº›å±€é™æ€§çš„å› ç´ ä¹‹ä¸€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Softmaxçš„æ›¿ä»£æ–¹æ¡ˆâ€”â€”å¸¦ç¬¦å·ç¼©æ”¾å¹³å‡æ³•ï¼ˆSSAï¼‰ï¼Œä»¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬è¯æ˜SSAåœ¨ICLä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰äº†æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­ï¼ŒSSAåœ¨å¤šä¸ªæ—©æœŸå­¦ä¹ NLPåŸºå‡†æµ‹è¯•å’Œè¯­è¨€å­¦æ¢æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä½¿ç”¨Softmaxçš„Transformeræ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14685v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹é¢çš„èƒ½åŠ›å·²å¤‡å—å…³æ³¨ï¼Œä½†å…¶å¤„ç†æ¶‰åŠé‡è¯ï¼ˆå¦‚â€œæ‰€æœ‰â€å’Œâ€œä¸€äº›â€ï¼‰çš„è¯­ä¹‰ä»»åŠ¡ä»¥åŠçº¿æ€§å‡½æ•°ä»»åŠ¡çš„å±€é™æ€§ä¹Ÿæ—¥ç›Šå‡¸æ˜¾ã€‚ç ”ç©¶å‘ç°ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¯„åˆ†å‡½æ•°Softmaxæ˜¯é™åˆ¶ä¹‹ä¸€ã€‚ä¸ºç¼“è§£è¿™äº›é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è¯„åˆ†å‡½æ•°â€”â€”Scaled Signed Averagingï¼ˆSSAï¼‰ã€‚åœ¨ICLä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºSoftmaxï¼Œä¸”åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®çš„å¤šé¡¹æ—©æœŸå­¦ä¹ NLPåŸºå‡†æµ‹è¯•å’Œè¯­è¨€å­¦æ¢æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹é¢è™½å—å…³æ³¨ï¼Œä½†åœ¨å¤„ç†æ¶‰åŠé‡è¯çš„è¯­ä¹‰ä»»åŠ¡åŠçº¿æ€§å‡½æ•°ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Softmaxä½œä¸ºæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¯„åˆ†å‡½æ•°ï¼Œæ˜¯è¿™äº›é™åˆ¶å› ç´ ä¹‹ä¸€ã€‚</li>
<li>ä¸ºæ”¹å–„æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæå‡ºäº†Scaled Signed Averagingï¼ˆSSAï¼‰è¿™ä¸€æ–°å‹è¯„åˆ†å‡½æ•°ã€‚</li>
<li>SSAåœ¨ICLä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºSoftmaxã€‚</li>
<li>SSAåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå¤šé¡¹æ—©æœŸå­¦ä¹ NLPåŸºå‡†æµ‹è¯•å’Œè¯­è¨€å­¦æ¢æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SSAçš„å¼•å…¥æœ‰åŠ©äºæå‡è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bd9e8c3495d42cf7895f0930feef9062~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102616&auth_key=1760102616-0-0-14f822ad1860ea0b9011c26a5d446aec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d752081c225c043003d6f0538341e611~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102623&auth_key=1760102623-0-0-70585d57732b08bf48a69d15fb951cbd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d957aebb0de3ec2be31891128f338b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102630&auth_key=1760102630-0-0-716151c520490c0e7c37e1896faf15c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="FLEx-Personalized-Federated-Learning-for-Mixture-of-Experts-LLMs-via-Expert-Grafting"><a href="#FLEx-Personalized-Federated-Learning-for-Mixture-of-Experts-LLMs-via-Expert-Grafting" class="headerlink" title="FLEx: Personalized Federated Learning for Mixture-of-Experts LLMs via   Expert Grafting"></a>FLEx: Personalized Federated Learning for Mixture-of-Experts LLMs via   Expert Grafting</h2><p><strong>Authors:Fan Liu, Bikang Pan, Zhongyi Wang, Xi Yao, Xiaoying Tang, Jingya Wang, Ye Shi</strong></p>
<p>Federated instruction tuning of large language models (LLMs) is challenged by significant data heterogeneity across clients, demanding robust personalization. The Mixture of Experts (MoE) architecture, where experts can specialize in distinct data patterns, presents a natural architectural solution to this challenge. The inherent sparsity of the MoE architecture, achieved by selectively activating experts, poses a significant challenge to its integration with federated learning (FL). Conventional FL frameworks, designed for dense models, naively aggregate all expert parameters irrespective of their local activation patterns. This naive approach not only undermines MoEâ€™s dynamic sparsity but also risks corrupting the world knowledge within pretrained experts. To address this, we propose FLEx (Federated LLMs with Personalized Experts), a novel framework that leverages pretrained MoE-based LLMs for efficient personalization. By aggregating only the shared non-expert parameters, FLEx significantly reduces communication overhead and preserves the world knowledge stored within the frozen pretrained experts. For personalization, we introduce a novel expert grafting mechanism that leverages dynamic sparsity to construct a client-specific expert from selected components of pretrained experts, tailored to local data. This grafted expert is then fine-tuned locally alongside the gating mechanism. This joint training enables the model to learn when to leverage the shared knowledge from frozen experts and when to employ the personalized one. Evaluations on diverse, non-IID instruction tuning datasets show that FLEx consistently outperforms federated baselines on average, while demonstrating strong knowledge preservation on the knowledge-driven benchmark MMLU. Our code is available at \href{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/FLEx-8F12%7D%7B/texttt%7Bhttps://anonymous.4open.science/r/FLEx-8F12%7D%7D">https://anonymous.4open.science/r/FLEx-8F12}{\texttt{https://anonymous.4open.science/r/FLEx-8F12}}</a>. </p>
<blockquote>
<p>è”åˆè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢ä¸´å®¢æˆ·é—´æ•°æ®çš„å·¨å¤§å¼‚è´¨æ€§æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™è¦æ±‚å¼ºå¤§çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ä¸­çš„ä¸“å®¶å¯ä»¥ä¸“æ³¨äºä¸åŒçš„æ•°æ®æ¨¡å¼ï¼Œä¸ºæ­¤ç±»æŒ‘æˆ˜æä¾›äº†è‡ªç„¶çš„æ¶æ„è§£å†³æ–¹æ¡ˆã€‚MoEæ¶æ„çš„å†…åœ¨ç¨€ç–æ€§ï¼Œé€šè¿‡æœ‰é€‰æ‹©åœ°æ¿€æ´»ä¸“å®¶æ¥å®ç°ï¼Œå¯¹å…¶ä¸è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰çš„é›†æˆæå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„FLæ¡†æ¶ï¼Œè®¾è®¡ä¸ºé€‚ç”¨äºå¯†é›†æ¨¡å‹ï¼Œä¼šæ— å·®åˆ«åœ°èšåˆæ‰€æœ‰ä¸“å®¶å‚æ•°ï¼Œè€Œä¸è€ƒè™‘å…¶æœ¬åœ°æ¿€æ´»æ¨¡å¼ã€‚è¿™ç§ç®€å•çš„æ–¹æ³•ä¸ä»…ç ´åäº†MoEçš„åŠ¨æ€ç¨€ç–æ€§ï¼Œè€Œä¸”è¿˜å¯èƒ½ç ´åé¢„è®­ç»ƒä¸“å®¶ä¸­çš„ä¸–ç•ŒçŸ¥è¯†ã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FLExï¼ˆå¸¦æœ‰ä¸ªæ€§åŒ–ä¸“å®¶çš„è”é‚¦LLMï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºäºMoEçš„é¢„è®­ç»ƒLLMè¿›è¡Œæœ‰æ•ˆä¸ªæ€§åŒ–ã€‚FLExåªèšåˆå…±äº«çš„éä¸“å®¶å‚æ•°ï¼Œä»è€Œæ˜¾è‘—å‡å°‘é€šä¿¡å¼€é”€å¹¶ä¿ç•™å†»ç»“çš„é¢„è®­ç»ƒä¸“å®¶å†…çš„ä¸–ç•ŒçŸ¥è¯†ã€‚å¯¹äºä¸ªæ€§åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä¸“å®¶å«æ¥æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨åŠ¨æ€ç¨€ç–æ€§ä»é¢„è®­ç»ƒä¸“å®¶ä¸­é€‰æ‹©ç»„ä»¶æ„å»ºé’ˆå¯¹ç‰¹å®šå®¢æˆ·çš„ä¸“å®¶ï¼Œä»¥é€‚åº”æœ¬åœ°æ•°æ®ã€‚ç„¶åï¼Œå°†æ­¤å«æ¥çš„ä¸“å®¶ä¸é—¨æ§æœºåˆ¶ä¸€èµ·è¿›è¡Œæœ¬åœ°å¾®è°ƒã€‚è¿™ç§è”åˆè®­ç»ƒä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä½•æ—¶åˆ©ç”¨æ¥è‡ªå†»ç»“çš„ä¸“å®¶çš„å…±äº«çŸ¥è¯†ï¼Œä»¥åŠä½•æ—¶ä½¿ç”¨ä¸ªæ€§åŒ–çš„çŸ¥è¯†ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00965v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è”é‚¦å­¦ä¹ ä¸­åº”ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è”é‚¦æŒ‡ä»¤è°ƒä¼˜é¢ä¸´æ•°æ®å¼‚æ„æ€§çš„æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡å¼ºå¤§çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚MoEæ¶æ„å…è®¸ä¸“å®¶ä¸“æ³¨äºä¸åŒçš„æ•°æ®æ¨¡å¼ï¼Œä¸ºè§£å†³æ­¤æŒ‘æˆ˜æä¾›äº†å¤©ç„¶æ¶æ„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒMoEæ¶æ„çš„å†…åœ¨ç¨€ç–æ€§ç»™å…¶ä¸è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰çš„èåˆå¸¦æ¥äº†éš¾é¢˜ã€‚ä¼ ç»ŸFLæ¡†æ¶æ— æ³•æœ‰æ•ˆå¤„ç†MoEçš„ç¨€ç–æ€§ï¼Œä¸”å¯èƒ½ç ´åé¢„è®­ç»ƒä¸“å®¶çš„å…¨çƒçŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FLExæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„MoEåŸºLLMè¿›è¡Œé«˜æ•ˆä¸ªæ€§åŒ–ã€‚FLExä»…èšåˆå…±äº«çš„éä¸“å®¶å‚æ•°ï¼Œå¤§å¤§é™ä½äº†é€šä¿¡å¼€é”€å¹¶ä¿ç•™äº†é¢„è®­ç»ƒä¸“å®¶å†…çš„å…¨çƒçŸ¥è¯†ã€‚ä¸ºå®ç°ä¸ªæ€§åŒ–ï¼Œå¼•å…¥äº†ä¸“å®¶å«æ¥æœºåˆ¶ï¼Œåˆ©ç”¨åŠ¨æ€ç¨€ç–æ€§æ„å»ºé’ˆå¯¹æœ¬åœ°æ•°æ®çš„å®¢æˆ·ç‰¹å®šä¸“å®¶ã€‚å«æ¥çš„ä¸“å®¶ä¸é—¨æ§æœºåˆ¶ä¸€èµ·è¿›è¡Œæœ¬åœ°å¾®è°ƒã€‚è¿™ç§è”åˆè®­ç»ƒä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å…±äº«çŸ¥è¯†ä¸ä¸ªæ€§åŒ–çŸ¥è¯†ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒFLExåœ¨å¤šæ ·åŒ–ã€éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºè”é‚¦åŸºå‡†æµ‹è¯•ï¼ŒåŒæ—¶åœ¨çŸ¥è¯†é©±åŠ¨å‹MMLUåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´æ•°æ®å¼‚æ„æ€§çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¼ºå¤§çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚</li>
<li>æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸ºè§£å†³æ­¤æŒ‘æˆ˜æä¾›äº†å¤©ç„¶è§£å†³æ–¹æ¡ˆï¼Œä½†ä¸å…¶èåˆå­˜åœ¨å›°éš¾ã€‚</li>
<li>FLExæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„MoEåŸºLLMè¿›è¡Œé«˜æ•ˆä¸ªæ€§åŒ–ï¼Œä»…èšåˆå…±äº«çš„éä¸“å®¶å‚æ•°ï¼Œé™ä½é€šä¿¡å¼€é”€å¹¶ä¿ç•™å…¨çƒçŸ¥è¯†ã€‚</li>
<li>FLExå¼•å…¥ä¸“å®¶å«æ¥æœºåˆ¶ï¼Œåˆ©ç”¨åŠ¨æ€ç¨€ç–æ€§æ„å»ºé’ˆå¯¹æœ¬åœ°æ•°æ®çš„å®¢æˆ·ç‰¹å®šä¸“å®¶ã€‚</li>
<li>å«æ¥çš„ä¸“å®¶ä¸é—¨æ§æœºåˆ¶è”åˆè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å…±äº«çŸ¥è¯†ä¸ä¸ªæ€§åŒ–çŸ¥è¯†ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚</li>
<li>FLExåœ¨å¤šæ ·åŒ–ã€éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºè”é‚¦åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e80bb9a9d7ca5cdb0293db555d02cfa0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102637&auth_key=1760102637-0-0-a1764c45ba922e4353381db0ba0cfce3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8fe1e746bfd81aa4380da9e0095517fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102644&auth_key=1760102644-0-0-2ab76207e9e1651b0492fe88ec4a449a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a8017169b0b12e77aebf8e7d7f6f6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102651&auth_key=1760102651-0-0-f0d4d0c54febc8b2bea3556bee8cdef1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Enhancing-Transformers-Through-Conditioned-Embedded-Tokens"><a href="#Enhancing-Transformers-Through-Conditioned-Embedded-Tokens" class="headerlink" title="Enhancing Transformers Through Conditioned Embedded Tokens"></a>Enhancing Transformers Through Conditioned Embedded Tokens</h2><p><strong>Authors:Hemanth Saratchandran, Simon Lucey</strong></p>
<p>Transformers have transformed modern machine learning, driving breakthroughs in computer vision, natural language processing, and robotics. At the core of their success lies the attention mechanism, which enables the modeling of global dependencies among input tokens. However, we reveal that the attention block in transformers suffers from inherent ill-conditioning, which hampers gradient-based optimization and leads to inefficient training. To address this, we develop a theoretical framework that establishes a direct relationship between the conditioning of the attention block and that of the embedded tokenized data. Building on this insight, we introduce conditioned embedded tokens, a method that systematically modifies the embedded tokens to improve the conditioning of the attention mechanism. Our analysis demonstrates that this approach significantly mitigates ill-conditioning, leading to more stable and efficient training. We validate our methodology across various transformer architectures, achieving consistent improvements in image classification, object detection, instance segmentation, and natural language processing, highlighting its broad applicability and effectiveness. </p>
<blockquote>
<p>è½¬æ¢å™¨ï¼ˆTransformersï¼‰å·²ç»æ”¹å˜äº†ç°ä»£æœºå™¨å­¦ä¹ ï¼Œæ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨äººæŠ€æœ¯çš„çªç ´ã€‚å…¶æ ¸å¿ƒæˆåŠŸçš„å…³é”®åœ¨äºæ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ï¼Œå®ƒèƒ½å¤Ÿå®ç°è¾“å…¥æ ‡è®°ä¹‹é—´å…¨å±€ä¾èµ–å…³ç³»çš„å»ºæ¨¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ­ç¤ºå‡ºè½¬æ¢å™¨çš„æ³¨æ„åŠ›å—å­˜åœ¨å›ºæœ‰çš„ä¸é€‚å®šæ€§é—®é¢˜ï¼Œè¿™é˜»ç¢äº†åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç›´æ¥å…³è”æ³¨æ„åŠ›å—å’ŒåµŒå…¥æ ‡è®°åŒ–æ•°æ®çš„æ¡ä»¶ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¡ä»¶åµŒå…¥æ ‡è®°ï¼ˆConditioned Embedded Tokensï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç³»ç»Ÿåœ°ä¿®æ”¹äº†åµŒå…¥æ ‡è®°ï¼Œä»¥æ”¹å–„æ³¨æ„åŠ›æœºåˆ¶çš„æ¡ä»¶ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—å‡è½»äº†ä¸é€‚å®šæ€§ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†æ›´åŠ ç¨³å®šå’Œé«˜æ•ˆã€‚æˆ‘ä»¬åœ¨å„ç§è½¬æ¢å™¨æ¶æ„ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰æ–¹é¢å®ç°äº†æŒç»­æ€§çš„æ”¹è¿›ï¼Œçªæ˜¾äº†å…¶å¹¿æ³›é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12789v2">PDF</a> ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Transformerä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ä¸ºç°ä»£æœºå™¨å­¦ä¹ å¸¦æ¥äº†å˜é©ï¼Œæ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨äººæŠ€æœ¯çš„çªç ´ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ­ç¤ºäº†Transformerä¸­çš„æ³¨æ„åŠ›å—å­˜åœ¨å›ºæœ‰çš„ä¸é€‚å®šæ€§é—®é¢˜ï¼Œè¿™é˜»ç¢äº†åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å¹¶å¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥å…³è”æ³¨æ„åŠ›å—çš„é€‚å®šæ€§ä¸åµŒå…¥ä»¤ç‰Œæ•°æ®çš„é€‚å®šæ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†æ¡ä»¶åµŒå…¥ä»¤ç‰Œæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç³»ç»Ÿåœ°ä¿®æ”¹åµŒå…¥ä»¤ç‰Œä»¥æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„é€‚å®šæ€§ã€‚åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†ä¸é€‚å®šæ€§é—®é¢˜ï¼Œä½¿è®­ç»ƒæ›´åŠ ç¨³å®šå’Œé«˜æ•ˆã€‚æˆ‘ä»¬åœ¨å„ç§Transformeræ¶æ„ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æŒç»­çš„æ”¹è¿›ï¼Œå‡¸æ˜¾äº†å…¶å¹¿æ³›çš„åº”ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformerçš„æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†ç°ä»£æœºå™¨å­¦ä¹ çš„çªç ´ï¼Œæ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰ã€NLPå’Œæœºå™¨äººæŠ€æœ¯çš„è¿›æ­¥ã€‚</li>
<li>Transformerä¸­çš„æ³¨æ„åŠ›å—å­˜åœ¨å›ºæœ‰çš„ä¸é€‚å®šæ€§é—®é¢˜ï¼Œå½±å“è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹äº†ç†è®ºæ¡†æ¶ï¼Œå…³è”æ³¨æ„åŠ›å—çš„é€‚å®šæ€§ä¸åµŒå…¥ä»¤ç‰Œæ•°æ®çš„é€‚å®šæ€§ã€‚</li>
<li>æå‡ºæ¡ä»¶åµŒå…¥ä»¤ç‰Œæ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹åµŒå…¥ä»¤ç‰Œæé«˜æ³¨æ„åŠ›æœºåˆ¶çš„é€‚å®šæ€§ã€‚</li>
<li>æ¡ä»¶åµŒå…¥ä»¤ç‰Œæ–¹æ³•æœ‰æ•ˆç¼“è§£äº†æ³¨æ„åŠ›å—çš„ä¸é€‚å®šæ€§é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§Transformeræ¶æ„ä¸­éªŒè¯äº†æœ‰æ•ˆæ€§ï¼Œæé«˜äº†å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-df9b7ccf0d325ae053bd49dd7a3016ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102658&auth_key=1760102658-0-0-d6f6ee557d27fdeefef0819c9f73db50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a67479dfd70f33cd76edf90d161ea818~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102665&auth_key=1760102665-0-0-bc9279739f78d6a886cee4abd00bb91b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66c2df2587d2a9580d98e1e4db090fdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102672&auth_key=1760102672-0-0-74f38da11980020071d31b44e89564f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FVQ-A-Large-Scale-Dataset-and-an-LMM-based-Method-for-Face-Video-Quality-Assessment"><a href="#FVQ-A-Large-Scale-Dataset-and-an-LMM-based-Method-for-Face-Video-Quality-Assessment" class="headerlink" title="FVQ: A Large-Scale Dataset and an LMM-based Method for Face Video   Quality Assessment"></a>FVQ: A Large-Scale Dataset and an LMM-based Method for Face Video   Quality Assessment</h2><p><strong>Authors:Sijing Wu, Yunhao Li, Ziwen Xu, Yixuan Gao, Huiyu Duan, Wei Sun, Guangtao Zhai</strong></p>
<p>Face video quality assessment (FVQA) deserves to be explored in addition to general video quality assessment (VQA), as face videos are the primary content on social media platforms and human visual system (HVS) is particularly sensitive to human faces. However, FVQA is rarely explored due to the lack of large-scale FVQA datasets. To fill this gap, we present the first large-scale in-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face videos together with corresponding mean opinion score (MOS) annotations. Along with the FVQ-20K dataset, we further propose a specialized FVQA method named FVQ-Rater to achieve human-like rating and scoring for face video, which is the first attempt to explore the potential of large multimodal models (LMMs) for the FVQA task. Concretely, we elaborately extract multi-dimensional features including spatial features, temporal features, and face-specific features (i.e., portrait features and face embeddings) to provide comprehensive visual information, and take advantage of the LoRA-based instruction tuning technique to achieve quality-specific fine-tuning, which shows superior performance on both FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive analysis demonstrate the significant potential of the FVQ-20K dataset and FVQ-Rater method in promoting the development of FVQA. </p>
<blockquote>
<p>è„¸éƒ¨è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆFVQAï¼‰é™¤äº†é€šç”¨è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰å¤–ï¼Œå€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢ã€‚è„¸éƒ¨è§†é¢‘æ˜¯ç¤¾äº¤åª’ä½“å¹³å°çš„ä¸»è¦å†…å®¹ï¼Œè€Œäººçœ¼è§†è§‰ç³»ç»Ÿå¯¹äººè„¸å°¤ä¸ºæ•æ„Ÿã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡çš„FVQAæ•°æ®é›†ï¼ŒFVQAçš„ç ”ç©¶å¾ˆå°‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¨å‡ºäº†å¤§è§„æ¨¡çš„é‡ç”ŸFVQAæ•°æ®é›†FVQ-20Kï¼Œå®ƒåŒ…å«2ä¸‡æ®µé‡ç”Ÿè„¸éƒ¨è§†é¢‘ä»¥åŠç›¸åº”çš„å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰æ³¨é‡Šã€‚é™¤äº†FVQ-20Kæ•°æ®é›†å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸“é—¨çš„FVQAæ–¹æ³•ï¼Œåä¸ºFVQ-Raterï¼Œå®ç°å¯¹äººè„¸è§†é¢‘çš„æ‹Ÿäººè¯„åˆ†ï¼Œè¿™æ˜¯é¦–æ¬¡æ¢ç´¢å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨FVQAä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç²¾å¿ƒæå–äº†åŒ…æ‹¬ç©ºé—´ç‰¹å¾ã€æ—¶é—´ç‰¹å¾å’Œé¢éƒ¨ç‰¹å®šç‰¹å¾ï¼ˆå³è‚–åƒç‰¹å¾å’Œé¢éƒ¨åµŒå…¥ï¼‰åœ¨å†…çš„å¤šç»´ç‰¹å¾ï¼Œä»¥æä¾›å…¨é¢çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨åŸºäºLoRAçš„æŒ‡ä»¤è°ƒæ•´æŠ€æœ¯å®ç°è´¨é‡ç‰¹å®šçš„å¾®è°ƒï¼Œè¿™åœ¨FVQ-20Kå’ŒCFVQAæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å¤§é‡çš„å®éªŒå’Œç»¼åˆåˆ†æè¯æ˜äº†FVQ-20Kæ•°æ®é›†å’ŒFVQ-Rateræ–¹æ³•åœ¨æ¨åŠ¨FVQAå‘å±•ä¸­çš„æ˜¾è‘—æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09255v2">PDF</a> Accepted by ACM MM 2025. Project page:   <a target="_blank" rel="noopener" href="https://github.com/wsj-sjtu/FVQ">https://github.com/wsj-sjtu/FVQ</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>é™¤äº†é€šç”¨è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰ï¼Œé’ˆå¯¹è„¸éƒ¨è§†é¢‘çš„è¯„ä¼°ï¼ˆFVQAï¼‰ä¹Ÿå€¼å¾—ç ”ç©¶ã€‚è„¸éƒ¨è§†é¢‘æ˜¯ç¤¾äº¤åª’ä½“å¹³å°çš„ä¸»è¦å†…å®¹ï¼Œä¸”äººç±»è§†è§‰ç³»ç»Ÿå¯¹äººè„¸ç‰¹åˆ«æ•æ„Ÿã€‚ç„¶è€Œï¼ŒFVQAçš„ç ”ç©¶å› ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†è€Œå—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡é‡å¤–FVQAæ•°æ®é›†FVQ-20Kï¼ŒåŒ…å«20,000ä¸ªé‡å¤–è„¸éƒ¨è§†é¢‘åŠå…¶å¯¹åº”çš„å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸“é—¨çš„FVQAæ–¹æ³•FVQ-Raterï¼Œå®ç°å¯¹è„¸éƒ¨è§†é¢‘çš„äººç±»å¼è¯„åˆ†ï¼Œè¿™æ˜¯é¦–æ¬¡æ¢ç´¢å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨FVQAä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬ç²¾å¿ƒæå–äº†åŒ…æ‹¬ç©ºé—´ç‰¹å¾ã€æ—¶é—´ç‰¹å¾å’Œé¢éƒ¨ç‰¹å®šç‰¹å¾åœ¨å†…çš„å¤šç»´ç‰¹å¾ï¼Œåˆ©ç”¨LoRAåŸºäºæŒ‡ä»¤çš„å¾®è°ƒæŠ€æœ¯å®ç°è´¨é‡ç‰¹å®šçš„ç²¾ç»†è°ƒæ•´ï¼Œåœ¨FVQ-20Kå’ŒCFVQAæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è„¸éƒ¨è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆFVQAï¼‰æ˜¯ç¤¾äº¤åª’ä½“æ—¶ä»£çš„é‡è¦ç ”ç©¶é¢†åŸŸï¼Œå› ä¸ºè„¸éƒ¨è§†é¢‘æ˜¯ä¸»æµå†…å®¹ã€‚</li>
<li>äººç±»è§†è§‰ç³»ç»Ÿå¯¹è„¸éƒ¨è´¨é‡éå¸¸æ•æ„Ÿï¼Œä½¿å¾—FVQAç ”ç©¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡çš„FVQAæ•°æ®é›†é™åˆ¶äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</li>
<li>å¼•å…¥äº†é¦–ä¸ªå¤§è§„æ¨¡é‡å¤–FVQAæ•°æ®é›†FVQ-20Kï¼ŒåŒ…å«20,000ä¸ªå¸¦æ³¨é‡Šçš„è„¸éƒ¨è§†é¢‘ã€‚</li>
<li>æå‡ºäº†FVQ-Rateræ–¹æ³•ï¼Œèƒ½æ¨¡æ‹Ÿäººç±»å¯¹é¢éƒ¨è§†é¢‘çš„è´¨é‡è¯„åˆ†ã€‚</li>
<li>FVQ-Rateråˆ©ç”¨å¤šç»´ç‰¹å¾æå–å’ŒåŸºäºLoRAçš„è´¨é‡ç‰¹å®šå¾®è°ƒæŠ€æœ¯ï¼Œå®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>FVQ-20Kæ•°æ®é›†å’ŒFVQ-Rateræ–¹æ³•åœ¨ä¿ƒè¿›FVQAé¢†åŸŸå‘å±•æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-412dea2329e499a2e87daefcb3ad642c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102680&auth_key=1760102680-0-0-d44a11fe53ce57d27a1d995aeb71d113&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5b739b60dc0d9e79d9098cbc07d00f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102689&auth_key=1760102689-0-0-357fbd8a62858d6010108686d42d3a48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dafaa1cff29c656ca1682e031cd68be2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102696&auth_key=1760102696-0-0-cabb26490ab5cd7c669eb0df97785243&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92ca8b5b65839734914bf4b6e08c6cbd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102702&auth_key=1760102702-0-0-de86a82d9bdcbdb92a6487e1aa628e66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32ee485762790aded252972b57ea3e01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102709&auth_key=1760102709-0-0-95e61263c84b8dab8c700f7b02edf369&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fcd38650890324ddc6010dd103bd9068~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102717&auth_key=1760102717-0-0-4103f403127aa4d061bdb763e8628ea2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science"><a href="#AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science" class="headerlink" title="AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science"></a>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science</h2><p><strong>Authors:Chenyue Li, Wen Deng, Mengqian Lu, Binhang Yuan</strong></p>
<p>The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges and boosting scientific discovery in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. Toward this end, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. AtmosSci-Bench features a dual-format design comprising both multiple-choice questions (MCQs) and open-ended questions (OEQs), enabling scalable automated evaluation alongside deeper analysis of conceptual understanding. We employ a template-based MCQ generation framework to create diverse, graduate-level problems with symbolic perturbation, while OEQs are used to probe open-ended reasoning. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate services by offering a standard and rigorous evaluation framework. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench">https://github.com/Relaxed-System-Lab/AtmosSci-Bench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œä¸ºè§£å†³å¤§æ°”ç§‘å­¦ä¸­çš„å¤æ‚æŒ‘æˆ˜å’Œæ¨åŠ¨ç§‘å­¦å‘å±•æä¾›äº†å˜é©æ€§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¦åœ¨è¿™ä¸€é¢†åŸŸæœ‰æ•ˆåœ°åˆ©ç”¨LLMï¼Œéœ€è¦ç¨³å¥è€Œå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AtmosSci-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMåœ¨äº”ä¸ªå¤§æ°”ç§‘å­¦æ ¸å¿ƒç±»åˆ«çš„é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼šæ°´æ–‡å­¦ã€å¤§æ°”åŠ¨åŠ›å­¦ã€å¤§æ°”ç‰©ç†å­¦ã€åœ°çƒç‰©ç†å­¦å’Œç‰©ç†æµ·æ´‹å­¦ã€‚AtmosSci-Benché‡‡ç”¨åŒæ ¼å¼è®¾è®¡ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰å’Œå¼€æ”¾å¼é—®é¢˜ï¼ˆOEQsï¼‰ï¼Œæ—¢èƒ½å¤Ÿè¿›è¡Œå¯æ‰©å±•çš„è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œåˆèƒ½æ·±å…¥åˆ†ææ¦‚å¿µç†è§£ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºæ¨¡æ¿çš„MCQç”Ÿæˆæ¡†æ¶æ¥åˆ›å»ºå…·æœ‰ç¬¦å·æ‰°åŠ¨çš„å¤šæ ·åŒ–ã€ç ”ç©¶ç”Ÿæ°´å¹³çš„é—®é¢˜ï¼Œè€ŒOEQsåˆ™ç”¨äºæµ‹è¯•å¼€æ”¾å¼æ¨ç†ã€‚æˆ‘ä»¬å¯¹å…·æœ‰ä»£è¡¨æ€§çš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå°†å…¶åˆ†ä¸ºå››ç»„ï¼šæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€é«˜çº§æ¨ç†æ¨¡å‹ã€æ•°å­¦å¢å¼ºæ¨¡å‹å’Œé¢†åŸŸç‰¹å®šæ°”å€™æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†æå¯¹LLMåœ¨å¤§æ°”ç§‘å­¦ä¸­çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›æä¾›äº†ä¸€äº›æœ‰è¶£çš„è§è§£ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒAtmosSci-Benchå¯ä»¥é€šè¿‡æä¾›æ ‡å‡†å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œæˆä¸ºæ¨åŠ¨LLMåœ¨æ°”å€™æœåŠ¡ä¸­åº”ç”¨çš„é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Relaxed-System-Lab/AtmosSci-Benchä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01159v3">PDF</a> 37 pages, 4 figures, 13 tables</p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿…é€Ÿè¿›æ­¥ï¼Œä¸ºåº”å¯¹å¤§æ°”ç§‘å­¦ä¸­çš„å¤æ‚æŒ‘æˆ˜å’Œä¿ƒè¿›ç§‘å­¦å‘ç°æä¾›äº†å˜é©æ€§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¦åœ¨è¿™ä¸€é¢†åŸŸæœ‰æ•ˆåœ°åˆ©ç”¨LLMï¼Œéœ€è¦å¼ºå¤§è€Œå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AtmosSci-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨äº”ä¸ªå¤§æ°”ç§‘å­¦æ ¸å¿ƒç±»åˆ«çš„é—®é¢˜ä¸Šçš„è¡¨ç°ï¼šæ°´æ–‡å­¦ã€å¤§æ°”åŠ¨åŠ›å­¦ã€å¤§æ°”ç‰©ç†å­¦ã€åœ°çƒç‰©ç†å­¦å’Œç‰©ç†æµ·æ´‹å­¦ã€‚AtmosSci-Benché‡‡ç”¨åŒé‡æ ¼å¼è®¾è®¡ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰å’Œå¼€æ”¾æ€§é—®é¢˜ï¼ˆOEQsï¼‰ï¼Œä»¥å®ç°å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–è¯„ä¼°ä»¥åŠå¯¹æ¦‚å¿µç†è§£çš„æ·±å…¥åˆ†æã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºæ¨¡æ¿çš„MCQç”Ÿæˆæ¡†æ¶æ¥åˆ›å»ºå…·æœ‰ç¬¦å·æ‰°åŠ¨çš„å¤šæ ·åŒ–ã€ç ”ç©¶ç”Ÿæ°´å¹³çš„é—®é¢˜ï¼Œè€ŒOEQsåˆ™ç”¨äºæ¢ç©¶å¼€æ”¾å¼æ¨ç†ã€‚æˆ‘ä»¬å¯¹å…·æœ‰ä»£è¡¨æ€§çš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå°†å®ƒä»¬åˆ†ä¸ºå››ä¸ªç±»åˆ«ï¼šæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€é«˜çº§æ¨ç†æ¨¡å‹ã€æ•°å­¦å¢å¼ºæ¨¡å‹å’Œç‰¹å®šé¢†åŸŸçš„æ°”å€™æ¨¡å‹ã€‚æˆ‘ä»¬çš„åˆ†ææä¾›äº†å…³äºLLMåœ¨è§£å†³å¤§æ°”ç§‘å­¦æ–¹é¢çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›çš„æœ‰è¶£è§è§£ã€‚æˆ‘ä»¬ç›¸ä¿¡AtmosSci-Benchå¯ä»¥é€šè¿‡æä¾›æ ‡å‡†å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ï¼Œæˆä¸ºæ¨è¿›LLMåœ¨æ°”å€™æœåŠ¡ä¸­åº”ç”¨çš„è‡³å…³é‡è¦çš„æ­¥éª¤ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Relaxed-System-Lab/AtmosSci-Benchä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤§æ°”ç§‘å­¦é—®é¢˜æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>AtmosSci-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨äº”ä¸ªå¤§æ°”ç§‘å­¦æ ¸å¿ƒé¢†åŸŸçš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨åŒé‡æ ¼å¼è®¾è®¡ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜ï¼Œä»¥å…¨é¢è¯„ä¼°LLMçš„æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç¬¦å·æ‰°åŠ¨åˆ›å»ºå¤šæ ·åŒ–çš„é—®é¢˜ï¼Œä»¥æµ‹è¯•LLMçš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>å¯¹ä¸åŒç±»å‹çš„LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€é«˜çº§æ¨ç†æ¨¡å‹ç­‰ã€‚</li>
<li>AtmosSci-Benchä¸ºæ¨è¿›LLMåœ¨æ°”å€™æœåŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ ‡å‡†è¯„ä¼°æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c498a6b9c01cde020eb4718d78d3a561~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102724&auth_key=1760102724-0-0-28e7295c25a98821374f46c3508dd687&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0cdf467947d9829232054cf6ad30602f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102732&auth_key=1760102732-0-0-0193833799024bf0973c46d7cdc44c34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adb9269d5845a0d8bc5e5455bdbdfac6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102740&auth_key=1760102740-0-0-7e34cd33ab471fc657ece54ccf8bc90d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Imagining-the-Unseen-Generative-Location-Modeling-for-Object-Placement"><a href="#Imagining-the-Unseen-Generative-Location-Modeling-for-Object-Placement" class="headerlink" title="Imagining the Unseen: Generative Location Modeling for Object Placement"></a>Imagining the Unseen: Generative Location Modeling for Object Placement</h2><p><strong>Authors:Jooyeol Yun, Davide Abati, Mohamed Omran, Jaegul Choo, Amirhossein Habibian, Auke Wiggers</strong></p>
<p>Location modeling, or determining where non-existing objects could feasibly appear in a scene, has the potential to benefit numerous computer vision tasks, from automatic object insertion to scene creation in virtual reality. Yet, this capability remains largely unexplored to date. In this paper, we develop a generative location model that, given an object class and an image, learns to predict plausible bounding boxes for such an object. Our approach first tokenizes the image and target object class, then decodes bounding box coordinates through an autoregressive transformer. This formulation effectively addresses two core challenges in locatio modeling: the inherent one-to-many nature of plausible locations, and the sparsity of existing location modeling datasets, where fewer than 1% of valid placements are labeled. Furthermore, we incorporate Direct Preference Optimization to leverage negative labels, refining the spatial predictions. Empirical evaluations reveal that our generative location model achieves superior placement accuracy on the OPA dataset as compared to discriminative baselines and image composition approaches. We further test our model in the context of object insertion, where it proposes locations for an off-the-shelf inpainting model to render objects. In this respect, our proposal exhibits improved visual coherence relative to state-of-the-art instruction-tuned editing methods, demonstrating a high-performing location modelâ€™s utility in a downstream application. </p>
<blockquote>
<p>ä½ç½®å»ºæ¨¡ï¼Œæˆ–è€…ç¡®å®šåœºæ™¯ä¸­ä¸å­˜åœ¨çš„ç‰©ä½“å¯èƒ½å‡ºç°çš„ä½ç½®ï¼Œå…·æœ‰ä¸ºä¼—å¤šè®¡ç®—æœºè§†è§‰ä»»åŠ¡å¸¦æ¥æ½œåœ¨ç›Šå¤„çš„æ½œåŠ›ï¼Œä»è‡ªåŠ¨ç‰©ä½“æ’å…¥åˆ°è™šæ‹Ÿç°å®åœºæ™¯åˆ›å»ºã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›è‡³ä»Šå°šæœªå¾—åˆ°å……åˆ†çš„æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”Ÿæˆå¼ä½ç½®æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç»™å®šçš„ç›®æ ‡å¯¹è±¡å’Œå›¾åƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿé¢„æµ‹è¯¥å¯¹è±¡çš„åˆç†è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆå¯¹å›¾åƒå’Œç›®æ ‡å¯¹è±¡ç±»åˆ«è¿›è¡Œæ ‡è®°åŒ–ï¼Œç„¶åé€šè¿‡è‡ªå›å½’å˜å‹å™¨è§£ç è¾¹ç•Œæ¡†åæ ‡ã€‚è¿™ç§è¡¨è¿°æœ‰æ•ˆåœ°è§£å†³äº†ä½ç½®å»ºæ¨¡ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šåˆç†ä½ç½®çš„å›ºæœ‰çš„ä¸€åˆ°å¤šå¯¹åº”å…³ç³»ï¼Œä»¥åŠç°æœ‰ä½ç½®å»ºæ¨¡æ•°æ®é›†ç¨€ç–çš„é—®é¢˜ï¼Œå…¶ä¸­åªæœ‰ä¸åˆ°1%çš„æœ‰æ•ˆä½ç½®è¢«æ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†ç›´æ¥åå¥½ä¼˜åŒ–æ¥åˆ©ç”¨è´Ÿé¢æ ‡ç­¾ï¼Œå¯¹ç©ºé—´é¢„æµ‹è¿›è¡Œç»†åŒ–ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆä½ç½®æ¨¡å‹åœ¨OPAæ•°æ®é›†ä¸Šçš„æ”¾ç½®ç²¾åº¦ä¼˜äºåˆ¤åˆ«åŸºå‡†å’Œå›¾åƒç»„åˆæ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨ç‰©ä½“æ’å…¥çš„ä¸Šä¸‹æ–‡ä¸­æµ‹è¯•äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå®ƒä¸ºç°æˆçš„å›¾åƒä¿®å¤æ¨¡å‹å‘ˆç°ç‰©ä½“ä½ç½®å»ºè®®ã€‚åœ¨è¿™æ–¹é¢ï¼Œæˆ‘ä»¬çš„ææ¡ˆä¸æœ€æ–°çš„æŒ‡ä»¤è°ƒæ•´ç¼–è¾‘æ–¹æ³•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„è§†è§‰è¿è´¯æ€§ï¼Œè¯æ˜äº†é«˜æ€§èƒ½ä½ç½®æ¨¡å‹åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13564v2">PDF</a> Accepted by ICCV 2025 DRL4Real Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆå¼ä½ç½®æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç›®æ ‡å¯¹è±¡å’Œå›¾åƒé¢„æµ‹å…¶åˆç†çš„è¾¹ç•Œæ¡†ä½ç½®ã€‚æ¨¡å‹é¦–å…ˆå¯¹å›¾åƒå’Œç›®æ ‡å¯¹è±¡è¿›è¡Œæ ‡è®°åŒ–ï¼Œç„¶åé€šè¿‡è‡ªå›å½’è½¬æ¢å™¨è§£ç è¾¹ç•Œæ¡†åæ ‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä½ç½®å»ºæ¨¡ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šä¸€æ˜¯åˆç†ä½ç½®å…·æœ‰ä¸€å¯¹å¤šçš„ç‰¹ç‚¹ï¼›äºŒæ˜¯ç°æœ‰çš„ä½ç½®å»ºæ¨¡æ•°æ®é›†ç¨€å°‘ã€‚é€šè¿‡ç»“åˆç›´æ¥åå¥½ä¼˜åŒ–å’Œè´Ÿæ ·æœ¬ï¼Œä¼˜åŒ–äº†ç©ºé—´é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæœ¬æ–‡æ¨¡å‹åœ¨OPAæ•°æ®é›†ä¸Šçš„æ”¾ç½®å‡†ç¡®åº¦ä¼˜äºåˆ¤åˆ«å¼åŸºå‡†å’Œå›¾åƒç»„åˆæ–¹æ³•ã€‚åœ¨å¯¹è±¡æ’å…¥çš„ä¸Šä¸‹æ–‡ä¸­è¿›ä¸€æ­¥æµ‹è¯•äº†æ¨¡å‹ï¼Œç»“æœè¡¨æ˜æ¨¡å‹èƒ½å¤Ÿæå‡ºé€‚åˆè¡¥å…¨æ¨¡å‹çš„ç‰©ä½“ä½ç½®ï¼Œä¸æœ€æ–°çš„æŒ‡ä»¤è°ƒä¼˜ç¼–è¾‘æ–¹æ³•ç›¸æ¯”ï¼Œè§†è§‰è¿è´¯æ€§æœ‰æ‰€æå‡ã€‚è¯¥æ¨¡å‹çš„ä½ç½®å»ºæ¨¡æ€§èƒ½ä¼˜å¼‚ï¼Œåœ¨ä¸‹æ¸¸åº”ç”¨ä¸­æœ‰å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå¼ä½ç½®æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç›®æ ‡å¯¹è±¡å’Œå›¾åƒé¢„æµ‹åˆç†çš„è¾¹ç•Œæ¡†ä½ç½®ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ ‡è®°åŒ–å’Œè‡ªå›å½’è½¬æ¢å™¨è§£ç æ¥è§£å†³ä½ç½®å»ºæ¨¡çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹è§£å†³äº†åˆç†ä½ç½®ä¸€å¯¹å¤šå’Œç°æœ‰æ•°æ®é›†ç¨€å°‘çš„é—®é¢˜ã€‚</li>
<li>ç»“åˆç›´æ¥åå¥½ä¼˜åŒ–å’Œè´Ÿæ ·æœ¬æå‡äº†ç©ºé—´é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨OPAæ•°æ®é›†ä¸Šçš„æ”¾ç½®å‡†ç¡®åº¦ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åœ¨å¯¹è±¡æ’å…¥çš„ä¸Šä¸‹æ–‡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„å®ç”¨æ€§ï¼Œæé«˜äº†è§†è§‰è¿è´¯æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-97df018755febf27a4f56d083677e5ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102748&auth_key=1760102748-0-0-f74291c0c7ba441747cdef8c0e7b1cd2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97f123f4174c8649774194ed51986170~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102755&auth_key=1760102755-0-0-5ae3791608476b3d9cde82dfe7c8ea66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e73590676a48f2fc316f081aee61505~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102761&auth_key=1760102761-0-0-32e114461130e4ff24a1bf6515fc9a41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-411f7c383634bf9fe1b4d66ddfd1b553~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102768&auth_key=1760102768-0-0-740c3dafe9e764a8ea7f71e636c61c92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3738a348446cafec1daa4445dbb56b90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102775&auth_key=1760102775-0-0-4773bf781c4ca111b8e8c07ed4f097ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-1cb8a4b5ab6999a2190e744d64b8aadd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083139&auth_key=1760083139-0-0-9d2ec30427e235b33afe1d1e5f64497b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Multi-Objective Multi-Agent Path Finding with Lexicographic Cost   Preferences
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d2897c8fa1f46a4fcd6c88e66d56fe68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082573&auth_key=1760082573-0-0-84dce57a4322036d03a148160941ef91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  LeMAJ (Legal LLM-as-a-Judge) Bridging Legal Reasoning and LLM   Evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
