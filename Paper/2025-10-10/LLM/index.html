<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-10-10  Vibe Checker Aligning Code Evaluation with Human Preference">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-5d30dc1b2c174d4bb19a9f51df938ba9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082923&auth_key=1760082923-0-0-161031898c3a921cfcc49d31605667a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-10-更新"><a href="#2025-10-10-更新" class="headerlink" title="2025-10-10 更新"></a>2025-10-10 更新</h1><h2 id="Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference"><a href="#Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference" class="headerlink" title="Vibe Checker: Aligning Code Evaluation with Human Preference"></a>Vibe Checker: Aligning Code Evaluation with Human Preference</h2><p><strong>Authors:Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings, Jiao Sun</strong></p>
<p>Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models’ code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding. </p>
<blockquote>
<p>大规模语言模型（LLM）已经推动了氛围编码的发展，用户利用LLM生成代码并借助自然语言交互进行迭代优化，直到通过他们的氛围检查。氛围检查与真实世界的用户偏好有关，并且超越了功能范围：解决方案不仅要能够正确运行，而且要结构清晰、保持意图并且始终正确。然而，当前的代码评估仍然侧重于准确抓住用户意图的前k个选项，仅关注功能正确性，忽视了用户经常应用的非功能性指令。在本文中，我们假设指令遵循是氛围检查中缺失的部分，除了功能正确性外，它代表了人类在编码过程中的偏好。为了量化模型遵循代码指令的能力并使用可测量的信号进行评估，我们提出了VeriCode，这是一个包含30个可验证代码指令的目录，并配备了相应的确定性验证器。我们使用目录来增强现有的评估套件，从而形成了Vibe Checker，一个可以评估代码指令遵循和功能正确性的测试平台。在对31个领先的大型语言模型进行评估后，我们发现即使是最强大的模型也很难遵循多个指令，并且会出现明显的功能回归。最重要的是，功能正确性和指令遵循的组合分数与人类的偏好关联度最高，后者在实际编程任务中成为主要区别因素。我们的工作确定了氛围检查的核心因素，为基准测试和研发更符合用户编码偏好的模型提供了具体路径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07315v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）推动了“氛围编码”，用户利用LLM生成并通过自然语言交互迭代完善代码，直至满足他们的氛围感知。然而，当前代码评估主要关注功能正确性，忽略了用户的非功能指令。本文提出指令遵循是氛围感知中缺失的重要部分，并代表了人类编码中的偏好。为了量化模型遵循代码指令的能力，我们提出了VeriCode，包含30种可验证的代码指令及其相应的确定性验证器。我们使用此分类来增强现有的评估套件，从而建立了一个评估代码指令遵循和功能正确性的测试平台Vibe Checker。评估领先的LLM时发现，即使在强大的模型也难以遵循多个指令，表现出明显的功能回归。最重要的是，与人类偏好最相关的是功能正确性和指令遵循的复合分数，其中后者在现实编程任务中成为主要区别。本文的研究为氛围感知提供了核心因素的具体路径，为更符合用户编码偏好的模型评估和研发提供了方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）推动了氛围编码，允许用户通过自然语言交互生成和迭代完善代码。</li>
<li>当前的代码评估主要关注功能正确性，忽略了用户的非功能指令。</li>
<li>指令遵循是氛围感知编码中重要的部分，代表人类编码中的偏好。</li>
<li>提出了VeriCode分类，包含30种可验证的代码指令及其验证器。</li>
<li>通过建立Vibe Checker测试平台，可以评估代码指令遵循和功能正确性。</li>
<li>评估发现，即使是强大的LLM也难以完全遵循多个指令，存在功能回归现象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7a5581bd66b3b5f63ccf5be1157bd3fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082930&auth_key=1760082930-0-0-23dc53906448752fd64407908ae4a066&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ecd893a686c8fca91e01687151efa5bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082938&auth_key=1760082938-0-0-0670dd5cfa4a87a7d962daeab1829fc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce20eece145704c33016a42b614cdc98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082945&auth_key=1760082945-0-0-6735917d4ac9b8a32b5b126ad6e33066&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57e86c8476b803b2ffdf5b1932087d62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082952&auth_key=1760082952-0-0-18cad3d6ac544d9a35afca10743cb8b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prompt-Synthesize-Fine-Tune-A-Secure-Code-Generation-Recipe"><a href="#Prompt-Synthesize-Fine-Tune-A-Secure-Code-Generation-Recipe" class="headerlink" title="Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe"></a>Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe</h2><p><strong>Authors:Junjie Li, Fazle Rabbi, Bo Yang, Song Wang, Jinqiu Yang</strong></p>
<p>Although Large Language Models (LLMs) show promising solutions to automated code generation, they often produce insecure code that threatens software security. Current approaches (e.g., SafeCoder) to improve secure code generation suffer from limited and imbalanced datasets, reducing their effectiveness and generalizability. In this work, we present Secure-Instruct, a novel framework that automatically synthesizes high-quality vulnerable and secure code examples, generates fine-tuning instructions, and instruction-tunes LLMs to align task description and secure code generation abilities. We evaluate Secure-Instruct on four representative LLMs using two benchmarks: our own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44 CWEs, all without overlap with Secure-Instruct’s synthetic instruction-tuning dataset, while CWEval covers 31 CWEs with 119 manually verified security-critical tasks. We find that Secure-Instruct improves not only the security but also the functional correctness of the generated code. On CWEBench, Secure-Instruct substantially improves secure code generation, giving a 14.3% average increase in secure ratio over the pretrained models and outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14% increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained models, and surpasses SafeCoder by 15.8% and 6.8% respectively. </p>
<blockquote>
<p>尽管大型语言模型（LLM）在自动代码生成方面显示出有前途的解决方案，但它们通常会产生威胁软件安全的不安全代码。当前改进安全代码生成的方法（例如SafeCoder）受限于数据集的大小和不平衡，降低了其有效性和通用性。在这项工作中，我们提出了Secure-Instruct，一个能够自动合成高质量脆弱和安全代码示例、生成微调指令、并根据指令调整LLM以匹配任务描述和安全代码生成能力的新型框架。我们在四个代表性LLM上评估了Secure-Instruct，使用了两个基准测试：我们自己的CWEBench和现有的CWEval。CWEBench包含44个CWE的93个场景，所有场景与Secure-Instruct的合成指令调整数据集没有重叠，而CWEval涵盖31个CWE的119个经过手动验证的安全关键任务。我们发现Secure-Instruct不仅提高了生成的代码的安全性，还提高了其功能性正确性。在CWEBench上，Secure-Instruct显著提高了安全代码生成能力，使安全比率较预训练模型平均提高了14.3%，并优于SafeCoder 7.6%。在CWEval上，Secure-Instruct在Func-Sec@1指标上较预训练模型提高了CodeLlama-7B的14%和Mistral-7B的5.8%，并分别超过了SafeCoder的15.8%和6.8%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07189v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自动化代码生成方面展现出巨大潜力，但常常生成威胁软件安全的代码。当前如SafeCoder等改善安全代码生成的方法受限于数据集，影响效果和泛化能力。本研究提出Secure-Instruct框架，可自动合成高质量的安全和漏洞代码示例，生成微调指令并对LLM进行指令微调，以提升任务描述与安全代码生成能力的对齐。在自定义的CWEBench和现有的CWEval两个基准测试上评估显示，Secure-Instruct不仅提高了代码的安全性，还提升了功能正确性。相较于预训练模型，Secure-Instruct在CWEBench上的安全比率平均提高了14.3%，并优于SafeCoder的7.6%。在CWEval上，Secure-Instruct对CodeLlama-7B和Mistral-7B的Func-Sec@1分别提高了14%和5.8%，并且优于SafeCoder。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自动化代码生成时易产生威胁软件安全的代码。</li>
<li>当前改善安全代码生成的方法如SafeCoder受限于数据集，影响效果和泛化能力。</li>
<li>Secure-Instruct框架通过自动合成高质量的安全和漏洞代码示例，并生成微调指令来提升LLM的性能。</li>
<li>Secure-Instruct提高了代码的安全性和功能正确性。</li>
<li>在CWEBench基准测试中，Secure-Instruct相较于预训练模型安全比率平均提高了14.3%，并优于SafeCoder。</li>
<li>在CWEval基准测试中，Secure-Instruct对CodeLlama-7B和Mistral-7B的Func-Sec@1有所提高。</li>
<li>Secure-Instruct框架的提出为改善LLM在安全代码生成方面的不足提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07189">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-397bde8b1075cbf7f5f054794a64df67~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082959&auth_key=1760082959-0-0-62d32986315006ad15002bcaeb8c1a4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63775f37e07fc3114fbf848892771d4d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082968&auth_key=1760082968-0-0-e238dc00a110539e7c327b29a44e0772&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a29498bdcee89500a6bafc54fa3d7c72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082974&auth_key=1760082974-0-0-449da3cbc39b71d19a065a59e380abe4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TRIM-Token-wise-Attention-Derived-Saliency-for-Data-Efficient-Instruction-Tuning"><a href="#TRIM-Token-wise-Attention-Derived-Saliency-for-Data-Efficient-Instruction-Tuning" class="headerlink" title="TRIM: Token-wise Attention-Derived Saliency for Data-Efficient   Instruction Tuning"></a>TRIM: Token-wise Attention-Derived Saliency for Data-Efficient   Instruction Tuning</h2><p><strong>Authors:Manish Nagaraj, Sakshi Choudhary, Utkarsh Saxena, Deepak Ravikumar, Kaushik Roy</strong></p>
<p>Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though curating them remains challenging. Existing methods often rely on coarse, sample-level signals like gradients, an approach that is computationally expensive and overlooks fine-grained features. To address this, we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a forward-only, token-centric framework. Instead of using gradients, TRIM operates by matching underlying representational patterns identified via attention-based “fingerprints” from a handful of target samples. Such an approach makes TRIM highly efficient and uniquely sensitive to the structural features that define a task. Coresets selected by our method consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. By avoiding expensive backward passes, TRIM achieves this at a fraction of the computational cost. These findings establish TRIM as a scalable and efficient alternative for building high-quality instruction-tuning datasets. </p>
<blockquote>
<p>指令微调对于将大型语言模型（LLM）与下游任务对齐至关重要，通常依赖于大规模、多样化的语料库。然而，小型高质量子集，也称为核心集，可以产生相当或更好的结果，尽管对其进行策划仍然具有挑战性。现有方法通常依赖于粗略的样本级信号，如梯度，这种方法计算量大，并且忽略了细粒度特征。为了解决这个问题，我们引入了TRIM（通过可解释的多层注意力获取令牌相关性），这是一个仅前向、以令牌为中心的框架。TRIM不通过梯度工作，而是通过匹配少量目标样本中通过注意力“指纹”识别出的潜在表示模式来运行。这种方法使TRIM具有高效率，并且对定义任务的结构特征具有独特敏感性。通过我们的方法选择的核心集在下游任务上始终超过最新基线高达9%，并在某些设置下甚至超过全数据精细调整的性能。通过避免昂贵的反向传递，TRIM以较小的计算成本实现了这一点。这些发现证明TRIM是一种可扩展和高效的构建高质量指令调整数据集的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07118v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）的指令调整对于下游任务至关重要，通常依赖于大量、多样的语料库。然而，小的、高质量的子集（称为核心集）可以产生相当或更好的结果，但挑选它们仍然具有挑战性。现有方法通常依赖于粗糙的样本级信号，如梯度，这种方法计算量大，忽略了细微的特征。为解决这一问题，我们推出TRIM（通过可解释的多层注意力获取令牌相关性），这是一个仅前向传播的、以令牌为中心的框架。TRIM不通过梯度工作，而是匹配来自少量目标样本的基于注意力的“指纹”来识别潜在的模式。这种方法使TRIM既高效又独特地关注定义任务的结构特征。我们的方法选择的核心集在下游任务上始终优于最新基线高达9%，并在某些情况下甚至超过了全数据微调的性能。通过避免昂贵的反向传播，TRIM以极低的计算成本实现了这一目标。这些发现证明了TRIM作为一种可扩展和高效的指令调整数据集构建方法的地位。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令调整对于LLM的下游任务至关重要，需要用到大量、多样的语料库进行支持。</li>
<li>核心集的选择是提高指令调整效率的关键，但其挑选过程具有挑战性。</li>
<li>现有方法通常依赖样本级信号如梯度进行核心集选择，计算量大且忽略细微特征。</li>
<li>TRIM框架通过匹配基于注意力“指纹”的潜在模式进行核心集选择，无需依赖梯度信息。</li>
<li>TRIM方法选择的核心集在下游任务上表现优异，相较于最新基线有显著提升。</li>
<li>TRIM方法避免了昂贵的反向传播过程，计算效率极高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cae524edf5bd9a184fff3d22b02bb973~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099167&auth_key=1760099167-0-0-d0a13dcaa22b5e662399a7d1eb00dd96&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89d6699e4d8e4ae7b13c407c975edcf3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083046&auth_key=1760083046-0-0-4055c8ded77a99b4aaaad320899101c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0449b7ae9671b4cf72636f913854e246~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083053&auth_key=1760083053-0-0-1962fb6141e783af7e84d28a33cdeaac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27c6f538feaf13f618ee1128f6c1ee59~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083060&auth_key=1760083060-0-0-793d40fbdc9b97cf07a6ec13738a2d7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4945dd839fc7ef83c0a87db41e18fca9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083066&auth_key=1760083066-0-0-1f532c9ab424366cf524a52ed27c8b34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GPT-5-Model-Corrected-GPT-4V’s-Chart-Reading-Errors-Not-Prompting"><a href="#GPT-5-Model-Corrected-GPT-4V’s-Chart-Reading-Errors-Not-Prompting" class="headerlink" title="GPT-5 Model Corrected GPT-4V’s Chart Reading Errors, Not Prompting"></a>GPT-5 Model Corrected GPT-4V’s Chart Reading Errors, Not Prompting</h2><p><strong>Authors:Kaichun Yang, Jian Chen</strong></p>
<p>We present a quantitative evaluation to understand the effect of zero-shot large-language model (LLMs) and prompting uses on chart reading tasks. We asked LLMs to answer 107 visualization questions to compare inference accuracies between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances, where GPT-4V failed to produce correct answers. Our results show that model architecture dominates the inference accuracy: GPT5 largely improved accuracy, while prompt variants yielded only small effects. Pre-registration of this work is available here: <a target="_blank" rel="noopener" href="https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3">https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3</a>; the Google Drive materials are here:<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view">https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view</a>. </p>
<blockquote>
<p>我们对零样本大型语言模型（LLM）在图表阅读任务中的影响进行了定量评估，并探讨了提示的使用效果。我们要求LLM回答107个可视化问题，以比较GPT-5和GPT-4V在多模态困难图像实例上的推理准确性，其中GPT-4V未能给出正确答案。我们的结果表明，模型架构对推理精度起主导作用：GPT-5大大提高了精度，而提示变体只产生了微小影响。本工作的预注册信息可在此处查看：<a target="_blank" rel="noopener" href="https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3">链接</a>；谷歌驱动材料可在此处查看：<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view">链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要评估了零样本大型语言模型（LLMs）在图表阅读任务中的应用效果，对比了GPT-5和GPT-4V在解决可视化问题上的推理准确性。研究发现，模型架构对推理准确性起主导作用，GPT-5显著提高了准确性，而提示变体只产生了较小的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文对零样本大型语言模型（LLMs）在图表阅读任务中的效果进行了定量评估。</li>
<li>对比了GPT-5和GPT-4V在解决可视化问题上的推理准确性。</li>
<li>模型架构对推理准确性有主导作用，GPT-5表现更优秀。</li>
<li>提示变体对推理准确性的影响较小。</li>
<li>提供了研究的前注册链接和Google Drive材料供读者参考。</li>
<li>研究结果表明，在解决困难图像实例时，选择合适的模型架构是关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bc18414005d2eed9c057dcb285a52a8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099175&auth_key=1760099175-0-0-c979514984b77d73dc7f9cc3cc5e5f3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27559b03e3bc20b4de39b5916f325146~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099183&auth_key=1760099183-0-0-c73dab33fc5b34689f0fd5abde2c0033&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03222eb75a296162d926d6e9c1648e78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099190&auth_key=1760099190-0-0-266545a4d0531291b1bf1d66f220225b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1359bf837cdd653cb1fbc640aec07eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099196&auth_key=1760099196-0-0-3565e2b99c8368866d62d09044812638&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-caa6ea8c1350e7bb1eae0e73a7ad9598~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099203&auth_key=1760099203-0-0-e6668f4674c587b26dca77b5ab0235fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeFormer-Transformer-with-Attention-Modulation-Empowered-by-Temporal-Characteristics-for-Time-Series-Forecasting"><a href="#TimeFormer-Transformer-with-Attention-Modulation-Empowered-by-Temporal-Characteristics-for-Time-Series-Forecasting" class="headerlink" title="TimeFormer: Transformer with Attention Modulation Empowered by Temporal   Characteristics for Time Series Forecasting"></a>TimeFormer: Transformer with Attention Modulation Empowered by Temporal   Characteristics for Time Series Forecasting</h2><p><strong>Authors:Zhipeng Liu, Peibo Duan, Xuan Tang, Baixin Li, Yongsheng Huang, Mingyang Geng, Changsheng Zhang, Bin Zhang, Binwu Wang</strong></p>
<p>Although Transformers excel in natural language processing, their extension to time series forecasting remains challenging due to insufficient consideration of the differences between textual and temporal modalities. In this paper, we develop a novel Transformer architecture designed for time series data, aiming to maximize its representational capacity. We identify two key but often overlooked characteristics of time series: (1) unidirectional influence from the past to the future, and (2) the phenomenon of decaying influence over time. These characteristics are introduced to enhance the attention mechanism of Transformers. We propose TimeFormer, whose core innovation is a self-attention mechanism with two modulation terms (MoSA), designed to capture these temporal priors of time series under the constraints of the Hawkes process and causal masking. Additionally, TimeFormer introduces a framework based on multi-scale and subsequence analysis to capture semantic dependencies at different temporal scales, enriching the temporal dependencies. Extensive experiments conducted on multiple real-world datasets show that TimeFormer significantly outperforms state-of-the-art methods, achieving up to a 7.45% reduction in MSE compared to the best baseline and setting new benchmarks on 94.04% of evaluation metrics. Moreover, we demonstrate that the MoSA mechanism can be broadly applied to enhance the performance of other Transformer-based models. </p>
<blockquote>
<p>尽管Transformer在自然语言处理方面表现出色，但由于未能充分考虑到文本和时间模态之间的差异，将其应用于时间序列预测仍然具有挑战性。在本文中，我们开发了一种针对时间序列数据的新型Transformer架构，旨在最大限度地提高其表示能力。我们确定了时间序列的两个关键但常被忽视的特征：（1）过去对未来单向影响；（2）随时间流逝影响衰减的现象。这些特性被引入到增强Transformer的注意力机制中。我们提出了TimeFormer，其核心创新点是一种带有两个调制项（MoSA）的自注意力机制，旨在在满足霍克斯过程和因果掩码约束的条件下，捕捉这些时间序列的时间先验信息。此外，TimeFormer还引入了一个基于多尺度和子序列分析的框架，以捕获不同时间尺度上的语义依赖关系，丰富时间依赖性。在多个真实世界数据集上进行的广泛实验表明，TimeFormer显著优于最新方法，在均方误差（MSE）方面相比最佳基线降低了高达7.45%，在评价指标上达到了94.04%的新基准。此外，我们证明了MoSA机制可以广泛应用于提高其他基于Transformer的模型性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06680v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>虽然Transformer在自然语言处理方面表现出色，但在时间序列预测方面的应用仍然具有挑战性，因为未能充分考虑文本和时间模态之间的差异。本文提出了一种针对时间序列数据的新型Transformer架构，旨在提高其表示能力。文章强调了时间序列的两个关键但常被忽视的特性：过去对未来单向影响以及影响随时间衰减的现象。为此，文章提出了TimeFormer，其核心创新在于设计了一种带有两个调制项的自注意力机制（MoSA），能够在霍克斯过程和因果掩码的约束下捕捉这些时间先验知识。此外，TimeFormer还引入了一个基于多尺度和子序列分析的框架，以在不同的时间尺度上捕捉语义依赖关系，丰富时间依赖性。在多个真实数据集上进行的广泛实验表明，TimeFormer显著优于最新方法，在均方误差上最多减少7.45%，并在94.04%的评估指标上达到新的基准水平。此外，我们还证明了MoSA机制可以广泛应用于提高其他基于Transformer的模型性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>虽然Transformer在自然语言处理方面表现出色，但在时间序列预测的应用上存在挑战。</li>
<li>文章提出了针对时间序列数据的新型Transformer架构——TimeFormer。</li>
<li>TimeFormer考虑了时间序列的两个关键特性：过去对未来的单向影响以及影响随时间衰减的现象。</li>
<li>TimeFormer引入了带有两个调制项的自注意力机制（MoSA），以捕捉时间先验知识。</li>
<li>TimeFormer在多个真实数据集上表现出显著优势，相比最佳基线方法降低了MSE。</li>
<li>MoSA机制可广泛应用于增强其他基于Transformer的模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06680">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2306c7363eb2084a41e0aa5d79edf664~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099210&auth_key=1760099210-0-0-9c91a1ccfa7ed3a6c35fea60eb169aed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-538643174db22a58d97784abe22eda6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102466&auth_key=1760102466-0-0-94f43dc656404954d2d6f740bf43435c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b2c9837752d35767aba318f445f6fd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099281&auth_key=1760099281-0-0-fa301e7e1e9af23c7cb80026b67e5d5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reproducibility-Study-of-“XRec-Large-Language-Models-for-Explainable-Recommendation”"><a href="#Reproducibility-Study-of-“XRec-Large-Language-Models-for-Explainable-Recommendation”" class="headerlink" title="Reproducibility Study of “XRec: Large Language Models for Explainable   Recommendation”"></a>Reproducibility Study of “XRec: Large Language Models for Explainable   Recommendation”</h2><p><strong>Authors:Ranjan Mishra, Julian I. Bibo, Quinten van Engelen, Henk Schaapman</strong></p>
<p>In this study, we reproduced the work done in the paper “XRec: Large Language Models for Explainable Recommendation” by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRec’s Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at <a target="_blank" rel="noopener" href="https://github.com/julianbibo/xrec-reproducibility">https://github.com/julianbibo/xrec-reproducibility</a>. </p>
<blockquote>
<p>在这项研究中，我们重新实现了Ma等人于2024年发表的论文“XRec：用于可解释推荐的大型语言模型”中的工作。原始作者介绍了XRec，这是一个模型无关的协同指令微调框架，它使大型语言模型（LLM）能够为用户生成推荐提供全面的解释。我们的目标是复制原始论文的结果，但使用Llama 3作为评估的LLM，而不是GPT-3.5-turbo。我们基于Ma等人提供的源代码（2024年）来实现我们的目标。我们的工作通过修改XRec的Mixture of Experts模块的输入嵌入或删除其输出嵌入来扩展原始论文。根据我们的结果，XRec有效地生成了个性化解释，通过引入协同信息提高了其稳定性。然而，XRec并未在所有指标上始终优于所有基线模型。我们的进一步分析进一步强调了Mixture of Experts嵌入在塑造解释结构中的重要性，展示了协同信号如何与语言建模进行交互。通过我们的工作，我们提供了一个开源评估实现，增强了研究者和实践者的可访问性。我们的完整代码仓库可在<a target="_blank" rel="noopener" href="https://github.com/julianbibo/xrec-reproducibility%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/julianbibo/xrec-reproducibility找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究中，成功复制了Ma等人（于报告截至时间次年）《基于大型语言模型的解释性推荐研究》（简称“XRec”）的成果，旨在验证该推荐模型的泛化性，并将开源工具扩展以适应解释的交互模式。以基于语言模型的协同调整指令对原代码进行改进，并发现XRec在生成个性化解释方面表现良好，但并非在所有指标上均优于基线模型。研究亮点在于揭示了专家混合嵌入在解释结构形成中的关键作用，且有助于展示协同信号与语言模型的相互作用机制。更多详细研究资料可访问相关代码库（链接：<a target="_blank" rel="noopener" href="https://github.com/julianbibo/xrec-reproducibility%EF%BC%89%E3%80%82">https://github.com/julianbibo/xrec-reproducibility）。</a></p>
<p><strong>Key Takeaways</strong></p>
<p>以下是根据提供的文本提炼出的关键见解要点：</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a7f0d32406138763b110c379c4e8708d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099288&auth_key=1760099288-0-0-6a2fc9f0f767b4e73fe44cd141049dbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4e2439cc819cc7708c6f93f065fd533~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099296&auth_key=1760099296-0-0-241ccd7ce01934afb674ce7d0ee4da2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Knowledge-Graph-Guided-Multi-Agent-Distillation-for-Reliable-Industrial-Question-Answering-with-Datasets"><a href="#Knowledge-Graph-Guided-Multi-Agent-Distillation-for-Reliable-Industrial-Question-Answering-with-Datasets" class="headerlink" title="Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial   Question Answering with Datasets"></a>Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial   Question Answering with Datasets</h2><p><strong>Authors:Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang</strong></p>
<p>Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/erwinmsmith/KG-MAD/">https://github.com/erwinmsmith/KG-MAD/</a>. </p>
<blockquote>
<p>产业问答系统（QA）对安全性和可靠性的要求高于通用对话模型，因为在设备故障诊断等高风险场景中出错会产生严重后果。虽然多智能体大型语言模型增强了推理深度，但它们存在不可控迭代和不可验证输出的问题，传统蒸馏方法难以将协作推理能力转移到轻量级、可部署的学生模型。为了应对这些挑战，我们提出了知识图谱引导的多智能体系统蒸馏（KG-MASD）。我们的方法将蒸馏公式化为马尔可夫决策过程，并融合知识图谱作为可验证的结构化先验来丰富状态表示并确保收敛。通过结合协作推理和知识接地，KG-MASD生成高置信度指令调整数据，并将推理深度和可验证性联合蒸馏成适用于边缘部署的紧凑学生模型。在工业QA数据集上的实验表明，与基线相比，KG-MASD的准确率提高了2.4%至20.1%，并显著提高了可靠性，可在安全关键工业场景中实现可信AI部署。代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/erwinmsmith/KG-MAD/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/erwinmsmith/KG-MAD/获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06240v1">PDF</a> 41 pages, 12 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>工业问答系统相较于通用对话模型需要更高的安全性和可靠性，因为高风险的场景（如设备故障诊断）中的错误可能导致严重后果。为解决多代理大型语言模型带来的推理深度增强但迭代不可控、输出不可验证的问题，以及传统蒸馏方法难以将协作推理能力转移到轻量级、可部署的学生模型的问题，提出了知识图谱引导的多代理系统蒸馏（KG-MASD）。该方法将蒸馏过程制定为马尔可夫决策过程，并引入知识图谱作为可验证的结构化先验知识来丰富状态表示并保障收敛性。通过将协作推理与知识定位相结合，KG-MASD生成了高置信度的指令调整数据，并将推理深度和可验证性联合蒸馏成适合边缘部署的紧凑学生模型。在工业问答数据集上的实验表明，KG-MASD相较于基线方法提高了2.4%至20.1%的准确率，并显著增强了可靠性，实现了安全关键工业场景中可信人工智能的部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工业问答系统需要高安全性和可靠性，因为高风险的错误可能导致严重后果。</li>
<li>多代理大型语言模型能提高推理深度但存在迭代不可控和输出不可验证的问题。</li>
<li>传统蒸馏方法难以将协作推理能力转移到轻量级、可部署的学生模型。</li>
<li>KG-MASD方法结合知识图谱和马尔可夫决策过程来解决上述问题。</li>
<li>知识图谱作为结构化先验知识，丰富状态表示并保障收敛性。</li>
<li>KG-MASD通过结合协作推理和知识定位，生成高置信度的指令调整数据。</li>
<li>KG-MASD提高了在工业问答数据集上的准确率，并显著增强了模型的可靠性，适用于安全关键的工业场景部署。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06240">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0781b0bb3bdfa667d300507ae681ffad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099303&auth_key=1760099303-0-0-17c0a9cc81423637178c827b85af0ebb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9905c54f5e6c89da8cbe2c5552924a02~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099311&auth_key=1760099311-0-0-69513e1a523dec45912fa7f4c68de331&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AWARE-Beyond-Sentence-Boundaries-A-Contextual-Transformer-Framework-for-Identifying-Cultural-Capital-in-STEM-Narratives"><a href="#AWARE-Beyond-Sentence-Boundaries-A-Contextual-Transformer-Framework-for-Identifying-Cultural-Capital-in-STEM-Narratives" class="headerlink" title="AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework   for Identifying Cultural Capital in STEM Narratives"></a>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework   for Identifying Cultural Capital in STEM Narratives</h2><p><strong>Authors:Khalid Mehtab Khan, Anagha Kulkarni</strong></p>
<p>Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model’s awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model’s vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative. </p>
<blockquote>
<p>在学生的反思中识别文化资本（CC）主题，可以提供有价值的见解，有助于培养课堂中的公平学习环境。然而，诸如志向目标或家庭支持等主题通常被编织成叙事，而非以关键词的形式直接出现，这使得它们难以被标准的NLP模型检测，这些模型孤立地处理句子。核心挑战源于缺乏意识，因为标准模型是在一般语料库上进行预训练的，这使得它们对数据的特定领域语言和叙事上下文一无所知。为了解决这一问题，我们引入了AWARE框架，该框架系统地尝试提高转换器模型对此类细微任务的意识。AWARE有三个核心组件：1）领域意识，使模型的词汇适应学生反思的语言风格；2）语境意识，生成句子嵌入，意识到整篇文章的语境；3）类别重叠意识，采用多标签策略来识别单个句子中共存的主题。我们的结果表明，通过使模型明确意识到输入的特性，AWARE在Macro-F1上超出了强基线2.1个百分点，并在所有主题上都显示出可观的改进。这项工作为任何依赖于叙事上下文的文本分类任务提供了稳健且可推广的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04983v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了在学生的反思中识别文化资本主题的重要性，这有助于促进课堂中的公平学习环境。文章指出，主题如志向目标或家庭支持通常融入叙事之中，而非以关键词的形式出现，这使得标准NLP模型难以孤立地处理句子来检测这些主题。为解决此问题，本文提出了AWARE框架，通过三个核心组件提高transformer模型对此类任务的敏锐度：领域敏锐度、语境敏锐度和类别重叠敏锐度。结果显示，通过使模型明确意识到输入的特性，AWARE在宏观F1分数上超越了强基线2.1个百分点，并在所有主题上都取得了显著的改进。这为任何依赖于叙事语境的文本分类任务提供了稳健且可推广的方法论。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>识别学生反思中的文化资本主题有助于促进公平的学习环境。</li>
<li>主题常融入叙事中，而非以关键词形式出现，这使得标准NLP模型难以检测。</li>
<li>AWARE框架包括三个核心组件：领域敏锐度、语境敏锐度和类别重叠敏锐度。</li>
<li>AWARE框架通过提高模型的敏锐度，在宏观F1分数上超越了强基线。</li>
<li>AWARE框架在所有主题上都取得了显著的改进。</li>
<li>此方法适用于任何依赖于叙事语境的文本分类任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04983">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-763c3416f387290d115a32cd32a0cfc7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099318&auth_key=1760099318-0-0-be616adb31c428ad1b00a21ef6e29d6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7858f8afd2033d970909f41d3f0d2ca1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099325&auth_key=1760099325-0-0-2f1aa6fea3d81814f0161530db801d60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-302d3f041026412a9fcb80960ce42fec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099332&auth_key=1760099332-0-0-d89dc480f3dd60f587997d48a2972d41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d4e126271fb53402545a3603211f988~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099338&auth_key=1760099338-0-0-1c3605f1c4fe91e86f09c7c88ea35b92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-659bb768de122e4210eb2d43acbcf2c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099345&auth_key=1760099345-0-0-be994f9a9932cc981eee600e11912ead&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9d075576ac5224fac7e934e32b7299a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099351&auth_key=1760099351-0-0-c0b4bff513490a08b748370c19d77637&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Language-Model-Based-Text-to-Audio-Generation-Anti-Causally-Aligned-Collaborative-Residual-Transformers"><a href="#Language-Model-Based-Text-to-Audio-Generation-Anti-Causally-Aligned-Collaborative-Residual-Transformers" class="headerlink" title="Language Model Based Text-to-Audio Generation: Anti-Causally Aligned   Collaborative Residual Transformers"></a>Language Model Based Text-to-Audio Generation: Anti-Causally Aligned   Collaborative Residual Transformers</h2><p><strong>Authors:Juncheng Wang, Chao Xu, Cheng Yu, Zhe Hu, Haoyu Xie, Guoqi Yu, Lei Shang, Shujun Wang</strong></p>
<p>While language models (LMs) paired with residual vector quantization (RVQ) tokenizers have shown promise in text-to-audio (T2A) generation, they still lag behind diffusion-based models by a non-trivial margin. We identify a critical dilemma underpinning this gap: incorporating more RVQ layers improves audio reconstruction fidelity but exceeds the generation capacity of conventional LMs. To address this, we first analyze RVQ dynamics and uncover two key limitations: 1) orthogonality of features across RVQ layers hinders effective LMs training, and 2) descending semantic richness in tokens from deeper RVQ layers exacerbates exposure bias during autoregressive decoding. Based on these insights, we propose Siren, a novel LM-based framework that employs multiple isolated transformers with causal conditioning and anti-causal alignment via reinforcement learning. Extensive experiments demonstrate that Siren outperforms both existing LM-based and diffusion-based T2A systems, achieving state-of-the-art results. By bridging the representational strengths of LMs with the fidelity demands of audio synthesis, our approach repositions LMs as competitive contenders against diffusion models in T2A tasks. Moreover, by aligning audio representations with linguistic structures, Siren facilitates a promising pathway toward unified multi-modal generation frameworks. </p>
<blockquote>
<p>虽然语言模型（LMs）与残差向量量化（RVQ）分词器的结合在文本到音频（T2A）生成中显示出潜力，但它们仍然落后于基于扩散的模型相当大的距离。我们发现了造成这一差距的关键困境：增加RVQ层数可以提高音频重建的保真度，但超出了传统语言模型的生成能力。为了解决这个问题，我们首先分析了RVQ的动态特性，并发现了两个关键限制：1）RVQ层之间的特征正交性阻碍了有效的语言模型训练；2）来自更深RVQ层的令牌语义丰富性的下降加剧了自回归解码过程中的暴露偏差。基于这些见解，我们提出了Siren，这是一种基于语言模型的新型框架，它采用多个独立的变压器，通过强化学习实现因果条件和非因果对齐。大量实验表明，Siren在基于语言模型和基于扩散的T2A系统中表现出色，取得了最新结果。通过弥合了语言模型的表征强度与音频合成的保真度需求，我们的方法使语言模型在T2A任务中成为与扩散模型竞争的竞争者。此外，通过将音频表示与语言结构对齐，Siren为统一的多模式生成框架提供了有希望的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04577v1">PDF</a> Accepted to EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>基于语言模型（LMs）与残差向量量化（RVQ）标记器在文本转音频（T2A）生成中的应用，本研究提出了一个新的框架——Siren。分析表明，引入更多RVQ层可以提高音频重建的保真度，但超出了传统语言模型的生成能力。针对这一问题，本研究分析了RVQ的动力学，并揭示了两个关键局限性。为此，本研究提出采用多个独立的Transformer架构与因果条件、逆向学习相结合的策略来解决这些问题。实验结果证实，Siren不仅在基于语言模型的系统中表现出色，而且超越了许多基于扩散的系统，成为最新的技术成果。该方法不仅提升了语言模型的表达能力，满足了音频合成的保真度要求，还重新确立了语言模型在文本转音频任务中的竞争力地位。此外，通过音频表示与语言结构的对齐，Siren为统一的多模态生成框架提供了可行的路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型与残差向量量化在文本转音频生成领域有应用前景，但仍落后于扩散模型。</li>
<li>增加RVQ层可提高音频重建的保真度，但超出语言模型的生成能力。</li>
<li>RVQ动力学存在两个关键局限性：特征正交性影响语言模型训练，深层RVQ层中的语义丰富度下降加剧了自回归解码过程中的暴露偏差。</li>
<li>提出Siren框架，采用多个独立Transformer架构结合因果条件和逆向学习来解决上述问题。</li>
<li>Siren在文本转音频任务中表现优异，超越了现有的语言模型和扩散模型。</li>
<li>Siren重新确立了语言模型在文本转音频任务中的竞争力地位。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e821e1bc76dbd4873466fb0d871ecbbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099359&auth_key=1760099359-0-0-03f127749ef696e9558d4cf1c4c18309&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98bc30e4f5aefda1eabe571454685dcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099366&auth_key=1760099366-0-0-75cf9f65fee7241f518d20ce601d2198&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d10667c1cc9d0b64ad18ecb67dd3db9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099373&auth_key=1760099373-0-0-dde77eff8c36a93bdd4600c445031f65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5693fdeef6e61281cef3b723b356fe15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099380&auth_key=1760099380-0-0-f8bfc82cab146f33f3f565f83771d27d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling"><a href="#CALM-Before-the-STORM-Unlocking-Native-Reasoning-for-Optimization-Modeling" class="headerlink" title="CALM Before the STORM: Unlocking Native Reasoning for Optimization   Modeling"></a>CALM Before the STORM: Unlocking Native Reasoning for Optimization   Modeling</h2><p><strong>Authors:Zhengyang Tang, Zihan Ye, Chenyu Huang, Xuhan Huang, Chengpeng Li, Sihang Li, Guanhua Chen, Ming Yan, Zizhuo Wang, Hongyuan Zha, Dayiheng Liu, Benyou Wang</strong></p>
<p>Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs – In particular, we show that direct fine-tuning on traditional \textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs’ inherent reasoning abilities, we propose \textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks. </p>
<blockquote>
<p>大型推理模型（LRMs）在复杂多步推理中展示了强大的能力，为自动化优化建模带来了新的机遇。然而，现有的域适应方法，最初是为早期的指令调优模型设计的，往往无法利用现代LRMs的先进推理模式。特别是，我们表明直接在传统的\textit{非反思}数据集上进行微调会导致有限的收益。为了充分利用LRMs的固有推理能力，我们提出了\textbf{CALM（修正适应与轻量级修改）}，这是一个框架，它逐步在优化建模任务中细化LRMs的固有推理模式。在CALM中，专家干预者识别推理缺陷并提供简洁的纠正提示，LRM会将其纳入以产生改进的推理轨迹。这些干预只修改了不到2.6%的生成令牌，但通过监督微调实现了高质量数据的软适应。适应后的模型再通过强化学习进一步改进。基于CALM，我们开发了\textbf{STORM（智能思考优化推理模型）}，这是一个拥有4B参数的LRM，在五个流行的优化建模基准测试上达到了平均准确率68.9%的新水平，与671B LRM的性能相匹配。这些结果表明，动态、基于提示的数据合成既保留了现代LRMs的固有推理模式又放大了其优势，为在具有挑战性的优化建模任务上实现专家级性能提供了更有效、更可扩展的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04204v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）在复杂多步推理方面展现出强大的能力，为自动化优化建模带来了新的机遇。然而，传统的领域自适应方法未能充分利用现代LRMs的先进推理模式。为了充分发挥LRMs的固有推理能力，提出了CALM（Corrective Adaptation with Lightweight Modification）框架，通过渐进式地在原生推理模式下优化建模任务来完善LRMs。专家干预者识别推理缺陷并提供简洁的纠正提示，LRMs结合这些提示改进推理轨迹。这些干预仅修改不到2.6%的生成令牌，但通过监督微调实现了高质量数据的软适应。在此基础上，开发了STORM（Smart Thinking Optimization Reasoning Model），在五个流行的优化建模基准测试中实现了平均准确率68.9%，与671B的LRM性能相匹配。这表明基于动态提示的数据合成既保留了又放大了现代LRMs的固有推理模式，为在具有挑战性的优化建模任务上实现专家级性能提供了更有效、更可扩展的途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMs在复杂多步推理方面表现出强大的能力，为自动化优化建模带来了新的机遇。</li>
<li>传统领域自适应方法未能充分利用现代LRMs的先进推理模式。</li>
<li>CALM框架通过渐进式完善LRMs的推理能力，结合专家干预者的纠正提示改进推理轨迹。</li>
<li>CALM框架中的干预仅修改少量生成令牌，实现高质量数据的软适应。</li>
<li>基于CALM框架开发的STORM模型在多个优化建模基准测试中达到最新状态平均准确率68.9%。</li>
<li>动态、基于提示的数据合成方法既保留了又放大了现代LRMs的固有推理模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04204">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e19556561ea77dc86afa2f9f96b583a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099387&auth_key=1760099387-0-0-441a56d89227763e2f74a45faadb23b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea0015ba8f4f0741f48f653d8a232a34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099394&auth_key=1760099394-0-0-74338d9ab43b622f2cf7fc2d5792a98b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46c1e89315262497649602883fb73195~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102473&auth_key=1760102473-0-0-0ebabf2db78864a2fefc237305965673&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-faedeaff81fb108777b18cc84b35c6da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102480&auth_key=1760102480-0-0-17da6d88c359d755353cc03bb4e7c1d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50ceb5b6335f247d27bc0b883ad7c0e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102487&auth_key=1760102487-0-0-747514130c078864c7ba5634d71144da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Rare-Text-Semantics-Were-Always-There-in-Your-Diffusion-Transformer"><a href="#Rare-Text-Semantics-Were-Always-There-in-Your-Diffusion-Transformer" class="headerlink" title="Rare Text Semantics Were Always There in Your Diffusion Transformer"></a>Rare Text Semantics Were Always There in Your Diffusion Transformer</h2><p><strong>Authors:Seil Kang, Woojung Han, Dayun Ju, Seong Jae Hwang</strong></p>
<p>Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiT’s outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface. </p>
<blockquote>
<p>从基于流和扩散的转换器开始，多模态扩散转换器（MM-DiTs）已经改变了文本到视觉生成的方式，因其出色的视觉逼真度而受到赞誉。随着这些模型的进步，用户不断用富有想象力或罕见的提示来挑战界限，但在使用这些先进模型生成内容时仍然会出现短板，因为那些罕见的提示所表达的概念在预训练阶段留下的印记通常不够深刻。在本文中，我们提出了一种简单而有效的干预措施，可以在MM-DiTs内部提取罕见的语义内容，无需额外的训练步骤、数据、去噪时间优化或依赖外部模块（如大型语言模型）。特别是MM-DiT内在的联合注意机制在变压器块内顺序地更新文本嵌入和图像嵌入。我们发现通过联合注意块之前在文本令牌嵌入周围数学扩展表示性盆地（通过方差规模扩大），罕见的语义内容会清晰地出现在MM-DiT的输出中。此外，我们的结果在文本到视觉任务中表现良好，包括文本到图像、文本到视频和文本驱动的图像编辑。我们的工作邀请生成模型揭示用户意图的语义内容，这些语义内容虽然曾经隐藏但已准备好浮现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03886v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong><br>     多模态扩散转换器（MM-DiT）基于流和扩散技术，重塑了文本到视觉的生成，以其出色的视觉逼真度而受到赞誉。然而，对于先进模型而言，用户提供的富有想象力的或罕见的提示仍然会在生成时遇到困难，因为这些罕见概念在预训练阶段留下的印记通常较为微弱。本文提出了一种简单而有效的干预措施，无需额外的训练步骤、数据、去噪时间优化或依赖外部模块（如大型语言模型），即可在MM-DiT内部呈现罕见的语义。通过数学上扩大文本令牌嵌入周围的表示盆地，即通过在联合注意力块之前扩大方差比例，我们发现罕见的语义在MM-DiT的输出中清晰出现。此外，我们的结果有效地跨文本到视觉任务进行推广，包括文本到图像、文本到视频和文本驱动的图像编辑。本研究邀请生成模型揭示用户意图中隐藏的语义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态扩散转换器（MM-DiT）已重塑文本到视觉生成领域，展现高视觉逼真度。</li>
<li>用户提供的罕见或富有想象力的提示对先进模型仍具挑战，因为这些模型在预训练阶段难以捕捉这些罕见概念。</li>
<li>本文提出了一种简单有效的干预措施，无需额外训练、数据或依赖外部模块，即可在MM-DiT中呈现罕见语义。</li>
<li>通过数学上扩大文本嵌入的表示范围（即方差比例扩大），MM-DiT能够清晰输出罕见语义。</li>
<li>该方法有效跨多种文本到视觉任务推广，包括文本到图像、文本到视频以及文本驱动的图像编辑。</li>
<li>该研究有助于生成模型更好地理解和揭示用户意图中的隐藏语义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6d455ea1d58e8f4b99df1b95970229de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102494&auth_key=1760102494-0-0-c78821dd2be16ee950621feb0cddf3f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d30dc1b2c174d4bb19a9f51df938ba9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102501&auth_key=1760102501-0-0-1ddc9e10ad8825bd366c0fa2853ba27a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e09e3037928d8661bbfefd2af6cb4b98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102508&auth_key=1760102508-0-0-bbadda14674dacc5ff8bd7af1d6de628&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a4fbfa9159baf938a5d8be010be45d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102515&auth_key=1760102515-0-0-3d7b60337908a498ca85a404f87dca1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8aee1867d3ee54755098b412d1379af0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102522&auth_key=1760102522-0-0-6004022fd1eaa0d2c9618ab995f60351&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DHQA-4D-Perceptual-Quality-Assessment-of-Dynamic-4D-Digital-Human"><a href="#DHQA-4D-Perceptual-Quality-Assessment-of-Dynamic-4D-Digital-Human" class="headerlink" title="DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human"></a>DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human</h2><p><strong>Authors:Yunhao Li, Sijing Wu, Yucheng Zhu, Huiyu Duan, Zicheng Zhang, Guangtao Zhai</strong></p>
<p>With the rapid development of 3D scanning and reconstruction technologies, dynamic digital human avatars based on 4D meshes have become increasingly popular. A high-precision dynamic digital human avatar can be applied to various fields such as game production, animation generation, and remote immersive communication. However, these 4D human avatar meshes are prone to being degraded by various types of noise during the processes of collection, compression, and transmission, thereby affecting the viewing experience of users. In light of this fact, quality assessment of dynamic 4D digital humans becomes increasingly important. In this paper, we first propose a large-scale dynamic digital human quality assessment dataset, DHQA-4D, which contains 32 high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D human meshes degraded by 11 textured distortions, as well as their corresponding textured and non-textured mean opinion scores (MOSs). Equipped with DHQA-4D dataset, we analyze the influence of different types of distortion on human perception for textured dynamic 4D meshes and non-textured dynamic 4D meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model (LMM) based approach that is able to assess both textured 4D meshes and non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts multi-dimensional features, including visual features from a projected 2D video, motion features from cropped video clips, and geometry features from the 4D human mesh to provide comprehensive quality-related information. Then we utilize a LMM model to integrate the multi-dimensional features and conduct a LoRA-based instruction tuning technique to teach the LMM model to predict the quality scores. Extensive experimental results on the DHQA-4D dataset demonstrate the superiority of our DynaMesh-Rater method over previous quality assessment methods. </p>
<blockquote>
<p>随着3D扫描和重建技术的快速发展，基于4D网格的动态数字人类化身变得越来越流行。高精度动态数字人类化身可应用于游戏制作、动画生成和远程沉浸式通信等各种领域。然而，这些4D人类化身网格在收集、压缩和传输过程中容易受到各种噪声的干扰，从而影响用户的使用体验。鉴于此，对动态4D数字人类的质量评估变得日益重要。在本文中，我们首先提出了大规模动态数字人类质量评估数据集DHQA-4D，其中包含32个高质量真实扫描的4D人类网格序列、1920个由11种纹理失真降级的失真纹理4D人类网格，以及它们相应的纹理和非纹理平均意见得分（MOSs）。配备DHQA-4D数据集，我们分析了不同类型失真对纹理动态4D网格和非纹理动态4D网格的人类感知的影响。此外，我们提出了DynaMesh-Rater，这是一种基于大型多模态模型（LMM）的新方法，能够评估纹理4D网格和非纹理4D网格。具体来说，DynaMesh-Rater精心提取了多维特征，包括从投影的2D视频中提取的视觉特征、从裁剪的视频片段中提取的运动特征以及从4D人类网格中提取的几何特征，以提供全面的质量相关信息。然后，我们利用LMM模型来整合这些多维特征，并采用基于LoRA的指令微调技术来教导LMM模型预测质量分数。在DHQA-4D数据集上的广泛实验结果证明了我们的DynaMesh-Rater方法相较于之前的质量评估方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03874v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着3D扫描和重建技术的快速发展，基于4D网格的动态数字人类化身越来越受欢迎。可应用于游戏制作、动画制作和远程沉浸式通信等领域。然而，这些4D人类化身网格在收集、压缩和传输过程中容易受到各种噪声的影响，从而影响用户体验。因此，对动态4D数字人的质量评估变得尤为重要。本文首先提出大规模动态数字人类质量评估数据集DHQA-4D，包含32个高质量真实扫描的4D人类网格序列、1920个由11种纹理失真产生的失真纹理4D人类网格及其相应的纹理和非纹理平均意见得分（MOSs）。配备DHQA-4D数据集，我们分析了不同类型失真对纹理动态4D网格和非纹理动态4D网格的人类感知的影响。此外，我们提出了基于动态网格评分器（DynaMesh-Rater）的新型大型多模态模型（LMM）方法，能够评估纹理和非纹理的4D网格。该方法通过提取多维特征，包括来自投影的二维视频的视觉特征、来自裁剪的视频剪辑的运动特征以及来自4D人类网格的几何特征，提供全面的质量相关信息。然后利用LMM模型整合多维特征，并采用LoRA技术进行指令微调，教导模型预测质量分数。在DHQA-4D数据集上的广泛实验结果表明，我们的DynaMesh-Rater方法优于先前的质量评估方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态数字人类化身使用基于4D网格的技术变得越来越流行，可应用于游戏制作、动画制作和远程沉浸式通信。</li>
<li>动态4D数字人的质量评估变得重要，因为这些网格在收集、压缩和传输过程中容易因噪声而质量下降。</li>
<li>DHQA-4D数据集包含各种高质量和失真纹理的4D人类网格序列及其对应的平均意见得分（MOSs），用于分析不同类型失真对感知的影响。</li>
<li>提出了一种新型的多模态模型方法——动态网格评分器（DynaMesh-Rater），能够评估纹理和非纹理的4D网格的质量。</li>
<li>DynaMesh-Rater通过提取多维特征来提供全面的质量相关信息，并利用大型多模态模型（LMM）进行预测。</li>
<li>DHQA-4D数据集上的实验结果表明，与其他质量评估方法相比，DynaMesh-Rater具有优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03874">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d9861ea4f44acf7b269d58615d0bcd26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102531&auth_key=1760102531-0-0-7d3cb5e77c665e690ad7c9ff1317e5cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4abf6cecfdaa7491fe820c0da4d26db1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102538&auth_key=1760102538-0-0-54944e08a45d713287801a73667a574e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cdffdee00aa479849ae0af9bddb7983~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102545&auth_key=1760102545-0-0-2773ee624d36f0b310452f708968facd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f98977ebcc1a2d5457db64ee164c5940~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102552&auth_key=1760102552-0-0-85aa6dc3f0c1804da1224ce814c5f779&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ef2a8c99be1df644063d71e290e0aa2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102560&auth_key=1760102560-0-0-a12edd471e56a040db3963460b434509&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-053e974ef63cab08476b30cb74d0c0ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102566&auth_key=1760102566-0-0-06476018d4928baec6ac6cbc1c3beb8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rainbow-Padding-Mitigating-Early-Termination-in-Instruction-Tuned-Diffusion-LLMs"><a href="#Rainbow-Padding-Mitigating-Early-Termination-in-Instruction-Tuned-Diffusion-LLMs" class="headerlink" title="Rainbow Padding: Mitigating Early Termination in Instruction-Tuned   Diffusion LLMs"></a>Rainbow Padding: Mitigating Early Termination in Instruction-Tuned   Diffusion LLMs</h2><p><strong>Authors:Bumjun Kim, Dongjae Jeon, Dueun Kim, Wonje Jeung, Albert No</strong></p>
<p>Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \texttt{<eos>} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \texttt{<eos>} as both termination and padding, which concentrates probability mass on \texttt{<eos>} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \texttt{<eos>} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/quasar529/rainbow-padding">https://github.com/quasar529/rainbow-padding</a>. </p>
<blockquote>
<p>扩散大型语言模型（dLLMs）作为自回归模型的有前途的替代方案而出现，它提供了灵活的生成顺序和在复杂推理任务上的强大性能。然而，经过指令调整的dLLMs表现出我们所谓的“<eos>溢出”的关键漏洞：随着分配序列长度的增加，响应却矛盾地变得更短，陷入提前终止或退化为一连串的<eos>标记。虽然在实践中已经注意到这个问题，但尚未对其进行系统分析。我们追溯其根源到<eos>的双重角色，既是终止符又是填充符，这将在后面的位置集中概率质量，并向后传播以触发提前终止。为了解决这一问题，我们引入了Rainbow Padding，这是一种简单的补救措施，用重复的填充令牌循环替换重复的<eos>占位符，分散概率质量并打破<eos>的主导地位。实验表明，Rainbow Padding极大地提高了长度稳健性和输出质量，只需七个填充令牌就足以防止提前终止。此外，该方法可以有效地集成到现有的指令调整模型中：只需对少量数据进行LoRA微调一个周期即可取得显著改进，这使得此解决方案非常实用。代码可在<a target="_blank" rel="noopener" href="https://github.com/quasar529/rainbow-padding%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/quasar529/rainbow-padding公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03680v1">PDF</a> 25 pages. Project page available   at~\url{<a target="_blank" rel="noopener" href="https://ai-isl.github.io/rainbow-padding%7D">https://ai-isl.github.io/rainbow-padding}</a></p>
<p><strong>Summary</strong></p>
<p>扩散大型语言模型（dLLMs）作为对自回归模型的有前途的替代方案出现，具有灵活的生成顺序和强大的复杂推理任务性能。然而，指令调优的dLLMs存在一个被称为“溢出”的关键漏洞：随着分配序列长度的增加，响应却变得较短，导致过早终止或退化为一串的“<eos>”标记。本文追踪了问题的根源，并提出了彩虹填充法（Rainbow Padding）作为解决方案，通过替换重复的“<eos>”占位符为不同的填充令牌，从而打破“<eos>”的主导地位。实验表明，Rainbow Padding显著提高了长度稳健性和输出质量，仅使用七个填充令牌就足以防止过早终止。此外，该方法可高效集成到现有指令调优模型中，通过LoRA微调少量数据即可完成显著改进，具有很高的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>dLLMs作为一种新兴技术，具有灵活的生成顺序和强大的复杂推理任务性能。</li>
<li>指令调优的dLLMs存在“溢出”问题，即在序列长度增加时响应反而变短。</li>
<li>“溢出”问题的根源在于“<eos>”标记的双重角色（终止和填充）。</li>
<li>Rainbow Padding方法通过替换重复的“<eos>”标记为不同的填充令牌来解决这个问题。</li>
<li>实验证明Rainbow Padding能显著提高长度稳健性和输出质量。</li>
<li>仅使用少量填充令牌（如七个）即可防止过早终止。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03680">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ff120074cae7aa14033574bb72d5295f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102573&auth_key=1760102573-0-0-77f35a1f51d49fe3e1293710872580ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78feead996d7929521ea3030ae00ad17~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102581&auth_key=1760102581-0-0-838363a5152c9de84763cf4875384e6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af67d3347678c7f1bc31ab2371828c9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102588&auth_key=1760102588-0-0-87b9da575b03cbf64b27a5f438ee7ede&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98a3ddad08ff6be1636c3838624e0684~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102595&auth_key=1760102595-0-0-5bed9a31457858b87aba60c386514d55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f014ba0c296f83cefc370a8a9994a2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102602&auth_key=1760102602-0-0-25cc1c1a752e4b41862e40296bd4783c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e3462769be733a94fd458c680fb79be0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102609&auth_key=1760102609-0-0-40f0d45292c79e3a244a627e8edd23b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers"><a href="#Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers" class="headerlink" title="Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers"></a>Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers</h2><p><strong>Authors:Omar Naim, Swarnadeep Bhar, Jérôme Bolte, Nicholas Asher</strong></p>
<p>While Large Language models’ abilities for in-context learning (ICL) have drawn much attention, we examine some of its limitations on semantic tasks involving quantifiers like “all” and “some”, as well as on tasks with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these limitations. We propose scaled signed averaging (SSA), a novel alternative to Softmax to mitigate these problems. We show that SSA significantly improves performance on our ICL tasks. In addition, SSA outperforms transformer models with Softmax on several early learning NLP benchmarks and linguistic probing tasks on zero and few-shot settings. </p>
<blockquote>
<p>虽然大型语言模型在上下文学习（ICL）的能力上得到了广泛关注，但我们对一些语义任务的局限性进行了探究，这些任务涉及量词“所有”、“一些”，以及涉及线性函数的任务。我们确定了注意力机制中的评分函数Softmax是导致这些局限性的因素之一。为此，我们提出了Softmax的替代方案——带符号缩放平均法（SSA），以缓解这些问题。我们证明SSA在ICL任务上的表现有了显著提高。此外，在零样本和少样本设置中，SSA在多个早期学习NLP基准测试和语言学探测任务上的表现优于使用Softmax的Transformer模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14685v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在上下文学习（ICL）方面的能力已备受关注，但其处理涉及量词（如“所有”和“一些”）的语义任务以及线性函数任务的局限性也日益凸显。研究发现，注意力机制中的评分函数Softmax是限制之一。为缓解这些问题，提出一种新型的评分函数——Scaled Signed Averaging（SSA）。在ICL任务上的表现显著优于Softmax，且在零样本和少样本设置的多项早期学习NLP基准测试和语言学探测任务中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在上下文学习（ICL）方面虽受关注，但在处理涉及量词的语义任务及线性函数任务时存在局限性。</li>
<li>Softmax作为注意力机制中的评分函数，是这些限制因素之一。</li>
<li>为改善模型在这些任务上的表现，提出了Scaled Signed Averaging（SSA）这一新型评分函数。</li>
<li>SSA在ICL任务上的表现显著优于Softmax。</li>
<li>SSA在零样本和少样本设置下，多项早期学习NLP基准测试和语言学探测任务中表现优异。</li>
<li>SSA的引入有助于提升语言模型在处理复杂任务时的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14685">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bd9e8c3495d42cf7895f0930feef9062~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102616&auth_key=1760102616-0-0-14f822ad1860ea0b9011c26a5d446aec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d752081c225c043003d6f0538341e611~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102623&auth_key=1760102623-0-0-70585d57732b08bf48a69d15fb951cbd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d957aebb0de3ec2be31891128f338b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102630&auth_key=1760102630-0-0-716151c520490c0e7c37e1896faf15c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="FLEx-Personalized-Federated-Learning-for-Mixture-of-Experts-LLMs-via-Expert-Grafting"><a href="#FLEx-Personalized-Federated-Learning-for-Mixture-of-Experts-LLMs-via-Expert-Grafting" class="headerlink" title="FLEx: Personalized Federated Learning for Mixture-of-Experts LLMs via   Expert Grafting"></a>FLEx: Personalized Federated Learning for Mixture-of-Experts LLMs via   Expert Grafting</h2><p><strong>Authors:Fan Liu, Bikang Pan, Zhongyi Wang, Xi Yao, Xiaoying Tang, Jingya Wang, Ye Shi</strong></p>
<p>Federated instruction tuning of large language models (LLMs) is challenged by significant data heterogeneity across clients, demanding robust personalization. The Mixture of Experts (MoE) architecture, where experts can specialize in distinct data patterns, presents a natural architectural solution to this challenge. The inherent sparsity of the MoE architecture, achieved by selectively activating experts, poses a significant challenge to its integration with federated learning (FL). Conventional FL frameworks, designed for dense models, naively aggregate all expert parameters irrespective of their local activation patterns. This naive approach not only undermines MoE’s dynamic sparsity but also risks corrupting the world knowledge within pretrained experts. To address this, we propose FLEx (Federated LLMs with Personalized Experts), a novel framework that leverages pretrained MoE-based LLMs for efficient personalization. By aggregating only the shared non-expert parameters, FLEx significantly reduces communication overhead and preserves the world knowledge stored within the frozen pretrained experts. For personalization, we introduce a novel expert grafting mechanism that leverages dynamic sparsity to construct a client-specific expert from selected components of pretrained experts, tailored to local data. This grafted expert is then fine-tuned locally alongside the gating mechanism. This joint training enables the model to learn when to leverage the shared knowledge from frozen experts and when to employ the personalized one. Evaluations on diverse, non-IID instruction tuning datasets show that FLEx consistently outperforms federated baselines on average, while demonstrating strong knowledge preservation on the knowledge-driven benchmark MMLU. Our code is available at \href{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/FLEx-8F12%7D%7B/texttt%7Bhttps://anonymous.4open.science/r/FLEx-8F12%7D%7D">https://anonymous.4open.science/r/FLEx-8F12}{\texttt{https://anonymous.4open.science/r/FLEx-8F12}}</a>. </p>
<blockquote>
<p>联合训练大型语言模型（LLM）在面临客户间数据的巨大异质性时面临挑战，这要求强大的个性化能力。专家混合（MoE）架构中的专家可以专注于不同的数据模式，为此类挑战提供了自然的架构解决方案。MoE架构的内在稀疏性，通过有选择地激活专家来实现，对其与联邦学习（FL）的集成提出了重大挑战。传统的FL框架，设计为适用于密集模型，会无差别地聚合所有专家参数，而不考虑其本地激活模式。这种简单的方法不仅破坏了MoE的动态稀疏性，而且还可能破坏预训练专家中的世界知识。</p>
</blockquote>
<p>为了解决这一问题，我们提出了FLEx（带有个性化专家的联邦LLM）这一新型框架，该框架利用基于MoE的预训练LLM进行有效个性化。FLEx只聚合共享的非专家参数，从而显著减少通信开销并保留冻结的预训练专家内的世界知识。对于个性化，我们引入了一种新的专家嫁接机制，该机制利用动态稀疏性从预训练专家中选择组件构建针对特定客户的专家，以适应本地数据。然后，将此嫁接的专家与门控机制一起进行本地微调。这种联合训练使模型能够学习何时利用来自冻结的专家的共享知识，以及何时使用个性化的知识。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00965v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在联邦学习中应用混合专家（MoE）架构的挑战与解决方案。针对大型语言模型（LLM）的联邦指令调优面临数据异构性的挑战，要求模型具备强大的个性化能力。MoE架构允许专家专注于不同的数据模式，为解决此挑战提供了天然架构解决方案。然而，MoE架构的内在稀疏性给其与联邦学习（FL）的融合带来了难题。传统FL框架无法有效处理MoE的稀疏性，且可能破坏预训练专家的全球知识。为此，本文提出了FLEx框架，利用预训练的MoE基LLM进行高效个性化。FLEx仅聚合共享的非专家参数，大大降低了通信开销并保留了预训练专家内的全球知识。为实现个性化，引入了专家嫁接机制，利用动态稀疏性构建针对本地数据的客户特定专家。嫁接的专家与门控机制一起进行本地微调。这种联合训练使模型能够在共享知识与个性化知识之间进行切换。评估表明，FLEx在多样化、非独立同分布（non-IID）指令调整数据集上的表现优于联邦基准测试，同时在知识驱动型MMLU基准测试上表现出强大的知识保留能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习中大型语言模型（LLM）面临数据异构性的挑战，需要强大的个性化能力。</li>
<li>混合专家（MoE）架构为解决此挑战提供了天然解决方案，但与其融合存在困难。</li>
<li>FLEx框架利用预训练的MoE基LLM进行高效个性化，仅聚合共享的非专家参数，降低通信开销并保留全球知识。</li>
<li>FLEx引入专家嫁接机制，利用动态稀疏性构建针对本地数据的客户特定专家。</li>
<li>嫁接的专家与门控机制联合训练，使模型能够在共享知识与个性化知识之间进行切换。</li>
<li>FLEx在多样化、非独立同分布（non-IID）指令调整数据集上的表现优于联邦基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e80bb9a9d7ca5cdb0293db555d02cfa0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102637&auth_key=1760102637-0-0-a1764c45ba922e4353381db0ba0cfce3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8fe1e746bfd81aa4380da9e0095517fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102644&auth_key=1760102644-0-0-2ab76207e9e1651b0492fe88ec4a449a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a8017169b0b12e77aebf8e7d7f6f6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102651&auth_key=1760102651-0-0-f0d4d0c54febc8b2bea3556bee8cdef1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Enhancing-Transformers-Through-Conditioned-Embedded-Tokens"><a href="#Enhancing-Transformers-Through-Conditioned-Embedded-Tokens" class="headerlink" title="Enhancing Transformers Through Conditioned Embedded Tokens"></a>Enhancing Transformers Through Conditioned Embedded Tokens</h2><p><strong>Authors:Hemanth Saratchandran, Simon Lucey</strong></p>
<p>Transformers have transformed modern machine learning, driving breakthroughs in computer vision, natural language processing, and robotics. At the core of their success lies the attention mechanism, which enables the modeling of global dependencies among input tokens. However, we reveal that the attention block in transformers suffers from inherent ill-conditioning, which hampers gradient-based optimization and leads to inefficient training. To address this, we develop a theoretical framework that establishes a direct relationship between the conditioning of the attention block and that of the embedded tokenized data. Building on this insight, we introduce conditioned embedded tokens, a method that systematically modifies the embedded tokens to improve the conditioning of the attention mechanism. Our analysis demonstrates that this approach significantly mitigates ill-conditioning, leading to more stable and efficient training. We validate our methodology across various transformer architectures, achieving consistent improvements in image classification, object detection, instance segmentation, and natural language processing, highlighting its broad applicability and effectiveness. </p>
<blockquote>
<p>转换器（Transformers）已经改变了现代机器学习，推动了计算机视觉、自然语言处理和机器人技术的突破。其核心成功的关键在于注意力机制（Attention Mechanism），它能够实现输入标记之间全局依赖关系的建模。然而，我们揭示出转换器的注意力块存在固有的不适定性问题，这阻碍了基于梯度的优化，导致训练效率低下。为了解决这一问题，我们建立了一个理论框架，直接关联注意力块和嵌入标记化数据的条件。基于这一见解，我们引入了条件嵌入标记（Conditioned Embedded Tokens）方法，该方法系统地修改了嵌入标记，以改善注意力机制的条件。我们的分析表明，这种方法显著减轻了不适定性，导致训练和推理更加稳定和高效。我们在各种转换器架构中验证了我们的方法，在图像分类、目标检测、实例分割和自然语言处理等方面实现了持续性的改进，突显了其广泛适用性和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12789v2">PDF</a> ICCV 2025</p>
<p><strong>摘要</strong></p>
<p>Transformer中的注意力机制为现代机器学习带来了变革，推动了计算机视觉、自然语言处理和机器人技术的突破。然而，本文揭示了Transformer中的注意力块存在固有的不适定性问题，这阻碍了基于梯度的优化并导致训练效率低下。为解决这一问题，我们建立了一个理论框架，该框架直接关联注意力块的适定性与嵌入令牌数据的适定性。在此基础上，我们提出了条件嵌入令牌方法，该方法系统地修改嵌入令牌以提高注意力机制的适定性。分析表明，该方法有效缓解了不适定性问题，使训练更加稳定和高效。我们在各种Transformer架构中验证了该方法的有效性，在图像分类、目标检测、实例分割和自然语言处理方面取得了持续的改进，凸显了其广泛的应用性和有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Transformer的注意力机制实现了现代机器学习的突破，推动了计算机视觉、NLP和机器人技术的进步。</li>
<li>Transformer中的注意力块存在固有的不适定性问题，影响训练效率和稳定性。</li>
<li>本文建立了理论框架，关联注意力块的适定性与嵌入令牌数据的适定性。</li>
<li>提出条件嵌入令牌方法，通过修改嵌入令牌提高注意力机制的适定性。</li>
<li>条件嵌入令牌方法有效缓解了注意力块的不适定性问题。</li>
<li>该方法在多种Transformer架构中验证了有效性，提高了图像分类、目标检测、实例分割和自然语言处理的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12789">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-df9b7ccf0d325ae053bd49dd7a3016ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102658&auth_key=1760102658-0-0-d6f6ee557d27fdeefef0819c9f73db50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a67479dfd70f33cd76edf90d161ea818~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102665&auth_key=1760102665-0-0-bc9279739f78d6a886cee4abd00bb91b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66c2df2587d2a9580d98e1e4db090fdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102672&auth_key=1760102672-0-0-74f38da11980020071d31b44e89564f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FVQ-A-Large-Scale-Dataset-and-an-LMM-based-Method-for-Face-Video-Quality-Assessment"><a href="#FVQ-A-Large-Scale-Dataset-and-an-LMM-based-Method-for-Face-Video-Quality-Assessment" class="headerlink" title="FVQ: A Large-Scale Dataset and an LMM-based Method for Face Video   Quality Assessment"></a>FVQ: A Large-Scale Dataset and an LMM-based Method for Face Video   Quality Assessment</h2><p><strong>Authors:Sijing Wu, Yunhao Li, Ziwen Xu, Yixuan Gao, Huiyu Duan, Wei Sun, Guangtao Zhai</strong></p>
<p>Face video quality assessment (FVQA) deserves to be explored in addition to general video quality assessment (VQA), as face videos are the primary content on social media platforms and human visual system (HVS) is particularly sensitive to human faces. However, FVQA is rarely explored due to the lack of large-scale FVQA datasets. To fill this gap, we present the first large-scale in-the-wild FVQA dataset, FVQ-20K, which contains 20,000 in-the-wild face videos together with corresponding mean opinion score (MOS) annotations. Along with the FVQ-20K dataset, we further propose a specialized FVQA method named FVQ-Rater to achieve human-like rating and scoring for face video, which is the first attempt to explore the potential of large multimodal models (LMMs) for the FVQA task. Concretely, we elaborately extract multi-dimensional features including spatial features, temporal features, and face-specific features (i.e., portrait features and face embeddings) to provide comprehensive visual information, and take advantage of the LoRA-based instruction tuning technique to achieve quality-specific fine-tuning, which shows superior performance on both FVQ-20K and CFVQA datasets. Extensive experiments and comprehensive analysis demonstrate the significant potential of the FVQ-20K dataset and FVQ-Rater method in promoting the development of FVQA. </p>
<blockquote>
<p>脸部视频质量评估（FVQA）除了通用视频质量评估（VQA）外，值得进一步探索。脸部视频是社交媒体平台的主要内容，而人眼视觉系统对人脸尤为敏感。然而，由于缺乏大规模的FVQA数据集，FVQA的研究很少。为了填补这一空白，我们首次推出了大规模的野生FVQA数据集FVQ-20K，它包含2万段野生脸部视频以及相应的平均意见得分（MOS）注释。除了FVQ-20K数据集外，我们还提出了一种专门的FVQA方法，名为FVQ-Rater，实现对人脸视频的拟人评分，这是首次探索大型多模态模型（LMMs）在FVQA任务中的潜力。具体来说，我们精心提取了包括空间特征、时间特征和面部特定特征（即肖像特征和面部嵌入）在内的多维特征，以提供全面的视觉信息，并利用基于LoRA的指令调整技术实现质量特定的微调，这在FVQ-20K和CFVQA数据集上都表现出卓越的性能。大量的实验和综合分析证明了FVQ-20K数据集和FVQ-Rater方法在推动FVQA发展中的显著潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09255v2">PDF</a> Accepted by ACM MM 2025. Project page:   <a target="_blank" rel="noopener" href="https://github.com/wsj-sjtu/FVQ">https://github.com/wsj-sjtu/FVQ</a></p>
<p><strong>摘要</strong></p>
<p>除了通用视频质量评估（VQA），针对脸部视频的评估（FVQA）也值得研究。脸部视频是社交媒体平台的主要内容，且人类视觉系统对人脸特别敏感。然而，FVQA的研究因缺乏大规模数据集而受到限制。为此，我们推出了首个大规模野外FVQA数据集FVQ-20K，包含20,000个野外脸部视频及其对应的平均意见得分（MOS）注释。我们还提出了一种专门的FVQA方法FVQ-Rater，实现对脸部视频的人类式评分，这是首次探索大型多模态模型（LMMs）在FVQA任务中的潜力。我们精心提取了包括空间特征、时间特征和面部特定特征在内的多维特征，利用LoRA基于指令的微调技术实现质量特定的精细调整，在FVQ-20K和CFVQA数据集上表现出卓越的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>脸部视频质量评估（FVQA）是社交媒体时代的重要研究领域，因为脸部视频是主流内容。</li>
<li>人类视觉系统对脸部质量非常敏感，使得FVQA研究具有挑战性。</li>
<li>缺乏大规模的FVQA数据集限制了该领域的研究进展。</li>
<li>引入了首个大规模野外FVQA数据集FVQ-20K，包含20,000个带注释的脸部视频。</li>
<li>提出了FVQ-Rater方法，能模拟人类对面部视频的质量评分。</li>
<li>FVQ-Rater利用多维特征提取和基于LoRA的质量特定微调技术，实现卓越性能。</li>
<li>FVQ-20K数据集和FVQ-Rater方法在促进FVQA领域发展方面具有显著潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-412dea2329e499a2e87daefcb3ad642c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102680&auth_key=1760102680-0-0-d44a11fe53ce57d27a1d995aeb71d113&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5b739b60dc0d9e79d9098cbc07d00f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102689&auth_key=1760102689-0-0-357fbd8a62858d6010108686d42d3a48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dafaa1cff29c656ca1682e031cd68be2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102696&auth_key=1760102696-0-0-cabb26490ab5cd7c669eb0df97785243&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-92ca8b5b65839734914bf4b6e08c6cbd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102702&auth_key=1760102702-0-0-de86a82d9bdcbdb92a6487e1aa628e66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32ee485762790aded252972b57ea3e01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102709&auth_key=1760102709-0-0-95e61263c84b8dab8c700f7b02edf369&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fcd38650890324ddc6010dd103bd9068~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102717&auth_key=1760102717-0-0-4103f403127aa4d061bdb763e8628ea2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science"><a href="#AtmosSci-Bench-Evaluating-the-Recent-Advance-of-Large-Language-Model-for-Atmospheric-Science" class="headerlink" title="AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science"></a>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science</h2><p><strong>Authors:Chenyue Li, Wen Deng, Mengqian Lu, Binhang Yuan</strong></p>
<p>The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges and boosting scientific discovery in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. Toward this end, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. AtmosSci-Bench features a dual-format design comprising both multiple-choice questions (MCQs) and open-ended questions (OEQs), enabling scalable automated evaluation alongside deeper analysis of conceptual understanding. We employ a template-based MCQ generation framework to create diverse, graduate-level problems with symbolic perturbation, while OEQs are used to probe open-ended reasoning. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate services by offering a standard and rigorous evaluation framework. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench">https://github.com/Relaxed-System-Lab/AtmosSci-Bench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的快速发展，特别是在其推理能力方面，为解决大气科学中的复杂挑战和推动科学发展提供了变革性潜力。然而，要在这一领域有效地利用LLM，需要稳健而全面的评估基准。为此，我们推出了AtmosSci-Bench，这是一个新颖的基准测试，旨在系统评估LLM在五个大气科学核心类别的问题上的性能：水文学、大气动力学、大气物理学、地球物理学和物理海洋学。AtmosSci-Bench采用双格式设计，包括多项选择题（MCQs）和开放式问题（OEQs），既能够进行可扩展的自动化评估，又能深入分析概念理解。我们采用基于模板的MCQ生成框架来创建具有符号扰动的多样化、研究生水平的问题，而OEQs则用于测试开放式推理。我们对具有代表性的LLM进行了全面评估，将其分为四组：指令调整模型、高级推理模型、数学增强模型和领域特定气候模型。我们的分析对LLM在大气科学中的推理和问题解决能力提供了一些有趣的见解。我们相信，AtmosSci-Bench可以通过提供标准和严格的评估框架，成为推动LLM在气候服务中应用的重要一步。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Relaxed-System-Lab/AtmosSci-Bench上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01159v3">PDF</a> 37 pages, 4 figures, 13 tables</p>
<p><strong>摘要</strong><br>大型语言模型（LLM）在推理能力方面的迅速进步，为应对大气科学中的复杂挑战和促进科学发现提供了变革性潜力。然而，要在这一领域有效地利用LLM，需要强大而全面的评估基准。为此，我们推出了AtmosSci-Bench，这是一个新颖的基准测试，旨在系统地评估LLM在五个大气科学核心类别的问题上的表现：水文学、大气动力学、大气物理学、地球物理学和物理海洋学。AtmosSci-Bench采用双重格式设计，包括多项选择题（MCQs）和开放性问题（OEQs），以实现可扩展的自动化评估以及对概念理解的深入分析。我们采用基于模板的MCQ生成框架来创建具有符号扰动的多样化、研究生水平的问题，而OEQs则用于探究开放式推理。我们对具有代表性的LLM进行了全面评估，将它们分为四个类别：指令调优模型、高级推理模型、数学增强模型和特定领域的气候模型。我们的分析提供了关于LLM在解决大气科学方面的推理和问题解决能力的有趣见解。我们相信AtmosSci-Bench可以通过提供标准和严格的评估框架，成为推进LLM在气候服务中应用的至关重要的步骤。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/AtmosSci-Bench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Relaxed-System-Lab/AtmosSci-Bench上找到。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）在解决大气科学问题方面具有巨大潜力。</li>
<li>AtmosSci-Bench是一个新的基准测试，旨在评估LLM在五个大气科学核心领域的问题解决能力。</li>
<li>该基准测试采用双重格式设计，包括多项选择题和开放性问题，以全面评估LLM的推理和理解能力。</li>
<li>通过符号扰动创建多样化的问题，以测试LLM的适应性和灵活性。</li>
<li>对不同类型的LLM进行了全面评估，包括指令调优模型、高级推理模型等。</li>
<li>AtmosSci-Bench为推进LLM在气候服务中的应用提供了标准评估框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01159">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c498a6b9c01cde020eb4718d78d3a561~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102724&auth_key=1760102724-0-0-28e7295c25a98821374f46c3508dd687&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0cdf467947d9829232054cf6ad30602f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102732&auth_key=1760102732-0-0-0193833799024bf0973c46d7cdc44c34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adb9269d5845a0d8bc5e5455bdbdfac6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102740&auth_key=1760102740-0-0-7e34cd33ab471fc657ece54ccf8bc90d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Imagining-the-Unseen-Generative-Location-Modeling-for-Object-Placement"><a href="#Imagining-the-Unseen-Generative-Location-Modeling-for-Object-Placement" class="headerlink" title="Imagining the Unseen: Generative Location Modeling for Object Placement"></a>Imagining the Unseen: Generative Location Modeling for Object Placement</h2><p><strong>Authors:Jooyeol Yun, Davide Abati, Mohamed Omran, Jaegul Choo, Amirhossein Habibian, Auke Wiggers</strong></p>
<p>Location modeling, or determining where non-existing objects could feasibly appear in a scene, has the potential to benefit numerous computer vision tasks, from automatic object insertion to scene creation in virtual reality. Yet, this capability remains largely unexplored to date. In this paper, we develop a generative location model that, given an object class and an image, learns to predict plausible bounding boxes for such an object. Our approach first tokenizes the image and target object class, then decodes bounding box coordinates through an autoregressive transformer. This formulation effectively addresses two core challenges in locatio modeling: the inherent one-to-many nature of plausible locations, and the sparsity of existing location modeling datasets, where fewer than 1% of valid placements are labeled. Furthermore, we incorporate Direct Preference Optimization to leverage negative labels, refining the spatial predictions. Empirical evaluations reveal that our generative location model achieves superior placement accuracy on the OPA dataset as compared to discriminative baselines and image composition approaches. We further test our model in the context of object insertion, where it proposes locations for an off-the-shelf inpainting model to render objects. In this respect, our proposal exhibits improved visual coherence relative to state-of-the-art instruction-tuned editing methods, demonstrating a high-performing location model’s utility in a downstream application. </p>
<blockquote>
<p>位置建模，或者确定场景中不存在的物体可能出现的位置，具有为众多计算机视觉任务带来潜在益处的潜力，从自动物体插入到虚拟现实场景创建。然而，这种能力至今尚未得到充分的探索。在本文中，我们开发了一种生成式位置模型，该模型在给定的目标对象和图像的情况下，能够预测该对象的合理边界框。我们的方法首先对图像和目标对象类别进行标记化，然后通过自回归变压器解码边界框坐标。这种表述有效地解决了位置建模中的两个核心挑战：合理位置的固有的一到多对应关系，以及现有位置建模数据集稀疏的问题，其中只有不到1%的有效位置被标记。此外，我们结合了直接偏好优化来利用负面标签，对空间预测进行细化。经验评估表明，我们的生成位置模型在OPA数据集上的放置精度优于判别基准和图像组合方法。我们进一步在物体插入的上下文中测试了我们的模型，它为现成的图像修复模型呈现物体位置建议。在这方面，我们的提案与最新的指令调整编辑方法相比，表现出更高的视觉连贯性，证明了高性能位置模型在下游应用中的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13564v2">PDF</a> Accepted by ICCV 2025 DRL4Real Workshop</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种生成式位置模型，能够根据目标对象和图像预测其合理的边界框位置。模型首先对图像和目标对象进行标记化，然后通过自回归转换器解码边界框坐标。该方法解决了位置建模中的两个核心挑战：一是合理位置具有一对多的特点；二是现有的位置建模数据集稀少。通过结合直接偏好优化和负样本，优化了空间预测的准确性。经验评估表明，本文模型在OPA数据集上的放置准确度优于判别式基准和图像组合方法。在对象插入的上下文中进一步测试了模型，结果表明模型能够提出适合补全模型的物体位置，与最新的指令调优编辑方法相比，视觉连贯性有所提升。该模型的位置建模性能优异，在下游应用中有实用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>生成式位置模型能够根据目标对象和图像预测合理的边界框位置。</li>
<li>模型通过标记化和自回归转换器解码来解决位置建模的核心挑战。</li>
<li>模型解决了合理位置一对多和现有数据集稀少的问题。</li>
<li>结合直接偏好优化和负样本提升了空间预测的准确性。</li>
<li>模型在OPA数据集上的放置准确度优于其他方法。</li>
<li>模型在对象插入的上下文中表现出良好的实用性，提高了视觉连贯性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13564">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-97df018755febf27a4f56d083677e5ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102748&auth_key=1760102748-0-0-f74291c0c7ba441747cdef8c0e7b1cd2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97f123f4174c8649774194ed51986170~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102755&auth_key=1760102755-0-0-5ae3791608476b3d9cde82dfe7c8ea66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e73590676a48f2fc316f081aee61505~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102761&auth_key=1760102761-0-0-32e114461130e4ff24a1bf6515fc9a41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-411f7c383634bf9fe1b4d66ddfd1b553~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102768&auth_key=1760102768-0-0-740c3dafe9e764a8ea7f71e636c61c92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3738a348446cafec1daa4445dbb56b90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102775&auth_key=1760102775-0-0-4773bf781c4ca111b8e8c07ed4f097ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-1cb8a4b5ab6999a2190e744d64b8aadd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083139&auth_key=1760083139-0-0-9d2ec30427e235b33afe1d1e5f64497b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-10-10  Multi-Objective Multi-Agent Path Finding with Lexicographic Cost   Preferences
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d2897c8fa1f46a4fcd6c88e66d56fe68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082573&auth_key=1760082573-0-0-84dce57a4322036d03a148160941ef91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-10-10  LeMAJ (Legal LLM-as-a-Judge) Bridging Legal Reasoning and LLM   Evaluation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
