<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c7356733d2d17e36a80ff2f1f8d36a27')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-10-æ›´æ–°"><a href="#2025-10-10-æ›´æ–°" class="headerlink" title="2025-10-10 æ›´æ–°"></a>2025-10-10 æ›´æ–°</h1><h2 id="Few-Shot-Adaptation-Benchmark-for-Remote-Sensing-Vision-Language-Models"><a href="#Few-Shot-Adaptation-Benchmark-for-Remote-Sensing-Vision-Language-Models" class="headerlink" title="Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models"></a>Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models</h2><p><strong>Authors:Karim El Khoury, Maxime Zanella, Christophe De Vleeschouwer, Benoit Macq</strong></p>
<p>Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable potential thanks to large-scale pretraining, achieving strong zero-shot performance on various tasks. However, their ability to generalize in low-data regimes, such as few-shot learning, remains insufficiently explored. In this work, we present the first structured benchmark for evaluating few-shot adaptation methods on RSVLMs. We conduct comprehensive experiments across ten remote sensing scene classification datasets, applying five widely used few-shot adaptation strategies to three state-of-the-art RSVLMs with varying backbones. Our findings reveal that models with similar zero-shot performance can exhibit markedly different behavior under few-shot adaptation, with some RSVLMs being inherently more amenable to such adaptation than others. The variability of performance and the absence of a clear winner among existing methods highlight the need for the development of more robust methods for few-shot adaptation tailored to RS. To facilitate future research, we provide a reproducible benchmarking framework and open-source code to systematically evaluate RSVLMs under few-shot conditions. The source code is publicly available on Github: <a target="_blank" rel="noopener" href="https://github.com/elkhouryk/fewshot_RSVLMs">https://github.com/elkhouryk/fewshot_RSVLMs</a> </p>
<blockquote>
<p>é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆRSVLMsï¼‰ç”±äºå¤§è§„æ¨¡é¢„è®­ç»ƒè€Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¦‚å°æ ·æœ¬å­¦ä¹ ï¼Œä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡ä¸ºè¯„ä¼°RSVLMsçš„å°æ ·æœ¬é€‚åº”æ–¹æ³•å»ºç«‹äº†ç»“æ„åŒ–åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨åä¸ªé¥æ„Ÿåœºæ™¯åˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢å®éªŒï¼Œå°†äº”ç§å¹¿æ³›ä½¿ç”¨çš„å°æ ·æœ¬é€‚åº”ç­–ç•¥åº”ç”¨äºä¸‰ç§å…·æœ‰ä¸åŒèƒŒéª¨çš„æœ€å…ˆè¿›RSVLMsã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå…·æœ‰ç›¸ä¼¼é›¶æ ·æœ¬æ€§èƒ½çš„æ¨¡å‹åœ¨å°æ ·æœ¬é€‚åº”ä¸‹å¯èƒ½ä¼šè¡¨ç°å‡ºæˆªç„¶ä¸åŒçš„è¡Œä¸ºï¼Œæœ‰äº›RSVLMså¤©ç”Ÿæ›´é€‚åˆè¿™ç§é€‚åº”ã€‚æ€§èƒ½çš„å¯å˜æ€§å’Œç°æœ‰æ–¹æ³•ä¹‹é—´æ²¡æœ‰æ˜æ˜¾èƒœå‡ºè€…ï¼Œè¿™å‡¸æ˜¾äº†éœ€è¦å¼€å‘æ›´ç¨³å¥çš„é’ˆå¯¹RSçš„å°æ ·æœ¬é€‚åº”æ–¹æ³•çš„å¿…è¦æ€§ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„åŸºå‡†æµ‹è¯•æ¡†æ¶å’Œå¼€æºä»£ç ï¼Œä»¥åœ¨æœ‰é™æ ·æœ¬æ¡ä»¶ä¸‹ç³»ç»Ÿåœ°è¯„ä¼°RSVLMsã€‚æºä»£ç å¯åœ¨Githubä¸Šå…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/elkhouryk/fewshot_RSVLMs">https://github.com/elkhouryk/fewshot_RSVLMs</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07135v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆRSVLMsï¼‰çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›å·²ç»å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä»å¾…å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ„å»ºäº†è¯„ä¼°RSVLMså°æ ·æœ¬é€‚åº”æ–¹æ³•çš„åŸºå‡†æµ‹è¯•ï¼Œå¯¹åç§é¥æ„Ÿåœºæ™¯åˆ†ç±»æ•°æ®é›†è¿›è¡Œå…¨é¢å®éªŒï¼Œå°†äº”ç§å¸¸ç”¨çš„å°æ ·æœ¬é€‚åº”ç­–ç•¥åº”ç”¨äºä¸‰ç§å…ˆè¿›çš„RSVLMsã€‚ç ”ç©¶å‘ç°ï¼Œå…·æœ‰ç›¸ä¼¼é›¶æ ·æœ¬æ€§èƒ½çš„æ¨¡å‹åœ¨å°æ ·æœ¬é€‚åº”ä¸‹è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå‡¸æ˜¾å‡ºå¼€å‘æ›´é€‚åˆé¥æ„Ÿé¢†åŸŸçš„å°æ ·æœ¬é€‚åº”æ–¹æ³•çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RSVLMsåœ¨é›¶æ ·æœ¬å­¦ä¹ æ–¹é¢å·²è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å°æ ·æœ¬å­¦ä¹ åœ¨RSVLMsä¸­çš„ç ”ç©¶ä»å¾…æ·±å…¥ã€‚</li>
<li>ä¸åŒRSVLMsåœ¨å°æ ·æœ¬é€‚åº”ä¸‹çš„è¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>ç°æœ‰å°æ ·æœ¬é€‚åº”ç­–ç•¥åœ¨RSVLMsä¸­æ— æ˜ç¡®ä¼˜èƒœè€…ã€‚</li>
<li>éœ€è¦å¼€å‘æ›´é€‚ç”¨äºé¥æ„Ÿé¢†åŸŸçš„å°æ ·æœ¬é€‚åº”æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯é‡ç°çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°RSVLMsçš„å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31b1cfbe1959928e502dd40d1e513f5e" align="middle">
<img src="https://picx.zhimg.com/v2-2a4bd54ce5d12fdccfec5099dc6e7d44" align="middle">
<img src="https://picx.zhimg.com/v2-ad437d0adb9a1f8ae2209b87fa847f5d" align="middle">
<img src="https://picx.zhimg.com/v2-0f10a9fdfedf83818fd67311cc18b9cb" align="middle">
<img src="https://picx.zhimg.com/v2-838ebe99efa5f3b1f93ca0d703164ec3" align="middle">
<img src="https://picx.zhimg.com/v2-12b94fd3528a36ef89eecb99f86f1407" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Crossing-Domains-without-Labels-Distant-Supervision-for-Term-Extraction"><a href="#Crossing-Domains-without-Labels-Distant-Supervision-for-Term-Extraction" class="headerlink" title="Crossing Domains without Labels: Distant Supervision for Term Extraction"></a>Crossing Domains without Labels: Distant Supervision for Term Extraction</h2><p><strong>Authors:Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank</strong></p>
<p>Automatic Term Extraction (ATE) is a critical component in downstream NLP tasks such as document tagging, ontology construction and patent analysis. Current state-of-the-art methods require expensive human annotation and struggle with domain transfer, limiting their practical deployment. This highlights the need for more robust, scalable solutions and realistic evaluation settings. To address this, we introduce a comprehensive benchmark spanning seven diverse domains, enabling performance evaluation at both the document- and corpus-levels. Furthermore, we propose a robust LLM-based model that outperforms both supervised cross-domain encoder models and few-shot learning baselines and performs competitively with its GPT-4o teacher on this benchmark. The first step of our approach is generating psuedo-labels with this black-box LLM on general and scientific domains to ensure generalizability. Building on this data, we fine-tune the first LLMs for ATE. To further enhance document-level consistency, oftentimes needed for downstream tasks, we introduce lightweight post-hoc heuristics. Our approach exceeds previous approaches on 5&#x2F;7 domains with an average improvement of 10 percentage points. We release our dataset and fine-tuned models to support future research in this area. </p>
<blockquote>
<p>è‡ªåŠ¨æœ¯è¯­æå–ï¼ˆATEï¼‰æ˜¯ä¸‹æ¸¸NLPä»»åŠ¡ï¼ˆå¦‚æ–‡æ¡£æ ‡è®°ã€æœ¬ä½“æ„å»ºå’Œä¸“åˆ©åˆ†æï¼‰ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å½“å‰æœ€å‰æ²¿çš„æ–¹æ³•éœ€è¦æ˜‚è´µçš„äººåŠ›æ ‡æ³¨ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸè¿ç§»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œé™åˆ¶äº†å…¶å®é™…éƒ¨ç½²ã€‚è¿™å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥ã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆå’Œç°å®çš„è¯„ä¼°ç¯å¢ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¶µç›–ä¸ƒä¸ªä¸åŒé¢†åŸŸçš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿåœ¨æ–‡æ¡£å’Œè¯­æ–™åº“ä¸¤ä¸ªå±‚é¢è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¨³å¥çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä»…ä¼˜äºæœ‰ç›‘ç£çš„è·¨åŸŸç¼–ç å™¨æ¨¡å‹å’Œå°‘æ ·æœ¬å­¦ä¹ åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­ä¸GPT-4oæ•™å¸ˆè¡¨ç°ç›¸å½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ç¬¬ä¸€æ­¥æ˜¯åˆ©ç”¨è¿™ä¸ªé»‘ç›’å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬é¢†åŸŸå’Œç§‘å­¦é¢†åŸŸç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»¥ç¡®ä¿å…¶é€šç”¨æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¹ç¬¬ä¸€æ‰¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒATEå¾®è°ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºä¸‹æ¸¸ä»»åŠ¡é€šå¸¸éœ€è¦çš„æ–‡æ¡£çº§ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è½»é‡çº§çš„åéªŒå¯å‘å¼æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨5&#x2F;7çš„é¢†åŸŸä¸Šè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†10ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬å…¬å¼€æˆ‘ä»¬çš„æ•°æ®é›†å’Œå¾®è°ƒæ¨¡å‹ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06838v1">PDF</a> Accepted at EMNLP Industry Track 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨æœ¯è¯­æŠ½å–ï¼ˆATEï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œå¦‚æ–‡æ¡£æ ‡æ³¨ã€æœ¬ä½“æ„å»ºå’Œä¸“åˆ©åˆ†æã€‚é’ˆå¯¹å½“å‰å…ˆè¿›æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººåŠ›æ ‡æ³¨å’Œé¢†åŸŸè¿ç§»å›°éš¾çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§å…¨é¢çš„è·¨ä¸ƒä¸ªä¸åŒé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ–‡æ¡£å’Œè¯­æ–™åº“çº§åˆ«çš„æ€§èƒ½ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¨³å¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç›‘ç£è·¨åŸŸç¼–ç å™¨æ¨¡å‹å’Œå°‘æ ·æœ¬å­¦ä¹ åŸºçº¿ï¼Œå¹¶ä¸GPT-4oæ•™å¸ˆç«äº‰ã€‚æ–‡ç« é¦–å…ˆé€šè¿‡é€šç”¨å’Œç§‘å­¦é¢†åŸŸç”Ÿæˆä¼ªæ ‡ç­¾æ¥ç¡®ä¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¾®è°ƒäº†é¦–ä¸ªATEé¢†åŸŸçš„LLMã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„æ–‡æ¡£çº§åˆ«ä¸€è‡´æ€§ï¼Œæ–‡ç« å¼•å…¥äº†è½»é‡çº§äº‹åå¯å‘å¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨äº”ä¸ªé¢†åŸŸè¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†10ä¸ªç™¾åˆ†ç‚¹ã€‚æ–‡ç« è¿˜å…¬å¼€äº†æ•°æ®é›†å’Œå¾®è°ƒæ¨¡å‹ï¼Œä»¥æ”¯æŒæœªæ¥åœ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ATEåœ¨NLPä¸‹æ¸¸ä»»åŠ¡ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¦‚æ–‡æ¡£æ ‡æ³¨ã€æœ¬ä½“æ„å»ºå’Œä¸“åˆ©åˆ†æã€‚</li>
<li>å½“å‰å…ˆè¿›æ–¹æ³•å­˜åœ¨äººåŠ›æ ‡æ³¨æˆæœ¬é«˜å’Œé¢†åŸŸè¿ç§»å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è·¨ä¸ƒä¸ªä¸åŒé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æ–‡æ¡£å’Œè¯­æ–™åº“çº§åˆ«çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºLLMçš„ç¨³å¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆä¼ªæ ‡ç­¾ç¡®ä¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¾®è°ƒäº†é¦–ä¸ªATEé¢†åŸŸçš„LLMã€‚</li>
<li>å¼•å…¥äº†è½»é‡çº§äº‹åå¯å‘å¼æ–¹æ³•æ¥æé«˜æ–‡æ¡£çº§åˆ«çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a77a21600cd7b21fa85d384457072f7f" align="middle">
<img src="https://picx.zhimg.com/v2-020a199fc0c0e116f050b02fdc1a33e4" align="middle">
<img src="https://picx.zhimg.com/v2-b6abfcd205e5413f37ae35b93d302d4b" align="middle">
<img src="https://picx.zhimg.com/v2-a9d63390de76a08f721061a4cb4c0bbb" align="middle">
<img src="https://picx.zhimg.com/v2-7301eae5a0a3b1631eba26181066ada4" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AISysRev-â€“-LLM-based-Tool-for-Title-abstract-Screening"><a href="#AISysRev-â€“-LLM-based-Tool-for-Title-abstract-Screening" class="headerlink" title="AISysRev â€“ LLM-based Tool for Title-abstract Screening"></a>AISysRev â€“ LLM-based Tool for Title-abstract Screening</h2><p><strong>Authors:Aleksi Huotala, Miikka Kuutila, Olli-Pekka Turtio, Mika MÃ¤ntylÃ¤</strong></p>
<p>Systematic reviews are a standard practice for summarizing the state of evidence in software engineering. Conducting systematic reviews is laborious, especially during the screening or study selection phase, where the number of papers can be overwhelming. During this phase, papers are assessed against inclusion and exclusion criteria based on their titles and abstracts. Recent research has demonstrated that large language models (LLMs) can perform title-abstract screening at a level comparable to that of a masterâ€™s student. While LLMs cannot be fully trusted, they can help, for example, in Rapid Reviews, which try to expedite the review process. Building on recent research, we developed AiSysRev, an LLM-based screening tool implemented as a web application running in a Docker container. The tool accepts a CSV file containing paper titles and abstracts. Users specify inclusion and exclusion criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev supports both zero-shot and few-shot screening, and also allows for manual screening through interfaces that display LLM results as guidance for human reviewers.We conducted a trial study with 137 papers using the tool. Our findings indicate that papers can be classified into four categories: Easy Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary cases, where LLMs are prone to errors, highlight the need for human intervention. While LLMs do not replace human judgment in systematic reviews, they can significantly reduce the burden of assessing large volumes of scientific literature. Video: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jVbEj4Y4tQI">https://www.youtube.com/watch?v=jVbEj4Y4tQI</a> Tool: <a target="_blank" rel="noopener" href="https://github.com/EvoTestOps/AISysRev">https://github.com/EvoTestOps/AISysRev</a> </p>
<blockquote>
<p>ç³»ç»Ÿç»¼è¿°æ˜¯è½¯ä»¶å·¥ç¨‹ä¸­æ€»ç»“è¯æ®çŠ¶å†µçš„æ ‡å‡†å®è·µã€‚è¿›è¡Œç³»ç»Ÿç»¼è¿°æ˜¯ä¸€é¡¹è‰°å·¨çš„å·¥ä½œï¼Œç‰¹åˆ«æ˜¯åœ¨ç­›é€‰æˆ–ç ”ç©¶é€‰æ‹©é˜¶æ®µï¼Œè®ºæ–‡æ•°é‡å¯èƒ½éå¸¸åºå¤§ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œå°†æ ¹æ®è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦æ¥è¯„ä¼°å…¶æ˜¯å¦ç¬¦åˆçº³å…¥å’Œæ’é™¤æ ‡å‡†ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ ‡é¢˜æ‘˜è¦ç­›é€‰çš„èƒ½åŠ›å¯ä»¥è¾¾åˆ°ç¡•å£«ç”Ÿçš„æ°´å¹³ã€‚è™½ç„¶ä¸èƒ½å®Œå…¨ä¿¡ä»»LLMsï¼Œä½†å®ƒä»¬å¯ä»¥åœ¨æŸäº›æƒ…å†µä¸‹æä¾›å¸®åŠ©ï¼Œä¾‹å¦‚åœ¨å¿«é€Ÿå®¡æŸ¥ä¸­å°è¯•åŠ å¿«å®¡æŸ¥è¿‡ç¨‹ã€‚åŸºäºè¿‘æœŸç ”ç©¶ï¼Œæˆ‘ä»¬å¼€å‘äº†AiSysRevï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„ç­›é€‰å·¥å…·ï¼Œä»¥Webåº”ç”¨ç¨‹åºçš„å½¢å¼å®ç°ï¼Œè¿è¡Œåœ¨Dockerå®¹å™¨ä¸­ã€‚è¯¥å·¥å…·æ¥å—åŒ…å«è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦çš„CSVæ–‡ä»¶ã€‚ç”¨æˆ·æŒ‡å®šçº³å…¥å’Œæ’é™¤æ ‡å‡†ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡OpenRouterä½¿ç”¨å¤šä¸ªLLMsè¿›è¡Œç­›é€‰ã€‚AiSysRevæ”¯æŒé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç­›é€‰ï¼Œå¹¶å…è®¸é€šè¿‡æ˜¾ç¤ºLLMç»“æœä½œä¸ºäººç±»è¯„å®¡è€…çš„æŒ‡å¯¼æ¥è¿›è¡Œæ‰‹åŠ¨ç­›é€‰ã€‚æˆ‘ä»¬å¯¹åŒ…å«137ç¯‡è®ºæ–‡çš„å·¥å…·è¿›è¡Œäº†è¯•éªŒæ€§ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®ºæ–‡å¯åˆ†ä¸ºå››ç±»ï¼šæ˜“äºåŒ…å«çš„ã€æ˜“äºæ’é™¤çš„ã€è¾¹ç•ŒåŒ…å«çš„ã€è¾¹ç•Œæ’é™¤çš„ã€‚è¾¹ç•Œæ¡ˆä¾‹æ˜¯LLMså®¹æ˜“å‡ºç°é”™è¯¯çš„åœ°æ–¹ï¼Œè¿™å¼ºè°ƒäº†äººå·¥å¹²é¢„çš„å¿…è¦æ€§ã€‚è™½ç„¶LLMsä¸èƒ½å–ä»£ç³»ç»Ÿç»¼è¿°ä¸­çš„äººçš„åˆ¤æ–­ï¼Œä½†å®ƒä»¬å¯ä»¥æ˜¾è‘—å‡å°‘è¯„ä¼°å¤§é‡ç§‘å­¦æ–‡çŒ®çš„è´Ÿæ‹…ã€‚è§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jVbEj4Y4tQI">https://www.youtube.com/watch?v=jVbEj4Y4tQI</a> å·¥å…·ï¼š<a target="_blank" rel="noopener" href="https://github.com/EvoTestOps/AISysRev">https://github.com/EvoTestOps/AISysRev</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06708v1">PDF</a> 4 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ç³»ç»Ÿå®¡æŸ¥åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„æ ‡å‡†å®è·µï¼Œå¹¶æŒ‡å‡ºå…¶ä¸­çš„ç­›é€‰é˜¶æ®µå·¥ä½œé‡å·¨å¤§ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥æ‰§è¡Œæ ‡é¢˜æ‘˜è¦ç­›é€‰ï¼Œä¸ç¡•å£«ç”Ÿçš„æ°´å¹³ç›¸å½“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼€å‘äº†AiSysRevå·¥å…·ï¼Œæ”¯æŒé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç­›é€‰ï¼Œå¹¶å…è®¸é€šè¿‡æ˜¾ç¤ºLLMç»“æœæ¥æŒ‡å¯¼äººå·¥å®¡æŸ¥è€…è¿›è¡Œæ‰‹åŠ¨ç­›é€‰ã€‚ç ”ç©¶å‘ç°ï¼Œè®ºæ–‡å¯åˆ†ä¸ºå››ç±»ï¼Œå…¶ä¸­è¾¹ç•Œæ¡ˆä¾‹éœ€è¦äººå·¥å¹²é¢„ã€‚è™½ç„¶LLMsä¸èƒ½å®Œå…¨å–ä»£ç³»ç»Ÿå®¡æŸ¥ä¸­çš„äººçš„åˆ¤æ–­åŠ›ï¼Œä½†å®ƒä»¬å¯ä»¥æ˜¾è‘—å‡å°‘è¯„ä¼°å¤§é‡ç§‘å­¦æ–‡çŒ®çš„è´Ÿæ‹…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³»ç»Ÿå®¡æŸ¥æ˜¯è½¯ä»¶å·¥ç¨‹è¯æ®æ€»ç»“çš„æ ‡å‡†å®è·µï¼Œå…¶ä¸­ç­›é€‰é˜¶æ®µå°¤ä¸ºè€—æ—¶ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥æ‰§è¡Œæ ‡é¢˜å’Œæ‘˜è¦çš„ç­›é€‰ï¼Œæ•ˆç‡ä¸ç¡•å£«ç”Ÿç›¸å½“ã€‚</li>
<li>AiSysRevå·¥å…·åŸºäºLLMï¼Œæ”¯æŒé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç­›é€‰ï¼Œå¹¶å¯æ‰‹åŠ¨ç­›é€‰ã€‚</li>
<li>åœ¨ä½¿ç”¨AiSysRevè¿›è¡Œçš„è¯•éªŒç ”ç©¶ä¸­ï¼Œè®ºæ–‡è¢«åˆ†ä¸ºå››ç±»ï¼šEasy Includes, Easy Excludes, Boundary Includes, å’Œ Boundary Excludesã€‚</li>
<li>è¾¹ç•Œæ¡ˆä¾‹è¡¨æ˜LLMså­˜åœ¨é”™è¯¯å€¾å‘ï¼Œéœ€è¦äººå·¥å¹²é¢„ã€‚</li>
<li>LLMsä¸èƒ½æ›¿ä»£ç³»ç»Ÿå®¡æŸ¥ä¸­çš„äººçš„åˆ¤æ–­åŠ›ï¼Œä½†å¯æ˜¾è‘—å‡å°‘è¯„ä¼°å¤§é‡ç§‘å­¦æ–‡çŒ®çš„å·¥ä½œé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d092559212274511806f6b8e86fe9cc" align="middle">
<img src="https://picx.zhimg.com/v2-6893525dec1dcd294f1aabf0778a2be6" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cross-Embodiment-Dexterous-Hand-Articulation-Generation-via-Morphology-Aware-Learning"><a href="#Cross-Embodiment-Dexterous-Hand-Articulation-Generation-via-Morphology-Aware-Learning" class="headerlink" title="Cross-Embodiment Dexterous Hand Articulation Generation via   Morphology-Aware Learning"></a>Cross-Embodiment Dexterous Hand Articulation Generation via   Morphology-Aware Learning</h2><p><strong>Authors:Heng Zhang, Kevin Yuchen Ma, Mike Zheng Shou, Weisi Lin, Yan Wu</strong></p>
<p>Dexterous grasping with multi-fingered hands remains challenging due to high-dimensional articulations and the cost of optimization-based pipelines. Existing end-to-end methods require training on large-scale datasets for specific hands, limiting their ability to generalize across different embodiments. We propose an eigengrasp-based, end-to-end framework for cross-embodiment grasp generation. From a handâ€™s morphology description, we derive a morphology embedding and an eigengrasp set. Conditioned on these, together with the object point cloud and wrist pose, an amplitude predictor regresses articulation coefficients in a low-dimensional space, which are decoded into full joint articulations. Articulation learning is supervised with a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant motions and injects morphology-specific structure. In simulation on unseen objects across three dexterous hands, our model attains a 91.9% average grasp success rate with less than 0.4 seconds inference per grasp. With few-shot adaptation to an unseen hand, it achieves 85.6% success on unseen objects in simulation, and real-world experiments on this few-shot generalized hand achieve an 87% success rate. The code and additional materials will be made available upon publication on our project website <a target="_blank" rel="noopener" href="https://connor-zh.github.io/cross_embodiment_dexterous_grasping">https://connor-zh.github.io/cross_embodiment_dexterous_grasping</a>. </p>
<blockquote>
<p>çµå·§çš„å¤šæŒ‡æ‰‹æŠ“å–ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå…¶é«˜ç»´åº¦çš„å…³èŠ‚è¿åŠ¨å’ŒåŸºäºä¼˜åŒ–çš„ç®¡é“çš„æˆæœ¬è¾ƒé«˜ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯æ–¹æ³•éœ€è¦åœ¨ç‰¹å®šçš„æ‰‹çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸åŒæ‰‹éƒ¨çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›ºæœ‰æ¡æ³•çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºè·¨æ‰‹éƒ¨å½¢æ€çš„æŠ“å–ç”Ÿæˆã€‚æ ¹æ®æ‰‹éƒ¨å½¢æ€æè¿°ï¼Œæˆ‘ä»¬å¾—å‡ºå½¢æ€åµŒå…¥å’Œå›ºæœ‰æ¡æ³•é›†ã€‚åŸºäºè¿™äº›æ¡ä»¶ï¼Œä»¥åŠç‰©ä½“ç‚¹äº‘å’Œæ‰‹è…•å§¿æ€ï¼ŒæŒ¯å¹…é¢„æµ‹å™¨ä¼šå›å½’ä½ç»´ç©ºé—´ä¸­çš„å…³èŠ‚è¿åŠ¨ç³»æ•°ï¼Œç„¶åå°†å…¶è§£ç ä¸ºå®Œæ•´çš„å…³èŠ‚è¿åŠ¨ã€‚å…³èŠ‚è¿åŠ¨å­¦ä¹ æ˜¯é€šè¿‡è¿åŠ¨æ„ŸçŸ¥å…³èŠ‚è¿åŠ¨æŸå¤±ï¼ˆKALï¼‰è¿›è¡Œç›‘ç£çš„ï¼Œå®ƒå¼ºè°ƒæŒ‡å°–ç›¸å…³è¿åŠ¨å¹¶æ³¨å…¥å½¢æ€ç‰¹å®šçš„ç»“æ„ã€‚åœ¨ä¸‰ç§çµå·§æ‰‹ä¸Šå¯¹æœªè§è¿‡çš„ç‰©ä½“è¿›è¡Œä»¿çœŸå®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†å¹³å‡91.9%çš„æŠ“å–æˆåŠŸç‡ï¼Œæ¯æ¬¡æŠ“å–çš„æ¨ç†æ—¶é—´ä¸åˆ°0.4ç§’ã€‚é€šè¿‡å‡ æ¬¡é€‚åº”æœªè§è¿‡çš„æ‰‹éƒ¨ï¼Œå®ƒåœ¨ä»¿çœŸä¸­å¯¹æœªè§è¿‡çš„ç‰©ä½“çš„æˆåŠŸç‡è¾¾åˆ°äº†85.6%ï¼Œå¹¶åœ¨å®é™…è¿›è¡Œçš„å…³äºè¯¥å°‘æ•°é•œå¤´é€šç”¨åŒ–çš„æ‰‹éƒ¨å®éªŒä¸­è¾¾åˆ°äº†87%çš„æˆåŠŸç‡ã€‚ç›¸å…³ä»£ç å’Œé¢å¤–ææ–™å°†åœ¨æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://connor-zh.github.io/cross_embodiment_dexterous_grasping">https://connor-zh.github.io/cross_embodiment_dexterous_grasping</a>ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06068v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç‰¹å¾æŠ“æ¡ï¼ˆeigengraspï¼‰çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºè·¨å½¢æ€æŠ“æ¡ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡æ‰‹çš„å½¢æ€æè¿°ç”Ÿæˆå½¢æ€åµŒå…¥å’Œç‰¹å¾æŠ“æ¡é›†ï¼Œå†ç»“åˆç‰©ä½“ç‚¹äº‘å’Œæ‰‹è…•å§¿æ€ï¼Œé€šè¿‡æŒ¯å¹…é¢„æµ‹å™¨åœ¨ä½ç»´ç©ºé—´ä¸­å›å½’å…³èŠ‚æ´»åŠ¨ç³»æ•°ï¼Œæœ€åè§£ç ä¸ºå…¨å…³èŠ‚æ´»åŠ¨ã€‚å­¦ä¹ å…³èŠ‚æ´»åŠ¨é€šè¿‡ç›‘ç£åŠ¨è§‰å…³èŠ‚æŸå¤±ï¼ˆKALï¼‰ï¼Œå¼ºè°ƒæŒ‡å°–ç›¸å…³è¿åŠ¨å’Œæ³¨å…¥å½¢æ€ç‰¹å®šç»“æ„ã€‚æ¨¡å‹åœ¨ä»¿çœŸä¸­æœªè§ç‰©ä½“è·¨ä¸‰ç§çµå·§æ‰‹çš„å¹³å‡æŠ“æ¡æˆåŠŸç‡è¾¾åˆ°91.9%ï¼Œå•æ¬¡æŠ“æ¡æ¨ç†æ—¶é—´ä¸åˆ°0.4ç§’ã€‚å¯¹æœªè§çš„æ‰‹è¿›è¡Œå°‘é‡æ ·æœ¬é€‚åº”åï¼Œåœ¨ä»¿çœŸä¸­æœªè§ç‰©ä½“çš„æˆåŠŸç‡è¾¾åˆ°85.6%ï¼Œåœ¨å®é™…å®éªŒä¸­çš„æˆåŠŸç‡è¾¾åˆ°87%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åŸºäºç‰¹å¾æŠ“æ¡çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºè·¨ä¸åŒå½¢æ€çš„æŠ“æ¡ç”Ÿæˆã€‚</li>
<li>é€šè¿‡æ‰‹çš„å½¢æ€æè¿°ç”Ÿæˆå½¢æ€åµŒå…¥å’Œç‰¹å¾æŠ“æ¡é›†ã€‚</li>
<li>ç»“åˆç‰©ä½“ç‚¹äº‘å’Œæ‰‹è…•å§¿æ€ï¼Œé€šè¿‡æŒ¯å¹…é¢„æµ‹å™¨å›å½’å…³èŠ‚æ´»åŠ¨ç³»æ•°ã€‚</li>
<li>é‡‡ç”¨ä½ç»´ç©ºé—´å­¦ä¹ å…³èŠ‚æ´»åŠ¨ï¼Œå¹¶è§£ç ä¸ºå…¨å…³èŠ‚æ´»åŠ¨ã€‚</li>
<li>ç›‘ç£å­¦ä¹ é€šè¿‡åŠ¨è§‰å…³èŠ‚æŸå¤±ï¼ˆKALï¼‰ï¼Œå¼ºè°ƒæŒ‡å°–ç›¸å…³è¿åŠ¨å’Œå½¢æ€ç‰¹å®šç»“æ„ã€‚</li>
<li>åœ¨ä»¿çœŸç¯å¢ƒä¸­ï¼Œæ¨¡å‹å¯¹æœªè§ç‰©ä½“å’Œæœªè§æ‰‹çš„æŠ“æ¡è¡¨ç°å‡ºé«˜æˆåŠŸç‡å’Œå¿«é€Ÿæ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3616979acf7ddad5d9591bc530a808e" align="middle">
<img src="https://picx.zhimg.com/v2-c113eaea4a7b9e59a1751bba83f267b6" align="middle">
<img src="https://picx.zhimg.com/v2-be5ad6fc02934e1f147905d407af699d" align="middle">
<img src="https://picx.zhimg.com/v2-0e77d7b3dd5bff4cf1e70f514d485fdf" align="middle">
<img src="https://picx.zhimg.com/v2-cc6af866ee6488bd5e0bd466039122ad" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GLVD-Guided-Learned-Vertex-Descent"><a href="#GLVD-Guided-Learned-Vertex-Descent" class="headerlink" title="GLVD: Guided Learned Vertex Descent"></a>GLVD: Guided Learned Vertex Descent</h2><p><strong>Authors:Pol Caselles Rico, Francesc Moreno Noguer</strong></p>
<p>Existing 3D face modeling methods usually depend on 3D Morphable Models, which inherently constrain the representation capacity to fixed shape priors. Optimization-based approaches offer high-quality reconstructions but tend to be computationally expensive. In this work, we introduce GLVD, a hybrid method for 3D face reconstruction from few-shot images that extends Learned Vertex Descent (LVD) by integrating per-vertex neural field optimization with global structural guidance from dynamically predicted 3D keypoints. By incorporating relative spatial encoding, GLVD iteratively refines mesh vertices without requiring dense 3D supervision. This enables expressive and adaptable geometry reconstruction while maintaining computational efficiency. GLVD achieves state-of-the-art performance in single-view settings and remains highly competitive in multi-view scenarios, all while substantially reducing inference time. </p>
<blockquote>
<p>ç°æœ‰çš„3Däººè„¸å»ºæ¨¡æ–¹æ³•é€šå¸¸ä¾èµ–äº3Då¯å˜å½¢æ¨¡å‹ï¼Œè¿™å›ºæœ‰åœ°é™åˆ¶äº†å…¶è¡¨ç¤ºèƒ½åŠ›ï¼Œåªèƒ½è¡¨ç°ä¸ºå›ºå®šçš„å½¢çŠ¶å…ˆéªŒã€‚åŸºäºä¼˜åŒ–çš„æ–¹æ³•è™½ç„¶èƒ½æä¾›é«˜è´¨é‡çš„é‡å»ºï¼Œä½†è®¡ç®—æˆæœ¬å¾€å¾€å¾ˆé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†GLVDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä»å°‘é‡å›¾åƒè¿›è¡Œ3Däººè„¸é‡å»ºçš„æ··åˆæ–¹æ³•ã€‚GLVDæ‰©å±•äº†å­¦ä¹ é¡¶ç‚¹ä¸‹é™æ³•ï¼ˆLVDï¼‰ï¼Œé€šè¿‡æ•´åˆé¡¶ç‚¹ç¥ç»åœºä¼˜åŒ–ä¸åŠ¨æ€é¢„æµ‹çš„å…¨å±€ç»“æ„æŒ‡å¯¼çš„3Då…³é”®ç‚¹ï¼Œå®ç°äº†åœ¨å°‘é‡å›¾åƒä¸‹çš„3Däººè„¸é‡å»ºã€‚é€šè¿‡èå…¥ç›¸å¯¹ç©ºé—´ç¼–ç ï¼ŒGLVDèƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–ç½‘æ ¼é¡¶ç‚¹ï¼Œè€Œæ— éœ€å¯†é›†çš„3Dç›‘ç£ã€‚è¿™å®ç°äº†åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œè¿›è¡Œç”ŸåŠ¨ä¸”å¯é€‚åº”çš„å‡ ä½•é‡å»ºã€‚GLVDåœ¨å•è§†å›¾è®¾ç½®ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤šè§†å›¾åœºæ™¯ä¸­ä»å…·æœ‰é«˜åº¦ç«äº‰åŠ›ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†æ¨ç†æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06046v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå°‘é‡å›¾åƒçš„3Däººè„¸é‡å»ºçš„æ··åˆæ–¹æ³•GLVDï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é¡¶ç‚¹ç¥ç»åœºä¼˜åŒ–å’ŒåŠ¨æ€é¢„æµ‹çš„å…¨å±€ç»“æ„æŒ‡å¯¼ï¼Œå®ç°äº†é«˜æ•ˆã€çµæ´»çš„äººè„¸é‡å»ºã€‚GLVDé€šè¿‡å¼•å…¥ç›¸å¯¹ç©ºé—´ç¼–ç ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¯†é›†3Dç›‘ç£çš„æƒ…å†µä¸‹è¿­ä»£ä¼˜åŒ–ç½‘æ ¼é¡¶ç‚¹ï¼Œè¾¾åˆ°è¡¨æƒ…ä¸°å¯Œã€é€‚åº”æ€§å¼ºä¸”è®¡ç®—æ•ˆç‡é«˜çš„å‡ ä½•é‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLVDæ˜¯ä¸€ç§ç”¨äºä»å°‘é‡å›¾åƒè¿›è¡Œ3Däººè„¸é‡å»ºçš„æ··åˆæ–¹æ³•ã€‚</li>
<li>GLVDç»“åˆäº†é¡¶ç‚¹ç¥ç»åœºä¼˜åŒ–å’Œå…¨å±€ç»“æ„æŒ‡å¯¼ï¼Œæé«˜äº†é‡å»ºçš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>ç›¸å¯¹ç©ºé—´ç¼–ç çš„å¼•å…¥ä½¿å¾—GLVDèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¯†é›†3Dç›‘ç£çš„æƒ…å†µä¸‹è¿­ä»£ä¼˜åŒ–ç½‘æ ¼é¡¶ç‚¹ã€‚</li>
<li>GLVDåœ¨å•è§†å›¾å’Œå¤šè§†å›¾åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>GLVDå¤§å¹…é™ä½äº†æ¨ç†æ—¶é—´ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>GLVDåœ¨äººè„¸é‡å»ºä¸­å®ç°äº†è¡¨æƒ…ä¸°å¯Œã€é€‚åº”æ€§å¼ºçš„äººè„¸å‡ ä½•é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-422fe3bf11c3d18db720d515814b4765" align="middle">
<img src="https://picx.zhimg.com/v2-1501d4133c93e74ec2e61cd6995cb298" align="middle">
<img src="https://picx.zhimg.com/v2-1e19e6e9a095de611898a5f7b6aee396" align="middle">
<img src="https://picx.zhimg.com/v2-3a587d4cf8c0682268bd3e825fb0354e" align="middle">
<img src="https://picx.zhimg.com/v2-ac7b665bae4c95fc4808dddccd97527e" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Training-Free-Time-Series-Classification-via-In-Context-Reasoning-with-LLM-Agents"><a href="#Training-Free-Time-Series-Classification-via-In-Context-Reasoning-with-LLM-Agents" class="headerlink" title="Training-Free Time Series Classification via In-Context Reasoning with   LLM Agents"></a>Training-Free Time Series Classification via In-Context Reasoning with   LLM Agents</h2><p><strong>Authors:Songyuan Sui, Zihang Xu, Yu-Neng Chuang, Kwei-Herng Lai, Xia Hu</strong></p>
<p>Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at <a target="_blank" rel="noopener" href="https://github.com/SongyuanSui/FETATSC">https://github.com/SongyuanSui/FETATSC</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆTSCï¼‰æ¶µç›–äº†å¤šç§åº”ç”¨åœºæ™¯ï¼Œä½†æ ‡æ³¨æ•°æ®é€šå¸¸ç¨€ç¼ºï¼Œä½¿å¾—é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”ä¸å¤Ÿçµæ´»ã€‚æœ€è¿‘ä»¥æ¨ç†ä¸ºå¯¼å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£æ—¶é—´åºåˆ—æ¨¡å¼æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†çº¯é›¶æ ·æœ¬ä½¿ç”¨ä»ç„¶ä¸å¤Ÿç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†FETAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºèŒƒä¾‹çš„ä¸Šä¸‹æ–‡æ¨ç†çš„å®Œå…¨æ— è®­ç»ƒTSCå¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚FETAå°†å¤šå…ƒåºåˆ—åˆ†è§£æˆé€šé“çº§åˆ«çš„å­é—®é¢˜ï¼Œä¸ºæ¯ä¸ªé€šé“æ£€ç´¢å‡ ä¸ªç»“æ„ç›¸ä¼¼çš„æ ‡æ³¨èŒƒä¾‹ï¼Œå¹¶åˆ©ç”¨æ¨ç†LLMæ¥æ¯”è¾ƒæŸ¥è¯¢ä¸è¿™äº›èŒƒä¾‹ï¼Œä»¥è‡ªæˆ‘è¯„ä¼°çš„ä¿¡å¿ƒäº§ç”Ÿé€šé“çº§åˆ«çš„æ ‡ç­¾ï¼›ä¸€ä¸ªä¿¡å¿ƒåŠ æƒèšåˆå™¨éšåèåˆæ‰€æœ‰é€šé“å†³ç­–ã€‚è¿™ç§è®¾è®¡æ¶ˆé™¤äº†å¯¹é¢„è®­ç»ƒæˆ–å¾®è°ƒçš„éœ€æ±‚ï¼Œé€šè¿‡åˆ é™¤æ— å…³é€šé“å’Œæ§åˆ¶è¾“å…¥é•¿åº¦æ¥æé«˜æ•ˆç‡ï¼Œå¹¶é€šè¿‡èŒƒä¾‹æ¥åœ°å’Œä¿¡å¿ƒä¼°è®¡å¢å¼ºå¯è§£é‡Šæ€§ã€‚åœ¨ä¹ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„UEAæ•°æ®é›†ä¸Šï¼ŒFETAåœ¨å®Œå…¨æ— è®­ç»ƒçš„è®¾ç½®ä¸‹å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†å¤šä¸ªè®­ç»ƒè¿‡çš„åŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ä¸Šä¸‹æ–‡æ¨ç†æ¡†æ¶å¯ä»¥å°†LLMè½¬å˜ä¸ºæ— éœ€ä»»ä½•å‚æ•°è®­ç»ƒçš„ç«äº‰å‹ã€å³æ’å³ç”¨TSCæ±‚è§£å™¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SongyuanSui/FETATSC">https://github.com/SongyuanSui/FETATSC</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05950v1">PDF</a> 8 pages main content, 12 pages total including appendix, 1 figure</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹æ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆTSCï¼‰ä¸­æ ‡ç­¾æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºèŒƒä¾‹çš„ä¸Šä¸‹æ–‡æ¨ç†çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶FETAï¼Œç”¨äºæ— è®­ç»ƒTSCã€‚FETAå°†å¤šå…ƒåºåˆ—åˆ†è§£ä¸ºé€šé“çº§å­é—®é¢˜ï¼Œé€šè¿‡èŒƒä¾‹æ£€ç´¢å’Œæ¨ç†LLMè¿›è¡Œæ¯”è¾ƒï¼Œäº§ç”Ÿå…·æœ‰è‡ªæˆ‘è¯„ä¼°ç½®ä¿¡åº¦çš„é€šé“çº§æ ‡ç­¾ã€‚ä¸€ä¸ªåŸºäºç½®ä¿¡åº¦åŠ æƒçš„èšåˆå™¨èåˆæ‰€æœ‰é€šé“å†³ç­–ï¼Œæ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œæé«˜äº†æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚åœ¨UEAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFETAåœ¨å®Œå…¨æ— è®­ç»ƒè®¾ç½®ä¸‹å–å¾—äº†å¼ºå¤§çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆTSCï¼‰ä¸­æ ‡ç­¾æ•°æ®ç¨€ç¼ºï¼Œä»»åŠ¡ç‰¹å®šè®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”ä¸å¤Ÿçµæ´»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºèŒƒä¾‹çš„ä¸Šä¸‹æ–‡æ¨ç†çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶FETAï¼Œç”¨äºæ— è®­ç»ƒTSCã€‚</li>
<li>FETAé€šè¿‡åˆ†è§£å¤šå…ƒåºåˆ—ä¸ºé€šé“çº§å­é—®é¢˜ï¼Œæ£€ç´¢ç»“æ„ç›¸ä¼¼çš„æ ‡è®°èŒƒä¾‹ï¼Œåˆ©ç”¨æ¨ç†LLMè¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>äº§ç”Ÿé€šé“çº§æ ‡ç­¾å…·æœ‰è‡ªæˆ‘è¯„ä¼°çš„ç½®ä¿¡åº¦ã€‚</li>
<li>ç½®ä¿¡åº¦åŠ æƒèšåˆå™¨èåˆæ‰€æœ‰é€šé“å†³ç­–ã€‚</li>
<li>FETAæ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œæé«˜äº†æ•ˆç‡ï¼Œé€šè¿‡èŒƒä¾‹æ¥åœ°å’Œç½®ä¿¡åº¦ä¼°è®¡å¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨UEAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFETAåœ¨æ— è®­ç»ƒè®¾ç½®ä¸‹å–å¾—äº†å¼ºå¤§çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38b2e1614f21fcf8c4926fe7c4dabe0f" align="middle">
<img src="https://picx.zhimg.com/v2-79b2accfe7dcb236d9d5bf274ef05e99" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Syn-Diag-An-LLM-based-Synergistic-Framework-for-Generalizable-Few-shot-Fault-Diagnosis-on-the-Edge"><a href="#Syn-Diag-An-LLM-based-Synergistic-Framework-for-Generalizable-Few-shot-Fault-Diagnosis-on-the-Edge" class="headerlink" title="Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot   Fault Diagnosis on the Edge"></a>Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot   Fault Diagnosis on the Edge</h2><p><strong>Authors:Zijun Jia, Shuang Liang, Jinsong Yu</strong></p>
<p>Industrial fault diagnosis faces the dual challenges of data scarcity and the difficulty of deploying large AI models in resource-constrained environments. This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that leverages Large Language Models to overcome these limitations in few-shot fault diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic Synergy, which aligns signal features with the LLMâ€™s semantic space through cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically constructs contextual prompts to enhance diagnostic accuracy with limited samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create a lightweight, efficient edge model capable of online updates via a shared decision space. Extensive experiments on six datasets covering different CWRU and SEU working conditions show that Syn-Diag significantly outperforms existing methods, especially in 1-shot and cross-condition scenarios. The edge model achieves performance comparable to the cloud version while reducing model size by 83% and latency by 50%, offering a practical, robust, and deployable paradigm for modern intelligent diagnostics. </p>
<blockquote>
<p>å·¥ä¸šæ•…éšœæ£€æµ‹é¢ä¸´ç€æ•°æ®ç¨€ç¼ºå’Œåœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²å¤§å‹AIæ¨¡å‹çš„åŒé‡æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„äº‘è¾¹ååŒæ¡†æ¶Syn-Diagï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥å…‹æœå°æ ·ä¾‹æ•…éšœæ£€æµ‹ä¸­çš„è¿™äº›é™åˆ¶ã€‚Syn-Diagå»ºç«‹åœ¨ä¸‰å±‚æœºåˆ¶ä¸Šï¼š1ï¼‰è§†è§‰è¯­ä¹‰ååŒï¼Œé€šè¿‡è·¨æ¨¡æ€é¢„è®­ç»ƒå°†ä¿¡å·ç‰¹å¾ä¸LLMçš„è¯­ä¹‰ç©ºé—´å¯¹é½ï¼›2ï¼‰å†…å®¹æ„ŸçŸ¥æ¨ç†ï¼ŒåŠ¨æ€æ„å»ºä¸Šä¸‹æ–‡æç¤ºï¼Œä»¥æé«˜æœ‰é™æ ·æœ¬çš„è¯Šæ–­å‡†ç¡®æ€§ï¼›3ï¼‰äº‘è¾¹ååŒï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦åˆ›å»ºè½»é‡çº§ã€é«˜æ•ˆçš„è¾¹ç¼˜æ¨¡å‹ï¼Œé€šè¿‡å…±äº«å†³ç­–ç©ºé—´è¿›è¡Œåœ¨çº¿æ›´æ–°ã€‚åœ¨æ¶µç›–ä¸åŒCWRUå’ŒSEUå·¥ä½œæ¡ä»¶çš„å…­ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSyn-Diagæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨1-shotå’Œè·¨åœºæ™¯æ¡ä»¶ä¸‹ã€‚è¾¹ç¼˜æ¨¡å‹çš„æ€§èƒ½ä¸äº‘ç‰ˆæœ¬ç›¸å½“ï¼ŒåŒæ—¶æ¨¡å‹å¤§å°å‡å°‘äº†83%ï¼Œå»¶è¿Ÿé™ä½äº†50%ï¼Œä¸ºç°ä»£æ™ºèƒ½è¯Šæ–­æä¾›äº†å®ç”¨ã€ç¨³å¥å’Œå¯éƒ¨ç½²çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSyn-Diagçš„äº‘ç«¯ååŒè¯Šæ–­æ¡†æ¶ï¼Œç”¨äºè§£å†³å·¥ä¸šæ•…éšœè¯Šæ–­ä¸­çš„æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹éƒ¨ç½²å›°éš¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰è¯­ä¹‰ååŒã€å†…å®¹æ„ŸçŸ¥æ¨ç†å’Œäº‘ç«¯ååŒæœºåˆ¶ï¼Œå®ç°äº†åœ¨å°‘é‡æ ·æœ¬ä¸‹çš„é«˜æ•ˆæ•…éšœè¯Šæ–­ã€‚å®éªŒè¯æ˜ï¼ŒSyn-Diagåœ¨å¤šç§æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å•æ ·æœ¬å’Œè·¨æ¡ä»¶åœºæ™¯ä¸‹çš„è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚è¾¹ç¼˜æ¨¡å‹çš„æ€§èƒ½ä¸äº‘ç«¯ç‰ˆæœ¬ç›¸å½“ï¼ŒåŒæ—¶å‡å°äº†æ¨¡å‹ä½“ç§¯å’Œå»¶è¿Ÿï¼Œä¸ºç°ä»£æ™ºèƒ½è¯Šæ–­æä¾›äº†å®ç”¨ã€ç¨³å¥å’Œå¯éƒ¨ç½²çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Syn-Diagæ¡†æ¶è§£å†³äº†å·¥ä¸šæ•…éšœæ•°æ®ç¨€ç¼ºå’Œå¤§å‹AIæ¨¡å‹éƒ¨ç½²å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è§†è§‰è¯­ä¹‰ååŒï¼Œå°†ä¿¡å·ç‰¹å¾ä¸LLMè¯­ä¹‰ç©ºé—´å¯¹é½ã€‚</li>
<li>å†…å®¹æ„ŸçŸ¥æ¨ç†æœºåˆ¶æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ ·æœ¬çš„æœ‰é™æ€§ã€‚</li>
<li>äº‘ç«¯ååŒåˆ©ç”¨çŸ¥è¯†è’¸é¦åˆ›å»ºè½»é‡çº§è¾¹ç¼˜æ¨¡å‹ï¼Œæ”¯æŒåœ¨çº¿æ›´æ–°ã€‚</li>
<li>å®éªŒè¯æ˜Syn-Diagåœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å•æ ·æœ¬å’Œè·¨æ¡ä»¶åœºæ™¯ä¸‹ã€‚</li>
<li>è¾¹ç¼˜æ¨¡å‹æ€§èƒ½ä¸äº‘ç«¯ç‰ˆæœ¬ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°äº†æ¨¡å‹ä½“ç§¯å’Œå»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7356733d2d17e36a80ff2f1f8d36a27" align="middle">
<img src="https://picx.zhimg.com/v2-4f68dd520e83fd2f40c1ccc4c9dfd2d2" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="nnSAM2-nnUNet-Enhanced-One-Prompt-SAM2-for-Few-shot-Multi-Modality-Segmentation-and-Composition-Analysis-of-Lumbar-Paraspinal-Muscles"><a href="#nnSAM2-nnUNet-Enhanced-One-Prompt-SAM2-for-Few-shot-Multi-Modality-Segmentation-and-Composition-Analysis-of-Lumbar-Paraspinal-Muscles" class="headerlink" title="nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality   Segmentation and Composition Analysis of Lumbar Paraspinal Muscles"></a>nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality   Segmentation and Composition Analysis of Lumbar Paraspinal Muscles</h2><p><strong>Authors:Zhongyi Zhang, Julie A. Hides, Enrico De Martino, Abdul Joseph Fofanah, Gervase Tuxworth</strong></p>
<p>Purpose: To develop and validate No-New SAM2 (nnsam2) for few-shot segmentation of lumbar paraspinal muscles using only a single annotated slice per dataset, and to assess its statistical comparability with expert measurements across multi-sequence MRI and multi-protocol CT.   Methods: We retrospectively analyzed 1,219 scans (19,439 slices) from 762 participants across six datasets. Six slices (one per dataset) served as labeled examples, while the remaining 19,433 slices were used for testing. In this minimal-supervision setting, nnsam2 used single-slice SAM2 prompts to generate pseudo-labels, which were pooled across datasets and refined through three sequential, independent nnU-Net models. Segmentation performance was evaluated using the Dice similarity coefficient (DSC), and automated measurements-including muscle volume, fat ratio, and CT attenuation-were assessed with two one-sided tests (TOST) and intraclass correlation coefficients (ICC).   Results: nnsam2 outperformed vanilla SAM2, its medical variants, TotalSegmentator, and the leading few-shot method, achieving DSCs of 0.94-0.96 on MR images and 0.92-0.93 on CT. Automated and expert measurements were statistically equivalent for muscle volume (MRI&#x2F;CT), CT attenuation, and Dixon fat ratio (TOST, P &lt; 0.05), with consistently high ICCs (0.86-1.00).   Conclusion: We developed nnsam2, a state-of-the-art few-shot framework for multi-modality LPM segmentation, producing muscle volume (MRI&#x2F;CT), attenuation (CT), and fat ratio (Dixon MRI) measurements that were statistically comparable to expert references. Validated across multimodal, multicenter, and multinational cohorts, and released with open code and data, nnsam2 demonstrated high annotation efficiency, robust generalizability, and reproducibility. </p>
<blockquote>
<p>ç›®çš„ï¼šæ—¨åœ¨ä¸ºè…°æ¤æ—è‚Œè‚‰çš„å°‘æ ·æœ¬åˆ†å‰²å¼€å‘å¹¶éªŒè¯No-New SAM2ï¼ˆnnsam2ï¼‰ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æ¯ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªæ ‡æ³¨åˆ‡ç‰‡ï¼Œå¹¶è¯„ä¼°å…¶åœ¨å¤šåºåˆ—MRIå’Œå¤šåè®®CTä¸Šä¸ä¸“å®¶æµ‹é‡çš„ç»Ÿè®¡å¯æ¯”æ€§ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬å›é¡¾æ€§åˆ†æäº†6ä¸ªæ•°æ®é›†ä¸­çš„762åå‚ä¸è€…çš„1,219æ¬¡æ‰«æï¼ˆå…±19,439ä¸ªåˆ‡ç‰‡ï¼‰ã€‚å…¶ä¸­å…­ä¸ªåˆ‡ç‰‡ï¼ˆæ¯ä¸ªæ•°æ®é›†ä¸€ä¸ªï¼‰ä½œä¸ºæ ‡è®°ç¤ºä¾‹ï¼Œå…¶ä½™19,433ä¸ªåˆ‡ç‰‡ç”¨äºæµ‹è¯•ã€‚åœ¨è¿™ç§æœ€å°ç›‘ç£è®¾ç½®ä¸‹ï¼Œnnsam2ä½¿ç”¨å•ä¸ªåˆ‡ç‰‡çš„SAM2æç¤ºæ¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾è¢«æ±‡é›†åœ¨æ•°æ®é›†ä¸­ï¼Œå¹¶é€šè¿‡ä¸‰ä¸ªé¡ºåºç‹¬ç«‹çš„nnU-Netæ¨¡å‹è¿›è¡Œæ”¹è¿›ã€‚ä½¿ç”¨Diceç›¸ä¼¼æ€§ç³»æ•°ï¼ˆDSCï¼‰è¯„ä¼°åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨ä¸¤æ¬¡å•ä¾§æ£€éªŒï¼ˆTOSTï¼‰å’Œç»„å†…ç›¸å…³ç³»æ•°ï¼ˆICCï¼‰è¯„ä¼°è‡ªåŠ¨åŒ–æµ‹é‡ï¼ŒåŒ…æ‹¬è‚Œè‚‰ä½“ç§¯ã€è„‚è‚ªæ¯”å’ŒCTè¡°å‡ã€‚ç»“æœï¼šnnsam2çš„è¡¨ç°ä¼˜äºåŸºç¡€SAM2ã€å…¶åŒ»å­¦å˜ä½“ã€TotalSegmentatorä»¥åŠé¢†å…ˆçš„å°‘æ ·æœ¬æ–¹æ³•ï¼Œåœ¨MRå›¾åƒä¸Šè¾¾åˆ°0.94-0.96çš„DSCï¼Œåœ¨CTä¸Šè¾¾åˆ°0.92-0.93çš„DSCã€‚è‡ªåŠ¨åŒ–å’Œä¸“å®¶æµ‹é‡çš„è‚Œè‚‰ä½“ç§¯ï¼ˆMRI&#x2F;CTï¼‰ã€CTè¡°å‡å’ŒDixonè„‚è‚ªæ¯”ï¼ˆTOSTï¼ŒP &lt; 0.05ï¼‰åœ¨ç»Ÿè®¡ä¸Šæ˜¯ç›¸å½“çš„ï¼Œå¹¶ä¸”å…·æœ‰å§‹ç»ˆè¾ƒé«˜çš„ICCï¼ˆ0.86-1.00ï¼‰ã€‚ç»“è®ºï¼šæˆ‘ä»¬å¼€å‘äº†æœ€å…ˆè¿›çš„å°‘æ ·æœ¬æ¡†æ¶nnsam2ï¼Œç”¨äºå¤šæ¨¡æ€LPMåˆ†å‰²ï¼Œå¯ç”Ÿæˆä¸ä¸“å®¶å‚è€ƒç›¸æ¯”å…·æœ‰ç»Ÿè®¡å¯æ¯”æ€§çš„è‚Œè‚‰ä½“ç§¯ï¼ˆMRI&#x2F;CTï¼‰ã€è¡°å‡ï¼ˆCTï¼‰å’Œè„‚è‚ªæ¯”ï¼ˆDixon MRIï¼‰æµ‹é‡å€¼ã€‚ç»è¿‡è·¨å¤šæ¨¡æ€ã€å¤šä¸­å¿ƒå’Œè·¨å›½é˜Ÿåˆ—çš„éªŒè¯ï¼Œå¹¶ä»¥å¼€æ”¾ä»£ç å’Œæ•°æ®å‘å¸ƒï¼Œnnsam2æ˜¾ç¤ºå‡ºé«˜çš„æ ‡æ³¨æ•ˆç‡ã€ç¨³å¥çš„é€šç”¨æ€§å’Œå¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05555v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>nnsam2æ–¹æ³•ç”¨äºåŸºäºå°‘é‡æ ‡æ³¨æ•°æ®çš„è…°æ¤æ—è‚Œè‚‰åˆ†å‰²ï¼Œä»…ä½¿ç”¨æ¯ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªæ ‡æ³¨åˆ‡ç‰‡ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§åºåˆ—MRIå’Œå¤šåè®®CTä¸Šçš„è¡¨ç°ä¸ä¸“å®¶æµ‹é‡å…·æœ‰ç»Ÿè®¡å¯æ¯”æ€§ã€‚é€šè¿‡å¯¹å¤§é‡æ‰«ææ•°æ®çš„å›é¡¾æ€§åˆ†æå’Œç‹¬ç«‹æ¨¡å‹éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºnnsam2çš„åˆ†å‰²æ€§èƒ½ä¼˜ç§€ï¼Œè‡ªåŠ¨åŒ–æµ‹é‡ç»“æœä¸ä¸“å®¶æµ‹é‡ç»“æœç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>nnsam2æ˜¯ä¸€ç§ç”¨äºè…°æ¤æ—è‚Œè‚‰å°‘æ ·æœ¬åˆ†å‰²çš„æ–¹æ³•ï¼Œä»…éœ€è¦æ¯ä¸ªæ•°æ®é›†çš„ä¸€ä¸ªæ ‡æ³¨åˆ‡ç‰‡ã€‚</li>
<li>nnsam2åœ¨å¤šç§åºåˆ—MRIå’Œå¤šåè®®CTä¸Šå…·æœ‰ä¼˜ç§€çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>nnsam2ä½¿ç”¨å•åˆ‡ç‰‡SAM2æç¤ºç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶è·¨æ•°æ®é›†æ•´åˆå’Œç²¾ç‚¼ã€‚</li>
<li>é€šè¿‡Dice similarity coefficient (DSC)è¯„ä¼°åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è‡ªåŠ¨åŒ–æµ‹é‡ç»“æœä¸ä¸“å®¶æµ‹é‡ç»“æœå…·æœ‰ç»Ÿè®¡ç­‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è‚Œè‚‰ä½“ç§¯ã€è„‚è‚ªæ¯”å’ŒCTè¡°å‡ã€‚</li>
<li>nnsam2å…·æœ‰é«˜çš„æ ‡æ³¨æ•ˆç‡ã€ç¨³å¥çš„é€šç”¨æ€§å’Œå¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e3d78d43314b93194234bf3cb4c9483" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Attention-Enhanced-Prototypical-Learning-for-Few-Shot-Infrastructure-Defect-Segmentation"><a href="#Attention-Enhanced-Prototypical-Learning-for-Few-Shot-Infrastructure-Defect-Segmentation" class="headerlink" title="Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure   Defect Segmentation"></a>Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure   Defect Segmentation</h2><p><strong>Authors:Christina Thrainer, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Christian Guetl, Steven Sloan, Kendall N. Niles, Ken Pathak</strong></p>
<p>Few-shot semantic segmentation is vital for deep learning-based infrastructure inspection applications, where labeled training examples are scarce and expensive. Although existing deep learning frameworks perform well, the need for extensive labeled datasets and the inability to learn new defect categories with little data are problematic. We present our Enhanced Feature Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert and sewer defect categories using a prototypical learning framework. Our approach has three main contributions: (1) adaptive E-FPN encoder using InceptionSepConv blocks and depth-wise separable convolutions for efficient multi-scale feature extraction; (2) prototypical learning with masked average pooling for powerful prototype generation from small support examples; and (3) attention-based feature representation through global self-attention, local self-attention and cross-attention. Comprehensive experimentation on challenging infrastructure inspection datasets illustrates that the method achieves excellent few-shot performance, with the best configuration being 8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way classification testing. The self-attention method had the most significant performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over baselines. Our framework addresses the critical need to rapidly respond to new defect types in infrastructure inspection systems with limited new training data that lead to more efficient and economical maintenance plans for critical infrastructure systems. </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²å¯¹äºåŸºäºæ·±åº¦å­¦ä¹ çš„è®¾æ–½æ£€æµ‹åº”ç”¨è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™äº›åº”ç”¨çš„è®­ç»ƒæ ·æœ¬æ—¢ç¨€ç¼ºåˆæ˜‚è´µã€‚å°½ç®¡ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶è¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„éœ€æ±‚ä»¥åŠæ— æ³•ç”¨å°‘é‡æ•°æ®å­¦ä¹ æ–°ç¼ºé™·ç±»åˆ«çš„é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬æå‡ºäº†å¢å¼ºå‹ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆE-FPNï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨åŸå‹å­¦ä¹ æ¡†æ¶å¯¹æ¶µæ´å’Œä¸‹æ°´é“ç¼ºé™·ç±»åˆ«è¿›è¡Œå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦æœ‰ä¸‰ä¸ªè´¡çŒ®ï¼šï¼ˆ1ï¼‰ä½¿ç”¨InceptionSepConvå—å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„è‡ªé€‚åº”E-FPNç¼–ç å™¨ï¼Œè¿›è¡Œé«˜æ•ˆçš„å¤šå°ºåº¦ç‰¹å¾æå–ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨æ©æ¨¡å¹³å‡æ± åŒ–çš„åŸå‹å­¦ä¹ ï¼Œä»å°é‡æ”¯æŒæ ·æœ¬ä¸­ç”Ÿæˆå¼ºå¤§çš„åŸå‹ï¼›ï¼ˆ3ï¼‰åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾è¡¨ç¤ºï¼Œé€šè¿‡å…¨å±€è‡ªæ³¨æ„åŠ›ã€å±€éƒ¨è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ã€‚å…·æœ‰æŒ‘æˆ˜æ€§çš„è®¾æ–½æ£€æµ‹æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å‡ºè‰²çš„å°‘æ ·æœ¬æ€§èƒ½ï¼Œæœ€ä½³é…ç½®ä¸º8è·¯5æ¬¡è®­ç»ƒçš„é…ç½®ï¼Œåœ¨2è·¯åˆ†ç±»æµ‹è¯•ä¸­è¾¾åˆ°82.55%çš„F1åˆ†æ•°å’Œ72.26%çš„mIoUã€‚è‡ªæ³¨æ„åŠ›æ–¹æ³•å…·æœ‰æœ€æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸æ¯”åŸºçº¿æé«˜äº†2.57%çš„F1åˆ†æ•°å’Œ2.9%çš„mIoUã€‚æˆ‘ä»¬çš„æ¡†æ¶è§£å†³äº†è®¾æ–½æ£€æµ‹ç³»ç»Ÿä¸­å¯¹æœ‰é™æ–°è®­ç»ƒæ•°æ®å¿«é€Ÿå“åº”æ–°ç¼ºé™·ç±»å‹çš„è¿«åˆ‡éœ€æ±‚ï¼Œä»è€Œä¸ºå…³é”®è®¾æ–½ç³»ç»Ÿåˆ¶å®šæ›´é«˜æ•ˆã€æ›´ç»æµçš„ç»´æŠ¤è®¡åˆ’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05266v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ·±åº¦å­¦ä¹ åœ¨åŸºç¡€è®¾æ–½æ£€æµ‹ä¸­åº”ç”¨çš„few-shotè¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œæå‡ºä¸€ç§å¢å¼ºå‹ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆE-FPNï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åŸå‹å­¦ä¹ æ¡†æ¶å¯¹æ¡¥æ¶µå’Œæ’æ°´æ²Ÿç¼ºé™·ç±»åˆ«è¿›è¡Œfew-shotè¯­ä¹‰åˆ†å‰²ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬è‡ªé€‚åº”E-FPNç¼–ç å™¨ã€åŸå‹å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºç¡€è®¾æ–½æ£€æµ‹æ•°æ®é›†ä¸Šå–å¾—äº†è‰¯å¥½çš„few-shotæ€§èƒ½ï¼Œä¸”è‡ªæ³¨æ„åŠ›æ–¹æ³•å…·æœ‰æœ€ä½³è¡¨ç°ï¼Œæ˜¾è‘—æé«˜äº†åŸºçº¿æ€§èƒ½ã€‚è¯¥æ¡†æ¶æ»¡è¶³äº†åœ¨æœ‰é™æ–°è®­ç»ƒæ•°æ®ä¸‹å¯¹åŸºç¡€è®¾æ–½æ£€æµ‹ç³»ç»Ÿä¸­æ–°ç¼ºé™·ç±»å‹çš„å¿«é€Ÿå“åº”éœ€æ±‚ï¼Œæœ‰åŠ©äºæé«˜å…³é”®åŸºç¡€è®¾æ–½ç³»ç»Ÿçš„ç»´æŠ¤æ•ˆç‡å’Œç»æµæ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shotè¯­ä¹‰åˆ†å‰²åœ¨æ·±åº¦å­¦ä¹ ä¸ºåŸºç¡€è®¾æ–½æ£€æµ‹åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹æ ‡æ³¨è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶å­˜åœ¨å¯¹æ–°ç¼ºé™·ç±»åˆ«å­¦ä¹ ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„E-FPNæ¡†æ¶åŒ…æ‹¬è‡ªé€‚åº”E-FPNç¼–ç å™¨ã€åŸå‹å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶ä¸‰ä¸ªä¸»è¦è´¡çŒ®ã€‚</li>
<li>è‡ªé€‚åº”E-FPNç¼–ç å™¨é€šè¿‡InceptionSepConvå—å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯å®ç°é«˜æ•ˆå¤šå°ºåº¦ç‰¹å¾æå–ã€‚</li>
<li>åŸå‹å­¦ä¹ é‡‡ç”¨æ©ç å¹³å‡æ± åŒ–ç”Ÿæˆå¼ºå¤§åŸå‹ï¼ŒåŸºäºå°è§„æ¨¡æ”¯æŒæ ·æœ¬ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶åŒ…æ‹¬å…¨å±€è‡ªæ³¨æ„åŠ›ã€å±€éƒ¨è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ï¼Œæœ‰åŠ©äºæå‡ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5827881180b5fc35b65b2860aedd5e58" align="middle">
<img src="https://picx.zhimg.com/v2-12f64b9c91e8adecbc7b5085decf48eb" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HyperVLA-Efficient-Inference-in-Vision-Language-Action-Models-via-Hypernetworks"><a href="#HyperVLA-Efficient-Inference-in-Vision-Language-Action-Models-via-Hypernetworks" class="headerlink" title="HyperVLA: Efficient Inference in Vision-Language-Action Models via   Hypernetworks"></a>HyperVLA: Efficient Inference in Vision-Language-Action Models via   Hypernetworks</h2><p><strong>Authors:Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson</strong></p>
<p>Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MasterXiong/HyperVLA">https://github.com/MasterXiong/HyperVLA</a> </p>
<blockquote>
<p>åŸºäºå…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„è¯­è¨€å’Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶åœ¨å¤§è§„æ¨¡æœºå™¨äººæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹æœ€è¿‘ä½œä¸ºå­¦ä¹ é€šç”¨æœºå™¨äººç­–ç•¥çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰VLAçš„ä¸€ä¸ªå…³é”®ç¼ºç‚¹æ˜¯æ¨ç†æˆæœ¬æé«˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†HyperVLAè§£å†³æ–¹æ¡ˆã€‚ä¸ç°æœ‰çš„åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½ä¼šæ¿€æ´»æ•´ä¸ªæ¨¡å‹çš„å•ä¸€VLAä¸åŒï¼ŒHyperVLAé‡‡ç”¨äº†ä¸€ç§åŸºäºè¶…ç½‘ç»œï¼ˆHNï¼‰çš„æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨æ¨ç†æ—¶åªæ¿€æ´»ä¸€å°éƒ¨åˆ†ç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼ŒåŒæ—¶åœ¨è®­ç»ƒæ—¶ä»ä¿ç•™å®¹çº³å¤šç§å¤šä»»åŠ¡è¡Œä¸ºæ‰€éœ€çš„é«˜æ¨¡å‹å®¹é‡ã€‚æˆåŠŸè®­ç»ƒåŸºäºHNçš„VLAå¹¶ä¸å®¹æ˜“ï¼Œå› æ­¤HyperVLAåŒ…å«ä¸€äº›å…³é”®ç®—æ³•è®¾è®¡åŠŸèƒ½æ¥æé«˜å…¶æ€§èƒ½ï¼ŒåŒ…æ‹¬é€‚å½“åˆ©ç”¨ç°æœ‰è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€HNå½’ä¸€åŒ–ä»¥åŠåŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚ä¸å•ä¸€VLAç›¸æ¯”ï¼ŒHyperVLAåœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œå°‘é‡é€‚åº”æ–¹é¢è¾¾åˆ°äº†ç›¸ä¼¼ç”šè‡³æ›´é«˜çš„æˆåŠŸç‡ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†æ¨ç†æˆæœ¬ã€‚ä¸æœ€å…ˆè¿›çš„VLAæ¨¡å‹OpenVLAç›¸æ¯”ï¼ŒHyperVLAåœ¨æµ‹è¯•æ—¶å‡å°‘äº†è¢«æ¿€æ´»å‚æ•°çš„æ•°é‡ï¼ˆå‡å°‘90å€ï¼‰ï¼Œå¹¶åŠ å¿«äº†æ¨ç†é€Ÿåº¦ï¼ˆåŠ é€Ÿ120å€ï¼‰ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/MasterXiong/HyperVLA%E3%80%82">https://github.com/MasterXiong/HyperVLAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04898v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè¯­è¨€å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„é€šç”¨æœºå™¨äººç­–ç•¥å­¦ä¹ æ–°æ–¹æ³•â€”â€”HyperVLAã€‚è¯¥æ–¹æ³•ä½¿ç”¨è¶…ç½‘ç»œï¼ˆHNï¼‰æ¶æ„ï¼Œåœ¨æ¨ç†æ—¶ä»…æ¿€æ´»å°å‹çš„ç‰¹å®šä»»åŠ¡ç­–ç•¥ï¼Œé™ä½äº†æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™äº†åœ¨è®­ç»ƒæœŸé—´é€‚åº”å¤šæ ·å¤šä»»åŠ¡è¡Œä¸ºæ‰€éœ€çš„é«˜æ¨¡å‹å®¹é‡ã€‚HyperVLAè¿˜åŒ…æ‹¬ä¸€äº›å…³é”®ç®—æ³•è®¾è®¡ç‰¹ç‚¹ï¼Œå¦‚åˆ©ç”¨ç°æœ‰è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€HNå½’ä¸€åŒ–å’ŒåŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„å•ä¸€ä½“VLAsï¼ŒHyperVLAåœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œå°‘é‡é€‚åº”æ–¹é¢å–å¾—äº†ç›¸ä¼¼ç”šè‡³æ›´é«˜çš„æˆåŠŸç‡ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ã€‚ä¸ç°æœ‰çš„é¡¶å°–VLAæ¨¡å‹OpenVLAç›¸æ¯”ï¼ŒHyperVLAåœ¨æµ‹è¯•æ—¶æ¿€æ´»çš„å‚æ•°æ•°é‡å‡å°‘äº†90å€ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†120å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyperVLAæ˜¯ä¸€ç§åŸºäºè¯­è¨€å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„æœºå™¨äººç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰VLAsé«˜æ¨ç†æˆæœ¬çš„é—®é¢˜ã€‚</li>
<li>HyperVLAé‡‡ç”¨è¶…ç½‘ç»œï¼ˆHNï¼‰æ¶æ„ï¼Œæ¨ç†æ—¶ä»…æ¿€æ´»å°å‹çš„ä»»åŠ¡ç‰¹å®šç­–ç•¥ã€‚</li>
<li>HyperVLAåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ç•™é«˜æ¨¡å‹å®¹é‡ä»¥é€‚åº”å¤šç§ä»»åŠ¡è¡Œä¸ºã€‚</li>
<li>HyperVLAåˆ©ç”¨ç°æœ‰è§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åŒ…æ‹¬HNå½’ä¸€åŒ–å’ŒåŠ¨ä½œç”Ÿæˆç­–ç•¥ç­‰å…³é”®ç®—æ³•è®¾è®¡ç‰¹ç‚¹ã€‚</li>
<li>ä¸ä¼ ç»Ÿå•ä¸€ä½“VLAsç›¸æ¯”ï¼ŒHyperVLAåœ¨æ³›åŒ–å’Œé€‚åº”æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>HyperVLAè¾ƒç°æœ‰çš„é¡¶å°–VLAæ¨¡å‹OpenVLAï¼Œæ˜¾è‘—å‡å°‘äº†æµ‹è¯•æ—¶æ¿€æ´»çš„å‚æ•°æ•°é‡å’Œæ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eda2b8956b83118d080c97b92cef1bb1" align="middle">
<img src="https://picx.zhimg.com/v2-f9fb7d5e0c7afec9026cf85a267cae41" align="middle">
<img src="https://picx.zhimg.com/v2-0b89a545dac868205e7366aeb1df9078" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAFA-SNN-Sparsity-Aware-On-Device-Few-Shot-Class-Incremental-Learning-with-Fast-Adaptive-Structure-of-Spiking-Neural-Network"><a href="#SAFA-SNN-Sparsity-Aware-On-Device-Few-Shot-Class-Incremental-Learning-with-Fast-Adaptive-Structure-of-Spiking-Neural-Network" class="headerlink" title="SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning   with Fast-Adaptive Structure of Spiking Neural Network"></a>SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning   with Fast-Adaptive Structure of Spiking Neural Network</h2><p><strong>Authors:Huijing Zhang, Muyang Cao, Linshan Jiang, Xin Du, Di Yu, Changze Lv, Shuiguang Deng</strong></p>
<p>Continuous learning of novel classes is crucial for edge devices to preserve data privacy and maintain reliable performance in dynamic environments. However, the scenario becomes particularly challenging when data samples are insufficient, requiring on-device few-shot class-incremental learning (FSCIL) to maintain consistent model performance. Although existing work has explored parameter-efficient FSCIL frameworks based on artificial neural networks (ANNs), their deployment is still fundamentally constrained by limited device resources. Inspired by neural mechanisms, Spiking neural networks (SNNs) process spatiotemporal information efficiently, offering lower energy consumption, greater biological plausibility, and compatibility with neuromorphic hardware than ANNs. In this work, we present an SNN-based method for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We first propose sparsity-conditioned neuronal dynamics, in which most neurons remain stable while a subset stays active, thereby mitigating catastrophic forgetting. To further cope with spike non-differentiability in gradient estimation, we employ zeroth-order optimization. Moreover, during incremental learning sessions, we enhance the discriminability of new classes through subspace projection, which alleviates overfitting to novel classes. Extensive experiments conducted on two standard benchmark datasets (CIFAR100 and Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture, and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods, specifically achieving at least 4.01% improvement at the last incremental session on Mini-ImageNet and 20% lower energy cost over baseline methods with practical implementation. </p>
<blockquote>
<p>æŒç»­å­¦ä¹ æ–°ç±»åˆ«å¯¹äºè¾¹ç¼˜è®¾å¤‡åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¿æŒæ•°æ®éšç§å’Œå¯é æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“æ•°æ®æ ·æœ¬ä¸è¶³æ—¶ï¼Œæƒ…å†µå˜å¾—å°¤ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦è®¾å¤‡ä¸Šçš„å°æ ·æœ¬ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ¥ç»´æŒæ¨¡å‹çš„æŒç»­æ€§èƒ½ã€‚è™½ç„¶ç°æœ‰å·¥ä½œå·²ç»æ¢ç´¢äº†åŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„å‚æ•°æœ‰æ•ˆFSCILæ¡†æ¶ï¼Œä½†å…¶éƒ¨ç½²ä»ç„¶å—åˆ°æœ‰é™è®¾å¤‡èµ„æºçš„æ ¹æœ¬åˆ¶çº¦ã€‚å—ç¥ç»æœºåˆ¶çš„å¯å‘ï¼Œè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰èƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ—¶ç©ºä¿¡æ¯ï¼Œä¸ANNç›¸æ¯”ï¼Œå…·æœ‰æ›´ä½çš„èƒ½è€—ã€æ›´å¤§çš„ç”Ÿç‰©åˆç†æ€§å’Œä¸ç¥ç»å½¢æ€ç¡¬ä»¶çš„å…¼å®¹æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºSNNçš„è®¾å¤‡ä¸ŠFSCILæ–¹æ³•ï¼Œå³ç¨€ç–æ„ŸçŸ¥å’Œå¿«é€Ÿè‡ªé€‚åº”SNNï¼ˆSAFA-SNNï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºç¨€ç–æ¡ä»¶ç¥ç»å…ƒåŠ¨æ€ï¼Œå…¶ä¸­å¤§å¤šæ•°ç¥ç»å…ƒä¿æŒç¨³å®šï¼Œè€Œä¸€å°éƒ¨åˆ†ç¥ç»å…ƒä¿æŒæ´»è·ƒï¼Œä»è€Œå‡è½»ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥åº”å¯¹æ¢¯åº¦ä¼°è®¡ä¸­çš„è„‰å†²ä¸å¯å¾®æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨é›¶é˜¶ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œåœ¨å¢é‡å­¦ä¹ æœŸé—´ï¼Œæˆ‘ä»¬é€šè¿‡å­ç©ºé—´æŠ•å½±å¢å¼ºæ–°ç±»åˆ«çš„è¾¨åˆ«åŠ›ï¼Œä»è€Œå‡è½»å¯¹æ–°ç±»åˆ«çš„è¿‡åº¦æ‹Ÿåˆã€‚åœ¨CIFAR100å’ŒMini-ImageNetä¸¤ä¸ªæ ‡å‡†åŸºå‡†æ•°æ®é›†ä»¥åŠCIFAR-10-DVSã€DVS128gestureå’ŒN-Caltech1ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAFA-SNNä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨Mini-ImageNetçš„æœ€åä¸€ä¸ªå¢é‡ä¼šè¯ä¸Šè‡³å°‘æé«˜äº†4.01%ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­å®ç°äº†æ¯”åŸºå‡†æ–¹æ³•ä½20%çš„èƒ½è€—æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè„‰å†²ç¥ç»ç½‘ç»œçš„é«˜æ•ˆåœ¨çº¿å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ã€‚ä¸ºè§£å†³è¾¹ç¼˜è®¾å¤‡åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¿æŠ¤æ•°æ®éšç§å¹¶ä¿æŒå¯é æ€§èƒ½çš„é—®é¢˜ï¼Œç ”ç©¶æå‡ºä¸€ç§åŸºäºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰çš„åœ¨çº¿å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ æ–¹æ³•â€”â€”ç¨€ç–æ„ŸçŸ¥å¿«é€Ÿè‡ªé€‚åº”SNNï¼ˆSAFA-SNNï¼‰ã€‚é€šè¿‡ç¨€ç–æ€§æ¡ä»¶ç¥ç»å…ƒåŠ¨æ€ã€é›¶é˜¶ä¼˜åŒ–å’Œå­ç©ºé—´æŠ•å½±ç­‰æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ•ˆçš„å¢é‡å­¦ä¹ å’Œè‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAFA-SNNåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¾¹ç¼˜è®¾å¤‡åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡ŒæŒç»­çš„æ–°å‹ç±»åˆ«å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>æ•°æ®æ ·æœ¬ä¸è¶³æ—¶ï¼Œéœ€è¦é‡‡ç”¨åŸºäºè„‰å†²ç¥ç»ç½‘ç»œçš„å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ æ–¹æ³•ï¼ˆFSCILï¼‰ã€‚</li>
<li>SAFA-SNNæ–¹æ³•é€šè¿‡ç¨€ç–æ€§æ¡ä»¶ç¥ç»å…ƒåŠ¨æ€å‡è½»ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨é›¶é˜¶ä¼˜åŒ–åº”å¯¹è„‰å†²çš„éå¯å¾®æ€§ï¼Œåœ¨æ¢¯åº¦ä¼°è®¡ä¸­è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å­ç©ºé—´æŠ•å½±æŠ€æœ¯ç”¨äºæé«˜æ–°ç±»åˆ«çš„å¯åˆ†è¾¨æ€§ï¼Œå‡è½»å¯¹æ–°ç±»åˆ«çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜SAFA-SNNåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e3cfec71f93594b0960135c27fdd93b" align="middle">
<img src="https://picx.zhimg.com/v2-a96a8d913428391dea974e84394717aa" align="middle">
<img src="https://picx.zhimg.com/v2-c7afefb38335b5d16d081f9f1abe461f" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»ä»…æœ‰çš„å‡ ä¸ªæ ‡è®°æ”¯æŒæ ·æœ¬ä¸­è¯†åˆ«å‡ºæ–°æ¦‚å¿µã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡èå…¥é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥å¢å¼ºæ”¯æŒç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨å®é™…å®ä¾‹ä¸­çš„å®šä½ï¼Œå®ƒä»¬ä»ç„¶ä¼šé­å—ä¸è§†è§‰è¯æ®ç›¸çŸ›ç›¾çš„å¹»æƒ³è¯­ä¹‰çš„å›°æ‰°ï¼Œå¯¼è‡´äº§ç”Ÿå˜ˆæ‚çš„æŒ‡å¯¼å’Œæ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè§†è§‰ä¸æ–‡æœ¬æ¡¥æ¥çš„å°‘é‡å­¦ä¹ ï¼ˆVT-FSLï¼‰ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ç²¾ç¡®è·¨æ¨¡æ€æç¤ºï¼Œè¿™äº›æç¤ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ”¯æŒå›¾åƒï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½æ— ç¼é›†æˆå®ƒä»¬ã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚å…·ä½“è€Œè¨€ï¼ŒCIPæ ¹æ®ç±»åå’Œè¾…åŠ©å›¾åƒå¯¹LLMè¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥åœ¨å•ä¸ªç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ç”Ÿæˆç²¾ç¡®çš„ç±»æè¿°ã€‚è¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚è¿™äº›æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºè¡¥å……çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„è¾…åŠ©æ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAé€šè¿‡æœ€å°åŒ–å®ƒä»¬æ‰€è·¨è¶Šçš„3ç»´å¹³è¡Œå››è¾¹å½¢çš„æ ¸åŒ–ä½“ç§¯æ¥è”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰äº†æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°äº†ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡å¼é›†æˆã€‚æ‰€æå‡ºçš„VT-FSLæ–¹æ³•åœ¨åŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦å°‘é‡å­¦ä¹ åœºæ™¯åœ¨å†…çš„åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/peacelwh/VT-FSLæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å°æ ·å­¦ä¹ ï¼ˆFSLï¼‰çš„ä¸€ç§æ–°æ–¹æ³•â€”â€”ç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å°æ ·å­¦ä¹ æ¡†æ¶ï¼ˆVT-FSLï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºå’ŒåŸºäºæ”¯æŒå›¾åƒçš„è¿­ä»£æç¤ºï¼Œæ— ç¼é›†æˆå®ƒä»¬é€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½ã€‚å®ƒä¸»è¦åŒ…æ‹¬è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ã€‚CIPä»¥ç±»åå’Œå›¾åƒä¸ºæ¡ä»¶æ¥ç”Ÿæˆç²¾ç¡®ç±»æè¿°ï¼Œè¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°å‹è¯­ä¹‰çš„ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚åˆæˆå›¾åƒä½œä¸ºè¾…åŠ©è§†è§‰æç¤ºï¼Œä¸æè¿°å…±åŒä¸ºæœ‰é™çš„æ”¯æŒæ•°æ®æä¾›äº†é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§è¡¥å¿ã€‚CGAåˆ™é€šè¿‡æœ€å°åŒ–è·¨è¶Šçš„ä¸‰ç»´å¹³è¡Œå››è¾¹å½¢çš„æ ¸åŒ–ä½“ç§¯æ¥è”åˆå¯¹é½èåˆåçš„æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰äº†æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°äº†ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚VT-FSLæ–¹æ³•åœ¨åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æœ€æ–°æ€§èƒ½æ ‡å‡†ï¼ŒåŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç²¾ç»†åˆ†ç±»å°æ ·å­¦ä¹ åœºæ™¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Few-shot learning (FSL)æ—¨åœ¨ä»å°‘é‡æ ‡è®°æ ·æœ¬ä¸­è¯†åˆ«æ–°æ¦‚å¿µã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶é€šè¿‡å¢åŠ é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥å¢å¼ºæ”¯æŒç‰¹å¾ã€‚</li>
<li>ç¼ºä¹åŸºäºå®é™…å®ä¾‹çš„æ¥åœ°ä¼šå¯¼è‡´è¯­ä¹‰ä¸Šçš„è™šæ„ç°è±¡å’Œè§†è§‰è¯æ®ç›¸çŸ›ç›¾ï¼Œé€ æˆæ˜‚è´µçš„ä¿®æ­£å’Œå˜ˆæ‚çš„å¼•å¯¼ã€‚</li>
<li>æå‡ºçš„VT-FSLæ¡†æ¶ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ„å»ºäº†ç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºå’ŒåŸºäºæ”¯æŒå›¾åƒçš„è¿­ä»£æç¤ºã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚</li>
<li>CIPé€šè¿‡ç”Ÿæˆç²¾ç¡®ç±»æè¿°æ¥ä¸°å¯Œå¯¹æ–°å‹è¯­ä¹‰çš„ç†è§£ï¼Œå¹¶å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚åˆæˆå›¾åƒä½œä¸ºè¾…åŠ©è§†è§‰æç¤ºï¼Œæä¾›äº†é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§çš„è¡¥å¿ã€‚</li>
<li>CGAé€šè¿‡æœ€å°åŒ–è·¨è¶Šçš„ä¸‰ç»´å¹³è¡Œå››è¾¹å½¢çš„æ ¸åŒ–ä½“ç§¯æ¥è”åˆå¯¹é½æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºï¼Œå®ç°äº†ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d52355fbe28b9263d60b1f090617a30" align="middle">
<img src="https://picx.zhimg.com/v2-9af705fa082c84d4e881514f8fff464d" align="middle">
<img src="https://picx.zhimg.com/v2-fda6a8e6593952cbc22784b0fba62ed6" align="middle">
<img src="https://picx.zhimg.com/v2-76bea1641940b6f14a37bc0429de3487" align="middle">
<img src="https://picx.zhimg.com/v2-1a09d3087398134cf568e19fcf367318" align="middle">
<img src="https://picx.zhimg.com/v2-b2dfdfdfab44386fe2ba27acf9d36e05" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers"><a href="#Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers" class="headerlink" title="Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers"></a>Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers</h2><p><strong>Authors:Omar Naim, Swarnadeep Bhar, JÃ©rÃ´me Bolte, Nicholas Asher</strong></p>
<p>While Large Language modelsâ€™ abilities for in-context learning (ICL) have drawn much attention, we examine some of its limitations on semantic tasks involving quantifiers like â€œallâ€ and â€œsomeâ€, as well as on tasks with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these limitations. We propose scaled signed averaging (SSA), a novel alternative to Softmax to mitigate these problems. We show that SSA significantly improves performance on our ICL tasks. In addition, SSA outperforms transformer models with Softmax on several early learning NLP benchmarks and linguistic probing tasks on zero and few-shot settings. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹é¢çš„èƒ½åŠ›å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†æˆ‘ä»¬ç ”ç©¶äº†ä¸€äº›å…¶åœ¨æ¶‰åŠé‡è¯ï¼ˆå¦‚â€œæ‰€æœ‰â€å’Œâ€œä¸€äº›â€ï¼‰çš„è¯­ä¹‰ä»»åŠ¡ä»¥åŠçº¿æ€§å‡½æ•°ä»»åŠ¡ä¸Šçš„å±€é™æ€§ã€‚æˆ‘ä»¬ç¡®å®šäº†æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¯„åˆ†å‡½æ•°Softmaxæ˜¯è¿™äº›å±€é™æ€§çš„ä¸€ä¸ªå› ç´ ã€‚æˆ‘ä»¬æå‡ºäº†Scaled Signed Averagingï¼ˆSSAï¼‰è¿™ä¸€æ–°å‹çš„Softmaxæ›¿ä»£æ–¹æ¡ˆæ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬è¯æ˜SSAåœ¨æˆ‘ä»¬çš„ICLä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼ŒSSAåœ¨å¤šä¸ªæ—©æœŸå­¦ä¹ NLPåŸºå‡†æµ‹è¯•å’Œè¯­è¨€å­¦æ¢æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä½¿ç”¨Softmaxçš„Transformeræ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14685v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰è™½ç„¶å¤‡å—å…³æ³¨ï¼Œä½†åœ¨æ¶‰åŠé‡è¯ï¼ˆå¦‚â€œæ‰€æœ‰â€å’Œâ€œä¸€äº›â€ï¼‰çš„è¯­ä¹‰ä»»åŠ¡ä»¥åŠçº¿æ€§å‡½æ•°ä»»åŠ¡ä¸Šå­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚æœ¬æ–‡å‘ç°æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¯„åˆ†å‡½æ•°Softmaxæ˜¯è¿™äº›å±€é™æ€§çš„åŸå› ä¹‹ä¸€ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä½¿ç”¨ä¸€ç§åä¸ºScaled Signed Averagingï¼ˆSSAï¼‰çš„æ–°å‹è¯„åˆ†å‡½æ•°æ¥æ›¿ä»£Softmaxï¼Œä»¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSSAåœ¨ICLä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„æ—©æœŸå­¦ä¹ NLPåŸºå‡†æµ‹è¯•å’Œè¯­è¨€å­¦æ¢æµ‹ä»»åŠ¡ä¸­ä¼˜äºä½¿ç”¨Softmaxçš„transformeræ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰åœ¨æ¶‰åŠé‡è¯å’Œçº¿æ€§å‡½æ•°çš„ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Softmaxä½œä¸ºæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è¯„åˆ†å‡½æ•°ï¼Œæ˜¯è¿™äº›å±€é™æ€§çš„åŸå› ä¹‹ä¸€ã€‚</li>
<li>æå‡ºäº†Scaled Signed Averagingï¼ˆSSAï¼‰æ–°å‹è¯„åˆ†å‡½æ•°ï¼Œä»¥æ›¿ä»£Softmaxï¼Œæ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SSAåœ¨ICLä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>SSAåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„æ—©æœŸå­¦ä¹ NLPåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SSAä¼˜äºä½¿ç”¨Softmaxçš„transformeræ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd9e8c3495d42cf7895f0930feef9062" align="middle">
<img src="https://picx.zhimg.com/v2-d752081c225c043003d6f0538341e611" align="middle">
<img src="https://picx.zhimg.com/v2-8d957aebb0de3ec2be31891128f338b9" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Thinking-with-Nothinking-Calibration-A-New-In-Context-Learning-Paradigm-in-Reasoning-Large-Language-Models"><a href="#Thinking-with-Nothinking-Calibration-A-New-In-Context-Learning-Paradigm-in-Reasoning-Large-Language-Models" class="headerlink" title="Thinking with Nothinking Calibration: A New In-Context Learning Paradigm   in Reasoning Large Language Models"></a>Thinking with Nothinking Calibration: A New In-Context Learning Paradigm   in Reasoning Large Language Models</h2><p><strong>Authors:Haotian Wu, Bo Xu, Yao Shu, Menglin Yang, Chengwei Qin</strong></p>
<p>Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt with two different answers. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT), thinking twice and majority voting. Moreover, it achieves comparable in-distribution performance to training-based SOTA reasoning method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing the importance of structural thinking diversity and the benefits of consistency check. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰é€šè¿‡ç»“æ„åŒ–åŠå¤šæ­¥éª¤æ¨ç†å±•ç°äº†æ˜¾è‘—çš„å®åŠ›ã€‚å°½ç®¡å…ˆå‰çš„ç ”ç©¶ä¸»è¦èšç„¦äºæ”¹è¿›å…¶è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œä½†å®ƒä»¬å¯¹äºæƒ…å¢ƒå†…å­¦ä¹ ï¼ˆICLï¼‰çš„æ½œåŠ›å´é²œæœ‰æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ— æ€è€ƒæ ¡å‡†æ€è€ƒâ€ï¼ˆJointThinkingï¼‰è¿™ä¸€å…¨æ–°çš„ICLèŒƒå¼ï¼Œå®ƒæç¤ºæ¨¡å‹å¹¶è¡Œç”Ÿæˆä¸¤ç§ç­”æ¡ˆï¼šä¸€ç§æ˜¯æ€è€ƒæ¨¡å¼ï¼Œå¦ä¸€ç§æ˜¯æ— æ€è€ƒæ¨¡å¼ã€‚å½“ä¸¤ä¸ªåˆæ­¥å›ç­”ä¸ä¸€è‡´æ—¶ï¼Œæ‰ä¼šè§¦å‘ç¬¬äºŒè½®æ€è€ƒï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰ä¸¤ä¸ªä¸åŒç­”æ¡ˆçš„å•ä¸€æç¤ºã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒJointThinkingæ˜¾è‘—ä¼˜äºå°‘é•œå¤´æ€ç»´é“¾ï¼ˆCoTï¼‰ã€ä¸‰æ€è€Œåè¡Œå’Œå¤šæ•°æŠ•ç¥¨ã€‚è€Œä¸”ï¼Œå®ƒåœ¨åŸºäºè®­ç»ƒçš„çŠ¶æ€æœ€ä¼˜æ¨ç†æ–¹æ³•ä¸Šå®ç°äº†ç›¸å½“çš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨ç¦»åˆ†å¸ƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ ¡å‡†æœºåˆ¶è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œæ˜¾ç¤ºäº†ç»“æ„æ€§æ€ç»´å¤šæ ·æ€§çš„é‡è¦æ€§ä»¥åŠä¸€è‡´æ€§æ£€æŸ¥çš„å¥½å¤„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å®é™…ä¸ç†æƒ³æ¨ç†ä¹‹é—´çš„æ€§èƒ½å·®è·éšç€ç¬¬äºŒæ¬¡æ€è€ƒæ—¶æ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œç¼©å°ï¼Œè¿™è¡¨æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥RLLMsçš„ICLç ”ç©¶æå‡ºäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03363v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿‘æœŸå¤‡å—ç©ç›®ï¼Œé€šè¿‡ç»“æ„å’Œå¤šæ­¥éª¤æ¨ç†å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡å·²æœ‰å¤§é‡å…³äºæ”¹è¿›å…¶è®­ç»ƒå’Œæ¨ç†ç­–ç•¥çš„ç ”ç©¶ï¼Œä½†å…¶ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œæ— æ€è€ƒæ ¡å‡†æ€è€ƒâ€ï¼ˆJointThinkingï¼‰çš„æ–°ICLèŒƒå¼ï¼Œå®ƒé¼“åŠ±æ¨¡å‹åŒæ—¶ç”Ÿæˆä¸¤ç§ç­”æ¡ˆï¼šä¸€ç§ä¸ºæ€è€ƒæ¨¡å¼ï¼Œå¦ä¸€ç§ä¸ºæ— æ€è€ƒæ¨¡å¼ã€‚ä»…åœ¨ä¸¤ç§åˆå§‹å›ç­”ä¸ä¸€è‡´æ—¶ï¼Œæ‰ä¼šè§¦å‘ç¬¬äºŒè½®æ€è€ƒã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒJointThinkingæ˜¾è‘—ä¼˜äºå°‘æ ·æœ¬æ€ç»´é“¾ï¼ˆCoTï¼‰ã€äºŒæ¬¡æ€è€ƒå’Œå¤šæ•°æŠ•ç¥¨ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å†…éƒ¨åˆ†å¸ƒä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸åŸºäºè®­ç»ƒçš„æœ€æ–°æ¨ç†æ–¹æ³•ç›¸å½“ï¼Œä½†åœ¨å¤–éƒ¨åˆ†å¸ƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æœ¬æ–‡å¯¹æ ¡å‡†æœºåˆ¶è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå¼ºè°ƒäº†ç»“æ„æ€§æ€ç»´å¤šæ ·æ€§å’Œä¸€è‡´æ€§æ£€æŸ¥çš„é‡è¦æ€§ã€‚åŒæ—¶è§‚å¯Ÿåˆ°ï¼Œéšç€æ¨¡å‹å¤§å°çš„å¢åŠ ï¼Œå®é™…ä¸ç†æƒ³æ¨ç†ä¹‹é—´çš„æ€§èƒ½å·®è·ç¼©å°ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»“æ„å’Œå¤šæ­¥éª¤æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>JointThinkingæ˜¯ä¸€ç§æ–°çš„ICLèŒƒå¼ï¼Œé€šè¿‡ç”Ÿæˆä¸¤ç§ç­”æ¡ˆï¼ˆæ€è€ƒæ¨¡å¼å’Œæ— æ€è€ƒæ¨¡å¼ï¼‰æ¥è§¦å‘ç¬¬äºŒè½®æ€è€ƒã€‚</li>
<li>JointThinkingåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå°‘æ ·æœ¬æ€ç»´é“¾ã€äºŒæ¬¡æ€è€ƒå’Œå¤šæ•°æŠ•ç¥¨ã€‚</li>
<li>JointThinkingåœ¨å†…éƒ¨å’Œå¤–éƒ¨ä»»åŠ¡åˆ†å¸ƒä¸Šå‡è¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ã€‚</li>
<li>ç³»ç»Ÿåˆ†æå¼ºè°ƒäº†ç»“æ„æ€§æ€ç»´å¤šæ ·æ€§å’Œä¸€è‡´æ€§æ£€æŸ¥åœ¨æ ¡å‡†æœºåˆ¶ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28c457b4da0e8983aae91e8c1800fb7b" align="middle">
<img src="https://picx.zhimg.com/v2-e742849f41d295ebe9510388a4166ba1" align="middle">
<img src="https://picx.zhimg.com/v2-4e9db0d32d11f431ada16366ee070af8" align="middle">
<img src="https://picx.zhimg.com/v2-406b20217aeed349e6639720ed5bcabb" align="middle">
<img src="https://picx.zhimg.com/v2-dd09f8a51aec2a99ac8e79da5b8ade0b" align="middle">
<img src="https://picx.zhimg.com/v2-b44d4c5257a4285d28ddf447935c4223" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GLiDRE-Generalist-Lightweight-model-for-Document-level-Relation-Extraction"><a href="#GLiDRE-Generalist-Lightweight-model-for-Document-level-Relation-Extraction" class="headerlink" title="GLiDRE: Generalist Lightweight model for Document-level Relation   Extraction"></a>GLiDRE: Generalist Lightweight model for Document-level Relation   Extraction</h2><p><strong>Authors:Robin Armingaud, Romaric BesanÃ§on</strong></p>
<p>Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to complex interactions between entities across sentences. While supervised models have achieved strong results in fully resourced settings, their behavior with limited training data remains insufficiently studied. We introduce GLiDRE, a new compact model for document-level relation extraction, designed to work efficiently in both supervised and few-shot settings. Experiments in both low-resource supervised training and few-shot meta-learning benchmarks show that our approach outperforms existing methods in data-constrained scenarios, establishing a new state-of-the-art in few-shot document-level relation extraction. Our code will be publicly available. </p>
<blockquote>
<p>å…³ç³»æŠ½å–ï¼ˆREï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå…¶æ–‡æ¡£çº§å˜ä½“ç”±äºå¥å­é—´å®ä½“çš„å¤æ‚äº¤äº’è€Œå¸¦æ¥é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ç›‘ç£æ¨¡å‹åœ¨å…¨èµ„æºä¸°å¯Œçš„ç¯å¢ƒä¸­å–å¾—äº†å¼ºå¤§çš„æ•ˆæœï¼Œä½†åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹çš„è¡¨ç°ä»ç„¶ç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†GLiDREï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç´§å‡‘å‹æ–‡æ¡£çº§å…³ç³»æŠ½å–æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨ç›‘ç£å’Œå°æ ·æœ¬ç¯å¢ƒä¸­éƒ½èƒ½é«˜æ•ˆå·¥ä½œã€‚åœ¨ä½èµ„æºç›‘ç£è®­ç»ƒå’Œå°‘æ ·æœ¬å…ƒå­¦ä¹ åŸºå‡†æµ‹è¯•çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®å—é™çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å°‘æ ·æœ¬æ–‡æ¡£çº§å…³ç³»æŠ½å–ä¸­å»ºç«‹äº†æ–°çš„æœ€æ–°æŠ€æœ¯çŠ¶æ€ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00757v2">PDF</a> Submitted to ARR October</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GLiDREæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ–‡æ¡£çº§å…³ç³»æŠ½å–çš„æ–°é¢–ç´§å‡‘æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ—¨åœ¨å®ç°åœ¨æœ‰ç›‘ç£å­¦ä¹ å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹çš„é«˜æ•ˆè¿ä½œã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ•°æ®å—é™çš„ç›‘ç£è®­ç»ƒå’Œå°‘æ ·æœ¬å…ƒå­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®å—é™çš„åœºæ™¯ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæ–‡æ¡£çº§å…³ç³»æŠ½å–é¢†åŸŸæ ‘ç«‹äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLiDREæ˜¯ä¸€ä¸ªç”¨äºæ–‡æ¡£çº§å…³ç³»æŠ½å–çš„ç´§å‡‘æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹æ—¨åœ¨åœ¨æœ‰ç›‘ç£å­¦ä¹ å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹é«˜æ•ˆè¿ä½œã€‚</li>
<li>åœ¨æ•°æ®å—é™çš„ç›‘ç£è®­ç»ƒä¸­ï¼ŒGLiDREè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬å…ƒå­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGLiDREä¸ºæ–‡æ¡£çº§å…³ç³»æŠ½å–é¢†åŸŸæ ‘ç«‹äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚</li>
<li>GLiDREæ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤æ‚çš„å®ä½“é—´äº¤äº’ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåº”å¯¹æ–‡æ¡£ä¸­çš„è·¨å¥å­å®ä½“å…³ç³»æŠ½å–æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5cb7991706cdc5e18d71f2de88018787" align="middle">
<img src="https://picx.zhimg.com/v2-8a923be0eb47871d49495aaea9dac17a" align="middle">
<img src="https://picx.zhimg.com/v2-c40055895592eea7c9198a520b7f86cd" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 17 mAP! Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl">https://github.com/roboflow/rf100-vl</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a>. </p>
<blockquote>
<p>è®­ç»ƒäºäº’è”ç½‘è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§ç‰©ä½“ï¼ˆå¦‚æ±½è½¦ã€å¡è½¦å’Œè¡Œäººï¼‰ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½æ˜¾è‘—ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ¨å¹¿åˆ°å…¶é¢„è®­ç»ƒä¸­æ²¡æœ‰å‡ºç°çš„åˆ†å¸ƒå¤–çš„ç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ—¶ä»é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬ä¸»å¼ ï¼Œä¸å…¶ç®€å•åœ°ä½¿ç”¨æ›´å¤šçš„è§†è§‰æ•°æ®é‡æ–°è®­ç»ƒVLMsï¼Œä¸å¦‚ä½¿ç”¨åŒ…å«å‡ ä¸ªè§†è§‰ç¤ºä¾‹å’Œä¸°å¯Œçš„æ–‡æœ¬æè¿°çš„æ³¨é‡ŠæŒ‡ä»¤æ¥å¯¹é½VLMsçš„æ–°æ¦‚å¿µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªå¤šæ¨¡å¼å¯¹è±¡æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œå…¶ä¸­åŒ…å«çš„æ¦‚å¿µå¤šæ ·ä¸”åœ¨å…¶VLMé¢„è®­ç»ƒä¸­å¹¶ä¸å¸¸è§ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šå¯¹æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬ã€å°æ ·æµ‹è¯•ã€åŠç›‘ç£å’Œå®Œå…¨ç›‘ç£ä¸‹çš„è¯„ä¼°ï¼Œè¿™å…è®¸åœ¨ä¸åŒæ•°æ®æ¨¡å¼ä¸‹è¿›è¡Œæ¯”è¾ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åƒGroundingDINOå’ŒQwen2.5-VLè¿™æ ·çš„VLMåœ¨Roboflow100-VLä¸­çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡ä½äº2%ï¼Œè¿™è¯æ˜äº†å°æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœ€è¿‘çš„CVPR 2025åŸºç¡€FSODç«èµ›å¹¶ä»ç¤¾åŒºä¸­åˆ†äº«äº†ä¸€äº›è§è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå† å†›å›¢é˜Ÿçš„è¡¨ç°è¶…å‡ºæˆ‘ä»¬çš„åŸºçº¿æ°´å¹³é«˜è¾¾17 mAPï¼æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl%E5%92%8Chttps://universe.roboflow.com/rf100-vl/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/roboflow/rf100-vlå’Œhttps://universe.roboflow.com/rf100-vl/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v3">PDF</a> The first two authors contributed equally. This work has been   accepted to the Neural Information Processing Systems (NeurIPS) 2025 Datasets   &amp; Benchmark Track. Project Page: <a target="_blank" rel="noopener" href="https://rf100-vl.org/">https://rf100-vl.org/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹äº’è”ç½‘è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§ç‰©ä½“ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æ³›åŒ–åˆ°éåˆ†å¸ƒç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªå¤šæ¨¡å¼å¯¹è±¡æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œå…¶ä¸­åŒ…å«å„ç§ä¸å¸¸è§äºVLMé¢„è®­ç»ƒçš„æ¦‚å¿µã€‚ä½œè€…åœ¨ä¸åŒçš„æ•°æ®ç¯å¢ƒä¸‹è¯„ä¼°äº†ç°æœ‰æ¨¡å‹ï¼Œå‘ç°ä¸€äº›æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡ä½äº2%ï¼Œçªæ˜¾äº†å°‘æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†CVPR 2025åŸºç¡€FSODç«èµ›çš„è§è§£ï¼Œå¹¶åˆ†äº«äº†ç¤¾åŒºæ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§ç‰©ä½“ä¸Šå®ç°äº†å‡ºè‰²çš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ³›åŒ–åˆ°éåˆ†å¸ƒç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥Roboflow100-VLï¼Œä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–æ¦‚å¿µçš„å¤§è§„æ¨¡å¤šæ¨¡å¼å¯¹è±¡æ£€æµ‹æ•°æ®é›†ã€‚</li>
<li>ä¸€äº›æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡è¾ƒä½ï¼Œçªæ˜¾å°‘æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚</li>
<li>CVPR 2025åŸºç¡€FSODç«èµ›æä¾›äº†æœ‰å…³æ¨¡å‹æ€§èƒ½çš„æ·±å…¥è§è§£ã€‚</li>
<li>è·èƒœå›¢é˜Ÿåœ¨æ¯”èµ›ä¸­æ˜¾è‘—è¶…è¶Šäº†åŸºçº¿17 mAPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8425046653ec2d2d25050c898fcec42d" align="middle">
<img src="https://picx.zhimg.com/v2-39f97d32ec01bf4be40727f5d59a3bbe" align="middle">
<img src="https://picx.zhimg.com/v2-94525ec6f3d591f43894fc2c1a976188" align="middle">
<img src="https://picx.zhimg.com/v2-0731706e142f12ec7985fe7ab5630eda" align="middle">
<img src="https://picx.zhimg.com/v2-7d3cea226c0073c3c69f1cb43dcf6bcd" align="middle">
<img src="https://picx.zhimg.com/v2-5efc4abbd8b1bfc8bfe9d36ce9cd2d36" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction"><a href="#ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction" class="headerlink" title="ReLI: A Language-Agnostic Approach to Human-Robot Interaction"></a>ReLI: A Language-Agnostic Approach to Human-Robot Interaction</h2><p><strong>Authors:Linus Nwankwo, Bjoern Ellensohn, Ozan Ã–zdenizci, Elmar Rueckert</strong></p>
<p>Adapting autonomous agents for real-world industrial, domestic, and other daily tasks is currently gaining momentum. However, in global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human-specified tasks regardless of the language remains an unsolved problem. To address this, we propose ReLI, a language-agnostic approach that enables autonomous agents to converse naturally, semantically reason about their environment, and perform downstream tasks, regardless of the task instructionâ€™s modality or linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow conversational interactions. Further, we perform cross-lingual adaptation of the models to ensure that ReLI generalises across the global languages. To demonstrate ReLIâ€™s robustness, we conducted extensive experiments on various short- and long-horizon tasks, including zero- and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on $140$ languages involving $70K+$ multi-turn conversations. On average, ReLI achieved over $90%\pm0.2$ accuracy in cross-lingual instruction parsing and task execution success. These results demonstrate its potential to advance natural human-agent interaction in the real world while championing inclusive and linguistic diversity. Demos and resources will be public at: <a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/">https://linusnep.github.io/ReLI/</a>. </p>
<blockquote>
<p>é€‚åº”è‡ªä¸»ä»£ç†ç”¨äºç°å®ä¸–ç•Œä¸­çš„å·¥ä¸šã€å®¶å±…å’Œå…¶ä»–æ—¥å¸¸ä»»åŠ¡æ­£æ—¥ç›Šå—åˆ°é‡è§†ã€‚ç„¶è€Œï¼Œåœ¨å…¨çƒæˆ–è·¨è¯­è¨€åº”ç”¨ç¯å¢ƒä¸­ï¼Œç¡®ä¿ä¸ç¯å¢ƒçš„æœ‰æ•ˆäº’åŠ¨å¹¶æ‰§è¡Œä¸å—é™åˆ¶çš„äººç±»æŒ‡å®šä»»åŠ¡ï¼Œæ— è®ºä½¿ç”¨ä½•ç§è¯­è¨€ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReLIï¼Œè¿™æ˜¯ä¸€ç§è¯­è¨€æ— å…³çš„æ–¹æ³•ï¼Œä½¿è‡ªä¸»ä»£ç†èƒ½å¤Ÿè‡ªç„¶åœ°è¿›è¡Œå¯¹è¯ï¼Œå¯¹å…¶ç¯å¢ƒè¿›è¡Œè¯­ä¹‰æ¨ç†ï¼Œå¹¶æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œæ— è®ºä»»åŠ¡æŒ‡ä»¤çš„æ¨¡å¼æˆ–è¯­è¨€èµ·æºå¦‚ä½•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå°†å®ƒä»¬è½¬åŒ–ä¸ºè¯­è¨€åˆ°è¡ŒåŠ¨æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡è‡ªç„¶ã€è‡ªç”±çš„å¯¹è¯å¼äº¤äº’æä¾›å¸¸è¯†æ¨ç†å’Œé«˜çº§æœºå™¨äººæ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†è·¨è¯­è¨€é€‚é…ï¼Œä»¥ç¡®ä¿ReLIåœ¨å…¨çƒèŒƒå›´å†…å„ç§è¯­è¨€ä¸­éƒ½èƒ½é€šç”¨ã€‚ä¸ºäº†éªŒè¯ReLIçš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬åœ¨å„ç§çŸ­æœŸå’Œé•¿æœŸä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç©ºé—´å¯¼èˆªã€åœºæ™¯ä¿¡æ¯æ£€ç´¢å’ŒæŸ¥è¯¢å¯¼å‘ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠè¶…è¿‡7ä¸‡å¤šæ¬¡å›åˆå¯¹è¯çš„140ç§è¯­è¨€ä¸Šå¯¹æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚å¹³å‡è€Œè¨€ï¼ŒReLIåœ¨è·¨è¯­è¨€æŒ‡ä»¤è§£æå’Œä»»åŠ¡æ‰§è¡Œæ–¹é¢çš„æˆåŠŸç‡è¾¾åˆ°äº†è¶…è¿‡$90%\pm0.2$çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†å…¶åœ¨æ¨åŠ¨ç°å®ä¸–ç•Œä¸­è‡ªç„¶çš„äººæœºäº¤äº’æ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶å€¡å¯¼åŒ…å®¹æ€§å’Œè¯­è¨€å¤šæ ·æ€§ã€‚ç›¸å…³æ¼”ç¤ºå’Œèµ„æºå°†åœ¨ï¼š<a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/%E5%85%AC%E5%BC%80%E3%80%82">https://linusnep.github.io/ReLI/å…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01862v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é’ˆå¯¹è‡ªä¸»ä»£ç†äººåœ¨çœŸå®ä¸–ç•Œä¸­çš„åº”ç”¨é—®é¢˜ï¼Œæå‡ºä¸€ç§è¯­è¨€æ— å…³çš„æ–¹æ³•ReLIã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä½¿è‡ªä¸»ä»£ç†äººè¿›è¡Œè‡ªç„¶å¯¹è¯ï¼Œç†è§£ç¯å¢ƒè¯­ä¹‰ï¼Œæ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œæ— è®ºä»»åŠ¡æŒ‡ä»¤çš„æ–¹å¼æˆ–è¯­è¨€å¦‚ä½•ã€‚å®éªŒè¯æ˜ï¼ŒReLIåœ¨ä¸åŒè¯­è¨€å’Œä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„è¡¨ç°ï¼Œèƒ½å¤Ÿå®ç°è·¨è¯­è¨€é€‚åº”ï¼Œå¹¶å…·å¤‡é«˜åº¦çš„è‡ªç„¶è¯­è¨€äº¤äº’èƒ½åŠ›ã€‚ReLIæ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬å·¥ä¸šã€å®¶åº­å’Œå…¶ä»–æ—¥å¸¸ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ã€‚èµ„æºå¯é€šè¿‡ç½‘å€è¿›è¡Œè®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/">https://linusnep.github.io/ReLI/</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­å…³é”®çš„è§è§£æ¦‚è¿°ï¼š</p>
<ol>
<li>ReLIæ˜¯ä¸€ç§è¯­è¨€æ— å…³çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»ä»£ç†äººåœ¨å…¨çƒæˆ–è·¨è¯­è¨€ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡æ—¶çš„è¯­è¨€éšœç¢é—®é¢˜ã€‚</li>
<li>ReLIé€šè¿‡å°†å¤§è§„æ¨¡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è½¬åŒ–ä¸ºè¯­è¨€åˆ°è¡ŒåŠ¨æ¨¡å‹ï¼Œå®ç°è‡ªç„¶å¯¹è¯å’Œæœºå™¨äººæ§åˆ¶ã€‚</li>
<li>ReLIè¿›è¡Œäº†è·¨è¯­è¨€é€‚åº”ï¼Œç¡®ä¿å…¶åœ¨å…¨çƒèŒƒå›´å†…çš„å¤šç§è¯­è¨€ä¸­çš„é€šç”¨æ€§ã€‚</li>
<li>ReLIåœ¨å¤šç§çŸ­æœŸå’Œé•¿æœŸä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç©ºé—´å¯¼èˆªã€åœºæ™¯ä¿¡æ¯æ£€ç´¢å’ŒæŸ¥è¯¢å¯¼å‘ä»»åŠ¡ç­‰ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReLIåœ¨è·¨è¯­è¨€æŒ‡ä»¤è§£æå’Œä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡æ–¹é¢è¾¾åˆ°äº†è¶…è¿‡90%çš„å¹³å‡å‡†ç¡®ç‡ã€‚</li>
<li>ReLIå±•ç¤ºäº†åœ¨è‡ªç„¶äººæœºäº¤äº’ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒåŒ…å®¹æ€§å’Œè¯­è¨€å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ac6ba0a1907ecf479275a2c3fadf531" align="middle">
<img src="https://picx.zhimg.com/v2-bca48201cacdcfd198170d52444f7194" align="middle">
<img src="https://picx.zhimg.com/v2-2dfd3d427e9b64ce2d1742b38eab7af6" align="middle">
<img src="https://picx.zhimg.com/v2-8118bd0271def320f17336396e753f4a" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AutoPDL-Automatic-Prompt-Optimization-for-LLM-Agents"><a href="#AutoPDL-Automatic-Prompt-Optimization-for-LLM-Agents" class="headerlink" title="AutoPDL: Automatic Prompt Optimization for LLM Agents"></a>AutoPDL: Automatic Prompt Optimization for LLM Agents</h2><p><strong>Authors:Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel</strong></p>
<p>The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å–å†³äºå¦‚ä½•å¯¹å…¶è¿›è¡Œæç¤ºï¼Œæç¤ºçš„é€‰æ‹©èŒƒå›´åŒ…æ‹¬é«˜çº§æç¤ºæ¨¡å¼ï¼ˆä¾‹å¦‚Zero-Shotã€CoTã€ReActã€ReWOOï¼‰å’Œç‰¹å®šæç¤ºå†…å®¹ï¼ˆæŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼‰ã€‚æ‰‹åŠ¨è°ƒæ•´è¿™ç§ç»“åˆæ–¹å¼æ—¢ç¹çåˆå®¹æ˜“å‡ºç°é”™è¯¯ï¼Œè€Œä¸”é’ˆå¯¹ç‰¹å®šçš„LLMå’Œä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AutoPDLï¼Œä¸€ç§å‘ç°è‰¯å¥½LLMä»£ç†é…ç½®è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬å°†è¿™ä¸ªé—®é¢˜æ„å»ºä¸ºä¸€ä¸ªç»“æ„åŒ–AutoMLé—®é¢˜ï¼Œåœ¨ä»£ç†å’Œéä»£ç†æç¤ºæ¨¡å¼åŠç¤ºä¾‹çš„ç»„åˆç©ºé—´ä¸Šè¿›è¡Œè§£å†³ï¼Œä½¿ç”¨è¿ç»­å‡åŠæ³•æœ‰æ•ˆåœ°éå†è¿™ä¸ªç©ºé—´ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½¿ç”¨PDLæç¤ºç¼–ç¨‹è¯­è¨€çš„åº“æ¥å®ç°å¸¸è§çš„æç¤ºæ¨¡å¼ã€‚AutoPDLè§£å†³æ–¹æ¡ˆæ˜¯å¯è¯»ã€å¯ç¼–è¾‘ã€å¯æ‰§è¡Œçš„PDLç¨‹åºï¼Œä½¿ç”¨æ­¤åº“ã€‚è¿™ç§æ–¹æ³•è¿˜å®ç°äº†æºåˆ°æºçš„ä¼˜åŒ–ï¼Œå…è®¸äººç±»å‚ä¸å¾ªç¯ä¼˜åŒ–å’Œé‡ç”¨ã€‚åœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¸ƒä¸ªLLMï¼ˆä»3Båˆ°70Bå‚æ•°ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå‡†ç¡®ç‡æŒç»­æé«˜ï¼ˆ9.21Â±15.46ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œæœ€é«˜è¾¾åˆ°67.5ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”æ˜¾ç¤ºæ‰€é€‰çš„æç¤ºç­–ç•¥åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¹‹é—´æœ‰æ‰€ä¸åŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04365v4">PDF</a> Presented at AutoML 2025 (Methods Track); to be published in   proceedings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å–å†³äºå¦‚ä½•å¯¹å…¶è¿›è¡Œæç¤ºï¼Œæ¶‰åŠé«˜çº§æç¤ºæ¨¡å¼ï¼ˆå¦‚é›¶å‡»ã€ä¸Šä¸‹æ–‡æç¤ºã€ååº”å’ŒReWOOï¼‰å’Œç‰¹å®šæç¤ºå†…å®¹ï¼ˆæŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼‰ã€‚æ‰‹åŠ¨è°ƒæ•´è¿™ç§ç»„åˆæ—¢ç¹çåˆå®¹æ˜“å‡ºé”™ï¼Œä¸”é’ˆå¯¹ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AutoPDLï¼Œä¸€ç§ç”¨äºå‘ç°è‰¯å¥½å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†é…ç½®çš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†å…¶æ„å»ºä¸ºä¸€ä¸ªç»“æ„åŒ–è‡ªåŠ¨æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œåœ¨ä»£ç†å’Œéä»£ç†æç¤ºæ¨¡å¼åŠç¤ºä¾‹çš„ç»„åˆç©ºé—´ä¸­é«˜æ•ˆå¯¼èˆªã€‚å¼•å…¥ä¸€ä¸ªä½¿ç”¨PDLæç¤ºç¼–ç¨‹è¯­è¨€çš„å¸¸è§æç¤ºæ¨¡å¼åº“ã€‚AutoPDLè§£å†³æ–¹æ¡ˆæ˜¯å¯è¯»ã€å¯ç¼–è¾‘å’Œå¯æ‰§è¡Œçš„ä½¿ç”¨è¯¥åº“çš„PDLç¨‹åºã€‚è¯¥æ–¹æ³•è¿˜æ”¯æŒæºåˆ°æºçš„ä¼˜åŒ–ï¼Œå…è®¸äººä¸ºå‚ä¸ä¼˜åŒ–å’Œæ”¹è¿›ã€‚åœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¸ƒä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå‡†ç¡®æ€§æŒç»­æé«˜ï¼ˆå¢åŠ 9.21Â±15.46ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œæœ€é«˜è¾¾67.5ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨æ˜é€‰æ‹©çš„æç¤ºç­–ç•¥åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¹‹é—´æœ‰æ‰€ä¸åŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å—æç¤ºæ–¹å¼å½±å“ï¼ŒåŒ…æ‹¬é«˜çº§æç¤ºæ¨¡å¼å’Œå…·ä½“æç¤ºå†…å®¹ã€‚</li>
<li>æ‰‹åŠ¨è°ƒæ•´æç¤ºç»„åˆæ—¢ç¹çåˆæ˜“å‡ºé”™ã€‚</li>
<li>AutoPDLæ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œç”¨äºå‘ç°è‰¯å¥½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†é…ç½®ã€‚</li>
<li>AutoPDLå°†é—®é¢˜æ„å»ºä¸ºç»“æ„åŒ–è‡ªåŠ¨æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œåœ¨ä»£ç†å’Œéä»£ç†æç¤ºæ¨¡å¼åŠç¤ºä¾‹çš„ç»„åˆç©ºé—´ä¸­é«˜æ•ˆå¯¼èˆªã€‚</li>
<li>å¼•å…¥PDLæç¤ºç¼–ç¨‹è¯­è¨€çš„å¸¸è§æç¤ºæ¨¡å¼åº“ï¼Œä½¿AutoPDLè§£å†³æ–¹æ¡ˆæ›´åŠ äººæ€§åŒ–ã€å¯ç¼–è¾‘å’Œæ‰§è¡Œã€‚</li>
<li>AutoPDLæ”¯æŒæºåˆ°æºçš„ä¼˜åŒ–ï¼Œå…è®¸äººä¸ºå‚ä¸ä¼˜åŒ–å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fd26ed8ad320dd275f062f698885255" align="middle">
<img src="https://picx.zhimg.com/v2-fdb37e2d07509aafb24da166e397e2c4" align="middle">
<img src="https://picx.zhimg.com/v2-1b71f89fd39f141bf09b3fd3839a7e68" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano PatrÃ­cio, Isabel Rio-Torto, Jaime S. Cardoso, LuÃ­s F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the model output on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>åœ¨åŒ»ç–—å·¥ä½œæµä¸­é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆçš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ ‡æ³¨æ•°æ®å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºä¹ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸæ¨¡å‹è¾“å‡ºåœ¨ä¸€ç»„é¢„å®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µä¸Šæ¥è§£å†³åè€…çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡åŸºäºæ¦‚å¿µçš„è§£é‡Šæé«˜çš„è§£é‡Šæ€§æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ã€‚è€Œä¸”ï¼Œå¦‚æœéœ€è¦æ·»åŠ æ–°æ¦‚å¿µï¼Œæ•´ä¸ªç³»ç»Ÿéœ€è¦é‡æ–°è®­ç»ƒã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å°‘é‡åœºæ™¯è®¾ç½®ä¸­çš„å‡ºè‰²è¡¨ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå³CBVLMï¼Œå®ƒè§£å†³äº†ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬æç¤ºLVLMå›ç­”æ¦‚å¿µæ˜¯å¦å‡ºç°åœ¨è¾“å…¥å›¾åƒä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬è®©LVLMåŸºäºä¹‹å‰çš„æ¦‚å¿µé¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚æ­¤å¤–ï¼Œåœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬éƒ½èå…¥äº†ä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œè´Ÿè´£é€‰æ‹©æœ€ä½³ç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡åŸºäºé¢„æµ‹æ¦‚å¿µçš„æœ€ç»ˆè¯Šæ–­ï¼Œæˆ‘ä»¬ç¡®ä¿äº†è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡åˆ©ç”¨LVLMsçš„å°‘é‡æ ·æœ¬èƒ½åŠ›ï¼Œæˆ‘ä»¬å¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªåŒ»ç–—æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»ç–—ï¼‰LVLMçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¡¨æ˜CBVLMåœ¨ä¸éœ€è¦ä»»ä½•è®­ç»ƒå’Œåªéœ€å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºCBMså’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æœ‰ç›‘ç£æ–¹æ³•ã€‚æ›´å¤šå…³äºæˆ‘ä»¬é¡¹ç›®çš„ä¿¡æ¯è¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/]">https://cristianopatricio.github.io/CBVLM/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v3">PDF</a> Accepted for publication in Computers in Biology and Medicine</p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»ç–—è§£å†³æ–¹æ¡ˆé¢ä¸´æ ‡æ³¨æ•°æ®å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§ä¸¤å¤§æŒ‘æˆ˜ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸæ¨¡å‹è¾“å‡ºåœ¨é¢„å®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µé›†ä¸Šæ¥è§£å†³è§£é‡Šæ€§é—®é¢˜ï¼Œä½†è¿™ä¹Ÿå¢åŠ äº†æ ‡æ³¨è´Ÿæ‹…ï¼Œä¸”æ·»åŠ æ–°æ¦‚å¿µéœ€é‡æ–°è®­ç»ƒæ•´ä¸ªç³»ç»Ÿã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„å‡ºè‰²è¡¨ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„CBVLMæ–¹æ³•ï¼ŒåŒæ—¶åº”å¯¹ä¸Šè¿°ä¸¤å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å¯¼LVLMåˆ¤æ–­è¾“å…¥å›¾åƒä¸­æ˜¯å¦å­˜åœ¨ç‰¹å®šæ¦‚å¿µï¼Œå¹¶åŸºäºé¢„æµ‹çš„æ¦‚å¿µå¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥æ£€ç´¢æ¨¡å—ï¼Œè´Ÿè´£é€‰æ‹©æœ€ä½³æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡å°†æœ€ç»ˆè¯Šæ–­ç»“æœå»ºç«‹åœ¨é¢„æµ‹æ¦‚å¿µä¸Šï¼Œæˆ‘ä»¬ç¡®ä¿äº†å¯è§£é‡Šæ€§ï¼Œå¹¶åˆ©ç”¨LVLMsçš„å°‘æ ·æœ¬èƒ½åŠ›å¤§å¹…é™ä½æ ‡æ³¨æˆæœ¬ã€‚å®éªŒè¯æ˜ï¼ŒCBVLMåœ¨å››ä¸ªåŒ»ç–—æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»ç–—ï¼‰LVLMä¸Šçš„è¡¨ç°å‡ä¼˜äºCBMså’Œä»»åŠ¡ç‰¹å®šçš„ç›‘ç£æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å³å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å’ŒåŒ»ç–—å·¥ä½œæµä¸­çš„æŒ‘æˆ˜åŒ…æ‹¬æ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§ä¸è¶³ã€‚</li>
<li>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸæ¨¡å‹è¾“å‡ºåœ¨é¢„å®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µé›†ä¸Šè§£å†³è§£é‡Šæ€§é—®é¢˜ï¼Œä½†å¢åŠ äº†æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>CBVLMæ–¹æ³•ç»“åˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡å¼•å¯¼LVLMåˆ¤æ–­ç‰¹å®šæ¦‚å¿µçš„å­˜åœ¨å¹¶å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚</li>
<li>CBVLMæ–¹æ³•å¼•å…¥æ£€ç´¢æ¨¡å—é€‰æ‹©æœ€ä½³æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>CBVLMé€šè¿‡é¢„æµ‹æ¦‚å¿µç¡®ä¿è§£é‡Šæ€§ï¼Œå¹¶åˆ©ç”¨LVLMsçš„å°‘æ ·æœ¬èƒ½åŠ›é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>å®éªŒè¯æ˜CBVLMåœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºCBMså’Œä»»åŠ¡ç‰¹å®šç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15ec5a9c5d00e6bc7dcf25c49384c7ca" align="middle">
<img src="https://picx.zhimg.com/v2-36b80d22716191da8fc4c178f927e398" align="middle">
<img src="https://picx.zhimg.com/v2-4b386e2ae3c4946b3e59685e95f4804b" align="middle">
<img src="https://picx.zhimg.com/v2-be192d43deb0d0b33637393de5cd9c1f" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VICON-Vision-In-Context-Operator-Networks-for-Multi-Physics-Fluid-Dynamics-Prediction"><a href="#VICON-Vision-In-Context-Operator-Networks-for-Multi-Physics-Fluid-Dynamics-Prediction" class="headerlink" title="VICON: Vision In-Context Operator Networks for Multi-Physics Fluid   Dynamics Prediction"></a>VICON: Vision In-Context Operator Networks for Multi-Physics Fluid   Dynamics Prediction</h2><p><strong>Authors:Yadi Cao, Yuxuan Liu, Liu Yang, Rose Yu, Hayden Schaeffer, Stanley Osher</strong></p>
<p>In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose Vision In-Context Operator Networks (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICONâ€™s adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9% compared to DPOT and 44.7% compared to MPP, while requiring only 72.5% and 34.8% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in imperfect measurement systems where sampling frequencies may differ or frames might be dropped - common challenges in real-world settings - without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41% relative performance degradation compared to 71.37%-74.49% degradation in baseline methods, demonstrating its versatility for deploying in realistic applications. Our scripts for processing datasets and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Eydcao/VICON">https://github.com/Eydcao/VICON</a>. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡æ“ä½œç½‘ç»œï¼ˆICONsï¼‰å·²ç»å±•ç¤ºäº†åœ¨å¤šç§åå¾®åˆ†æ–¹ç¨‹ä¸­ä½¿ç”¨å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ æ“ä½œçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ICONå°†æ¯ä¸ªç©ºé—´ç‚¹è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„æ ‡è®°ï¼Œåœ¨å¤„ç†é«˜ç»´ç©ºé—´çš„å¯†é›†æ•°æ®æ—¶ï¼Œè®¡ç®—æ•ˆç‡ä¸¥é‡å—é™ã€‚æˆ‘ä»¬æå‡ºäº†è§†è§‰ä¸Šä¸‹æ–‡æ“ä½œç½‘ç»œï¼ˆVICONï¼‰ï¼Œå®ƒç»“åˆäº†è§†è§‰è½¬æ¢å™¨æ¶æ„ï¼Œé€šè¿‡å—æ“ä½œé«˜æ•ˆåœ°å¤„ç†äºŒç»´æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™ICONå¯¹å¤šç‰©ç†ç³»ç»Ÿå’Œä¸åŒæ—¶é—´æ­¥é•¿çš„é€‚åº”æ€§ã€‚åœ¨ä¸‰ä¸ªæµä½“åŠ¨åŠ›å­¦åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒVICONæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯DPOTå’ŒMPPï¼Œä¸DPOTç›¸æ¯”å¹³å‡æœ€åä¸€æ­¥æ»šåŠ¨è¯¯å·®å‡å°‘äº†37.9%ï¼Œä¸MPPç›¸æ¯”å‡å°‘äº†44.7%ï¼ŒåŒæ—¶ä»…éœ€è¦å®ƒä»¬å„è‡ªæ¨ç†æ—¶é—´çš„72.5%å’Œ34.8%ã€‚VICONè‡ªç„¶åœ°æ”¯æŒå…·æœ‰ä¸åŒæ—¶é—´æ­¥é•¿çš„çµæ´»æ»šåŠ¨ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æµ‹é‡ç³»ç»Ÿä¸å®Œå–„çš„æƒ…å†µä¸‹è¿›è¡Œå³æ—¶éƒ¨ç½²ï¼Œå…¶ä¸­é‡‡æ ·é¢‘ç‡å¯èƒ½æœ‰æ‰€ä¸åŒæˆ–å¸§å¯èƒ½ä¼šä¸¢å¤±â€”â€”ç°å®ç¯å¢ƒä¸­çš„å¸¸è§æŒ‘æˆ˜â€”â€”æ— éœ€é‡æ–°è®­ç»ƒæˆ–æ’å€¼ã€‚åœ¨è¿™äº›ç°å®åœºæ™¯ä¸­ï¼ŒVICONè¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ï¼Œç›¸å¯¹æ€§èƒ½ä¸‹é™åªæœ‰24.41%ï¼Œè€ŒåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ä¸‹é™åœ¨71.37%-74.49%ä¹‹é—´ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®åº”ç”¨ä¸­çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¤„ç†è„šæœ¬å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Eydcao/VICON">https://github.com/Eydcao/VICON</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16063v4">PDF</a> update after NIPS suggestions</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†In-Context Operator Networksï¼ˆICONsï¼‰åœ¨å¤„ç†éƒ¨åˆ†å¾®åˆ†æ–¹ç¨‹æ—¶çš„å­¦ä¹ èƒ½åŠ›ï¼Œä½†å…¶åœ¨å¤„ç†é«˜ç»´ç©ºé—´å¯†é›†æ•°æ®æ—¶è®¡ç®—æ•ˆç‡å—é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Vision In-Context Operator Networksï¼ˆVICONï¼‰ï¼Œç»“åˆè§†è§‰è½¬æ¢å™¨æ¶æ„ï¼Œé€šè¿‡è¡¥ä¸æ“ä½œé«˜æ•ˆå¤„ç†äºŒç»´æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™ICONåœ¨å¤šç‰©ç†ç³»ç»Ÿå’Œä¸åŒæ—¶é—´æ­¥é•¿çš„é€‚åº”æ€§ã€‚åœ¨ä¸‰ä¸ªæµä½“åŠ¨åŠ›å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVICONæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºå‡†ï¼Œå‡å°‘äº†å¹³å‡æœ€åä¸€æ­¥æ»šåŠ¨è¯¯å·®ï¼Œå¹¶æé«˜äº†æ¨ç†æ—¶é—´çš„æ•ˆç‡ã€‚VICONæ”¯æŒçµæ´»çš„æ—¶é—´æ­¥é•¿ç­–ç•¥ï¼Œå¯åœ¨çœŸå®ä¸–ç•Œçš„ä¸å®Œç¾æµ‹é‡ç³»ç»Ÿä¸­éƒ¨ç½²ï¼Œé¢å¯¹é‡‡æ ·é¢‘ç‡å·®å¼‚æˆ–å¸§ä¸¢å¤±ç­‰æŒ‘æˆ˜æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¤„ç†è„šæœ¬å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICONsèƒ½å¤Ÿåœ¨ä¸åŒçš„éƒ¨åˆ†å¾®åˆ†æ–¹ç¨‹ä¸­å­¦ä¹ æ“ä½œç¬¦ï¼Œä½†åœ¨å¤„ç†é«˜ç»´ç©ºé—´å¯†é›†æ•°æ®æ—¶è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>VISION IN-CONTEXT OPERATOR NETWORKSï¼ˆVICONï¼‰é€šè¿‡ç»“åˆè§†è§‰è½¬æ¢å™¨æ¶æ„è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†äºŒç»´æ•°æ®ã€‚</li>
<li>VICONåœ¨æµä½“åŠ¨åŠ›å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºå‡†ï¼Œå‡å°‘äº†å¹³å‡æœ€åä¸€æ­¥æ»šåŠ¨è¯¯å·®ï¼Œå¹¶æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
<li>VICONæ”¯æŒçµæ´»çš„æ—¶é—´æ­¥é•¿ç­–ç•¥ï¼Œé€‚åº”çœŸå®ä¸–ç•Œçš„ä¸å®Œç¾æµ‹é‡ç³»ç»Ÿã€‚</li>
<li>VICONåœ¨å¤„ç†é‡‡æ ·é¢‘ç‡å·®å¼‚æˆ–å¸§ä¸¢å¤±ç­‰æŒ‘æˆ˜æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</li>
<li>VICONå…·æœ‰å¤šç‰©ç†ç³»ç»Ÿå’Œå¤šå˜æ—¶é—´æ­¥é•¿çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a432e76f87dd6286dccb76d397f744e8" align="middle">
<img src="https://picx.zhimg.com/v2-d246e38442a4e70f3d65c97da090e2fd" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f4acd61c8d7272201da7c7f3b421176a" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph   Regularization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c847d5b1dd97a51c096bd79df71ca723" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  GIIFT Graph-guided Inductive Image-free Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
