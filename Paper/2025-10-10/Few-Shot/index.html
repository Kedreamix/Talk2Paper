<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-10-10  Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-c7356733d2d17e36a80ff2f1f8d36a27~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036702&auth_key=1760036702-0-0-040bfff8a24c0304050662e2132723e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-10-更新"><a href="#2025-10-10-更新" class="headerlink" title="2025-10-10 更新"></a>2025-10-10 更新</h1><h2 id="Few-Shot-Adaptation-Benchmark-for-Remote-Sensing-Vision-Language-Models"><a href="#Few-Shot-Adaptation-Benchmark-for-Remote-Sensing-Vision-Language-Models" class="headerlink" title="Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models"></a>Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models</h2><p><strong>Authors:Karim El Khoury, Maxime Zanella, Christophe De Vleeschouwer, Benoit Macq</strong></p>
<p>Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable potential thanks to large-scale pretraining, achieving strong zero-shot performance on various tasks. However, their ability to generalize in low-data regimes, such as few-shot learning, remains insufficiently explored. In this work, we present the first structured benchmark for evaluating few-shot adaptation methods on RSVLMs. We conduct comprehensive experiments across ten remote sensing scene classification datasets, applying five widely used few-shot adaptation strategies to three state-of-the-art RSVLMs with varying backbones. Our findings reveal that models with similar zero-shot performance can exhibit markedly different behavior under few-shot adaptation, with some RSVLMs being inherently more amenable to such adaptation than others. The variability of performance and the absence of a clear winner among existing methods highlight the need for the development of more robust methods for few-shot adaptation tailored to RS. To facilitate future research, we provide a reproducible benchmarking framework and open-source code to systematically evaluate RSVLMs under few-shot conditions. The source code is publicly available on Github: <a target="_blank" rel="noopener" href="https://github.com/elkhouryk/fewshot_RSVLMs">https://github.com/elkhouryk/fewshot_RSVLMs</a> </p>
<blockquote>
<p>遥感视觉语言模型（RSVLMs）由于大规模预训练而显示出显著潜力，在各种任务上实现了强大的零样本性能。然而，它们在低数据环境下的泛化能力，如小样本学习，仍未得到充分探索。在这项工作中，我们首次为评估RSVLMs的小样本适应方法建立了结构化基准测试。我们在十个遥感场景分类数据集上进行全面实验，将五种广泛使用的小样本适应策略应用于三种具有不同背骨的最先进RSVLMs。我们的研究发现，具有相似零样本性能的模型在小样本适应下可能会表现出截然不同的行为，有些RSVLMs天生更适合这种适应。性能的可变性和现有方法之间没有明显胜出者，这凸显了需要开发更稳健的针对RS的小样本适应方法的必要性。为了促进未来的研究，我们提供了一个可复制的基准测试框架和开源代码，以在有限样本条件下系统地评估RSVLMs。源代码可在Github上公开获取：<a target="_blank" rel="noopener" href="https://github.com/elkhouryk/fewshot_RSVLMs">https://github.com/elkhouryk/fewshot_RSVLMs</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07135v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>远程遥感视觉语言模型（RSVLMs）的零样本学习能力已经展现出显著潜力。然而，它们在低数据环境下的泛化能力，尤其是小样本学习能力，仍待充分探索。本研究首次构建了评估RSVLMs小样本适应方法的基准测试，对十种遥感场景分类数据集进行全面实验，将五种常用的小样本适应策略应用于三种先进的RSVLMs。研究发现，具有相似零样本性能的模型在小样本适应下表现差异显著，凸显出开发更适合遥感领域的小样本适应方法的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RSVLMs在零样本学习方面已表现出显著潜力。</li>
<li>小样本学习在RSVLMs中的研究仍待深入。</li>
<li>不同RSVLMs在小样本适应下的表现差异显著。</li>
<li>现有小样本适应策略在RSVLMs中无明确优胜者。</li>
<li>需要开发更适用于遥感领域的小样本适应方法。</li>
<li>研究提供了一个可重现的基准测试框架，用于评估RSVLMs的小样本学习能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07135">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-31b1cfbe1959928e502dd40d1e513f5e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036526&auth_key=1760036526-0-0-a3ac06ae6cecd1fa74993b612c5fa1e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2a4bd54ce5d12fdccfec5099dc6e7d44~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036534&auth_key=1760036534-0-0-42727ecf365718f987ba601a6ab58f9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad437d0adb9a1f8ae2209b87fa847f5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036540&auth_key=1760036540-0-0-b9763ab6983cef981c81676e7705857e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f10a9fdfedf83818fd67311cc18b9cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036546&auth_key=1760036546-0-0-d38b7b8ef20b1597861de97ffd3d7692&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-838ebe99efa5f3b1f93ca0d703164ec3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036553&auth_key=1760036553-0-0-87a22dd3e59a337c27c7b3da086cbf1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12b94fd3528a36ef89eecb99f86f1407~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036560&auth_key=1760036560-0-0-6afa53fdadbe00cfbc0e18bdd5173feb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Crossing-Domains-without-Labels-Distant-Supervision-for-Term-Extraction"><a href="#Crossing-Domains-without-Labels-Distant-Supervision-for-Term-Extraction" class="headerlink" title="Crossing Domains without Labels: Distant Supervision for Term Extraction"></a>Crossing Domains without Labels: Distant Supervision for Term Extraction</h2><p><strong>Authors:Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank</strong></p>
<p>Automatic Term Extraction (ATE) is a critical component in downstream NLP tasks such as document tagging, ontology construction and patent analysis. Current state-of-the-art methods require expensive human annotation and struggle with domain transfer, limiting their practical deployment. This highlights the need for more robust, scalable solutions and realistic evaluation settings. To address this, we introduce a comprehensive benchmark spanning seven diverse domains, enabling performance evaluation at both the document- and corpus-levels. Furthermore, we propose a robust LLM-based model that outperforms both supervised cross-domain encoder models and few-shot learning baselines and performs competitively with its GPT-4o teacher on this benchmark. The first step of our approach is generating psuedo-labels with this black-box LLM on general and scientific domains to ensure generalizability. Building on this data, we fine-tune the first LLMs for ATE. To further enhance document-level consistency, oftentimes needed for downstream tasks, we introduce lightweight post-hoc heuristics. Our approach exceeds previous approaches on 5&#x2F;7 domains with an average improvement of 10 percentage points. We release our dataset and fine-tuned models to support future research in this area. </p>
<blockquote>
<p>自动术语提取（ATE）是下游NLP任务（如文档标记、本体构建和专利分析）中的关键组成部分。当前最前沿的方法需要昂贵的人力标注，并且在领域迁移方面存在困难，限制了其实际部署。这强调了需要更稳健、可扩展的解决方案和现实的评估环境。为了解决这一问题，我们引入了一个涵盖七个不同领域的综合基准测试，能够在文档和语料库两个层面进行性能评估。此外，我们提出了一个稳健的基于大型语言模型（LLM）的模型，该模型不仅优于有监督的跨域编码器模型和少样本学习基准测试，而且在我们的基准测试中与GPT-4o教师表现相当。我们的方法的第一步是利用这个黑盒大型语言模型在一般领域和科学领域生成伪标签，以确保其通用性。在此基础上，我们对第一批大型语言模型进行ATE微调。为了进一步增强下游任务通常需要的文档级一致性，我们引入了轻量级的后验启发式方法。我们的方法在5&#x2F;7的领域上超越了以前的方法，平均提高了10个百分点。我们公开我们的数据集和微调模型，以支持该领域的未来研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06838v1">PDF</a> Accepted at EMNLP Industry Track 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了自动术语抽取（ATE）在自然语言处理（NLP）下游任务中的重要性，如文档标注、本体构建和专利分析。针对当前先进方法依赖昂贵的人力标注和领域迁移困难的问题，文章提出了一种全面的跨七个不同领域的基准测试，以评估文档和语料库级别的性能。文章还提出了一种基于大型语言模型（LLM）的稳健模型，该模型在基准测试上表现出优异的性能，超越了监督跨域编码器模型和少样本学习基线，并与GPT-4o教师竞争。文章首先通过通用和科学领域生成伪标签来确保模型的泛化能力，并在此基础上微调了首个ATE领域的LLM。为了进一步提高下游任务所需的文档级别一致性，文章引入了轻量级事后启发式方法。该方法在五个领域超过了以前的方法，平均提高了10个百分点。文章还公开了数据集和微调模型，以支持未来在该领域的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ATE在NLP下游任务中起关键作用，如文档标注、本体构建和专利分析。</li>
<li>当前先进方法存在人力标注成本高和领域迁移困难的问题。</li>
<li>引入了一个全面的跨七个不同领域的基准测试，用于评估模型在文档和语料库级别的性能。</li>
<li>提出了一种基于LLM的稳健模型，该模型在基准测试上表现出优异性能。</li>
<li>通过生成伪标签确保模型的泛化能力，并微调了首个ATE领域的LLM。</li>
<li>引入了轻量级事后启发式方法来提高文档级别的一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06838">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a77a21600cd7b21fa85d384457072f7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036568&auth_key=1760036568-0-0-64d7c970117c60c8a13acb55f3472a0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-020a199fc0c0e116f050b02fdc1a33e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036575&auth_key=1760036575-0-0-52d0b038cdf551ad8aca70ae448480b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6abfcd205e5413f37ae35b93d302d4b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036582&auth_key=1760036582-0-0-5df888cf68a5264a77b06c7ac0318af7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a9d63390de76a08f721061a4cb4c0bbb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036589&auth_key=1760036589-0-0-30071e1d77b7bfddff6f23b882bc0ae9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7301eae5a0a3b1631eba26181066ada4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036595&auth_key=1760036595-0-0-15ec2db24fa7742b209c6b6da88fe14f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AISysRev-–-LLM-based-Tool-for-Title-abstract-Screening"><a href="#AISysRev-–-LLM-based-Tool-for-Title-abstract-Screening" class="headerlink" title="AISysRev – LLM-based Tool for Title-abstract Screening"></a>AISysRev – LLM-based Tool for Title-abstract Screening</h2><p><strong>Authors:Aleksi Huotala, Miikka Kuutila, Olli-Pekka Turtio, Mika Mäntylä</strong></p>
<p>Systematic reviews are a standard practice for summarizing the state of evidence in software engineering. Conducting systematic reviews is laborious, especially during the screening or study selection phase, where the number of papers can be overwhelming. During this phase, papers are assessed against inclusion and exclusion criteria based on their titles and abstracts. Recent research has demonstrated that large language models (LLMs) can perform title-abstract screening at a level comparable to that of a master’s student. While LLMs cannot be fully trusted, they can help, for example, in Rapid Reviews, which try to expedite the review process. Building on recent research, we developed AiSysRev, an LLM-based screening tool implemented as a web application running in a Docker container. The tool accepts a CSV file containing paper titles and abstracts. Users specify inclusion and exclusion criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev supports both zero-shot and few-shot screening, and also allows for manual screening through interfaces that display LLM results as guidance for human reviewers.We conducted a trial study with 137 papers using the tool. Our findings indicate that papers can be classified into four categories: Easy Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary cases, where LLMs are prone to errors, highlight the need for human intervention. While LLMs do not replace human judgment in systematic reviews, they can significantly reduce the burden of assessing large volumes of scientific literature. Video: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jVbEj4Y4tQI">https://www.youtube.com/watch?v=jVbEj4Y4tQI</a> Tool: <a target="_blank" rel="noopener" href="https://github.com/EvoTestOps/AISysRev">https://github.com/EvoTestOps/AISysRev</a> </p>
<blockquote>
<p>系统综述是软件工程中总结证据状况的标准实践。进行系统综述是一项艰巨的工作，特别是在筛选或研究选择阶段，论文数量可能非常庞大。在这一阶段，将根据论文的标题和摘要来评估其是否符合纳入和排除标准。最近的研究表明，大型语言模型（LLMs）进行标题摘要筛选的能力可以达到硕士生的水平。虽然不能完全信任LLMs，但它们可以在某些情况下提供帮助，例如在快速审查中尝试加快审查过程。基于近期研究，我们开发了AiSysRev，这是一个基于LLM的筛选工具，以Web应用程序的形式实现，运行在Docker容器中。该工具接受包含论文标题和摘要的CSV文件。用户指定纳入和排除标准。用户可以通过OpenRouter使用多个LLMs进行筛选。AiSysRev支持零样本和少样本筛选，并允许通过显示LLM结果作为人类评审者的指导来进行手动筛选。我们对包含137篇论文的工具进行了试验性研究。我们的研究结果表明，论文可分为四类：易于包含的、易于排除的、边界包含的、边界排除的。边界案例是LLMs容易出现错误的地方，这强调了人工干预的必要性。虽然LLMs不能取代系统综述中的人的判断，但它们可以显著减少评估大量科学文献的负担。视频：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jVbEj4Y4tQI">https://www.youtube.com/watch?v=jVbEj4Y4tQI</a> 工具：<a target="_blank" rel="noopener" href="https://github.com/EvoTestOps/AISysRev">https://github.com/EvoTestOps/AISysRev</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06708v1">PDF</a> 4 pages</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了系统审查在软件工程中的标准实践，并指出其中的筛选阶段工作量巨大。近期研究表明，大型语言模型（LLMs）可以执行标题摘要筛选，与硕士生的水平相当。在此基础上，开发了AiSysRev工具，支持零样本和少样本筛选，并允许通过显示LLM结果来指导人工审查者进行手动筛选。研究发现，论文可分为四类，其中边界案例需要人工干预。虽然LLMs不能完全取代系统审查中的人的判断力，但它们可以显著减少评估大量科学文献的负担。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>系统审查是软件工程证据总结的标准实践，其中筛选阶段尤为耗时。</li>
<li>大型语言模型（LLMs）可以执行标题和摘要的筛选，效率与硕士生相当。</li>
<li>AiSysRev工具基于LLM，支持零样本和少样本筛选，并可手动筛选。</li>
<li>在使用AiSysRev进行的试验研究中，论文被分为四类：Easy Includes, Easy Excludes, Boundary Includes, 和 Boundary Excludes。</li>
<li>边界案例表明LLMs存在错误倾向，需要人工干预。</li>
<li>LLMs不能替代系统审查中的人的判断力，但可显著减少评估大量科学文献的工作量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06708">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4d092559212274511806f6b8e86fe9cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036603&auth_key=1760036603-0-0-24bd972ffec82d719a6a0dd5808985f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6893525dec1dcd294f1aabf0778a2be6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036610&auth_key=1760036610-0-0-16264605748f4214253c3439f57b8333&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cross-Embodiment-Dexterous-Hand-Articulation-Generation-via-Morphology-Aware-Learning"><a href="#Cross-Embodiment-Dexterous-Hand-Articulation-Generation-via-Morphology-Aware-Learning" class="headerlink" title="Cross-Embodiment Dexterous Hand Articulation Generation via   Morphology-Aware Learning"></a>Cross-Embodiment Dexterous Hand Articulation Generation via   Morphology-Aware Learning</h2><p><strong>Authors:Heng Zhang, Kevin Yuchen Ma, Mike Zheng Shou, Weisi Lin, Yan Wu</strong></p>
<p>Dexterous grasping with multi-fingered hands remains challenging due to high-dimensional articulations and the cost of optimization-based pipelines. Existing end-to-end methods require training on large-scale datasets for specific hands, limiting their ability to generalize across different embodiments. We propose an eigengrasp-based, end-to-end framework for cross-embodiment grasp generation. From a hand’s morphology description, we derive a morphology embedding and an eigengrasp set. Conditioned on these, together with the object point cloud and wrist pose, an amplitude predictor regresses articulation coefficients in a low-dimensional space, which are decoded into full joint articulations. Articulation learning is supervised with a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant motions and injects morphology-specific structure. In simulation on unseen objects across three dexterous hands, our model attains a 91.9% average grasp success rate with less than 0.4 seconds inference per grasp. With few-shot adaptation to an unseen hand, it achieves 85.6% success on unseen objects in simulation, and real-world experiments on this few-shot generalized hand achieve an 87% success rate. The code and additional materials will be made available upon publication on our project website <a target="_blank" rel="noopener" href="https://connor-zh.github.io/cross_embodiment_dexterous_grasping">https://connor-zh.github.io/cross_embodiment_dexterous_grasping</a>. </p>
<blockquote>
<p>灵巧的多指手抓取仍然是一个挑战，主要是由于其高维度的关节运动和基于优化的管道的成本较高。现有的端到端方法需要在特定的手的大规模数据集上进行训练，这限制了其在不同手部的泛化能力。我们提出了一种基于固有握法的端到端框架，用于跨手部形态的抓取生成。根据手部形态描述，我们得出形态嵌入和固有握法集。基于这些条件，以及物体点云和手腕姿态，振幅预测器会回归低维空间中的关节运动系数，然后将其解码为完整的关节运动。关节运动学习是通过运动感知关节运动损失（KAL）进行监督的，它强调指尖相关运动并注入形态特定的结构。在三种灵巧手上对未见过的物体进行仿真实验，我们的模型达到了平均91.9%的抓取成功率，每次抓取的推理时间不到0.4秒。通过几次适应未见过的手部，它在仿真中对未见过的物体的成功率达到了85.6%，并在实际进行的关于该少数镜头通用化的手部实验中达到了87%的成功率。相关代码和额外材料将在我们的项目网站<a target="_blank" rel="noopener" href="https://connor-zh.github.io/cross_embodiment_dexterous_grasping">https://connor-zh.github.io/cross_embodiment_dexterous_grasping</a>上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06068v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于特征抓握（eigengrasp）的端到端框架，用于跨形态抓握生成。该框架通过手的形态描述生成形态嵌入和特征抓握集，再结合物体点云和手腕姿态，通过振幅预测器在低维空间中回归关节活动系数，最后解码为全关节活动。学习关节活动通过监督动觉关节损失（KAL），强调指尖相关运动和注入形态特定结构。模型在仿真中未见物体跨三种灵巧手的平均抓握成功率达到91.9%，单次抓握推理时间不到0.4秒。对未见的手进行少量样本适应后，在仿真中未见物体的成功率达到85.6%，在实际实验中的成功率达到87%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种基于特征抓握的端到端框架，用于跨不同形态的抓握生成。</li>
<li>通过手的形态描述生成形态嵌入和特征抓握集。</li>
<li>结合物体点云和手腕姿态，通过振幅预测器回归关节活动系数。</li>
<li>采用低维空间学习关节活动，并解码为全关节活动。</li>
<li>监督学习通过动觉关节损失（KAL），强调指尖相关运动和形态特定结构。</li>
<li>在仿真环境中，模型对未见物体和未见手的抓握表现出高成功率和快速推理时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c3616979acf7ddad5d9591bc530a808e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036618&auth_key=1760036618-0-0-20e08d05fd345719d26bef79cf2687a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c113eaea4a7b9e59a1751bba83f267b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036626&auth_key=1760036626-0-0-71fca11f8db3087995c167fd3f00c03c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be5ad6fc02934e1f147905d407af699d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036633&auth_key=1760036633-0-0-0b0b00d0f5357a6f0cf26596b9ce16fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e77d7b3dd5bff4cf1e70f514d485fdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036639&auth_key=1760036639-0-0-2a5ce6bde0f6d80a11132c85e3ed28bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc6af866ee6488bd5e0bd466039122ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036646&auth_key=1760036646-0-0-8b8599d87203b3d450702f85437e559f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GLVD-Guided-Learned-Vertex-Descent"><a href="#GLVD-Guided-Learned-Vertex-Descent" class="headerlink" title="GLVD: Guided Learned Vertex Descent"></a>GLVD: Guided Learned Vertex Descent</h2><p><strong>Authors:Pol Caselles Rico, Francesc Moreno Noguer</strong></p>
<p>Existing 3D face modeling methods usually depend on 3D Morphable Models, which inherently constrain the representation capacity to fixed shape priors. Optimization-based approaches offer high-quality reconstructions but tend to be computationally expensive. In this work, we introduce GLVD, a hybrid method for 3D face reconstruction from few-shot images that extends Learned Vertex Descent (LVD) by integrating per-vertex neural field optimization with global structural guidance from dynamically predicted 3D keypoints. By incorporating relative spatial encoding, GLVD iteratively refines mesh vertices without requiring dense 3D supervision. This enables expressive and adaptable geometry reconstruction while maintaining computational efficiency. GLVD achieves state-of-the-art performance in single-view settings and remains highly competitive in multi-view scenarios, all while substantially reducing inference time. </p>
<blockquote>
<p>现有的3D人脸建模方法通常依赖于3D可变形模型，这固有地限制了其表示能力，只能表现为固定的形状先验。基于优化的方法虽然能提供高质量的重建，但计算成本往往很高。在这项工作中，我们引入了GLVD，这是一种用于从少量图像进行3D人脸重建的混合方法。GLVD扩展了学习顶点下降法（LVD），通过整合顶点神经场优化与动态预测的全局结构指导的3D关键点，实现了在少量图像下的3D人脸重建。通过融入相对空间编码，GLVD能够迭代优化网格顶点，而无需密集的3D监督。这实现了在保持计算效率的同时，进行生动且可适应的几何重建。GLVD在单视图设置中达到了最先进的性能，在多视图场景中仍具有高度竞争力，同时大大降低了推理时间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06046v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于少量图像的3D人脸重建的混合方法GLVD，该方法结合了顶点神经场优化和动态预测的全局结构指导，实现了高效、灵活的人脸重建。GLVD通过引入相对空间编码，能够在不需要密集3D监督的情况下迭代优化网格顶点，达到表情丰富、适应性强且计算效率高的几何重建效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLVD是一种用于从少量图像进行3D人脸重建的混合方法。</li>
<li>GLVD结合了顶点神经场优化和全局结构指导，提高了重建的质量和效率。</li>
<li>相对空间编码的引入使得GLVD能够在不需要密集3D监督的情况下迭代优化网格顶点。</li>
<li>GLVD在单视图和多视图场景下均表现出卓越性能。</li>
<li>GLVD大幅降低了推理时间，提高了计算效率。</li>
<li>GLVD在人脸重建中实现了表情丰富、适应性强的人脸几何重建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06046">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-422fe3bf11c3d18db720d515814b4765~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036653&auth_key=1760036653-0-0-844cb16eae200a7fd79e198f6be1725a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1501d4133c93e74ec2e61cd6995cb298~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036660&auth_key=1760036660-0-0-9f8f5dd91134e6ce39116ea19ce4723c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e19e6e9a095de611898a5f7b6aee396~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036667&auth_key=1760036667-0-0-96d2826663cdd300cb87b036a13631c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a587d4cf8c0682268bd3e825fb0354e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036674&auth_key=1760036674-0-0-cc983c5bc2abb5b8ca33ac17f8f1b3ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac7b665bae4c95fc4808dddccd97527e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036681&auth_key=1760036681-0-0-f520a8da42059693b843ebb6618b547f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Training-Free-Time-Series-Classification-via-In-Context-Reasoning-with-LLM-Agents"><a href="#Training-Free-Time-Series-Classification-via-In-Context-Reasoning-with-LLM-Agents" class="headerlink" title="Training-Free Time Series Classification via In-Context Reasoning with   LLM Agents"></a>Training-Free Time Series Classification via In-Context Reasoning with   LLM Agents</h2><p><strong>Authors:Songyuan Sui, Zihang Xu, Yu-Neng Chuang, Kwei-Herng Lai, Xia Hu</strong></p>
<p>Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at <a target="_blank" rel="noopener" href="https://github.com/SongyuanSui/FETATSC">https://github.com/SongyuanSui/FETATSC</a>. </p>
<blockquote>
<p>时间序列分类（TSC）涵盖了多种应用场景，但标注数据通常稀缺，使得针对特定任务的训练成本高昂且不够灵活。最近以推理为导向的大型语言模型（LLM）在理解时间序列模式方面显示出潜力，但纯零样本使用仍然不够理想。我们提出了FETA，这是一个基于范例的上下文推理的完全无训练TSC多智能体框架。FETA将多元序列分解成通道级别的子问题，为每个通道检索几个结构相似的标注范例，并利用推理LLM来比较查询与这些范例，以自我评估的信心产生通道级别的标签；一个信心加权聚合器随后融合所有通道决策。这种设计消除了对预训练或微调的需求，通过删除无关通道和控制输入长度来提高效率，并通过范例接地和信心估计增强可解释性。在九个具有挑战性的UEA数据集上，FETA在完全无训练的设置下实现了较高的准确性，超越了多个训练过的基线。这些结果表明，多智能体上下文推理框架可以将LLM转变为无需任何参数训练的竞争型、即插即用TSC求解器。代码可在<a target="_blank" rel="noopener" href="https://github.com/SongyuanSui/FETATSC">https://github.com/SongyuanSui/FETATSC</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05950v1">PDF</a> 8 pages main content, 12 pages total including appendix, 1 figure</p>
<p><strong>Summary</strong>：</p>
<p>针对时间序列分类（TSC）中标签数据稀缺的问题，提出了一种基于范例的上下文推理的多智能体框架FETA，用于无训练TSC。FETA将多元序列分解为通道级子问题，通过范例检索和推理LLM进行比较，产生具有自我评估置信度的通道级标签。一个基于置信度加权的聚合器融合所有通道决策，无需预训练或微调，提高了效率和可解释性。在UEA数据集上的实验表明，FETA在完全无训练设置下取得了强大的准确性。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>时间序列分类（TSC）中标签数据稀缺，任务特定训练成本高昂且不够灵活。</li>
<li>提出了一种基于范例的上下文推理的多智能体框架FETA，用于无训练TSC。</li>
<li>FETA通过分解多元序列为通道级子问题，检索结构相似的标记范例，利用推理LLM进行比较。</li>
<li>产生通道级标签具有自我评估的置信度。</li>
<li>置信度加权聚合器融合所有通道决策。</li>
<li>FETA无需预训练或微调，提高了效率，通过范例接地和置信度估计增强了可解释性。</li>
<li>在UEA数据集上的实验表明，FETA在无训练设置下取得了强大的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05950">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-38b2e1614f21fcf8c4926fe7c4dabe0f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036688&auth_key=1760036688-0-0-fc41cb5c0a101111571c573cd71b62b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79b2accfe7dcb236d9d5bf274ef05e99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036695&auth_key=1760036695-0-0-b131c694d4e10954b5b14fecf830c583&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Syn-Diag-An-LLM-based-Synergistic-Framework-for-Generalizable-Few-shot-Fault-Diagnosis-on-the-Edge"><a href="#Syn-Diag-An-LLM-based-Synergistic-Framework-for-Generalizable-Few-shot-Fault-Diagnosis-on-the-Edge" class="headerlink" title="Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot   Fault Diagnosis on the Edge"></a>Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot   Fault Diagnosis on the Edge</h2><p><strong>Authors:Zijun Jia, Shuang Liang, Jinsong Yu</strong></p>
<p>Industrial fault diagnosis faces the dual challenges of data scarcity and the difficulty of deploying large AI models in resource-constrained environments. This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that leverages Large Language Models to overcome these limitations in few-shot fault diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic Synergy, which aligns signal features with the LLM’s semantic space through cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically constructs contextual prompts to enhance diagnostic accuracy with limited samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create a lightweight, efficient edge model capable of online updates via a shared decision space. Extensive experiments on six datasets covering different CWRU and SEU working conditions show that Syn-Diag significantly outperforms existing methods, especially in 1-shot and cross-condition scenarios. The edge model achieves performance comparable to the cloud version while reducing model size by 83% and latency by 50%, offering a practical, robust, and deployable paradigm for modern intelligent diagnostics. </p>
<blockquote>
<p>工业故障检测面临着数据稀缺和在资源受限环境中部署大型AI模型的双重挑战。本文介绍了一种新型的云边协同框架Syn-Diag，它利用大型语言模型来克服小样例故障检测中的这些限制。Syn-Diag建立在三层机制上：1）视觉语义协同，通过跨模态预训练将信号特征与LLM的语义空间对齐；2）内容感知推理，动态构建上下文提示，以提高有限样本的诊断准确性；3）云边协同，利用知识蒸馏创建轻量级、高效的边缘模型，通过共享决策空间进行在线更新。在涵盖不同CWRU和SEU工作条件的六个数据集上的大量实验表明，Syn-Diag显著优于现有方法，特别是在1-shot和跨场景条件下。边缘模型的性能与云版本相当，同时模型大小减少了83%，延迟降低了50%，为现代智能诊断提供了实用、稳健和可部署的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Syn-Diag的云端协同诊断框架，用于解决工业故障诊断中的数据稀缺和模型部署困难的问题。该框架利用大型语言模型，通过视觉语义协同、内容感知推理和云端协同机制，实现了在少量样本下的高效故障诊断。实验证明，Syn-Diag在多种数据集上显著优于现有方法，特别是单样本和跨条件场景下的表现更为出色。边缘模型的性能与云端版本相当，同时减小了模型体积和延迟，为现代智能诊断提供了实用、稳健和可部署的范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Syn-Diag框架解决了工业故障数据稀缺和大型AI模型部署困难的问题。</li>
<li>通过视觉语义协同，将信号特征与LLM语义空间对齐。</li>
<li>内容感知推理机制提高了诊断的准确性和样本的有限性。</li>
<li>云端协同利用知识蒸馏创建轻量级边缘模型，支持在线更新。</li>
<li>实验证明Syn-Diag在多种数据集上表现优异，特别是在单样本和跨条件场景下。</li>
<li>边缘模型性能与云端版本相当，同时显著减小了模型体积和延迟。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c7356733d2d17e36a80ff2f1f8d36a27~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036702&auth_key=1760036702-0-0-040bfff8a24c0304050662e2132723e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f68dd520e83fd2f40c1ccc4c9dfd2d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036710&auth_key=1760036710-0-0-e8f92e36672e6c4d6a674e871989a8e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="nnSAM2-nnUNet-Enhanced-One-Prompt-SAM2-for-Few-shot-Multi-Modality-Segmentation-and-Composition-Analysis-of-Lumbar-Paraspinal-Muscles"><a href="#nnSAM2-nnUNet-Enhanced-One-Prompt-SAM2-for-Few-shot-Multi-Modality-Segmentation-and-Composition-Analysis-of-Lumbar-Paraspinal-Muscles" class="headerlink" title="nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality   Segmentation and Composition Analysis of Lumbar Paraspinal Muscles"></a>nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality   Segmentation and Composition Analysis of Lumbar Paraspinal Muscles</h2><p><strong>Authors:Zhongyi Zhang, Julie A. Hides, Enrico De Martino, Abdul Joseph Fofanah, Gervase Tuxworth</strong></p>
<p>Purpose: To develop and validate No-New SAM2 (nnsam2) for few-shot segmentation of lumbar paraspinal muscles using only a single annotated slice per dataset, and to assess its statistical comparability with expert measurements across multi-sequence MRI and multi-protocol CT.   Methods: We retrospectively analyzed 1,219 scans (19,439 slices) from 762 participants across six datasets. Six slices (one per dataset) served as labeled examples, while the remaining 19,433 slices were used for testing. In this minimal-supervision setting, nnsam2 used single-slice SAM2 prompts to generate pseudo-labels, which were pooled across datasets and refined through three sequential, independent nnU-Net models. Segmentation performance was evaluated using the Dice similarity coefficient (DSC), and automated measurements-including muscle volume, fat ratio, and CT attenuation-were assessed with two one-sided tests (TOST) and intraclass correlation coefficients (ICC).   Results: nnsam2 outperformed vanilla SAM2, its medical variants, TotalSegmentator, and the leading few-shot method, achieving DSCs of 0.94-0.96 on MR images and 0.92-0.93 on CT. Automated and expert measurements were statistically equivalent for muscle volume (MRI&#x2F;CT), CT attenuation, and Dixon fat ratio (TOST, P &lt; 0.05), with consistently high ICCs (0.86-1.00).   Conclusion: We developed nnsam2, a state-of-the-art few-shot framework for multi-modality LPM segmentation, producing muscle volume (MRI&#x2F;CT), attenuation (CT), and fat ratio (Dixon MRI) measurements that were statistically comparable to expert references. Validated across multimodal, multicenter, and multinational cohorts, and released with open code and data, nnsam2 demonstrated high annotation efficiency, robust generalizability, and reproducibility. </p>
<blockquote>
<p>目的：旨在为腰椎旁肌肉的少样本分割开发并验证No-New SAM2（nnsam2），该方法仅使用每个数据集的一个标注切片，并评估其在多序列MRI和多协议CT上与专家测量的统计可比性。方法：我们回顾性分析了6个数据集中的762名参与者的1,219次扫描（共19,439个切片）。其中六个切片（每个数据集一个）作为标记示例，其余19,433个切片用于测试。在这种最小监督设置下，nnsam2使用单个切片的SAM2提示来生成伪标签，这些标签被汇集在数据集中，并通过三个顺序独立的nnU-Net模型进行改进。使用Dice相似性系数（DSC）评估分割性能，并使用两次单侧检验（TOST）和组内相关系数（ICC）评估自动化测量，包括肌肉体积、脂肪比和CT衰减。结果：nnsam2的表现优于基础SAM2、其医学变体、TotalSegmentator以及领先的少样本方法，在MR图像上达到0.94-0.96的DSC，在CT上达到0.92-0.93的DSC。自动化和专家测量的肌肉体积（MRI&#x2F;CT）、CT衰减和Dixon脂肪比（TOST，P &lt; 0.05）在统计上是相当的，并且具有始终较高的ICC（0.86-1.00）。结论：我们开发了最先进的少样本框架nnsam2，用于多模态LPM分割，可生成与专家参考相比具有统计可比性的肌肉体积（MRI&#x2F;CT）、衰减（CT）和脂肪比（Dixon MRI）测量值。经过跨多模态、多中心和跨国队列的验证，并以开放代码和数据发布，nnsam2显示出高的标注效率、稳健的通用性和可重复性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05555v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>nnsam2方法用于基于少量标注数据的腰椎旁肌肉分割，仅使用每个数据集的一个标注切片。该方法在多种序列MRI和多协议CT上的表现与专家测量具有统计可比性。通过对大量扫描数据的回顾性分析和独立模型验证，结果显示nnsam2的分割性能优秀，自动化测量结果与专家测量结果相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>nnsam2是一种用于腰椎旁肌肉少样本分割的方法，仅需要每个数据集的一个标注切片。</li>
<li>nnsam2在多种序列MRI和多协议CT上具有优秀的分割性能。</li>
<li>nnsam2使用单切片SAM2提示生成伪标签，并跨数据集整合和精炼。</li>
<li>通过Dice similarity coefficient (DSC)评估分割性能。</li>
<li>自动化测量结果与专家测量结果具有统计等效性，包括肌肉体积、脂肪比和CT衰减。</li>
<li>nnsam2具有高的标注效率、稳健的通用性和可重复性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05555">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1e3d78d43314b93194234bf3cb4c9483~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036717&auth_key=1760036717-0-0-0b70b515c1bd5c0a61b5aca8b30f5c7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Attention-Enhanced-Prototypical-Learning-for-Few-Shot-Infrastructure-Defect-Segmentation"><a href="#Attention-Enhanced-Prototypical-Learning-for-Few-Shot-Infrastructure-Defect-Segmentation" class="headerlink" title="Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure   Defect Segmentation"></a>Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure   Defect Segmentation</h2><p><strong>Authors:Christina Thrainer, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Christian Guetl, Steven Sloan, Kendall N. Niles, Ken Pathak</strong></p>
<p>Few-shot semantic segmentation is vital for deep learning-based infrastructure inspection applications, where labeled training examples are scarce and expensive. Although existing deep learning frameworks perform well, the need for extensive labeled datasets and the inability to learn new defect categories with little data are problematic. We present our Enhanced Feature Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert and sewer defect categories using a prototypical learning framework. Our approach has three main contributions: (1) adaptive E-FPN encoder using InceptionSepConv blocks and depth-wise separable convolutions for efficient multi-scale feature extraction; (2) prototypical learning with masked average pooling for powerful prototype generation from small support examples; and (3) attention-based feature representation through global self-attention, local self-attention and cross-attention. Comprehensive experimentation on challenging infrastructure inspection datasets illustrates that the method achieves excellent few-shot performance, with the best configuration being 8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way classification testing. The self-attention method had the most significant performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over baselines. Our framework addresses the critical need to rapidly respond to new defect types in infrastructure inspection systems with limited new training data that lead to more efficient and economical maintenance plans for critical infrastructure systems. </p>
<blockquote>
<p>少样本语义分割对于基于深度学习的设施检测应用至关重要，因为这些应用的训练样本既稀缺又昂贵。尽管现有的深度学习框架表现良好，但对大量标注数据集的需求以及无法用少量数据学习新缺陷类别的问题仍然存在。我们提出了增强型特征金字塔网络（E-FPN）框架，利用原型学习框架对涵洞和下水道缺陷类别进行少样本语义分割。我们的方法主要有三个贡献：（1）使用InceptionSepConv块和深度可分离卷积的自适应E-FPN编码器，进行高效的多尺度特征提取；（2）利用掩模平均池化的原型学习，从小量支持样本中生成强大的原型；（3）基于注意力的特征表示，通过全局自注意力、局部自注意力和交叉注意力。具有挑战性的设施检测数据集上的综合实验表明，该方法实现了出色的少样本性能，最佳配置为8路5次训练的配置，在2路分类测试中达到82.55%的F1分数和72.26%的mIoU。自注意力方法具有最显著的性能提升，相比基线提高了2.57%的F1分数和2.9%的mIoU。我们的框架解决了设施检测系统中对有限新训练数据快速响应新缺陷类型的迫切需求，从而为关键设施系统制定更高效、更经济的维护计划。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05266v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对深度学习在基础设施检测中应用的few-shot语义分割问题，提出一种增强型特征金字塔网络（E-FPN）框架。该框架使用原型学习框架对桥涵和排水沟缺陷类别进行few-shot语义分割。主要贡献包括自适应E-FPN编码器、原型学习和注意力机制。实验表明，该方法在具有挑战性的基础设施检测数据集上取得了良好的few-shot性能，且自注意力方法具有最佳表现，显著提高了基线性能。该框架满足了在有限新训练数据下对基础设施检测系统中新缺陷类型的快速响应需求，有助于提高关键基础设施系统的维护效率和经济效益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot语义分割在深度学习为基础设施检测应用中至关重要，特别是在缺乏标注训练样本的情况下。</li>
<li>现有深度学习框架存在对新缺陷类别学习不足的问题。</li>
<li>提出的E-FPN框架包括自适应E-FPN编码器、原型学习和注意力机制三个主要贡献。</li>
<li>自适应E-FPN编码器通过InceptionSepConv块和深度可分离卷积实现高效多尺度特征提取。</li>
<li>原型学习采用掩码平均池化生成强大原型，基于小规模支持样本。</li>
<li>注意力机制包括全局自注意力、局部自注意力和交叉注意力，有助于提升特征表示能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5827881180b5fc35b65b2860aedd5e58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036725&auth_key=1760036725-0-0-befb799209a1b8ea4ef46306386d7ddc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12f64b9c91e8adecbc7b5085decf48eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036732&auth_key=1760036732-0-0-8b9cb88ec2161fd68faf0fd90c169782&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HyperVLA-Efficient-Inference-in-Vision-Language-Action-Models-via-Hypernetworks"><a href="#HyperVLA-Efficient-Inference-in-Vision-Language-Action-Models-via-Hypernetworks" class="headerlink" title="HyperVLA: Efficient Inference in Vision-Language-Action Models via   Hypernetworks"></a>HyperVLA: Efficient Inference in Vision-Language-Action Models via   Hypernetworks</h2><p><strong>Authors:Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson</strong></p>
<p>Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MasterXiong/HyperVLA">https://github.com/MasterXiong/HyperVLA</a> </p>
<blockquote>
<p>基于具有强大泛化能力的语言和视觉基础模型，并在大规模机器人数据上进行训练，视觉-语言-动作（VLA）模型最近作为学习通用机器人策略的一种有前途的方法而出现。然而，现有VLA的一个关键缺点是推理成本极高。针对这一问题，本文提出了HyperVLA解决方案。与现有的在训练和推理过程中都会激活整个模型的单一VLA不同，HyperVLA采用了一种基于超网络（HN）的新型架构，该架构在推理时只激活一小部分特定任务的策略，同时在训练时仍保留容纳多种多任务行为所需的高模型容量。成功训练基于HN的VLA并不容易，因此HyperVLA包含一些关键算法设计功能来提高其性能，包括适当利用现有视觉基础模型的先验知识、HN归一化以及动作生成策略。与单一VLA相比，HyperVLA在零样本泛化和少量适应方面达到了相似甚至更高的成功率，同时大大降低了推理成本。与最先进的VLA模型OpenVLA相比，HyperVLA在测试时减少了被激活参数的数量（减少90倍），并加快了推理速度（加速120倍）。代码公开在<a target="_blank" rel="noopener" href="https://github.com/MasterXiong/HyperVLA%E3%80%82">https://github.com/MasterXiong/HyperVLA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04898v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于语言和视觉基础模型的通用机器人策略学习新方法——HyperVLA。该方法使用超网络（HN）架构，在推理时仅激活小型的特定任务策略，降低了推理成本，同时保留了在训练期间适应多样多任务行为所需的高模型容量。HyperVLA还包括一些关键算法设计特点，如利用现有视觉基础模型的先验知识、HN归一化和动作生成策略。相较于传统的单一体VLAs，HyperVLA在零样本泛化和少量适应方面取得了相似甚至更高的成功率，同时显著降低了推理成本。与现有的顶尖VLA模型OpenVLA相比，HyperVLA在测试时激活的参数数量减少了90倍，推理速度提高了120倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyperVLA是一种基于语言和视觉基础模型的机器人策略学习方法，旨在解决现有VLAs高推理成本的问题。</li>
<li>HyperVLA采用超网络（HN）架构，推理时仅激活小型的任务特定策略。</li>
<li>HyperVLA在训练过程中保留高模型容量以适应多种任务行为。</li>
<li>HyperVLA利用现有视觉基础模型的先验知识，并包括HN归一化和动作生成策略等关键算法设计特点。</li>
<li>与传统单一体VLAs相比，HyperVLA在泛化和适应方面表现优越。</li>
<li>HyperVLA较现有的顶尖VLA模型OpenVLA，显著减少了测试时激活的参数数量和推理时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04898">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-eda2b8956b83118d080c97b92cef1bb1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036740&auth_key=1760036740-0-0-b24d93cbcef9b03a28bc207199854524&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9fb7d5e0c7afec9026cf85a267cae41~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036747&auth_key=1760036747-0-0-bde5110f6c96adf9e20a36e7f4f0abb3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b89a545dac868205e7366aeb1df9078~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036754&auth_key=1760036754-0-0-d52c5e5ce09a003e79a161e74a626d19&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAFA-SNN-Sparsity-Aware-On-Device-Few-Shot-Class-Incremental-Learning-with-Fast-Adaptive-Structure-of-Spiking-Neural-Network"><a href="#SAFA-SNN-Sparsity-Aware-On-Device-Few-Shot-Class-Incremental-Learning-with-Fast-Adaptive-Structure-of-Spiking-Neural-Network" class="headerlink" title="SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning   with Fast-Adaptive Structure of Spiking Neural Network"></a>SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning   with Fast-Adaptive Structure of Spiking Neural Network</h2><p><strong>Authors:Huijing Zhang, Muyang Cao, Linshan Jiang, Xin Du, Di Yu, Changze Lv, Shuiguang Deng</strong></p>
<p>Continuous learning of novel classes is crucial for edge devices to preserve data privacy and maintain reliable performance in dynamic environments. However, the scenario becomes particularly challenging when data samples are insufficient, requiring on-device few-shot class-incremental learning (FSCIL) to maintain consistent model performance. Although existing work has explored parameter-efficient FSCIL frameworks based on artificial neural networks (ANNs), their deployment is still fundamentally constrained by limited device resources. Inspired by neural mechanisms, Spiking neural networks (SNNs) process spatiotemporal information efficiently, offering lower energy consumption, greater biological plausibility, and compatibility with neuromorphic hardware than ANNs. In this work, we present an SNN-based method for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We first propose sparsity-conditioned neuronal dynamics, in which most neurons remain stable while a subset stays active, thereby mitigating catastrophic forgetting. To further cope with spike non-differentiability in gradient estimation, we employ zeroth-order optimization. Moreover, during incremental learning sessions, we enhance the discriminability of new classes through subspace projection, which alleviates overfitting to novel classes. Extensive experiments conducted on two standard benchmark datasets (CIFAR100 and Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture, and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods, specifically achieving at least 4.01% improvement at the last incremental session on Mini-ImageNet and 20% lower energy cost over baseline methods with practical implementation. </p>
<blockquote>
<p>持续学习新类别对于边缘设备在动态环境中保持数据隐私和可靠性能至关重要。然而，当数据样本不足时，情况变得尤为具有挑战性，需要设备上的小样本类别增量学习（FSCIL）来维持模型的持续性能。虽然现有工作已经探索了基于人工神经网络（ANN）的参数有效FSCIL框架，但其部署仍然受到有限设备资源的根本制约。受神经机制的启发，脉冲神经网络（SNNs）能够高效处理时空信息，与ANN相比，具有更低的能耗、更大的生物合理性和与神经形态硬件的兼容性。在这项工作中，我们提出了一种基于SNN的设备上FSCIL方法，即稀疏感知和快速自适应SNN（SAFA-SNN）。我们首先提出稀疏条件神经元动态，其中大多数神经元保持稳定，而一小部分神经元保持活跃，从而减轻灾难性遗忘。为了进一步应对梯度估计中的脉冲不可微性，我们采用零阶优化。此外，在增量学习期间，我们通过子空间投影增强新类别的辨别力，从而减轻对新类别的过度拟合。在CIFAR100和Mini-ImageNet两个标准基准数据集以及CIFAR-10-DVS、DVS128gesture和N-Caltech1个基准数据集上进行的大量实验表明，SAFA-SNN优于基准方法，特别是在Mini-ImageNet的最后一个增量会话上至少提高了4.01%，并且在实践中实现了比基准方法低20%的能耗成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于脉冲神经网络的高效在线少样本类增量学习。为解决边缘设备在动态环境中保护数据隐私并保持可靠性能的问题，研究提出一种基于脉冲神经网络（SNN）的在线少样本类增量学习方法——稀疏感知快速自适应SNN（SAFA-SNN）。通过稀疏性条件神经元动态、零阶优化和子空间投影等技术，实现了高效的增量学习和良好的性能表现。实验结果表明，SAFA-SNN在多个数据集上优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>边缘设备在动态环境中进行持续的新型类别学习至关重要。</li>
<li>数据样本不足时，需要采用基于脉冲神经网络的少样本类增量学习方法（FSCIL）。</li>
<li>SAFA-SNN方法通过稀疏性条件神经元动态减轻灾难性遗忘问题。</li>
<li>采用零阶优化应对脉冲的非可微性，在梯度估计中进行优化。</li>
<li>子空间投影技术用于提高新类别的可分辨性，减轻对新类别的过度拟合。</li>
<li>实验结果表明SAFA-SNN在多个数据集上的性能优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03648">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4e3cfec71f93594b0960135c27fdd93b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036761&auth_key=1760036761-0-0-67bcd2b742e463e1026a570fa155ffc2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a96a8d913428391dea974e84394717aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036769&auth_key=1760036769-0-0-efa8f0fc20ca33147a60ce99c997ce1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7afefb38335b5d16d081f9f1abe461f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036776&auth_key=1760036776-0-0-c95102511f9ed88bf6c6d97b07bad214&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>少量学习（FSL）旨在从仅有的几个标记支持样本中识别出新概念。最近的研究通过融入额外的语义信息或设计复杂的语义融合模块来增强支持特征。然而，由于缺乏在实际实例中的定位，它们仍然会遭受与视觉证据相矛盾的幻想语义的困扰，导致产生嘈杂的指导和昂贵的修正。为了解决这些问题，我们提出了一种新的框架——利用大型语言模型（LLM）进行视觉与文本桥接的少量学习（VT-FSL）。该框架构建了精确跨模态提示，这些提示基于大型语言模型（LLM）和支持图像，通过几何感知对齐无缝集成它们。它主要由跨模态迭代提示（CIP）和跨模态几何对齐（CGA）组成。具体而言，CIP根据类名和辅助图像对LLM进行条件处理，以在单个结构化推理过程中迭代生成精确的类描述。这些描述不仅丰富了对新颖类的语义理解，还实现了语义一致图像的零样本合成。这些描述和合成图像分别作为补充的文本和视觉提示，提供高级类语义和低级类内多样性，以弥补有限的辅助数据。此外，CGA通过最小化它们所跨越的3维平行四边形的核化体积来联合对齐融合的文本、支持和合成视觉表示。它捕捉了所有表示之间的全局和非线性关系，实现了结构化且一致的多模式集成。所提出的VT-FSL方法在包括标准、跨域和细粒度少量学习场景在内的十个不同基准测试上创下了最新技术水平。代码可在[<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/peacelwh/VT-FSL找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>摘要</strong></p>
<p>本文介绍了针对小样学习（FSL）的一种新方法——结合视觉和文本与大型语言模型（LLM）的小样学习框架（VT-FSL）。该框架通过构建精确的跨模态提示和基于支持图像的迭代提示，无缝集成它们通过几何感知对齐。它主要包括跨模态迭代提示（CIP）和跨模态几何对齐（CGA）。CIP以类名和图像为条件来生成精确类描述，这些描述不仅丰富了对新型语义的理解，还实现了语义一致图像的零样本合成。合成图像作为辅助视觉提示，与描述共同为有限的支持数据提供了高级类语义和低级类内多样性补偿。CGA则通过最小化跨越的三维平行四边形的核化体积来联合对齐融合后的文本、支持和合成视觉表示。它捕捉了所有表示之间的全局和非线性关系，实现了结构化且一致的多模态集成。VT-FSL方法在十个不同基准测试中建立了最新性能标准，包括标准、跨域和精细分类小样学习场景。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Few-shot learning (FSL)旨在从少量标记样本中识别新概念。</li>
<li>最近的研究通过增加额外的语义信息或设计复杂的语义融合模块来增强支持特征。</li>
<li>缺乏基于实际实例的接地会导致语义上的虚构现象和视觉证据相矛盾，造成昂贵的修正和嘈杂的引导。</li>
<li>提出的VT-FSL框架结合了视觉和文本与大型语言模型（LLM），构建了精确的跨模态提示和基于支持图像的迭代提示。它主要由跨模态迭代提示（CIP）和跨模态几何对齐（CGA）组成。</li>
<li>CIP通过生成精确类描述来丰富对新型语义的理解，并实现了语义一致图像的零样本合成。合成图像作为辅助视觉提示，提供了高级类语义和低级类内多样性的补偿。</li>
<li>CGA通过最小化跨越的三维平行四边形的核化体积来联合对齐文本、支持和合成视觉表示，实现了结构化且一致的多模态集成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0d52355fbe28b9263d60b1f090617a30~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036783&auth_key=1760036783-0-0-823b4438a87e004808326b7f2be497f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9af705fa082c84d4e881514f8fff464d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036790&auth_key=1760036790-0-0-61551928e9fc2a08b49db41b9320e0a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fda6a8e6593952cbc22784b0fba62ed6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036796&auth_key=1760036796-0-0-0d5d9d6111bfb4799e630c6b50f0ce28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-76bea1641940b6f14a37bc0429de3487~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036803&auth_key=1760036803-0-0-25d2b29256218e35de33310ad8be04f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a09d3087398134cf568e19fcf367318~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036809&auth_key=1760036809-0-0-a82a3174f990a161961ec3e8722ff109&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2dfdfdfab44386fe2ba27acf9d36e05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036816&auth_key=1760036816-0-0-f849fc7ba8936b049c1a464772d6cfa1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers"><a href="#Scaled-Signed-Averaging-Improves-In-Context-and-Early-Learning-Benchmark-Performance-in-Small-Transformers" class="headerlink" title="Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers"></a>Scaled Signed Averaging Improves In-Context and Early Learning Benchmark   Performance in Small Transformers</h2><p><strong>Authors:Omar Naim, Swarnadeep Bhar, Jérôme Bolte, Nicholas Asher</strong></p>
<p>While Large Language models’ abilities for in-context learning (ICL) have drawn much attention, we examine some of its limitations on semantic tasks involving quantifiers like “all” and “some”, as well as on tasks with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these limitations. We propose scaled signed averaging (SSA), a novel alternative to Softmax to mitigate these problems. We show that SSA significantly improves performance on our ICL tasks. In addition, SSA outperforms transformer models with Softmax on several early learning NLP benchmarks and linguistic probing tasks on zero and few-shot settings. </p>
<blockquote>
<p>虽然大型语言模型在上下文学习（ICL）方面的能力已经引起了广泛关注，但我们研究了一些其在涉及量词（如“所有”和“一些”）的语义任务以及线性函数任务上的局限性。我们确定了注意力机制中的评分函数Softmax是这些局限性的一个因素。我们提出了Scaled Signed Averaging（SSA）这一新型的Softmax替代方案来缓解这些问题。我们证明SSA在我们的ICL任务上显著提高性能。此外，在零样本和少样本设置下，SSA在多个早期学习NLP基准测试和语言学探测任务上的表现优于使用Softmax的Transformer模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14685v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的上下文学习能力（ICL）虽然备受关注，但在涉及量词（如“所有”和“一些”）的语义任务以及线性函数任务上存在一些局限性。本文发现注意力机制中的评分函数Softmax是这些局限性的原因之一。为此，本文提出了使用一种名为Scaled Signed Averaging（SSA）的新型评分函数来替代Softmax，以缓解这些问题。实验表明，SSA在ICL任务上的性能显著提高，并且在零样本和少样本环境下的早期学习NLP基准测试和语言学探测任务中优于使用Softmax的transformer模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的上下文学习能力（ICL）在涉及量词和线性函数的任务中存在局限性。</li>
<li>Softmax作为注意力机制中的评分函数，是这些局限性的原因之一。</li>
<li>提出了Scaled Signed Averaging（SSA）新型评分函数，以替代Softmax，改善模型性能。</li>
<li>SSA在ICL任务上的性能显著提高。</li>
<li>SSA在零样本和少样本环境下的早期学习NLP基准测试中表现优异。</li>
<li>SSA优于使用Softmax的transformer模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14685">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bd9e8c3495d42cf7895f0930feef9062~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036824&auth_key=1760036824-0-0-76d5d0aa43650cd58b3ce3e478b69bd8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d752081c225c043003d6f0538341e611~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036831&auth_key=1760036831-0-0-04a08f2b7671b0b9dd63c35dc795f146&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d957aebb0de3ec2be31891128f338b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036837&auth_key=1760036837-0-0-64cdc023e2f807913c3ae6235a1ec238&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Thinking-with-Nothinking-Calibration-A-New-In-Context-Learning-Paradigm-in-Reasoning-Large-Language-Models"><a href="#Thinking-with-Nothinking-Calibration-A-New-In-Context-Learning-Paradigm-in-Reasoning-Large-Language-Models" class="headerlink" title="Thinking with Nothinking Calibration: A New In-Context Learning Paradigm   in Reasoning Large Language Models"></a>Thinking with Nothinking Calibration: A New In-Context Learning Paradigm   in Reasoning Large Language Models</h2><p><strong>Authors:Haotian Wu, Bo Xu, Yao Shu, Menglin Yang, Chengwei Qin</strong></p>
<p>Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt with two different answers. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT), thinking twice and majority voting. Moreover, it achieves comparable in-distribution performance to training-based SOTA reasoning method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing the importance of structural thinking diversity and the benefits of consistency check. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs. </p>
<blockquote>
<p>近期，大型推理语言模型（RLLMs）通过结构化及多步骤推理展现了显著的实力。尽管先前的研究主要聚焦于改进其训练和推理策略，但它们对于情境内学习（ICL）的潜力却鲜有探索。为了填补这一空白，我们提出了“无思考校准思考”（JointThinking）这一全新的ICL范式，它提示模型并行生成两种答案：一种是思考模式，另一种是无思考模式。当两个初步回答不一致时，才会触发第二轮思考，并使用一个带有两个不同答案的单一提示。在多个推理基准测试上的广泛实验表明，JointThinking显著优于少镜头思维链（CoT）、三思而后行和多数投票。而且，它在基于训练的状态最优推理方法上实现了相当的表现，同时在离分布任务上表现出显著的优势。我们进一步对校准机制进行了系统分析，显示了结构性思维多样性的重要性以及一致性检查的好处。此外，我们观察到实际与理想推理之间的性能差距随着第二次思考时模型规模的增加而缩小，这表明了我们的方法具有很强的可扩展性。最后，我们讨论了当前的局限性，并为未来RLLMs的ICL研究提出了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03363v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的推理能力近期备受瞩目，通过结构和多步骤推理展现出卓越的性能。尽管已有大量关于改进其训练和推理策略的研究，但其上下文学习（ICL）的潜力尚未得到充分探索。本文提出一种名为“无思考校准思考”（JointThinking）的新ICL范式，它鼓励模型同时生成两种答案：一种为思考模式，另一种为无思考模式。仅在两种初始回答不一致时，才会触发第二轮思考。在多个推理基准测试上的广泛实验表明，JointThinking显著优于少样本思维链（CoT）、二次思考和多数投票。此外，它在内部分布任务上的表现与基于训练的最新推理方法相当，但在外部分布任务上表现出显著优势。本文对校准机制进行了系统分析，强调了结构性思维多样性和一致性检查的重要性。同时观察到，随着模型大小的增加，实际与理想推理之间的性能差距缩小，这表明我们方法具有很强的可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在结构和多步骤推理方面表现出卓越性能。</li>
<li>上下文学习（ICL）在大型语言模型中的潜力尚未充分探索。</li>
<li>JointThinking是一种新的ICL范式，通过生成两种答案（思考模式和无思考模式）来触发第二轮思考。</li>
<li>JointThinking在多个推理基准测试上表现优异，优于少样本思维链、二次思考和多数投票。</li>
<li>JointThinking在内部和外部任务分布上均表现出强大性能。</li>
<li>系统分析强调了结构性思维多样性和一致性检查在校准机制中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-28c457b4da0e8983aae91e8c1800fb7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036844&auth_key=1760036844-0-0-d2d20d7195948569884e7195ebc5a3ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e742849f41d295ebe9510388a4166ba1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036852&auth_key=1760036852-0-0-2d6ff821f61b43dea292abbefed76f85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4e9db0d32d11f431ada16366ee070af8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036859&auth_key=1760036859-0-0-d12f04b3814dcaca14f68b237b2b3ea5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-406b20217aeed349e6639720ed5bcabb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036866&auth_key=1760036866-0-0-5a95eaf49a85ba06a808a46859af532c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd09f8a51aec2a99ac8e79da5b8ade0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036873&auth_key=1760036873-0-0-e54015b91106bba5d03ca746cc9fc94f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b44d4c5257a4285d28ddf447935c4223~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036879&auth_key=1760036879-0-0-a9df26d8c24e4bd8e9a21e98e346d4dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GLiDRE-Generalist-Lightweight-model-for-Document-level-Relation-Extraction"><a href="#GLiDRE-Generalist-Lightweight-model-for-Document-level-Relation-Extraction" class="headerlink" title="GLiDRE: Generalist Lightweight model for Document-level Relation   Extraction"></a>GLiDRE: Generalist Lightweight model for Document-level Relation   Extraction</h2><p><strong>Authors:Robin Armingaud, Romaric Besançon</strong></p>
<p>Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to complex interactions between entities across sentences. While supervised models have achieved strong results in fully resourced settings, their behavior with limited training data remains insufficiently studied. We introduce GLiDRE, a new compact model for document-level relation extraction, designed to work efficiently in both supervised and few-shot settings. Experiments in both low-resource supervised training and few-shot meta-learning benchmarks show that our approach outperforms existing methods in data-constrained scenarios, establishing a new state-of-the-art in few-shot document-level relation extraction. Our code will be publicly available. </p>
<blockquote>
<p>关系抽取（RE）是自然语言处理中的一项基本任务，其文档级变体由于句子间实体的复杂交互而带来重大挑战。虽然监督模型在全资源丰富的环境中取得了强大的效果，但在有限训练数据下的表现仍然研究不足。我们引入了GLiDRE，这是一种新的紧凑型文档级关系抽取模型，旨在在监督和小样本环境中都能高效工作。在低资源监督训练和少样本元学习基准测试的实验表明，我们的方法在数据受限的场景中表现优于现有方法，在少样本文档级关系抽取中建立了新的最新技术状态。我们的代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00757v2">PDF</a> Submitted to ARR October</p>
<p><strong>Summary</strong></p>
<p>本文介绍了GLiDRE模型，这是一种针对文档级关系抽取的新颖紧凑模型。该模型旨在实现在有监督学习和少样本场景下的高效运作。实验表明，在数据受限的监督训练和少样本元学习基准测试中，该方法在数据受限的场景中优于现有方法，为文档级关系抽取领域树立了新的里程碑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLiDRE是一个用于文档级关系抽取的紧凑模型。</li>
<li>该模型旨在在有监督学习和少样本场景下高效运作。</li>
<li>在数据受限的监督训练中，GLiDRE表现优于现有方法。</li>
<li>在少样本元学习基准测试中，GLiDRE为文档级关系抽取领域树立了新的里程碑。</li>
<li>GLiDRE模型能够处理复杂的实体间交互。</li>
<li>该模型能够应对文档中的跨句子实体关系抽取挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00757">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5cb7991706cdc5e18d71f2de88018787~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036887&auth_key=1760036887-0-0-50bb6fcb2f27a36d1bb93bdbaab13742&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a923be0eb47871d49495aaea9dac17a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036916&auth_key=1760036916-0-0-835b7fbd2d91432eca3ccee000ebd3aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c40055895592eea7c9198a520b7f86cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036923&auth_key=1760036923-0-0-040f2bb75b4983822ab269a6e14c65a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 17 mAP! Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl">https://github.com/roboflow/rf100-vl</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a>. </p>
<blockquote>
<p>训练于互联网规模数据的视觉语言模型（VLMs）在常见物体（如汽车、卡车和行人）上的零样本检测性能显著。然而，最先进的模型在推广到其预训练中没有出现的分布外的类别、任务和成像模式时仍面临困难。我们主张，与其简单地使用更多的视觉数据重新训练VLMs，不如使用包含几个视觉示例和丰富的文本描述的注释指令来对齐VLMs的新概念。为此，我们推出了Roboflow100-VL，这是一个包含100个多模式对象检测数据集的大规模集合，其中包含的概念多样且在其VLM预训练中并不常见。我们在基准测试上对最先进的模型进行了零样本、小样测试、半监督和完全监督下的评估，这允许在不同数据模式下进行比较。值得注意的是，我们发现像GroundingDINO和Qwen2.5-VL这样的VLM在Roboflow100-VL中的具有挑战性的医学影像数据集上的零样本准确率低于2%，这证明了小样本概念对齐的必要性。最后，我们讨论了最近的CVPR 2025基础FSOD竞赛并从社区中分享了一些见解。值得注意的是，冠军团队的表现超出我们的基线水平高达17 mAP！我们的代码和数据集可以在<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl%E5%92%8Chttps://universe.roboflow.com/rf100-vl/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/roboflow/rf100-vl和https://universe.roboflow.com/rf100-vl/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v3">PDF</a> The first two authors contributed equally. This work has been   accepted to the Neural Information Processing Systems (NeurIPS) 2025 Datasets   &amp; Benchmark Track. Project Page: <a target="_blank" rel="noopener" href="https://rf100-vl.org/">https://rf100-vl.org/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对互联网规模数据的视觉语言模型（VLMs）在常见物体上的零样本检测性能。然而，现有模型在泛化到非分布类别、任务和成像模式方面仍存在挑战。为此，作者引入了Roboflow100-VL，这是一个包含100个多模式对象检测数据集的大规模集合，其中包含各种不常见于VLM预训练的概念。作者在不同的数据环境下评估了现有模型，发现一些模型在具有挑战性的医疗图像数据集上的零样本准确率低于2%，突显了少样本概念对齐的必要性。此外，本文还讨论了CVPR 2025基础FSOD竞赛的见解，并分享了社区洞察。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在常见物体上实现了出色的零样本检测性能。</li>
<li>现有模型在泛化到非分布类别、任务和成像模式时仍面临挑战。</li>
<li>引入Roboflow100-VL，一个包含多样化概念的大规模多模式对象检测数据集。</li>
<li>一些模型在挑战性医疗图像数据集上的零样本准确率较低，突显少样本概念对齐的必要性。</li>
<li>CVPR 2025基础FSOD竞赛提供了有关模型性能的深入见解。</li>
<li>获胜团队在比赛中显著超越了基线17 mAP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8425046653ec2d2d25050c898fcec42d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036930&auth_key=1760036930-0-0-c5271d219723ae786819375ce5b9d94a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39f97d32ec01bf4be40727f5d59a3bbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036937&auth_key=1760036937-0-0-33eef3609295541a4b1bea0e0bd6331c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94525ec6f3d591f43894fc2c1a976188~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036944&auth_key=1760036944-0-0-2d2c6aed9849569930a1ffe715ef9b58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0731706e142f12ec7985fe7ab5630eda~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036951&auth_key=1760036951-0-0-f3a24adcc34a6c40dd9e76d30949f701&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d3cea226c0073c3c69f1cb43dcf6bcd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036958&auth_key=1760036958-0-0-ae8dc82ec4ccbcc742b89759eeb2a8dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5efc4abbd8b1bfc8bfe9d36ce9cd2d36~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036964&auth_key=1760036964-0-0-ccb22d57e034313a6eda6a22d2d5530d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction"><a href="#ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction" class="headerlink" title="ReLI: A Language-Agnostic Approach to Human-Robot Interaction"></a>ReLI: A Language-Agnostic Approach to Human-Robot Interaction</h2><p><strong>Authors:Linus Nwankwo, Bjoern Ellensohn, Ozan Özdenizci, Elmar Rueckert</strong></p>
<p>Adapting autonomous agents for real-world industrial, domestic, and other daily tasks is currently gaining momentum. However, in global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human-specified tasks regardless of the language remains an unsolved problem. To address this, we propose ReLI, a language-agnostic approach that enables autonomous agents to converse naturally, semantically reason about their environment, and perform downstream tasks, regardless of the task instruction’s modality or linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow conversational interactions. Further, we perform cross-lingual adaptation of the models to ensure that ReLI generalises across the global languages. To demonstrate ReLI’s robustness, we conducted extensive experiments on various short- and long-horizon tasks, including zero- and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on $140$ languages involving $70K+$ multi-turn conversations. On average, ReLI achieved over $90%\pm0.2$ accuracy in cross-lingual instruction parsing and task execution success. These results demonstrate its potential to advance natural human-agent interaction in the real world while championing inclusive and linguistic diversity. Demos and resources will be public at: <a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/">https://linusnep.github.io/ReLI/</a>. </p>
<blockquote>
<p>适应自主代理用于现实世界中的工业、家居和其他日常任务正日益受到重视。然而，在全球或跨语言应用环境中，确保与环境的有效互动并执行不受限制的人类指定任务，无论使用何种语言，仍然是一个未解决的问题。为了解决这一问题，我们提出了ReLI，这是一种语言无关的方法，使自主代理能够自然地进行对话，对其环境进行语义推理，并执行下游任务，无论任务指令的模式或语言起源如何。首先，我们基于大规模预训练基础模型，将它们转化为语言到行动模型，可以直接通过自然、自由的对话式交互提供常识推理和高级机器人控制。此外，我们对模型进行了跨语言适配，以确保ReLI在全球范围内各种语言中都能通用。为了验证ReLI的稳健性，我们在各种短期和长期任务上进行了广泛实验，包括零样本和少样本空间导航、场景信息检索和查询导向任务。我们在涉及超过7万多次回合对话的140种语言上对性能进行了评估。平均而言，ReLI在跨语言指令解析和任务执行方面的成功率达到了超过$90%\pm0.2$的准确率。这些结果证明了其在推动现实世界中自然的人机交互方面的潜力，同时倡导包容性和语言多样性。相关演示和资源将在：<a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/%E5%85%AC%E5%BC%80%E3%80%82">https://linusnep.github.io/ReLI/公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01862v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了针对自主代理人在真实世界中的应用问题，提出一种语言无关的方法ReLI。该方法能够使自主代理人进行自然对话，理解环境语义，执行下游任务，无论任务指令的方式或语言如何。实验证明，ReLI在不同语言和任务上具有良好的表现，能够实现跨语言适应，并具备高度的自然语言交互能力。ReLI模型具有广泛的应用前景，包括工业、家庭和其他日常任务中的实际应用。资源可通过网址进行访问：<a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/">https://linusnep.github.io/ReLI/</a>。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本中关键的见解概述：</p>
<ol>
<li>ReLI是一种语言无关的方法，旨在解决自主代理人在全球或跨语言环境中执行任务时的语言障碍问题。</li>
<li>ReLI通过将大规模预训练基础模型转化为语言到行动模型，实现自然对话和机器人控制。</li>
<li>ReLI进行了跨语言适应，确保其在全球范围内的多种语言中的通用性。</li>
<li>ReLI在多种短期和长期任务上进行了广泛实验验证，包括零样本和少样本空间导航、场景信息检索和查询导向任务等。</li>
<li>实验结果显示，ReLI在跨语言指令解析和任务执行成功率方面达到了超过90%的平均准确率。</li>
<li>ReLI展示了在自然人机交互中的潜力，并强调包容性和语言多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5ac6ba0a1907ecf479275a2c3fadf531~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036972&auth_key=1760036972-0-0-e0fef36cf1e45a8cb2760c427bb41edf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bca48201cacdcfd198170d52444f7194~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036979&auth_key=1760036979-0-0-d321c5b6a9ab2448c1ebb51313f59049&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2dfd3d427e9b64ce2d1742b38eab7af6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036987&auth_key=1760036987-0-0-48f5a04f57cc5777a6b4dfb64070f579&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8118bd0271def320f17336396e753f4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760036993&auth_key=1760036993-0-0-a5b66a26ad9c57ab6333293dc67693a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AutoPDL-Automatic-Prompt-Optimization-for-LLM-Agents"><a href="#AutoPDL-Automatic-Prompt-Optimization-for-LLM-Agents" class="headerlink" title="AutoPDL: Automatic Prompt Optimization for LLM Agents"></a>AutoPDL: Automatic Prompt Optimization for LLM Agents</h2><p><strong>Authors:Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel</strong></p>
<p>The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks. </p>
<blockquote>
<p>大型语言模型的性能取决于如何对其进行提示，提示的选择范围包括高级提示模式（例如Zero-Shot、CoT、ReAct、ReWOO）和特定提示内容（指令和少量示例）。手动调整这种结合方式既繁琐又容易出现错误，而且针对特定的LLM和任务。因此，本文提出了AutoPDL，一种发现良好LLM代理配置自动化方法。我们将这个问题构建为一个结构化AutoML问题，在代理和非代理提示模式及示例的组合空间上进行解决，使用连续减半法有效地遍历这个空间。我们引入了一个使用PDL提示编程语言的库来实现常见的提示模式。AutoPDL解决方案是可读、可编辑、可执行的PDL程序，使用此库。这种方法还实现了源到源的优化，允许人类参与循环优化和重用。在三个任务和七个LLM（从3B到70B参数）上的评估显示，准确率持续提高（9.21±15.46个百分点），最高达到67.5个百分点，并且显示所选的提示策略在不同模型和任务之间有所不同。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04365v4">PDF</a> Presented at AutoML 2025 (Methods Track); to be published in   proceedings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型的性能取决于如何对其进行提示，涉及高级提示模式（如零击、上下文提示、反应和ReWOO）和特定提示内容（指令和少量示例）。手动调整这种组合既繁琐又容易出错，且针对特定的大型语言模型和任务。因此，本文提出了AutoPDL，一种用于发现良好大型语言模型代理配置的自动化方法。该方法将其构建为一个结构化自动机器学习问题，在代理和非代理提示模式及示例的组合空间中高效导航。引入一个使用PDL提示编程语言的常见提示模式库。AutoPDL解决方案是可读、可编辑和可执行的使用该库的PDL程序。该方法还支持源到源的优化，允许人为参与优化和改进。在三个任务和七个大型语言模型上的评估显示，准确性持续提高（增加9.21±15.46个百分点），最高达67.5个百分点，表明选择的提示策略在不同模型和任务之间有所不同。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的性能受提示方式影响，包括高级提示模式和具体提示内容。</li>
<li>手动调整提示组合既繁琐又易出错。</li>
<li>AutoPDL是一种自动化方法，用于发现良好的大型语言模型代理配置。</li>
<li>AutoPDL将问题构建为结构化自动机器学习问题，在代理和非代理提示模式及示例的组合空间中高效导航。</li>
<li>引入PDL提示编程语言的常见提示模式库，使AutoPDL解决方案更加人性化、可编辑和执行。</li>
<li>AutoPDL支持源到源的优化，允许人为参与优化和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04365">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9fd26ed8ad320dd275f062f698885255~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037000&auth_key=1760037000-0-0-9621e25d618b375dd2fb54983c66ff65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdb37e2d07509aafb24da166e397e2c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037007&auth_key=1760037007-0-0-57254bdb25ab412b487ae6a29bba6996&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1b71f89fd39f141bf09b3fd3839a7e68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037014&auth_key=1760037014-0-0-1b9fc86e468554e14e2e2da6f86161ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano Patrício, Isabel Rio-Torto, Jaime S. Cardoso, Luís F. Teixeira, João C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the model output on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>在医疗工作流中采用基于深度学习的解决方案的主要挑战是标注数据可用性和系统解释性的缺乏。概念瓶颈模型（CBMs）通过约束模型输出在一组预定义和可解释的概念上来解决后者的问题。然而，通过基于概念的解释提高的解释性意味着更高的标注负担。而且，如果需要添加新概念，整个系统需要重新训练。受大型视觉语言模型（LVLMs）在少量场景设置中的出色表现的启发，我们提出了一种简单而有效的方法，即CBVLM，它解决了上述两个挑战。首先，对于每个概念，我们提示LVLM回答概念是否出现在输入图像中。然后，我们让LVLM基于之前的概念预测对图像进行分类。此外，在两个阶段中，我们都融入了一个检索模块，负责选择最佳示例进行上下文学习。通过基于预测概念的最终诊断，我们确保了解释性，并通过利用LVLMs的少量样本能力，我们大大降低了标注成本。我们通过四个医疗数据集和十二个（通用和医疗）LVLM的广泛实验验证了我们的方法，并表明CBVLM在不需要任何训练和只需少量标注样本的情况下，始终优于CBMs和针对特定任务的有监督方法。更多关于我们项目的信息请访问：[<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/]">https://cristianopatricio.github.io/CBVLM/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v3">PDF</a> Accepted for publication in Computers in Biology and Medicine</p>
<p><strong>Summary</strong><br>     基于深度学习的医疗解决方案面临标注数据可用性和系统解释性两大挑战。概念瓶颈模型（CBMs）通过约束模型输出在预定义和可解释的概念集上来解决解释性问题，但这也增加了标注负担，且添加新概念需重新训练整个系统。受大型视觉语言模型（LVLMs）在少样本环境下的出色表现的启发，我们提出一种简单有效的CBVLM方法，同时应对上述两大挑战。我们引导LVLM判断输入图像中是否存在特定概念，并基于预测的概念对图像进行分类。同时，我们引入检索模块，负责选择最佳样本进行上下文学习。通过将最终诊断结果建立在预测概念上，我们确保了可解释性，并利用LVLMs的少样本能力大幅降低标注成本。实验证明，CBVLM在四个医疗数据集和十二个（通用和医疗）LVLM上的表现均优于CBMs和任务特定的监督方法，且无需任何训练，仅使用少量标注样本即可。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习和医疗工作流中的挑战包括标注数据的可用性和系统解释性不足。</li>
<li>概念瓶颈模型（CBMs）通过约束模型输出在预定义和可解释的概念集上解决解释性问题，但增加了标注负担。</li>
<li>CBVLM方法结合大型视觉语言模型（LVLMs）解决上述问题，通过引导LVLM判断特定概念的存在并对图像进行分类。</li>
<li>CBVLM方法引入检索模块选择最佳样本进行上下文学习。</li>
<li>CBVLM通过预测概念确保解释性，并利用LVLMs的少样本能力降低标注成本。</li>
<li>实验证明CBVLM在多个医疗数据集上的表现优于CBMs和任务特定监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-15ec5a9c5d00e6bc7dcf25c49384c7ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037021&auth_key=1760037021-0-0-c9c76aec7beef7316945e8d6e19eaff9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36b80d22716191da8fc4c178f927e398~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037028&auth_key=1760037028-0-0-2c48d5ee50dcd0b23dc0fe453ad75380&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b386e2ae3c4946b3e59685e95f4804b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037035&auth_key=1760037035-0-0-2f257abc7a7c13496e34fc5a5a85c986&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be192d43deb0d0b33637393de5cd9c1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037042&auth_key=1760037042-0-0-4ff80d2720dd7dfad7cc2439530d805c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VICON-Vision-In-Context-Operator-Networks-for-Multi-Physics-Fluid-Dynamics-Prediction"><a href="#VICON-Vision-In-Context-Operator-Networks-for-Multi-Physics-Fluid-Dynamics-Prediction" class="headerlink" title="VICON: Vision In-Context Operator Networks for Multi-Physics Fluid   Dynamics Prediction"></a>VICON: Vision In-Context Operator Networks for Multi-Physics Fluid   Dynamics Prediction</h2><p><strong>Authors:Yadi Cao, Yuxuan Liu, Liu Yang, Rose Yu, Hayden Schaeffer, Stanley Osher</strong></p>
<p>In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose Vision In-Context Operator Networks (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICON’s adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9% compared to DPOT and 44.7% compared to MPP, while requiring only 72.5% and 34.8% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in imperfect measurement systems where sampling frequencies may differ or frames might be dropped - common challenges in real-world settings - without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41% relative performance degradation compared to 71.37%-74.49% degradation in baseline methods, demonstrating its versatility for deploying in realistic applications. Our scripts for processing datasets and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Eydcao/VICON">https://github.com/Eydcao/VICON</a>. </p>
<blockquote>
<p>上下文操作网络（ICONs）已经展示了在多种偏微分方程中使用少量上下文学习操作的能力。然而，现有的ICON将每个空间点视为一个单独的标记，在处理高维空间的密集数据时，计算效率严重受限。我们提出了视觉上下文操作网络（VICON），它结合了视觉转换器架构，通过块操作高效地处理二维数据，同时保留ICON对多物理系统和不同时间步长的适应性。在三个流体动力学基准测试中进行了评估，VICON显著优于最新基线技术DPOT和MPP，与DPOT相比平均最后一步滚动误差减少了37.9%，与MPP相比减少了44.7%，同时仅需要它们各自推理时间的72.5%和34.8%。VICON自然地支持具有不同时间步长的灵活滚动策略，能够在测量系统不完善的情况下进行即时部署，其中采样频率可能有所不同或帧可能会丢失——现实环境中的常见挑战——无需重新训练或插值。在这些现实场景中，VICON表现出惊人的稳健性，相对性能下降只有24.41%，而基线方法的性能下降在71.37%-74.49%之间，证明了其在现实应用中的通用性。我们的数据集处理脚本和代码可在<a target="_blank" rel="noopener" href="https://github.com/Eydcao/VICON">https://github.com/Eydcao/VICON</a>公开获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16063v4">PDF</a> update after NIPS suggestions</p>
<p><strong>Summary</strong></p>
<p>本文介绍了In-Context Operator Networks（ICONs）在处理部分微分方程时的学习能力，但其在处理高维空间密集数据时计算效率受限。为此，提出了Vision In-Context Operator Networks（VICON），结合视觉转换器架构，通过补丁操作高效处理二维数据，同时保留ICON在多物理系统和不同时间步长的适应性。在三个流体动力学基准测试中，VICON显著优于现有技术基准，减少了平均最后一步滚动误差，并提高了推理时间的效率。VICON支持灵活的时间步长策略，可在真实世界的不完美测量系统中部署，面对采样频率差异或帧丢失等挑战时表现出强大的稳健性。代码和数据集处理脚本已公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICONs能够在不同的部分微分方程中学习操作符，但在处理高维空间密集数据时计算效率低下。</li>
<li>VISION IN-CONTEXT OPERATOR NETWORKS（VICON）通过结合视觉转换器架构解决了这个问题，能够高效处理二维数据。</li>
<li>VICON在流体动力学基准测试中显著优于现有技术基准，减少了平均最后一步滚动误差，并提高了推理效率。</li>
<li>VICON支持灵活的时间步长策略，适应真实世界的不完美测量系统。</li>
<li>VICON在处理采样频率差异或帧丢失等挑战时表现出强大的稳健性。</li>
<li>VICON具有多物理系统和多变时间步长的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16063">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a432e76f87dd6286dccb76d397f744e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037049&auth_key=1760037049-0-0-8904c863cd2ab7e6428641a670964241&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d246e38442a4e70f3d65c97da090e2fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760037057&auth_key=1760037057-0-0-db6fbf4898be9352fad02bfb4d8453b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-f4acd61c8d7272201da7c7f3b421176a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038216&auth_key=1760038216-0-0-462918055775179ffc5927be140ec7e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-10-10  Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph   Regularization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c847d5b1dd97a51c096bd79df71ca723~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083434&auth_key=1760083434-0-0-2e6e9a636f09ac8790bc7887027bfbff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-10-10  GIIFT Graph-guided Inductive Image-free Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
