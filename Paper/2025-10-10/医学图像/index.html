<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-10  Validation of Various Normalization Methods for Brain Tumor   Segmentation Can Federated Learning Overcome This Heterogeneity?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-7e4c436ac5f510674cbe676065e934a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049798&auth_key=1760049798-0-0-50b80f9a76b73bef9b9252391c2cffaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    22.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    91 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-10-更新"><a href="#2025-10-10-更新" class="headerlink" title="2025-10-10 更新"></a>2025-10-10 更新</h1><h2 id="Validation-of-Various-Normalization-Methods-for-Brain-Tumor-Segmentation-Can-Federated-Learning-Overcome-This-Heterogeneity"><a href="#Validation-of-Various-Normalization-Methods-for-Brain-Tumor-Segmentation-Can-Federated-Learning-Overcome-This-Heterogeneity" class="headerlink" title="Validation of Various Normalization Methods for Brain Tumor   Segmentation: Can Federated Learning Overcome This Heterogeneity?"></a>Validation of Various Normalization Methods for Brain Tumor   Segmentation: Can Federated Learning Overcome This Heterogeneity?</h2><p><strong>Authors:Jan Fiszer, Dominika Ciupek, Maciej Malawski</strong></p>
<p>Deep learning (DL) has been increasingly applied in medical imaging, however, it requires large amounts of data, which raises many challenges related to data privacy, storage, and transfer. Federated learning (FL) is a training paradigm that overcomes these issues, though its effectiveness may be reduced when dealing with non-independent and identically distributed (non-IID) data. This study simulates non-IID conditions by applying different MRI intensity normalization techniques to separate data subsets, reflecting a common cause of heterogeneity. These subsets are then used for training and testing models for brain tumor segmentation. The findings provide insights into the influence of the MRI intensity normalization methods on segmentation models, both training and inference. Notably, the FL methods demonstrated resilience to inconsistently normalized data across clients, achieving the 3D Dice score of 92%, which is comparable to a centralized model (trained using all data). These results indicate that FL is a solution to effectively train high-performing models without violating data privacy, a crucial concern in medical applications. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/SanoScience/fl-varying-normalization">https://github.com/SanoScience/fl-varying-normalization</a>. </p>
<blockquote>
<p>深度学习（DL）在医学成像领域的应用日益广泛，然而，它需要大量的数据，这就引发了与数据隐私、存储和传输相关的许多挑战。联合学习（FL）是一种克服这些问题的训练范式，但在处理非独立同分布（non-IID）数据时，其效果可能会降低。本研究通过应用不同的MRI强度归一化技术来模拟非IID条件，将数据集分离，以反映异质性的常见原因。这些子集随后用于训练和测试脑肿瘤分割模型。研究结果提供了MRI强度归一化方法对分割模型训练和推理影响的见解。值得注意的是，联邦学习方法对客户端间归一化不一致的数据表现出韧性，实现了3DDice分数为92%，与集中模型（使用全部数据训练）相当。这些结果表明，联邦学习是有效训练高性能模型而不违反数据隐私的解决方案，这是医学应用中一个关键的问题。代码可在<a target="_blank" rel="noopener" href="https://github.com/SanoScience/fl-varying-normalization%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SanoScience/fl-varying-normalization获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07126v1">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习在医学成像领域应用广泛，但需要大量数据，带来数据隐私、存储和传输方面的挑战。联邦学习（FL）克服了这些问题，但在处理非独立同分布（non-IID）数据时效果可能降低。本研究通过应用不同的MRI强度归一化技术来模拟非IID条件，用于训练和测试脑肿瘤分割模型。研究发现MRI强度归一化方法对分割模型的影响，联邦学习方法对不一致归一化数据具有韧性，3D Dice评分达到92%，与集中模型相当。这证明联邦学习是有效训练高性能模型且不会侵犯数据隐私的解决方案，这在医学应用中至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在医学成像中广泛应用，但需大量数据，引发数据隐私、存储和传输挑战。</li>
<li>联邦学习克服这些问题，但在处理非独立同分布（non-IID）数据时效果可能降低。</li>
<li>研究通过MRI强度归一化技术模拟非IID条件，用于训练和测试脑肿瘤分割模型。</li>
<li>联邦学习方法对不一致归一化数据具有韧性。</li>
<li>3D Dice评分达到92%，与集中模型相当，证明联邦学习的有效性。</li>
<li>联邦学习是有效训练高性能模型且不会侵犯数据隐私的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d5dbdf7bb38033867ed2fd4e73a50921~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049687&auth_key=1760049687-0-0-718a0457a5305049e875c8c1c426933f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc0eaa4ff83ac53ff983d244bf2af33e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049693&auth_key=1760049693-0-0-1cd1866c6debcdc0b9a6628e7d8f80fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9edb48424c13c236bcd743e47b622860~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049700&auth_key=1760049700-0-0-4f0a8d715fa3156460a77aa98d7b61cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1416cfd7f404dc553b5108aaa065ed03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049707&auth_key=1760049707-0-0-2a5378280a1e1a60c0a028b56759d27a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking"><a href="#U-Bench-A-Comprehensive-Understanding-of-U-Net-through-100-Variant-Benchmarking" class="headerlink" title="U-Bench: A Comprehensive Understanding of U-Net through 100-Variant   Benchmarking"></a>U-Bench: A Comprehensive Understanding of U-Net through 100-Variant   Benchmarking</h2><p><strong>Authors:Fenghe Tang, Chengqi Dong, Wenxin Ma, Zikang Xu, Heqin Zhu, Zihang Jiang, Rongsheng Wang, Yuhao Wang, Chenxu Wu, Shaohua Kevin Zhou</strong></p>
<p>Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: <a target="_blank" rel="noopener" href="https://fenghetan9.github.io/ubench">https://fenghetan9.github.io/ubench</a>. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/U-Bench">https://github.com/FengheTan9/U-Bench</a>. </p>
<blockquote>
<p>过去十年里，U-Net在医学图像分割领域占据主导地位，并催生了数千种U型变体。尽管其应用广泛，但仍缺乏一个综合基准来系统地评估它们的性能和实用性，这主要是因为统计验证不足，以及对不同数据集的效率和推广的考虑有限。为了弥补这一差距，我们推出了U-Bench，这是第一个大规模、统计严谨的基准，评估了28个数据集的100种U-Net变体以及10种成像模式。我们的贡献分为三个部分：（1）全面评估：U-Bench从三个关键维度对模型进行评估：统计稳健性、零射击泛化能力和计算效率。我们引入了一种新型指标U-Score，它联合捕获性能效率权衡，为模型进展提供了面向部署的视角。（2）系统分析与模型选择指南：我们对大规模评估的关键发现进行了总结，并对数据集特性和架构范式对模型性能的影响进行了系统分析。基于这些见解，我们提出了一个模型顾问代理，帮助研究人员为特定数据集和任务选择最合适的模型。（3）公开可用：我们提供了所有代码、模型、协议和权重，使社区能够复制我们的结果并用未来方法对基准测试进行扩展。总之，U-Bench不仅揭示了以前评估中的差距，而且为下一个十年U-Net基分割模型的公平、可重复和实际相关的基准测试奠定了基础。项目可通过以下网址访问：<a target="_blank" rel="noopener" href="https://fenghetan9.github.io/ubench">https://fenghetan9.github.io/ubench</a>。代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/FengheTan9/U-Bench">https://github.com/FengheTan9/U-Bench</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07041v1">PDF</a> 54 pages. The project can be accessed at:   <a target="_blank" rel="noopener" href="https://fenghetan9.github.io/ubench">https://fenghetan9.github.io/ubench</a>. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/U-Bench">https://github.com/FengheTan9/U-Bench</a></p>
<p><strong>摘要</strong><br>    十年来，U-Net在医学图像分割中占据主导地位，并催生出数千种U型变体。尽管其广泛应用，但目前仍缺乏综合基准测试来系统地评估它们的性能和实用性，这主要是因为统计验证不足以及对不同数据集效率与泛化能力的有限考虑。为了弥补这一空白，我们推出了U-Bench，这是第一个大规模、统计严谨的基准测试，评估了100种U-Net变体在28个数据集和10种成像模态下的性能。我们的贡献有三方面：首先，全面评估：U-Bench从三个关键维度评估模型：统计稳健性、零样本泛化能力和计算效率。我们引入了一种新的指标U-Score，它联合捕捉性能与效率的权衡，为模型进展提供了面向部署的视角。其次，系统分析与模型选择指南：我们总结了大规模评估的关键发现，并对数据集特性和架构范式对模型性能的影响进行了系统分析。基于这些见解，我们提出了一个模型顾问代理，指导研究人员为特定数据集和任务选择最合适的模型。最后，公共可用性：我们提供了所有代码、模型、协议和权重，使社区能够复制我们的结果并用未来的方法扩展基准测试。总之，U-Bench不仅揭示了以往评估中的差距，还为未来十年U-Net基分割模型的公平、可重复和实用基准测试奠定了基础。项目可通过<a target="_blank" rel="noopener" href="https://fenghetan9.github.io/ubench">链接</a>访问。代码可在<a target="_blank" rel="noopener" href="https://github.com/FengheTan9/U-Bench">链接</a>找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-Net在医学图像分割中占据主导地位，衍生出众多变体。</li>
<li>缺乏综合基准测试来评估U-Net变体的性能。</li>
<li>U-Bench作为首个大规模、统计严谨的基准测试，评估了U-Net变体的性能。</li>
<li>U-Bench从统计稳健性、零样本泛化能力和计算效率三个维度评估模型。</li>
<li>引入新型评估指标U-Score，以评估性能与效率的权衡。</li>
<li>基于大规模评估结果，提供了模型选择指南。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bd3a320a58b3e97945d6cf0c56625171~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049714&auth_key=1760049714-0-0-97e0fe40968917731e691ab5910277f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f0c8eb8974be795c862707e8be9311b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049721&auth_key=1760049721-0-0-4b9c759557984917703c86035309c80f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d48281532542ed263a3a3a87eaba3e11~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049728&auth_key=1760049728-0-0-6a103bb8faf007b8595e94ca178b4287&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9675254518d7738bee85b677f88d5b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049735&auth_key=1760049735-0-0-0e337771dfcea3b02b7b9ec1576d2863&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fa7760d8e60f9d0394236866d44538b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049742&auth_key=1760049742-0-0-6c5e0d75ab98faf2f39df9ed25154545&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="1FLAT-a-Firmamento-based-catalog-of-AGN-in-Fermi-LAT-high-Galactic-latitude-γ-ray-sources"><a href="#1FLAT-a-Firmamento-based-catalog-of-AGN-in-Fermi-LAT-high-Galactic-latitude-γ-ray-sources" class="headerlink" title="1FLAT: a Firmamento-based catalog of AGN in Fermi-LAT high Galactic   latitude γ-ray sources"></a>1FLAT: a Firmamento-based catalog of AGN in Fermi-LAT high Galactic   latitude γ-ray sources</h2><p><strong>Authors:P. Giommi, M. Doro, M. Gouvêa, L. Fronte, F. Metruccio, F. Arneodo, U. Barres de Almeida, S. Di Pippo, T. Kerscher, A. Macció, B. Mazzon, M. Morrone, E. Prandini, A. Rodríguez, A. Ruina, N. Sahakyan, L. Silveri, D. Tripathi</strong></p>
<p>We present a systematic reassessment of 5,062 high-Galactic latitude gamma-ray sources from the Fermi-LAT 4FGL-DR4 catalog using Firmamento, a web-based platform for multi-frequency source discovery and analysis. Our goal is to provide an independent evaluation of LAT gamma-ray source associations through alternative spectral and spatial methods that combine recent and legacy survey data, supplemented by human supervision of spectral energy distributions (SEDs), source morphology, flux variability, and template-based comparisons. Firmamento confirms the 4FGL-DR4 and 4LAC-DR3 counterparts or unassociated sources in 4,493 cases (88.8%), demonstrating the robustness of both approaches. Beyond this general agreement, we identify 421 new blazar counterparts among previously unassociated sources, thereby reducing the fraction of unidentified extragalactic Fermi-LAT sources from 25% to 17%. In addition, in 64 cases we find alternative blazar associations, while in 49 instances we do not confirm the 4FGL-DR4 association. For all confirmed blazar counterparts we provide homogeneous estimates of synchrotron peak frequency and peak flux using machine-learning and template-based methods; these agree with 4LAC-DR3 values in most cases, though significant discrepancies appear for a few dozen sources, often due to improved X-ray coverage. The primary outcome of this work is the 1st Firmamento LAT AGN table (1FLAT), made publicly available through the Firmamento platform (<a target="_blank" rel="noopener" href="https://firmamento.nyuad.nyu.edu/">https://firmamento.nyuad.nyu.edu</a>), where all related multi-wavelength data and images are available. The project involved extensive manual validation and benefited from the active participation of graduate and undergraduate students, highlighting the platform’s value for both research and education. </p>
<blockquote>
<p>我们利用Firmamento这一基于网络的多频率源发现与分析平台，对费米LAT 4FGL-DR4目录中的5062个高银道纬度伽马射线源进行了系统的重新评估。我们的目标是通过对光谱和空间方法的替代方案，提供对LAT伽马射线源关联的独立评估。这种方法结合了最新的和传统的观测数据，辅以谱能量分布（SEDs）、源形态、流量变化和基于模板的比较等人为监督。Firmamento确认了4FGL-DR4和4LAC-DR3的对应物或未关联的源在4493个案例中的存在（占88.8%），证明了这两种方法的稳健性。除了这种普遍的一致性外，我们在之前的未关联源中确定了421个新的耀星对应物，从而将未识别的费米LAT外星系的源比例从25%降至17%。此外，在64个案例中我们找到了替代的耀星关联，而在另外49个案例中我们没有确认其对应于已知对象的信息是否关联与实际的4FGL-DR4有所不同。对于所有确认的耀星对应物，我们使用机器学习和基于模板的方法提供了关于同步峰值频率和峰值流量的均匀估计值；这些在多数情况下与大多数案例中的共识相一致。我们通过比较获得大量证据表明少量例外主要是受益于我们进一步完善的X射线覆盖范围得到了显著提升和关键的数据校准技术提升及具有深远见解的专家分析。这项工作的主要成果是首个Firmamento LAT AGN表（简称“LAT”），通过Firmamento平台公开提供（网址为：<a target="_blank" rel="noopener" href="https://firmamento.nyuad.nyu.edu).所本综合多线测度宽度的数据和图像都通过该平台公开访问和使用.这一项目的推进也得益于大量的专业团队及知识资源以及耗费时间数据的资深分析以及多方面的支持和协作的成果在此项目的实施过程中包含了大量手工操作以及专家级别的知识辅助它涉及的研究包括专业知识和技术支持在不同领域的深度应用和融合强调其价值不仅在于科学研究同时也对教育事业有着积极的推动作用./">https://firmamento.nyuad.nyu.edu）。所有相关的多波长数据和图像都在该平台公开访问和使用。（项目还包括大量的手动验证流程），同时得益于研究生和本科生的积极参与从而强调了该平台对于研究和教育的价值。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06962v1">PDF</a> Accepted for publication in ApJS</p>
<p><strong>Summary</strong></p>
<p>这是一篇关于使用Firmamento平台对高银道纬度伽马射线源进行系统性重新评估的研究。该研究旨在通过结合多种频谱和空间方法，结合最新的调研数据，对4FGL-DR4目录中的5,062个伽马射线源进行独立评估。通过这一评估，确认了大多数源，同时识别出了一批新的耀发对应体，降低了未识别的费米-LAT外星源的比例。同时提供了统一的同步峰值频率和峰值流量估计。最重要的成果是公开可用的Firmamento平台上的首个LAT AGN表（1FLAT），该平台提供了所有相关的多波长数据和图像。此项目涉及广泛的手动验证，并得益于学生和本科生积极参与，突显了其在研究和教育方面的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用Firmamento平台对高银道纬度伽马射线源进行系统评估。</li>
<li>对4FGL-DR4目录中的伽马射线源进行独立评估，确认大多数源并发现新的耀发对应体。</li>
<li>降低了未识别的费米-LAT外星源的比例。</li>
<li>为所有确认的耀发对应体提供了统一的同步峰值频率和峰值流量估计。</li>
<li>公开可用的Firmamento平台上的首个LAT AGN表（1FLAT）。</li>
<li>该项目涉及广泛的手动验证，以确保数据的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1ab498cd591220a0af9540d57ea199c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049749&auth_key=1760049749-0-0-eacb7eb615c6a10dd05f2bcc9fd8f331&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3afbdc7745af6d8106b9e2c886fa2cd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049757&auth_key=1760049757-0-0-6c59dbd56554a536ab7dc7040da41de6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ce8d0267fcaec62b4ca05890652f212~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049763&auth_key=1760049763-0-0-e237b28e9e2cc93d4b8e79205ecd4e96&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e13107133945b3279b76684a510bba06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049771&auth_key=1760049771-0-0-2e97b8998dfc52c3bb56c80f09b2d943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Lung-Infection-Severity-Prediction-Using-Transformers-with-Conditional-TransMix-Augmentation-and-Cross-Attention"><a href="#Lung-Infection-Severity-Prediction-Using-Transformers-with-Conditional-TransMix-Augmentation-and-Cross-Attention" class="headerlink" title="Lung Infection Severity Prediction Using Transformers with Conditional   TransMix Augmentation and Cross-Attention"></a>Lung Infection Severity Prediction Using Transformers with Conditional   TransMix Augmentation and Cross-Attention</h2><p><strong>Authors:Bouthaina Slika, Fadi Dornaika, Fares Bougourzi, Karim Hammoudi</strong></p>
<p>Lung infections, particularly pneumonia, pose serious health risks that can escalate rapidly, especially during pandemics. Accurate AI-based severity prediction from medical imaging is essential to support timely clinical decisions and optimize patient outcomes. In this work, we present a novel method applicable to both CT scans and chest X-rays for assessing lung infection severity. Our contributions are twofold: (i) QCross-Att-PVT, a Transformer-based architecture that integrates parallel encoders, a cross-gated attention mechanism, and a feature aggregator to capture rich multi-scale features; and (ii) Conditional Online TransMix, a custom data augmentation strategy designed to address dataset imbalance by generating mixed-label image patches during training. Evaluated on two benchmark datasets, RALO CXR and Per-COVID-19 CT, our method consistently outperforms several state-of-the-art deep learning models. The results emphasize the critical role of data augmentation and gated attention in improving both robustness and predictive accuracy. This approach offers a reliable, adaptable tool to support clinical diagnosis, disease monitoring, and personalized treatment planning. The source code of this work is available at <a target="_blank" rel="noopener" href="https://github.com/bouthainas/QCross-Att-PVT">https://github.com/bouthainas/QCross-Att-PVT</a>. </p>
<blockquote>
<p>肺部感染，尤其是肺炎，会带来严重的健康风险，特别是在大流行期间，这些风险可能会迅速升级。因此，利用医学图像进行准确的AI严重程度预测对于支持及时的临床决策和优化患者结果至关重要。在这项工作中，我们提出了一种可用于CT扫描和胸部X射线评估肺部感染严重程度的新型方法。我们的贡献有两方面：（i）QCross-Att-PVT，这是一种基于Transformer的架构，它集成了并行编码器、交叉门控注意机制和特征聚合器，以捕获丰富的多尺度特征；（ii）Conditional Online TransMix，这是一种自定义的数据增强策略，旨在通过训练过程中生成混合标签图像补丁来解决数据集不平衡问题。在RALO CXR和Per-COVID-19 CT两个基准数据集上评估，我们的方法始终优于几种最先进的深度学习模型。结果强调了数据增强和门控注意在改善稳健性和预测精度方面的关键作用。该方法提供了一个可靠、灵活的工具，可支持临床诊断、疾病监测和个性化治疗计划。该工作的源代码可在<a target="_blank" rel="noopener" href="https://github.com/bouthainas/QCross-Att-PVT">https://github.com/bouthainas/QCross-Att-PVT</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06887v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种用于评估肺部感染严重程度的全新方法，适用于CT扫描和胸部X光。该方法采用基于Transformer的架构QCross-Att-PVT，融合了并行编码器、交叉门控注意机制和特征聚合器，以捕捉丰富的多尺度特征。此外，还提出了自定义数据增强策略Conditional Online TransMix，以解决数据集不平衡问题。在两家基准数据集上的评估结果表明，该方法优于多个最新深度学习模型，强调了数据增强和门控注意在提高稳健性和预测精度方面的关键作用。此工具可为临床诊断、疾病监测和个性化治疗计划提供可靠支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种基于AI的方法用于评估肺部感染严重性。</li>
<li>方法采用新型架构QCross-Att-PVT，整合了多种技术如并行编码器、交叉门控注意机制以及特征聚合器。</li>
<li>提出数据增强策略Conditional Online TransMix以处理数据集不平衡问题。</li>
<li>在两个基准数据集上的表现优于其他先进深度学习模型。</li>
<li>数据增强和门控注意在提高模型的稳健性和预测精度方面发挥了关键作用。</li>
<li>此方法可作为临床辅助诊断、疾病监测和个性化治疗的有效工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06887">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f9946ff17f81f50773bbd8cac2073b47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049779&auth_key=1760049779-0-0-b090a26b0ec3695c31bfa444edc6cd9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f8256a9152a94e76e1326133894724b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049786&auth_key=1760049786-0-0-789158892974280f24bd787911b85def&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd1ce5a876b30c0919df7672e524b756~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049792&auth_key=1760049792-0-0-f19598ba2e326c1b8e2b59d74b12e8f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7e4c436ac5f510674cbe676065e934a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049798&auth_key=1760049798-0-0-50b80f9a76b73bef9b9252391c2cffaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-74fff3e76c2db40ac1f1287207bc1a9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049805&auth_key=1760049805-0-0-8841a495666f35b54100ea9b732ff800&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hard-X-ray-view-of-two-γ-ray-detected-low-luminosity-active-galactic-nuclei-NGC-315-and-NGC-4261"><a href="#Hard-X-ray-view-of-two-γ-ray-detected-low-luminosity-active-galactic-nuclei-NGC-315-and-NGC-4261" class="headerlink" title="Hard X-ray view of two $γ$-ray detected low-luminosity active   galactic nuclei: NGC 315 and NGC 4261"></a>Hard X-ray view of two $γ$-ray detected low-luminosity active   galactic nuclei: NGC 315 and NGC 4261</h2><p><strong>Authors:Yuwei Yu, Jin Zhang</strong></p>
<p>Aims. The accretion disk of low-luminosity active galactic nuclei (LLAGNs) is a radiatively inefficient accretion flow (RIAF). Our goal is to find evidence of RIAF radiation from LLAGNs with jets and analyze their radiation properties, which also adds samples to future research on LLAGNs. Methods. Weconducted an analysis of the X-ray data obtained from NuSTAR and XMM-Newton observations of NGC 315 and NGC 4261, encompassing both timing and spectral investigations. The joint X-ray spectra of the two LLAGNs were fitted using various functional forms and radiative models in XSPEC. Results. No significant variability on timescales of days is observed for both NGC 315 and NGC 4261. The X-ray continuum emission of NGC 315 is suitable for cutoff power-law (PL) fitting, yielding a cutoff energy of Ecut &#x3D; 18.45 keV, which is the lowest value found in LLAGNssofar. In contrast, the X-ray continuum of NGC 4261 is composed of two PL components, with no signs of a cutoff energy. A prominent neutral Fe K{\alpha} line is observed in NGC 315, while an ionized Fe XXV line is seen in NGC 4261. The derived reflection fractions are R &#x3D; 0.61 for NGC 315 and R &#x3D; 0.18 for NGC 4579. Neither NGC 315 nor NGC 4261 shows evidence of a Compton reflection bump. Conclusions. The X-ray spectral characteristics support the RIAF emission as the dominant origin of the X-rays in both sources, although an additional soft PL component originating from the inner jet is observed in NGC 4261. The higher reflection fraction compared to other LLAGNs, along with the detection of a neutral Fe K{\alpha} line, suggests the existence of a truncated accretion disk with a relatively small radius in NGC 315. Bremsstrahlung radiation appears to be the dominant cooling mechanism for the plasma in NGC315, while Comptonization within the RIAF is more likely responsible for the X-ray emission in NGC 4261. </p>
<blockquote>
<p>摘要。低光度活动星系核（LLAGNs）的吸积盘是一种辐射效率较低的吸积流（RIAF）。我们的目标是寻找LLAGNs中有喷流的RIAF辐射的证据，并分析其辐射特性，这也为LLAGNs的进一步研究增加了样本。方法。我们对NGC 315和NGC 4261的NuSTAR和XMM-Newton观测所得的X射线数据进行了分析，包括时序和光谱分析。使用XSPEC中的各种函数形式和辐射模型对这两个LLAGNs的联合X射线光谱进行拟合。结果。NGC 315和NGC 4261在几天的时间尺度上均没有观察到显著的变化。NGC 315的X射线连续谱适合用截止幂律（PL）拟合，得到截止能量Ecut&#x3D;18.45 keV，这是迄今为止在LLAGNs中发现的最低值。相比之下，NGC 4261的X射线连续谱由两个PL分量组成，没有截止能量的迹象。在NGC 315中观察到明显的中性Fe Kα线，而在NGC 4261中观察到电离Fe XXV线。推导出的反射率分别为R&#x3D;0.61（NGC 315）和R&#x3D;0.18（NGC 4579）。NGC 315和NGC 4261均没有显示出康普顿反射峰的证据。结论。X射线光谱特征支持RIAF发射是这两个源X射线的主要来源，尽管在NGC 4261中观察到来自内部喷射器的额外软PL分量。与其他LLAGNs相比，较高的反射率以及中性Fe Kα线的检测，表明NGC 315中存在一个半径较小的截断吸积盘。NGC 315中的轫致辐射似乎是等离子体主要的冷却机制，而在NGC 4261中，RIAF内的康普顿化更可能是X射线发射的原因。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06810v1">PDF</a> 11 pages, 6 figures, 6 tables</p>
<p><strong>Summary</strong><br>     本文研究低光度活动星系核（LLAGNs）的辐射特性，特别是其射流辐射的证据。通过对NGC 315和NGC 4261的X射线数据进行分析，发现两者的X射线连续发射与RIAF（辐射低效吸积流）相关。NGC 315的X射线连续发射适合用截断幂律拟合，而NGC 4261则包含两个幂律成分。两者均未显示康普顿反射峰的证据。研究结果支持RIAF辐射是这两个源X射线的主要来源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLAGNs的辐射特性研究旨在寻找RIAF的证据，并增加对未来LLAGNs研究的样本。</li>
<li>对NGC 315和NGC 4261的X射线数据进行了时序和光谱分析。</li>
<li>NGC 315的X射线连续发射适合用截断幂律模型拟合，截断能量Ecut较低。</li>
<li>NGC 4261的X射线连续发射包含两个幂律成分，未发现截断能量。</li>
<li>中性Fe Kα线在NGC 315中明显可见，而在NGC 4261中观察到电离的Fe XXV线。</li>
<li>反射分数R在NGC 315中较高，暗示存在截断半径较小的吸积盘。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06810">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ad5082826c603317c44c8df81709b187~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049813&auth_key=1760049813-0-0-425faffe9e178bda2a4fd8296a20f3d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-650321aade243f73c73a6fc3c4d9be53~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049820&auth_key=1760049820-0-0-e8691db1d40c9aa60c5ebe0780eed4f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5556c2f14abe5c2b213e11f562f3d226~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049827&auth_key=1760049827-0-0-4db4711a5db5fab8fe5cd730f71baffb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2775a98f1a7905a069c0ad335fad9484~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049834&auth_key=1760049834-0-0-1bc2d2bb80eeb93760254020288c7044&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-840c5ebf427d5a1ad2ac448c8b4e5415~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049840&auth_key=1760049840-0-0-1bca8abb4043b3b00b50f6c35bd0f8f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c21d276a8349fa750e0ea72adcaa8a43~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049847&auth_key=1760049847-0-0-8f4c0556bf9d7ef57614327002adce20&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e29e6d06ea166f52b2439725d2be5ce3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049854&auth_key=1760049854-0-0-89dadbfc6841476bbf53322e84633381&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fitzpatrick-Thresholding-for-Skin-Image-Segmentation"><a href="#Fitzpatrick-Thresholding-for-Skin-Image-Segmentation" class="headerlink" title="Fitzpatrick Thresholding for Skin Image Segmentation"></a>Fitzpatrick Thresholding for Skin Image Segmentation</h2><p><strong>Authors:Duncan Stothers, Sophia Xu, Carlie Reeves, Lia Gracey</strong></p>
<p>Accurate estimation of the body surface area (BSA) involved by a rash, such as psoriasis, is critical for assessing rash severity, selecting an initial treatment regimen, and following clinical treatment response. Attempts at segmentation of inflammatory skin disease such as psoriasis perform markedly worse on darker skin tones, potentially impeding equitable care. We assembled a psoriasis dataset sourced from six public atlases, annotated for Fitzpatrick skin type, and added detailed segmentation masks for every image. Reference models based on U-Net, ResU-Net, and SETR-small are trained without tone information. On the tuning split we sweep decision thresholds and select (i) global optima and (ii) per Fitzpatrick skin tone optima for Dice and binary IoU. Adapting Fitzpatrick specific thresholds lifted segmentation performance for the darkest subgroup (Fitz VI) by up to +31 % bIoU and +24 % Dice on UNet, with consistent, though smaller, gains in the same direction for ResU-Net (+25 % bIoU, +18 % Dice) and SETR-small (+17 % bIoU, +11 % Dice). Because Fitzpatrick skin tone classifiers trained on Fitzpatrick-17k now exceed 95 % accuracy, the cost of skin tone labeling required for this technique has fallen dramatically. Fitzpatrick thresholding is simple, model-agnostic, requires no architectural changes, no re-training, and is virtually cost free. We demonstrate the inclusion of Fitzpatrick thresholding as a potential future fairness baseline. </p>
<blockquote>
<p>准确估计体表面积（BSA）的皮炎病变区域，如牛皮癣等，对于评估皮炎严重程度、选择初始治疗方案以及跟踪临床治疗反应至关重要。对牛皮癣等炎症性皮肤疾病进行分割的尝试在深色肤色上的表现明显较差，可能阻碍公平的医疗护理。我们收集了一个牛皮癣数据集，该数据集来源于六个公开图谱，根据Fitzpatrick皮肤类型进行标注，并为每张图像添加了详细的分割掩膜。基于U-Net、ResU-Net和SETR-small的参考模型在不使用肤色信息的情况下进行训练。在调优分割时，我们调整决策阈值，并选择（i）全局最优和（ii）针对Fitzpatrick皮肤类型的最优阈值，以进行Dice和二进制IoU评估。适应Fitzpatrick特定阈值提高了最暗亚组（Fitz VI）的分割性能，在UNet上bIoU提高了+31%，Dice提高了+24%，ResU-Net和SETR-small也有一致且较小的提升（+25% bIoU，+18% Dice和+17% bIoU，+11% Dice）。由于基于Fitzpatrick-17k训练的Fitzpatrick皮肤色调分类器的准确率现在超过95%，因此这项技术所需的肤色标签成本已大幅下降。Fitzpatrick阈值设置简单，与模型无关，不需要进行架构更改、重新训练，并且几乎无需成本。我们展示了将Fitzpatrick阈值设置作为未来公平基准线的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06655v1">PDF</a> Accepted to MICCAI 2025 ISIC Workshop. 24 minute Oral presentation   given. Awarded “Best Paper - Honorable Mention”</p>
<p><strong>摘要</strong></p>
<p>本文关注于准确估算由皮炎（如银屑病）引起的体表面积（BSA），这对评估皮炎严重程度、选择初始治疗方案以及跟踪临床治疗反应至关重要。针对肤色较暗的人群，皮炎等炎症性皮肤病的分割效果较差，可能影响公平护理。本研究收集了一个包含六种公开图谱的银屑病数据集，按Fitzpatrick皮肤类型进行标注，并为每张图像添加了详细的分割掩膜。基于U-Net、ResU-Net和SETR-small的参考模型在不使用肤色信息的情况下进行训练。在调整分割阈值时，我们选择了全局最优和针对Fitzpatrick皮肤类型的最优阈值，以评估Dice和二进制IoU。适应Fitzpatrick特定阈值可提高最暗亚组（Fitz VI）的分割性能，在UNet上bIoU和Dice分别提高了+31%和+24%，ResU-Net和SETR-small也有类似的提高。由于基于Fitzpatrick-17k训练的Fitzpatrick皮肤色调分类器的准确度现在超过了95%，因此此技术所需的肤色标签成本已大幅下降。Fitzpatrick阈值设置简单，模型通用性强，无需进行架构更改或重新训练，且几乎无需成本。我们展示了将Fitzpatrick阈值设置作为未来公平性的基准的潜力。</p>
<p><strong>要点</strong></p>
<ol>
<li>准确估算皮炎涉及的体表面积对于评估皮炎严重程度和治疗反应至关重要。</li>
<li>在肤色较暗的人群中，皮炎分割效果较差，可能影响公平护理。</li>
<li>研究收集了包含多种图像的银屑病数据集，并按Fitzpatrick皮肤类型标注。</li>
<li>研究采用了U-Net、ResU-Net和SETR-small模型进行训练，无需使用肤色信息。</li>
<li>通过适应Fitzpatrick特定阈值，可以显著提高最暗肤色的亚组的分割性能。</li>
<li>Fitzpatrick皮肤色调分类器的准确度现在超过了95%，降低了肤色标签的成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b7cd94a9b27bd64e75683ac98d5fbf8c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049861&auth_key=1760049861-0-0-362a3af6d56a2d6c59d1c4e1ac6594c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50e355081ab0dedec62ef730e8b21aa5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049868&auth_key=1760049868-0-0-4ce35fb1f7250cfa21b0c50b8c581dcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91071309cc62cb43acf405afeab2931c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049874&auth_key=1760049874-0-0-2fcbf766d7105987c95d58ed151972cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d83000f3fee33906f45d18fd6c6d3bf4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083478&auth_key=1760083478-0-0-85977bdefbd7a0820b03474fb24e3b54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3fd89aadaa0d2004aa4d2af8abb269d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083485&auth_key=1760083485-0-0-dc9c3f482229f2d664a7d2dafd6f8b78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FEAorta-A-Fully-Automated-Framework-for-Finite-Element-Analysis-of-the-Aorta-From-3D-CT-Images"><a href="#FEAorta-A-Fully-Automated-Framework-for-Finite-Element-Analysis-of-the-Aorta-From-3D-CT-Images" class="headerlink" title="FEAorta: A Fully Automated Framework for Finite Element Analysis of the   Aorta From 3D CT Images"></a>FEAorta: A Fully Automated Framework for Finite Element Analysis of the   Aorta From 3D CT Images</h2><p><strong>Authors:Jiasong Chen, Linchen Qian, Ruonan Gong, Christina Sun, Tongran Qin, Thuy Pham, Caitlin Martin, Mohammad Zafar, John Elefteriades, Wei Sun, Liang Liang</strong></p>
<p>Aortic aneurysm disease ranks consistently in the top 20 causes of death in the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal bulging of thoracic aortic wall and it is a leading cause of death in adults. From the perspective of biomechanics, rupture occurs when the stress acting on the aortic wall exceeds the wall strength. Wall stress distribution can be obtained by computational biomechanical analyses, especially structural Finite Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be calculated by comparing stress with material strength using a material failure model. Although these engineering tools are currently available for TAA rupture risk assessment on patient specific level, clinical adoption has been limited due to two major barriers: labor intensive 3D reconstruction current patient specific anatomical modeling still relies on manual segmentation, making it time consuming and difficult to scale to a large patient population, and computational burden traditional FEA simulations are resource intensive and incompatible with time sensitive clinical workflows. The second barrier was successfully overcome by our team through the development of the PyTorch FEA library and the FEA DNN integration framework. By incorporating the FEA functionalities within PyTorch FEA and applying the principle of static determinacy, we reduced the FEA based stress computation time to approximately three minutes per case. Moreover, by integrating DNN and FEA through the PyTorch FEA library, our approach further decreases the computation time to only a few seconds per case. This work focuses on overcoming the first barrier through the development of an end to end deep neural network capable of generating patient specific finite element meshes of the aorta directly from 3D CT images. </p>
<blockquote>
<p>主动脉瘤疾病一直是美国人口死亡原因的前20名之一。胸主动脉瘤表现为胸主动脉壁异常凸起，是成人死亡的主要原因之一。从生物力学角度看，当作用于主动脉壁的应力超过其强度时，就会发生破裂。通过计算生物力学分析，特别是结构有限元分析，可以获得壁应力分布。对于风险评估，通过材料失效模型比较应力与材料强度，可以计算胸主动脉瘤的破裂概率。尽管目前这些工程工具可用于针对特定患者的胸主动脉瘤破裂风险评估，但由于两大障碍，临床采用受到了限制：一是目前针对特定患者的解剖模型重建仍依赖于手动分割，这使得其耗时且难以扩展到大量患者；二是传统有限元分析模拟计算量大，不符合时间敏感的临床工作流程。我们团队成功克服了第二个障碍，通过开发PyTorch有限元库和有限元深度神经网络集成框架。通过在PyTorch有限元库中融入有限元功能并应用静定原理，我们将基于有限元的应力计算时间减少到大约每例三分钟。此外，通过深度神经网络与有限元分析集成，我们的方法将计算时间进一步降低到每例仅几秒钟。本工作专注于开发端到端的深度神经网络，以克服第一个障碍，该网络能够从3D CT图像直接生成针对特定患者的有限元素网格。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06621v1">PDF</a> </p>
<p><strong>Summary</strong><br>     主动脉动脉瘤疾病是美国人口死亡的主要原因之一。从生物力学角度看，胸主动脉瘤的破裂是因为主动脉壁上的应力超过了其承受力。本文开发了一种端到端的深度神经网络，用于直接从3D CT图像生成患者的特定有限元网格模型，克服了当前评估胸主动脉瘤破裂风险的两大障碍，提高了评估效率和准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>主动脉动脉瘤疾病是美国人口死亡的主要原因之一，特别是在成人群体中。</li>
<li>从生物力学角度看，胸主动脉瘤的破裂是因为主动脉壁上的应力超过了其承受力。</li>
<li>胸主动脉瘤破裂风险的评估受限于两个主要障碍：当前患者特定解剖模型的建立依赖于手动分割，过程耗时且难以应用于大规模患者群体；传统的有限元分析（FEA）模拟计算量大，不适用于时间敏感的临床工作流程。</li>
<li>开发PyTorch FEA库和FEA深度神经网络（DNN）集成框架成功克服了第二个障碍，将有限元分析的应力计算时间减少到大约每例三分钟。</li>
<li>通过在PyTorch FEA库中集成DNN和FEA，进一步将计算时间减少到每例仅几秒钟。</li>
<li>本文的重点是开发一种端到端的深度神经网络，该网络能够直接从3D CT图像生成患者的特定有限元网格模型，以克服第一个障碍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a112059b3e05068959da39b04655a631~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083493&auth_key=1760083493-0-0-0b4c81a1a5a8a9f7cad0222dbea40041&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff6e4c3f2b28d754aa41a7ff4b1e01a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083500&auth_key=1760083500-0-0-68049adfca0e27735be89cfc3f73ad47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5febed2a3cd59f3cd1309403ab6fc13f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099827&auth_key=1760099827-0-0-4008538c92b499d5967475b1cb6e421b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Self-supervised-Physics-guided-Model-with-Implicit-Representation-Regularization-for-Fast-MRI-Reconstruction"><a href="#Self-supervised-Physics-guided-Model-with-Implicit-Representation-Regularization-for-Fast-MRI-Reconstruction" class="headerlink" title="Self-supervised Physics-guided Model with Implicit Representation   Regularization for Fast MRI Reconstruction"></a>Self-supervised Physics-guided Model with Implicit Representation   Regularization for Fast MRI Reconstruction</h2><p><strong>Authors:Jingran Xu, Yuanyuan Liu, Yanjie Zhu</strong></p>
<p>Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its widespread application is limited by prolonged scan times. Fast MRI reconstruction techniques effectively reduce acquisition duration by reconstructing high-fidelity MR images from undersampled k-space data. In recent years, deep learning-based methods have demonstrated remarkable progress in this field, with self-supervised and unsupervised learning approaches proving particularly valuable in scenarios where fully sampled data are difficult to obtain. This paper proposes a novel zero-shot self-supervised reconstruction framework named UnrollINR, which enables scan-specific MRI reconstruction without relying on external training data. The method adopts a physics-guided unrolled iterative reconstruction architecture and introduces Implicit Neural Representation (INR) as a regularization prior to effectively constrain the solution space. By combining a deep unrolled structure with the powerful implicit representation capability of INR, the model’s interpretability and reconstruction performance are enhanced. Experimental results demonstrate that even at a high acceleration rate of 10, UnrollINR achieves superior reconstruction performance compared to the supervised learning method, validating the superiority of the proposed method. </p>
<blockquote>
<p>磁共振成像（MRI）是一种重要的临床诊断工具，但其广泛应用受限于扫描时间的延长。快速MRI重建技术通过从欠采样的k空间数据中重建高保真MR图像，有效地减少了采集时间。近年来，基于深度学习的方法在该领域取得了显著的进步，自监督和无监督的学习方法在难以获取全采样数据的情况下，特别具有价值。本文提出了一种新型的零样本自监督重建框架，名为UnrollINR，它能够在不依赖外部训练数据的情况下实现特定扫描的MRI重建。该方法采用物理引导展开的迭代重建架构，并引入隐式神经表示（INR）作为正则化先验，以有效地约束解空间。通过将深度展开结构与INR的强大隐式表示能力相结合，提高了模型的可解释性和重建性能。实验结果表明，即使在高达10倍的加速率下，UnrollINR的重建性能也优于监督学习方法，验证了所提方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06611v1">PDF</a> </p>
<p><strong>Summary</strong><br>    MRI重建技术利用深度学习实现高保真度快速扫描。本研究提出一种新型零样本自监督重建框架UnrollINR，可在无需外部训练数据的情况下实现特定扫描MRI重建。结合物理引导展开迭代重建架构与隐式表示（INR）先验，增强模型可解释性和重建性能。实验证明，在加速率为10时，UnrollINR相较于监督学习方法仍表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRI是一种重要的临床诊断工具，但其应用受限于长时间的扫描。</li>
<li>快速MRI重建技术通过从欠采样的k-space数据中重建高保真图像来减少采集时间。</li>
<li>深度学习在MRI重建领域取得显著进展，特别是在难以获取全采样数据的情况下，自监督和无监督学习方法尤为有价值。</li>
<li>本研究提出了一种新型的零样本自监督重建框架UnrollINR，无需外部训练数据即可实现特定扫描MRI重建。</li>
<li>UnrollINR结合物理引导展开迭代重建架构和隐式表示（INR）先验，增强了模型的解释性和重建性能。</li>
<li>实验结果表明，UnrollINR在高速率（如加速率为10）下仍能实现优异的重建性能，优于监督学习方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2123579ff643e697c88a07eb5dd2bfaa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083572&auth_key=1760083572-0-0-837998312fe6216353d42e0e20d636e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-440002aef046452346191200f6a7dc54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083579&auth_key=1760083579-0-0-21dca6e6c627b8e5f5388d405368f855&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adb5b6d268ebe509ed2577e8be0bd4d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083586&auth_key=1760083586-0-0-8ef2e528190dada3de96ca31ee0fa69a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improving-Artifact-Robustness-for-CT-Deep-Learning-Models-Without-Labeled-Artifact-Images-via-Domain-Adaptation"><a href="#Improving-Artifact-Robustness-for-CT-Deep-Learning-Models-Without-Labeled-Artifact-Images-via-Domain-Adaptation" class="headerlink" title="Improving Artifact Robustness for CT Deep Learning Models Without   Labeled Artifact Images via Domain Adaptation"></a>Improving Artifact Robustness for CT Deep Learning Models Without   Labeled Artifact Images via Domain Adaptation</h2><p><strong>Authors:Justin Cheung, Samuel Savine, Calvin Nguyen, Lin Lu, Alhassan S. Yasin</strong></p>
<p>Deep learning models which perform well on images from their training distribution can degrade substantially when applied to new distributions. If a CT scanner introduces a new artifact not present in the training labels, the model may misclassify the images. Although modern CT scanners include design features which mitigate these artifacts, unanticipated or difficult-to-mitigate artifacts can still appear in practice. The direct solution of labeling images from this new distribution can be costly. As a more accessible alternative, this study evaluates domain adaptation as an approach for training models that maintain classification performance despite new artifacts, even without corresponding labels. We simulate ring artifacts from detector gain error in sinogram space and evaluate domain adversarial neural networks (DANN) against baseline and augmentation-based approaches on the OrganAMNIST abdominal CT dataset. Our results demonstrate that baseline models trained only on clean images fail to generalize to images with ring artifacts, and traditional augmentation with other distortion types provides no improvement on unseen artifact domains. In contrast, the DANN approach successfully maintains high classification accuracy on ring artifact images using only unlabeled artifact data during training, demonstrating the viability of domain adaptation for artifact robustness. The domain-adapted model achieved classification performance on ring artifact test data comparable to models explicitly trained with labeled artifact images, while also showing unexpected generalization to uniform noise. These findings provide empirical evidence that domain adaptation can effectively address distribution shift in medical imaging without requiring expensive expert labeling of new artifact distributions, suggesting promise for deployment in clinical settings where novel artifacts may emerge. </p>
<blockquote>
<p>在深学习效果模型中，针对其训练图像分布的效果显著，但当应用于新的分布时性能会明显下降。如果CT扫描仪引入了训练标签中不存在的新的伪影，模型可能会误分类图像。尽管现代CT扫描仪包括减少这些伪影的设计功能，但在实践中仍可能出现无法预见或难以消除的伪影。直接解决新分布图像标签的问题成本高昂。作为更可行的替代方案，本研究评估了域适应作为一种训练模型的方法，该方法能够在新的伪影出现的情况下保持分类性能，即使在没有相应标签的情况下也是如此。我们在辛诺图空间中模拟了由检测器增益误差引起的环形伪影，并在OrganAMNIST腹部CT数据集上对域对抗神经网络（DANN）与基线方法和基于增强的方法进行了评估。结果表明，仅在干净图像上训练的基线模型无法推广到带有环形伪影的图像，而其他失真类型的传统增强对未见过的伪影域没有改善作用。相比之下，DANN方法仅使用无标签的伪影数据对环形伪影图像进行训练，成功地保持了较高的分类精度，证明了域适应在伪影稳健性方面的可行性。域适应模型在环形伪影测试数据上的分类性能与用带标签的伪影图像明确训练的模型相当，并且还显示出对均匀噪声的意外泛化能力。这些发现提供了实证证据表明，域适应可以有效地解决医学影像中的分布转移问题，而无需对新伪影分布进行昂贵的专家标注，这在可能出现新型伪影的临床环境中部署时具有广阔的前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06584v1">PDF</a> 8 pages, 12 figures, 1 table</p>
<p><strong>摘要</strong></p>
<p>深度学习模型在图像分布上表现良好，但当应用于新的分布时，性能可能会大幅下降。如果CT扫描仪引入新的训练中不存在的伪影，模型可能会对图像误分类。尽管现代CT扫描仪设计了一些减少伪影的特征，但在实践中仍然可能出现难以预见或难以消除的伪影。对此新分布进行标注的直接解决方案成本高昂。作为更可行的替代方案，本研究评估了域适应作为训练模型的方法，即使面对新伪影，也能保持分类性能，无需对应标签。我们在辛图拉空间模拟了因探测器增益误差而产生的环形伪影，并在OrganAMNIST腹部CT数据集上评估了域对抗神经网络（DANN）与基准方法和基于增强方法的效果。结果表明，仅对干净图像进行训练的基准模型无法推广到带有环形伪影的图像，而其他类型畸变的传统增强方法对未见过的伪影域没有任何改进。相比之下，DANN方法仅使用训练过程中的无标签伪影数据，便成功地在环形伪影图像上保持了较高的分类精度，证明了域适应在伪影稳健性方面的可行性。域适应模型的分类性能与明确使用标记伪影图像训练的模型在环形伪影测试数据上的表现相当，并且还意外地推广到了均匀噪声。这些发现提供了实证证据表明，域适应可以有效地解决医学成像中的分布转移问题，而无需昂贵的专家为新伪影分布进行标注，这为在临床环境中部署提供了希望，因为那里可能会出现新的伪影。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>深度学习模型在面对新的图像分布时可能会遭受性能下降的问题，特别是在医学图像领域。</li>
<li>当CT扫描仪引入新的伪影时，模型可能对图像产生误分类。</li>
<li>现代CT扫描仪的设计特性虽然能减少伪影，但在实践中仍可能出现不可预见或难以消除的伪影。</li>
<li>直接对新分布的图像进行标注成本高昂且不可行。</li>
<li>域适应是一种有效的训练模型方法，能在面对新伪影时保持分类性能，且无需对应标签。</li>
<li>在模拟环形伪影的情况下，域对抗神经网络（DANN）表现出优异的性能，能够在无标签伪影数据的训练中保持高分类精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06584">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5b2b5ca59a53cdcffc3a9bad22c331d9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083594&auth_key=1760083594-0-0-677e791211ec6d3d9acdd50f90444b23&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b76b48361079841bc0d79647c42c4984~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083601&auth_key=1760083601-0-0-e7ab1ceda166bcef1a3ff86195d4da26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-abf9b2b535d993383b069d50f5d6625c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083608&auth_key=1760083608-0-0-72caf81f374fb9718555c853ee388389&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71cfd51212f07bd65174a0ef52299be0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083614&auth_key=1760083614-0-0-8c637c13c200a50d84e5c7e974647307&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16a3634ddc8d531b1151ba14d9ab2e93~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083621&auth_key=1760083621-0-0-5de1886cfe77dd4dc62ecebefb8debca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d4e4e793be993219f223fb6fc002f8cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083628&auth_key=1760083628-0-0-444d4e3f80e3d21176044661d3d689f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-709fce7edfc95649a49d0a5213a7d699~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083635&auth_key=1760083635-0-0-d91297f45850cc995ad07142519cd654&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-945c034241f6f939cc6a5cb69b832e0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099833&auth_key=1760099833-0-0-4d5a72899d11023692d1b5093c2a3300&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Conditional-Denoising-Diffusion-Model-Based-Robust-MR-Image-Reconstruction-from-Highly-Undersampled-Data"><a href="#Conditional-Denoising-Diffusion-Model-Based-Robust-MR-Image-Reconstruction-from-Highly-Undersampled-Data" class="headerlink" title="Conditional Denoising Diffusion Model-Based Robust MR Image   Reconstruction from Highly Undersampled Data"></a>Conditional Denoising Diffusion Model-Based Robust MR Image   Reconstruction from Highly Undersampled Data</h2><p><strong>Authors:Mohammed Alsubaie, Wenxi Liu, Linxia Gu, Ovidiu C. Andronesi, Sirani M. Perera, Xianqi Li</strong></p>
<p>Magnetic Resonance Imaging (MRI) is a critical tool in modern medical diagnostics, yet its prolonged acquisition time remains a critical limitation, especially in time-sensitive clinical scenarios. While undersampling strategies can accelerate image acquisition, they often result in image artifacts and degraded quality. Recent diffusion models have shown promise for reconstructing high-fidelity images from undersampled data by learning powerful image priors; however, most existing approaches either (i) rely on unsupervised score functions without paired supervision or (ii) apply data consistency only as a post-processing step. In this work, we introduce a conditional denoising diffusion framework with iterative data-consistency correction, which differs from prior methods by embedding the measurement model directly into every reverse diffusion step and training the model on paired undersampled-ground truth data. This hybrid design bridges generative flexibility with explicit enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that our framework consistently outperforms recent state-of-the-art deep learning and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing perceptual improvements more faithfully. These results demonstrate that integrating conditional supervision with iterative consistency updates yields substantial improvements in both pixel-level fidelity and perceptual realism, establishing a principled and practical advance toward robust, accelerated MRI reconstruction. </p>
<blockquote>
<p>磁共振成像（MRI）是现代医学诊断中的重要工具，但其长时间的采集时间仍是其关键限制，特别是在时间敏感的临床场景中。尽管欠采样策略可以加速图像采集，但它们通常会导致图像出现伪影和质量下降。最近的扩散模型通过学习强大的图像先验来从欠采样数据中重建高保真图像，显示出很大的潜力；然而，大多数现有方法（i）依赖于无配对监督的无监督分数函数，或者（ii）仅将数据一致性作为后处理步骤应用。在这项工作中，我们引入了一个具有迭代数据一致性校正的条件去噪扩散框架，它与先前的方法不同，将测量模型直接嵌入到每个反向扩散步骤中，并在配对欠采样-真实数据上训练模型。这种混合设计结合了生成灵活性和对MRI物理的显式强制执行。在fastMRI数据集上的实验表明，我们的框架在结构相似性度量（SSIM）、峰值信噪比（PSNR）和局部感知图像相似性（LPIPS）方面始终优于最新的深度学习和扩散方法。LPIPS更真实地捕捉到了感知改进。这些结果表明，将条件监督与迭代一致性更新相结合，在像素级保真度和感知真实性方面都取得了重大改进，为实现稳健、加速的MRI重建提供了有原则且实用的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06335v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出了一种结合条件去噪扩散框架与迭代数据一致性校正的方法，用于加速磁共振成像（MRI）的重建过程。该方法将测量模型直接嵌入到每个反向扩散步骤中，并在配对欠采样-真实数据上进行训练。实验表明，该方法在SSIM、PSNR和LPIPS指标上均优于最新的深度学习和扩散方法，特别是在感知改善方面表现更出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>磁共振成像（MRI）是现代医学诊断中的重要工具，但其长时间的采集时间仍是限制其应用的关键因素。</li>
<li>欠采样策略可以加速图像采集，但可能导致图像出现伪影和质量下降。</li>
<li>扩散模型在从未完全采样的数据中重建高质量图像方面显示出潜力。</li>
<li>现有方法大多依赖于无监督的评分函数或仅将数据一致性作为后处理步骤。</li>
<li>本文介绍了一种结合条件去噪扩散框架和迭代数据一致性校正的方法，将测量模型直接嵌入到每个反向扩散步骤中。</li>
<li>该方法在配对欠采样-真实数据上进行训练，在SSIM、PSNR和LPIPS指标上表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06335">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ed8b3160742ce192f18a08457140bd45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099842&auth_key=1760099842-0-0-483988e1c39295cad6e0691a36055b6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a3182fbbb9dc6009f8d71fba37c7e63~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099849&auth_key=1760099849-0-0-c5ae15f96354c43b929e3737761326e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f790d41f7e1893c270c91d01918e509a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099856&auth_key=1760099856-0-0-e0b8e14d75fc63856027292555b08484&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c3db9fbaa3d5f95b052d749358bf897~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099862&auth_key=1760099862-0-0-026d09cc2c096ee613a6a34e0fbbfe5d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7790fa011158758e7e700c74906aa7db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099869&auth_key=1760099869-0-0-ffe9d90427acdc6489fa75912dc27e04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b18c9aed058fc63e92c26f872bc905e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099876&auth_key=1760099876-0-0-627cae5beddfb80038a6bc1ad4411f51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Soft-Evidence-Fused-Graph-Neural-Network-for-Cancer-Driver-Gene-Identification-across-Multi-View-Biological-Graphs"><a href="#Soft-Evidence-Fused-Graph-Neural-Network-for-Cancer-Driver-Gene-Identification-across-Multi-View-Biological-Graphs" class="headerlink" title="Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene   Identification across Multi-View Biological Graphs"></a>Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene   Identification across Multi-View Biological Graphs</h2><p><strong>Authors:Bang Chen, Lijun Guo, Houli Fan, Wentao He, Rong Zhang</strong></p>
<p>Identifying cancer driver genes (CDGs) is essential for understanding cancer mechanisms and developing targeted therapies. Graph neural networks (GNNs) have recently been employed to identify CDGs by capturing patterns in biological interaction networks. However, most GNN-based approaches rely on a single protein-protein interaction (PPI) network, ignoring complementary information from other biological networks. Some studies integrate multiple networks by aligning features with consistency constraints to learn unified gene representations for CDG identification. However, such representation-level fusion often assumes congruent gene relationships across networks, which may overlook network heterogeneity and introduce conflicting information. To address this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), a novel framework for CDG identification across multiple networks at the decision level. Instead of enforcing feature-level consistency, SEFGNN treats each biological network as an independent evidence source and performs uncertainty-aware fusion at the decision level using Dempster-Shafer Theory (DST). To alleviate the risk of overconfidence from DST, we further introduce a Soft Evidence Smoothing (SES) module that improves ranking stability while preserving discriminative performance. Experiments on three cancer datasets show that SEFGNN consistently outperforms state-of-the-art baselines and exhibits strong potential in discovering novel CDGs. </p>
<blockquote>
<p>识别癌症驱动基因（CDGs）对于理解癌症机制和开发靶向疗法至关重要。图神经网络（GNNs）最近被用来通过捕捉生物交互网络中的模式来识别CDGs。然而，大多数基于GNN的方法都依赖于单一的蛋白质-蛋白质相互作用（PPI）网络，忽略了其他生物网络中的补充信息。一些研究通过特征对齐和一致性约束来整合多个网络，以学习统一的基因表示来进行CDG识别。然而，这种表示级别的融合通常假设网络之间的基因关系是一致的，这可能会忽略网络的异质性并引入冲突信息。为了解决这一问题，我们提出了Soft-Evidence Fusion Graph Neural Network（SEFGNN），这是一个在决策层面进行跨多个网络CDG识别的新型框架。与强制特征层面的一致性不同，SEFGNN将每个生物网络视为独立的证据来源，并使用Dempster-Shafer理论（DST）在决策层面进行不确定性感知融合。为了进一步减轻DST过度自信的风险，我们进一步引入了Soft Evidence Smoothing（SES）模块，该模块提高了排名稳定性，同时保留了鉴别性能。在三个癌症数据集上的实验表明，SEFGNN始终优于最先进的基线，并在发现新的CDGs方面表现出强大的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06290v1">PDF</a> 8pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于图神经网络（GNNs）的新方法——软证据融合图神经网络（SEFGNN），用于在多个网络层面识别癌症驱动基因（CDGs）。SEFGNN利用Dempster-Shafer理论（DST）进行决策层面的不确定性感知融合，而非特征层面的融合，从而提高对多种生物网络信息的利用效率。同时，引入软证据平滑（SES）模块，提高排名稳定性并保持鉴别性能。在三个癌症数据集上的实验表明，SEFGNN持续超越现有基线，并在发现新的CDGs方面展现出强大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNs已被用于识别癌症驱动基因（CDGs），通过捕捉生物交互网络中的模式来实现。</li>
<li>现有方法大多依赖单一的蛋白质-蛋白质交互（PPI）网络，忽略了其他生物网络中的互补信息。</li>
<li>一些研究通过特征对齐和一致性约束来整合多个网络，学习统一的基因表示来进行CDG识别，但这种方法可能忽略网络异质性和引入冲突信息。</li>
<li>SEFGNN框架被提出，用于在决策层面跨多个网络进行CDG识别。</li>
<li>SEFGNN不强制特征一致性，而是将每个生物网络视为独立的证据源，使用Dempster-Shafer理论（DST）进行不确定性感知的融合。</li>
<li>为了减轻DST可能带来的过度自信风险，引入了Soft Evidence Smoothing（SES）模块，提高了排名稳定性并保持鉴别性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cc00e06e3f424a2cfa75f684324b61cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099884&auth_key=1760099884-0-0-742f770f16f15302312512b93ad2e94e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff80b884679afcd072b844e6877aff05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099891&auth_key=1760099891-0-0-4aaff962253207f39dbebcbd7177a8e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be74822e95330a8e62c5f7561407415c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099899&auth_key=1760099899-0-0-31235dbce8f04bce56d31482270c08c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b979032f0dfa4d550cd665a90ff20c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099907&auth_key=1760099907-0-0-e8790d85d4887ce0671ceda7ba69e8d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3de8c14de8b03efe26f3704101cdec4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099914&auth_key=1760099914-0-0-83c6a6d2aa4283a8d4a1c549a3733e0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc361f2d2ea90471b1da56b148e96a1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099921&auth_key=1760099921-0-0-5f26cbd3aaa3bc850dc1cb2d7cae2823&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5a04dec660140f23eb744c879fe6ad4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099928&auth_key=1760099928-0-0-8bce19b709d9dc228627b934f5fd5732&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1664751aeb33cd85ad37434343615ac0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099935&auth_key=1760099935-0-0-56e02a46f93b0e5f507f6078df1f4081&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96ba56e16f4a321d98c7d4db2feb0fee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099943&auth_key=1760099943-0-0-a8bd9a74f0807501d7c92da6b7b3762a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SER-Diff-Synthetic-Error-Replay-Diffusion-for-Incremental-Brain-Tumor-Segmentation"><a href="#SER-Diff-Synthetic-Error-Replay-Diffusion-for-Incremental-Brain-Tumor-Segmentation" class="headerlink" title="SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor   Segmentation"></a>SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor   Segmentation</h2><p><strong>Authors:Sashank Makanaboyina</strong></p>
<p>Incremental brain tumor segmentation is critical for models that must adapt to evolving clinical datasets without retraining on all prior data. However, catastrophic forgetting, where models lose previously acquired knowledge, remains a major obstacle. Recent incremental learning frameworks with knowledge distillation partially mitigate forgetting but rely heavily on generative replay or auxiliary storage. Meanwhile, diffusion models have proven effective for refining tumor segmentations, but have not been explored in incremental learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the first framework that unifies diffusion-based refinement with incremental learning. SER-Diff leverages a frozen teacher diffusion model to generate synthetic error maps from past tasks, which are replayed during training on new tasks. A dual-loss formulation combining Dice loss for new data and knowledge distillation loss for replayed errors ensures both adaptability and retention. Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff consistently outperforms prior methods. It achieves the highest Dice scores of 95.8%, 94.9%, and 94.6%, along with the lowest HD95 values of 4.4 mm, 4.7 mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only mitigates catastrophic forgetting but also delivers more accurate and anatomically coherent segmentations across evolving datasets. </p>
<blockquote>
<p>增量脑肿瘤分割对于必须适应不断演变的临床数据集而无需对所有先前数据进行再训练模型至关重要。然而，灾难性遗忘（模型失去先前获得的知识）仍然是主要障碍。最近的增量学习框架通过知识蒸馏在一定程度上减轻了遗忘，但严重依赖于生成回放或辅助存储。同时，扩散模型在细化肿瘤分割方面已被证明是有效的，但在增量学习环境中尚未得到探索。我们提出了合成误差回放扩散（SER-Diff），这是第一个将基于扩散的精炼与增量学习统一起来的框架。SER-Diff利用冻结的教师扩散模型生成合成误差图，这些误差图会在新任务训练期间进行回放。结合Dice损失（用于新数据）和知识蒸馏损失（用于回放误差）的双损失公式确保了适应性和保留性。在BraTS2020、BraTS2021和BraTS2023上的实验表明，SER-Diff始终优于先前的方法。它达到了最高的Dice分数分别为95.8％、94.9％和94.6％，以及最低的HD95值分别为4.4毫米、4.7毫米和4.9毫米。这些结果表明，SER-Diff不仅减轻了灾难性遗忘，而且在不断发展的数据集中提供了更准确和解剖结构连贯的分割。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06283v1">PDF</a> </p>
<p><strong>Summary</strong><br>    本文提出了一种结合扩散模型与增量学习的新框架——合成错误回放扩散（SER-Diff），用于逐步优化肿瘤分割模型，在不重新训练所有数据的情况下适应不断发展的临床数据集。通过利用冻结的教师扩散模型生成合成误差图，并对其进行回放训练，SER-Diff有效解决了模型遗忘先前知识的问题，同时保证了对新数据的适应性和知识的保留。实验结果表明，SER-Diff在BraTS数据集上的表现优于其他方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>增量脑肿瘤分割对于适应不断发展的临床数据集至关重要，但模型遗忘先前知识的问题仍是主要挑战。</li>
<li>现有的增量学习框架借助知识蒸馏减轻遗忘，但依赖生成回放或辅助存储。</li>
<li>扩散模型已被证明能有效改进肿瘤分割，但在增量学习环境中尚未被探索。</li>
<li>提出了一种新的框架——合成错误回放扩散（SER-Diff），结合了扩散模型的精炼和增量学习。</li>
<li>SER-Diff利用冻结的教师扩散模型生成合成误差图，这些误差图在新任务训练期间进行回放。</li>
<li>通过结合Dice损失和回放误差的知识蒸馏损失，确保模型的适应性和知识保留。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06283">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b3264ae4ed4da34c744787c124abb30a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099951&auth_key=1760099951-0-0-1968ee26800d373f525be4532aba3d30&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2838e4df9d2bbab515c028b26e72c24c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099959&auth_key=1760099959-0-0-c7c8f918d8b79cb6b9756666cc9b8bcb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Total-Variation-Regularized-Framework-for-Epilepsy-Related-MRI-Image-Segmentation"><a href="#A-Total-Variation-Regularized-Framework-for-Epilepsy-Related-MRI-Image-Segmentation" class="headerlink" title="A Total Variation Regularized Framework for Epilepsy-Related MRI Image   Segmentation"></a>A Total Variation Regularized Framework for Epilepsy-Related MRI Image   Segmentation</h2><p><strong>Authors:Mehdi Rabiee, Sergio Greco, Reza Shahbazian, Irina Trubitsyna</strong></p>
<p>Focal Cortical Dysplasia (FCD) is a primary cause of drug-resistant epilepsy and is difficult to detect in brain {magnetic resonance imaging} (MRI) due to the subtle and small-scale nature of its lesions. Accurate segmentation of FCD regions in 3D multimodal brain MRI images is essential for effective surgical planning and treatment. However, this task remains highly challenging due to the limited availability of annotated FCD datasets, the extremely small size and weak contrast of FCD lesions, the complexity of handling 3D multimodal inputs, and the need for output smoothness and anatomical consistency, which is often not addressed by standard voxel-wise loss functions. This paper presents a new framework for segmenting FCD regions in 3D brain MRI images. We adopt state-of-the-art transformer-enhanced encoder-decoder architecture and introduce a novel loss function combining Dice loss with an anisotropic {Total Variation} (TV) term. This integration encourages spatial smoothness and reduces false positive clusters without relying on post-processing. The framework is evaluated on a public FCD dataset with 85 epilepsy patients and demonstrates superior segmentation accuracy and consistency compared to standard loss formulations. The model with the proposed TV loss shows an 11.9% improvement on the Dice coefficient and 13.3% higher precision over the baseline model. Moreover, the number of false positive clusters is reduced by 61.6% </p>
<blockquote>
<p>皮层发育不良（Focal Cortical Dysplasia，简称FCD）是导致药物难治性癫痫的主要原因之一。由于其病变具有细微和小规模的特点，在脑部磁共振成像（MRI）中难以检测。在3D多模态脑部MRI图像中对FCD区域进行准确的分割对于有效的手术规划和治疗至关重要。然而，这一任务仍然极具挑战性，主要是由于标注的FCD数据集有限、FCD病变极小且对比度极弱、处理3D多模态输入的复杂性以及需要输出平滑和解剖一致性，而标准像素级损失函数通常无法解决这一问题。本文提出了一种新的框架，用于在3D脑部MRI图像中分割FCD区域。我们采用了最先进的增强型转换器编码器-解码器架构，并引入了一种结合Dice损失和各项异性总变差（Total Variation，简称TV）术语的新型损失函数。这种集成鼓励空间平滑性，并减少假阳性簇，无需依赖后处理。该框架在包含85名癫痫患者的公共FCD数据集上进行了评估，并展示了与标准损失公式相比的优越分割精度和一致性。采用所提TV损失的模型在Dice系数上提高了11.9%，并且在精度上比基线模型提高了13.3%。此外，假阳性簇的数量减少了61.6%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06276v1">PDF</a> </p>
<p><strong>摘要</strong><br>    本文提出了一种新的框架，用于在3D脑MRI图像中准确分割Focal Cortical Dysplasia（FCD）区域。采用先进的基于transformer的编码器-解码器架构，并引入了一种结合Dice损失和各项异性Total Variation（TV）术语的新型损失函数。该框架在公共FCD数据集上进行了评估，包含85名癫痫患者，展示了出色的分割精度和一致性。与传统的损失函数相比，新框架提高了Dice系数的11.9%，精确度提高了13.3%，并减少了61.6%的误报簇数量。</p>
<p><strong>要点</strong></p>
<ol>
<li>FCD是药物难治性癫痫的主要原因，其在脑MRI中的检测由于病变细微且小规模而具有挑战性。</li>
<li>准确分割FCD区域对于有效的手术规划和治疗至关重要。</li>
<li>当前FCD区域分割面临挑战，包括数据集标注有限、FCD病变尺寸小且对比度低、处理3D多模态输入的复杂性以及需要输出平滑和解剖一致性的要求。</li>
<li>本文提出了一种新的框架，采用先进的基于transformer的编码器-解码器架构进行FCD区域分割。</li>
<li>引入了一种结合Dice损失和各项异性Total Variation（TV）术语的新型损失函数，鼓励空间平滑性并减少误报簇，无需依赖后处理。</li>
<li>在公共FCD数据集上的评估显示，新框架在分割准确性和一致性方面表现出优越性，与基准模型相比，Dice系数提高了11.9%，精确度提高了13.3%，误报簇数量减少了61.6%。</li>
<li>新框架的提出有望改善FCD的诊断和治疗，为癫痫患者的手术规划提供更有力的支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06276">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d575f24daf56be6670c038b2a36af51b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099966&auth_key=1760099966-0-0-37a2d967892a4a77e0665a89d446c099&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c357ae80dbb04d871cb3b65930c90f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102789&auth_key=1760102789-0-0-764d676370ad5daa63b50b21cd4f5213&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Universal-Models-for-Medical-Image-Segmentation-via-Weakly-Supervised-In-Context-Learning"><a href="#Efficient-Universal-Models-for-Medical-Image-Segmentation-via-Weakly-Supervised-In-Context-Learning" class="headerlink" title="Efficient Universal Models for Medical Image Segmentation via Weakly   Supervised In-Context Learning"></a>Efficient Universal Models for Medical Image Segmentation via Weakly   Supervised In-Context Learning</h2><p><strong>Authors:Jiesi Hu, Yanwu Yang, Zhiyu Ye, Jinyan Zhou, Jianfeng Cao, Hanyang Peng, Ting Ma</strong></p>
<p>Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/jiesihu/Weak-ICL">https://github.com/jiesihu/Weak-ICL</a>. </p>
<blockquote>
<p>针对医学图像分割的通用模型，如交互式和上下文学习（ICL）模型等，虽然具有较强的泛化能力，但需要大量的标注。交互式模型需要针对每张图像进行多次用户提示，而ICL则依赖于密集的像素级标签。为解决这一问题，我们提出了弱监督上下文学习（WS-ICL）这一新的ICL范式，它利用弱提示（如边界框或点）而不是密集标签来进行上下文学习。这种方法通过消除对精细掩膜和所有图像重复用户提示的需求，显著减少了标注工作量。我们在三个独立的基准测试上对提出的WS-ICL模型进行了评估。实验结果表明，WS-ICL在标注成本显著降低的情况下实现了与常规ICL模型相当的性能。此外，即使在交互式范式下，WS-ICL也具有很强的竞争力。这些发现表明，WS-ICL是朝着更高效、更统一的医学图像分割通用模型迈出的有前途的一步。我们的代码和模型已在<a target="_blank" rel="noopener" href="https://github.com/jiesihu/Weak-ICL%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/jiesihu/Weak-ICL公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05899v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像分割的通用模型，如交互式和上下文学习（ICL）模型，虽然具有良好的泛化能力，但需要大量的标注。交互式模型需要针对每张图像进行多次用户提示，而ICL则依赖于密集的像素级标签。为解决这一问题，我们提出了弱监督上下文学习（WS-ICL）这一新的ICL模式，它利用弱提示（如边界框或点）而不是密集标签来提供上下文。这种方法通过消除对精细掩膜和所有图像重复用户提示的需求，大大降低了标注工作量。我们在三个独立的基准测试上对WS-ICL模型进行了评估。实验结果表明，WS-ICL在性能上可与常规ICL模型相媲美，同时大大降低了标注成本。此外，即使在交互式模式下，WS-ICL也表现出强大的竞争力。这些发现表明WS-ICL是朝着更高效、更统一的医学图像分割通用模型的重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割的通用模型需要解决标注成本高昂的问题。</li>
<li>交互式和上下文学习（ICL）模型具有良好的泛化能力，但需要大量标注。</li>
<li>WS-ICL是一种新的ICL模式，利用弱提示来提供上下文信息，显著降低了标注工作量。</li>
<li>WS-ICL模型性能与常规ICL模型相当，且更节省标注成本。</li>
<li>WS-ICL在交互式模式下也表现出强大的竞争力。</li>
<li>实验结果在三个基准测试上验证了WS-ICL的有效性和优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05899">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8a5292bc220ea1ed6572a212d89d1b19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100039&auth_key=1760100039-0-0-227183f7b29c5d019829e9a1e4ffab1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9eceb461264db6c6105295bd162d8311~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100047&auth_key=1760100047-0-0-9e687d577d5e75d86f7b8634b62a56b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba7ce3cab4ca9533495c8b3688cc820e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760100053&auth_key=1760100053-0-0-91ac86d820604a2d99953f500022fc9c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc5f78297c6f0373bf41e51a444d4fdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102796&auth_key=1760102796-0-0-28569ee50af46babcff3c26e533eed36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91ab70b2458a03b0a0eaea80e5071d7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102802&auth_key=1760102802-0-0-420f59340093dc099b07993a6dca7276&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8d64bbf0d07cb523a6bfbb8370f1a55~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102809&auth_key=1760102809-0-0-46ad05e6a83e4378cfe59f6b1aa59177&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="acia-workflows-Automated-Single-cell-Imaging-Analysis-for-Scalable-and-Deep-Learning-based-Live-cell-Imaging-Analysis-Workflows"><a href="#acia-workflows-Automated-Single-cell-Imaging-Analysis-for-Scalable-and-Deep-Learning-based-Live-cell-Imaging-Analysis-Workflows" class="headerlink" title="acia-workflows: Automated Single-cell Imaging Analysis for Scalable and   Deep Learning-based Live-cell Imaging Analysis Workflows"></a>acia-workflows: Automated Single-cell Imaging Analysis for Scalable and   Deep Learning-based Live-cell Imaging Analysis Workflows</h2><p><strong>Authors:Johannes Seiffarth, Keitaro Kasahara, Michelle Bund, Benita Lückel, Richard D. Paul, Matthias Pesch, Lennart Witting, Michael Bott, Dietrich Kohlheyer, Katharina Nöh</strong></p>
<p>Live-cell imaging (LCI) technology enables the detailed spatio-temporal characterization of living cells at the single-cell level, which is critical for advancing research in the life sciences, from biomedical applications to bioprocessing. High-throughput setups with tens to hundreds of parallel cell cultivations offer the potential for robust and reproducible insights. However, these insights are obscured by the large amount of LCI data recorded per experiment. Recent advances in state-of-the-art deep learning methods for cell segmentation and tracking now enable the automated analysis of such large data volumes, offering unprecedented opportunities to systematically study single-cell dynamics. The next key challenge lies in integrating these powerful tools into accessible, flexible, and user-friendly workflows that support routine application in biological research. In this work, we present acia-workflows, a platform that combines three key components: (1) the Automated live-Cell Imaging Analysis (acia) Python library, which supports the modular design of image analysis pipelines offering eight deep learning segmentation and tracking approaches; (2) workflows that assemble the image analysis pipeline, its software dependencies, documentation, and visualizations into a single Jupyter Notebook, leading to accessible, reproducible and scalable analysis workflows; and (3) a collection of application workflows showcasing the analysis and customization capabilities in real-world applications. Specifically, we present three workflows to investigate various types of microfluidic LCI experiments ranging from growth rate comparisons to precise, minute-resolution quantitative analyses of individual dynamic cells responses to changing oxygen conditions. Our collection of more than ten application workflows is open source and publicly available at <a target="_blank" rel="noopener" href="https://github.com/JuBiotech/acia-workflows">https://github.com/JuBiotech/acia-workflows</a>. </p>
<blockquote>
<p>活细胞成像（LCI）技术能够在单细胞水平上对活细胞进行详细的时空特征表征，这对于推动生命科学领域的研究至关重要，从生物医学应用到生物加工。具有数十到数百个并行细胞培养的高通量设置提供了获得稳健且可重复见解的潜力。然而，这些见解被每次实验记录的大量LCI数据所掩盖。最近先进的深度学习细胞分割和追踪方法的进步现在能够实现此类大数据量的自动分析，为系统研究单细胞动态提供了前所未有的机会。下一个关键挑战在于将这些强大工具集成到可访问、灵活且用户友好的工作流中，以支持生物学研究中的常规应用。在这项工作中，我们推出了acia-workflows平台，该平台结合了三个关键组件：（1）Automated live-Cell Imaging Analysis（acia）Python库，支持图像分析管道模块化设计，提供八种深度学习分割和追踪方法；（2）将图像分析管道、其软件依赖项、文档和可视化组装成单个Jupyter Notebook的工作流，从而实现可访问、可重复和可扩展的分析工作流；（3）展示实际应用程序中分析和定制能力的一系列应用工作流。具体来说，我们推出了三种工作流，用于研究从生长速率比较到对变化氧气条件下单个动态细胞的精确分钟分辨率定量分析的各种类型的微流体LCI实验。我们的超过十个应用工作流是开源的，可在<a target="_blank" rel="noopener" href="https://github.com/JuBiotech/acia-workflows%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JuBiotech/acia-workflows公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05886v2">PDF</a> </p>
<p><strong>Summary</strong><br>     活体细胞成像技术可实现单细胞水平的时空特征详细表征，对生命科学领域的研究至关重要。深度学习方法的最新进展为细胞分割和追踪提供了自动化分析大型数据集的机会，为研究单细胞动态提供了前所未有的机会。当前的关键挑战在于将这些强大工具集成到易于访问、灵活和用户友好的工作流中，以支持生物学研究中的常规应用。在此，我们展示了acia-workflows平台，该平台结合了自动化活细胞成像分析库、图像分析管道的工作流以及展示实际应用的分析和定制能力的工作流集合。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>活细胞成像技术对于生命科学领域的研究至关重要，能够实现单细胞水平的详细时空表征。</li>
<li>深度学习方法的最新进展为细胞分割和追踪的自动化分析提供了机会。</li>
<li>目前挑战在于将自动化分析工具集成到易于访问、灵活和用户友好的工作流中。</li>
<li>acia-workflows平台结合了自动化活细胞成像分析库、图像分析管道的工作流以及展示工作流集合的应用程序。</li>
<li>提供了超过十种应用工作流，均为开源并可在公开渠道获取。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d3b02196d6b5e12dd5f070d38f59dbfd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102817&auth_key=1760102817-0-0-1bc1a23ffc74778cb463b6a8a3274c74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4e31bf6e5ba629e965c7206a07726bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144617&auth_key=1760144617-0-0-6072e12985f5753ebbf605f638c33bdd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TFM-Dataset-A-Novel-Multi-task-Dataset-and-Integrated-Pipeline-for-Automated-Tear-Film-Break-Up-Segmentation"><a href="#TFM-Dataset-A-Novel-Multi-task-Dataset-and-Integrated-Pipeline-for-Automated-Tear-Film-Break-Up-Segmentation" class="headerlink" title="TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for   Automated Tear Film Break-Up Segmentation"></a>TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for   Automated Tear Film Break-Up Segmentation</h2><p><strong>Authors:Guangrong Wan, Jun liu, Qiyang Zhou, Tang tang, Lianghao Shi, Wenjun Luo, TingTing Xu</strong></p>
<p>Tear film break-up (TFBU) analysis is critical for diagnosing dry eye syndrome, but automated TFBU segmentation remains challenging due to the lack of annotated datasets and integrated solutions. This paper introduces the Tear Film Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task tear film analysis, comprising 15 high-resolution videos (totaling 6,247 frames) annotated with three vision tasks: frame-level classification (‘clear’, ‘closed’, ‘broken’, ‘blur’), Placido Ring detection, and pixel-wise TFBU area segmentation. Leveraging this dataset, we first propose TF-Net, a novel and efficient baseline segmentation model. TF-Net incorporates a MobileOne-mini backbone with re-parameterization techniques and an enhanced feature pyramid network to achieve a favorable balance between accuracy and computational efficiency for real-time clinical applications. We further establish benchmark performance on the TFM segmentation subset by comparing TF-Net against several state-of-the-art medical image segmentation models. Furthermore, we design TF-Collab, a novel integrated real-time pipeline that synergistically leverages models trained on all three tasks of the TFM dataset. By sequentially orchestrating frame classification for BUT determination, pupil region localization for input standardization, and TFBU segmentation, TF-Collab fully automates the analysis. Experimental results demonstrate the effectiveness of the proposed TF-Net and TF-Collab, providing a foundation for future research in ocular surface diagnostics. Our code and the TFM datasets are available at <a target="_blank" rel="noopener" href="https://github.com/glory-wan/TF-Net">https://github.com/glory-wan/TF-Net</a> </p>
<blockquote>
<p>泪膜破裂分析（TFBU）对于干眼综合征的诊断至关重要，但由于缺乏标注数据集和集成解决方案，自动化TFBU分割仍然是一个挑战。本文介绍了泪膜多任务（TFM）数据集，这是用于多任务泪膜分析的首个综合数据集，包含15个高清视频（共6247帧），标注了三个视觉任务：帧级别分类（清晰、闭合、破裂、模糊）、普拉萨多环检测和像素级TFBU区域分割。利用该数据集，我们首次提出了TF-Net，这是一种新颖且高效的分割基线模型。TF-Net采用MobileOne-mini骨干网与重新参数化技术，并增强特征金字塔网络，以实现准确性和计算效率的平衡，适用于实时临床应用。我们进一步通过将TF-Net与几种最先进的医学图像分割模型进行比较，在TFM分割子集上建立了基准性能。此外，我们设计了TF-Collab，这是一个新颖的一体化实时管道，它协同利用在TFM数据集所有三个任务上训练的模型。通过按顺序协调帧分类以确定BUT、定位瞳孔区域以实现输入标准化和TFBU分割，TF-Collab实现了分析的全自动化。实验结果表明TF-Net和TF-Collab的有效性，为眼表诊断的未来研究提供了基础。我们的代码和TFM数据集可在<a target="_blank" rel="noopener" href="https://github.com/glory-wan/TF-Net%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/glory-wan/TF-Net上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05615v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对干眼综合征诊断中的泪膜破裂分析的重要性，并指出自动化TFBU分割面临的挑战。为解决这些问题，本文引入了Tear Film Multi-task（TFM）数据集，这是首个用于多任务泪膜分析的综合数据集。此外，还提出了TF-Net，一种高效且准确的分割模型，能够在实时临床应用中实现准确性和计算效率之间的良好平衡。最后，建立了一个集成实时管道TF-Collab，该管道能够协同利用在TFM数据集所有任务上训练的模型，全自动进行泪膜分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了泪膜破裂分析在干眼综合征诊断中的重要性。</li>
<li>自动化TFBU分割面临缺乏标注数据集和综合解决方案的挑战。</li>
<li>引入了Tear Film Multi-task（TFM）数据集，包含用于多任务泪膜分析的高分辨率视频。</li>
<li>提出了TF-Net，一种用于实时临床应用的高效且准确的分割模型。</li>
<li>TF-Net采用MobileOne-mini作为骨干网，并通过参数化技术和增强的特征金字塔网络实现准确性和计算效率的平衡。</li>
<li>建立了一个集成实时管道TF-Collab，该管道能全自动进行泪膜分析，包括帧分类、瞳孔区域定位和TFBU分割。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05615">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bf0f1a08a0a614e893aa691595c10d04~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102890&auth_key=1760102890-0-0-500d432bf094055dfd74c4745cfa1070&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45f18c3a2836b05e44aed922f31236bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102897&auth_key=1760102897-0-0-8acc686f5de8fcdc429bf2cf12cc1f1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b60f84ef2d07c0c0724c6055654c7847~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102905&auth_key=1760102905-0-0-294cc8301111df30f3e3d7c804d1ae83&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b920b779ce1c14250dc3e6a9bc0229b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102911&auth_key=1760102911-0-0-b95e0fb251d35d61e8b0040b02a49469&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c8b505a2f5b3a59bb2983809f628f3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102918&auth_key=1760102918-0-0-fabafa81c5e7917a8e573628b3c66848&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b08565542fdbe064d72ddf3b4de8999~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102925&auth_key=1760102925-0-0-5687af9d4f8b94e005bb7f2049366a2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Advances-in-Medical-Image-Segmentation-A-Comprehensive-Survey-with-a-Focus-on-Lumbar-Spine-Applications"><a href="#Advances-in-Medical-Image-Segmentation-A-Comprehensive-Survey-with-a-Focus-on-Lumbar-Spine-Applications" class="headerlink" title="Advances in Medical Image Segmentation: A Comprehensive Survey with a   Focus on Lumbar Spine Applications"></a>Advances in Medical Image Segmentation: A Comprehensive Survey with a   Focus on Lumbar Spine Applications</h2><p><strong>Authors:Ahmed Kabil, Ghada Khoriba, Mina Yousef, Essam A. Rashed</strong></p>
<p>Medical Image Segmentation (MIS) stands as a cornerstone in medical image analysis, playing a pivotal role in precise diagnostics, treatment planning, and monitoring of various medical conditions. This paper presents a comprehensive and systematic survey of MIS methodologies, bridging the gap between traditional image processing techniques and modern deep learning approaches. The survey encompasses thresholding, edge detection, region-based segmentation, clustering algorithms, and model-based techniques while also delving into state-of-the-art deep learning architectures such as Convolutional Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely adopted U-Net and its variants. Moreover, integrating attention mechanisms, semi-supervised learning, generative adversarial networks (GANs), and Transformer-based models is thoroughly explored. In addition to covering established methods, this survey highlights emerging trends, including hybrid architectures, cross-modality learning, federated and distributed learning frameworks, and active learning strategies, which aim to address challenges such as limited labeled datasets, computational complexity, and model generalizability across diverse imaging modalities. Furthermore, a specialized case study on lumbar spine segmentation is presented, offering insights into the challenges and advancements in this relatively underexplored anatomical region. Despite significant progress in the field, critical challenges persist, including dataset bias, domain adaptation, interpretability of deep learning models, and integration into real-world clinical workflows. </p>
<blockquote>
<p>医学影像分割（MIS）是医学影像分析中的基石技术，在精确诊断、治疗规划和各种医疗状况监测中发挥着关键作用。本文对MIS方法进行全面系统的综述，弥合了传统图像处理技术与现代深度学习技术之间的差距。综述内容包括阈值分割、边缘检测、基于区域的分割、聚类算法和基于模型的技巧，同时深入探讨了最新的深度学习架构，如卷积神经网络（CNN）、全卷积网络（FCN）以及广泛采用的U-Net及其变体。此外，本文还深入探讨了集成注意力机制、半监督学习、生成对抗网络（GANs）和基于Transformer的模型。除了涵盖已建立的方法外，本文还强调了新兴趋势，包括混合架构、跨模态学习、联邦和分布式学习框架以及主动学习策略，旨在解决诸如有限标记数据集、计算复杂度和模型在不同成像方式下的泛化能力挑战。此外，还对腰椎分割进行了专项案例研究，深入探讨了这一相对未被充分研究的解剖区域的挑战和进展。尽管该领域取得了重大进展，但仍存在关键挑战，包括数据集偏见、域适应、深度学习模型的解释性和融入现实临床工作流程等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03318v1">PDF</a> Computers in Biology and Medicine (to appear)</p>
<p><strong>Summary</strong></p>
<p>本文全面系统地综述了医学图像分割（MIS）的方法，涵盖了从传统图像处理技术到现代深度学习方法的过渡。文章介绍了包括阈值分割、边缘检测、基于区域的分割、聚类算法和模型基础技术等在内的分割方法，并深入探讨了卷积神经网络（CNNs）、全卷积网络（FCNs）以及广泛采用的U-Net及其变体等最新深度学习架构。此外，文章还探讨了集成注意力机制、半监督学习、生成对抗网络（GANs）和基于Transformer的模型等趋势。文章还通过腰椎分割的案例研究，展示了该领域的挑战和进展。尽管有所进展，但仍存在数据集偏见、域适应、深度学习模型的可解释性和与真实临床工作流程的融合等挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>医学图像分割（MIS）是医学图像分析的核心，对精确诊断、治疗规划和各种医疗状况的监督至关重要。</li>
<li>本文综述了MIS的传统图像处理方法，如阈值分割、边缘检测等，以及现代深度学习方法，如卷积神经网络等。</li>
<li>文章还探讨了集成注意力机制、半监督学习、生成对抗网络等趋势，并介绍了新兴趋势，如混合架构、跨模态学习等。</li>
<li>腰椎分割的案例研究展示了该领域的挑战和进展。</li>
<li>尽管MIS有所进展，但仍面临数据集偏见、域适应、模型可解释性和与临床工作流程的融合等挑战。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03318">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e4cd82510eee36698d242dc5b7c6866c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102932&auth_key=1760102932-0-0-2975962aa3d551d0697c7f6d1044f811&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f673e2cb32b8859abdcc041e10beb31b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102940&auth_key=1760102940-0-0-d6e23796b4d762e585d8aa28f1658123&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER"><a href="#Semantic-Similarity-in-Radiology-Reports-via-LLMs-and-NER" class="headerlink" title="Semantic Similarity in Radiology Reports via LLMs and NER"></a>Semantic Similarity in Radiology Reports via LLMs and NER</h2><p><strong>Authors:Beth Pearson, Ahmed Adnan, Zahraa S. Abdallah</strong></p>
<p>Radiology report evaluation is a crucial part of radiologists’ training and plays a key role in ensuring diagnostic accuracy. As part of the standard reporting workflow, a junior radiologist typically prepares a preliminary report, which is then reviewed and edited by a senior radiologist to produce the final report. Identifying semantic differences between preliminary and final reports is essential for junior doctors, both as a training tool and to help uncover gaps in clinical knowledge. While AI in radiology is a rapidly growing field, the application of large language models (LLMs) remains challenging due to the need for specialised domain knowledge. In this paper, we explore the ability of LLMs to provide explainable and accurate comparisons of reports in the radiology domain. We begin by comparing the performance of several LLMs in comparing radiology reports. We then assess a more traditional approach based on Named-Entity-Recognition (NER). However, both approaches exhibit limitations in delivering accurate feedback on semantic similarity. To address this, we propose Llama-EntScore, a semantic similarity scoring method using a combination of Llama 3.1 and NER with tunable weights to emphasise or de-emphasise specific types of differences. Our approach generates a quantitative similarity score for tracking progress and also gives an interpretation of the score that aims to offer valuable guidance in reviewing and refining their reporting. We find our method achieves 67% exact-match accuracy and 93% accuracy within +&#x2F;- 1 when compared to radiologist-provided ground truth scores - outperforming both LLMs and NER used independently. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports">https://github.com/otmive/llama_reports</a> </p>
<blockquote>
<p>放射学报告评估是放射科医生培训的重要组成部分，对于确保诊断准确性起着关键作用。作为标准报告工作流的一部分，初级放射科医生通常会编写初步报告，然后由高级放射科医生审查并编辑以产生最终报告。识别初步报告和最终报告之间的语义差异对初级医生至关重要，既作为培训工具，也有助于发现临床知识方面的差距。虽然人工智能在放射学领域是一个快速发展的领域，但由于需要专业的领域知识，大型语言模型（LLM）的应用仍然具有挑战性。在本文中，我们探讨了大型语言模型在放射学报告比较方面的解释性和准确性能力。我们首先比较了多个大型语言模型在比较放射学报告方面的性能。然后评估了基于命名实体识别（NER）的更传统的方法。然而，这两种方法在提供语义相似性准确反馈方面都表现出局限性。为了解决这一问题，我们提出了Llama-EntScore方法，这是一种使用Llama 3.1和NER的组合的语义相似性评分方法，通过调整权重来强调或淡化特定类型的差异。我们的方法生成一个定量相似性分数来跟踪进度，并给出一个分数解释，旨在提供有价值的指导来审查和修正报告。我们发现，与放射科医生提供的真实分数相比，我们的方法达到了67%的精确匹配准确率和93%的+&#x2F;- 1准确率，超过了单独使用的大型语言模型和NER的性能。相关代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/otmive/llama_reports">https://github.com/otmive/llama_reports</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03102v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）在放射学报告比较中的应用，并提出了一种新的语义相似性评分方法Llama-EntScore。该方法结合了Llama 3.1和命名实体识别（NER），可以生成定量相似性评分并解释评分，旨在提供有价值的指导，帮助医生评估和精进报告质量。实验结果显示，该方法在放射学报告评估中的准确率高于单纯使用LLMs和NER。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>放射学报告评估是放射科医生培训的关键环节，有助于确保诊断准确性。</li>
<li>初步报告与最终报告之间的语义差异分析对初级医生具有训练价值，有助于发现临床知识上的不足。</li>
<li>大型语言模型（LLMs）在放射学领域的应用正逐渐受到关注，但存在专业领域知识需求的挑战。</li>
<li>单一的LLMs和命名实体识别（NER）方法在评估报告相似性方面存在局限性。</li>
<li>提出了Llama-EntScore方法，结合了Llama 3.1和NER，并可通过调整权重来强调或淡化特定类型的差异。</li>
<li>Llama-EntScore方法实现了较高的评估准确率，与放射科医生提供的真实评分相比，精确匹配准确率为67%，在±1范围内的准确率为93%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c9e68be5e328595700d63064858cc2b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102948&auth_key=1760102948-0-0-2e3fab942e49a8e2835e121ec0e13030&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis"><a href="#Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis" class="headerlink" title="Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis"></a>Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis</h2><p><strong>Authors:Ruilang Wang, Shuotong Xu, Bowen Liu, Runlin Huang, Donglong Chen, Weifeng Su</strong></p>
<p>The scarcity of annotated data in specialized domains such as medical imaging presents significant challenges to training robust vision models. While self-supervised masked image modeling (MIM) offers a promising solution, existing approaches largely rely on random high-ratio masking, leading to inefficiency and poor semantic alignment. Moreover, region-aware variants typically depend on reconstruction heuristics or supervised signals, limiting their adaptability across tasks and modalities. We propose Mask What Matters, a controllable text-guided masking framework for self-supervised medical image analysis. By leveraging vision-language models for prompt-based region localization, our method flexibly applies differentiated masking to emphasize diagnostically relevant regions while reducing redundancy in background areas. This controllable design enables better semantic alignment, improved representation learning, and stronger cross-task generalizability. Comprehensive evaluation across multiple medical imaging modalities, including brain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently outperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1 percentage points in classification accuracy, +1.3 in box average precision (BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it achieves these improvements with substantially lower overall masking ratios (e.g., 40% vs. 70%). This work demonstrates that controllable, text-driven masking can enable semantically aligned self-supervised learning, advancing the development of robust vision models for medical image analysis. </p>
<blockquote>
<p>在医学成像等专业领域，标注数据的稀缺性给训练稳健的视觉模型带来了巨大的挑战。虽然自监督的掩码图像建模（MIM）提供了很有前途的解决方案，但现有的方法大多依赖于随机的高比例掩码，导致效率低下和语义对齐不佳。此外，区域感知变体通常依赖于重建启发式或监督信号，这限制了它们在任务和模态之间的适应性。我们提出了“Mask What Matters”（掩藏关键信息），这是一种用于自监督医学图像分析的可控文本引导式掩码框架。通过利用基于视觉语言模型的提示进行区域定位，我们的方法可以灵活地应用差异化掩码，以强调与诊断相关的区域，同时减少背景区域的冗余信息。这种可控的设计实现了更好的语义对齐、改进了表征学习，并增强了跨任务的泛化能力。在包括脑部MRI、胸部CT和肺部X射线等多种医学影像模态上的全面评估表明，“Mask What Matters”始终优于现有的MIM方法（例如SparK），在分类准确度上提高了高达+3.1个百分点，框平均精度（BoxAP）提高了+1.3，掩膜平均精度（MaskAP）在检测方面提高了+1.1。值得注意的是，它在实现这些改进的同时，整体掩码比例大大降低（例如，40%对70%）。这项工作表明，可控的、文本驱动的掩码可以实现对齐语义的自监督学习，推动医学图像分析领域稳健视觉模型的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23054v2">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像领域面临标注数据稀缺的挑战，自监督掩膜图像建模（MIM）是解决该问题的一种有前景的方法。然而，现有方法大多依赖随机高比例掩膜，导致效率低下和语义对齐不佳。本文提出Mask What Matters，一种可控文本引导掩膜框架，用于自监督医学图像分析。该方法通过利用视觉语言模型进行基于提示的区域定位，灵活应用差异化掩膜，强调诊断相关区域，减少背景区域的冗余。该可控设计实现了更好的语义对齐、改进了表示学习和更强的跨任务泛化能力。在多种医学成像模态上的综合评估表明，Mask What Matters在分类精度、框平均精度和掩膜平均精度方面均优于现有MIM方法，且实现这些改进的同时使用更低的总体掩膜比率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像领域面临标注数据稀缺的挑战。</li>
<li>自监督掩膜图像建模（MIM）是解决该问题的一种有前景的方法，但现有方法存在随机高比例掩膜导致的效率低下和语义对齐不佳的问题。</li>
<li>Mask What Matters是一种可控文本引导掩膜框架，用于自监督医学图像分析。</li>
<li>该方法利用视觉语言模型进行基于提示的区域定位，实现差异化掩膜，强调诊断相关区域。</li>
<li>Mask What Matters实现了更好的语义对齐、改进了表示学习和更强的跨任务泛化能力。</li>
<li>在多种医学成像模态上的综合评估表明，Mask What Matters优于现有MIM方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23054">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bb7d1fad51aa5e24ffda5efaa8b1987e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102956&auth_key=1760102956-0-0-ef8b4a73ceeaabe4db45946eaca130ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a9b77f37528ad3b0dbd3a78e5592431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102964&auth_key=1760102964-0-0-567119c3c695ea1d7b0beb33f941eb19&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe1061d4ddbf133c81b10db1428f0a39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102972&auth_key=1760102972-0-0-13237b9780a2eec8d55f21770674bbc5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b9a818f2a1ca134a250753278bd4fc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102979&auth_key=1760102979-0-0-e111a7c6db390a7b5f9ddae1c760c459&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EMedNeXt-An-Enhanced-Brain-Tumor-Segmentation-Framework-for-Sub-Saharan-Africa-using-MedNeXt-V2-with-Deep-Supervision"><a href="#EMedNeXt-An-Enhanced-Brain-Tumor-Segmentation-Framework-for-Sub-Saharan-Africa-using-MedNeXt-V2-with-Deep-Supervision" class="headerlink" title="EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan   Africa using MedNeXt V2 with Deep Supervision"></a>EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan   Africa using MedNeXt V2 with Deep Supervision</h2><p><strong>Authors:Ahmed Jaheen, Abdelrahman Elsayed, Damir Kim, Daniil Tikhonov, Matheus Scatolin, Mohor Banerjee, Qiankun Ji, Mostafa Salem, Hu Wang, Sarim Hashmi, Mohammad Yaqub</strong></p>
<p>Brain cancer affects millions worldwide, and in nearly every clinical setting, doctors rely on magnetic resonance imaging (MRI) to diagnose and monitor gliomas. However, the current standard for tumor quantification through manual segmentation of multi-parametric MRI is time-consuming, requires expert radiologists, and is often infeasible in under-resourced healthcare systems. This problem is especially pronounced in low-income regions, where MRI scanners are of lower quality and radiology expertise is scarce, leading to incorrect segmentation and quantification. In addition, the number of acquired MRI scans in Africa is typically small. To address these challenges, the BraTS-Lighthouse 2025 Challenge focuses on robust tumor segmentation in sub-Saharan Africa (SSA), where resource constraints and image quality degradation introduce significant shifts. In this study, we present EMedNeXt – an enhanced brain tumor segmentation framework based on MedNeXt V2 with deep supervision and optimized post-processing pipelines tailored for SSA. EMedNeXt introduces three key contributions: a larger region of interest, an improved nnU-Net v2-based architectural skeleton, and a robust model ensembling system. Evaluated on the hidden validation set, our solution achieved an average LesionWise DSC of 0.897 with an average LesionWise NSD of 0.541 and 0.84 at a tolerance of 0.5 mm and 1.0 mm, respectively. </p>
<blockquote>
<p>脑癌影响全球数百万人，在几乎所有的临床环境中，医生都依赖磁共振成像（MRI）来诊断和监测胶质瘤。然而，通过多参数MRI进行肿瘤量化的手动分割是目前的标准，这一过程既耗时又需要专业放射科医生，且在资源不足的卫生保健系统中通常不可行。这一问题在低收入地区尤为突出，那里的MRI扫描仪质量较低且缺乏放射学专家，导致分割和量化不准确。此外，非洲获得的MRI扫描次数通常较少。为了应对这些挑战，BraTS-Lighthouse 2025挑战赛的重点是撒哈拉以南非洲（SSA）的稳健肿瘤分割，那里的资源约束和图像质量退化引入了重大变化。在这项研究中，我们提出了EMedNeXt——一个基于MedNeXt V2的增强型脑肿瘤分割框架，具有深度监督和针对SSA优化的后处理管道。EMedNeXt有三个主要贡献：更大的感兴趣区域、改进的nnU-Net v2基础架构和稳健的模型集成系统。在隐藏验证集上评估，我们的解决方案平均LesionWise DSC达到0.897，平均LesionWise NSD为0.541和0.84，容忍度分别为0.5毫米和1.0毫米。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23256v2">PDF</a> Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)</p>
<p><strong>Summary</strong><br>     针对非洲地区医疗资源匮乏、MRI扫描质量较差的问题，EMedNeXt框架被提出以解决脑胶质瘤分割的挑战。该框架优化了肿瘤分割流程，引入了更大感兴趣区域、改进型nnU-Net v2架构和稳健模型集成系统，并在隐藏验证集上取得了良好的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前的脑癌诊断主要依赖MRI，但在资源不足的卫生系统中进行肿瘤量化分割仍然面临挑战。特别是在低收入和医疗资源匮乏的地区，如非洲。</li>
<li>EMedNeXt框架是为了解决在撒哈拉以南非洲地区（SSA）进行稳健的肿瘤分割问题而开发的。这一地区面临着资源约束和图像质量下降的挑战。</li>
<li>EMedNeXt框架基于MedNeXt V2进行了增强设计，引入了深度监督和优化后的后处理管道。</li>
<li>该框架有三个关键贡献：更大的感兴趣区域、改进的nnU-Net v2架构和稳健的模型集成系统。</li>
<li>在隐藏验证集上进行的评估显示，EMedNeXt的解决方案在LesionWise DSC上取得了平均0.897的成绩，LesionWise NSD的平均成绩为0.541和0.84（容忍度为0.5毫米和1毫米）。这表明该框架在肿瘤分割方面具有良好的性能。</li>
<li>该研究强调了开发适用于资源受限地区的医疗技术的必要性，并展示了如何利用现代机器学习技术来改善全球公共卫生状况。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-28af5e7fe2cbc39eb4bd00206c895873~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102986&auth_key=1760102986-0-0-7482cbbf39dd2b67f978b90f7e9293b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca8811a6cc12935992263eb4eeff0a8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102995&auth_key=1760102995-0-0-c7d813cd2cdc561f2c7959562d8c3455&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf377d2ed4e195e8ff779c92d967cfba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103001&auth_key=1760103001-0-0-bdd07d61a2a5eaebfb2fd81f44250f81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ae128f5648daa610889e8ac3d0ea4314~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103009&auth_key=1760103009-0-0-c5bb4fdfa8e6b5b57ee13221816941c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7287f66981914f6072452faeb537131~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103016&auth_key=1760103016-0-0-73c7014e616f8abd42efaf6a8371e488&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc984c7a0532b7be8dfa071e32958b8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760103024&auth_key=1760103024-0-0-efe02f2dbbc462007fa3b7981ef1bfff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c981097f21d942ca619f24ad33241a35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760083705&auth_key=1760083705-0-0-09ffbb7b8bcb24f1514393a2288a555f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-10-10  Making Machines Sound Sarcastic LLM-Enhanced and Retrieval-Guided   Sarcastic Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-5c1dac7ca7b84dc25e84f11de04be385~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047566&auth_key=1760047566-0-0-7b2f1a428a78e17daf3a2cf76315d024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-10  StyleKeeper Prevent Content Leakage using Negative Visual Query   Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
