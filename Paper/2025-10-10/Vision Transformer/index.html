<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-01f14e51fb37b42c86f429d230077784~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039898&auth_key=1760039898-0-0-0676ac706508e704f6ac42180d7399ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-10-æ›´æ–°"><a href="#2025-10-10-æ›´æ–°" class="headerlink" title="2025-10-10 æ›´æ–°"></a>2025-10-10 æ›´æ–°</h1><h2 id="Evaluating-Fundus-Specific-Foundation-Models-for-Diabetic-Macular-Edema-Detection"><a href="#Evaluating-Fundus-Specific-Foundation-Models-for-Diabetic-Macular-Edema-Detection" class="headerlink" title="Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema   Detection"></a>Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema   Detection</h2><p><strong>Authors:Franco Javier Arellano, JosÃ© Ignacio Orlando</strong></p>
<p>Diabetic Macular Edema (DME) is a leading cause of vision loss among patients with Diabetic Retinopathy (DR). While deep learning has shown promising results for automatically detecting this condition from fundus images, its application remains challenging due the limited availability of annotated data. Foundation Models (FM) have emerged as an alternative solution. However, it is unclear if they can cope with DME detection in particular. In this paper, we systematically compare different FM and standard transfer learning approaches for this task. Specifically, we compare the two most popular FM for retinal imagesâ€“RETFound and FLAIRâ€“and an EfficientNet-B0 backbone, across different training regimes and evaluation settings in IDRiD, MESSIDOR-2 and OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do not consistently outperform fine-tuned CNNs in this task. In particular, an EfficientNet-B0 ranked first or second in terms of area under the ROC and precision&#x2F;recall curves in most evaluation settings, with RETFound only showing promising results in OEFI. FLAIR, on the other hand, demonstrated competitive zero-shot performance, achieving notable AUC-PR scores when prompted appropriately. These findings reveal that FM might not be a good tool for fine-grained ophthalmic tasks such as DME detection even after fine-tuning, suggesting that lightweight CNNs remain strong baselines in data-scarce environments. </p>
<blockquote>
<p>ç³–å°¿ç—…æ€§é»„æ–‘æ°´è‚¿ï¼ˆDMEï¼‰æ˜¯ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆDRï¼‰æ‚£è€…è§†åŠ›ä¸§å¤±çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨æ£€æµ‹è¿™ç§ç–¾ç—…ä»çœ¼åº•å›¾åƒä¸­æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ç”±äºæ ‡æ³¨æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼Œå…¶åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰å·²ç»å‡ºç°ä½œä¸ºä¸€ç§æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦èƒ½åº”å¯¹DMEæ£€æµ‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ç”¨äºæ­¤ä»»åŠ¡çš„ä¸åŒFMå’Œæ ‡å‡†è¿ç§»å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ç”¨äºè§†ç½‘è†œå›¾åƒçš„ä¸¤ä¸ªæœ€æµè¡Œçš„åŸºç¡€æ¨¡å‹RETFoundå’ŒFLAIRä»¥åŠEfficientNet-B0éª¨å¹²ç½‘ï¼Œåœ¨IDRiDã€MESSIDOR-2å’ŒOCT-and-Eye-Fundus-Imagesï¼ˆOEFIï¼‰çš„ä¸åŒè®­ç»ƒæ–¹æ¡ˆå’Œè¯„ä¼°è®¾ç½®ä¸­è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡è§„æ¨¡åºå¤§ï¼ŒFMåœ¨æ­¤ä»»åŠ¡ä¸Šå¹¶ä¸å§‹ç»ˆä¼˜äºå¾®è°ƒåçš„CNNã€‚ç‰¹åˆ«æ˜¯EfficientNet-B0åœ¨ROCæ›²çº¿å’Œç²¾ç¡®åº¦&#x2F;å¬å›ç‡æ›²çº¿ä¸‹çš„åŒºåŸŸæ–¹é¢æ’åé å‰æˆ–ç¬¬äºŒï¼Œåœ¨å¤§å¤šæ•°è¯„ä¼°è®¾ç½®ä¸­ï¼ŒRETFoundä»…åœ¨OEFIä¸Šæ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœã€‚å¦ä¸€æ–¹é¢ï¼ŒFLAIRè¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œåœ¨é€‚å½“æç¤ºæ—¶è·å¾—äº†æ˜¾è‘—çš„AUC-PRåˆ†æ•°ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå³ä½¿åœ¨å¾®è°ƒåï¼ŒFMå¯èƒ½ä¸æ˜¯ç”¨äºç²¾ç»†çš„çœ¼ç§‘ä»»åŠ¡ï¼ˆå¦‚DMEæ£€æµ‹ï¼‰çš„å¥½å·¥å…·ï¼Œè¿™è¡¨æ˜åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­ï¼Œè½»é‡çº§çš„CNNä»ç„¶æ˜¯å¾ˆå¼ºçš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07277v1">PDF</a> Accepted for publication at SIPAIM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¯¹æ¯”ç ”ç©¶äº†Foundation Modelsï¼ˆFMï¼‰ä¸æ ‡å‡†è¿ç§»å­¦ä¹ åœ¨ç³–å°¿ç—…é»„æ–‘æ°´è‚¿ï¼ˆDMEï¼‰æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚å¯¹æ¯”äº†RETFoundå’ŒFLAIRä¸¤ç§è§†ç½‘è†œå›¾åƒå¸¸ç”¨çš„FMåŠEfficientNet-B0éª¨æ¶ç½‘ç»œï¼Œåœ¨ä¸åŒè®­ç»ƒæ¨¡å¼å’Œè¯„ä¼°è®¾ç½®ä¸‹çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡FMè§„æ¨¡è¾ƒå¤§ï¼Œä½†åœ¨è¯¥ä»»åŠ¡ä¸Šå¹¶æœªæŒç»­è¶…è¶Šç²¾ç»†è°ƒæ•´çš„CNNã€‚EfficientNet-B0åœ¨å¤šæ•°è¯„ä¼°è®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒRETFoundä»…åœ¨OEFIä¸­è¡¨ç°è¾ƒå¥½ã€‚è€ŒFLAIRåœ¨é€‚å½“æç¤ºä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚è¿™äº›å‘ç°æ˜¾ç¤ºï¼Œå¯¹äºç²¾ç»†çœ¼ç§‘ä»»åŠ¡å¦‚DMEæ£€æµ‹ï¼ŒFMå¯èƒ½å¹¶ä¸é€‚ç”¨ï¼Œå³ä½¿åœ¨å¾®è°ƒåï¼Œè½»é‡çº§CNNåœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­ä»æ˜¯å¼ºæœ‰åŠ›çš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FMåœ¨DMEæ£€æµ‹ä¸Šçš„è¡¨ç°å¹¶ä¸æŒç»­ä¼˜äºç²¾ç»†è°ƒæ•´çš„CNNã€‚</li>
<li>EfficientNet-B0åœ¨å¤šæ•°è¯„ä¼°è®¾ç½®ä¸­çš„è¡¨ç°æ’åå‰åˆ—ã€‚</li>
<li>RETFoundä»…åœ¨OEFIæ•°æ®é›†ä¸­è¡¨ç°å‡ºè¾ƒå¥½æ€§èƒ½ã€‚</li>
<li>FLAIRåœ¨é€‚å½“æç¤ºä¸‹å±•ç°å‡ºé›¶æ ·æœ¬æ€§èƒ½çš„ç«äº‰åŠ›ã€‚</li>
<li>FMåœ¨ç²¾ç»†çœ¼ç§‘ä»»åŠ¡ä¸Šçš„é€‚ç”¨æ€§å—é™ï¼Œå³ä½¿ç»è¿‡å¾®è°ƒã€‚</li>
<li>å¯¹äºæ•°æ®ç¨€ç¼ºç¯å¢ƒï¼Œè½»é‡çº§CNNä»æ˜¯å¼ºæœ‰åŠ›çš„åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9af6ecdfff80538e784f913d1f05f632~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039765&auth_key=1760039765-0-0-2e72a794fe88ec3b7753134b7859ac43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f9bff88fe7b259ff56efe4c09cb2923~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039772&auth_key=1760039772-0-0-cb0b96a41911fb46545b8d9cc8b7e98f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e56e81846985f0015bcf4d179d27a4d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039779&auth_key=1760039779-0-0-a0858510849e1b643d471995fff0b278&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Continual-Learning-for-Image-Captioning-through-Improved-Image-Text-Alignment"><a href="#Continual-Learning-for-Image-Captioning-through-Improved-Image-Text-Alignment" class="headerlink" title="Continual Learning for Image Captioning through Improved Image-Text   Alignment"></a>Continual Learning for Image Captioning through Improved Image-Text   Alignment</h2><p><strong>Authors:Bertram Taetz, Gal Bordelius</strong></p>
<p>Generating accurate and coherent image captions in a continual learning setting remains a major challenge due to catastrophic forgetting and the difficulty of aligning evolving visual concepts with language over time. In this work, we propose a novel multi-loss framework for continual image captioning that integrates semantic guidance through prompt-based continual learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone, our approach combines standard cross-entropy loss with three additional components: (1) a prompt-based cosine similarity loss that aligns image embeddings with synthetically constructed prompts encoding objects, attributes, and actions; (2) a CLIP-style loss that promotes alignment between image embeddings and target caption embedding; and (3) a language-guided contrastive loss that employs a triplet loss to enhance class-level discriminability between tasks. Notably, our approach introduces no additional overhead at inference time and requires no prompts during caption generation. We find that this approach mitigates catastrophic forgetting, while achieving better semantic caption alignment compared to state-of-the-art methods. The code can be found via the following link <a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a> Gepardius&#x2F;Taetz_Bordelius_Continual_ImageCaptioning. </p>
<blockquote>
<p>åœ¨æŒç»­å­¦ä¹ ç¯å¢ƒä¸‹ç”Ÿæˆå‡†ç¡®è¿è´¯çš„å›¾åƒæ ‡é¢˜ä»ç„¶å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¾éš¾æ€§é—å¿˜ä»¥åŠéšç€æ—¶é—´æ¨ç§»å°†ä¸æ–­æ¼”å˜çš„è§†è§‰æ¦‚å¿µä¸è¯­è¨€è¿›è¡Œå¯¹é½çš„å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºæŒç»­å›¾åƒæ ‡é¢˜ç”Ÿæˆæå‡ºäº†ä¸€ç§æ–°å‹å¤šæŸå¤±æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŸºäºæç¤ºçš„è¿ç»­å­¦ä¹ å’Œå¯¹æ¯”å¯¹é½æ¥é›†æˆè¯­ä¹‰æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨é¢„è®­ç»ƒçš„ViT-GPT-2ä¸»å¹²ç½‘ç»œä¸Šï¼Œå°†æ ‡å‡†äº¤å‰ç†µæŸå¤±ä¸å¦å¤–ä¸‰ä¸ªç»„ä»¶ç›¸ç»“åˆï¼šï¼ˆ1ï¼‰åŸºäºæç¤ºçš„ä½™å¼¦ç›¸ä¼¼æ€§æŸå¤±ï¼Œè¯¥æŸå¤±å°†å›¾åƒåµŒå…¥ä¸ç¼–ç å¯¹è±¡ã€å±æ€§å’ŒåŠ¨ä½œçš„åˆæˆæç¤ºè¿›è¡Œå¯¹é½ï¼›ï¼ˆ2ï¼‰CLIPé£æ ¼çš„æŸå¤±ï¼Œè¯¥æŸå¤±ä¿ƒè¿›å›¾åƒåµŒå…¥ä¸ç›®æ ‡æ ‡é¢˜åµŒå…¥ä¹‹é—´çš„å¯¹é½ï¼›ï¼ˆ3ï¼‰è¯­è¨€å¼•å¯¼å¯¹æ¯”æŸå¤±ï¼Œè¯¥æŸå¤±é‡‡ç”¨ä¸‰å…ƒç»„æŸå¤±æ¥æé«˜ä»»åŠ¡é—´çš„ç±»çº§è¾¨åˆ«èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†æ—¶ä¸ä¼šå¼•å…¥ä»»ä½•é¢å¤–å¼€é”€ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆæ ‡é¢˜æ—¶ä¸éœ€è¦ä»»ä½•æç¤ºã€‚æˆ‘ä»¬å‘ç°è¿™ç§æ–¹æ³•å‡è½»äº†ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å®ç°äº†æ›´å¥½çš„è¯­ä¹‰æ ‡é¢˜å¯¹é½ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gepardius/Taetz_Bordelius_Continual_ImageCaptioning%E3%80%82">https://github.com/Gepardius/Taetz_Bordelius_Continual_ImageCaptioningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06009v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æŒç»­å­¦ä¹ ç¯å¢ƒä¸‹å›¾åƒå­—å¹•ç”Ÿæˆçš„æ–°å¤šæŸå¤±æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŸºäºæç¤ºçš„è¿ç»­å­¦ä¹ å’Œå¯¹æ¯”å¯¹é½æ¥æä¾›è¯­ä¹‰æŒ‡å¯¼ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨é¢„è®­ç»ƒçš„ViT-GPT-2ä¸»å¹²ä¸Šï¼Œç»“åˆäº†æ ‡å‡†äº¤å‰ç†µæŸå¤±ä¸ä¸‰ç§é¢å¤–çš„æŸå¤±ç»„ä»¶ï¼ŒåŒ…æ‹¬åŸºäºæç¤ºçš„ä½™å¼¦ç›¸ä¼¼æ€§æŸå¤±ã€CLIPé£æ ¼çš„æŸå¤±ä»¥åŠè¯­è¨€æŒ‡å¯¼çš„å¯¹æ¯”æŸå¤±ã€‚è¯¥æ–¹æ³•åœ¨å‡è½»ç¾éš¾æ€§é—å¿˜çš„åŒæ—¶ï¼Œå®ç°äº†æ›´å¥½çš„è¯­ä¹‰å­—å¹•å¯¹é½ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæŸå¤±æ¡†æ¶ï¼Œç”¨äºæŒç»­å­¦ä¹ ç¯å¢ƒä¸‹çš„å›¾åƒå­—å¹•ç”Ÿæˆã€‚</li>
<li>æ¡†æ¶é›†æˆäº†åŸºäºæç¤ºçš„è¿ç»­å­¦ä¹ å’Œå¯¹æ¯”å¯¹é½ï¼Œä»¥æä¾›è¯­ä¹‰æŒ‡å¯¼ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„ViT-GPT-2ä½œä¸ºä¸»å¹²ï¼Œå¹¶ç»“åˆå¤šç§æŸå¤±ç»„ä»¶ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å¼•å…¥åŸºäºæç¤ºçš„ä½™å¼¦ç›¸ä¼¼æ€§æŸå¤±ï¼Œå°†å›¾åƒåµŒå…¥ä¸åˆæˆæç¤ºè¿›è¡Œå¯¹é½ã€‚</li>
<li>é‡‡ç”¨CLIPé£æ ¼çš„æŸå¤±ï¼Œä¿ƒè¿›å›¾åƒåµŒå…¥ä¸ç›®æ ‡å­—å¹•åµŒå…¥ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>ä½¿ç”¨è¯­è¨€æŒ‡å¯¼çš„å¯¹æ¯”æŸå¤±ï¼Œé‡‡ç”¨ä¸‰å…ƒç»„æŸå¤±å¢å¼ºä»»åŠ¡é—´çš„ç±»çº§é‰´åˆ«èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c52c49827e7feb742390381182ba3a52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039786&auth_key=1760039786-0-0-e853c2dabafbd654bd9a8f554ac7636c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30d22b088d89919172320210f85086e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039793&auth_key=1760039793-0-0-35b6e291d0e1b7af1a3462b332c006ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7741028a6b8871af5c2fc6b14aeb7136~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039800&auth_key=1760039800-0-0-dea45384aeeb7e95c179a30320c32bd6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Leveraging-Vision-Transformers-for-Enhanced-Classification-of-Emotions-using-ECG-Signals"><a href="#Leveraging-Vision-Transformers-for-Enhanced-Classification-of-Emotions-using-ECG-Signals" class="headerlink" title="Leveraging Vision Transformers for Enhanced Classification of Emotions   using ECG Signals"></a>Leveraging Vision Transformers for Enhanced Classification of Emotions   using ECG Signals</h2><p><strong>Authors:Pubudu L. Indrasiri, Bipasha Kashyap, Pubudu N. Pathirana</strong></p>
<p>Biomedical signals provide insights into various conditions affecting the human body. Beyond diagnostic capabilities, these signals offer a deeper understanding of how specific organs respond to an individualâ€™s emotions and feelings. For instance, ECG data can reveal changes in heart rate variability linked to emotional arousal, stress levels, and autonomic nervous system activity. This data offers a window into the physiological basis of our emotional states. Recent advancements in the field diverge from conventional approaches by leveraging the power of advanced transformer architectures, which surpass traditional machine learning and deep learning methods. We begin by assessing the effectiveness of the Vision Transformer (ViT), a forefront model in image classification, for identifying emotions in imaged ECGs. Following this, we present and evaluate an improved version of ViT, integrating both CNN and SE blocks, aiming to bolster performance on imaged ECGs associated with emotion detection. Our method unfolds in two critical phases: first, we apply advanced preprocessing techniques for signal purification and converting signals into interpretable images using continuous wavelet transform and power spectral density analysis; second, we unveil a performance-boosted vision transformer architecture, cleverly enhanced with convolutional neural network components, to adeptly tackle the challenges of emotion recognition. Our methodologyâ€™s robustness and innovation were thoroughly tested using ECG data from the YAAD and DREAMER datasets, leading to remarkable outcomes. For the YAAD dataset, our approach outperformed existing state-of-the-art methods in classifying seven unique emotional states, as well as in valence and arousal classification. Similarly, in the DREAMER dataset, our method excelled in distinguishing between valence, arousal and dominance, surpassing current leading techniques. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦ä¿¡å·ä¸ºå½±å“äººä½“çš„å„ç§çŠ¶å†µæä¾›äº†æ·±å…¥è§è§£ã€‚é™¤äº†è¯Šæ–­èƒ½åŠ›ä¹‹å¤–ï¼Œè¿™äº›ä¿¡å·è¿˜æä¾›äº†å…³äºç‰¹å®šå™¨å®˜å¦‚ä½•å“åº”ä¸ªäººæƒ…ç»ªå’Œæ„Ÿå—çš„æ›´æ·±å±‚æ¬¡ç†è§£ã€‚ä¾‹å¦‚ï¼Œå¿ƒç”µå›¾æ•°æ®å¯ä»¥æ­ç¤ºä¸æƒ…ç»ªæ¿€å‘ã€å‹åŠ›æ°´å¹³å’Œè‡ªä¸»ç¥ç»ç³»ç»Ÿæ´»åŠ¨ç›¸å…³çš„å¿ƒç‡å˜å¼‚æ€§å˜åŒ–ã€‚è¿™äº›æ•°æ®ä¸ºæˆ‘ä»¬æä¾›äº†æƒ…ç»ªçŠ¶æ€çš„ç”Ÿç†åŸºç¡€çš„ä¸€ä¸ªçª—å£ã€‚è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„å˜å‹å™¨æ¶æ„çš„åŠ›é‡æ¥åç¦»ä¼ ç»Ÿæ–¹æ³•ï¼Œè¿™äº›å…ˆè¿›çš„å˜å‹å™¨æ¶æ„è¶…è¶Šäº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆè¯„ä¼°Vision Transformerï¼ˆViTï¼‰åœ¨å¿ƒç”µå›¾å›¾åƒä¸­çš„æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å±•ç¤ºå¹¶è¯„ä¼°ViTçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬ç»“åˆäº†CNNå’ŒSEå—ï¼Œæ—¨åœ¨æé«˜ä¸æƒ…æ„Ÿæ£€æµ‹ç›¸å…³çš„å¿ƒç”µå›¾å›¾åƒçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åº”ç”¨å…ˆè¿›çš„é¢„å¤„ç†æŠ€æœ¯æ¥å‡€åŒ–ä¿¡å·ï¼Œå¹¶ä½¿ç”¨è¿ç»­å°æ³¢å˜æ¢å’ŒåŠŸç‡è°±å¯†åº¦åˆ†æå°†ä¿¡å·è½¬æ¢ä¸ºå¯è§£é‡Šçš„å›¾åƒï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ€§èƒ½å¢å¼ºçš„è§†è§‰å˜å‹å™¨æ¶æ„ï¼Œå·§å¦™åœ°ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œç»„ä»¶ï¼Œä»¥æœ‰æ•ˆåº”å¯¹æƒ…æ„Ÿè¯†åˆ«çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ç¨³å¥æ€§å’Œåˆ›æ–°æ€§é€šè¿‡ä½¿ç”¨YAADå’ŒDREAMERæ•°æ®é›†çš„å¿ƒç”µå›¾æ•°æ®è¿›è¡Œäº†å½»åº•æµ‹è¯•ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚å¯¹äºYAADæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†ç±»ä¸ƒç§ç‹¬ç‰¹æƒ…ç»ªçŠ¶æ€ä»¥åŠä»·å€¼å’Œå…´å¥‹åˆ†ç±»æ–¹é¢å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚åŒæ ·ï¼Œåœ¨DREAMERæ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒºåˆ†ä»·å€¼ã€å…´å¥‹å’Œæ”¯é…æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å½“å‰é¢†å…ˆçš„æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05826v1">PDF</a> 14pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿç‰©åŒ»å­¦ä¿¡å·åœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¿ƒç”µå›¾æ•°æ®åœ¨æ­ç¤ºæƒ…æ„Ÿå˜åŒ–æ–¹é¢çš„ä½œç”¨ã€‚æ–‡ç« è¿˜æ¢è®¨äº†åˆ©ç”¨å…ˆè¿›çš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«çš„æœ€æ–°è¿›å±•ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ”¹è¿›ç‰ˆçš„ViTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†CNNå’ŒSEæ¨¡å—ï¼Œä»¥æé«˜åœ¨æƒ…æ„Ÿæ£€æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚ç»è¿‡æµ‹è¯•ï¼Œè¯¥æ–¹æ³•åœ¨æƒ…æ„Ÿåˆ†ç±»æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿç‰©åŒ»å­¦ä¿¡å·æä¾›å…³äºäººä½“å„ç§çŠ¶å†µçš„è§è§£ï¼Œå¹¶å¯ä»¥æ·±å…¥äº†è§£ç‰¹å®šå™¨å®˜å¯¹ä¸ªä½“æƒ…æ„Ÿå’Œæ„Ÿå—çš„å“åº”ã€‚</li>
<li>ECGæ•°æ®èƒ½å¤Ÿæ­ç¤ºä¸æƒ…æ„Ÿæ¿€åŠ¨ã€å‹åŠ›æ°´å¹³å’Œè‡ªä¸»ç¥ç»ç³»ç»Ÿæ´»åŠ¨ç›¸å…³çš„å¿ƒç‡å˜åŒ–ã€‚</li>
<li>å…ˆè¿›çš„transformeræ¶æ„åœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå·²è¢«åº”ç”¨äºæƒ…ç»ªå¿ƒç”µå›¾çš„æƒ…æ„Ÿè¯†åˆ«ã€‚</li>
<li>æ”¹è¿›çš„ViTæ¨¡å‹ç»“åˆäº†CNNå’ŒSEæ¨¡å—ï¼Œä»¥æé«˜åœ¨æƒ…æ„Ÿæ£€æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šä¿¡å·å‡€åŒ–çš„é¢„å¤„ç†æŠ€æœ¯å’Œä½¿ç”¨è¿ç»­å°æ³¢å˜æ¢å’ŒåŠŸç‡è°±å¯†åº¦åˆ†æå°†ä¿¡å·è½¬æ¢ä¸ºå¯è§£é‡Šçš„å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•ç»è¿‡æµ‹è¯•ï¼Œåœ¨æƒ…æ„Ÿåˆ†ç±»æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨YAADå’ŒDREAMERæ•°æ®é›†ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-76e7130db210e8a6500d984bb45a5af2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039828&auth_key=1760039828-0-0-8abf0826318f947d83f35cea7ff0ec41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ecb27390931271316a33f9bdc97f76f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039835&auth_key=1760039835-0-0-71fcd104db8f20983995eef331279c0b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c019431b59892b68516c5544abe3a5da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039842&auth_key=1760039842-0-0-ff2f35d7aa761a80ce1347d02f08752c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c1e682c6c9b29819c2d7dd8b9843a599~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039849&auth_key=1760039849-0-0-8cb82878befdc22f9e99d1043e6ff553&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Neuroplastic-Modular-Framework-Cross-Domain-Image-Classification-of-Garbage-and-Industrial-Surfaces"><a href="#Neuroplastic-Modular-Framework-Cross-Domain-Image-Classification-of-Garbage-and-Industrial-Surfaces" class="headerlink" title="Neuroplastic Modular Framework: Cross-Domain Image Classification of   Garbage and Industrial Surfaces"></a>Neuroplastic Modular Framework: Cross-Domain Image Classification of   Garbage and Industrial Surfaces</h2><p><strong>Authors:Debojyoti Ghosh, Soumya K Ghosh, Adrijit Goswami</strong></p>
<p>Efficient and accurate classification of waste and industrial surface defects is essential for ensuring sustainable waste management and maintaining high standards in quality control. This paper introduces the Neuroplastic Modular Classifier, a novel hybrid architecture designed for robust and adaptive image classification in dynamic environments. The model combines a ResNet-50 backbone for localized feature extraction with a Vision Transformer (ViT) to capture global semantic context. Additionally, FAISS-based similarity retrieval is incorporated to provide a memory-like reference to previously encountered data, enriching the modelâ€™s feature space. A key innovation of our architecture is the neuroplastic modular design composed of expandable, learnable blocks that dynamically grow during training when performance plateaus. Inspired by biological learning systems, this mechanism allows the model to adapt to data complexity over time, improving generalization. Beyond garbage classification, we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2), which involves industrial defect detection on metal surfaces. Experimental results across domains show that the proposed architecture outperforms traditional static models in both accuracy and adaptability. The Neuroplastic Modular Classifier offers a scalable, high-performance solution for real-world image classification, with strong applicability in both environmental and industrial domains. </p>
<blockquote>
<p>æœ‰æ•ˆä¸”ç²¾ç¡®åœ°åˆ†ç±»åºŸç‰©å’Œå·¥ä¸šè¡¨é¢ç¼ºé™·å¯¹äºç¡®ä¿å¯æŒç»­çš„åºŸç‰©ç®¡ç†å’Œä¿æŒé«˜è´¨é‡çš„æ ‡å‡†è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–åˆ†ç±»å™¨ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºåŠ¨æ€ç¯å¢ƒä¸­çš„ç¨³å¥å’Œè‡ªé€‚åº”å›¾åƒåˆ†ç±»è®¾è®¡çš„æ–°å‹æ··åˆæ¶æ„ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ResNet-50ä¸»å¹²è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¥æ•è·å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼ŒåŸºäºFAISSçš„ç›¸ä¼¼æ€§æ£€ç´¢è¢«ç»“åˆï¼Œä»¥æä¾›å¯¹å…ˆå‰é‡åˆ°çš„æ•°æ®çš„è®°å¿†å‚è€ƒï¼Œä¸°å¯Œäº†æ¨¡å‹çš„ç‰¹å¾ç©ºé—´ã€‚æˆ‘ä»¬æ¶æ„çš„ä¸€ä¸ªå…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–è®¾è®¡ï¼Œå®ƒç”±å¯æ‰©å±•çš„å­¦ä¹ å—ç»„æˆï¼Œå½“æ€§èƒ½è¾¾åˆ°ç¨³å®šæ°´å¹³æ—¶ï¼Œè¿™äº›å—ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¢é•¿ã€‚å—ç”Ÿç‰©å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œè¿™ç§æœºåˆ¶å…è®¸æ¨¡å‹éšç€æ—¶é—´çš„æ¨ç§»é€‚åº”æ•°æ®å¤æ‚æ€§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚é™¤äº†åƒåœ¾åˆ†ç±»ä¹‹å¤–ï¼Œæˆ‘ä»¬åœ¨Kolektorè¡¨é¢ç¼ºé™·æ•°æ®é›†2ï¼ˆKolektorSDD2ï¼‰ä¸ŠéªŒè¯äº†æ¨¡å‹ï¼Œè¯¥æ•°æ®é›†æ¶‰åŠé‡‘å±è¡¨é¢ä¸Šçš„å·¥ä¸šç¼ºé™·æ£€æµ‹ã€‚è·¨é¢†åŸŸçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¶æ„åœ¨å‡†ç¡®æ€§å’Œé€‚åº”æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„é™æ€æ¨¡å‹ã€‚ç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–åˆ†ç±»å™¨ä¸ºç°å®ä¸–ç•Œçš„å›¾åƒåˆ†ç±»æä¾›äº†å¯æ‰©å±•ã€é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨ç¯å¢ƒå’Œå·¥ä¸šé¢†åŸŸéƒ½æœ‰å¾ˆå¼ºçš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05071v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–åˆ†ç±»å™¨çš„å…¨æ–°æ··åˆæ¶æ„ï¼Œè¯¥æ¶æ„æ—¨åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°ç¨³å¥å’Œè‡ªé€‚åº”çš„å›¾åƒåˆ†ç±»ã€‚å®ƒç»“åˆäº†ResNet-50éª¨å¹²ç½‘è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’ŒVision Transformerï¼ˆViTï¼‰æ•æ‰å…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†åŸºäºFAISSçš„ç›¸ä¼¼æ€§æ£€ç´¢ï¼Œä¸ºå·²é‡åˆ°çš„æ•°æ®æä¾›å†…å­˜å¼å‚è€ƒï¼Œä¸°å¯Œäº†æ¨¡å‹çš„ç‰¹å¾ç©ºé—´ã€‚å…¶å…³é”®åˆ›æ–°ç‚¹åœ¨äºç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–è®¾è®¡ï¼Œç”±å¯æ‰©å±•ã€å¯å­¦ä¹ çš„å—ç»„æˆï¼Œå½“æ€§èƒ½è¾¾åˆ°ç“¶é¢ˆæ—¶ï¼Œè¿™äº›å—ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¢é•¿ã€‚è¯¥æœºåˆ¶å—åˆ°ç”Ÿç‰©å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œå…è®¸æ¨¡å‹éšæ—¶é—´é€‚åº”æ•°æ®å¤æ‚æ€§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¶æ„ä¸ä»…é€‚ç”¨äºåƒåœ¾åˆ†ç±»ï¼Œè¿˜åœ¨é‡‘å±è¡¨é¢å·¥ä¸šç¼ºé™·æ£€æµ‹é¢†åŸŸçš„Kolektor Surface Defect Dataset 2æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¶æ„åœ¨å‡†ç¡®æ€§å’Œé€‚åº”æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿé™æ€æ¨¡å‹ã€‚ç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–åˆ†ç±»å™¨ä¸ºç°å®ä¸–ç•Œå›¾åƒåˆ†ç±»æä¾›äº†å¯æ‰©å±•ã€é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨ç¯å¢ƒå’Œå·¥ä¸šé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–åˆ†ç±»å™¨æ˜¯ä¸€ä¸ªä¸ºåŠ¨æ€ç¯å¢ƒè®¾è®¡çš„æ··åˆæ¶æ„ï¼Œç”¨äºç¨³å¥å’Œè‡ªé€‚åº”çš„å›¾åƒåˆ†ç±»ã€‚</li>
<li>ç»“åˆResNet-50éª¨å¹²ç½‘è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’ŒVision Transformerï¼ˆViTï¼‰è¿›è¡Œå…¨å±€è¯­ä¹‰ä¸Šä¸‹æ–‡æ•æ‰ã€‚</li>
<li>åŸºäºFAISSçš„ç›¸ä¼¼æ€§æ£€ç´¢ä¸ºå·²é‡åˆ°çš„æ•°æ®æä¾›å†…å­˜å¼å‚è€ƒï¼Œä¸°å¯Œæ¨¡å‹ç‰¹å¾ç©ºé—´ã€‚</li>
<li>ç¥ç»å¯å¡‘æ€§æ¨¡å—åŒ–è®¾è®¡çš„å…³é”®åˆ›æ–°ç‚¹åœ¨äºå…¶ç”±å¯æ‰©å±•ã€å¯å­¦ä¹ çš„å—ç»„æˆï¼Œå¯åŠ¨æ€å¢é•¿ä»¥åº”å¯¹æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>è¯¥æ¶æ„å—åˆ°ç”Ÿç‰©å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œèƒ½å¤Ÿé€‚åº”æ•°æ®å¤æ‚æ€§å˜åŒ–ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ä»…åœ¨åƒåœ¾åˆ†ç±»ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¿˜åœ¨å·¥ä¸šç¼ºé™·æ£€æµ‹é¢†åŸŸçš„Kolektor Surface Defect Dataset 2æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨å‡†ç¡®æ€§å’Œé€‚åº”æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿé™æ€æ¨¡å‹ï¼Œä¸ºç°å®ä¸–ç•Œå›¾åƒåˆ†ç±»æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-01f14e51fb37b42c86f429d230077784~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039898&auth_key=1760039898-0-0-0676ac706508e704f6ac42180d7399ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bd540e90e30d5434bed65ea7ac5a9e6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039905&auth_key=1760039905-0-0-86903e2a71e6f271f570ec99b667352c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ViTs-Teaching-Machines-to-See-Time-Series-Anomalies-Like-Human-Experts"><a href="#ViTs-Teaching-Machines-to-See-Time-Series-Anomalies-Like-Human-Experts" class="headerlink" title="ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts"></a>ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts</h2><p><strong>Authors:Zexin Wang, Changhua Pei, Yang Liu, Hengyue Jiang, Quan Zhou, Haotian Si, Hang Cui, Jianhui Li, Gaogang Xie, Jingjing Li, Dan Pei</strong></p>
<p>Web service administrators must ensure the stability of multiple systems by promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving the goal of â€œtrain once, infer across scenariosâ€ remains a fundamental challenge for time series anomaly detection models. Beyond improving zero-shot generalization, such models must also flexibly handle sequences of varying lengths during inference, ranging from one hour to one week, without retraining. Conventional approaches rely on sliding-window encoding and self-supervised learning, which restrict inference to fixed-length inputs. Large Language Models (LLMs) have demonstrated remarkable zero-shot capabilities across general domains. However, when applied to time series data, they face inherent limitations due to context length. To address this issue, we propose ViTs, a Vision-Language Model (VLM)-based framework that converts time series curves into visual representations. By rescaling time series images, temporal dependencies are preserved while maintaining a consistent input size, thereby enabling efficient processing of arbitrarily long sequences without context constraints. Training VLMs for this purpose introduces unique challenges, primarily due to the scarcity of aligned time series image-text data. To overcome this, we employ an evolutionary algorithm to automatically generate thousands of high-quality image-text pairs and design a three-stage training pipeline consisting of: (1) time series knowledge injection, (2) anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive experiments demonstrate that ViTs substantially enhance the ability of VLMs to understand and detect anomalies in time series data. All datasets and code will be publicly released at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/ViTs-C484/">https://anonymous.4open.science/r/ViTs-C484/</a>. </p>
<blockquote>
<p>ç½‘ç»œæœåŠ¡ç®¡ç†å‘˜å¿…é¡»é€šè¿‡åŠæ—¶æ£€æµ‹å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆKPIï¼‰ä¸­çš„å¼‚å¸¸æƒ…å†µï¼Œç¡®ä¿å¤šä¸ªç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚å®ç°â€œä¸€æ¬¡è®­ç»ƒï¼Œè·¨åœºæ™¯æ¨æ–­â€çš„ç›®æ ‡ä»ç„¶æ˜¯æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„åŸºæœ¬æŒ‘æˆ˜ã€‚é™¤äº†æé«˜é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å¤–ï¼Œæ­¤ç±»æ¨¡å‹åœ¨æ¨æ–­è¿‡ç¨‹ä¸­è¿˜å¿…é¡»çµæ´»å¤„ç†é•¿åº¦ä¸ä¸€çš„åºåˆ—ï¼Œä»ä¸€å°æ—¶åˆ°ä¸€å‘¨ä¸ç­‰ï¼Œè€Œæ— éœ€è¿›è¡Œå†è®­ç»ƒã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºæ»‘åŠ¨çª—å£ç¼–ç å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼Œå®ƒä»¬å°†æ¨æ–­é™åˆ¶åœ¨å›ºå®šé•¿åº¦çš„è¾“å…¥ä¸Šã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸€èˆ¬é¢†åŸŸè¡¨ç°å‡ºäº†å¼•äººæ³¨ç›®çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºæ—¶é—´åºåˆ—æ•°æ®æ—¶ï¼Œå®ƒä»¬ç”±äºä¸Šä¸‹æ–‡é•¿åº¦è€Œé¢ä¸´å›ºæœ‰å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ViTsï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¡†æ¶ï¼Œå°†æ—¶é—´åºåˆ—æ›²çº¿è½¬æ¢ä¸ºè§†è§‰è¡¨ç¤ºã€‚é€šè¿‡é‡æ–°ç¼©æ”¾æ—¶é—´åºåˆ—å›¾åƒï¼Œå¯ä»¥ä¿ç•™æ—¶é—´ä¾èµ–æ€§ï¼ŒåŒæ—¶ä¿æŒä¸€è‡´çš„è¾“å…¥å¤§å°ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ï¼Œä¸å—ä¸Šä¸‹æ–‡çº¦æŸã€‚ä¸ºæ­¤ç›®çš„è®­ç»ƒVLMå¼•å…¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå¯¹é½çš„æ—¶é—´åºåˆ—å›¾åƒæ–‡æœ¬æ•°æ®ç¨€ç¼ºã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é‡‡ç”¨è¿›åŒ–ç®—æ³•è‡ªåŠ¨ç”Ÿæˆæ•°åƒå¯¹é«˜è´¨é‡å›¾åƒæ–‡æœ¬ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé˜¶æ®µçš„è®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æ—¶é—´åºåˆ—çŸ¥è¯†æ³¨å…¥ï¼Œï¼ˆ2ï¼‰å¼‚å¸¸æ£€æµ‹å¢å¼ºï¼Œä»¥åŠï¼ˆ3ï¼‰å¼‚å¸¸æ¨ç†ç»†åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒViTsæ˜¾è‘—å¢å¼ºäº†VLMå¯¹æ—¶é—´åºåˆ—æ•°æ®ä¸­å¼‚å¸¸çš„ç†è§£å’Œæ£€æµ‹èƒ½åŠ›ã€‚æ‰€æœ‰æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/ViTs-C484/%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://anonymous.4open.science/r/ViTs-C484/å…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04710v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹é¢ä¸´å›ºå®šè¾“å…¥é•¿åº¦å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„åŒé‡æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¡†æ¶ViTsï¼Œå¯å°†æ—¶é—´åºåˆ—æ›²çº¿è½¬æ¢ä¸ºè§†è§‰è¡¨ç¤ºï¼Œå®ç°ä»»æ„é•¿åº¦åºåˆ—çš„é«˜æ•ˆå¤„ç†ã€‚é€šè¿‡ç¼©æ”¾æ—¶é—´åºåˆ—å›¾åƒï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒæ—¶é—´ä¾èµ–æ€§çš„åŒæ—¶ç»´æŒæ’å®šè¾“å…¥å¤§å°ï¼Œè§£å†³äº†è¯­å¢ƒé•¿åº¦é™åˆ¶çš„é—®é¢˜ã€‚ç ”ç©¶é‡‡ç”¨è¿›åŒ–ç®—æ³•ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“å®ç°æ—¶é—´åºåˆ—çŸ¥è¯†æ³¨å…¥ã€å¼‚å¸¸æ£€æµ‹å¢å¼ºå’Œå¼‚å¸¸æ¨ç†ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒViTsæ˜¾è‘—æé«˜äº†VLMåœ¨ç†è§£å’Œæ£€æµ‹æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç›‘æ§ç³»ç»Ÿæ€§èƒ½è¦æ±‚åŠæ—¶æ£€æµ‹å…³é”®ç»©æ•ˆæŒ‡æ ‡ï¼ˆKPIsï¼‰çš„å¼‚å¸¸æ³¢åŠ¨ï¼Œä»¥ç»´æŒç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚</li>
<li>å®ç°è·¨åœºæ™¯çš„æ¨¡å‹åº”ç”¨ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…³äºæ—¶é—´ç³»åˆ—æ•°æ®çš„é›¶æ ·æœ¬æ³›åŒ–é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚æ»‘åŠ¨çª—å£ç¼–ç å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨å¤„ç†å›ºå®šé•¿åº¦è¾“å…¥æ–¹é¢æœ‰é™åˆ¶ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸€èˆ¬é¢†åŸŸå…·æœ‰å‡ºè‰²çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†æ—¶é—´åºåˆ—æ•°æ®æ—¶é¢ä¸´è¯­å¢ƒé•¿åº¦é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>ViTsæ¨¡å‹é€šè¿‡å°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºè§†è§‰è¡¨ç¤ºæ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ç°ä»»æ„é•¿åº¦åºåˆ—çš„é«˜æ•ˆå¤„ç†ã€‚</li>
<li>é€šè¿‡ç¼©æ”¾æ—¶é—´åºåˆ—å›¾åƒæ¥ä¿æŒæ—¶é—´ä¾èµ–æ€§å’Œæ’å®šè¾“å…¥å¤§å°ï¼Œè§£å†³äº†è¯­å¢ƒé•¿åº¦çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è¿›åŒ–ç®—æ³•ç”Ÿæˆå›¾åƒæ–‡æœ¬å¯¹ä»¥å…‹æœæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“æé«˜æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6ccff1c9d8dcdaedb1000d132ec8c1e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039912&auth_key=1760039912-0-0-02a98f0835348b40d1f9c3bf6954192f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cccc7f1b4fadfe65a228e90877aa3e48~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039920&auth_key=1760039920-0-0-c09786b47048339f849e917a5719da5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed106e22d34d9effe704edc48126aa0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039926&auth_key=1760039926-0-0-961bbbdeeb65fe63495e9ea3dd658164&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4fed8fbb9b34f1b08b41f4052e6d0e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039933&auth_key=1760039933-0-0-a1960b30794e9ffeeca5d9eafad0ddbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-44cf3a805a48d9d7f984b8a64bf1d56f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039939&auth_key=1760039939-0-0-2adb3b37b2fb419b96e0546e8fda60e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04812cbfe5bb1a341bba86cf17bdeca3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039946&auth_key=1760039946-0-0-1bbf7bb517c68fc025ec8cdbccfcbb75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-290599dd99e431cb5c90aaf2101cead5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039952&auth_key=1760039952-0-0-93d2253ba9e89687db772ef24cd8516a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63ce7773711da598a78d30d25cf75203~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039958&auth_key=1760039958-0-0-08d2de6f93831585830150ce7279b73c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AgentTypo-Adaptive-Typographic-Prompt-Injection-Attacks-against-Black-box-Multimodal-Agents"><a href="#AgentTypo-Adaptive-Typographic-Prompt-Injection-Attacks-against-Black-box-Multimodal-Agents" class="headerlink" title="AgentTypo: Adaptive Typographic Prompt Injection Attacks against   Black-box Multimodal Agents"></a>AgentTypo: Adaptive Typographic Prompt Injection Attacks against   Black-box Multimodal Agents</h2><p><strong>Authors:Yanjie Li, Yiming Cao, Dong Wang, Bin Xiao</strong></p>
<p>Multimodal agents built on large vision-language models (LVLMs) are increasingly deployed in open-world settings but remain highly vulnerable to prompt injection, especially through visual inputs. We introduce AgentTypo, a black-box red-teaming framework that mounts adaptive typographic prompt injection by embedding optimized text into webpage images. Our automatic typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction by substituting captioners while minimizing human detectability via a stealth loss, with a Tree-structured Parzen Estimator guiding black-box optimization over text placement, size, and color. To further enhance attack strength, we develop AgentTypo-pro, a multi-LLM system that iteratively refines injection prompts using evaluation feedback and retrieves successful past examples for continual learning. Effective prompts are abstracted into generalizable strategies and stored in a strategy repository, enabling progressive knowledge accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark across Classifieds, Shopping, and Reddit scenarios show that AgentTypo significantly outperforms the latest image-based attacks such as AgentAttack. On GPT-4o agents, our image-only attack raises the success rate from 0.23 to 0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also outperforming the latest baselines. Our findings reveal that AgentTypo poses a practical and potent threat to multimodal agents and highlight the urgent need for effective defense. </p>
<blockquote>
<p>åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€ä»£ç†åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„éƒ¨ç½²ï¼Œä½†ä»ç„¶å¾ˆå®¹æ˜“å—åˆ°æç¤ºæ³¨å…¥çš„å¨èƒï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è§†è§‰è¾“å…¥ã€‚æˆ‘ä»¬å¼•å…¥äº†AgentTypoï¼Œè¿™æ˜¯ä¸€ä¸ªé»‘ç®±çº¢é˜Ÿæ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥ç½‘é¡µå›¾åƒä¸­çš„ä¼˜åŒ–æ–‡æœ¬æ¥å®ç°è‡ªé€‚åº”çš„æ’ç‰ˆæç¤ºæ³¨å…¥ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨æ’ç‰ˆæç¤ºæ³¨å…¥ï¼ˆATPIï¼‰ç®—æ³•é€šè¿‡æ›¿ä»£å­—å¹•æ¥æœ€å¤§åŒ–æç¤ºé‡å»ºï¼ŒåŒæ—¶é€šè¿‡éšå½¢æŸå¤±æ¥æœ€å°åŒ–äººç±»æ£€æµ‹èƒ½åŠ›ï¼Œæ ‘ç»“æ„Parzenä¼°è®¡å™¨æŒ‡å¯¼é»‘ç®±ä¼˜åŒ–æ–‡æœ¬æ”¾ç½®ã€å¤§å°å’Œé¢œè‰²ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ”»å‡»å¼ºåº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†AgentTypo-proï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šLLMç³»ç»Ÿï¼Œå¯ä»¥è¿­ä»£ä¼˜åŒ–æ³¨å…¥æç¤ºï¼Œåˆ©ç”¨è¯„ä¼°åé¦ˆæ£€ç´¢æˆåŠŸçš„è¿‡å»ç¤ºä¾‹è¿›è¡ŒæŒç»­å­¦ä¹ ã€‚æœ‰æ•ˆçš„æç¤ºè¢«æŠ½è±¡ä¸ºå¯æ¨å¹¿çš„ç­–ç•¥ï¼Œå¹¶å­˜å‚¨åœ¨ç­–ç•¥åº“ä¸­ï¼Œä»¥ä¾¿åœ¨æœªæ¥çš„æ”»å‡»ä¸­è¿›è¡Œæ¸è¿›çš„çŸ¥è¯†ç§¯ç´¯å’Œé‡ç”¨ã€‚åœ¨VWA-AdvåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè·¨åˆ†ç±»ã€è´­ç‰©å’ŒRedditåœºæ™¯çš„AgentTypoæ˜¾è‘—ä¼˜äºæœ€æ–°çš„åŸºäºå›¾åƒçš„æ”»å‡»ï¼Œå¦‚AgentAttackã€‚åœ¨GPT-4oä»£ç†ä¸Šï¼Œæˆ‘ä»¬çš„çº¯å›¾åƒæ”»å‡»å°†æˆåŠŸç‡ä»0.23æé«˜åˆ°0.45ï¼Œåœ¨GPT-4Vã€GPT-4o-miniã€Gemini 1.5 Proå’ŒClaude 3 Opusä¸Šç»“æœä¸€è‡´ã€‚åœ¨å›¾åƒ+æ–‡æœ¬è®¾ç½®ä¸­ï¼ŒAgentTypoè¾¾åˆ°äº†0.68çš„æ”»å‡»æˆåŠŸç‡ï¼Œä¹Ÿè¶…è¿‡äº†æœ€æ–°çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒAgentTypoå¯¹å¤šæ¨¡æ€ä»£ç†æ„æˆäº†å®é™…ä¸”å¼ºå¤§çš„å¨èƒï¼Œå¹¶å¼ºè°ƒäº†å¯¹æœ‰æ•ˆé˜²å¾¡çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04257v1">PDF</a> 13 pages, 8 figures. Submitted to IEEE Transactions on Information   Forensics &amp; Security</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ä»£ç†çš„å®‰å…¨å¨èƒã€‚é’ˆå¯¹è¿™ä¸€å¨èƒï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åä¸ºAgentTypoçš„é»‘ç®±çº¢é˜Ÿæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç½‘é¡µå›¾åƒä¸­çš„ä¼˜åŒ–æ–‡æœ¬åµŒå…¥æ¥å®ç°è‡ªé€‚åº”çš„æ’ç‰ˆæç¤ºæ³¨å…¥ã€‚AgentTypoåŒ…æ‹¬è‡ªåŠ¨æ’ç‰ˆæç¤ºæ³¨å…¥ç®—æ³•å’ŒAgentTypo-proå¤šLLMç³»ç»Ÿï¼Œå‰è€…é€šè¿‡æœ€å¤§åŒ–æç¤ºé‡å»ºå¹¶é€šè¿‡éšè”½æŸå¤±æœ€å°åŒ–äººç±»å¯æ£€æµ‹æ€§æ¥å®ç°æ³¨å…¥ï¼Œåè€…åˆ™é€šè¿‡è¯„ä¼°åé¦ˆå’Œæ£€ç´¢æˆåŠŸçš„è¿‡å»ç¤ºä¾‹æ¥è¿›è¡Œè¿­ä»£ä¼˜åŒ–æ³¨å…¥æç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒAgentTypoæ˜¾è‘—ä¼˜äºæœ€æ–°çš„å›¾åƒæ”»å‡»ï¼Œå¦‚AgentAttackï¼Œå¹¶åœ¨GPT-4oä»£ç†ä¸ŠæˆåŠŸæé«˜äº†æ”»å‡»æˆåŠŸç‡ã€‚ç ”ç©¶æ­ç¤ºäº†AgentTypoå¯¹å¤šæ¨¡æ€ä»£ç†çš„å®é™…å¨èƒå’Œæ½œåœ¨é£é™©ï¼Œå¹¶å¼ºè°ƒäº†é˜²å¾¡çš„ç´§è¿«æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä»£ç†åœ¨å¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­çš„éƒ¨ç½²æ—¥ç›Šæ™®éï¼Œä½†é¢ä¸´é€šè¿‡è§†è§‰è¾“å…¥è¿›è¡Œæç¤ºæ³¨å…¥çš„é«˜åº¦è„†å¼±æ€§ã€‚</li>
<li>AgentTypoæ˜¯ä¸€ç§é»‘ç®±çº¢é˜Ÿæ¡†æ¶ï¼Œå¯é€šè¿‡ç½‘é¡µå›¾åƒä¸­çš„ä¼˜åŒ–æ–‡æœ¬åµŒå…¥å®ç°è‡ªé€‚åº”æ’ç‰ˆæç¤ºæ³¨å…¥ã€‚</li>
<li>AgentTypoåŒ…æ‹¬è‡ªåŠ¨æ’ç‰ˆæç¤ºæ³¨å…¥ç®—æ³•å’ŒAgentTypo-proå¤šLLMç³»ç»Ÿï¼Œå‰è€…åˆ©ç”¨æ ‘ç»“æ„Parzenä¼°è®¡å™¨æŒ‡å¯¼æ–‡æœ¬æ”¾ç½®ã€å¤§å°å’Œé¢œè‰²çš„é»‘ç®±ä¼˜åŒ–ã€‚</li>
<li>AgentTypoæ˜¾è‘—ä¼˜äºç°æœ‰çš„å›¾åƒæ”»å‡»æ–¹æ³•ï¼Œå¦‚AgentAttackï¼Œå¹¶èƒ½æœ‰æ•ˆæé«˜GPT-4oä»£ç†çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>åœ¨å›¾åƒ+æ–‡æœ¬è®¾ç½®ä¸­ï¼ŒAgentTypoå®ç°é«˜æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>AgentTypoæ­ç¤ºäº†å¯¹å¤šæ¨¡æ€ä»£ç†çš„å®é™…å¨èƒå’Œæ½œåœ¨é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-63bf458dedae5a5619d7af66f6eb2189~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039967&auth_key=1760039967-0-0-8707547c60e1625cb0ff62197920fae3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b28134042b8928a31417147f5aadf9c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039975&auth_key=1760039975-0-0-30da5ebdf87737ca81844190d6c7f81c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa0ba42220b6f2a2620fe132307e7685~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039982&auth_key=1760039982-0-0-c28c22343ded88a515602651956d9857&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-045dd00e7b9ac616a018a3c2229b3705~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039989&auth_key=1760039989-0-0-217be75ef507e19aa47c0703741e9fe4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04339d864fbba326ceff8603e3db573e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039996&auth_key=1760039996-0-0-bba7a28e23e0a2de8012285e7cc1e272&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UGround-Towards-Unified-Visual-Grounding-with-Unrolled-Transformers"><a href="#UGround-Towards-Unified-Visual-Grounding-with-Unrolled-Transformers" class="headerlink" title="UGround: Towards Unified Visual Grounding with Unrolled Transformers"></a>UGround: Towards Unified Visual Grounding with Unrolled Transformers</h2><p><strong>Authors:Rui Qian, Xin Yin, Chuanhang Deng, Zhiyuan Peng, Jian Xiong, Wei Zhai, Dejing Dou</strong></p>
<p>We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm that dynamically selects intermediate layers across \textbf{U}nrolled transformers as <code>mask as prompt&#39;&#39;, diverging from the prevailing pipeline that leverages the fixed last hidden layer as </code>\texttt{<SEG>} as promptâ€™â€™. UGround addresses two primary challenges posed by the prevailing paradigm: (1) its reliance on the fixed last hidden layer, which sequentially amplifies cumulative errors arising from layer-by-layer propagation without intermediate correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly projects textual embeddings into visual space without explicit spatial cues (\eg, coordinates). Central to UGround is Policy-Prompted Masking, which comprises two key components: Stochastic Skip Connection (SSC) and Mask as Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer layers, enabling dynamic layer selection at which it connects to the vision model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer, MasP uses the similarity map derived from the \texttt{<SEG>} token and image tokens as a soft logit mask to prompt SAM for mask generation, offering explicit spatial cues through its activation regions. To validate the effectiveness of UGround, we, for the first time, have unified visual grounding within a single framework from an attribute perspective, spanning from traditional refer expression segmentation to newly proposed reasoning segmentation, single-target to multi-target, positive query to false premise (empty target). All codes and models are publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/rui-qian/UGround%7D%7Bhttps://github.com/rui-qian/UGround%7D">https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†UGroundï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰å®šä½èŒƒå¼ï¼Œå®ƒåŠ¨æ€é€‰æ‹©å±•å¼€çš„è½¬æ¢å™¨ä¸­çš„ä¸­é—´å±‚ä½œä¸ºâ€œæ©ç æç¤ºâ€ï¼Œè¿™ä¸æµè¡Œçš„åˆ©ç”¨å›ºå®šæœ€åä¸€ä¸ªéšè—å±‚ä½œä¸ºâ€œåˆ†å‰²ç¬¦æç¤ºâ€çš„æµæ°´çº¿æ–¹æ³•ä¸åŒã€‚UGroundè§£å†³äº†ç°æœ‰èŒƒå¼æ‰€é¢ä¸´çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š(1)å®ƒå¯¹å›ºå®šæœ€åä¸€ä¸ªéšè—å±‚çš„ä¾èµ–ï¼Œè¯¥éšè—å±‚ä¼šé€å±‚ä¼ æ’­ç´¯ç§¯è¯¯å·®ï¼Œè€Œæ²¡æœ‰ä¸­é—´ä¿®æ­£ï¼Œå¯¼è‡´è¯¯å·®é¡ºåºæ”¾å¤§ï¼›(2)å®ƒä½¿ç”¨åˆ†å‰²ç¬¦ä½œä¸ºæç¤ºï¼Œè¿™ä¼šå°†æ–‡æœ¬åµŒå…¥éšå¼åœ°æŠ•å½±åˆ°è§†è§‰ç©ºé—´ï¼Œè€Œæ²¡æœ‰æ˜ç¡®çš„ç©ºé—´çº¿ç´¢ï¼ˆä¾‹å¦‚åæ ‡ï¼‰ã€‚UGroundçš„æ ¸å¿ƒæ˜¯ç­–ç•¥æç¤ºæ©ç ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šéšæœºè·³è·ƒè¿æ¥ï¼ˆSSCï¼‰å’Œæ©ç æç¤ºï¼ˆMasPï¼‰ã€‚SSCæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡éšæœºé‡‡æ ·å…è®¸æ¯ä¸ªåˆ†å‰²ç¬¦æ ‡è®°åœ¨å±•å¼€çš„è½¬æ¢å™¨å±‚ä¹‹é—´æ»‘åŠ¨ï¼Œå®ç°åœ¨å“ªä¸ªå±‚è¿æ¥åˆ°è§†è§‰æ¨¡å‹ï¼ˆä¾‹å¦‚SAMï¼‰çš„è·³è·ƒè¿æ¥ã€‚ç»™å®šæ‰€é€‰çš„éšè—å±‚ï¼ŒMasPä½¿ç”¨ä»åˆ†å‰²ç¬¦æ ‡è®°å’Œå›¾åƒæ ‡è®°æ´¾ç”Ÿçš„ç›¸ä¼¼åº¦å›¾ä½œä¸ºè½¯é€»è¾‘æ©ç æ¥æç¤ºSAMè¿›è¡Œæ©ç ç”Ÿæˆï¼Œé€šè¿‡å…¶æ¿€æ´»åŒºåŸŸæä¾›æ˜ç¡®çš„ç©ºé—´çº¿ç´¢ã€‚ä¸ºäº†éªŒè¯UGroundçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨å•ä¸€æ¡†æ¶å†…ä»å±æ€§è§’åº¦ç»Ÿä¸€äº†è§†è§‰å®šä½ä»»åŠ¡ï¼Œæ¶µç›–äº†ä»ä¼ ç»ŸæŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²åˆ°æœ€æ–°æå‡ºçš„æ¨ç†åˆ†å‰²ã€å•ç›®æ ‡åˆ°å¤šç›®æ ‡ã€æ­£å‘æŸ¥è¯¢åˆ°é”™è¯¯å‰æï¼ˆç©ºç›®æ ‡ï¼‰ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rui-qian/UGround%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/rui-qian/UGroundä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03853v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/rui-qian/UGround">https://github.com/rui-qian/UGround</a></p>
<p><strong>Summary</strong><br>UGroundæ˜¯ä¸€ç§ç»Ÿä¸€è§†è§‰å®šä½èŒƒå¼ï¼Œå®ƒé€šè¿‡åŠ¨æ€é€‰æ‹©å±•å¼€çš„å˜å‹å™¨ä¸­çš„ä¸­é—´å±‚ä½œä¸ºâ€œæ©è†œæç¤ºâ€ï¼Œè§£å†³äº†ç°æœ‰èŒƒå¼ä¸­å›ºå®šä½¿ç”¨æœ€åä¸€éšè—å±‚ä½œä¸ºæç¤ºä»¥åŠç”¨<code>&lt;SEG&gt;</code>ä½œä¸ºæç¤ºçš„ä¸¤å¤§æŒ‘æˆ˜ã€‚UGroundçš„å…³é”®åœ¨äºç­–ç•¥æç¤ºæ©è†œï¼ŒåŒ…æ‹¬éšæœºè·³è·ƒè¿æ¥å’Œæ©è†œæç¤ºä¸¤éƒ¨åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UGroundæå‡ºä¸€ç§ç»Ÿä¸€è§†è§‰å®šä½èŒƒå¼ï¼Œè§£å†³äº†ç°æœ‰èŒƒå¼ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚</li>
<li>UGroundé€šè¿‡åŠ¨æ€é€‰æ‹©å±•å¼€çš„å˜å‹å™¨ä¸­çš„ä¸­é—´å±‚ä½œä¸ºâ€œæ©è†œæç¤ºâ€ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>UGroundè§£å†³äº†å›ºå®šä½¿ç”¨æœ€åä¸€éšè—å±‚ä½œä¸ºæç¤ºæ‰€å¸¦æ¥çš„é—®é¢˜ï¼Œå¦‚è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>UGroundé€šè¿‡ç­–ç•¥æç¤ºæ©è†œï¼ˆåŒ…æ‹¬éšæœºè·³è·ƒè¿æ¥å’Œæ©è†œæç¤ºï¼‰å®ç°äº†æœ‰æ•ˆçš„ç©ºé—´æç¤ºã€‚</li>
<li>UGroundé¦–æ¬¡åœ¨ä¸€ä¸ªæ¡†æ¶å†…ç»Ÿä¸€è§†è§‰å®šä½ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿå‚è€ƒè¡¨è¾¾å¼åˆ†å‰²å’Œæ–°å…´æ¨ç†åˆ†å‰²ç­‰ã€‚</li>
<li>UGroundæ”¯æŒå•ç›®æ ‡åˆ°å¤šç›®æ ‡ï¼Œæ­£å‘æŸ¥è¯¢åˆ°é”™è¯¯å‰æï¼ˆç©ºç›®æ ‡ï¼‰ç­‰å¤šç§åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f9fbe06fd13e9d6e6d191735adb4b9b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040003&auth_key=1760040003-0-0-f7170aec478cde8ceb55a32f5addf306&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8931ed9ea221cc196b098f15baf9e0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040010&auth_key=1760040010-0-0-ad4609ba174ddbc6462d3961b2747d21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1225efac952e90b72d81d64a602c3f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040017&auth_key=1760040017-0-0-ff65128565dd4c7ea96d7900015f75ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b25b7a6f99813ec76b4f9023eefb92ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040023&auth_key=1760040023-0-0-799683e1ab13c7034aaf977841e51710&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da6a16b82438c803d53b385f248c57da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040030&auth_key=1760040030-0-0-faa50d8b3b5a27b74d9373d872d24190&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DuPLUS-Dual-Prompt-Vision-Language-Framework-for-Universal-Medical-Image-Segmentation-and-Prognosis"><a href="#DuPLUS-Dual-Prompt-Vision-Language-Framework-for-Universal-Medical-Image-Segmentation-and-Prognosis" class="headerlink" title="DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical   Image Segmentation and Prognosis"></a>DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical   Image Segmentation and Prognosis</h2><p><strong>Authors:Numan Saeed, Tausifa Jan Saleem, Fadillah Maani, Muhammad Ridzuan, Hu Wang, Mohammad Yaqub</strong></p>
<p>Deep learning for medical imaging is hampered by task-specific models that lack generalizability and prognostic capabilities, while existing â€˜universalâ€™ approaches suffer from simplistic conditioning and poor medical semantic understanding. To address these limitations, we introduce DuPLUS, a deep learning framework for efficient multi-modal medical image analysis. DuPLUS introduces a novel vision-language framework that leverages hierarchical semantic prompts for fine-grained control over the analysis task, a capability absent in prior universal models. To enable extensibility to other medical tasks, it includes a hierarchical, text-controlled architecture driven by a unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize across three imaging modalities, ten different anatomically various medical datasets, encompassing more than 30 organs and tumor types. It outperforms the state-of-the-art task specific and universal models on 8 out of 10 datasets. We demonstrate extensibility of its text-controlled architecture by seamless integration of electronic health record (EHR) data for prognosis prediction, and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI) of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks and modalities from varying centers, establishing DuPLUS as a versatile and clinically relevant solution for medical image analysis. The code for this work is made available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DuPLUS-6C52">https://anonymous.4open.science/r/DuPLUS-6C52</a> </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å½±åƒæ·±åº¦å­¦ä¹ çš„å±€é™æ€§ï¼Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ç¼ºä¹é€šç”¨æ€§å’Œé¢„åèƒ½åŠ›ï¼Œè€Œç°æœ‰çš„â€œé€šç”¨â€æ–¹æ³•åˆ™å­˜åœ¨ç®€å•çš„æ¡ä»¶è®¾å®šå’ŒåŒ»å­¦è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†DuPLUSï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆå¤šæ¨¡æ€åŒ»å­¦å½±åƒåˆ†æçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚DuPLUSå¼•å…¥äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åˆ†å±‚è¯­ä¹‰æç¤ºå¯¹åˆ†æä»»åŠ¡è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œè¿™æ˜¯å…ˆå‰é€šç”¨æ¨¡å‹æ‰€ä¸å…·å¤‡çš„åŠŸèƒ½ã€‚ä¸ºäº†ä½¿å…¶èƒ½å¤Ÿæ‰©å±•åˆ°å…¶ä»–åŒ»å­¦ä»»åŠ¡ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„åŒé‡æç¤ºæœºåˆ¶é©±åŠ¨çš„åˆ†å±‚æ–‡æœ¬æ§åˆ¶æ¶æ„ã€‚åœ¨åˆ†å‰²æ–¹é¢ï¼ŒDuPLUSèƒ½å¤Ÿåœ¨ä¸‰ç§æˆåƒæ¨¡å¼ã€åä¸ªä¸åŒçš„è§£å‰–ç»“æ„å„å¼‚çš„åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œæ¨å¹¿ï¼Œæ¶µç›–è¶…è¿‡30ä¸ªå™¨å®˜å’Œè‚¿ç˜¤ç±»å‹ã€‚å®ƒåœ¨8ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„ç‰¹å®šä»»åŠ¡å’Œé€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡æ— ç¼é›†æˆç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®æ¥è¿›è¡Œé¢„åé¢„æµ‹ï¼Œå±•ç¤ºäº†å…¶æ–‡æœ¬æ§åˆ¶æ¶æ„çš„æ‰©å±•æ€§ï¼Œåœ¨å¤´é¢ˆç™Œæ•°æ®é›†ä¸Šï¼ŒDuPLUSçš„å¥‘åˆæŒ‡æ•°ï¼ˆCIï¼‰è¾¾åˆ°äº†0.69ã€‚å‚æ•°é«˜æ•ˆçš„å¾®è°ƒä½¿å¾—å®ƒèƒ½å¤Ÿè¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡å’Œæ¨¡å¼ï¼Œä»å„ç§ä¸­å¿ƒè¿›è¡Œåº”ç”¨ï¼Œç¡®ç«‹äº†DuPLUSåœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„é€šç”¨æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚è¯¥å·¥ä½œçš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DuPLUS-6C52">åŒ¿åé“¾æ¥</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03483v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—å½±åƒåˆ†æçš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶DuPLUSã€‚è¯¥æ¡†æ¶é‡‡ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æŠ€æœ¯ï¼Œé€šè¿‡å±‚æ¬¡åŒ–çš„è¯­ä¹‰æç¤ºè¿›è¡Œç²¾ç»†çš„ä»»åŠ¡æ§åˆ¶ï¼Œå…‹æœäº†ç°æœ‰é€šç”¨æ¨¡å‹çš„å±€é™æ€§ã€‚DuPLUSå…·å¤‡è·¨å¤šæ¨¡æ€åŒ»ç–—å½±åƒåˆ†æçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šå®ç°è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶æˆåŠŸæ•´åˆç”µå­å¥åº·è®°å½•æ•°æ®ç”¨äºé¢„åé¢„æµ‹ã€‚å…¶å‚æ•°é«˜æ•ˆçš„å¾®è°ƒèƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡å’Œæ¨¡æ€ï¼Œæˆä¸ºä¸´åºŠç›¸å…³åŒ»ç–—å½±åƒåˆ†æçš„å¤šåŠŸèƒ½è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DuPLUSæ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»ç–—å½±åƒåˆ†æçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»»åŠ¡ç‰¹å®šæ¨¡å‹ç¼ºä¹é€šç”¨æ€§å’Œé¢„åèƒ½åŠ›çš„é—®é¢˜ã€‚</li>
<li>å®ƒé‡‡ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æŠ€æœ¯ï¼Œå¹¶å¼•å…¥å±‚æ¬¡åŒ–çš„è¯­ä¹‰æç¤ºï¼Œå®ç°äº†å¯¹åˆ†æä»»åŠ¡çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>DuPLUSå…·å¤‡åœ¨å¤šæ¨¡æ€åŒ»ç–—å½±åƒåˆ†æä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šå®ç°è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿæ— ç¼æ•´åˆç”µå­å¥åº·è®°å½•æ•°æ®ç”¨äºé¢„åé¢„æµ‹ã€‚</li>
<li>DuPLUSé€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒèƒ½åŠ›ï¼Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡å’Œæ¨¡æ€ã€‚</li>
<li>å®ƒåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°çš„ä»»åŠ¡ç‰¹å®šå’Œé€šç”¨æ¨¡å‹ã€‚</li>
<li>DuPLUSçš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f25244c1db868ea5c12acc3678de6a69~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040037&auth_key=1760040037-0-0-541a3a6d147ee2ff4e6088d9b69c0781&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d4bad778822430361e8f093aede1f941~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040044&auth_key=1760040044-0-0-779eb86abb95a62089daabb93b080188&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b5ad01b154fc6b721849fb3e6551a47b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040051&auth_key=1760040051-0-0-1a94c5631b94031dd91e8d3b8fb3fff2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a754fd34bc4c23ae30558af09b2a2273~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040058&auth_key=1760040058-0-0-066155e0d893ea3f331ebaa43d6ff9b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»ä»…æœ‰çš„å°‘é‡æ ‡è®°æ ·æœ¬ä¸­è¯†åˆ«å‡ºæ–°æ¦‚å¿µã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡èå…¥é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥æå‡æ”¯æ’‘ç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨å®é™…å®ä¾‹ä¸­çš„å®šä½ï¼Œå®ƒä»¬ä»ç„¶ä¼šå‡ºç°ä¸è§†è§‰è¯æ®ç›¸çŸ›ç›¾çš„å¹»è§‰è¯­ä¹‰ï¼Œå¯¼è‡´äº§ç”Ÿå˜ˆæ‚çš„æŒ‡å¯¼å’Œæ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†è§‰ä¸æ–‡æœ¬èåˆå°‘é‡å­¦ä¹ ï¼ˆVT-FSLï¼‰ï¼Œå®ƒé€šè¿‡ç²¾ç¡®è·¨æ¨¡æ€æç¤ºå’Œæ”¯æ’‘å›¾åƒï¼Œæ„å»ºåŸºäºLLMçš„è·¨æ¨¡æ€æ¡¥æ¢ã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚å…·ä½“æ¥è¯´ï¼ŒCIPä»¥ç±»åå’Œæ”¯æ’‘å›¾åƒä¸ºæ¡ä»¶ï¼Œåœ¨å•æ¬¡ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ç”Ÿæˆç²¾ç¡®çš„ç±»æè¿°ã€‚è¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´çš„åˆæˆå›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚è¿™äº›æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºè¡¥å……çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æ’‘æ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAé€šè¿‡æœ€å°åŒ–å®ƒä»¬æ‰€è·¨è¶Šçš„3ç»´å¹³è¡Œå››è¾¹å½¢çš„å†…æ ¸ä½“ç§¯ï¼Œè”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æ’‘å’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰äº†æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°äº†ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡æ€èåˆã€‚æ‰€æå‡ºçš„VT-FSLæ–¹æ³•åœ¨åŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç²¾ç»†ç²’åº¦å°‘é‡å­¦ä¹ åœºæ™¯åœ¨å†…çš„åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/peacelwh/VT-FSLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§è§£å†³å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰é—®é¢˜çš„æ–°å‹æ¡†æ¶â€”â€”VT-FSLã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒæ”¯æŒï¼Œé€šè¿‡ç²¾ç¡®è·¨æ¨¡æ€æç¤ºå’Œå‡ ä½•å¯¹é½ï¼Œæé«˜æ”¯æŒç‰¹å¾çš„è´¨é‡ã€‚å®ƒä¸»è¦åŒ…æ‹¬è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ã€‚CIPé€šè¿‡ç±»åå’Œå›¾åƒæ”¯æŒç”Ÿæˆç²¾ç¡®ç±»æè¿°ï¼Œè€ŒCGAåˆ™é€šè¿‡æœ€å°åŒ–æ ¸åŒ–ä½“ç§¯çš„ä¸‰ç»´å¹³è¡Œå¤šé¢ä½“æ¥å¯¹é½æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚VT-FSLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VT-FSLæ˜¯ä¸€ç§æ–°å‹çš„å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯†åˆ«æ–°æ¦‚å¿µçš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ¥æé«˜æ”¯æŒç‰¹å¾çš„è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶å¼•å…¥è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç²¾ç¡®ç±»æè¿°ï¼Œå¹¶é€šè¿‡åˆæˆå›¾åƒä¸°å¯Œè¯­ä¹‰ç†è§£ã€‚è¿™äº›æè¿°å’Œå›¾åƒåˆ†åˆ«æä¾›é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æŒæ•°æ®ã€‚</li>
<li>VT-FSLè¿˜åŒ…æ‹¬è·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ï¼Œé€šè¿‡æœ€å°åŒ–æ ¸åŒ–ä½“ç§¯çš„ä¸‰ç»´å¹³è¡Œå¤šé¢ä½“æ¥å¯¹é½æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚è¿™æœ‰åŠ©äºæ•æ‰æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°ç»“æ„åŒ–å’Œä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚</li>
<li>VT-FSLåœ¨å¤šä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦çš„å°æ ·æœ¬å­¦ä¹ åœºæ™¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0d52355fbe28b9263d60b1f090617a30~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040065&auth_key=1760040065-0-0-c60e5360187ac998e7e831b38a89b90f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9af705fa082c84d4e881514f8fff464d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040072&auth_key=1760040072-0-0-b2ce67b899b935f72d303227a4bbdda2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fda6a8e6593952cbc22784b0fba62ed6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040078&auth_key=1760040078-0-0-e530cc64cdcc4f0d5ea119ef0f91501f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-76bea1641940b6f14a37bc0429de3487~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040085&auth_key=1760040085-0-0-267d76846a22c2d5bc11f3b2f8a4f618&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a09d3087398134cf568e19fcf367318~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040091&auth_key=1760040091-0-0-8adcb34f7f5349b43db924b9fc814742&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2dfdfdfab44386fe2ba27acf9d36e05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040097&auth_key=1760040097-0-0-8ca0586c66954d33c302e5ac504b4805&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis"><a href="#Mask-What-Matters-Controllable-Text-Guided-Masking-for-Self-Supervised-Medical-Image-Analysis" class="headerlink" title="Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis"></a>Mask What Matters: Controllable Text-Guided Masking for Self-Supervised   Medical Image Analysis</h2><p><strong>Authors:Ruilang Wang, Shuotong Xu, Bowen Liu, Runlin Huang, Donglong Chen, Weifeng Su</strong></p>
<p>The scarcity of annotated data in specialized domains such as medical imaging presents significant challenges to training robust vision models. While self-supervised masked image modeling (MIM) offers a promising solution, existing approaches largely rely on random high-ratio masking, leading to inefficiency and poor semantic alignment. Moreover, region-aware variants typically depend on reconstruction heuristics or supervised signals, limiting their adaptability across tasks and modalities. We propose Mask What Matters, a controllable text-guided masking framework for self-supervised medical image analysis. By leveraging vision-language models for prompt-based region localization, our method flexibly applies differentiated masking to emphasize diagnostically relevant regions while reducing redundancy in background areas. This controllable design enables better semantic alignment, improved representation learning, and stronger cross-task generalizability. Comprehensive evaluation across multiple medical imaging modalities, including brain MRI, chest CT, and lung X-ray, shows that Mask What Matters consistently outperforms existing MIM methods (e.g., SparK), achieving gains of up to +3.1 percentage points in classification accuracy, +1.3 in box average precision (BoxAP), and +1.1 in mask average precision (MaskAP) for detection. Notably, it achieves these improvements with substantially lower overall masking ratios (e.g., 40% vs. 70%). This work demonstrates that controllable, text-driven masking can enable semantically aligned self-supervised learning, advancing the development of robust vision models for medical image analysis. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸï¼Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ä¸ºè®­ç»ƒç¨³å¥çš„è§†è§‰æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶è‡ªç›‘ç£çš„æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æä¾›äº†å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºéšæœºçš„é«˜æ¯”ä¾‹æ©ç ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œè¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æ­¤å¤–ï¼ŒåŒºåŸŸæ„ŸçŸ¥å˜ä½“é€šå¸¸ä¾èµ–äºé‡å»ºå¯å‘å¼æˆ–ç›‘ç£ä¿¡å·ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä»»åŠ¡å’Œæ¨¡æ€ä¹‹é—´çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬æå‡ºäº†â€œMask What Mattersâ€ï¼ˆæ©ç å…³é”®å†…å®¹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯æ§çš„æ–‡æœ¬å¼•å¯¼å¼æ©ç æ¡†æ¶ï¼Œç”¨äºè‡ªç›‘ç£åŒ»å­¦å›¾åƒåˆ†æã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºæç¤ºçš„åŒºåŸŸå®šä½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥çµæ´»åœ°åº”ç”¨å·®å¼‚åŒ–æ©ç ï¼Œä»¥å¼ºè°ƒè¯Šæ–­ç›¸å…³çš„åŒºåŸŸï¼ŒåŒæ—¶å‡å°‘èƒŒæ™¯åŒºåŸŸçš„å†—ä½™ä¿¡æ¯ã€‚è¿™ç§å¯æ§çš„è®¾è®¡å®ç°äº†æ›´å¥½çš„è¯­ä¹‰å¯¹é½ã€æ”¹è¿›äº†è¡¨å¾å­¦ä¹ å’Œæ›´å¼ºçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€çš„å…¨é¢è¯„ä¼°ä¸­ï¼ŒåŒ…æ‹¬è„‘MRIã€èƒ¸éƒ¨CTå’Œè‚ºéƒ¨Xå°„çº¿ï¼Œæ˜¾ç¤ºMask What Matterså§‹ç»ˆä¼˜äºç°æœ‰çš„MIMæ–¹æ³•ï¼ˆä¾‹å¦‚SparKï¼‰ï¼Œåœ¨åˆ†ç±»å‡†ç¡®åº¦ä¸Šæé«˜äº†é«˜è¾¾+3.1ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¡†å¹³å‡ç²¾åº¦ï¼ˆBoxAPï¼‰æé«˜äº†+1.3ï¼Œæ©è†œå¹³å‡ç²¾åº¦ï¼ˆMaskAPï¼‰åœ¨æ£€æµ‹æ–¹é¢æé«˜äº†+1.1ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å®ç°è¿™äº›æ”¹è¿›çš„åŒæ—¶ï¼Œæ•´ä½“æ©ç æ¯”ä¾‹å¤§å¤§é™ä½ï¼ˆä¾‹å¦‚ï¼Œ40%å¯¹70%ï¼‰ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œå¯æ§çš„ã€æ–‡æœ¬é©±åŠ¨çš„æ©ç å¯ä»¥å®ç°è¯­ä¹‰å¯¹é½çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œæ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸç¨³å¥è§†è§‰æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23054v2">PDF</a> </p>
<p><strong>Summary</strong><br>    åŒ»å­¦å›¾åƒé¢†åŸŸç¼ºå°‘æ ‡æ³¨æ•°æ®å¯¹è®­ç»ƒç¨³å¥çš„è§†è§‰æ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ã€‚ç°æœ‰è‡ªç›‘ç£çš„æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æ–¹æ³•å¤§å¤šä¾èµ–éšæœºé«˜æ¯”ä¾‹æ©ç ï¼Œå­˜åœ¨æ•ˆç‡ä¸é«˜å’Œè¯­ä¹‰å¯¹é½å·®çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºMask What Mattersï¼Œä¸€ç§å¯æ§æ–‡æœ¬å¼•å¯¼çš„æ©ç æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„è‡ªç›‘ç£å­¦ä¹ ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºæç¤ºçš„åŒºåŸŸå®šä½ï¼Œçµæ´»åº”ç”¨å·®å¼‚åŒ–æ©ç ï¼Œå¼ºè°ƒè¯Šæ–­ç›¸å…³åŒºåŸŸï¼Œå‡å°‘èƒŒæ™¯åŒºåŸŸçš„å†—ä½™ã€‚å¯æ§è®¾è®¡å®ç°äº†æ›´å¥½çš„è¯­ä¹‰å¯¹é½ã€æ”¹è¿›äº†è¡¨å¾å­¦ä¹ å’Œæ›´å¼ºçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€çš„è¯„ä¼°ä¸­ï¼ŒMask What MattersæŒç»­è¡¨ç°ä¼˜äºç°æœ‰MIMæ–¹æ³•ï¼Œåˆ†ç±»å‡†ç¡®åº¦æé«˜è¾¾+3.1ä¸ªç™¾åˆ†ç‚¹ï¼Œæ¡†å¹³å‡ç²¾åº¦ï¼ˆBoxAPï¼‰å’Œæ©è†œå¹³å‡ç²¾åº¦ï¼ˆMaskAPï¼‰æ£€æµ‹åˆ†åˆ«æé«˜+1.3å’Œ+1.1ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒé¢†åŸŸç¼ºä¹æ ‡æ³¨æ•°æ®ï¼Œå¯¹è®­ç»ƒè§†è§‰æ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>è‡ªç›‘ç£çš„æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰MIMæ–¹æ³•ä¸»è¦ä¾èµ–éšæœºé«˜æ¯”ä¾‹æ©ç ï¼Œå¯¼è‡´æ•ˆç‡ä¸é«˜å’Œè¯­ä¹‰å¯¹é½å·®ã€‚</li>
<li>Mask What Mattersæ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŒºåŸŸå®šä½ï¼Œå®ç°å¯æ§çš„æ–‡æœ¬å¼•å¯¼æ©ç ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒè¯Šæ–­ç›¸å…³åŒºåŸŸï¼Œå‡å°‘èƒŒæ™¯åŒºåŸŸçš„å†—ä½™ï¼Œå®ç°æ›´å¥½çš„è¯­ä¹‰å¯¹é½ã€‚</li>
<li>Mask What Mattersåœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€çš„è¯„ä¼°ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç›¸æ¯”ç°æœ‰MIMæ–¹æ³•æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bb7d1fad51aa5e24ffda5efaa8b1987e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040104&auth_key=1760040104-0-0-04a309589868f7ab9485ae0c2542c352&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a9b77f37528ad3b0dbd3a78e5592431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040112&auth_key=1760040112-0-0-2b9b5e9d3c7bf3119695c0df9404e62e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe1061d4ddbf133c81b10db1428f0a39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040118&auth_key=1760040118-0-0-fa3ba5ba9e3ce93e52826d2886154b60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b9a818f2a1ca134a250753278bd4fc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040125&auth_key=1760040125-0-0-82bc206a28852b158b90e4b6eea92ee7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ImageNet-trained-CNNs-are-not-biased-towards-texture-Revisiting-feature-reliance-through-controlled-suppression"><a href="#ImageNet-trained-CNNs-are-not-biased-towards-texture-Revisiting-feature-reliance-through-controlled-suppression" class="headerlink" title="ImageNet-trained CNNs are not biased towards texture: Revisiting feature   reliance through controlled suppression"></a>ImageNet-trained CNNs are not biased towards texture: Revisiting feature   reliance through controlled suppression</h2><p><strong>Authors:Tom Burgert, Oliver Stoll, Paolo Rota, BegÃ¼m Demir</strong></p>
<p>The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at <a target="_blank" rel="noopener" href="https://github.com/tomburgert/feature-reliance">https://github.com/tomburgert/feature-reliance</a>. </p>
<blockquote>
<p>å‡è®¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æœ¬è´¨ä¸Šå…·æœ‰çº¹ç†åå‘æ€§ï¼Œè¿™ä¸€å‡è®¾åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½±å“äº†æ·±åº¦å­¦ä¹ ä¸­ç‰¹å¾ä½¿ç”¨çš„è®¨è®ºã€‚æˆ‘ä»¬é€šè¿‡é‡æ–°å®¡è§†Geirhosç­‰äººæç¤ºçº¿ç´¢å†²çªå®éªŒä¸­çš„å±€é™æ€§æ¥é‡æ–°æ¢è®¨è¿™ä¸€å‡è®¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢†åŸŸæ— å…³æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æŠ‘åˆ¶å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²çº¿ç´¢æ¥é‡åŒ–ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œé¿å…äº†å¼ºåˆ¶é€‰æ‹©å†²çªé€ æˆçš„æ··æ·†ã€‚é€šè¿‡åœ¨å—æ§æŠ‘åˆ¶æ¡ä»¶ä¸‹å¯¹äººç±»å’Œç¥ç»ç½‘ç»œè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°CNNå¹¶éæœ¬è´¨ä¸Šå…·æœ‰çº¹ç†åå‘æ€§ï¼Œè€Œæ˜¯ä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–å¯ä»¥é€šè¿‡ç°ä»£è®­ç»ƒç­–ç•¥æˆ–æ¶æ„ï¼ˆå¦‚ConvNeXtã€Vision Transformerï¼‰å¤§å¹…å‡è½»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å±•äº†è®¡ç®—æœºè§†è§‰ã€åŒ»å­¦æˆåƒå’Œé¥æ„Ÿåˆ†æçš„åˆ†æèŒƒå›´ï¼Œå‘ç°ä¾èµ–æ¨¡å¼å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼šè®¡ç®—æœºè§†è§‰æ¨¡å‹ä¼˜å…ˆè€ƒè™‘å½¢çŠ¶ï¼ŒåŒ»å­¦æˆåƒæ¨¡å‹å¼ºè°ƒé¢œè‰²ï¼Œé¥æ„Ÿæ¨¡å‹åˆ™è¡¨ç°å‡ºå¯¹çº¹ç†çš„æ›´å¼ºçƒˆä¾èµ–ã€‚ä»£ç å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/tomburgert/feature-reliance%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tomburgert/feature-relianceè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20234v2">PDF</a> Accepted at NeurIPS 2025 (oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦é’ˆå¯¹CNNæ¨¡å‹å­˜åœ¨çš„çº¹ç†åå¥½é—®é¢˜è¿›è¡Œäº†æ¢è®¨ï¼Œå¹¶å¯¹ç°æœ‰ç ”ç©¶ä¸­çš„ç¼ºé™·è¿›è¡Œäº†é˜è¿°ã€‚æ–‡ç« æå‡ºä¸€ä¸ªåŸºäºæ§åˆ¶å®éªŒçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å®šé‡ç ”ç©¶CNNå¯¹å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²ç‰¹å¾çš„ä¾èµ–ç¨‹åº¦ã€‚ç ”ç©¶å‘ç°CNNå¹¶éå¤©ç”Ÿåå¥½çº¹ç†ï¼Œè€Œæ˜¯ä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ã€‚é€šè¿‡ç°ä»£è®­ç»ƒç­–ç•¥æˆ–æ¶æ„ï¼ˆå¦‚ConvNeXtå’ŒViTsï¼‰ï¼Œè¿™ç§ä¾èµ–æ€§å¯ä»¥å¾—åˆ°æ˜¾è‘—ç¼“è§£ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†ä¸åŒé¢†åŸŸæ¨¡å‹çš„ç‰¹å¾ä¾èµ–æ¨¡å¼å·®å¼‚ï¼Œå¦‚è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¾§é‡äºå½¢çŠ¶ç‰¹å¾ï¼ŒåŒ»å­¦å½±åƒæ¨¡å‹å¼ºè°ƒé¢œè‰²ç‰¹å¾ï¼Œé¥æ„Ÿæ¨¡å‹åˆ™æ›´ä¾èµ–äºçº¹ç†ç‰¹å¾ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„ä¸»è¦è§‚ç‚¹å’Œå‘ç°ï¼š</p>
<ol>
<li>æ–‡ç« é‡æ–°è€ƒå¯Ÿäº†CNNæ¨¡å‹æ˜¯å¦å¤©ç”Ÿåå¥½çº¹ç†çš„é—®é¢˜ï¼Œå¹¶é’ˆå¯¹ç°æœ‰ç ”ç©¶çš„å±€é™è¿›è¡Œäº†è®¨è®ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶å®éªŒå®šé‡ç ”ç©¶CNNå¯¹å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²ç‰¹å¾çš„ä¾èµ–ç¨‹åº¦ã€‚</li>
<li>ç ”ç©¶å‘ç°CNNä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ï¼Œè€Œéå¤©ç”Ÿåå¥½çº¹ç†ã€‚</li>
<li>ç°ä»£è®­ç»ƒç­–ç•¥æˆ–æ¶æ„ï¼ˆå¦‚ConvNeXtå’ŒViTsï¼‰å¯ä»¥æ˜¾è‘—å‡å°‘CNNå¯¹ç‰¹å®šç‰¹å¾çš„ä¾èµ–ã€‚</li>
<li>ä¸åŒé¢†åŸŸçš„æ¨¡å‹ç‰¹å¾ä¾èµ–æ¨¡å¼å­˜åœ¨å·®å¼‚ï¼Œå¦‚è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¾§é‡äºå½¢çŠ¶ç‰¹å¾ï¼ŒåŒ»å­¦å½±åƒæ¨¡å‹æ³¨é‡é¢œè‰²ç‰¹å¾ï¼Œé¥æ„Ÿæ¨¡å‹æ›´ä¾èµ–äºçº¹ç†ç‰¹å¾ã€‚</li>
<li>æ–‡ç« æä¾›äº†è·¨è®¡ç®—æœºè§†è§‰ã€åŒ»å­¦æˆåƒå’Œé¥æ„Ÿç­‰å¤šä¸ªé¢†åŸŸçš„ç ”ç©¶ç»“æœï¼Œå±•ç°äº†å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0898dd4c8cdf2932a7d6e290995c6ec0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040132&auth_key=1760040132-0-0-1f4b3311b3cdb66632eb73fd915977a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6192fd49d32e11aaa74272ff6edccf34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040139&auth_key=1760040139-0-0-fcfe74b5dccd606df68f3c98c95599d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4e394c009733b6cd98245241dc38ac88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040146&auth_key=1760040146-0-0-aa50940e49a5d23a3cbc45f0b2472502&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e4d9d98a2016e9ce91c67861aeb5cd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040152&auth_key=1760040152-0-0-2e82b9bc1111978c281ca0ccd2d1d90d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-58ae21870b673b091dbf7a121b76d528~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040159&auth_key=1760040159-0-0-2430412849973906ed60982f17fe28e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Do-We-Need-All-the-Synthetic-Data-Targeted-Synthetic-Image-Augmentation-via-Diffusion-Models"><a href="#Do-We-Need-All-the-Synthetic-Data-Targeted-Synthetic-Image-Augmentation-via-Diffusion-Models" class="headerlink" title="Do We Need All the Synthetic Data? Targeted Synthetic Image Augmentation   via Diffusion Models"></a>Do We Need All the Synthetic Data? Targeted Synthetic Image Augmentation   via Diffusion Models</h2><p><strong>Authors:Dang Nguyen, Jiping Li, Jinghao Zheng, Baharan Mirzasoleiman</strong></p>
<p>Synthetically augmenting training datasets with diffusion models has been an effective strategy for improving generalization of image classifiers. However, existing techniques struggle to ensure the diversity of generation and increase the size of the data by up to 10-30x to improve the in-distribution performance. In this work, we show that synthetically augmenting part of the data that is not learned early in training with faithful images-containing same features but different noise-outperforms augmenting the entire dataset. By analyzing a two-layer CNN, we prove that this strategy improves generalization by promoting homogeneity in feature learning speed without amplifying noise. Our extensive experiments show that by augmenting only 30%-40% of the data, our method boosts generalization by up to 2.8% in a variety of scenarios, including training ResNet, ViT, ConvNeXt, and Swin Transformer on CIFAR-10&#x2F;100, and TinyImageNet, with various optimizers including SGD and SAM. Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on CIFAR-100 and TinyImageNet. </p>
<blockquote>
<p>é€šè¿‡æ‰©æ•£æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œåˆæˆå¢å¼ºï¼Œæ˜¯æé«˜å›¾åƒåˆ†ç±»å™¨æ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨ç¡®ä¿ç”Ÿæˆçš„å¤šæ ·æ€§å’Œé€šè¿‡æ‰©å¤§æ•°æ®è§„æ¨¡ï¼ˆæœ€å¤šè¾¾10-30å€ï¼‰æ¥æé«˜æ•°æ®é›†çš„æ³›åŒ–æ€§èƒ½æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§æ–°çš„åˆæˆå¢å¼ºç­–ç•¥ï¼Œå³åªå¯¹è®­ç»ƒåˆæœŸæœªè¢«å­¦ä¹ åˆ°çš„éƒ¨åˆ†æ•°æ®è¿›è¡Œå¢å¼ºï¼Œé€šè¿‡æ·»åŠ åŒ…å«ç›¸åŒç‰¹å¾ä½†å¸¦æœ‰ä¸åŒå™ªå£°çš„çœŸå®å›¾åƒæ¥è¶…è¶Šå¯¹æ•´ä¸ªæ•°æ®é›†çš„å¢å¼ºã€‚é€šè¿‡åˆ†æä¸¤å±‚å·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥ç­–ç•¥é€šè¿‡ä¿ƒè¿›ç‰¹å¾å­¦ä¹ é€Ÿåº¦çš„å‡ä¸€æ€§ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ä¼šæ”¾å¤§å™ªå£°ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ä»…å¢å¼ºæ•°æ®çš„30%-40%ï¼Œåœ¨å„ç§åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ³›åŒ–æ€§èƒ½é«˜è¾¾2.8%ã€‚è¿™äº›åœºæ™¯åŒ…æ‹¬åœ¨CIFAR-10&#x2F;100å’ŒTinyImageNetä¸Šè®­ç»ƒResNetã€ViTã€ConvNeXtå’ŒSwin Transformerç­‰æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨åŒ…æ‹¬SGDå’ŒSAMåœ¨å†…çš„å„ç§ä¼˜åŒ–å™¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸SGDç»“åˆä½¿ç”¨æ—¶ï¼Œåœ¨CIFAR-100å’ŒTinyImageNetä¸Šçš„æ€§èƒ½è¶…è¿‡äº†å½“å‰æœ€ä½³ä¼˜åŒ–å™¨SAMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21574v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é€šè¿‡å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œåˆæˆå¢å¼ºï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æé«˜å›¾åƒåˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ç§æœ‰æ•ˆç­–ç•¥ã€‚ä½†ç°æœ‰æŠ€æœ¯åœ¨ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§å’Œè§„æ¨¡æ–¹é¢ä»æœ‰ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œåªå¯¹è®­ç»ƒåˆæœŸæœªå­¦ä¼šçš„æ•°æ®éƒ¨åˆ†è¿›è¡Œåˆæˆå¢å¼ºï¼Œé€šè¿‡åŠ å…¥ç›¸åŒç‰¹å¾ä½†å«æœ‰ä¸åŒå™ªå£°çš„å›¾åƒï¼Œè€Œéå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œå¢å¼ºã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§ç­–ç•¥èƒ½æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä¿ƒè¿›ç‰¹å¾å­¦ä¹ é€Ÿåº¦çš„å‡ä¸€æ€§ï¼ŒåŒæ—¶ä¸ä¼šæ”¾å¤§å™ªå£°ã€‚ä»…å¯¹30%-40%çš„æ•°æ®è¿›è¡Œå¢å¼ºï¼Œå°±èƒ½åœ¨å¤šç§åœºæ™¯ä¸‹æé«˜æ³›åŒ–æ€§èƒ½è¾¾2.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºåˆæˆå¢å¼ºè®­ç»ƒæ•°æ®é›†ï¼Œæé«˜å›¾åƒåˆ†ç±»å™¨æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ•°æ®å¢å¼ºæŠ€æœ¯é¢ä¸´ç”Ÿæˆæ•°æ®å¤šæ ·æ€§å’Œè§„æ¨¡æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡ç­–ç•¥ä»…å¯¹è®­ç»ƒåˆæœŸæœªæŒæ¡çš„éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆæˆå¢å¼ºã€‚</li>
<li>é€šè¿‡æ·»åŠ å«æœ‰ç›¸åŒç‰¹å¾ä½†ä¸åŒå™ªå£°çš„å›¾åƒï¼Œç­–ç•¥è¡¨ç°å‡ºä¼˜äºå…¨é¢å¢å¼ºçš„æ•ˆæœã€‚</li>
<li>ç­–ç•¥ä¿ƒè¿›ç‰¹å¾å­¦ä¹ é€Ÿåº¦çš„å‡ä¸€æ€§ï¼ŒåŒæ—¶ä¸æ”¾å¤§å™ªå£°ã€‚</li>
<li>ä»…å¯¹éƒ¨åˆ†æ•°æ®è¿›è¡Œå¢å¼ºï¼Œåœ¨å¤šç§åœºæ™¯ä¸‹æ˜¾è‘—æé«˜æ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-04de6d40e9c5666bd77378c765d013f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040166&auth_key=1760040166-0-0-f4b9a15c0ed89cd4f0780eaa06899c93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-539e21db637b6646d5a73008b15be342~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040173&auth_key=1760040173-0-0-b6f0881cfa11fe0ca6660a88aee7b4e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AutoMiSeg-Automatic-Medical-Image-Segmentation-via-Test-Time-Adaptation-of-Foundation-Models"><a href="#AutoMiSeg-Automatic-Medical-Image-Segmentation-via-Test-Time-Adaptation-of-Foundation-Models" class="headerlink" title="AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation   of Foundation Models"></a>AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation   of Foundation Models</h2><p><strong>Authors:Xingjian Li, Qifeng Wu, Adithya S. Ubaradka, Yiran Ding, Colleen Que, Runmin Jiang, Jianhua Xing, Tianyang Wang, Min Xu</strong></p>
<p>Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., â€œsegment the optic disc in an eye fundus imageâ€), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline not only substantially surpasses the previously best-performing method, yielding a 69% relative improvement in accuracy (Dice Score from 42.53 to 71.81), but also performs competitively with weakly-prompted interactive foundation models. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒåˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„ä¸“å®¶å·¥ä½œï¼Œä¾‹å¦‚é€šè¿‡æ ‡æ³¨å¤§é‡è®­ç»ƒæ•°æ®é›†æˆ–åœ¨æ¨ç†æ—¶é—´ä¸ºæ¯ä¸ªæ–°ç—…ä¾‹æä¾›æç¤ºã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é›¶æ ·æœ¬è‡ªåŠ¨åˆ†å‰²ç®¡é“ï¼Œå®ƒç»“åˆäº†ç°æˆçš„è§†è§‰è¯­è¨€å’ŒåŸºç¡€åˆ†å‰²æ¨¡å‹ã€‚ç»™å®šä¸€å¼ åŒ»å­¦å›¾åƒå’Œä»»åŠ¡å®šä¹‰ï¼ˆä¾‹å¦‚ï¼Œâ€œåœ¨çœ¼åº•å›¾åƒä¸­åˆ†å‰²è§†ç¥ç»ç›˜â€ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å®šä½æ¨¡å‹ç”Ÿæˆåˆå§‹è¾¹ç•Œæ¡†ï¼Œç„¶åé€šè¿‡å¢å¼ºæç¤ºçš„è§†è§‰æç¤ºå¢å¼ºæ¨¡å—è¿›è¡Œå¤„ç†ï¼Œæœ€åç”±å¯æç¤ºçš„åˆ†å‰²æ¨¡å‹ç”Ÿæˆæœ€ç»ˆæ©ç ã€‚ä¸ºäº†åº”å¯¹é¢†åŸŸå·®è·å’Œç»“æœéªŒè¯çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶é—´è‡ªé€‚åº”æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸€ç»„å¯å­¦ä¹ çš„é€‚é…å™¨ï¼Œç”¨äºå°†åŒ»å­¦è¾“å…¥ä¸åŸºç¡€æ¨¡å‹è¡¨ç¤ºå¯¹é½ã€‚å…¶è¶…å‚æ•°é€šè¿‡è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œä¼˜åŒ–ï¼Œç”±ä»£ç†éªŒè¯æ¨¡å‹æŒ‡å¯¼ï¼Œæ— éœ€çœŸå®æ ‡ç­¾ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸ºè·¨ä¸åŒä»»åŠ¡çš„é›¶æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç®¡é“åœ¨ä¸ƒä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚é€šè¿‡é€‚å½“çš„åˆ†è§£å’Œæµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼Œæˆ‘ä»¬çš„å…¨è‡ªåŠ¨ç®¡é“ä¸ä»…æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰è¡¨ç°æœ€å¥½çš„æ–¹æ³•ï¼Œåœ¨å‡†ç¡®åº¦ä¸Šæé«˜äº†69%ï¼ˆDiceå¾—åˆ†ä»42.53æé«˜åˆ°71.81ï¼‰ï¼Œè€Œä¸”åœ¨å¼±æç¤ºäº¤äº’å¼åŸºç¡€æ¨¡å‹ä¸­ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17931v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»ç–—å›¾åƒåˆ†å‰²å¯¹äºä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„ä¸“å®¶åŠªåŠ›ï¼Œä¾‹å¦‚é€šè¿‡æ ‡æ³¨å¤§å‹è®­ç»ƒæ•°æ®é›†æˆ–åœ¨æ¨ç†æ—¶é—´ä¸ºæ¯ä¸ªæ–°ç—…ä¾‹æä¾›æç¤ºã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é›¶æ ·æœ¬è‡ªåŠ¨åˆ†å‰²ç®¡é“ï¼Œå®ƒç»“åˆäº†ç°æˆçš„è§†è§‰è¯­è¨€å’ŒåŸºç¡€åˆ†å‰²æ¨¡å‹ã€‚ç»™å®šåŒ»ç–—å›¾åƒå’Œä»»åŠ¡å®šä¹‰ï¼ˆä¾‹å¦‚ï¼Œâ€œåœ¨çœ¼åº•å›¾åƒä¸­åˆ†å‰²è§†ç›˜â€ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æ¥åœ°æ¨¡å‹ç”Ÿæˆåˆå§‹è¾¹ç•Œæ¡†ï¼Œç„¶åé€šè¿‡å¢å¼ºæç¤ºçš„è§†è§‰æç¤ºå¢å¼ºæ¨¡å—ï¼Œè¿™äº›æç¤ºéšåè¢«å¯æç¤ºçš„åˆ†å‰²æ¨¡å‹å¤„ç†ä»¥äº§ç”Ÿæœ€ç»ˆè’™ç‰ˆã€‚ä¸ºäº†è§£å†³é¢†åŸŸå·®è·å’Œç»“æœéªŒè¯çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶é—´é€‚åº”æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸€ç»„å¯å­¦ä¹ çš„é€‚é…å™¨ï¼Œç”¨äºå°†åŒ»ç–—è¾“å…¥ä¸åŸºç¡€æ¨¡å‹è¡¨ç¤ºå¯¹é½ã€‚å…¶è¶…å‚æ•°é€šè¿‡è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œä¼˜åŒ–ï¼Œç”±æ— éœ€çœŸå®æ ‡ç­¾çš„ä»£ç†éªŒè¯æ¨¡å‹æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸ºè·¨ä¸åŒä»»åŠ¡çš„é›¶æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç®¡é“åœ¨ä¸ƒä¸ªä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚é€šè¿‡é€‚å½“çš„åˆ†è§£å’Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼Œæˆ‘ä»¬çš„å…¨è‡ªåŠ¨ç®¡é“ä¸ä»…æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰è¡¨ç°æœ€ä½³çš„æ–¹æ³•ï¼Œåœ¨å‡†ç¡®åº¦ä¸Šç›¸å¯¹æé«˜äº†69%ï¼ˆDiceå¾—åˆ†ä»42.53æé«˜åˆ°71.81ï¼‰ï¼Œè€Œä¸”åœ¨å¼±æç¤ºäº¤äº’å¼åŸºç¡€æ¨¡å‹ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬è‡ªåŠ¨åŒ»ç–—å›¾åƒåˆ†å‰²ç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’ŒåŸºç¡€åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨æ¥åœ°æ¨¡å‹ç”Ÿæˆåˆå§‹è¾¹ç•Œæ¡†ï¼Œç„¶ååˆ©ç”¨è§†è§‰æç¤ºå¢å¼ºæ¨¡å—å¢å¼ºæç¤ºï¼Œå†è¿›è¡Œå¤„ç†ä»¥äº§ç”Ÿæœ€ç»ˆè’™ç‰ˆã€‚</li>
<li>é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸç‰¹æœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚é¢†åŸŸå·®è·å’Œç»“æœéªŒè¯ï¼Œè®ºæ–‡å¼•å…¥äº†æµ‹è¯•æ—¶é—´é€‚åº”æ¡†æ¶å’Œå¯å­¦ä¹ é€‚é…å™¨ã€‚</li>
<li>è¯¥ç®¡é“çš„è¶…å‚æ•°ä¼˜åŒ–æ˜¯é€šè¿‡è´å¶æ–¯ä¼˜åŒ–å®ç°çš„ï¼Œå¹¶ç”±æ— éœ€çœŸå®æ ‡ç­¾çš„ä»£ç†éªŒè¯æ¨¡å‹æŒ‡å¯¼ã€‚</li>
<li>è¯¥ç®¡é“åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœä»¤äººé¼“èˆï¼Œç›¸æ¯”ä¹‹å‰æœ€ä½³æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
<li>é€šè¿‡é€‚å½“çš„åˆ†è§£å’Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼Œè¯¥ç®¡é“çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨å¼±æç¤ºäº¤äº’å¼åŸºç¡€æ¨¡å‹ä¸­ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-13c7cb222ce3f7c4027634919a3b8253~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040181&auth_key=1760040181-0-0-b24b848326e4b1d242d0dbbc0559b398&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe144fb768b3ae7f90579b5daf66687e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040188&auth_key=1760040188-0-0-0e966e28a96314d2745089da62a9481e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db9038c169af92f3122f5872c731e32b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040195&auth_key=1760040195-0-0-3de81bcea54de8fb9c23580925f96957&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ac45edede43bac2333b49f0a2b18cb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040201&auth_key=1760040201-0-0-c62b8494c61b304ecb0b8b817cded9b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano PatrÃ­cio, Isabel Rio-Torto, Jaime S. Cardoso, LuÃ­s F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the model output on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>åœ¨åŒ»ç–—å·¥ä½œæµç¨‹ä¸­é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œæ­¤ç±»ç³»ç»Ÿç¼ºä¹å¯è§£é‡Šæ€§ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰é€šè¿‡çº¦æŸæ¨¡å‹è¾“å‡ºåœ¨ä¸€ç»„é¢„å…ˆå®šä¹‰å’Œäººç±»å¯è§£é‡Šçš„æ¦‚å¿µä¸Šæ¥è§£å†³åè€…çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡åŸºäºæ¦‚å¿µçš„è§£é‡Šæ‰€å¢åŠ çš„å¯è§£é‡Šæ€§æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ã€‚è€Œä¸”ï¼Œå¦‚æœè¦æ·»åŠ æ–°æ¦‚å¿µï¼Œéœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªç³»ç»Ÿã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸­çš„å‡ºè‰²è¡¨ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå³CBVLMï¼Œå®ƒè§£å†³äº†ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬æç¤ºLVLMå›ç­”æ¦‚å¿µæ˜¯å¦å‡ºç°åœ¨è¾“å…¥å›¾åƒä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬è®©LVLMåŸºäºä¹‹å‰çš„æ¦‚å¿µé¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è€Œä¸”ï¼Œåœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬éƒ½èå…¥äº†ä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œè´Ÿè´£é€‰æ‹©æœ€ä½³ç¤ºä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å†…å­¦ä¹ ã€‚é€šè¿‡å°†æœ€ç»ˆè¯Šæ–­å»ºç«‹åœ¨é¢„æµ‹çš„æ¦‚å¿µä¸Šï¼Œæˆ‘ä»¬ç¡®ä¿äº†å¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡åˆ©ç”¨LVLMçš„å°‘é‡æ ·æœ¬èƒ½åŠ›ï¼Œæˆ‘ä»¬å¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªåŒ»ç–—æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»ç–—ï¼‰LVLMçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¡¨æ˜CBVLMå§‹ç»ˆä¼˜äºCBMå’Œç‰¹å®šä»»åŠ¡ç›‘ç£æ–¹æ³•ï¼Œè€Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒï¼Œåªéœ€ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š[ç½‘å€]ï¼ˆ<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/%EF%BC%89%E3%80%82">https://cristianopatricio.github.io/CBVLM/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v3">PDF</a> Accepted for publication in Computers in Biology and Medicine</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCBVLMçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å·¥ä½œæµç¨‹ä¸­é¢ä¸´çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šæ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºä¹ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒCBVLMä¸ä»…æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œè¿˜é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹æ¦‚å¿µå¹¶åŸºäºè¿™äº›é¢„æµ‹è¿›è¡Œå›¾åƒåˆ†ç±»ï¼ŒåŒæ—¶ç»“åˆæ£€ç´¢æ¨¡å—é€‰æ‹©æœ€ä½³ä¸Šä¸‹æ–‡å­¦ä¹ ç¤ºä¾‹ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒCBVLMåœ¨å››ä¸ªåŒ»ç–—æ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰å’Œä»»åŠ¡ç‰¹å®šç›‘ç£æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CBVLMæ–¹æ³•æ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šæ ‡æ³¨æ•°æ®å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒCBVLMæé«˜æ¨¡å‹è§£é‡Šæ€§å¹¶é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>CBVLMé€šè¿‡é¢„æµ‹æ¦‚å¿µå¹¶åŸºäºè¿™äº›æ¦‚å¿µè¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œå®ç°åŒ»ç–—å›¾åƒçš„åˆ†æå’Œè¯Šæ–­ã€‚</li>
<li>æ£€ç´¢æ¨¡å—çš„é€‰æ‹©æœ‰åŠ©äºåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­é€‰æ‹©æœ€ä½³ç¤ºä¾‹ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>CBVLMåœ¨å››ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„è¡¨ç°æŒç»­ä¼˜äºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰å’Œä»»åŠ¡ç‰¹å®šç›‘ç£æ–¹æ³•ã€‚</li>
<li>CBVLMæ–¹æ³•æ— éœ€ä»»ä½•è®­ç»ƒï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ï¼Œå…·æœ‰å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-15ec5a9c5d00e6bc7dcf25c49384c7ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040208&auth_key=1760040208-0-0-e104863a4e9c15898a74aa18e53fd9fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36b80d22716191da8fc4c178f927e398~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040216&auth_key=1760040216-0-0-78c218a60bff6eb0fcc55302520986b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b386e2ae3c4946b3e59685e95f4804b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040256&auth_key=1760040256-0-0-d40ff2ebef09cad4369533d7bfcfed3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be192d43deb0d0b33637393de5cd9c1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040263&auth_key=1760040263-0-0-2938871f5043ee6a90ba1817baa645a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Towards-a-Multimodal-Large-Language-Model-with-Pixel-Level-Insight-for-Biomedicine"><a href="#Towards-a-Multimodal-Large-Language-Model-with-Pixel-Level-Insight-for-Biomedicine" class="headerlink" title="Towards a Multimodal Large Language Model with Pixel-Level Insight for   Biomedicine"></a>Towards a Multimodal Large Language Model with Pixel-Level Insight for   Biomedicine</h2><p><strong>Authors:Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang</strong></p>
<p>In recent years, Multimodal Large Language Models (MLLM) have achieved notable advancements, demonstrating the feasibility of developing an intelligent biomedical assistant. However, current biomedical MLLMs predominantly focus on image-level understanding and restrict interactions to textual commands, thus limiting their capability boundaries and the flexibility of usage. In this paper, we introduce a novel end-to-end multimodal large language model for the biomedical domain, named MedPLIB, which possesses pixel-level understanding. Excitingly, it supports visual question answering (VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE) multi-stage training strategy, which divides MoE into separate training phases for a visual-language expert model and a pixel-grounding expert model, followed by fine-tuning using MoE. This strategy effectively coordinates multitask learning while maintaining the computational cost at inference equivalent to that of a single expert model. To advance the research of biomedical MLLMs, we introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA), which comprises an array of 8 modalities for complex medical imaging question answering and image region understanding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes across multiple medical visual language tasks. More importantly, in zero-shot evaluations for the pixel grounding task, MedPLIB leads the best small and large models by margins of 19.7 and 15.6 respectively on the mDice metric. The codes, data, and model checkpoints will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/ShawnHuang497/MedPLIB">https://github.com/ShawnHuang497/MedPLIB</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¯æ˜äº†å¼€å‘æ™ºèƒ½ç”Ÿç‰©åŒ»å­¦åŠ©ç†çš„å¯è¡Œæ€§ã€‚ç„¶è€Œï¼Œå½“å‰ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„MLLMä¸»è¦ä¾§é‡äºå›¾åƒçº§åˆ«çš„ç†è§£ï¼Œå¹¶å°†äº’åŠ¨é™åˆ¶åœ¨æ–‡æœ¬å‘½ä»¤ä¸Šï¼Œä»è€Œé™åˆ¶äº†å…¶èƒ½åŠ›è¾¹ç•Œå’Œä½¿ç”¨çš„çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æ–°å‹ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåä¸ºMedPLIBï¼Œå®ƒå…·æœ‰åƒç´ çº§çš„ç†è§£èƒ½åŠ›ã€‚ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œå®ƒæ”¯æŒè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€ä»»æ„çš„åƒç´ çº§æç¤ºï¼ˆç‚¹ã€è¾¹ç•Œæ¡†å’Œè‡ªç”±å½¢å¼å½¢çŠ¶ï¼‰å’Œåƒç´ çº§å®šä½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†MoEåˆ†ä¸ºè§†è§‰è¯­è¨€ä¸“å®¶æ¨¡å‹å’Œåƒç´ å®šä½ä¸“å®¶æ¨¡å‹çš„å•ç‹¬è®­ç»ƒé˜¶æ®µï¼Œç„¶åä½¿ç”¨MoEè¿›è¡Œå¾®è°ƒã€‚è¿™ç§ç­–ç•¥æœ‰æ•ˆåœ°åè°ƒäº†å¤šä»»åŠ¡å­¦ä¹ ï¼ŒåŒæ—¶åœ¨æ¨ç†é˜¶æ®µçš„è®¡ç®—æˆæœ¬ç›¸å½“äºå•ä¸ªä¸“å®¶æ¨¡å‹ã€‚ä¸ºäº†æ¨åŠ¨ç”Ÿç‰©åŒ»å­¦MLLMçš„ç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŒ»ç–—å¤æ‚è§†è§‰é—®ç­”æ•°æ®é›†ï¼ˆMeCoVQAï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”¨äºå¤æ‚åŒ»ç–—å½±åƒé—®ç­”å’Œå›¾åƒåŒºåŸŸç†è§£çš„8ç§æ¨¡æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedPLIBåœ¨å¤šä¸ªåŒ»ç–—è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨åƒç´ å®šä½ä»»åŠ¡çš„é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼ŒMedPLIBåœ¨å°æ¨¡å‹å’Œå¤§æ¨¡å‹ä¸Šçš„mDiceæŒ‡æ ‡åˆ†åˆ«é¢†å…ˆäº†19.7å’Œ15.6çš„å·®è·ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShawnHuang497/MedPLIB%E4%B8%8A%E3%80%82">https://github.com/ShawnHuang497/MedPLIBä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09278v3">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æ–°å‹ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”MedPLIBï¼Œå®ƒå…·å¤‡åƒç´ çº§ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒè§†è§‰é—®ç­”ã€ä»»æ„åƒç´ çº§æç¤ºå’Œåƒç´ çº§å®šä½ã€‚æå‡ºä¸€ç§æ–°é¢–çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆåè°ƒå¤šä»»åŠ¡å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ—¶çš„è®¡ç®—æˆæœ¬ä¸å•ä¸€ä¸“å®¶æ¨¡å‹ç›¸å½“ã€‚ä¸ºæ¨è¿›ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ï¼Œå¼•å…¥äº†åŒ»ç–—å¤æ‚è§†è§‰é—®ç­”æ•°æ®é›†ï¼ˆMeCoVQAï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedPLIBåœ¨å¤šä¸ªåŒ»ç–—è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œå¹¶åœ¨åƒç´ å®šä½ä»»åŠ¡çš„é›¶æ ·æœ¬è¯„ä¼°ä¸­é¢†å…ˆå…¶ä»–æœ€ä½³å°å‹å’Œå¤§å‹æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedPLIBæ˜¯ä¸€ä¸ªå…·æœ‰åƒç´ çº§ç†è§£èƒ½åŠ›çš„ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ”¯æŒè§†è§‰é—®ç­”ã€ä»»æ„åƒç´ çº§æç¤ºå’Œåƒç´ çº§å®šä½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„MoEå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆåè°ƒå¤šä»»åŠ¡å­¦ä¹ ã€‚</li>
<li>MedPLIBåœ¨å¤šä¸ªåŒ»ç–—è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æˆæœã€‚</li>
<li>å¼•å…¥äº†åŒ»ç–—å¤æ‚è§†è§‰é—®ç­”æ•°æ®é›†MeCoVQAï¼ŒåŒ…å«8ç§æ¨¡æ€ï¼Œç”¨äºå¤æ‚åŒ»ç–—å½±åƒé—®ç­”å’Œå›¾åƒåŒºåŸŸç†è§£ç ”ç©¶ã€‚</li>
<li>MedPLIBåœ¨åƒç´ å®šä½ä»»åŠ¡çš„é›¶æ ·æœ¬è¯„ä¼°ä¸­è¡¨ç°é¢†å…ˆã€‚</li>
<li>MedPLIBçš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€æä¾›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-610f9ee25d80e928726b149bcd9a95f2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040307&auth_key=1760040307-0-0-d7488574671b5838ce08055e0e01400b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a3a38a7b375c84f447d61ebaeb625271~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040315&auth_key=1760040315-0-0-800df521df122318c39ab0f378decdc2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-629514beb8b69cc83cd1919dbf4a0457~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040321&auth_key=1760040321-0-0-b3002ee2e862133ad90930e438caf461&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-205ff77a32141da56772930c670ba953~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040327&auth_key=1760040327-0-0-ba37dfa2c527b8fcf9d39408c6396e35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb7e869d6dbf6d745cff959e82db9d3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040334&auth_key=1760040334-0-0-996a1844bc497b3a550c1512d511ca2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-833d94232999c5bd034947fa459a2012~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040341&auth_key=1760040341-0-0-d8d9fa3220e592d59aef91f7240478ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-8425046653ec2d2d25050c898fcec42d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041232&auth_key=1760041232-0-0-83bee0d4ce97485eef8a5c583c80a0d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d831a58042b2e65afc4b0f12b7f137c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760038822&auth_key=1760038822-0-0-b476aff58a53b7de14a03bac10ca6a5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Flow4Agent Long-form Video Understanding via Motion Prior from Optical   Flow
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
