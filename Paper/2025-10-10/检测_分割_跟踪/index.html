<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-10-10  Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-8425046653ec2d2d25050c898fcec42d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041232&auth_key=1760041232-0-0-83bee0d4ce97485eef8a5c583c80a0d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-10-更新"><a href="#2025-10-10-更新" class="headerlink" title="2025-10-10 更新"></a>2025-10-10 更新</h1><h2 id="Semantic-Segmentation-Algorithm-Based-on-Light-Field-and-LiDAR-Fusion"><a href="#Semantic-Segmentation-Algorithm-Based-on-Light-Field-and-LiDAR-Fusion" class="headerlink" title="Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion"></a>Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion</h2><p><strong>Authors:Jie Luo, Yuxuan Jiang, Xin Jin, Mingyu Liu, Yihui Fan</strong></p>
<p>Semantic segmentation serves as a cornerstone of scene understanding in autonomous driving but continues to face significant challenges under complex conditions such as occlusion. Light field and LiDAR modalities provide complementary visual and spatial cues that are beneficial for robust perception; however, their effective integration is hindered by limited viewpoint diversity and inherent modality discrepancies. To address these challenges, the first multimodal semantic segmentation dataset integrating light field data and point cloud data is proposed. Based on this dataset, we proposed a multi-modal light field point-cloud fusion segmentation network(Mlpfseg), incorporating feature completion and depth perception to segment both camera images and LiDAR point clouds simultaneously. The feature completion module addresses the density mismatch between point clouds and image pixels by performing differential reconstruction of point-cloud feature maps, enhancing the fusion of these modalities. The depth perception module improves the segmentation of occluded objects by reinforcing attention scores for better occlusion awareness. Our method outperforms image-only segmentation by 1.71 Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38 mIoU, demonstrating its effectiveness. </p>
<blockquote>
<p>语义分割是自动驾驶场景理解的核心技术，但在遮挡等复杂条件下仍面临巨大挑战。光场和激光雷达模态提供了有益的视觉和空间线索，有助于实现稳健的感知；然而，由于视点多样性有限和固有模态差异，它们的有效集成受到了阻碍。为了应对这些挑战，我们提出了第一个融合光场数据和点云数据的多模态语义分割数据集。基于此数据集，我们提出了一种多模态光场点云融合分割网络（Mlpfseg），该网络结合了特征补全和深度感知，可同时分割相机图像和激光雷达点云。特征补全模块通过点云特征图的差异重建解决了点云与图像像素之间的密度不匹配问题，增强了这些模态的融合。深度感知模块通过加强注意力分数来提高遮挡物体的分割，从而提高遮挡意识。我们的方法相较于仅使用图像的分割方法提高了1.71的平均交并比（mIoU），相较于仅使用点云的分割方法提高了2.38的mIoU，证明了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对自主驾驶中的语义分割问题，提出的多模态轻量级点云融合分割网络（Mlpfseg）。该网络结合了光场数据和点云数据，通过特征补全和深度感知模块，提高了遮挡物体的分割效果。实验结果表明，该方法在光场和点云数据融合方面表现出优越性，相比仅使用图像或点云数据的分割方法，分别提高了1.71和2.38的Mean Intersection over Union（mIoU）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割在自主驾驶场景理解中扮演着重要角色，但在复杂条件下如遮挡等仍面临挑战。</li>
<li>光场和LiDAR模态提供视觉和空间线索，有益于稳健的感知。</li>
<li>多模态语义分割数据集的提出，整合了光场数据和点云数据，为解决视角多样性和模态差异问题提供了基础。</li>
<li>介绍了Mlpfseg网络，该网络能够同时处理相机图像和LiDAR点云。</li>
<li>特征补全模块解决了点云和图像像素之间的密度不匹配问题，通过点云特征图的差异重建增强了模态的融合。</li>
<li>深度感知模块提高了遮挡物体的分割效果，通过加强注意力分数来提升遮挡意识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9a0f427ac1f0c3148285626fc8be06ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760040997&auth_key=1760040997-0-0-c38711257901c7b0c1a1bbd6e9a9b2ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d60ef7a36a2c7443942e4622a72c44d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041038&auth_key=1760041038-0-0-3ee525d8abc4e1f3b0d87289acd3ce67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3da14ae6435d952f7443d35aaba3e4e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041044&auth_key=1760041044-0-0-349d2f810849093636e328f2ccdd218c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f9b7b54ac2ddb3871c7678ac912b7a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041072&auth_key=1760041072-0-0-77f9a52bdbd0e4f5fd3bd17a3fc9abc3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8412e310757a4f74df981a751aceece0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041079&auth_key=1760041079-0-0-8ac9728a0e47a1df8cb7735b17aa998d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ALISE-Annotation-Free-LiDAR-Instance-Segmentation-for-Autonomous-Driving"><a href="#ALISE-Annotation-Free-LiDAR-Instance-Segmentation-for-Autonomous-Driving" class="headerlink" title="ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous   Driving"></a>ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous   Driving</h2><p><strong>Authors:Yongxuan Lyu, Guangfeng Jiang, Hongsi Liu, Jun Liu</strong></p>
<p>The manual annotation of outdoor LiDAR point clouds for instance segmentation is extremely costly and time-consuming. Current methods attempt to reduce this burden but still rely on some form of human labeling. To completely eliminate this dependency, we introduce ALISE, a novel framework that performs LiDAR instance segmentation without any annotations. The central challenge is to generate high-quality pseudo-labels in a fully unsupervised manner. Our approach starts by employing Vision Foundation Models (VFMs), guided by text and images, to produce initial pseudo-labels. We then refine these labels through a dedicated spatio-temporal voting module, which combines 2D and 3D semantics for both offline and online optimization. To achieve superior feature learning, we further introduce two forms of semantic supervision: a set of 2D prior-based losses that inject visual knowledge into the 3D network, and a novel prototype-based contrastive loss that builds a discriminative feature space by exploiting 3D semantic consistency. This comprehensive design results in significant performance gains, establishing a new state-of-the-art for unsupervised 3D instance segmentation. Remarkably, our approach even outperforms MWSIS, a method that operates with supervision from ground-truth (GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%). </p>
<blockquote>
<p>激光雷达点云的手动标注实例分割极为耗费成本和时间。当前的方法试图减少这种负担，但仍然需要某种形式的人工标注。为了完全消除这种依赖，我们引入了ALISE这一全新框架，该框架可在无需任何标注的情况下执行激光雷达实例分割。最大的挑战在于以完全无监督的方式生成高质量伪标签。我们的方法始于利用视觉基础模型（VFMs），通过文本和图像指导来生成初始伪标签。随后通过专门的时空投票模块对标签进行细化处理，该模块结合了用于离线与在线优化的二维和三维语义。为了实现卓越的特征学习，我们还引入了两种形式的语义监督：一组基于二维先验的损失将视觉知识注入三维网络中，以及一种新型基于原型对比损失构建判别特征空间通过利用三维语义一致性。这一全面的设计带来了显著的性能提升，为无监督的三维实例分割建立了新的最先进的水平。值得注意的是，我们的方法甚至超过了在真实地面数据进行监督运算的方法MWSIS（在MAP上提高了2.53％，从原来的百分比是通过对等的实体的从性能的一种数字大小 ，达领先水平的能力相对不错的专家当前标准平均精度是 百分之五十点九五是优于先前的一个最好的系统的结果达到百分之四十八点四十二）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05752v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于LiDAR的点云数据实例分割标注成本高昂且耗时。研究团队推出全新框架ALISE，无需人工标注即可实现LiDAR实例分割。该框架通过视觉基础模型生成高质量伪标签，并利用时空投票模块进行精细化处理，结合2D和3D语义进行在线和离线优化。通过引入两种语义监督方式，实现了卓越的特征学习能力，并实现了无监督的3D实例分割的新水平。相较于依赖GT 2D边界框的MWSIS方法，性能提升了2.53%。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>室外LiDAR点云数据的实例分割标注成本高昂且耗时。</li>
<li>现有方法尝试减少人工标注的依赖，但仍需某种形式的标注。</li>
<li>ALISE框架无需任何标注即可实现LiDAR实例分割。</li>
<li>通过视觉基础模型生成高质量伪标签，并采用时空投票模块进行精细化处理。</li>
<li>结合了在线和离线优化、利用多种形式的语义监督来提升特征学习能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05752">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7d888c64053599887757a816e50edfb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041086&auth_key=1760041086-0-0-5f9811119a3f62e4cebfc43dd4dbb5fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a1aed8741ec2a3f5bcc79ddbf161d5d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041094&auth_key=1760041094-0-0-9f8b70c57804ef77765f435778777e70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5a1c100c6eefc53b4f69641df07a612~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041101&auth_key=1760041101-0-0-af68fc64625ac23a66b5b5600a58894f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Incremental-Object-Detection-with-Prompt-based-Methods"><a href="#Incremental-Object-Detection-with-Prompt-based-Methods" class="headerlink" title="Incremental Object Detection with Prompt-based Methods"></a>Incremental Object Detection with Prompt-based Methods</h2><p><strong>Authors:Matthias Neuwirth-Trapp, Maarten Bieshaar, Danda Pani Paudel, Luc Van Gool</strong></p>
<p>Visual prompt-based methods have seen growing interest in incremental learning (IL) for image classification. These approaches learn additional embedding vectors while keeping the model frozen, making them efficient to train. However, no prior work has applied such methods to incremental object detection (IOD), leaving their generalizability unclear. In this paper, we analyze three different prompt-based methods under a complex domain-incremental learning setting. We additionally provide a wide range of reference baselines for comparison. Empirically, we show that the prompt-based approaches we tested underperform in this setting. However, a strong yet practical method, combining visual prompts with replaying a small portion of previous data, achieves the best results. Together with additional experiments on prompt length and initialization, our findings offer valuable insights for advancing prompt-based IL in IOD. </p>
<blockquote>
<p>基于视觉提示的方法在图像分类的增量学习（IL）中日益受到关注。这些方法在学习额外的嵌入向量时保持模型冻结，使它们训练效率很高。然而，之前的工作并未将此类方法应用于增量目标检测（IOD），导致其泛化性不明确。在本文中，我们在复杂的域增量学习环境下分析了三种不同的基于提示的方法。我们还提供了一系列广泛的参考基线进行对比。从经验上看，我们在该环境下测试的基于提示的方法表现不佳。然而，一种强大而实用的方法，即将视觉提示与回放少量先前数据相结合，取得了最佳效果。此外，关于提示长度和初始化的附加实验为我们的研究提供了宝贵的见解，有助于推进基于提示的IL在IOD领域的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14599v2">PDF</a> Accepted to ICCV Workshops 2025: v2 update affiliation</p>
<p><strong>Summary</strong></p>
<p>视觉提示基方法在图像分类的增量学习（IL）中日益受到关注。这些方法在保持模型冻结的同时学习额外的嵌入向量，使训练效率提高。然而，先前的工作尚未将这些方法应用于增量目标检测（IOD），其通用性尚不清楚。本文在复杂的域增量学习环境下分析了三种不同的基于提示的方法。我们还提供了一系列广泛的参考基线用于比较。经验表明，我们在这种环境下测试的提示方法表现不佳。然而，一种强大而实用的方法，将视觉提示与回放少量先前数据相结合，取得了最佳效果。关于提示长度和初始化的附加实验为我们的研究提供了宝贵的见解，有助于推动基于提示的IL在IOD领域的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于视觉提示的方法在增量学习（IL）中受到关注，特别是在图像分类领域。</li>
<li>这些方法通过学习额外的嵌入向量并保持模型冻结来提高训练效率。</li>
<li>目前尚未有将基于视觉提示的方法应用于增量目标检测（IOD）的研究。</li>
<li>在复杂的域增量学习环境下，三种不同的基于提示的方法表现不佳。</li>
<li>结合视觉提示和回放少量先前数据的实用方法取得了最佳效果。</li>
<li>提示长度和初始化对基于提示的IL在IOD中的性能有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1ab82aceb54ba55831b2173e8b1adf5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041108&auth_key=1760041108-0-0-cbfd13e66def8fd3be93aa6ab86b23ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b90b287f234ea8745d64eb403e29c123~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041115&auth_key=1760041115-0-0-1355ec7a0fd0e24b116f1fdd626addb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11c83a212682860b2a7ae03d985b7e4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041121&auth_key=1760041121-0-0-50f59640ec6f8b4ef285372eca598b44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-29adab109d2ff96f12c6c5f5d3976616~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041128&auth_key=1760041128-0-0-8909f831173a4d8b50190fea354b6a6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e09a2f58baed776806d6e6012994969b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041135&auth_key=1760041135-0-0-014c67bf11e5798f44530bd7567cc1b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SurfDist-Interpretable-Three-Dimensional-Instance-Segmentation-Using-Curved-Surface-Patches"><a href="#SurfDist-Interpretable-Three-Dimensional-Instance-Segmentation-Using-Curved-Surface-Patches" class="headerlink" title="SurfDist: Interpretable Three-Dimensional Instance Segmentation Using   Curved Surface Patches"></a>SurfDist: Interpretable Three-Dimensional Instance Segmentation Using   Curved Surface Patches</h2><p><strong>Authors:Jackson Borchardt, Saul Kato</strong></p>
<p>We present SurfDist, a convolutional neural network architecture for three-dimensional volumetric instance segmentation. SurfDist enables prediction of instances represented as closed surfaces composed of smooth parametric surface patches, specifically bicubic B&#39;ezier triangles. SurfDist is a modification of the popular model architecture StarDist-3D which breaks StarDist-3D’s coupling of instance parameterization dimension and instance voxel resolution, and it produces predictions which may be upsampled to arbitrarily high resolutions without introduction of voxelization artifacts.   For datasets with blob-shaped instances, common in biomedical imaging, SurfDist can outperform StarDist-3D with more compact instance parameterizations. We detail SurfDist’s technical implementation and show one synthetic and one real-world dataset for which it outperforms StarDist-3D. These results demonstrate that interpretable instance surface models can be learned effectively alongside instance membership. </p>
<blockquote>
<p>我们提出了SurfDist，这是一种用于三维体积实例分割的卷积神经网络架构。SurfDist能够预测以光滑的参数曲面补丁（特别是双三次Bézier三角形）组成的闭合表面表示的实例。SurfDist是流行模型架构StarDist-3D的改进版，它打破了StarDist-3D实例参数化维度和实例体素分辨率之间的耦合，并产生预测结果，可将其上采样到任意高分辨率，而不会引入体素化伪影。对于在生物医学成像中常见的点状实例数据集，SurfDist可以使用更紧凑的实例参数化表现来超越StarDist-3D。我们详细介绍了SurfDist的技术实现，并展示了一个合成数据集和一个真实世界数据集，在这些数据集上它的表现超过了StarDist-3D。这些结果表明，可解释的实例表面模型可以与实例成员身份一起有效地学习。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08223v2">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>SurfDist是一种用于三维体积实例分割的卷积神经网络架构。它可预测以光滑参数曲面补丁（特别是三次贝塞尔三角形）组成的闭合表面表示的实例。SurfDist改进了流行的模型架构StarDist-3D，打破了实例参数化维度和实例体素分辨率之间的耦合，并可以产生可上采样到任意高分辨率的预测结果，而不会引入体素化伪影。对于在生物医学成像中常见的具有块状实例的数据集，SurfDist可以使用更紧凑的实例参数化来超越StarDist-3D的性能。我们详细描述了SurfDist的技术实现，并展示了一个合成数据集和一个真实世界数据集，在这些数据集上它的性能优于StarDist-3D。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SurfDist是一种用于三维体积实例分割的卷积神经网络架构。</li>
<li>SurfDist能够预测以光滑参数曲面补丁表示的闭合表面实例。</li>
<li>SurfDist改进了StarDist-3D模型，打破了实例参数化维度和实例体素分辨率之间的耦合。</li>
<li>SurfDist产生的预测结果可以上采样到任意高分辨率，且不会引入体素化伪影。</li>
<li>对于具有块状实例的数据集，SurfDist的性能优于StarDist-3D。</li>
<li>SurfDist的技术实现细节被详细介绍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08223">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-db00cc380bdf9e1c07529a08a402c7a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041143&auth_key=1760041143-0-0-e0646c768c4bf31ee511a821582c5c1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5868711c58ef210c55dd3bd32b09931~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041150&auth_key=1760041150-0-0-302337597a65efab3238f6ec26cf1df6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8572349630fea2871d2d76ecd059ef45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041156&auth_key=1760041156-0-0-60add6b2ffc305c0fb6463ceada7577a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-747a072682be514d5ac1424ad44d52c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041163&auth_key=1760041163-0-0-a859e7d58e0f66889ed3069fee59b388&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-444df6082424f1d8bc555a572011e46f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041170&auth_key=1760041170-0-0-1718a6cd2f51249282fbd6219236c9d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-086b1534ff60c6a870ec769f205e2178~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041176&auth_key=1760041176-0-0-ab44b9436f148f1fe6c6a348d2b06a27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6757c031dc01805834862e94a0853ee1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041183&auth_key=1760041183-0-0-535630c511318f196ac1af1634f1b561&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c855f682dd87df90ee22328c21f15027~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041189&auth_key=1760041189-0-0-32d1e79c41d23da5a82d4da481753ff3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d4c0f14e4a72201c4161b5c59928e56~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041196&auth_key=1760041196-0-0-d974d120be89581188c9127b8310b3f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Fully-Spiking-Neural-Networks-for-Unified-Frame-Event-Object-Tracking"><a href="#Fully-Spiking-Neural-Networks-for-Unified-Frame-Event-Object-Tracking" class="headerlink" title="Fully Spiking Neural Networks for Unified Frame-Event Object Tracking"></a>Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</h2><p><strong>Authors:Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu</strong></p>
<p>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency. </p>
<blockquote>
<p>将图像和事件流融合为实现复杂环境中稳健的视觉对象跟踪的有前途的方法。然而，当前的融合方法虽然性能较高，但计算开销较大，难以有效地从事件流中提取稀疏、异步信息，未能利用事件驱动脉冲范式的节能优势。为了应对这一挑战，我们提出了首个全脉冲帧事件跟踪框架，名为SpikeFET。该网络实现了脉冲范式内的卷积局部特征提取和基于Transformer的全局建模的协同融合，有效地融合了帧和事件数据。为了解决因卷积填充而导致的翻译不变性退化问题，我们引入了随机补丁模块（RPM），通过随机空间重组和可学习类型编码消除位置偏差，同时保留残差结构。此外，我们提出了一种时空正则化（STR）策略，通过强制潜在空间中时间模板特征的时空一致性，克服由不对称特征引起的相似度度量退化问题。在多个基准测试上的广泛实验表明，所提出的框架在跟踪精度上优于现有方法，同时显著降低了功耗，在性能和效率之间达到了最佳平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20834v2">PDF</a> Accepted by NeurIPS2025</p>
<p><strong>Summary</strong>：图像与事件流融合是实现复杂环境中稳健视觉对象跟踪的潜在方法。当前融合方法虽高性能但计算开销大，难以高效提取稀疏异步事件流信息，未能充分利用事件驱动脉冲模式的节能优势。为解决此挑战，我们提出首个全脉冲帧事件跟踪框架SpikeFET，实现卷积局部特征提取与基于Transformer全局建模的协同融合。为克服卷积填充引起的翻译不变性退化，引入随机补丁模块RPM，通过随机空间重组和学习类型编码保留残差结构。此外，提出时空正则化策略STR，通过潜在空间中时空模板特征的时空一致性克服不对称特征的相似性度量退化。实验证明，该方法在多个基准测试上实现优越跟踪性能，显著降低功耗，实现性能与效率之间的优化平衡。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>图像和事件流的融合对于实现复杂环境中的稳健视觉对象跟踪具有潜力。</li>
<li>当前融合方法计算开销大，难以高效处理稀疏、异步的事件流信息。</li>
<li>SpikeFET框架实现卷积局部特征提取与基于Transformer的全局建模的协同融合，有效融合帧和事件数据。</li>
<li>随机补丁模块RPM消除位置偏差，通过随机空间重组和可学习类型编码保留残差结构。</li>
<li>时空正则化策略STR克服不对称特征的相似性度量退化，通过潜在空间中时空模板特征的时空一致性增强性能。</li>
<li>提出的框架在多个基准测试上实现优越跟踪性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bdf6b0af6d8c8769bf8e56cce878dfb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041203&auth_key=1760041203-0-0-b1114bb09cd1fa14de9d245898e2e457&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd4a2f890f75dadbdda55f33afc97191~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041210&auth_key=1760041210-0-0-90614220cc05b50fa62774288687beb7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd7c976ad66f94d5061b22c349e264f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041217&auth_key=1760041217-0-0-f6db930cf9aa84a2b45360445709f444&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a6ff0680de9e1ace7b6d1832e9891fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041224&auth_key=1760041224-0-0-e42b23c843b1945d5022618cd9958dd4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 17 mAP! Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl">https://github.com/roboflow/rf100-vl</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a>. </p>
<blockquote>
<p>基于互联网规模数据训练的视觉语言模型（VLMs）在常见对象（如汽车、卡车和行人）的零样本检测性能上取得了显著的成果。然而，最先进的模型仍然难以推广到其预训练中没有出现的类别、任务和成像模式。我们主张，与其简单地使用更多的视觉数据重新训练VLMs，不如将VLM与包含少量视觉示例和丰富文本描述的新概念标注指令对齐。为此，我们推出了Roboflow100-VL，这是一个包含一百个多模态对象检测数据集的大规模集合，其中包含的概念在VLM预训练中并不常见。我们在我们的基准测试上对最先进的模型进行了零样本、少样本、半监督和全监督的设置评估，允许跨数据领域进行比较。值得注意的是，我们发现像GroundingDINO和Qwen2.5-VL这样的VLM在Roboflow100-VL中的挑战性医学成像数据集上的零样本准确率低于百分之二，这显示了少样本概念对齐的需求。最后，我们讨论了近期CVPR 2025的基础FSOD竞赛并与社区分享了一些见解。值得一提的是，冠军团队在我们的基线基础上提高了17 mAP！我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl%E5%92%8Chttps://universe.roboflow.com/rf100-vl/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/roboflow/rf100-vl和https://universe.roboflow.com/rf100-vl/获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v3">PDF</a> The first two authors contributed equally. This work has been   accepted to the Neural Information Processing Systems (NeurIPS) 2025 Datasets   &amp; Benchmark Track. Project Page: <a target="_blank" rel="noopener" href="https://rf100-vl.org/">https://rf100-vl.org/</a></p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了在大型数据集上训练的视觉语言模型（VLMs）在零样本检测任务上的表现。尽管它们可以在通用对象上实现出色的性能，但在面对超出分布范围的新类别、任务和成像模式时，它们通常无法很好地泛化。作者提出了一种通过标注指令与少量视觉示例和丰富的文本描述来对齐VLMs的新概念的方法。为了评估模型性能，作者引入了Roboflow100-VL基准测试集，其中包括各种在VLM预训练中很少遇到的多样概念。文章详细分析了现有模型在各种不同场景下的性能表现，并对Roboflow的挑战结果进行了讨论。最后，作者分享了数据集和代码资源。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>VLMs在零样本检测任务上表现出色，但在面对超出分布范围的新类别和任务时泛化能力受限。</li>
<li>Roboflow100-VL是一个大型多模态对象检测数据集，包含多种不常见于VLM预训练的概念。</li>
<li>仅通过增加视觉数据重新训练VLMs不足以提高对新概念的泛化能力。</li>
<li>对齐VLMs到新概念的一种方法是使用包含少量视觉示例和丰富文本描述的标注指令。</li>
<li>在零样本、少样本、半监督以及全监督环境下评估模型性能，有助于比较不同数据环境下的模型表现。</li>
<li>在Roboflow的挑战中，采用GroundingDINO和Qwen2.5-VL等VLMs在某些医学图像数据集上的零样本准确度低于百分之二（精度较差）。这表明了对齐少量概念的需要。另外还分享了社区参与的CVPR 2025 FSOD竞赛的结果。最终的赢家超越基准分数17 mAP的好成绩！欢迎访问我们开源的数据集和代码链接<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl">https://github.com/roboflow/rf100-vl</a> 和 <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/%E3%80%82">https://universe.roboflow.com/rf100-vl/。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8425046653ec2d2d25050c898fcec42d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041232&auth_key=1760041232-0-0-83bee0d4ce97485eef8a5c583c80a0d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39f97d32ec01bf4be40727f5d59a3bbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041239&auth_key=1760041239-0-0-cdb36d38a994e421683bb6eed06ea7c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94525ec6f3d591f43894fc2c1a976188~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041246&auth_key=1760041246-0-0-52b1a7769fdeb2caef98d76758383632&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0731706e142f12ec7985fe7ab5630eda~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041253&auth_key=1760041253-0-0-0c404b52c260fb4f51394cd3a52d93ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d3cea226c0073c3c69f1cb43dcf6bcd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041259&auth_key=1760041259-0-0-4cc8dcd08f2be788b0079c3df8c63e89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5efc4abbd8b1bfc8bfe9d36ce9cd2d36~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041266&auth_key=1760041266-0-0-936ca364cffd24edc8cccaf9bd76d926&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Leveraging-Confident-Image-Regions-for-Source-Free-Domain-Adaptive-Object-Detection"><a href="#Leveraging-Confident-Image-Regions-for-Source-Free-Domain-Adaptive-Object-Detection" class="headerlink" title="Leveraging Confident Image Regions for Source-Free Domain-Adaptive   Object Detection"></a>Leveraging Confident Image Regions for Source-Free Domain-Adaptive   Object Detection</h2><p><strong>Authors:Mohamed Lamine Mekhalfi, Davide Boscaini, Fabio Poiesi</strong></p>
<p>Source-free domain-adaptive object detection is an interesting but scarcely addressed topic. It aims at adapting a source-pretrained detector to a distinct target domain without resorting to source data during adaptation. So far, there is no data augmentation scheme tailored to source-free domain-adaptive object detection. To this end, this paper presents a novel data augmentation approach that cuts out target image regions where the detector is confident, augments them along with their respective pseudo-labels, and joins them into a challenging target image to adapt the detector. As the source data is out of reach during adaptation, we implement our approach within a teacher-student learning paradigm to ensure that the model does not collapse during the adaptation procedure. We evaluated our approach on three adaptation benchmarks of traffic scenes, scoring new state-of-the-art on two of them. </p>
<blockquote>
<p>无源域自适应目标检测是一个有趣但鲜有研究的话题。它的目标是将源预训练检测器适应到不同的目标域，而在适应过程中不依赖源数据。迄今为止，还没有针对无源域自适应目标检测定制的数据增强方案。为此，本文提出了一种新的数据增强方法，该方法会裁剪出目标图像中检测器信心充足的区域，对这些区域及其相应的伪标签进行增强，并将其合并到具有挑战性的目标图像中，以适应检测器。由于适应过程中无法访问源数据，我们在师生学习范式内实施我们的方法，以确保模型在适应过程中不会崩溃。我们在三个交通场景的自适应基准测试集上对我们的方法进行了评估，并在其中两个基准测试集上取得了最新状态。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10081v2">PDF</a> </p>
<p><strong>Summary</strong><br>目标检测自适应在不同领域之间有着重要的应用，本文提出了一种无源数据增强方法，通过裁剪目标图像区域并进行增强，以适应目标检测器。采用教师学生学习范式确保模型在适应过程中不会崩溃，并在交通场景的三个自适应基准测试中取得了新的成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>源自由域自适应目标检测是一个有趣但尚未充分解决的问题，目的是将一个预训练检测器适应到一个特定的目标域，且不需要使用源数据进行适应。</li>
<li>本文提出了一种新的数据增强方法，专门针对源自由域自适应目标检测设计。</li>
<li>该方法通过裁剪目标图像区域并进行增强，同时配合伪标签，将这些区域合并成一个具有挑战性的目标图像，以适应检测器。</li>
<li>由于在适应过程中无法获取源数据，因此采用了教师学生学习范式来确保模型不会崩溃。</li>
<li>该方法在交通场景的三个自适应基准测试中进行了评估，并在其中两个基准测试中取得了新的最佳成绩。</li>
<li>此研究为解决目标检测在不同领域间的自适应问题提供了新的视角和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10081">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-487a33f367d434ab0b5e3c85c3b0049a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041273&auth_key=1760041273-0-0-f9a6b53fde854d26bd6d8eba8617394e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eba8d9bcca034326ddef6cb6cc8377a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041281&auth_key=1760041281-0-0-3b5cda2adec0028e46913ac0b0d96d1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7b3ce7be15445dd900f2167945a8f23~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041288&auth_key=1760041288-0-0-25ec67365d7ec2dbc150ef46f4b7458d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7b1126bae1edfa50a2de79159b0b9ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041295&auth_key=1760041295-0-0-5367e6d95c760d231f54c9829ac35610&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-39f8493578ee364bc197068b17f4e99b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760041662&auth_key=1760041662-0-0-542f3e7355fb7ba99d365b1e1eec2f61&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-10-10  DiffMI Breaking Face Recognition Privacy via Diffusion-Driven   Training-Free Model Inversion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-01f14e51fb37b42c86f429d230077784~resize:0:q75.jpg?source=1f5c5e47&expiration=1760039898&auth_key=1760039898-0-0-0676ac706508e704f6ac42180d7399ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-10-10  Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema   Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
