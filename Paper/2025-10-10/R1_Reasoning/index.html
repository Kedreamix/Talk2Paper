<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  LeMAJ (Legal LLM-as-a-Judge) Bridging Legal Reasoning and LLM   Evaluation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d2897c8fa1f46a4fcd6c88e66d56fe68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082573&auth_key=1760082573-0-0-84dce57a4322036d03a148160941ef91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    23.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    94 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-10-æ›´æ–°"><a href="#2025-10-10-æ›´æ–°" class="headerlink" title="2025-10-10 æ›´æ–°"></a>2025-10-10 æ›´æ–°</h1><h2 id="LeMAJ-Legal-LLM-as-a-Judge-Bridging-Legal-Reasoning-and-LLM-Evaluation"><a href="#LeMAJ-Legal-LLM-as-a-Judge-Bridging-Legal-Reasoning-and-LLM-Evaluation" class="headerlink" title="LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM   Evaluation"></a>LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM   Evaluation</h2><p><strong>Authors:Joseph Enguehard, Morgane Van Ermengem, Kate Atkinson, Sujeong Cha, Arijit Ghosh Chowdhury, Prashanth Kallur Ramaswamy, Jeremy Roghair, Hannah R Marlowe, Carina Suzana Negreanu, Kitty Boxall, Diana Mincu</strong></p>
<p>Evaluating large language model (LLM) outputs in the legal domain presents unique challenges due to the complex and nuanced nature of legal analysis. Current evaluation approaches either depend on reference data, which is costly to produce, or use standardized assessment methods, both of which have significant limitations for legal applications.   Although LLM-as-a-Judge has emerged as a promising evaluation technique, its reliability and effectiveness in legal contexts depend heavily on evaluation processes unique to the legal industry and how trustworthy the evaluation appears to the human legal expert. This is where existing evaluation methods currently fail and exhibit considerable variability.   This paper aims to close the gap: a) we break down lengthy responses into â€˜Legal Data Pointsâ€™ (LDPs), self-contained units of information, and introduce a novel, reference-free evaluation methodology that reflects how lawyers evaluate legal answers; b) we demonstrate that our method outperforms a variety of baselines on both our proprietary dataset and an open-source dataset (LegalBench); c) we show how our method correlates more closely with human expert evaluations and helps improve inter-annotator agreement; and finally d) we open source our Legal Data Points for a subset of LegalBench used in our experiments, allowing the research community to replicate our results and advance research in this vital area of LLM evaluation on legal question-answering. </p>
<blockquote>
<p>è¯„ä¼°æ³•å¾‹é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºç»“æœé¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºæ³•å¾‹åˆ†æçš„å¤æ‚æ€§å’Œç»†å¾®å·®åˆ«ã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•è¦ä¹ˆä¾èµ–äºå‚è€ƒæ•°æ®ï¼ˆæˆæœ¬é«˜æ˜‚ï¼‰ï¼Œè¦ä¹ˆä½¿ç”¨æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œä¸¤è€…åœ¨æ³•å¾‹åº”ç”¨ä¸­éƒ½æœ‰æ˜¾è‘—å±€é™æ€§ã€‚è™½ç„¶â€œLLMä½œä¸ºæ³•å®˜â€å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è¯„ä¼°æŠ€æœ¯ï¼Œä½†å…¶åœ¨æ³•å¾‹ç¯å¢ƒä¸­çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ³•å¾‹è¡Œä¸šç‰¹æœ‰çš„è¯„ä¼°æµç¨‹ä»¥åŠäººç±»æ³•å¾‹ä¸“å®¶å¯¹è¯„ä¼°çš„ä¿¡ä»»åº¦ã€‚è¿™æ­£æ˜¯ç°æœ‰è¯„ä¼°æ–¹æ³•ç›®å‰å¤±è´¥å¹¶è¡¨ç°å‡ºç›¸å½“å¤§çš„å¯å˜æ€§çš„åœ°æ–¹ã€‚æœ¬æ–‡æ—¨åœ¨ç¼©å°è¿™ä¸€å·®è·ï¼šaï¼‰æˆ‘ä»¬å°†å†—é•¿çš„å›åº”åˆ†è§£ä¸ºâ€œæ³•å¾‹æ•°æ®ç‚¹â€ï¼ˆLDPsï¼‰ï¼Œå³ç‹¬ç«‹çš„ä¿¡æ¯å•å…ƒï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ— å‚è€ƒè¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åæ˜ äº†å¾‹å¸ˆå¦‚ä½•è¯„ä¼°æ³•å¾‹ç­”æ¡ˆï¼›bï¼‰æˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨æˆ‘ä»¬çš„ä¸“æœ‰æ•°æ®é›†å’Œå¼€æºæ•°æ®é›†ï¼ˆLegalBenchï¼‰ä¸Šä¼˜äºå„ç§åŸºçº¿ï¼›cï¼‰æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•å¦‚ä½•æ›´ç´§å¯†åœ°ä¸äººç±»ä¸“å®¶è¯„ä¼°ç›¸å…³ï¼Œå¹¶æœ‰åŠ©äºæé«˜è·¨æ³¨é‡Šå™¨çš„ä¸€è‡´æ€§ï¼›æœ€ådï¼‰æˆ‘ä»¬å…¬å¼€äº†å®éªŒä¸­ä½¿ç”¨çš„LegalBenchå­é›†çš„Legal Data Pointsï¼Œè®©ç ”ç©¶ç•Œèƒ½å¤Ÿå¤åˆ¶æˆ‘ä»¬çš„ç»“æœå¹¶æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„LLMè¯„ä¼°ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07243v1">PDF</a> Published in Natural Legal Language Processing - EMNLP Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹æ³•å¾‹é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹è¾“å‡ºçš„è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€æ— éœ€å‚è€ƒçš„è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†å†—é•¿çš„ç­”æ¡ˆåˆ†è§£ä¸ºâ€œæ³•å¾‹æ•°æ®ç‚¹â€ï¼ˆLDPsï¼‰ï¼Œåæ˜ å¾‹å¸ˆå¦‚ä½•è¯„ä¼°æ³•å¾‹ç­”æ¡ˆï¼Œå¹¶åœ¨è‡ªæœ‰æ•°æ®é›†å’Œå¼€æºæ•°æ®é›†ï¼ˆLegalBenchï¼‰ä¸Šè¡¨ç°å‡ºä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ›´è´´è¿‘äººç±»ä¸“å®¶è¯„ä¼°ï¼Œæé«˜äº†æ ‡æ³¨è€…é—´çš„ä¸€è‡´æ€§ï¼Œå¹¶å…¬å¼€å®éªŒä¸­ä½¿ç”¨çš„ä¸€éƒ¨åˆ†LegalBenchçš„æ³•å¾‹æ•°æ®ç‚¹ï¼Œä¾›ç ”ç©¶ç¤¾åŒºå¤åˆ¶ç»“æœå¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ³•å¾‹é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºçš„è¯„ä¼°é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› æ³•å¾‹åˆ†æå…·æœ‰å¤æ‚æ€§å’Œç»†å¾®å·®åˆ«ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„å‚è€ƒæ•°æ®æˆ–ä½¿ç”¨æ ‡å‡†åŒ–è¯„ä¼°æ–¹æ³•ï¼Œåœ¨æ³•å¾‹åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—å±€é™æ€§ã€‚</li>
<li>LLM-as-a-JudgeæŠ€æœ¯è™½ç„¶å…·æœ‰å‰æ™¯ï¼Œä½†å…¶å¯é æ€§å’Œæœ‰æ•ˆæ€§å–å†³äºæ³•å¾‹è¡Œä¸šç‹¬ç‰¹çš„è¯„ä¼°è¿‡ç¨‹ä»¥åŠè¯„ä¼°ç»“æœå¯¹äººç±»ç¤¾ä¼šæ³•å¾‹ä¸“å®¶çš„å¯ä¿¡åº¦ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€å‚è€ƒçš„è¯„ä¼°æ–¹æ³•ï¼Œå°†ç­”æ¡ˆåˆ†è§£ä¸ºâ€œæ³•å¾‹æ•°æ®ç‚¹â€ï¼ˆLDPsï¼‰ï¼Œä»¥åæ˜ å¾‹å¸ˆå¦‚ä½•è¯„ä¼°æ³•å¾‹ç­”æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è‡ªæœ‰æ•°æ®é›†å’Œå¼€æºæ•°æ®é›†ï¼ˆLegalBenchï¼‰ä¸Šçš„æ€§èƒ½ä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•æ›´è´´è¿‘äººç±»ä¸“å®¶è¯„ä¼°ï¼Œæé«˜äº†æ ‡æ³¨è€…é—´çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e9881513434033de2c736e71b788b724~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031800&auth_key=1760031800-0-0-fd9997e65923b3d6ded9fd4a458070cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-278e2a1a0158b4fe7147207f521d2f19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082579&auth_key=1760082579-0-0-d9ad44c431781e57e68396756dbe8c98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a75decafde37a656d0b65e14ade3901~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082586&auth_key=1760082586-0-0-17bfa5bef14509b9dd6eb494930a8683&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e0e73781a8531fa428ba3378f733c90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082593&auth_key=1760082593-0-0-7c52fdca336278e67b6766bb7ac279fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hybrid-Reinforcement-When-Reward-Is-Sparse-Itâ€™s-Better-to-Be-Dense"><a href="#Hybrid-Reinforcement-When-Reward-Is-Sparse-Itâ€™s-Better-to-Be-Dense" class="headerlink" title="Hybrid Reinforcement: When Reward Is Sparse, Itâ€™s Better to Be Dense"></a>Hybrid Reinforcement: When Reward Is Sparse, Itâ€™s Better to Be Dense</h2><p><strong>Authors:Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu, Yixuan Li, Jason E Weston, Ping Yu</strong></p>
<p>Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittleâ€“many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning. </p>
<blockquote>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†è®­ç»ƒè¶Šæ¥è¶Šä¾èµ–äºå¯éªŒè¯çš„å¥–åŠ±ï¼šæä¾›0-1æ­£ç¡®æ€§ä¿¡å·çš„ç¡®å®šæ€§æ£€æŸ¥å™¨ã€‚è™½ç„¶å¯é ï¼Œä½†è¿™æ ·çš„äºŒå…ƒåé¦ˆæ˜¯è„†å¼±çš„â€”â€”è®¸å¤šä»»åŠ¡æ¥å—éƒ¨åˆ†æ­£ç¡®æˆ–æ›¿ä»£ç­”æ¡ˆï¼Œè¿™äº›ç­”æ¡ˆä¼šè¢«éªŒè¯å™¨ä½ä¼°ï¼Œè€Œç”±æ­¤äº§ç”Ÿçš„å…¨æœ‰å…¨æ— çš„ç›‘ç£é™åˆ¶äº†å­¦ä¹ ã€‚å¥–åŠ±æ¨¡å‹æä¾›äº†æ›´ä¸°å¯Œçš„è¿ç»­åé¦ˆï¼Œå¯ä»¥ä½œä¸ºéªŒè¯å™¨çš„è¡¥å……ç›‘ç£ä¿¡å·ã€‚æˆ‘ä»¬å¼•å…¥äº†HEROï¼ˆæ··åˆé›†æˆå¥–åŠ±ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥ç»“æ„åŒ–çš„æ–¹å¼å°†éªŒè¯å™¨ä¿¡å·ä¸å¥–åŠ±æ¨¡å‹åˆ†æ•°è¿›è¡Œé›†æˆã€‚HEROé‡‡ç”¨åˆ†å±‚å½’ä¸€åŒ–ï¼Œåœ¨éªŒè¯å™¨å®šä¹‰çš„ç»„å†…éƒ¨å¯¹å¥–åŠ±æ¨¡å‹åˆ†æ•°è¿›è¡Œçº¦æŸï¼Œåœ¨ä¿æŒæ­£ç¡®æ€§çš„åŒæ—¶æé«˜è´¨é‡åŒºåˆ†åº¦ï¼Œå¹¶é‡‡ç”¨æ–¹å·®æ„ŸçŸ¥åŠ æƒæ³•æ¥å¼ºè°ƒå¯†é›†ä¿¡å·æœ€é‡è¦çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºã€‚åœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHEROå§‹ç»ˆä¼˜äºä»…ä½¿ç”¨RMå’Œä»…ä½¿ç”¨éªŒè¯å™¨çš„åŸºçº¿ï¼Œåœ¨å¯éªŒè¯å’Œéš¾ä»¥éªŒè¯çš„ä»»åŠ¡ä¸Šéƒ½æœ‰æ˜¾è‘—çš„æå‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ··åˆå¥–åŠ±è®¾è®¡æ—¢ä¿ç•™äº†éªŒè¯å™¨çš„ç¨³å®šæ€§ï¼Œåˆåˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„ç»†å¾®å·®åˆ«æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07242v1">PDF</a> 20 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†åè®­ç»ƒè¶Šæ¥è¶Šä¾èµ–äºå¯éªŒè¯çš„å¥–åŠ±ï¼Œå³æä¾›0-1æ­£ç¡®æ€§çš„ç¡®å®šæ€§æ£€æŸ¥å™¨ã€‚è™½ç„¶å¯é ï¼Œä½†äºŒè¿›åˆ¶åé¦ˆæ˜¯è„†å¼±çš„â€”â€”è®¸å¤šä»»åŠ¡å­˜åœ¨éƒ¨åˆ†æ­£ç¡®æˆ–æ›¿ä»£ç­”æ¡ˆï¼ŒéªŒè¯å™¨æ— æ³•ç»™äºˆå……åˆ†è®¤å¯ï¼Œè€Œä¸”å…¨æœ‰å…¨æ— çš„ç›‘ç£æ–¹å¼é™åˆ¶äº†å­¦ä¹ ã€‚å¥–åŠ±æ¨¡å‹æä¾›ä¸°å¯Œçš„è¿ç»­åé¦ˆï¼Œå¯ä»¥ä½œä¸ºéªŒè¯å™¨çš„è¾…åŠ©ç›‘ç£ä¿¡å·ã€‚æœ¬æ–‡ä»‹ç»äº†HEROï¼ˆæ··åˆé›†æˆå¥–åŠ±ä¼˜åŒ–ï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥ç»“æ„åŒ–çš„æ–¹å¼å°†éªŒè¯å™¨ä¿¡å·ä¸å¥–åŠ±æ¨¡å‹åˆ†æ•°è¿›è¡Œé›†æˆã€‚HEROé‡‡ç”¨åˆ†å±‚å½’ä¸€åŒ–ï¼Œåœ¨éªŒè¯å™¨å®šä¹‰çš„ç»„å†…éƒ¨å¯¹å¥–åŠ±æ¨¡å‹åˆ†æ•°è¿›è¡Œçº¦æŸï¼Œæ—¢ä¿æŒæ­£ç¡®æ€§åˆç»†åŒ–è´¨é‡å·®å¼‚ï¼Œå¹¶é‡‡ç”¨æ–¹å·®æ„ŸçŸ¥åŠ æƒæ³•ï¼Œå¼ºè°ƒæç¤ºå¯†é›†ä¿¡å·æœ€ä¸ºå…³é”®çš„éš¾é¢˜ã€‚åœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHEROå§‹ç»ˆä¼˜äºä»…ä½¿ç”¨å¥–åŠ±æ¨¡å‹æˆ–ä»…ä½¿ç”¨éªŒè¯å™¨çš„åŸºå‡†æµ‹è¯•ï¼Œåœ¨å¯éªŒè¯å’Œéš¾ä»¥éªŒè¯çš„ä»»åŠ¡ä¸Šéƒ½æœ‰æ˜¾è‘—çš„æå‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ··åˆå¥–åŠ±è®¾è®¡åœ¨ä¿æŒéªŒè¯å™¨ç¨³å®šæ€§çš„åŒæ—¶ï¼Œåˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„ç»†å¾®å·®åˆ«æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†åè®­ç»ƒä¾èµ–å¯éªŒè¯çš„å¥–åŠ±æ¥æä¾›ç¡®å®šæ€§æ£€æŸ¥ã€‚</li>
<li>äºŒè¿›åˆ¶åé¦ˆè™½ç„¶å¯é ï¼Œä½†å­˜åœ¨å¯¹éƒ¨åˆ†æ­£ç¡®æˆ–æ›¿ä»£ç­”æ¡ˆçš„å¿½è§†é—®é¢˜ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹æä¾›ä¸°å¯Œçš„è¿ç»­åé¦ˆï¼Œä½œä¸ºéªŒè¯å™¨çš„è¾…åŠ©ç›‘ç£ä¿¡å·ã€‚</li>
<li>HEROæ¡†æ¶é›†æˆäº†éªŒè¯å™¨ä¿¡å·ä¸å¥–åŠ±æ¨¡å‹åˆ†æ•°ï¼Œé€šè¿‡åˆ†å±‚å½’ä¸€åŒ–å’Œæ–¹å·®æ„ŸçŸ¥åŠ æƒæ³•ä¼˜åŒ–å­¦ä¹ ã€‚</li>
<li>HEROåœ¨å¤šç§æ•°å­¦æ¨ç†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¯¹å¯éªŒè¯å’Œéš¾ä»¥éªŒè¯çš„ä»»åŠ¡éƒ½æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>æ··åˆå¥–åŠ±è®¾è®¡åœ¨ä¿æŒéªŒè¯å™¨ç¨³å®šæ€§çš„åŒæ—¶ï¼Œåˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„ç»†å¾®å·®åˆ«æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-eab3fe9c0a26f5bc46df89c3af6bd138~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082600&auth_key=1760082600-0-0-25c84911fccaf88702421fbfd1084a45&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6bcae09e84ff54658e3a7d5576930efe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082607&auth_key=1760082607-0-0-9bce0dff81a26b697a6f7c8f7493a085&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Benchmarking-LLM-Causal-Reasoning-with-Scientifically-Validated-Relationships"><a href="#Benchmarking-LLM-Causal-Reasoning-with-Scientifically-Validated-Relationships" class="headerlink" title="Benchmarking LLM Causal Reasoning with Scientifically Validated   Relationships"></a>Benchmarking LLM Causal Reasoning with Scientifically Validated   Relationships</h2><p><strong>Authors:Donggyu Lee, Sungwon Park, Yerin Hwang, Hyunwoo Oh, Hyoshin Kim, Jungwon Kim, Meeyoung Cha, Sangyoon Park, Jihee Kim</strong></p>
<p>Causal reasoning is fundamental for Large Language Models (LLMs) to understand genuine cause-and-effect relationships beyond pattern matching. Existing benchmarks suffer from critical limitations such as reliance on synthetic data and narrow domain coverage. We introduce a novel benchmark constructed from casually identified relationships extracted from top-tier economics and finance journals, drawing on rigorous methodologies including instrumental variables, difference-in-differences, and regression discontinuity designs. Our benchmark comprises 40,379 evaluation items covering five task types across domains such as health, environment, technology, law, and culture. Experimental results on eight state-of-the-art LLMs reveal substantial limitations, with the best model achieving only 57.6% accuracy. Moreover, model scale does not consistently translate to superior performance, and even advanced reasoning models struggle with fundamental causal relationship identification. These findings underscore a critical gap between current LLM capabilities and demands of reliable causal reasoning in high-stakes applications. </p>
<blockquote>
<p>å› æœæ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç†è§£è¶…è¶Šæ¨¡å¼åŒ¹é…çš„çœŸæ­£å› æœå…³ç³»è‡³å…³é‡è¦ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å…³é”®å±€é™æ€§ï¼Œä¾‹å¦‚ä¾èµ–åˆæˆæ•°æ®å’Œé¢†åŸŸè¦†ç›–èŒƒå›´ç‹­çª„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ˜¯ä»é¡¶çº§ç»æµå­¦å’Œé‡‘èæœŸåˆŠä¸­æå–çš„å› æœå…³ç³»æ„å»ºçš„ï¼Œå€Ÿé‰´äº†åŒ…æ‹¬å·¥å…·å˜é‡ã€å·®å¼‚ä¸­çš„å·®å¼‚å’Œå›å½’æ–­ç‚¹è®¾è®¡åœ¨å†…çš„ä¸¥æ ¼æ–¹æ³•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…å«40379ä¸ªè¯„ä¼°é¡¹ç›®ï¼Œæ¶µç›–å¥åº·ã€ç¯å¢ƒã€æŠ€æœ¯ã€æ³•å¾‹å’Œæ–‡åŒ–ç­‰é¢†åŸŸçš„äº”ç§ä»»åŠ¡ç±»å‹ã€‚åœ¨å…«ç§æœ€å…ˆè¿›çš„LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œæœ€ä½³æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º57.6%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è§„æ¨¡å¹¶ä¸ä¸€è‡´åœ°è½¬åŒ–ä¸ºæ›´å¥½çš„æ€§èƒ½ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„æ¨ç†æ¨¡å‹åœ¨åŸºæœ¬å› æœå…³ç³»è¯†åˆ«æ–¹é¢ä¹Ÿé¢ä¸´å›°éš¾ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰LLMèƒ½åŠ›ä¸é«˜é£é™©åº”ç”¨ä¸­å¯é å› æœæ¨ç†éœ€æ±‚ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07231v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£çœŸå®çš„å› æœå…³ç³»è€Œéç®€å•çš„æ¨¡å¼åŒ¹é…æ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œå¦‚ä¾èµ–åˆæˆæ•°æ®å’Œé¢†åŸŸè¦†ç›–æœ‰é™ã€‚æœ¬ç ”ç©¶æ¨å‡ºäº†ä¸€ç§åŸºäºç»æµå­¦å’Œé¡¶å°–é‡‘èæœŸåˆŠä¸­ç»è¿‡å› æœåˆ†æéªŒè¯çš„å…³ç³»çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨ä»ªå™¨å˜é‡ã€å·®å¼‚å·®å¼‚å’Œå›å½’æ–­è£‚ç‚¹è®¾è®¡ç­‰ä¸¥è°¨æ–¹æ³•ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†å¥åº·ã€ç¯å¢ƒã€æŠ€æœ¯ã€æ³•å¾‹å’Œæ–‡åŒ–ç­‰äº”ä¸ªä»»åŠ¡ç±»å‹ï¼Œå…±åŒ…å«è¯„ä¼°é¡¹ç›®40,379é¡¹ã€‚å¯¹å…«ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ°çº¦ä¸€åŠçš„å‡†ç¡®æ€§ï¼ˆä»…ä¸ºå¤§çº¦åŠæ•°ï¼‰ã€‚è€Œä¸”æ¨¡å‹è§„æ¨¡å¹¶ä¸ä¸€å®šå¯¼è‡´æ€§èƒ½æé«˜ï¼Œå³ä½¿åœ¨å¤æ‚çš„æ¨ç†æ¨¡å‹ä¸­ï¼Œå¯¹åŸºæœ¬å› æœå…³ç³»çš„è¯†åˆ«ä¹Ÿæœ‰å¾ˆå¤§çš„å›°éš¾ã€‚è¿™è¡¨æ˜å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„å¯é å› æœæ¨ç†èƒ½åŠ›è¿˜å­˜åœ¨å¾ˆå¤§çš„å·®è·ã€‚è¯¥æ€»ç»“ä»æ•´ä¸ªæ–‡æœ¬çš„è§’åº¦æ¦‚æ‹¬äº†ä¸»è¦å†…å®¹å’Œæ ¸å¿ƒè§‚ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦è¶…è¶Šæ¨¡å¼åŒ¹é…ä»¥ç†è§£çœŸå®çš„å› æœå…³ç³»ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨ç¼ºé™·ï¼Œå¦‚ä¾èµ–åˆæˆæ•°æ®å’Œé¢†åŸŸè¦†ç›–æœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŸºäºç»æµå­¦å’Œé¡¶å°–é‡‘èæœŸåˆŠä¸­çš„å› æœå…³ç³»æ•°æ®ã€‚</li>
<li>æ–°åŸºå‡†æµ‹è¯•åŒ…å«å¤šç§ä»»åŠ¡ç±»å‹ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼ŒåŒ…å«å¤§é‡è¯„ä¼°é¡¹ç›®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœæ¨ç†æ–¹é¢çš„å‡†ç¡®æ€§æœ‰é™ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å¹¶ä¸æ€»æ˜¯å¯¼è‡´æ€§èƒ½æé«˜ï¼Œå°¤å…¶åœ¨åŸºæœ¬çš„å› æœå…³ç³»è¯†åˆ«æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9ff75149eca8c90e3044698e93cb16c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082616&auth_key=1760082616-0-0-80c56891aa533cc2d8b3ade273f278f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-236f1f6199f694223ea79d2243d2c3e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082623&auth_key=1760082623-0-0-caf9935ad2dcf121d47491fc1917af6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ab9178e0ad1b4170526fd402938905b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082630&auth_key=1760082630-0-0-373d1d78f7ea57d22c423390e3b49e03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c6a6daa2725786f6612bcb5977dba4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082637&auth_key=1760082637-0-0-e5e1e6bc713615caeb1c09ba7cc1d244&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3581ddad77f5ddd58eb332963c6991b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082644&auth_key=1760082644-0-0-ce54bcd39b2b4299df9f27b0f2fd2b42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-821b009b78b7e1c91bf8f436dda31780~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082651&auth_key=1760082651-0-0-e8d10b70fdf6d3880be0e1f5300f02ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Customer-R1-Personalized-Simulation-of-Human-Behaviors-via-RL-based-LLM-Agent-in-Online-Shopping"><a href="#Customer-R1-Personalized-Simulation-of-Human-Behaviors-via-RL-based-LLM-Agent-in-Online-Shopping" class="headerlink" title="Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM   Agent in Online Shopping"></a>Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM   Agent in Online Shopping</h2><p><strong>Authors:Ziyi Wang, Yuxuan Lu, Yimeng Zhang, Jing Huang, Dakuo Wang</strong></p>
<p>Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a userâ€™s persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches usersâ€™ action distribution, indicating higher fidelity in personalized behavior simulation. </p>
<blockquote>
<p>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿåˆ†æ­¥éª¤çš„äººç±»è¡Œä¸ºå·²æˆä¸ºæ–°å…´çš„ç ”ç©¶æ–¹å‘ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§å®é™…é¢†åŸŸã€‚è™½ç„¶å…ˆå‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æç¤ºã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œåœ¨æ¨¡æ‹Ÿåˆ†æ­¥éª¤è¡Œä¸ºæ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬ä¸»è¦å­¦ä¹ ç¾¤ä½“å±‚é¢çš„ç­–ç•¥ï¼Œè€Œä¸è€ƒè™‘ç”¨æˆ·çš„ä¸ªæ€§ï¼Œä»è€Œäº§ç”Ÿé€šç”¨è€Œéä¸ªæ€§åŒ–çš„æ¨¡æ‹Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šå¦‚ä½•ä½¿LLMä»£ç†æ›´å¥½åœ°æ¨¡æ‹Ÿä¸ªæ€§åŒ–ç”¨æˆ·è¡Œä¸ºï¼Ÿæˆ‘ä»¬ä»‹ç»äº†Customer-R1ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„åœ¨çº¿è´­ç‰©ç¯å¢ƒä¸­ä¸ªæ€§åŒ–åˆ†æ­¥éª¤ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç­–ç•¥ä¾èµ–äºæ˜ç¡®çš„ä¸ªæ€§ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¡ŒåŠ¨æ­£ç¡®æ€§å¥–åŠ±ä¿¡å·ä¼˜åŒ–ä¸‹ä¸€æ­¥çš„åˆç†æ€§è¡ŒåŠ¨ç”Ÿæˆã€‚åœ¨OPeRAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCustomer-R1ä¸ä»…åœ¨ä¸‹ä¸€è¡ŒåŠ¨é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºäºæç¤ºå’ŒSFTçš„åŸºçº¿ï¼Œè€Œä¸”åœ¨ç”¨æˆ·è¡ŒåŠ¨åˆ†å¸ƒä¸Šæ›´åŠ åŒ¹é…ï¼Œè¿™è¡¨æ˜åœ¨ä¸ªæ€§åŒ–è¡Œä¸ºæ¨¡æ‹Ÿæ–¹é¢å…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07230v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿäººç±»è¡Œä¸ºå·²æˆä¸ºæ–°å…´ç ”ç©¶æ–¹å‘ï¼Œå¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚æœ¬æ–‡æå‡ºCustomer-R1æ–¹æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åœ¨çº¿è´­ç‰©ç¯å¢ƒä¸­æ¨¡æ‹Ÿä¸ªæ€§åŒ–çš„ç”¨æˆ·è¡Œä¸ºã€‚è¯¥æ–¹æ³•æ ¹æ®æ˜ç¡®çš„ä¸ªæ€§ç‰¹å¾åˆ¶å®šç­–ç•¥ï¼Œå¹¶é€šè¿‡è¡ŒåŠ¨æ­£ç¡®æ€§å¥–åŠ±ä¿¡å·ä¼˜åŒ–ä¸‹ä¸€æ­¥çš„åˆç†æ€§è¡ŒåŠ¨ç”Ÿæˆã€‚åœ¨OPeRAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCustomer-R1åœ¨è¡ŒåŠ¨é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºäºæç¤ºå’Œå¾®è°ƒçš„æ–¹æ³•ï¼Œå¹¶æ›´å¥½åœ°åŒ¹é…ç”¨æˆ·è¡ŒåŠ¨åˆ†å¸ƒï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ä¸ªæ€§åŒ–è¡Œä¸ºæ¨¡æ‹Ÿä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨è¢«ç”¨äºæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯åœ¨åœ¨çº¿è´­ç‰©ç¯å¢ƒä¸­ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å­¦ä¹ ç¾¤ä½“å±‚é¢çš„ç­–ç•¥ï¼Œæ— æ³•æ ¹æ®ç”¨æˆ·çš„ä¸ªæ€§è¿›è¡Œæ¨¡æ‹Ÿï¼Œå¯¼è‡´æ¨¡æ‹Ÿç»“æœç¼ºä¹ä¸ªæ€§åŒ–ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Customer-R1æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸ªæ€§åŒ–ã€åˆ†æ­¥éª¤çš„ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿæ–¹æ³•ã€‚</li>
<li>Customer-R1æ–¹æ³•æ ¹æ®æ˜ç¡®çš„ä¸ªæ€§ç‰¹å¾åˆ¶å®šç­–ç•¥ï¼Œä¼˜åŒ–ä¸‹ä¸€æ­¥è¡ŒåŠ¨çš„åˆç†æ€§ç”Ÿæˆã€‚</li>
<li>åœ¨OPeRAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCustomer-R1åœ¨è¡ŒåŠ¨é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºæç¤ºå’Œå¾®è°ƒçš„æ–¹æ³•ã€‚</li>
<li>Customer-R1èƒ½æ›´å¥½åœ°åŒ¹é…ç”¨æˆ·è¡ŒåŠ¨åˆ†å¸ƒï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„ä¸ªæ€§åŒ–è¡Œä¸ºæ¨¡æ‹Ÿä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a6dc9b828df8cf0922fdc346aef126c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082658&auth_key=1760082658-0-0-98bf906c2462d6bc48bc127382647b04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b0e010d86f1439067e4c6c8cb197152~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082666&auth_key=1760082666-0-0-a655db43cda887c8a7ed922539f450c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c1a32a6d9a1cb2221554b2013aa0dcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082672&auth_key=1760082672-0-0-b3fc6ec5f8ee93e78212e68686251d4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e4924c9548ae064168e8fd778932fa4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082679&auth_key=1760082679-0-0-e906c00c27967262a736f7294ea5e845&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f23c896ca8627d4133366e99772dc64c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082685&auth_key=1760082685-0-0-2737661d1ecb1d4f54b0707a12c3c4b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TIGeR-Tool-Integrated-Geometric-Reasoning-in-Vision-Language-Models-for-Robotics"><a href="#TIGeR-Tool-Integrated-Geometric-Reasoning-in-Vision-Language-Models-for-Robotics" class="headerlink" title="TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for   Robotics"></a>TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for   Robotics</h2><p><strong>Authors:Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang</strong></p>
<p>Vision-Language Models (VLMs) have shown remarkable capabilities in spatial reasoning, yet they remain fundamentally limited to qualitative precision and lack the computational precision required for real-world robotics. Current approaches fail to leverage metric cues from depth sensors and camera calibration, instead reducing geometric problems to pattern recognition tasks that cannot deliver the centimeter-level accuracy essential for robotic manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel framework that transforms VLMs from perceptual estimators to geometric computers by enabling them to generate and execute precise geometric computations through external tools. Rather than attempting to internalize complex geometric operations within neural networks, TIGeR empowers models to recognize geometric reasoning requirements, synthesize appropriate computational code, and invoke specialized libraries for exact calculations. To support this paradigm, we introduce TIGeR-300K, a comprehensive tool-invocation-oriented dataset covering point transformations, pose estimation, trajectory generation, and spatial compatibility verification, complete with tool invocation sequences and intermediate computations. Through a two-stage training pipeline combining supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) with our proposed hierarchical reward design, TIGeR achieves SOTA performance on geometric reasoning benchmarks while demonstrating centimeter-level precision in real-world robotic manipulation tasks. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç©ºé—´æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åŸºæœ¬ä¸Šå±€é™äºå®šæ€§ç²¾åº¦ï¼Œç¼ºä¹ç°å®ä¸–ç•Œæœºå™¨äººæŠ€æœ¯æ‰€éœ€çš„è®¡ç®—ç²¾åº¦ã€‚å½“å‰çš„æ–¹æ³•æœªèƒ½åˆ©ç”¨æ·±åº¦ä¼ æ„Ÿå™¨å’Œç›¸æœºæ ‡å®šçš„åº¦é‡çº¿ç´¢ï¼Œè€Œæ˜¯å°†å‡ ä½•é—®é¢˜ç®€åŒ–ä¸ºæ¨¡å¼è¯†åˆ«ä»»åŠ¡ï¼Œæ— æ³•æä¾›æœºå™¨äººæ“ä½œæ‰€éœ€çš„å˜ç±³çº§ç²¾åº¦ã€‚æˆ‘ä»¬æå‡ºTIGeRï¼ˆå·¥å…·é›†æˆå‡ ä½•æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å¯ç”¨å¤–éƒ¨å·¥å…·ï¼Œå°†VLMsä»æ„ŸçŸ¥ä¼°è®¡å™¨è½¬å˜ä¸ºå‡ ä½•è®¡ç®—æœºï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆå¹¶æ‰§è¡Œç²¾ç¡®çš„å‡ ä½•è®¡ç®—ã€‚TIGeRä¸æ˜¯å°è¯•åœ¨ç¥ç»ç½‘ç»œå†…éƒ¨æ‰§è¡Œå¤æ‚çš„å‡ ä½•æ“ä½œï¼Œè€Œæ˜¯èµ‹èƒ½æ¨¡å‹è¯†åˆ«å‡ ä½•æ¨ç†éœ€æ±‚ï¼Œåˆæˆé€‚å½“çš„è®¡ç®—ä»£ç ï¼Œå¹¶è°ƒç”¨ä¸“ç”¨åº“è¿›è¡Œç²¾ç¡®è®¡ç®—ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€èŒƒå¼ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TIGeR-300Kï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å·¥å…·è°ƒç”¨å¯¼å‘æ•°æ®é›†ï¼Œæ¶µç›–ç‚¹å˜æ¢ã€å§¿æ€ä¼°è®¡ã€è½¨è¿¹ç”Ÿæˆå’Œç©ºé—´å…¼å®¹æ€§éªŒè¯ï¼Œé…å¤‡å·¥å…·è°ƒç”¨åºåˆ—å’Œä¸­é—´è®¡ç®—ã€‚é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ä»¥åŠæˆ‘ä»¬æå‡ºçš„åˆ†å±‚å¥–åŠ±è®¾è®¡ï¼ŒTIGeRåœ¨å‡ ä½•æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶åœ¨ç°å®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°äº†å˜ç±³çº§çš„ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07181v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç©ºé—´æ¨ç†ä¸Šå±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨è®¡ç®—ç²¾åº¦ä¸Šä»æœ‰æœ¬è´¨é™åˆ¶ï¼Œæ— æ³•æ»¡è¶³çœŸå®ä¸–ç•Œæœºå™¨äººæ‰€éœ€çš„å˜ç±³çº§ç²¾åº¦ã€‚æå‡ºçš„TIGeRæ¡†æ¶èƒ½å°†VLMsä»æ„ŸçŸ¥ä¼°è®¡å™¨è½¬å˜ä¸ºå‡ ä½•è®¡ç®—æœºï¼Œé€šè¿‡å¤–éƒ¨å·¥å…·æ‰§è¡Œç²¾ç¡®å‡ ä½•è®¡ç®—ã€‚TIGeRä¸å°è¯•åœ¨ç¥ç»ç½‘ç»œå†…éƒ¨æ‰§è¡Œå¤æ‚å‡ ä½•æ“ä½œï¼Œè€Œæ˜¯è®©æ¨¡å‹è¯†åˆ«å‡ ä½•æ¨ç†éœ€æ±‚ï¼Œåˆæˆé€‚å½“çš„è®¡ç®—ä»£ç ï¼Œå¹¶è°ƒç”¨ä¸“ç”¨åº“è¿›è¡Œç²¾ç¡®è®¡ç®—ã€‚å¼•è¿›TIGeR-300Kæ•°æ®é›†ä»¥æ”¯æŒæ­¤æ¡†æ¶ï¼ŒåŒ…å«ç‚¹è½¬æ¢ã€å§¿æ€ä¼°è®¡ã€è½¨è¿¹ç”Ÿæˆå’Œç©ºé—´å…¼å®¹æ€§éªŒè¯ç­‰ï¼Œå¹¶é™„æœ‰å·¥å…·è°ƒç”¨åºåˆ—å’Œä¸­é—´è®¡ç®—ã€‚é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ä»¥åŠåˆ†å±‚å¥–åŠ±è®¾è®¡ï¼ŒTIGeRåœ¨å‡ ä½•æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å±•ç°å‡ºå˜ç±³çº§ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsè™½åœ¨ç©ºé—´æ¨ç†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®¡ç®—ç²¾åº¦ä¸Šä»æœ‰å±€é™ï¼Œå°¤å…¶ç¼ºä¹ç”¨äºæœºå™¨äººæ“ä½œçš„å˜ç±³çº§ç²¾åº¦ã€‚</li>
<li>TIGeRæ¡†æ¶èƒ½å°†VLMsä»æ„ŸçŸ¥ä¼°è®¡å™¨è½¬å˜ä¸ºèƒ½æ‰§è¡Œç²¾ç¡®å‡ ä½•è®¡ç®—çš„å‡ ä½•è®¡ç®—æœºã€‚</li>
<li>TIGeRä¸ä¾èµ–ç¥ç»ç½‘ç»œå†…éƒ¨æ‰§è¡Œå¤æ‚å‡ ä½•æ“ä½œï¼Œè€Œæ˜¯ä½¿æ¨¡å‹èƒ½è¯†åˆ«å‡ ä½•æ¨ç†éœ€æ±‚ï¼Œå¹¶åˆæˆé€‚å½“çš„è®¡ç®—ä»£ç ã€‚</li>
<li>å¼•è¿›TIGeR-300Kæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§å‡ ä½•æ¨ç†ä»»åŠ¡ï¼Œå¹¶é™„æœ‰å·¥å…·è°ƒç”¨åºåˆ—å’Œä¸­é—´è®¡ç®—ï¼Œä»¥æ”¯æŒTIGeRæ¡†æ¶ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å¾®è°ƒï¼ŒTIGeRåœ¨å‡ ä½•æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>TIGeRæ¡†æ¶å®ç°äº†SOTAæ€§èƒ½ï¼ŒåŒæ—¶åœ¨çœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ä¿æŒäº†å˜ç±³çº§ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07181">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1c18392667e01736aaa259807bd55142~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082693&auth_key=1760082693-0-0-c20392c5a605911ff380a9e4d81973e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-534fdbe6209d4619be9e8e9e0c98988f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082700&auth_key=1760082700-0-0-2b564d5a0318d2a17627d285cefd28d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93514ca3539faff0585af7f41fe4b482~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082708&auth_key=1760082708-0-0-5aa221f8ba896ad192147ee94ba387fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-15cbd72e586b542d3062b7eb58a757ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098912&auth_key=1760098912-0-0-1f2952314d47fc67047a942fac4749c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f14136daf0821169ec7e4f0c03acf38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082779&auth_key=1760082779-0-0-7a4762c51f8296ec26f5e5e1dff50db7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reasoning-for-Hierarchical-Text-Classification-The-Case-of-Patents"><a href="#Reasoning-for-Hierarchical-Text-Classification-The-Case-of-Patents" class="headerlink" title="Reasoning for Hierarchical Text Classification: The Case of Patents"></a>Reasoning for Hierarchical Text Classification: The Case of Patents</h2><p><strong>Authors:Lekang Jiang, Wenjun Sun, Stephan Goetz</strong></p>
<p>Hierarchical text classification (HTC) assigns documents to multiple levels of a pre-defined taxonomy. Automated patent subject classification represents one of the hardest HTC scenarios because of domain knowledge difficulty and a huge number of labels. Prior approaches only output a flat label set, which offers little insight into the reason behind predictions. Therefore, we propose Reasoning for Hierarchical Classification (RHC), a novel framework that reformulates HTC as a step-by-step reasoning task to sequentially deduce hierarchical labels. RHC trains large language models (LLMs) in two stages: a cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning format and a reinforcement learning (RL) stage to enhance multi-step reasoning ability. RHC demonstrates four advantages in our experiments. (1) Effectiveness: RHC surpasses previous baselines and outperforms the supervised fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2) Explainability: RHC produces natural-language justifications before prediction to facilitate human inspection. (3) Scalability: RHC scales favorably with model size with larger gains compared to standard fine-tuning. (4) Applicability: Beyond patents, we further demonstrate that RHC achieves state-of-the-art performance on other widely used HTC benchmarks, which highlights its broad applicability. </p>
<blockquote>
<p>å±‚æ¬¡æ–‡æœ¬åˆ†ç±»ï¼ˆHTCï¼‰å°†æ–‡æ¡£åˆ†é…ç»™é¢„å®šä¹‰åˆ†ç±»æ³•ä¸­çš„å¤šä¸ªå±‚æ¬¡ã€‚è‡ªåŠ¨ä¸“åˆ©ä¸»é¢˜åˆ†ç±»ä»£è¡¨äº†æœ€å›°éš¾çš„HTCåœºæ™¯ä¹‹ä¸€ï¼Œå› ä¸ºæ¶‰åŠé¢†åŸŸçŸ¥è¯†éš¾åº¦å’Œå¤§é‡çš„æ ‡ç­¾ã€‚å…ˆå‰çš„æ–¹æ³•åªè¾“å‡ºä¸€ä¸ªæ‰å¹³çš„æ ‡ç­¾é›†ï¼Œè¿™æä¾›äº†å…³äºé¢„æµ‹åŸå› çš„å¾ˆå°‘æ´å¯Ÿã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚åˆ†ç±»çš„ç†ç”±ï¼ˆRHCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒå°†HTCé‡æ–°åˆ¶å®šä¸ºä¸€ä¸ªé€æ­¥æ¨ç†ä»»åŠ¡ï¼Œä»¥é¡ºåºæ¨æ–­å±‚æ¬¡æ ‡ç­¾ã€‚RHCåœ¨ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼šå†·å¯åŠ¨é˜¶æ®µä½¿è¾“å‡ºä¸æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ ¼å¼å¯¹é½ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µå¢å¼ºå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒRHCæ˜¾ç¤ºäº†å››ä¸ªä¼˜åŠ¿ã€‚ï¼ˆ1ï¼‰æœ‰æ•ˆæ€§ï¼šRHCè¶…è¶Šäº†ä¹‹å‰çš„åŸºçº¿ï¼Œå¹¶åœ¨å‡†ç¡®æ€§æ–¹é¢æ¯”ç›‘ç£å¾®è°ƒç‰ˆæœ¬é«˜å‡ºçº¦3%ï¼ŒåŒæ—¶å®è§‚F1å¾—åˆ†ä¹Ÿè¾ƒé«˜ã€‚ï¼ˆ2ï¼‰å¯è§£é‡Šæ€§ï¼šRHCåœ¨é¢„æµ‹ä¹‹å‰äº§ç”Ÿè‡ªç„¶è¯­è¨€ä¾æ®ï¼Œä¾¿äºäººå·¥æ£€æŸ¥ã€‚ï¼ˆ3ï¼‰å¯æ‰©å±•æ€§ï¼šRHCéšç€æ¨¡å‹å¤§å°çš„å¢åŠ è€Œè¡¨ç°è‰¯å¥½ï¼Œä¸æ ‡å‡†å¾®è°ƒç›¸æ¯”ï¼Œæ”¶ç›Šæ›´å¤§ã€‚ï¼ˆ4ï¼‰é€‚ç”¨æ€§ï¼šé™¤äº†ä¸“åˆ©ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯æ˜RHCåœ¨å…¶ä»–å¹¿æ³›ä½¿ç”¨çš„HTCåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07167v1">PDF</a> 15 pages, 10 tables, 3 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>åˆ†å±‚æ–‡æœ¬åˆ†ç±»ï¼ˆHTCï¼‰å°†æ–‡æ¡£åˆ†é…åˆ°é¢„å®šä¹‰çš„åˆ†ç±»ä½“ç³»ä¸­çš„å¤šä¸ªå±‚çº§ã€‚è‡ªåŠ¨åŒ–ä¸“åˆ©ä¸»é¢˜åˆ†ç±»æ˜¯éš¾åº¦æœ€å¤§çš„HTCåœºæ™¯ä¹‹ä¸€ï¼Œå› ä¸ºæ¶‰åŠé¢†åŸŸçŸ¥è¯†ä¸”æ ‡ç­¾æ•°é‡åºå¤§ã€‚å…ˆå‰çš„æ–¹æ³•åªè¾“å‡ºæ‰å¹³çš„æ ‡ç­¾é›†ï¼Œå¯¹äºé¢„æµ‹èƒŒåçš„åŸå› æä¾›çš„ä¿¡æ¯å¾ˆå°‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†çº§åˆ†ç±»æ¨ç†ï¼ˆRHCï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒå°†HTCé‡æ–°æ„å»ºä¸ºé€æ­¥æ¨ç†ä»»åŠ¡ä»¥æ¨å¯¼å±‚æ¬¡æ ‡ç­¾ã€‚RHCåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼šå†·å¯åŠ¨é˜¶æ®µä½¿è¾“å‡ºä¸æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ ¼å¼å¯¹é½ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µå¢å¼ºå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRHCå…·æœ‰å››å¤§ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰æœ‰æ•ˆæ€§ï¼šåœ¨å‡†ç¡®ç‡ä¸å®F1ä¸Šï¼ŒRHCè¶…è¶Šå…ˆå‰åŸºçº¿å¹¶é«˜å‡ºç›‘ç£å¾®è°ƒåŸºå‡†çº¦3%ã€‚ï¼ˆ2ï¼‰å¯è§£é‡Šæ€§ï¼šRHCåœ¨é¢„æµ‹å‰äº§ç”Ÿè‡ªç„¶è¯­è¨€è¯æ˜ï¼Œä¾¿äºäººå·¥æ£€æŸ¥ã€‚ï¼ˆ3ï¼‰å¯æ‰©å±•æ€§ï¼šä¸æ ‡å‡†å¾®è°ƒç›¸æ¯”ï¼ŒRHCæ›´åˆ©äºæ¨¡å‹è§„æ¨¡æ‰©å±•ã€‚ï¼ˆ4ï¼‰é€‚ç”¨æ€§ï¼šé™¤äº†ä¸“åˆ©åˆ†ç±»ï¼ŒRHCåœ¨å…¶ä»–å¹¿æ³›ä½¿ç”¨çš„HTCåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªåŠ¨åŒ–ä¸“åˆ©ä¸»é¢˜åˆ†ç±»æ˜¯åˆ†å±‚æ–‡æœ¬åˆ†ç±»ï¼ˆHTCï¼‰ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»…è¾“å‡ºæ‰å¹³æ ‡ç­¾é›†ï¼Œç¼ºä¹é¢„æµ‹èƒŒåçš„è§£é‡Šæ€§ã€‚</li>
<li>RHCæ¡†æ¶å°†HTCé‡æ–°è®¾è®¡ä¸ºé€æ­¥æ¨ç†ä»»åŠ¡ï¼Œä»¥æ¨å¯¼å±‚æ¬¡æ ‡ç­¾ã€‚</li>
<li>RHCæ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå†·å¯åŠ¨é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>RHCåœ¨å¤šä¸ªæ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼ŒåŒ…æ‹¬æœ‰æ•ˆæ€§ã€å¯è§£é‡Šæ€§ã€å¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§ã€‚</li>
<li>RHCæé«˜äº†çº¦3%çš„å‡†ç¡®ç‡å’Œå®F1å€¼ï¼Œè¶…è¶Šå…ˆå‰çš„åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-be7861974e4f9b3f3eff630a1265059a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082787&auth_key=1760082787-0-0-45224ae8142162c441481d3ceae336f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee1c1fd033bb0652d891504435f933bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082794&auth_key=1760082794-0-0-6c62323a58b0c75ba0ea3dae064423d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1b9f954c1c6de7f90e426dc52ebd2033~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082801&auth_key=1760082801-0-0-ab0d07e5bb3b8bc90ff67b43da299c10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98402e7a06063132a18d63c026817dee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082807&auth_key=1760082807-0-0-7c9c2ba745b3c7d84dbb3357ed398b5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-60ea8153af0f1d30ffc58b5652d11a4d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082814&auth_key=1760082814-0-0-e2ca8771ba9bfa04f18013847a5c6e33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3cb73bd20b8e93da41f955ca862f5332~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082822&auth_key=1760082822-0-0-ac5ad025d2c27eb2b6a21f512e6846b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-426cd59618c261d83a45e89793f70674~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082828&auth_key=1760082828-0-0-12aea2880d3f33378c03dbf2e13ac3ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AI-for-Abolition-A-Participatory-Design-Approach"><a href="#AI-for-Abolition-A-Participatory-Design-Approach" class="headerlink" title="AI for Abolition? A Participatory Design Approach"></a>AI for Abolition? A Participatory Design Approach</h2><p><strong>Authors:Carolyn Wang, Avriel Epps, Taylor Ferrari, Ra Ames</strong></p>
<p>The abolitionist community faces challenges from both the carceral state and oppressive technologies which, by empowering the ruling class who have the resources to develop artificial intelligence (AI), serve to entrench societal inequities even more deeply. This paper presents a case study in participatory design with transformative and restorative justice practitioners with the goal of designing an AI system to support their work. By co-designing an evaluation framework for large language models with the practitioners, we hope to push back against the exclusionary status quo of AI and extent AIâ€™s potentiality to a historically marginalized community. </p>
<blockquote>
<p>åºŸé™¤ä¸»ä¹‰ç¤¾ç¾¤æ—¢é¢ä¸´æ¥è‡ªç›‘ç‰¢å›½å®¶çš„æŒ‘æˆ˜ï¼Œä¹Ÿé¢ä¸´æ¥è‡ªå‹è¿«æŠ€æœ¯çš„æŒ‘æˆ˜ã€‚è¿™äº›æŠ€æœ¯èµ‹äºˆæœ‰èµ„æºæ¥å‘å±•äººå·¥æ™ºèƒ½çš„ç»Ÿæ²»é˜¶çº§åŠ›é‡ï¼Œè¿›è€ŒåŠ å‰§äº†ç¤¾ä¼šä¸å¹³ç­‰ã€‚æœ¬æ–‡é€šè¿‡å‚ä¸å¼è®¾è®¡çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæºæ‰‹æ”¹é©å’Œæ¢å¤æ­£ä¹‰çš„ä»ä¸šè€…ï¼Œæ—¨åœ¨è®¾è®¡æ”¯æŒä»–ä»¬å·¥ä½œçš„AIç³»ç»Ÿã€‚é€šè¿‡ä¸ä»ä¸šè€…å…±åŒè®¾è®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰“ç ´AIçš„æ’æ–¥ç°çŠ¶ï¼Œå°†AIçš„æ½œåŠ›å»¶ä¼¸åˆ°å†å²ä¸Šè¢«è¾¹ç¼˜åŒ–çš„ç¤¾ç¾¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07156v1">PDF</a> 8 pages, to be published in the Workshop Proceedings of the HHAI 2025   Conference</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½æ”¯æŒä¸‹çš„è½¬å‹ä¸æ¢å¤æ­£ä¹‰å®è·µè€…å‚ä¸è®¾è®¡ç ”ç©¶æ—¨åœ¨é€šè¿‡åˆä½œè®¾è®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥åº”å¯¹ç”±ä¸“åˆ¶å›½å®¶å’Œå‹è¿«æŠ€æœ¯å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½çš„æ½œåœ¨å¯èƒ½æ€§å‘å†å²è¾¹ç¼˜ç¾¤ä½“å»¶ä¼¸ï¼Œæ”¹å˜ç°æœ‰çš„æ’æ–¥ç°çŠ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åºŸé™¤ä¸»ä¹‰ç¤¾åŒºé¢ä¸´æ¥è‡ªä¸“åˆ¶å›½å®¶å’Œå‹è¿«æŠ€æœ¯çš„æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åŠ å‰§äº†ç¤¾ä¼šä¸å¹³ç­‰ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½æ”¯æŒä¸‹çš„è½¬å‹ä¸æ¢å¤æ­£ä¹‰å®è·µè€…å‚ä¸è®¾è®¡ç ”ç©¶ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>é€šè¿‡åˆä½œè®¾è®¡è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•å¹¶ä½¿å…¶æ›´å¥½åœ°æœåŠ¡äºåºŸé™¤ä¸»ä¹‰ç¤¾åŒºçš„éœ€æ±‚ã€‚</li>
<li>æ­¤é¡¹ç ”ç©¶æ—¨åœ¨å¯¹æŠ—å½“å‰äººå·¥æ™ºèƒ½çš„æ’æ–¥ç°çŠ¶ï¼Œå°†äººå·¥æ™ºèƒ½çš„æ½œåŠ›å»¶ä¼¸åˆ°å†å²è¾¹ç¼˜ç¾¤ä½“ã€‚</li>
<li>æ­¤ç ”ç©¶å¼ºè°ƒäººå·¥æ™ºèƒ½çš„å‘å±•éœ€è¦ä¸ç¤¾ä¼šå…¬æ­£å’ŒåŒ…å®¹æ€§ç›¸ç»“åˆã€‚</li>
<li>é€šè¿‡å‚ä¸å¼è®¾è®¡ç ”ç©¶ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œè§£å†³åºŸé™¤ä¸»ä¹‰ç¤¾åŒºé¢ä¸´çš„æŒ‘æˆ˜å’Œéœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3e6272766a7ca2992cb1c6b33e1bdc37~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082836&auth_key=1760082836-0-0-31abe32c3b205b85d3c0ee2bdc297b2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9102ea50e62c56c8fb9707064401c76e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082844&auth_key=1760082844-0-0-602599bc5476882c5857149c018b84d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-562050aed2f07d030fe38a136360d8ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082851&auth_key=1760082851-0-0-efb02cfd7d72617469111117ba2148c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods"><a href="#Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods" class="headerlink" title="Are We Using the Right Benchmark: An Evaluation Framework for Visual   Token Compression Methods"></a>Are We Using the Right Benchmark: An Evaluation Framework for Visual   Token Compression Methods</h2><p><strong>Authors:Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Xin Zou, Yuqian Fu, Bin Ren, Linfeng Zhang, Xuming Hu</strong></p>
<p>Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at <a target="_blank" rel="noopener" href="https://github.com/Chenfei-Liao/VTC-Bench">https://github.com/Chenfei-Liao/VTC-Bench</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŠ é€Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†ä¸»è¦ä¾§é‡äºè§†è§‰ä»¤ç‰Œå‹ç¼©ã€‚è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§é€šå¸¸æ˜¯é€šè¿‡æµ‹é‡åœ¨æ—¢å®šåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®æ€§ä¸‹é™æ¥è¯„ä¼°çš„ï¼Œæ¯”è¾ƒå‹ç¼©å‰åçš„æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•æœ€åˆæ˜¯ä¸ºäº†è¯„ä¼°MLLMsçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„ï¼Œè€Œä¸æ˜¯ç”¨æ¥è¯„ä¼°å‹ç¼©æŠ€æœ¯ã€‚å› æ­¤ï¼Œç›´æ¥åº”ç”¨äºè§†è§‰ä»¤ç‰Œå‹ç¼©ä¼šäº§ç”Ÿä»»åŠ¡ä¸åŒ¹é…çš„é—®é¢˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œç®€å•çš„å›¾åƒä¸‹é‡‡æ ·åœ¨å¤šä¸ªäººä»¬å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­ä¸€ç›´ä¼˜äºè®¸å¤šå…ˆè¿›çš„å‹ç¼©æ–¹æ³•ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä»¥ä¸‹ç°è±¡ï¼šï¼ˆiï¼‰å½“å‰åŸºå‡†æµ‹è¯•å¯¹äºè§†è§‰ä»¤ç‰Œå‹ç¼©ä»»åŠ¡è€Œè¨€å­˜åœ¨å™ªå£°ã€‚ï¼ˆiiï¼‰ä¸‹é‡‡æ ·èƒ½å¤Ÿä½œä¸ºæ•°æ®è¿‡æ»¤å™¨æ¥è¯„ä¼°è§†è§‰ä»¤ç‰Œå‹ç¼©ä»»åŠ¡ä¸­æ ·æœ¬çš„éš¾åº¦ã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†VTC-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå®ƒç»“åˆäº†æ•°æ®è¿‡æ»¤æœºåˆ¶æ¥æ¶ˆé™¤ç°æœ‰åŸºå‡†æµ‹è¯•çš„å™ªå£°ï¼Œä»è€Œèƒ½å¤Ÿæ›´å…¬å¹³ã€æ›´å‡†ç¡®åœ°è¯„ä¼°è§†è§‰ä»¤ç‰Œå‹ç¼©æ–¹æ³•ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chenfei-Liao/VTC-Bench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Chenfei-Liao/VTC-Benchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07143v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†åŠ é€Ÿç ”ç©¶ä¸»è¦èšç„¦äºè§†è§‰ä»¤ç‰Œå‹ç¼©ã€‚ç„¶è€Œï¼Œå½“å‰è¯„ä¼°æ–¹æ³•å­˜åœ¨ä»»åŠ¡ä¸åŒ¹é…çš„é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å‹ç¼©æŠ€æœ¯å¯èƒ½ä¸å¤Ÿå‡†ç¡®ã€‚ç ”ç©¶å‘ç°ç®€å•å›¾åƒä¸‹é‡‡æ ·åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºè®¸å¤šé«˜çº§å‹ç¼©æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†VTC-Benchè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•°æ®è¿‡æ»¤æœºåˆ¶å¯¹ç°æœ‰åŸºå‡†æµ‹è¯•è¿›è¡Œå»å™ªï¼Œä»¥æ›´å…¬å¹³ã€æ›´å‡†ç¡®åœ°è¯„ä¼°è§†è§‰ä»¤ç‰Œå‹ç¼©æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†åŠ é€Ÿé›†ä¸­åœ¨è§†è§‰ä»¤ç‰Œå‹ç¼©ä¸Šã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è§†è§‰ä»¤ç‰Œå‹ç¼©ä»»åŠ¡æ—¶å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>ç®€å•å›¾åƒä¸‹é‡‡æ ·åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¯¹è§†è§‰ä»¤ç‰Œå‹ç¼©ä»»åŠ¡çš„è¯„ä¼°å­˜åœ¨å™ªå£°ã€‚</li>
<li>æ•°æ®ä¸‹é‡‡æ ·å¯ä½œä¸ºè§†è§‰ä»¤ç‰Œå‹ç¼©ä»»åŠ¡ä¸­æ ·æœ¬éš¾åº¦çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>å¼•å…¥VTC-Benchè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ•°æ®è¿‡æ»¤æœºåˆ¶å¯¹ç°æœ‰åŸºå‡†æµ‹è¯•è¿›è¡Œå»å™ªã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2b73515f9a5c2659b805a9bb7553b474~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098924&auth_key=1760098924-0-0-c0c41728d704c26d0d4eefa30be01614&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad3199dde263fd4bdd855d13eabe083c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098932&auth_key=1760098932-0-0-53e23d37f562495f772d7e46b3084714&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca70b11220adf7496365776b018b8278~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098940&auth_key=1760098940-0-0-c7b7cb6c06b6fec6b565b2348575fe5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eae84b2e7e4ace9fa6fb81864deb5bce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098949&auth_key=1760098949-0-0-6764642e985c9c53cb713d942016a614&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TrackVLA-Unleashing-Reasoning-and-Memory-Capabilities-in-VLA-Models-for-Embodied-Visual-Tracking"><a href="#TrackVLA-Unleashing-Reasoning-and-Memory-Capabilities-in-VLA-Models-for-Embodied-Visual-Tracking" class="headerlink" title="TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models   for Embodied Visual Tracking"></a>TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models   for Embodied Visual Tracking</h2><p><strong>Authors:Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang</strong></p>
<p>Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the targetâ€™s relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios. </p>
<blockquote>
<p>è§†è§‰è·Ÿè¸ªï¼ˆEVTï¼‰æ˜¯ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ï¼Œä¸ºä¼´ä¾£æœºå™¨äººã€å¯¼èˆªæœºå™¨äººå’ŒæœåŠ¡åŠ©ç†ç­‰å®é™…åº”ç”¨æä¾›äº†æ”¯æŒï¼Œå…¶ä¸­æŒç»­è·Ÿè¸ªç§»åŠ¨ç›®æ ‡è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„è¿›å±•å·²ç»å®ç°äº†å¤æ‚å’Œéç»“æ„åŒ–åœºæ™¯ä¸­çš„è¯­è¨€å¼•å¯¼è·Ÿè¸ªã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„ç©ºé—´æ¨ç†å’Œæœ‰æ•ˆçš„ä¸´æ—¶è®°å¿†ï¼Œå¯¼è‡´åœ¨ä¸¥é‡é®æŒ¡æˆ–å­˜åœ¨ç±»ä¼¼å¹²æ‰°ç‰©çš„æƒ…å†µä¸‹å‡ºç°å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TrackVLA++ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®æ¨¡å—â€”â€”ç©ºé—´æ¨ç†æœºåˆ¶å’Œç›®æ ‡è¯†åˆ«è®°å¿†ï¼ˆTIMï¼‰æ¥å¢å¼ºè§†è§‰è·Ÿè¸ªåŠŸèƒ½ã€‚æ¨ç†æ¨¡å—å¼•å…¥äº†ç§°ä¸ºPolar-CoTçš„â€œæ€ç»´é“¾â€èŒƒå¼ï¼Œæ¨æ–­ç›®æ ‡çš„ç›¸å¯¹ä½ç½®å¹¶å°†å…¶ç¼–ç ä¸ºç´§å‡‘çš„æåæ ‡ä»¤ç‰Œä»¥è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ã€‚è¿™äº›ç©ºé—´å…ˆéªŒçš„å¼•å¯¼ä¸‹ï¼ŒTIMé‡‡ç”¨é—¨æ§æ›´æ–°ç­–ç•¥æ¥ä¿ç•™é•¿æœŸç›®æ ‡è®°å¿†ï¼Œç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§å¹¶ç¼“è§£é•¿æ—¶é—´é®æŒ¡è¿‡ç¨‹ä¸­çš„ç›®æ ‡ä¸¢å¤±ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTrackVLA++åœ¨è‡ªæˆ‘ä¸­å¿ƒå’Œå¤šç›¸æœºè®¾ç½®ä¸­çš„å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„EVT-Bench DTåˆ†å‰²æµ‹è¯•ä¸­ï¼ŒTrackVLA++åˆ†åˆ«è¶…è¶Šäº†ä¹‹å‰é¢†å…ˆçš„æ–¹æ³•5.1å’Œ12ã€‚æ­¤å¤–ï¼ŒTrackVLA++è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€å’Œé®æŒ¡åœºæ™¯ä¸­å®ç°ç¨³å¥çš„å®æ—¶è·Ÿè¸ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07134v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://pku-epic.github.io/TrackVLA-plus-plus-Web/">https://pku-epic.github.io/TrackVLA-plus-plus-Web/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Embodied Visual Trackingï¼ˆEVTï¼‰çš„é‡è¦æ€§åŠå…¶åœ¨å®é™…åº”ç”¨å¦‚é™ªä¼´æœºå™¨äººã€å¯¼èˆªæœºå™¨äººå’ŒæœåŠ¡åŠ©æ‰‹ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è·Ÿè¸ªå¤±è´¥é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹â€”â€”TrackVLA++ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥ç©ºé—´æ¨ç†æœºåˆ¶å’Œç›®æ ‡è¯†åˆ«è®°å¿†ï¼ˆTIMï¼‰æ¥å¢å¼ºè·Ÿè¸ªæ€§èƒ½ã€‚å…¶ä¸­ï¼Œç©ºé—´æ¨ç†æ¨¡å—é‡‡ç”¨Chain-of-Thoughtï¼ˆPolar-CoTï¼‰èŒƒå¼æ¥æ¨æ–­ç›®æ ‡ç›¸å¯¹ä½ç½®å¹¶ç¼–ç ä¸ºæåæ ‡æ ‡è®°ç”¨äºåŠ¨ä½œé¢„æµ‹ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šï¼ŒTrackVLA++å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„EVT-Bench DT splitä¸Šè¶…è¶Šäº†ç°æœ‰é¢†å…ˆæ–¹æ³•ã€‚åŒæ—¶ï¼ŒTrackVLA++è¿˜è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯åœ¨åŠ¨æ€å’Œé®æŒ¡åœºæ™¯ä¸­å®ç°ç¨³å¥çš„å®æ—¶è·Ÿè¸ªã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³äºæœ¬æ–‡çš„ä¸»è¦è§‚ç‚¹ï¼š</p>
<ol>
<li>Embodied Visual Trackingï¼ˆEVTï¼‰æ˜¯ä¸€ç§åŸºæœ¬èƒ½åŠ›ï¼Œä¸ºä¼´ä¾£æœºå™¨äººã€å¯¼èˆªæœºå™¨äººå’ŒæœåŠ¡åŠ©æ‰‹ç­‰å®é™…åº”ç”¨æä¾›æ”¯æŒï¼Œå…¶ä¸­æŒç»­è·Ÿè¸ªç§»åŠ¨ç›®æ ‡æ˜¯è‡³å…³é‡è¦çš„ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤æ‚å’Œä¸ç»“æ„åŒ–çš„åœºæ™¯ä¸­çš„è¯­è¨€å¼•å¯¼è·Ÿè¸ªå­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹æ˜ç¡®çš„æ—¶ç©ºæ¨ç†å’Œæœ‰æ•ˆçš„è®°å¿†æœºåˆ¶ã€‚</li>
<li>TrackVLA++æ˜¯ä¸€ç§æ–°å‹çš„Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>TrackVLA++å¼•å…¥äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç©ºé—´æ¨ç†æœºåˆ¶å’Œç›®æ ‡è¯†åˆ«è®°å¿†ï¼ˆTIMï¼‰ã€‚</li>
<li>ç©ºé—´æ¨ç†æ¨¡å—é‡‡ç”¨Chain-of-Thoughtï¼ˆPolar-CoTï¼‰èŒƒå¼æ¥æ¨æ–­ç›®æ ‡çš„ç›¸å¯¹ä½ç½®ï¼Œå¹¶ç¼–ç ä¸ºæåæ ‡æ ‡è®°ç”¨äºåŠ¨ä½œé¢„æµ‹ã€‚</li>
<li>åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTrackVLA++è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”åœ¨æŒ‘æˆ˜æ€§çš„EVT-Bench DT splitä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a2cdf982e30f46c7fdbd48699d73566c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098957&auth_key=1760098957-0-0-31d0e2a51d77c74c17b5b33cdc16b840&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3803a674a35f1561f1c1d7c4e0959666~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098965&auth_key=1760098965-0-0-4d84ef4eb3ac79fce7eaae9d5e2e0886&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-830f3d9f0cd54359f43dd58f368ca241~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098972&auth_key=1760098972-0-0-ea1663aacf73b8ac67d990472360e884&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94d8dc6f9705b67401007497eacec3c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098980&auth_key=1760098980-0-0-0eef94eaba8366cbdffb9172fc0b826f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models"><a href="#Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models" class="headerlink" title="Search-R3: Unifying Reasoning and Embedding Generation in Large Language   Models"></a>Search-R3: Unifying Reasoning and Embedding Generation in Large Language   Models</h2><p><strong>Authors:Yuntao Gui, James Cheng</strong></p>
<p>Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMsâ€™ chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the modelâ€™s ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: <a target="_blank" rel="noopener" href="https://github.com/ytgui/Search-R3">https://github.com/ytgui/Search-R3</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å‡ºè‰²çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ£€ç´¢ä»»åŠ¡ä¸­çš„åˆ©ç”¨å´ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†Search-R3ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡é€‚åº”LLMä»¥ç”Ÿæˆæœç´¢åµŒå…¥ä½œä¸ºæ¨ç†è¿‡ç¨‹ç›´æ¥è¾“å‡ºçš„æ–°å‹æ¡†æ¶ï¼Œä»¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨LLMçš„é“¾å¼æ€ç»´åŠŸèƒ½ï¼Œé€šè¿‡é€æ­¥æ¨ç†å’Œå¤æ‚è¯­ä¹‰åˆ†æï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´æœ‰æ•ˆçš„åµŒå…¥ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ç§äº’è¡¥æœºåˆ¶å®ç°äº†è¿™ä¸€ç‚¹ã€‚ï¼ˆ1ï¼‰æœ‰ç›‘ç£å­¦ä¹ é˜¶æ®µä½¿æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„åµŒå…¥ï¼›ï¼ˆ2ï¼‰ä¸€ç§ä¼˜åŒ–åµŒå…¥ç”Ÿæˆä¸æ¨ç†çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªä¸“é—¨çš„RLç¯å¢ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†ä¸æ–­å˜åŒ–çš„åµŒå…¥è¡¨ç¤ºï¼Œè€Œæ— éœ€åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£æ—¶éƒ½é‡æ–°ç¼–ç æ•´ä¸ªè¯­æ–™åº“ã€‚æˆ‘ä»¬åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œé€šè¿‡ç»Ÿä¸€æ¨ç†å’ŒåµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼ŒSearch-R3æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚è¿™ç§é›†æˆåè®­ç»ƒçš„æ–¹æ³•åœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†å’Œæœ‰æ•ˆä¿¡æ¯æ£€ç´¢çš„å¤æ‚çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ytgui/Search-R3">https://github.com/ytgui/Search-R3</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07048v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½å…·æœ‰å‡ºè‰²çš„è‡ªç„¶è¯­è¨€ç†è§£åŠ›ï¼Œä½†åœ¨æ£€ç´¢ä»»åŠ¡ä¸­çš„åˆ©ç”¨å´ä¸å°½å¦‚äººæ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSearch-R3æ¡†æ¶ï¼Œé€šè¿‡é€‚åº”LLMç”Ÿæˆæœç´¢åµŒå…¥ä½œä¸ºå…¶æ¨ç†è¿‡ç¨‹çš„ç›´æ¥è¾“å‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMçš„é“¾å¼æ€ç»´ï¼Œé€šè¿‡åˆ†æ­¥æ¨ç†å’Œå¤æ‚è¯­ä¹‰åˆ†æï¼Œäº§ç”Ÿæ›´æœ‰æ•ˆçš„åµŒå…¥ã€‚é€šè¿‡ä¸‰ç§äº’è¡¥æœºåˆ¶å®ç°ï¼š1ï¼‰ç›‘ç£å­¦ä¹ é˜¶æ®µä½¿æ¨¡å‹å…·å¤‡ç”Ÿæˆä¼˜è´¨åµŒå…¥çš„èƒ½åŠ›ï¼›2ï¼‰å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–åµŒå…¥ç”Ÿæˆä¸æ¨ç†è¿‡ç¨‹ï¼›3ï¼‰ä¸“é—¨è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒå¯é«˜æ•ˆå¤„ç†ä¸æ–­å˜åŒ–çš„åµŒå…¥è¡¨ç¤ºï¼Œæ— éœ€åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£æ—¶é‡æ–°ç¼–ç æ•´ä¸ªè¯­æ–™åº“ã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œé€šè¿‡ç»Ÿä¸€æ¨ç†å’ŒåµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼ŒSearch-R3æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚æ­¤é›†æˆåè®­ç»ƒæ–¹æ³•ä¸ºå¤„ç†éœ€è¦é«˜çº§æ¨ç†å’Œæœ‰æ•ˆä¿¡æ¯æ£€ç´¢çš„å¤æ‚çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æä¾›äº†é‡å¤§è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨æ£€ç´¢ä»»åŠ¡ä¸­çš„åˆ©ç”¨ç‡ä¸é«˜ã€‚</li>
<li>Search-R3æ¡†æ¶é€šè¿‡é€‚åº”LLMç”Ÿæˆæœç´¢åµŒå…¥ï¼Œè§£å†³è¿™ä¸€é™åˆ¶ã€‚</li>
<li>Search-R3åˆ©ç”¨LLMçš„é“¾å¼æ€ç»´ï¼Œè¿›è¡Œåˆ†æ­¥æ¨ç†å’Œå¤æ‚è¯­ä¹‰åˆ†æï¼Œäº§ç”Ÿæ›´æœ‰æ•ˆçš„åµŒå…¥ã€‚</li>
<li>é€šè¿‡ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ åŠä¸“é—¨è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒç­‰ä¸‰ç§æœºåˆ¶å®ç°Search-R3ã€‚</li>
<li>ç›‘ç£å­¦ä¹ é˜¶æ®µåŸ¹å…»æ¨¡å‹ç”Ÿæˆä¼˜è´¨åµŒå…¥çš„èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºä¼˜åŒ–åµŒå…¥ç”Ÿæˆä¸æ¨ç†çš„ååŒä½œç”¨ã€‚</li>
<li>ä¸“é—¨è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒèƒ½é«˜æ•ˆå¤„ç†ä¸æ–­å˜åŒ–çš„åµŒå…¥è¡¨ç¤ºï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f161198c8c7596c30e1bbcc4600b2962~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098988&auth_key=1760098988-0-0-51bdc5d11928e605ea99c35fdcdbca76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16071a02f873a11a94deb96c5c907c49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760098996&auth_key=1760098996-0-0-8946040649fb520ac43d3566349d66f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c7e3985ecfa7fb5173598da3ddce5db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099003&auth_key=1760099003-0-0-3f2e435a85b59e10312c05fd16a2ecfa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b53f2d7512a40d97a3df7b619000282~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099009&auth_key=1760099009-0-0-9ecbca3beb4bd0bb851bac3995c4d9a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b55ab32539610a50abcf3ea846566696~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099016&auth_key=1760099016-0-0-7f463c5e7c3aee2dc0ce75309ee5e86e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdef2d147a2d20cedb68c1404a8b6a2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102079&auth_key=1760102079-0-0-3edd9cea4165e6cc1212c549ab00f743&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Tool-Augmented-Policy-Optimization-Synergizing-Reasoning-and-Adaptive-Tool-Use-with-Reinforcement-Learning"><a href="#Tool-Augmented-Policy-Optimization-Synergizing-Reasoning-and-Adaptive-Tool-Use-with-Reinforcement-Learning" class="headerlink" title="Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive   Tool Use with Reinforcement Learning"></a>Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive   Tool Use with Reinforcement Learning</h2><p><strong>Authors:Wenxun Wu, Yuanyang Li, Guhan Chen, Linyue Wang, Hongyang Chen</strong></p>
<p>Recent advances in large language models (LLMs) have popularized test-time scaling, where models generate additional reasoning tokens before producing final answers. These approaches have demonstrated significant performance improvements on benchmarks involving mathematical reasoning. However, language models relying solely on direct inference still struggle with tasks demanding up-to-date knowledge or computational tools such as calculators and code interpreters for complex arithmetic operations. To overcome these limitations, we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement learning framework that systematically integrates multi-hop reasoning with adaptive tool-calling capabilities. Our approach employs a modified version of Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm, which we adapt specifically for tool invocation scenarios, enabling models to dynamically interleave complex reasoning with on-demand tool usage (including search APIs and Python interpreters).   To support this research, we introduce two new datasets: TAPO-easy-60K and TAPO-hard-18K, specifically designed to train and evaluate both fact-based reasoning and mathematical calculation capabilities. Our experiments on Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach, with both models achieving state-of-the-art performance on tasks requiring external knowledge and mathematical computation among methods with comparable parameters. Notably, TAPO achieves more efficient tool utilization than baseline methods while preventing excessive calls caused by reward hacking. These results highlight the significant potential of combining advanced reasoning with tool usage to enhance model performance in knowledge-intensive and computationally demanding tasks. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»æ™®åŠäº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼Œå…¶ä¸­æ¨¡å‹åœ¨äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆä¹‹å‰ä¼šç”Ÿæˆé¢å¤–çš„æ¨ç†æ ‡è®°ã€‚è¿™äº›æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ç›¸å…³çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚ç„¶è€Œï¼Œä»…ä¾èµ–ç›´æ¥æ¨ç†çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦æœ€æ–°çŸ¥è¯†æˆ–è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—å™¨ã€ä»£ç è§£é‡Šå™¨ç­‰ç”¨äºå¤æ‚ç®—æœ¯è¿ç®—çš„å·¥å…·ï¼‰çš„ä»»åŠ¡æ—¶ä»ç„¶æ„Ÿåˆ°å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†å·¥å…·å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼ˆTAPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç³»ç»Ÿåœ°ç»“åˆäº†å¤šè·³æ¨ç†å’Œè‡ªé€‚åº”çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§ä¿®æ”¹åçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æœ€è¿‘å¼€å‘çš„RLèŒƒå¼ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œä¸“é—¨çš„é€‚åº”ï¼Œç”¨äºå·¥å…·è°ƒç”¨åœºæ™¯ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€åœ°å°†å¤æ‚çš„æ¨ç†ä¸æŒ‰éœ€çš„å·¥å…·ä½¿ç”¨ï¼ˆåŒ…æ‹¬æœç´¢APIå’ŒPythonè§£é‡Šå™¨ç­‰ï¼‰äº¤ç»‡åœ¨ä¸€èµ·ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ï¼šTAPO-easy-60Kå’ŒTAPO-hard-18Kï¼Œå®ƒä»¬ä¸“é—¨è®¾è®¡ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åŸºäºäº‹å®å’Œæ•°å­¦è®¡ç®—èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨Qwen2.5-3Bå’ŒQwen2.5-7Bæ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨éœ€è¦å¤–éƒ¨çŸ¥è¯†å’Œæ•°å­¦è®¡ç®—çš„ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨å‚æ•°ç›¸å½“çš„æ–¹æ³•ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTAPOå®ç°äº†æ¯”åŸºçº¿æ–¹æ³•æ›´æœ‰æ•ˆçš„å·¥å…·åˆ©ç”¨ï¼ŒåŒæ—¶é˜²æ­¢äº†å› å¥–åŠ±ç ´è§£è€Œå¯¼è‡´çš„è¿‡åº¦è°ƒç”¨ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å°†é«˜çº§æ¨ç†ä¸å·¥å…·ä½¿ç”¨ç›¸ç»“åˆï¼Œåœ¨æé«˜çŸ¥è¯†å¯†é›†å‹å’Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07038v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†æµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯çš„æ™®åŠï¼Œè¯¥æŠ€æœ¯é€šè¿‡åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå‰ç”Ÿæˆé¢å¤–çš„æ¨ç†æ ‡è®°æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»…ä¾èµ–ç›´æ¥æ¨ç†çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦æœ€æ–°çŸ¥è¯†æˆ–è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—å™¨ã€ä»£ç è§£é‡Šå™¨è¿›è¡Œå¤æ‚ç®—æœ¯è¿ç®—ï¼‰çš„ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºTAPOï¼ˆå·¥å…·å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼‰çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç³»ç»Ÿåœ°ç»“åˆäº†å¤šè·³æ¨ç†ä¸è‡ªé€‚åº”å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚TAPOé‡‡ç”¨åŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰çš„ä¿®æ”¹ç‰ˆï¼Œé’ˆå¯¹å·¥å…·è°ƒç”¨åœºæ™¯è¿›è¡Œç‰¹å®šé€‚é…ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚æ¨ç†ä¸å³æ—¶å·¥å…·ä½¿ç”¨ä¹‹é—´çµæ´»äº¤æ›¿ï¼ˆåŒ…æ‹¬æœç´¢APIå’ŒPythonè§£é‡Šå™¨ï¼‰ã€‚ä¸ºæ”¯æŒè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼šTAPO-easy-60Kå’ŒTAPO-hard-18Kï¼Œä¸“é—¨è®¾è®¡ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åŸºäºäº‹å®å’Œæ•°å­¦è®¡ç®—çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒTAPOæ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ¨¡å‹åœ¨éœ€è¦å¤–éƒ¨çŸ¥è¯†å’Œæ•°å­¦è®¡ç®—çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¾¾åˆ°åŒç±»å‚æ•°æ¨¡å‹çš„æœ€ä½³æ°´å¹³ï¼ŒåŒæ—¶å®ç°æ›´é«˜æ•ˆçš„å·¥å…·åˆ©ç”¨ï¼Œé˜²æ­¢å› å¥–åŠ±é»‘å®¢è¡Œä¸ºè€Œå¯¼è‡´çš„è¿‡åº¦è°ƒç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯æé«˜æ€§èƒ½ï¼Œç”Ÿæˆé¢å¤–çš„æ¨ç†æ ‡è®°æ¥ä¼˜åŒ–æœ€ç»ˆç­”æ¡ˆã€‚</li>
<li>ä»…ä¾èµ–ç›´æ¥æ¨ç†çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦æœ€æ–°çŸ¥è¯†æˆ–è®¡ç®—å·¥å…·çš„ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>TAPOæ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆå¤šè·³æ¨ç†ä¸è‡ªé€‚åº”å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå…‹æœä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>TAPOé‡‡ç”¨ä¿®æ”¹ç‰ˆçš„DAPOï¼Œé’ˆå¯¹å·¥å…·è°ƒç”¨åœºæ™¯è¿›è¡Œé€‚é…ã€‚</li>
<li>æ¨¡å‹èƒ½åœ¨å¤æ‚æ¨ç†ä¸å³æ—¶å·¥å…·ä½¿ç”¨ä¹‹é—´çµæ´»äº¤æ›¿ã€‚</li>
<li>ä¸ºæ”¯æŒç ”ç©¶ï¼Œå¼•å…¥äº†ä¸¤ä¸ªä¸“é—¨è®¾è®¡çš„æ–°æ•°æ®é›†ï¼šTAPO-easy-60Kå’ŒTAPO-hard-18Kã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6405c3ac929152208f0739b659d5c686~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099087&auth_key=1760099087-0-0-f911629a2ed1a6368dee0f5bb1878c69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a9e518ee1ac6f62fdb90982b17f772ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760099095&auth_key=1760099095-0-0-9ff5b3c291dc334162e72e015a67b4e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-047901f767fdd4a09c0a0a976ff2164a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102087&auth_key=1760102087-0-0-315607659054a30b762a150076478bfb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2e0ff731c55b1d2efaa96cbfaafe3491~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102093&auth_key=1760102093-0-0-50a7d77a9d824a98c5f29ecbb63c058e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c096495dd95f67b9e05ecf580939af0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102100&auth_key=1760102100-0-0-ae46c14877496886f1ee537f170074bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d2897c8fa1f46a4fcd6c88e66d56fe68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102107&auth_key=1760102107-0-0-c2072fdc503cd5181e4c00a1ac8fc5fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SaFeR-VLM-Toward-Safety-aware-Fine-grained-Reasoning-in-Multimodal-Models"><a href="#SaFeR-VLM-Toward-Safety-aware-Fine-grained-Reasoning-in-Multimodal-Models" class="headerlink" title="SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal   Models"></a>SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal   Models</h2><p><strong>Authors:Huahui Yi, Kun Wang, Qiankun Li, Miao Yu, Liang Lin, Gongli Xi, Hao Wu, Xuming Hu, Kang Li, Yang Liu</strong></p>
<p>Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal reasoning but often amplify safety risks under adversarial or unsafe prompts, a phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at the output level and do not constrain the reasoning process, leaving models exposed to implicit risks. In this paper, we propose SaFeR-VLM, a safety-aligned reinforcement learning framework that embeds safety directly into multimodal reasoning. The framework integrates four components: (I) QI-Safe-10K, a curated dataset emphasizing safety-critical and reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations undergo reflection and correction instead of being discarded; (III) structured reward modeling with multi-dimensional weighted criteria and explicit penalties for hallucinations and contradictions; and (IV) GRPO optimization, which reinforces both safe and corrected trajectories. This unified design shifts safety from a passive safeguard to an active driver of reasoning, enabling scalable and generalizable safety-aware reasoning. SaFeR-VLM further demonstrates robustness against both explicit and implicit risks, supporting dynamic and interpretable safety decisions beyond surface-level filtering. SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and helpfulness across six benchmarks, surpassing both same-scale and $&gt;10\times$ larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B. Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points respectively on safety metrics, achieving this improvement without any degradation in helpfulness performance. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/HarveyYi/SaFeR-VLM">https://github.com/HarveyYi/SaFeR-VLM</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆMLRMsï¼‰å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¯¹æŠ—æ€§æˆ–ä¸å®‰å…¨æç¤ºä¸‹å¾€å¾€ä¼šæ”¾å¤§å®‰å…¨é£é™©ï¼Œæˆ‘ä»¬å°†è¿™ä¸€ç°è±¡ç§°ä¸ºâ€œæ¨ç†ç¨â€ã€‚ç°æœ‰çš„é˜²å¾¡æ‰‹æ®µä¸»è¦ä½œç”¨äºè¾“å‡ºå±‚é¢ï¼Œå¹¶ä¸èƒ½çº¦æŸæ¨ç†è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹é¢ä¸´éšæ€§é£é™©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SaFeR-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸å®‰å…¨å¯¹é½çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥å°†å®‰å…¨åµŒå…¥å¤šæ¨¡æ€æ¨ç†ä¸­ã€‚è¯¥æ¡†æ¶é›†æˆäº†å››ä¸ªç»„ä»¶ï¼šï¼ˆIï¼‰QI-Safe-10Kï¼Œä¸€ä¸ªå¼ºè°ƒå®‰å…¨å…³é”®å’Œæ¨ç†æ•æ„Ÿæ¡ˆä¾‹çš„å®šåˆ¶æ•°æ®é›†ï¼›ï¼ˆIIï¼‰å®‰å…¨æ„ŸçŸ¥å›æ”¾ï¼Œå…¶ä¸­ä¸å®‰å…¨çš„ç”Ÿæˆç‰©ä¼šè¿›è¡Œåæ€å’Œä¿®æ­£è€Œä¸æ˜¯è¢«ä¸¢å¼ƒï¼›ï¼ˆIIIï¼‰ç»“æ„åŒ–å¥–åŠ±å»ºæ¨¡ï¼Œå…·æœ‰å¤šç»´åº¦åŠ æƒæ ‡å‡†å’Œæ˜¾å¼çš„å¹»è§‰ä¸çŸ›ç›¾æƒ©ç½šï¼›ï¼ˆIVï¼‰GRPOä¼˜åŒ–ï¼Œå¼ºåŒ–å®‰å…¨å’Œä¿®æ­£çš„è½¨è¿¹ã€‚è¿™ä¸€ç»Ÿä¸€è®¾è®¡å°†å®‰å…¨ä»è¢«åŠ¨ä¿éšœè½¬å˜ä¸ºæ¨ç†çš„ä¸»åŠ¨é©±åŠ¨å› ç´ ï¼Œå®ç°äº†å¯ä¼¸ç¼©å’Œé€šç”¨çš„å®‰å…¨æ„ŸçŸ¥æ¨ç†ã€‚SaFeR-VLMè¿›ä¸€æ­¥è¯æ˜äº†å®ƒå¯¹æ˜¾æ€§é£é™©å’Œéšæ€§é£é™©çš„ç¨³å¥æ€§ï¼Œæ”¯æŒè¶…è¶Šè¡¨é¢å±‚æ¬¡çš„è¿‡æ»¤è¿›è¡ŒåŠ¨æ€å’Œå¯è§£é‡Šçš„å®‰å…¨å†³ç­–ã€‚SaFeR-VLM-3Båœ¨å®‰å…¨å’Œæœ‰ç”¨æ€§æ–¹é¢çš„å¹³å‡æ€§èƒ½åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°70.13å’Œ78.97ï¼Œè¶…è¶Šäº†åŒç­‰è§„æ¨¡å’Œå¤§äº10å€çš„æ¨¡å‹ï¼Œå¦‚Skywork-R1V3-38Bã€Qwen2.5VL-72Bå’ŒGLM4.5V-106Bã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSaFeR-VLM-7Bå—ç›Šäºå…¶å¢åŠ çš„è§„æ¨¡ï¼Œåœ¨å®‰å…¨æ€§æŒ‡æ ‡ä¸Šè¶…è¶Šäº†GPT-5-miniå’ŒGemini-2.5-Flashï¼Œåˆ†åˆ«é«˜å‡º6.47å’Œ16.76ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶æœ‰ç”¨æ€§æ€§èƒ½æ²¡æœ‰ä»»ä½•ä¸‹é™ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HarveyYi/SaFeR-VLM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HarveyYi/SaFeR-VLMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06871v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆMLRMsï¼‰åœ¨é¢ä¸´å¯¹æŠ—æ€§æˆ–ä¸å®‰å…¨æç¤ºæ—¶æ”¾å¤§çš„å®‰å…¨é£é™©ç°è±¡ï¼Œç§°ä¹‹ä¸ºâ€œæ¨ç†ç¨â€ã€‚ç°æœ‰é˜²å¾¡ç­–ç•¥ä¸»è¦ä½œç”¨äºè¾“å‡ºå±‚é¢ï¼Œå¹¶æœªå¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œçº¦æŸï¼Œä½¿å¾—æ¨¡å‹é¢ä¸´éšæ€§é£é™©ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SaFeR-VLMå®‰å…¨å¯¹é½çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥åµŒå…¥å¤šæ¨¡æ€æ¨ç†çš„å®‰å…¨æ€§ã€‚æ¡†æ¶åŒ…å«å››ä¸ªç»„ä»¶ï¼Œå®ç°äº†å¯¹å®‰å…¨æ€§çš„ä¸»åŠ¨é©±åŠ¨ï¼Œå¹¶åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œè¶…è¶Šäº†åŒç­‰è§„æ¨¡å’Œæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚SaFeR-VLMçš„ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆMLRMsï¼‰åœ¨å¯¹æŠ—æ€§æˆ–ä¸å®‰å…¨æç¤ºä¸‹ä¼šæ”¾å¤§å®‰å…¨é£é™©ï¼Œç§°ä¸ºâ€œæ¨ç†ç¨â€ã€‚</li>
<li>ç°æœ‰é˜²å¾¡ç­–ç•¥ä¸»è¦å…³æ³¨è¾“å‡ºå±‚é¢ï¼Œç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„çº¦æŸï¼Œå¯¼è‡´æ¨¡å‹é¢ä¸´éšæ€§é£é™©ã€‚</li>
<li>SaFeR-VLMæ˜¯ä¸€ä¸ªå®‰å…¨å¯¹é½çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥åµŒå…¥å¤šæ¨¡æ€æ¨ç†çš„å®‰å…¨æ€§ã€‚</li>
<li>SaFeR-VLMåŒ…å«å››ä¸ªç»„ä»¶ï¼šå¼ºè°ƒå®‰å…¨å…³é”®å’Œæ¨ç†æ•æ„Ÿæ¡ˆä¾‹çš„æ•°æ®é›†ã€å®‰å…¨æ„ŸçŸ¥çš„å›æ»šã€ç»“æ„åŒ–çš„å¥–åŠ±å»ºæ¨¡å’ŒGRPOä¼˜åŒ–ã€‚</li>
<li>SaFeR-VLMå®ç°äº†ä»è¢«åŠ¨ä¿éšœåˆ°ä¸»åŠ¨é©±åŠ¨æ¨ç†çš„å®‰å…¨æ€§çš„è½¬å˜ã€‚</li>
<li>SaFeR-VLMåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œè¶…è¶Šäº†åŒç­‰è§„æ¨¡å’Œæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-db356343f055e1c6215cf0c640f408db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102114&auth_key=1760102114-0-0-77b334f8771183f68709f6fc8394a68f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4eab6ab12a7e1b6b96b0de7fcb33c84a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102122&auth_key=1760102122-0-0-9eec57da120b86416ca1dafd0478dc85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLA-RL-Training"><a href="#RLinf-VLA-A-Unified-and-Efficient-Framework-for-VLA-RL-Training" class="headerlink" title="RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training"></a>RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training</h2><p><strong>Authors:Hongzhi Zang, Mingjie Wei, Si Xu, Yongji Wu, Zhen Guo, Yuanqing Wang, Hao Lin, Liangzhi Shi, Yuqing Xie, Zhexuan Xu, Zhihao Liu, Kang Chen, Wenhao Tang, Quanlu Zhang, Weinan Zhang, Chao Yu, Yu Wang</strong></p>
<p>Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11% across 130 LIBERO tasks and 97.66% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence. </p>
<blockquote>
<p>è¿‘æœŸè§†è§‰å’Œè¯­è¨€åŸºç¡€æ¨¡å‹åœ¨è·¨æ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—è¿›å±•æ¿€å‘äº†äººä»¬çš„æå¤§å…´è¶£ï¼Œè¿™äº›è¿›æ­¥æ¿€åŠ±ç€å°†è¿™ç§èƒ½åŠ›æ‹“å±•åˆ°ä½“æ„Ÿåœºæ™¯é€šè¿‡è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°VLAæ¨¡å‹ä»é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨åˆ†å¸ƒè½¬ç§»çš„æƒ…å†µä¸‹ç”±äºè¯¯å·®ç´¯ç§¯è€Œæ— æ³•å®ç°é€šç”¨åŒ–ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡äº¤äº’ç›´æ¥ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å°è¯•ä»ç„¶é›¶æ•£ï¼Œç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„å¹³å°ï¼Œæ— æ³•åœ¨æ¨¡å‹æ¶æ„å’Œç®—æ³•è®¾è®¡ä¸Šè¿›è¡Œå…¬å¹³å’Œç³»ç»Ÿçš„æ¯”è¾ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†RLinf-VLAï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯æ‰©å±•çš„VLAæ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç»Ÿä¸€é«˜æ•ˆæ¡†æ¶ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨é«˜åº¦çµæ´»çš„èµ„æºé…ç½®è®¾è®¡ï¼Œè§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸VLAè®­ç»ƒä¸­æ¸²æŸ“ã€è®­ç»ƒå’Œæ¨ç†é›†æˆæ–¹é¢çš„æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯é’ˆå¯¹GPUå¹¶è¡Œæ¨¡æ‹Ÿå™¨ï¼ŒRLinf-VLAå®ç°äº†æ–°å‹æ··åˆç»†ç²’åº¦ç®¡é“åˆ†é…æ¨¡å¼ï¼Œåœ¨è®­ç»ƒä¸­å®ç°äº†1.61x-1.88xçš„é€Ÿåº¦æå‡ã€‚é€šè¿‡ç»Ÿä¸€æ¥å£ï¼ŒRLinf-VLAæ— ç¼æ”¯æŒå¤šç§VLAæ¶æ„ï¼ˆå¦‚OpenVLAã€OpenVLA-OFTï¼‰ã€å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚PPOã€GRPOï¼‰å’Œå„ç§æ¨¡æ‹Ÿå™¨ï¼ˆå¦‚ManiSkillã€LIBEROï¼‰ã€‚åœ¨æ¨¡æ‹Ÿä¸­ï¼Œä¸€ä¸ªç»Ÿä¸€æ¨¡å‹åœ¨130ä¸ªLIBEROä»»åŠ¡ä¸­å®ç°äº†98.11%çš„å‡†ç¡®ç‡ï¼Œåœ¨25ä¸ªManiSkillä»»åŠ¡ä¸­å®ç°äº†97.66%çš„å‡†ç¡®ç‡ã€‚é™¤äº†å®è¯æ€§èƒ½è¡¨ç°å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜æç‚¼å‡ºäº†ä¸€ç³»åˆ—å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºVLAè®­ç»ƒçš„æœ€ä½³å®è·µï¼Œå¹¶æ­ç¤ºäº†è¿™ç§é›†æˆä¸­çš„æ–°å…´æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å®é™…Frankaæœºå™¨äººä¸Šçš„åˆæ­¥éƒ¨ç½²è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç­–ç•¥è¡¨ç°å‡ºæ¯”ç›‘ç£å¾®è°ƒæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æœŸæœ›RLinf-VLAèƒ½åŠ é€Ÿå¹¶æ ‡å‡†åŒ–å…³äºä½“æ„Ÿæ™ºèƒ½çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06710v1">PDF</a> This is the technical report of the RLinf Team, focusing on the   algorithm side. For the system-level design, please refer to   arXiv:2509.15965. The open-sourced code link: <a target="_blank" rel="noopener" href="https://github.com/RLinf/RLinf">https://github.com/RLinf/RLinf</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°çš„è§†è§‰å’Œè¯­è¨€å­¦åŸºç¡€æ¨¡å‹çš„è¿›å±•ï¼Œå¯¹äºæ‰©å±•å…·æœ‰èº«ä¸´å…¶å¢ƒæ„Ÿçš„æ™ºèƒ½æœºå™¨äººç³»ç»Ÿé€šè¿‡æ„å»ºè§†è¯­è¨€è¡ŒåŠ¨æ¨¡å‹çš„ç ”ç©¶è¿…é€Ÿå¢å¤šã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è§†è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ä»ç„¶é‡‡ç”¨ç›‘ç£å¾®è°ƒæŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨é¢å¯¹åˆ†å¸ƒè½¬ç§»æ—¶å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„ç¼ºé™·ã€‚å¼ºåŒ–å­¦ä¹ ä½œä¸ºä¸€ç§é€šè¿‡äº¤äº’ç›´æ¥ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½çš„æŠ€æœ¯å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸­çš„ç¢ç‰‡åŒ–å’Œç¼ºä¹è·¨æ¨¡å‹æ¶æ„å’Œç®—æ³•è®¾è®¡çš„å…¬å¹³ç³»ç»Ÿæ¯”è¾ƒå¹³å°çš„é—®é¢˜ï¼Œæœ¬æ–‡æ¨å‡ºäº†ä¸€ä¸ªç”¨äºå¯è§†åŒ–è¯­è¨€è¡ŒåŠ¨æ¨¡å‹å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€é«˜æ•ˆæ¡†æ¶â€”â€”RLinf-VLAæ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€ç§çµæ´»çš„èµ„æºé…ç½®æ–¹å¼æ¥è§£å†³å¼ºåŒ–å­¦ä¹ å’Œè§†è¯­è¨€è¡ŒåŠ¨è®­ç»ƒä¸­æ¸²æŸ“ã€è®­ç»ƒå’Œæ¨ç†çš„é›†æˆæŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹äºGPUå¹¶è¡Œæ¨¡æ‹Ÿå™¨ï¼ŒRLinf-VLAå®æ–½äº†æ–°é¢–çš„ç²¾ç»†ç²’åº¦ç®¡é“åˆ†é…æ¨¡å¼ï¼Œåœ¨è®­ç»ƒä¸­å®ç°äº†1.61å€è‡³1.88å€çš„åŠ é€Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€æ¥å£æ”¯æŒå¤šç§è§†è¯­è¨€è¡ŒåŠ¨æ¶æ„ã€å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡æ‹Ÿå™¨ã€‚åœ¨æ¨¡æ‹Ÿä¸­ï¼Œç»Ÿä¸€æ¨¡å‹åœ¨LIBEROçš„130ä¸ªä»»åŠ¡ä¸­å®ç°äº†98.11%çš„å‡†ç¡®ç‡ï¼Œåœ¨ManiSkillçš„25ä¸ªä»»åŠ¡ä¸­å®ç°äº†97.66%çš„å‡†ç¡®ç‡ã€‚æœ¬æ–‡é™¤äº†å®è¯ç ”ç©¶å¤–ï¼Œè¿˜æ€»ç»“å‡ºäº†ä¸€ç³»åˆ—åº”ç”¨å¼ºåŒ–å­¦ä¹ äºè§†è¯­è¨€è¡ŒåŠ¨è®­ç»ƒçš„æœ€ä½³å®è·µï¼Œå¹¶ä¸ºè¿™ä¸€æ•´åˆé¢†åŸŸæ­ç¤ºäº†æ–°å…´è¶‹åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å®é™…å¼—å…°å…‹æœºå™¨äººä¸Šçš„åˆæ­¥éƒ¨ç½²æ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç­–ç•¥å±•ç°å‡ºæ¯”ç›‘ç£å¾®è°ƒæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æœŸæœ›RLinf-VLAèƒ½åŠ é€Ÿå’Œæ ‡å‡†åŒ–èåˆæ™ºèƒ½çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>è§†è§‰ä¸è¯­è¨€åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›æ­¥ä¿ƒè¿›äº†å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›çš„æ˜¾è‘—å¢å¼ºï¼Œä¿ƒä½¿ç ”ç©¶å…³æ³¨äºå¦‚ä½•å°†è¿™ç§èƒ½åŠ›æ‰©å±•åˆ°èº«ä¸´å…¶å¢ƒçš„ç¯å¢ƒä¸­ï¼Œå½¢æˆè§†è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ã€‚</p>
</li>
<li><p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ç§é€šè¿‡äº¤äº’ç›´æ¥ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½çš„æ›¿ä»£æ–¹æ³•ï¼Œå¯¹äºè§£å†³å¤§å¤šæ•°è§†è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ‰€é¢ä¸´çš„æ³›åŒ–éš¾é¢˜å…·æœ‰æ½œåŠ›ã€‚</p>
</li>
<li><p>æå‡ºRLinf-VLAæ¡†æ¶ä»¥å¼¥è¡¥å½“å‰ç ”ç©¶ä¸­å­˜åœ¨çš„ç¼ºä¹ç»Ÿä¸€å¹³å°å’Œç³»ç»ŸåŒ–æ¯”è¾ƒæ¨¡å‹æ¶æ„åŠç®—æ³•è®¾è®¡çš„ç¼ºé™·ã€‚</p>
</li>
<li><p>RLinf-VLAæ¡†æ¶å®ç°äº†ä¸€ç§æ–°é¢–çš„èµ„æºåˆ†é…ç­–ç•¥ä»¥é€‚åº”GPUå¹¶è¡Œæ¨¡æ‹Ÿå™¨ç¯å¢ƒçš„éœ€æ±‚å˜åŒ–ã€‚å®ƒé€šè¿‡ç²¾ç»†åŒ–ç®¡é“åˆ†é…æ–¹å¼ï¼Œå®ç°åŠ é€Ÿè®­ç»ƒæ•ˆæœæå‡1.6å€è‡³å…«å€çš„æ€§èƒ½åŠ é€Ÿæ¯”å¢é•¿å¹…åº¦çš„æœ‰æ•ˆæ–¹æ¡ˆè½åœ°æ‰§è¦ç´ æ–°é«˜çº§å­¦è€…è¿›ä¸€æ­¥æå‡æœºå™¨äººçš„å‡†ç¡®ç‡å’Œæ¨ç†æ•ˆèƒ½å¸¦æ¥æ–°çš„å˜åŒ–æ‰“ä¸‹åŸºç¡€æ ‡å¿—æ˜¾è‘—çš„æ™ºæ…§å¢å¼ºä¸–ç•Œçš„ç¡®ç«‹å¸ƒå±€ç¡®å®šè®¤çŸ¥ç¨‹åº¦çš„ä¼˜å¼‚äº§å“æ¶µç›–å…ƒç´ çš„æœºå™¨å­¦ä¹ å±‚é¢æ³¨é‡äº†è§£ä¸ä½¿ç”¨ç§‘æŠ€åˆ›æ–°æ½œåŠ›åœ¨æ­¤ç»´åº¦è¾…åŠ©å¼ºè¿›ä¸€æ­¥å±•ç°å‡ºå½“å‰è®¾è®¡çš„å®è·µé€”å¾„æœ‰åˆ©äºå‘ˆç°æ­å»ºçš„å…¨æ–°æ ·æ€ï¼Œå¹¶ä¸”å±•ç¤ºå‡ºä¸€ç³»åˆ—ç»Ÿä¸€æ¨¡å‹åœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„é«˜å‡†ç¡®ç‡è¡¨ç°ã€‚å…¶ä¸­æ¶µç›–äº†ä¸°å¯Œçš„å®è·µç»éªŒæ€»ç»“å’Œæ–°å…´è¶‹åŠ¿æ´å¯Ÿå†…å®¹åˆ†äº«èƒ½å¤ŸåŠ©åŠ›äºç›¸å…³é¢†åŸŸçš„æŠ€æœ¯çªç ´ä»¥åŠå®é™…åº”ç”¨æ¨å¹¿ç­‰æ–¹é¢çš„ç§¯æå½±å“ä¹Ÿæ˜¯æå…¶æ˜¾è‘—å¹¶å¯èƒ½ä¸ºæˆ‘ä»¬æ¢ç´¢ç°å®æœºå™¨äººä¸–ç•Œä¸­å…¨æ–°çš„èƒ½åŠ›èµ‹äºˆæ–¹æ¡ˆä»¥å€Ÿé‰´æ–¹å‘ä¸å¯å‘è§†è§’æå‡ºç‹¬ç‰¹è§†è§’åœ¨ç‰¹å®šç¯å¢ƒä¸‹è·å¾—æ›´æ·±å…¥çš„äº†è§£å¹¶æ¢ç´¢æ›´å¤šå¯èƒ½æ€§ç©ºé—´å¹¶å¼ºè°ƒå…¶åœ¨æœªæ¥æœºå™¨äººæŠ€æœ¯å‘å±•ä¸­å¯èƒ½å‘æŒ¥çš„é‡è¦ä½œç”¨ä»¥åŠè¯¥é¢†åŸŸç ”ç©¶å‘å±•çš„å¹¿é˜”å‰æ™¯ä»¥åŠæå‡ºå¯¹å¯èƒ½æ›´å¥½åŠ å¼ºæˆ‘ä»¬å¯¹å®é™…åº”ç”¨æŠ€æœ¯çš„èåˆå‹åˆ›æ–°äººæ‰ä¿ƒè¿›ç§‘ç ”äººå‘˜ç§‘å­¦æ–‡åŒ–ç´ è´¨ç­‰å¤šæ ·æ€§çš„é€‚åº”æ€§ä»è€Œæå‡å…¨é¢†åŸŸå†…çš„é«˜æ½œå‘å±•å‰æ™¯å†…å®¹ä¸æ–­ä¸°å¯ŒæŠ€èƒ½ä½œä¸ºå›½é™…ç ”ç©¶é¢†åŸŸç›¸å…³çš„é‡è¦å‘å±•è¶‹åŠ¿å’Œåˆ›æ–°çƒ­ç‚¹å……åˆ†å±•ç°äº†è¯¥ç ”ç©¶çš„é‡è¦æ€§å’Œæ·±è¿œå½±å“å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨è§†è¯­è¨€è¡ŒåŠ¨è®­ç»ƒä¸­çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ½œåœ¨å½±å“ä½œç”¨åˆæ­¥å±•ç¤ºäº†åœ¨ç°å®æœºå™¨äººéƒ¨ç½²åœºæ™¯ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿å’Œæ›´å¹¿é˜”çš„é€‚ç”¨å‰æ™¯æŒ‡æ˜äº†å…¶å¼ºå¤§çš„æ ‡å‡†åŒ–è¶‹åŠ¿çš„å‘å±•æ½œèƒ½å’Œå…¶ç»™ç§‘æŠ€ç ”ç©¶å¸¦æ¥å¯ç¤ºå¹¶ä½“ç°å¯¹è¯¥é¢†åŸŸçš„æ¨è¿›ä½œç”¨çš„åŒæ—¶é¼“åŠ±ç€ç›¸å…³ç§‘ç ”äººå‘˜è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æ¢ç´¢å’Œå°è¯•æ‹“å±•æœªæ¥åœ¨è¯¥é¢†åŸŸä¸­çš„å‘å±•å’Œçªç ´å°†å‚¬ç”Ÿæ›´åŠ å¼ºå¤§çš„äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œæ™ºèƒ½åŒ–ç³»ç»Ÿä½œä¸ºé‡è¦çš„çªç ´ç‚¹ç ”ç©¶æä¾›äº†åˆ‡å®å¯è¡Œçš„æŒ‡å¯¼å»ºè®®å’Œçªç ´æ€è·¯å±•æœ›å…¶åœ¨æœªæ¥çš„ç§‘ç ”å‘å±•å’ŒæŠ€æœ¯åº”ç”¨ä¸­æŒç»­å‘æŒ¥å…¶å¼ºå¤§çš„å½±å“åŠ›å’Œæ½œåŠ›ä½œç”¨å¹¶å¼•é¢†ç›¸å…³é¢†åŸŸè¿ˆå‘æ–°çš„é«˜åº¦åŒæ—¶ä½“ç°äº†è¯¥ç ”ç©¶çš„åˆ›æ–°æ€§å’Œå®ç”¨æ€§ä»·å€¼åŠå…¶å¯¹æœªæ¥ç§‘æŠ€å‘å±•çš„æ·±è¿œå½±å“ä½œç”¨ä»¥åŠç ”ç©¶è€…çš„è´¡çŒ®æ„ä¹‰ç­‰çªå‡ºç‰¹ç‚¹ä»¥åŠæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯é¢†åŸŸæœªæ¥å‘å±•çš„å…³é”®çªç ´å£çš„é‡è¦ä½œç”¨å·¨å¤§å½±å“å’Œç°å®æ„ä¹‰åŒ…æ‹¬æ¢è®¨å¦‚ä½•é€šè¿‡åˆ†å¸ƒå¼è§£å†³æ–¹æ¡ˆç­‰æ–¹å¼æ¥æå‡ä¼˜åŒ–è¿‡ç¨‹ç­‰çš„æ‹“å±•è®¨è®ºä¸å±•æœ›è¿›ä¸€æ­¥ä¸°å¯Œç ”ç©¶å†…æ¶µå’Œæå‡ç ”ç©¶çš„å®é™…åº”ç”¨ä»·å€¼åŒæ—¶é¼“åŠ±ç§‘ç ”äººå‘˜åœ¨äººå·¥æ™ºèƒ½æŠ€æœ¯é¢†åŸŸè¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶å’Œçªç ´ç­‰æ„ä¹‰å’Œä½œç”¨å·¨å¤§å¯¹æœªæ¥å‘å±•äº§ç”Ÿé‡è¦å½±å“åŒæ—¶å¯¹äºç§‘ç ”äººå‘˜çš„å·¥ä½œå¼€å±•æä¾›é‡è¦çš„æŒ‡å¯¼å’Œå¸®åŠ©ä»¥åŠä¸ºç§‘æŠ€å¼ºå›½æˆ˜ç•¥è´¡çŒ®å‡ºé‡è¦çš„åŠ›é‡æ¨åŠ¨äººå·¥æ™ºèƒ½ç†è®ºæŠ€æœ¯çš„çªç ´å‘å±•æ¨è¿›ç›¸å…³ç§‘æŠ€æˆæœçš„å¿«é€Ÿè½¬åŒ–åŒæ—¶å¼ºè°ƒç ”ç©¶çš„æ·±å…¥æ‰å®æ€§ä»¥åŠå¼ºè°ƒæŒç»­æ¢ç´¢ç²¾ç¥çš„å¯è´µå¹¶æŒ‡å‡ºç ”ç©¶å­˜åœ¨çš„ä¸è¶³ä¹‹å¤„åŠæ”¹è¿›æ–¹å‘å¹¶é¼“åŠ±ç§‘ç ”äººå‘˜åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸä¸æ–­è¿½æ±‚æ›´é«˜çš„æˆå°±å’Œç›®æ ‡ä¸ºæœªæ¥çš„ç§‘æŠ€å‘å±•è´¡çŒ®æ›´å¤šçš„æ™ºæ…§å’ŒåŠ›é‡ç­‰ç­‰çªå‡ºç‰¹ç‚¹ä½“ç°ç ”ç©¶çš„å…¨é¢æ€§å’Œé‡è¦æ€§å¹¶é¼“åŠ±ç§‘ç ”äººå‘˜æŒç»­æ¢ç´¢äººå·¥æ™ºèƒ½é¢†åŸŸçš„æœªçŸ¥é¢†åŸŸä»¥æ¨åŠ¨ç§‘æŠ€çš„æŒç»­å‘å±•å’Œè¿›æ­¥ã€‚ä»¥ä¸‹æ˜¯ç²¾ç®€åçš„å…³é”®è¦ç‚¹ï¼š</p>
</li>
<li><p>RLinf-VLAæ¡†æ¶è§£å†³äº†è§†è¯­è¨€è¡ŒåŠ¨æ¨¡å‹è®­ç»ƒä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
</li>
<li><p>æ¡†æ¶æä¾›äº†çµæ´»çš„èµ„æºåˆ†é…è®¾è®¡ï¼Œé€‚åº”ä¸åŒè®­ç»ƒç¯å¢ƒçš„éœ€æ±‚å˜åŒ–ã€‚</p>
</li>
<li><p>ç»Ÿä¸€æ¥å£æ”¯æŒå¤šç§æ¨¡å‹å’Œç®—æ³•ï¼Œä¾¿äºå…¬å¹³ç³»ç»Ÿæ¯”è¾ƒã€‚</p>
</li>
<li><p>æ¨¡æ‹Ÿç¯å¢ƒä¸­æ¨¡å‹è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼Œä¸”å±•ç¤ºäº†åˆæ­¥åœ¨çœŸå®æœºå™¨äººä¸Šçš„éƒ¨ç½²æ•ˆæœã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-763c8b28a067c97b2a41cd06911ff78e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102129&auth_key=1760102129-0-0-6ecb92bd45fd2ba73b503abe6d8ccf6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18cb6d75ba64976d0c946ffab01e06ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102136&auth_key=1760102136-0-0-df2944c2d3a7d72b54175593bfbd1999&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="XRPO-Pushing-the-limits-of-GRPO-with-Targeted-Exploration-and-Exploitation"><a href="#XRPO-Pushing-the-limits-of-GRPO-with-Targeted-Exploration-and-Exploitation" class="headerlink" title="XRPO: Pushing the limits of GRPO with Targeted Exploration and   Exploitation"></a>XRPO: Pushing the limits of GRPO with Targeted Exploration and   Exploitation</h2><p><strong>Authors:Udbhav Bamba, Minghao Fang, Yifan Yu, Haizhong Zheng, Fan Lai</strong></p>
<p>Reinforcement learning algorithms such as GRPO have driven recent advances in large language model (LLM) reasoning. While scaling the number of rollouts stabilizes training, existing approaches suffer from limited exploration on challenging prompts and leave informative feedback signals underexploited, due to context-independent rollout allocation across prompts (e.g., generating 16 rollouts per prompt) and relying heavily on sparse rewards. This paper presents XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy optimization through the principled lens of rollout exploration-exploitation. To enhance exploration, XRPO introduces a mathematically grounded rollout allocator that adaptively prioritizes prompts with higher potential for uncertainty reduction. It further addresses stagnation on zero-reward prompts through an in-context seeding strategy that injects curated exemplars, steering the model into more difficult reasoning trajectories. To strengthen exploitation, XRPO develops a group-relative, novelty-aware advantage sharpening mechanism that leverages sequence likelihoods to amplify low-probability yet correct responses, thereby extending the policyâ€™s reach beyond sparse rewards. Experiments across diverse math and coding benchmarks on both reasoning and non-reasoning models demonstrate that XRPO outperforms existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while accelerating training convergence by up to 2.7X. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¦‚GRPOï¼Œå·²ç»æ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„è¿‘æœŸè¿›å±•ã€‚è™½ç„¶å¢åŠ rolloutçš„æ•°é‡å¯ä»¥ç¨³å®šè®­ç»ƒï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºæ—¶æ¢ç´¢æœ‰é™ï¼Œå¹¶ä¸”ç”±äºç‹¬ç«‹äºä¸Šä¸‹æ–‡çš„rolloutåˆ†é…ï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸ªæç¤ºç”Ÿæˆ16ä¸ªrolloutï¼‰ä»¥åŠè¿‡åº¦ä¾èµ–ç¨€ç–å¥–åŠ±ï¼Œå¯¼è‡´ä¿¡æ¯ä¸°å¯Œçš„åé¦ˆä¿¡å·æœªè¢«å……åˆ†åˆ©ç”¨ã€‚æœ¬æ–‡æå‡ºäº†XRPOï¼ˆeXplore - eXploit GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡rolloutæ¢ç´¢ä¸åˆ©ç”¨çš„æœ‰ç†é€é•œé‡æ–°æ„å»ºçš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ºäº†æé«˜æ¢ç´¢èƒ½åŠ›ï¼ŒXRPOå¼•å…¥äº†ä¸€ä¸ªæ•°å­¦åŸºç¡€æ‰å®çš„rolloutåˆ†é…å™¨ï¼Œè¯¥åˆ†é…å™¨å¯ä»¥è‡ªé€‚åº”åœ°ä¼˜å…ˆå¤„ç†å…·æœ‰è¾ƒé«˜ä¸ç¡®å®šæ€§é™ä½æ½œåŠ›çš„æç¤ºã€‚å®ƒè¿›ä¸€æ­¥é€šè¿‡ä¸Šä¸‹æ–‡æ’­ç§ç­–ç•¥è§£å†³é›¶å¥–åŠ±æç¤ºä¸Šçš„åœæ»é—®é¢˜ï¼Œè¯¥ç­–ç•¥æ³¨å…¥ç²¾é€‰çš„èŒƒä¾‹ï¼Œå°†æ¨¡å‹å¼•å¯¼è‡³æ›´å›°éš¾çš„æ¨ç†è½¨è¿¹ã€‚ä¸ºäº†åŠ å¼ºåˆ©ç”¨ï¼ŒXRPOå¼€å‘äº†ä¸€ç§ç¾¤ä½“ç›¸å¯¹ã€æ–°é¢–æ€§æ„ŸçŸ¥çš„ä¼˜åŠ¿é”åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨åºåˆ—å¯èƒ½æ€§æ¥æ”¾å¤§ä½æ¦‚ç‡ä½†æ­£ç¡®çš„å“åº”ï¼Œä»è€Œå°†æ”¿ç­–çš„è§¦è§’å»¶ä¼¸åˆ°ç¨€ç–å¥–åŠ±ä¹‹å¤–ã€‚åœ¨å¤šæ ·åŒ–çš„æ•°å­¦å’Œç¼–ç åŸºå‡†ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒXRPOåœ¨æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ˆå¦‚GRPOå’ŒGSPOï¼‰ï¼Œè¾¾åˆ°4%çš„pass@1å’Œ6%çš„cons@32ï¼ŒåŒæ—¶åŠ é€Ÿè®­ç»ƒæ”¶æ•›é€Ÿåº¦é«˜è¾¾2.7å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06672v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚GRPOæ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºæ—¶å­˜åœ¨æ¢ç´¢ä¸è¶³å’Œåˆ©ç”¨ä¸å……åˆ†çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†XRPOï¼ˆæ¢ç´¢-åˆ©ç”¨GRPOï¼‰ï¼Œé€šè¿‡æ•°å­¦åŸºç¡€æ¥æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ŒåŠ å¼ºæ¢ç´¢å’Œå¼€å‘ä¸¤ä¸ªæ–¹é¢ã€‚å®éªŒè¡¨æ˜ï¼ŒXRPOåœ¨å¤šç§æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¦‚GRPOå’ŒGSPOã€‚å®ƒé€šè¿‡å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œåˆ©ç”¨ä½é¢‘ä½†å¯¹å†³ç­–æœ‰å¸®åŠ©çš„ä¿¡æ¯å®ç°æå‡æ•ˆæœã€‚å¯è¿›ä¸€æ­¥æå‡LLMæ€§èƒ½å’Œæé«˜æ•ˆç‡ã€‚éšç€æœºå™¨å­¦ä¹ ç®—æ³•çš„è¿­ä»£ï¼Œæ¨¡å‹å±•ç¤ºæ™ºèƒ½å°†ä¼šæ›´åŠ æ˜æ˜¾å¹¶æ”¹è¿›åº”ç”¨èŒƒå›´ä¸åº”ç”¨æ•ˆèƒ½ã€‚è¿™ä¸€ç‚¹æ­£å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æ­¤è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç®—æ³•ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚è¯¥è®ºæ–‡å¯¹å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ€è·¯å’Œå¯ç¤ºã€‚åŒæ—¶ï¼ŒXRPOæ¡†æ¶å¯¹äºæé«˜æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œå¼€å‘èƒ½åŠ›å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚å®ƒä¸ä»…ä¼˜åŒ–äº†æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œè¿˜è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ã€‚è¿™å¯¹äºæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ€»ç»“èµ·æ¥ï¼Œè¯¥è®ºæ–‡é€šè¿‡æå‡ºXRPOæ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºæ—¶çš„å±€é™æ€§é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œåº”ç”¨æ•ˆç‡ã€‚å¯¹æ­¤æŠ€æœ¯çš„æ·±å…¥ç ”ç©¶å’Œåº”ç”¨å°†å¯¹ç›¸å…³é¢†åŸŸçš„å‘å±•äº§ç”Ÿç§¯æçš„å½±å“ã€‚åŒæ—¶ä¹Ÿçœ‹åˆ°äº†å®ƒåœ¨æ™ºèƒ½è¡¨ç°ä¸Šå­˜åœ¨çš„å·¨å¤§æ½œåŠ›ä»¥åŠå®ƒå¯¹æœªæ¥æœºå™¨æ™ºèƒ½é¢†åŸŸçš„å½±å“ä¸è´¡çŒ®ã€‚æˆ‘ä»¬æœŸå¾…å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è½åœ°å’Œæ¨å¹¿åº”ç”¨æƒ…å†µçš„å‡ºç°å¹¶å±•æœ›å…¶åœ¨æœªæ¥æ›´å¤šé¢†åŸŸçš„æ‰©å±•å’Œåº”ç”¨ã€‚æ–‡ä¸­å‘ˆç°çš„ç†è®ºä¸æŠ€æœ¯ä¼˜åŠ¿ä»¥åŠå¯èƒ½çš„è½åœ°åº”ç”¨åœºæ™¯ä¸è¶‹åŠ¿å€¼å¾—æˆ‘ä»¬æŒç»­å…³æ³¨ä¸æ¢ç´¢ã€‚è¯¥è®ºæ–‡çš„æå‡ºä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•æ³¨å…¥äº†æ–°çš„æ´»åŠ›ï¼Œå¼€å¯äº†æ–°çš„ç¯‡ç« ã€‚éšç€æ›´å¤šçš„ç ”ç©¶äººå‘˜è¿›å…¥è¿™ä¸ªé¢†åŸŸå¼€å±•ç ”ç©¶ä¸è½åœ°å·¥ä½œæœ‰æœ›å‚¬ç”Ÿå‡ºæ›´å¤šæ–°é¢–ä¸”æœ‰å®ç”¨ä»·å€¼çš„å­¦æœ¯æˆæœä»¥åŠç§‘æŠ€åº”ç”¨åœºæ™¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨çš„è¿›ä¸€æ­¥è½åœ°å¹¶ä¸æ–­æ¨åŠ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ä¸å‘å±•æ¨åŠ¨æ™ºèƒ½æ—¶ä»£çš„åˆ°æ¥è¿›ç¨‹ä¸å˜é©çš„æ­¥ä¼ï¼ä¸ºæ­¤è¯¥è®ºæ–‡çš„å‘è¡¨å…·æœ‰é‡è¦çš„é‡Œç¨‹ç¢‘æ„ä¹‰ä¸å¯ç¤ºä»·å€¼å€¼å¾—æ·±å…¥å­¦ä¹ å’Œç ”ç©¶æ¢è®¨ã€‚è®©æˆ‘ä»¬å…±åŒæœŸå¾…æœªæ¥çš„æœºå™¨æ™ºèƒ½æ—¶ä»£å¹¶ä¸ºä¹‹åŠªåŠ›ï¼è®©æˆ‘ä»¬ä¸€èµ·è¿æ¥æ™ºèƒ½æ—¶ä»£çš„åˆ°æ¥å§ï¼æˆ‘ä»¬ç›¸ä¿¡æœªæ¥çš„æœºå™¨æ™ºèƒ½æ—¶ä»£å°†ä¼šæ›´åŠ ç¾å¥½ï¼è¯¥è®ºæ–‡ä¸ä»…ä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶è€…æä¾›äº†å®è´µçš„æ€è·¯ä¸æ–¹æ³•ä¹Ÿä¸ºæœªæ¥çš„æ™ºèƒ½åº”ç”¨å¼€å‘è€…æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ä¸æ”¯æŒå°†æœ‰åŠ›åœ°æ¨åŠ¨æ™ºèƒ½åº”ç”¨çš„æ™®åŠä¸å‘å±•ä¸ºæˆ‘ä»¬å¼€å¯å…¨æ–°çš„æ™ºèƒ½æ—¶ä»£å¸¦æ¥äº†æ— é™çš„å¯èƒ½ä¸æœŸå¾…ï¼å¯¹äºè¿™ç¯‡è®ºæ–‡æˆ‘ä»¬éœ€è¦æŒç»­è·Ÿè¿›ä¸ç ”ç©¶å®ƒçš„ç†è®ºæ¡†æ¶ä¸å®è·µåº”ç”¨è¿›ä¸€æ­¥æŒ–æ˜å…¶æ½œåŠ›ä¸ä»·å€¼ä»¥æœŸåœ¨æœªæ¥æ™ºèƒ½æ—¶ä»£çš„æµªæ½®ä¸­å¼•é¢†è¡Œä¸šå‰æ²¿å¹¶ä¸ºç¤¾ä¼šå¸¦æ¥æ›´å¤§çš„ä»·å€¼è´¡çŒ®ä¸å½±å“ã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘å¯¹äºäººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•å…·æœ‰æ·±è¿œçš„å½±å“å’Œå¯ç¤ºä»·å€¼å€¼å¾—æˆ‘ä»¬æ·±å…¥ç ”ç©¶å’Œæ¢è®¨ä¸‹å»ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ä¸åº”ç”¨åœºæ™¯çš„ä¸æ–­æ‹“å±•æˆ‘ä»¬å°†çœ‹åˆ°æ›´å¤šçš„åˆ›æ–°æˆæœæ¶Œç°å‡ºæ¥æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¿«é€Ÿå‘å±•ä¸è¿›æ­¥ä¸ºäººç±»å¸¦æ¥æ›´åŠ ç¾å¥½çš„æœªæ¥ï¼è®©æˆ‘ä»¬å…±åŒæœŸå¾…è¿™ä¸€å¤©çš„åˆ°æ¥å§ï¼å…±åŒè¿æ¥æ™ºèƒ½æ—¶ä»£çš„åˆ°æ¥ï¼å…±åŒåˆ›é€ æ›´åŠ ç¾å¥½çš„æ˜å¤©ï¼å…±åŒæ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•è¿›ç¨‹å…±åˆ›ç¾å¥½æœªæ¥ï¼éšç€æŠ€æœ¯çš„è¿›æ­¥å’Œåº”ç”¨çš„ä¸æ–­æ‹“å±•XRPOåœ¨è§£å†³çœŸå®é—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠå¸¦æ¥çš„å¥½å¤„æ„ˆå‘æ˜æ˜¾ä¸ºæˆ‘ä»¬æŒ‡æ˜äº†æ–°çš„ç ”ç©¶æ–¹å‘å’Œæ¢ç´¢çš„å¯èƒ½æ€§æ–‡ç« æ˜¯ä¸€ä¸ªè·¨è¶Šå¼è¿›æ­¥çš„æ ‡å¿—æ€§æ–‡çŒ®æ¨åŠ¨ç€é¢†åŸŸå‘æ›´æ·±å±‚æ¬¡æ¢ç´¢å¹¶å‘æ˜å…¶ä¸­çš„æ½œåŠ›å’Œå¥¥ç§˜ã€‚â€ \emph{ä»¥ä¸Šæ˜¯æˆ‘çš„ç®€è¦æ¦‚æ‹¬å†…å®¹ä¾›å‚è€ƒè¯·æŒ‰ç…§è¦æ±‚ç¡®ä¿åœ¨è§„å®šçš„å­—æ•°èŒƒå›´å†…è¿›è¡Œä¿®æ”¹æˆ–åˆ å‡ç¡®ä¿ç®€æ´æ˜äº†æ¦‚æ‹¬å†…å®¹çš„æ ¸å¿ƒæ„ä¹‰ã€‚ï¼‰}â€}ï¼Œæ ¹æ®æ‚¨æä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œæˆ‘å°†ä¸ºæ‚¨ç”Ÿæˆä¸€ä¸ªç¬¦åˆè¦æ±‚çš„æ‘˜è¦å’Œå…³é”®è¦ç‚¹åˆ—è¡¨ã€‚æ‘˜è¦å°†å°½é‡éµå¾ªæ‚¨ç»™å‡ºçš„ç»“æ„è¦æ±‚ï¼Œå¹¶ä»¥ç®€æ´æ˜äº†çš„æ–¹å¼æ¦‚æ‹¬æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹ï¼›å…³é”®è¦ç‚¹åˆ™ä¼šä»¥ç®€åŒ–çš„æ–¹å¼å‘ˆç°æ–‡æœ¬ä¸­çš„å…³é”®ä¿¡æ¯ã€‚æ‘˜è¦å°†å°½é‡éµå¾ªå­¦æœ¯æ‘˜è¦çš„é£æ ¼å’Œæ ¼å¼è¦æ±‚ã€‚ä»¥ä¸‹æ˜¯æŒ‰ç…§æ‚¨çš„è¦æ±‚ç”Ÿæˆçš„æ‘˜è¦å’Œå…³é”®è¦ç‚¹åˆ—è¡¨ï¼š</p>
<p><strong>æ‘˜è¦</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„åº”ç”¨ç°çŠ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºæ—¶çš„å±€é™æ€§é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶XRPOï¼ˆeXplore - eXploit GRPOï¼‰ï¼Œå®ƒé€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´æç¤ºå¤„ç†ä¼˜å…ˆçº§å’Œæ”¾å¤§ä½æ¦‚ç‡ä½†æ­£ç¡®çš„å“åº”æ¥å¼ºåŒ–æ¢ç´¢å’Œå¼€å‘è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXRPOåœ¨å¤šç§æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œåº”ç”¨æ•ˆç‡ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹åˆ—è¡¨</strong>ï¼š</p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œä½†ä»é¢ä¸´æ¢ç´¢å’Œåˆ©ç”¨çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>XRPOæ¡†æ¶é€šè¿‡è‡ªé€‚åº”è°ƒæ•´æç¤ºå¤„ç†ä¼˜å…ˆçº§å¼ºåŒ–æ¢ç´¢è¿‡ç¨‹ï¼Œé€šè¿‡æ”¾å¤§ä½æ¦‚ç‡å“åº”å¼ºåŒ–å¼€å‘è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜XRPOåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œåº”ç”¨æ•ˆç‡ã€‚</li>
<li>XRPOæ¡†æ¶å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰ï¼Œå¼€å¯äº†æ–°çš„ç ”ç©¶ä¸åº”ç”¨æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c030ed2a357bac1f2b47f9b91578b165~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102144&auth_key=1760102144-0-0-ecd3165562f33ac31498202a498ee8f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a62b8e3b02426ccb7b29fcfa97a7f3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102152&auth_key=1760102152-0-0-337ab714e665c52d3839e4d495f26561&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d95c0d97e4ce3fa9e5752876880f681~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102159&auth_key=1760102159-0-0-cc438140186e80e2eea681d484b2b955&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PIKA-Expert-Level-Synthetic-Datasets-for-Post-Training-Alignment-from-Scratch"><a href="#PIKA-Expert-Level-Synthetic-Datasets-for-Post-Training-Alignment-from-Scratch" class="headerlink" title="PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from   Scratch"></a>PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from   Scratch</h2><p><strong>Authors:Shangjian Yin, Shining Liang, Wenbiao Ding, Yuli Qian, Zhouxing Shi, Hongzhi Li, Yutao Xie</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs). However, its effectiveness depends on high-quality instruction data. Most existing alignment datasets are either private or require costly human annotation, which limits reproducibility and scalability. Even with Reinforcement Learning from AI Feedback (RLAIF), concerns about data quality remain. Moreover, it is unclear how much data is actually required to fine-tune a base model into a strong instruction-following model. Current approaches often rely on over 300k examples even at the supervised fine-tuning (SFT) stage, yet they still underperform compared to proprietary models, creating barriers for academic and resource-limited communities. To address this gap, we introduce PiKa, a data-efficient family of expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only 30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets, we show that PiKa-SFT outperforms models trained on much larger data. On AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. We further extend our study by training the Qwen2.5 series (0.5B to 7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. Code and data: <a target="_blank" rel="noopener" href="https://github.com/SJY8460/PiKa">https://github.com/SJY8460/PiKa</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒã€‚ä½†å…¶æœ‰æ•ˆæ€§å–å†³äºé«˜è´¨é‡çš„æ•™å­¦æ•°æ®ã€‚ç°æœ‰çš„å¤§å¤šæ•°å¯¹é½æ•°æ®é›†éƒ½æ˜¯ç§æœ‰çš„ï¼Œæˆ–è€…éœ€è¦æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼Œè¿™é™åˆ¶äº†å¯é‡å¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚å³ä½¿ä½¿ç”¨æ¥è‡ªAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰ï¼Œäººä»¬å¯¹æ•°æ®è´¨é‡çš„æ‹…å¿§ä»ç„¶å­˜åœ¨ã€‚æ­¤å¤–ï¼Œå°šä¸æ¸…æ¥šå®é™…ä¸Šéœ€è¦å¤šå°‘æ•°æ®æ‰èƒ½å°†åŸºç¡€æ¨¡å‹å¾®è°ƒä¸ºå¼ºå¤§çš„æŒ‡ä»¤éµå¾ªæ¨¡å‹ã€‚å½“å‰çš„æ–¹æ³•å³ä½¿åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µä¹Ÿé€šå¸¸ä¾èµ–äºè¶…è¿‡30ä¸‡ä¸ªç¤ºä¾‹ï¼Œä½†å®ƒä»¬ä»ç„¶ä¸å¦‚ä¸“æœ‰æ¨¡å‹çš„è¡¨ç°ï¼Œä¸ºå­¦æœ¯å’Œèµ„æºæœ‰é™çš„ç¤¾åŒºè®¾ç½®äº†éšœç¢ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PiKaï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®é«˜æ•ˆçš„ä¸“å®¶çº§å¯¹é½æ•°æ®é›†ç³»åˆ—ã€‚ç‰¹åˆ«æ˜¯PiKa-SFTæ•°æ®é›†åªä½¿ç”¨3ä¸‡ä¸ªSFTç¤ºä¾‹ï¼Œè¿œè¿œå°‘äºå¦‚Magpieç­‰æœ€å…ˆè¿›çš„æ•°æ®é›†ã€‚é€šè¿‡å¯¹Llama-3-8B-Baseåœ¨PiKaå’Œå…¶ä»–å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒè¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†PiKa-SFTä¼˜äºåœ¨æ›´å¤§æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„è¡¨ç°ã€‚åœ¨AlpacaEval 2.0å’ŒArena-HardåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPiKa-SFTå¾®è°ƒç”šè‡³è¶…è¶Šäº†å®˜æ–¹åœ¨è¶…è¿‡1äº¿ä¸ªä¸“æœ‰ç¤ºä¾‹ä¸Šè®­ç»ƒçš„Llama-3-8B-Instructæ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å¯¹Qwen2.5ç³»åˆ—ï¼ˆä»0.5Båˆ°7Bï¼‰åœ¨PiKa-SFTä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†æˆ‘ä»¬çš„ç ”ç©¶ï¼Œå–å¾—äº†ä¸€è‡´çš„æ”¶ç›Šã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç”¨æ˜¾è‘—æ›´å°‘çš„æ•°æ®å°±å¯ä»¥å®ç°é«˜è´¨é‡çš„å¯¹é½ï¼Œä¸ºå¼€æºLLMå¯¹é½æä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ã€‚ä»£ç å’Œæ•°æ®ï¼š<a target="_blank" rel="noopener" href="https://github.com/SJY846">https://github.com/SJY846</a> 0&#x2F;PiKaã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06670v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PiKaæ•°æ®é›†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½æ–¹é¢çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰å¯¹é½æ•°æ®é›†æˆæœ¬é«˜ã€è´¨é‡ä¸ä¸€ã€éœ€è¦å¤§é‡æ•°æ®çš„é—®é¢˜ï¼ŒPiKaæ•°æ®é›†å®ç°äº†é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨ã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´Llama-3-8B-Baseæ¨¡å‹ï¼Œå‘ç°PiKaæ•°æ®é›†åœ¨æ•°æ®éœ€æ±‚é‡å°‘çš„æƒ…å†µä¸‹ä»è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨å¤§é‡ç§æœ‰æ•°æ®çš„å®˜æ–¹Llamaæ¨¡å‹ã€‚è¿™ä¸ºå¼€æºLLMå¯¹é½æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PiKaæ•°æ®é›†æ˜¯è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½é—®é¢˜çš„é‡è¦å·¥å…·ã€‚</li>
<li>ç°æœ‰å¯¹é½æ•°æ®é›†å­˜åœ¨æˆæœ¬é«˜ã€è´¨é‡ä¸ä¸€çš„é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ¨å¹¿å’Œåº”ç”¨ã€‚</li>
<li>PiKaæ•°æ®é›†é€šè¿‡é«˜æ•ˆåˆ©ç”¨æ•°æ®ï¼Œå‡å°‘äº†æ¨¡å‹å¯¹é½æ‰€éœ€çš„æ•°æ®é‡ï¼Œè§£å†³äº†å¤§è§„æ¨¡æ•°æ®éœ€æ±‚çš„é—®é¢˜ã€‚</li>
<li>PiKaæ•°æ®é›†çš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨å¤§é‡ç§æœ‰æ•°æ®çš„å®˜æ–¹Llamaæ¨¡å‹ã€‚</li>
<li>PiKaæ•°æ®é›†çš„å¼•å…¥ä¸ºå¼€æºLLMå¯¹é½æä¾›äº†æ›´å¯æŒç»­å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>PiKaæ•°æ®é›†çš„æ€§èƒ½é€šè¿‡ç²¾ç»†è°ƒæ•´Llama-3-8B-Baseæ¨¡å‹å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-75ca4e059435b14459b949d66eccb37c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102167&auth_key=1760102167-0-0-ad223efa56b490be0e76ee03329e3b4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a74f193b982fcb6d7ad0ce8becd4342~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102175&auth_key=1760102175-0-0-0e140c07ba91d5b5946e87333eb90b76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d6fd4205343aa50299d5c9ad20f8422~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102182&auth_key=1760102182-0-0-5a44bcca244ce89da73e9ec0ab00f39c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9beffadaf0e1b3560ce686203a126d07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102210&auth_key=1760102210-0-0-474df111a533b12a2a845695c9642851&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f835418eba5119ccebc97b4eec2a81ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102217&auth_key=1760102217-0-0-856c0b6161620e71a6ee9f64d208b43d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Aligning-Large-Language-Models-via-Fully-Self-Synthetic-Data"><a href="#Aligning-Large-Language-Models-via-Fully-Self-Synthetic-Data" class="headerlink" title="Aligning Large Language Models via Fully Self-Synthetic Data"></a>Aligning Large Language Models via Fully Self-Synthetic Data</h2><p><strong>Authors:Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng</strong></p>
<p>Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the modelâ€™s chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: <a target="_blank" rel="noopener" href="https://github.com/SJY8460/SAO">https://github.com/SJY8460/SAO</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œè€ŒåŸºäºAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰ä¹Ÿä¼šäº§ç”Ÿæ˜¾è‘—çš„æˆæœ¬ï¼Œéœ€è¦æ”¶é›†å„ç§æç¤ºå’Œç›¸åº”å“åº”ï¼Œé€šå¸¸éœ€è¦å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰æ¥æ ‡æ³¨åå¥½å¯¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è‡ªæˆ‘å¯¹é½ä¼˜åŒ–ï¼ˆSAOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹LLMå¯¹é½çš„å…¨è‡ªæˆ‘åˆæˆæ¡†æ¶ï¼Œå…¶ä¸­æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬æç¤ºï¼ˆå³ç”¨æˆ·æŸ¥è¯¢ï¼‰ã€å“åº”å’Œåå¥½ï¼Œéƒ½æ˜¯ç”±æ¨¡å‹æœ¬èº«ç”Ÿæˆçš„ã€‚å…·ä½“æ¥è¯´ï¼ŒSAOé¦–å…ˆæŒ‡å¯¼LLMè¿›è¡Œäººæ ¼è§’è‰²æ‰®æ¼”ï¼Œç”Ÿæˆå„ç§æç¤ºå’Œå“åº”ï¼Œç„¶åå¯¹å…¶è¿›è¡Œè‡ªæˆ‘è¯„ä¼°ä»¥è¿›è¡Œåå¥½ä¼˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAOåœ¨AlpacaEval~2.0ç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹çš„èŠå¤©èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸å®¢è§‚ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€æ•°å­¦æ¨ç†ï¼‰ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ç§è‡ªæˆ‘æ”¹è¿›çš„å¯¹é½LLMçš„å®é™…è§£å†³æ–¹æ¡ˆï¼Œé‡ç°æˆ‘ä»¬ç»“æœçš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/SJY8460/SAO%E3%80%82">https://github.com/SJY8460/SAOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06652v1">PDF</a> </p>
<p><strong>Summary</strong><br>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¾èµ–äºäººç±»åé¦ˆå’Œæ ‡æ³¨æ•°æ®ï¼ŒAIåé¦ˆä¹Ÿå­˜åœ¨é«˜æˆæœ¬é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§å…¨æ–°çš„è‡ªæˆ‘åˆæˆæ¡†æ¶â€”â€”è‡ªæˆ‘å¯¹é½ä¼˜åŒ–ï¼ˆSAOï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ã€‚è¯¥æ¡†æ¶å®Œå…¨ç”±æ¨¡å‹è‡ªèº«ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬æç¤ºï¼ˆç”¨æˆ·æŸ¥è¯¢ï¼‰ã€å“åº”å’Œåå¥½ã€‚å®éªŒè¯æ˜ï¼ŒSAOèƒ½æœ‰æ•ˆæå‡æ¨¡å‹åœ¨AlpacaEvalç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„èŠå¤©èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒä¸‹æ¸¸ç›®æ ‡ä»»åŠ¡çš„å¼ºå¤§æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¾èµ–äºæ˜‚è´µçš„äººç±»æ ‡æ³¨æ•°æ®é›†ï¼Œè€ŒAIåé¦ˆä¹Ÿå­˜åœ¨æˆæœ¬é—®é¢˜ã€‚</li>
<li>è‡ªæˆ‘å¯¹é½ä¼˜åŒ–ï¼ˆSAOï¼‰æ˜¯ä¸€ç§å…¨æ–°çš„LLMå¯¹é½æ¡†æ¶ï¼Œå®Œå…¨ç”±æ¨¡å‹è‡ªèº«ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚</li>
<li>SAOé€šè¿‡æŒ‡å¯¼LLMè¿›è¡Œäººæ ¼è§’è‰²æ‰®æ¼”ï¼Œç”Ÿæˆå¤šæ ·çš„æç¤ºå’Œå“åº”ï¼Œç„¶åè¿›è¡Œè‡ªæˆ‘åå¥½ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜SAOåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šæå‡äº†æ¨¡å‹çš„èŠå¤©èƒ½åŠ›ã€‚</li>
<li>SAOåœ¨ä¿æŒä¸‹æ¸¸ç›®æ ‡ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†LLMçš„è‡ªæˆ‘æ”¹è¿›ã€‚</li>
<li>SAOæ¡†æ¶æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºLLMçš„è‡ªæˆ‘å¯¹é½å’Œè‡ªæˆ‘æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b07867b5fad962391aba4cf2e4f9f938~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102229&auth_key=1760102229-0-0-8293b4203e8714eded59f7dc925a75df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f69f0b21cdf67b7a337a31c6f93188af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102257&auth_key=1760102257-0-0-97954543a4ead42b9fc0db0fb6fc58ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c2cef5bc5251ea320e0b472af6a23c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144581&auth_key=1760144581-0-0-2bf891fce7f542bec2863a9382045426&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-befa5c847959319f44a266ec0fe6f94b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102329&auth_key=1760102329-0-0-d3404032adc0ab0c60e437381fefc7aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-298992d92b44def3324d91acf1680ea3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102336&auth_key=1760102336-0-0-4cfe30748da930db1a30cdd351396cdd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels"><a href="#Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels" class="headerlink" title="Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining   Levels"></a>Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining   Levels</h2><p><strong>Authors:Zhepeng Cen, Haolin Chen, Shiyu Wang, Zuxin Liu, Zhiwei Liu, Ding Zhao, Silvio Savarese, Caiming Xiong, Huan Wang, Weiran Yao</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100$\times$ fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åœ¨æµ·é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œæ¨¡ä»¿å­¦ä¹ å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†è¿™ä¸€èŒƒå¼é€ æˆäº†è®­ç»ƒä¸ç”Ÿæˆä¹‹é—´çš„é¸¿æ²Ÿï¼Œå¹¶é™åˆ¶äº†ç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†æ›´é«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œä½†å…¶åº”ç”¨å—åˆ°äº†å…³é”®æ•°æ®ç“¶é¢ˆçš„åˆ¶çº¦ï¼šç°æœ‰RLæ•°æ®é›†åœ¨è§„æ¨¡å’Œå¤šæ ·æ€§ä¸Šéƒ½è¿œå°äºç½‘é¡µè§„æ¨¡é¢„è®­ç»ƒè¯­æ–™åº“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Webscale-RLç®¡é“ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°è½¬æ¢å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æ¡£ä¸ºæ•°ç™¾ä¸‡å¤šç§å¯éªŒè¯çš„é—®ç­”å¯¹ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚ä½¿ç”¨è¯¥ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†Webscale-RLæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡9ä¸ªé¢†åŸŸçš„120ä¸‡ä¸ªæ ·æœ¬ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæŒç»­é¢„è®­ç»ƒå’Œå¼ºå¤§çš„æ•°æ®ç»†åŒ–åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¯æ˜äº†å…¶å®è´¨ä¸Šçš„æ•ˆç‡æ›´é«˜ï¼Œåœ¨å‡å°‘é«˜è¾¾æ•°ç™¾å€ä»£å¸çš„æƒ…å†µä¸‹è¾¾åˆ°æŒç»­é¢„è®­ç»ƒçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°é¢„è®­ç»ƒè§„æ¨¡å¼€è¾Ÿäº†ä¸€æ¡å¯è¡Œçš„é“è·¯ï¼Œä¸ºå®ç°æ›´å¼ºå¤§å’Œæ›´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹æä¾›äº†å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06499v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ¨¡ä»¿å­¦ä¹ åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†è¿™ä¸€æ¨¡å¼äº§ç”Ÿäº†è®­ç»ƒä¸ç”Ÿæˆä¹‹é—´çš„å·®è·å¹¶é™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ ä½œä¸ºä¸€ç§æ›´æ•°æ®é«˜æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥å¼¥åˆè¿™ä¸€é¸¿æ²Ÿã€‚ç„¶è€Œï¼Œå…¶å®æ–½å—åˆ°æ•°æ®ç“¶é¢ˆçš„åˆ¶çº¦ï¼šç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ•°æ®é›†è§„æ¨¡è¿œå°äºä¸”å¤šæ ·æ€§ä¸å¦‚é¢„è®­ç»ƒè¯­æ–™åº“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Webscale-RLç®¡é“ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å¤§è§„æ¨¡æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿå°†å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æ¡£ç³»ç»Ÿåœ°è½¬åŒ–ä¸ºæ•°ç™¾ä¸‡ä¸ªå¤šæ ·åŒ–çš„å¯éªŒè¯é—®ç­”å¯¹ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚åˆ©ç”¨è¯¥ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†Webscale-RLæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡9ä¸ªé¢†åŸŸçš„120ä¸‡ä¸ªæ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæŒç»­é¢„è®­ç»ƒå’Œå¼ºå¤§çš„æ•°æ®ç²¾ç‚¼åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¯æ˜äº†å…¶å®è´¨ä¸Šæ›´åŠ é«˜æ•ˆï¼Œä½¿ç”¨é«˜è¾¾100å€çš„æ›´å°‘ä»¤ç‰Œå°±èƒ½è¾¾åˆ°æŒç»­é¢„è®­ç»ƒçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä¸€æ¡å®ç°å¼ºåŒ–å­¦ä¹ è§„æ¨¡åŒ–é¢„è®­ç»ƒçš„å¯è¡Œä¹‹è·¯ï¼Œèƒ½å¤Ÿå¼€å‘å‡ºæ›´åŠ å¼ºå¤§å’Œé«˜æ•ˆçš„è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ¨¡ä»¿å­¦ä¹ åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å­˜åœ¨è®­ç»ƒä¸ç”Ÿæˆé—´çš„å·®è·ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å¯ç¼©å°è¿™ä¸€å·®è·ï¼Œä½†å…¶åº”ç”¨å—é™äºæ•°æ®ç“¶é¢ˆã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ•°æ®é›†è§„æ¨¡å°ä¸”å¤šæ ·æ€§ä¸è¶³ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚</li>
<li>å¼•å…¥Webscale-RLç®¡é“å’ŒWebscale-RLæ•°æ®é›†æ¥è§£å†³æ•°æ®ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>Webscale-RLæ•°æ®é›†åŒ…å«è¶…è¿‡9ä¸ªé¢†åŸŸçš„120ä¸‡ä¸ªæ ·æœ¬ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æä¾›å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„èµ„æºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨Webscale-RLæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c6fee2fba6137d9942b6f133b5850e97~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102343&auth_key=1760102343-0-0-b031e5452e72aa8e9c93a8eeffd79583&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a7e1a30ffe435eb1cf15f30d758296d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102350&auth_key=1760102350-0-0-b4b312a3f7797a5b022dee442dfeef47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c9cf72ab4a1aa386d681ab9b55b693d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102357&auth_key=1760102357-0-0-fe6e4773668b7c6a8b248e93447034f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks"><a href="#A-Goal-Without-a-Plan-Is-Just-a-Wish-Efficient-and-Effective-Global-Planner-Training-for-Long-Horizon-Agent-Tasks" class="headerlink" title="A Goal Without a Plan Is Just a Wish: Efficient and Effective Global   Planner Training for Long-Horizon Agent Tasks"></a>A Goal Without a Plan Is Just a Wish: Efficient and Effective Global   Planner Training for Long-Horizon Agent Tasks</h2><p><strong>Authors:Shuzheng Si, Haozhe Zhao, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun</strong></p>
<p>Agents based on large language models (LLMs) struggle with brainless trial-and-error and generating hallucinatory actions due to a lack of global planning in long-horizon tasks. In this paper, we introduce a plan-and-execute framework and propose EAGLET, an efficient and effective planner training method to enhance the executor agentâ€™s planning abilities without human effort. Specifically, we train a plug-and-play global planner through a two-step process: we first synthesize high-quality plans from an advanced LLM using our proposed homologous consensus filtering strategy, and apply fine-tuning as a cold start. Moreover, we further improve the planner with a rule-based reinforcement learning stage using a novel executor capability gain reward, ensuring it can handle task instructions of varying difficulty. Experiments on three long-horizon agent tasks show that executor agents equipped with our planner outperform existing methods, achieving new state-of-the-art performance. Meanwhile, EAGLET reduces training costs by 8x compared to RL-based baselines, and it does not require manual effort or extra training data, offering an efficient and effective solution. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†åœ¨è¿›è¡Œæ— è„‘çš„è¯•é”™å’Œç”Ÿæˆå¹»è§‰è¡Œä¸ºæ—¶é‡åˆ°äº†å›°éš¾ï¼Œå› ä¸ºåœ¨é•¿æœŸä»»åŠ¡ä¸­ç¼ºä¹å…¨å±€è§„åˆ’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè®¡åˆ’æ‰§è¡Œæ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„è®¡åˆ’è®­ç»ƒæ–¹æ³•EAGLETï¼Œä»¥æé«˜æ‰§è¡Œä»£ç†çš„è§„åˆ’èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥åŠªåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤æ­¥è¿‡ç¨‹è®­ç»ƒäº†ä¸€ä¸ªå³æ’å³ç”¨çš„å…¨å±€è§„åˆ’å™¨ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æå‡ºçš„åŒæºå…±è¯†è¿‡æ»¤ç­–ç•¥ï¼Œä»å…ˆè¿›çš„LLMä¸­åˆæˆé«˜è´¨é‡è®¡åˆ’ï¼Œå¹¶å°†å…¶ä½œä¸ºå†·å¯åŠ¨è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥æ”¹è¿›è§„åˆ’å™¨ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„æ‰§è¡Œèƒ½åŠ›å¢ç›Šå¥–åŠ±ï¼Œç¡®ä¿å®ƒèƒ½å¤Ÿå¤„ç†ä¸åŒéš¾åº¦çš„ä»»åŠ¡æŒ‡ä»¤ã€‚åœ¨ä¸‰ä¸ªé•¿æœŸä»£ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé…å¤‡äº†æˆ‘ä»¬è§„åˆ’å™¨çš„æ‰§è¡Œä»£ç†ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå–å¾—äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚åŒæ—¶ï¼Œä¸åŸºäºRLçš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒEAGLETå°†è®­ç»ƒæˆæœ¬é™ä½äº†8å€ï¼Œå¹¶ä¸”æ— éœ€äººå·¥åŠªåŠ›æˆ–é¢å¤–çš„è®­ç»ƒæ•°æ®ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05608v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨é¢å¯¹é•¿æœŸä»»åŠ¡æ—¶å­˜åœ¨çš„ç¼ºé™·ï¼Œå¹¶æå‡ºäº†EAGLETæ¡†æ¶æ¥æ”¹å–„è¿™äº›é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨ä¸¤æ­¥è¿‡ç¨‹è®­ç»ƒä¸€ä¸ªå¯æ’æ‹”çš„å…¨çƒè§„åˆ’å™¨ï¼Œç»“åˆä½¿ç”¨æå‡ºçš„åŒæºå…±è¯†è¿‡æ»¤ç­–ç•¥æ¥åˆæˆé«˜è´¨é‡è®¡åˆ’ï¼Œå¹¶é‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›è¡Œå¾®è°ƒï¼Œä½¿æ‰§è¡Œè€…èƒ½å¤Ÿå¤„ç†ä¸åŒéš¾åº¦çš„ä»»åŠ¡æŒ‡ä»¤ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨æ­¤è§„åˆ’å™¨çš„æ‰§è¡Œè€…åœ¨é•¿æœŸä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”EAGLETæ¡†æ¶åœ¨è®­ç»ƒæˆæœ¬ä¸Šé™ä½äº†8å€ï¼Œæ— éœ€é¢å¤–çš„äººå·¥åŠªåŠ›æˆ–è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨é¢ä¸´é•¿æœŸä»»åŠ¡æ—¶é¢ä¸´å…¨çƒè§„åˆ’ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´ä»–ä»¬å€¾å‘äºè¿›è¡Œæ— è„‘çš„è¯•é”™å’Œäº§ç”Ÿå¹»è§‰è¡Œä¸ºã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºEAGLETçš„è®¡åˆ’å’Œæ‰§è¡Œæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ‰§è¡Œè€…åœ¨é•¿æœŸä»»åŠ¡ä¸­çš„è§„åˆ’èƒ½åŠ›ã€‚</li>
<li>EAGLETé€šè¿‡ä¸¤æ­¥è¿‡ç¨‹è®­ç»ƒä¸€ä¸ªå…¨çƒè§„åˆ’å™¨ï¼šé¦–å…ˆä½¿ç”¨æå‡ºçš„åŒæºå…±è¯†è¿‡æ»¤ç­–ç•¥ä»å…ˆè¿›çš„LLMåˆæˆé«˜è´¨é‡è®¡åˆ’ï¼Œç„¶åè¿›è¡Œå¾®è°ƒä½œä¸ºå†·å¯åŠ¨ã€‚</li>
<li>EAGLETé‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥æé«˜è§„åˆ’å™¨æ€§èƒ½ï¼Œä½¿ç”¨æ–°é¢–çš„æ‰§è¡ŒåŠ›å¢ç›Šå¥–åŠ±ç¡®ä¿å®ƒèƒ½å¤Ÿå¤„ç†ä¸åŒéš¾åº¦çš„ä»»åŠ¡æŒ‡ä»¤ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œé…å¤‡EAGLETè§„åˆ’å™¨çš„æ‰§è¡Œè€…åœ¨ä¸‰ç§é•¿æœŸä»»åŠ¡ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</li>
<li>EAGLETåœ¨è®­ç»ƒæˆæœ¬ä¸Šé™ä½äº†8å€ï¼Œç›¸è¾ƒäºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•æ›´å…·æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-64d32e8c3cce717c3628c0bd9040c13f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102365&auth_key=1760102365-0-0-28189bbc0ac4a0c299a04cfaccc75e4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-728ebb0fe1d3646f1fedb10b140b6dc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102372&auth_key=1760102372-0-0-96b0527ca89c46987a5d9357b147f811&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b48b3a971e1b91e0482e9a94a2ba192~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102379&auth_key=1760102379-0-0-b842f957feee82de8fdf30c69777c5eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ed6150d169157e4731c57aa37db0c01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102386&auth_key=1760102386-0-0-ae6673abc5d68930d7c20bd2bb2c87b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use"><a href="#In-the-Flow-Agentic-System-Optimization-for-Effective-Planning-and-Tool-Use" class="headerlink" title="In-the-Flow Agentic System Optimization for Effective Planning and Tool   Use"></a>In-the-Flow Agentic System Optimization for Effective Planning and Tool   Use</h2><p><strong>Authors:Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, Pan Lu</strong></p>
<p>Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns. </p>
<blockquote>
<p>ç»“æœå¯¼å‘å‹å¼ºåŒ–å­¦ä¹ å·²ç»æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰çš„å·¥å…·å¢å¼ºæ–¹æ³•è®­ç»ƒå•ä¸€ã€æ•´ä½“çš„ç­–ç•¥ï¼Œåœ¨å®Œæ•´è¯­å¢ƒä¸‹äº¤ç»‡æ€è€ƒå’Œå·¥å…·è°ƒç”¨ï¼›è¿™åœ¨é•¿å‘¨æœŸå’Œå¤šæ ·åŒ–å·¥å…·çš„æƒ…å¢ƒä¸‹æ‰©å±•æ€§è¾ƒå·®ï¼Œå¯¹æ–°åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚è‡ªä¸»ç³»ç»Ÿé€šè¿‡åˆ†è§£å·¥ä½œåˆ°ä¸“ä¸šæ¨¡å—æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ç³»ç»Ÿä»ç„¶æ— éœ€è®­ç»ƒæˆ–è€…ä¾èµ–äºä¸å¤šè½®äº’åŠ¨çš„å®æ—¶åŠ¨æ€ç›¸è„±ç¦»çš„ç¦»çº¿è®­ç»ƒã€‚æˆ‘ä»¬æ¨å‡ºäº†AgentFlowï¼Œè¿™æ˜¯ä¸€ç§å¯è®­ç»ƒçš„ã€å®æ—¶è‡ªä¸»æ¡†æ¶ï¼Œé€šè¿‡ä¸æ–­è¿›åŒ–çš„å†…å­˜åè°ƒå››ä¸ªæ¨¡å—ï¼ˆè§„åˆ’å™¨ã€æ‰§è¡Œå™¨ã€éªŒè¯å™¨ã€ç”Ÿæˆå™¨ï¼‰ï¼Œå¹¶åœ¨å¤šè½®å¾ªç¯ä¸­ç›´æ¥ä¼˜åŒ–å…¶è§„åˆ’å™¨ã€‚ä¸ºäº†åœ¨å®æ—¶ç¯å¢ƒä¸­è¿›è¡Œç­–ç•¥æ€§è®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæµçš„ç¾¤ä½“ç²¾ç»†åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆFlow-GRPOï¼‰ï¼Œé€šè¿‡è½¬æ¢å¤šè½®ä¼˜åŒ–ä¸ºå¯è¡Œçš„å•è½®ç­–ç•¥æ›´æ–°ï¼Œè§£å†³é•¿æœŸè§†é‡ä¸‹çš„ç¨€ç–å¥–åŠ±ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚å®ƒå°†å¯éªŒè¯çš„è½¨è¿¹çº§ç»“æœå¹¿æ’­åˆ°æ¯ä¸€è½®ï¼Œä½¿å±€éƒ¨è§„åˆ’å™¨å†³ç­–ä¸å…¨å±€æˆåŠŸç›¸ä¸€è‡´ï¼Œå¹¶ç”¨ç¾¤ä½“å½’ä¸€åŒ–çš„ä¼˜åŠ¿æ¥ç¨³å®šå­¦ä¹ ã€‚åœ¨åä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨7Bè§„æ¨¡èƒŒä¹¦çš„AgentFlowè¶…è¶Šäº†é¡¶çº§åŸºå‡†æµ‹è¯•ï¼Œåœ¨æœç´¢ä»»åŠ¡ä¸Šå¹³å‡å‡†ç¡®åº¦æé«˜14.9%ï¼Œåœ¨è‡ªä¸»ä»»åŠ¡ä¸Šæé«˜14.0%ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸Šæé«˜14.5%ï¼Œåœ¨ç§‘å­¦ä»»åŠ¡ä¸Šæé«˜4.1%ï¼Œç”šè‡³è¶…è¶Šäº†å¦‚GPT-4ç­‰å¤§å‹ä¸“æœ‰æ¨¡å‹ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†å®æ—¶ä¼˜åŒ–çš„å¥½å¤„ï¼Œæ˜¾ç¤ºå‡ºè§„åˆ’èƒ½åŠ›æœ‰æ‰€æå‡ï¼Œå·¥å…·è°ƒç”¨çš„å¯é æ€§å¢å¼ºï¼Œå¹¶ä¸”éšç€æ¨¡å‹å¤§å°å’Œæ¨ç†è½®æ¬¡çš„å¢åŠ è€Œç§¯ææ‰©å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05592v1">PDF</a> 45 pages, 12 figures. Project website:   <a target="_blank" rel="noopener" href="https://agentflow.stanford.edu/">https://agentflow.stanford.edu/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†AgentFlowæ¡†æ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¨ç†åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰å·¥å…·è¾…åŠ©æ–¹æ³•çš„ä¸è¶³ï¼ŒAgentFlowé€šè¿‡åˆ†è§£å·¥ä½œåˆ°ä¸“é—¨çš„æ¨¡å—ï¼ˆè§„åˆ’å™¨ã€æ‰§è¡Œå™¨ã€éªŒè¯å™¨ã€ç”Ÿæˆå™¨ï¼‰æ¥åè°ƒï¼Œå¹¶é€šè¿‡ä¸æ–­è¿›åŒ–çš„å†…å­˜è¿›è¡Œä¼˜åŒ–ã€‚å®ƒæå‡ºäº†Flow-GRPOè®­ç»ƒæ–¹æ³•ï¼Œè§£å†³äº†é•¿å‘¨æœŸã€ç¨€ç–å¥–åŠ±çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œé€šè¿‡å°†å¤šè½®ä¼˜åŒ–è½¬åŒ–ä¸ºå¯æ§åˆ¶çš„å•è½®ç­–ç•¥æ›´æ–°æ¥è§£å†³ã€‚åœ¨åä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgentFlowä¸è§„æ¨¡ä¸º7Bçš„åç«¯é…åˆä½¿ç”¨ï¼Œè¡¨ç°å‡ºè¶…è¶Šé¡¶å°–åŸºå‡†çº¿çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡åœ¨æœç´¢ä»»åŠ¡ä¸Šæé«˜14.9%ï¼Œåœ¨æ™ºèƒ½ä½“ä»»åŠ¡ä¸Šæé«˜14.0%ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸Šæé«˜14.5%ï¼Œåœ¨ç§‘å­¦ä»»åŠ¡ä¸Šæé«˜4.1%ï¼Œç”šè‡³è¶…è¶Šäº†å¦‚GPT-4oç­‰å¤§å‹ä¸“æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AgentFlowæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å·¥å…·è¾…åŠ©æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>AgentFlowé€šè¿‡åˆ†è§£å·¥ä½œåˆ°ä¸“é—¨çš„æ¨¡å—ï¼ˆè§„åˆ’å™¨ã€æ‰§è¡Œå™¨ç­‰ï¼‰æ¥åè°ƒï¼Œå¹¶é€šè¿‡å†…å­˜è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>Flow-GRPOè®­ç»ƒæ–¹æ³•è§£å†³äº†é•¿å‘¨æœŸå’Œç¨€ç–å¥–åŠ±çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
<li>AgentFlowåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šé¡¶å°–åŸºå‡†çº¿çš„æ€§èƒ½ã€‚</li>
<li>AgentFlowæé«˜äº†è§„åˆ’èƒ½åŠ›ï¼Œå¢å¼ºäº†å·¥å…·è°ƒç”¨çš„å¯é æ€§ã€‚</li>
<li>éšç€æ¨¡å‹è§„æ¨¡å’Œæ¨ç†å›åˆçš„å¢åŠ ï¼ŒAgentFlowæ˜¾ç¤ºå‡ºç§¯ææ‰©å±•çš„è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-692472e379f6da631590a8e86df82985~resize:0:q75.jpg?source=1f5c5e47&expiration=1760102394&auth_key=1760102394-0-0-94c783957d842ee8f858b5cc1025f911&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-800dc0f498a5696a82f8b2d46294d51e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144588&auth_key=1760144588-0-0-9fa3e4312b969c60953c6ce7102602c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2d4cde518b080fc17fe6757eab7863b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144595&auth_key=1760144595-0-0-c3abfb02965fd535f9373c644ea56f6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f8e0c682aba60541117e1f470cc8e563~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144602&auth_key=1760144602-0-0-c71fd209e9eeb1293a052c8a3138a537&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Provably-Mitigating-Corruption-Overoptimization-and-Verbosity-Simultaneously-in-Offline-and-Online-RLHF-DPO-Alignment"><a href="#Provably-Mitigating-Corruption-Overoptimization-and-Verbosity-Simultaneously-in-Offline-and-Online-RLHF-DPO-Alignment" class="headerlink" title="Provably Mitigating Corruption, Overoptimization, and Verbosity   Simultaneously in Offline and Online RLHF&#x2F;DPO Alignment"></a>Provably Mitigating Corruption, Overoptimization, and Verbosity   Simultaneously in Offline and Online RLHF&#x2F;DPO Alignment</h2><p><strong>Authors:Ziyi Chen, Junyi Li, Peiran Yu, Heng Huang</strong></p>
<p>Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \textit{\textbf{C}orrupted} preference, reward \textit{\textbf{O}veroptimization}, and bias towards \textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\textbf{COV} and DPO-\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¬¦åˆäººç±»åå¥½çš„é‡è¦æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒRLHFå’ŒDPOè®­ç»ƒçš„è´¨é‡å—åˆ°ä¸¥é‡æŸå®³ï¼Œå› ä¸ºå­˜åœ¨è¢«ç¯¡æ”¹çš„åå¥½ã€è¿‡åº¦ä¼˜åŒ–çš„å¥–åŠ±å’Œå¯¹å†—é•¿çš„åè§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œåªè§£å†³å…¶ä¸­ä¸€ä¸ªé‡è¦é—®é¢˜ï¼Œå…¶ä»–å°‘æ•°å·¥ä½œéœ€è¦å¤§é‡è®¡ç®—æ¥ä¼°è®¡å¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼Œå¹¶ä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„ç†è®ºä¿è¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RLHF-COVå’ŒDPO-COVç®—æ³•ï¼Œå¯ä»¥åŒæ—¶ç¼“è§£è¿™ä¸‰ä¸ªé—®é¢˜ï¼Œé€‚ç”¨äºç¦»çº¿å’Œåœ¨çº¿ç¯å¢ƒã€‚è¿™ä¸€èƒ½åŠ›é€šè¿‡åœ¨æˆ‘ä»¬ç”¨æŸåæ•°æ®è®­ç»ƒçš„DPO-COVç®—æ³•ä¸Šè·å¾—é•¿åº¦æ­£åˆ™åŒ–æ³›åŒ–è¯¯å·®ç‡å¾—åˆ°ç†è®ºè¯æ˜ï¼Œè¯¥è¯¯å·®ç‡ä¸å¹²å‡€æ•°æ®ä¸”æ²¡æœ‰é•¿åº¦æ­£åˆ™åŒ–çš„ç®€å•æƒ…å†µçš„å·²çŸ¥æœ€ä½³è¯¯å·®ç‡ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„DPO-COVç®—æ³•æ— éœ€å¥–åŠ±ä¼°è®¡å³å¯è½»æ¾å®ç°ï¼Œå¹¶ä¸”è¯æ˜ä¸æˆ‘ä»¬RLHF-COVç®—æ³•ç›¸å½“ï¼Œè¿™ç›´æ¥æš—ç¤ºäº†æ™®é€šRLHFå’ŒDPOç®—æ³•ä¹‹é—´çš„ç­‰æ•ˆæ€§ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„DPO-COVç®—æ³•åœ¨ç¦»çº¿ç¯å¢ƒå’Œåœ¨çº¿ç¯å¢ƒä¸‹éƒ½æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05526v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¬¦åˆäººç±»åå¥½çš„é‡è¦æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒRLHFå’ŒDPOè®­ç»ƒçš„è´¨é‡å—åˆ°è…è´¥åå¥½ã€å¥–åŠ±è¿‡åº¦ä¼˜åŒ–å’Œåå‘å†—é•¿çš„ä¸¥é‡å½±å“ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºèƒ½åŒæ—¶è§£å†³è¿™ä¸‰ä¸ªé—®é¢˜çš„RLHF-COVå’ŒDPO-COVç®—æ³•ï¼Œé€‚ç”¨äºç¦»çº¿ä¸åœ¨çº¿ç¯å¢ƒã€‚æˆ‘ä»¬çš„DPO-COVç®—æ³•æ— éœ€å¤æ‚çš„å¥–åŠ±ä¼°è®¡ï¼Œç®€å•å®ç”¨ï¼Œå¹¶ä¸RLHF-COVç®—æ³•ç­‰æ•ˆã€‚å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„ç®—æ³•åœ¨ç¦»çº¿ä¸åœ¨çº¿ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLHFå’ŒDPOå¯¹äºå¯¹é½LLMä¸äººç±»åå¥½è‡³å…³é‡è¦ã€‚</li>
<li>RLHFå’ŒDPOè®­ç»ƒå—åˆ°è…è´¥åå¥½ã€å¥–åŠ±è¿‡åº¦ä¼˜åŒ–å’Œåå‘å†—é•¿çš„å½±å“ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤§å¤šåªèƒ½è§£å†³å…¶ä¸­ä¸€ä¸ªé—®é¢˜ï¼Œä¸”éœ€è¦å¤§é‡è®¡ç®—æ¥ä¼°è®¡å¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„ç†è®ºä¿è¯ã€‚</li>
<li>æå‡ºçš„RLHF-COVå’ŒDPO-COVç®—æ³•èƒ½åŒæ—¶è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€‚ç”¨äºç¦»çº¿ä¸åœ¨çº¿ç¯å¢ƒã€‚</li>
<li>DPO-COVç®—æ³•æ— éœ€å¤æ‚çš„å¥–åŠ±ä¼°è®¡ï¼Œç®€å•å®ç”¨ï¼Œä¸”ä¸RLHF-COVç®—æ³•ç­‰æ•ˆã€‚</li>
<li>åœ¨ç†è®ºå±‚é¢è¯æ˜ï¼ŒDPO-COVç®—æ³•åœ¨å—æ±¡æŸ“æ•°æ®ä¸Šè®­ç»ƒçš„æ³›åŒ–é”™è¯¯ç‡ä¸æ¸…æ´æ•°æ®ä¸Šè®­ç»ƒçš„ç®€å•æƒ…å†µçš„æœ€ä½³å·²çŸ¥ç‡ç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5f2f712e2bdf87e5873269eeee1cfab5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760144609&auth_key=1760144609-0-0-58454290d45f14f4a458e4b5caa715fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-5d30dc1b2c174d4bb19a9f51df938ba9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760082923&auth_key=1760082923-0-0-161031898c3a921cfcc49d31605667a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-10  Vibe Checker Aligning Code Evaluation with Human Preference
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-09/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d40d333a67bd787daf8a676abd9b8649~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028083&auth_key=1760028083-0-0-8f9b90aabbce6c3ac39fe0f79a346015&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-09  Paper2Video Automatic Video Generation from Scientific Papers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
