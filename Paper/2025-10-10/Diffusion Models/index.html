<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-10  StyleKeeper Prevent Content Leakage using Negative Visual Query   Guidance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-5c1dac7ca7b84dc25e84f11de04be385~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047566&auth_key=1760047566-0-0-7b2f1a428a78e17daf3a2cf76315d024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-10-更新"><a href="#2025-10-10-更新" class="headerlink" title="2025-10-10 更新"></a>2025-10-10 更新</h1><h2 id="StyleKeeper-Prevent-Content-Leakage-using-Negative-Visual-Query-Guidance"><a href="#StyleKeeper-Prevent-Content-Leakage-using-Negative-Visual-Query-Guidance" class="headerlink" title="StyleKeeper: Prevent Content Leakage using Negative Visual Query   Guidance"></a>StyleKeeper: Prevent Content Leakage using Negative Visual Query   Guidance</h2><p><strong>Authors:Jaeseok Jeong, Junho Kim, Gayoung Lee, Yunjey Choi, Youngjung Uh</strong></p>
<p>In the domain of text-to-image generation, diffusion models have emerged as powerful tools. Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content. However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style. To address this issue, we 1) extend classifier-free guidance (CFG) to utilize swapping self-attention and propose 2) negative visual query guidance (NVQG) to reduce the transfer of unwanted contents. NVQG employs negative score by intentionally simulating content leakage scenarios that swap queries instead of key and values of self-attention layers from visual style prompts. This simple yet effective method significantly reduces content leakage. Furthermore, we provide careful solutions for using a real image as visual style prompts. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, reflecting the style of the references, and ensuring that resulting images match the text prompts. Our code is available \href{<a target="_blank" rel="noopener" href="https://github.com/naver-ai/StyleKeeper%7D%7Bhere%7D">https://github.com/naver-ai/StyleKeeper}{here}</a>. </p>
<blockquote>
<p>在文本到图像生成领域，扩散模型已崭露头角成为强大的工具。最近，关于视觉提示的研究，即使用图像作为提示，已经实现对风格和内容的更精确控制。然而，现有方法常常存在内容泄露的问题，即视觉风格提示中的不需要的元素与预期风格一起转移。为了解决这一问题，我们1）扩展无分类器引导（CFG），利用交换自注意力，并提出2）负视觉查询引导（NVQG）以减少不想要内容的转移。NVQG通过有意模拟内容泄露情景而采用负分，在这种情景中，交换查询而不是自注意力层的键和值来自视觉风格提示。这种简单而有效的方法显著减少了内容泄露。此外，我们还为使用真实图像作为视觉风格提示提供了精心解决方案。通过各种风格和文本提示的广泛评估，我们的方法证明优于现有方法，能反映参考的风格，并确保生成的图像符合文本提示。我们的代码&lt;这里。可在<a target="_blank" rel="noopener" href="https://github.com/naver-ai/StyleKeeper">https://github.com/naver-ai/StyleKeeper</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06827v1">PDF</a> Accepted to ICCV 2025; CVPRW AI4CC 2024 (Best Paper + Oral)</p>
<p><strong>Summary</strong></p>
<p>文本生成领域中的扩散模型已成为强大的工具。最近，关于视觉提示的研究使得图像作为提示更为精准地控制风格和内容。然而，现有方法常常存在内容泄露问题，即视觉风格提示中的不期望元素会一并转移。为解决此问题，研究团队扩展了无分类器引导（CFG），采用交换自注意力，并提出负视觉查询引导（NVQG）来减少不想要内容的转移。NVQG通过故意模拟内容泄露情景，采用负分数，交换自注意力层的查询而不是键和值，从而有效减少内容泄露。此外，该研究还为使用真实图像作为视觉风格提示提供了精细解决方案。通过跨各种风格和文本提示的广泛评估，该方法展现出对现有方法的优势，能够反映参考的风格并确保生成的图像与文本提示相匹配。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本转图像生成领域表现出强大的能力。</li>
<li>视觉提示研究提高了对图像风格和内容的精准控制。</li>
<li>现有方法存在内容泄露问题，即不期望的元素会被转移。</li>
<li>研究团队通过扩展无分类器引导并引入负视觉查询引导来解决内容泄露问题。</li>
<li>负视觉查询引导通过模拟内容泄露情景并采用负分数来减少不必要内容的转移。</li>
<li>该方法提供使用真实图像作为视觉风格提示的精细解决方案。</li>
<li>经过广泛评估，该方法在跨各种风格和文本提示的情况下表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-054cc441a2095df47722186071620655~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047490&auth_key=1760047490-0-0-356f70ce629bd60db52b582bc82877f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08ec1441f9a35fab27543023485d3ec9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047498&auth_key=1760047498-0-0-7550161b15b472b69c63534c684dd4d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d77befce2fd69be0bb3ebf28fdd3d672~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047505&auth_key=1760047505-0-0-f8c972fc11e2cd94a794e0555a7fe842&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df8eaf9c3d081451dc840ab95dbb2310~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047512&auth_key=1760047512-0-0-a6d8e5470409cd55cb15cb31a48494b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-947578257cc4e760557c23e8af49083d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047518&auth_key=1760047518-0-0-e8dc2889c009c3de85969f041db53aa2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot"><a href="#OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot" class="headerlink" title="OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot"></a>OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot</h2><p><strong>Authors:Junhan Zhu, Hesong Wang, Mingluo Su, Zefang Wang, Huan Wang</strong></p>
<p>Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality. </p>
<blockquote>
<p>大规模文本到图像的扩散模型虽然功能强大，但计算成本高昂。现有的单次网络剪枝方法由于扩散模型的迭代去噪性质，几乎无法直接应用于其中。为了弥补这一空白，本文提出了OBS-Diff，这是一种新型的一次性剪枝框架，能够实现大规模文本到图像扩散模型的精确、无需训练压缩。具体来说，（i）OBS-Diff重新焕发了经典的最优脑外科医生（OBS）的活力，使其适应现代扩散模型的复杂架构，并支持多种剪枝粒度，包括非结构化、N：M半结构化和结构化（MHA头和FFN神经元）稀疏性；（ii）为了将剪枝标准与扩散过程的迭代动态相一致，我们从误差累积的角度审视问题，提出了一种新的时间步感知海森构造，结合了对数减少加权方案，为早期的时间步赋予更大的重要性，以减轻潜在的误差累积；（iii）此外，还提出了一种计算效率高的分组顺序剪枝策略，以摊销昂贵的校准过程。大量实验表明，OBS-Diff在扩散模型的一站式剪枝方面达到了最新水平，实现了推理加速，视觉质量几乎没有下降。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06751v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为OBS-Diff的新型一次性修剪框架，该框架能够对大规模文本到图像扩散模型进行准确且无需训练即可进行压缩。它支持多种修剪粒度，并基于误差累积视角提出了一个全新的时间步感知的海森矩阵构建方案。此外，为了降低校准过程的成本，还提出了一种计算效率高的分组顺序修剪策略。实验表明，OBS-Diff在扩散模型的一站式修剪方面达到了最新水平，实现了推理加速，并且在视觉质量上几乎没有损失。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OBS-Diff框架解决了大型文本到图像扩散模型计算成本高的问题，通过无需训练的压缩方式实现了一站式修剪。</li>
<li>OBS-Diff适应了现代扩散模型的复杂架构，并支持多种修剪粒度，包括非结构化、N:M半结构化和结构化（MHA头和FFN神经元）稀疏性。</li>
<li>基于误差累积视角，提出了新的时间步感知的海森矩阵构建方案，为修剪标准与扩散过程的迭代动态提供了对齐。</li>
<li>通过为早期时间步赋予更大重要性来减轻潜在误差积累的问题。</li>
<li>提出了一种计算高效的分组顺序修剪策略，以平衡昂贵的校准过程。</li>
<li>实验结果证明了OBS-Diff在扩散模型的一站式修剪方面的优越性，实现了推理加速。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06751">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f4b6410d7198ef5e7b95cf091f51e044~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047526&auth_key=1760047526-0-0-996a43e8084b87bbbdb83f1573e022a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7bd51d69c4e5562db0bbc68a1b8e4d4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047533&auth_key=1760047533-0-0-a14f1364628c35f082eadbed3e2fbb5d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Diffusion-Model-for-Regular-Time-Series-Generation-from-Irregular-Data-with-Completion-and-Masking"><a href="#A-Diffusion-Model-for-Regular-Time-Series-Generation-from-Irregular-Data-with-Completion-and-Masking" class="headerlink" title="A Diffusion Model for Regular Time Series Generation from Irregular Data   with Completion and Masking"></a>A Diffusion Model for Regular Time Series Generation from Irregular Data   with Completion and Masking</h2><p><strong>Authors:Gal Fadlon, Idan Arbiv, Nimrod Berman, Omri Azencot</strong></p>
<p>Generating realistic time series data is critical for applications in healthcare, finance, and science. However, irregular sampling and missing values present significant challenges. While prior methods address these irregularities, they often yield suboptimal results and incur high computational costs. Recent advances in regular time series generation, such as the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable generative capabilities by transforming time series into image representations, making them a promising solution. However, extending ImagenTime to irregular sequences using simple masking introduces “unnatural” neighborhoods, where missing values replaced by zeros disrupt the learning process. To overcome this, we propose a novel two-step framework: first, a Time Series Transformer completes irregular sequences, creating natural neighborhoods; second, a vision-based diffusion model with masking minimizes dependence on the completed values. This approach leverages the strengths of both completion and masking, enabling robust and efficient generation of realistic time series. Our method achieves state-of-the-art performance, achieving a relative improvement in discriminative score by $70%$ and in computational cost by $85%$. Code is at <a target="_blank" rel="noopener" href="https://github.com/azencot-group/ImagenI2R">https://github.com/azencot-group/ImagenI2R</a>. </p>
<blockquote>
<p>生成真实的时间序列数据对于医疗保健、金融和科学等领域的应用至关重要。然而，不规则采样和缺失值带来了重大挑战。尽管先前的方法解决了这些不规则性，但它们通常产生次优结果并产生高昂的计算成本。最近，基于扩散的ImagenTime模型等常规时间序列生成方面的进展，通过将时间序列转换为图像表示，展示了强大、快速和可扩展的生成能力，成为了一种有前景的解决方案。然而，将ImagenTime扩展到不规则序列时，使用简单掩码会引入“不自然”的邻居，其中缺失值被零替换会破坏学习过程。为了克服这一点，我们提出了一种新的两步框架：首先，时间序列变压器完成不规则序列，创建自然邻居；其次，带有掩码的基于视觉的扩散模型最小化对完成值的依赖。这种方法结合了补全和掩码的优点，能够实现稳健和高效的真实时间序列生成。我们的方法达到了最先进的性能，判别得分相对提高了70%，计算成本降低了8 结。代码地址是：<a target="_blank" rel="noopener" href="https://github.com/azencot-group/ImagenI2R%E3%80%82">https://github.com/azencot-group/ImagenI2R。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06699v1">PDF</a> Accepted to NeurIPS 2025; The first two authors contributed equally   and are co-leading authors</p>
<p><strong>Summary</strong></p>
<p>文本生成技术在时间序列数据的实际应用中，面临着不规则采样和缺失值等挑战。传统的处理方法往往效果不尽如人意且计算成本高昂。近期提出的ImagenTime模型通过将时间序列转化为图像表示，展现了强大的生成能力。然而，对于不规则序列的扩展应用，简单掩码会导致“不自然”的邻域出现，影响学习进程。为解决这一问题，提出一种新型两步框架：首先使用时间序列转换器完成不规则序列，创建自然邻域；其次采用带有掩码的视觉基础扩散模型，减少对完成值的依赖。该方法结合了完成和掩码的优势，实现了稳健且高效的时间序列生成。此方法达到业界领先水平，判别得分相对提升70%，计算成本降低85%。相关代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成真实的时间序列数据在医疗保健、金融和科学等领域具有重要性。</li>
<li>不规则采样和缺失值是时间序列数据生成中的关键挑战。</li>
<li>现有方法虽能解决这些不规则性，但效果欠佳且计算成本较高。</li>
<li>ImagenTime模型通过将时间序列转化为图像表示展现了强大的生成能力。</li>
<li>对于不规则序列的扩展，简单掩码会导致学习过程中的“不自然”邻域问题。</li>
<li>提出的新型两步框架结合了完成和掩码的优势，实现了稳健且高效的时间序列生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06699">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-26d60953d49df444bac92360a6cc7c1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047540&auth_key=1760047540-0-0-e0932e75eaba52022d06b5d5901eb7da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d7a9e98cd3085ef6ee3e696f57ac0bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047547&auth_key=1760047547-0-0-d255a4db5fdca18ba2ffd85a34b6ceed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d70272aab7b783138d6dd0107bd3ebbb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047554&auth_key=1760047554-0-0-ca7bcd7a537526b0fc7f532dfddb2065&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8bcd6556236a8834ce5affbe515e0e0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047560&auth_key=1760047560-0-0-e61edde17f70ab932a6e389bf95e2e40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c1dac7ca7b84dc25e84f11de04be385~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047566&auth_key=1760047566-0-0-7b2f1a428a78e17daf3a2cf76315d024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d91cc2a9897ae00b17dd43e8ce694fcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047573&auth_key=1760047573-0-0-fd3b5d735fc500705847b50198724f6f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Conditional-Denoising-Diffusion-Model-Based-Robust-MR-Image-Reconstruction-from-Highly-Undersampled-Data"><a href="#Conditional-Denoising-Diffusion-Model-Based-Robust-MR-Image-Reconstruction-from-Highly-Undersampled-Data" class="headerlink" title="Conditional Denoising Diffusion Model-Based Robust MR Image   Reconstruction from Highly Undersampled Data"></a>Conditional Denoising Diffusion Model-Based Robust MR Image   Reconstruction from Highly Undersampled Data</h2><p><strong>Authors:Mohammed Alsubaie, Wenxi Liu, Linxia Gu, Ovidiu C. Andronesi, Sirani M. Perera, Xianqi Li</strong></p>
<p>Magnetic Resonance Imaging (MRI) is a critical tool in modern medical diagnostics, yet its prolonged acquisition time remains a critical limitation, especially in time-sensitive clinical scenarios. While undersampling strategies can accelerate image acquisition, they often result in image artifacts and degraded quality. Recent diffusion models have shown promise for reconstructing high-fidelity images from undersampled data by learning powerful image priors; however, most existing approaches either (i) rely on unsupervised score functions without paired supervision or (ii) apply data consistency only as a post-processing step. In this work, we introduce a conditional denoising diffusion framework with iterative data-consistency correction, which differs from prior methods by embedding the measurement model directly into every reverse diffusion step and training the model on paired undersampled-ground truth data. This hybrid design bridges generative flexibility with explicit enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that our framework consistently outperforms recent state-of-the-art deep learning and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing perceptual improvements more faithfully. These results demonstrate that integrating conditional supervision with iterative consistency updates yields substantial improvements in both pixel-level fidelity and perceptual realism, establishing a principled and practical advance toward robust, accelerated MRI reconstruction. </p>
<blockquote>
<p>磁共振成像（MRI）是现代医学诊断中的重要工具，但其漫长的采集时间仍然是一个关键的限制，特别是在时间敏感的临床场景中。尽管欠采样策略可以加速图像采集，但它们通常会导致图像出现伪影和质量下降。最近的扩散模型显示出通过学习强大的图像先验知识从欠采样数据中重建高保真图像的潜力；然而，大多数现有方法要么（i）依赖无配对监督的无监督分数函数，要么（ii）仅将数据一致性作为后处理步骤应用。在这项工作中，我们引入了一个具有迭代数据一致性校正的条件去噪扩散框架，它与先前的方法的不同之处在于，它将测量模型直接嵌入到每个反向扩散步骤中，并在配对欠采样-真实数据上训练模型。这种混合设计结合了生成灵活性和MRI物理的显式实施。在fastMRI数据集上的实验表明，我们的框架在结构相似性度量（SSIM）、峰值信噪比（PSNR）和局部感知图像感知相似性（LPIPS）上始终优于最新的深度学习和扩散方法，LPIPS更真实地捕捉到了感知改进。这些结果表明，将条件监督与迭代一致性更新相结合，在像素级保真度和感知真实性方面都取得了显著改进，为实现稳健、加速的MRI重建提供了有原则和实际进步的突破。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06335v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用扩散模型加速磁共振成像（MRI）的技术。传统的MRI采集时间较长，而扩散模型能够在保证图像质量的同时，通过学习方法加速图像采集。本文提出了一种结合条件去噪扩散框架和迭代数据一致性校正的新方法，将测量模型直接嵌入到每个反向扩散步骤中，并在配对欠采样-真实数据上进行训练。实验证明，该方法在SSIM、PSNR和LPIPS等指标上均优于最新的深度学习和扩散方法，尤其在感知改善方面表现更出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在MRI图像重建中有巨大潜力，能够在保证图像质量的同时加速采集。</li>
<li>现有方法主要依赖无监督得分函数或仅将数据一致性作为后处理步骤，而新方法则将测量模型嵌入每个反向扩散步骤中。</li>
<li>新方法结合了条件监督与迭代一致性更新，显著提高了像素级保真度和感知现实性。</li>
<li>该方法在SSIM、PSNR和LPIPS等指标上的表现均优于其他方法，尤其在感知质量方面。</li>
<li>集成扩散模型与MRI技术为加速MRI重建提供了理论上的进步。</li>
<li>新方法将生成灵活性与MRI物理的显式实施相结合，实现了更好的图像重建效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06335">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ed8b3160742ce192f18a08457140bd45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047581&auth_key=1760047581-0-0-7327e9124e61a5024bc61205794ceba5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a3182fbbb9dc6009f8d71fba37c7e63~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047588&auth_key=1760047588-0-0-b7a4a7f8abb1efd95491d0552913a7b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f790d41f7e1893c270c91d01918e509a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047595&auth_key=1760047595-0-0-c1b7879fe4c5adfa39f14095c71bb7af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c3db9fbaa3d5f95b052d749358bf897~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047602&auth_key=1760047602-0-0-96b2a6d6630eb6e20d90c5b3bd4bdb3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7790fa011158758e7e700c74906aa7db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047608&auth_key=1760047608-0-0-b865850d23cfdea1d0fff092ace38738&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b18c9aed058fc63e92c26f872bc905e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047615&auth_key=1760047615-0-0-fb494b1f13fe8e0ccad10f521c1603e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding"><a href="#Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding" class="headerlink" title="Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal   Generation and Understanding"></a>Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal   Generation and Understanding</h2><p><strong>Authors:Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, Jinbin Bai, Qian Yu, Dengyang Jiang, Yuandong Pu, Haoxing Chen, Le Zhuo, Junjun He, Gen Luo, Tianbin Li, Ming Hu, Jin Ye, Shenglong Ye, Bo Zhang, Chang Xu, Wenhai Wang, Hongsheng Li, Guangtao Zhai, Tianfan Xue, Bin Fu, Xiaohong Liu, Yu Qiao, Yihao Liu</strong></p>
<p>We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: <a target="_blank" rel="noopener" href="https://synbol.github.io/Lumina-DiMOO">https://synbol.github.io/Lumina-DiMOO</a>. </p>
<blockquote>
<p>我们介绍Lumina-DiMOO，这是一个开放源码的基础模型，用于无缝多模式生成和理解。Lumina-DiMOO通过采用完全离散的扩散模型来处理各种模态的输入和输出，从而与之前的统一模型相区别。这种创新的方法使得Lumina-DiMOO相比之前的自回归（AR）或混合AR-Diffusion范式实现更高的采样效率，并能熟练地支持广泛的多模式任务，包括文本到图像生成、图像到图像生成（例如图像编辑、主题驱动生成和图像修复等），以及图像理解。Lumina-DiMOO在多个基准测试上实现了最先进的性能表现，超越了现有的开源统一多模式模型。为了促进多模式和离散扩散模型的进一步研究，我们向社区发布我们的代码和检查点。项目页面：<a target="_blank" rel="noopener" href="https://synbol.github.io/Lumina-DiMOO%E3%80%82">https://synbol.github.io/Lumina-DiMOO。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06308v1">PDF</a> 33 pages, 13 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>Lumina-DiMOO是一款开源的跨模态生成与理解基础模型，采用全离散扩散建模技术，实现了高效采样并支持多种跨模态任务，包括文本生成图像、图像编辑、主题驱动生成和图像修复等，并在多个基准测试中达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumina-DiMOO是一个开源的跨模态模型。</li>
<li>它采用全离散扩散建模技术处理多模态输入输出。</li>
<li>Lumina-DiMOO实现了高效采样，相比之前的AR或混合AR-Diffusion模型有优势。</li>
<li>该模型支持多种跨模态任务，如文本生成图像、图像编辑、主题驱动生成和图像修复等。</li>
<li>Lumina-DiMOO在多个基准测试中达到了领先水平，超越了现有的开源多模态模型。</li>
<li>代码和检查点已发布到社区，以推动多模态和离散扩散模型的研究进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06308">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6422c901ddfafa0820afba6e5e65b0fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047622&auth_key=1760047622-0-0-12cd53fa30c2b6983cae65cda2f81cca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-431980fee627d0ee8e4c771278422706~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047629&auth_key=1760047629-0-0-f8210441f7f654a7e4198d3cec0934c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40a311e10807d3a783be879e5b37d600~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047636&auth_key=1760047636-0-0-350f4c67175a38ac5a38a432e40f2cb6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5373fd9e451689914dcbe89719b98b14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047643&auth_key=1760047643-0-0-a68fc11c177ab6c46925f4f214df34fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RespoDiff-Dual-Module-Bottleneck-Transformation-for-Responsible-Faithful-T2I-Generation"><a href="#RespoDiff-Dual-Module-Bottleneck-Transformation-for-Responsible-Faithful-T2I-Generation" class="headerlink" title="RespoDiff: Dual-Module Bottleneck Transformation for Responsible &amp;   Faithful T2I Generation"></a>RespoDiff: Dual-Module Bottleneck Transformation for Responsible &amp;   Faithful T2I Generation</h2><p><strong>Authors:Silpa Vadakkeeveetil Sreelatha, Sauradip Nag, Muhammad Awais, Serge Belongie, Anjan Dutta</strong></p>
<p>The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by 20% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. Code will be released upon acceptance. </p>
<blockquote>
<p>扩散模型的快速发展已经实现了高保真和语义丰富的文本到图像生成；然而，确保公平和安全仍然是一个开放性的挑战。现有方法通常以提高公平性和安全性为代价来牺牲语义保真和图像质量。在这项工作中，我们提出了RespoDiff，这是一种用于负责任的文本到图像生成的新型框架，它对扩散模型的中间瓶颈表示进行了双重模块转换。我们的方法引入了两个独特的学习模块：一个专注于捕获和执行负责任的概念，如公平性和安全性；另一个则致力于保持与中性提示的语义对齐。为了促进双重学习过程，我们引入了一种新型得分匹配目标，使模块之间实现有效协调。我们的方法在确保语义对齐的同时，通过优化两个目标而毫不妥协图像保真度，实现了负责任生成的前沿突破。我们的方法在不同且未见过的提示下，提高了负责任和语义连贯的生成能力达20%。此外，它能无缝集成到大型模型如SDXL中，提高公平性和安全性。代码将在接受后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15257v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型的快速发展使得高保真和语义丰富的文本到图像生成成为可能，但保证公平和安全仍然是一个挑战。现有方法往往在提高公平和安全性的同时牺牲了语义保真和图像质量。本文提出一种新型框架RespoDiff，通过扩散模型的中间瓶颈表示进行双重模块转换，实现负责任的文本到图像生成。该方法引入两个独立的学习模块，一个专注于捕捉和执行负责任的概念，如公平和安全，另一个致力于保持与中性提示的语义对齐。采用新型得分匹配目标，促进模块间的有效协调。该方法在保证语义对齐的同时优化两个目标，不损害图像保真度，提高了负责任和语义连贯的生成能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的快速发展促进了文本到图像的高保真和语义丰富生成。</li>
<li>保证公平和安全在文本到图像生成中仍然是一个挑战。</li>
<li>现有方法在提高公平和安全性的同时，往往会牺牲语义保真和图像质量。</li>
<li>提出了一种新型框架RespoDiff，通过双重模块转换实现负责任的文本到图像生成。</li>
<li>RespoDiff框架包括两个独立的学习模块，分别关注捕捉和执行负责任的概念以及保持与中性提示的语义对齐。</li>
<li>采用新型得分匹配目标，有效协调两个模块的学习过程。</li>
<li>该方法在保障语义对齐的同时优化两个目标，提高了图像生成的公平性和安全性，同时不损害图像保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-608579db2de69b091d4905ef2523cf98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047649&auth_key=1760047649-0-0-a02572efd88e183bf30a89e0f98db701&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7f336b6d92ebec4eb3dafd34e8325d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047657&auth_key=1760047657-0-0-74fe86c132f8bc2de7859e53a20af534&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding"><a href="#MoRE-Brain-Routed-Mixture-of-Experts-for-Interpretable-and-Generalizable-Cross-Subject-fMRI-Visual-Decoding" class="headerlink" title="MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding"></a>MoRE-Brain: Routed Mixture of Experts for Interpretable and   Generalizable Cross-Subject fMRI Visual Decoding</h2><p><strong>Authors:Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun</strong></p>
<p>Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain’s high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: <a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain">https://github.com/yuxiangwei0808/MoRE-Brain</a>. </p>
<blockquote>
<p>从功能磁共振成像（fMRI）中解码视觉体验为我们理解人类感知并开发先进的脑机接口提供了强大的途径。然而，目前的进展往往优先最大化重建保真度，却忽视了可解释性这一对于获取神经科学洞察力的关键方面。为了解决这一差距，我们提出了MoRE-Brain，这是一个神经启发的框架，旨在实现高保真、可适应和可解释的视觉重建。MoRE-Brain独特地采用了一种层次化的混合专家架构，其中不同的专家处理来自功能相关体素组的fMRI信号，模仿专门的脑网络。专家首先被训练将fMRI编码到固定的CLIP空间中。然后，一个微调过的扩散模型在专家输出的指导下，通过一种新的双阶段路由机制合成图像，该机制在扩散过程中动态权衡专家的贡献。MoRE-Brain提供了三个主要的进步：首先，它引入了一种基于脑网络原理的新型混合专家架构，用于神经解码。其次，它通过共享核心专家网络并仅适应特定主题的路由器，实现了跨主题的有效泛化。第三，它提供了增强的机械洞察力，因为明确的路由可以精确地揭示不同的模拟脑区域如何塑造重建图像的语义和空间属性。大量实验验证了MoRE-Brain的高重建保真度，瓶颈分析进一步证明了它有效利用fMRI信号的能力，区分了真正的神经解码和过度依赖生成先验。因此，MoRE-Brain标志着朝着更具通用性和可解释的基于fMRI的视觉解码迈出了重大的一步。代码将很快在<a target="_blank" rel="noopener" href="https://github.com/yuxiangwei0808/MoRE-Brain%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/yuxiangwei0808/MoRE-Brain上公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15946v3">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong><br>解码fMRI中的视觉体验是了解人类感知力和开发先进脑机接口的重要渠道。针对现有解码策略的不足，提出MoRE-Brain框架，实现高保真、可适应和可解释的视觉重建。该框架采用基于神经网络的混合专家架构处理fMRI信号，并引入CLIP空间和扩散模型进行图像合成。MoRE-Brain主要贡献在于引入基于脑网络原理的混合专家架构、实现跨主体高效泛化以及提供增强的机械洞察力。该框架将为解码fMRI提供新的思路和方法。代码即将公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoRE-Brain框架利用混合专家架构进行神经解码，旨在解决现有策略中重视重建保真度而忽视解释性的问题。</li>
<li>该框架采用基于神经网络的策略处理fMRI信号，模拟专门化的脑网络。</li>
<li>MoRE-Brain引入CLIP空间进行编码，并使用扩散模型合成图像，通过双重阶段路由机制指导专家输出。</li>
<li>MoRE-Brain实现了跨主体泛化，通过共享核心专家网络并仅调整主体特定路由器。</li>
<li>明确的路由机制为重建图像的语义和空间属性提供了机械洞察力，显示了不同脑区如何影响图像重建过程。</li>
<li>广泛实验验证了MoRE-Brain的高重建保真度，瓶颈分析进一步证明了其在利用fMRI信号方面的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15946">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a5ee0067a49f254ae1bdeaf6cb1d6515~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047664&auth_key=1760047664-0-0-3d377831bcc84c377212bbe2e594d95b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8e12ed1b6adc5fb3a97f076cec9470ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047671&auth_key=1760047671-0-0-e4453ca34ba6deeb3ac5ebd3db8b67f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-645e01c92190ccd6f4f3b22d1c901e02~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047678&auth_key=1760047678-0-0-560dbf6feea872e17b96ce9435902f7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8de6a61ae5f5f1b4e04a184b09a34118~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047685&auth_key=1760047685-0-0-a590b3ae784aeff97e0ade05a639a92c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebc9394b2aac8a597cc316f64846fee4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760047692&auth_key=1760047692-0-0-1a2c289e4bf005b8724b7da519378f6f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-7e4c436ac5f510674cbe676065e934a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760049798&auth_key=1760049798-0-0-50b80f9a76b73bef9b9252391c2cffaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-10  Validation of Various Normalization Methods for Brain Tumor   Segmentation Can Federated Learning Overcome This Heterogeneity?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-dafbb93c92742d19c3bf732080370a7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760046795&auth_key=1760046795-0-0-1a74f7df909cca562122e2a9ef26a679&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-10-10  VGGT-X When VGGT Meets Dense Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
