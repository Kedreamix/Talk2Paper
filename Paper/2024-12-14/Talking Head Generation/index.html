<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Talking Head Generation"><meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-14  EmoDubber Towards High Quality and Emotion Controllable Movie Dubbing"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>Talking Head Generation | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>归档</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://pic1.zhimg.com/v2-81e5e229543f6946871bebdf09beb67f.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Talking Head Generation</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/Talking-Head-Generation/"><span class="chip bg-color">Talking Head Generation</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">Talking Head Generation</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> 发布日期:&nbsp;&nbsp; 2024-12-14</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> 更新日期:&nbsp;&nbsp; 2024-12-14</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> 文章字数:&nbsp;&nbsp; 4.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> 阅读时长:&nbsp;&nbsp; 17 分</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> 阅读次数:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-12-14-更新"><a href="#2024-12-14-更新" class="headerlink" title="2024-12-14 更新"></a>2024-12-14 更新</h1><h2 id="EmoDubber-Towards-High-Quality-and-Emotion-Controllable-Movie-Dubbing"><a href="#EmoDubber-Towards-High-Quality-and-Emotion-Controllable-Movie-Dubbing" class="headerlink" title="EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing"></a>EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing</h2><p><strong>Authors:Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van den Hengel, Jian Yang, Qingming Huang</strong></p><p>Given a piece of text, a video clip, and a reference audio, the movie dubbing task aims to generate speech that aligns with the video while cloning the desired voice. The existing methods have two primary deficiencies: (1) They struggle to simultaneously hold audio-visual sync and achieve clear pronunciation; (2) They lack the capacity to express user-defined emotions. To address these problems, we propose EmoDubber, an emotion-controllable dubbing architecture that allows users to specify emotion type and emotional intensity while satisfying high-quality lip sync and pronunciation. Specifically, we first design Lip-related Prosody Aligning (LPA), which focuses on learning the inherent consistency between lip motion and prosody variation by duration level contrastive learning to incorporate reasonable alignment. Then, we design Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences by efficient conformer to improve speech intelligibility. Next, the speaker identity adapting module aims to decode acoustics prior and inject the speaker style embedding. After that, the proposed Flow-based User Emotion Controlling (FUEC) is used to synthesize waveform by flow matching prediction network conditioned on acoustics prior. In this process, the FUEC determines the gradient direction and guidance scale based on the user’s emotion instructions by the positive and negative guidance mechanism, which focuses on amplifying the desired emotion while suppressing others. Extensive experimental results on three benchmark datasets demonstrate favorable performance compared to several state-of-the-art methods.</p><blockquote><p>给定一段文本、一个视频片段和参考音频，电影配音任务旨在生成与视频对齐的语音，同时克隆所需的语音。现有方法存在两个主要缺陷：一是难以同时保持音视频同步和清晰的发音；二是缺乏表达用户定义情绪的能力。为了解决这些问题，我们提出了EmoDubber，一种情感可控的配音架构，允许用户指定情绪类型和情绪强度，同时满足高质量的唇部同步和发音。具体来说，我们首先设计了唇相关韵律对齐（LPA），通过时长级别对比学习来关注唇动和韵律变化之间的内在一致性，以进行合理对齐。然后，我们设计了发音增强（PE）策略，通过高效卷积神经网络融合视频级音素序列来提高语音清晰度。接下来，说话人身份适应模块旨在解码声音先验并注入说话人风格嵌入。之后，我们提出基于流的用户情绪控制（FUEC）方法，通过流匹配预测网络以声音先验为条件合成波形。在此过程中，FUEC通过正负引导机制根据用户的情绪指令确定梯度方向和引导尺度，专注于放大所需情绪同时抑制其他情绪。在三个基准数据集上的大量实验结果表明，与几种最新方法相比，我们的方法具有优越的性能。</p></blockquote><p><strong>论文及项目相关链接</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08988v1">PDF</a> Under review</p><p><strong>Summary</strong>：<br>针对电影配音任务，提出一种情感可控的配音架构EmoDubber，可让用户指定情感类型和情感强度，同时实现高质量的唇同步和发音。通过设计唇相关韵律对齐（LPA）和发音增强（PE）策略，解决现有方法的不足。进一步，通过流动用户情绪控制（FUEC）合成波形，根据用户的情绪指令确定梯度方向和指导规模，放大所需情绪并抑制其他情绪。在三个基准数据集上的广泛实验结果表明，与几种最新方法相比具有优越性能。</p><p><strong>Key Takeaways</strong>：</p><ol><li>EmoDubber是一个情感可控的电影配音架构，可指定情感类型和强度。</li><li>设计了唇相关韵律对齐（LPA）策略，学习唇动和韵律之间的内在一致性。</li><li>引入发音增强（PE）策略，通过高效卷积神经网络融合视频级语音音素序列，提高语音清晰度。</li><li>通过流动用户情绪控制（FUEC）合成波形，根据用户情绪指令调整梯度方向和指导规模。</li><li>EmoDubber能同时实现高质量的唇同步和发音。</li><li>广泛实验结果表明，EmoDubber在三个基准数据集上的性能优于几种最新方法。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08988">Cool Papers</a></strong></p><details><summary>点击展开 Cool Papers 详细信息</summary><p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p><p class="faq-a"><strong>A</strong>: 这篇论文提出了一个名为EmoDubber的电影配音架构，旨在解决现有电影配音方法中的两个主要缺陷：</p><ol><li><p><strong>音视频同步与清晰发音的挑战</strong>：现有的方法在生成与视频同步的语音时，往往难以同时保持音视频同步和清晰的发音。</p></li><li><p><strong>缺乏情绪表达能力</strong>：现有的方法通常缺乏表达用户定义情绪的能力，这限制了它们在电影后期制作中的应用，尤其是在情感表达方面。</p></li></ol><p>为了解决这些问题，EmoDubber架构允许用户指定所需的情绪类型和情绪强度，同时确保高质量的唇部同步和清晰的发音。具体来说，EmoDubber通过以下几个关键模块来实现这一目标：</p><ul><li>**Lip-related Prosody Aligning (LPA)**：通过时序级别的对比学习，学习唇动和韵律变化之间的固有一致性，以实现合理的对齐。</li><li>**Pronunciation Enhancing (PE)**：通过扩展视频级别的音素序列并使用高效Conformer模型融合，以提高语音的可理解性。</li><li>**Speaker Identity Adapting (SIA)**：通过注入说话人风格嵌入来生成声学先验信息。</li><li>**Flow-based User Emotion Controlling (FUEC)**：基于流匹配预测网络，根据用户的情绪指令合成波形，同时通过正负引导机制（PNGM）灵活调整情绪强度。</li></ul><p>通过这些模块，EmoDubber能够在保持音视频同步和清晰发音的同时，实现情绪的可控合成，从而满足电影制作或爱好者对配音质量的需求。</p><p></p><p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p><p class="faq-a"><strong>A</strong>: 根据提供的论文内容，相关研究可以分为以下几个领域：</p><ol><li><p><strong>Visual Voice Cloning (V2C)</strong>:</p><ul><li>该领域专注于生成与角色口型同步，并且具有参考音频中展示的嗓音风格的文本语音。一些工作专注于改善多说话人场景中的说话人身份识别，例如：<ul><li>V2CNet [3] 和 VDTTS [15] 使用预训练的 GE2E [47] 来获得独特且归一化的言语嵌入。</li><li>StyleDubber [6] 应用多尺度学习将风格线索应用于 GE2E 提取的嵌入。</li><li>Speak2Dub [50] 采用预训练策略在 TTS 语料库上提高表达性。</li></ul></li></ul></li><li><p><strong>Flow Matching 和 Classifier Guidance</strong>:</p><ul><li>Flow Matching 是一种无需模拟的方法，用于训练连续归一化流（CNFs）模型，这些模型可以模拟任意概率路径，并捕获由扩散过程表示的概率轨迹。它在图像生成和几何领域表现出色，例如：<ul><li>Stable Diffusion 3 [10], Lumina-T2X [11], 和 EQUIFM [44]。</li></ul></li><li>Classifier Guidance (CG) 被广泛用于控制特定属性，例如文本到图像和情感 TTS [13, 26, 45]。</li></ul></li><li><p><strong>情感表达和控制</strong>:</p><ul><li>一些研究 [18] 表明，配音中的情感强度可以影响听众的情感和对电影的心理感知。因此，电影的后期制作可以通过控制情感表达来弥补先前录音的不足。</li></ul></li><li><p><strong>音频-视觉同步评估</strong>:</p><ul><li>使用 Lip Sync Error Distance (LSE-D) 和 Lip Sync Error Confidence (LSE-C) 等指标来量化生成语音与视频之间的同步性，这些指标基于预训练的 SyncNet [5]。</li></ul></li><li><p><strong>语音质量评估</strong>:</p><ul><li>使用 Word Error Rate (WER) [36] 来衡量发音准确性，并使用 Speaker Encoder Cosine Similarity (SECS) 来评估生成配音与参考音频之间的音色一致性。</li></ul></li><li><p><strong>情感评估</strong>:</p><ul><li>使用 Intensity Score 来衡量生成音频中的情感强度，这比之前使用平均分类概率的方法更精细。</li></ul></li></ol><p>这些相关研究为 EmoDubber 架构的开发提供了理论基础和技术背景，使其能够在保持音视频同步和清晰发音的同时，实现情绪的可控合成。</p><p></p><p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p><p class="faq-a"><strong>A</strong>: 论文通过提出一个名为EmoDubber的架构来解决电影配音中存在的问题，具体解决方案如下：</p><h3 id="1-Lip-related-Prosody-Aligning-LPA"><a href="#1-Lip-related-Prosody-Aligning-LPA" class="headerlink" title="1. Lip-related Prosody Aligning (LPA)"></a>1. Lip-related Prosody Aligning (LPA)</h3><ul><li><strong>目标</strong>：学习唇动和韵律变化之间的固有一致性，以实现正确的音视频对齐。</li><li><strong>方法</strong>：通过时序级别的对比学习（Duration level contrastive learning），控制语音速度，使得模型能够推理正确的音视频对齐。</li></ul><h3 id="2-Pronunciation-Enhancing-PE"><a href="#2-Pronunciation-Enhancing-PE" class="headerlink" title="2. Pronunciation Enhancing (PE)"></a>2. Pronunciation Enhancing (PE)</h3><ul><li><strong>目标</strong>：提高语音的清晰度和可理解性。</li><li><strong>方法</strong>：通过单调对齐搜索（Monotonic alignment search, MAS）扩展视频级别的音素序列，并与LPA的输出通过高效的Conformer模型进行融合。</li></ul><h3 id="3-Speaker-Identity-Adapting-SIA"><a href="#3-Speaker-Identity-Adapting-SIA" class="headerlink" title="3. Speaker Identity Adapting (SIA)"></a>3. Speaker Identity Adapting (SIA)</h3><ul><li><strong>目标</strong>：生成声学先验信息，并注入说话人的风格。</li><li><strong>方法</strong>：从PE模块的输出中吸收融合序列，并注入来自参考说话人的样式嵌入，以生成目标声学先验信息。</li></ul><h3 id="4-Flow-based-User-Emotion-Controlling-FUEC"><a href="#4-Flow-based-User-Emotion-Controlling-FUEC" class="headerlink" title="4. Flow-based User Emotion Controlling (FUEC)"></a>4. Flow-based User Emotion Controlling (FUEC)</h3><ul><li><strong>目标</strong>：根据用户指定的情绪指令合成波形，同时渲染用户指定的情绪。</li><li><strong>方法</strong>：使用基于流匹配的预测网络，并结合正负引导机制（Positive and Negative Guidance Mechanism, PNGM），动态调整流匹配向量场预测过程，以实现情绪强度的灵活控制。</li></ul><h3 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h3><ul><li><strong>多尺度风格学习</strong>：通过多尺度学习来应用风格线索，增强说话人特征。</li><li><strong>上下文视频场景的融合</strong>：通过结合前后视频场景来增强生成语音的韵律表达。</li><li><strong>情感控制</strong>：通过情感专家分类器和流匹配预测网络，根据用户的情绪指令动态调整情绪强度。</li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li><strong>数据集</strong>：在三个基准数据集（Chem, GRID, V2C-Animation）上进行了广泛的实验。</li><li><strong>性能</strong>：与几种最先进的方法相比，EmoDubber在音视频同步、发音清晰度和情感控制方面均展现出优越性能。</li></ul><p>通过上述方法，EmoDubber能够在保持音视频同步和清晰发音的同时，实现情绪的可控合成，满足用户对电影配音质量和情感表达的需求。</p><p></p><p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p><p class="faq-a"><strong>A</strong>: 论文中进行了一系列实验来验证EmoDubber架构的性能和效果。以下是实验的详细内容：</p><h3 id="1-实施细节"><a href="#1-实施细节" class="headerlink" title="1. 实施细节"></a>1. 实施细节</h3><ul><li>视频帧采样率为25 FPS，所有音频重新采样为16kHz。</li><li>唇部区域调整为96×96，并在ResNet-18上进行预训练。</li><li>STFT（短时傅里叶变换）的窗口长度、帧大小和跳跃长度分别为640、1024和160。</li><li>流预测网络在LibriSpeech上进行预训练。</li><li>使用PyTorch在GeForce RTX 4090 GPU上进行训练和推理。</li></ul><h3 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2. 数据集"></a>2. 数据集</h3><ul><li><strong>Chem</strong>：一个流行的配音数据集，记录了化学老师在课堂上的讲话。</li><li><strong>GRID</strong>：一个多说话人的配音基准数据集。</li><li><strong>V2C-Animation</strong>：一个用于动画电影配音的多说话人数据集，包含身份和情感注释。</li></ul><h3 id="3-评估指标"><a href="#3-评估指标" class="headerlink" title="3. 评估指标"></a>3. 评估指标</h3><ul><li><strong>音视频同步评估</strong>：使用Lip Sync Error Distance (LSE-D) 和 Lip Sync Error Confidence (LSE-C)。</li><li><strong>语音质量评估</strong>：使用Word Error Rate (WER)、Speaker Encoder Cosine Similarity (SECS)、Mel Cepstral Distortion Dynamic Time Warping (MCD) 和MCD-SL。</li><li><strong>情感评估</strong>：使用Intensity Score来衡量生成音频中的情感强度。</li><li><strong>主观评估</strong>：使用MOS-naturalness (MOS-N) 和 MOS-similarity (MOS-S) 来评估生成语音的自然度和目标语音的识别度。</li></ul><h3 id="4-与现有技术的比较"><a href="#4-与现有技术的比较" class="headerlink" title="4. 与现有技术的比较"></a>4. 与现有技术的比较</h3><ul><li>与没有情感控制功能的SOTA（State-of-the-Art）配音模型进行比较，包括Fastspeech2、StyleSpeech、Face-TTS、V2C-Net、HPMDubbing、Speaker2Dub和StyleDubber等。</li></ul><h3 id="5-消融研究"><a href="#5-消融研究" class="headerlink" title="5. 消融研究"></a>5. 消融研究</h3><ul><li>通过移除EmoDubber架构中的不同模块（LPA、PE、SIA）来评估每个模块对整体性能的贡献。</li></ul><h3 id="6-情感控制评估"><a href="#6-情感控制评估" class="headerlink" title="6. 情感控制评估"></a>6. 情感控制评估</h3><ul><li><strong>强度控制结果</strong>：展示了EmoDubber在Chem基准上通过调整正负引导尺度（α和β）来控制情感强度的能力。</li><li><strong>情感零样本转换</strong>：在没有情感标签的Chem数据集上重新合成具有五种不同情感的语音，与现有的情感TTS基线模型GenerSpeech进行比较。</li><li><strong>情感语音质量结果</strong>：在Chem测试集上测试了七种情感的语音质量，验证情感控制对音频-视觉同步、发音清晰度和说话人相似性的影响。</li></ul><h3 id="7-定性结果"><a href="#7-定性结果" class="headerlink" title="7. 定性结果"></a>7. 定性结果</h3><ul><li>邀请志愿者在三个视频上表达预期的情感和强度，并使用EmoDubber生成的音频样本进行可视化比较。</li></ul><p>这些实验全面评估了EmoDubber在音视频同步、语音质量、情感控制和用户自定义情感强度方面的表现，并与现有技术进行了比较，证明了其优越性能。</p><p></p><p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p><p class="faq-a"><strong>A</strong>: 尽管EmoDubber在电影配音任务中取得了显著的性能，但仍有一些领域可以进一步探索和改进：</p><h3 id="1-模型泛化能力"><a href="#1-模型泛化能力" class="headerlink" title="1. 模型泛化能力"></a>1. 模型泛化能力</h3><ul><li><strong>多语言和方言支持</strong>：当前模型主要针对单一语言，探索多语言和方言的支持可以扩大模型的应用范围。</li><li><strong>跨领域泛化</strong>：研究模型在不同类型的视频（如动画、纪录片、访谈等）和不同场景下的泛化能力。</li></ul><h3 id="2-情感表达的精细度"><a href="#2-情感表达的精细度" class="headerlink" title="2. 情感表达的精细度"></a>2. 情感表达的精细度</h3><ul><li><strong>更丰富的情感类别</strong>：目前EmoDubber主要处理几种基本情感，可以考虑引入更细致的情感类别，如“惊讶”、“轻蔑”等。</li><li><strong>情感转换的平滑性</strong>：研究如何在不同情感之间平滑过渡，使配音听起来更自然。</li></ul><h3 id="3-模型训练和优化"><a href="#3-模型训练和优化" class="headerlink" title="3. 模型训练和优化"></a>3. 模型训练和优化</h3><ul><li><strong>训练效率</strong>：探索更高效的训练策略，如迁移学习、半监督学习等，以减少训练时间和资源消耗。</li><li><strong>模型压缩和加速</strong>：研究模型压缩技术，以便在资源受限的设备上部署EmoDubber。</li></ul><h3 id="4-用户交互和控制"><a href="#4-用户交互和控制" class="headerlink" title="4. 用户交互和控制"></a>4. 用户交互和控制</h3><ul><li><strong>用户界面</strong>：开发直观的用户界面，让用户更容易地指定情感类型和强度，以及实时预览配音效果。</li><li><strong>个性化定制</strong>：允许用户上传自己的语音样本，使模型能够学习并模仿用户的特定语音特征。</li></ul><h3 id="5-音频和视频质量"><a href="#5-音频和视频质量" class="headerlink" title="5. 音频和视频质量"></a>5. 音频和视频质量</h3><ul><li><strong>更高分辨率的音频和视频</strong>：探索使用更高分辨率的音频和视频输入，以提高最终输出的质量。</li><li><strong>噪声和复杂背景的处理</strong>：研究如何在嘈杂或复杂背景中保持高质量的唇音同步和清晰的发音。</li></ul><h3 id="6-多模态融合"><a href="#6-多模态融合" class="headerlink" title="6. 多模态融合"></a>6. 多模态融合</h3><ul><li><strong>更多的视觉信息</strong>：除了唇动，还可以考虑融合面部表情、肢体语言等其他视觉信息，以增强情感表达。</li><li><strong>音频-视觉不一致的处理</strong>：研究如何处理音频和视频不一致的情况，例如，当视频中的说话人快速移动或遮挡嘴唇时。</li></ul><h3 id="7-伦理和隐私问题"><a href="#7-伦理和隐私问题" class="headerlink" title="7. 伦理和隐私问题"></a>7. 伦理和隐私问题</h3><ul><li><strong>数据隐私保护</strong>：确保在训练和使用过程中保护个人隐私，特别是在使用真实人物的视频时。</li><li><strong>伦理使用指南</strong>：制定使用指南，防止EmoDubber被用于制造虚假信息或侵犯版权。</li></ul><p>这些探索点不仅可以推动EmoDubber技术的发展，还有助于提高电影配音的自然度和表现力，同时也需要关注其在实际应用中的伦理和隐私问题。</p><p></p><p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p><p class="faq-a"><strong>A</strong>: 这篇论文提出了一个名为EmoDubber的情绪可控电影配音架构，旨在生成与视频同步的语音，同时复制所需的声音，并允许用户指定情绪类型和情绪强度。以下是论文的主要内容总结：</p><h3 id="1-问题定义"><a href="#1-问题定义" class="headerlink" title="1. 问题定义"></a>1. 问题定义</h3><ul><li>电影配音任务需要将文本转换成语音，与视频角色的口型同步，并表现出参考音频中的声音特征。</li><li>现有方法存在两个主要问题：音视频同步与清晰发音难以同时保证，以及缺乏表达用户定义情绪的能力。</li></ul><h3 id="2-EmoDubber架构"><a href="#2-EmoDubber架构" class="headerlink" title="2. EmoDubber架构"></a>2. EmoDubber架构</h3><ul><li>**Lip-related Prosody Aligning (LPA)**：通过时序级别的对比学习，学习唇动和韵律变化之间的一致性，以实现音视频对齐。</li><li>**Pronunciation Enhancing (PE)**：通过扩展视频级别的音素序列并使用高效Conformer模型融合，提高语音的可理解性。</li><li>**Speaker Identity Adapting (SIA)**：生成声学先验信息，并注入说话人的风格。</li><li>**Flow-based User Emotion Controlling (FUEC)**：基于流匹配预测网络，根据用户的情绪指令合成波形，并通过正负引导机制灵活调整情绪强度。</li></ul><h3 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h3><ul><li>在三个基准数据集（Chem, GRID, V2C-Animation）上进行了广泛的实验。</li><li>使用了多种评估指标，包括音视频同步评估、语音质量评估、情感评估和主观评估。</li><li>实验结果表明，EmoDubber在保持音视频同步和清晰发音的同时，能够实现情绪的可控合成，并优于几种最先进的方法。</li></ul><h3 id="4-贡献"><a href="#4-贡献" class="headerlink" title="4. 贡献"></a>4. 贡献</h3><ul><li>提出了EmoDubber，一个可控情绪配音架构，允许用户指定所需情绪，同时满足高质量唇部同步和清晰发音。</li><li>设计了具有正负引导的FUEC，动态调整流匹配向量场预测过程，实现灵活的情绪强度控制。</li><li>通过时序级别的对比学习和音素增强策略，实现了高质量的唇部同步和清晰发音。</li></ul><h3 id="5-未来工作"><a href="#5-未来工作" class="headerlink" title="5. 未来工作"></a>5. 未来工作</h3><ul><li>探索多语言和方言支持、情感表达的精细度、模型训练和优化、用户交互和控制、音频和视频质量、多模态融合以及伦理和隐私问题等方向。</li></ul><p>EmoDubber的提出为电影配音领域带来了新的可能性，通过结合先进的语音合成技术和情感控制机制，提高了配音的自然度和表现力。</p><p></p><p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p><p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing，链接是 https://arxiv.org/pdf/2412.08988 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2412.08988 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里<i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p></details><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-81e5e229543f6946871bebdf09beb67f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a33ea94e71766c98551a180c106c7452.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-42081eaa0f5f5450d9fbdd8522d49eb1.jpg" align="middle"></details></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-14/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-14/Talking%20Head%20Generation/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明:</i></span> <span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/Talking-Head-Generation/"><span class="chip bg-color">Talking Head Generation</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="far fa-dot-circle"></i> &nbsp;本篇</div><div class="card"><a href="/Talk2Paper/Paper/2024-12-14/Talking%20Head%20Generation/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-81e5e229543f6946871bebdf09beb67f.jpg" class="responsive-img" alt="Talking Head Generation"> <span class="card-title">Talking Head Generation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-14 EmoDubber Towards High Quality and Emotion Controllable Movie Dubbing</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-14</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">Talking Head Generation</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/Talking-Head-Generation/"><span class="chip bg-color">Talking Head Generation</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-12-14/Diffusion%20Models/"><div class="card-image"><img src="https://picx.zhimg.com/v2-7b724097ef34d3cdf88e704f2443e0f3.jpg" class="responsive-img" alt="Diffusion Models"> <span class="card-title">Diffusion Models</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-14 Image Generation Diversity Issues and How to Tame Them</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-14</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">Diffusion Models</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/Diffusion-Models/"><span class="chip bg-color">Diffusion Models</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">5676k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="本站已运行 "+c+" 天",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="本站已运行 "+l+" 年 "+c+" 天",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Σ(っ °Д °;)っ诶，页面崩溃了嘛？",clearTimeout(st)):(document.title="φ(゜▽゜*)♪咦，又好了！",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>