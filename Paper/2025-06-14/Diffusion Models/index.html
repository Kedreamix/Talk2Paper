<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-14  SceneCompleter Dense 3D Scene Completion for Generative Novel View   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-afb3e0f9e81b089497a900201e0aa3c3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    63 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-14-更新"><a href="#2025-06-14-更新" class="headerlink" title="2025-06-14 更新"></a>2025-06-14 更新</h1><h2 id="SceneCompleter-Dense-3D-Scene-Completion-for-Generative-Novel-View-Synthesis"><a href="#SceneCompleter-Dense-3D-Scene-Completion-for-Generative-Novel-View-Synthesis" class="headerlink" title="SceneCompleter: Dense 3D Scene Completion for Generative Novel View   Synthesis"></a>SceneCompleter: Dense 3D Scene Completion for Generative Novel View   Synthesis</h2><p><strong>Authors:Weiliang Chen, Jiayi Bi, Yuanhui Huang, Wenzhao Zheng, Yueqi Duan</strong></p>
<p>Generative models have gained significant attention in novel view synthesis (NVS) by alleviating the reliance on dense multi-view captures. However, existing methods typically fall into a conventional paradigm, where generative models first complete missing areas in 2D, followed by 3D recovery techniques to reconstruct the scene, which often results in overly smooth surfaces and distorted geometry, as generative models struggle to infer 3D structure solely from RGB data. In this paper, we propose SceneCompleter, a novel framework that achieves 3D-consistent generative novel view synthesis through dense 3D scene completion. SceneCompleter achieves both visual coherence and 3D-consistent generative scene completion through two key components: (1) a geometry-appearance dual-stream diffusion model that jointly synthesizes novel views in RGBD space; (2) a scene embedder that encodes a more holistic scene understanding from the reference image. By effectively fusing structural and textural information, our method demonstrates superior coherence and plausibility in generative novel view synthesis across diverse datasets. Project Page: <a target="_blank" rel="noopener" href="https://chen-wl20.github.io/SceneCompleter">https://chen-wl20.github.io/SceneCompleter</a> </p>
<blockquote>
<p>生成模型通过减少对密集多视角捕捉的依赖，在新型视图合成（NVS）中获得了极大的关注。然而，现有方法通常遵循一种传统范式，即生成模型首先在2D中完成缺失区域，然后采用3D恢复技术进行场景重建，这往往导致表面过于平滑和几何失真，因为生成模型很难仅从RGB数据中推断出3D结构。在本文中，我们提出了SceneCompleter，这是一种通过密集3D场景完成实现3D一致生成新型视图合成的新框架。SceneCompleter通过两个关键组件实现了视觉连贯性和3D一致生成场景完成：（1）几何外观双流扩散模型，在RGBD空间中联合合成新型视图；（2）场景嵌入器，从参考图像中编码更整体的场景理解。通过有效地融合结构和纹理信息，我们的方法在多种数据集上的生成新型视图合成中表现出卓越的一致性和合理性。项目页面：<a target="_blank" rel="noopener" href="https://chen-wl20.github.io/SceneCompleter">https://chen-wl20.github.io/SceneCompleter</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10981v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为SceneCompleter的新型框架，用于通过密集的3D场景补全实现3D一致性的生成式新型视图合成。该框架通过结合几何和纹理信息，实现了视觉连贯性和3D一致性，提高了生成式新型视图合成的连贯性和可信度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型在新型视图合成（NVS）中受到关注，能够减少对密集多视图捕捉的依赖。</li>
<li>现有方法通常遵循先2D补全再3D恢复的常规模式，但会导致表面过于平滑和几何失真。</li>
<li>SceneCompleter框架通过密集的3D场景补全实现3D一致性生成新型视图合成。</li>
<li>SceneCompleter包含两个关键组件：几何外观双流扩散模型和场景嵌入器。</li>
<li>几何外观双流扩散模型在RGBD空间中联合合成新型视图。</li>
<li>场景嵌入器从参考图像编码更整体的场景理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d202288523a503c67dd2f6a0917e4cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59325d4d64469dc0edcae4790398b3dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76f2232bfa2a31f00c46f45b4d47e794.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning"><a href="#MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning" class="headerlink" title="MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning"></a>MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning</h2><p><strong>Authors:Yuxuan Luo, Yuhui Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian</strong></p>
<p>In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learning–a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target image’s core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficits–low entity fidelity, weak relations, and clutter–with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmark’s difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs. </p>
<blockquote>
<p>本文介绍了一项新知识图像生成任务，以及大规模多学科分层知识图像生成基准测试（MMMG），以检测图像生成模型的推理能力。知识图像在人类文明和人类学习机制中占据核心地位，这一事实由双重编码理论和图像优势效应强调。生成这样的图像具有挑战性，需要多模态推理，将世界知识与像素级地面融合成清晰的解释性视觉。为了进行全面的评估，MMMG提供了4456对专家验证（知识）图像提示，涵盖10个学科，6个教育水平，以及图表、图表和思维导图等多样化的知识形式。为了消除评估过程中的混淆复杂性，我们采用统一的知识图谱（KG）表示法。每个KG明确界定了目标图像的核心实体及其依赖关系。我们还介绍了MMMG评分来评估生成的知识图像。该指标结合了事实准确性（通过知识图谱之间的图编辑距离来衡量）和视觉清晰度评估。对16个最先进的文本到图像生成模型的全面评估揭示了严重的推理缺陷——实体保真度低、关系弱和杂乱——GPT-4o的MMMG评分仅为50.20，凸显了基准测试的困难。为了促进进一步的进步，我们发布了FLUX-Reason（MMMG评分为34.45），这是一个有效的开放基线，它结合了推理大型语言模型与扩散模型，并在16000个精选的知识图像提示对上进行了训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10963v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一项新知识图像生成任务，并伴随着大规模多学科多层级知识图像生成基准（MMMG）的出现，以检测图像生成模型的推理能力。知识图像在人类文明和人类学习机制中占据核心地位，生成这类图像需要多模态推理，融合世界知识与像素级地面实况，形成清晰的解释性视觉。MMMG提供了4456张专家验证的知识图像提示对，涵盖10个学科、6个教育级别和多种知识形式。为全面评估，采用统一的知识图谱表示，消除评估中的混淆复杂性。引入MMMG评分来评估生成的知识图像，结合核心实体及其依赖性的图谱编辑距离与视觉清晰度评估。对16款最先进的文本到图像生成模型的全面评估显示存在严重的推理缺陷，GPT-4o在MMMG评分中仅得50.20，突显了基准的难度。为促进行业进步，发布了一个有效的开放基线FLUX-Reason，其MMMG评分为34.45，结合了推理大型语言模型与扩散模型，并在16000个精选的知识图像提示对上进行了训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了知识图像生成作为新任务，以及大规模多学科多层级知识图像生成基准（MMMG）以测试图像生成模型的推理能力。</li>
<li>知识图像对于人类文明与学习的核心重要性被强调。</li>
<li>生成知识图像需要多模态推理，融合世界知识与像素级视觉表达。</li>
<li>MMMG包含专家验证的4456个知识图像提示对，涵盖多个学科、教育级别和知识形式。</li>
<li>统一采用知识图谱表示以进行综合评价，并消除复杂性混淆。</li>
<li>引入MMMG评分评估生成的知识图像质量，包括事实准确性和视觉清晰度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10963">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fa6208a0529fb712dc324be70c96a457.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-759b43668dcc73b54c3573d07f8cd979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61f5cc35af2995dc9d941893ca19df72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a377a146a57f5f4f3673b420023513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6417f68225408b87a977c020f398d7f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f2970b49a2b8102ee26dc5923c90338.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SpectralAR-Spectral-Autoregressive-Visual-Generation"><a href="#SpectralAR-Spectral-Autoregressive-Visual-Generation" class="headerlink" title="SpectralAR: Spectral Autoregressive Visual Generation"></a>SpectralAR: Spectral Autoregressive Visual Generation</h2><p><strong>Authors:Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Yueqi Duan, Jie Zhou, Jiwen Lu</strong></p>
<p>Autoregressive visual generation has garnered increasing attention due to its scalability and compatibility with other modalities compared with diffusion models. Most existing methods construct visual sequences as spatial patches for autoregressive generation. However, image patches are inherently parallel, contradicting the causal nature of autoregressive modeling. To address this, we propose a Spectral AutoRegressive (SpectralAR) visual generation framework, which realizes causality for visual sequences from the spectral perspective. Specifically, we first transform an image into ordered spectral tokens with Nested Spectral Tokenization, representing lower to higher frequency components. We then perform autoregressive generation in a coarse-to-fine manner with the sequences of spectral tokens. By considering different levels of detail in images, our SpectralAR achieves both sequence causality and token efficiency without bells and whistles. We conduct extensive experiments on ImageNet-1K for image reconstruction and autoregressive generation, and SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project page: <a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a>. </p>
<blockquote>
<p>自回归视觉生成由于其可扩展性以及与扩散模型相比与其他模态的兼容性而受到越来越多的关注。大多数现有方法通过构建空间补丁来进行自回归生成视觉序列。然而，图像补丁本质上是并行的，这与自回归模型的因果性质相矛盾。为了解决这一问题，我们提出了一种谱自回归（SpectralAR）视觉生成框架，从谱的角度实现了视觉序列的因果性。具体来说，我们首先使用嵌套谱令牌化将图像转换为有序的谱令牌，代表从低到高的频率分量。然后，我们在谱令牌序列上以从粗到细的方式执行自回归生成。通过考虑图像的不同层次细节，我们的SpectralAR在不增加额外复杂性的情况下实现了序列的因果性和令牌效率。我们在ImageNet-1K上进行了大量的图像重建和自回归生成实验，SpectralAR仅使用64个令牌和3.1亿个参数就实现了3.02的gFID。项目页面：<a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10962v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a></p>
<p><strong>Summary</strong><br>     谱自回归（SpectralAR）视觉生成框架采用谱视角实现视觉序列的因果性，通过嵌套谱令牌化将图像转化为有序谱令牌，实现粗到细的自动回归生成。该方法考虑图像不同层次的细节，实现序列因果性和令牌效率。在ImageNet-1K上的实验表明，SpectralAR在图像重建和自动回归生成方面表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自回归视觉生成受到关注，因其可扩展性与其他模态的兼容性。</li>
<li>现有方法构建视觉序列作为空间补丁进行自回归生成，但图像补丁本质上是并行的，与自回归模型的因果性质相矛盾。</li>
<li>谱自回归（SpectralAR）框架从谱角度实现视觉序列的因果性。</li>
<li>SpectralAR使用嵌套谱令牌化将图像转化为有序谱令牌。</li>
<li>该方法实现粗到细的自动回归生成，考虑图像不同层次的细节。</li>
<li>SpectralAR在ImageNet-1K上的图像重建和自动回归生成实验表现出色，达到3.02 gFID，仅使用64个令牌和310M参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f593045d6e96db8496e4cb7139f376a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d97c749368d00e22063b7b19885512.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-238744431ff7f422ed998ae3b941127e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2ccc14ec6ed220d153354326150eab7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d4ad6dff0330057862e861bb841dc6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26b98dd1561b37605d4b0184f1a9da91.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ME-Trigger-Element-Combination-Backdoor-Attack-on-Copyright-Infringement"><a href="#ME-Trigger-Element-Combination-Backdoor-Attack-on-Copyright-Infringement" class="headerlink" title="ME: Trigger Element Combination Backdoor Attack on Copyright   Infringement"></a>ME: Trigger Element Combination Backdoor Attack on Copyright   Infringement</h2><p><strong>Authors:Feiyu Yang, Siyuan Liang, Aishan Liu, Dacheng Tao</strong></p>
<p>The capability of generative diffusion models (DMs) like Stable Diffusion (SD) in replicating training data could be taken advantage of by attackers to launch the Copyright Infringement Attack, with duplicated poisoned image-text pairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew outstanding performance in attacking SD in text-to-image tasks. However, the feasible data resources in this area are still limited, some of them are even constrained or prohibited due to the issues like copyright ownership or inappropriate contents; And not all of the images in current datasets are suitable for the proposed attacking methods; Besides, the state-of-the-art (SoTA) performance of SBD is far from ideal when few generated poisoning samples could be adopted for attacks. In this paper, we raised new datasets accessible for researching in attacks like SBD, and proposed Multi-Element (ME) attack method based on SBD by increasing the number of poisonous visual-text elements per poisoned sample to enhance the ability of attacking, while importing Discrete Cosine Transform (DCT) for the poisoned samples to maintain the stealthiness. The Copyright Infringement Rate (CIR) &#x2F; First Attack Epoch (FAE) we got on the two new datasets were 16.78% &#x2F; 39.50 and 51.20% &#x2F; 23.60, respectively close to or even outperformed benchmark Pokemon and Mijourney datasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI and DCT earned CIR &#x2F; FAE of 0.23% &#x2F; 84.00 and 12.73% &#x2F; 65.50, both better than original SBD, which failed to attack at all. </p>
<blockquote>
<p>生成式扩散模型（如Stable Diffusion）在复制训练数据方面的能力可能会被攻击者利用来发动版权侵犯攻击，使用重复的毒化图像文本对。最近提出的SilentBadDiffusion（SBD）方法在文本到图像的任务中对SD的攻击表现出色。然而，这一领域的可用数据资源仍然有限，其中一些受到版权或内容不当等问题的限制或禁止；此外，当前数据集并非所有图像都适合用于提出的攻击方法。除此之外，当采用较少的生成毒化样本进行攻击时，SBD的最新性能表现并不理想。在本文中，我们推出了可用于研究类似SBD攻击的新数据集，并基于SBD提出了Multi-Element（ME）攻击方法，通过增加每个毒化样本中的有毒视觉文本元素数量来提升攻击能力，同时引入离散余弦变换（DCT）以保持隐蔽性。我们在两个新数据集上的版权侵犯率（CIR）&#x2F;首次攻击周期（FAE）分别为16.78%&#x2F;39.50%和51.20%&#x2F;23.60%，接近甚至超过了基准数据集Pokemon和Mijourney的表现。在低子采样比率（5%，6个毒化样本）的条件下，MESI和DCT获得了CIR&#x2F;FAE为0.23%&#x2F;84.00%和12.73%&#x2F;65.50%，均优于原始SBD，后者根本无法进行攻击。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10776v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>生成性扩散模型（如Stable Diffusion）在复制训练数据方面的能力可能被攻击者利用，通过复制有毒的图像-文本对来发动版权侵权攻击。本文提出了Multi-Element（ME）攻击方法，基于SilentBadDiffusion（SBD），通过增加每个有毒样本中的有毒视觉-文本元素数量来提高攻击能力，并采用离散余弦变换（DCT）保持隐蔽性。在新提出的两个数据集上，我们的版权侵权率（CIR）&#x2F;首次攻击周期（FAE）接近或甚至超过了基准数据集。在较低的子采样比条件下，结合MESI和DCT技术，我们的攻击方法取得了显著的成果，明显优于原始的SBD。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型如Stable Diffusion的复制训练数据能力可能被攻击者用于进行版权侵权攻击。</li>
<li>SilentBadDiffusion是一种新的攻击方法，在文本到图像任务中对Stable Diffusion具有出色的攻击性能。</li>
<li>当前数据集存在限制，部分数据集因版权或内容不当而受到约束或禁止。</li>
<li>并非当前数据集中的所有图像都适合用于提出的攻击方法。</li>
<li>提出了一种新的数据集，可用于研究如SBD的攻击方法。</li>
<li>Multi-Element（ME）攻击方法基于SBD，通过增加每个有毒样本中的有毒视觉-文本元素数量来提高攻击能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10776">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-563f364d124ed1746cbc255f39e11b68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-feadf34e87e6e459cec0ee7791cc92b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63cf61f13e8aa66fe4088fd88ead366d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06f21a1a430ba2e698cec7703b9aabf2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models"><a href="#Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models" class="headerlink" title="Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent   Diffusion Models"></a>Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent   Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Ilias Stogiannidis, Junyu Yan, Alison Q. O’Neil, Sotirios A. Tsaftaris</strong></p>
<p>Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available. </p>
<blockquote>
<p>近年来，潜在扩散模型在文本引导的图像合成中取得了显著成果。在自然（RGB）图像领域，最新研究表明，此类模型可以适应各种视觉语言下游任务，且几乎无需监督。相反，在医学成像领域，文本到图像的潜在扩散模型仍然相对未被充分研究，这主要是由于数据可用性有限（例如，由于隐私担忧）。在这项工作中，我们重点关注胸部X射线模态。首先，我们证明标准的文本条件潜在扩散模型还没有学会将自由文本放射学报告中的临床相关信息与给定扫描的相应区域对齐。为了解决这个问题，我们提出了一个微调框架，以改进预训练模型中的多模态对齐，使其能够高效地用于下游任务，如短语定位。我们的方法在标准数据集（MS-CXR）上达到了最新技术水平，并在非分布数据（VinDr-CXR）上表现出稳健的性能。我们的代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10633v1">PDF</a> 14 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>文本介绍了潜在扩散模型在自然图像领域的成功应用，以及其在医学成像领域中的相对探索不足，特别是在医疗图像文本转图像方面的应用。文章针对胸部X射线模态提出了一种微调框架，以提高预训练模型的多模态对齐能力，使其能够高效用于下游任务如短语定位。该方法在标准数据集MS-CXR上达到了新的技术水平，并在超出分布的数据集VinDr-CXR上展现出稳健的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>潜在扩散模型在自然图像领域的文本引导图像合成中取得了显著成果。</li>
<li>在医学成像领域，文本转图像的潜在扩散模型相对探索不足，主要由于数据可用性的限制。</li>
<li>文章针对胸部X射线模态的模型对齐问题进行了深入研究。</li>
<li>提出了一种微调框架，旨在提高预训练模型的多模态对齐能力。</li>
<li>该方法在标准数据集MS-CXR上实现了新的技术水平。</li>
<li>模型在超出分布的数据集VinDr-CXR上展现出稳健的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-27b80e05982ab3ad87e589c3677a0e91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78406b024805e620118e7f4b5250b47b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a21d355d7538d5e2962e0af3cd2c021.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TexTailor-Customized-Text-aligned-Texturing-via-Effective-Resampling"><a href="#TexTailor-Customized-Text-aligned-Texturing-via-Effective-Resampling" class="headerlink" title="TexTailor: Customized Text-aligned Texturing via Effective Resampling"></a>TexTailor: Customized Text-aligned Texturing via Effective Resampling</h2><p><strong>Authors:Suin Lee, Dae-Shik Kim</strong></p>
<p>We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these approaches lead to a gradual shift in texture properties across viewpoints due to (1) insufficient integration of previously synthesized textures at each viewpoint during the diffusion process and (2) the autoregressive nature of the texture synthesis process. Moreover, the predefined selection of camera positions, which does not account for the object’s geometry, limits the effective use of texture information synthesized from different viewpoints, ultimately degrading overall texture consistency. In TexTailor, we address these issues by (1) applying a resampling scheme that repeatedly integrates information from previously synthesized textures within the diffusion process, and (2) fine-tuning a depth-aware diffusion model on these resampled textures. During this process, we observed that using only a few training images restricts the model’s original ability to generate high-fidelity images aligned with the conditioning, and therefore propose an performance preservation loss to mitigate this issue. Additionally, we improve the synthesis of view-consistent textures by adaptively adjusting camera positions based on the object’s geometry. Experiments on a subset of the Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures. The source code for TexTailor is available at <a target="_blank" rel="noopener" href="https://github.com/Adios42/Textailor">https://github.com/Adios42/Textailor</a> </p>
<blockquote>
<p>我们提出了TexTailor，这是一种从文本描述生成一致物体纹理的新方法。现有的文本到纹理合成方法利用深度感知扩散模型逐步生成图像，并在预定的多个视角合成纹理。然而，这些方法由于在扩散过程中（1）没有在每个视点上充分整合之前合成的纹理信息，（2）纹理合成过程的自回归性质，导致在不同视角之间纹理属性的逐渐变化。此外，预定相机位置的选择没有考虑到物体的几何结构，限制了从不同视角合成的纹理信息的有效使用，最终降低了纹理的整体一致性。在TexTailor中，我们通过（1）应用一种重采样方案，在扩散过程中反复整合之前合成的纹理信息，（2）对深度感知扩散模型进行微调，以解决这些问题。在此过程中，我们发现仅使用少量训练图像会限制模型生成与条件相符的高保真图像的能力，因此提出了一种性能保持损失来缓解这个问题。此外，我们通过根据物体的几何结构自适应地调整相机位置，改进了视图一致的纹理合成。在Objaverse数据集和ShapeNet汽车数据集的子集上的实验表明，TexTailor在合成视图一致的纹理方面优于现有最先进的方法。TexTailor的源代码可在<a target="_blank" rel="noopener" href="https://github.com/Adios42/Textailor">https://github.com/Adios42/Textailor</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10612v1">PDF</a> Submitted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>TexTailor是一种从文本描述生成一致物体纹理的新方法。它解决了现有文本到纹理合成方法在扩散过程中因纹理信息集成不足和纹理合成过程的自回归性质导致的视角间纹理属性逐渐变化的问题。通过应用重采样方案和微调深度感知扩散模型，TexTailor提高了纹理的一致性。此外，该方法还通过性能保持损失和基于物体几何自适应调整摄像头位置，改进了视图一致的纹理合成。实验表明，TexTailor在合成视图一致的纹理方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TexTailor是一种用于从文本描述生成一致物体纹理的新方法。</li>
<li>现有文本到纹理合成方法存在视角间纹理属性逐渐变化的问题。</li>
<li>TexTailor通过应用重采样方案和微调深度感知扩散模型解决了这一问题。</li>
<li>性能保持损失用于解决使用少量训练图像限制模型生成高质量图像的问题。</li>
<li>TexTailor通过自适应调整摄像头位置，基于物体几何改进了视图一致的纹理合成。</li>
<li>实验表明，TexTailor在合成视图一致的纹理方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8303a556773021fa8169d1043ea1de89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd1d514866977c39175e3ad5778cc017.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9e486ca7c2db84cbe5f583de6dfe06c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6a071af14afdbe11139ae37fcc6054e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9c605478c778956877c4bd93db580e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-resolution-efficient-image-generation-from-WiFi-CSI-using-a-pretrained-latent-diffusion-model"><a href="#High-resolution-efficient-image-generation-from-WiFi-CSI-using-a-pretrained-latent-diffusion-model" class="headerlink" title="High-resolution efficient image generation from WiFi CSI using a   pretrained latent diffusion model"></a>High-resolution efficient image generation from WiFi CSI using a   pretrained latent diffusion model</h2><p><strong>Authors:Eshan Ramesh, Nishio Takayuki</strong></p>
<p>We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM’s denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM’s pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability. </p>
<blockquote>
<p>我们提出了LatentCSI，这是一种利用预训练的潜在扩散模型（LDM）从WiFi CSI测量生成物理环境图像的新型方法。不同于依赖复杂且计算密集的技术（如GANs）的先前方法，我们的方法采用轻量级神经网络直接将CSI振幅映射到LDM的潜在空间。然后，我们对潜在表示应用LDM的去噪扩散模型，并使用文本指导，然后使用LDM的预训练解码器进行解码，以获得高分辨率图像。这种设计绕过了像素空间图像生成的挑战，避免了传统图像到图像管道中通常需要的显式图像编码阶段，从而实现了高效的高质量图像合成。我们在两个数据集上验证了我们的方法：我们使用现成的WiFi设备和相机收集的宽带CSI数据集；以及公开可用的MM-Fi数据集的子集。结果表明，LatentCSI在计算效率和感知质量方面都优于直接在地面上训练的基准图像的可比复杂性，同时，通过其独特的文本指导可控性提供了实际优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10605v1">PDF</a> 6 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>基于WiFi CSI测量的物理环境图像生成新方法LatentCSI的研究介绍。该方法利用预训练的潜在扩散模型（LDM），通过轻量级神经网络将CSI振幅映射到LDM的潜在空间，再应用LDM的降噪扩散模型进行文本引导的图像生成。该方法绕过像素空间图像生成的挑战，避免了传统图像到图像的管道所需的显式图像编码阶段，实现了高效高质量图像合成。在收集的宽带CSI数据集和公开可用的MM-Fi数据集上验证了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LatentCSI是一种基于WiFi CSI测量的物理环境图像生成新方法。</li>
<li>该方法利用预训练的潜在扩散模型（LDM）进行图像生成。</li>
<li>LatentCSI使用轻量级神经网络将CSI振幅映射到LDM的潜在空间。</li>
<li>方法应用LDM的降噪扩散模型进行文本引导的图像生成。</li>
<li>该方法避免了像素空间图像生成的挑战和显式图像编码阶段。</li>
<li>LatentCSI实现了高效且高质量的图像合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10605">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-50a204208f6a2eb9490386ba4f22d16c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-479d243450434ea721b14ee0d4aecd62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b038f02929a04ad6d70436ee07f8f05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58f54bc84b644d62c8e1ce1338a978da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f923f27663d2fd29b6546a82014a35b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4f775cd3111d69053aefaadbdd98340.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75263e7de639f69302aae9ae3bb63a7d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Conditional-diffusion-models-for-guided-anomaly-detection-in-brain-images-using-fluid-driven-anomaly-randomization"><a href="#Conditional-diffusion-models-for-guided-anomaly-detection-in-brain-images-using-fluid-driven-anomaly-randomization" class="headerlink" title="Conditional diffusion models for guided anomaly detection in brain   images using fluid-driven anomaly randomization"></a>Conditional diffusion models for guided anomaly detection in brain   images using fluid-driven anomaly randomization</h2><p><strong>Authors:Ana Lawry Aguila, Peirong Liu, Oula Puonti, Juan Eugenio Iglesias</strong></p>
<p>Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our model’s ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased&#x2F;healthy images. </p>
<blockquote>
<p>监督式机器学习已在脑MRI病理检测中实现了高精度，但这需要来自病患的训练数据，在某些情况下可能并不容易获得，例如在罕见疾病的情况下。基于重建的无监督异常检测，特别是使用扩散模型的方法，已在医疗领域受到欢迎，因为它可以仅对健康图像进行训练，从而无需大量特定疾病的群体。这些方法假设在正常数据上训练的模型无法准确表示或重建异常。然而，这一假设常常失败，因为模型无法重建健康组织或准确重建异常区域（即无法消除异常）。在这项工作中，我们引入了一种新型的条件扩散模型框架，用于脑MRI中的异常检测和健康图像重建。我们的弱监督方法将合成生成的伪病理图像集成到建模过程中，以更好地引导健康图像的重建。为了生成这些伪病理图像，我们采用流体驱动的异常随机化方法来增强辅助数据集中的真实病理分割图，确保合成异常既真实又解剖连贯。我们使用合成异常数据集和ATLAS数据集中的真实病理来评估我们模型检测病理的能力。在广泛的实验中，我们的模型：（i）始终优于变分自编码器以及有条件和无条件的潜在扩散；（ii）在大多数数据集上，超越了使用配对疾病&#x2F;健康图像的监督修复方法的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10233v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本论文提出了一种基于扩散模型的新条件扩散模型框架，用于医学图像中的异常检测和健康图像重建。通过引入合成伪病理图像进行弱监督训练，指导模型重建健康图像。实验表明，该模型在合成异常数据集和真实病理数据集上的表现均优于其他方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型被用于医学图像中的异常检测和健康图像重建。</li>
<li>该模型利用合成伪病理图像进行弱监督训练。</li>
<li>伪病理图像是通过流体驱动异常随机化技术生成的，增强了现实病理分割图的可用性。</li>
<li>模型通过集成合成伪病理图像，可以更好地重建健康图像。</li>
<li>该模型在合成异常数据集上的表现持续优于变分自动编码器以及其他有条件和无条件的潜在扩散模型。</li>
<li>在大多数数据集上，该模型的性能甚至超越了使用配对疾病&#x2F;健康图像的监督修复方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10233">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f05fa6420a3004beb80f4cbf15b042c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08d7e9fec1db5ed9ed98c8b405e62b03.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPARKE-Scalable-Prompt-Aware-Diversity-Guidance-in-Diffusion-Models-via-RKE-Score"><a href="#SPARKE-Scalable-Prompt-Aware-Diversity-Guidance-in-Diffusion-Models-via-RKE-Score" class="headerlink" title="SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via   RKE Score"></a>SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via   RKE Score</h2><p><strong>Authors:Mohammad Jalali, Haoyu Lei, Amin Gohari, Farzan Farnia</strong></p>
<p>Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware R&#39;eny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: <a target="_blank" rel="noopener" href="https://mjalali.github.io/SPARKE">https://mjalali.github.io/SPARKE</a> </p>
<blockquote>
<p>扩散模型在高保真图像合成和提示引导生成建模方面取得了显著的成功。然而，在提示引导的扩散模型生成的样本中确保足够的多样性仍然是一个挑战，特别是在提示跨越广泛语义谱且需要在语义相似提示之间以提示感知的方式评估生成数据的多样性时。最近的方法通过引入基于多样性的度量来指导更丰富的生成。在这项工作中，我们提出了可扩展的提示感知Rényi核熵多样性指导（SPARKE）方法，以扩展基于多样性度量的方法，用于提示感知的多样性指导。SPARKE利用条件熵进行多样性指导，它可以根据相似提示动态调整多样性测量，并能够实现提示感知的多样性控制。虽然基于熵的指导方法提高了提示感知的多样性，但它对矩阵基于的熵分数的依赖在大型生成设置中构成了计算挑战。为了解决这一问题，我们关注条件潜在RKE评分指导的特殊情况，将一般熵度量的O(n^3)的熵计算和基于梯度的优化复杂性降低到O(n)。降低的计算复杂性允许在多个提示上进行数千轮生成的多样性引导采样。我们在多个文本到图像的扩散模型上对SPARKE方法进行了数值测试，结果表明，该方法提高了生成数据的提示感知多样性，且没有产生显著的计算成本。我们在项目页面上发布了我们的代码：<a target="_blank" rel="noopener" href="https://mjalali.github.io/SPARKE">https://mjalali.github.io/SPARKE</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Diffusion模型在图像合成和提示引导生成建模方面的出色表现。然而，对于确保提示引导扩散模型的生成样本具有足够的多样性仍是一大挑战，特别是在语义谱广泛的提示和需要在语义相似的提示之间进行提示感知的多样性评估时。本文提出了可扩展的提示感知Rényi核熵多样性指导（SPARKE）方法来解决这一问题。SPARKE采用基于条件熵的多样性指导，该方法能够根据相似的提示动态调节多样性测量，实现提示感知的多样性控制。虽然熵指导方法提高了提示感知的多样性，但其依赖于矩阵的熵分数在计算大型生成场景中存在一定的挑战。为此，本文专注于条件潜在RKE得分指导的特殊案例，将熵计算和基于梯度的优化复杂度从一般熵测量的O(n^3)降低到O(n)。减少了计算复杂度，允许在多个提示上进行数千轮带多样性的采样。本文在多个文本到图像的扩散模型上对SPARKE方法进行了数值测试，表明该方法在提高生成数据的提示感知多样性的同时不会带来重大计算成本。我们的代码已在项目页面上发布：<a target="_blank" rel="noopener" href="https://mjalali.github.io/SPARKE">https://mjalali.github.io/SPARKE</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion模型在图像合成和提示引导生成建模方面表现出显著的成功。</li>
<li>在广泛的语义提示下确保生成样本的多样性是一个挑战。</li>
<li>提出了一种新的方法——SPARKE，通过条件熵实现提示感知的多样性指导。</li>
<li>SPARKE能够根据相似的提示动态调整多样性测量。</li>
<li>相较于一般熵测量的O(n^3)，SPARKE专注于条件潜在RKE得分指导，将计算复杂度降低到O(n)。</li>
<li>SPARKE方法提高了生成数据的提示感知多样性，同时不会带来显著的计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10173">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19b3e1439ce1e152412a161e1c08ad3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18948a9d5bd7d17057436a68f45bc115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c8f757bde736b9aeb76b6c07a80c52a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning"><a href="#LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning" class="headerlink" title="LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning"></a>LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning</h2><p><strong>Authors:Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue</strong></p>
<p>Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods. </p>
<blockquote>
<p>使用扩散模型进行视频编辑已经在生成高质量视频编辑方面取得了显著成果。然而，当前的方法常常依赖于大规模预训练，这限制了特定编辑的灵活性。虽然基于首帧引导编辑能够控制首帧，但对于后续帧的控制则不足。为解决这一问题，我们提出了一种基于掩码的LoRA（低秩适配）调优方法，该方法能够适应预训练好的图像到视频（I2V）模型，实现灵活的视频编辑。我们的方法能够保留背景区域，同时实现可控的编辑传播。这一解决方案在不改变模型架构的情况下，实现了高效且可适应的视频编辑。为了更好地引导这一过程，我们引入了额外的参考，如不同的观点或代表性的场景状态，它们作为内容展开的视觉锚点。我们采用掩码驱动的LoRA调优策略来解决控制挑战，该策略将预训练的图像到视频模型适应于编辑上下文。模型必须从两个独特的信息源中学习：输入视频提供空间结构和运动线索，而参考图像提供外观指导。空间掩膜通过动态调制模型所关注的重点，实现特定区域的学习，确保每个区域都能从适当的信息源中汲取知识。实验结果表明，我们的方法在视频编辑性能上优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10082v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的视频编辑已生成高质量编辑视频，取得了显著成果。针对现有方法依赖大规模预训练、缺乏特定编辑灵活性以及第一帧引导编辑缺乏后续帧灵活性的问题，本文提出了一种基于掩膜的LoRA（低秩适应）调优方法，用于适应预训练图像到视频（I2V）模型的灵活视频编辑。该方法可保留背景区域，实现可控编辑传播，提供高效、可适应的视频编辑，无需更改模型架构。为了更好地引导这一过程，本文引入了额外的参考，如不同的观点或代表性的场景状态，作为内容展开的视觉锚点。通过掩膜驱动的LoRA调优策略，解决控制挑战，使预训练图像到视频模型适应编辑上下文。模型从两个不同来源学习：输入视频提供空间结构和运动线索，参考图像提供外观指导。空间掩膜通过动态调制模型关注的区域，确保每个区域都从适当的来源中绘制信息。实验结果表明，该方法在视频编辑性能上优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在视频编辑中生成高质量结果。</li>
<li>当前方法依赖大规模预训练，缺乏特定编辑的灵活性。</li>
<li>提出基于掩膜的LoRA调优方法，适应预训练I2V模型进行灵活视频编辑。</li>
<li>方法可保留背景区域，实现可控编辑传播。</li>
<li>引入额外参考，如不同观点或场景状态，作为内容展开的视觉锚点。</li>
<li>采用掩膜驱动的LoRA调优策略，使模型适应编辑上下文。</li>
<li>模型从输入视频和参考图像两个来源学习，空间掩膜确保区域特定学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10082">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d68b6950080a3e2d7b7aadf086f03ab9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53ab42514b300b74be0259d543e6d0e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d6fcf3b0ac9b7f7e8a2c2220138e3b9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations"><a href="#Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations" class="headerlink" title="Towards Reliable Identification of Diffusion-based Image Manipulations"></a>Towards Reliable Identification of Diffusion-based Image Manipulations</h2><p><strong>Authors:Alex Costanzino, Woody Bayliss, Juil Sock, Marc Gorriz Blanch, Danijela Horak, Ivan Laptev, Philip Torr, Fabio Pizzati</strong></p>
<p>Changing facial expressions, gestures, or background details may dramatically alter the meaning conveyed by an image. Notably, recent advances in diffusion models greatly improve the quality of image manipulation while also opening the door to misuse. Identifying changes made to authentic images, thus, becomes an important task, constantly challenged by new diffusion-based editing tools. To this end, we propose a novel approach for ReliAble iDentification of inpainted AReas (RADAR). RADAR builds on existing foundation models and combines features from different image modalities. It also incorporates an auxiliary contrastive loss that helps to isolate manipulated image patches. We demonstrate these techniques to significantly improve both the accuracy of our method and its generalisation to a large number of diffusion models. To support realistic evaluation, we further introduce BBC-PAIR, a new comprehensive benchmark, with images tampered by 28 diffusion models. Our experiments show that RADAR achieves excellent results, outperforming the state-of-the-art in detecting and localising image edits made by both seen and unseen diffusion models. Our code, data and models will be publicly available at <a target="_blank" rel="noopener" href="https://alex-costanzino.github.io/radar/">https://alex-costanzino.github.io/radar/</a>. </p>
<blockquote>
<p>改变面部表情、手势或背景细节可能会极大地改变图像所传达的意义。值得注意的是，扩散模型的最新进展虽然大大提高了图像操作的质量，但同时也打开了滥用的大门。因此，识别对真实图像所做的改变成为了一项重要的任务，这一任务不断受到新的基于扩散的编辑工具的挑战。为此，我们提出了一种新的可靠识别图像修复区域的方法（RADAR）。RADAR建立在现有的基础模型上，结合了不同图像模态的特征。它还引入了一个辅助对比损失，有助于隔离被操纵的图像补丁。我们证明这些技术可以显著提高我们方法的准确性，并且其可以推广到大量扩散模型。为了支持真实评估，我们还引入了BBC-PAIR，这是一个新的综合基准测试，包含被28种扩散模型篡改过的图像。我们的实验表明，RADAR取得了优异的结果，在检测和定位已知和未知的扩散模型对图像进行的编辑方面均优于现有技术。我们的代码、数据和模型将在<a target="_blank" rel="noopener" href="https://alex-costanzino.github.io/radar/%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://alex-costanzino.github.io/radar/公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05466v2">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://alex-costanzino.github.io/radar/">https://alex-costanzino.github.io/radar/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于扩散模型的新型图像编辑技术所带来的挑战，并提出了一种名为RADAR的新方法，用于可靠地识别图像中的编辑区域。RADAR结合不同图像模态的特征，并引入辅助对比损失以区分编辑过的图像区域。此外，为了支持真实评估，还推出了BBC-PAIR新基准测试，包含由28种扩散模型修改的图像。实验表明，RADAR在检测和定位由已知和未知扩散模型进行的图像编辑方面取得了卓越成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的进步提高了图像操纵的质量，同时也增加了误用的风险。</li>
<li>提出了一种新的方法RADAR，用于识别图像中的编辑区域。</li>
<li>RADAR结合了不同图像模态的特征，并引入辅助对比损失以提高准确性和泛化能力。</li>
<li>推出了BBC-PAIR基准测试，支持对图像编辑检测方法的真实评估。</li>
<li>RADAR在检测和定位由多种扩散模型进行的图像编辑方面表现出卓越性能。</li>
<li>RADAR方法不仅在已知扩散模型上表现良好，对未知扩散模型也具有强大的检测能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05466">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2ee96aff50d70666abe7e941024a3fca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af2166a234b6d1a78d8fac83caa42cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84359b69dca16787536bd8d3beb9726.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6eaf4c668fffd453db25aedf5883b7f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Sparc3D-Sparse-Representation-and-Construction-for-High-Resolution-3D-Shapes-Modeling"><a href="#Sparc3D-Sparse-Representation-and-Construction-for-High-Resolution-3D-Shapes-Modeling" class="headerlink" title="Sparc3D: Sparse Representation and Construction for High-Resolution 3D   Shapes Modeling"></a>Sparc3D: Sparse Representation and Construction for High-Resolution 3D   Shapes Modeling</h2><p><strong>Authors:Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen</strong></p>
<p>High-fidelity 3D object synthesis remains significantly more challenging than 2D image generation due to the unstructured nature of mesh data and the cubic complexity of dense volumetric grids. Existing two-stage pipelines-compressing meshes with a VAE (using either 2D or 3D supervision), followed by latent diffusion sampling-often suffer from severe detail loss caused by inefficient representations and modality mismatches introduced in VAE. We introduce Sparc3D, a unified framework that combines a sparse deformable marching cubes representation Sparcubes with a novel encoder Sparconv-VAE. Sparcubes converts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by scattering signed distance and deformation fields onto a sparse cube, allowing differentiable optimization. Sparconv-VAE is the first modality-consistent variational autoencoder built entirely upon sparse convolutional networks, enabling efficient and near-lossless 3D reconstruction suitable for high-resolution generative modeling through latent diffusion. Sparc3D achieves state-of-the-art reconstruction fidelity on challenging inputs, including open surfaces, disconnected components, and intricate geometry. It preserves fine-grained shape details, reduces training and inference cost, and integrates naturally with latent diffusion models for scalable, high-resolution 3D generation. </p>
<blockquote>
<p>高保真3D对象合成相较于2D图像生成仍然更具挑战性，这主要是由于网格数据的非结构化和密集体积网格的立方复杂性。现有的两阶段流程（使用VAE（使用二维或三维监督）压缩网格，然后进行潜在扩散采样）常常因VAE中引入的低效表示和模态不匹配而导致细节严重损失。我们引入了Sparc3D，这是一个结合了稀疏可变形行进立方体表示Sparcubes和新型编码器Sparconv-VAE的统一框架。Sparcubes通过将带符号距离和变形场散射到稀疏立方体上，将原始网格转换为高分辨率（1024^3）且具有任意拓扑的表面，从而实现可微分优化。Sparconv-VAE是第一个完全基于稀疏卷积网络的一致模态自动编码器，能够实现高效且接近无损的3D重建，适合通过潜在扩散进行高分辨率生成建模。Sparc3D在具有挑战性的输入上实现了最先进的重建保真度，包括开放表面、断开组件和精细几何。它保留了精细的形状细节，降低了训练和推理成本，并与潜在扩散模型自然集成，可实现可扩展的高分辨率3D生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14521v3">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://lizhihao6.github.io/Sparc3D">https://lizhihao6.github.io/Sparc3D</a></p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了针对三维物体合成的新技术挑战及解决方案。由于网格数据的非结构化和密集体积网格的复杂性，传统的两阶段流水线经常遭受损失细节的困扰。作者提出名为SParc3D的统一框架，通过引入稀疏可变形立体推进模块（Sparcubes）和新型编码器（Sparconv-VAE），实现了高效且无损的三维重建，适用于通过潜在扩散模型进行高分辨率生成建模。此技术在多种复杂输入上达到前所未有的重建保真度，并可自然集成到潜在扩散模型中，实现可扩展的高分辨率三维生成。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键要点总结：</p>
<ul>
<li>高保真三维物体合成比二维图像生成更具挑战性，主要由于网格数据的非结构化和密集体积网格的复杂性。</li>
<li>传统两阶段流水线方法常常由于无效的代表和模态不匹配导致的细节损失。</li>
<li>SParc3D框架结合了稀疏可变形立体推进模块（Sparcubes）和新型编码器（Sparconv-VAE）。</li>
<li>Sparcubes可将原始网格转化为高分辨率表面，并支持任意拓扑结构。</li>
<li>Sparconv-VAE是首个完全基于稀疏卷积网络的一致模态自动编码器，可实现高效且无损的三维重建。</li>
<li>SParc3D在复杂输入上实现了高保真重建，包括开放表面、断开组件和精细几何结构。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14521">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ed242398226713dbea91aa85a12c1806.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a38a1caa3bd9294fd6e88711b25e82e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67ab921954ad207b3077ab9d8fd05efc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Free-Graph-Generation-with-Next-Scale-Prediction"><a href="#Diffusion-Free-Graph-Generation-with-Next-Scale-Prediction" class="headerlink" title="Diffusion-Free Graph Generation with Next-Scale Prediction"></a>Diffusion-Free Graph Generation with Next-Scale Prediction</h2><p><strong>Authors:Samuel Belkadi, Steve Hong, Marian Chen, Miruna Cretu, Charles Harris, Pietro Lio</strong></p>
<p>Autoregressive models excel in efficiency and plug directly into the transformer ecosystem, delivering robust generalization, predictable scalability, and seamless workflows such as fine-tuning and parallelized training. However, they require an explicit sequence order, which contradicts the unordered nature of graphs. In contrast, diffusion models maintain permutation invariance and enable one-shot generation but require up to thousands of denoising steps and additional features for expressivity, leading to high computational costs. Inspired by recent breakthroughs in image generation, especially the success of visual autoregressive methods, we propose MAG, a novel diffusion-free graph generation framework based on next-scale prediction. By leveraging a hierarchy of latent representations, the model progressively generates scales of the entire graph without the need for explicit node ordering. Experiments on both generic and molecular graph datasets demonstrated the potential of this method, achieving inference speedups of up to three orders of magnitude over state-of-the-art methods, while preserving high-quality generation. </p>
<blockquote>
<p>自回归模型在效率方面表现出色，并且能够直接插入transformer生态系统，提供稳健的泛化能力、可预测的扩展性以及微调和平行训练等无缝工作流程。然而，它们需要明确的序列顺序，这与图的无序性质相矛盾。相比之下，扩散模型保持置换不变性，能够实现一次生成，但需要高达数千步的去噪步骤和额外的特征来表达，导致计算成本高昂。受最近图像生成领域突破，尤其是视觉自回归方法成功的启发，我们提出了MAG，这是一种基于下一尺度预测的新型无扩散图生成框架。通过利用潜在表示层次结构，该模型无需显式节点排序即可逐步生成整个图的尺度。在通用和图分子数据集上的实验证明了该方法的潜力，与最新方法相比，实现了高达三个数量级的推理速度提升，同时保持高质量生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23612v2">PDF</a> Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>文本介绍了一种新型的无扩散图生成框架MAG，它通过利用分层潜在表示来渐进生成整个图，无需明确的节点顺序。MAG结合了自回归模型的优点，如效率、通用性、可预测的可扩展性和无缝工作流程，同时解决了自回归模型对序列顺序的依赖问题。在通用和图结构分子数据集上的实验表明，MAG实现了高达三个数量级的推理速度提升，同时保持了高质量生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自回归模型具有效率、通用性、可预测的可扩展性和无缝工作流程等优点，但需要明确的序列顺序。</li>
<li>扩散模型具有排列不变性和一次生成能力，但计算成本高，需要数千个去噪步骤和额外的特征来表达性。</li>
<li>新型的无扩散图生成框架MAG结合了自回归模型的优点，通过利用分层潜在表示来生成图，解决了自回归模型对节点顺序的依赖问题。</li>
<li>MAG实现了高达三个数量级的推理速度提升，同时保持了高质量生成。</li>
<li>MAG框架无需明确的节点顺序即可渐进生成整个图的尺度。</li>
<li>在不同数据集上的实验表明，MAG具有良好的通用性和潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3a4ed2104df713d5451b51e057adeb08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c434e24f5d030838ad5b049e17c06b1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8afafdf73a0ffd978890ad964295592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1968f996cf4c625eab0fe49d2cf2759.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0affbb029ccc48f2e2c90a4796c06312.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models"><a href="#Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models" class="headerlink" title="Training-Free Safe Denoisers for Safe Use of Diffusion Models"></a>Training-Free Safe Denoisers for Safe Use of Diffusion Models</h2><p><strong>Authors:Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mijung Park</strong></p>
<p>There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely. </p>
<blockquote>
<p>关于强大的扩散模型（DMs）的安全问题日益受到关注，因为它们经常被误用于产生不适当、不安全的工作（NSFW）内容或生成版权材料或那些希望被遗忘的个人的数据。许多现有方法通过依赖基于文本的反向提示或重新训练DMs以消除某些特征或样本来解决这些问题。在本文中，我们采用了完全不同的方法，通过利用否定集（例如，不安全的图像、版权数据或需要排除的数据点）直接修改采样轨迹，避免数据分布的特定区域，而无需重新训练或微调DMs。我们正式推导了安全和不安全预期去噪样本之间的关系，从而形成了我们的安全去噪器，确保最终样本远离要否定的区域。受推导的启发，我们开发了一种实用算法，该算法在文本条件、类别条件和无条件图像生成场景中成功产生了高质量样本，同时避免了数据分布的否定区域。这些结果暗示了我们的无训练安全去噪器在更安全地使用DM方面的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08011v3">PDF</a> Preprint</p>
<p><strong>摘要</strong><br>扩散模型经常被滥用，产生不适当、不安全的（NSFW）内容或者生成版权材料和个人遗忘的数据。许多现有方法主要依赖文本负向提示或重新训练模型来解决这些问题。本研究采用完全不同的方法，通过利用否定集避免数据分布特定区域，无需重新训练或微调扩散模型，直接修改采样轨迹。我们正式推导安全和非安全去噪样本之间的关系，从而得到我们的安全去噪器，确保最终样本远离否定区域。受到推导的启发，我们开发了一种实用算法，成功生成高质量样本，同时在文本条件、类别条件和无条件图像生成场景中避免否定区域。这些结果暗示了我们的无训练安全去噪器在更安全地使用扩散模型方面的巨大潜力。</p>
<p><strong>要点摘要</strong></p>
<ol>
<li>扩散模型经常被误用，产生不适当或不安全内容以及版权或个人隐私相关材料。</li>
<li>传统方法主要依赖文本负向提示或重新训练模型来解决问题。</li>
<li>本研究通过利用否定集直接修改采样轨迹，无需重新训练模型。</li>
<li>正式推导安全和非安全去噪样本间的关系，形成安全去噪器概念。</li>
<li>安全去噪器能确保生成的样本远离否定区域。</li>
<li>开发实用算法可在不同场景（如文本条件、类别条件和无条件图像生成）成功生成高质量样本并避免否定区域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08011">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-881de76d8c7f1592bd0b30243d00adf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87e98c6fa2d3208afa40a593c2ee32cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc0cb3dd7b6b60b8330b57b077145259.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Factorized-Video-Autoencoders-for-Efficient-Generative-Modelling"><a href="#Factorized-Video-Autoencoders-for-Efficient-Generative-Modelling" class="headerlink" title="Factorized Video Autoencoders for Efficient Generative Modelling"></a>Factorized Video Autoencoders for Efficient Generative Modelling</h2><p><strong>Authors:Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia</strong></p>
<p>Latent variable generative models have emerged as powerful tools for generative tasks including image and video synthesis. These models are enabled by pretrained autoencoders that map high resolution data into a compressed lower dimensional latent space, where the generative models can subsequently be developed while requiring fewer computational resources. Despite their effectiveness, the direct application of latent variable models to higher dimensional domains such as videos continues to pose challenges for efficient training and inference. In this paper, we propose an autoencoder that projects volumetric data onto a four-plane factorized latent space that grows sublinearly with the input size, making it ideal for higher dimensional data like videos. The design of our factorized model supports straightforward adoption in a number of conditional generation tasks with latent diffusion models (LDMs), such as class-conditional generation, frame prediction, and video interpolation. Our results show that the proposed four-plane latent space retains a rich representation needed for high-fidelity reconstructions despite the heavy compression, while simultaneously enabling LDMs to operate with significant improvements in speed and memory. </p>
<blockquote>
<p>潜在变量生成模型作为生成任务（包括图像和视频合成）的强大工具已经出现。这些模型由预训练的自动编码器提供支持，将高分辨率数据映射到压缩的低维潜在空间，在此空间中，可以在较少的计算资源下开发生成模型。尽管这些模型非常有效，但直接将潜在变量模型应用于更高维度领域（如视频）仍然对高效训练和推理提出了挑战。在本文中，我们提出了一种将体积数据投影到随输入大小增长而亚线性增长的四平面分解潜在空间的自动编码器，使其成为适合处理高维数据（如视频）的理想选择。我们的分解模型设计支持在具有潜在扩散模型的多种条件生成任务中直接使用，例如类别条件生成、帧预测和视频插值等。结果表明，所提出的四平面潜在空间在保留重建所需丰富表示的同时实现了强烈压缩，同时使潜在扩散模型能够以速度和内存上的显著改善运行。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04452v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了潜在变量生成模型在图像和视频合成等生成任务中的强大作用。通过预训练的自动编码器将高分辨率数据映射到压缩的低维潜在空间，生成模型可以在较少的计算资源下开发。针对直接在视频等更高维度领域应用潜在变量模型面临的挑战，本文提出了一种将体积数据投影到四平面分解潜在空间的自动编码器，其随输入大小的增长呈亚线性，适用于视频等更高维度数据。该分解模型设计支持在带有潜在扩散模型（LDMs）的多个条件生成任务中的直接应用，如类别条件生成、帧预测和视频插值。结果表明，所提出的四平面潜在空间在重度压缩的同时保留了丰富的表示，同时使LDMs在速度和内存方面实现了显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>潜在变量生成模型已成为图像和视频合成等生成任务的强大工具。</li>
<li>预训练的自动编码器将高分辨率数据映射到低维潜在空间，减少计算资源需求。</li>
<li>将体积数据投影到四平面分解潜在空间的方法适用于更高维度的数据，如视频。</li>
<li>四平面分解模型设计支持在条件生成任务中的直接应用，如类别条件生成、帧预测和视频插值。</li>
<li>四平面潜在空间在压缩的同时保留了丰富的表示，可实现高保真重构。</li>
<li>该方法显著提高了潜在扩散模型（LDMs）在速度和内存方面的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04452">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-afb3e0f9e81b089497a900201e0aa3c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b33f3785bff1473a1575453ad2f5344e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74385ab96342973cc77348707169580d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dd482871d7629b31eeccafe9f985a0b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Plug-and-Play-image-restoration-with-Stochastic-deNOising-REgularization"><a href="#Plug-and-Play-image-restoration-with-Stochastic-deNOising-REgularization" class="headerlink" title="Plug-and-Play image restoration with Stochastic deNOising REgularization"></a>Plug-and-Play image restoration with Stochastic deNOising REgularization</h2><p><strong>Authors:Marien Renaud, Jean Prost, Arthur Leclaire, Nicolas Papadakis</strong></p>
<p>Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, both quantitatively and qualitatively. </p>
<blockquote>
<p>Plug-and-Play（PnP）算法是一类迭代算法，通过结合物理模型和深度神经网络进行正则化，解决图像反问题。尽管它们能产生令人印象深刻的图像恢复结果，但这些算法依赖于在非标准去噪器上对图像的使用，随着迭代的进行，图像越来越不噪声化，这与基于扩散模型（DM）的近期算法形成对比，其中去噪器只应用于重新加噪声的图像。我们提出了一种新的PnP框架，称为随机去噪正则化（SNORE），它仅在具有适当水平的噪声图像上应用去噪器。它基于显式随机正则化，导致一种解决不适定反问题的随机梯度下降算法。我们提供了该算法及其退火扩展的收敛性分析。实验证明，SNORE在去模糊和修复任务上与最先进的方法相比具有竞争力和优势，无论是在数量上还是在质量上都是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.01779v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在图像逆问题处理中，Plug-and-Play（PnP）算法结合了物理模型和深度神经网络进行正则化。针对PnP算法在迭代过程中对不同噪声水平图像的非标准去噪使用方式，我们提出了一种新的PnP框架——Stochastic deNOising REgularization（SNORE）。SNORE只在适当噪声水平的图像上应用去噪器，基于明确的随机正则化，为解决不适定的逆问题提供了随机梯度下降算法。实验证明，SNORE在去模糊和补全任务上均具有良好的竞争性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PnP算法结合物理模型和深度神经网络进行图像逆问题的处理。</li>
<li>PnP算法在迭代过程中对噪声逐渐减弱的图像使用去噪器的方式与基于Diffusion Models的算法不同。</li>
<li>提出了新的PnP框架SNORE，只在适当噪声水平的图像上应用去噪器。</li>
<li>SNORE基于明确的随机正则化，为解决不适定的逆问题提供了随机梯度下降算法。</li>
<li>SNORE在收敛性方面进行了分析，并提供了退火扩展的方法。</li>
<li>实验证明，SNORE在去模糊和补全任务上具有良好的性能，与现有方法相比具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.01779">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d89da227791bc3737d3c052bf2b9e22.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-22c307ff2862f361125a11c7c90838b0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-14  SpectralAR Spectral Autoregressive Visual Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5df2a46b4102e410cb6aed2c254111aa.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-06-14  PointGS Point Attention-Aware Sparse View Synthesis with Gaussian   Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
