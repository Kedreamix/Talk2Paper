<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  SceneCompleter Dense 3D Scene Completion for Generative Novel View   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-afb3e0f9e81b089497a900201e0aa3c3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-14-æ›´æ–°"><a href="#2025-06-14-æ›´æ–°" class="headerlink" title="2025-06-14 æ›´æ–°"></a>2025-06-14 æ›´æ–°</h1><h2 id="SceneCompleter-Dense-3D-Scene-Completion-for-Generative-Novel-View-Synthesis"><a href="#SceneCompleter-Dense-3D-Scene-Completion-for-Generative-Novel-View-Synthesis" class="headerlink" title="SceneCompleter: Dense 3D Scene Completion for Generative Novel View   Synthesis"></a>SceneCompleter: Dense 3D Scene Completion for Generative Novel View   Synthesis</h2><p><strong>Authors:Weiliang Chen, Jiayi Bi, Yuanhui Huang, Wenzhao Zheng, Yueqi Duan</strong></p>
<p>Generative models have gained significant attention in novel view synthesis (NVS) by alleviating the reliance on dense multi-view captures. However, existing methods typically fall into a conventional paradigm, where generative models first complete missing areas in 2D, followed by 3D recovery techniques to reconstruct the scene, which often results in overly smooth surfaces and distorted geometry, as generative models struggle to infer 3D structure solely from RGB data. In this paper, we propose SceneCompleter, a novel framework that achieves 3D-consistent generative novel view synthesis through dense 3D scene completion. SceneCompleter achieves both visual coherence and 3D-consistent generative scene completion through two key components: (1) a geometry-appearance dual-stream diffusion model that jointly synthesizes novel views in RGBD space; (2) a scene embedder that encodes a more holistic scene understanding from the reference image. By effectively fusing structural and textural information, our method demonstrates superior coherence and plausibility in generative novel view synthesis across diverse datasets. Project Page: <a target="_blank" rel="noopener" href="https://chen-wl20.github.io/SceneCompleter">https://chen-wl20.github.io/SceneCompleter</a> </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹é€šè¿‡å‡å°‘å¯¹å¯†é›†å¤šè§†è§’æ•æ‰çš„ä¾èµ–ï¼Œåœ¨æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰ä¸­è·å¾—äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éµå¾ªä¸€ç§ä¼ ç»ŸèŒƒå¼ï¼Œå³ç”Ÿæˆæ¨¡å‹é¦–å…ˆåœ¨2Dä¸­å®Œæˆç¼ºå¤±åŒºåŸŸï¼Œç„¶åé‡‡ç”¨3Dæ¢å¤æŠ€æœ¯è¿›è¡Œåœºæ™¯é‡å»ºï¼Œè¿™å¾€å¾€å¯¼è‡´è¡¨é¢è¿‡äºå¹³æ»‘å’Œå‡ ä½•å¤±çœŸï¼Œå› ä¸ºç”Ÿæˆæ¨¡å‹å¾ˆéš¾ä»…ä»RGBæ•°æ®ä¸­æ¨æ–­å‡º3Dç»“æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SceneCompleterï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¯†é›†3Dåœºæ™¯å®Œæˆå®ç°3Dä¸€è‡´ç”Ÿæˆæ–°å‹è§†å›¾åˆæˆçš„æ–°æ¡†æ¶ã€‚SceneCompleteré€šè¿‡ä¸¤ä¸ªå…³é”®ç»„ä»¶å®ç°äº†è§†è§‰è¿è´¯æ€§å’Œ3Dä¸€è‡´ç”Ÿæˆåœºæ™¯å®Œæˆï¼šï¼ˆ1ï¼‰å‡ ä½•å¤–è§‚åŒæµæ‰©æ•£æ¨¡å‹ï¼Œåœ¨RGBDç©ºé—´ä¸­è”åˆåˆæˆæ–°å‹è§†å›¾ï¼›ï¼ˆ2ï¼‰åœºæ™¯åµŒå…¥å™¨ï¼Œä»å‚è€ƒå›¾åƒä¸­ç¼–ç æ›´æ•´ä½“çš„åœºæ™¯ç†è§£ã€‚é€šè¿‡æœ‰æ•ˆåœ°èåˆç»“æ„å’Œçº¹ç†ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„ç”Ÿæˆæ–°å‹è§†å›¾åˆæˆä¸­è¡¨ç°å‡ºå“è¶Šçš„ä¸€è‡´æ€§å’Œåˆç†æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://chen-wl20.github.io/SceneCompleter">https://chen-wl20.github.io/SceneCompleter</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10981v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSceneCompleterçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å¯†é›†çš„3Dåœºæ™¯è¡¥å…¨å®ç°3Dä¸€è‡´æ€§çš„ç”Ÿæˆå¼æ–°å‹è§†å›¾åˆæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå‡ ä½•å’Œçº¹ç†ä¿¡æ¯ï¼Œå®ç°äº†è§†è§‰è¿è´¯æ€§å’Œ3Dä¸€è‡´æ€§ï¼Œæé«˜äº†ç”Ÿæˆå¼æ–°å‹è§†å›¾åˆæˆçš„è¿è´¯æ€§å’Œå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰ä¸­å—åˆ°å…³æ³¨ï¼Œèƒ½å¤Ÿå‡å°‘å¯¹å¯†é›†å¤šè§†å›¾æ•æ‰çš„ä¾èµ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸éµå¾ªå…ˆ2Dè¡¥å…¨å†3Dæ¢å¤çš„å¸¸è§„æ¨¡å¼ï¼Œä½†ä¼šå¯¼è‡´è¡¨é¢è¿‡äºå¹³æ»‘å’Œå‡ ä½•å¤±çœŸã€‚</li>
<li>SceneCompleteræ¡†æ¶é€šè¿‡å¯†é›†çš„3Dåœºæ™¯è¡¥å…¨å®ç°3Dä¸€è‡´æ€§ç”Ÿæˆæ–°å‹è§†å›¾åˆæˆã€‚</li>
<li>SceneCompleteråŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå‡ ä½•å¤–è§‚åŒæµæ‰©æ•£æ¨¡å‹å’Œåœºæ™¯åµŒå…¥å™¨ã€‚</li>
<li>å‡ ä½•å¤–è§‚åŒæµæ‰©æ•£æ¨¡å‹åœ¨RGBDç©ºé—´ä¸­è”åˆåˆæˆæ–°å‹è§†å›¾ã€‚</li>
<li>åœºæ™¯åµŒå…¥å™¨ä»å‚è€ƒå›¾åƒç¼–ç æ›´æ•´ä½“çš„åœºæ™¯ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d202288523a503c67dd2f6a0917e4cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59325d4d64469dc0edcae4790398b3dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76f2232bfa2a31f00c46f45b4d47e794.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning"><a href="#MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning" class="headerlink" title="MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning"></a>MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning</h2><p><strong>Authors:Yuxuan Luo, Yuhui Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian</strong></p>
<p>In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learningâ€“a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target imageâ€™s core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficitsâ€“low entity fidelity, weak relations, and clutterâ€“with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmarkâ€™s difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°çŸ¥è¯†å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä»¥åŠå¤§è§„æ¨¡å¤šå­¦ç§‘åˆ†å±‚çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰ï¼Œä»¥æ£€æµ‹å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œäººç±»å­¦ä¹ æœºåˆ¶ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œè¿™ä¸€äº‹å®ç”±åŒé‡ç¼–ç ç†è®ºå’Œå›¾åƒä¼˜åŠ¿æ•ˆåº”å¼ºè°ƒã€‚ç”Ÿæˆè¿™æ ·çš„å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦å¤šæ¨¡æ€æ¨ç†ï¼Œå°†ä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§åœ°é¢èåˆæˆæ¸…æ™°çš„è§£é‡Šæ€§è§†è§‰ã€‚ä¸ºäº†è¿›è¡Œå…¨é¢çš„è¯„ä¼°ï¼ŒMMMGæä¾›äº†4456å¯¹ä¸“å®¶éªŒè¯ï¼ˆçŸ¥è¯†ï¼‰å›¾åƒæç¤ºï¼Œæ¶µç›–10ä¸ªå­¦ç§‘ï¼Œ6ä¸ªæ•™è‚²æ°´å¹³ï¼Œä»¥åŠå›¾è¡¨ã€å›¾è¡¨å’Œæ€ç»´å¯¼å›¾ç­‰å¤šæ ·åŒ–çš„çŸ¥è¯†å½¢å¼ã€‚ä¸ºäº†æ¶ˆé™¤è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ··æ·†å¤æ‚æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºæ³•ã€‚æ¯ä¸ªKGæ˜ç¡®ç•Œå®šäº†ç›®æ ‡å›¾åƒçš„æ ¸å¿ƒå®ä½“åŠå…¶ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†MMMGè¯„åˆ†æ¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒã€‚è¯¥æŒ‡æ ‡ç»“åˆäº†äº‹å®å‡†ç¡®æ€§ï¼ˆé€šè¿‡çŸ¥è¯†å›¾è°±ä¹‹é—´çš„å›¾ç¼–è¾‘è·ç¦»æ¥è¡¡é‡ï¼‰å’Œè§†è§‰æ¸…æ™°åº¦è¯„ä¼°ã€‚å¯¹16ä¸ªæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°æ­ç¤ºäº†ä¸¥é‡çš„æ¨ç†ç¼ºé™·â€”â€”å®ä½“ä¿çœŸåº¦ä½ã€å…³ç³»å¼±å’Œæ‚ä¹±â€”â€”GPT-4oçš„MMMGè¯„åˆ†ä»…ä¸º50.20ï¼Œå‡¸æ˜¾äº†åŸºå‡†æµ‹è¯•çš„å›°éš¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†FLUX-Reasonï¼ˆMMMGè¯„åˆ†ä¸º34.45ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¼€æ”¾åŸºçº¿ï¼Œå®ƒç»“åˆäº†æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨16000ä¸ªç²¾é€‰çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10963v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°çŸ¥è¯†å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¹¶ä¼´éšç€å¤§è§„æ¨¡å¤šå­¦ç§‘å¤šå±‚çº§çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†ï¼ˆMMMGï¼‰çš„å‡ºç°ï¼Œä»¥æ£€æµ‹å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œäººç±»å­¦ä¹ æœºåˆ¶ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œç”Ÿæˆè¿™ç±»å›¾åƒéœ€è¦å¤šæ¨¡æ€æ¨ç†ï¼Œèåˆä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§åœ°é¢å®å†µï¼Œå½¢æˆæ¸…æ™°çš„è§£é‡Šæ€§è§†è§‰ã€‚MMMGæä¾›äº†4456å¼ ä¸“å®¶éªŒè¯çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œæ¶µç›–10ä¸ªå­¦ç§‘ã€6ä¸ªæ•™è‚²çº§åˆ«å’Œå¤šç§çŸ¥è¯†å½¢å¼ã€‚ä¸ºå…¨é¢è¯„ä¼°ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºï¼Œæ¶ˆé™¤è¯„ä¼°ä¸­çš„æ··æ·†å¤æ‚æ€§ã€‚å¼•å…¥MMMGè¯„åˆ†æ¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒï¼Œç»“åˆæ ¸å¿ƒå®ä½“åŠå…¶ä¾èµ–æ€§çš„å›¾è°±ç¼–è¾‘è·ç¦»ä¸è§†è§‰æ¸…æ™°åº¦è¯„ä¼°ã€‚å¯¹16æ¬¾æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºå­˜åœ¨ä¸¥é‡çš„æ¨ç†ç¼ºé™·ï¼ŒGPT-4oåœ¨MMMGè¯„åˆ†ä¸­ä»…å¾—50.20ï¼Œçªæ˜¾äº†åŸºå‡†çš„éš¾åº¦ã€‚ä¸ºä¿ƒè¿›è¡Œä¸šè¿›æ­¥ï¼Œå‘å¸ƒäº†ä¸€ä¸ªæœ‰æ•ˆçš„å¼€æ”¾åŸºçº¿FLUX-Reasonï¼Œå…¶MMMGè¯„åˆ†ä¸º34.45ï¼Œç»“åˆäº†æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨16000ä¸ªç²¾é€‰çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†çŸ¥è¯†å›¾åƒç”Ÿæˆä½œä¸ºæ–°ä»»åŠ¡ï¼Œä»¥åŠå¤§è§„æ¨¡å¤šå­¦ç§‘å¤šå±‚çº§çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†ï¼ˆMMMGï¼‰ä»¥æµ‹è¯•å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>çŸ¥è¯†å›¾åƒå¯¹äºäººç±»æ–‡æ˜ä¸å­¦ä¹ çš„æ ¸å¿ƒé‡è¦æ€§è¢«å¼ºè°ƒã€‚</li>
<li>ç”ŸæˆçŸ¥è¯†å›¾åƒéœ€è¦å¤šæ¨¡æ€æ¨ç†ï¼Œèåˆä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§è§†è§‰è¡¨è¾¾ã€‚</li>
<li>MMMGåŒ…å«ä¸“å®¶éªŒè¯çš„4456ä¸ªçŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œæ¶µç›–å¤šä¸ªå­¦ç§‘ã€æ•™è‚²çº§åˆ«å’ŒçŸ¥è¯†å½¢å¼ã€‚</li>
<li>ç»Ÿä¸€é‡‡ç”¨çŸ¥è¯†å›¾è°±è¡¨ç¤ºä»¥è¿›è¡Œç»¼åˆè¯„ä»·ï¼Œå¹¶æ¶ˆé™¤å¤æ‚æ€§æ··æ·†ã€‚</li>
<li>å¼•å…¥MMMGè¯„åˆ†è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬äº‹å®å‡†ç¡®æ€§å’Œè§†è§‰æ¸…æ™°åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa6208a0529fb712dc324be70c96a457.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-759b43668dcc73b54c3573d07f8cd979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61f5cc35af2995dc9d941893ca19df72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a377a146a57f5f4f3673b420023513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6417f68225408b87a977c020f398d7f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f2970b49a2b8102ee26dc5923c90338.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SpectralAR-Spectral-Autoregressive-Visual-Generation"><a href="#SpectralAR-Spectral-Autoregressive-Visual-Generation" class="headerlink" title="SpectralAR: Spectral Autoregressive Visual Generation"></a>SpectralAR: Spectral Autoregressive Visual Generation</h2><p><strong>Authors:Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Yueqi Duan, Jie Zhou, Jiwen Lu</strong></p>
<p>Autoregressive visual generation has garnered increasing attention due to its scalability and compatibility with other modalities compared with diffusion models. Most existing methods construct visual sequences as spatial patches for autoregressive generation. However, image patches are inherently parallel, contradicting the causal nature of autoregressive modeling. To address this, we propose a Spectral AutoRegressive (SpectralAR) visual generation framework, which realizes causality for visual sequences from the spectral perspective. Specifically, we first transform an image into ordered spectral tokens with Nested Spectral Tokenization, representing lower to higher frequency components. We then perform autoregressive generation in a coarse-to-fine manner with the sequences of spectral tokens. By considering different levels of detail in images, our SpectralAR achieves both sequence causality and token efficiency without bells and whistles. We conduct extensive experiments on ImageNet-1K for image reconstruction and autoregressive generation, and SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project page: <a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a>. </p>
<blockquote>
<p>è‡ªå›å½’è§†è§‰ç”Ÿæˆç”±äºå…¶å¯æ‰©å±•æ€§ä»¥åŠä¸æ‰©æ•£æ¨¡å‹ç›¸æ¯”ä¸å…¶ä»–æ¨¡æ€çš„å…¼å®¹æ€§è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é€šè¿‡æ„å»ºç©ºé—´è¡¥ä¸æ¥è¿›è¡Œè‡ªå›å½’ç”Ÿæˆè§†è§‰åºåˆ—ã€‚ç„¶è€Œï¼Œå›¾åƒè¡¥ä¸æœ¬è´¨ä¸Šæ˜¯å¹¶è¡Œçš„ï¼Œè¿™ä¸è‡ªå›å½’æ¨¡å‹çš„å› æœæ€§è´¨ç›¸çŸ›ç›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è°±è‡ªå›å½’ï¼ˆSpectralARï¼‰è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œä»è°±çš„è§’åº¦å®ç°äº†è§†è§‰åºåˆ—çš„å› æœæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨åµŒå¥—è°±ä»¤ç‰ŒåŒ–å°†å›¾åƒè½¬æ¢ä¸ºæœ‰åºçš„è°±ä»¤ç‰Œï¼Œä»£è¡¨ä»ä½åˆ°é«˜çš„é¢‘ç‡åˆ†é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨è°±ä»¤ç‰Œåºåˆ—ä¸Šä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼æ‰§è¡Œè‡ªå›å½’ç”Ÿæˆã€‚é€šè¿‡è€ƒè™‘å›¾åƒçš„ä¸åŒå±‚æ¬¡ç»†èŠ‚ï¼Œæˆ‘ä»¬çš„SpectralARåœ¨ä¸å¢åŠ é¢å¤–å¤æ‚æ€§çš„æƒ…å†µä¸‹å®ç°äº†åºåˆ—çš„å› æœæ€§å’Œä»¤ç‰Œæ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ImageNet-1Kä¸Šè¿›è¡Œäº†å¤§é‡çš„å›¾åƒé‡å»ºå’Œè‡ªå›å½’ç”Ÿæˆå®éªŒï¼ŒSpectralARä»…ä½¿ç”¨64ä¸ªä»¤ç‰Œå’Œ3.1äº¿ä¸ªå‚æ•°å°±å®ç°äº†3.02çš„gFIDã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10962v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a></p>
<p><strong>Summary</strong><br>     è°±è‡ªå›å½’ï¼ˆSpectralARï¼‰è§†è§‰ç”Ÿæˆæ¡†æ¶é‡‡ç”¨è°±è§†è§’å®ç°è§†è§‰åºåˆ—çš„å› æœæ€§ï¼Œé€šè¿‡åµŒå¥—è°±ä»¤ç‰ŒåŒ–å°†å›¾åƒè½¬åŒ–ä¸ºæœ‰åºè°±ä»¤ç‰Œï¼Œå®ç°ç²—åˆ°ç»†çš„è‡ªåŠ¨å›å½’ç”Ÿæˆã€‚è¯¥æ–¹æ³•è€ƒè™‘å›¾åƒä¸åŒå±‚æ¬¡çš„ç»†èŠ‚ï¼Œå®ç°åºåˆ—å› æœæ€§å’Œä»¤ç‰Œæ•ˆç‡ã€‚åœ¨ImageNet-1Kä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpectralARåœ¨å›¾åƒé‡å»ºå’Œè‡ªåŠ¨å›å½’ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’è§†è§‰ç”Ÿæˆå—åˆ°å…³æ³¨ï¼Œå› å…¶å¯æ‰©å±•æ€§ä¸å…¶ä»–æ¨¡æ€çš„å…¼å®¹æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ„å»ºè§†è§‰åºåˆ—ä½œä¸ºç©ºé—´è¡¥ä¸è¿›è¡Œè‡ªå›å½’ç”Ÿæˆï¼Œä½†å›¾åƒè¡¥ä¸æœ¬è´¨ä¸Šæ˜¯å¹¶è¡Œçš„ï¼Œä¸è‡ªå›å½’æ¨¡å‹çš„å› æœæ€§è´¨ç›¸çŸ›ç›¾ã€‚</li>
<li>è°±è‡ªå›å½’ï¼ˆSpectralARï¼‰æ¡†æ¶ä»è°±è§’åº¦å®ç°è§†è§‰åºåˆ—çš„å› æœæ€§ã€‚</li>
<li>SpectralARä½¿ç”¨åµŒå¥—è°±ä»¤ç‰ŒåŒ–å°†å›¾åƒè½¬åŒ–ä¸ºæœ‰åºè°±ä»¤ç‰Œã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°ç²—åˆ°ç»†çš„è‡ªåŠ¨å›å½’ç”Ÿæˆï¼Œè€ƒè™‘å›¾åƒä¸åŒå±‚æ¬¡çš„ç»†èŠ‚ã€‚</li>
<li>SpectralARåœ¨ImageNet-1Kä¸Šçš„å›¾åƒé‡å»ºå’Œè‡ªåŠ¨å›å½’ç”Ÿæˆå®éªŒè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°3.02 gFIDï¼Œä»…ä½¿ç”¨64ä¸ªä»¤ç‰Œå’Œ310Må‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f593045d6e96db8496e4cb7139f376a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d97c749368d00e22063b7b19885512.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-238744431ff7f422ed998ae3b941127e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2ccc14ec6ed220d153354326150eab7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d4ad6dff0330057862e861bb841dc6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26b98dd1561b37605d4b0184f1a9da91.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ME-Trigger-Element-Combination-Backdoor-Attack-on-Copyright-Infringement"><a href="#ME-Trigger-Element-Combination-Backdoor-Attack-on-Copyright-Infringement" class="headerlink" title="ME: Trigger Element Combination Backdoor Attack on Copyright   Infringement"></a>ME: Trigger Element Combination Backdoor Attack on Copyright   Infringement</h2><p><strong>Authors:Feiyu Yang, Siyuan Liang, Aishan Liu, Dacheng Tao</strong></p>
<p>The capability of generative diffusion models (DMs) like Stable Diffusion (SD) in replicating training data could be taken advantage of by attackers to launch the Copyright Infringement Attack, with duplicated poisoned image-text pairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew outstanding performance in attacking SD in text-to-image tasks. However, the feasible data resources in this area are still limited, some of them are even constrained or prohibited due to the issues like copyright ownership or inappropriate contents; And not all of the images in current datasets are suitable for the proposed attacking methods; Besides, the state-of-the-art (SoTA) performance of SBD is far from ideal when few generated poisoning samples could be adopted for attacks. In this paper, we raised new datasets accessible for researching in attacks like SBD, and proposed Multi-Element (ME) attack method based on SBD by increasing the number of poisonous visual-text elements per poisoned sample to enhance the ability of attacking, while importing Discrete Cosine Transform (DCT) for the poisoned samples to maintain the stealthiness. The Copyright Infringement Rate (CIR) &#x2F; First Attack Epoch (FAE) we got on the two new datasets were 16.78% &#x2F; 39.50 and 51.20% &#x2F; 23.60, respectively close to or even outperformed benchmark Pokemon and Mijourney datasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI and DCT earned CIR &#x2F; FAE of 0.23% &#x2F; 84.00 and 12.73% &#x2F; 65.50, both better than original SBD, which failed to attack at all. </p>
<blockquote>
<p>ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰åœ¨å¤åˆ¶è®­ç»ƒæ•°æ®æ–¹é¢çš„èƒ½åŠ›å¯èƒ½ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨æ¥å‘åŠ¨ç‰ˆæƒä¾µçŠ¯æ”»å‡»ï¼Œä½¿ç”¨é‡å¤çš„æ¯’åŒ–å›¾åƒæ–‡æœ¬å¯¹ã€‚æœ€è¿‘æå‡ºçš„SilentBadDiffusionï¼ˆSBDï¼‰æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸­å¯¹SDçš„æ”»å‡»è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™ä¸€é¢†åŸŸçš„å¯ç”¨æ•°æ®èµ„æºä»ç„¶æœ‰é™ï¼Œå…¶ä¸­ä¸€äº›å—åˆ°ç‰ˆæƒæˆ–å†…å®¹ä¸å½“ç­‰é—®é¢˜çš„é™åˆ¶æˆ–ç¦æ­¢ï¼›æ­¤å¤–ï¼Œå½“å‰æ•°æ®é›†å¹¶éæ‰€æœ‰å›¾åƒéƒ½é€‚åˆç”¨äºæå‡ºçš„æ”»å‡»æ–¹æ³•ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå½“é‡‡ç”¨è¾ƒå°‘çš„ç”Ÿæˆæ¯’åŒ–æ ·æœ¬è¿›è¡Œæ”»å‡»æ—¶ï¼ŒSBDçš„æœ€æ–°æ€§èƒ½è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¯ç”¨äºç ”ç©¶ç±»ä¼¼SBDæ”»å‡»çš„æ–°æ•°æ®é›†ï¼Œå¹¶åŸºäºSBDæå‡ºäº†Multi-Elementï¼ˆMEï¼‰æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡å¢åŠ æ¯ä¸ªæ¯’åŒ–æ ·æœ¬ä¸­çš„æœ‰æ¯’è§†è§‰æ–‡æœ¬å…ƒç´ æ•°é‡æ¥æå‡æ”»å‡»èƒ½åŠ›ï¼ŒåŒæ—¶å¼•å…¥ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ä»¥ä¿æŒéšè”½æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ–°æ•°æ®é›†ä¸Šçš„ç‰ˆæƒä¾µçŠ¯ç‡ï¼ˆCIRï¼‰&#x2F;é¦–æ¬¡æ”»å‡»å‘¨æœŸï¼ˆFAEï¼‰åˆ†åˆ«ä¸º16.78%&#x2F;39.50%å’Œ51.20%&#x2F;23.60%ï¼Œæ¥è¿‘ç”šè‡³è¶…è¿‡äº†åŸºå‡†æ•°æ®é›†Pokemonå’ŒMijourneyçš„è¡¨ç°ã€‚åœ¨ä½å­é‡‡æ ·æ¯”ç‡ï¼ˆ5%ï¼Œ6ä¸ªæ¯’åŒ–æ ·æœ¬ï¼‰çš„æ¡ä»¶ä¸‹ï¼ŒMESIå’ŒDCTè·å¾—äº†CIR&#x2F;FAEä¸º0.23%&#x2F;84.00%å’Œ12.73%&#x2F;65.50%ï¼Œå‡ä¼˜äºåŸå§‹SBDï¼Œåè€…æ ¹æœ¬æ— æ³•è¿›è¡Œæ”»å‡»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10776v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰åœ¨å¤åˆ¶è®­ç»ƒæ•°æ®æ–¹é¢çš„èƒ½åŠ›å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œé€šè¿‡å¤åˆ¶æœ‰æ¯’çš„å›¾åƒ-æ–‡æœ¬å¯¹æ¥å‘åŠ¨ç‰ˆæƒä¾µæƒæ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†Multi-Elementï¼ˆMEï¼‰æ”»å‡»æ–¹æ³•ï¼ŒåŸºäºSilentBadDiffusionï¼ˆSBDï¼‰ï¼Œé€šè¿‡å¢åŠ æ¯ä¸ªæœ‰æ¯’æ ·æœ¬ä¸­çš„æœ‰æ¯’è§†è§‰-æ–‡æœ¬å…ƒç´ æ•°é‡æ¥æé«˜æ”»å‡»èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ä¿æŒéšè”½æ€§ã€‚åœ¨æ–°æå‡ºçš„ä¸¤ä¸ªæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„ç‰ˆæƒä¾µæƒç‡ï¼ˆCIRï¼‰&#x2F;é¦–æ¬¡æ”»å‡»å‘¨æœŸï¼ˆFAEï¼‰æ¥è¿‘æˆ–ç”šè‡³è¶…è¿‡äº†åŸºå‡†æ•°æ®é›†ã€‚åœ¨è¾ƒä½çš„å­é‡‡æ ·æ¯”æ¡ä»¶ä¸‹ï¼Œç»“åˆMESIå’ŒDCTæŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ”»å‡»æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œæ˜æ˜¾ä¼˜äºåŸå§‹çš„SBDã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionçš„å¤åˆ¶è®­ç»ƒæ•°æ®èƒ½åŠ›å¯èƒ½è¢«æ”»å‡»è€…ç”¨äºè¿›è¡Œç‰ˆæƒä¾µæƒæ”»å‡»ã€‚</li>
<li>SilentBadDiffusionæ˜¯ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸­å¯¹Stable Diffusionå…·æœ‰å‡ºè‰²çš„æ”»å‡»æ€§èƒ½ã€‚</li>
<li>å½“å‰æ•°æ®é›†å­˜åœ¨é™åˆ¶ï¼Œéƒ¨åˆ†æ•°æ®é›†å› ç‰ˆæƒæˆ–å†…å®¹ä¸å½“è€Œå—åˆ°çº¦æŸæˆ–ç¦æ­¢ã€‚</li>
<li>å¹¶éå½“å‰æ•°æ®é›†ä¸­çš„æ‰€æœ‰å›¾åƒéƒ½é€‚åˆç”¨äºæå‡ºçš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†ï¼Œå¯ç”¨äºç ”ç©¶å¦‚SBDçš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>Multi-Elementï¼ˆMEï¼‰æ”»å‡»æ–¹æ³•åŸºäºSBDï¼Œé€šè¿‡å¢åŠ æ¯ä¸ªæœ‰æ¯’æ ·æœ¬ä¸­çš„æœ‰æ¯’è§†è§‰-æ–‡æœ¬å…ƒç´ æ•°é‡æ¥æé«˜æ”»å‡»èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-563f364d124ed1746cbc255f39e11b68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-feadf34e87e6e459cec0ee7791cc92b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63cf61f13e8aa66fe4088fd88ead366d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06f21a1a430ba2e698cec7703b9aabf2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models"><a href="#Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models" class="headerlink" title="Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent   Diffusion Models"></a>Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent   Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Ilias Stogiannidis, Junyu Yan, Alison Q. Oâ€™Neil, Sotirios A. Tsaftaris</strong></p>
<p>Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨è‡ªç„¶ï¼ˆRGBï¼‰å›¾åƒé¢†åŸŸï¼Œæœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œæ­¤ç±»æ¨¡å‹å¯ä»¥é€‚åº”å„ç§è§†è§‰è¯­è¨€ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¸”å‡ ä¹æ— éœ€ç›‘ç£ã€‚ç›¸åï¼Œåœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä»ç„¶ç›¸å¯¹æœªè¢«å……åˆ†ç ”ç©¶ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ•°æ®å¯ç”¨æ€§æœ‰é™ï¼ˆä¾‹å¦‚ï¼Œç”±äºéšç§æ‹…å¿§ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨èƒ¸éƒ¨Xå°„çº¿æ¨¡æ€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜æ ‡å‡†çš„æ–‡æœ¬æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿˜æ²¡æœ‰å­¦ä¼šå°†è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„ä¸´åºŠç›¸å…³ä¿¡æ¯ä¸ç»™å®šæ‰«æçš„ç›¸åº”åŒºåŸŸå¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¾®è°ƒæ¡†æ¶ï¼Œä»¥æ”¹è¿›é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å¤šæ¨¡æ€å¯¹é½ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆåœ°ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚çŸ­è¯­å®šä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†ï¼ˆMS-CXRï¼‰ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨éåˆ†å¸ƒæ•°æ®ï¼ˆVinDr-CXRï¼‰ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10633v1">PDF</a> 14 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„æˆåŠŸåº”ç”¨ï¼Œä»¥åŠå…¶åœ¨åŒ»å­¦æˆåƒé¢†åŸŸä¸­çš„ç›¸å¯¹æ¢ç´¢ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—å›¾åƒæ–‡æœ¬è½¬å›¾åƒæ–¹é¢çš„åº”ç”¨ã€‚æ–‡ç« é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿æ¨¡æ€æå‡ºäº†ä¸€ç§å¾®è°ƒæ¡†æ¶ï¼Œä»¥æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„å¤šæ¨¡æ€å¯¹é½èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆç”¨äºä¸‹æ¸¸ä»»åŠ¡å¦‚çŸ­è¯­å®šä½ã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†MS-CXRä¸Šè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨è¶…å‡ºåˆ†å¸ƒçš„æ•°æ®é›†VinDr-CXRä¸Šå±•ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„æ–‡æœ¬å¼•å¯¼å›¾åƒåˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œæ–‡æœ¬è½¬å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ç›¸å¯¹æ¢ç´¢ä¸è¶³ï¼Œä¸»è¦ç”±äºæ•°æ®å¯ç”¨æ€§çš„é™åˆ¶ã€‚</li>
<li>æ–‡ç« é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿æ¨¡æ€çš„æ¨¡å‹å¯¹é½é—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„å¤šæ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†MS-CXRä¸Šå®ç°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>æ¨¡å‹åœ¨è¶…å‡ºåˆ†å¸ƒçš„æ•°æ®é›†VinDr-CXRä¸Šå±•ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27b80e05982ab3ad87e589c3677a0e91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78406b024805e620118e7f4b5250b47b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a21d355d7538d5e2962e0af3cd2c021.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TexTailor-Customized-Text-aligned-Texturing-via-Effective-Resampling"><a href="#TexTailor-Customized-Text-aligned-Texturing-via-Effective-Resampling" class="headerlink" title="TexTailor: Customized Text-aligned Texturing via Effective Resampling"></a>TexTailor: Customized Text-aligned Texturing via Effective Resampling</h2><p><strong>Authors:Suin Lee, Dae-Shik Kim</strong></p>
<p>We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these approaches lead to a gradual shift in texture properties across viewpoints due to (1) insufficient integration of previously synthesized textures at each viewpoint during the diffusion process and (2) the autoregressive nature of the texture synthesis process. Moreover, the predefined selection of camera positions, which does not account for the objectâ€™s geometry, limits the effective use of texture information synthesized from different viewpoints, ultimately degrading overall texture consistency. In TexTailor, we address these issues by (1) applying a resampling scheme that repeatedly integrates information from previously synthesized textures within the diffusion process, and (2) fine-tuning a depth-aware diffusion model on these resampled textures. During this process, we observed that using only a few training images restricts the modelâ€™s original ability to generate high-fidelity images aligned with the conditioning, and therefore propose an performance preservation loss to mitigate this issue. Additionally, we improve the synthesis of view-consistent textures by adaptively adjusting camera positions based on the objectâ€™s geometry. Experiments on a subset of the Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures. The source code for TexTailor is available at <a target="_blank" rel="noopener" href="https://github.com/Adios42/Textailor">https://github.com/Adios42/Textailor</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†TexTailorï¼Œè¿™æ˜¯ä¸€ç§ä»æ–‡æœ¬æè¿°ç”Ÿæˆä¸€è‡´ç‰©ä½“çº¹ç†çš„æ–°æ–¹æ³•ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°çº¹ç†åˆæˆæ–¹æ³•åˆ©ç”¨æ·±åº¦æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹é€æ­¥ç”Ÿæˆå›¾åƒï¼Œå¹¶åœ¨é¢„å®šçš„å¤šä¸ªè§†è§’åˆæˆçº¹ç†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç”±äºåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ï¼ˆ1ï¼‰æ²¡æœ‰åœ¨æ¯ä¸ªè§†ç‚¹ä¸Šå……åˆ†æ•´åˆä¹‹å‰åˆæˆçš„çº¹ç†ä¿¡æ¯ï¼Œï¼ˆ2ï¼‰çº¹ç†åˆæˆè¿‡ç¨‹çš„è‡ªå›å½’æ€§è´¨ï¼Œå¯¼è‡´åœ¨ä¸åŒè§†è§’ä¹‹é—´çº¹ç†å±æ€§çš„é€æ¸å˜åŒ–ã€‚æ­¤å¤–ï¼Œé¢„å®šç›¸æœºä½ç½®çš„é€‰æ‹©æ²¡æœ‰è€ƒè™‘åˆ°ç‰©ä½“çš„å‡ ä½•ç»“æ„ï¼Œé™åˆ¶äº†ä»ä¸åŒè§†è§’åˆæˆçš„çº¹ç†ä¿¡æ¯çš„æœ‰æ•ˆä½¿ç”¨ï¼Œæœ€ç»ˆé™ä½äº†çº¹ç†çš„æ•´ä½“ä¸€è‡´æ€§ã€‚åœ¨TexTailorä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ï¼ˆ1ï¼‰åº”ç”¨ä¸€ç§é‡é‡‡æ ·æ–¹æ¡ˆï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åå¤æ•´åˆä¹‹å‰åˆæˆçš„çº¹ç†ä¿¡æ¯ï¼Œï¼ˆ2ï¼‰å¯¹æ·±åº¦æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒå›¾åƒä¼šé™åˆ¶æ¨¡å‹ç”Ÿæˆä¸æ¡ä»¶ç›¸ç¬¦çš„é«˜ä¿çœŸå›¾åƒçš„èƒ½åŠ›ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ€§èƒ½ä¿æŒæŸå¤±æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ ¹æ®ç‰©ä½“çš„å‡ ä½•ç»“æ„è‡ªé€‚åº”åœ°è°ƒæ•´ç›¸æœºä½ç½®ï¼Œæ”¹è¿›äº†è§†å›¾ä¸€è‡´çš„çº¹ç†åˆæˆã€‚åœ¨Objaverseæ•°æ®é›†å’ŒShapeNetæ±½è½¦æ•°æ®é›†çš„å­é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTexTailoråœ¨åˆæˆè§†å›¾ä¸€è‡´çš„çº¹ç†æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚TexTailorçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Adios42/Textailor">https://github.com/Adios42/Textailor</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10612v1">PDF</a> Submitted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>TexTailoræ˜¯ä¸€ç§ä»æ–‡æœ¬æè¿°ç”Ÿæˆä¸€è‡´ç‰©ä½“çº¹ç†çš„æ–°æ–¹æ³•ã€‚å®ƒè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°çº¹ç†åˆæˆæ–¹æ³•åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å› çº¹ç†ä¿¡æ¯é›†æˆä¸è¶³å’Œçº¹ç†åˆæˆè¿‡ç¨‹çš„è‡ªå›å½’æ€§è´¨å¯¼è‡´çš„è§†è§’é—´çº¹ç†å±æ€§é€æ¸å˜åŒ–çš„é—®é¢˜ã€‚é€šè¿‡åº”ç”¨é‡é‡‡æ ·æ–¹æ¡ˆå’Œå¾®è°ƒæ·±åº¦æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼ŒTexTailoræé«˜äº†çº¹ç†çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜é€šè¿‡æ€§èƒ½ä¿æŒæŸå¤±å’ŒåŸºäºç‰©ä½“å‡ ä½•è‡ªé€‚åº”è°ƒæ•´æ‘„åƒå¤´ä½ç½®ï¼Œæ”¹è¿›äº†è§†å›¾ä¸€è‡´çš„çº¹ç†åˆæˆã€‚å®éªŒè¡¨æ˜ï¼ŒTexTailoråœ¨åˆæˆè§†å›¾ä¸€è‡´çš„çº¹ç†æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TexTailoræ˜¯ä¸€ç§ç”¨äºä»æ–‡æœ¬æè¿°ç”Ÿæˆä¸€è‡´ç‰©ä½“çº¹ç†çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–‡æœ¬åˆ°çº¹ç†åˆæˆæ–¹æ³•å­˜åœ¨è§†è§’é—´çº¹ç†å±æ€§é€æ¸å˜åŒ–çš„é—®é¢˜ã€‚</li>
<li>TexTailoré€šè¿‡åº”ç”¨é‡é‡‡æ ·æ–¹æ¡ˆå’Œå¾®è°ƒæ·±åº¦æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ€§èƒ½ä¿æŒæŸå¤±ç”¨äºè§£å†³ä½¿ç”¨å°‘é‡è®­ç»ƒå›¾åƒé™åˆ¶æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„é—®é¢˜ã€‚</li>
<li>TexTailoré€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ‘„åƒå¤´ä½ç½®ï¼ŒåŸºäºç‰©ä½“å‡ ä½•æ”¹è¿›äº†è§†å›¾ä¸€è‡´çš„çº¹ç†åˆæˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTexTailoråœ¨åˆæˆè§†å›¾ä¸€è‡´çš„çº¹ç†æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8303a556773021fa8169d1043ea1de89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd1d514866977c39175e3ad5778cc017.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9e486ca7c2db84cbe5f583de6dfe06c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6a071af14afdbe11139ae37fcc6054e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9c605478c778956877c4bd93db580e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-resolution-efficient-image-generation-from-WiFi-CSI-using-a-pretrained-latent-diffusion-model"><a href="#High-resolution-efficient-image-generation-from-WiFi-CSI-using-a-pretrained-latent-diffusion-model" class="headerlink" title="High-resolution efficient image generation from WiFi CSI using a   pretrained latent diffusion model"></a>High-resolution efficient image generation from WiFi CSI using a   pretrained latent diffusion model</h2><p><strong>Authors:Eshan Ramesh, Nishio Takayuki</strong></p>
<p>We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDMâ€™s denoising diffusion model to the latent representation with text-based guidance before decoding using the LDMâ€™s pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†LatentCSIï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä»WiFi CSIæµ‹é‡ç”Ÿæˆç‰©ç†ç¯å¢ƒå›¾åƒçš„æ–°å‹æ–¹æ³•ã€‚ä¸åŒäºä¾èµ–å¤æ‚ä¸”è®¡ç®—å¯†é›†çš„æŠ€æœ¯ï¼ˆå¦‚GANsï¼‰çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œç›´æ¥å°†CSIæŒ¯å¹…æ˜ å°„åˆ°LDMçš„æ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æ½œåœ¨è¡¨ç¤ºåº”ç”¨LDMçš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼ï¼Œç„¶åä½¿ç”¨LDMçš„é¢„è®­ç»ƒè§£ç å™¨è¿›è¡Œè§£ç ï¼Œä»¥è·å¾—é«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¿™ç§è®¾è®¡ç»•è¿‡äº†åƒç´ ç©ºé—´å›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œé¿å…äº†ä¼ ç»Ÿå›¾åƒåˆ°å›¾åƒç®¡é“ä¸­é€šå¸¸éœ€è¦çš„æ˜¾å¼å›¾åƒç¼–ç é˜¶æ®µï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„é«˜è´¨é‡å›¾åƒåˆæˆã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨ç°æˆçš„WiFiè®¾å¤‡å’Œç›¸æœºæ”¶é›†çš„å®½å¸¦CSIæ•°æ®é›†ï¼›ä»¥åŠå…¬å¼€å¯ç”¨çš„MM-Fiæ•°æ®é›†çš„å­é›†ã€‚ç»“æœè¡¨æ˜ï¼ŒLatentCSIåœ¨è®¡ç®—æ•ˆç‡å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢éƒ½ä¼˜äºç›´æ¥åœ¨åœ°é¢ä¸Šè®­ç»ƒçš„åŸºå‡†å›¾åƒçš„å¯æ¯”å¤æ‚æ€§ï¼ŒåŒæ—¶ï¼Œé€šè¿‡å…¶ç‹¬ç‰¹çš„æ–‡æœ¬æŒ‡å¯¼å¯æ§æ€§æä¾›äº†å®é™…ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10605v1">PDF</a> 6 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºWiFi CSIæµ‹é‡çš„ç‰©ç†ç¯å¢ƒå›¾åƒç”Ÿæˆæ–°æ–¹æ³•LatentCSIçš„ç ”ç©¶ä»‹ç»ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œé€šè¿‡è½»é‡çº§ç¥ç»ç½‘ç»œå°†CSIæŒ¯å¹…æ˜ å°„åˆ°LDMçš„æ½œåœ¨ç©ºé—´ï¼Œå†åº”ç”¨LDMçš„é™å™ªæ‰©æ•£æ¨¡å‹è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»•è¿‡åƒç´ ç©ºé—´å›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œé¿å…äº†ä¼ ç»Ÿå›¾åƒåˆ°å›¾åƒçš„ç®¡é“æ‰€éœ€çš„æ˜¾å¼å›¾åƒç¼–ç é˜¶æ®µï¼Œå®ç°äº†é«˜æ•ˆé«˜è´¨é‡å›¾åƒåˆæˆã€‚åœ¨æ”¶é›†çš„å®½å¸¦CSIæ•°æ®é›†å’Œå…¬å¼€å¯ç”¨çš„MM-Fiæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LatentCSIæ˜¯ä¸€ç§åŸºäºWiFi CSIæµ‹é‡çš„ç‰©ç†ç¯å¢ƒå›¾åƒç”Ÿæˆæ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è¿›è¡Œå›¾åƒç”Ÿæˆã€‚</li>
<li>LatentCSIä½¿ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œå°†CSIæŒ¯å¹…æ˜ å°„åˆ°LDMçš„æ½œåœ¨ç©ºé—´ã€‚</li>
<li>æ–¹æ³•åº”ç”¨LDMçš„é™å™ªæ‰©æ•£æ¨¡å‹è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•é¿å…äº†åƒç´ ç©ºé—´å›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜å’Œæ˜¾å¼å›¾åƒç¼–ç é˜¶æ®µã€‚</li>
<li>LatentCSIå®ç°äº†é«˜æ•ˆä¸”é«˜è´¨é‡çš„å›¾åƒåˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50a204208f6a2eb9490386ba4f22d16c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-479d243450434ea721b14ee0d4aecd62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b038f02929a04ad6d70436ee07f8f05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58f54bc84b644d62c8e1ce1338a978da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f923f27663d2fd29b6546a82014a35b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4f775cd3111d69053aefaadbdd98340.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75263e7de639f69302aae9ae3bb63a7d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Conditional-diffusion-models-for-guided-anomaly-detection-in-brain-images-using-fluid-driven-anomaly-randomization"><a href="#Conditional-diffusion-models-for-guided-anomaly-detection-in-brain-images-using-fluid-driven-anomaly-randomization" class="headerlink" title="Conditional diffusion models for guided anomaly detection in brain   images using fluid-driven anomaly randomization"></a>Conditional diffusion models for guided anomaly detection in brain   images using fluid-driven anomaly randomization</h2><p><strong>Authors:Ana Lawry Aguila, Peirong Liu, Oula Puonti, Juan Eugenio Iglesias</strong></p>
<p>Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our modelâ€™s ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased&#x2F;healthy images. </p>
<blockquote>
<p>ç›‘ç£å¼æœºå™¨å­¦ä¹ å·²åœ¨è„‘MRIç—…ç†æ£€æµ‹ä¸­å®ç°äº†é«˜ç²¾åº¦ï¼Œä½†è¿™éœ€è¦æ¥è‡ªç—…æ‚£çš„è®­ç»ƒæ•°æ®ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å¹¶ä¸å®¹æ˜“è·å¾—ï¼Œä¾‹å¦‚åœ¨ç½•è§ç–¾ç—…çš„æƒ…å†µä¸‹ã€‚åŸºäºé‡å»ºçš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå·²åœ¨åŒ»ç–—é¢†åŸŸå—åˆ°æ¬¢è¿ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»…å¯¹å¥åº·å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæ— éœ€å¤§é‡ç‰¹å®šç–¾ç—…çš„ç¾¤ä½“ã€‚è¿™äº›æ–¹æ³•å‡è®¾åœ¨æ­£å¸¸æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ— æ³•å‡†ç¡®è¡¨ç¤ºæˆ–é‡å»ºå¼‚å¸¸ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾å¸¸å¸¸å¤±è´¥ï¼Œå› ä¸ºæ¨¡å‹æ— æ³•é‡å»ºå¥åº·ç»„ç»‡æˆ–å‡†ç¡®é‡å»ºå¼‚å¸¸åŒºåŸŸï¼ˆå³æ— æ³•æ¶ˆé™¤å¼‚å¸¸ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºè„‘MRIä¸­çš„å¼‚å¸¸æ£€æµ‹å’Œå¥åº·å›¾åƒé‡å»ºã€‚æˆ‘ä»¬çš„å¼±ç›‘ç£æ–¹æ³•å°†åˆæˆç”Ÿæˆçš„ä¼ªç—…ç†å›¾åƒé›†æˆåˆ°å»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œä»¥æ›´å¥½åœ°å¼•å¯¼å¥åº·å›¾åƒçš„é‡å»ºã€‚ä¸ºäº†ç”Ÿæˆè¿™äº›ä¼ªç—…ç†å›¾åƒï¼Œæˆ‘ä»¬é‡‡ç”¨æµä½“é©±åŠ¨çš„å¼‚å¸¸éšæœºåŒ–æ–¹æ³•æ¥å¢å¼ºè¾…åŠ©æ•°æ®é›†ä¸­çš„çœŸå®ç—…ç†åˆ†å‰²å›¾ï¼Œç¡®ä¿åˆæˆå¼‚å¸¸æ—¢çœŸå®åˆè§£å‰–è¿è´¯ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆå¼‚å¸¸æ•°æ®é›†å’ŒATLASæ•°æ®é›†ä¸­çš„çœŸå®ç—…ç†æ¥è¯„ä¼°æˆ‘ä»¬æ¨¡å‹æ£€æµ‹ç—…ç†çš„èƒ½åŠ›ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ï¼šï¼ˆiï¼‰å§‹ç»ˆä¼˜äºå˜åˆ†è‡ªç¼–ç å™¨ä»¥åŠæœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„æ½œåœ¨æ‰©æ•£ï¼›ï¼ˆiiï¼‰åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šï¼Œè¶…è¶Šäº†ä½¿ç”¨é…å¯¹ç–¾ç—…&#x2F;å¥åº·å›¾åƒçš„ç›‘ç£ä¿®å¤æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10233v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹å’Œå¥åº·å›¾åƒé‡å»ºã€‚é€šè¿‡å¼•å…¥åˆæˆä¼ªç—…ç†å›¾åƒè¿›è¡Œå¼±ç›‘ç£è®­ç»ƒï¼ŒæŒ‡å¯¼æ¨¡å‹é‡å»ºå¥åº·å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆå¼‚å¸¸æ•°æ®é›†å’ŒçœŸå®ç—…ç†æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºåŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹å’Œå¥åº·å›¾åƒé‡å»ºã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨åˆæˆä¼ªç—…ç†å›¾åƒè¿›è¡Œå¼±ç›‘ç£è®­ç»ƒã€‚</li>
<li>ä¼ªç—…ç†å›¾åƒæ˜¯é€šè¿‡æµä½“é©±åŠ¨å¼‚å¸¸éšæœºåŒ–æŠ€æœ¯ç”Ÿæˆçš„ï¼Œå¢å¼ºäº†ç°å®ç—…ç†åˆ†å‰²å›¾çš„å¯ç”¨æ€§ã€‚</li>
<li>æ¨¡å‹é€šè¿‡é›†æˆåˆæˆä¼ªç—…ç†å›¾åƒï¼Œå¯ä»¥æ›´å¥½åœ°é‡å»ºå¥åº·å›¾åƒã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨åˆæˆå¼‚å¸¸æ•°æ®é›†ä¸Šçš„è¡¨ç°æŒç»­ä¼˜äºå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ä»¥åŠå…¶ä»–æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½ç”šè‡³è¶…è¶Šäº†ä½¿ç”¨é…å¯¹ç–¾ç—…&#x2F;å¥åº·å›¾åƒçš„ç›‘ç£ä¿®å¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f05fa6420a3004beb80f4cbf15b042c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08d7e9fec1db5ed9ed98c8b405e62b03.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPARKE-Scalable-Prompt-Aware-Diversity-Guidance-in-Diffusion-Models-via-RKE-Score"><a href="#SPARKE-Scalable-Prompt-Aware-Diversity-Guidance-in-Diffusion-Models-via-RKE-Score" class="headerlink" title="SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via   RKE Score"></a>SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via   RKE Score</h2><p><strong>Authors:Mohammad Jalali, Haoyu Lei, Amin Gohari, Farzan Farnia</strong></p>
<p>Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware R&#39;eny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: <a target="_blank" rel="noopener" href="https://mjalali.github.io/SPARKE">https://mjalali.github.io/SPARKE</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒåˆæˆå’Œæç¤ºå¼•å¯¼ç”Ÿæˆå»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨æç¤ºå¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬ä¸­ç¡®ä¿è¶³å¤Ÿçš„å¤šæ ·æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æç¤ºè·¨è¶Šå¹¿æ³›è¯­ä¹‰è°±ä¸”éœ€è¦åœ¨è¯­ä¹‰ç›¸ä¼¼æç¤ºä¹‹é—´ä»¥æç¤ºæ„ŸçŸ¥çš„æ–¹å¼è¯„ä¼°ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§æ—¶ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡å¼•å…¥åŸºäºå¤šæ ·æ€§çš„åº¦é‡æ¥æŒ‡å¯¼æ›´ä¸°å¯Œçš„ç”Ÿæˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¯æ‰©å±•çš„æç¤ºæ„ŸçŸ¥RÃ©nyiæ ¸ç†µå¤šæ ·æ€§æŒ‡å¯¼ï¼ˆSPARKEï¼‰æ–¹æ³•ï¼Œä»¥æ‰©å±•åŸºäºå¤šæ ·æ€§åº¦é‡çš„æ–¹æ³•ï¼Œç”¨äºæç¤ºæ„ŸçŸ¥çš„å¤šæ ·æ€§æŒ‡å¯¼ã€‚SPARKEåˆ©ç”¨æ¡ä»¶ç†µè¿›è¡Œå¤šæ ·æ€§æŒ‡å¯¼ï¼Œå®ƒå¯ä»¥æ ¹æ®ç›¸ä¼¼æç¤ºåŠ¨æ€è°ƒæ•´å¤šæ ·æ€§æµ‹é‡ï¼Œå¹¶èƒ½å¤Ÿå®ç°æç¤ºæ„ŸçŸ¥çš„å¤šæ ·æ€§æ§åˆ¶ã€‚è™½ç„¶åŸºäºç†µçš„æŒ‡å¯¼æ–¹æ³•æé«˜äº†æç¤ºæ„ŸçŸ¥çš„å¤šæ ·æ€§ï¼Œä½†å®ƒå¯¹çŸ©é˜µåŸºäºçš„ç†µåˆ†æ•°çš„ä¾èµ–åœ¨å¤§å‹ç”Ÿæˆè®¾ç½®ä¸­æ„æˆäº†è®¡ç®—æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å…³æ³¨æ¡ä»¶æ½œåœ¨RKEè¯„åˆ†æŒ‡å¯¼çš„ç‰¹æ®Šæƒ…å†µï¼Œå°†ä¸€èˆ¬ç†µåº¦é‡çš„O(n^3)çš„ç†µè®¡ç®—å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å¤æ‚æ€§é™ä½åˆ°O(n)ã€‚é™ä½çš„è®¡ç®—å¤æ‚æ€§å…è®¸åœ¨å¤šä¸ªæç¤ºä¸Šè¿›è¡Œæ•°åƒè½®ç”Ÿæˆçš„å¤šæ ·æ€§å¼•å¯¼é‡‡æ ·ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸Šå¯¹SPARKEæ–¹æ³•è¿›è¡Œäº†æ•°å€¼æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†ç”Ÿæˆæ•°æ®çš„æç¤ºæ„ŸçŸ¥å¤šæ ·æ€§ï¼Œä¸”æ²¡æœ‰äº§ç”Ÿæ˜¾è‘—çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬åœ¨é¡¹ç›®é¡µé¢ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://mjalali.github.io/SPARKE">https://mjalali.github.io/SPARKE</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusionæ¨¡å‹åœ¨å›¾åƒåˆæˆå’Œæç¤ºå¼•å¯¼ç”Ÿæˆå»ºæ¨¡æ–¹é¢çš„å‡ºè‰²è¡¨ç°ã€‚ç„¶è€Œï¼Œå¯¹äºç¡®ä¿æç¤ºå¼•å¯¼æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ ·æœ¬å…·æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰è°±å¹¿æ³›çš„æç¤ºå’Œéœ€è¦åœ¨è¯­ä¹‰ç›¸ä¼¼çš„æç¤ºä¹‹é—´è¿›è¡Œæç¤ºæ„ŸçŸ¥çš„å¤šæ ·æ€§è¯„ä¼°æ—¶ã€‚æœ¬æ–‡æå‡ºäº†å¯æ‰©å±•çš„æç¤ºæ„ŸçŸ¥RÃ©nyiæ ¸ç†µå¤šæ ·æ€§æŒ‡å¯¼ï¼ˆSPARKEï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚SPARKEé‡‡ç”¨åŸºäºæ¡ä»¶ç†µçš„å¤šæ ·æ€§æŒ‡å¯¼ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ç›¸ä¼¼çš„æç¤ºåŠ¨æ€è°ƒèŠ‚å¤šæ ·æ€§æµ‹é‡ï¼Œå®ç°æç¤ºæ„ŸçŸ¥çš„å¤šæ ·æ€§æ§åˆ¶ã€‚è™½ç„¶ç†µæŒ‡å¯¼æ–¹æ³•æé«˜äº†æç¤ºæ„ŸçŸ¥çš„å¤šæ ·æ€§ï¼Œä½†å…¶ä¾èµ–äºçŸ©é˜µçš„ç†µåˆ†æ•°åœ¨è®¡ç®—å¤§å‹ç”Ÿæˆåœºæ™¯ä¸­å­˜åœ¨ä¸€å®šçš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä¸“æ³¨äºæ¡ä»¶æ½œåœ¨RKEå¾—åˆ†æŒ‡å¯¼çš„ç‰¹æ®Šæ¡ˆä¾‹ï¼Œå°†ç†µè®¡ç®—å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å¤æ‚åº¦ä»ä¸€èˆ¬ç†µæµ‹é‡çš„O(n^3)é™ä½åˆ°O(n)ã€‚å‡å°‘äº†è®¡ç®—å¤æ‚åº¦ï¼Œå…è®¸åœ¨å¤šä¸ªæç¤ºä¸Šè¿›è¡Œæ•°åƒè½®å¸¦å¤šæ ·æ€§çš„é‡‡æ ·ã€‚æœ¬æ–‡åœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸Šå¯¹SPARKEæ–¹æ³•è¿›è¡Œäº†æ•°å€¼æµ‹è¯•ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æé«˜ç”Ÿæˆæ•°æ®çš„æç¤ºæ„ŸçŸ¥å¤šæ ·æ€§çš„åŒæ—¶ä¸ä¼šå¸¦æ¥é‡å¤§è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨é¡¹ç›®é¡µé¢ä¸Šå‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://mjalali.github.io/SPARKE">https://mjalali.github.io/SPARKE</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusionæ¨¡å‹åœ¨å›¾åƒåˆæˆå’Œæç¤ºå¼•å¯¼ç”Ÿæˆå»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æˆåŠŸã€‚</li>
<li>åœ¨å¹¿æ³›çš„è¯­ä¹‰æç¤ºä¸‹ç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”SPARKEï¼Œé€šè¿‡æ¡ä»¶ç†µå®ç°æç¤ºæ„ŸçŸ¥çš„å¤šæ ·æ€§æŒ‡å¯¼ã€‚</li>
<li>SPARKEèƒ½å¤Ÿæ ¹æ®ç›¸ä¼¼çš„æç¤ºåŠ¨æ€è°ƒæ•´å¤šæ ·æ€§æµ‹é‡ã€‚</li>
<li>ç›¸è¾ƒäºä¸€èˆ¬ç†µæµ‹é‡çš„O(n^3)ï¼ŒSPARKEä¸“æ³¨äºæ¡ä»¶æ½œåœ¨RKEå¾—åˆ†æŒ‡å¯¼ï¼Œå°†è®¡ç®—å¤æ‚åº¦é™ä½åˆ°O(n)ã€‚</li>
<li>SPARKEæ–¹æ³•æé«˜äº†ç”Ÿæˆæ•°æ®çš„æç¤ºæ„ŸçŸ¥å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¸ä¼šå¸¦æ¥æ˜¾è‘—çš„è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19b3e1439ce1e152412a161e1c08ad3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18948a9d5bd7d17057436a68f45bc115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c8f757bde736b9aeb76b6c07a80c52a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning"><a href="#LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning" class="headerlink" title="LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning"></a>LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning</h2><p><strong>Authors:Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue</strong></p>
<p>Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods. </p>
<blockquote>
<p>ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘ç¼–è¾‘å·²ç»åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¸¸å¸¸ä¾èµ–äºå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚è™½ç„¶åŸºäºé¦–å¸§å¼•å¯¼ç¼–è¾‘èƒ½å¤Ÿæ§åˆ¶é¦–å¸§ï¼Œä½†å¯¹äºåç»­å¸§çš„æ§åˆ¶åˆ™ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©ç çš„LoRAï¼ˆä½ç§©é€‚é…ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”é¢„è®­ç»ƒå¥½çš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œå®ç°çµæ´»çš„è§†é¢‘ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä¿ç•™èƒŒæ™¯åŒºåŸŸï¼ŒåŒæ—¶å®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ã€‚è¿™ä¸€è§£å†³æ–¹æ¡ˆåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯é€‚åº”çš„è§†é¢‘ç¼–è¾‘ã€‚ä¸ºäº†æ›´å¥½åœ°å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å¤–çš„å‚è€ƒï¼Œå¦‚ä¸åŒçš„è§‚ç‚¹æˆ–ä»£è¡¨æ€§çš„åœºæ™¯çŠ¶æ€ï¼Œå®ƒä»¬ä½œä¸ºå†…å®¹å±•å¼€çš„è§†è§‰é”šç‚¹ã€‚æˆ‘ä»¬é‡‡ç”¨æ©ç é©±åŠ¨çš„LoRAè°ƒä¼˜ç­–ç•¥æ¥è§£å†³æ§åˆ¶æŒ‘æˆ˜ï¼Œè¯¥ç­–ç•¥å°†é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚åº”äºç¼–è¾‘ä¸Šä¸‹æ–‡ã€‚æ¨¡å‹å¿…é¡»ä»ä¸¤ä¸ªç‹¬ç‰¹çš„ä¿¡æ¯æºä¸­å­¦ä¹ ï¼šè¾“å…¥è§†é¢‘æä¾›ç©ºé—´ç»“æ„å’Œè¿åŠ¨çº¿ç´¢ï¼Œè€Œå‚è€ƒå›¾åƒæä¾›å¤–è§‚æŒ‡å¯¼ã€‚ç©ºé—´æ©è†œé€šè¿‡åŠ¨æ€è°ƒåˆ¶æ¨¡å‹æ‰€å…³æ³¨çš„é‡ç‚¹ï¼Œå®ç°ç‰¹å®šåŒºåŸŸçš„å­¦ä¹ ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½èƒ½ä»é€‚å½“çš„ä¿¡æ¯æºä¸­æ±²å–çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘ç¼–è¾‘æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10082v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç¼–è¾‘å·²ç”Ÿæˆé«˜è´¨é‡ç¼–è¾‘è§†é¢‘ï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒã€ç¼ºä¹ç‰¹å®šç¼–è¾‘çµæ´»æ€§ä»¥åŠç¬¬ä¸€å¸§å¼•å¯¼ç¼–è¾‘ç¼ºä¹åç»­å¸§çµæ´»æ€§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ©è†œçš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œç”¨äºé€‚åº”é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹çš„çµæ´»è§†é¢‘ç¼–è¾‘ã€‚è¯¥æ–¹æ³•å¯ä¿ç•™èƒŒæ™¯åŒºåŸŸï¼Œå®ç°å¯æ§ç¼–è¾‘ä¼ æ’­ï¼Œæä¾›é«˜æ•ˆã€å¯é€‚åº”çš„è§†é¢‘ç¼–è¾‘ï¼Œæ— éœ€æ›´æ”¹æ¨¡å‹æ¶æ„ã€‚ä¸ºäº†æ›´å¥½åœ°å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œæœ¬æ–‡å¼•å…¥äº†é¢å¤–çš„å‚è€ƒï¼Œå¦‚ä¸åŒçš„è§‚ç‚¹æˆ–ä»£è¡¨æ€§çš„åœºæ™¯çŠ¶æ€ï¼Œä½œä¸ºå†…å®¹å±•å¼€çš„è§†è§‰é”šç‚¹ã€‚é€šè¿‡æ©è†œé©±åŠ¨çš„LoRAè°ƒä¼˜ç­–ç•¥ï¼Œè§£å†³æ§åˆ¶æŒ‘æˆ˜ï¼Œä½¿é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚åº”ç¼–è¾‘ä¸Šä¸‹æ–‡ã€‚æ¨¡å‹ä»ä¸¤ä¸ªä¸åŒæ¥æºå­¦ä¹ ï¼šè¾“å…¥è§†é¢‘æä¾›ç©ºé—´ç»“æ„å’Œè¿åŠ¨çº¿ç´¢ï¼Œå‚è€ƒå›¾åƒæä¾›å¤–è§‚æŒ‡å¯¼ã€‚ç©ºé—´æ©è†œé€šè¿‡åŠ¨æ€è°ƒåˆ¶æ¨¡å‹å…³æ³¨çš„åŒºåŸŸï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½ä»é€‚å½“çš„æ¥æºä¸­ç»˜åˆ¶ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç¼–è¾‘æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç¼–è¾‘ä¸­ç”Ÿæˆé«˜è´¨é‡ç»“æœã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç¼ºä¹ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºåŸºäºæ©è†œçš„LoRAè°ƒä¼˜æ–¹æ³•ï¼Œé€‚åº”é¢„è®­ç»ƒI2Væ¨¡å‹è¿›è¡Œçµæ´»è§†é¢‘ç¼–è¾‘ã€‚</li>
<li>æ–¹æ³•å¯ä¿ç•™èƒŒæ™¯åŒºåŸŸï¼Œå®ç°å¯æ§ç¼–è¾‘ä¼ æ’­ã€‚</li>
<li>å¼•å…¥é¢å¤–å‚è€ƒï¼Œå¦‚ä¸åŒè§‚ç‚¹æˆ–åœºæ™¯çŠ¶æ€ï¼Œä½œä¸ºå†…å®¹å±•å¼€çš„è§†è§‰é”šç‚¹ã€‚</li>
<li>é‡‡ç”¨æ©è†œé©±åŠ¨çš„LoRAè°ƒä¼˜ç­–ç•¥ï¼Œä½¿æ¨¡å‹é€‚åº”ç¼–è¾‘ä¸Šä¸‹æ–‡ã€‚</li>
<li>æ¨¡å‹ä»è¾“å…¥è§†é¢‘å’Œå‚è€ƒå›¾åƒä¸¤ä¸ªæ¥æºå­¦ä¹ ï¼Œç©ºé—´æ©è†œç¡®ä¿åŒºåŸŸç‰¹å®šå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d68b6950080a3e2d7b7aadf086f03ab9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53ab42514b300b74be0259d543e6d0e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d6fcf3b0ac9b7f7e8a2c2220138e3b9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations"><a href="#Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations" class="headerlink" title="Towards Reliable Identification of Diffusion-based Image Manipulations"></a>Towards Reliable Identification of Diffusion-based Image Manipulations</h2><p><strong>Authors:Alex Costanzino, Woody Bayliss, Juil Sock, Marc Gorriz Blanch, Danijela Horak, Ivan Laptev, Philip Torr, Fabio Pizzati</strong></p>
<p>Changing facial expressions, gestures, or background details may dramatically alter the meaning conveyed by an image. Notably, recent advances in diffusion models greatly improve the quality of image manipulation while also opening the door to misuse. Identifying changes made to authentic images, thus, becomes an important task, constantly challenged by new diffusion-based editing tools. To this end, we propose a novel approach for ReliAble iDentification of inpainted AReas (RADAR). RADAR builds on existing foundation models and combines features from different image modalities. It also incorporates an auxiliary contrastive loss that helps to isolate manipulated image patches. We demonstrate these techniques to significantly improve both the accuracy of our method and its generalisation to a large number of diffusion models. To support realistic evaluation, we further introduce BBC-PAIR, a new comprehensive benchmark, with images tampered by 28 diffusion models. Our experiments show that RADAR achieves excellent results, outperforming the state-of-the-art in detecting and localising image edits made by both seen and unseen diffusion models. Our code, data and models will be publicly available at <a target="_blank" rel="noopener" href="https://alex-costanzino.github.io/radar/">https://alex-costanzino.github.io/radar/</a>. </p>
<blockquote>
<p>æ”¹å˜é¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿æˆ–èƒŒæ™¯ç»†èŠ‚å¯èƒ½ä¼šæå¤§åœ°æ”¹å˜å›¾åƒæ‰€ä¼ è¾¾çš„æ„ä¹‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•è™½ç„¶å¤§å¤§æé«˜äº†å›¾åƒæ“ä½œçš„è´¨é‡ï¼Œä½†åŒæ—¶ä¹Ÿæ‰“å¼€äº†æ»¥ç”¨çš„å¤§é—¨ã€‚å› æ­¤ï¼Œè¯†åˆ«å¯¹çœŸå®å›¾åƒæ‰€åšçš„æ”¹å˜æˆä¸ºäº†ä¸€é¡¹é‡è¦çš„ä»»åŠ¡ï¼Œè¿™ä¸€ä»»åŠ¡ä¸æ–­å—åˆ°æ–°çš„åŸºäºæ‰©æ•£çš„ç¼–è¾‘å·¥å…·çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯é è¯†åˆ«å›¾åƒä¿®å¤åŒºåŸŸçš„æ–¹æ³•ï¼ˆRADARï¼‰ã€‚RADARå»ºç«‹åœ¨ç°æœ‰çš„åŸºç¡€æ¨¡å‹ä¸Šï¼Œç»“åˆäº†ä¸åŒå›¾åƒæ¨¡æ€çš„ç‰¹å¾ã€‚å®ƒè¿˜å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©å¯¹æ¯”æŸå¤±ï¼Œæœ‰åŠ©äºéš”ç¦»è¢«æ“çºµçš„å›¾åƒè¡¥ä¸ã€‚æˆ‘ä»¬è¯æ˜è¿™äº›æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æˆ‘ä»¬æ–¹æ³•çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”å…¶å¯ä»¥æ¨å¹¿åˆ°å¤§é‡æ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†æ”¯æŒçœŸå®è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†BBC-PAIRï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¢«28ç§æ‰©æ•£æ¨¡å‹ç¯¡æ”¹è¿‡çš„å›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRADARå–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œåœ¨æ£€æµ‹å’Œå®šä½å·²çŸ¥å’ŒæœªçŸ¥çš„æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œçš„ç¼–è¾‘æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://alex-costanzino.github.io/radar/%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://alex-costanzino.github.io/radar/å…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05466v2">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://alex-costanzino.github.io/radar/">https://alex-costanzino.github.io/radar/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹å›¾åƒç¼–è¾‘æŠ€æœ¯æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºRADARçš„æ–°æ–¹æ³•ï¼Œç”¨äºå¯é åœ°è¯†åˆ«å›¾åƒä¸­çš„ç¼–è¾‘åŒºåŸŸã€‚RADARç»“åˆä¸åŒå›¾åƒæ¨¡æ€çš„ç‰¹å¾ï¼Œå¹¶å¼•å…¥è¾…åŠ©å¯¹æ¯”æŸå¤±ä»¥åŒºåˆ†ç¼–è¾‘è¿‡çš„å›¾åƒåŒºåŸŸã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒçœŸå®è¯„ä¼°ï¼Œè¿˜æ¨å‡ºäº†BBC-PAIRæ–°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ç”±28ç§æ‰©æ•£æ¨¡å‹ä¿®æ”¹çš„å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨æ£€æµ‹å’Œå®šä½ç”±å·²çŸ¥å’ŒæœªçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œçš„å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†å“è¶Šæˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æé«˜äº†å›¾åƒæ“çºµçš„è´¨é‡ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ äº†è¯¯ç”¨çš„é£é™©ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•RADARï¼Œç”¨äºè¯†åˆ«å›¾åƒä¸­çš„ç¼–è¾‘åŒºåŸŸã€‚</li>
<li>RADARç»“åˆäº†ä¸åŒå›¾åƒæ¨¡æ€çš„ç‰¹å¾ï¼Œå¹¶å¼•å…¥è¾…åŠ©å¯¹æ¯”æŸå¤±ä»¥æé«˜å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨å‡ºäº†BBC-PAIRåŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå¯¹å›¾åƒç¼–è¾‘æ£€æµ‹æ–¹æ³•çš„çœŸå®è¯„ä¼°ã€‚</li>
<li>RADARåœ¨æ£€æµ‹å’Œå®šä½ç”±å¤šç§æ‰©æ•£æ¨¡å‹è¿›è¡Œçš„å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>RADARæ–¹æ³•ä¸ä»…åœ¨å·²çŸ¥æ‰©æ•£æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¯¹æœªçŸ¥æ‰©æ•£æ¨¡å‹ä¹Ÿå…·æœ‰å¼ºå¤§çš„æ£€æµ‹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05466">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2ee96aff50d70666abe7e941024a3fca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af2166a234b6d1a78d8fac83caa42cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84359b69dca16787536bd8d3beb9726.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6eaf4c668fffd453db25aedf5883b7f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Sparc3D-Sparse-Representation-and-Construction-for-High-Resolution-3D-Shapes-Modeling"><a href="#Sparc3D-Sparse-Representation-and-Construction-for-High-Resolution-3D-Shapes-Modeling" class="headerlink" title="Sparc3D: Sparse Representation and Construction for High-Resolution 3D   Shapes Modeling"></a>Sparc3D: Sparse Representation and Construction for High-Resolution 3D   Shapes Modeling</h2><p><strong>Authors:Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen</strong></p>
<p>High-fidelity 3D object synthesis remains significantly more challenging than 2D image generation due to the unstructured nature of mesh data and the cubic complexity of dense volumetric grids. Existing two-stage pipelines-compressing meshes with a VAE (using either 2D or 3D supervision), followed by latent diffusion sampling-often suffer from severe detail loss caused by inefficient representations and modality mismatches introduced in VAE. We introduce Sparc3D, a unified framework that combines a sparse deformable marching cubes representation Sparcubes with a novel encoder Sparconv-VAE. Sparcubes converts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by scattering signed distance and deformation fields onto a sparse cube, allowing differentiable optimization. Sparconv-VAE is the first modality-consistent variational autoencoder built entirely upon sparse convolutional networks, enabling efficient and near-lossless 3D reconstruction suitable for high-resolution generative modeling through latent diffusion. Sparc3D achieves state-of-the-art reconstruction fidelity on challenging inputs, including open surfaces, disconnected components, and intricate geometry. It preserves fine-grained shape details, reduces training and inference cost, and integrates naturally with latent diffusion models for scalable, high-resolution 3D generation. </p>
<blockquote>
<p>é«˜ä¿çœŸ3Då¯¹è±¡åˆæˆç›¸è¾ƒäº2Då›¾åƒç”Ÿæˆä»ç„¶æ›´å…·æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç½‘æ ¼æ•°æ®çš„éç»“æ„åŒ–å’Œå¯†é›†ä½“ç§¯ç½‘æ ¼çš„ç«‹æ–¹å¤æ‚æ€§ã€‚ç°æœ‰çš„ä¸¤é˜¶æ®µæµç¨‹ï¼ˆä½¿ç”¨VAEï¼ˆä½¿ç”¨äºŒç»´æˆ–ä¸‰ç»´ç›‘ç£ï¼‰å‹ç¼©ç½‘æ ¼ï¼Œç„¶åè¿›è¡Œæ½œåœ¨æ‰©æ•£é‡‡æ ·ï¼‰å¸¸å¸¸å› VAEä¸­å¼•å…¥çš„ä½æ•ˆè¡¨ç¤ºå’Œæ¨¡æ€ä¸åŒ¹é…è€Œå¯¼è‡´ç»†èŠ‚ä¸¥é‡æŸå¤±ã€‚æˆ‘ä»¬å¼•å…¥äº†Sparc3Dï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†ç¨€ç–å¯å˜å½¢è¡Œè¿›ç«‹æ–¹ä½“è¡¨ç¤ºSparcubeså’Œæ–°å‹ç¼–ç å™¨Sparconv-VAEçš„ç»Ÿä¸€æ¡†æ¶ã€‚Sparcubesé€šè¿‡å°†å¸¦ç¬¦å·è·ç¦»å’Œå˜å½¢åœºæ•£å°„åˆ°ç¨€ç–ç«‹æ–¹ä½“ä¸Šï¼Œå°†åŸå§‹ç½‘æ ¼è½¬æ¢ä¸ºé«˜åˆ†è¾¨ç‡ï¼ˆ1024^3ï¼‰ä¸”å…·æœ‰ä»»æ„æ‹“æ‰‘çš„è¡¨é¢ï¼Œä»è€Œå®ç°å¯å¾®åˆ†ä¼˜åŒ–ã€‚Sparconv-VAEæ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºç¨€ç–å·ç§¯ç½‘ç»œçš„ä¸€è‡´æ¨¡æ€è‡ªåŠ¨ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆä¸”æ¥è¿‘æ— æŸçš„3Dé‡å»ºï¼Œé€‚åˆé€šè¿‡æ½œåœ¨æ‰©æ•£è¿›è¡Œé«˜åˆ†è¾¨ç‡ç”Ÿæˆå»ºæ¨¡ã€‚Sparc3Dåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è¾“å…¥ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºä¿çœŸåº¦ï¼ŒåŒ…æ‹¬å¼€æ”¾è¡¨é¢ã€æ–­å¼€ç»„ä»¶å’Œç²¾ç»†å‡ ä½•ã€‚å®ƒä¿ç•™äº†ç²¾ç»†çš„å½¢çŠ¶ç»†èŠ‚ï¼Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œå¹¶ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹è‡ªç„¶é›†æˆï¼Œå¯å®ç°å¯æ‰©å±•çš„é«˜åˆ†è¾¨ç‡3Dç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14521v3">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://lizhihao6.github.io/Sparc3D">https://lizhihao6.github.io/Sparc3D</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹ä¸‰ç»´ç‰©ä½“åˆæˆçš„æ–°æŠ€æœ¯æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚ç”±äºç½‘æ ¼æ•°æ®çš„éç»“æ„åŒ–å’Œå¯†é›†ä½“ç§¯ç½‘æ ¼çš„å¤æ‚æ€§ï¼Œä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæµæ°´çº¿ç»å¸¸é­å—æŸå¤±ç»†èŠ‚çš„å›°æ‰°ã€‚ä½œè€…æå‡ºåä¸ºSParc3Dçš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç¨€ç–å¯å˜å½¢ç«‹ä½“æ¨è¿›æ¨¡å—ï¼ˆSparcubesï¼‰å’Œæ–°å‹ç¼–ç å™¨ï¼ˆSparconv-VAEï¼‰ï¼Œå®ç°äº†é«˜æ•ˆä¸”æ— æŸçš„ä¸‰ç»´é‡å»ºï¼Œé€‚ç”¨äºé€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜åˆ†è¾¨ç‡ç”Ÿæˆå»ºæ¨¡ã€‚æ­¤æŠ€æœ¯åœ¨å¤šç§å¤æ‚è¾“å…¥ä¸Šè¾¾åˆ°å‰æ‰€æœªæœ‰çš„é‡å»ºä¿çœŸåº¦ï¼Œå¹¶å¯è‡ªç„¶é›†æˆåˆ°æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°å¯æ‰©å±•çš„é«˜åˆ†è¾¨ç‡ä¸‰ç»´ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹æ€»ç»“ï¼š</p>
<ul>
<li>é«˜ä¿çœŸä¸‰ç»´ç‰©ä½“åˆæˆæ¯”äºŒç»´å›¾åƒç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§ï¼Œä¸»è¦ç”±äºç½‘æ ¼æ•°æ®çš„éç»“æ„åŒ–å’Œå¯†é›†ä½“ç§¯ç½‘æ ¼çš„å¤æ‚æ€§ã€‚</li>
<li>ä¼ ç»Ÿä¸¤é˜¶æ®µæµæ°´çº¿æ–¹æ³•å¸¸å¸¸ç”±äºæ— æ•ˆçš„ä»£è¡¨å’Œæ¨¡æ€ä¸åŒ¹é…å¯¼è‡´çš„ç»†èŠ‚æŸå¤±ã€‚</li>
<li>SParc3Dæ¡†æ¶ç»“åˆäº†ç¨€ç–å¯å˜å½¢ç«‹ä½“æ¨è¿›æ¨¡å—ï¼ˆSparcubesï¼‰å’Œæ–°å‹ç¼–ç å™¨ï¼ˆSparconv-VAEï¼‰ã€‚</li>
<li>Sparcubeså¯å°†åŸå§‹ç½‘æ ¼è½¬åŒ–ä¸ºé«˜åˆ†è¾¨ç‡è¡¨é¢ï¼Œå¹¶æ”¯æŒä»»æ„æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>Sparconv-VAEæ˜¯é¦–ä¸ªå®Œå…¨åŸºäºç¨€ç–å·ç§¯ç½‘ç»œçš„ä¸€è‡´æ¨¡æ€è‡ªåŠ¨ç¼–ç å™¨ï¼Œå¯å®ç°é«˜æ•ˆä¸”æ— æŸçš„ä¸‰ç»´é‡å»ºã€‚</li>
<li>SParc3Dåœ¨å¤æ‚è¾“å…¥ä¸Šå®ç°äº†é«˜ä¿çœŸé‡å»ºï¼ŒåŒ…æ‹¬å¼€æ”¾è¡¨é¢ã€æ–­å¼€ç»„ä»¶å’Œç²¾ç»†å‡ ä½•ç»“æ„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ed242398226713dbea91aa85a12c1806.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a38a1caa3bd9294fd6e88711b25e82e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67ab921954ad207b3077ab9d8fd05efc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Free-Graph-Generation-with-Next-Scale-Prediction"><a href="#Diffusion-Free-Graph-Generation-with-Next-Scale-Prediction" class="headerlink" title="Diffusion-Free Graph Generation with Next-Scale Prediction"></a>Diffusion-Free Graph Generation with Next-Scale Prediction</h2><p><strong>Authors:Samuel Belkadi, Steve Hong, Marian Chen, Miruna Cretu, Charles Harris, Pietro Lio</strong></p>
<p>Autoregressive models excel in efficiency and plug directly into the transformer ecosystem, delivering robust generalization, predictable scalability, and seamless workflows such as fine-tuning and parallelized training. However, they require an explicit sequence order, which contradicts the unordered nature of graphs. In contrast, diffusion models maintain permutation invariance and enable one-shot generation but require up to thousands of denoising steps and additional features for expressivity, leading to high computational costs. Inspired by recent breakthroughs in image generation, especially the success of visual autoregressive methods, we propose MAG, a novel diffusion-free graph generation framework based on next-scale prediction. By leveraging a hierarchy of latent representations, the model progressively generates scales of the entire graph without the need for explicit node ordering. Experiments on both generic and molecular graph datasets demonstrated the potential of this method, achieving inference speedups of up to three orders of magnitude over state-of-the-art methods, while preserving high-quality generation. </p>
<blockquote>
<p>è‡ªå›å½’æ¨¡å‹åœ¨æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”èƒ½å¤Ÿç›´æ¥æ’å…¥transformerç”Ÿæ€ç³»ç»Ÿï¼Œæä¾›ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€å¯é¢„æµ‹çš„æ‰©å±•æ€§ä»¥åŠå¾®è°ƒå’Œå¹³è¡Œè®­ç»ƒç­‰æ— ç¼å·¥ä½œæµç¨‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬éœ€è¦æ˜ç¡®çš„åºåˆ—é¡ºåºï¼Œè¿™ä¸å›¾çš„æ— åºæ€§è´¨ç›¸çŸ›ç›¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‰©æ•£æ¨¡å‹ä¿æŒç½®æ¢ä¸å˜æ€§ï¼Œèƒ½å¤Ÿå®ç°ä¸€æ¬¡ç”Ÿæˆï¼Œä½†éœ€è¦é«˜è¾¾æ•°åƒæ­¥çš„å»å™ªæ­¥éª¤å’Œé¢å¤–çš„ç‰¹å¾æ¥è¡¨è¾¾ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚å—æœ€è¿‘å›¾åƒç”Ÿæˆé¢†åŸŸçªç ´ï¼Œå°¤å…¶æ˜¯è§†è§‰è‡ªå›å½’æ–¹æ³•æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†MAGï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸‹ä¸€å°ºåº¦é¢„æµ‹çš„æ–°å‹æ— æ‰©æ•£å›¾ç”Ÿæˆæ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨æ½œåœ¨è¡¨ç¤ºå±‚æ¬¡ç»“æ„ï¼Œè¯¥æ¨¡å‹æ— éœ€æ˜¾å¼èŠ‚ç‚¹æ’åºå³å¯é€æ­¥ç”Ÿæˆæ•´ä¸ªå›¾çš„å°ºåº¦ã€‚åœ¨é€šç”¨å’Œå›¾åˆ†å­æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æ½œåŠ›ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾ä¸‰ä¸ªæ•°é‡çº§çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23612v2">PDF</a> Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ— æ‰©æ•£å›¾ç”Ÿæˆæ¡†æ¶MAGï¼Œå®ƒé€šè¿‡åˆ©ç”¨åˆ†å±‚æ½œåœ¨è¡¨ç¤ºæ¥æ¸è¿›ç”Ÿæˆæ•´ä¸ªå›¾ï¼Œæ— éœ€æ˜ç¡®çš„èŠ‚ç‚¹é¡ºåºã€‚MAGç»“åˆäº†è‡ªå›å½’æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¦‚æ•ˆç‡ã€é€šç”¨æ€§ã€å¯é¢„æµ‹çš„å¯æ‰©å±•æ€§å’Œæ— ç¼å·¥ä½œæµç¨‹ï¼ŒåŒæ—¶è§£å†³äº†è‡ªå›å½’æ¨¡å‹å¯¹åºåˆ—é¡ºåºçš„ä¾èµ–é—®é¢˜ã€‚åœ¨é€šç”¨å’Œå›¾ç»“æ„åˆ†å­æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAGå®ç°äº†é«˜è¾¾ä¸‰ä¸ªæ•°é‡çº§çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’æ¨¡å‹å…·æœ‰æ•ˆç‡ã€é€šç”¨æ€§ã€å¯é¢„æµ‹çš„å¯æ‰©å±•æ€§å’Œæ— ç¼å·¥ä½œæµç¨‹ç­‰ä¼˜ç‚¹ï¼Œä½†éœ€è¦æ˜ç¡®çš„åºåˆ—é¡ºåºã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰æ’åˆ—ä¸å˜æ€§å’Œä¸€æ¬¡ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œéœ€è¦æ•°åƒä¸ªå»å™ªæ­¥éª¤å’Œé¢å¤–çš„ç‰¹å¾æ¥è¡¨è¾¾æ€§ã€‚</li>
<li>æ–°å‹çš„æ— æ‰©æ•£å›¾ç”Ÿæˆæ¡†æ¶MAGç»“åˆäº†è‡ªå›å½’æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åˆ©ç”¨åˆ†å±‚æ½œåœ¨è¡¨ç¤ºæ¥ç”Ÿæˆå›¾ï¼Œè§£å†³äº†è‡ªå›å½’æ¨¡å‹å¯¹èŠ‚ç‚¹é¡ºåºçš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>MAGå®ç°äº†é«˜è¾¾ä¸‰ä¸ªæ•°é‡çº§çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡ç”Ÿæˆã€‚</li>
<li>MAGæ¡†æ¶æ— éœ€æ˜ç¡®çš„èŠ‚ç‚¹é¡ºåºå³å¯æ¸è¿›ç”Ÿæˆæ•´ä¸ªå›¾çš„å°ºåº¦ã€‚</li>
<li>åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAGå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a4ed2104df713d5451b51e057adeb08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c434e24f5d030838ad5b049e17c06b1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8afafdf73a0ffd978890ad964295592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1968f996cf4c625eab0fe49d2cf2759.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0affbb029ccc48f2e2c90a4796c06312.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models"><a href="#Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models" class="headerlink" title="Training-Free Safe Denoisers for Safe Use of Diffusion Models"></a>Training-Free Safe Denoisers for Safe Use of Diffusion Models</h2><p><strong>Authors:Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mijung Park</strong></p>
<p>There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely. </p>
<blockquote>
<p>å…³äºå¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å®‰å…¨é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸è¢«è¯¯ç”¨äºäº§ç”Ÿä¸é€‚å½“ã€ä¸å®‰å…¨çš„å·¥ä½œï¼ˆNSFWï¼‰å†…å®¹æˆ–ç”Ÿæˆç‰ˆæƒææ–™æˆ–é‚£äº›å¸Œæœ›è¢«é—å¿˜çš„ä¸ªäººçš„æ•°æ®ã€‚è®¸å¤šç°æœ‰æ–¹æ³•é€šè¿‡ä¾èµ–åŸºäºæ–‡æœ¬çš„åå‘æç¤ºæˆ–é‡æ–°è®­ç»ƒDMsä»¥æ¶ˆé™¤æŸäº›ç‰¹å¾æˆ–æ ·æœ¬æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å®Œå…¨ä¸åŒçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¦å®šé›†ï¼ˆä¾‹å¦‚ï¼Œä¸å®‰å…¨çš„å›¾åƒã€ç‰ˆæƒæ•°æ®æˆ–éœ€è¦æ’é™¤çš„æ•°æ®ç‚¹ï¼‰ç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ï¼Œé¿å…æ•°æ®åˆ†å¸ƒçš„ç‰¹å®šåŒºåŸŸï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒDMsã€‚æˆ‘ä»¬æ­£å¼æ¨å¯¼äº†å®‰å…¨å’Œä¸å®‰å…¨é¢„æœŸå»å™ªæ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå½¢æˆäº†æˆ‘ä»¬çš„å®‰å…¨å»å™ªå™¨ï¼Œç¡®ä¿æœ€ç»ˆæ ·æœ¬è¿œç¦»è¦å¦å®šçš„åŒºåŸŸã€‚å—æ¨å¯¼çš„å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å®ç”¨ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ–‡æœ¬æ¡ä»¶ã€ç±»åˆ«æ¡ä»¶å’Œæ— æ¡ä»¶å›¾åƒç”Ÿæˆåœºæ™¯ä¸­æˆåŠŸäº§ç”Ÿäº†é«˜è´¨é‡æ ·æœ¬ï¼ŒåŒæ—¶é¿å…äº†æ•°æ®åˆ†å¸ƒçš„å¦å®šåŒºåŸŸã€‚è¿™äº›ç»“æœæš—ç¤ºäº†æˆ‘ä»¬çš„æ— è®­ç»ƒå®‰å…¨å»å™ªå™¨åœ¨æ›´å®‰å…¨åœ°ä½¿ç”¨DMæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08011v3">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong><br>æ‰©æ•£æ¨¡å‹ç»å¸¸è¢«æ»¥ç”¨ï¼Œäº§ç”Ÿä¸é€‚å½“ã€ä¸å®‰å…¨çš„ï¼ˆNSFWï¼‰å†…å®¹æˆ–è€…ç”Ÿæˆç‰ˆæƒææ–™å’Œä¸ªäººé—å¿˜çš„æ•°æ®ã€‚è®¸å¤šç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬è´Ÿå‘æç¤ºæˆ–é‡æ–°è®­ç»ƒæ¨¡å‹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å®Œå…¨ä¸åŒçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¦å®šé›†é¿å…æ•°æ®åˆ†å¸ƒç‰¹å®šåŒºåŸŸï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ã€‚æˆ‘ä»¬æ­£å¼æ¨å¯¼å®‰å…¨å’Œéå®‰å…¨å»å™ªæ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå¾—åˆ°æˆ‘ä»¬çš„å®‰å…¨å»å™ªå™¨ï¼Œç¡®ä¿æœ€ç»ˆæ ·æœ¬è¿œç¦»å¦å®šåŒºåŸŸã€‚å—åˆ°æ¨å¯¼çš„å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å®ç”¨ç®—æ³•ï¼ŒæˆåŠŸç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬æ¡ä»¶ã€ç±»åˆ«æ¡ä»¶å’Œæ— æ¡ä»¶å›¾åƒç”Ÿæˆåœºæ™¯ä¸­é¿å…å¦å®šåŒºåŸŸã€‚è¿™äº›ç»“æœæš—ç¤ºäº†æˆ‘ä»¬çš„æ— è®­ç»ƒå®‰å…¨å»å™ªå™¨åœ¨æ›´å®‰å…¨åœ°ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç»å¸¸è¢«è¯¯ç”¨ï¼Œäº§ç”Ÿä¸é€‚å½“æˆ–ä¸å®‰å…¨å†…å®¹ä»¥åŠç‰ˆæƒæˆ–ä¸ªäººéšç§ç›¸å…³ææ–™ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬è´Ÿå‘æç¤ºæˆ–é‡æ–°è®­ç»ƒæ¨¡å‹æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡åˆ©ç”¨å¦å®šé›†ç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æ­£å¼æ¨å¯¼å®‰å…¨å’Œéå®‰å…¨å»å™ªæ ·æœ¬é—´çš„å…³ç³»ï¼Œå½¢æˆå®‰å…¨å»å™ªå™¨æ¦‚å¿µã€‚</li>
<li>å®‰å…¨å»å™ªå™¨èƒ½ç¡®ä¿ç”Ÿæˆçš„æ ·æœ¬è¿œç¦»å¦å®šåŒºåŸŸã€‚</li>
<li>å¼€å‘å®ç”¨ç®—æ³•å¯åœ¨ä¸åŒåœºæ™¯ï¼ˆå¦‚æ–‡æœ¬æ¡ä»¶ã€ç±»åˆ«æ¡ä»¶å’Œæ— æ¡ä»¶å›¾åƒç”Ÿæˆï¼‰æˆåŠŸç”Ÿæˆé«˜è´¨é‡æ ·æœ¬å¹¶é¿å…å¦å®šåŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-881de76d8c7f1592bd0b30243d00adf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87e98c6fa2d3208afa40a593c2ee32cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc0cb3dd7b6b60b8330b57b077145259.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Factorized-Video-Autoencoders-for-Efficient-Generative-Modelling"><a href="#Factorized-Video-Autoencoders-for-Efficient-Generative-Modelling" class="headerlink" title="Factorized Video Autoencoders for Efficient Generative Modelling"></a>Factorized Video Autoencoders for Efficient Generative Modelling</h2><p><strong>Authors:Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia</strong></p>
<p>Latent variable generative models have emerged as powerful tools for generative tasks including image and video synthesis. These models are enabled by pretrained autoencoders that map high resolution data into a compressed lower dimensional latent space, where the generative models can subsequently be developed while requiring fewer computational resources. Despite their effectiveness, the direct application of latent variable models to higher dimensional domains such as videos continues to pose challenges for efficient training and inference. In this paper, we propose an autoencoder that projects volumetric data onto a four-plane factorized latent space that grows sublinearly with the input size, making it ideal for higher dimensional data like videos. The design of our factorized model supports straightforward adoption in a number of conditional generation tasks with latent diffusion models (LDMs), such as class-conditional generation, frame prediction, and video interpolation. Our results show that the proposed four-plane latent space retains a rich representation needed for high-fidelity reconstructions despite the heavy compression, while simultaneously enabling LDMs to operate with significant improvements in speed and memory. </p>
<blockquote>
<p>æ½œåœ¨å˜é‡ç”Ÿæˆæ¨¡å‹ä½œä¸ºç”Ÿæˆä»»åŠ¡ï¼ˆåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘åˆæˆï¼‰çš„å¼ºå¤§å·¥å…·å·²ç»å‡ºç°ã€‚è¿™äº›æ¨¡å‹ç”±é¢„è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨æä¾›æ”¯æŒï¼Œå°†é«˜åˆ†è¾¨ç‡æ•°æ®æ˜ å°„åˆ°å‹ç¼©çš„ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œåœ¨æ­¤ç©ºé—´ä¸­ï¼Œå¯ä»¥åœ¨è¾ƒå°‘çš„è®¡ç®—èµ„æºä¸‹å¼€å‘ç”Ÿæˆæ¨¡å‹ã€‚å°½ç®¡è¿™äº›æ¨¡å‹éå¸¸æœ‰æ•ˆï¼Œä½†ç›´æ¥å°†æ½œåœ¨å˜é‡æ¨¡å‹åº”ç”¨äºæ›´é«˜ç»´åº¦é¢†åŸŸï¼ˆå¦‚è§†é¢‘ï¼‰ä»ç„¶å¯¹é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†æå‡ºäº†æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ä½“ç§¯æ•°æ®æŠ•å½±åˆ°éšè¾“å…¥å¤§å°å¢é•¿è€Œäºšçº¿æ€§å¢é•¿çš„å››å¹³é¢åˆ†è§£æ½œåœ¨ç©ºé—´çš„è‡ªåŠ¨ç¼–ç å™¨ï¼Œä½¿å…¶æˆä¸ºé€‚åˆå¤„ç†é«˜ç»´æ•°æ®ï¼ˆå¦‚è§†é¢‘ï¼‰çš„ç†æƒ³é€‰æ‹©ã€‚æˆ‘ä»¬çš„åˆ†è§£æ¨¡å‹è®¾è®¡æ”¯æŒåœ¨å…·æœ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å¤šç§æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­ç›´æ¥ä½¿ç”¨ï¼Œä¾‹å¦‚ç±»åˆ«æ¡ä»¶ç”Ÿæˆã€å¸§é¢„æµ‹å’Œè§†é¢‘æ’å€¼ç­‰ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å››å¹³é¢æ½œåœ¨ç©ºé—´åœ¨ä¿ç•™é‡å»ºæ‰€éœ€ä¸°å¯Œè¡¨ç¤ºçš„åŒæ—¶å®ç°äº†å¼ºçƒˆå‹ç¼©ï¼ŒåŒæ—¶ä½¿æ½œåœ¨æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»¥é€Ÿåº¦å’Œå†…å­˜ä¸Šçš„æ˜¾è‘—æ”¹å–„è¿è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04452v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ½œåœ¨å˜é‡ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆç­‰ç”Ÿæˆä»»åŠ¡ä¸­çš„å¼ºå¤§ä½œç”¨ã€‚é€šè¿‡é¢„è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨å°†é«˜åˆ†è¾¨ç‡æ•°æ®æ˜ å°„åˆ°å‹ç¼©çš„ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œç”Ÿæˆæ¨¡å‹å¯ä»¥åœ¨è¾ƒå°‘çš„è®¡ç®—èµ„æºä¸‹å¼€å‘ã€‚é’ˆå¯¹ç›´æ¥åœ¨è§†é¢‘ç­‰æ›´é«˜ç»´åº¦é¢†åŸŸåº”ç”¨æ½œåœ¨å˜é‡æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†ä½“ç§¯æ•°æ®æŠ•å½±åˆ°å››å¹³é¢åˆ†è§£æ½œåœ¨ç©ºé—´çš„è‡ªåŠ¨ç¼–ç å™¨ï¼Œå…¶éšè¾“å…¥å¤§å°çš„å¢é•¿å‘ˆäºšçº¿æ€§ï¼Œé€‚ç”¨äºè§†é¢‘ç­‰æ›´é«˜ç»´åº¦æ•°æ®ã€‚è¯¥åˆ†è§£æ¨¡å‹è®¾è®¡æ”¯æŒåœ¨å¸¦æœ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„å¤šä¸ªæ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­çš„ç›´æ¥åº”ç”¨ï¼Œå¦‚ç±»åˆ«æ¡ä»¶ç”Ÿæˆã€å¸§é¢„æµ‹å’Œè§†é¢‘æ’å€¼ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å››å¹³é¢æ½œåœ¨ç©ºé—´åœ¨é‡åº¦å‹ç¼©çš„åŒæ—¶ä¿ç•™äº†ä¸°å¯Œçš„è¡¨ç¤ºï¼ŒåŒæ—¶ä½¿LDMsåœ¨é€Ÿåº¦å’Œå†…å­˜æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨å˜é‡ç”Ÿæˆæ¨¡å‹å·²æˆä¸ºå›¾åƒå’Œè§†é¢‘åˆæˆç­‰ç”Ÿæˆä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚</li>
<li>é¢„è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨å°†é«˜åˆ†è¾¨ç‡æ•°æ®æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œå‡å°‘è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>å°†ä½“ç§¯æ•°æ®æŠ•å½±åˆ°å››å¹³é¢åˆ†è§£æ½œåœ¨ç©ºé—´çš„æ–¹æ³•é€‚ç”¨äºæ›´é«˜ç»´åº¦çš„æ•°æ®ï¼Œå¦‚è§†é¢‘ã€‚</li>
<li>å››å¹³é¢åˆ†è§£æ¨¡å‹è®¾è®¡æ”¯æŒåœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­çš„ç›´æ¥åº”ç”¨ï¼Œå¦‚ç±»åˆ«æ¡ä»¶ç”Ÿæˆã€å¸§é¢„æµ‹å’Œè§†é¢‘æ’å€¼ã€‚</li>
<li>å››å¹³é¢æ½œåœ¨ç©ºé—´åœ¨å‹ç¼©çš„åŒæ—¶ä¿ç•™äº†ä¸°å¯Œçš„è¡¨ç¤ºï¼Œå¯å®ç°é«˜ä¿çœŸé‡æ„ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨é€Ÿåº¦å’Œå†…å­˜æ–¹é¢çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-afb3e0f9e81b089497a900201e0aa3c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b33f3785bff1473a1575453ad2f5344e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74385ab96342973cc77348707169580d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dd482871d7629b31eeccafe9f985a0b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Plug-and-Play-image-restoration-with-Stochastic-deNOising-REgularization"><a href="#Plug-and-Play-image-restoration-with-Stochastic-deNOising-REgularization" class="headerlink" title="Plug-and-Play image restoration with Stochastic deNOising REgularization"></a>Plug-and-Play image restoration with Stochastic deNOising REgularization</h2><p><strong>Authors:Marien Renaud, Jean Prost, Arthur Leclaire, Nicolas Papadakis</strong></p>
<p>Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, both quantitatively and qualitatively. </p>
<blockquote>
<p>Plug-and-Playï¼ˆPnPï¼‰ç®—æ³•æ˜¯ä¸€ç±»è¿­ä»£ç®—æ³•ï¼Œé€šè¿‡ç»“åˆç‰©ç†æ¨¡å‹å’Œæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ï¼Œè§£å†³å›¾åƒåé—®é¢˜ã€‚å°½ç®¡å®ƒä»¬èƒ½äº§ç”Ÿä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒæ¢å¤ç»“æœï¼Œä½†è¿™äº›ç®—æ³•ä¾èµ–äºåœ¨éæ ‡å‡†å»å™ªå™¨ä¸Šå¯¹å›¾åƒçš„ä½¿ç”¨ï¼Œéšç€è¿­ä»£çš„è¿›è¡Œï¼Œå›¾åƒè¶Šæ¥è¶Šä¸å™ªå£°åŒ–ï¼Œè¿™ä¸åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰çš„è¿‘æœŸç®—æ³•å½¢æˆå¯¹æ¯”ï¼Œå…¶ä¸­å»å™ªå™¨åªåº”ç”¨äºé‡æ–°åŠ å™ªå£°çš„å›¾åƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„PnPæ¡†æ¶ï¼Œç§°ä¸ºéšæœºå»å™ªæ­£åˆ™åŒ–ï¼ˆSNOREï¼‰ï¼Œå®ƒä»…åœ¨å…·æœ‰é€‚å½“æ°´å¹³çš„å™ªå£°å›¾åƒä¸Šåº”ç”¨å»å™ªå™¨ã€‚å®ƒåŸºäºæ˜¾å¼éšæœºæ­£åˆ™åŒ–ï¼Œå¯¼è‡´ä¸€ç§è§£å†³ä¸é€‚å®šåé—®é¢˜çš„éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚æˆ‘ä»¬æä¾›äº†è¯¥ç®—æ³•åŠå…¶é€€ç«æ‰©å±•çš„æ”¶æ•›æ€§åˆ†æã€‚å®éªŒè¯æ˜ï¼ŒSNOREåœ¨å»æ¨¡ç³Šå’Œä¿®å¤ä»»åŠ¡ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›å’Œä¼˜åŠ¿ï¼Œæ— è®ºæ˜¯åœ¨æ•°é‡ä¸Šè¿˜æ˜¯åœ¨è´¨é‡ä¸Šéƒ½æ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.01779v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å›¾åƒé€†é—®é¢˜å¤„ç†ä¸­ï¼ŒPlug-and-Playï¼ˆPnPï¼‰ç®—æ³•ç»“åˆäº†ç‰©ç†æ¨¡å‹å’Œæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ã€‚é’ˆå¯¹PnPç®—æ³•åœ¨è¿­ä»£è¿‡ç¨‹ä¸­å¯¹ä¸åŒå™ªå£°æ°´å¹³å›¾åƒçš„éæ ‡å‡†å»å™ªä½¿ç”¨æ–¹å¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„PnPæ¡†æ¶â€”â€”Stochastic deNOising REgularizationï¼ˆSNOREï¼‰ã€‚SNOREåªåœ¨é€‚å½“å™ªå£°æ°´å¹³çš„å›¾åƒä¸Šåº”ç”¨å»å™ªå™¨ï¼ŒåŸºäºæ˜ç¡®çš„éšæœºæ­£åˆ™åŒ–ï¼Œä¸ºè§£å†³ä¸é€‚å®šçš„é€†é—®é¢˜æä¾›äº†éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚å®éªŒè¯æ˜ï¼ŒSNOREåœ¨å»æ¨¡ç³Šå’Œè¡¥å…¨ä»»åŠ¡ä¸Šå‡å…·æœ‰è‰¯å¥½çš„ç«äº‰æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PnPç®—æ³•ç»“åˆç‰©ç†æ¨¡å‹å’Œæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå›¾åƒé€†é—®é¢˜çš„å¤„ç†ã€‚</li>
<li>PnPç®—æ³•åœ¨è¿­ä»£è¿‡ç¨‹ä¸­å¯¹å™ªå£°é€æ¸å‡å¼±çš„å›¾åƒä½¿ç”¨å»å™ªå™¨çš„æ–¹å¼ä¸åŸºäºDiffusion Modelsçš„ç®—æ³•ä¸åŒã€‚</li>
<li>æå‡ºäº†æ–°çš„PnPæ¡†æ¶SNOREï¼Œåªåœ¨é€‚å½“å™ªå£°æ°´å¹³çš„å›¾åƒä¸Šåº”ç”¨å»å™ªå™¨ã€‚</li>
<li>SNOREåŸºäºæ˜ç¡®çš„éšæœºæ­£åˆ™åŒ–ï¼Œä¸ºè§£å†³ä¸é€‚å®šçš„é€†é—®é¢˜æä¾›äº†éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚</li>
<li>SNOREåœ¨æ”¶æ•›æ€§æ–¹é¢è¿›è¡Œäº†åˆ†æï¼Œå¹¶æä¾›äº†é€€ç«æ‰©å±•çš„æ–¹æ³•ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSNOREåœ¨å»æ¨¡ç³Šå’Œè¡¥å…¨ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.01779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d89da227791bc3737d3c052bf2b9e22.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-22c307ff2862f361125a11c7c90838b0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  SpectralAR Spectral Autoregressive Visual Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5df2a46b4102e410cb6aed2c254111aa.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  PointGS Point Attention-Aware Sparse View Synthesis with Gaussian   Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
