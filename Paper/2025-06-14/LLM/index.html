<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  AutoMind Adaptive Knowledgeable Agent for Automated Data Science">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-90de5cd9ba73d7e8bc828d401a22efc8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    71 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-14-æ›´æ–°"><a href="#2025-06-14-æ›´æ–°" class="headerlink" title="2025-06-14 æ›´æ–°"></a>2025-06-14 æ›´æ–°</h1><h2 id="AutoMind-Adaptive-Knowledgeable-Agent-for-Automated-Data-Science"><a href="#AutoMind-Adaptive-Knowledgeable-Agent-for-Automated-Data-Science" class="headerlink" title="AutoMind: Adaptive Knowledgeable Agent for Automated Data Science"></a>AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</h2><p><strong>Authors:Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨è§£å†³ç°å®ä¸–ç•Œçš„æ•°æ®ç§‘å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚LLMé©±åŠ¨çš„æ•°æ®ç§‘å­¦ä»£ç†è™½ç„¶æœ‰æœ›è‡ªåŠ¨åŒ–æ•´ä¸ªæœºå™¨å­¦ä¹ ç®¡é“ï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„å®é™…åº”ç”¨æ•ˆæœä»ç„¶æœ‰é™ã€‚ç°æœ‰çš„æ¡†æ¶ä¾èµ–äºåƒµåŒ–ã€é¢„å…ˆå®šä¹‰çš„å·¥ä½œæµç¨‹å’Œç¼ºä¹çµæ´»æ€§çš„ç¼–ç ç­–ç•¥ï¼Œå› æ­¤åªèƒ½åœ¨ç›¸å¯¹ç®€å•ã€ç»å…¸çš„é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè€Œæ— æ³•æ•æ‰äººç±»ä»ä¸šè€…ä¸ºå¤æ‚ã€åˆ›æ–°ä»»åŠ¡å¸¦æ¥çš„å®é™…ç»éªŒã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†AutoMindï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªé€‚åº”çš„çŸ¥è¯†å‹LLMä»£ç†æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®è¿›å±•å…‹æœäº†è¿™äº›ä¸è¶³ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªç²¾é€‰çš„ä¸“å®¶çŸ¥è¯†åº“ï¼Œä¸ºä»£ç†æä¾›é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŸºäºçŸ¥è¯†çš„æ ‘æœç´¢ç®—æ³•ï¼Œæˆ˜ç•¥æ€§åœ°æ¢ç´¢å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼›ï¼ˆ3ï¼‰ä¸€ç§è‡ªé€‚åº”ç¼–ç ç­–ç•¥ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€è°ƒæ•´ä»£ç ç”Ÿæˆã€‚åœ¨ä¸¤ä¸ªè‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAutoMindç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯åŸºçº¿å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯å®äº†å…¶åœ¨æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè´¨é‡ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œçªå‡ºäº†AutoMindæ˜¯æœç€å…¨è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„ç¨³å¥æ–¹å‘è¿ˆå‡ºçš„é«˜æ•ˆçš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10974v1">PDF</a> Ongoing work. Code is at <a target="_blank" rel="noopener" href="https://github.com/innovatingAI/AutoMind">https://github.com/innovatingAI/AutoMind</a></p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³ç°å®æ•°æ®ç§‘å­¦é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿä»¥è‡ªåŠ¨åŒ–æ•´ä¸ªæœºå™¨å­¦ä¹ ç®¡é“ä¸ºæ‰¿è¯ºã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å®é™…æœ‰æ•ˆæ€§å—é™äºç°æœ‰æ¡†æ¶çš„åƒµåŒ–é¢„å®šä¹‰å·¥ä½œæµç¨‹å’Œç¼ºä¹çµæ´»æ€§çš„ç¼–ç ç­–ç•¥ï¼Œåªèƒ½åœ¨ç›¸å¯¹ç®€å•çš„é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œæ— æ³•æ•è·äººç±»å®è·µè€…åœ¨å¤æ‚åˆ›æ–°ä»»åŠ¡ä¸­çš„ç»éªŒçŸ¥è¯†ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§è‡ªé€‚åº”çŸ¥è¯†å‹LLMä»£ç†æ¡†æ¶AutoMindï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®è¿›å±•å…‹æœäº†è¿™äº›ç¼ºé™·ï¼šä¸€æ˜¯ä¸“ä¸šçŸ¥è¯†çš„çŸ¥è¯†åº“ï¼Œä½¿ä»£ç†èå…¥é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼›äºŒæ˜¯æ™ºèƒ½çŸ¥è¯†æ ‘æœç´¢ç®—æ³•ï¼Œæˆ˜ç•¥æ€§åœ°æ¢ç´¢å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼›ä¸‰æ˜¯è‡ªé€‚åº”ç¼–ç ç­–ç•¥ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€è°ƒæ•´ä»£ç ç”Ÿæˆã€‚åœ¨ä¸¤é¡¹è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°è¡¨æ˜ï¼ŒAutoMindç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯åŸºçº¿å…·æœ‰å“è¶Šæ€§èƒ½ã€‚é™„åŠ åˆ†æè¯å®äº†å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè§£å†³æ–¹æ¡ˆè´¨é‡çš„æœ‰åˆ©å› ç´ ï¼Œå‡¸æ˜¾AutoMindä½œä¸ºå®ç°å…¨è‡ªåŠ¨æ•°æ®ç§‘å­¦çš„ç¨³å¥ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨è§£å†³ç°å®æ•°æ®ç§‘å­¦é—®é¢˜æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨å®é™…åº”ç”¨çš„å±€é™æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ¡†æ¶ä¾èµ–äºé¢„å®šä¹‰çš„å·¥ä½œæµç¨‹å’Œç¼ºä¹çµæ´»æ€§çš„ç¼–ç ç­–ç•¥ï¼Œéš¾ä»¥å¤„ç†å¤æ‚ä»»åŠ¡ã€‚</li>
<li>AutoMindæ˜¯ä¸€ç§æ–°å‹çš„LLMä»£ç†æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°æ¥å…‹æœè¿™äº›ç¼ºé™·ã€‚</li>
<li>AutoMindå…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„çŸ¥è¯†åº“ã€æ™ºèƒ½çŸ¥è¯†æ ‘æœç´¢ç®—æ³•å’Œè‡ªé€‚åº”ç¼–ç ç­–ç•¥ã€‚</li>
<li>åœ¨è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAutoMindè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>é™„åŠ åˆ†æè¯å®äº†AutoMindçš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè§£å†³æ–¹æ¡ˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ddc4263f6afc7372e07b1d8f8e5b0a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c908ddff0e5cf9fad3fc48fd56414739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-556dcd236ebee08fc1fc76df7990070e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-045489ee1597b32415081a739233afcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6455bcf45351a88702ae486eaa11f212.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Farseer-A-Refined-Scaling-Law-in-Large-Language-Models"><a href="#Farseer-A-Refined-Scaling-Law-in-Large-Language-Models" class="headerlink" title="Farseer: A Refined Scaling Law in Large Language Models"></a>Farseer: A Refined Scaling Law in Large Language Models</h2><p><strong>Authors:Houyi Li, Wenzhen Zheng, Qiufeng Wang, Zhenyu Ding, Haoying Wang, Zili Wang, Shijie Xuyang, Ning Ding, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang</strong></p>
<p>Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation. To bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface $L(N,D)$, Farseer achieves a significantly better fit to empirical data than prior laws (e.g., Chinchillaâ€™s law). Our methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, improving upon Chinchillaâ€™s law by reducing extrapolation error by 433%. This allows for the reliable evaluation of competing training strategies across all $(N,D)$ settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. Furthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are comprehensively open-sourcing all models, data, results, and logs at <a target="_blank" rel="noopener" href="https://github.com/Farseer-Scaling-Law/Farseer">https://github.com/Farseer-Scaling-Law/Farseer</a> to foster further research. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ä»·æä¸ºé«˜æ˜‚ï¼Œå½¢æˆäº†ä¸€ä¸ªå…³é”®çš„æ‰©å±•å·®è·ï¼Œå°è§„æ¨¡å®éªŒçš„è§è§£å¾€å¾€æ— æ³•è½¬ç§»åˆ°èµ„æºå¯†é›†å‹çš„ç”Ÿäº§ç³»ç»Ÿä¸­ï¼Œä»è€Œé˜»ç¢äº†æœ‰æ•ˆçš„åˆ›æ–°ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Farseerï¼Œä¸€ä¸ªæ–°å‹ä¸”ç²¾ç»†çš„æ‰©å±•å®šå¾‹ï¼Œå¯ä»¥åœ¨å„ç§è§„æ¨¡ä¸Šæä¾›å¢å¼ºçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚é€šè¿‡ç³»ç»Ÿåœ°æ„å»ºæ¨¡å‹æŸå¤±æ›²é¢$L(N,D)$ï¼ŒFarseeræ¯”ä»¥å¾€å®šå¾‹ï¼ˆä¾‹å¦‚Chinchillaå®šå¾‹ï¼‰æ›´èƒ½è´´åˆå®é™…æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†å‡†ç¡®ã€ç¨³å¥ä¸”é«˜åº¦é€šç”¨çš„é¢„æµ‹ï¼Œè¡¨ç°å‡ºå‡ºè‰²çš„å¤–æ¨èƒ½åŠ›ï¼Œå°†Chinchillaå®šå¾‹çš„å¤–æ¨è¯¯å·®é™ä½äº†43.3%ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿåœ¨æ‰€æœ‰$(N,D)$è®¾ç½®ä¸‹å¯é åœ°è¯„ä¼°ç«äº‰æ€§çš„è®­ç»ƒç­–ç•¥ï¼Œä»å°è§„æ¨¡æ¶ˆèç ”ç©¶å¾—å‡ºçš„ç»“è®ºå¯ä»¥è‡ªä¿¡åœ°å¤–æ¨åˆ°é¢„æµ‹å¤§è§„æ¨¡æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFarseerå¯¹ç°ä»£LLMè®­ç»ƒæä¾›äº†æœ€ä½³è®¡ç®—åˆ†é…çš„æ–°è§è§£ï¼Œæ›´å¥½åœ°åæ˜ äº†å…¶å¾®å¦™éœ€æ±‚ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†å¤§çº¦1000ä¸ªä¸åŒè§„æ¨¡å’Œé…ç½®çš„LLMæ¨¡å‹å¥—ä»¶ï¼Œæ¶ˆè€—äº†å¤§çº¦3ç™¾ä¸‡NVIDIA H100 GPUå°æ—¶ã€‚æˆ‘ä»¬å…¨é¢åœ°åœ¨<a target="_blank" rel="noopener" href="https://github.com/Farseer-Scaling-Law/Farseer%E5%BC%80%E6%BA%90%E6%89%80%E6%9C%89%E6%A8%A1%E5%9E%8B%E3%80%81%E6%95%B0%E6%8D%AE%E3%80%81%E7%BB%93%E6%9E%9C%E5%92%8C%E6%97%A5%E5%BF%97%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/Farseer-Scaling-Law/Farseerå¼€æºæ‰€æœ‰æ¨¡å‹ã€æ•°æ®ã€ç»“æœå’Œæ—¥å¿—ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10972v1">PDF</a> 34</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹LLMè®­ç»ƒæˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºæ–°çš„è§„æ¨¡å®šå¾‹Farseerï¼Œå…·æœ‰æ›´é«˜çš„é¢„æµ‹ç²¾åº¦å’Œæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºæ¨¡å‹æŸå¤±è¡¨é¢ï¼ŒFarseerèƒ½æ›´å¥½åœ°æ‹Ÿåˆå®è¯æ•°æ®ï¼Œå¹¶æä¾›äº†å…³äºè®¡ç®—åˆ†é…çš„æ–°è§è§£ã€‚é€šè¿‡è®­ç»ƒå¤§é‡LLMéªŒè¯äº†Farseerçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å…¨é¢å¼€æºæ‰€æœ‰æ¨¡å‹å’Œæ—¥å¿—ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMè®­ç»ƒå­˜åœ¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œå½±å“äº†åˆ›æ–°æ•ˆç‡ã€‚</li>
<li>Farseeræ˜¯ä¸€ç§æ–°çš„è§„æ¨¡å®šå¾‹ï¼Œèƒ½æé«˜LLMè®­ç»ƒçš„é¢„æµ‹ç²¾åº¦ã€‚</li>
<li>Farseeré€šè¿‡æ„å»ºæ¨¡å‹æŸå¤±è¡¨é¢L(N,D)æ¥æ›´å¥½åœ°æ‹Ÿåˆå®è¯æ•°æ®ã€‚</li>
<li>Farseerç›¸å¯¹äºä¹‹å‰çš„è§„æ¨¡å®šå¾‹ï¼ˆå¦‚Chinchillaâ€™s lawï¼‰å…·æœ‰æ›´å¥½çš„è¡¨ç°ã€‚</li>
<li>Farseerèƒ½å¤Ÿå‡†ç¡®ã€ç¨³å¥åœ°é¢„æµ‹ä¸åŒè®­ç»ƒç­–ç•¥çš„æ•ˆæœï¼Œå¹¶å±•ç°å‡ºä¼˜ç§€çš„å¤–æ¨èƒ½åŠ›ã€‚</li>
<li>Farseeræä¾›äº†å…³äºè®¡ç®—åˆ†é…çš„æ–°è§è§£ï¼Œæ›´å¥½åœ°åæ˜ äº†ç°ä»£LLMè®­ç»ƒçš„å¤æ‚éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-440cd4029d22eadfb1481f678b1fc795.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-756636504420a5ef96373e09d3c60804.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c8b79f245270139f51a819f45150daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e1c42d5e2b3eeb45611fbfbc974135.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66c10ce38446a83887185b48fc7a6313.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Beyond-Attention-or-Similarity-Maximizing-Conditional-Diversity-for-Token-Pruning-in-MLLMs"><a href="#Beyond-Attention-or-Similarity-Maximizing-Conditional-Diversity-for-Token-Pruning-in-MLLMs" class="headerlink" title="Beyond Attention or Similarity: Maximizing Conditional Diversity for   Token Pruning in MLLMs"></a>Beyond Attention or Similarity: Maximizing Conditional Diversity for   Token Pruning in MLLMs</h2><p><strong>Authors:Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang</strong></p>
<p>In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named CDPruner, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95% and CUDA latency by 78%, while maintaining 94% of the original accuracy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Theia-4869/CDPruner">https://github.com/Theia-4869/CDPruner</a>. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ï¼Œè¾“å…¥è§†è§‰æ ‡è®°çš„é•¿åº¦é€šå¸¸æ˜æ˜¾é•¿äºæ–‡æœ¬æ ‡è®°çš„é•¿åº¦ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬å¾ˆé«˜ã€‚è®¸å¤šç ”ç©¶æ—¨åœ¨é€šè¿‡åˆ é™¤å†—ä½™çš„è§†è§‰æ ‡è®°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºåŸºäºæ³¨æ„åŠ›çš„ä¿®å‰ªï¼Œè¿™ç§æ–¹æ³•ä¼šä¿ç•™è®¸å¤šé‡å¤æ ‡è®°ï¼Œè¦ä¹ˆä½¿ç”¨åŸºäºç›¸ä¼¼åº¦çš„ä¿®å‰ªï¼Œå¿½è§†äº†æŒ‡ä»¤ç›¸å…³æ€§ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¶…è¶Šäº†æ³¨æ„åŠ›å’Œç›¸ä¼¼åº¦ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCDPrunerçš„æ–°å‹è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ€å¤§é™åº¦åœ°æé«˜äº†ä¿ç•™æ ‡è®°çš„æ¡ä»¶å¤šæ ·æ€§ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†åŸºäºæŒ‡ä»¤çš„è§†è§‰æ ‡è®°ä¹‹é—´çš„æ¡ä»¶ç›¸ä¼¼æ€§ï¼Œç„¶åä½¿ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰é‡æ–°è¡¨è¿°æ ‡è®°ä¿®å‰ªé—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–æ‰€é€‰å­é›†çš„æ¡ä»¶å¤šæ ·æ€§ã€‚æå‡ºçš„CDPruneræ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³ï¼Œå¯è½»æ¾åº”ç”¨äºå„ç§MLLMã€‚åœ¨å¤šç§MLLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCDPruneråœ¨å„é¡¹è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æŠ€æœ¯å‰æ²¿ã€‚é€šè¿‡DPPæœ€å¤§åŒ–æ¡ä»¶å¤šæ ·æ€§ï¼Œæ‰€é€‰å­é›†å¯ä»¥æ›´å¥½åœ°ä»£è¡¨è¾“å…¥å›¾åƒåŒæ—¶ç´§å¯†éµå¾ªç”¨æˆ·æŒ‡ä»¤ï¼Œä»è€Œåœ¨å…·æœ‰é«˜é™ä½ç‡çš„æƒ…å†µä¸‹ä¿æŒå‡ºè‰²çš„æ€§èƒ½ã€‚å°†CDPruneråº”ç”¨äºLLaVAæ—¶ï¼Œå®ƒå‡å°‘äº†95%çš„FLOPså’Œ78%çš„CUDAå»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒ94%çš„åŸå§‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Theia-4869/CDPruner%E3%80%82">https://github.com/Theia-4869/CDPrunerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10967v1">PDF</a> 22 pages, 5 figures, code: <a target="_blank" rel="noopener" href="https://github.com/Theia-4869/CDPruner">https://github.com/Theia-4869/CDPruner</a>,   project page: <a target="_blank" rel="noopener" href="https://theia-4869.github.io/CDPruner">https://theia-4869.github.io/CDPruner</a></p>
<p><strong>Summary</strong><br>åœ¨å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œè§†è§‰æ ‡è®°è¾“å…¥é•¿åº¦å¾€å¾€è¿œè¶…æ–‡æœ¬ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬è¾ƒé«˜ã€‚ç°æœ‰ç ”ç©¶è¯•å›¾é€šè¿‡å»é™¤å†—ä½™è§†è§‰æ ‡è®°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCDPrunerçš„è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–ä¿ç•™æ ‡è®°çš„æ¡ä»¶å¤šæ ·æ€§æ¥é€‰æ‹©å­é›†ã€‚è¯¥æ–¹æ³•å®šä¹‰äº†åœ¨æŒ‡ä»¤æ¡ä»¶ä¸‹çš„è§†è§‰æ ‡è®°é—´çš„æ¡ä»¶ç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰é‡æ–°è¡¨è¿°æ ‡è®°ä¿®å‰ªé—®é¢˜ã€‚CDPruneræ–¹æ³•æ— éœ€è®­ç»ƒä¸”é€‚ç”¨äºå„ç§MLLMsã€‚å®éªŒè¡¨æ˜ï¼ŒCDPruneråœ¨å„é¡¹è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æœ€æ–°çºªå½•ã€‚é€šè¿‡æœ€å¤§åŒ–æ¡ä»¶å¤šæ ·æ€§ï¼Œæ‰€é€‰å­é›†èƒ½æ›´å¥½åœ°ä»£è¡¨è¾“å…¥å›¾åƒå¹¶ç´§å¯†éµå¾ªç”¨æˆ·æŒ‡ä»¤ï¼Œå³ä½¿åœ¨é«˜ç¼©å‡æ¯”ä¾‹ä¸‹ä¹Ÿèƒ½ä¿æŒå¼ºåŠ²æ€§èƒ½ã€‚åœ¨LLaVAä¸Šåº”ç”¨CDPrunerå¯å°†FLOPså‡å°‘95%ï¼ŒCUDAå»¶è¿Ÿå‡å°‘78%ï¼ŒåŒæ—¶ä¿æŒ94%çš„åŸå§‹å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsä¸­è§†è§‰æ ‡è®°è¾“å…¥é•¿åº¦é€šå¸¸è¿œè¶…æ–‡æœ¬ï¼Œå¯¼è‡´é«˜æ¨ç†æˆæœ¬ã€‚</li>
<li>ç°æœ‰è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œå¦‚ä¿ç•™è¿‡å¤šé‡å¤æ ‡è®°æˆ–å¿½è§†æŒ‡ä»¤ç›¸å…³æ€§ã€‚</li>
<li>CDPruneræ–¹æ³•é€šè¿‡æœ€å¤§åŒ–ä¿ç•™æ ‡è®°çš„æ¡ä»¶å¤šæ ·æ€§æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>CDPrunerå®šä¹‰äº†åœ¨æŒ‡ä»¤æ¡ä»¶ä¸‹çš„è§†è§‰æ ‡è®°é—´çš„æ¡ä»¶ç›¸ä¼¼æ€§ã€‚</li>
<li>ä½¿ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰é‡æ–°è¡¨è¿°æ ‡è®°ä¿®å‰ªé—®é¢˜ã€‚</li>
<li>CDPruneræ–¹æ³•æ— éœ€è®­ç»ƒï¼Œé€‚ç”¨äºå„ç§MLLMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b95702e5f8f700a654a7d9bda4636f58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-847badb8da0daa52ad6186866570b4dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45e2878ebca8f2e6b8aaec9a41dbb7fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41da0335f18516dd6c57eeb496574ad4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a166bd06126ff11a8ed63e7b5caec8ed.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GENMANIP-LLM-driven-Simulation-for-Generalizable-Instruction-Following-Manipulation"><a href="#GENMANIP-LLM-driven-Simulation-for-Generalizable-Instruction-Following-Manipulation" class="headerlink" title="GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following   Manipulation"></a>GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following   Manipulation</h2><p><strong>Authors:Ning Gao, Yilun Chen, Shuai Yang, Xinyi Chen, Yang Tian, Hao Li, Haifeng Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang</strong></p>
<p>Robotic manipulation in real-world settings remains challenging, especially regarding robust generalization. Existing simulation platforms lack sufficient support for exploring how policies adapt to varied instructions and scenarios. Thus, they lag behind the growing interest in instruction-following foundation models like LLMs, whose adaptability is crucial yet remains underexplored in fair comparisons. To bridge this gap, we introduce GenManip, a realistic tabletop simulation platform tailored for policy generalization studies. It features an automatic pipeline via LLM-driven task-oriented scene graph to synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To systematically assess generalization, we present GenManip-Bench, a benchmark of 200 scenarios refined via human-in-the-loop corrections. We evaluate two policy types: (1) modular manipulation systems integrating foundation models for perception, reasoning, and planning, and (2) end-to-end policies trained through scalable data collection. Results show that while data scaling benefits end-to-end methods, modular systems enhanced with foundation models generalize more effectively across diverse scenarios. We anticipate this platform to facilitate critical insights for advancing policy generalization in realistic conditions. Project Page: <a target="_blank" rel="noopener" href="https://genmanip.axi404.top/">https://genmanip.axi404.top/</a>. </p>
<blockquote>
<p>æœºå™¨äººæ“ä½œåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„è¡¨ç°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨³å¥æ³›åŒ–æ–¹é¢ã€‚ç°æœ‰çš„æ¨¡æ‹Ÿå¹³å°ç¼ºä¹è¶³å¤Ÿçš„æ”¯æŒæ¥æ¢ç´¢ç­–ç•¥å¦‚ä½•é€‚åº”ä¸åŒçš„æŒ‡ä»¤å’Œåœºæ™¯ã€‚å› æ­¤ï¼Œå®ƒä»¬è½åäºå¯¹æŒ‡ä»¤éµå¾ªåŸºç¡€æ¨¡å‹ï¼ˆå¦‚LLMï¼‰æ—¥ç›Šå¢é•¿çš„å…´è¶£ï¼Œè¿™äº›æ¨¡å‹çš„é€‚åº”æ€§è‡³å…³é‡è¦ï¼Œä½†åœ¨å…¬å¹³æ¯”è¾ƒä¸­ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†GenManipï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºç­–ç•¥æ³›åŒ–ç ”ç©¶è®¾è®¡çš„ç°å®æ¡Œé¢æ¨¡æ‹Ÿå¹³å°ã€‚å®ƒé€šè¿‡LLMé©±åŠ¨çš„ä»»åŠ¡å¯¼å‘åœºæ™¯å›¾ï¼Œå»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨ç®¡é“ï¼Œåˆ©ç”¨10Kä¸ªæ³¨é‡Šçš„3Då¯¹è±¡èµ„äº§åˆæˆå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GenManip-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡äººæœºå¾ªç¯æ ¡æ­£ç²¾ç»†æŒ‘é€‰çš„200ä¸ªåœºæ™¯åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§ç­–ç•¥ç±»å‹ï¼šï¼ˆ1ï¼‰é›†æˆæ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’åŸºç¡€æ¨¡å‹çš„æ¨¡å—åŒ–æ“ä½œç³»ç»Ÿï¼›ï¼ˆ2ï¼‰é€šè¿‡å¯æ‰©å±•çš„æ•°æ®æ”¶é›†è¿›è¡Œç«¯åˆ°ç«¯ç­–ç•¥è®­ç»ƒã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ•°æ®è§„æ¨¡åŒ–å¯¹ç«¯åˆ°ç«¯æ–¹æ³•æœ‰ç›Šï¼Œä½†ç»“åˆäº†åŸºç¡€æ¨¡å‹çš„æ¨¡å—åŒ–ç³»ç»Ÿåœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›æ›´ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬é¢„æœŸè¿™ä¸ªå¹³å°å°†ä¸ºåœ¨çœŸå®æ¡ä»¶ä¸‹æå‡ç­–ç•¥æ³›åŒ–èƒ½åŠ›æä¾›å…³é”®è§è§£ã€‚é¡¹ç›®é¡µé¢ï¼š[ç½‘å€]ï¼ˆå·²è¶…é“¾æ¥ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10966v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£æ¨¡æ‹Ÿå¹³å°GenManipå¡«è¡¥äº†åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸçš„ç©ºç™½ï¼Œå®ƒç‰¹åˆ«é€‚ç”¨äºç­–ç•¥é€šç”¨æ€§ç ”ç©¶ã€‚è¯¥å¹³å°é€šè¿‡LLMé©±åŠ¨çš„é¢å‘ä»»åŠ¡åœºæ™¯å›¾å®ç°è‡ªåŠ¨åŒ–ç®¡é“ï¼Œèƒ½å¤Ÿåˆæˆå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚è¯¥å¹³å°è¿˜æä¾›åŸºå‡†æµ‹è¯•GenManip-Benchä»¥ç³»ç»Ÿè¯„ä¼°ç­–ç•¥æ³›åŒ–èƒ½åŠ›ï¼Œä¸”èƒ½æµ‹è¯•ä¸¤ç±»ç­–ç•¥â€”â€”é›†æˆæ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’çš„åŸºç¡€æ¨¡å‹çš„æ¨¡å—åŒ–æ“ä½œç³»ç»Ÿå’Œé€šè¿‡å¤§è§„æ¨¡æ•°æ®é‡‡é›†è®­ç»ƒå¾—åˆ°çš„ç«¯åˆ°ç«¯ç­–ç•¥ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å—åŒ–ç³»ç»Ÿåœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚æœŸå¾…æ­¤å¹³å°èƒ½ä¸ºç°å®æ¡ä»¶ä¸‹çš„ç­–ç•¥æ³›åŒ–ç ”ç©¶æä¾›é‡è¦è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GenManipæ˜¯ä¸€ä¸ªé¢å‘ç­–ç•¥é€šç”¨åŒ–ç ”ç©¶çš„ç°å®é¤æ¡Œæ¨¡æ‹Ÿå¹³å°ã€‚</li>
<li>å¹³å°é‡‡ç”¨LLMé©±åŠ¨çš„é¢å‘ä»»åŠ¡åœºæ™¯å›¾æŠ€æœ¯ï¼Œå¯åˆæˆå¤§è§„æ¨¡å¤šæ ·åŒ–ä»»åŠ¡ã€‚</li>
<li>GenManip-Benchä½œä¸ºåŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶çš„ä¸¤ç±»ç­–ç•¥åŒ…æ‹¬æ¨¡å—åŒ–æ“ä½œç³»ç»Ÿå’Œç«¯åˆ°ç«¯ç­–ç•¥ã€‚</li>
<li>æ¨¡å—åŒ–ç­–ç•¥åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>å¹³å°é¢„æœŸä¸ºç°å®æ¡ä»¶ä¸‹çš„ç­–ç•¥æ³›åŒ–ç ”ç©¶æä¾›å…³é”®è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-930cf3e501c276f2295517a26de03686.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e97c9b5c5c3b2278ae2a68cb63886d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51d6f42eda8a079396d632f017da66b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0093ae059216994a7fa1973beec48672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62b08c946ce9e421d3a8b2cc6213846c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3070527335c8164d571b45af6bc5e32d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ChineseHarm-Bench-A-Chinese-Harmful-Content-Detection-Benchmark"><a href="#ChineseHarm-Bench-A-Chinese-Harmful-Content-Detection-Benchmark" class="headerlink" title="ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark"></a>ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark</h2><p><strong>Authors:Kangwei Liu, Siyuan Cheng, Bozhong Tian, Xiaozhuan Liang, Yuyang Yin, Meng Han, Ningyu Zhang, Bryan Hooi, Xi Chen, Shumin Deng</strong></p>
<p>Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/ChineseHarm-bench">https://github.com/zjunlp/ChineseHarm-bench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨æ£€æµ‹æœ‰å®³å†…å®¹ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼ŒååŠ©ç®¡ç†å‘˜è¯†åˆ«æ”¿ç­–è¿è§„è¡Œä¸ºï¼Œæé«˜å†…å®¹å®¡æŸ¥çš„æ•´ä½“æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç›®å‰æœ‰å®³å†…å®¹æ£€æµ‹çš„èµ„æºä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œä¸­æ–‡æ•°æ®é›†ä»ç„¶ç¨€ç¼ºä¸”èŒƒå›´æœ‰é™ã€‚æˆ‘ä»¬ä¸ºä¸­æ–‡å†…å®¹å±å®³æ£€æµ‹æä¾›äº†ä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šæ³¨é‡Šçš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ¶µç›–äº†å…­ä¸ªä»£è¡¨æ€§ç±»åˆ«ï¼Œå¹¶å®Œå…¨ç”±çœŸå®ä¸–ç•Œæ•°æ®æ„å»ºã€‚æˆ‘ä»¬çš„æ³¨é‡Šè¿‡ç¨‹è¿˜äº§ç”Ÿäº†çŸ¥è¯†è§„åˆ™åº“ï¼Œæä¾›äº†æ˜ç¡®çš„ä¸“å®¶çŸ¥è¯†ï¼Œä»¥ååŠ©LLMæ£€æµ‹ä¸­æ–‡æœ‰å®³å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªèåˆäº†äººå·¥æ³¨é‡ŠçŸ¥è¯†è§„åˆ™å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸­éšå«çŸ¥è¯†çš„å¢å¼ºåŸºçº¿ï¼Œä½¿å°å‹æ¨¡å‹ä¹Ÿèƒ½å®ç°ä¸å›½å®¶å…ˆè¿›LLMç›¸å½“çš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/ChineseHarm-bench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/ChineseHarm-benchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10960v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨æ£€æµ‹æœ‰å®³å†…å®¹ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¯ååŠ©å†…å®¹ç®¡ç†è€…è¯†åˆ«æ”¿ç­–è¿è§„è¡Œä¸ºï¼Œæé«˜å†…å®¹å®¡æŸ¥çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç›®å‰çš„æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œä¸­æ–‡æ•°æ®é›†ç¨€ç¼ºä¸”èŒƒå›´æœ‰é™ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šæ³¨é‡Šçš„ä¸­æ–‡å†…å®¹å±å®³æ£€æµ‹åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªä»£è¡¨æ€§ç±»åˆ«ï¼Œå®Œå…¨ç”±çœŸå®ä¸–ç•Œæ•°æ®æ„å»ºã€‚æˆ‘ä»¬çš„æ³¨é‡Šè¿‡ç¨‹è¿˜äº§ç”Ÿäº†çŸ¥è¯†è§„åˆ™åº“ï¼Œä¸ºLLMæä¾›æ˜ç¡®çš„ä¸“å®¶çŸ¥è¯†ä»¥ååŠ©æ£€æµ‹ä¸­æ–‡æœ‰å®³å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†èåˆäººç±»æ³¨é‡ŠçŸ¥è¯†è§„åˆ™å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸­éšå«çŸ¥è¯†çš„åŸºçº¿æ–¹æ³•ï¼Œä½¿å°å‹æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°å‰æ²¿LLMçš„æ€§èƒ½æ°´å¹³ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/ChineseHarm-bench">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨æ£€æµ‹æœ‰å®³å†…å®¹ä»»åŠ¡ä¸­çš„åº”ç”¨é€æ¸æ™®åŠï¼Œæé«˜äº†å†…å®¹å®¡æŸ¥çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ç›®å‰é’ˆå¯¹ä¸­æ–‡çš„æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºååˆ†æœ‰é™ã€‚</li>
<li>æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šæ³¨é‡Šçš„ä¸­æ–‡å†…å®¹å±å®³æ£€æµ‹åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç±»åˆ«ï¼ŒåŸºäºçœŸå®ä¸–ç•Œæ•°æ®ã€‚</li>
<li>æ³¨é‡Šè¿‡ç¨‹ä¸­äº§ç”Ÿäº†çŸ¥è¯†è§„åˆ™åº“ï¼Œä¸ºLLMæä¾›æ˜ç¡®çš„ä¸“å®¶çŸ¥è¯†ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†èåˆäººç±»æ³¨é‡ŠçŸ¥è¯†è§„åˆ™å’ŒLLMä¸­éšå«çŸ¥è¯†çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿å°å‹æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°å‰æ²¿æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31da4ad1c7dd371df6869166a95fb7a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e251fe5364f8431b65e8e4dd2800694.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427485c8c85e6e2b28d3bd0aaf41a648.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SWE-Factory-Your-Automated-Factory-for-Issue-Resolution-Training-Data-and-Evaluation-Benchmarks"><a href="#SWE-Factory-Your-Automated-Factory-for-Issue-Resolution-Training-Data-and-Evaluation-Benchmarks" class="headerlink" title="SWE-Factory: Your Automated Factory for Issue Resolution Training Data   and Evaluation Benchmarks"></a>SWE-Factory: Your Automated Factory for Issue Resolution Training Data   and Evaluation Benchmarks</h2><p><strong>Authors:Lianghong Guo, Yanlin Wang, Caihua Li, Pengyu Yang, Jiachi Chen, Wei Tao, Yingtian Zou, Duyu Tang, Zibin Zheng</strong></p>
<p>Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of $0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at <a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/swe-factory">https://github.com/DeepSoftwareAnalytics/swe-factory</a>. </p>
<blockquote>
<p>æ„å»ºç”¨äºGitHubé—®é¢˜è§£å†³æ–¹æ¡ˆçš„å¤§è§„æ¨¡æ•°æ®é›†å¯¹äºè®­ç»ƒå’Œè¯„ä»·å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½¯ä»¶å·¥ç¨‹èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåˆ›å»ºæ­¤ç±»åŸºå‡†çš„ä¼ ç»Ÿè¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§ä¸”åŠ³åŠ¨å¯†é›†å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¾ç½®è¯„ä¼°ç¯å¢ƒã€è¯„ä¼°æµ‹è¯•ç»“æœå’ŒéªŒè¯ä»»åŠ¡å®ä¾‹çš„é˜¶æ®µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SWE-Factoryï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜çš„è‡ªåŠ¨åŒ–ç®¡é“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„ç®¡é“é›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒè‡ªåŠ¨åŒ–ç»„ä»¶ã€‚</p>
</blockquote>
<p>é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†SWE-Builderï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¯ä»¥è‡ªåŠ¨æ„å»ºè¯„ä¼°ç¯å¢ƒï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“è¿›è¡Œåä½œã€è¿­ä»£å¾ªç¯ï¼Œå¹¶åˆ©ç”¨ç¯å¢ƒå†…å­˜æ± æé«˜æ•ˆç‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„åŸºäºé€€å‡ºä»£ç çš„è¯„åˆ†æ–¹æ³•ï¼Œæ¶ˆé™¤äº†éœ€è¦æ‰‹åŠ¨ç¼–å†™è‡ªå®šä¹‰è§£æå™¨çš„éœ€æ±‚ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™äº›å¯é çš„é€€å‡ºä»£ç ä¿¡å·è‡ªåŠ¨è¿›è¡Œfail2passéªŒè¯è¿‡ç¨‹ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10954v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“SWE-Factoryï¼Œç”¨äºè§£å†³GitHubé—®é¢˜è§£æä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†æ„å»ºçš„æŒ‘æˆ˜ã€‚è¯¥ç®¡é“é›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒè‡ªåŠ¨åŒ–ç»„ä»¶ï¼šSWE-Builderã€æ ‡å‡†åŒ–çš„é€€å‡ºä»£ç è¯„åˆ†æ–¹æ³•å’Œfail2passéªŒè¯è¿‡ç¨‹çš„è‡ªåŠ¨åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®¡é“å¯ä»¥æœ‰æ•ˆåœ°æ„å»ºä»»åŠ¡å®ä¾‹ï¼Œå¹¶æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SWE-Factoryæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ•°æ®é›†æ„å»ºçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯GitHubé—®é¢˜è§£æä»»åŠ¡çš„æ•°æ®é›†æ„å»ºã€‚</li>
<li>ç®¡é“åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒè‡ªåŠ¨åŒ–ç»„ä»¶ï¼šSWE-Builderç”¨äºè‡ªåŠ¨åŒ–è¯„ä¼°ç¯å¢ƒæ„å»ºï¼Œæ ‡å‡†åŒ–çš„é€€å‡ºä»£ç è¯„åˆ†æ–¹æ³•ç”¨äºè¯„åˆ†æµ‹è¯•æˆæœï¼Œä»¥åŠè‡ªåŠ¨åŒ–çš„fail2passéªŒè¯è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSWE-Builderå¯ä»¥æœ‰æ•ˆåœ°æ„å»ºä»»åŠ¡å®ä¾‹ï¼Œå¹¶æé«˜æ•ˆç‡ã€‚ä¸GPT-4.1-miniå’ŒGemini-2.5-flashç»“åˆä½¿ç”¨æ—¶ï¼Œå¯ä»¥æ„å»ºå¤§é‡æœ‰æ•ˆçš„ä»»åŠ¡å®ä¾‹ã€‚</li>
<li>é€€å‡ºä»£ç è¯„åˆ†æ–¹æ³•è¾¾åˆ°100%çš„å‡†ç¡®æ€§ï¼Œä¸æ‰‹åŠ¨æ£€æŸ¥ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>è‡ªåŠ¨åŒ–fail2passéªŒè¯è¾¾åˆ°é«˜ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚</li>
<li>ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b81e71b2bdb2706c60497e45c55ae2ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8aeb3d59c8bedcd4e5ef83d530c60d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e49dea6784e7b9ed1be796263cc2848f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2c22871ed9d590f15d1f316460255c1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Execution-Guided-Line-by-Line-Code-Generation"><a href="#Execution-Guided-Line-by-Line-Code-Generation" class="headerlink" title="Execution Guided Line-by-Line Code Generation"></a>Execution Guided Line-by-Line Code Generation</h2><p><strong>Authors:Boaz Lavon, Shahar Katz, Lior Wolf</strong></p>
<p>We present a novel approach to neural code generation that incorporates real-time execution signals into the language model generation process. While large language models (LLMs) have demonstrated impressive code generation capabilities, they typically do not utilize execution feedback during inference, a critical signal that human programmers regularly leverage. Our method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically incorporates execution signals as the model generates code, providing line-by-line feedback that guides the generation process toward executable solutions. EG-CFG employs a multi-stage process: first, we conduct beam search to sample candidate program completions for each line; second, we extract execution signals by executing these candidates against test cases; and finally, we incorporate these signals into the prompt during generation. By maintaining consistent signals across tokens within the same line and refreshing signals at line boundaries, our approach provides coherent guidance while preserving syntactic structure. Moreover, the method naturally supports native parallelism at the task level in which multiple agents operate in parallel, exploring diverse reasoning paths and collectively generating a broad set of candidate solutions. Our experiments across diverse coding tasks demonstrate that EG-CFG significantly improves code generation performance compared to standard approaches, achieving state-of-the-art results across various levels of complexity, from foundational problems to challenging competitive programming tasks. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/boazlavon/eg_cfg">https://github.com/boazlavon/eg_cfg</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†å®æ—¶æ‰§è¡Œä¿¡å·èå…¥è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„æ–°å‹ç¥ç»ç½‘ç»œä»£ç ç”Ÿæˆæ–¹æ³•ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šå¸¸ä¸åˆ©ç”¨æ‰§è¡Œåé¦ˆï¼Œè¿™æ˜¯äººç±»ç¨‹åºå‘˜ç»å¸¸åˆ©ç”¨çš„å…³é”®ä¿¡å·ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæ‰§è¡Œå¼•å¯¼æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆEG-CFGï¼‰ï¼Œåœ¨ç”Ÿæˆä»£ç æ—¶åŠ¨æ€èå…¥æ‰§è¡Œä¿¡å·ï¼Œæä¾›é€è¡Œåé¦ˆï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœç€å¯æ‰§è¡Œè§£å†³æ–¹æ¡ˆè¿›è¡Œã€‚EG-CFGé‡‡ç”¨å¤šé˜¶æ®µè¿‡ç¨‹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬è¿›è¡Œé›†æŸæœç´¢ï¼Œä¸ºæ¯ä¸€è¡Œé‡‡æ ·ç¨‹åºå®Œæˆå€™é€‰ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æµ‹è¯•æ¡ˆä¾‹æ‰§è¡Œè¿™äº›å€™é€‰é¡¹æ¥æå–æ‰§è¡Œä¿¡å·ï¼›æœ€åï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆæ—¶å°†è¿™äº›ä¿¡å·èå…¥æç¤ºã€‚é€šè¿‡åœ¨åŒä¸€è¡Œçš„æ ‡è®°ä¹‹é—´ä¿æŒä¸€è‡´çš„ä¿¡å·å¹¶åœ¨è¡Œè¾¹ç•Œåˆ·æ–°ä¿¡å·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒè¯­æ³•ç»“æ„çš„åŒæ—¶æä¾›äº†è¿è´¯çš„æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è‡ªç„¶åœ°æ”¯æŒä»»åŠ¡å±‚é¢çš„åŸç”Ÿå¹¶è¡Œæ€§ï¼Œå¤šä¸ªä»£ç†å¹¶è¡Œæ“ä½œï¼Œæ¢ç´¢ä¸åŒçš„æ¨ç†è·¯å¾„å¹¶å…±åŒç”Ÿæˆä¸€ç»„å¹¿æ³›çš„å€™é€‰è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨å¤šç§ç¼–ç ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†æ–¹æ³•ç›¸æ¯”ï¼ŒEG-CFGæ˜¾è‘—æé«˜äº†ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œåœ¨å„ç§å¤æ‚ç¨‹åº¦ä¸Šéƒ½è¾¾åˆ°äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬åŸºç¡€é—®é¢˜åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„ç«èµ›ç¼–ç¨‹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/boazlavon/eg_cfg%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/boazlavon/eg_cfgæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10948v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºäº†ä¸€ç§å°†å®æ—¶æ‰§è¡Œä¿¡å·èå…¥è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºç¥ç»ä»£ç ç”Ÿæˆã€‚è¯¥æ–¹æ³•ç§°ä¸ºæ‰§è¡Œå¼•å¯¼çš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆEG-CFGï¼‰ï¼Œåœ¨ç”Ÿæˆä»£ç æ—¶åŠ¨æ€èå…¥æ‰§è¡Œä¿¡å·ï¼Œæä¾›é€è¡Œåé¦ˆï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœå¯æ‰§è¡Œè§£å†³æ–¹æ¡ˆå‘å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒEG-CFGåœ¨å¤šç§ç¼–ç ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†ä»£ç ç”Ÿæˆæ€§èƒ½ï¼Œå®ç°äº†ä»åŸºç¡€é—®é¢˜åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„ç«èµ›ç¼–ç¨‹ä»»åŠ¡çš„æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†EG-CFGæ–¹æ³•ï¼Œå°†å®æ—¶æ‰§è¡Œä¿¡å·èå…¥è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>EG-CFGåœ¨ç”Ÿæˆä»£ç æ—¶åŠ¨æ€åˆ©ç”¨æ‰§è¡Œä¿¡å·ï¼Œæä¾›é€è¡Œåé¦ˆã€‚</li>
<li>EG-CFGé€šè¿‡æŸæœç´¢é‡‡æ ·å€™é€‰ç¨‹åºå®Œæˆï¼Œé€šè¿‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹æå–æ‰§è¡Œä¿¡å·ï¼Œå¹¶å°†è¿™äº›ä¿¡å·èå…¥æç¤ºä¸­è¿›è¡Œç”Ÿæˆã€‚</li>
<li>EG-CFGæ–¹æ³•ä¿æŒåŒä¸€è¡Œå†…æ ‡è®°ä¹‹é—´çš„ä¸€è‡´ä¿¡å·ï¼Œå¹¶åœ¨è¡Œè¾¹ç•Œåˆ·æ–°ä¿¡å·ï¼Œä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›è¿è´¯çš„å¼•å¯¼ï¼ŒåŒæ—¶ä¿ç•™è¯­æ³•ç»“æ„ã€‚</li>
<li>EG-CFGæ–¹æ³•æ”¯æŒåŸç”Ÿå¹¶è¡Œæ€§ï¼Œå¤šä¸ªä»£ç†å¹¶è¡Œæ“ä½œï¼Œæ¢ç´¢ä¸åŒçš„æ¨ç†è·¯å¾„ï¼Œå¹¶å…±åŒç”Ÿæˆå¹¿æ³›çš„å€™é€‰è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒEG-CFGåœ¨å¤šç§ç¼–ç ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†ä»£ç ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>EG-CFGæ–¹æ³•å®ç°äº†ä»åŸºç¡€é—®é¢˜åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„ç«èµ›ç¼–ç¨‹ä»»åŠ¡çš„æœ€ä½³ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e277385ea215010ee1274441d624bb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ceae215fab5b60085461ca8a8b1c373.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71cdc07a8495b3370d3bf09cc516665f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Self-Adapting-Language-Models"><a href="#Self-Adapting-Language-Models" class="headerlink" title="Self-Adapting Language Models"></a>Self-Adapting Language Models</h2><p><strong>Authors:Adam Zweiger, Jyothish Pari, Han Guo, Ekin AkyÃ¼rek, Yoon Kim, Pulkit Agrawal</strong></p>
<p>Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the modelâ€™s own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at <a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal">https://jyopari.github.io/posts/seal</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†å®ƒä»¬æ˜¯é™æ€çš„ï¼›ç¼ºä¹æ ¹æ®æ–°ä»»åŠ¡ã€çŸ¥è¯†æˆ–ç¤ºä¾‹è°ƒæ•´å…¶æƒé‡çš„æœºåˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSEALï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿LLMèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆè‡ªå·±çš„å¾®è°ƒæ•°æ®å’Œæ›´æ–°æŒ‡ä»¤è¿›è¡Œè‡ªæˆ‘é€‚åº”ã€‚å¯¹äºæ–°çš„è¾“å…¥ï¼Œæ¨¡å‹ä¼šäº§ç”Ÿè‡ªæˆ‘ç¼–è¾‘ç”Ÿæˆçš„å†…å®¹ï¼Œè¿™å¯èƒ½ä»¥ä¸åŒçš„æ–¹å¼é‡æ–°ç»„ç»‡ä¿¡æ¯ï¼ŒæŒ‡å®šä¼˜åŒ–è¶…å‚æ•°ï¼Œæˆ–è°ƒç”¨æ•°æ®å¢å¼ºå’ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°å·¥å…·ã€‚é€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™äº›è‡ªæˆ‘ç¼–è¾‘ä¼šå¯¼è‡´æŒä¹…çš„æƒé‡æ›´æ–°ï¼Œä»è€Œå®ç°æŒä¹…çš„é€‚åº”ã€‚ä¸ºäº†è®­ç»ƒæ¨¡å‹ä»¥äº§ç”Ÿæœ‰æ•ˆçš„è‡ªæˆ‘ç¼–è¾‘ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯ï¼Œä»¥æ›´æ–°æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–äºå•ç‹¬çš„è‡ªé€‚åº”æ¨¡å—æˆ–è¾…åŠ©ç½‘ç»œçš„åšæ³•ï¼ŒSEALç›´æ¥ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆæ¥æ§åˆ¶å…¶è‡ªé€‚åº”è¿‡ç¨‹ã€‚åœ¨çŸ¥è¯†èå…¥å’Œå°‘é‡ç¤ºä¾‹æ¨å¹¿æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒSEALæ˜¯æœç€èƒ½å¤Ÿè¿›è¡Œè‡ªæˆ‘å¯¼å‘é€‚åº”çš„è¯­è¨€æ¨¡å‹è¿ˆå‡ºçš„æœ‰å‰é€”çš„ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ç½‘ç«™å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal]%E8%AE%BF%E9%97%AE%E3%80%82">https://jyopari.github.io/posts/seal]è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10943v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å¼ºå¤§ä½†é™æ€å›ºåŒ–ï¼Œæ— æ³•æ ¹æ®æ–°ä»»åŠ¡ã€çŸ¥è¯†æˆ–èŒƒä¾‹è°ƒæ•´è‡ªèº«æƒé‡ä»¥é€‚åº”å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºSelf-Adapting LLMsï¼ˆSEALï¼‰æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆè‡ªèº«çš„å¾®è°ƒæ•°æ®å’Œæ›´æ–°æŒ‡ä»¤å®ç°è‡ªæˆ‘é€‚åº”ã€‚é¢å¯¹æ–°è¾“å…¥ï¼Œæ¨¡å‹ä¼šè¿›è¡Œè‡ªæˆ‘ç¼–è¾‘ç”Ÿæˆï¼Œä»¥ä¸åŒæ–¹å¼é‡ç»„ä¿¡æ¯ï¼Œè®¾å®šä¼˜åŒ–è¶…å‚æ•°ï¼Œæˆ–è°ƒç”¨æ•°æ®å¢å¼ºå·¥å…·å’ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™äº›è‡ªæˆ‘ç¼–è¾‘å¯¼è‡´æŒä¹…æ€§æƒé‡æ›´æ–°ï¼Œå®ç°é•¿æœŸé€‚åº”ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥è®­ç»ƒæ¨¡å‹äº§ç”Ÿæœ‰æ•ˆçš„è‡ªæˆ‘ç¼–è¾‘ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯ã€‚ä¸ä¾èµ–å•ç‹¬é€‚åº”æ¨¡å—æˆ–è¾…åŠ©ç½‘ç»œçš„æ–¹æ³•ä¸åŒï¼ŒSEALç›´æ¥ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆæ¥æ§åˆ¶å…¶é€‚åº”è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSEALåœ¨çŸ¥è¯†èå…¥å’Œå°‘é‡æ ·æœ¬æ³›åŒ–æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæ˜¯å‘èƒ½å¤Ÿè‡ªæˆ‘å®šå‘é€‚åº”çš„è¯­è¨€æ¨¡å‹è¿ˆè¿›çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå­˜åœ¨é™æ€å›ºåŒ–é—®é¢˜ï¼Œæ— æ³•æ ¹æ®æ–°ä»»åŠ¡ã€çŸ¥è¯†æˆ–èŒƒä¾‹è‡ªæˆ‘é€‚åº”ã€‚</li>
<li>Self-Adapting LLMsï¼ˆSEALï¼‰æ¡†æ¶èƒ½å¤Ÿè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½¿LLMé€šè¿‡ç”Ÿæˆè‡ªæˆ‘ç¼–è¾‘å®ç°è‡ªæˆ‘é€‚åº”ã€‚</li>
<li>æ¨¡å‹é¢å¯¹æ–°è¾“å…¥ä¼šè¿›è¡Œè‡ªæˆ‘ç¼–è¾‘ç”Ÿæˆï¼ŒåŒ…æ‹¬ä¿¡æ¯é‡ç»„ã€ä¼˜åŒ–è¶…å‚æ•°è®¾å®šç­‰ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè‡ªæˆ‘ç¼–è¾‘ä¼šå¯¼è‡´æŒä¹…æ€§æƒé‡æ›´æ–°ï¼Œå®ç°é•¿æœŸé€‚åº”ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯è®­ç»ƒæ¨¡å‹è¿›è¡Œè‡ªæˆ‘ç¼–è¾‘ï¼Œä»¥ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚</li>
<li>ä¸å…¶ä»–é€‚åº”æ¨¡å—æˆ–è¾…åŠ©ç½‘ç»œæ–¹æ³•ä¸åŒï¼ŒSEALç›´æ¥ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆæ§åˆ¶é€‚åº”è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜SEALåœ¨çŸ¥è¯†èå…¥å’Œå°‘é‡æ ·æœ¬æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ec1605afba63907ca3459385fafa44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f114ce44336f5ab60f35250dc70fe9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a753461583488b92e0b9e22800090050.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6b487a779381eff91524b491c9f3e49.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NoLoCo-No-all-reduce-Low-Communication-Training-Method-for-Large-Models"><a href="#NoLoCo-No-all-reduce-Low-Communication-Training-Method-for-Large-Models" class="headerlink" title="NoLoCo: No-all-reduce Low Communication Training Method for Large Models"></a>NoLoCo: No-all-reduce Low Communication Training Method for Large Models</h2><p><strong>Authors:Jari Kolehmainen, Nikolay Blagoev, John Donaghy, OÄŸuzhan Ersoy, Christopher Nies</strong></p>
<p>Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. Several recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster. These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.   In this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.   We benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to $4%$ faster convergence rate with wide range of model sizes and accelerator counts. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸æ˜¯åœ¨åŒ…å«æ•°ä»¥ä¸‡è®¡çš„åŠ é€Ÿå™¨çš„é›†ç¾¤ä¸Šé€šè¿‡ä¼˜åŒ–æ–¹æ³•è¿›è¡Œçš„ï¼Œè¿™äº›åŠ é€Ÿå™¨é€šè¿‡é«˜é€Ÿäº’è”è¿›è¡Œé€šä¿¡ã€‚æ‰©å¤§è¿™äº›é›†ç¾¤çš„è§„æ¨¡æˆæœ¬é«˜æ˜‚ï¼Œç”šè‡³å¯èƒ½ä¸åˆ‡å®é™…ï¼Œä»è€Œå¯¹å¯è®­ç»ƒæ¨¡å‹çš„å¤§å°æ–½åŠ é™åˆ¶ã€‚æœ€è¿‘çš„ä¸€äº›ç ”ç©¶æå‡ºäº†é€šä¿¡å¯†é›†åº¦è¾ƒä½çš„è®­ç»ƒæ–¹æ³•ï¼Œé¿å…äº†é«˜åº¦äº’è”è®¡ç®—é›†ç¾¤çš„éœ€æ±‚ã€‚è¿™äº›æœ€å…ˆè¿›çš„ä½é€šä¿¡è®­ç»ƒæ–¹æ³•ä»ç„¶é‡‡ç”¨æ¨¡å‹å‚æ•°çš„åŒæ­¥æ­¥éª¤ï¼Œå½“åœ¨æ‰€æœ‰æ¨¡å‹å‰¯æœ¬ä¸Šæ‰§è¡Œæ—¶ï¼Œè¿™åœ¨ä½å¸¦å®½ç½‘ç»œä¸Šå¯èƒ½ä¼šå˜å¾—æˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•NoLoCoï¼Œå®ƒä¸ä¼šåœ¨è®­ç»ƒæœŸé—´æ˜ç¡®åŒæ­¥æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œå› æ­¤ä¸éœ€è¦ä»»ä½•é›†ä½“é€šä¿¡ã€‚NoLoCoé€šè¿‡NesterovåŠ¨é‡ä¼˜åŒ–å™¨çš„æ–°é¢–å˜ä½“æ¥éšå¼åœ°åŒæ­¥æ¨¡å‹æƒé‡ï¼Œé€šè¿‡éƒ¨åˆ†å¹³å‡éšæœºé€‰æ‹©çš„å¦ä¸€ä¸ªæ¨¡å‹çš„æƒé‡æ¥å®ç°ã€‚æˆ‘ä»¬ä¸ºæˆ‘ä»¬æå‡ºçš„ä¼˜åŒ–å™¨æä¾›äº†ç†è®ºæ”¶æ•›åˆ†æä»¥åŠè¯­è¨€æ¨¡å‹è®­ç»ƒçš„å®è¯ç»“æœã€‚æˆ‘ä»¬åœ¨èŒƒå›´å¹¿æ³›çš„åŠ é€Ÿå™¨æ•°é‡å’Œæ¨¡å‹å¤§å°ï¼ˆä»1.25äº¿åˆ°68äº¿å‚æ•°ï¼‰ä¸Šå¯¹NoLoCoè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰€éœ€çš„é€šä¿¡å¼€é”€è¿œå°äºå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œè®­ç»ƒï¼Œç”šè‡³å°äºå¹¿æ³›ä½¿ç”¨çš„ä½é€šä¿¡è®­ç»ƒæ–¹æ³•DiLoCoã€‚åŒæ­¥æ­¥éª¤æœ¬èº«ä¼°è®¡æ¯”DiLoCoåœ¨å‡ ç™¾ä¸ªåŠ é€Ÿå™¨ä¸Šè¿›è¡Œäº’è”ç½‘è®­ç»ƒæ—¶ä½¿ç”¨çš„æ‰€æœ‰å‡å°‘ä¸€ä¸ªæ•°é‡çº§ã€‚æˆ‘ä»¬ä¹Ÿæ²¡æœ‰ä»»ä½•å…¨å±€é˜»å¡é€šä¿¡ï¼Œè¿™å‡å°‘äº†åŠ é€Ÿå™¨çš„ç©ºé—²æ—¶é—´ã€‚ä¸DiLoCoç›¸æ¯”ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°åœ¨å¹¿æ³›çš„æ¨¡å‹å¤§å°å’ŒåŠ é€Ÿå™¨æ•°é‡ä¸‹ï¼Œæ”¶æ•›é€Ÿåº¦æé«˜äº†é«˜è¾¾4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10911v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°å‹ä¼˜åŒ–æ–¹æ³•NoLoCoï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶æ— éœ€æ˜ç¡®åŒæ­¥æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œå› æ­¤æ— éœ€ä»»ä½•é›†ä½“é€šä¿¡ã€‚NoLoCoé€šè¿‡ä¸€ç§æ–°å‹NesterovåŠ¨é‡ä¼˜åŒ–å™¨çš„å˜ä½“ï¼Œé€šè¿‡éšæœºé€‰æ‹©å¦ä¸€ä¸ªæ¨¡å‹æƒé‡è¿›è¡Œéƒ¨åˆ†å¹³å‡ï¼Œä»è€Œéšå¼åœ°åŒæ­¥æ¨¡å‹æƒé‡ã€‚æˆ‘ä»¬ä¸ºæ‰€æå‡ºçš„ä¼˜åŒ–å™¨æä¾›äº†ç†è®ºæ”¶æ•›åˆ†æå’Œè¯­è¨€æ¨¡å‹è®­ç»ƒçš„å®è¯ç»“æœã€‚åœ¨å¹¿æ³›çš„åŠ é€Ÿå™¨è®¡æ•°å’Œæ¨¡å‹å¤§å°ï¼ˆä»1.25äº¿åˆ°68äº¿å‚æ•°ï¼‰ä¸Šï¼Œæˆ‘ä»¬å¯¹NoLoCoè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è¯¥æ–¹æ³•æ¯”å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œè®­ç»ƒç”šè‡³å¹¿æ³›ä½¿ç”¨çš„ä½é€šä¿¡è®­ç»ƒæ–¹æ³•DiLoCoå…·æœ‰æ›´ä½çš„é€šä¿¡å¼€é”€ã€‚åŒæ­¥æ­¥éª¤æœ¬èº«ä¼°è®¡æ¯”DiLoCoåœ¨å…¨é‡æ•°ç™¾ä¸ªåŠ é€Ÿå™¨é€šè¿‡äº’è”ç½‘è¿›è¡Œè®­ç»ƒæ—¶ä½¿ç”¨çš„all-reduceå¿«ä¸€ä¸ªæ•°é‡çº§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ²¡æœ‰å…¨å±€é˜»å¡é€šä¿¡ï¼Œè¿™å‡å°‘äº†åŠ é€Ÿå™¨çš„é—²ç½®æ—¶é—´ã€‚ä¸DiLoCoç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨å¹¿æ³›çš„æ¨¡å‹å¤§å°å’ŒåŠ é€Ÿå™¨è®¡æ•°ä¸Šè§‚å¯Ÿåˆ°é«˜è¾¾4%çš„æ›´å¿«æ”¶æ•›ç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>NoLoCoæ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–æ–¹æ³•ï¼Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶æ— éœ€é›†ä½“é€šä¿¡ã€‚</li>
<li>NoLoCoé€šè¿‡éƒ¨åˆ†å¹³å‡éšæœºé€‰æ‹©çš„æ¨¡å‹æƒé‡éšå¼åŒæ­¥æ¨¡å‹ã€‚</li>
<li>NoLoCoåœ¨å¹¿æ³›çš„åŠ é€Ÿå™¨è®¡æ•°å’Œæ¨¡å‹å¤§å°ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œé€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>NoLoCoç›¸æ¯”DiLoCoå…·æœ‰æ›´ä½çš„é€šä¿¡å¼€é”€å’Œæ›´å¿«çš„åŒæ­¥æ­¥éª¤ã€‚</li>
<li>NoLoCoæ²¡æœ‰å…¨å±€é˜»å¡é€šä¿¡ï¼Œå‡å°‘äº†åŠ é€Ÿå™¨çš„é—²ç½®æ—¶é—´ã€‚</li>
<li>ä¸DiLoCoç›¸æ¯”ï¼ŒNoLoCoåœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8deb3553b6b28efe30912ae1905186b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-402b91f4dcbe0663dcc5ba77d15a7510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1621884689bcc17abb8d3c942b5a122.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Revisiting-Transformers-with-Insights-from-Image-Filtering"><a href="#Revisiting-Transformers-with-Insights-from-Image-Filtering" class="headerlink" title="Revisiting Transformers with Insights from Image Filtering"></a>Revisiting Transformers with Insights from Image Filtering</h2><p><strong>Authors:Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen</strong></p>
<p>The self-attention mechanism, a cornerstone of Transformer-based state-of-the-art deep learning architectures, is largely heuristic-driven and fundamentally challenging to interpret. Establishing a robust theoretical foundation to explain its remarkable success and limitations has therefore become an increasingly prominent focus in recent research. Some notable directions have explored understanding self-attention through the lens of image denoising and nonparametric regression. While promising, existing frameworks still lack a deeper mechanistic interpretation of various architectural components that enhance self-attention, both in its original formulation and subsequent variants. In this work, we aim to advance this understanding by developing a unifying image processing framework, capable of explaining not only the self-attention computation itself but also the role of components such as positional encoding and residual connections, including numerous later variants. We also pinpoint potential distinctions between the two concepts building upon our framework, and make effort to close this gap. We introduce two independent architectural modifications within transformers. While our primary objective is interpretability, we empirically observe that image processing-inspired modifications can also lead to notably improved accuracy and robustness against data contamination and adversaries across language and vision tasks as well as better long sequence understanding. </p>
<blockquote>
<p>è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯å½“ä¸‹æœ€æµè¡Œçš„åŸºäºTransformerçš„æ·±åº¦å­¦ä¹ æ¶æ„çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œå…¶å¯å‘å¼çš„ç‰¹ç‚¹ä½¿å¾—å…¶éš¾ä»¥ä»æ ¹æœ¬ä¸Šè¿›è¡Œè§£é‡Šã€‚å› æ­¤ï¼Œä¸ºäº†è§£é‡Šå…¶åœ¨ä¼—å¤šä»»åŠ¡ä¸Šçš„æ˜¾è‘—æˆåŠŸå’Œå±€é™æ€§ï¼Œå»ºç«‹åšå®çš„ç†è®ºåŸºç¡€æˆä¸ºäº†æœ€è¿‘ç ”ç©¶çš„é‡è¦ç„¦ç‚¹ã€‚ä¸€äº›æ˜¾è‘—çš„ç ”ç©¶æ–¹å‘å°è¯•é€šè¿‡å›¾åƒå»å™ªå’Œéå‚æ•°å›å½’æ¥ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚å°½ç®¡è¿™äº›æ–¹å‘å‰æ™¯å¹¿é˜”ï¼Œä½†ç°æœ‰çš„æ¡†æ¶ä»ç„¶ç¼ºä¹å¯¹å¢å¼ºè‡ªæ³¨æ„åŠ›çš„å„ç§æ¶æ„ç»„ä»¶çš„æ›´æ·±å…¥çš„æœºæ¢°è§£é‡Šï¼Œæ— è®ºåœ¨å…¶åŸå§‹å½¢å¼è¿˜æ˜¯éšåçš„å˜ä½“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡å¼€å‘ä¸€ä¸ªç»Ÿä¸€çš„å›¾åƒå¤„ç†æ¡†æ¶æ¥æå‡è¿™ç§ç†è§£ï¼Œè¿™ä¸ªæ¡†æ¶ä¸ä»…èƒ½å¤Ÿè§£é‡Šè‡ªæ³¨æ„åŠ›è®¡ç®—æœ¬èº«ï¼Œè¿˜èƒ½å¤Ÿè§£é‡Šç»„ä»¶å¦‚ä½ç½®ç¼–ç å’Œæ®‹å·®è¿æ¥çš„ä½œç”¨ï¼ŒåŒ…æ‹¬è®¸å¤šåç»­å˜ä½“ã€‚æˆ‘ä»¬è¿˜æŒ‡å‡ºäº†åœ¨æˆ‘ä»¬æ¡†æ¶ä¸‹è¿™ä¸¤ä¸ªæ¦‚å¿µä¹‹é—´çš„æ½œåœ¨åŒºåˆ«ï¼Œå¹¶åŠªåŠ›ç¼©å°è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬åœ¨Transformerä¸­å¼•å…¥äº†ä¸¤ç§ç‹¬ç«‹çš„æ¶æ„ä¿®æ”¹ã€‚è™½ç„¶æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è§£é‡Šæ€§ï¼Œä½†ç»éªŒè§‚å¯Ÿè¡¨æ˜ï¼Œå—å›¾åƒå¤„ç†å¯å‘çš„ä¿®æ”¹è¿˜å¯ä»¥æ˜¾è‘—æé«˜è¯­è¨€å’Œè§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä»¥åŠæ›´å¥½åœ°å¤„ç†æ•°æ®æ±¡æŸ“å’Œå¯¹æ‰‹æ”»å‡»çš„é—®é¢˜ï¼ŒåŒæ—¶å®ç°å¯¹é•¿åºåˆ—çš„æ›´å¥½ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10371v1">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ—¨åœ¨é€šè¿‡æ„å»ºç»Ÿä¸€çš„å›¾åƒå¤„ç†æ¡†æ¶ï¼Œæ·±å…¥è§£æTransformeræ¶æ„ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åŠå…¶ç»„ä»¶ï¼ˆå¦‚ä½ç½®ç¼–ç å’Œæ®‹å·®è¿æ¥ï¼‰ï¼Œè§£é‡Šå…¶æˆåŠŸå’Œé™åˆ¶ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸¤ä¸ªç‹¬ç«‹çš„æ¶æ„ä¿®æ”¹ï¼Œæ—¨åœ¨æé«˜è§£é‡Šæ€§ï¼ŒåŒæ—¶å‘ç°è¿™äº›ä¿®æ”¹å¯æ˜¾è‘—æé«˜è¯­è¨€å’Œè§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« å…³æ³¨äºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±å…¥ç†è§£å’Œè§£æï¼Œå°¤å…¶æ˜¯åœ¨Transformeræ¶æ„ä¸­ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å›¾åƒå¤„ç†æ¡†æ¶ï¼Œç”¨ä»¥è§£é‡Šè‡ªæ³¨æ„åŠ›è®¡ç®—åŠå…¶ç»„ä»¶å¦‚ä½ç½®ç¼–ç å’Œæ®‹å·®è¿æ¥ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ä»…è§£é‡Šäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿˜è§£é‡Šäº†åç»­çš„å„ç§å˜ä½“ã€‚</li>
<li>å¼•å…¥äº†ä¸¤ä¸ªç‹¬ç«‹çš„æ¶æ„ä¿®æ”¹ï¼Œä¸»è¦ç›®æ ‡æ˜¯æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>è¿™äº›ä¿®æ”¹ä¸ä»…æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œè¿˜æé«˜äº†è¯­è¨€å’Œè§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨ç†è§£å’Œå¤„ç†å›¾åƒæ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå»å™ªå’Œéå‚æ•°å›å½’æ–¹é¢çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8f756cda8384da68355d14ddf277e9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42dcac916b96af7a62ac174ffa9dab6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cce9e830695dea8672f7c09b1ceccdb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention"><a href="#On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention" class="headerlink" title="On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention"></a>On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention</h2><p><strong>Authors:Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella</strong></p>
<p>Large language models (LLMs) excel at capturing global token dependencies via self-attention but face prohibitive compute and memory costs on lengthy inputs. While sub-quadratic methods (e.g., linear attention) can reduce these costs, they often degrade accuracy due to overemphasizing recent tokens. In this work, we first propose dual-state linear attention (DSLA), a novel design that maintains two specialized hidden states-one for preserving historical context and one for tracking recency-thereby mitigating the short-range bias typical of linear-attention architectures. To further balance efficiency and accuracy under dynamic workload conditions, we introduce DSLA-Serve, an online adaptive distillation framework that progressively replaces Transformer layers with DSLA layers at inference time, guided by a sensitivity-based layer ordering. DSLA-Serve uses a chained fine-tuning strategy to ensure that each newly converted DSLA layer remains consistent with previously replaced layers, preserving the overall quality. Extensive evaluations on commonsense reasoning, long-context QA, and text summarization demonstrate that DSLA-Serve yields 2.3x faster inference than Llama2-7B and 3.0x faster than the hybrid Zamba-7B, while retaining comparable performance across downstream tasks. Our ablation studies show that DSLAâ€™s dual states capture both global and local dependencies, addressing the historical-token underrepresentation seen in prior linear attentions. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve">https://github.com/utnslab/DSLA-Serve</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿é€šè¿‡è‡ªæ³¨æ„åŠ›æ•æ‰å…¨å±€ä»¤ç‰Œä¾èµ–å…³ç³»ï¼Œä½†åœ¨å¤„ç†é•¿è¾“å…¥æ—¶é¢ä¸´ç€è®¡ç®—é‡å¤§å’Œå†…å­˜æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚è™½ç„¶æ¬¡äºŒæ¬¡æ–¹æ³•ï¼ˆå¦‚çº¿æ€§æ³¨æ„åŠ›ï¼‰å¯ä»¥é™ä½è¿™äº›æˆæœ¬ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå› ä¸ºè¿‡åˆ†å¼ºè°ƒæœ€è¿‘çš„ä»¤ç‰Œè€Œé™ä½å‡†ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†åŒæ€çº¿æ€§æ³¨æ„åŠ›ï¼ˆDSLAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®¾è®¡ï¼Œå®ƒç»´æŠ¤äº†ä¸¤ä¸ªä¸“é—¨çš„éšè—çŠ¶æ€â€”â€”ä¸€ä¸ªç”¨äºä¿ç•™å†å²ä¸Šä¸‹æ–‡ï¼Œå¦ä¸€ä¸ªç”¨äºè·Ÿè¸ªæœ€è¿‘çš„çŠ¶æ€â€”â€”ä»è€Œå‡è½»äº†çº¿æ€§æ³¨æ„åŠ›æ¶æ„é€šå¸¸å­˜åœ¨çš„çŸ­ç¨‹åå·®ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–åŠ¨æ€å·¥ä½œé‡æ¡ä»¶ä¸‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSLA-Serveï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€æ­¥ç”¨DSLAå±‚æ›¿æ¢Transformerå±‚ï¼Œä»¥åŸºäºæ•æ„Ÿæ€§çš„å±‚åºä¸ºæŒ‡å¯¼ã€‚DSLA-Serveä½¿ç”¨é“¾å¼å¾®è°ƒç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªæ–°è½¬æ¢çš„DSLAå±‚ä¸ä¹‹å‰æ›¿æ¢çš„å±‚ä¿æŒä¸€è‡´ï¼Œä»è€Œä¿æŒæ•´ä½“è´¨é‡ã€‚åœ¨å¸¸è¯†æ¨ç†ã€é•¿æ–‡æœ¬é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ç­‰æ–¹é¢çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDSLA-Serveç›¸å¯¹äºLlama2-7Bè¿›è¡Œæ¨ç†çš„é€Ÿåº¦æé«˜äº†2.3å€ï¼Œç›¸å¯¹äºæ··åˆZamba-7Båˆ™æé«˜äº†3.0å€ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ç›¸å½“ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒDSLAçš„åŒæ€èƒ½å¤Ÿæ•æ‰å…¨å±€å’Œå±€éƒ¨ä¾èµ–å…³ç³»ï¼Œè§£å†³äº†å…ˆå‰çº¿æ€§æ³¨æ„åŠ›ä¸­å†å²ä»¤ç‰Œè¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve%E3%80%82">https://github.com/utnslab/DSLA-Serveã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09316v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMåœ¨æ•è·å…¨å±€tokenä¾èµ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é«˜æ˜‚çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬é™åˆ¶äº†å…¶åœ¨é•¿è¾“å…¥ä¸Šçš„åº”ç”¨ã€‚è™½ç„¶é‡‡ç”¨å­äºŒæ¬¡æ–¹æ³•ï¼ˆå¦‚çº¿æ€§æ³¨æ„åŠ›ï¼‰å¯ä»¥é™ä½è¿™äº›æˆæœ¬ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå› ä¸ºè¿‡åº¦å¼ºè°ƒæœ€è¿‘çš„tokenè€Œé™ä½å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†åŒæ€çº¿æ€§æ³¨æ„åŠ›ï¼ˆDSLAï¼‰è¿™ä¸€æ–°å‹è®¾è®¡ï¼Œé€šè¿‡ç»´æŠ¤ä¸¤ä¸ªä¸“é—¨åŒ–çš„éšè—çŠ¶æ€æ¥åŒæ—¶å…³æ³¨å†å²ä¸Šä¸‹æ–‡å’Œè¿½è¸ªè¿‘æœŸä¿¡æ¯ï¼Œä»è€Œå‡è½»çº¿æ€§æ³¨æ„åŠ›æ¶æ„çš„çŸ­ç¨‹åè§ã€‚ä¸ºäº†å¹³è¡¡åŠ¨æ€å·¥ä½œè´Ÿè½½æ¡ä»¶ä¸‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†DSLA-Serveï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶é€æ­¥ç”¨DSLAå±‚æ›¿æ¢Transformerå±‚ï¼Œå¹¶æ ¹æ®æ•æ„Ÿæ€§ç¡®å®šå±‚åºã€‚DSLA-Serveé‡‡ç”¨é“¾å¼å¾®è°ƒç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªæ–°è½¬æ¢çš„DSLAå±‚ä¸ä¹‹å‰æ›¿æ¢çš„å±‚ä¿æŒä¸€è‡´ï¼Œä»è€Œä¿æŒæ•´ä½“è´¨é‡ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒDSLA-Serveåœ¨å¸¸è¯†æ¨ç†ã€é•¿æ–‡æœ¬é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­çš„æ¨ç†é€Ÿåº¦æ¯”Llama2-7Bå¿«2.3å€ï¼Œæ¯”æ··åˆæ¨¡å‹Zamba-7Bå¿«3.0å€ï¼ŒåŒæ—¶ä¿æŒä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ç›¸å½“ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒDSLAçš„åŒçŠ¶æ€èƒ½å¤Ÿæ•è·å…¨å±€å’Œå±€éƒ¨ä¾èµ–å…³ç³»ï¼Œè§£å†³äº†å…ˆå‰çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ä¸­å†å²tokenè¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve%E3%80%82">https://github.com/utnslab/DSLA-Serveã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMé€šè¿‡è‡ªæˆ‘å…³æ³¨æ•è·å…¨å±€tokenä¾èµ–ï¼Œä½†é¢ä¸´è®¡ç®—ä¸å†…å­˜æˆæœ¬é—®é¢˜ã€‚</li>
<li>å­äºŒæ¬¡æ–¹æ³•å¦‚çº¿æ€§æ³¨æ„åŠ›è™½å¯é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†å¯èƒ½å¯¼è‡´å‡†ç¡®æ€§ä¸‹é™ã€‚</li>
<li>DSLAè®¾è®¡é€šè¿‡ç»´æŠ¤ä¸¤ä¸ªéšè—çŠ¶æ€å¹³è¡¡å†å²ä¸è¿‘æœŸä¿¡æ¯ï¼Œå‡è½»çº¿æ€§æ³¨æ„åŠ›çš„çŸ­ç¨‹åè§ã€‚</li>
<li>DSLA-Serveæ˜¯é¦–ä¸ªåœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶ï¼Œèƒ½åœ¨æ¨ç†æ—¶åŠ¨æ€æ›¿æ¢æ¨¡å‹å±‚ï¼Œä»¥æé«˜æ•ˆç‡å¹¶å¹³è¡¡å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡é“¾å¼å¾®è°ƒç­–ç•¥ç¡®ä¿æ¨¡å‹æ€§èƒ½ç¨³å®šï¼Œå³ä½¿è¿›è¡Œå±‚æ›¿æ¢ã€‚</li>
<li>DSLA-Serveæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œç›¸è¾ƒäºLlama2-7Bå¿«2.3å€ï¼Œç›¸è¾ƒäºZamba-7Bå¿«3å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-653290d503a710e753c3098cd8ccaf13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60c67094e0bcd3f621e31431b29f3ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90de5cd9ba73d7e8bc828d401a22efc8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Sailing-by-the-Stars-A-Survey-on-Reward-Models-and-Learning-Strategies-for-Learning-from-Rewards"><a href="#Sailing-by-the-Stars-A-Survey-on-Reward-Models-and-Learning-Strategies-for-Learning-from-Rewards" class="headerlink" title="Sailing by the Stars: A Survey on Reward Models and Learning Strategies   for Learning from Rewards"></a>Sailing by the Stars: A Survey on Reward Models and Learning Strategies   for Learning from Rewards</h2><p><strong>Authors:Xiaobao Wu</strong></p>
<p>Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (RLHF, RLAIF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities for diverse tasks. In this survey, we present a comprehensive overview of learning from rewards, from the perspective of reward models and learning strategies across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at <a target="_blank" rel="noopener" href="https://github.com/bobxwu/learning-from-rewards-llm-papers">https://github.com/bobxwu/learning-from-rewards-llm-papers</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°å‘å±•å·²ä»é¢„è®­ç»ƒæ‰©å±•è½¬å‘åè®­ç»ƒå’Œæµ‹è¯•æ—¶æ‰©å±•ã€‚åœ¨è¿™äº›å‘å±•ä¸­ï¼Œå‡ºç°äº†ä¸€ä¸ªå…³é”®çš„ç»Ÿä¸€èŒƒå¼ï¼šä»å¥–åŠ±ä¸­å­¦ä¹ ï¼Œå…¶ä¸­å¥–åŠ±ä¿¡å·å……å½“å¼•å¯¼LLMè¡Œä¸ºçš„æŒ‡å¼•æ˜Ÿã€‚å®ƒå·²æˆä¸ºä¸€ç³»åˆ—æµè¡ŒæŠ€æœ¯çš„åŸºçŸ³ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFã€RLAIFã€DPOå’ŒGRPOï¼‰ã€å¥–åŠ±å¼•å¯¼è§£ç å’Œäº‹åä¿®æ­£ç­‰ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œè¿™ç§èŒƒå¼å®ç°äº†ä»è¢«åŠ¨å­¦ä¹ é™æ€æ•°æ®åˆ°ä¸»åŠ¨ä»åŠ¨æ€åé¦ˆä¸­å­¦ä¹ çš„è½¬å˜ã€‚è¿™ä¸ºLLMèµ‹äºˆäº†å¯¹é½åå¥½å’Œå¤šæ ·åŒ–ä»»åŠ¡çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»å¥–åŠ±æ¨¡å‹å’Œè·¨è®­ç»ƒã€æ¨ç†å’Œåæ¨ç†é˜¶æ®µçš„å­¦ä¹ ç­–ç•¥çš„è§’åº¦ï¼Œå…¨é¢æ¦‚è¿°äº†ä»å¥–åŠ±ä¸­å­¦ä¹ çš„æ¦‚å†µã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è®¨è®ºäº†å¥–åŠ±æ¨¡å‹çš„åŸºå‡†æµ‹è¯•å’Œä¸»è¦åº”ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚ç›¸å…³è®ºæ–‡é›†åˆè¯·è§ <a target="_blank" rel="noopener" href="https://github.com/bobxwu/learning-from-rewards-llm-papers%E3%80%82">https://github.com/bobxwu/learning-from-rewards-llm-papersã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02686v2">PDF</a> 36 Pages</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•å·²ä»é¢„è®­ç»ƒæ‰©å±•è½¬å‘åè®­ç»ƒä¸æµ‹è¯•æ—¶çš„æ‰©å±•ã€‚åœ¨è¿™äº›å‘å±•ä¸­ï¼Œä¸€ä¸ªå…³é”®ç»Ÿä¸€èŒƒå¼æ­£åœ¨å…´èµ·ï¼šåŸºäºå¥–åŠ±çš„å­¦ä¹ ã€‚å¥–åŠ±ä¿¡å·ä½œä¸ºæŒ‡å¯¼LLMè¡Œä¸ºçš„çš„å…³é”®ï¼Œå·²å¹¿æ³›åº”ç”¨äºå„ç§æµè¡ŒæŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFã€RLAIFã€DPOå’ŒGRPOï¼‰ã€å¥–åŠ±å¼•å¯¼è§£ç å’ŒåéªŒæ ¡æ­£ç­‰ã€‚è¯¥èŒƒå¼ä½¿LLMä»è¢«åŠ¨å­¦ä¹ é™æ€æ•°æ®è½¬å‘ä¸»åŠ¨ä»åŠ¨æ€åé¦ˆä¸­å­¦ä¹ ï¼Œä¸ºå¤šæ ·åŒ–ä»»åŠ¡æä¾›äº†å¯¹é½åå¥½å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†åŸºäºå¥–åŠ±çš„å­¦ä¹ ï¼Œæ¶µç›–äº†å¥–åŠ±æ¨¡å‹å’Œå­¦ä¹ ç­–ç•¥åœ¨è®­ç»ƒã€æ¨ç†å’Œåæ¨ç†é˜¶æ®µçš„è§†è§’ï¼Œå¹¶è¿›ä¸€æ­¥è®¨è®ºäº†å¥–åŠ±æ¨¡å‹çš„åŸºå‡†å’Œä¸»è¦åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å·²ç»ä»é¢„è®­ç»ƒæ‰©å±•è½¬å‘äº†åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´æ‰©å±•ã€‚</li>
<li>åŸºäºå¥–åŠ±çš„å­¦ä¹ æ˜¯LLMå‘å±•ä¸­çš„å…³é”®ç»Ÿä¸€èŒƒå¼ã€‚</li>
<li>å¥–åŠ±ä¿¡å·åœ¨æŒ‡å¯¼LLMè¡Œä¸ºä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</li>
<li>åŸºäºå¥–åŠ±çš„å­¦ä¹ å·²ç»åº”ç”¨äºå¤šç§æŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ã€å¥–åŠ±å¼•å¯¼è§£ç å’ŒåéªŒæ ¡æ­£ã€‚</li>
<li>åŸºäºå¥–åŠ±çš„å­¦ä¹ ä½¿LLMèƒ½å¤Ÿä»åŠ¨æ€åé¦ˆä¸­ä¸»åŠ¨å­¦ä¹ ï¼Œä¸ºå¤šæ ·åŒ–ä»»åŠ¡æä¾›å¯¹é½åå¥½å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« å…¨é¢æ¦‚è¿°äº†åŸºäºå¥–åŠ±çš„å­¦ä¹ ï¼ŒåŒ…æ‹¬å…¶åœ¨è®­ç»ƒã€æ¨ç†å’Œåæ¨ç†é˜¶æ®µçš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5aace9210abe630d677e3a5eafea9f61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ec48bcdc9ad1835e16a6dabb184f9a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a1258b5e5ebc11cc5968a0bd9478b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f55fc02860c598719485487386b5d711.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improving-LLM-Safety-Alignment-with-Dual-Objective-Optimization"><a href="#Improving-LLM-Safety-Alignment-with-Dual-Objective-Optimization" class="headerlink" title="Improving LLM Safety Alignment with Dual-Objective Optimization"></a>Improving LLM Safety Alignment with Dual-Objective Optimization</h2><p><strong>Authors:Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</strong></p>
<p>Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at <a target="_blank" rel="noopener" href="https://github.com/wicai24/DOOR-Alignment">https://github.com/wicai24/DOOR-Alignment</a> </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒæ—¶å®‰å…¨å¯¹é½æŠ€æœ¯ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„å½±å“ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§å¹¿æ³›éƒ¨ç½²çš„å¯¹é½æ–¹æ³•ï¼Œå…¶åœ¨å®éªŒå’Œç†è®ºç¯å¢ƒä¸­å‡å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå…¶æŸå¤±å‡½æ•°å¯¹äºæ‹’ç»å­¦ä¹ æ¥è¯´å¹¶ä¸ç†æƒ³ã€‚é€šè¿‡åŸºäºæ¢¯åº¦çš„åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«äº†è¿™äº›ä¸è¶³ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†DPOç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªç»„ä»¶ï¼šï¼ˆ1ï¼‰ç¨³å¥æ‹’ç»è®­ç»ƒï¼Œå³ä½¿åœ¨äº§ç”Ÿéƒ¨åˆ†ä¸å®‰å…¨ç”Ÿæˆç‰©çš„æƒ…å†µä¸‹ä¹Ÿé¼“åŠ±æ‹’ç»ï¼›ï¼ˆ2ï¼‰æœ‰é’ˆå¯¹æ€§åœ°å¿˜è®°æœ‰å®³çŸ¥è¯†ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†LLMå¯¹å„ç§è¶Šç‹±æ”»å‡»çš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬è·¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„é¢„å¡«å……ã€åç¼€å’Œå¤šè½®æ”»å‡»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŸºäºå¥–åŠ±çš„ä»¤ç‰Œçº§åŠ æƒæœºåˆ¶æ¥è¿›è¡Œæ‹’ç»å­¦ä¹ ï¼Œä»è€Œå¼ºè°ƒå…³é”®çš„æ‹’ç»ä»¤ç‰Œï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†å¯¹æŠ—æ¶æ„åˆ©ç”¨æ—¶çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¯¹è¶Šç‹±æ”»å‡»çš„ç¨³å¥æ€§ä¸è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»¤ç‰Œåˆ†å¸ƒè½¬å˜ä»¥åŠæ‹’ç»å’Œæœ‰å®³ä»¤ç‰Œçš„å†…éƒ¨è¡¨ç¤ºæœ‰å…³ï¼Œä¸ºæœªæ¥çš„LLMå®‰å…¨å¯¹é½ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æ–¹å‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wicai24/DOOR-Alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wicai24/DOOR-Alignmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03710v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒæ—¶çš„å®‰å…¨å¯¹é½æŠ€æœ¯é¢ä¸´ç‰¢ç‹±çªç ´æ”»å‡»çš„é£é™©ã€‚æœ¬æ–‡æŒ‡å‡ºäº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿™ä¸€å¹¿æ³›ä½¿ç”¨çš„å¯¹é½æ–¹æ³•åœ¨å®éªŒå’Œç†è®ºä¸Šçš„å±€é™æ€§ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ¢¯åº¦åˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œå°†DPOç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªç»„ä»¶ï¼šä¸€æ˜¯å¥å£®çš„æ‹’ç»è®­ç»ƒï¼Œé¼“åŠ±å³ä½¿åœ¨éƒ¨åˆ†ä¸å®‰å…¨ç”Ÿæˆçš„æƒ…å†µä¸‹ä¹Ÿè¿›è¡Œæ‹’ç»ï¼›äºŒæ˜¯é’ˆå¯¹æ€§åœ°é—å¿˜æœ‰å®³çŸ¥è¯†ã€‚æ­¤æ–¹æ³•å¤§å¤§æé«˜äº†LLMå¯¹å„ç§ç‰¢ç‹±çªç ´æ”»å‡»çš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬å¡«å……ã€åç¼€å’Œå¤šè½®æ”»å‡»ç­‰ï¼Œå¹¶åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–åœºæ™¯ä¸­å‡è¡¨ç°ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§é€šè¿‡å¥–åŠ±æœºåˆ¶å¯¹æ‹’ç»å­¦ä¹ è¿›è¡Œä»¤ç‰Œçº§åŠ æƒçš„æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜äº†å¯¹å¯¹æŠ—æ€§åˆ©ç”¨çš„ç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¯¹ç‰¢ç‹±çªç ´æ”»å‡»çš„ç¨³å¥æ€§ä¸è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»¤ç‰Œåˆ†å¸ƒè½¬å˜ä»¥åŠæ‹’ç»å’Œæœ‰å®³ä»¤ç‰Œçš„å†…éƒ¨è¡¨å¾æœ‰å…³ï¼Œä¸ºæœªæ¥LLMå®‰å…¨å¯¹é½çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒæ—¶çš„å®‰å…¨å¯¹é½é¢ä¸´ç‰¢ç‹±çªç ´æ”»å‡»çš„é£é™©ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>æ”¹è¿›çš„å®‰å…¨å¯¹é½æ–¹æ³•åŒ…æ‹¬å¥å£®çš„æ‹’ç»è®­ç»ƒå’Œé’ˆå¯¹æ€§åœ°é—å¿˜æœ‰å®³çŸ¥è¯†ã€‚</li>
<li>è¿™ç§æ–¹æ³•æé«˜äº†LLMå¯¹å„ç§ç‰¢ç‹±çªç ´æ”»å‡»çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é€šè¿‡å¥–åŠ±æœºåˆ¶å¯¹æ‹’ç»å­¦ä¹ è¿›è¡Œä»¤ç‰Œçº§åŠ æƒçš„æ–¹æ³•ã€‚</li>
<li>ç¨³å¥æ€§ä¸è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»¤ç‰Œåˆ†å¸ƒè½¬å˜å’Œå†…éƒ¨è¡¨å¾æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d75be47ebdabf387e6a1840aee9b9ef1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0525f5f9412251de311303dfab8f26cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606b348b91950dc35d5c9e6d61f3635b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07200417e4d883120e50f869825a7dd1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficiently-Identifying-Watermarked-Segments-in-Mixed-Source-Texts"><a href="#Efficiently-Identifying-Watermarked-Segments-in-Mixed-Source-Texts" class="headerlink" title="Efficiently Identifying Watermarked Segments in Mixed-Source Texts"></a>Efficiently Identifying Watermarked Segments in Mixed-Source Texts</h2><p><strong>Authors:Xuandong Zhao, Chenwen Liao, Yu-Xiang Wang, Lei Li</strong></p>
<p>Text watermarks in large language models (LLMs) are increasingly used to detect synthetic text, mitigating misuse cases like fake news and academic dishonesty. While existing watermarking detection techniques primarily focus on classifying entire documents as watermarked or not, they often neglect the common scenario of identifying individual watermark segments within longer, mixed-source documents. Drawing inspiration from plagiarism detection systems, we propose two novel methods for partial watermark detection. First, we develop a geometry cover detection framework aimed at determining whether there is a watermark segment in long text. Second, we introduce an adaptive online learning algorithm to pinpoint the precise location of watermark segments within the text. Evaluated on three popular watermarking techniques (KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves high accuracy, significantly outperforming baseline methods. Moreover, our framework is adaptable to other watermarking techniques, offering new insights for precise watermark detection. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/llm-watermark-location">https://github.com/XuandongZhao/llm-watermark-location</a> </p>
<blockquote>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–‡æœ¬æ°´å°è¶Šæ¥è¶Šè¢«ç”¨æ¥æ£€æµ‹åˆæˆæ–‡æœ¬ï¼Œå‡è½»æ»¥ç”¨æƒ…å†µï¼Œå¦‚å‡æ–°é—»å’Œå­¦æœ¯ä¸ç«¯è¡Œä¸ºã€‚å°½ç®¡ç°æœ‰çš„æ°´å°æ£€æµ‹æŠ€æœ¯åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾§é‡äºå°†æ•´ä¸ªæ–‡æ¡£åˆ†ç±»ä¸ºæ°´å°æ–‡æ¡£æˆ–éæ°´å°æ–‡æ¡£ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå¿½ç•¥åœ¨è¾ƒé•¿ã€æ··åˆæ¥æºçš„æ–‡æ¡£ä¸­è¯†åˆ«å•ä¸ªæ°´å°ç‰‡æ®µçš„å¸¸è§åœºæ™¯ã€‚ä»æŠ„è¢­æ£€æµ‹ç³»ç»Ÿä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç”¨äºéƒ¨åˆ†æ°´å°æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå‡ ä½•è¦†ç›–æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨ç¡®å®šé•¿æ–‡æœ¬ä¸­æ˜¯å¦å­˜åœ¨æ°´å°ç‰‡æ®µã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”åœ¨çº¿å­¦ä¹ ç®—æ³•ï¼Œä»¥å‡†ç¡®ç¡®å®šæ–‡æœ¬ä¸­æ°´å°ç‰‡æ®µçš„ç¡®åˆ‡ä½ç½®ã€‚æˆ‘ä»¬åœ¨ä¸‰ç§æµè¡Œçš„æ°´å°æŠ€æœ¯ï¼ˆKGW-Watermarkã€Unigram-Watermarkå’ŒGumbel-Watermarkï¼‰ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯é€‚åº”å…¶ä»–æ°´å°æŠ€æœ¯ï¼Œä¸ºç²¾ç¡®çš„æ°´å°æ£€æµ‹æä¾›äº†æ–°çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/llm-watermark-location">https://github.com/XuandongZhao/llm-watermark-location</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03600v2">PDF</a> ACL 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ°´å°åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ç”¨äºæ£€æµ‹åˆæˆæ–‡æœ¬ï¼Œç¼“è§£è™šå‡æ–°é—»å’Œå­¦æœ¯ä¸ç«¯ç­‰æ»¥ç”¨æƒ…å†µã€‚ç°æœ‰æ°´å°æ£€æµ‹æŠ€æœ¯åœ¨è¯†åˆ«æ•´ä¸ªæ–‡æ¡£æ˜¯å¦å¸¦æœ‰æ°´å°æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¯†åˆ«æ··åˆæ¥æºæ–‡æ¡£ä¸­å•ä¸ªæ°´å°ç‰‡æ®µæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬ç ”ç©¶å€Ÿé‰´æŠ„è¢­æ£€æµ‹ç³»ç»Ÿï¼Œæå‡ºä¸¤ç§æ–°å‹éƒ¨åˆ†æ°´å°æ£€æµ‹æ–¹æ³•ã€‚é¦–å…ˆï¼Œå¼€å‘å‡ ä½•è¦†ç›–æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨ç¡®å®šé•¿æ–‡æœ¬ä¸­æ˜¯å¦å­˜åœ¨æ°´å°ç‰‡æ®µï¼›å…¶æ¬¡ï¼Œå¼•å…¥è‡ªé€‚åº”åœ¨çº¿å­¦ä¹ ç®—æ³•ï¼Œç²¾ç¡®å®šä½æ–‡æœ¬ä¸­æ°´å°ç‰‡æ®µçš„ä½ç½®ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ç§æµè¡Œæ°´å°æŠ€æœ¯ä¸Šçš„å‡†ç¡®æ€§é«˜ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸”å¯é€‚åº”å…¶ä»–æ°´å°æŠ€æœ¯ï¼Œä¸ºç²¾ç¡®æ°´å°æ£€æµ‹æä¾›æ–°è§è§£ã€‚ç›¸å…³ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/llm-watermark-location">https://github.com/XuandongZhao/llm-watermark-location</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æ°´å°åœ¨LLMä¸­ç”¨äºæ£€æµ‹åˆæˆæ–‡æœ¬ï¼Œåº”å¯¹è™šå‡æ–°é—»å’Œå­¦æœ¯ä¸ç«¯é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ°´å°æ£€æµ‹æŠ€æœ¯ä¸»è¦å…³æ³¨æ•´ä¸ªæ–‡æ¡£çš„è¯†åˆ«ï¼Œä½†åœ¨è¯†åˆ«æ··åˆæ–‡æ¡£ä¸­çš„å•ä¸ªæ°´å°ç‰‡æ®µæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºä¸¤ç§æ–°å‹éƒ¨åˆ†æ°´å°æ£€æµ‹æ–¹æ³•ï¼šå‡ ä½•è¦†ç›–æ£€æµ‹æ¡†æ¶å’Œè‡ªé€‚åº”åœ¨çº¿å­¦ä¹ ç®—æ³•ã€‚</li>
<li>å‡ ä½•è¦†ç›–æ£€æµ‹æ¡†æ¶æ—¨åœ¨ç¡®å®šé•¿æ–‡æœ¬ä¸­æ˜¯å¦å­˜åœ¨æ°´å°ç‰‡æ®µã€‚</li>
<li>è‡ªé€‚åº”åœ¨çº¿å­¦ä¹ ç®—æ³•å¯ç²¾ç¡®å®šä½æ–‡æœ¬ä¸­æ°´å°ç‰‡æ®µçš„ä½ç½®ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸‰ç§æµè¡Œæ°´å°æŠ€æœ¯ä¸Šçš„å‡†ç¡®æ€§é«˜ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6abb02c30f56f817277749b7a4761433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cb6459cfc2da2dfeb46eb68dc030b87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832f732fdaba68262fb2809cc9819d8e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLM-Cure-LLM-based-Competitor-User-Review-Analysis-for-Feature-Enhancement"><a href="#LLM-Cure-LLM-based-Competitor-User-Review-Analysis-for-Feature-Enhancement" class="headerlink" title="LLM-Cure: LLM-based Competitor User Review Analysis for Feature   Enhancement"></a>LLM-Cure: LLM-based Competitor User Review Analysis for Feature   Enhancement</h2><p><strong>Authors:Maram Assi, Safwat Hassan, Ying Zou</strong></p>
<p>The exponential growth of the mobile app market underscores the importance of constant innovation and rapid response to user demands. As user satisfaction is paramount to the success of a mobile application (app), developers typically rely on user reviews, which represent user feedback that includes ratings and comments to identify areas for improvement. However, the sheer volume of user reviews poses challenges in manual analysis, necessitating automated approaches. Existing automated approaches either analyze only the target apps reviews, neglecting the comparison of similar features to competitors or fail to provide suggestions for feature enhancement. To address these gaps, we propose a Large Language Model (LLM)-based Competitive User Review Analysis for Feature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically generate suggestion s for mobile app feature improvements. More specifically, LLM-Cure identifies and categorizes features within reviews by applying LLMs. When provided with a complaint in a user review, LLM-Cure curates highly rated (4 and 5 stars) reviews in competing apps related to the complaint and proposes potential improvements tailored to the target application. We evaluate LLM-Cure on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates that LLM-Cure significantly outperforms the state-of-the-art approaches in assigning features to reviews by up to 13% in F1-score, up to 16% in recall and up to 11% in precision. Additionally, LLM-Cure demonstrates its capability to provide suggestions for resolving user complaints. We verify the suggestions using the release notes that reflect the changes of features in the target mobile app. LLM-Cure achieves a promising average of 73% of the implementation of the provided suggestions. </p>
<blockquote>
<p>ç§»åŠ¨åº”ç”¨å¸‚åœºçš„æŒ‡æ•°çº§å¢é•¿çªæ˜¾äº†æŒç»­åˆ›æ–°å’Œè¿…é€Ÿå“åº”ç”¨æˆ·éœ€æ±‚çš„é‡è¦æ€§ã€‚å¯¹äºç§»åŠ¨åº”ç”¨ï¼ˆAPPï¼‰æ¥è¯´ï¼Œç”¨æˆ·æ»¡æ„åº¦æ˜¯æˆåŠŸçš„å…³é”®ï¼Œå› æ­¤å¼€å‘è€…é€šå¸¸ä¾èµ–ç”¨æˆ·è¯„è®ºæ¥äº†è§£ç”¨æˆ·åé¦ˆï¼ŒåŒ…æ‹¬è¯„åˆ†å’Œè¯„è®ºï¼Œä»¥æ‰¾å‡ºæ”¹è¿›çš„é¢†åŸŸã€‚ç„¶è€Œï¼Œå¤§é‡çš„ç”¨æˆ·è¯„è®ºç»™æ‰‹åŠ¨åˆ†æå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œéœ€è¦è‡ªåŠ¨åŒ–çš„æ–¹æ³•ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•è¦ä¹ˆåªåˆ†æç›®æ ‡åº”ç”¨çš„è¯„è®ºï¼Œå¿½è§†ä¸ç«äº‰å¯¹æ‰‹ç›¸ä¼¼åŠŸèƒ½çš„æ¯”è¾ƒï¼Œè¦ä¹ˆæ— æ³•æä¾›åŠŸèƒ½å¢å¼ºçš„å»ºè®®ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«äº‰æ€§ç”¨æˆ·è¯„è®ºåˆ†æåŠŸèƒ½å¢å¼ºï¼ˆLLM-Cureï¼‰æ–¹æ³•ã€‚LLM-Cureæ–¹æ³•åˆ©ç”¨LLMsè‡ªåŠ¨ä¸ºç›®æ ‡åº”ç”¨ç”Ÿæˆæ”¹è¿›å»ºè®®ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒLLM-Cureé€šè¿‡åº”ç”¨LLMsæ¥è¯†åˆ«å’Œåˆ†ç±»è¯„è®ºä¸­çš„åŠŸèƒ½ã€‚å½“æ”¶åˆ°ç”¨æˆ·è¯„è®ºä¸­çš„æŠ•è¯‰æ—¶ï¼ŒLLM-Cureä¼šç­–åˆ’ä¸æŠ•è¯‰ç›¸å…³çš„ç«äº‰å¯¹æ‰‹çš„é«˜è¯„åˆ†ï¼ˆ4æ˜Ÿå’Œ5æ˜Ÿï¼‰è¯„è®ºï¼Œå¹¶é’ˆå¯¹ç›®æ ‡åº”ç”¨æå‡ºé’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®ã€‚æˆ‘ä»¬å¯¹70ä¸ªæµè¡Œçš„Androidåº”ç”¨çš„105ä¸‡ç¯‡è¯„è®ºè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨åˆ†é…åŠŸèƒ½åˆ°è¯„è®ºæ–¹é¢ï¼ŒLLM-Cureæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒF1åˆ†æ•°æé«˜è¾¾13%ï¼Œå¬å›ç‡æé«˜è¾¾16%ï¼Œå‡†ç¡®ç‡æé«˜è¾¾11%ã€‚æ­¤å¤–ï¼ŒLLM-Cureæ˜¾ç¤ºå‡ºæä¾›è§£å†³ç”¨æˆ·æŠ•è¯‰å»ºè®®çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡åæ˜ ç›®æ ‡ç§»åŠ¨åº”ç”¨åŠŸèƒ½å˜åŒ–çš„å‘å¸ƒè¯´æ˜éªŒè¯äº†è¿™äº›å»ºè®®ã€‚LLM-Cureåœ¨å®æ–½æä¾›çš„å»ºè®®æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„73%çš„å¹³å‡æˆç»©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15724v2">PDF</a> 25 pages</p>
<p><strong>Summary</strong></p>
<p>åœ¨ç§»åŠ¨åº”ç”¨å¸‚åœºä¸­ï¼Œç”¨æˆ·åé¦ˆçš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ï¼Œåˆ›æ–°é€Ÿåº¦å’Œå“åº”ç”¨æˆ·éœ€æ±‚çš„è¿…é€Ÿæ€§æ˜¯æ¨åŠ¨åº”ç”¨æˆåŠŸçš„é‡è¦å› ç´ ã€‚ä¸ºäº†æ›´å¥½åœ°åº”å¯¹æŒ‘æˆ˜å¹¶æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«äº‰æ€§ç”¨æˆ·è¯„è®ºåˆ†æåŠŸèƒ½å¢å¼ºæ–¹æ³•ï¼ˆLLM-Cureï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨åˆ†æç”¨æˆ·è¯„è®ºå¹¶ç”Ÿæˆé’ˆå¯¹æ€§çš„ç§»åŠ¨åº”ç”¨åŠŸèƒ½æ”¹è¿›å»ºè®®ã€‚LLM-Cureä¸ä»…èƒ½å¤Ÿè¯†åˆ«å’Œåˆ†ç±»è¯„è®ºä¸­çš„åŠŸèƒ½ï¼Œè¿˜å¯ä»¥é’ˆå¯¹ç”¨æˆ·è¯„è®ºä¸­çš„æŠ•è¯‰ï¼Œæå–åŒç±»ç«äº‰åº”ç”¨çš„é«˜åº¦è¯„ä»·è¯„è®ºï¼Œæå‡ºé’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®ã€‚å®éªŒè¯æ˜ï¼ŒLLM-Cureåœ¨åŠŸèƒ½è¯„ä»·æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å…·æœ‰è¾ƒå¼ºçš„æä¾›æ”¹è¿›å»ºè®®çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨åº”ç”¨å¸‚åœºå‘ˆç°æŒ‡æ•°çº§å¢é•¿ï¼Œçªæ˜¾å‡ºä¸æ–­åˆ›æ–°å’Œè¿…é€Ÿå“åº”ç”¨æˆ·éœ€æ±‚çš„é‡è¦æ€§ã€‚</li>
<li>ç”¨æˆ·åé¦ˆå¯¹äºç§»åŠ¨åº”ç”¨æˆåŠŸè‡³å…³é‡è¦ï¼Œå…¶ä¸­ç”¨æˆ·è¯„è®ºåŒ…å«ä¸°å¯Œçš„ç”¨æˆ·åé¦ˆï¼ŒåŒ…æ‹¬è¯„åˆ†å’Œè¯„è®ºã€‚</li>
<li>é¢å¯¹å¤§é‡ç”¨æˆ·è¯„è®ºçš„æŒ‘æˆ˜ï¼Œéœ€è¦è‡ªåŠ¨åŒ–åˆ†ææ–¹æ³•ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å…¨é¢åˆ†æç›®æ ‡åº”ç”¨çš„è¯„è®ºæˆ–ä¸å…¶ä»–ç«å“è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>LLM-Cureè§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åˆ†æå’Œè¯†åˆ«ç”¨æˆ·è¯„è®ºä¸­çš„åŠŸèƒ½å’Œé—®é¢˜ï¼Œä¸ºç§»åŠ¨åº”ç”¨æä¾›é’ˆå¯¹æ€§çš„åŠŸèƒ½æ”¹è¿›å»ºè®®ã€‚</li>
<li>LLM-Cureèƒ½å¤Ÿé’ˆå¯¹ç”¨æˆ·è¯„è®ºä¸­çš„æŠ•è¯‰ï¼Œæå–ç«å“çš„é«˜åº¦è¯„ä»·è¯„è®ºä½œä¸ºå‚è€ƒï¼Œæå‡ºæ”¹è¿›å»ºè®®ã€‚</li>
<li>å®éªŒè¯æ˜LLM-Cureåœ¨åŠŸèƒ½è¯„ä»·æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„F1å¾—åˆ†ã€å¬å›ç‡å’Œç²¾ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f9389bd72e4eb502f755d201f769050.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a59bcb22013fdf2398be3121d1865a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99bef848ef9ff12ed1722a4d4ae00999.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Weak-to-Strong-Jailbreaking-on-Large-Language-Models"><a href="#Weak-to-Strong-Jailbreaking-on-Large-Language-Models" class="headerlink" title="Weak-to-Strong Jailbreaking on Large Language Models"></a>Weak-to-Strong Jailbreaking on Large Language Models</h2><p><strong>Authors:Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang</strong></p>
<p>Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient inference time attack for aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attackâ€™s key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe modelâ€™s decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at <a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong">https://github.com/XuandongZhao/weak-to-strong</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œå¯¼è‡´ç”Ÿæˆæœ‰å®³ã€ä¸é“å¾·æˆ–åè§çš„æ–‡æœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¶Šç‹±æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¼±åˆ°å¼ºçš„è¶Šç‹±æ”»å‡»ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ”»å‡»ï¼Œç”¨äºé’ˆå¯¹å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰å®³æ–‡æœ¬ã€‚æˆ‘ä»¬çš„ä¸»è¦ç›´è§‰æ˜¯åŸºäºè¿™æ ·çš„è§‚å¯Ÿï¼šè¶Šç‹±å’Œå¯¹é½çš„æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ä»…åœ¨äºå®ƒä»¬çš„åˆå§‹è§£ç åˆ†å¸ƒã€‚å¼±åˆ°å¼ºæ”»å‡»çš„å…³é”®æŠ€æœ¯æ´å¯ŸåŠ›åœ¨äºä½¿ç”¨ä¸¤ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆä¸€ä¸ªå®‰å…¨æ¨¡å‹å’Œä¸€ä¸ªä¸å®‰å…¨æ¨¡å‹ï¼‰æ¥å¯¹æŠ—æ€§åœ°ä¿®æ”¹ä¸€ä¸ªæ›´å¤§çš„å®‰å…¨æ¨¡å‹çš„è§£ç æ¦‚ç‡ã€‚æˆ‘ä»¬åœ¨æ¥è‡ªä¸‰ä¸ªç»„ç»‡çš„äº”ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°äº†å¼±åˆ°å¼ºæ”»å‡»ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå°†é”™ä½ç‡æé«˜åˆ°è¶…è¿‡99%ï¼Œæ¯ä¸ªä¾‹å­åªéœ€è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ é€’ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªç´§è¿«çš„å®‰å…¨é—®é¢˜ï¼Œéœ€è¦åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ—¶äºˆä»¥è§£å†³ã€‚ä½œä¸ºåˆæ­¥å°è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é˜²å¾¡ç­–ç•¥æ¥ä¿æŠ¤å…å—æ­¤ç±»æ”»å‡»ï¼Œä½†åˆ›å»ºæ›´å…ˆè¿›çš„é˜²å¾¡æªæ–½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¤åˆ¶è¯¥æ–¹æ³•çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XuandongZhao/weak-to-strongæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.17256v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜“å—åˆ°â€œè¶Šç‹±æ”»å‡»â€ï¼Œå¯¼è‡´ç”Ÿæˆæœ‰å®³ã€ä¸é“å¾·æˆ–åè§çš„æ–‡æœ¬ã€‚ç°æœ‰è¶Šç‹±æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºå¼±åˆ°å¼ºè¶Šç‹±æ”»å‡»ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ”»å‡»ï¼Œç”¨äºé’ˆå¯¹å¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹äº§ç”Ÿæœ‰å®³æ–‡æœ¬ã€‚æˆ‘ä»¬çš„å…³é”®ç›´è§‰æ˜¯åŸºäºè§‚å¯Ÿï¼Œå³è¶Šç‹±å’Œå¯¹é½çš„æ¨¡å‹åªåœ¨åˆå§‹è§£ç åˆ†å¸ƒä¸Šæœ‰æ‰€ä¸åŒã€‚å¼±åˆ°å¼ºæ”»å‡»çš„å…³é”®æŠ€æœ¯è§è§£æ˜¯ä½¿ç”¨ä¸¤ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆä¸€ä¸ªå®‰å…¨å’Œä¸€ä¸ªä¸å®‰å…¨çš„æ¨¡å‹ï¼‰æ¥å¯¹æŠ—æ€§åœ°ä¿®æ”¹ä¸€ä¸ªæ›´å¤§çš„å®‰å…¨æ¨¡å‹çš„è§£ç æ¦‚ç‡ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¼€æºLLMä¸Šè¯„ä¼°äº†å¼±åˆ°å¼ºæ”»å‡»ï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå°†é”™ä½ç‡æé«˜åˆ°è¶…è¿‡99%ï¼Œæ¯ä¸ªä¾‹å­åªéœ€ä¸€æ¬¡å‰å‘ä¼ é€’ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å¯¹é½LLMæ—¶äºŸå¾…è§£å†³çš„é‡å¤§é—®é¢˜ã€‚ä½œä¸ºåˆæ­¥å°è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é˜²å¾¡ç­–ç•¥æ¥å¯¹æŠ—æ­¤ç±»æ”»å‡»ï¼Œä½†åˆ›å»ºæ›´å…ˆè¿›çš„é˜²å¾¡æªæ–½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé¢ä¸´â€œè¶Šç‹±æ”»å‡»â€çš„é£é™©ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆæœ‰å®³ã€ä¸é“å¾·æˆ–åè§çš„æ–‡æœ¬ã€‚</li>
<li>ç°æœ‰è¶Šç‹±æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>å¼±åˆ°å¼ºè¶Šç‹±æ”»å‡»æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹å¤§è¯­è¨€æ¨¡å‹çš„è§£ç æ¦‚ç‡æ¥äº§ç”Ÿæœ‰å®³æ–‡æœ¬ã€‚</li>
<li>å¼±åˆ°å¼ºæ”»å‡»åŸºäºä¸¤ä¸ªè¾ƒå°æ¨¡å‹çš„å¯¹æŠ—æ€§ä¿®æ”¹æ¥å®ç°å¯¹å¤§å‹å®‰å…¨æ¨¡å‹çš„æ”»å‡»ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªLLMä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºé«˜é”™ä½ç‡ã€‚</li>
<li>å¯¹é½LLMæ—¶å­˜åœ¨äºŸå¾…è§£å†³çš„é‡å¤§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.17256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5cf04f45b0d982ba02648043fe90bd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f338bbab6820a6802371d02328f413c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9506ab26adfc6608d9078259eb5ab3fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ba39c18bc457bb3f75da136820f581.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GPT4RoI-Instruction-Tuning-Large-Language-Model-on-Region-of-Interest"><a href="#GPT4RoI-Instruction-Tuning-Large-Language-Model-on-Region-of-Interest" class="headerlink" title="GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"></a>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</h2><p><strong>Authors:Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo</strong></p>
<p>Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code and model can be found at <a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a>. </p>
<blockquote>
<p>é€šè¿‡åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šè°ƒæ•´è§†è§‰æŒ‡ä»¤ä»¥å¤„ç†å›¾åƒæ–‡æœ¬å¯¹ï¼Œå·²ç»å®ç°äº†é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åŒºåŸŸæ–‡æœ¬å¯¹ï¼Œé™åˆ¶äº†å…¶åœ¨ç²¾ç»†ç²’åº¦å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç©ºé—´æŒ‡ä»¤è°ƒæ•´ï¼ˆSpatial Instruction Tuningï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‡ä»¤ä¸­å¼•å…¥äº†æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRegion of Interestï¼ŒRoIï¼‰çš„å¼•ç”¨ã€‚åœ¨å‘é€åˆ°LLMä¹‹å‰ï¼Œè¯¥å¼•ç”¨è¢«æ›¿æ¢ä¸ºRoIç‰¹å¾ï¼Œå¹¶ä¸è¯­è¨€åµŒå…¥äº¤æ›¿ä½œä¸ºåºåˆ—ã€‚æˆ‘ä»¬çš„æ¨¡å‹GPT4RoIåœ¨7ä¸ªåŒºåŸŸæ–‡æœ¬å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ä¹‹å‰çš„å›¾åƒçº§åˆ«æ¨¡å‹ç›¸æ¯”ï¼Œå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„äº¤äº’å’Œå¯¹è¯ä½“éªŒã€‚ï¼ˆ1ï¼‰è¶…è¶Šè¯­è¨€çš„äº¤äº’ï¼šç”¨æˆ·å¯ä»¥é€šè¿‡è¯­è¨€å’Œç»˜åˆ¶è¾¹ç•Œæ¡†ä¸æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä»¥çµæ´»åœ°è°ƒæ•´å¼•ç”¨ç²’åº¦ã€‚ï¼ˆ2ï¼‰å¤šåŠŸèƒ½å¤šæ¨¡æ€èƒ½åŠ›ï¼šGPT4RoIå¯ä»¥æŒ–æ˜æ¯ä¸ªRoIå†…çš„å„ç§å±æ€§ä¿¡æ¯ï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥åŸºäºå¸¸è¯†å¯¹å¤šä¸ªRoIè¿›è¡Œæ¨ç†ã€‚åœ¨è§†è§‰å¸¸è¯†æ¨ç†ï¼ˆVCRï¼‰æ•°æ®é›†ä¸Šï¼ŒGPT4RoIå–å¾—äº†81.6%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ˆç¬¬äºŒåæ˜¯75.6%ï¼‰ï¼Œå‡ ä¹è¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½85.0%ã€‚ä»£ç å’Œæ¨¡å‹å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03601v5">PDF</a> ECCV2024-Workshop, Camera-ready</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„èƒ½åŠ›å·²è¾¾æˆé€šç”¨å‹è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åŒºåŸŸæ–‡æœ¬å¯¹ï¼Œé™åˆ¶äº†å…¶åœ¨ç²¾ç»†ç²’åº¦å¤šæ¨¡å¼ç†è§£ä¸Šçš„è¿›å±•ã€‚æœ¬æ–‡æå‡ºç©ºé—´æŒ‡ä»¤è°ƒæ•´ï¼Œå¼•å…¥æŒ‡ä»¤ä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰å‚è€ƒã€‚åœ¨å‘é€åˆ°LLMä¹‹å‰ï¼Œå‚è€ƒè¢«æ›¿æ¢ä¸ºRoIç‰¹å¾ï¼Œå¹¶ä¸è¯­è¨€åµŒå…¥äº¤é”™ä½œä¸ºåºåˆ—ã€‚æˆ‘ä»¬çš„æ¨¡å‹GPT4RoIï¼Œåœ¨7ä¸ªåŒºåŸŸæ–‡æœ¬å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ä¹‹å‰çš„å›¾åƒçº§åˆ«æ¨¡å‹ç›¸æ¯”ï¼Œå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„äº¤äº’å¼å’Œå¯¹è¯ä½“éªŒã€‚ï¼ˆ1ï¼‰è¶…è¶Šè¯­è¨€çš„äº¤äº’ï¼šç”¨æˆ·å¯ä»¥é€šè¿‡è¯­è¨€å’Œç»˜åˆ¶è¾¹ç•Œæ¡†ä¸æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œçµæ´»è°ƒæ•´å¼•ç”¨ç²’åº¦ã€‚ï¼ˆ2ï¼‰å¤šåŠŸèƒ½å¤šæ¨¡å¼èƒ½åŠ›ï¼šGPT4RoIå¯ä»¥æŒ–æ˜æ¯ä¸ªRoIå†…çš„å„ç§å±æ€§ä¿¡æ¯ï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥åŸºäºå¸¸è¯†å¯¹å¤šä¸ªRoIè¿›è¡Œæ¨ç†ã€‚åœ¨è§†è§‰å¸¸è¯†æ¨ç†ï¼ˆVCRï¼‰æ•°æ®é›†ä¸Šï¼ŒGPT4RoIå–å¾—äº†81.6%çš„æƒŠäººå‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ˆç¬¬äºŒåæ˜¯75.6%ï¼‰ï¼Œå‡ ä¹æ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½85.0%ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jshilong/GPT4RoI">https://github.com/jshilong/GPT4RoI</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç©ºé—´æŒ‡ä»¤è°ƒæ•´å¼•å…¥æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRoIï¼‰åˆ°æŒ‡ä»¤ä¸­ï¼Œæé«˜äº†LLMå¯¹å›¾åƒæ–‡æœ¬å¯¹çš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>GPT4RoIæ¨¡å‹èƒ½åœ¨ç²¾ç»†ç²’åº¦ä¸Šç†è§£å›¾åƒï¼Œé€šè¿‡è¯­è¨€å’Œç»˜åˆ¶è¾¹ç•Œæ¡†è¿›è¡Œäº¤äº’è°ƒæ•´ã€‚</li>
<li>GPT4RoIå…·æœ‰æŒ–æ˜å›¾åƒä¸­å¤šä¸ªå±æ€§çš„èƒ½åŠ›ï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶ã€æè´¨ã€åŠ¨ä½œç­‰ã€‚</li>
<li>GPT4RoIåœ¨è§†è§‰å¸¸è¯†æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡è¾¾åˆ°81.6%ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚</li>
<li>GPT4RoIå‡ ä¹æ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ï¼Œä½“ç°å…¶åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„å¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å¯å…¬å¼€è®¿é—®ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.03601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bacceaa5b739488680cd4a31fc507c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-944f644a69a7edec1a619783b9e14c67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e00f0f683d10ba6d3bf62809c67610.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae0aae73c23b336f88844796f445819e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  AutoMind Adaptive Knowledgeable Agent for Automated Data Science
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5a051708ed9f49f286318281ca651ba6.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  MMMG A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
