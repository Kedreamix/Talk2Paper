<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  Self-Adapting Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3ec1605afba63907ca3459385fafa44d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    39 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-14-æ›´æ–°"><a href="#2025-06-14-æ›´æ–°" class="headerlink" title="2025-06-14 æ›´æ–°"></a>2025-06-14 æ›´æ–°</h1><h2 id="Self-Adapting-Language-Models"><a href="#Self-Adapting-Language-Models" class="headerlink" title="Self-Adapting Language Models"></a>Self-Adapting Language Models</h2><p><strong>Authors:Adam Zweiger, Jyothish Pari, Han Guo, Ekin AkyÃ¼rek, Yoon Kim, Pulkit Agrawal</strong></p>
<p>Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the modelâ€™s own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at <a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal">https://jyopari.github.io/posts/seal</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å¼ºå¤§ä½†ç›¸å¯¹é™æ€ï¼Œå®ƒä»¬ç¼ºä¹é€‚åº”æ–°ä»»åŠ¡ã€çŸ¥è¯†æˆ–ç¤ºä¾‹çš„æœºåˆ¶æ¥è°ƒæ•´å…¶æƒé‡ã€‚æˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSEALï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿LLMèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆè‡ªå·±çš„å¾®è°ƒæ•°æ®å’Œæ›´æ–°æŒ‡ä»¤è¿›è¡Œè‡ªæˆ‘é€‚åº”ã€‚å¯¹äºæ–°çš„è¾“å…¥ï¼Œæ¨¡å‹ä¼šäº§ç”Ÿè‡ªæˆ‘ç¼–è¾‘çš„ä¸€ä»£äº§å“ï¼Œè¿™å¯èƒ½ä¼šä»¥ä¸åŒçš„æ–¹å¼é‡ç»„ä¿¡æ¯ï¼ŒæŒ‡å®šä¼˜åŒ–è¶…å‚æ•°ï¼Œæˆ–è°ƒç”¨æ•°æ®å¢å¼ºå·¥å…·å’ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°å·¥å…·ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™äº›è‡ªæˆ‘ç¼–è¾‘ä¼šå¯¼è‡´æŒä¹…çš„æƒé‡æ›´æ–°ï¼Œä»è€Œå®ç°æŒä¹…çš„é€‚åº”ã€‚ä¸ºäº†è®­ç»ƒæ¨¡å‹ä»¥äº§ç”Ÿæœ‰æ•ˆçš„è‡ªæˆ‘ç¼–è¾‘ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯ï¼Œä»¥æ›´æ–°æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚ä¸åŒäºä¾èµ–å•ç‹¬é€‚åº”æ¨¡å—æˆ–è¾…åŠ©ç½‘ç»œçš„å…ˆå‰æ–¹æ³•ï¼ŒSEALç›´æ¥ä½¿ç”¨æ¨¡å‹æœ¬èº«çš„ç”Ÿæˆæ¥æ§åˆ¶å…¶é€‚åº”è¿‡ç¨‹ã€‚åœ¨çŸ¥è¯†èå…¥å’Œå°‘é‡åœºæ™¯ä¸‹çš„é€šç”¨åŒ–å®éªŒè¡¨æ˜ï¼ŒSEALæ˜¯æœç€èƒ½å¤Ÿè‡ªæˆ‘æŒ‡å¯¼é€‚åº”çš„è¯­è¨€æ¨¡å‹æ–¹å‘è¿ˆå‡ºçš„ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ç½‘ç«™å’Œä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal">https://jyopari.github.io/posts/seal</a> ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10943v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹å¼ºå¤§ä½†é™æ€ï¼Œæ— æ³•é€‚åº”æ–°ä»»åŠ¡ã€çŸ¥è¯†æˆ–ç¤ºä¾‹ã€‚æˆ‘ä»¬æå‡ºSelf-Adapting LLMsï¼ˆSEALï¼‰æ¡†æ¶ï¼Œä½¿LLMsèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆè‡ªå·±çš„å¾®è°ƒæ•°æ®å’Œæ›´æ–°æŒ‡ä»¤è¿›è¡Œè‡ªæˆ‘é€‚åº”ã€‚ç»™å®šæ–°è¾“å…¥ï¼Œæ¨¡å‹äº§ç”Ÿè‡ªæˆ‘ç¼–è¾‘ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å®ç°æŒä¹…æƒé‡æ›´æ–°å’Œé•¿æœŸé€‚åº”ã€‚æˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯è®­ç»ƒæ¨¡å‹äº§ç”Ÿæœ‰æ•ˆçš„è‡ªæˆ‘ç¼–è¾‘ï¼Œä»¥æ›´æ–°æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚ä¸åŒäºä¾èµ–é¢å¤–é€‚åº”æ¨¡å—æˆ–è¾…åŠ©ç½‘ç»œçš„å…ˆå‰æ–¹æ³•ï¼ŒSEALç›´æ¥ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆæ¥æ§åˆ¶å…¶é€‚åº”è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsç¼ºä¹é€‚åº”æ–°ä»»åŠ¡ã€çŸ¥è¯†æˆ–ç¤ºä¾‹çš„æœºåˆ¶ã€‚</li>
<li>Self-Adapting LLMsï¼ˆSEALï¼‰æ¡†æ¶ä½¿LLMsèƒ½å¤Ÿè‡ªæˆ‘é€‚åº”ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç”Ÿæˆè‡ªæˆ‘ç¼–è¾‘å®ç°è‡ªæˆ‘é€‚åº”ï¼ŒåŒ…æ‹¬ä¿¡æ¯é‡ç»„ã€ä¼˜åŒ–è¶…å‚æ•°ã€æ•°æ®å¢å¼ºå’ŒåŸºäºæ¢¯åº¦çš„æ›´æ–°ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½¿è‡ªæˆ‘ç¼–è¾‘å¯¼è‡´æŒä¹…çš„æƒé‡æ›´æ–°å’Œé•¿æœŸé€‚åº”ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¾ªç¯è®­ç»ƒæ¨¡å‹äº§ç”Ÿæœ‰æ•ˆçš„è‡ªæˆ‘ç¼–è¾‘ã€‚</li>
<li>ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥è¯„ä¼°æ›´æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>SEALä¸åŒäºå…ˆå‰æ–¹æ³•ï¼Œå®ƒç›´æ¥ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ç”Ÿæˆæ§åˆ¶é€‚åº”è¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ec1605afba63907ca3459385fafa44d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f114ce44336f5ab60f35250dc70fe9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a753461583488b92e0b9e22800090050.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6b487a779381eff91524b491c9f3e49.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Diffusion-Duality"><a href="#The-Diffusion-Duality" class="headerlink" title="The Diffusion Duality"></a>The Diffusion Duality</h2><p><strong>Authors:Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, Volodymyr Kuleshov</strong></p>
<p>Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: <a target="_blank" rel="noopener" href="http://s-sahoo.github.io/duo">http://s-sahoo.github.io/duo</a> </p>
<blockquote>
<p>å‡åŒ€çŠ¶æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹å› å…¶å›ºæœ‰çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›è€Œå…·æœ‰å¿«é€Ÿæ–‡æœ¬ç”Ÿæˆçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸è¢«è‡ªå›å½’æ¨¡å‹å’Œæ©ç æ‰©æ•£æ¨¡å‹æ‰€è¶…è¶Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨ä¸€ä¸ªå…³é”®è§è§£æ¥ç¼©å°æ€§èƒ½å·®è·ï¼šå‡åŒ€çŠ¶æ€æ‰©æ•£è¿‡ç¨‹è‡ªç„¶æ¥è‡ªäºåŸºç¡€çš„é«˜æ–¯æ‰©æ•£ã€‚æˆ‘ä»¬çš„æ–¹æ³•Duoï¼Œä»é«˜æ–¯æ‰©æ•£è½¬ç§»å¼ºå¤§æŠ€æœ¯ï¼Œæé«˜äº†è®­ç»ƒå’Œé‡‡æ ·çš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”±é«˜æ–¯è¿‡ç¨‹å¼•å¯¼çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å‡å°‘æ–¹å·®ä½¿è®­ç»ƒé€Ÿåº¦ç¿»å€ã€‚ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥è®­ç»ƒçš„æ¨¡å‹åœ¨7ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„3ä¸ªä¸Šå®ç°äº†é›¶å°„å‡»å›°æƒ‘åº¦è¶…è¿‡è‡ªå›å½’æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ç¦»æ•£ä¸€è‡´æ€§è’¸é¦ï¼Œè¯¥ç®—æ³•å°†ä¸€è‡´æ€§è’¸é¦ä»è¿ç»­ç¯å¢ƒé€‚åº”åˆ°ç¦»æ•£ç¯å¢ƒã€‚è¯¥ç®—æ³•é€šè¿‡åŠ é€Ÿé‡‡æ ·é€Ÿåº¦ä¸¤ä¸ªæ•°é‡çº§ï¼Œå®ç°äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­çš„å°‘æ­¥éª¤ç”Ÿæˆã€‚æˆ‘ä»¬å·²åœ¨é¡¹ç›®é¡µé¢ä¸Šæä¾›äº†ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼š<a target="_blank" rel="noopener" href="http://s-sahoo.github.io/duo">http://s-sahoo.github.io/duo</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10892v1">PDF</a> ICML 2025. We provide the code at: <a target="_blank" rel="noopener" href="https://github.com/s-sahoo/duo">https://github.com/s-sahoo/duo</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå‡åŒ€æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å€Ÿé‰´é«˜æ–¯æ‰©æ•£è¿‡ç¨‹ä¸­çš„å…³é”®æŠ€æœ¯æ¥æå‡å…¶æ€§èƒ½ã€‚æ–‡ç« æå‡ºäº†ä¸¤ç§æ”¹è¿›ç­–ç•¥ï¼šä¸€æ˜¯å¼•å…¥é«˜æ–¯è¿‡ç¨‹å¼•å¯¼çš„è¯¾ç¨‹ä½“ç³»å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å‡å°‘æ–¹å·®æ¥åŠ å€è®­ç»ƒé€Ÿåº¦ï¼›äºŒæ˜¯åœ¨ç¦»æ•£ç¯å¢ƒä¸­é€‚é…ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œå®ç°äº†ç¦»æ•£ä¸€è‡´æ€§è’¸é¦ã€‚è¿™äº›æŠ€æœ¯ä½¿å¾—æ‰©æ•£è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬å›°æƒ‘åº¦åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šè‡ªå›å½’æ¨¡å‹ï¼Œå¹¶å®ç°äº†å¿«é€Ÿé‡‡æ ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå‡åŒ€æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•å…·æœ‰è‡ªæˆ‘æ ¡æ­£çš„æ½œåŠ›ï¼Œä½†ä¹‹å‰è¢«è‡ªå›å½’æ¨¡å‹å’Œæ©æ¨¡æ‰©æ•£æ¨¡å‹æ‰€è¶…è¶Šã€‚</li>
<li>æ–‡ç« æå‡ºäº†å€Ÿé‰´é«˜æ–¯æ‰©æ•£è¿‡ç¨‹ä¸­çš„æŠ€æœ¯æ¥æå‡å‡åŒ€æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥é«˜æ–¯è¿‡ç¨‹å¼•å¯¼çš„è¯¾ç¨‹ä½“ç³»å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å‡å°‘æ–¹å·®æ¥åŠ å€è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šè‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ç¦»æ•£ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½¿å¾—æ‰©æ•£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°å¿«é€Ÿé‡‡æ ·ï¼Œå¹¶è§£é”äº†å°‘æ­¥ç”Ÿæˆçš„èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†å°†è¿ç»­è®¾ç½®ä¸­çš„æŠ€æœ¯é€‚é…åˆ°ç¦»æ•£ç¯å¢ƒä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æ–‡ç« æä¾›äº†ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe13f5d9025e8eabb9985bf44538e970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c387fcdacae32c684fe65200a40bd1d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniFluids-Unified-Physics-Pre-trained-Modeling-of-Fluid-Dynamics"><a href="#OmniFluids-Unified-Physics-Pre-trained-Modeling-of-Fluid-Dynamics" class="headerlink" title="OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics"></a>OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics</h2><p><strong>Authors:Rui Zhang, Qi Meng, Han Wan, Yang Liu, Zhi-Ming Ma, Hao Sun</strong></p>
<p>High-fidelity and efficient simulation of fluid dynamics drive progress in various scientific and engineering applications. Traditional computational fluid dynamics methods offer strong interpretability and guaranteed convergence, but rely on fine spatial and temporal meshes, incurring prohibitive computational costs. Physics-informed neural networks (PINNs) and neural operators aim to accelerate PDE solvers using deep learning techniques. However, PINNs require extensive retraining and careful tuning, and purely data-driven operators demand large labeled datasets. Hybrid physics-aware methods embed numerical discretizations into network architectures or loss functions, but achieve marginal speed gains and become unstable when balancing coarse priors against high-fidelity measurements. To this end, we introduce OmniFluids, a unified physics pre-trained operator learning framework that integrates physics-only pre-training, coarse-grid operator distillation, and few-shot fine-tuning, which enables fast inference and accurate prediction under limited or zero data supervision. For architectural design, the key components of OmniFluids include a mixture of operators, a multi-frame decoder, and factorized Fourier layers, which enable efficient and scalable modeling of diverse physical tasks while maintaining seamless integration with physics-based supervision. Across a broad range of two- and three-dimensional benchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven methods in flow field reconstruction and turbulence statistics accuracy, delivering 10-100x speedups compared to classical solvers, and accurately recovers unknown physical parameters from sparse, noisy data. This work establishes a new paradigm for efficient and generalizable surrogate modeling in complex fluid systems under limited data availability. </p>
<blockquote>
<p>æµä½“åŠ¨åŠ›å­¦çš„é«˜ç²¾åº¦å’Œé«˜æ•ˆæ¨¡æ‹Ÿæ¨åŠ¨äº†å„ç§ç§‘å­¦å’Œå·¥ç¨‹åº”ç”¨çš„è¿›æ­¥ã€‚ä¼ ç»Ÿçš„è®¡ç®—æµä½“åŠ¨åŠ›å­¦æ–¹æ³•è™½ç„¶å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§å’Œä¿è¯æ”¶æ•›æ€§ï¼Œä½†å®ƒä»¬ä¾èµ–äºç²¾ç»†çš„ç©ºé—´å’Œæ—¶é—´ç½‘æ ¼ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰å’Œç¥ç»ç®—å­æ—¨åœ¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯åŠ é€Ÿåå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨ã€‚ç„¶è€Œï¼ŒPINNséœ€è¦å¤§é‡çš„é‡æ–°è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´ï¼Œè€Œçº¯ç²¹çš„æ•°æ®é©±åŠ¨ç®—å­åˆ™éœ€è¦å¤§é‡æ ‡è®°æ•°æ®é›†ã€‚æ··åˆç‰©ç†æ„ŸçŸ¥æ–¹æ³•å°†æ•°å€¼ç¦»æ•£åŒ–åµŒå…¥ç½‘ç»œæ¶æ„æˆ–æŸå¤±å‡½æ•°ä¸­ï¼Œä½†åœ¨å¹³è¡¡ç²—ç•¥å…ˆéªŒå’Œé«˜ç²¾åº¦æµ‹é‡æ—¶ï¼Œåªå®ç°äº†æœ‰é™çš„åŠ é€Ÿï¼Œå¹¶ä¸”å˜å¾—ä¸ç¨³å®šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniFluidsï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç‰©ç†é¢„è®­ç»ƒç®—å­å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä»…ç‰©ç†é¢„è®­ç»ƒã€ç²—ç½‘æ ¼ç®—å­è’¸é¦å’Œå°‘é‡å¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨æœ‰é™æˆ–æ— æ•°æ®ç›‘ç£çš„æƒ…å†µä¸‹å®ç°å¿«é€Ÿæ¨ç†å’Œå‡†ç¡®é¢„æµ‹ã€‚åœ¨æ¶æ„è®¾è®¡æ–¹é¢ï¼ŒOmniFluidsçš„å…³é”®ç»„ä»¶åŒ…æ‹¬æ··åˆç®—å­ã€å¤šå¸§è§£ç å™¨å’Œå› å­åŒ–å‚…é‡Œå¶å±‚ï¼Œè¿™äº›ç»„ä»¶èƒ½å¤Ÿåœ¨ç»´æŒä¸åŸºäºç‰©ç†çš„ç›‘ç£æ— ç¼é›†æˆçš„åŒæ—¶ï¼Œå®ç°å„ç§ç‰©ç†ä»»åŠ¡çš„é«˜æ•ˆå’Œå¯æ‰©å±•å»ºæ¨¡ã€‚åœ¨å¹¿æ³›çš„äºŒç»´å’Œä¸‰ç»´åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOmniFluidsåœ¨æµåœºé‡å»ºå’Œæ¹æµç»Ÿè®¡å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„äººå·¥æ™ºèƒ½é©±åŠ¨æ–¹æ³•ï¼Œä¸ç»å…¸æ±‚è§£å™¨ç›¸æ¯”å®ç°äº†10-100å€çš„åŠ é€Ÿï¼Œå¹¶èƒ½ä»ç¨€ç–ã€å˜ˆæ‚çš„æ•°æ®ä¸­å‡†ç¡®æ¢å¤æœªçŸ¥çš„ç‰©ç†å‚æ•°ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨æœ‰é™æ•°æ®å¯ç”¨æ€§çš„æƒ…å†µä¸‹ï¼Œå¤æ‚æµä½“ç³»ç»Ÿä¸­é«˜æ•ˆä¸”å¯æ¨å¹¿çš„æ›¿ä»£å»ºæ¨¡å»ºç«‹äº†æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OmniFluidsï¼Œä¸€ä¸ªç»Ÿä¸€ç‰©ç†é¢„è®­ç»ƒç®—å­å­¦ä¹ æ¡†æ¶ï¼Œèåˆäº†ç‰©ç†é¢„è®­ç»ƒã€ç²—ç½‘æ ¼ç®—å­è’¸é¦å’Œå°‘é‡ç²¾ç»†è°ƒæ•´æŠ€æœ¯ã€‚è¯¥æ¡†æ¶å®ç°äº†å¿«é€Ÿæ¨æ–­å’Œæœ‰é™æˆ–é›¶æ•°æ®ç›‘ç£ä¸‹çš„å‡†ç¡®é¢„æµ‹ã€‚OmniFluidsçš„å…³é”®ç»„ä»¶åŒ…æ‹¬æ··åˆç®—å­ã€å¤šå¸§è§£ç å™¨å’Œåˆ†è§£å‚…é‡Œå¶å±‚ï¼Œèƒ½é«˜æ•ˆä¸”è§„æ¨¡åŒ–åœ°æ¨¡æ‹Ÿå¤šç§ç‰©ç†ä»»åŠ¡ï¼ŒåŒæ—¶æ— ç¼é›†æˆç‰©ç†åŸºç¡€ç›‘ç£ã€‚åœ¨å¹¿æ³›çš„äºŒç»´å’Œä¸‰ç»´åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOmniFluidsåœ¨æµåœºé‡å»ºå’Œæ¹æµç»Ÿè®¡å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„äººå·¥æ™ºèƒ½é©±åŠ¨æ–¹æ³•ï¼Œä¸ç»å…¸æ±‚è§£å™¨ç›¸æ¯”å®ç°äº†10-100å€çš„é€Ÿåº¦æå‡ï¼Œå¹¶èƒ½ä»ç¨€ç–ã€å˜ˆæ‚çš„æ•°æ®ä¸­å‡†ç¡®æ¢å¤æœªçŸ¥çš„ç‰©ç†å‚æ•°ã€‚è¿™ä¸€å·¥ä½œå»ºç«‹äº†åœ¨æœ‰é™æ•°æ®ä¸‹å¤æ‚æµä½“ç³»ç»Ÿä¸­é«˜æ•ˆä¸”å¯é€šç”¨çš„æ›¿ä»£å»ºæ¨¡çš„æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniFluidsæ˜¯ä¸€ä¸ªç»Ÿä¸€ç‰©ç†é¢„è®­ç»ƒç®—å­å­¦ä¹ æ¡†æ¶ï¼Œèåˆäº†ç‰©ç†é¢„è®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†ç²—ç½‘æ ¼ç®—å­è’¸é¦å’Œå°‘é‡ç²¾ç»†è°ƒæ•´æŠ€æœ¯ï¼Œå®ç°å¿«é€Ÿæ¨æ–­å’Œæœ‰é™æ•°æ®ç›‘ç£ä¸‹çš„å‡†ç¡®é¢„æµ‹ã€‚</li>
<li>OmniFluidsåŒ…æ‹¬æ··åˆç®—å­ã€å¤šå¸§è§£ç å™¨å’Œåˆ†è§£å‚…é‡Œå¶å±‚ç­‰å…³é”®ç»„ä»¶ã€‚</li>
<li>OmniFluidsèƒ½é«˜æ•ˆä¸”è§„æ¨¡åŒ–åœ°æ¨¡æ‹Ÿå¤šç§ç‰©ç†ä»»åŠ¡ï¼ŒåŒæ—¶æ— ç¼é›†æˆç‰©ç†åŸºç¡€ç›‘ç£ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOmniFluidsæ˜¾è‘—ä¼˜äºæœ€æ–°çš„äººå·¥æ™ºèƒ½é©±åŠ¨æ–¹æ³•ï¼Œå®ç°äº†æµåœºé‡å»ºå’Œæ¹æµç»Ÿè®¡çš„é«˜å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿè®¡ç®—æ–¹æ³•ç›¸æ¯”ï¼ŒOmniFluidsæä¾›äº†10-100å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e1b2ada2f3038f2e9c86c359c3ff9935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-610059c9f2d208ab174ff1eed4c323f9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-Driven-Personalized-Answer-Generation-and-Evaluation"><a href="#LLM-Driven-Personalized-Answer-Generation-and-Evaluation" class="headerlink" title="LLM-Driven Personalized Answer Generation and Evaluation"></a>LLM-Driven Personalized Answer Generation and Evaluation</h2><p><strong>Authors:Mohammadreza Molavi, Mohammadreza Tavakoli, Mohammad Moein, Abdolali Faraji, GÃ¡bor KismihÃ³k</strong></p>
<p>Online learning has experienced rapid growth due to its flexibility and accessibility. Personalization, adapted to the needs of individual learners, is crucial for enhancing the learning experience, particularly in online settings. A key aspect of personalization is providing learners with answers customized to their specific questions. This paper therefore explores the potential of Large Language Models (LLMs) to generate personalized answers to learnersâ€™ questions, thereby enhancing engagement and reducing the workload on educators. To evaluate the effectiveness of LLMs in this context, we conducted a comprehensive study using the StackExchange platform in two distinct areas: language learning and programming. We developed a framework and a dataset for validating automatically generated personalized answers. Subsequently, we generated personalized answers using different strategies, including 0-shot, 1-shot, and few-shot scenarios. The generated answers were evaluated using three methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our findings indicated that providing LLMs with examples of desired answers (from the learner or similar learners) can significantly enhance the LLMsâ€™ ability to tailor responses to individual learnersâ€™ needs. </p>
<blockquote>
<p>åœ¨çº¿å­¦ä¹ å› å…¶çµæ´»æ€§å’Œå¯è®¿é—®æ€§è€Œç»å†äº†å¿«é€Ÿå¢é•¿ã€‚ä¸ªæ€§åŒ–é€‚åº”ä¸ªåˆ«å­¦ä¹ è€…çš„éœ€æ±‚ï¼Œå¯¹äºå¢å¼ºå­¦ä¹ ä½“éªŒï¼Œç‰¹åˆ«æ˜¯åœ¨åœ¨çº¿ç¯å¢ƒä¸­ï¼Œè‡³å…³é‡è¦ã€‚ä¸ªæ€§åŒ–çš„å…³é”®æ–¹é¢æ˜¯ä¸ºå­¦ä¹ è€…æä¾›é’ˆå¯¹å…¶ç‰¹å®šé—®é¢˜çš„å®šåˆ¶ç­”æ¡ˆã€‚å› æ­¤ï¼Œæœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºå­¦ä¹ è€…çš„é—®é¢˜ç”Ÿæˆä¸ªæ€§åŒ–ç­”æ¡ˆçš„æ½œåŠ›ï¼Œä»è€Œæé«˜å‚ä¸åº¦ï¼Œå‡è½»æ•™è‚²å·¥ä½œè€…çš„è´Ÿæ‹…ã€‚ä¸ºäº†è¯„ä¼°LLMåœ¨æ­¤èƒŒæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†StackExchangeå¹³å°åœ¨è¯­è¨€å­¦ä¹ å’Œç¼–ç¨‹ä¸¤ä¸ªä¸åŒçš„é¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶å’Œæ•°æ®é›†æ¥éªŒè¯è‡ªåŠ¨ç”Ÿæˆçš„ä¸ªæ€§åŒ–ç­”æ¡ˆã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸åŒçš„ç­–ç•¥ç”Ÿæˆäº†ä¸ªæ€§åŒ–ç­”æ¡ˆï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹å’Œå‡ ä¸ªæ ·ä¾‹åœºæ™¯ã€‚ç”Ÿæˆçš„ç­”æ¡ˆé€šè¿‡ä¸‰ç§æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼š1. BERTScoreã€2. LLMè¯„ä¼°å’Œ3.äººå·¥è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå‘LLMæä¾›æœŸæœ›ç­”æ¡ˆçš„ç¤ºä¾‹ï¼ˆæ¥è‡ªå­¦ä¹ è€…æˆ–ç±»ä¼¼çš„å­¦ä¹ è€…ï¼‰å¯ä»¥æ˜¾è‘—æé«˜LLMé€‚åº”ä¸ªåˆ«å­¦ä¹ è€…éœ€æ±‚çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10829v1">PDF</a> This is the preprint version of a paper accepted at AIED 2025. The   final version will be published by Springer</p>
<p><strong>Summary</strong></p>
<p>åœ¨çº¿å­¦ä¹ å› å…¶çµæ´»æ€§å’Œå¯è®¿é—®æ€§è€Œè¿…é€Ÿå¢é•¿ã€‚ä¸ªæ€§åŒ–é€‚åº”ä¸ªåˆ«å­¦ä¹ è€…çš„éœ€æ±‚å¯¹äºæå‡å­¦ä¹ ä½“éªŒå°¤ä¸ºé‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åœ¨çº¿ç¯å¢ƒä¸­ã€‚æœ¬æ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºå­¦ä¹ è€…ç”Ÿæˆä¸ªæ€§åŒ–ç­”æ¡ˆçš„æ½œåŠ›ï¼Œä»è€Œæé«˜å‚ä¸åº¦å’Œå‡è½»æ•™è‚²å·¥ä½œè€…çš„è´Ÿæ‹…ã€‚ä¸ºäº†è¯„ä¼°LLMsåœ¨æ­¤èƒŒæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨StackExchangeå¹³å°åœ¨è¯­è¨€å­¦ä¹ å’Œç¼–ç¨‹ä¸¤ä¸ªé¢†åŸŸè¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶å’Œæ•°æ®é›†æ¥éªŒè¯è‡ªåŠ¨ç”Ÿæˆçš„ä¸ªäººåŒ–ç­”æ¡ˆã€‚é€šè¿‡ä¸åŒçš„ç­–ç•¥ï¼ˆå¦‚é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ï¼‰ç”Ÿæˆä¸ªæ€§åŒ–ç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨ä¸‰ç§æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼šBERTScoreã€LLMè¯„ä¼°å’Œäººå·¥è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ä¸ºLLMsæä¾›æœŸæœ›ç­”æ¡ˆçš„ç¤ºä¾‹ï¼ˆæ¥è‡ªå­¦ä¹ è€…æˆ–ç±»ä¼¼å­¦ä¹ è€…ï¼‰å¯ä»¥æ˜¾è‘—æé«˜LLMsé€‚åº”ä¸ªåˆ«å­¦ä¹ è€…éœ€æ±‚çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿å­¦ä¹ çš„å¢é•¿å¾—ç›Šäºå…¶çµæ´»æ€§å’Œå¯è®¿é—®æ€§ã€‚</li>
<li>ä¸ªæ€§åŒ–å¯¹äºæå‡åœ¨çº¿å­¦ä¹ ä½“éªŒè‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ½œåŠ›ä¸ºå­¦ä¹ è€…ç”Ÿæˆä¸ªæ€§åŒ–ç­”æ¡ˆï¼Œä»è€Œæé«˜å‚ä¸åº¦å’Œå‡è½»æ•™è‚²è€…çš„å·¥ä½œè´Ÿæ‹…ã€‚</li>
<li>åœ¨è¯­è¨€å­¦ä¹ å’Œç¼–ç¨‹é¢†åŸŸè¿›è¡Œäº†LLMsçš„ç ”ç©¶è¯„ä¼°ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶å’Œæ•°æ®é›†æ¥éªŒè¯è‡ªåŠ¨ç”Ÿæˆçš„ä¸ªäººåŒ–ç­”æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡ä¸åŒçš„ç­–ç•¥ï¼ˆå¦‚é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ï¼‰ç”Ÿæˆä¸ªæ€§åŒ–ç­”æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10829">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a596d6b230433035bb2a5e8e9899412c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec3b3388a811aa15aec993723184e720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0a616a2653703504c007bed5dbfb39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15c415955005c7b857fdd20b15222a31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8368a821f868d1049f7ab1c23690c3e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches"><a href="#Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches" class="headerlink" title="Generalist Models in Medical Image Segmentation: A Survey and   Performance Comparison with Task-Specific Approaches"></a>Generalist Models in Medical Image Segmentation: A Survey and   Performance Comparison with Task-Specific Approaches</h2><p><strong>Authors:Andrea Moglia, Matteo Leccardi, Matteo Cavicchioli, Alice Maccarini, Marco Marcon, Luca Mainardi, Pietro Cerveri</strong></p>
<p>Following the successful paradigm shift of large language models, leveraging pre-training on a massive corpus of data and fine-tuning on different downstream tasks, generalist models have made their foray into computer vision. The introduction of Segment Anything Model (SAM) set a milestone on segmentation of natural images, inspiring the design of a multitude of architectures for medical image segmentation. In this survey we offer a comprehensive and in-depth investigation on generalist models for medical image segmentation. We start with an introduction on the fundamentals concepts underpinning their development. Then, we provide a taxonomy on the different declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on the recent SAM 2, on other innovative models trained on images alone, and others trained on both text and images. We thoroughly analyze their performances at the level of both primary research and best-in-literature, followed by a rigorous comparison with the state-of-the-art task-specific models. We emphasize the need to address challenges in terms of compliance with regulatory frameworks, privacy and security laws, budget, and trustworthy artificial intelligence (AI). Finally, we share our perspective on future directions concerning synthetic data, early fusion, lessons learnt from generalist models in natural language processing, agentic AI and physical AI, and clinical translation. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æˆåŠŸå®ç°èŒƒå¼è½¬å˜ä¹‹åï¼Œé€šè¿‡é¢„è®­ç»ƒå¤§é‡æ•°æ®è¯­æ–™åº“å¹¶åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œé€šç”¨æ¨¡å‹å·²ç»æ¶‰è¶³è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚Segment Anything Modelï¼ˆSAMï¼‰çš„å¼•å…¥ä¸ºè‡ªç„¶å›¾åƒåˆ†å‰²æ ‘ç«‹äº†é‡Œç¨‹ç¢‘ï¼Œæ¿€å‘äº†å¤šç§åŒ»ç–—å›¾åƒåˆ†å‰²æ¶æ„çš„è®¾è®¡çµæ„Ÿã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å¯¹åŒ»ç–—å›¾åƒåˆ†å‰²çš„é€šç”¨æ¨¡å‹è¿›è¡Œäº†å…¨é¢æ·±å…¥çš„ç ”ç©¶ã€‚é¦–å…ˆä»‹ç»äº†æ”¯æ’‘å®ƒä»¬å‘å±•çš„åŸºæœ¬æ¦‚å¿µã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€å¾®è°ƒã€é€‚é…å™¨ç­‰æ–¹é¢å¯¹SAMçš„ä¸åŒå‘å±•æ–¹å‘è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶ä»‹ç»äº†æœ€è¿‘çš„SAM 2ä»¥åŠå…¶ä»–å•ç‹¬åœ¨å›¾åƒä¸Šè®­ç»ƒçš„åˆ›æ–°æ¨¡å‹ï¼Œè¿˜æœ‰å…¶ä»–åŒæ—¶è®­ç»ƒæ–‡æœ¬å’Œå›¾åƒæ¨¡å‹ã€‚æˆ‘ä»¬å…¨é¢åˆ†æäº†å®ƒä»¬åœ¨åˆçº§ç ”ç©¶å’Œæ–‡çŒ®æœ€ä½³æ°´å¹³ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸æœ€æ–°çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼æ¯”è¾ƒã€‚æˆ‘ä»¬å¼ºè°ƒäº†éœ€è¦è§£å†³åˆè§„æ€§æ¡†æ¶ã€éšç§å’Œå®‰å…¨æ³•å¾‹ã€é¢„ç®—å’Œå¯ä¿¡äººå·¥æ™ºèƒ½ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†äº«äº†æˆ‘ä»¬å¯¹åˆæˆæ•°æ®ã€æ—©æœŸèåˆã€ä»è‡ªç„¶è¯­è¨€å¤„ç†é€šç”¨æ¨¡å‹ä¸­å­¦åˆ°çš„æ•™è®­ã€æ™ºèƒ½ä½“AIå’Œç‰©ç†AIä»¥åŠä¸´åºŠç¿»è¯‘çš„æœªæ¥æ–¹å‘çš„çœ‹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10825v1">PDF</a> 132 pages, 26 figures, 23 tables. Andrea Moglia and Matteo Leccardi   are equally contributing authors</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹æˆåŠŸèŒƒå¼è½¬ç§»åï¼Œé€šç”¨æ¨¡å‹å·²æ¶‰è¶³è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚Segment Anything Modelï¼ˆSAMï¼‰çš„å¼•å…¥ä¸ºè‡ªç„¶å›¾åƒåˆ†å‰²æ ‘ç«‹äº†é‡Œç¨‹ç¢‘ï¼Œæ¿€å‘äº†å¤šç§åŒ»ç–—å›¾åƒåˆ†å‰²æ¶æ„çš„è®¾è®¡ã€‚æœ¬æ–‡å…¨é¢æ·±å…¥åœ°è°ƒæŸ¥äº†åŒ»ç–—å›¾åƒåˆ†å‰²çš„é€šç”¨æ¨¡å‹ï¼Œä»åŸºæœ¬æ¦‚å¿µå…¥æ‰‹ï¼Œæä¾›SAMçš„ä¸åŒå˜ç§åˆ†ç±»ï¼Œå¦‚é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€å¾®è°ƒã€é€‚é…å™¨ç­‰ï¼Œå¹¶åˆ†æå…¶ä¸æœ€æ–°ä»»åŠ¡ç‰¹å®šæ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”ã€‚å¼ºè°ƒéœ€è¦è§£å†³åˆè§„æ€§ã€éšç§å’Œå®‰å…¨æ€§ã€é¢„ç®—åŠå¯ä¿¡äººå·¥æ™ºèƒ½ç­‰æŒ‘æˆ˜ã€‚å±•æœ›æœªæ¥æ–¹å‘ï¼ŒåŒ…æ‹¬åˆæˆæ•°æ®ã€æ—©æœŸèåˆã€è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é€šç”¨æ¨¡å‹æ•™è®­ã€ä»£ç†äººå·¥æ™ºèƒ½åŠç‰©ç†äººå·¥æ™ºèƒ½å’Œä¸´åºŠç¿»è¯‘ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„èŒƒå¼è½¬ç§»æ¨åŠ¨äº†é€šç”¨æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>Segment Anything Model (SAM) ä¸ºè‡ªç„¶å›¾åƒåˆ†å‰²æ ‘ç«‹äº†é‡Œç¨‹ç¢‘ã€‚</li>
<li>SAMçš„å¤šç§å˜ç§å¦‚é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€å¾®è°ƒã€é€‚é…å™¨ç­‰åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>é€šç”¨æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²æ–¹é¢çš„æ€§èƒ½ä¸æœ€æ–°ä»»åŠ¡ç‰¹å®šæ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚</li>
<li>é¢ä¸´åˆè§„æ€§ã€éšç§å’Œå®‰å…¨æ€§ã€é¢„ç®—ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦ä»è‡ªç„¶è¯­è¨€å¤„ç†çš„é€šç”¨æ¨¡å‹ä¸­å¸å–æ•™è®­ï¼Œå°†å…¶åº”ç”¨äºåŒ»ç–—å›¾åƒåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbd94bf513f6f2d305d0ffc2eca684d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95307b0eac671eb54b1fc8800f1922e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ffd82c8157d7e5d12b1568ce0e21a4d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain"><a href="#IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain" class="headerlink" title="IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain"></a>IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain</h2><p><strong>Authors:Hong Huang, Weixiang Sun, Zhijian Wu, Jingwen Niu, Donghuan Lu, Xian Wu, Yefeng Zheng</strong></p>
<p>Recent advances in vision-language models, such as CLIP, have significantly improved performance in zero- and few-shot anomaly detection (ZFSAD) tasks. However, most existing CLIP-based methods assume prior knowledge of categories and rely on carefully designed prompts tailored to specific scenarios. While these text prompts capture semantic information in the textual space, they often fail to distinguish normal and anomalous instances in the joint embedding space. Moreover, most ZFSAD approaches focus on industrial domains, with limited exploration in medical tasks. To address these limitations, we propose IQE-CLIP, a novel framework for ZFSAD in the medical domain. We show that query embeddings integrating both textual and instance-aware visual information serve as more effective indicators of anomalies. Specifically, we introduce class-based and learnable prompting tokens to better adapt CLIP to the medical setting. Furthermore, we design an instance-aware query module that extracts region-level contextual information from both modalities, enabling the generation of anomaly-sensitive embeddings. Extensive experiments on six medical datasets demonstrate that IQE-CLIP achieves state-of-the-art performance in both zero-shot and few-shot settings. Code and data are available at \href{<a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/%7D%7Bthis">https://github.com/hongh0/IQE-CLIP/}{this</a> https URL}. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¯¸å¦‚CLIPä¹‹ç±»çš„è§†è§‰è¯­è¨€æ¨¡å‹å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå·²åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZFSADï¼‰ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºCLIPçš„æ–¹æ³•å‡è®¾å¯¹ç±»åˆ«æœ‰å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¾èµ–äºé’ˆå¯¹ç‰¹å®šåœºæ™¯ç²¾å¿ƒè®¾è®¡çš„æç¤ºã€‚è™½ç„¶è¿™äº›æ–‡æœ¬æç¤ºæ•è·äº†æ–‡æœ¬ç©ºé—´ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨è”åˆåµŒå…¥ç©ºé—´ä¸­æ— æ³•åŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸çš„å®ä¾‹ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ZFSADæ–¹æ³•éƒ½é›†ä¸­åœ¨å·¥ä¸šé¢†åŸŸï¼Œåœ¨åŒ»ç–—ä»»åŠ¡æ–¹é¢çš„æ¢ç´¢æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†IQE-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒ»ç–—é¢†åŸŸçš„ZFSADçš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œèåˆäº†æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥è§†è§‰ä¿¡æ¯çš„æŸ¥è¯¢åµŒå…¥ä½œä¸ºå¼‚å¸¸çš„æ›´æœ‰æ•ˆæŒ‡æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºç±»åˆ«çš„å’Œå¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œï¼Œä»¥æ›´å¥½åœ°é€‚åº”CLIPåœ¨åŒ»ç–—ç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®ä¾‹æ„ŸçŸ¥æŸ¥è¯¢æ¨¡å—ï¼Œå¯ä»¥ä»ä¸¤ç§æ¨¡æ€ä¸­æå–åŒºåŸŸçº§åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆå¯¹å¼‚å¸¸æ•æ„Ÿçš„åµŒå…¥ã€‚åœ¨å…­ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIQE-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/]%E8%BF%99%E4%B8%AA%E9%93%BE%E6%8E%A5%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hongh0/IQE-CLIP/]è¿™ä¸ªé“¾æ¥ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10730v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€è¿‘ï¼ŒCLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CLIPæ–¹æ³•å¤§å¤šå‡å®šæœ‰ç±»åˆ«å…ˆéªŒçŸ¥è¯†å¹¶ä¾èµ–é’ˆå¯¹ç‰¹å®šåœºæ™¯çš„ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬æç¤ºã€‚æœ¬æ–‡æå‡ºIQE-CLIPæ¡†æ¶ï¼Œå°†æ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„è§†è§‰ä¿¡æ¯é›†æˆåˆ°æŸ¥è¯¢åµŒå…¥ä¸­ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ£€æµ‹åŒ»å­¦é¢†åŸŸçš„å¼‚å¸¸ã€‚é€šè¿‡å¼•å…¥åŸºäºç±»åˆ«çš„å¯å­¦ä¹ æç¤ºä»¤ç‰Œå’Œå®ä¾‹æ„ŸçŸ¥æŸ¥è¯¢æ¨¡å—ï¼ŒIQE-CLIPåœ¨å…­ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–ç±»åˆ«å…ˆéªŒçŸ¥è¯†å’Œé’ˆå¯¹ç‰¹å®šåœºæ™¯çš„æ–‡æœ¬æç¤ºã€‚</li>
<li>IQE-CLIPæ¡†æ¶é›†æˆæ–‡æœ¬å’Œå®ä¾‹æ„ŸçŸ¥çš„è§†è§‰ä¿¡æ¯åˆ°æŸ¥è¯¢åµŒå…¥ä¸­ï¼Œæœ‰æ•ˆæé«˜å¼‚å¸¸æ£€æµ‹æ•ˆæœã€‚</li>
<li>IQE-CLIPå¼•å…¥åŸºäºç±»åˆ«çš„å¯å­¦ä¹ æç¤ºä»¤ç‰Œï¼Œæ›´å¥½åœ°é€‚åº”åŒ»å­¦é¢†åŸŸã€‚</li>
<li>å®ä¾‹æ„ŸçŸ¥æŸ¥è¯¢æ¨¡å—èƒ½æå–ä¸¤ç§æ¨¡æ€çš„åŒºåŸŸçº§ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆå¯¹å¼‚å¸¸æ•æ„Ÿçš„åµŒå…¥ã€‚</li>
<li>IQE-CLIPåœ¨å…­ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac687c5ed6d72ac284dabfd2a676c383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f54646641f5923a7ef176de168cb89c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c51ef4e50943405dae4645a0783f91ca.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="NeuralNexus-at-BEA-2025-Shared-Task-Retrieval-Augmented-Prompting-for-Mistake-Identification-in-AI-Tutors"><a href="#NeuralNexus-at-BEA-2025-Shared-Task-Retrieval-Augmented-Prompting-for-Mistake-Identification-in-AI-Tutors" class="headerlink" title="NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for   Mistake Identification in AI Tutors"></a>NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for   Mistake Identification in AI Tutors</h2><p><strong>Authors:Numaan Naeem, Sarfraz Ahmad, Momina Ahsan, Hasan Iqbal</strong></p>
<p>This paper presents our system for Track 1: Mistake Identification in the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The task involves evaluating whether a tutorâ€™s response correctly identifies a mistake in a studentâ€™s mathematical reasoning. We explore four approaches: (1) an ensemble of machine learning models over pooled token embeddings from multiple pretrained language models (LMs); (2) a frozen sentence-transformer using [CLS] embeddings with an MLP classifier; (3) a history-aware model with multi-head attention between token-level history and response embeddings; and (4) a retrieval-augmented few-shot prompting system with a large language model (LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples, constructs structured prompts, and uses schema-guided output parsing to produce interpretable predictions. It outperforms all baselines, demonstrating the effectiveness of combining example-driven prompting with LLM reasoning for pedagogical feedback assessment. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/BEA_2025">https://github.com/NaumanNaeem/BEA_2025</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨Track 1ï¼šAIè¾…å¯¼è€å¸ˆæ•™å­¦æŠ€èƒ½è¯„ä¼°çš„BEA 2025å…±äº«ä»»åŠ¡ä¸­çš„é”™è¯¯è¯†åˆ«ç³»ç»Ÿã€‚è¯¥ä»»åŠ¡æ¶‰åŠè¯„ä¼°è¾…å¯¼è€å¸ˆçš„å›åº”æ˜¯å¦èƒ½æ­£ç¡®è¯†åˆ«å­¦ç”Ÿæ•°å­¦æ¨ç†ä¸­çš„é”™è¯¯ã€‚æˆ‘ä»¬æ¢ç´¢äº†å››ç§æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä½¿ç”¨æ¥è‡ªå¤šä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åˆå¹¶ä»¤ç‰ŒåµŒå…¥çš„æœºå™¨å­¦ä¹ æ¨¡å‹é›†åˆï¼›ï¼ˆ2ï¼‰ä½¿ç”¨[CLS]åµŒå…¥ä¸MLPåˆ†ç±»å™¨çš„å†»ç»“å¥å­è½¬æ¢å™¨ï¼›ï¼ˆ3ï¼‰åœ¨ä»¤ç‰Œçº§åˆ«å†å²ä¸å“åº”åµŒå…¥ä¹‹é—´å…·æœ‰å¤šå¤´æ³¨æ„åŠ›çš„å†å²æ„ŸçŸ¥æ¨¡å‹ï¼›ï¼ˆ4ï¼‰å¸¦æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å³GPT 4oçš„å¢å¼ºæ£€ç´¢æç¤ºç³»ç»Ÿã€‚æˆ‘ä»¬çš„æœ€ç»ˆç³»ç»Ÿæ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„ç¤ºä¾‹ï¼Œæ„å»ºç»“æ„åŒ–æç¤ºï¼Œå¹¶ä½¿ç”¨æ¨¡å¼å¼•å¯¼çš„è¾“å‡ºè§£ææ¥äº§ç”Ÿå¯è§£é‡Šçš„é¢„æµ‹ã€‚å®ƒè¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ï¼Œè¯æ˜äº†ç»“åˆç¤ºä¾‹é©±åŠ¨çš„æç¤ºå’ŒLLMæ¨ç†è¿›è¡Œæ•™å­¦åé¦ˆè¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/BEA_2025%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NaumanNaeem/BEA_2025æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10627v1">PDF</a> 6 pages, 2 figures, 1 table</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹BEA 2025å…±äº«ä»»åŠ¡ä¸­çš„è½¨é“1ï¼šäººå·¥æ™ºèƒ½è¾…å¯¼è€å¸ˆçš„æ•™å­¦èƒ½åŠ›è¯„ä¼°ä¸­çš„é”™è¯¯è¯†åˆ«ç³»ç»Ÿã€‚ä»»åŠ¡æ—¨åœ¨è¯„ä¼°è¾…å¯¼è€å¸ˆçš„å›åº”æ˜¯å¦èƒ½æ­£ç¡®è¯†åˆ«å­¦ç”Ÿæ•°å­¦æ¨ç†ä¸­çš„é”™è¯¯ã€‚æœ¬æ–‡æ¢ç´¢äº†å››ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå¤šä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹ã€ä½¿ç”¨å†»ç»“çš„å¥å­å˜æ¢å™¨å’Œå¤šå±‚æ„ŸçŸ¥å™¨åˆ†ç±»å™¨çš„[CLS]åµŒå…¥ã€å…·æœ‰ä»¤ç‰Œçº§åˆ«å†å²ä¸å“åº”åµŒå…¥ä¹‹é—´å¤šå¤´æ³¨æ„åŠ›çš„å†å²æ„ŸçŸ¥æ¨¡å‹ï¼Œä»¥åŠç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºå‹å°‘æ ·æœ¬æç¤ºç³»ç»Ÿï¼ˆå¦‚GPT 4oï¼‰ã€‚æœ€ç»ˆç³»ç»Ÿé€šè¿‡æ£€ç´¢ç›¸ä¼¼ç¤ºä¾‹ã€æ„å»ºç»“æ„åŒ–æç¤ºå’Œæ¨¡å¼å¼•å¯¼çš„è¾“å‡ºè§£ææ¥ç”Ÿæˆå¯è§£é‡Šçš„é¢„æµ‹ï¼Œè¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ï¼Œè¯æ˜äº†ç»“åˆç¤ºä¾‹é©±åŠ¨çš„æç¤ºå’ŒLLMæ¨ç†è¿›è¡Œæ•™å­¦æ³•åé¦ˆè¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹äººå·¥æ™ºèƒ½è¾…å¯¼è€å¸ˆæ•™å­¦è¯„ä¼°çš„ä»»åŠ¡â€”â€”é”™è¯¯è¯†åˆ«ç³»ç»Ÿã€‚</li>
<li>å››ç§æ–¹æ³•çš„æ¢ç´¢åŒ…æ‹¬åŸºäºé›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹çš„å“åº”é”™è¯¯è¯†åˆ«ç³»ç»Ÿã€‚</li>
<li>é‡‡ç”¨å†»ç»“çš„å¥å­å˜æ¢å™¨å’Œå¤šå±‚æ„ŸçŸ¥å™¨åˆ†ç±»å™¨æ¥å¤„ç†å“åº”ä¸ä»¤ç‰Œä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>å†å²æ„ŸçŸ¥æ¨¡å‹ç»“åˆäº†ä»¤ç‰Œçº§åˆ«å†å²å’Œå“åº”åµŒå…¥çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é‡‡ç”¨æ£€ç´¢å¢å¼ºå‹å°‘æ ·æœ¬æç¤ºç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ee68dc9cdc1b7ade3d46fef3eedc6458.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23c2995515e5e26960d32890ea9ecefe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47497a6ccc3902b959e7db1c4e3e2cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9245408d6408d296092444c26904ea4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PointGS-Point-Attention-Aware-Sparse-View-Synthesis-with-Gaussian-Splatting"><a href="#PointGS-Point-Attention-Aware-Sparse-View-Synthesis-with-Gaussian-Splatting" class="headerlink" title="PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian   Splatting"></a>PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian   Splatting</h2><p><strong>Authors:Lintao Xiang, Hongpei Zheng, Yating Huang, Qijun Yang, Hujun Yin</strong></p>
<p>3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods. </p>
<blockquote>
<p>3Dé«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰æ˜¯ä¸€ç§åˆ›æ–°çš„æ¸²æŸ“æŠ€æœ¯ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ˜ç¡®çš„3Dåœºæ™¯è¡¨ç¤ºåœ¨æ¸²æŸ“é€Ÿåº¦å’Œè§†è§‰è´¨é‡ä¸Šéƒ½è¶…è¶Šäº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚ç°æœ‰çš„3DGSæ–¹æ³•éœ€è¦å¤§é‡æ ¡å‡†çš„è§†å›¾æ¥ç”Ÿæˆä¸€è‡´å’Œå®Œæ•´çš„åœºæ™¯è¡¨ç¤ºã€‚å½“è¾“å…¥è§†å›¾æœ‰é™æ—¶ï¼Œ3DGSå®¹æ˜“è¿‡åº¦æ‹Ÿåˆè®­ç»ƒè§†å›¾ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡æ˜æ˜¾ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‚¹ç‰¹å¾æ„ŸçŸ¥çš„é«˜æ–¯æ¶‚æŠ¹æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°ä»ç¨€ç–è®­ç»ƒè§†å›¾çš„å®æ—¶é«˜è´¨é‡æ¸²æŸ“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨æœ€æ–°çš„ç«‹ä½“åŸºç¡€æ¨¡å‹æ¥ä¼°è®¡å‡†ç¡®çš„ç›¸æœºå§¿æ€å¹¶é‡å»ºç”¨äºé«˜æ–¯åˆå§‹åŒ–çš„å¯†é›†ç‚¹äº‘ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä»ç¨€ç–è¾“å…¥ä¸­é‡‡æ ·å’Œèšåˆå¤šå°ºåº¦2Då¤–è§‚ç‰¹å¾æ¥ç¼–ç æ¯ä¸ª3Dé«˜æ–¯çš„é¢œè‰²å±æ€§ã€‚ä¸ºäº†å¢å¼ºç‚¹çº§å¤–è§‚è¡¨ç¤ºï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‚¹äº¤äº’ç½‘ç»œï¼Œå…è®¸æ¯ä¸ªé«˜æ–¯ç‚¹ä¸æœ€è¿‘çš„é‚»å±…è¿›è¡Œäº¤äº’ã€‚è¿™äº›ä¸°å¯Œçš„ç‰¹å¾éšåé€šè¿‡ä¸¤ä¸ªè½»é‡çº§çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è§£ç ä¸ºé«˜æ–¯å‚æ•°ï¼Œç”¨äºæœ€ç»ˆæ¸²æŸ“ã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºäºNeRFçš„æ–¹æ³•ï¼Œå¹¶åœ¨å°æ ·æœ¬è®¾ç½®ä¸‹å®ç°äº†ä¸æœ€æ–°3DGSæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10335v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ˜¾å¼ä¸‰ç»´åœºæ™¯è¡¨ç¤ºçš„é«˜æ•ˆæ¸²æŸ“æŠ€æœ¯ä¸‰ç»´é«˜æ–¯æ’ç‰‡æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜æ¸²æŸ“é€Ÿåº¦å¹¶æå‡è§†è§‰è´¨é‡ã€‚å½“é¢ä¸´ç¨€ç–è®­ç»ƒè§†å›¾æ—¶ï¼Œæå‡ºäº†ä¸€ç§ç‚¹ç‰¹å¾æ„ŸçŸ¥é«˜æ–¯æ’ç‰‡æ¡†æ¶ä»¥å®ç°å®æ—¶é«˜è´¨é‡æ¸²æŸ“ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æœ€æ–°çš„ç«‹ä½“åŸºç¡€æ¨¡å‹ä¼°è®¡æ‘„åƒæœºå§¿æ€å¹¶é‡å»ºç”¨äºé«˜æ–¯åˆå§‹åŒ–çš„å¯†é›†ç‚¹äº‘ã€‚æ­¤å¤–ï¼Œæ¡†æ¶å¯¹ä¸‰ç»´é«˜æ–¯ç¼–ç è‰²å½©å±æ€§å¹¶å¢å¼ºç‚¹å¯¹ç‰¹å¾çš„ä»£è¡¨æ€§è¿›è¡Œè®¾è®¡ç½‘ç»œåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶æ„å»ºæ¨¡å‹äº¤äº’ç½‘å¹¶ä¸°å¯Œæ¯ä¸ªé«˜æ–¯ç‚¹çš„ç‰¹å¾è¡¨ç°ä»¥æ”¹è¿›æœ€åçš„æ¸²æŸ“ç»“æœã€‚åœ¨åŸºå‡†æµ‹è¯•å®éªŒä¸­éªŒè¯äº†æ–¹æ³•è¡¨ç°è¿œè¶…NeRFæ‰€è¾¾æˆå®ç°å…¨é¢ä¼˜è¶Šæ€§ç›¸æ¯”äºå…¶å®ƒé¢†å…ˆçš„é«˜æ–¯æ’ç‰‡æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºNeRFçš„æ–¹æ³•ï¼Œå¹¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹å®ç°äº†ä¸æœ€æ–°3DGSæ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGSæ–¹æ³•åœ¨æ¸²æŸ“é€Ÿåº¦å’Œè§†è§‰è´¨é‡æ–¹é¢è¶…è¶Šç¥ç»ç½‘ç»œè¾å°„åœºæ–¹æ³•ï¼ˆNeRFï¼‰ã€‚ä½†å…¶ç°æœ‰å±€é™åŒ…æ‹¬é¢å¯¹æœ‰é™è¾“å…¥è§†å›¾æ—¶è¡¨ç°å‡ºçš„è¿‡åº¦æ‹Ÿåˆå’Œæ¸²æŸ“è´¨é‡ä¸‹é™é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ä¸ªåä¸ºPoint-wise Feature-Aware Gaussian Splattingçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®ç°äº†ä»ç¨€ç–è®­ç»ƒè§†å›¾è¿›è¡Œå®æ—¶é«˜è´¨é‡æ¸²æŸ“ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æœ€æ–°çš„ç«‹ä½“åŸºç¡€æ¨¡å‹è¿›è¡Œç›¸æœºå§¿æ€ä¼°è®¡å’Œå¯†é›†ç‚¹äº‘é‡å»ºï¼›</li>
<li>Point-wise Feature-Aware Gaussian Splattingåˆ©ç”¨å¤šç‚¹è§†è§’æ•°æ®å’Œä¸åŒç»´åº¦çš„ä¿¡æ¯èåˆæ¥æå‡ç‚¹ç‰¹å¾çš„ä»£è¡¨æ€§ï¼Œé‡‡ç”¨åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç½‘ç»œè®¾è®¡æ¨¡å‹æ¥å¢å¼ºæ¯ä¸ªé«˜æ–¯ç‚¹çš„ç‰¹å¾è¡¨è¾¾ï¼›æœ€åè¿›è¡Œå…³é”®ç¼–ç åŠå‚æ•°åŒ–çš„Gaussianæ•°æ®å¤„ç†ä»¥ä¾¿ç”¨äºæœ€ç»ˆçš„æ¸²æŸ“æ•ˆæœå®ç°å¼ºåŒ–æ•´ä½“å±•ç¤ºæ°´å¹³åŠä¼˜ç‚¹å¼ºè°ƒæ— å®ç‰©å’Œæ— ç´ æçš„æ¸²æŸ“æŠ€æœ¯ï¼›</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d17a557513373ee03c744299c89bf2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5df2a46b4102e410cb6aed2c254111aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e09c01b5e63a10178054fcda4e554db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8471f3a39960344a9aa84b4ef08bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32362cab3c738eb4e8e8a7bf8c8220dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection"><a href="#Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection" class="headerlink" title="Few-Shot Learner Generalizes Across AI-Generated Image Detection"></a>Few-Shot Learner Generalizes Across AI-Generated Image Detection</h2><p><strong>Authors:Shiyu Wu, Jing Liu, Jing Li, Yequan Wang</strong></p>
<p>Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, these detectors suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space for effectively distinguishing unseen fake images using very few samples. Experiments show that FSD achieves state-of-the-art performance by $+11.6%$ average accuracy on the GenImage dataset with only $10$ additional samples. More importantly, our method is better capable of capturing the intra-category commonality in unseen images without further training. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/teheperinko541/Few-Shot-AIGI-Detector">https://github.com/teheperinko541/Few-Shot-AIGI-Detector</a>. </p>
<blockquote>
<p>å½“å‰åŸºäºå¤§å‹åˆæˆå›¾åƒæ•°æ®é›†è®­ç»ƒçš„å‡å†’å›¾åƒæ£€æµ‹å™¨åœ¨æœ‰é™çš„ç”Ÿæˆæ¨¡å‹ç ”ç©¶ä¸Šè¡¨ç°æ»¡æ„ã€‚ç„¶è€Œï¼Œè¿™äº›æ£€æµ‹å™¨åœ¨é¢å¯¹æœªçŸ¥æ¨¡å‹æ—¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œä»åœ¨çº¿ç”Ÿæˆæ¨¡å‹ä¸­æ”¶é›†è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®é€šå¸¸æˆæœ¬é«˜æ˜‚æˆ–ä¸å¯è¡Œã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Few-Shot Detectorï¼ˆFSDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„äººå·¥æ™ºèƒ½å›¾åƒæ£€æµ‹å™¨ï¼Œå®ƒé€šè¿‡éå¸¸æœ‰é™çš„æ ·æœ¬å­¦ä¹ ä¸“é—¨çš„åº¦é‡ç©ºé—´ï¼Œä»¥æœ‰æ•ˆåŒºåˆ†æœªçŸ¥çš„å‡å†’å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒFSDåœ¨GenImageæ•°æ®é›†ä¸Šé€šè¿‡ä»…å¢åŠ 10ä¸ªæ ·æœ¬å°±è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†+11.6%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•è·æœªçŸ¥å›¾åƒä¸­çš„ç±»åˆ«å†…å…±æ€§ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/teheperinko541/Few-Shot-AIGI-Detector%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/teheperinko541/Few-Shot-AIGI-Detectorä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08763v2">PDF</a> 12 pages, 6 figures, Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå½“å‰å‡å›¾åƒæ£€æµ‹å™¨åœ¨ç‰¹å®šåˆæˆå›¾åƒæ•°æ®é›†ä¸Šçš„è®­ç»ƒè¡¨ç°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹å™¨â€”â€”Few-Shot Detectorï¼ˆFSDï¼‰ã€‚è¯¥æ£€æµ‹å™¨èƒ½å¤Ÿåœ¨éå¸¸æœ‰é™çš„æ ·æœ¬ä¸‹ï¼Œé€šè¿‡å­¦ä¹ ä¸“é—¨çš„åº¦é‡ç©ºé—´æ¥æœ‰æ•ˆåŒºåˆ†æœªè§è¿‡çš„å‡å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨GenImageæ•°æ®é›†ä¸Šï¼ŒFSDé€šè¿‡ä½¿ç”¨ä»…10ä¸ªé¢å¤–æ ·æœ¬è¾¾åˆ°äº†è¡Œä¸šé¢†å…ˆçš„æ°´å¹³ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜äº†11.6%ã€‚å¹¶ä¸”FSDèƒ½æ›´å¥½çš„æ•æ‰æœªè§å›¾åƒä¸­çš„ç±»åˆ«å…±æ€§è€Œæ— éœ€è¿›ä¸€æ­¥çš„è®­ç»ƒã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰å‡å›¾åƒæ£€æµ‹å™¨åœ¨ç‰¹å®šåˆæˆå›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æœªè§æ¨¡å‹ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>æå‡ºäº†å…¨æ–°çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹å™¨â€”â€”Few-Shot Detectorï¼ˆFSDï¼‰ã€‚</li>
<li>FSDèƒ½å¤Ÿåœ¨éå¸¸æœ‰é™çš„æ ·æœ¬ä¸‹æœ‰æ•ˆåŒºåˆ†æœªè§è¿‡çš„å‡å›¾åƒã€‚</li>
<li>åœ¨GenImageæ•°æ®é›†ä¸Šï¼ŒFSDé€šè¿‡ä½¿ç”¨ä»…10ä¸ªé¢å¤–æ ·æœ¬è¾¾åˆ°äº†è¡Œä¸šé¢†å…ˆçš„æ°´å¹³ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜äº†11.6%ã€‚</li>
<li>FSDèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æœªè§å›¾åƒä¸­çš„ç±»åˆ«å…±æ€§ã€‚</li>
<li>FSDæ— éœ€è¿›ä¸€æ­¥çš„è®­ç»ƒå°±èƒ½å®ç°è¿™ä¸€åŠŸèƒ½ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ä¸è¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1e429420cd8c3d87562ac4e8a9ef0d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e624300d8a346afa4a692dbfab65a0e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e6ebefd80a359d53392ef39d38b37dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d728853e95fa96ea6e395d9ed7f20506.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fadf302a7678224439471eec691edb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5b6abeb2d2d427eb49c4f0a4703464.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RmGPT-A-Foundation-Model-with-Generative-Pre-trained-Transformer-for-Fault-Diagnosis-and-Prognosis-in-Rotating-Machinery"><a href="#RmGPT-A-Foundation-Model-with-Generative-Pre-trained-Transformer-for-Fault-Diagnosis-and-Prognosis-in-Rotating-Machinery" class="headerlink" title="RmGPT: A Foundation Model with Generative Pre-trained Transformer for   Fault Diagnosis and Prognosis in Rotating Machinery"></a>RmGPT: A Foundation Model with Generative Pre-trained Transformer for   Fault Diagnosis and Prognosis in Rotating Machinery</h2><p><strong>Authors:Yilin Wang, Yifei Yu, Kong Sun, Peixuan Lei, Yuxuan Zhang, Enrico Zio, Aiguo Xia, Yuanxiang Li</strong></p>
<p>In industry, the reliability of rotating machinery is critical for production efficiency and safety. Current methods of Prognostics and Health Management (PHM) often rely on task-specific models, which face significant challenges in handling diverse datasets with varying signal characteristics, fault modes and operating conditions. Inspired by advancements in generative pretrained models, we propose RmGPT, a unified model for diagnosis and prognosis tasks. RmGPT introduces a novel generative token-based framework, incorporating Signal Tokens, Prompt Tokens, Time-Frequency Task Tokens and Fault Tokens to handle heterogeneous data within a unified model architecture. We leverage self-supervised learning for robust feature extraction and introduce a next signal token prediction pretraining strategy, alongside efficient prompt learning for task-specific adaptation. Extensive experiments demonstrate that RmGPT significantly outperforms state-of-the-art algorithms, achieving near-perfect accuracy in diagnosis tasks and exceptionally low errors in prognosis tasks. Notably, RmGPT excels in few-shot learning scenarios, achieving 82% accuracy in 16-class one-shot experiments, highlighting its adaptability and robustness. This work establishes RmGPT as a powerful PHM foundation model for rotating machinery, advancing the scalability and generalizability of PHM solutions. \textbf{Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Pandalin98/RmGPT">https://github.com/Pandalin98/RmGPT</a>. </p>
<blockquote>
<p>åœ¨å·¥ä¸šé¢†åŸŸï¼Œæ—‹è½¬æœºæ¢°çš„å¯é æ€§å¯¹ç”Ÿäº§æ•ˆç‡å’Œå®‰å…¨è‡³å…³é‡è¦ã€‚ç›®å‰çš„çŠ¶æ€ç›‘æµ‹ä¸å¥åº·ç®¡ç†ï¼ˆPHMï¼‰æ–¹æ³•å¾€å¾€ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å…·æœ‰ä¸åŒä¿¡å·ç‰¹å¾ã€æ•…éšœæ¨¡å¼å’Œå·¥ä½œæ¡ä»¶çš„å¤šæ ·åŒ–æ•°æ®é›†æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚å—ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹å‘å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè¯Šæ–­å’Œé¢„åä»»åŠ¡çš„ç»Ÿä¸€æ¨¡å‹RmGPTã€‚RmGPTå¼•å…¥äº†ä¸€ç§æ–°å‹åŸºäºæ ‡è®°çš„ç”Ÿæˆå¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¿¡å·æ ‡è®°ã€æç¤ºæ ‡è®°ã€æ—¶é—´é¢‘ç‡ä»»åŠ¡æ ‡è®°å’Œæ•…éšœæ ‡è®°ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¶æ„ä¸­å¤„ç†å¼‚æ„æ•°æ®ã€‚æˆ‘ä»¬åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œç¨³å¥çš„ç‰¹å¾æå–ï¼Œå¹¶å¼•å…¥ä¸‹ä¸€ä¸ªä¿¡å·æ ‡è®°é¢„æµ‹é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠé«˜æ•ˆæç¤ºå­¦ä¹ è¿›è¡Œç‰¹å®šä»»åŠ¡çš„é€‚åº”ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRmGPTæ˜¾è‘—ä¼˜äºæœ€æ–°ç®—æ³•ï¼Œåœ¨è¯Šæ–­ä»»åŠ¡ä¸­å®ç°äº†è¿‘ä¹å®Œç¾çš„å‡†ç¡®ç‡ï¼Œåœ¨é¢„åä»»åŠ¡ä¸­é”™è¯¯ç‡æä½ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒRmGPTåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨16ç±»ä¸€æ¬¡æ€§å®éªŒä¸­å®ç°äº†8notifyAllå¹´çš„å‡†ç¡®ç‡ï¼Œçªæ˜¾äº†å…¶é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚è¯¥å·¥ä½œç¡®ç«‹äº†RmGPTä½œä¸ºæ—‹è½¬æœºæ¢°å¼ºå¤§çš„PHMåŸºç¡€æ¨¡å‹ï¼Œæé«˜äº†PHMè§£å†³æ–¹æ¡ˆçš„å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚**ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Pandalin98/RmGPT">https://github.com/Pandalin98/RmGPT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17604v2">PDF</a> This paper has been accepted for publication in the IEEE Internet of   Things Journal (IoT-J). The final version may differ slightly due to   editorial revisions. Please cite the journal version when available</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRmGPTçš„ç»Ÿä¸€æ¨¡å‹ï¼Œç”¨äºæ—‹è½¬æœºæ¢°çš„æ•…éšœè¯Šæ–­å’Œé¢„æµ‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºç”Ÿæˆå¼ä»¤ç‰Œçš„æ–°æ¡†æ¶ï¼Œå¯å¤„ç†ä¸åŒæ•°æ®é›†å’Œå¤æ‚ç¯å¢ƒä¸‹çš„ä¿¡å·ç‰¹å¾ã€‚æ¨¡å‹åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶å¼•å…¥æ–°çš„é¢„è®­ç»ƒç­–ç•¥è¿›è¡Œä¿¡å·ä»¤ç‰Œé¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒRmGPTåœ¨è¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºæé«˜çš„å‡†ç¡®æ€§ï¼Œåœ¨é¢„æµ‹ä»»åŠ¡ä¸­è¯¯å·®æä½ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚æ­¤æ¨¡å‹æ˜¯æ—‹è½¬æœºæ¢°é¢†åŸŸå¼ºå¤§çš„PHMåŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RmGPTæ˜¯ä¸€ä¸ªç”¨äºæ—‹è½¬æœºæ¢°æ•…éšœè¯Šæ–­å’Œé¢„æµ‹çš„ç»Ÿä¸€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ç”Ÿæˆå¼ä»¤ç‰Œæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¿¡å·ä»¤ç‰Œã€æç¤ºä»¤ç‰Œã€æ—¶é—´é¢‘ç‡ä»»åŠ¡ä»¤ç‰Œå’Œæ•…éšœä»¤ç‰Œï¼Œä»¥å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®ã€‚</li>
<li>RmGPTåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œç‰¹å¾æå–ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å¼•å…¥ä¿¡å·ä»¤ç‰Œé¢„æµ‹é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠé«˜æ•ˆçš„ä»»åŠ¡ç‰¹å®šæç¤ºå­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºRmGPTåœ¨è¯Šæ–­ä»»åŠ¡ä¸­å…·æœ‰æé«˜çš„å‡†ç¡®æ€§ï¼Œé¢„æµ‹ä»»åŠ¡ä¸­çš„è¯¯å·®å¾ˆä½ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒRmGPTè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3b50972519e1bd12449ac5f7102c5e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b97722b2578595b3c378508be055dee6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ddaf09862b86e16bdde87e1dfd6dad83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4347c831bee347f5ddbd9dbf0f8e96a7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3ffd82c8157d7e5d12b1568ce0e21a4d.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  Generalist Models in Medical Image Segmentation A Survey and   Performance Comparison with Task-Specific Approaches
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae0aae73c23b336f88844796f445819e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  AutoMind Adaptive Knowledgeable Agent for Automated Data Science
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
