<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-14  Self-Adapting Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3ec1605afba63907ca3459385fafa44d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-14-更新"><a href="#2025-06-14-更新" class="headerlink" title="2025-06-14 更新"></a>2025-06-14 更新</h1><h2 id="Self-Adapting-Language-Models"><a href="#Self-Adapting-Language-Models" class="headerlink" title="Self-Adapting Language Models"></a>Self-Adapting Language Models</h2><p><strong>Authors:Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, Pulkit Agrawal</strong></p>
<p>Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model’s own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at <a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal">https://jyopari.github.io/posts/seal</a>. </p>
<blockquote>
<p>大型语言模型（LLM）虽然强大但相对静态，它们缺乏适应新任务、知识或示例的机制来调整其权重。我们引入了自适应大型语言模型（SEAL）框架，该框架使LLM能够通过生成自己的微调数据和更新指令进行自我适应。对于新的输入，模型会产生自我编辑的一代产品，这可能会以不同的方式重组信息，指定优化超参数，或调用数据增强工具和基于梯度的更新工具。通过监督微调（SFT），这些自我编辑会导致持久的权重更新，从而实现持久的适应。为了训练模型以产生有效的自我编辑，我们使用强化学习循环，以更新模型的下游性能作为奖励信号。不同于依赖单独适应模块或辅助网络的先前方法，SEAL直接使用模型本身的生成来控制其适应过程。在知识融入和少量场景下的通用化实验表明，SEAL是朝着能够自我指导适应的语言模型方向迈出的一步。我们的网站和代码可在 <a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal">https://jyopari.github.io/posts/seal</a> 中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10943v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型强大但静态，无法适应新任务、知识或示例。我们提出Self-Adapting LLMs（SEAL）框架，使LLMs能够通过生成自己的微调数据和更新指令进行自我适应。给定新输入，模型产生自我编辑，通过监督微调（SFT）实现持久权重更新和长期适应。我们使用强化学习循环训练模型产生有效的自我编辑，以更新模型的下游性能作为奖励信号。不同于依赖额外适应模块或辅助网络的先前方法，SEAL直接使用模型自身的生成来控制其适应过程。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs缺乏适应新任务、知识或示例的机制。</li>
<li>Self-Adapting LLMs（SEAL）框架使LLMs能够自我适应。</li>
<li>模型通过生成自我编辑实现自我适应，包括信息重组、优化超参数、数据增强和基于梯度的更新。</li>
<li>监督微调（SFT）使自我编辑导致持久的权重更新和长期适应。</li>
<li>使用强化学习循环训练模型产生有效的自我编辑。</li>
<li>下游性能作为奖励信号来评估更新模型的性能。</li>
<li>SEAL不同于先前方法，它直接使用模型自身的生成控制适应过程。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10943">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3ec1605afba63907ca3459385fafa44d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f114ce44336f5ab60f35250dc70fe9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a753461583488b92e0b9e22800090050.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6b487a779381eff91524b491c9f3e49.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Diffusion-Duality"><a href="#The-Diffusion-Duality" class="headerlink" title="The Diffusion Duality"></a>The Diffusion Duality</h2><p><strong>Authors:Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, Volodymyr Kuleshov</strong></p>
<p>Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: <a target="_blank" rel="noopener" href="http://s-sahoo.github.io/duo">http://s-sahoo.github.io/duo</a> </p>
<blockquote>
<p>均匀状态离散扩散模型因其固有的自我校正能力而具有快速文本生成的潜力。然而，它们通常被自回归模型和掩码扩散模型所超越。在这项工作中，我们通过利用一个关键见解来缩小性能差距：均匀状态扩散过程自然来自于基础的高斯扩散。我们的方法Duo，从高斯扩散转移强大技术，提高了训练和采样的性能。首先，我们引入了一种由高斯过程引导的课程学习策略，通过减少方差使训练速度翻倍。用课程学习策略训练的模型在7个基准测试中的3个上实现了零射击困惑度超过自回归模型。其次，我们提出了离散一致性蒸馏，该算法将一致性蒸馏从连续环境适应到离散环境。该算法通过加速采样速度两个数量级，实现了扩散语言模型中的少步骤生成。我们已在项目页面上提供了代码和模型检查点：<a target="_blank" rel="noopener" href="http://s-sahoo.github.io/duo">http://s-sahoo.github.io/duo</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10892v1">PDF</a> ICML 2025. We provide the code at: <a target="_blank" rel="noopener" href="https://github.com/s-sahoo/duo">https://github.com/s-sahoo/duo</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于均匀态离散扩散模型的文本生成方法，通过借鉴高斯扩散过程中的关键技术来提升其性能。文章提出了两种改进策略：一是引入高斯过程引导的课程体系学习策略，通过减少方差来加倍训练速度；二是在离散环境中适配一致性蒸馏技术，实现了离散一致性蒸馏。这些技术使得扩散语言模型的零样本困惑度在多个基准测试中超越自回归模型，并实现了快速采样。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于均匀态离散扩散模型的文本生成方法具有自我校正的潜力，但之前被自回归模型和掩模扩散模型所超越。</li>
<li>文章提出了借鉴高斯扩散过程中的技术来提升均匀态离散扩散模型的性能。</li>
<li>引入高斯过程引导的课程体系学习策略，通过减少方差来加倍训练速度，并在某些基准测试中超越自回归模型。</li>
<li>提出了离散一致性蒸馏技术，该技术使得扩散语言模型能够实现快速采样，并解锁了少步生成的能力。</li>
<li>文章强调了将连续设置中的技术适配到离散环境中的重要性。</li>
<li>文章提供了代码和模型检查点以供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10892">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe13f5d9025e8eabb9985bf44538e970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c387fcdacae32c684fe65200a40bd1d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniFluids-Unified-Physics-Pre-trained-Modeling-of-Fluid-Dynamics"><a href="#OmniFluids-Unified-Physics-Pre-trained-Modeling-of-Fluid-Dynamics" class="headerlink" title="OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics"></a>OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics</h2><p><strong>Authors:Rui Zhang, Qi Meng, Han Wan, Yang Liu, Zhi-Ming Ma, Hao Sun</strong></p>
<p>High-fidelity and efficient simulation of fluid dynamics drive progress in various scientific and engineering applications. Traditional computational fluid dynamics methods offer strong interpretability and guaranteed convergence, but rely on fine spatial and temporal meshes, incurring prohibitive computational costs. Physics-informed neural networks (PINNs) and neural operators aim to accelerate PDE solvers using deep learning techniques. However, PINNs require extensive retraining and careful tuning, and purely data-driven operators demand large labeled datasets. Hybrid physics-aware methods embed numerical discretizations into network architectures or loss functions, but achieve marginal speed gains and become unstable when balancing coarse priors against high-fidelity measurements. To this end, we introduce OmniFluids, a unified physics pre-trained operator learning framework that integrates physics-only pre-training, coarse-grid operator distillation, and few-shot fine-tuning, which enables fast inference and accurate prediction under limited or zero data supervision. For architectural design, the key components of OmniFluids include a mixture of operators, a multi-frame decoder, and factorized Fourier layers, which enable efficient and scalable modeling of diverse physical tasks while maintaining seamless integration with physics-based supervision. Across a broad range of two- and three-dimensional benchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven methods in flow field reconstruction and turbulence statistics accuracy, delivering 10-100x speedups compared to classical solvers, and accurately recovers unknown physical parameters from sparse, noisy data. This work establishes a new paradigm for efficient and generalizable surrogate modeling in complex fluid systems under limited data availability. </p>
<blockquote>
<p>流体动力学的高精度和高效模拟推动了各种科学和工程应用的进步。传统的计算流体动力学方法虽然具有良好的可解释性和保证收敛性，但它们依赖于精细的空间和时间网格，计算成本高昂。物理信息神经网络（PINNs）和神经算子旨在使用深度学习技术加速偏微分方程求解器。然而，PINNs需要大量的重新训练和精细调整，而纯粹的数据驱动算子则需要大量标记数据集。混合物理感知方法将数值离散化嵌入网络架构或损失函数中，但在平衡粗略先验和高精度测量时，只实现了有限的加速，并且变得不稳定。为此，我们引入了OmniFluids，这是一个统一的物理预训练算子学习框架，它集成了仅物理预训练、粗网格算子蒸馏和少量微调，能够在有限或无数据监督的情况下实现快速推理和准确预测。在架构设计方面，OmniFluids的关键组件包括混合算子、多帧解码器和因子化傅里叶层，这些组件能够在维持与基于物理的监督无缝集成的同时，实现各种物理任务的高效和可扩展建模。在广泛的二维和三维基准测试中，OmniFluids在流场重建和湍流统计准确性方面显著优于最新的人工智能驱动方法，与经典求解器相比实现了10-100倍的加速，并能从稀疏、嘈杂的数据中准确恢复未知的物理参数。这项工作为在有限数据可用性的情况下，复杂流体系统中高效且可推广的替代建模建立了新范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了OmniFluids，一个统一物理预训练算子学习框架，融合了物理预训练、粗网格算子蒸馏和少量精细调整技术。该框架实现了快速推断和有限或零数据监督下的准确预测。OmniFluids的关键组件包括混合算子、多帧解码器和分解傅里叶层，能高效且规模化地模拟多种物理任务，同时无缝集成物理基础监督。在广泛的二维和三维基准测试中，OmniFluids在流场重建和湍流统计准确性方面显著优于最新的人工智能驱动方法，与经典求解器相比实现了10-100倍的速度提升，并能从稀疏、嘈杂的数据中准确恢复未知的物理参数。这一工作建立了在有限数据下复杂流体系统中高效且可通用的替代建模的新范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniFluids是一个统一物理预训练算子学习框架，融合了物理预训练技术。</li>
<li>该框架集成了粗网格算子蒸馏和少量精细调整技术，实现快速推断和有限数据监督下的准确预测。</li>
<li>OmniFluids包括混合算子、多帧解码器和分解傅里叶层等关键组件。</li>
<li>OmniFluids能高效且规模化地模拟多种物理任务，同时无缝集成物理基础监督。</li>
<li>在基准测试中，OmniFluids显著优于最新的人工智能驱动方法，实现了流场重建和湍流统计的高准确性。</li>
<li>与传统计算方法相比，OmniFluids提供了10-100倍的速度提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e1b2ada2f3038f2e9c86c359c3ff9935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-610059c9f2d208ab174ff1eed4c323f9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-Driven-Personalized-Answer-Generation-and-Evaluation"><a href="#LLM-Driven-Personalized-Answer-Generation-and-Evaluation" class="headerlink" title="LLM-Driven Personalized Answer Generation and Evaluation"></a>LLM-Driven Personalized Answer Generation and Evaluation</h2><p><strong>Authors:Mohammadreza Molavi, Mohammadreza Tavakoli, Mohammad Moein, Abdolali Faraji, Gábor Kismihók</strong></p>
<p>Online learning has experienced rapid growth due to its flexibility and accessibility. Personalization, adapted to the needs of individual learners, is crucial for enhancing the learning experience, particularly in online settings. A key aspect of personalization is providing learners with answers customized to their specific questions. This paper therefore explores the potential of Large Language Models (LLMs) to generate personalized answers to learners’ questions, thereby enhancing engagement and reducing the workload on educators. To evaluate the effectiveness of LLMs in this context, we conducted a comprehensive study using the StackExchange platform in two distinct areas: language learning and programming. We developed a framework and a dataset for validating automatically generated personalized answers. Subsequently, we generated personalized answers using different strategies, including 0-shot, 1-shot, and few-shot scenarios. The generated answers were evaluated using three methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our findings indicated that providing LLMs with examples of desired answers (from the learner or similar learners) can significantly enhance the LLMs’ ability to tailor responses to individual learners’ needs. </p>
<blockquote>
<p>在线学习因其灵活性和可访问性而经历了快速增长。个性化适应个别学习者的需求，对于增强学习体验，特别是在在线环境中，至关重要。个性化的关键方面是为学习者提供针对其特定问题的定制答案。因此，本文探讨了大型语言模型（LLM）为学习者的问题生成个性化答案的潜力，从而提高参与度，减轻教育工作者的负担。为了评估LLM在此背景下的有效性，我们使用了StackExchange平台在语言学习和编程两个不同的领域进行了全面的研究。我们开发了一个框架和数据集来验证自动生成的个性化答案。随后，我们采用不同的策略生成了个性化答案，包括零样本、一个样例和几个样例场景。生成的答案通过三种方法进行评估：1. BERTScore、2. LLM评估和3.人工评估。我们的研究结果表明，向LLM提供期望答案的示例（来自学习者或类似的学习者）可以显著提高LLM适应个别学习者需求的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10829v1">PDF</a> This is the preprint version of a paper accepted at AIED 2025. The   final version will be published by Springer</p>
<p><strong>Summary</strong></p>
<p>在线学习因其灵活性和可访问性而迅速增长。个性化适应个别学习者的需求对于提升学习体验尤为重要，特别是在在线环境中。本文探索了大型语言模型（LLMs）为学习者生成个性化答案的潜力，从而提高参与度和减轻教育工作者的负担。为了评估LLMs在此背景下的有效性，我们利用StackExchange平台在语言学习和编程两个领域进行了全面的研究。我们开发了一个框架和数据集来验证自动生成的个人化答案。通过不同的策略（如零样本、单样本和少样本场景）生成个性化答案，并使用三种方法进行评估：BERTScore、LLM评估和人工评估。我们发现为LLMs提供期望答案的示例（来自学习者或类似学习者）可以显著提高LLMs适应个别学习者需求的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在线学习的增长得益于其灵活性和可访问性。</li>
<li>个性化对于提升在线学习体验至关重要。</li>
<li>大型语言模型（LLMs）有潜力为学习者生成个性化答案，从而提高参与度和减轻教育者的工作负担。</li>
<li>在语言学习和编程领域进行了LLMs的研究评估。</li>
<li>开发了一个框架和数据集来验证自动生成的个人化答案的有效性。</li>
<li>通过不同的策略（如零样本、单样本和少样本场景）生成个性化答案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10829">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a596d6b230433035bb2a5e8e9899412c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec3b3388a811aa15aec993723184e720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0a616a2653703504c007bed5dbfb39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15c415955005c7b857fdd20b15222a31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8368a821f868d1049f7ab1c23690c3e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches"><a href="#Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches" class="headerlink" title="Generalist Models in Medical Image Segmentation: A Survey and   Performance Comparison with Task-Specific Approaches"></a>Generalist Models in Medical Image Segmentation: A Survey and   Performance Comparison with Task-Specific Approaches</h2><p><strong>Authors:Andrea Moglia, Matteo Leccardi, Matteo Cavicchioli, Alice Maccarini, Marco Marcon, Luca Mainardi, Pietro Cerveri</strong></p>
<p>Following the successful paradigm shift of large language models, leveraging pre-training on a massive corpus of data and fine-tuning on different downstream tasks, generalist models have made their foray into computer vision. The introduction of Segment Anything Model (SAM) set a milestone on segmentation of natural images, inspiring the design of a multitude of architectures for medical image segmentation. In this survey we offer a comprehensive and in-depth investigation on generalist models for medical image segmentation. We start with an introduction on the fundamentals concepts underpinning their development. Then, we provide a taxonomy on the different declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on the recent SAM 2, on other innovative models trained on images alone, and others trained on both text and images. We thoroughly analyze their performances at the level of both primary research and best-in-literature, followed by a rigorous comparison with the state-of-the-art task-specific models. We emphasize the need to address challenges in terms of compliance with regulatory frameworks, privacy and security laws, budget, and trustworthy artificial intelligence (AI). Finally, we share our perspective on future directions concerning synthetic data, early fusion, lessons learnt from generalist models in natural language processing, agentic AI and physical AI, and clinical translation. </p>
<blockquote>
<p>在大型语言模型成功实现范式转变之后，通过预训练大量数据语料库并在不同下游任务上进行微调，通用模型已经涉足计算机视觉领域。Segment Anything Model（SAM）的引入为自然图像分割树立了里程碑，激发了多种医疗图像分割架构的设计灵感。在这篇综述中，我们对医疗图像分割的通用模型进行了全面深入的研究。首先介绍了支撑它们发展的基本概念。然后，我们根据零样本、少样本、微调、适配器等方面对SAM的不同发展方向进行了分类，并介绍了最近的SAM 2以及其他单独在图像上训练的创新模型，还有其他同时训练文本和图像模型。我们全面分析了它们在初级研究和文献最佳水平上的表现，并与最新的特定任务模型进行了严格比较。我们强调了需要解决合规性框架、隐私和安全法律、预算和可信人工智能等方面的挑战。最后，我们分享了我们对合成数据、早期融合、从自然语言处理通用模型中学到的教训、智能体AI和物理AI以及临床翻译的未来方向的看法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10825v1">PDF</a> 132 pages, 26 figures, 23 tables. Andrea Moglia and Matteo Leccardi   are equally contributing authors</p>
<p><strong>Summary</strong></p>
<p>大型语言模型成功范式转移后，通用模型已涉足计算机视觉领域。Segment Anything Model（SAM）的引入为自然图像分割树立了里程碑，激发了多种医疗图像分割架构的设计。本文全面深入地调查了医疗图像分割的通用模型，从基本概念入手，提供SAM的不同变种分类，如零样本、少样本、微调、适配器等，并分析其与最新任务特定模型的性能对比。强调需要解决合规性、隐私和安全性、预算及可信人工智能等挑战。展望未来方向，包括合成数据、早期融合、自然语言处理中的通用模型教训、代理人工智能及物理人工智能和临床翻译等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的范式转移推动了通用模型在医疗图像分割领域的应用。</li>
<li>Segment Anything Model (SAM) 为自然图像分割树立了里程碑。</li>
<li>SAM的多种变种如零样本、少样本、微调、适配器等在医疗图像分割领域有广泛应用。</li>
<li>通用模型在医疗图像分割方面的性能与最新任务特定模型进行了对比分析。</li>
<li>面临合规性、隐私和安全性、预算等方面的挑战。</li>
<li>需要从自然语言处理的通用模型中吸取教训，将其应用于医疗图像分割。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10825">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bbd94bf513f6f2d305d0ffc2eca684d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95307b0eac671eb54b1fc8800f1922e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ffd82c8157d7e5d12b1568ce0e21a4d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain"><a href="#IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain" class="headerlink" title="IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain"></a>IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly   Detection in Medical Domain</h2><p><strong>Authors:Hong Huang, Weixiang Sun, Zhijian Wu, Jingwen Niu, Donghuan Lu, Xian Wu, Yefeng Zheng</strong></p>
<p>Recent advances in vision-language models, such as CLIP, have significantly improved performance in zero- and few-shot anomaly detection (ZFSAD) tasks. However, most existing CLIP-based methods assume prior knowledge of categories and rely on carefully designed prompts tailored to specific scenarios. While these text prompts capture semantic information in the textual space, they often fail to distinguish normal and anomalous instances in the joint embedding space. Moreover, most ZFSAD approaches focus on industrial domains, with limited exploration in medical tasks. To address these limitations, we propose IQE-CLIP, a novel framework for ZFSAD in the medical domain. We show that query embeddings integrating both textual and instance-aware visual information serve as more effective indicators of anomalies. Specifically, we introduce class-based and learnable prompting tokens to better adapt CLIP to the medical setting. Furthermore, we design an instance-aware query module that extracts region-level contextual information from both modalities, enabling the generation of anomaly-sensitive embeddings. Extensive experiments on six medical datasets demonstrate that IQE-CLIP achieves state-of-the-art performance in both zero-shot and few-shot settings. Code and data are available at \href{<a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/%7D%7Bthis">https://github.com/hongh0/IQE-CLIP/}{this</a> https URL}. </p>
<blockquote>
<p>最近，诸如CLIP之类的视觉语言模型取得了重大进展，已在零样本和少样本异常检测（ZFSAD）任务中显著提高了性能。然而，大多数现有的基于CLIP的方法假设对类别有先验知识，并依赖于针对特定场景精心设计的提示。虽然这些文本提示捕获了文本空间中的语义信息，但它们通常在联合嵌入空间中无法区分正常和异常的实例。此外，大多数ZFSAD方法都集中在工业领域，在医疗任务方面的探索有限。为了解决这些局限性，我们提出了IQE-CLIP，这是一个用于医疗领域的ZFSAD的新型框架。我们表明，融合了文本和实例感知视觉信息的查询嵌入作为异常的更有效指标。具体来说，我们引入了基于类别的和可学习的提示令牌，以更好地适应CLIP在医疗环境中的使用。此外，我们设计了一个实例感知查询模块，可以从两种模态中提取区域级别的上下文信息，从而生成对异常敏感的嵌入。在六个医疗数据集上的广泛实验表明，IQE-CLIP在零样本和少样本设置中均达到了最新技术水平。代码和数据可在[<a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/]%E8%BF%99%E4%B8%AA%E9%93%BE%E6%8E%A5%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hongh0/IQE-CLIP/]这个链接中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10730v1">PDF</a> </p>
<p><strong>Summary</strong><br>     最近，CLIP等视觉语言模型在零样本和少样本异常检测任务上的性能有了显著提升。然而，现有的CLIP方法大多假定有类别先验知识并依赖针对特定场景的精心设计的文本提示。本文提出IQE-CLIP框架，将文本和实例感知的视觉信息集成到查询嵌入中，以更有效地检测医学领域的异常。通过引入基于类别的可学习提示令牌和实例感知查询模块，IQE-CLIP在六个医学数据集上实现了零样本和少样本设置中的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP等视觉语言模型在零样本和少样本异常检测任务上表现优异。</li>
<li>现有方法依赖类别先验知识和针对特定场景的文本提示。</li>
<li>IQE-CLIP框架集成文本和实例感知的视觉信息到查询嵌入中，有效提高异常检测效果。</li>
<li>IQE-CLIP引入基于类别的可学习提示令牌，更好地适应医学领域。</li>
<li>实例感知查询模块能提取两种模态的区域级上下文信息，生成对异常敏感的嵌入。</li>
<li>IQE-CLIP在六个医学数据集上实现了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ac687c5ed6d72ac284dabfd2a676c383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f54646641f5923a7ef176de168cb89c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c51ef4e50943405dae4645a0783f91ca.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="NeuralNexus-at-BEA-2025-Shared-Task-Retrieval-Augmented-Prompting-for-Mistake-Identification-in-AI-Tutors"><a href="#NeuralNexus-at-BEA-2025-Shared-Task-Retrieval-Augmented-Prompting-for-Mistake-Identification-in-AI-Tutors" class="headerlink" title="NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for   Mistake Identification in AI Tutors"></a>NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for   Mistake Identification in AI Tutors</h2><p><strong>Authors:Numaan Naeem, Sarfraz Ahmad, Momina Ahsan, Hasan Iqbal</strong></p>
<p>This paper presents our system for Track 1: Mistake Identification in the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The task involves evaluating whether a tutor’s response correctly identifies a mistake in a student’s mathematical reasoning. We explore four approaches: (1) an ensemble of machine learning models over pooled token embeddings from multiple pretrained language models (LMs); (2) a frozen sentence-transformer using [CLS] embeddings with an MLP classifier; (3) a history-aware model with multi-head attention between token-level history and response embeddings; and (4) a retrieval-augmented few-shot prompting system with a large language model (LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples, constructs structured prompts, and uses schema-guided output parsing to produce interpretable predictions. It outperforms all baselines, demonstrating the effectiveness of combining example-driven prompting with LLM reasoning for pedagogical feedback assessment. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/BEA_2025">https://github.com/NaumanNaeem/BEA_2025</a>. </p>
<blockquote>
<p>本文介绍了我们在Track 1：AI辅导老师教学技能评估的BEA 2025共享任务中的错误识别系统。该任务涉及评估辅导老师的回应是否能正确识别学生数学推理中的错误。我们探索了四种方法：（1）使用来自多个预训练语言模型的合并令牌嵌入的机器学习模型集合；（2）使用[CLS]嵌入与MLP分类器的冻结句子转换器；（3）在令牌级别历史与响应嵌入之间具有多头注意力的历史感知模型；（4）带有大型语言模型（LLM）即GPT 4o的增强检索提示系统。我们的最终系统检索语义相似的示例，构建结构化提示，并使用模式引导的输出解析来产生可解释的预测。它超越了所有基线，证明了结合示例驱动的提示和LLM推理进行教学反馈评估的有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/BEA_2025%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NaumanNaeem/BEA_2025找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10627v1">PDF</a> 6 pages, 2 figures, 1 table</p>
<p><strong>Summary</strong><br>本文介绍了针对BEA 2025共享任务中的轨道1：人工智能辅导老师的教学能力评估中的错误识别系统。任务旨在评估辅导老师的回应是否能正确识别学生数学推理中的错误。本文探索了四种方法，包括基于多个预训练语言模型的集成机器学习模型、使用冻结的句子变换器和多层感知器分类器的[CLS]嵌入、具有令牌级别历史与响应嵌入之间多头注意力的历史感知模型，以及结合大型语言模型（LLM）的检索增强型少样本提示系统（如GPT 4o）。最终系统通过检索相似示例、构建结构化提示和模式引导的输出解析来生成可解释的预测，超越了所有基线，证明了结合示例驱动的提示和LLM推理进行教学法反馈评估的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文介绍了针对人工智能辅导老师教学评估的任务——错误识别系统。</li>
<li>四种方法的探索包括基于集成机器学习模型的响应错误识别系统。</li>
<li>采用冻结的句子变换器和多层感知器分类器来处理响应与令牌之间的关系。</li>
<li>历史感知模型结合了令牌级别历史和响应嵌入的多头注意力机制。</li>
<li>采用检索增强型少样本提示系统结合了大型语言模型（LLM）技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ee68dc9cdc1b7ade3d46fef3eedc6458.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23c2995515e5e26960d32890ea9ecefe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47497a6ccc3902b959e7db1c4e3e2cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9245408d6408d296092444c26904ea4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PointGS-Point-Attention-Aware-Sparse-View-Synthesis-with-Gaussian-Splatting"><a href="#PointGS-Point-Attention-Aware-Sparse-View-Synthesis-with-Gaussian-Splatting" class="headerlink" title="PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian   Splatting"></a>PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian   Splatting</h2><p><strong>Authors:Lintao Xiang, Hongpei Zheng, Yating Huang, Qijun Yang, Hujun Yin</strong></p>
<p>3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods. </p>
<blockquote>
<p>3D高斯涂抹（3DGS）是一种创新的渲染技术，它通过利用明确的3D场景表示在渲染速度和视觉质量上都超越了神经辐射场（NeRF）。现有的3DGS方法需要大量校准的视图来生成一致和完整的场景表示。当输入视图有限时，3DGS容易过度拟合训练视图，导致渲染质量明显下降。为了解决这一局限性，我们提出了一种点特征感知的高斯涂抹框架，能够实现从稀疏训练视图的实时高质量渲染。具体来说，我们首先采用最新的立体基础模型来估计准确的相机姿态并重建用于高斯初始化的密集点云。然后，我们通过从稀疏输入中采样和聚合多尺度2D外观特征来编码每个3D高斯的颜色属性。为了增强点级外观表示，我们设计了一个基于自注意力机制的点交互网络，允许每个高斯点与最近的邻居进行交互。这些丰富的特征随后通过两个轻量级的多层感知器（MLP）解码为高斯参数，用于最终渲染。在多种基准测试上的广泛实验表明，我们的方法显著优于基于NeRF的方法，并在小样本设置下实现了与最新3DGS方法相当的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10335v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于显式三维场景表示的高效渲染技术三维高斯插片方法能够有效提高渲染速度并提升视觉质量。当面临稀疏训练视图时，提出了一种点特征感知高斯插片框架以实现实时高质量渲染。该框架采用最新的立体基础模型估计摄像机姿态并重建用于高斯初始化的密集点云。此外，框架对三维高斯编码色彩属性并增强点对特征的代表性进行设计网络基于自注意力机制构建模型交互网并丰富每个高斯点的特征表现以改进最后的渲染结果。在基准测试实验中验证了方法表现远超NeRF所达成实现全面优越性相比于其它领先的高斯插片方法具有竞争力。实验表明，该方法在多种基准测试上表现优异，显著优于基于NeRF的方法，并在少样本设置下实现了与最新3DGS方法相当的竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS方法在渲染速度和视觉质量方面超越神经网络辐射场方法（NeRF）。但其现有局限包括面对有限输入视图时表现出的过度拟合和渲染质量下降问题。为了解决这个问题，研究人员提出了一个名为Point-wise Feature-Aware Gaussian Splatting的新框架，该框架实现了从稀疏训练视图进行实时高质量渲染。该框架采用最新的立体基础模型进行相机姿态估计和密集点云重建；</li>
<li>Point-wise Feature-Aware Gaussian Splatting利用多点视角数据和不同维度的信息融合来提升点特征的代表性，采用基于自注意力机制的网络设计模型来增强每个高斯点的特征表达；最后进行关键编码及参数化的Gaussian数据处理以便用于最终的渲染效果实现强化整体展示水平及优点强调无实物和无素材的渲染技术；</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10335">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d17a557513373ee03c744299c89bf2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5df2a46b4102e410cb6aed2c254111aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e09c01b5e63a10178054fcda4e554db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8471f3a39960344a9aa84b4ef08bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32362cab3c738eb4e8e8a7bf8c8220dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection"><a href="#Few-Shot-Learner-Generalizes-Across-AI-Generated-Image-Detection" class="headerlink" title="Few-Shot Learner Generalizes Across AI-Generated Image Detection"></a>Few-Shot Learner Generalizes Across AI-Generated Image Detection</h2><p><strong>Authors:Shiyu Wu, Jing Liu, Jing Li, Yequan Wang</strong></p>
<p>Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, these detectors suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space for effectively distinguishing unseen fake images using very few samples. Experiments show that FSD achieves state-of-the-art performance by $+11.6%$ average accuracy on the GenImage dataset with only $10$ additional samples. More importantly, our method is better capable of capturing the intra-category commonality in unseen images without further training. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/teheperinko541/Few-Shot-AIGI-Detector">https://github.com/teheperinko541/Few-Shot-AIGI-Detector</a>. </p>
<blockquote>
<p>当前基于大型合成图像数据集训练的假冒图像检测器在有限的生成模型研究上表现满意。然而，这些检测器在面对未知模型时性能会显著下降。此外，从在线生成模型中收集足够的训练数据通常成本高昂或不可行。为了克服这些问题，我们提出了Few-Shot Detector（FSD），这是一种新型的人工智能图像检测器，它通过非常有限的样本学习专门的度量空间，以有效区分未知的假冒图像。实验表明，FSD在GenImage数据集上通过仅增加10个样本就达到了最先进的性能，平均准确率提高了+11.6%。更重要的是，我们的方法能够更好地捕获未知图像中的类别内共性，而无需进一步训练。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/teheperinko541/Few-Shot-AIGI-Detector%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/teheperinko541/Few-Shot-AIGI-Detector上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08763v2">PDF</a> 12 pages, 6 figures, Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>基于当前假图像检测器在特定合成图像数据集上的训练表现，本文提出了一种全新的AI生成图像检测器——Few-Shot Detector（FSD）。该检测器能够在非常有限的样本下，通过学习专门的度量空间来有效区分未见过的假图像。实验表明，在GenImage数据集上，FSD通过使用仅10个额外样本达到了行业领先的水平，平均准确度提高了11.6%。并且FSD能更好的捕捉未见图像中的类别共性而无需进一步的训练。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>当前假图像检测器在特定合成图像数据集上的表现良好，但在未见模型上的性能显著下降。</li>
<li>提出了全新的AI生成图像检测器——Few-Shot Detector（FSD）。</li>
<li>FSD能够在非常有限的样本下有效区分未见过的假图像。</li>
<li>在GenImage数据集上，FSD通过使用仅10个额外样本达到了行业领先的水平，平均准确度提高了11.6%。</li>
<li>FSD能够更好地捕捉未见图像中的类别共性。</li>
<li>FSD无需进一步的训练就能实现这一功能。</li>
<li>代码已公开在GitHub上，便于其他研究者使用与进一步开发。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1e429420cd8c3d87562ac4e8a9ef0d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e624300d8a346afa4a692dbfab65a0e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e6ebefd80a359d53392ef39d38b37dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d728853e95fa96ea6e395d9ed7f20506.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fadf302a7678224439471eec691edb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5b6abeb2d2d427eb49c4f0a4703464.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RmGPT-A-Foundation-Model-with-Generative-Pre-trained-Transformer-for-Fault-Diagnosis-and-Prognosis-in-Rotating-Machinery"><a href="#RmGPT-A-Foundation-Model-with-Generative-Pre-trained-Transformer-for-Fault-Diagnosis-and-Prognosis-in-Rotating-Machinery" class="headerlink" title="RmGPT: A Foundation Model with Generative Pre-trained Transformer for   Fault Diagnosis and Prognosis in Rotating Machinery"></a>RmGPT: A Foundation Model with Generative Pre-trained Transformer for   Fault Diagnosis and Prognosis in Rotating Machinery</h2><p><strong>Authors:Yilin Wang, Yifei Yu, Kong Sun, Peixuan Lei, Yuxuan Zhang, Enrico Zio, Aiguo Xia, Yuanxiang Li</strong></p>
<p>In industry, the reliability of rotating machinery is critical for production efficiency and safety. Current methods of Prognostics and Health Management (PHM) often rely on task-specific models, which face significant challenges in handling diverse datasets with varying signal characteristics, fault modes and operating conditions. Inspired by advancements in generative pretrained models, we propose RmGPT, a unified model for diagnosis and prognosis tasks. RmGPT introduces a novel generative token-based framework, incorporating Signal Tokens, Prompt Tokens, Time-Frequency Task Tokens and Fault Tokens to handle heterogeneous data within a unified model architecture. We leverage self-supervised learning for robust feature extraction and introduce a next signal token prediction pretraining strategy, alongside efficient prompt learning for task-specific adaptation. Extensive experiments demonstrate that RmGPT significantly outperforms state-of-the-art algorithms, achieving near-perfect accuracy in diagnosis tasks and exceptionally low errors in prognosis tasks. Notably, RmGPT excels in few-shot learning scenarios, achieving 82% accuracy in 16-class one-shot experiments, highlighting its adaptability and robustness. This work establishes RmGPT as a powerful PHM foundation model for rotating machinery, advancing the scalability and generalizability of PHM solutions. \textbf{Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Pandalin98/RmGPT">https://github.com/Pandalin98/RmGPT</a>. </p>
<blockquote>
<p>在工业领域，旋转机械的可靠性对生产效率和安全至关重要。目前的状态监测与健康管理（PHM）方法往往依赖于特定任务的模型，这些模型在处理具有不同信号特征、故障模式和工作条件的多样化数据集时面临巨大挑战。受生成式预训练模型发展的启发，我们提出了用于诊断和预后任务的统一模型RmGPT。RmGPT引入了一种新型基于标记的生成式框架，该框架结合了信号标记、提示标记、时间频率任务标记和故障标记，在一个统一的模型架构中处理异构数据。我们利用自监督学习进行稳健的特征提取，并引入下一个信号标记预测预训练策略，以及高效提示学习进行特定任务的适应。大量实验表明，RmGPT显著优于最新算法，在诊断任务中实现了近乎完美的准确率，在预后任务中错误率极低。值得一提的是，RmGPT在少样本学习场景中表现出色，在16类一次性实验中实现了8notifyAll年的准确率，突显了其适应性和稳健性。该工作确立了RmGPT作为旋转机械强大的PHM基础模型，提高了PHM解决方案的可扩展性和通用性。**代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/Pandalin98/RmGPT">https://github.com/Pandalin98/RmGPT</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17604v2">PDF</a> This paper has been accepted for publication in the IEEE Internet of   Things Journal (IoT-J). The final version may differ slightly due to   editorial revisions. Please cite the journal version when available</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为RmGPT的统一模型，用于旋转机械的故障诊断和预测。该模型采用基于生成式令牌的新框架，可处理不同数据集和复杂环境下的信号特征。模型利用自监督学习进行特征提取，并引入新的预训练策略进行信号令牌预测。实验表明，RmGPT在诊断任务中表现出极高的准确性，在预测任务中误差极低，尤其在少样本学习场景下表现优异。此模型是旋转机械领域强大的PHM基础模型，具有广泛的应用前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RmGPT是一个用于旋转机械故障诊断和预测的统一模型。</li>
<li>该模型采用生成式令牌框架，包括信号令牌、提示令牌、时间频率任务令牌和故障令牌，以处理不同类型的数据。</li>
<li>RmGPT利用自监督学习进行特征提取，提高模型的性能和泛化能力。</li>
<li>模型引入信号令牌预测预训练策略，以及高效的任务特定提示学习。</li>
<li>实验结果显示RmGPT在诊断任务中具有极高的准确性，预测任务中的误差很低。</li>
<li>在少样本学习场景下，RmGPT表现尤为出色，实现了高准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d3b50972519e1bd12449ac5f7102c5e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b97722b2578595b3c378508be055dee6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ddaf09862b86e16bdde87e1dfd6dad83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4347c831bee347f5ddbd9dbf0f8e96a7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3ffd82c8157d7e5d12b1568ce0e21a4d.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-14  Generalist Models in Medical Image Segmentation A Survey and   Performance Comparison with Task-Specific Approaches
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae0aae73c23b336f88844796f445819e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-14  AutoMind Adaptive Knowledgeable Agent for Automated Data Science
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
