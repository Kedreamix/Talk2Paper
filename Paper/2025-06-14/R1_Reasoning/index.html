<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  MMMG A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5a051708ed9f49f286318281ca651ba6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-14-æ›´æ–°"><a href="#2025-06-14-æ›´æ–°" class="headerlink" title="2025-06-14 æ›´æ–°"></a>2025-06-14 æ›´æ–°</h1><h2 id="MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning"><a href="#MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning" class="headerlink" title="MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning"></a>MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning</h2><p><strong>Authors:Yuxuan Luo, Yuhui Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian</strong></p>
<p>In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learningâ€“a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target imageâ€™s core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficitsâ€“low entity fidelity, weak relations, and clutterâ€“with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmarkâ€™s difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çŸ¥è¯†å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¹¶ä¼´éšç€å¤§è§„æ¨¡å¤šå­¦ç§‘å¤šçº§çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰ï¼Œä»¥æ¢æµ‹å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œäººç±»å­¦ä¹ æœºåˆ¶ä¸­ä¸€ç›´å æ®æ ¸å¿ƒåœ°ä½ï¼Œè¿™ä¸€äº‹å®ç”±åŒé‡ç¼–ç ç†è®ºå’Œå›¾åƒä¼˜åŠ¿æ•ˆåº”æ‰€å¼ºè°ƒã€‚ç”Ÿæˆè¿™æ ·çš„å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå®ƒè¦æ±‚å¤šæ¨¡æ€æ¨ç†ï¼Œå°†ä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§åŸºç¡€ç›¸ç»“åˆï¼Œç”Ÿæˆæ¸…æ™°è§£é‡Šæ€§çš„è§†è§‰å†…å®¹ã€‚ä¸ºäº†è¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒMMMGæä¾›äº†4456ä¸ªä¸“å®¶éªŒè¯çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œæ¶µç›–10ä¸ªå­¦ç§‘ï¼Œ6ä¸ªæ•™è‚²æ°´å¹³ï¼Œä»¥åŠå¤šæ ·åŒ–çš„çŸ¥è¯†å½¢å¼ï¼Œå¦‚å›¾è¡¨ã€å›¾è§£å’Œæ€ç»´å¯¼å›¾ã€‚ä¸ºäº†æ¶ˆé™¤è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ··æ·†å¤æ‚æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºæ³•ã€‚æ¯ä¸ªçŸ¥è¯†å›¾è°±éƒ½æ˜ç¡®ç•Œå®šäº†ç›®æ ‡å›¾åƒçš„æ ¸å¿ƒå®ä½“åŠå…¶ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å¼•å…¥äº†MMMGè¯„åˆ†æ¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒã€‚è¯¥æŒ‡æ ‡ç»“åˆäº†äº‹å®ä¿çœŸåº¦ï¼ˆé€šè¿‡çŸ¥è¯†å›¾è°±ä¹‹é—´çš„å›¾ç¼–è¾‘è·ç¦»æ¥è¡¡é‡ï¼‰å’Œè§†è§‰æ¸…æ™°åº¦è¯„ä¼°ã€‚å¯¹16ä¸ªæœ€å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°æ­ç¤ºäº†ä¸¥é‡çš„æ¨ç†ç¼ºé™·â€”â€”å®ä½“ä¿çœŸåº¦ä½ã€å…³ç³»è–„å¼±å’Œæ··ä¹±â€”â€”GPT-4oçš„MMMGè¯„åˆ†ä»…ä¸º50.20ï¼Œå‡¸æ˜¾äº†åŸºå‡†æµ‹è¯•çš„å›°éš¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†FLUX-Reasonï¼ˆMMMGè¯„åˆ†ä¸º34.45ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆä¸”å¼€æ”¾çš„åŸºçº¿ï¼Œå®ƒç»“åˆäº†æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨16000ä¸ªç²¾é€‰çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10963v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>çŸ¥è¯†å›¾åƒç”Ÿæˆè¢«æå‡ºä¸ºä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œå¹¶ä»‹ç»äº†å¤§è§„æ¨¡è·¨é¢†åŸŸã€åˆ†å±‚çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰æ¥æµ‹è¯•å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œè®¤çŸ¥æœºåˆ¶ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œç”Ÿæˆè¿™æ ·çš„å›¾åƒéœ€è¦èåˆå¤šå­¦ç§‘çŸ¥è¯†ï¼Œå¹¶ä»¥åƒç´ çº§çš„æ¸…æ™°åº¦è¿›è¡Œå¯è§†åŒ–æ¨ç†ã€‚MMMGæä¾›äº†åŒ…å«åå¤§å­¦ç§‘é¢†åŸŸã€å…­ä¸ªæ•™è‚²å±‚æ¬¡å’Œå¤šç§çŸ¥è¯†å½¢å¼çš„ä¸“å®¶éªŒè¯çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œå¹¶é‡‡ç”¨äº†ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºæ¥å…¨é¢è¯„ä¼°ç”Ÿæˆçš„å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†MMMGè¯„åˆ†æ¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒï¼Œè¯¥è¯„åˆ†ç»“åˆäº†äº‹å®å‡†ç¡®æ€§ä¸è§†è§‰æ¸…æ™°åº¦çš„è¯„ä¼°ã€‚å¯¹åå…­ç§å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç»¼åˆè¯„ä¼°è¡¨æ˜å­˜åœ¨ä¸¥é‡çš„æ¨ç†ç¼ºé™·ï¼ŒåŒ…æ‹¬å®ä½“ä¿çœŸåº¦ä½ã€å…³ç³»å¼±å’Œæ··ä¹±ç­‰ã€‚ä¸ºé¼“åŠ±è¿›ä¸€æ­¥å‘å±•ï¼Œå‘å¸ƒäº†ä¸€ä¸ªæœ‰æ•ˆçš„å¼€æ”¾åŸºçº¿FLUX-Reasonï¼Œå®ƒåœ¨MMMGè¯„åˆ†ä¸Šä¸º34.45ï¼Œç»“åˆäº†æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨1ä¸‡å…­åƒä¸ªç²¾é€‰çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†å›¾åƒç”Ÿæˆè¢«æå‡ºä¸ºä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œæ—¨åœ¨æµ‹è¯•å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>çŸ¥è¯†å›¾åƒåœ¨äººç±»å­¦ä¹ å’Œè®¤çŸ¥æœºåˆ¶ä¸­å æ®æ ¸å¿ƒåœ°ä½ã€‚</li>
<li>MMMGåŸºå‡†æµ‹è¯•æä¾›äº†è·¨å­¦ç§‘ã€åˆ†å±‚çš„å›¾åƒæç¤ºå¯¹ï¼Œä»¥è¯„ä¼°çŸ¥è¯†å›¾åƒçš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>å¼•å…¥MMMGè¯„åˆ†ç³»ç»Ÿæ¥ç»¼åˆè¯„ä¼°çŸ¥è¯†å›¾åƒçš„å‡†ç¡®æ€§å’Œè§†è§‰æ¸…æ™°åº¦ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨çŸ¥è¯†å›¾åƒç”Ÿæˆæ–¹é¢å­˜åœ¨æ¨ç†ç¼ºé™·ã€‚</li>
<li>GPT-4åœ¨å¤„ç†çŸ¥è¯†å›¾åƒç”Ÿæˆä»»åŠ¡æ—¶ä»…è·å¾—MMMGè¯„åˆ†50.20ï¼Œè¡¨æ˜è¯¥ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa6208a0529fb712dc324be70c96a457.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-759b43668dcc73b54c3573d07f8cd979.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61f5cc35af2995dc9d941893ca19df72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45a377a146a57f5f4f3673b420023513.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6417f68225408b87a977c020f398d7f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f2970b49a2b8102ee26dc5923c90338.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ReCUT-Balancing-Reasoning-Length-and-Accuracy-in-LLMs-via-Stepwise-Trails-and-Preference-Optimization"><a href="#ReCUT-Balancing-Reasoning-Length-and-Accuracy-in-LLMs-via-Stepwise-Trails-and-Preference-Optimization" class="headerlink" title="ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise   Trails and Preference Optimization"></a>ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise   Trails and Preference Optimization</h2><p><strong>Authors:Zhensheng Jin, Xinze Li, Yifan Ji, Chunyi Peng, Zhenghao Liu, Qi Shi, Yukun Yan, Shuo Wang, Furong Peng, Ge Yu</strong></p>
<p>Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of Large Language Models (LLMs). However, these methods often suffer from overthinking, leading to unnecessarily lengthy or redundant reasoning traces. Existing approaches attempt to mitigate this issue through curating multiple reasoning chains for training LLMs, but their effectiveness is often constrained by the quality of the generated data and prone to overfitting. To address the challenge, we propose Reasoning Compression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing the accuracy and length of reasoning trajectory. Specifically, ReCUT employs a stepwise exploration mechanism and a long-short switched sampling strategy, enabling LLMs to incrementally generate diverse reasoning paths. These paths are evaluated and used to construct preference pairs to train two specialized models (Gemini LLMs)-one optimized for reasoning accuracy, the other for shorter reasoning. A final integrated model is obtained by interpolating the parameters of these two models. Experimental results across multiple math reasoning datasets and backbone models demonstrate that ReCUT significantly reduces reasoning lengths by approximately 30-50%, while maintaining or improving reasoning accuracy compared to various baselines. All codes and data will be released via <a target="_blank" rel="noopener" href="https://github.com/NEUIR/ReCUT">https://github.com/NEUIR/ReCUT</a>. </p>
<blockquote>
<p>è¿‘æœŸChain-of-Thoughtï¼ˆCoTï¼‰æç¤ºæ–¹æ³•çš„è¿›æ­¥æå¤§åœ°æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸è¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´æ¨ç†ç—•è¿¹è¿‡äºå†—é•¿æˆ–å†—ä½™ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡ä¸ºè®­ç»ƒLLMç­›é€‰å¤šæ¡æ¨ç†é“¾æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å…¶æœ‰æ•ˆæ€§å¾€å¾€å—åˆ°ç”Ÿæˆæ•°æ®è´¨é‡çš„å½±å“ï¼Œå¹¶å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡é€æ­¥è¯•éªŒè¿›è¡Œæ¨ç†å‹ç¼©ï¼ˆReCUTï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡æ¨ç†è½¨è¿¹çš„å‡†ç¡®æ€§å’Œé•¿åº¦ã€‚å…·ä½“æ¥è¯´ï¼ŒReCUTé‡‡ç”¨é€æ­¥æ¢ç´¢æœºåˆ¶å’Œé•¿çŸ­åˆ‡æ¢é‡‡æ ·ç­–ç•¥ï¼Œä½¿LLMèƒ½å¤Ÿé€æ­¥ç”Ÿæˆå¤šæ ·çš„æ¨ç†è·¯å¾„ã€‚è¿™äº›è·¯å¾„ç»è¿‡è¯„ä¼°å¹¶ç”¨äºæ„å»ºåå¥½å¯¹ï¼Œä»¥è®­ç»ƒä¸¤ä¸ªä¸“ä¸šæ¨¡å‹ï¼ˆåŒå­æ˜ŸLLMï¼‰â€”â€”ä¸€ä¸ªä¼˜åŒ–æ¨ç†å‡†ç¡®æ€§ï¼Œå¦ä¸€ä¸ªä¼˜åŒ–è¾ƒçŸ­æ¨ç†ã€‚æœ€ç»ˆçš„ç»¼åˆæ¨¡å‹æ˜¯é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°æ’å€¼è·å¾—çš„ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†å’Œéª¨å¹²æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒReCUTåœ¨ä¿æŒæˆ–æé«˜æ¨ç†å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå°†æ¨ç†é•¿åº¦å‡å°‘äº†å¤§çº¦30-50%ï¼Œä¸å„ç§åŸºçº¿ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å°†é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/NEUIR/ReCUT%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NEUIR/ReCUTå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10822v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸChain-of-Thoughtï¼ˆCoTï¼‰æç¤ºæŠ€æœ¯çš„è¿›å±•å¤§å¹…æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸é™·å…¥è¿‡åº¦æ€è€ƒï¼Œç”Ÿæˆè¿‡é•¿çš„æ¨ç†è½¨è¿¹ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡ä¸ºè®­ç»ƒLLMç²¾é€‰å¤šæ¡æ¨ç†é“¾æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å…¶æ•ˆæœå—é™äºç”Ÿæˆæ•°æ®çš„è´¨é‡å¹¶æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºReasoning Compression Through Stepwise Trialsï¼ˆReCUTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡æ¨ç†è½¨è¿¹çš„å‡†ç¡®æ€§ä¸é•¿åº¦ã€‚ReCUTé‡‡ç”¨é€æ­¥æ¢ç´¢æœºåˆ¶å’Œé•¿çŸ­é‡‡æ ·ç­–ç•¥ï¼Œä½¿LLMé€æ­¥ç”Ÿæˆå¤šæ ·çš„æ¨ç†è·¯å¾„ã€‚è¯„ä¼°è¿™äº›è·¯å¾„å¹¶æ„å»ºåå¥½å¯¹ï¼Œç”¨äºè®­ç»ƒä¸¤ä¸ªä¸“ä¸šæ¨¡å‹ï¼ˆGemini LLMsï¼‰ï¼Œä¸€ä¸ªä¼˜åŒ–æ¨ç†å‡†ç¡®æ€§ï¼Œå¦ä¸€ä¸ªä¼˜åŒ–ç¼©çŸ­æ¨ç†ã€‚æœ€ç»ˆé›†æˆæ¨¡å‹æ˜¯é€šè¿‡è¿™ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°æ’å€¼è·å¾—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReCUTåœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†å’ŒèƒŒæ™¯æ¨¡å‹ä¸Šæ˜¾è‘—å‡å°‘äº†çº¦30-50%çš„æ¨ç†é•¿åº¦ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†ä¸å„ç§åŸºå‡†çº¿çš„æ¨ç†å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-Thought (CoT) prompting has improved the reasoning capabilities of Large Language Models (LLMs).</li>
<li>CoT methods often lead to unnecessarily lengthy or redundant reasoning traces, a new challenge.</li>
<li>Existing approaches attempt to solve this through curating multiple reasoning chains, but their effectiveness is limited.</li>
<li>ReCUT method aims to balance accuracy and length of reasoning trajectory.</li>
<li>ReCUT employs a stepwise exploration mechanism and a long-short switched sampling strategy.</li>
<li>ReCUT trains two specialized models (Gemini LLMs) with different optimization goals.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c522453acc7ab004eeedc311acfc631.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c323dbaa6529cf7c45ad66be4f0d355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bf5c17cd7c59070bb31ee35cd582d69.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Named-Entity-Transcription-with-Contextual-LLM-based-Revision"><a href="#Improving-Named-Entity-Transcription-with-Contextual-LLM-based-Revision" class="headerlink" title="Improving Named Entity Transcription with Contextual LLM-based Revision"></a>Improving Named Entity Transcription with Contextual LLM-based Revision</h2><p><strong>Authors:Viet Anh Trinh, Xinlu He, Jacob Whitehill</strong></p>
<p>With recent advances in modeling and the increasing amount of supervised training data, automatic speech recognition (ASR) systems have achieved remarkable performance on general speech. However, the word error rate (WER) of state-of-the-art ASR remains high for named entities. Since named entities are often the most critical keywords, misrecognizing them can affect all downstream applications, especially when the ASR system functions as the front end of a complex system. In this paper, we introduce a large language model (LLM) revision mechanism to revise incorrect named entities in ASR predictions by leveraging the LLMâ€™s reasoning ability as well as local context (e.g., lecture notes) containing a set of correct named entities. Finally, we introduce the NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses for development and testing. On this dataset, our proposed technique achieves up to 30% relative WER reduction for named entities. </p>
<blockquote>
<p>éšç€å»ºæ¨¡çš„æœ€æ–°è¿›å±•å’Œå¤§é‡ç›‘ç£è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é€šç”¨è¯­éŸ³ä¸Šçš„è¡¨ç°å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚ç„¶è€Œï¼Œå¯¹äºå‘½åå®ä½“ï¼Œæœ€æ–°å‰æ²¿çš„ASRæŠ€æœ¯çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä»ç„¶å¾ˆé«˜ã€‚ç”±äºå‘½åå®ä½“é€šå¸¸æ˜¯è‡³å…³é‡è¦çš„å…³é”®è¯ï¼Œè¯¯è¯†åˆ«å®ƒä»¬ä¼šå½±å“æ‰€æœ‰ä¸‹æ¸¸åº”ç”¨ï¼Œå°¤å…¶æ˜¯å½“ASRç³»ç»Ÿä½œä¸ºå¤æ‚ç³»ç»Ÿçš„å‰ç«¯æ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¿®æ­£æœºåˆ¶ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›å’ŒåŒ…å«ä¸€ç»„æ­£ç¡®å‘½åå®ä½“çš„å±€éƒ¨ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚è¯¾å ‚ç¬”è®°ï¼‰æ¥ä¿®æ­£ASRé¢„æµ‹ä¸­çš„é”™è¯¯å‘½åå®ä½“ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†NER-MIT-OpenCourseWareæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªMITè¯¾ç¨‹çš„45å°æ—¶æ•°æ®ï¼Œç”¨äºå¼€å‘å’Œæµ‹è¯•ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬æå‡ºçš„æŠ€æœ¯å®ç°äº†é«˜è¾¾30%çš„å‘½åå®ä½“ç›¸å¯¹WERé™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10779v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å»ºæ¨¡æŠ€æœ¯çš„è¿›æ­¥å’Œå¤§é‡ç›‘ç£è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é€šç”¨è¯­éŸ³ä¸Šçš„è¡¨ç°å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚ç„¶è€Œï¼Œå¯¹äºå‘½åå®ä½“ï¼Œæœ€å…ˆè¿›çš„ASRçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä»ç„¶å¾ˆé«˜ã€‚ç”±äºå‘½åå®ä½“é€šå¸¸æ˜¯å…³é”®çš„å…³é”®è¯ï¼Œè¯¯è¯†åˆ«å®ƒä»¬ä¼šå½±å“æ‰€æœ‰ä¸‹æ¸¸åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å½“ASRç³»ç»Ÿä½œä¸ºå¤æ‚ç³»ç»Ÿçš„å‰ç«¯æ—¶ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¿®æ­£æœºåˆ¶ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›å’ŒåŒ…å«æ­£ç¡®å‘½åå®ä½“çš„å±€éƒ¨ä¸Šä¸‹æ–‡ï¼ˆå¦‚è®²ä¹‰ç¬”è®°ï¼‰æ¥ä¿®æ­£ASRé¢„æµ‹ä¸­çš„é”™è¯¯å‘½åå®ä½“ã€‚åŒæ—¶ï¼Œè¿˜ä»‹ç»äº†NER-MIT-OpenCourseWareæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªMITè¯¾ç¨‹çš„45å°æ—¶æ•°æ®ï¼Œç”¨äºå¼€å‘å’Œæµ‹è¯•ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šï¼Œæ‰€æå‡ºçš„æŠ€æœ¯å®ç°äº†é«˜è¾¾30%çš„å‘½åå®ä½“ç›¸å¯¹WERé™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿåœ¨é€šç”¨è¯­éŸ³ä¸Šçš„è¡¨ç°å·²ç»æ˜¾è‘—æé«˜ï¼Œä½†åœ¨è¯†åˆ«å‘½åå®ä½“æ–¹é¢ä»å­˜åœ¨é«˜å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>å‘½åå®ä½“çš„è¯¯è¯†åˆ«å¯¹ä¸‹æ¸¸åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å½“ASRä½œä¸ºå¤æ‚ç³»ç»Ÿå‰ç«¯æ—¶ï¼Œå…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¿®æ­£æœºåˆ¶ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›å’Œå±€éƒ¨ä¸Šä¸‹æ–‡æ¥ä¿®æ­£ASRä¸­çš„å‘½åå®ä½“é”™è¯¯ã€‚</li>
<li>å±€éƒ¨ä¸Šä¸‹æ–‡ï¼ˆå¦‚è®²ä¹‰ç¬”è®°ï¼‰åŒ…å«æ­£ç¡®çš„å‘½åå®ä½“ï¼Œå¯å¸®åŠ©æé«˜ä¿®æ­£çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä»‹ç»äº†NER-MIT-OpenCourseWareæ•°æ®é›†ï¼ŒåŒ…å«45å°æ—¶çš„æ•°æ®ï¼Œç”¨äºASRç³»ç»Ÿå’Œå‘½åå®ä½“è¯†åˆ«çš„å¼€å‘å’Œæµ‹è¯•ã€‚</li>
<li>æ‰€æå‡ºçš„æŠ€æœ¯åœ¨NER-MIT-OpenCourseWareæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œé™ä½äº†é«˜è¾¾30%çš„å‘½åå®ä½“WERã€‚</li>
<li>è¿™é¡¹æŠ€æœ¯å¯¹äºæé«˜ASRç³»ç»Ÿçš„æ€§èƒ½å’Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å‘½åå®ä½“æ—¶ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89cd513bdee985a9c64e60c9c08b47a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d474963223da40d213a0026a91dd8759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c038b9bdaaeb35c007c951758b807f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a845b67223f4cadfaa2e578535aed83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea960a7d92bd3402df748f3a8e828371.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Think-before-You-Simulate-Symbolic-Reasoning-to-Orchestrate-Neural-Computation-for-Counterfactual-Question-Answering"><a href="#Think-before-You-Simulate-Symbolic-Reasoning-to-Orchestrate-Neural-Computation-for-Counterfactual-Question-Answering" class="headerlink" title="Think before You Simulate: Symbolic Reasoning to Orchestrate Neural   Computation for Counterfactual Question Answering"></a>Think before You Simulate: Symbolic Reasoning to Orchestrate Neural   Computation for Counterfactual Question Answering</h2><p><strong>Authors:Adam Ishay, Zhun Yang, Joohyung Lee, Ilgu Kang, Dongjae Lim</strong></p>
<p>Causal and temporal reasoning about video dynamics is a challenging problem. While neuro-symbolic models that combine symbolic reasoning with neural-based perception and prediction have shown promise, they exhibit limitations, especially in answering counterfactual questions. This paper introduces a method to enhance a neuro-symbolic model for counterfactual reasoning, leveraging symbolic reasoning about causal relations among events. We define the notion of a causal graph to represent such relations and use Answer Set Programming (ASP), a declarative logic programming method, to find how to coordinate perception and simulation modules. We validate the effectiveness of our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves state-of-the-art performance on the CLEVRER challenge, significantly outperforming existing models. In the case of the CRAFT benchmark, we leverage a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a dynamics simulator. Our findings show that this method can further improve its performance on counterfactual questions by providing alternative prompts instructed by symbolic causal reasoning. </p>
<blockquote>
<p>å…³äºè§†é¢‘åŠ¨æ€å› æœå’Œæ—¶é—´æ¨ç†æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚è™½ç„¶ç»“åˆç¬¦å·æ¨ç†ã€ç¥ç»æ„ŸçŸ¥å’Œé¢„æµ‹çš„ç¥ç»ç¬¦å·æ¨¡å‹æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†åœ¨å›ç­”åäº‹å®é—®é¢˜æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¢å¼ºç¥ç»ç¬¦å·æ¨¡å‹è¿›è¡Œåäº‹å®æ¨ç†çš„æ–¹æ³•ï¼Œåˆ©ç”¨å…³äºäº‹ä»¶ä¹‹é—´å› æœå…³ç³»çš„ç¬¦å·æ¨ç†ã€‚æˆ‘ä»¬å®šä¹‰äº†å› æœå›¾çš„æ¦‚å¿µæ¥è¡¨ç¤ºè¿™ç§å…³ç³»ï¼Œå¹¶ä½¿ç”¨å£°æ˜å¼é€»è¾‘ç¼–ç¨‹æ–¹æ³•ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰æ¥å¯»æ‰¾å¦‚ä½•åè°ƒæ„ŸçŸ¥å’Œæ¨¡æ‹Ÿæ¨¡å—ã€‚æˆ‘ä»¬åœ¨CLEVRERå’ŒCRAFTä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å¢å¼ºæ–¹æ³•åœ¨CLEVRERæŒ‘æˆ˜ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚åœ¨CRAFTåŸºå‡†æµ‹è¯•çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-3.5å’ŒGPT-4ï¼‰ä½œä¸ºåŠ¨æ€æ¨¡æ‹Ÿå™¨çš„ä»£ç†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥é€šè¿‡ç¬¦å·å› æœæ¨ç†æä¾›çš„æ›¿ä»£æç¤ºè¿›ä¸€æ­¥æ”¹è¿›å…¶åœ¨åäº‹å®é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10753v1">PDF</a> In Proceedings the IEEE&#x2F;CVF Winter Conference on Applications of   Computer Vision (WACV 2024)</p>
<p><strong>Summary</strong></p>
<p>æ–‡ä¸­ä»‹ç»äº†ä¸€ç§å¢å¼ºç¥ç»ç¬¦å·æ¨¡å‹è¿›è¡Œåäº‹å®æ¨ç†çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å› æœå›¾æ¥è¡¨å¾äº‹ä»¶é—´çš„å› æœå…³ç³»ï¼Œå¹¶ä½¿ç”¨ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰æ¥åè°ƒæ„ŸçŸ¥å’Œæ¨¡æ‹Ÿæ¨¡å—ã€‚åœ¨CLEVRERå’ŒCRAFTä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚åœ¨è§£å†³åäº‹å®é—®é¢˜æ—¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¦‚GPT-3.5å’ŒGPT-4ä½œä¸ºåŠ¨æ€æ¨¡æ‹Ÿå™¨çš„ä»£ç†ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡è§£å†³çš„æ˜¯è§†é¢‘åŠ¨æ€ä¸­çš„å› æœå’Œæ—¶é—´æ¨ç†é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
<li>ç¥ç»ç¬¦å·æ¨¡å‹ç»“åˆäº†ç¬¦å·æ¨ç†å’ŒåŸºäºç¥ç»çš„æ„ŸçŸ¥å’Œé¢„æµ‹ï¼Œä½†å®ƒä»¬åœ¨å›ç­”åäº‹å®é—®é¢˜æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†ä¸€ç§æ–¹æ³•æ¥å¢å¼ºç¥ç»ç¬¦å·æ¨¡å‹è¿›è¡Œåäº‹å®æ¨ç†ï¼Œåˆ©ç”¨å…³äºäº‹ä»¶é—´å› æœå…³ç³»çš„ç¬¦å·æ¨ç†ã€‚</li>
<li>è®ºæ–‡å®šä¹‰äº†å› æœå›¾çš„æ¦‚å¿µæ¥è¡¨å¾äº‹ä»¶é—´çš„å› æœå…³ç³»ã€‚</li>
<li>ä½¿ç”¨äº†ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰æ¥åè°ƒæ„ŸçŸ¥å’Œæ¨¡æ‹Ÿæ¨¡å—çš„å·¥ä½œæ–¹å¼ã€‚</li>
<li>è®ºæ–‡åœ¨CLEVRERå’ŒCRAFTåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ‰€æå‡ºçš„æ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨CLEVRERæŒ‘æˆ˜ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a0164ce926a00845aea3114b1118b1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76cc120218475d51dfc7beb82456ae29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9726947fa8e0e634a9e9670005b5eaca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-027c37f44b9dedf32c2f705796d71050.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-328c5b56fb0a03549f9184299e292a78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1af4fd8fdf543b73235d80092d76a28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80a47aa03c6d4eb8f92311b33c49b8ac.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TeleMath-A-Benchmark-for-Large-Language-Models-in-Telecom-Mathematical-Problem-Solving"><a href="#TeleMath-A-Benchmark-for-Large-Language-Models-in-Telecom-Mathematical-Problem-Solving" class="headerlink" title="TeleMath: A Benchmark for Large Language Models in Telecom Mathematical   Problem Solving"></a>TeleMath: A Benchmark for Large Language Models in Telecom Mathematical   Problem Solving</h2><p><strong>Authors:Vincenzo Colle, Mohamed Sana, Nicola Piovesan, Antonio De Domenico, Fadhel Ayed, Merouane Debbah</strong></p>
<p>The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨ç”µä¿¡é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œäººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³ç‰¹å®šé¢†åŸŸçš„æ•°å­¦å¯†é›†å‹ä»»åŠ¡çš„èƒ½åŠ›äº§ç”Ÿäº†å…´è¶£ã€‚å°½ç®¡æœ€è¿‘çš„è¿›å±•å·²ç»æé«˜äº†LLMåœ¨ä¸€èˆ¬æ•°å­¦æ¨ç†ä¸­çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ä¸“ä¸šé¢†åŸŸå†…çš„æœ‰æ•ˆæ€§ï¼Œå¦‚ä¿¡å·å¤„ç†ã€ç½‘ç»œä¼˜åŒ–å’Œæ€§èƒ½åˆ†æï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†TeleMathï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºè¯„ä¼°LLMè§£å†³ç”µä¿¡é¢†åŸŸå…·æœ‰æ•°å€¼è§£å†³æ–¹æ¡ˆçš„æ•°å­¦é—®é¢˜æ€§èƒ½è€Œè®¾è®¡çš„ç¬¬ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ã€‚TeleMathåŒ…å«äº†500ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–äº†ç”µä¿¡é¢†åŸŸçš„å¹¿æ³›ä¸»é¢˜ã€‚æœ¬æ–‡æ¦‚è¿°äº†æ‰€æè®®çš„é—®ç­”å¯¹ç”Ÿæˆç®¡é“ï¼Œä»ç”±ä¸»é¢˜ä¸“å®¶ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ç§å­å¼€å§‹ã€‚å¯¹ä¸€ç³»åˆ—å¼€æºLLMçš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨TeleMathä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹æ˜¯ä¸“ä¸ºæ•°å­¦æˆ–é€»è¾‘æ¨ç†è®¾è®¡çš„æœ€æ–°æ¨¡å‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå³ä½¿æ˜¯å‚æ•°ä¼—å¤šçš„é€šç”¨æ¨¡å‹ä¹Ÿç»å¸¸é¢ä¸´è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†æ•°æ®é›†å’Œè¯„ä¼°ä»£ç ï¼Œä»¥ä¾¿è½»æ¾å†ç°ç»“æœå¹¶æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10674v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½åœ¨ç”µä¿¡é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å¼•å‘äº†äººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³ç‰¹å®šé¢†åŸŸæ•°å­¦å¯†é›†å‹ä»»åŠ¡çš„èƒ½åŠ›çš„å…³æ³¨ã€‚å°½ç®¡æœ€è¿‘çš„è¿›å±•æé«˜äº†LLMsåœ¨ä¸€èˆ¬æ•°å­¦æ¨ç†ä¸­çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ä¿¡å·å¤„ç†å’Œç”µä¿¡ç½‘ç»œä¼˜åŒ–ç­‰ç‰¹å®šé¢†åŸŸçš„æ•ˆæœå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†TeleMathæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMè§£å†³ç”µä¿¡é¢†åŸŸæ•°å€¼è§£å†³æ–¹æ¡ˆçš„æ•°å­¦é—®é¢˜çš„æ€§èƒ½çš„é¦–ä¸ªåŸºå‡†æ•°æ®é›†ã€‚åŒ…å«äº”ç™¾é“é—®ç­”é¢˜å¯¹ï¼ŒTeleMathæ¶µç›–äº†ç”µä¿¡é¢†åŸŸçš„å¹¿æ³›ä¸»é¢˜ã€‚æœ¬æ–‡æ¦‚è¿°äº†é—®ç­”é¢˜ç”Ÿæˆç®¡é“çš„å»ºè®®ï¼Œä»ä¸»é¢˜ä¸“å®¶ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜å¼€å§‹ã€‚å¯¹ä¸€ç³»åˆ—å¼€æºLLMsçš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨TeleMathä¸Šè¡¨ç°æœ€å¥½çš„æ˜¯ä¸“ä¸ºæ•°å­¦æˆ–é€»è¾‘æ¨ç†è®¾è®¡çš„æœ€æ–°æ¨¡å‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šç”¨æ¨¡å‹å³ä½¿å‚æ•°ä¼—å¤šï¼Œä¹Ÿå¾€å¾€éš¾ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†æ•°æ®é›†å’Œè¯„ä¼°ä»£ç ï¼Œä»¥ä¾¿è½»æ¾é‡ç°ç»“æœå¹¶æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”µä¿¡é¢†åŸŸè§£å†³æ•°å­¦å¯†é›†å‹ä»»åŠ¡çš„èƒ½åŠ›é€æ¸å—åˆ°å…³æ³¨ã€‚</li>
<li>TeleMathæ•°æ®é›†çš„æ¨å‡ºæ˜¯ä¸ºäº†è¯„ä¼°LLMåœ¨è§£å†³ç”µä¿¡é¢†åŸŸæ•°å­¦é—®é¢˜çš„æ€§èƒ½ã€‚</li>
<li>TeleMathåŒ…å«äº”ç™¾é“é—®ç­”é¢˜å¯¹ï¼Œè¦†ç›–äº†ç”µä¿¡é¢†åŸŸçš„å¹¿æ³›ä¸»é¢˜ã€‚</li>
<li>LLMsåœ¨ä¸€èˆ¬æ•°å­¦æ¨ç†æ–¹é¢çš„è¿›å±•å·²ç»æé«˜äº†æ€§èƒ½ã€‚</li>
<li>ä¸“é—¨è®¾è®¡ç”¨äºæ•°å­¦æˆ–é€»è¾‘æ¨ç†çš„æœ€æ–°æ¨¡å‹åœ¨TeleMathä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ç‰¹å®šé¢†åŸŸçš„æ•°å­¦é—®é¢˜æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b4e5a3789c0dadc5a589e74a7eb84a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-214b6d043106e98ff1d69e4444ac1b0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8464ce31a0b58a6c4a445f1e425cb1bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b48ecef1d57dcc2f712ce047dd110351.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SoK-Evaluating-Jailbreak-Guardrails-for-Large-Language-Models"><a href="#SoK-Evaluating-Jailbreak-Guardrails-for-Large-Language-Models" class="headerlink" title="SoK: Evaluating Jailbreak Guardrails for Large Language Models"></a>SoK: Evaluating Jailbreak Guardrails for Large Language Models</h2><p><strong>Authors:Xunguang Wang, Zhenlan Ji, Wenxuan Wang, Zongjie Li, Daoyuan Wu, Shuai Wang</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety mechanisms. Guardrailsâ€“external defense mechanisms that monitor and control LLM interactionâ€“have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, explore their universality across attack types, and provide insights into optimizing defense combinations. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xunguangwang/SoK4JailbreakGuardrails">https://github.com/xunguangwang/SoK4JailbreakGuardrails</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å…¶éƒ¨ç½²è¿‡ç¨‹ä¸­æš´éœ²å‡ºé‡å¤§æ¼æ´ï¼Œç‰¹åˆ«æ˜¯å®¹æ˜“å—åˆ°ç»•è¿‡å®‰å…¨æœºåˆ¶çš„è¶Šç‹±æ”»å‡»ã€‚ä½œä¸ºå¤–éƒ¨é˜²å¾¡æœºåˆ¶ï¼Œç›‘æ§å’Œæ§åˆ¶LLMäº¤äº’çš„æŠ¤æ å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç›®å‰LLMæŠ¤æ çš„æ™¯è§‚å‘ˆç°ç¢ç‰‡åŒ–ï¼Œç¼ºä¹ç»Ÿä¸€çš„åˆ†ç±»å’Œå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚åœ¨è¿™ç¯‡ç³»ç»ŸåŒ–çŸ¥è¯†ï¼ˆSoKï¼‰è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹LLMçš„è¶Šç‹±æŠ¤æ è¿›è¡Œäº†é¦–æ¬¡å…¨é¢åˆ†æã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šç»´åº¦åˆ†ç±»æ³•ï¼Œæ²¿ç€å…­ä¸ªå…³é”®ç»´åº¦å¯¹æŠ¤æ è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå®‰å…¨æœ‰æ•ˆæ€§è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°å®ƒä»¬çš„å®é™…æ•ˆæœã€‚é€šè¿‡å¹¿æ³›çš„åˆ†æå’Œå®éªŒï¼Œæˆ‘ä»¬ç¡®å®šäº†ç°æœ‰æŠ¤æ æ–¹æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œæ¢ç´¢äº†å®ƒä»¬åœ¨æ”»å‡»ç±»å‹ä¸­çš„æ™®éæ€§ï¼Œå¹¶æä¾›äº†ä¼˜åŒ–é˜²å¾¡ç»„åˆçš„å»ºè®®ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†ç»“æ„åŒ–åŸºç¡€ï¼Œæ—¨åœ¨æŒ‡å¯¼ç¨³å¥çš„LLMæŠ¤æ çš„æœ‰åŸåˆ™å‘å±•å’Œéƒ¨ç½²ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xunguangwang/SoK4JailbreakGuardrails%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xunguangwang/SoK4JailbreakGuardrailsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10597v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­æš´éœ²å‡ºå¯¹è¶Šç‹±æ”»å‡»ç­‰å®‰å…¨æœºåˆ¶çš„é‡å¤§æ¼æ´ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå¤–éƒ¨é˜²å¾¡æœºåˆ¶â€”â€”æŠ¤æ ï¼ˆGuardrailsï¼‰åº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œç›®å‰LLMæŠ¤æ çš„æ™¯è§‚å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€åˆ†ç±»å’Œå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹LLMçš„è¶Šç‹±æŠ¤æ è¿›è¡Œå…¨é¢åˆ†æï¼Œæå‡ºä¸€ç§æ–°çš„å¤šç»´åº¦åˆ†ç±»æ–¹æ³•ï¼Œæ²¿ç€å…­ä¸ªå…³é”®ç»´åº¦å¯¹æŠ¤æ è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¼•å…¥å®‰å…¨-æ•ˆç‡-æ•ˆç”¨è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°å…¶å®è·µæ•ˆæœã€‚é€šè¿‡å¹¿æ³›çš„åˆ†æå’Œå®éªŒï¼Œæœ¬æ–‡ç¡®å®šäº†ç°æœ‰æŠ¤æ æ–¹æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œæ¢ç´¢äº†å®ƒä»¬åœ¨æ”»å‡»ç±»å‹ä¸­çš„é€šç”¨æ€§ï¼Œå¹¶ä¸ºä¼˜åŒ–é˜²å¾¡ç»„åˆæä¾›äº†è§è§£ã€‚æœ¬æ–‡çš„å·¥ä½œä¸ºæœªæ¥ç ”ç©¶å’Œå‘å±•æä¾›äº†ç»“æ„åŒ–åŸºç¡€ï¼Œæ—¨åœ¨æŒ‡å¯¼ç¨³å¥çš„LLMæŠ¤æ çš„åŸåˆ™æ€§å‘å±•å’Œéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²ä¸­é¢ä¸´è¶Šç‹±æ”»å‡»ç­‰å®‰å…¨æ¼æ´é—®é¢˜ã€‚</li>
<li>æŠ¤æ ï¼ˆGuardrailsï¼‰ä½œä¸ºå¤–éƒ¨é˜²å¾¡æœºåˆ¶å‡ºç°ï¼Œç”¨äºç›‘æµ‹å’Œæ§åˆ¶LLMäº¤äº’ã€‚</li>
<li>å½“å‰LLMæŠ¤æ å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€åˆ†ç±»å’Œå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>é¦–æ¬¡å¯¹LLMçš„è¶Šç‹±æŠ¤æ è¿›è¡Œå…¨é¢åˆ†æï¼Œæå‡ºæ–°çš„å¤šç»´åº¦åˆ†ç±»æ–¹æ³•ã€‚</li>
<li>å¼•å…¥å®‰å…¨-æ•ˆç‡-æ•ˆç”¨è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æŠ¤æ çš„å®è·µæ•ˆæœã€‚</li>
<li>é€šè¿‡åˆ†æå®éªŒç¡®å®šäº†ç°æœ‰æŠ¤æ æ–¹æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3de4e9abe6166c615e70e0652c1b5da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c52811c2fcb913902ba373d03a77d7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf8a93bee77546668b467678a54213e3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Primender-Sequence-A-Novel-Mathematical-Construct-for-Testing-Symbolic-Inference-and-AI-Reasoning"><a href="#Primender-Sequence-A-Novel-Mathematical-Construct-for-Testing-Symbolic-Inference-and-AI-Reasoning" class="headerlink" title="Primender Sequence: A Novel Mathematical Construct for Testing Symbolic   Inference and AI Reasoning"></a>Primender Sequence: A Novel Mathematical Construct for Testing Symbolic   Inference and AI Reasoning</h2><p><strong>Authors:Mohd Anwar Jamal Faiz</strong></p>
<p>This paper introduces the Primender sequence, a novel integer sequence defined by a hybrid rule that combines classical primality with modular digit-based conditions. Specifically, a number n is included in the sequence if it is prime or ends with a prime number of unit digit or any length. In other words, numbers which are primes or have at least one prime suffix. The resulting sequence exhibits a deterministic yet non-trivial structure, blending number-theoretic properties with symbolic patterning. We propose the Primender sequence as a benchmark for evaluating the symbolic reasoning capabilities of Large Language Models (LLMs). The study is motivated by the need for interpretable, rule-based testbeds that can assess an LLMâ€™s ability to infer hidden rules, validate mathematical hypotheses, and generalize symbolic logic at scale. A key hypothesis explored is: Whenever a number in the Primender sequence is exactly one more than the largest prime less than or equal to it, the difference between it and the previous number in the sequence is also 1. We design a structured prompt and evaluation framework to test this hypothesis across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek, Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying rule, validating the hypothesis, and generating the next 100,000 terms of the sequence. Comparative metrics such as rule inference accuracy, hypothesis evaluation, sequence validity, and symbolic explanation quality are used to assess model performance. This work contributes a novel mathematical construct and a reproducible methodology for benchmarking LLMs in symbolic reasoning, hypothesis testing, and scalable pattern generalization - bridging the domains of number theory, artificial intelligence, and software engineering. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Primenderåºåˆ—ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ•´æ•°åºåˆ—ï¼Œç”±æ··åˆè§„åˆ™å®šä¹‰ï¼Œè¯¥è§„åˆ™ç»“åˆäº†ç»å…¸è´¨æ•°æ€§ä¸åŸºäºæ¨¡æ•°çš„æ•°å­—æ¡ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœä¸€ä¸ªæ•°å­—næ˜¯è´¨æ•°æˆ–ä»¥è´¨æ•°çš„å•ä½æ•°å­—ç»“å°¾æˆ–å…·æœ‰ä»»ä½•é•¿åº¦ï¼Œåˆ™å®ƒåŒ…å«åœ¨åºåˆ—ä¸­ã€‚æ¢å¥è¯è¯´ï¼Œæ˜¯è´¨æ•°æˆ–è‡³å°‘æœ‰ä¸€ä¸ªè´¨æ•°åç¼€çš„æ•°å­—ã€‚æ‰€å¾—çš„åºåˆ—è¡¨ç°å‡ºç¡®å®šè€Œéå¹³å‡¡çš„ç»“æ„ï¼Œèåˆäº†æ•°è®ºå±æ€§ä¸ç¬¦å·æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºPrimenderåºåˆ—ä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¬¦å·æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¿™é¡¹ç ”ç©¶å—åˆ°éœ€è¦å¯è§£é‡Šã€åŸºäºè§„åˆ™çš„å®éªŒå°çš„é©±åŠ¨ï¼Œè¯¥å®éªŒå°å¯ä»¥è¯„ä¼°LLMæ¨æ–­éšè—è§„åˆ™ã€éªŒè¯æ•°å­¦å‡è®¾ä»¥åŠå¤§è§„æ¨¡æ¨å¹¿ç¬¦å·é€»è¾‘çš„èƒ½åŠ›ã€‚æ¢ç´¢çš„ä¸€ä¸ªå…³é”®å‡è®¾æ˜¯ï¼šæ¯å½“Primenderåºåˆ—ä¸­çš„æ•°å­—æ°å¥½æ¯”å°äºæˆ–ç­‰äºå®ƒçš„æœ€å¤§è´¨æ•°å¤§1æ—¶ï¼Œå®ƒä¸åºåˆ—ä¸­å‰ä¸€ä¸ªæ•°å­—çš„å·®å€¼ä¹Ÿä¸º1ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»“æ„åŒ–çš„æç¤ºå’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥æµ‹è¯•è¿™ä¸€å‡è®¾åœ¨å¤šä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ChatGPTã€Copilotã€DeepSeekã€Geminiã€Grokå’ŒLLaMAã€‚è¿™äº›æ¨¡å‹çš„ä»»åŠ¡æ˜¯è¯†åˆ«æ½œåœ¨è§„åˆ™ã€éªŒè¯å‡è®¾å¹¶ç”Ÿæˆåºåˆ—çš„ä¸‹ä¸€ä¸ª10ä¸‡ä¸ªæœ¯è¯­ã€‚ä½¿ç”¨è§„åˆ™æ¨ç†å‡†ç¡®æ€§ã€å‡è®¾è¯„ä¼°ã€åºåˆ—æœ‰æ•ˆæ€§å’Œç¬¦å·è§£é‡Šè´¨é‡ç­‰æ¯”è¾ƒæŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºç¬¦å·æ¨ç†ã€å‡è®¾æµ‹è¯•å’Œå¯æ‰©å±•æ¨¡å¼æ¦‚æ‹¬çš„LLMåŸºå‡†æµ‹è¯•æä¾›äº†æ–°çš„æ•°å­¦æ„é€ å’Œå¯å¤åˆ¶çš„æ–¹æ³•è®ºï¼Œåœ¨æ•°è®ºã€äººå·¥æ™ºèƒ½å’Œè½¯ä»¶å·¥ç¨‹é¢†åŸŸä¹‹é—´æ­å»ºäº†æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10585v1">PDF</a> 9 pages, 7 figures, 2 tables, 3 codes, oeis sequence A384735</p>
<p><strong>Summary</strong><br>Primenderåºåˆ—æ˜¯ä¸€ç§æ–°å‹æ•´æ•°åºåˆ—ï¼Œç»“åˆç»å…¸ç´ æ€§ä»¥åŠæ¨¡æ•°ä½åŸºæ•°æ¡ä»¶ä¸‹çš„æ··åˆè§„åˆ™å®šä¹‰ã€‚æ–‡ä¸­æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹å¯¹è¯¥åºåˆ—éšå«è§„åˆ™çš„æ¨ç†èƒ½åŠ›è¯„ä¼°æ–¹æ³•ï¼Œå¹¶è®¾è®¡å®éªŒæµ‹è¯•äº†å¤šä¸ªå…ˆè¿›æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥åºåˆ—å±•ç°å‡ºç¡®å®šä¸”å¤æ‚çš„ç»“æ„ï¼Œä¸ºæ•°å­¦ç†è®ºã€äººå·¥æ™ºèƒ½ä¸è½¯ä»¶å·¥ç¨‹é¢†åŸŸä¹‹é—´çš„æ¡¥æ¢æ­å»ºæä¾›äº†å¯å¤ç°çš„æ–¹æ³•è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Primenderåºåˆ—ç”±ç´ æ•°å’Œè‡³å°‘æœ‰ä¸€ä¸ªç´ æ•°åç¼€çš„æ•°å­—ç»„æˆï¼Œè¡¨ç°å‡ºä¸€ç§ç»“åˆæ•°å­—å’Œç¬¦å·æ¨¡å¼çš„æœ‰ç¡®å®šç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19cc5059a9aa4af9c5537e01d479104e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ed4a01f6d7181cca6620c1210e56712.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0b497b9d3bac2cf2a7311eb4868149d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8638a62f93945b05bdad6583347da136.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca20d2aa02d55e2fd8f5dc23c87dc9e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e51e0b589cbbe69e977b5f159a6ad0a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751644bf3248985cbf5fb9c5598e1a76.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LogiPlan-A-Structured-Benchmark-for-Logical-Planning-and-Relational-Reasoning-in-LLMs"><a href="#LogiPlan-A-Structured-Benchmark-for-Logical-Planning-and-Relational-Reasoning-in-LLMs" class="headerlink" title="LogiPlan: A Structured Benchmark for Logical Planning and Relational   Reasoning in LLMs"></a>LogiPlan: A Structured Benchmark for Logical Planning and Relational   Reasoning in LLMs</h2><p><strong>Authors:Yanan Cai, Ahmed Salem, Besmira Nushi, Mark Russinovich</strong></p>
<p>We introduce LogiPlan, a novel benchmark designed to evaluate the capabilities of large language models (LLMs) in logical planning and reasoning over complex relational structures. Logical relational reasoning is important for applications that may rely on LLMs to generate and query structured graphs of relations such as network infrastructure, knowledge bases, or business process schema. Our framework allows for dynamic variation of task complexity by controlling the number of objects, relations, and the minimum depth of relational chains, providing a fine-grained assessment of model performance across difficulty levels. LogiPlan encompasses three complementary tasks: (1) Plan Generation, where models must construct valid directed relational graphs meeting specified structural constraints; (2) Consistency Detection, testing modelsâ€™ ability to identify inconsistencies in relational structures; and (3) Comparison Question, evaluating modelsâ€™ capacity to determine the validity of queried relationships within a given graph. Additionally, we assess modelsâ€™ self-correction capabilities by prompting them to verify and refine their initial solutions. We evaluate state-of-the-art models including DeepSeek R1, Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B, O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant performance gaps that correlate with model scale and architecture. Our analysis demonstrates that while recent reasoning-enhanced models show promising results on simpler instances, they struggle with more complex configurations requiring deeper logical planning. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LogiPlanï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€»è¾‘è§„åˆ’å’Œå¤æ‚å…³ç³»ç»“æ„ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚é€»è¾‘å…³ç³»æ¨ç†å¯¹äºå¯èƒ½ä¾èµ–äºLLMç”Ÿæˆå’ŒæŸ¥è¯¢ç»“æ„åŒ–å…³ç³»å›¾çš„åº”ç”¨éå¸¸é‡è¦ï¼Œä¾‹å¦‚ç½‘ç»œåŸºç¡€è®¾æ–½ã€çŸ¥è¯†åº“æˆ–ä¸šåŠ¡æµç¨‹æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡æ§åˆ¶å¯¹è±¡æ•°é‡ã€å…³ç³»å’Œå…³ç³»é“¾çš„æœ€å°æ·±åº¦æ¥åŠ¨æ€æ”¹å˜ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œä¸ºä¸åŒéš¾åº¦çº§åˆ«æä¾›ç²¾ç»†çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°ã€‚LogiPlanåŒ…å«ä¸‰ä¸ªäº’è¡¥çš„ä»»åŠ¡ï¼šï¼ˆ1ï¼‰è®¡åˆ’ç”Ÿæˆï¼Œæ¨¡å‹å¿…é¡»æ„å»ºæ»¡è¶³ç‰¹å®šç»“æ„çº¦æŸçš„æœ‰æ•ˆæœ‰å‘å…³ç³»å›¾ï¼›ï¼ˆ2ï¼‰ä¸€è‡´æ€§æ£€æµ‹ï¼Œæµ‹è¯•æ¨¡å‹è¯†åˆ«å…³ç³»ç»“æ„ä¸­ä¸ä¸€è‡´çš„èƒ½åŠ›ï¼›ï¼ˆ3ï¼‰æ¯”è¾ƒé—®é¢˜ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ç¡®å®šç»™å®šå›¾ä¸­æŸ¥è¯¢å…³ç³»çš„æœ‰æ•ˆæ€§æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æç¤ºæ¨¡å‹éªŒè¯å’Œç²¾ç‚¼å…¶åˆå§‹è§£å†³æ–¹æ¡ˆæ¥è¯„ä¼°æ¨¡å‹çš„è‡ªæˆ‘çº æ­£èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†åŒ…æ‹¬DeepSeek R1ã€Gemini 2.0 Proã€Gemini 2 Flash Thinkingã€GPT-4.5ã€GPT-4oã€Llama 3.1 405Bã€O3-miniã€O1å’ŒClaude 3.7 Sonnetç­‰æœ€æ–°æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸æ¨¡å‹è§„æ¨¡å’Œæ¶æ„ç›¸å…³çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè™½ç„¶æœ€è¿‘çš„å¢å¼ºæ¨ç†æ¨¡å‹åœ¨ç®€å•å®ä¾‹ä¸Šè¡¨ç°å‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬åœ¨éœ€è¦æ›´æ·±åˆ»é€»è¾‘è§„åˆ’çš„å¤æ‚é…ç½®ä¸Šå´è¡¨ç°å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10527v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LogiPlanè¿™ä¸€æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘è§„åˆ’å’Œå¤æ‚å…³ç³»ç»“æ„ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚LogiPlanåŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼šè®¡åˆ’ç”Ÿæˆã€ä¸€è‡´æ€§æ£€æµ‹å’Œæ¯”è¾ƒé—®é¢˜ï¼Œé€šè¿‡æ§åˆ¶å¯¹è±¡æ•°é‡ã€å…³ç³»å’Œå…³ç³»é“¾çš„æœ€å°æ·±åº¦æ¥åŠ¨æ€è°ƒæ•´ä»»åŠ¡å¤æ‚åº¦ï¼Œä»è€Œç²¾ç»†è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒéš¾åº¦å±‚æ¬¡ä¸Šçš„æ€§èƒ½ã€‚æ–‡ç« è¿˜è¯„ä¼°äº†å‡ ç§æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ï¼Œå‘ç°æ¨¡å‹è§„æ¨¡å’Œæ¶æ„ä¸æ€§èƒ½å·®è·æœ‰æ˜¾è‘—å…³ç³»ï¼Œç®€å•å®ä¾‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚é…ç½®ä¸‹éœ€è¦æ›´æ·±å…¥çš„é€»è¾‘è§„åˆ’æ—¶åˆ™è¡¨ç°æŒ£æ‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LogiPlanæ˜¯ä¸€ç§æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘è§„åˆ’å’Œå¤æ‚å…³ç³»ç»“æ„ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼šè®¡åˆ’ç”Ÿæˆã€ä¸€è‡´æ€§æ£€æµ‹å’Œæ¯”è¾ƒé—®é¢˜ã€‚</li>
<li>é€šè¿‡æ§åˆ¶å¯¹è±¡æ•°é‡ã€å…³ç³»å’Œå…³ç³»é“¾çš„æœ€å°æ·±åº¦ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´ä»»åŠ¡å¤æ‚åº¦ã€‚</li>
<li>æ–‡ç« è¯„ä¼°äº†å‡ ç§æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬DeepSeek R1ã€GPT-4ç³»åˆ—ç­‰ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å’Œæ¶æ„ä¸æ€§èƒ½å·®è·æœ‰æ˜¾è‘—å…³ç³»ã€‚</li>
<li>åœ¨ç®€å•å®ä¾‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚æƒ…å†µä¸‹éœ€è¦æ›´æ·±å…¥é€»è¾‘è§„åˆ’æ—¶åˆ™è¡¨ç°æŒ£æ‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cdc5c748deb6709ea33491491000a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14ec488f4c876ac1ef4feda45e560259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c1cd2c45e4647d61e1403cb9d7d2e7e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning"><a href="#Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning" class="headerlink" title="Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning"></a>Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning</h2><p><strong>Authors:Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai</strong></p>
<p>Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientistsâ€™ First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries. </p>
<blockquote>
<p>ç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºåŸºäºä¿¡æ¯å¯†é›†çš„ç§‘å­¦æ•°æ®å’Œç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„å¤æ‚å¤šæ¨¡æ€æ¨ç†ã€‚å€ŸåŠ©ä¸“å®¶çº§ç§‘å­¦åŸºå‡†ï¼Œç§‘å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å®é™…å·¥ä½œæµç¨‹ä¸­å…·æœ‰å¢å¼ºè¿™ä¸€å‘ç°è¿‡ç¨‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç§‘å­¦åŸºå‡†å¤§å¤šä¾§é‡äºè¯„ä¼°MLLMçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªç›¸äº’å…³è”çš„æ°´å¹³æ¥è¯„ä¼°MLLMçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£å’Œç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒSFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯è¿‡çš„é—®ç­”å¯¹ï¼Œæ¶µç›–ä¸‰ç§é—®é¢˜ç±»å‹ï¼Œæ¶‰åŠ66ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¶µç›–äº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç›®å‰æœ€å…ˆè¿›çš„GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œè¿™è¡¨æ˜MLLMåœ¨ç§‘å­¦ç ”ç©¶é¢†åŸŸè¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚æˆ‘ä»¬å¸Œæœ›ä»SFEä¸­è·å¾—çš„æ•°æ®èƒ½ä¿ƒè¿›äººå·¥æ™ºèƒ½åœ¨ç§‘ç ”å‘ç°ä¸­çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10521v1">PDF</a> 82 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¿¡æ¯å¯†é›†å‹ç§‘å­¦æ•°æ®å’Œé¢†åŸŸç‰¹å®šä¸“ä¸šçŸ¥è¯†ï¼Œç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ã€‚ç§‘å­¦è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å€ŸåŠ©ä¸“å®¶çº§ç§‘å­¦åŸºå‡†ï¼Œå…·æœ‰å¢å¼ºç°å®å·¥ä½œæµç¨‹ä¸­çš„å‘ç°è¿‡ç¨‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç§‘å­¦åŸºå‡†ä¸»è¦ä¾§é‡äºè¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ç§‘å­¦å®¶é¦–æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªç›¸äº’è”ç³»çš„æ°´å¹³è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£ã€ç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚SFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯çš„é—®ç­”å¯¹ï¼Œæ¶‰åŠä¸‰ç§é—®é¢˜ç±»å‹ï¼Œè·¨è¶Š66ä¸ªè·¨æ¨¡æ€ä»»åŠ¡å’Œäº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚æœ€æ–°å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œè¿™è¡¨æ˜MLLMsåœ¨ç§‘å­¦é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦å‘ç°ç°åœ¨ä¾èµ–å¤æ‚çš„è·¨æ¨¡æ€æ¨ç†å’Œä¿¡æ¯å¯†é›†å‹ç§‘å­¦æ•°æ®ã€‚</li>
<li>ç§‘å­¦è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ‰æ½œåŠ›å¢å¼ºç°å®å·¥ä½œæµç¨‹ä¸­çš„å‘ç°è¿‡ç¨‹ã€‚</li>
<li>å½“å‰çš„ç§‘å­¦åŸºå‡†ä¸»è¦è¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¿½è§†äº†æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚</li>
<li>ç§‘å­¦å®¶é¦–æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†ç”¨äºè¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£å’Œç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚</li>
<li>SFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯çš„é—®ç­”å¯¹ï¼Œæ¶‰åŠå¤šç§é—®é¢˜ç±»å‹å’Œè·¨æ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>æœ€å…ˆè¿›çš„MLLMsåœ¨SFEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå…¶åœ¨ç§‘å­¦é¢†åŸŸæœ‰æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20a2c5f0519f89a3c02c3a208768152d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cbbcb207db5da5c0b8203d23b97b4db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-122fcd0cf5bc197d0dedc0a1a73382e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e346a7ed5d56d9af01b6b00ac708e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e579c00f906d25d004c5ecb1e3ff9026.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reliable-Reasoning-Path-Distilling-Effective-Guidance-for-LLM-Reasoning-with-Knowledge-Graphs"><a href="#Reliable-Reasoning-Path-Distilling-Effective-Guidance-for-LLM-Reasoning-with-Knowledge-Graphs" class="headerlink" title="Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning   with Knowledge Graphs"></a>Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning   with Knowledge Graphs</h2><p><strong>Authors:Yilin Xiao, Chuang Zhou, Qinggang Zhang, Bo Li, Qing Li, Xiao Huang</strong></p>
<p>Large language models (LLMs) often struggle with knowledge-intensive tasks due to a lack of background knowledge and a tendency to hallucinate. To address these limitations, integrating knowledge graphs (KGs) with LLMs has been intensively studied. Existing KG-enhanced LLMs focus on supplementary factual knowledge, but still struggle with solving complex questions. We argue that refining the relationships among facts and organizing them into a logically consistent reasoning path is equally important as factual knowledge itself. Despite their potential, extracting reliable reasoning paths from KGs poses the following challenges: the complexity of graph structures and the existence of multiple generated paths, making it difficult to distinguish between useful and redundant ones. To tackle these challenges, we propose the RRP framework to mine the knowledge graph, which combines the semantic strengths of LLMs with structural information obtained through relation embedding and bidirectional distribution learning. Additionally, we introduce a rethinking module that evaluates and refines reasoning paths according to their significance. Experimental results on two public datasets show that RRP achieves state-of-the-art performance compared to existing baseline methods. Moreover, RRP can be easily integrated into various LLMs to enhance their reasoning abilities in a plug-and-play manner. By generating high-quality reasoning paths tailored to specific questions, RRP distills effective guidance for LLM reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸å¸¸åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ–¹é¢è¡¨ç°æ¬ ä½³ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹èƒŒæ™¯çŸ¥è¯†å’Œå€¾å‘äºäº§ç”Ÿå¹»è§‰æ‰€å¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œå°†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸LLMé›†æˆå·²ç»å¾—åˆ°äº†æ·±å…¥ç ”ç©¶ã€‚ç°æœ‰çš„KGå¢å¼ºLLMä¸»è¦å…³æ³¨è¡¥å……äº‹å®çŸ¥è¯†ï¼Œä½†åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šä»ç„¶é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç²¾ç‚¼äº‹å®ä¹‹é—´çš„å…³ç³»å¹¶å°†å®ƒä»¬ç»„ç»‡æˆé€»è¾‘ä¸€è‡´çš„æ¨ç†è·¯å¾„ï¼Œä¸äº‹å®æœ¬èº«çš„çŸ¥è¯†åŒæ ·é‡è¦ã€‚å°½ç®¡å­˜åœ¨æ½œåŠ›ï¼Œä½†ä»KGä¸­æå–å¯é çš„æ¨ç†è·¯å¾„é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šå›¾ç»“æ„å¤æ‚æ€§ä»¥åŠå­˜åœ¨å¤šä¸ªç”Ÿæˆè·¯å¾„ï¼Œè¿™ä½¿å¾—åŒºåˆ†æœ‰ç”¨å’Œå†—ä½™è·¯å¾„å˜å¾—å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RRPæ¡†æ¶æ¥æŒ–æ˜çŸ¥è¯†å›¾è°±ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†LLMçš„è¯­ä¹‰ä¼˜åŠ¿ï¼Œå¹¶é€šè¿‡å…³ç³»åµŒå…¥å’ŒåŒå‘åˆ†å¸ƒå­¦ä¹ è·å¾—çš„ç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåæ€æ¨¡å—ï¼Œæ ¹æ®é‡è¦æ€§è¯„ä¼°å’Œç²¾ç‚¼æ¨ç†è·¯å¾„ã€‚åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒRRPè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è€Œä¸”ï¼ŒRRPå¯ä»¥è½»æ¾åœ°ä»¥å„ç§æ’ä»¶å½¢å¼é›†æˆåˆ°å„ç§LLMä¸­ï¼Œä»¥å¢å¼ºå®ƒä»¬çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡é’ˆå¯¹ç‰¹å®šé—®é¢˜ç”Ÿæˆé«˜è´¨é‡æ¨ç†è·¯å¾„ï¼ŒRRPä¸ºLLMæ¨ç†æä¾›äº†æœ‰æ•ˆçš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10508v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„ä¸è¶³ï¼Œå¦‚ç¼ºä¹èƒŒæ™¯çŸ¥è¯†å’Œæ˜“äº§ç”Ÿå¹»è§‰ï¼Œç ”ç©¶è€…æå‡ºå°†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä¸LLMsç»“åˆä»¥æ”¹è¿›å…¶æ€§èƒ½ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶å…³æ³¨è¡¥å……äº‹å®çŸ¥è¯†ï¼Œä½†åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶ä»é¢ä¸´å›°éš¾ã€‚æœ¬æ–‡ä¸»å¼ ç²¾ç‚¼äº‹å®ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å°†å…¶ç»„ç»‡æˆé€»è¾‘ä¸€è‡´çš„æ¨ç†è·¯å¾„ï¼Œè¿™åŒæ ·é‡è¦ã€‚ä»çŸ¥è¯†å›¾è°±ä¸­æå–å¯é çš„æ¨ç†è·¯å¾„é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å›¾å½¢ç»“æ„çš„å¤æ‚æ€§å’Œå¤šæ¡ç”Ÿæˆè·¯å¾„çš„å­˜åœ¨ï¼Œéš¾ä»¥åŒºåˆ†æœ‰ç”¨å’Œå†—ä½™çš„è·¯å¾„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºRRPæ¡†æ¶æ¥æŒ–æ˜çŸ¥è¯†å›¾è°±ï¼Œç»“åˆLLMsçš„è¯­ä¹‰ä¼˜åŠ¿ä¸é€šè¿‡å…³ç³»åµŒå…¥å’ŒåŒå‘åˆ†å¸ƒå­¦ä¹ è·å¾—çš„ç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå¼•å…¥åæ€æ¨¡å—æ ¹æ®é‡è¦æ€§è¯„ä¼°å’Œä¼˜åŒ–æ¨ç†è·¯å¾„ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRRPä¸ç°æœ‰åŸºçº¿æ–¹æ³•ç›¸æ¯”å…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°ä»¥æ’ä»¶å’Œæ’­æ”¾çš„æ–¹å¼é›†æˆåˆ°å„ç§LLMsä¸­ï¼Œä¸ºå…¶æ¨ç†èƒ½åŠ›æä¾›å¢å¼ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹èƒŒæ™¯çŸ¥è¯†å’Œæ˜“äº§ç”Ÿå¹»è§‰ã€‚</li>
<li>å°†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä¸LLMsç»“åˆä»¥å¢å¼ºå…¶æ€§èƒ½å·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨è¡¥å……äº‹å®çŸ¥è¯†ï¼Œä½†è§£å†³å¤æ‚é—®é¢˜æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç²¾ç‚¼äº‹å®ä¹‹é—´çš„å…³ç³»å¹¶ç»„æˆé€»è¾‘ä¸€è‡´çš„æ¨ç†è·¯å¾„æ˜¯æé«˜LLMsè§£å†³å¤æ‚é—®é¢˜èƒ½åŠ›çš„å…³é”®ã€‚</li>
<li>ä»çŸ¥è¯†å›¾è°±ä¸­æå–å¯é çš„æ¨ç†è·¯å¾„é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å›¾å½¢ç»“æ„çš„å¤æ‚æ€§å’Œå¤šæ¡ç”Ÿæˆè·¯å¾„çš„å­˜åœ¨ã€‚</li>
<li>RRPæ¡†æ¶ç»“åˆäº†LLMsçš„è¯­ä¹‰ä¼˜åŠ¿å’Œç»“æ„ä¿¡æ¯ï¼Œé€šè¿‡å…³ç³»åµŒå…¥å’ŒåŒå‘åˆ†å¸ƒå­¦ä¹ æ¥æŒ–æ˜çŸ¥è¯†å›¾è°±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b062ef01ffe5534d6626efc5eed7332a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9dc242074fb2353af550b65ec9b43e38.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OIBench-Benchmarking-Strong-Reasoning-Models-with-Olympiad-in-Informatics"><a href="#OIBench-Benchmarking-Strong-Reasoning-Models-with-Olympiad-in-Informatics" class="headerlink" title="OIBench: Benchmarking Strong Reasoning Models with Olympiad in   Informatics"></a>OIBench: Benchmarking Strong Reasoning Models with Olympiad in   Informatics</h2><p><strong>Authors:Yaoming Zhu, Junxin Wang, Yiyang Li, Lin Qiu, ZongYu Wang, Jun Xu, Xuezhi Cao, Yuhuai Wei, Mingshi Wang, Xunliang Cai, Rong Ma</strong></p>
<p>As models become increasingly sophisticated, conventional algorithm benchmarks are increasingly saturated, underscoring the need for more challenging benchmarks to guide future improvements in algorithmic reasoning. This paper introduces OIBench, a high-quality, private, and challenging olympiad-level informatics dataset comprising 250 carefully curated original problems. We detail the construction methodology of the benchmark, ensuring a comprehensive assessment across various programming paradigms and complexities, and we demonstrate its contamination-resistant properties via experiments. We propose Time&#x2F;Space Completion Curves for finer-grained efficiency analysis and enable direct human-model comparisons through high-level participant evaluations. Our experiments reveal that while open-source models lag behind closed-source counterparts, current SOTA models already outperform most human participants in both correctness and efficiency, while still being suboptimal compared to the canonical solutions. By releasing OIBench as a fully open-source resource (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AGI-Eval/OIBench">https://huggingface.co/datasets/AGI-Eval/OIBench</a>), we hope this benchmark will contribute to advancing code reasoning capabilities for future LLMs. </p>
<blockquote>
<p>éšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œä¼ ç»Ÿç®—æ³•åŸºå‡†æµ‹è¯•å·²ç»é€æ¸é¥±å’Œï¼Œè¿™å¼ºè°ƒäº†å¯¹æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ï¼Œä»¥æŒ‡å¯¼ç®—æ³•æ¨ç†çš„æœªæ¥æ”¹è¿›ã€‚æœ¬æ–‡ä»‹ç»äº†OIBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡ã€ç§å¯†ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¥¥æ—åŒ¹å…‹æ°´å¹³çš„ä¿¡æ¯åŒ–æ•°æ®é›†ï¼ŒåŒ…å«250ä¸ªç²¾å¿ƒæŒ‘é€‰çš„åŸå§‹é—®é¢˜ã€‚æˆ‘ä»¬è¯¦ç»†é˜è¿°äº†åŸºå‡†æµ‹è¯•æ„å»ºæ–¹æ³•ï¼Œç¡®ä¿å¯¹å„ç§ç¼–ç¨‹èŒƒå¼å’Œå¤æ‚åº¦è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶æŠ—æ±¡æŸ“ç‰¹æ€§ã€‚æˆ‘ä»¬æå‡ºæ—¶é—´&#x2F;ç©ºé—´å®Œæˆæ›²çº¿ï¼Œè¿›è¡Œæ›´ç²¾ç»†çš„æ•ˆç‡åˆ†æï¼Œå¹¶é€šè¿‡é«˜çº§å‚ä¸è€…è¯„ä¼°å®ç°äººæœºç›´æ¥æ¯”è¾ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å¼€æºæ¨¡å‹è½åäºä¸“æœ‰æ¨¡å‹ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ­£ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å·²ç»è¶…è¶Šäº†å¤§å¤šæ•°äººç±»å‚ä¸è€…ï¼Œä½†ä¸æ ‡å‡†è§£å†³æ–¹æ¡ˆç›¸æ¯”ä»ç„¶ä¸å¤Ÿç†æƒ³ã€‚æˆ‘ä»¬é€šè¿‡å®Œå…¨å¼€æºèµ„æºçš„æ–¹å¼å‘å¸ƒOIBenchï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AGI-Eval/OIBench%EF%BC%89%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E8%BF%99%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%B0%86%E4%B8%BA%E6%8F%90%E9%AB%98%E6%9C%AA%E6%9D%A5%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BB%A3%E7%A0%81%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E5%81%9A%E5%87%BA%E8%B4%A1%E7%8C%AE%E3%80%82">https://huggingface.co/datasets/AGI-Eval/OIBenchï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸ªåŸºå‡†æµ‹è¯•å°†ä¸ºæé«˜æœªæ¥å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç æ¨ç†èƒ½åŠ›åšå‡ºè´¡çŒ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10481v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€æ¨¡å‹æ—¥ç›Šå¤æ‚ï¼Œä¼ ç»Ÿç®—æ³•åŸºå‡†æµ‹è¯•é€æ¸é¥±å’Œï¼Œéœ€è¦æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•æ¥æŒ‡å¯¼ç®—æ³•æ¨ç†çš„æœªæ¥æ”¹è¿›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé«˜è´¨é‡ã€ç§å¯†ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„OIBenchæ•°æ®é›†ï¼ŒåŒ…å«ç²¾å¿ƒæŒ‘é€‰çš„åŸå§‹é—®é¢˜ã€‚æœ¬æ–‡è¯¦ç»†é˜è¿°äº†è¯¥åŸºå‡†æµ‹è¯•é›†æ„å»ºæ–¹æ³•è®ºçš„ç»¼åˆè¯„ä¼°ä½“ç³»åŠå®éªŒçš„é˜²æ±¡æŸ“å±æ€§ï¼Œå±•ç°äº†Time&#x2F;Space Completion Curvesåˆ†æã€‚è™½ç„¶ç›®å‰å­˜åœ¨æ¨¡å‹å’Œå‚ä¸è€…ä¹‹é—´æ•ˆèƒ½è¡¨ç°çš„å·®è·ï¼Œä½†è¯¥ç ”ç©¶é‡Šæ”¾å‡ºæ–°å¥‡çš„å…¬å¼€åŸºå‡†æµ‹è¯•ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹èƒ½åŠ›æ¨ç†è¿›æ­¥æ½œåŠ›ï¼Œä»¥æœŸèƒ½å¤Ÿä¼˜åŒ–å½“å‰ä½¿ç”¨çš„é¡¶å°–æ¨¡å‹çš„æ•ˆç‡è¡¨ç°ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>éšç€æ¨¡å‹å¤æ‚åº¦çš„æå‡ï¼Œä¼ ç»Ÿç®—æ³•åŸºå‡†æµ‹è¯•å·²ç»é¥±å’Œï¼Œéœ€è¦æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•æ¥æ¨åŠ¨ç®—æ³•æ¨ç†çš„è¿›æ­¥ã€‚</li>
<li>OIBenchæ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„åŸå§‹é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°ç¼–ç¨‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ•°æ®é›†å…·æœ‰é«˜è´¨é‡ã€ç§å¯†æ€§å’ŒæŒ‘æˆ˜æ€§ï¼Œé€‚ç”¨äºå„ç§ç¼–ç¨‹èŒƒå¼å’Œå¤æ‚åº¦çš„å…¨é¢è¯„ä¼°ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ•°æ®é›†çš„é˜²æ±¡æŸ“å±æ€§ã€‚</li>
<li>å¼•å…¥äº†Time&#x2F;Space Completion Curvesè¿›è¡Œæ›´ç²¾ç»†çš„æ•ˆç‡åˆ†æã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å¼€æºæ¨¡å‹åœ¨æ­£ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢è½åäºä¸“æœ‰æ¨¡å‹ï¼Œä½†å½“å‰é¡¶å°–æ¨¡å‹å·²ç»è¶…è¶Šäº†å¤§å¤šæ•°äººç±»å‚ä¸è€…çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸ç»å…¸è§£å†³æ–¹æ¡ˆç›¸æ¯”ä»æœ‰æ‰€ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8713e5515d26975d5307b89aa4355f8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c99253e6684464695e025ddae3b5be0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015dbbf54be13a3cde296897d978755a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1fbce4f76d3fcf5e77ce9a995e00a27.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Reasoning-RAG-via-System-1-or-System-2-A-Survey-on-Reasoning-Agentic-Retrieval-Augmented-Generation-for-Industry-Challenges"><a href="#Reasoning-RAG-via-System-1-or-System-2-A-Survey-on-Reasoning-Agentic-Retrieval-Augmented-Generation-for-Industry-Challenges" class="headerlink" title="Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic   Retrieval-Augmented Generation for Industry Challenges"></a>Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic   Retrieval-Augmented Generation for Industry Challenges</h2><p><strong>Authors:Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, Ziyue Li</strong></p>
<p>Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to overcome the knowledge limitations of Large Language Models (LLMs) by integrating external retrieval with language generation. While early RAG systems based on static pipelines have shown effectiveness in well-structured tasks, they struggle in real-world scenarios requiring complex reasoning, dynamic retrieval, and multi-modal integration. To address these challenges, the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds decision-making and adaptive tool use directly into the retrieval process. In this paper, we present a comprehensive review of Reasoning Agentic RAG methods, categorizing them into two primary systems: predefined reasoning, which follows fixed modular pipelines to boost reasoning, and agentic reasoning, where the model autonomously orchestrates tool interaction during inference. We analyze representative techniques under both paradigms, covering architectural design, reasoning strategies, and tool coordination. Finally, we discuss key research challenges and propose future directions to advance the flexibility, robustness, and applicability of reasoning agentic RAG systems. Our collection of the relevant research has been organized into a <a target="_blank" rel="noopener" href="https://github.com/ByebyeMonica/Reasoning-Agentic-RAG">https://github.com/ByebyeMonica/Reasoning-Agentic-RAG</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²ç»æˆä¸ºä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¤–éƒ¨æ£€ç´¢å’Œè¯­è¨€ç”Ÿæˆï¼Œå…‹æœäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†å±€é™æ€§ã€‚è™½ç„¶æ—©æœŸçš„åŸºäºé™æ€æµæ°´çº¿çš„RAGç³»ç»Ÿåœ¨ç»“æ„è‰¯å¥½çš„ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ï¼Œä½†åœ¨éœ€è¦å¤æ‚æ¨ç†ã€åŠ¨æ€æ£€ç´¢å’Œå¤šæ¨¡å¼é›†æˆçš„ç°å®åœºæ™¯ä¸­è¿˜é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶é¢†åŸŸå·²ç»è½¬å‘æ¨ç†ä»£ç†RAGï¼ˆReasoning Agentic RAGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†å†³ç­–åˆ¶å®šå’Œè‡ªé€‚åº”å·¥å…·ä½¿ç”¨ç›´æ¥åµŒå…¥åˆ°æ£€ç´¢è¿‡ç¨‹ä¸­çš„èŒƒå¼ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†æ¨ç†ä»£ç†RAGæ–¹æ³•ï¼Œå°†å®ƒä»¬åˆ†ä¸ºä¸¤å¤§ç±»ï¼šé¢„è®¾æ¨ç†ç³»ç»Ÿï¼Œå®ƒéµå¾ªå›ºå®šçš„æ¨¡å—åŒ–æµæ°´çº¿ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼›è‡ªä¸»æ¨ç†ç³»ç»Ÿï¼Œæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»åè°ƒå·¥å…·äº¤äº’ã€‚æˆ‘ä»¬åˆ†æäº†è¿™ä¸¤ç§èŒƒå¼ä¸‹çš„ä»£è¡¨æ€§æŠ€æœ¯ï¼Œæ¶µç›–äº†æ¶æ„è®¾è®¡ã€æ¨ç†ç­–ç•¥å’Œå·¥å…·åè°ƒã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å…³é”®çš„ç ”ç©¶æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æé«˜æ¨ç†ä»£ç†RAGç³»ç»Ÿçš„çµæ´»æ€§ã€é²æ£’æ€§å’Œé€‚ç”¨æ€§çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å·²å°†ç›¸å…³çš„ç ”ç©¶æ•´ç†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ByebyeMonica/Reasoning-Agentic-RAG">https://github.com/ByebyeMonica/Reasoning-Agentic-RAG</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10408v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶çš„å‘å±•å†ç¨‹åŠå…¶åœ¨å…‹æœå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çŸ¥è¯†å±€é™æ–¹é¢çš„ä½œç”¨ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†Reasoning Agentic RAGæ–¹æ³•ï¼Œå°†å…¶åˆ†ä¸ºé¢„è®¾æ¨ç†å’Œè‡ªä¸»æ¨ç†ä¸¤ç§ç³»ç»Ÿï¼Œå¹¶æ¢è®¨äº†ä»£è¡¨æ€§æŠ€æœ¯ã€æ¶æ„è®¾è®¡ã€æ¨ç†ç­–ç•¥å’Œå·¥å…·åè°ƒã€‚æœ€åè®¨è®ºäº†å…³é”®çš„ç ”ç©¶æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæé«˜Reasoning Agentic RAGç³»ç»Ÿçš„çµæ´»æ€§ã€é²æ£’æ€§å’Œé€‚ç”¨æ€§æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGæ¡†æ¶é€šè¿‡æ•´åˆå¤–éƒ¨æ£€ç´¢å’Œè¯­è¨€ç”Ÿæˆï¼Œå…‹æœäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å±€é™ã€‚</li>
<li>æ—©æœŸçš„RAGç³»ç»Ÿåœ¨ç»“æ„åŒ–ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ•ˆæœï¼Œä½†åœ¨éœ€è¦å¤æ‚æ¨ç†ã€åŠ¨æ€æ£€ç´¢å’Œå¤šæ¨¡å¼é›†æˆçš„ç°å®åœºæ™¯ä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå‡ºç°äº†Reasoning Agentic RAGèŒƒå¼ï¼Œå°†å†³ç­–åˆ¶å®šå’Œè‡ªé€‚åº”å·¥å…·ä½¿ç”¨ç›´æ¥åµŒå…¥åˆ°æ£€ç´¢è¿‡ç¨‹ä¸­ã€‚</li>
<li>Reasoning Agentic RAGæ–¹æ³•å¯åˆ†ä¸ºé¢„è®¾æ¨ç†å’Œè‡ªä¸»æ¨ç†ä¸¤ç§ç³»ç»Ÿã€‚</li>
<li>é¢„è®¾æ¨ç†éµå¾ªå›ºå®šçš„æ¨¡å—åŒ–ç®¡é“æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œè€Œè‡ªä¸»æ¨ç†åˆ™è®©æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»åœ°åè°ƒå·¥å…·äº¤äº’ã€‚</li>
<li>æ–‡ç« è¿˜è®¨è®ºäº†è¯¥é¢†åŸŸçš„å…³é”®ç ”ç©¶æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æé«˜ç³»ç»Ÿçš„çµæ´»æ€§ã€é²æ£’æ€§å’Œé€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b329e1374226212d2d3d263e6c9def9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a75b9ad6a08f1960a28c828fbc0aa423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-985080cd713079095e09d3a802afb030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48139704ef3a347d13cda3f9f7e3177b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0ff079946a1bf298ce06c5fb4bb5ebc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8e084dfd7c2ca6f455012acc630cc4f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PAG-Multi-Turn-Reinforced-LLM-Self-Correction-with-Policy-as-Generative-Verifier"><a href="#PAG-Multi-Turn-Reinforced-LLM-Self-Correction-with-Policy-as-Generative-Verifier" class="headerlink" title="PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative   Verifier"></a>PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative   Verifier</h2><p><strong>Authors:Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, Lin Yan</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on separate verifier models or require multi-stage self-correction training pipelines, which limit scalability. In this paper, we propose Policy as Generative Verifier (PAG), a simple and effective framework that empowers LLMs to self-correct by alternating between policy and verifier roles within a unified multi-turn reinforcement learning (RL) paradigm. Distinct from prior approaches that always generate a second attempt regardless of model confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only alleviates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse reasoning benchmarks highlight PAGâ€™s dual advancements: as a policy, it enhances direct generation and self-correction accuracy; as a verifier, its self-verification outperforms self-consistency. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥å¯é åœ°éªŒè¯å…¶è¾“å‡ºçš„æ­£ç¡®æ€§ã€‚é’ˆå¯¹è¿™ä¸€éªŒè¯æŒ‘æˆ˜çš„ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸ä¾èµ–äºå•ç‹¬çš„éªŒè¯å™¨æ¨¡å‹æˆ–éœ€è¦å¤šé˜¶æ®µè‡ªæˆ‘æ ¡æ­£è®­ç»ƒç®¡é“ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç­–ç•¥ä½œä¸ºç”ŸæˆéªŒè¯å™¨ï¼ˆPAGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡åœ¨ä¸€ä¸ªç»Ÿä¸€çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èŒƒå¼å†…äº¤æ›¿ä½¿ç”¨ç­–ç•¥å’ŒéªŒè¯å™¨è§’è‰²ï¼Œä½¿LLMèƒ½å¤Ÿè¿›è¡Œè‡ªæˆ‘æ ¡æ­£ã€‚ä¸åŒäºå…ˆå‰çš„æ€»æ˜¯ç”Ÿæˆç¬¬äºŒä¸ªç­”æ¡ˆè€Œä¸ç®¡æ¨¡å‹ç½®ä¿¡åº¦çš„æ–¹æ³•ï¼ŒPAGå¼•å…¥äº†é€‰æ‹©æ€§ä¿®è®¢æœºåˆ¶ï¼šåªæœ‰å½“å…¶è‡ªèº«çš„ç”ŸæˆéªŒè¯æ­¥éª¤æ£€æµ‹åˆ°é”™è¯¯æ—¶ï¼Œæ¨¡å‹æ‰ä¼šä¿®è®¢å…¶ç­”æ¡ˆã€‚è¿™ç§éªŒè¯åä¿®è®¢çš„å·¥ä½œæµç¨‹ä¸ä»…å‡è½»äº†æ¨¡å‹å´©æºƒçš„é—®é¢˜ï¼Œè¿˜åŒæ—¶æé«˜äº†æ¨ç†å’ŒéªŒè¯èƒ½åŠ›ã€‚åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒçªå‡ºäº†PAGçš„åŒé‡ä¼˜åŠ¿ï¼šä½œä¸ºç­–ç•¥ï¼Œå®ƒæé«˜äº†ç›´æ¥ç”Ÿæˆå’Œè‡ªæˆ‘æ ¡æ­£çš„å‡†ç¡®æ€§ï¼›ä½œä¸ºéªŒè¯å™¨ï¼Œå®ƒçš„è‡ªæˆ‘éªŒè¯è¶…è¶Šäº†è‡ªæˆ‘ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10406v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨éªŒè¯è¾“å‡ºæ­£ç¡®æ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰éªŒè¯æ–¹æ³•å¸¸ä¾èµ–äºå•ç‹¬çš„éªŒè¯å™¨æ¨¡å‹æˆ–å¤šé˜¶æ®µè‡ªæˆ‘ä¿®æ­£è®­ç»ƒç®¡é“ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºç­–ç•¥ä½œä¸ºç”ŸæˆéªŒè¯å™¨ï¼ˆPAGï¼‰ï¼Œé€šè¿‡ç»Ÿä¸€çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èŒƒå¼ï¼Œä½¿LLMsé€šè¿‡äº¤æ›¿çš„ç­–ç•¥å’ŒéªŒè¯å™¨è§’è‰²è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ã€‚ä¸åŒäºæ€»æ˜¯æ— æ¡ä»¶ç”Ÿæˆç¬¬äºŒä¸ªç­”æ¡ˆçš„æ–¹æ³•ï¼ŒPAGå¼•å…¥é€‰æ‹©æ€§ä¿®è®¢æœºåˆ¶ï¼šä»…åœ¨æ¨¡å‹è‡ªèº«çš„ç”ŸæˆéªŒè¯æ­¥éª¤æ£€æµ‹åˆ°é”™è¯¯æ—¶æ‰è¿›è¡Œä¿®è®¢ã€‚è¿™ç§éªŒè¯åä¿®è®¢çš„å·¥ä½œæµç¨‹ä¸ä»…å‡è½»äº†æ¨¡å‹å´©æºƒé—®é¢˜ï¼Œè¿˜åŒæ—¶æé«˜äº†æ¨ç†å’ŒéªŒè¯èƒ½åŠ›ã€‚è·¨å¤šç§æ¨ç†åŸºå‡†çš„å¹¿æ³›å®éªŒçªå‡ºäº†PAGçš„åŒé‡ä¼˜åŠ¿ï¼šä½œä¸ºç­–ç•¥ï¼Œå®ƒæé«˜äº†ç›´æ¥ç”Ÿæˆå’Œè‡ªæˆ‘ä¿®æ­£çš„å‡†ç¡®æ€§ï¼›ä½œä¸ºéªŒè¯å™¨ï¼Œå…¶è‡ªæˆ‘éªŒè¯æ•ˆæœä¼˜äºè‡ªæˆ‘ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨éªŒè¯è¾“å‡ºæ­£ç¡®æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰éªŒè¯æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ç‹¬çš„éªŒè¯å™¨æ¨¡å‹æˆ–å¤šé˜¶æ®µè‡ªæˆ‘ä¿®æ­£è®­ç»ƒç®¡é“ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„ç­–ç•¥ä½œä¸ºç”ŸæˆéªŒè¯å™¨ï¼ˆPAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿LLMsèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„å¤šè½®èŒƒå¼ä¸­è‡ªæˆ‘ä¿®æ­£ã€‚</li>
<li>PAGå¼•å…¥é€‰æ‹©æ€§ä¿®è®¢æœºåˆ¶ï¼Œä»…åœ¨æ¨¡å‹è‡ªèº«çš„ç”ŸæˆéªŒè¯æ­¥éª¤æ£€æµ‹åˆ°é”™è¯¯æ—¶æ‰è¿›è¡Œä¿®è®¢ã€‚</li>
<li>è¿™ç§éªŒè¯åä¿®è®¢çš„å·¥ä½œæµç¨‹æé«˜äº†æ¨¡å‹çš„æ¨ç†å’ŒéªŒè¯èƒ½åŠ›ï¼ŒåŒæ—¶å‡è½»äº†æ¨¡å‹å´©æºƒé—®é¢˜ã€‚</li>
<li>PAGç­–ç•¥æé«˜äº†ç›´æ¥ç”Ÿæˆå’Œè‡ªæˆ‘ä¿®æ­£çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0280c4b2bbf7314db7b1fc303cf8f720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74dfbada2632efeb66df8528174ba579.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3466368fa91cf2809720134559a751bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-211bc1b711266383969e9fcfb46409d5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="EQA-RM-A-Generative-Embodied-Reward-Model-with-Test-time-Scaling"><a href="#EQA-RM-A-Generative-Embodied-Reward-Model-with-Test-time-Scaling" class="headerlink" title="EQA-RM: A Generative Embodied Reward Model with Test-time Scaling"></a>EQA-RM: A Generative Embodied Reward Model with Test-time Scaling</h2><p><strong>Authors:Yuhang Chen, Zhen Tan, Tianlong Chen</strong></p>
<p>Reward Models (RMs), vital for large model alignment, are underexplored for complex embodied tasks like Embodied Question Answering (EQA) where nuanced evaluation of agentsâ€™ spatial, temporal, and logical understanding is critical yet not considered by generic approaches. We introduce EQA-RM, a novel generative multimodal reward model specifically architected for EQA, trained via our innovative Contrastive Group Relative Policy Optimization (C-GRPO) strategy to learn fine-grained behavioral distinctions. The generative nature of EQA-RM provides interpretable, structured reward feedback (beyond simple scalars), uniquely enabling test-time scaling to dynamically adjust evaluation granularity, from concise scores to detailed critiques of reasoning and grounding, at inference without retraining. Concurrently, we introduce EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning Qwen2-VL-2B-Instruct) achieves 61.9% accuracy on EQA-RM-Bench with only 700 samples, outperforming strong proprietary baselines, including Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art models such as RoVRM and VisualPRM. The code and dataset can be found here <a target="_blank" rel="noopener" href="https://github.com/UNITES-Lab/EQA-RM">https://github.com/UNITES-Lab/EQA-RM</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å¯¹äºå¤§å‹æ¨¡å‹å¯¹é½è‡³å…³é‡è¦ï¼Œåœ¨å¤æ‚çš„å®ä½“ä»»åŠ¡ï¼ˆå¦‚å®ä½“é—®ç­”ï¼ˆEQAï¼‰ï¼‰ä¸­å´è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚åœ¨EQAä¸­ï¼Œå¯¹ä»£ç†çš„ç©ºé—´ã€æ—¶é—´å’Œé€»è¾‘ç†è§£çš„å¾®å¦™è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†é€šç”¨æ–¹æ³•å°šæœªè€ƒè™‘è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸“é—¨ä¸ºEQAè®¾è®¡çš„å…¨æ–°ç”Ÿæˆå¼å¤šæ¨¡å¼å¥–åŠ±æ¨¡å‹EQA-RMï¼Œé€šè¿‡æˆ‘ä»¬åˆ›æ–°çš„å¯¹æ¯”ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆC-GRPOï¼‰ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ ç²¾ç»†çš„è¡Œä¸ºå·®å¼‚ã€‚EQA-RMçš„ç”Ÿæˆæ€§è´¨æä¾›äº†å¯è§£é‡Šçš„ç»“æ„åŒ–å¥–åŠ±åé¦ˆï¼ˆè¶…å‡ºç®€å•æ ‡é‡ï¼‰ï¼Œèƒ½å¤Ÿå”¯ä¸€åœ°åœ¨æµ‹è¯•æ—¶è¿›è¡Œç¼©æ”¾ï¼Œä»¥åŠ¨æ€è°ƒæ•´è¯„ä¼°ç²’åº¦ï¼Œä»ç®€æ´çš„åˆ†æ•°åˆ°æ¨ç†å’Œä¾æ®çš„è¯¦ç»†è¯„ä»·ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯è¿›è¡Œæ¨æ–­ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åŸºäºOpenEQAå»ºç«‹äº†æ–°çš„åŸºå‡†æµ‹è¯•EQARewardBenchï¼Œç”¨äºæ ‡å‡†åŒ–EQAå¥–åŠ±æ¨¡å‹çš„è¯„ä¼°ã€‚EQA-RMè¡¨ç°å‡ºè¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œé€šè¿‡å¾®è°ƒQwen2-VL-2B-Instructï¼Œåœ¨EQA-RM-Benchä¸Šå®ç°61.9%çš„å‡†ç¡®ç‡ï¼Œä»…ä½¿ç”¨700ä¸ªæ ·æœ¬å°±è¶…è¶Šäº†å¼ºå¤§çš„ä¸“æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬Gemini-2.5-Flashã€GPT-4oã€Claude-3.5-Haikuä»¥åŠå¼€æºçš„å…ˆè¿›æ¨¡å‹ï¼Œå¦‚RoVRMå’ŒVisualPRMã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/UNITES-Lab/EQA-RM">https://github.com/UNITES-Lab/EQA-RM</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10389v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤æ‚ä½“ä»»åŠ¡ï¼ˆå¦‚èº«ä¸´å…¶å¢ƒçš„é—®ç­”ï¼‰è®¾è®¡çš„å¥–åŠ±æ¨¡å‹EQA-RMï¼Œå…¶é€šè¿‡åˆ›æ–°çš„å¯¹æ¯”ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼ˆC-GRPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥ç²¾ç»†çš„è¡Œä¸ºå·®å¼‚ä¸ºåŸºç¡€è¿›è¡Œå­¦ä¹ ã€‚EQA-RMå…·æœ‰ç”Ÿæˆæ€§ï¼Œèƒ½æä¾›å¯è§£é‡Šçš„ã€ç»“æ„åŒ–çš„å¥–åŠ±åé¦ˆï¼Œæ”¯æŒåœ¨æµ‹è¯•æ—¶åŠ¨æ€è°ƒæ•´è¯„ä¼°ç²’åº¦ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŸºäºOpenEQAå»ºç«‹çš„EQARewardBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°EQAå¥–åŠ±æ¨¡å‹ã€‚EQA-RMåœ¨EQARewardBenchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†61.9%ï¼Œä»…ä½¿ç”¨700ä¸ªæ ·æœ¬å°±è¶…è¿‡äº†å¤šä¸ªå¼ºå¤§çš„ä¸“æœ‰åŸºå‡†æ¨¡å‹å’Œå¼€æºå…ˆè¿›æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EQA-RMæ˜¯é’ˆå¯¹èº«ä¸´å…¶å¢ƒé—®ç­”ç­‰å¤æ‚ä½“ä»»åŠ¡çš„å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>EQA-RMå…·æœ‰ç”Ÿæˆæ€§ï¼Œèƒ½æä¾›ç»“æ„åŒ–å¥–åŠ±åé¦ˆã€‚</li>
<li>C-GRPOç­–ç•¥ç”¨äºè®­ç»ƒEQA-RMï¼Œä»¥ç²¾ç»†çš„è¡Œä¸ºå·®å¼‚ä¸ºåŸºç¡€è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>EQA-RMæ”¯æŒåœ¨æµ‹è¯•æ—¶åŠ¨æ€è°ƒæ•´è¯„ä¼°ç²’åº¦ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>EQARewardBenchæ˜¯å»ºç«‹äºOpenEQAä¹‹ä¸Šçš„å¥–åŠ±æ¨¡å‹è¯„ä¼°åŸºå‡†ã€‚</li>
<li>EQA-RMåœ¨EQARewardBenchä¸Šçš„å‡†ç¡®ç‡é«˜ï¼Œä»…ä½¿ç”¨å°‘é‡æ ·æœ¬å°±è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2343e03d5fa54d597c15afc35504260f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5eeefa6e3974a4dbe07a5e67f3b41327.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d8edd6777a041fd58ced4d457147564.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TableRAG-A-Retrieval-Augmented-Generation-Framework-for-Heterogeneous-Document-Reasoning"><a href="#TableRAG-A-Retrieval-Augmented-Generation-Framework-for-Heterogeneous-Document-Reasoning" class="headerlink" title="TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous   Document Reasoning"></a>TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous   Document Reasoning</h2><p><strong>Authors:Xiaohan Yu, Pu Jian, Chong Chen</strong></p>
<p>Retrieval-Augmented Generation (RAG) has demonstrated considerable effectiveness in open-domain question answering. However, when applied to heterogeneous documents, comprising both textual and tabular components, existing RAG approaches exhibit critical limitations. The prevailing practice of flattening tables and chunking strategies disrupts the intrinsic tabular structure, leads to information loss, and undermines the reasoning capabilities of LLMs in multi-hop, global queries. To address these challenges, we propose TableRAG, an hybrid framework that unifies textual understanding and complex manipulations over tabular data. TableRAG iteratively operates in four steps: context-sensitive query decomposition, text retrieval, SQL programming and execution, and compositional intermediate answer generation. We also develop HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous reasoning capabilities. Experimental results demonstrate that TableRAG consistently outperforms existing baselines on both public datasets and our HeteQA, establishing a new state-of-the-art for heterogeneous document question answering. We release TableRAG at <a target="_blank" rel="noopener" href="https://github.com/yxh-y/TableRAG/tree/main">https://github.com/yxh-y/TableRAG/tree/main</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¼€æ”¾åŸŸé—®ç­”ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºåŒ…å«æ–‡æœ¬å’Œè¡¨æ ¼ç»„ä»¶çš„å¼‚è´¨æ–‡æ¡£æ—¶ï¼Œç°æœ‰çš„RAGæ–¹æ³•è¡¨ç°å‡ºé‡å¤§å±€é™æ€§ã€‚æµè¡Œçš„è¡¨æ ¼å¹³é“ºå’Œåˆ†ç‰‡ç­–ç•¥ç ´åäº†å†…åœ¨çš„è¡¨æ ¼ç»“æ„ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œå¹¶ç ´åäº†LLMåœ¨å¤šè·³å…¨å±€æŸ¥è¯¢ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TableRAGï¼Œè¿™æ˜¯ä¸€ä¸ªèåˆäº†æ–‡æœ¬ç†è§£å’Œè¡¨æ ¼æ•°æ®çš„å¤æ‚æ“ä½œçš„æ··åˆæ¡†æ¶ã€‚TableRAGä»¥å››ä¸ªæ­¥éª¤è¿­ä»£è¿è¡Œï¼šä¸Šä¸‹æ–‡æ•æ„Ÿçš„æŸ¥è¯¢åˆ†è§£ã€æ–‡æœ¬æ£€ç´¢ã€SQLç¼–ç¨‹å’Œæ‰§è¡Œï¼Œä»¥åŠç»„åˆä¸­é—´ç­”æ¡ˆç”Ÿæˆã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†HeteQAï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šè·³å¼‚è´¨æ¨ç†èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å…¬å…±æ•°æ®é›†è¿˜æ˜¯æˆ‘ä»¬çš„HeteQAä¸Šï¼ŒTableRAGå§‹ç»ˆä¼˜äºç°æœ‰åŸºå‡†ï¼Œä¸ºå¼‚è´¨æ–‡æ¡£é—®ç­”å»ºç«‹äº†æ–°çš„æœ€æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/yxh-y/TableRAG/tree/main%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86TableRAG%E3%80%82">https://github.com/yxh-y/TableRAG/tree/mainä¸Šå‘å¸ƒäº†TableRAGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10380v1">PDF</a> Under review. Codes are available at   <a target="_blank" rel="noopener" href="https://github.com/yxh-y/TableRAG/tree/main">https://github.com/yxh-y/TableRAG/tree/main</a></p>
<p><strong>Summary</strong></p>
<p>RAGåœ¨å¼€æ”¾åŸŸé—®ç­”ä¸­å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œä½†åœ¨å¤„ç†åŒ…å«æ–‡æœ¬å’Œè¡¨æ ¼çš„å¼‚è´¨æ–‡æ¡£æ—¶å­˜åœ¨å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•ç ´åè¡¨æ ¼çš„å†…åœ¨ç»“æ„ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±å¹¶å½±å“LLMsåœ¨å¤šè·³å…¨å±€æŸ¥è¯¢ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºTableRAGç»Ÿä¸€æ¡†æ¶ï¼Œèåˆæ–‡æœ¬ç†è§£å’Œè¡¨æ ¼æ•°æ®çš„å¤æ‚æ“ä½œã€‚TableRAGé€šè¿‡å››ä¸ªæ­¥éª¤è¿­ä»£æ“ä½œï¼šä¸Šä¸‹æ–‡æ•æ„Ÿçš„æŸ¥è¯¢åˆ†è§£ã€æ–‡æœ¬æ£€ç´¢ã€SQLç¼–ç¨‹ä¸æ‰§è¡Œï¼Œä»¥åŠç»„åˆä¸­é—´ç­”æ¡ˆç”Ÿæˆã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼€å‘HeteQAåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å¤šè·³å¼‚è´¨æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜TableRAGåœ¨å…¬å…±æ•°æ®é›†å’ŒHeteQAä¸Šçš„è¡¨ç°å‡è¶…è¶Šç°æœ‰åŸºçº¿ï¼Œæˆä¸ºå¼‚è´¨æ–‡æ¡£é—®ç­”çš„æ–°é‡Œç¨‹ç¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGåœ¨å¤„ç†åŒ…å«æ–‡æœ¬å’Œè¡¨æ ¼çš„å¼‚è´¨æ–‡æ¡£æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç ´åè¡¨æ ¼çš„å†…åœ¨ç»“æ„ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>TableRAGæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèåˆäº†æ–‡æœ¬ç†è§£å’Œè¡¨æ ¼æ•°æ®çš„å¤æ‚æ“ä½œã€‚</li>
<li>TableRAGé€šè¿‡å››ä¸ªæ­¥éª¤è¿›è¡Œè¿­ä»£æ“ä½œï¼šæŸ¥è¯¢åˆ†è§£ã€æ–‡æœ¬æ£€ç´¢ã€SQLç¼–ç¨‹ä¸æ‰§è¡Œï¼Œä»¥åŠç­”æ¡ˆç”Ÿæˆã€‚</li>
<li>HeteQAåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤šè·³å¼‚è´¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TableRAGåœ¨å…¬å…±æ•°æ®é›†å’ŒHeteQAä¸Šçš„è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>TableRAGæ¡†æ¶å·²è¢«å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20e0d39bfe5915f6378e15a4c2e82873.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae8b01ae628ca1cbda579b9e65546901.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42ea72c0b5b08f0fba058169830173a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6859f2408e636c667ea3a72a87a14b4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c28cb4d9c5dce74745d774cd27a181d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Optimus-3-Towards-Generalist-Multimodal-Minecraft-Agents-with-Scalable-Task-Experts"><a href="#Optimus-3-Towards-Generalist-Multimodal-Minecraft-Agents-with-Scalable-Task-Experts" class="headerlink" title="Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable   Task Experts"></a>Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable   Task Experts</h2><p><strong>Authors:Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Weili Guan, Dongmei Jiang, Liqiang Nie</strong></p>
<p>Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agentâ€™s reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: <a target="_blank" rel="noopener" href="https://cybertronagent.github.io/Optimus-3.github.io/">https://cybertronagent.github.io/Optimus-3.github.io/</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä»£ç†åœ¨å„ç§é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œåœ¨åƒMinecraftè¿™æ ·çš„å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºä¸€ä¸ªå…·å¤‡æ„ŸçŸ¥ã€è§„åˆ’ã€è¡ŒåŠ¨ã€æ¥åœ°å’Œåæ€èƒ½åŠ›çš„å…¨èƒ½ä»£ç†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç‰¹å®šé¢†åŸŸçš„æ•°æ®ä¸è¶³ã€ä¸åŒä»»åŠ¡ä¹‹é—´çš„å¹²æ‰°ä»¥åŠå¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­çš„è§†è§‰å¤šæ ·æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚1ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªçŸ¥è¯†å¢å¼ºæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¸ºä»£ç†å¼€å‘æä¾›å¯æ‰©å±•å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚2ï¼‰ä¸ºäº†å‡è½»ä¸åŒä»»åŠ¡ä¹‹é—´çš„å¹²æ‰°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œå…·æœ‰ä»»åŠ¡çº§è·¯ç”±ã€‚3.æˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šæ¨¡æ€æ¨ç†å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜ä»£ç†åœ¨Minecraftä¸­å¤„ç†è§†è§‰å¤šæ ·æ€§çš„æ¨ç†èƒ½åŠ›ã€‚åŸºäºè¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Optimus-3ï¼Œä¸€ä¸ªé€‚ç”¨äºMinecraftçš„é€šç”¨ä»£ç†ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOptimus-3åœ¨Minecraftç¯å¢ƒä¸­çš„å„ç§ä»»åŠ¡ä¸Šè¶…è¶Šäº†é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç°æœ‰çš„æœ€å…ˆè¿›çš„ä»£ç†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cybertronagent.github.io/Optimus-3/">https://cybertronagent.github.io/Optimus-3/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10357v1">PDF</a> 24 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼ŒåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä»£ç†åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨åƒMinecraftè¿™æ ·çš„å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºä¸€ä¸ªå…·å¤‡æ„ŸçŸ¥ã€è§„åˆ’ã€è¡ŒåŠ¨ã€æ¥åœ°å’Œåæ€ç­‰èƒ½åŠ›çš„å…¨èƒ½ä»£ç†ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚ç‰¹å®šé¢†åŸŸçš„ç¼ºä¹ã€å¼‚æ„ä»»åŠ¡é—´çš„å¹²æ‰°ä»¥åŠå¼€æ”¾ä¸–ç•Œä¸­çš„è§†è§‰å¤šæ ·æ€§ã€‚æœ¬æ–‡é€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šä¸€æ˜¯æå‡ºçŸ¥è¯†å¢å¼ºæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¸ºä»£ç†å¼€å‘æä¾›å¯æ‰©å±•å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼›äºŒæ˜¯å¼•å…¥å…·æœ‰ä»»åŠ¡çº§è·¯ç”±çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œå‡è½»å¼‚æ„ä»»åŠ¡é—´çš„å¹²æ‰°ï¼›ä¸‰æ˜¯å¼€å‘å¤šæ¨¡æ€æ¨ç†å¢å¼ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæé«˜ä»£ç†åœ¨Minecraftä¸­åº”å¯¹è§†è§‰å¤šæ ·æ€§çš„æ¨ç†èƒ½åŠ›ã€‚åŸºäºè¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Optimus-3ï¼Œä¸€ä¸ªé€‚ç”¨äºMinecraftçš„é€šç”¨ä»£ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOptimus-3åœ¨Minecraftç¯å¢ƒä¸­çš„ä¸€ç³»åˆ—ä»»åŠ¡ä¸­è¶…è¶Šäº†é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä»¥åŠç°æœ‰å…ˆè¿›ä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨å¤šä¸ªé¢†åŸŸæœ‰æ˜¾è‘—çš„è¿›æ­¥ã€‚</li>
<li>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒå¦‚Minecraftä¸­æ„å»ºå…¨èƒ½ä»£ç†å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é¢†åŸŸæ•°æ®ä¸è¶³ã€å¼‚æ„ä»»åŠ¡å¹²æ‰°å’Œè§†è§‰å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºçŸ¥è¯†å¢å¼ºæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¸ºä»£ç†å¼€å‘æä¾›é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>å¼•å…¥æ··åˆä¸“å®¶æ¶æ„ï¼Œé€šè¿‡ä»»åŠ¡çº§è·¯ç”±å‡è½»å¼‚æ„ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚</li>
<li>å¼€å‘å¤šæ¨¡æ€æ¨ç†å¢å¼ºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæé«˜ä»£ç†åº”å¯¹è§†è§‰å¤šæ ·æ€§çš„èƒ½åŠ›ã€‚</li>
<li>Optimus-3æ˜¯ä¸€ä¸ªé€‚ç”¨äºMinecraftçš„é€šç”¨ä»£ç†ï¼Œå®éªŒç»“æœä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-977f9afae2551010de72b6fb35715817.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d1c0959edea939ce9581690f0d24122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c1ae440490762ab07fe6693863332c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd62264b906ae56595a460ddcfb55564.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation"><a href="#Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation" class="headerlink" title="Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation"></a>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation</h2><p><strong>Authors:Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xingang Wang</strong></p>
<p>Recent advances in large language models, especially in natural language understanding and reasoning, have opened new possibilities for text-to-motion generation. Although existing approaches have made notable progress in semantic alignment and motion synthesis, they often rely on end-to-end mapping strategies that fail to capture deep linguistic structures and logical reasoning. Consequently, generated motions tend to lack controllability, consistency, and diversity. To address these limitations, we propose Motion-R1, a unified motion-language modeling framework that integrates a Chain-of-Thought mechanism. By explicitly decomposing complex textual instructions into logically structured action paths, Motion-R1 provides high-level semantic guidance for motion generation, significantly enhancing the modelâ€™s ability to interpret and execute multi-step, long-horizon, and compositionally rich commands. To train our model, we adopt Group Relative Policy Optimization, a reinforcement learning algorithm designed for large models, which leverages motion quality feedback to optimize reasoning chains and motion synthesis jointly. Extensive experiments across multiple benchmark datasets demonstrate that Motion-R1 achieves competitive or superior performance compared to state-of-the-art methods, particularly in scenarios requiring nuanced semantic understanding and long-term temporal coherence. The code, model and data will be publicly available. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†æ–¹é¢çš„è¿›å±•ä¸ºæ–‡æœ¬åˆ°è¿åŠ¨çš„ç”Ÿæˆå¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åœ¨è¯­ä¹‰å¯¹é½å’Œè¿åŠ¨åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç«¯åˆ°ç«¯çš„æ˜ å°„ç­–ç•¥ï¼Œæ— æ³•æ•æ‰æ·±å±‚è¯­è¨€ç»“æ„å’Œé€»è¾‘æ¨ç†ã€‚å› æ­¤ï¼Œç”Ÿæˆçš„è¿åŠ¨çš„å¯æ§æ€§ã€ä¸€è‡´æ€§å’Œå¤šæ ·æ€§å¾€å¾€ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Motion-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¿åŠ¨è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œå®ƒé›†æˆäº†é“¾å¼æ€ç»´æœºåˆ¶ã€‚é€šè¿‡æ˜¾å¼åœ°å°†å¤æ‚çš„æ–‡æœ¬æŒ‡ä»¤åˆ†è§£ä¸ºé€»è¾‘ç»“æ„åŒ–çš„è¡ŒåŠ¨è·¯å¾„ï¼ŒMotion-R1ä¸ºè¿åŠ¨ç”Ÿæˆæä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹è§£é‡Šå’Œæ‰§è¡Œå¤šæ­¥éª¤ã€é•¿æœŸå’Œç»„åˆä¸°å¯Œçš„å‘½ä»¤çš„èƒ½åŠ›ã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é’ˆå¯¹å¤§å‹æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥ç®—æ³•åˆ©ç”¨è¿åŠ¨è´¨é‡åé¦ˆæ¥è”åˆä¼˜åŒ–æ¨ç†é“¾å’Œè¿åŠ¨åˆæˆã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMotion-R1åœ¨éœ€è¦å¾®å¦™è¯­ä¹‰ç†è§£å’Œé•¿æœŸæ—¶é—´è¿è´¯æ€§çš„åœºæ™¯ä¸­ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10353v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå°¤å…¶æ˜¯è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†æ–¹é¢çš„è¿›å±•ï¼Œä¸ºæ–‡æœ¬åˆ°è¿åŠ¨çš„ç”Ÿæˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨è¯­ä¹‰å¯¹é½å’Œè¿åŠ¨åˆæˆæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œå¾€å¾€ä¾èµ–ç«¯åˆ°ç«¯çš„æ˜ å°„ç­–ç•¥ï¼Œæ— æ³•æ•æ‰æ·±å±‚è¯­è¨€ç»“æ„å’Œé€»è¾‘æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Motion-R1ï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¿åŠ¨è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´æœºåˆ¶æ˜¾å¼åœ°å°†å¤æ‚çš„æ–‡æœ¬æŒ‡ä»¤åˆ†è§£ä¸ºé€»è¾‘ç»“æ„åŒ–çš„åŠ¨ä½œè·¯å¾„ï¼Œä¸ºè¿åŠ¨ç”Ÿæˆæä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ã€‚é€šè¿‡é‡‡ç”¨é’ˆå¯¹å¤§å‹æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè”åˆä¼˜åŒ–æ¨ç†é“¾å’Œè¿åŠ¨åˆæˆã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMotion-R1åœ¨éœ€è¦å¾®å¦™è¯­ä¹‰ç†è§£å’Œé•¿æœŸæ—¶é—´è¿è´¯æ€§çš„åœºæ™¯ä¸­ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºæ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆæä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è¯­ä¹‰å¯¹é½å’Œè¿åŠ¨åˆæˆæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Motion-R1æ¡†æ¶é€šè¿‡é“¾å¼æ€ç»´æœºåˆ¶åˆ†è§£å¤æ‚æ–‡æœ¬æŒ‡ä»¤ä¸ºé€»è¾‘ç»“æ„åŒ–çš„åŠ¨ä½œè·¯å¾„ã€‚</li>
<li>Motion-R1é‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè”åˆä¼˜åŒ–æ¨ç†é“¾å’Œè¿åŠ¨åˆæˆã€‚</li>
<li>Motion-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¾®å¦™è¯­ä¹‰ç†è§£å’Œé•¿æœŸæ—¶é—´è¿è´¯æ€§çš„åœºæ™¯ä¸­ã€‚</li>
<li>Motion-R1æ¨¡å‹ã€ä»£ç å’Œæ•°æ®å°†å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb600fe5be4a64b2b6aa8a13f1ddeb07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc821b3a22a5c622efa4c17a2912ae49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0106ae2049aba135f37814fd5207b9e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e23504e7be4d3153c36085bb8c475e94.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="WGSR-Bench-Wargame-based-Game-theoretic-Strategic-Reasoning-Benchmark-for-Large-Language-Models"><a href="#WGSR-Bench-Wargame-based-Game-theoretic-Strategic-Reasoning-Benchmark-for-Large-Language-Models" class="headerlink" title="WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark   for Large Language Models"></a>WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark   for Large Language Models</h2><p><strong>Authors:Qiyue Yin, Pei Xu, Qiaozhe Li, Shengda Liu, Shengqi Shen, Tong Wang, Yihong Han, Xiaonan Zhao, Likun Yang, Shiyue Cao, Shiyu Qiu, Yuxuan Liu, Shizhao Yu, Lei Cui, Chengxin Yan, Jie Sun, Xiangquan Tang, Kaiqi Huang</strong></p>
<p>Recent breakthroughs in Large Language Models (LLMs) have led to a qualitative leap in artificial intelligenceâ€™ s performance on reasoning tasks, particularly demonstrating remarkable capabilities in mathematical, symbolic, and commonsense reasoning. However, as a critical component of advanced human cognition, strategic reasoning, i.e., the ability to assess multi-agent behaviors in dynamic environments, formulate action plans, and adapt strategies, has yet to be systematically evaluated or modeled. To address this gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark for LLMs using wargame as its evaluation environment. Wargame, a quintessential high-complexity strategic scenario, integrates environmental uncertainty, adversarial dynamics, and non-unique strategic choices, making it an effective testbed for assessing LLMsâ€™ capabilities in multi-agent decision-making, intent inference, and counterfactual reasoning. WGSR-Bench designs test samples around three core tasks, i.e., Environmental situation awareness, Opponent risk modeling and Policy generation, which serve as the core S-POE architecture, to systematically assess main abilities of strategic reasoning. Finally, an LLM-based wargame agent is designed to integrate these parts for a comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess the strengths and limitations of state-of-the-art LLMs in game-theoretic strategic reasoning and to advance research in large model-driven strategic intelligence. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªç ´å¯¼è‡´äººå·¥æ™ºèƒ½åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‘ç”Ÿäº†è´¨çš„å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä½œä¸ºå…ˆè¿›äººç±»è®¤çŸ¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œå³è¯„ä¼°å¤šæ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„è¡Œä¸ºã€åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ã€é€‚åº”ç­–ç•¥çš„èƒ½åŠ›ï¼Œå°šæœªå¾—åˆ°ç³»ç»Ÿçš„è¯„ä¼°æˆ–å»ºæ¨¡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ä»‹ç»äº†WGSR-Benchï¼Œè¿™æ˜¯ä½¿ç”¨æˆ˜äº‰æ¸¸æˆä½œä¸ºè¯„ä¼°ç¯å¢ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é¦–ä¸ªæˆ˜ç•¥æ¨ç†åŸºå‡†æµ‹è¯•ã€‚æˆ˜äº‰æ¸¸æˆæ˜¯ä¸€ä¸ªå…¸å‹çš„é«˜å¤æ‚åº¦æˆ˜ç•¥åœºæ™¯ï¼Œèåˆäº†ç¯å¢ƒä¸ç¡®å®šæ€§ã€å¯¹æŠ—æ€§åŠ¨æ€å’Œéå”¯ä¸€æˆ˜ç•¥é€‰æ‹©ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“å†³ç­–ã€æ„å›¾æ¨æ–­å’Œåå‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›çš„æœ‰æ•ˆæµ‹è¯•å¹³å°ã€‚WGSR-Benchå›´ç»•ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡è®¾è®¡æµ‹è¯•æ ·æœ¬ï¼Œå³ç¯å¢ƒæ€åŠ¿æ„ŸçŸ¥ã€å¯¹æ‰‹é£é™©å»ºæ¨¡å’Œæ”¿ç­–ç”Ÿæˆï¼Œä½œä¸ºS-POEæ¶æ„çš„æ ¸å¿ƒï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°ä¸»è¦æˆ˜ç•¥æ¨ç†èƒ½åŠ›ã€‚æœ€åï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æˆ˜äº‰æ¸¸æˆä»£ç†ï¼Œå°†è¿™äº›éƒ¨åˆ†æ•´åˆèµ·æ¥è¿›è¡Œå…¨é¢æˆ˜ç•¥æ¨ç†è¯„ä¼°ã€‚é€šè¿‡WGSR-Benchï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿè¯„ä¼°æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆç†è®ºæˆ˜ç•¥æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶æ¨åŠ¨å¤§å‹æ¨¡å‹é©±åŠ¨çš„æˆ˜ç•¥æ™ºèƒ½ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10264v1">PDF</a> 15 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å–å¾—äº†è´¨çš„é£è·ƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ˜ç•¥æ¨ç†ä½œä¸ºé«˜çº§äººç±»è®¤çŸ¥çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬è¯„ä¼°å¤šæ™ºèƒ½ä½“è¡Œä¸ºã€åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ã€é€‚åº”ç­–ç•¥ç­‰ï¼Œå°šæœªè¢«ç³»ç»Ÿè¯„ä¼°æˆ–å»ºæ¨¡ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¼•å…¥äº†WGSR-Benchï¼Œè¿™æ˜¯ä½¿ç”¨æˆ˜äº‰æ¸¸æˆä½œä¸ºè¯„ä¼°ç¯å¢ƒçš„ç¬¬ä¸€ä¸ªæˆ˜ç•¥æ¨ç†åŸºå‡†æµ‹è¯•ã€‚æˆ˜äº‰æ¸¸æˆæ˜¯ä¸€ä¸ªé«˜å¤æ‚åº¦çš„æˆ˜ç•¥åœºæ™¯ï¼Œèåˆäº†ç¯å¢ƒä¸ç¡®å®šæ€§ã€å¯¹æŠ—æ€§åŠ¨æ€å’Œéå”¯ä¸€æˆ˜ç•¥é€‰æ‹©ï¼Œæ˜¯è¯„ä¼°LLMåœ¨å¤šæ™ºèƒ½ä½“å†³ç­–ã€æ„å›¾æ¨æ–­å’Œåäº‹å®æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæµ‹è¯•å¹³å°ã€‚WGSR-Benchå›´ç»•ç¯å¢ƒæ€åŠ¿æ„ŸçŸ¥ã€å¯¹æ‰‹é£é™©å»ºæ¨¡å’Œæ”¿ç­–åˆ¶å®šä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡è®¾è®¡æµ‹è¯•ç”¨ä¾‹ï¼Œä½œä¸ºç³»ç»Ÿçš„S-POEæ¶æ„ï¼Œä»¥è¯„ä¼°æˆ˜ç•¥æ¨ç†çš„ä¸»è¦èƒ½åŠ›ã€‚æœ€åè®¾è®¡äº†ä¸€ä¸ªåŸºäºLLMçš„æˆ˜äº‰æ¸¸æˆä»£ç†ï¼Œä»¥æ•´åˆè¿™äº›éƒ¨åˆ†è¿›è¡Œå…¨é¢çš„æˆ˜ç•¥æ¨ç†è¯„ä¼°ã€‚å¸Œæœ›é€šè¿‡WGSR-Benchè¯„ä¼°æœ€æ–°LLMåœ¨æ¸¸æˆç†è®ºæˆ˜ç•¥æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶æ¨åŠ¨å¤§å‹æ¨¡å‹é©±åŠ¨çš„æˆ˜ç•¥æ™ºèƒ½ç ”ç©¶çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†æ–¹é¢ã€‚</li>
<li>æˆ˜ç•¥æ¨ç†ä½œä¸ºäººç±»é«˜çº§è®¤çŸ¥çš„å…³é”®éƒ¨åˆ†ï¼Œåœ¨LLMä¸­å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°æˆ–å»ºæ¨¡ã€‚</li>
<li>WGSR-Benchæ˜¯é¦–ä¸ªé’ˆå¯¹LLMçš„æˆ˜ç•¥æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨æˆ˜äº‰æ¸¸æˆä½œä¸ºè¯„ä¼°ç¯å¢ƒã€‚</li>
<li>æˆ˜äº‰æ¸¸æˆæ˜¯ä¸€ä¸ªå¤æ‚çš„æˆ˜ç•¥åœºæ™¯ï¼Œèƒ½æœ‰æ•ˆè¯„ä¼°LLMåœ¨å¤šæ™ºèƒ½ä½“å†³ç­–ã€æ„å›¾æ¨æ–­å’Œåäº‹å®æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>WGSR-Benchå›´ç»•ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡è®¾è®¡æµ‹è¯•ç”¨ä¾‹ï¼šç¯å¢ƒæ€åŠ¿æ„ŸçŸ¥ã€å¯¹æ‰‹é£é™©å»ºæ¨¡å’Œæ”¿ç­–åˆ¶å®šã€‚</li>
<li>åŸºäºLLMçš„æˆ˜äº‰æ¸¸æˆä»£ç†è¢«è®¾è®¡æ¥æ•´åˆè¿™äº›éƒ¨åˆ†ï¼Œè¿›è¡Œå…¨é¢çš„æˆ˜ç•¥æ¨ç†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65d1939ed576f0af1b1531965ac2c4e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7de476f9288162380dcee01c17b7155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6759523673367b059c9ffa5aa98eae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd99b176aefc11cb56bb215d7e6ece2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-256047ab99e2d41673c82ddca74663ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e2c3028bd23193f47eecfc6794e61c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7538f576ceb7c1971ee67c31d853f36.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TTT-Bench-A-Benchmark-for-Evaluating-Reasoning-Ability-with-Simple-and-Novel-Tic-Tac-Toe-style-Games"><a href="#TTT-Bench-A-Benchmark-for-Evaluating-Reasoning-Ability-with-Simple-and-Novel-Tic-Tac-Toe-style-Games" class="headerlink" title="TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and   Novel Tic-Tac-Toe-style Games"></a>TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and   Novel Tic-Tac-Toe-style Games</h2><p><strong>Authors:Prakamya Mishra, Jiang Liu, Jialian Wu, Xiaodong Yu, Zicheng Liu, Emad Barsoum</strong></p>
<p>Large reasoning models (LRMs) have demonstrated impressive reasoning capabilities across a broad range of tasks including Olympiad-level mathematical problems, indicating evidence of their complex reasoning abilities. While many reasoning benchmarks focus on the STEM domain, the ability of LRMs to reason correctly in broader task domains remains underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark that is designed to evaluate basic strategic, spatial, and logical reasoning abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games that humans can effortlessly solve from a young age. We propose a simple yet scalable programmatic approach for generating verifiable two-player game problems for TTT-Bench. Although these games are trivial for humans, they require reasoning about the intentions of the opponent, as well as the game boardâ€™s spatial configurations, to ensure a win. We evaluate a diverse set of state-of-the-art LRMs, and \textbf{discover that the models that excel at hard math problems frequently fail at these simple reasoning games}. Further testing reveals that our evaluated reasoning models score on average $\downarrow$ 41% &amp; $\downarrow$ 5% lower on TTT-Bench compared to MATH 500 &amp; AIME 2024 respectively, with larger models achieving higher performance using shorter reasoning traces, where most of the models struggle on long-term strategic reasoning situations on simple and new TTT-Bench tasks. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²åœ¨åŒ…æ‹¬å¥¥æ—åŒ¹å…‹çº§åˆ«æ•°å­¦é—®é¢˜åœ¨å†…çš„å¹¿æ³›ä»»åŠ¡ä¸­å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™è¯æ˜äº†å…¶å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶è®¸å¤šæ¨ç†åŸºå‡†æµ‹è¯•é›†ä¸­åœ¨STEMé¢†åŸŸï¼Œä½†LRMsåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡é¢†åŸŸä¸­è¿›è¡Œæ­£ç¡®æ¨ç†çš„èƒ½åŠ›ä»ç„¶è¢«å¿½è§†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>TTT-Bench</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡å››æ¬¾ä¸¤äººç©çš„äº•å­—æ£‹é£æ ¼æ¸¸æˆæ¥è¯„ä¼°LRMsçš„åŸºæœ¬æˆ˜ç•¥ã€ç©ºé—´å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›æ¸¸æˆæ˜¯äººç±»ä»å¹´è½»æ—¶å°±èƒ½è½»æ¾è§£å†³çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„ç¨‹åºåŒ–æ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆå¯éªŒè¯çš„ä¸¤äººæ¸¸æˆé—®é¢˜ï¼Œç”¨äºTTT-Benchã€‚è™½ç„¶è¿™äº›æ¸¸æˆå¯¹äººç±»æ¥è¯´å¾®ä¸è¶³é“ï¼Œä½†å®ƒä»¬éœ€è¦æ¨ç†å¯¹æ‰‹çš„æ„å›¾ä»¥åŠæ£‹ç›˜çš„ç©ºé—´é…ç½®ï¼Œä»¥ç¡®ä¿è·èƒœã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼Œ<strong>å‘ç°é‚£äº›åœ¨ç¡¬æ•°å­¦é—®é¢˜ä¸­è¡¨ç°å‡ºè‰²çš„æ¨¡å‹åœ¨è¿™äº›ç®€å•çš„æ¨ç†æ¸¸æˆä¸­ç»å¸¸å¤±è´¥</strong>ã€‚è¿›ä¸€æ­¥çš„æµ‹è¯•è¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ¨ç†æ¨¡å‹åœ¨TTT-Benchä¸Šçš„å¾—åˆ†å¹³å‡æ¯”MATH 500å’ŒAIME 2024ä½41%å’Œ5%ï¼Œè¾ƒå¤§çš„æ¨¡å‹åœ¨ä½¿ç”¨è¾ƒçŸ­æ¨ç†è½¨è¿¹æ—¶è¡¨ç°æ›´å¥½ï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨ç®€å•å’Œæ–°çš„TTT-Benchä»»åŠ¡çš„é•¿æœŸæˆ˜ç•¥æ¨ç†æƒ…å†µä¸‹è¡¨ç°å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10209v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹æ¨ç†æ¨¡å‹åœ¨åŒ…æ‹¬å¥¥èµ›æ•°å­¦é¢˜ç›®åœ¨å†…çš„å¤šç§ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŸºæœ¬ç­–ç•¥ã€ç©ºé—´åŠé€»è¾‘æ¨ç†æ–¹é¢çš„è¡¨ç°ä»å¾…æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†TTT-BenchåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å››æ¬¾ä¸¤äººç©çš„äº•å­—æ£‹é£æ ¼æ¸¸æˆè¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ã€‚è¯„ä¼°å‘ç°ï¼Œæ“…é•¿æ•°å­¦é—®é¢˜çš„æ¨¡å‹åœ¨è¿™äº›æ¸¸æˆä¸­å¸¸è¡¨ç°ä¸ä½³ï¼Œä¸”åœ¨ç®€å•ä»»åŠ¡ä¸Šæ›´ä¾èµ–çŸ­æœŸç­–ç•¥ã€‚å¤§å‹æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨é•¿è¿œç­–ç•¥ä¸Šä»æœ‰ä¸è¶³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§£å†³å¥¥èµ›æ•°å­¦é¢˜ç›®ã€‚</li>
<li>TTT-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨åŸºæœ¬ç­–ç•¥ã€ç©ºé—´å’Œé€»è¾‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å››æ¬¾äº•å­—æ£‹é£æ ¼æ¸¸æˆè¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼Œè¿™äº›æ¸¸æˆè™½ç„¶å¯¹äººç±»æ¥è¯´ç®€å•ï¼Œä½†éœ€è¦ç†è§£å¯¹æ‰‹æ„å›¾å’Œæ¸¸æˆç©ºé—´é…ç½®ã€‚</li>
<li>æ“…é•¿æ•°å­¦é—®é¢˜çš„æ¨¡å‹åœ¨äº•å­—æ£‹æ¸¸æˆä¸­è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºæ¨¡å‹åœ¨ä¸åŒç±»å‹ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>å¤§å‹æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨é•¿è¿œçš„ç­–ç•¥æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æ¨¡å‹åœ¨è§£å†³ç®€å•ä»»åŠ¡æ—¶æ›´ä¾èµ–çŸ­æœŸç­–ç•¥ï¼Œè€Œéé•¿æœŸè§„åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f00b9f078d2b8471ef70f3feda718058.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-710282092d2f4ee14680c3fb34d3ee28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a051708ed9f49f286318281ca651ba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8158b0554a7552378c808ef613cd2808.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c10b64e1eb70efcd83d62c5149f9f11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-307b900bea61af1e89163685b2755a5e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="D-LiFT-Improving-LLM-based-Decompiler-Backend-via-Code-Quality-driven-Fine-tuning"><a href="#D-LiFT-Improving-LLM-based-Decompiler-Backend-via-Code-Quality-driven-Fine-tuning" class="headerlink" title="D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven   Fine-tuning"></a>D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven   Fine-tuning</h2><p><strong>Authors:Muqi Zou, Hongyu Cai, Hongwei Wu, Zion Leonahenahe Basque, Arslan Khan, Berkay Celik,  Dave,  Tian, Antonio Bianchi,  Ruoyu,  Wang, Dongyan Xu</strong></p>
<p>Decompilers, which reconstruct human-readable source code from binary executables, are vital to many security tasks. Yet, despite recent advances, their output often suffers from syntactic and semantic errors and remains difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals significant limitations, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LiFT, an automated decompiler backend that harnesses and further trains LLMs to improve the quality of decompiled code via reinforcement learning (RL). Unlike prior work that overlooks preserving accuracy, D-LiFT adheres to a key principle for enhancing the quality of decompiled code: \textit{preserving accuracy while improving readability}. Central to D-LiFT, we propose D-SCORE, an integrated quality assessment system to score the decompiled code from multiple aspects. In line with our principle, D-SCORE assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Specifically, D-SCORE first verifies the syntactic and semantic correctness via the compiler and symbolic execution; only if a candidate is deemed accurate, it then evaluates readability using established metrics to compare the LLM output with the original decompiled code. The score will then be fed back to the LLM for fine-tuning. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled functions, as measured by D-SCORE. </p>
<blockquote>
<p>åç¼–è¯‘å™¨å°†ä»äºŒè¿›åˆ¶å¯æ‰§è¡Œæ–‡ä»¶é‡å»ºå‡ºäººç±»å¯è¯»çš„æºä»£ç ï¼Œè¿™å¯¹è®¸å¤šå®‰å…¨ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ€è¿‘æœ‰è¿›å±•ï¼Œå…¶è¾“å‡ºä»ç„¶å¸¸å¸¸å­˜åœ¨è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯ï¼Œéš¾ä»¥é˜…è¯»ã€‚æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œç ”ç©¶è€…å¼€å§‹æ¢ç´¢LLMå¯¹ä¼˜åŒ–åç¼–è¯‘å™¨è¾“å‡ºçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•çš„ç ”ç©¶æ­ç¤ºäº†é‡å¤§å±€é™æ€§ï¼Œä¾‹å¦‚å¼•å…¥æ–°çš„é”™è¯¯å’Œä¾èµ–ä¸å¯é çš„å‡†ç¡®æ€§éªŒè¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†D-LiFTï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¹¶è¿›ä¸€æ­¥è®­ç»ƒLLMé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜åç¼–è¯‘ä»£ç è´¨é‡çš„è‡ªåŠ¨åŒ–åç¼–è¯‘å™¨åç«¯ã€‚ä¸å¿½è§†ä¿æŒå‡†ç¡®æ€§çš„æ—©æœŸå·¥ä½œä¸åŒï¼ŒD-LiFTè‡´åŠ›äºæé«˜åç¼–è¯‘ä»£ç è´¨é‡çš„å…³é”®åŸåˆ™ï¼šåœ¨æé«˜å¯è¯»æ€§çš„åŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚åœ¨D-LiFTä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†D-SCOREï¼Œä¸€ä¸ªç»¼åˆçš„è´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œä»å¤šä¸ªæ–¹é¢å¯¹åç¼–è¯‘ä»£ç è¿›è¡Œè¯„åˆ†ã€‚æ ¹æ®æˆ‘ä»¬çš„åŸåˆ™ï¼ŒD-SCOREå¯¹ä»»ä½•ä¸å‡†ç¡®çš„è¾“å‡ºéƒ½ç»™äºˆä½åˆ†ï¼Œåªå¯¹é€šè¿‡å‡†ç¡®æ€§æ£€æŸ¥çš„ä»£ç çš„readabilityæˆäºˆé«˜åˆ†ã€‚å…·ä½“æ¥è¯´ï¼ŒD-SCOREé¦–å…ˆé€šè¿‡ç¼–è¯‘å™¨å’Œç¬¦å·æ‰§è¡ŒéªŒè¯è¯­æ³•å’Œè¯­ä¹‰çš„æ­£ç¡®æ€§ï¼›åªæœ‰å½“å€™é€‰è€…è¢«è§†ä¸ºå‡†ç¡®æ—¶ï¼Œå®ƒæ‰ä¼šä½¿ç”¨æ—¢å®šçš„åº¦é‡æ ‡å‡†æ¥è¯„ä¼°å¯è¯»æ€§ï¼Œå°†LLMè¾“å‡ºä¸åŸå§‹åç¼–è¯‘ä»£ç è¿›è¡Œæ¯”è¾ƒã€‚ç„¶åï¼Œåˆ†æ•°å°†åé¦ˆåˆ°LLMè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„å®ç°åŸºäºGhidraå’Œä¸€ç³»åˆ—LLMï¼Œæ˜¾è‘—æé«˜äº†coreutilså’Œutil-linuxé¡¹ç›®çš„å‡†ç¡®åç¼–è¯‘ä»£ç çš„è´¨é‡ã€‚ä¸æ²¡æœ‰D-SCOREé©±åŠ¨çš„å¾®è°ƒçš„åŸºçº¿LLMç›¸æ¯”ï¼ŒD-LiFTäº§ç”Ÿçš„åç¼–è¯‘å‡½æ•°æ”¹è¿›äº†55.3%ï¼Œè¿™æ˜¯é€šè¿‡D-SCOREè¡¡é‡çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10125v1">PDF</a> </p>
<p><strong>Summary</strong><br>    åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ½œåŠ›æ”¹è¿›åç¼–è¯‘å™¨çš„è¾“å‡ºè´¨é‡ï¼Œæå‡ä»£ç å¯è¯»æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§è‡ªåŠ¨åŒ–åç¼–è¯‘å™¨åç«¯D-LiFTï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥è®­ç»ƒLLMï¼Œæé«˜åç¼–è¯‘ä»£ç çš„è´¨é‡ã€‚D-LiFTéµå¾ªä¸€ä¸ªå…³é”®åŸåˆ™ï¼šåœ¨æé«˜å¯è¯»æ€§çš„åŒæ—¶ä¿è¯å‡†ç¡®æ€§ã€‚æå‡ºé›†æˆè´¨é‡è¯„ä¼°ç³»ç»ŸD-SCOREï¼Œä»å¤šæ–¹é¢å¯¹åç¼–è¯‘ä»£ç è¿›è¡Œè¯„åˆ†ï¼Œç¡®ä¿å‡†ç¡®æ€§çš„åŒæ—¶è¯„ä¼°å¯è¯»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åç¼–è¯‘å™¨å¯¹äºå®‰å…¨ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†å…¶è¾“å‡ºé€šå¸¸å­˜åœ¨è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯ï¼Œéš¾ä»¥é˜…è¯»ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ”¹è¿›åç¼–è¯‘å™¨è¾“å‡ºæ–¹é¢å±•ç°æ½œåŠ›ï¼Œä½†å­˜åœ¨å¼•å…¥æ–°é”™è¯¯å’Œä¾èµ–ä¸å¯é çš„å‡†ç¡®æ€§éªŒè¯ç­‰å±€é™æ€§ã€‚</li>
<li>D-LiFTæ˜¯ä¸€ç§è‡ªåŠ¨åŒ–åç¼–è¯‘å™¨åç«¯ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒLLMæé«˜åç¼–è¯‘ä»£ç è´¨é‡ï¼Œæ³¨é‡æé«˜å¯è¯»æ€§çš„åŒæ—¶ä¿è¯å‡†ç¡®æ€§ã€‚</li>
<li>D-SCOREæ˜¯é›†æˆè´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºå¤šæ–¹é¢è¯„ä¼°åç¼–è¯‘ä»£ç ï¼Œç¡®ä¿è¾“å‡ºå‡†ç¡®æ€§å¹¶é€šè¿‡ç¼–è¯‘å™¨å’Œç¬¦å·æ‰§è¡ŒéªŒè¯ã€‚</li>
<li>D-SCOREåœ¨å‡†ç¡®æ€§æ£€æŸ¥é€šè¿‡åï¼Œæ‰è¯„ä¼°å¯è¯»æ€§ï¼Œå¹¶ä½¿ç”¨ç°æœ‰åº¦é‡æ ‡å‡†æ¯”è¾ƒLLMè¾“å‡ºä¸åŸå§‹åç¼–è¯‘ä»£ç ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d15815873c9e9149700a4b3a06b6129b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36ff3b04f48c7e37eb875e85e327df60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd1a7bf4fec14b4fcd7fcd35b49c905d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93044a7a226801bc38b14f970866479.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-90de5cd9ba73d7e8bc828d401a22efc8.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  AutoMind Adaptive Knowledgeable Agent for Automated Data Science
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce442293e5f9981fa7bb76deb83ae172.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  HunyuanVideo-HOMA Generic Human-Object Interaction in Multimodal Driven   Human Animation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
