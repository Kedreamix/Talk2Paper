<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  SpectralAR Spectral Autoregressive Visual Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-22c307ff2862f361125a11c7c90838b0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-14-æ›´æ–°"><a href="#2025-06-14-æ›´æ–°" class="headerlink" title="2025-06-14 æ›´æ–°"></a>2025-06-14 æ›´æ–°</h1><h2 id="SpectralAR-Spectral-Autoregressive-Visual-Generation"><a href="#SpectralAR-Spectral-Autoregressive-Visual-Generation" class="headerlink" title="SpectralAR: Spectral Autoregressive Visual Generation"></a>SpectralAR: Spectral Autoregressive Visual Generation</h2><p><strong>Authors:Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Yueqi Duan, Jie Zhou, Jiwen Lu</strong></p>
<p>Autoregressive visual generation has garnered increasing attention due to its scalability and compatibility with other modalities compared with diffusion models. Most existing methods construct visual sequences as spatial patches for autoregressive generation. However, image patches are inherently parallel, contradicting the causal nature of autoregressive modeling. To address this, we propose a Spectral AutoRegressive (SpectralAR) visual generation framework, which realizes causality for visual sequences from the spectral perspective. Specifically, we first transform an image into ordered spectral tokens with Nested Spectral Tokenization, representing lower to higher frequency components. We then perform autoregressive generation in a coarse-to-fine manner with the sequences of spectral tokens. By considering different levels of detail in images, our SpectralAR achieves both sequence causality and token efficiency without bells and whistles. We conduct extensive experiments on ImageNet-1K for image reconstruction and autoregressive generation, and SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project page: <a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a>. </p>
<blockquote>
<p>è‡ªå›å½’è§†è§‰ç”Ÿæˆå› å…¶å¯æ‰©å±•æ€§å’Œä¸å…¶ä»–æ¨¡æ€çš„å…¼å®¹æ€§è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œç›¸æ¯”äºæ‰©æ•£æ¨¡å‹ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•å°†è§†è§‰åºåˆ—æ„å»ºä¸ºç©ºé—´è¡¥ä¸è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚ç„¶è€Œï¼Œå›¾åƒè¡¥ä¸æœ¬è´¨ä¸Šæ˜¯å¹¶è¡Œçš„ï¼Œä¸è‡ªå›å½’æ¨¡å‹çš„å› æœæ€§è´¨ç›¸çŸ›ç›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Spectral AutoRegressiveï¼ˆSpectralARï¼‰è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œä»å…‰è°±è§’åº¦å®ç°äº†è§†è§‰åºåˆ—çš„å› æœæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå°†å›¾åƒé€šè¿‡Nested Spectral Tokenizationè½¬åŒ–ä¸ºæœ‰åºçš„é¢‘è°±æ ‡è®°ç¬¦å·ï¼Œä»£è¡¨ä»ä½åˆ°é«˜çš„é¢‘ç‡åˆ†é‡ã€‚ç„¶åæˆ‘ä»¬åœ¨é¢‘è°±æ ‡è®°åºåˆ—ä¸­ä»¥ç”±ç²—åˆ°ç»†çš„æ–¹å¼æ‰§è¡Œè‡ªå›å½’ç”Ÿæˆã€‚é€šè¿‡è€ƒè™‘å›¾åƒä¸­çš„ä¸åŒå±‚æ¬¡çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬çš„SpectralARæ¡†æ¶åœ¨ä¸æ·»åŠ ä»»ä½•èŠ±å“¨çš„æƒ…å†µä¸‹å®ç°äº†åºåˆ—çš„å› æœæ€§å’Œæ ‡è®°çš„æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ImageNet-1Kä¸Šè¿›è¡Œäº†å¤§é‡çš„å›¾åƒé‡å»ºå’Œè‡ªå›å½’ç”Ÿæˆçš„å®éªŒï¼ŒSpectralARä»…ä½¿ç”¨64ä¸ªæ ‡è®°å’Œ3.1äº¿ä¸ªå‚æ•°å°±å®ç°äº†3.02çš„gFIDã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®ç½‘é¡µï¼š[<a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/]">https://huang-yh.github.io/spectralar/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10962v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a></p>
<p><strong>Summary</strong><br>     è°±è‡ªå›å½’ï¼ˆSpectralARï¼‰è§†è§‰ç”Ÿæˆæ¡†æ¶é€šè¿‡è°±è§†è§’å®ç°äº†è§†è§‰åºåˆ—çš„å› æœæ€§ï¼Œé€šè¿‡åµŒå¥—è°±ä»¤ç‰ŒåŒ–å°†å›¾åƒè½¬æ¢ä¸ºæœ‰åºçš„è°±ä»¤ç‰Œï¼Œè¿›è¡Œç²—ç»†çº§åˆ«çš„è‡ªå›å½’ç”Ÿæˆã€‚åœ¨ImageNet-1Kä¸Šè¿›è¡Œå®éªŒï¼Œå®ç°å›¾åƒé‡å»ºå’Œè‡ªå›å½’ç”Ÿæˆï¼Œå…·æœ‰é«˜æ•ˆåºåˆ—æ€§å’Œæ ‡è®°æ•ˆç‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://huang-yh.github.io/spectralar/">https://huang-yh.github.io/spectralar/</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’è§†è§‰ç”Ÿæˆå› å…¶å¯æ‰©å±•æ€§å’Œä¸å…¶ä»–æ¨¡æ€çš„å…¼å®¹æ€§è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ„å»ºçš„ç©ºé—´è¡¥ä¸ç”¨äºè‡ªå›å½’ç”Ÿæˆä¸è‡ªå›å½’æ¨¡å‹çš„å› æœæ€§è´¨ç›¸çŸ›ç›¾ã€‚</li>
<li>è°±è‡ªå›å½’ï¼ˆSpectralARï¼‰æ¡†æ¶ä»è°±è§’åº¦å®ç°è§†è§‰åºåˆ—çš„å› æœæ€§ã€‚</li>
<li>é€šè¿‡åµŒå¥—è°±ä»¤ç‰ŒåŒ–å°†å›¾åƒè½¬æ¢ä¸ºæœ‰åºçš„è°±ä»¤ç‰Œã€‚</li>
<li>ä»¥ç²—ç»†çº§åˆ«çš„æ–¹å¼è¿›è¡Œè‡ªå›å½’ç”Ÿæˆï¼Œè€ƒè™‘å›¾åƒçš„ä¸åŒå±‚æ¬¡ç»†èŠ‚ã€‚</li>
<li>åœ¨ImageNet-1Kä¸Šè¿›è¡Œå®éªŒï¼ŒSpectralARæ¡†æ¶å®ç°äº†é«˜æ•ˆçš„åºåˆ—æ€§å’Œæ ‡è®°æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f593045d6e96db8496e4cb7139f376a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5d97c749368d00e22063b7b19885512.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-238744431ff7f422ed998ae3b941127e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2ccc14ec6ed220d153354326150eab7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d4ad6dff0330057862e861bb841dc6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26b98dd1561b37605d4b0184f1a9da91.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Physics-informed-Machine-Learning-Analysis-for-Nanoscale-Grain-Mapping-by-Synchrotron-Laue-Microdiffraction"><a href="#Physics-informed-Machine-Learning-Analysis-for-Nanoscale-Grain-Mapping-by-Synchrotron-Laue-Microdiffraction" class="headerlink" title="Physics-informed Machine Learning Analysis for Nanoscale Grain Mapping   by Synchrotron Laue Microdiffraction"></a>Physics-informed Machine Learning Analysis for Nanoscale Grain Mapping   by Synchrotron Laue Microdiffraction</h2><p><strong>Authors:Ka Hung Chan, Xinyue Huang, Nobumichi Tamura, Xian Chen</strong></p>
<p>Understanding the grain morphology, orientation distribution, and crystal structure of nanocrystals is essential for optimizing the mechanical and physical properties of functional materials. Synchrotron X-ray Laue microdiffraction is a powerful technique for characterizing crystal structures and orientation mapping using focused X-rays. However, when grain sizes are smaller than the beam size, mixed peaks in the Laue pattern from neighboring grains limit the resolution of grain morphology mapping. We propose a physics-informed machine learning (PIML) approach that combines a CNN feature extractor with a physics-informed filtering algorithm to overcome the spatial resolution limits of X-rays, achieving nanoscale resolution for grain mapping. Our PIML method successfully resolves the grain size, orientation distribution, and morphology of Au nanocrystals through synchrotron microdiffraction scans, showing good agreement with electron backscatter diffraction results. This PIML-assisted synchrotron microdiffraction analysis can be generalized to other diffraction-based probes, enabling the characterization of nanosized structures with micron-sized probes. </p>
<blockquote>
<p>ç†è§£çº³ç±³æ™¶ä½“çš„æ™¶ç²’å½¢æ€ã€å–å‘åˆ†å¸ƒå’Œæ™¶ä½“ç»“æ„å¯¹äºä¼˜åŒ–åŠŸèƒ½ææ–™çš„æœºæ¢°å’Œç‰©ç†æ€§èƒ½è‡³å…³é‡è¦ã€‚åŒæ­¥è¾å°„Xå°„çº¿åŠ³å„æ˜¾å¾®è¡å°„æ˜¯ä¸€ç§å¼ºå¤§çš„è¡¨å¾æ™¶ä½“ç»“æ„å’Œå–å‘æ˜ å°„çš„æŠ€æœ¯ï¼Œä½¿ç”¨èšç„¦çš„Xå°„çº¿ã€‚ç„¶è€Œï¼Œå½“æ™¶ç²’å°ºå¯¸å°äºå…‰æŸå°ºå¯¸æ—¶ï¼Œæ¥è‡ªç›¸é‚»æ™¶ç²’çš„åŠ³å„å›¾è°±ä¸­çš„æ··åˆå³°ä¼šé™åˆ¶æ™¶ç²’å½¢æ€æ˜ å°„çš„åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œç‰¹å¾æå–å™¨å’Œç‰©ç†ä¿¡æ¯æ»¤æ³¢ç®—æ³•çš„åŸºäºç‰©ç†çŸ¥è¯†çš„æœºå™¨å­¦ä¹ ï¼ˆPIMLï¼‰æ–¹æ³•ï¼Œä»¥å…‹æœXå°„çº¿çš„ç©ºé—´åˆ†è¾¨ç‡é™åˆ¶ï¼Œå®ç°çº³ç±³çº§åˆ†è¾¨ç‡çš„æ™¶ç²’æ˜ å°„ã€‚æˆ‘ä»¬çš„PIMLæ–¹æ³•é€šè¿‡åŒæ­¥è¾å°„æ˜¾å¾®è¡å°„æ‰«ææˆåŠŸè§£æäº†é‡‘çº³ç±³æ™¶ä½“çš„æ™¶ç²’å°ºå¯¸ã€å–å‘åˆ†å¸ƒå’Œå½¢æ€ï¼Œä¸ç”µå­èƒŒæ•£å°„è¡å°„ç»“æœæ˜¾ç¤ºå‡ºè‰¯å¥½çš„ä¸€è‡´æ€§ã€‚è¿™ç§è¾…åŠ©PIMLçš„åŒæ­¥è¾å°„æ˜¾å¾®è¡å°„åˆ†æå¯æ¨å¹¿åˆ°å…¶ä»–åŸºäºè¡å°„çš„æ¢é’ˆï¼Œå®ç°å¯¹å¾®ç±³çº§æ¢é’ˆçš„çº³ç±³çº§ç»“æ„çš„è¡¨å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10937v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç†è§£çº³ç±³æ™¶ä½“çš„æ™¶ç²’å½¢æ€ã€å–å‘åˆ†å¸ƒå’Œæ™¶ä½“ç»“æ„å¯¹äºä¼˜åŒ–åŠŸèƒ½ææ–™çš„æœºæ¢°å’Œç‰©ç†æ€§è´¨è‡³å…³é‡è¦ã€‚åŒæ­¥è¾å°„Xå°„çº¿åŠ³åŸƒå¾®è¡å°„æ˜¯ä¸€ç§åˆ©ç”¨èšç„¦Xå°„çº¿è¡¨å¾æ™¶ä½“ç»“æ„å’Œå–å‘æ˜ å°„çš„å¼ºå¤§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå½“æ™¶ç²’å°ºå¯¸å°äºå…‰æŸå°ºå¯¸æ—¶ï¼Œæ¥è‡ªç›¸é‚»æ™¶ç²’çš„åŠ³åŸƒå›¾æ¡ˆä¸­çš„æ··åˆå³°ä¼šé™åˆ¶æ™¶ç²’å½¢æ€æ˜ å°„çš„åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œç‰¹å¾æå–å™¨å’Œç‰©ç†ä¿¡æ¯æ»¤æ³¢ç®—æ³•çš„ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ï¼ˆPIMLï¼‰æ–¹æ³•ï¼Œä»¥å…‹æœXå°„çº¿çš„ç©ºé—´åˆ†è¾¨ç‡é™åˆ¶ï¼Œå®ç°çº³ç±³çº§åˆ†è¾¨ç‡çš„æ™¶ç²’æ˜ å°„ã€‚æˆ‘ä»¬çš„PIMLæ–¹æ³•æˆåŠŸè§£å†³äº†é€šè¿‡åŒæ­¥åŠ é€Ÿå™¨å¾®è¡å°„æ‰«æè·å¾—çš„Auçº³ç±³æ™¶ä½“çš„æ™¶ç²’å°ºå¯¸ã€å–å‘åˆ†å¸ƒå’Œå½¢æ€ï¼Œä¸ç”µå­èƒŒæ•£å°„è¡å°„ç»“æœå…·æœ‰è‰¯å¥½çš„ä¸€è‡´æ€§ã€‚è¿™ç§PIMLè¾…åŠ©çš„åŒæ­¥è¾å°„å¾®è¡å°„åˆ†æå¯æ¨å¹¿åˆ°å…¶ä»–è¡å°„æ¢é’ˆï¼Œå®ç°å¯¹å¾®ç±³çº§æ¢é’ˆçš„çº³ç±³çº§ç»“æ„çš„è¡¨å¾ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>çº³ç±³æ™¶ä½“çš„æ™¶ç²’å½¢æ€ã€å–å‘åˆ†å¸ƒå’Œæ™¶ä½“ç»“æ„å¯¹åŠŸèƒ½ææ–™çš„æœºæ¢°å’Œç‰©ç†æ€§è´¨ä¼˜åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>åŒæ­¥è¾å°„Xå°„çº¿åŠ³åŸƒå¾®è¡å°„æ˜¯è¡¨å¾æ™¶ä½“ç»“æ„å’Œå–å‘æ˜ å°„çš„å¼ºå¤§æŠ€æœ¯ï¼Œä½†åœ¨å°æ™¶ç²’å°ºå¯¸ä¸‹å­˜åœ¨åˆ†è¾¨ç‡é™åˆ¶ã€‚</li>
<li>æå‡ºäº†ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œç‰¹å¾æå–å™¨å’Œç‰©ç†ä¿¡æ¯æ»¤æ³¢ç®—æ³•çš„ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ï¼ˆPIMLï¼‰æ–¹æ³•ã€‚</li>
<li>PIMLæ–¹æ³•æˆåŠŸè§£å†³é€šè¿‡åŒæ­¥åŠ é€Ÿå™¨å¾®è¡å°„æ‰«æè·å¾—çš„Auçº³ç±³æ™¶ä½“çš„æ™¶ç²’å°ºå¯¸ã€å–å‘åˆ†å¸ƒå’Œå½¢æ€ã€‚</li>
<li>PIMLæ–¹æ³•ä¸ç”µå­èƒŒæ•£å°„è¡å°„ç»“æœä¸€è‡´ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çº³ç±³å°ºåº¦ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>PIMLè¾…åŠ©çš„åŒæ­¥è¾å°„å¾®è¡å°„åˆ†æå¯æ¨å¹¿è‡³å…¶ä»–è¡å°„æŠ€æœ¯ï¼Œæé«˜çº³ç±³çº§ç»“æ„çš„è¡¨å¾èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c8465b623e016215d7eaa42964d6f6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e58d85bef6a4d5e511aa98641192049c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a9cccd24a1f577ec4688173c9c92249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fdb97df8573c135052edd761d0d894c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a0cad35bcf1ed4d074f23b045867b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1b7273db3fada33ac2227b2a823ae2e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Med-URWKV-Pure-RWKV-With-ImageNet-Pre-training-For-Medical-Image-Segmentation"><a href="#Med-URWKV-Pure-RWKV-With-ImageNet-Pre-training-For-Medical-Image-Segmentation" class="headerlink" title="Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image   Segmentation"></a>Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image   Segmentation</h2><p><strong>Authors:Zhenhuan Zhou</strong></p>
<p>Medical image segmentation is a fundamental and key technology in computer-aided diagnosis and treatment. Previous methods can be broadly classified into three categories: convolutional neural network (CNN) based, Transformer based, and hybrid architectures that combine both. However, each of them has its own limitations, such as restricted receptive fields in CNNs or the computational overhead caused by the quadratic complexity of Transformers. Recently, the Receptance Weighted Key Value (RWKV) model has emerged as a promising alternative for various vision tasks, offering strong long-range modeling capabilities with linear computational complexity. Some studies have also adapted RWKV to medical image segmentation tasks, achieving competitive performance. However, most of these studies focus on modifications to the Vision-RWKV (VRWKV) mechanism and train models from scratch, without exploring the potential advantages of leveraging pre-trained VRWKV models for medical image segmentation tasks. In this paper, we propose Med-URWKV, a pure RWKV-based architecture built upon the U-Net framework, which incorporates ImageNet-based pretraining to further explore the potential of RWKV in medical image segmentation tasks. To the best of our knowledge, Med-URWKV is the first pure RWKV segmentation model in the medical field that can directly reuse a large-scale pre-trained VRWKV encoder. Experimental results on seven datasets demonstrate that Med-URWKV achieves comparable or even superior segmentation performance compared to other carefully optimized RWKV models trained from scratch. This validates the effectiveness of using a pretrained VRWKV encoder in enhancing model performance. The codes will be released. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—ä¸­çš„ä¸€é¡¹åŸºæœ¬ä¸”å…³é”®çš„æŠ€æœ¯ã€‚ä¹‹å‰çš„æ–¹æ³•å¯ä»¥å¤§è‡´åˆ†ä¸ºä¸‰ç±»ï¼šåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ã€åŸºäºTransformerçš„ï¼Œä»¥åŠç»“åˆä¸¤è€…çš„æ··åˆæ¶æ„ã€‚ç„¶è€Œï¼Œæ¯ä¸€ç§æ–¹æ³•éƒ½æœ‰å…¶è‡ªèº«çš„å±€é™æ€§ï¼Œä¾‹å¦‚CNNçš„å—é™æ„Ÿå—é‡æˆ–Transformerçš„äºŒæ¬¡å¤æ‚æ€§æ‰€å¸¦æ¥çš„è®¡ç®—å¼€é”€ã€‚æœ€è¿‘ï¼ŒReceptance Weighted Key Valueï¼ˆRWKVï¼‰æ¨¡å‹ä½œä¸ºå„ç§è§†è§‰ä»»åŠ¡çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œå®ƒå…·æœ‰å¼ºå¤§çš„é•¿ç¨‹å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¶ä¸”å…·æœ‰çº¿æ€§çš„è®¡ç®—å¤æ‚æ€§ã€‚ä¸€äº›ç ”ç©¶ä¹Ÿå·²ç»å°†RWKVé€‚åº”äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå¹¶å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹Vision-RWKVï¼ˆVRWKVï¼‰æœºåˆ¶å’Œä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ä¸Šï¼Œè€Œæ²¡æœ‰æ¢ç´¢åˆ©ç”¨é¢„è®­ç»ƒçš„VRWKVæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„æ½œåœ¨ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºU-Netæ¡†æ¶çš„çº¯RWKVæ¶æ„Med-URWKVï¼Œå¹¶ç»“åˆImageNetè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥è¿›ä¸€æ­¥æ¢ç´¢RWKVåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMed-URWKVæ˜¯åŒ»å­¦é¢†åŸŸä¸­é¦–ä¸ªå¯ä»¥ç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒVRWKVç¼–ç å™¨çš„çº¯RWKVåˆ†å‰²æ¨¡å‹ã€‚åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„ç»è¿‡ç²¾å¿ƒä¼˜åŒ–çš„å…¶ä»–RWKVæ¨¡å‹ç›¸æ¯”ï¼ŒMed-URWKVçš„åˆ†å‰²æ€§èƒ½ç›¸å½“ç”šè‡³æ›´å¥½ã€‚è¿™éªŒè¯äº†ä½¿ç”¨é¢„è®­ç»ƒçš„VRWKVç¼–ç å™¨æé«˜æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†è¢«å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10858v1">PDF</a> Preprint Draft, 5 pages. This paper will be updated with a formal   version in the future, Copyright: College of Computer Science, Nankai   University. All rights reserved</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—ä¸­çš„åŸºç¡€æ€§å’Œé‡è¦æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Med-URWKVæ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨çº¯RWKVæ¶æ„ï¼ŒåŸºäºU-Netæ¡†æ¶æ„å»ºï¼Œå¹¶èå…¥ImageNeté¢„è®­ç»ƒæŠ€æœ¯ï¼Œæ—¨åœ¨æ¢ç´¢RWKVåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„RWKVæ¨¡å‹ç›¸æ¯”ï¼ŒMed-URWKVåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„åˆ†å‰²æ€§èƒ½ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼ŒéªŒè¯äº†ä½¿ç”¨é¢„è®­ç»ƒVRWKVç¼–ç å™¨æé«˜æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—ä¸­çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¯åˆ†ä¸ºCNNã€Transformerå’Œæ··åˆæ¶æ„ä¸‰ç±»ï¼Œä½†å„æœ‰å±€é™ã€‚</li>
<li>RWKVæ¨¡å‹å…·æœ‰å¼ºå¤§çš„é•¿ç¨‹å»ºæ¨¡èƒ½åŠ›å’Œçº¿æ€§è®¡ç®—å¤æ‚æ€§ï¼Œåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶å¼€å§‹å°è¯•å°†RWKVæ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå¹¶å–å¾—ç«äº‰æ€§ç»“æœã€‚</li>
<li>Med-URWKVæ˜¯çº¯RWKVæ¶æ„ï¼ŒåŸºäºU-Netæ¡†æ¶æ„å»ºï¼Œèå…¥ImageNeté¢„è®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>Med-URWKVæ˜¯é¦–ä¸ªå¯ç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒVRWKVç¼–ç å™¨çš„åŒ»å­¦é¢†åŸŸçº¯RWKVåˆ†å‰²æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ae914c3ed0eb161d08e503ecbef59089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-593441f44db3f77523d2834c3c618348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85aaf7bc5cf190c111166c9f93c18bed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b58645f852b52ffe7e2feaf2391f96c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches"><a href="#Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches" class="headerlink" title="Generalist Models in Medical Image Segmentation: A Survey and   Performance Comparison with Task-Specific Approaches"></a>Generalist Models in Medical Image Segmentation: A Survey and   Performance Comparison with Task-Specific Approaches</h2><p><strong>Authors:Andrea Moglia, Matteo Leccardi, Matteo Cavicchioli, Alice Maccarini, Marco Marcon, Luca Mainardi, Pietro Cerveri</strong></p>
<p>Following the successful paradigm shift of large language models, leveraging pre-training on a massive corpus of data and fine-tuning on different downstream tasks, generalist models have made their foray into computer vision. The introduction of Segment Anything Model (SAM) set a milestone on segmentation of natural images, inspiring the design of a multitude of architectures for medical image segmentation. In this survey we offer a comprehensive and in-depth investigation on generalist models for medical image segmentation. We start with an introduction on the fundamentals concepts underpinning their development. Then, we provide a taxonomy on the different declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on the recent SAM 2, on other innovative models trained on images alone, and others trained on both text and images. We thoroughly analyze their performances at the level of both primary research and best-in-literature, followed by a rigorous comparison with the state-of-the-art task-specific models. We emphasize the need to address challenges in terms of compliance with regulatory frameworks, privacy and security laws, budget, and trustworthy artificial intelligence (AI). Finally, we share our perspective on future directions concerning synthetic data, early fusion, lessons learnt from generalist models in natural language processing, agentic AI and physical AI, and clinical translation. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„èŒƒå¼æˆåŠŸè½¬å˜ï¼Œé€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¹¶åœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œé€šç”¨æ¨¡å‹å·²ç»æ¸—é€åˆ°è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚Segment Anything Modelï¼ˆSAMï¼‰çš„å¼•å…¥ä¸ºè‡ªç„¶å›¾åƒåˆ†å‰²æ ‘ç«‹äº†é‡Œç¨‹ç¢‘ï¼Œæ¿€å‘äº†å¤šç§åŒ»ç–—å›¾åƒåˆ†å‰²æ¶æ„çš„è®¾è®¡ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å¯¹åŒ»ç–—å›¾åƒåˆ†å‰²çš„é€šç”¨æ¨¡å‹è¿›è¡Œäº†å…¨é¢æ·±å…¥çš„ç ”ç©¶ã€‚é¦–å…ˆä»‹ç»äº†æ”¯æ’‘å®ƒä»¬å‘å±•çš„åŸºæœ¬æ¦‚å¿µã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€å¾®è°ƒã€é€‚é…å™¨ã€æœ€è¿‘çš„SAM 2ä»¥åŠå…¶ä»–å•ç‹¬åœ¨å›¾åƒä¸Šè®­ç»ƒçš„åˆ›æ–°æ¨¡å‹ç­‰æ–¹é¢ï¼Œå¯¹SAMçš„ä¸åŒå€¾å‘è¿›è¡Œäº†åˆ†ç±»ï¼Œä»¥åŠå…¶ä»–åŒæ—¶è®­ç»ƒæ–‡æœ¬å’Œå›¾åƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å…¨é¢åˆ†æäº†å®ƒä»¬åœ¨åˆçº§ç ”ç©¶å’Œæ–‡çŒ®æœ€ä½³æ°´å¹³ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ä¸æœ€æ–°çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼æ¯”è¾ƒã€‚æˆ‘ä»¬å¼ºè°ƒäº†éœ€è¦è§£å†³åˆè§„æ€§æ¡†æ¶ã€éšç§å’Œå®‰å…¨æ³•å¾‹ã€é¢„ç®—å’Œå¯ä¿¡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†äº«äº†å…³äºåˆæˆæ•°æ®ã€æ—©æœŸèåˆã€ä»è‡ªç„¶è¯­è¨€å¤„ç†çš„é€šç”¨æ¨¡å‹ä¸­å¸å–çš„æ•™è®­ã€æ™ºèƒ½ä¸»ä½“å’Œç‰©ç†AIä»¥åŠä¸´åºŠç¿»è¯‘çš„æœªæ¥æ–¹å‘çš„çœ‹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10825v1">PDF</a> 132 pages, 26 figures, 23 tables. Andrea Moglia and Matteo Leccardi   are equally contributing authors</p>
<p><strong>Summary</strong><br>     é€šç”¨æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„åº”ç”¨ç ”ç©¶ç»¼è¿°ã€‚ä»‹ç»äº†é€šç”¨æ¨¡å‹çš„åŸºæœ¬åŸç†å’Œåˆ†ç±»ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€å¾®è°ƒã€é€‚é…å™¨ç­‰æ–¹æ³•ï¼Œå¯¹æœ€è¿‘SAM 2å’Œå…¶ä»–åˆ›æ–°æ¨¡å‹è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚å¯¹æ¯”äº†æœ€æ–°ä»»åŠ¡ç‰¹å®šæ¨¡å‹ï¼Œå¼ºè°ƒäº†åˆè§„æ€§ã€éšç§å’Œå®‰å…¨ã€é¢„ç®—å’Œå¯ä¿¡äººå·¥æ™ºèƒ½ç­‰æŒ‘æˆ˜ã€‚å±•æœ›æœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå¼€å§‹åº”ç”¨ï¼ŒåŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®å’Œå¾®è°ƒä¸‹æ¸¸ä»»åŠ¡çš„æˆåŠŸæ¨¡å¼ã€‚</li>
<li>Segment Anything Model (SAM)ä¸ºè‡ªç„¶å›¾åƒåˆ†å‰²æ ‘ç«‹äº†é‡Œç¨‹ç¢‘ï¼Œå¯å‘äº†å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„çš„è®¾è®¡ã€‚</li>
<li>ç»¼è¿°ä»‹ç»äº†é€šç”¨æ¨¡å‹çš„åŸºæœ¬åŸç†å’Œåˆ†ç±»ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒç­‰æ–¹æ³•ã€‚</li>
<li>å¯¹SAM 2å’Œå…¶ä»–åˆ›æ–°æ¨¡å‹è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼ŒåŒ…æ‹¬åªé’ˆå¯¹å›¾åƒè®­ç»ƒçš„æ¨¡å‹ä»¥åŠåŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒçš„æ¨¡å‹ã€‚</li>
<li>ç»¼è¿°å¯¹æ¯”äº†æœ€æ–°ä»»åŠ¡ç‰¹å®šæ¨¡å‹ï¼Œå¹¶å¼ºè°ƒäº†æ€§èƒ½æŒ‘æˆ˜ã€‚</li>
<li>æŒ‡å‡ºéœ€è¦è§£å†³åˆè§„æ€§ã€éšç§å’Œå®‰å…¨ã€é¢„ç®—ä»¥åŠå¯ä¿¡äººå·¥æ™ºèƒ½ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bbd94bf513f6f2d305d0ffc2eca684d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95307b0eac671eb54b1fc8800f1922e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ffd82c8157d7e5d12b1568ce0e21a4d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Deformable-Image-Registration-with-Structural-Nonparametric-Smoothing"><a href="#Unsupervised-Deformable-Image-Registration-with-Structural-Nonparametric-Smoothing" class="headerlink" title="Unsupervised Deformable Image Registration with Structural Nonparametric   Smoothing"></a>Unsupervised Deformable Image Registration with Structural Nonparametric   Smoothing</h2><p><strong>Authors:Hang Zhang, Xiang Chen, Renjiu Hu, Rongguang Wang, Jinwei Zhang, Min Liu, Yaonan Wang, Gaolei Li, Xinxing Cheng, Jinming Duan</strong></p>
<p>Learning-based deformable image registration (DIR) accelerates alignment by amortizing traditional optimization via neural networks. Label supervision further enhances accuracy, enabling efficient and precise nonlinear alignment of unseen scans. However, images with sparse features amid large smooth regions, such as retinal vessels, introduce aperture and large-displacement challenges that unsupervised DIR methods struggle to address. This limitation occurs because neural networks predict deformation fields in a single forward pass, leaving fields unconstrained post-training and shifting the regularization burden entirely to network weights. To address these issues, we introduce SmoothProper, a plug-and-play neural module enforcing smoothness and promoting message passing within the networkâ€™s forward pass. By integrating a duality-based optimization layer with tailored interaction terms, SmoothProper efficiently propagates flow signals across spatial locations, enforces smoothness, and preserves structural consistency. It is model-agnostic, seamlessly integrates into existing registration frameworks with minimal parameter overhead, and eliminates regularizer hyperparameter tuning. Preliminary results on a retinal vessel dataset exhibiting aperture and large-displacement challenges demonstrate our method reduces registration error to 1.88 pixels on 2912x2912 images, marking the first unsupervised DIR approach to effectively address both challenges. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/tinymilky/SmoothProper">https://github.com/tinymilky/SmoothProper</a>. </p>
<blockquote>
<p>åŸºäºå­¦ä¹ çš„å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰é€šè¿‡ç¥ç»ç½‘ç»œæ‘Šé”€ä¼ ç»Ÿä¼˜åŒ–æ¥åŠ é€Ÿå¯¹é½ã€‚æ ‡ç­¾ç›‘ç£è¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ï¼Œå®ç°å¯¹æœªè§æ‰«æçš„é«˜æ•ˆä¸”ç²¾ç¡®çš„éçº¿æ€§å¯¹é½ã€‚ç„¶è€Œï¼Œåœ¨å¤§å‹å¹³æ»‘åŒºåŸŸä¸­ç‰¹å¾ç¨€ç–çš„å›¾åƒï¼ˆä¾‹å¦‚è§†ç½‘è†œè¡€ç®¡ï¼‰å¼•å…¥äº†å­”å¾„å’Œå¤§ä½ç§»æŒ‘æˆ˜ï¼Œæ— ç›‘ç£DIRæ–¹æ³•éš¾ä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¿™ç§å±€é™æ€§ä¹‹æ‰€ä»¥ä¼šå‡ºç°ï¼Œæ˜¯å› ä¸ºç¥ç»ç½‘ç»œåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­é¢„æµ‹å˜å½¢åœºï¼Œåœ¨è®­ç»ƒåç•™ä¸‹æœªçº¦æŸçš„åœºï¼Œå¹¶å°†æ­£åˆ™åŒ–è´Ÿæ‹…å®Œå…¨è½¬ç§»åˆ°ç½‘ç»œæƒé‡ä¸Šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SmoothProperï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œå¯å¼ºåˆ¶å®æ–½å¹³æ»‘å¹¶ä¿ƒè¿›ç½‘ç»œå‰å‘ä¼ é€’ä¸­çš„æ¶ˆæ¯ä¼ é€’ã€‚é€šè¿‡é›†æˆåŸºäºåŒåŸºä¼˜åŒ–çš„å±‚ä»¥åŠé‡èº«å®šåˆ¶çš„äº¤äº’é¡¹ï¼ŒSmoothProperå¯ä»¥æœ‰æ•ˆåœ°è·¨ç©ºé—´ä½ç½®ä¼ æ’­æµåŠ¨ä¿¡å·ï¼Œå¼ºåˆ¶å®æ–½å¹³æ»‘æ€§å¹¶ä¿æŒç»“æ„ä¸€è‡´æ€§ã€‚å®ƒæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œæ— ç¼é›†æˆåˆ°ç°æœ‰çš„é…å‡†æ¡†æ¶ä¸­ï¼Œå…·æœ‰æœ€å°‘çš„å‚æ•°å¼€é”€ï¼Œå¹¶æ¶ˆé™¤äº†æ­£åˆ™åŒ–è¶…å‚æ•°è°ƒæ•´ã€‚åœ¨æ˜¾ç¤ºå­”å¾„å’Œå¤§ä½ç§»æŒ‘æˆ˜çš„è§†ç½‘è†œè¡€ç®¡æ•°æ®é›†ä¸Šçš„åˆæ­¥ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†é…å‡†è¯¯å·®å‡å°‘åˆ°1.88åƒç´ ï¼ˆåœ¨2912x2912å›¾åƒä¸Šï¼‰ï¼Œæ ‡å¿—ç€ç¬¬ä¸€ä¸ªæœ‰æ•ˆåº”å¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜çš„æ— ç›‘ç£DIRæ–¹æ³•ã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tinymilky/SmoothProper">https://github.com/tinymilky/SmoothProper</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10813v1">PDF</a> Accepted for publication at Information Processing in Medical Imaging   (IPMI) 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå­¦ä¹ çš„å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰é€šè¿‡ç¥ç»ç½‘ç»œåŠ é€Ÿå¯¹é½ï¼Œæ ‡ç­¾ç›‘ç£æé«˜äº†å‡†ç¡®æ€§ï¼Œå®ç°äº†æœªè§æ‰«æçš„é«˜æ•ˆä¸”ç²¾ç¡®çš„éçº¿æ€§å¯¹é½ã€‚ç„¶è€Œï¼Œå¯¹äºå›¾åƒä¸­ç¨€ç–ç‰¹å¾å’Œå¤§å¹³æ»‘åŒºåŸŸï¼ˆå¦‚è§†ç½‘è†œè¡€ç®¡ï¼‰çš„æƒ…å†µï¼Œå­˜åœ¨å­”å¾„å’Œå¤§ä½ç§»æŒ‘æˆ˜ï¼Œæ— ç›‘ç£DIRæ–¹æ³•éš¾ä»¥è§£å†³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºSmoothProperï¼Œä¸€ä¸ªå³æ’å³ç”¨çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œå¼ºåˆ¶å®æ–½å¹³æ»‘å¹¶ä¿ƒè¿›ç½‘ç»œå‰å‘ä¼ é€’ä¸­çš„æ¶ˆæ¯ä¼ é€’ã€‚é€šè¿‡é›†æˆåŸºäºåŒä¼˜åŒ–å±‚çš„å®šåˆ¶äº¤äº’æœ¯è¯­ï¼ŒSmoothProperæœ‰æ•ˆåœ°ä¼ æ’­æµä¿¡å·ï¼Œå¼ºåˆ¶æ‰§è¡Œå¹³æ»‘æ“ä½œå¹¶ä¿ç•™ç»“æ„ä¸€è‡´æ€§ã€‚å®ƒæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é…å‡†æ¡†æ¶ä¸­ï¼Œå…·æœ‰æœ€å°çš„å‚æ•°å¼€é”€ï¼Œå¹¶æ¶ˆé™¤äº†æ­£åˆ™åŒ–è¶…å‚æ•°è°ƒæ•´ã€‚åˆæ­¥ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£å†³å­”å¾„å’Œå¤§ä½ç§»æŒ‘æˆ˜çš„è§†ç½‘è†œè¡€ç®¡æ•°æ®é›†ä¸Šï¼Œå°†æ³¨å†Œè¯¯å·®å‡å°‘åˆ°2912x2912å›¾åƒçš„1.88åƒç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ å‹å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰é€šè¿‡ç¥ç»ç½‘ç»œåŠ é€Ÿå›¾åƒå¯¹é½è¿‡ç¨‹ã€‚</li>
<li>æ ‡ç­¾ç›‘ç£è¿›ä¸€æ­¥æé«˜é…å‡†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>åœ¨å…·æœ‰ç¨€ç–ç‰¹å¾å’Œå¤§å¹³æ»‘åŒºåŸŸçš„å›¾åƒä¸­ï¼Œå­˜åœ¨å­”å¾„å’Œå¤§ä½ç§»çš„é…å‡†æŒ‘æˆ˜ã€‚</li>
<li>æ— ç›‘ç£DIRæ–¹æ³•åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>SmoothProperæ˜¯ä¸€ä¸ªæ–°çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œå¼ºåˆ¶å®æ–½å¹³æ»‘å¹¶ä¿ƒè¿›ç½‘ç»œå†…çš„æ¶ˆæ¯ä¼ é€’ã€‚</li>
<li>SmoothProperé€šè¿‡é›†æˆåŒä¼˜åŒ–å±‚å’Œå®šåˆ¶äº¤äº’æœ¯è¯­æ¥æœ‰æ•ˆä¼ æ’­æµä¿¡å·ã€å¼ºåˆ¶æ‰§è¡Œå¹³æ»‘æ“ä½œå¹¶ä¿ç•™ç»“æ„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-060f7a993b6ca2f36cad806b34855b49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cca0ea4c3c56281028eb69c6933d56a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c1925c1924277a72edc6453ace226ee.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Modality-AGnostic-Image-Cascade-MAGIC-for-Multi-Modality-Cardiac-Substructure-Segmentation"><a href="#Modality-AGnostic-Image-Cascade-MAGIC-for-Multi-Modality-Cardiac-Substructure-Segmentation" class="headerlink" title="Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac   Substructure Segmentation"></a>Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac   Substructure Segmentation</h2><p><strong>Authors:Nicholas Summerfield, Qisheng He, Alex Kuo, Ahmed I. Ghanem, Simeng Zhu, Chase Ruff, Joshua Pan, Anudeep Kumar, Prashant Nagpal, Jiwei Zhao, Ming Dong, Carri K. Glide-Hurst</strong></p>
<p>Cardiac substructures are essential in thoracic radiation therapy planning to minimize risk of radiation-induced heart disease. Deep learning (DL) offers efficient methods to reduce contouring burden but lacks generalizability across different modalities and overlapping structures. This work introduces and validates a Modality-AGnostic Image Cascade (MAGIC) for comprehensive and multi-modal cardiac substructure segmentation. MAGIC is implemented through replicated encoding and decoding branches of an nnU-Net-based, U-shaped backbone conserving the function of a single model. Twenty cardiac substructures (heart, chambers, great vessels (GVs), valves, coronary arteries (CAs), and conduction nodes) from simulation CT (Sim-CT), low-field MR-Linac, and cardiac CT angiography (CCTA) modalities were manually delineated and used to train (n&#x3D;76), validate (n&#x3D;15), and test (n&#x3D;30) MAGIC. Twelve comparison models (four segmentation subgroups across three modalities) were equivalently trained. All methods were compared for training efficiency and against reference contours using the Dice Similarity Coefficient (DSC) and two-tailed Wilcoxon Signed-Rank test (threshold, p&lt;0.05). Average DSC scores were 0.75(0.16) for Sim-CT, 0.68(0.21) for MR-Linac, and 0.80(0.16) for CCTA. MAGIC outperforms the comparison in 57% of cases, with limited statistical differences. MAGIC offers an effective and accurate segmentation solution that is lightweight and capable of segmenting multiple modalities and overlapping structures in a single model. MAGIC further enables clinical implementation by simplifying the computational requirements and offering unparalleled flexibility for clinical settings. </p>
<blockquote>
<p>å¿ƒè„å­ç»“æ„åœ¨èƒ¸éƒ¨æ”¾å°„æ²»ç–—è®¡åˆ’ä¸­è‡³å…³é‡è¦ï¼Œæ—¨åœ¨æœ€å°åŒ–è¾å°„è¯±å‘å¿ƒè„ç—…çš„é£é™©ã€‚æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•æ¥å‡å°‘è½®å»“ç»˜åˆ¶çš„å·¥ä½œé‡ï¼Œä½†ç¼ºä¹è·¨ä¸åŒæ¨¡æ€å’Œé‡å ç»“æ„çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä»‹ç»å¹¶éªŒè¯äº†ä¸€ç§æ¨¡æ€æ— å…³çš„å›¾åƒçº§è”ï¼ˆMAGICï¼‰æŠ€æœ¯ï¼Œç”¨äºå…¨é¢å’Œå¤šæ¨¡æ€å¿ƒè„å­ç»“æ„åˆ†å‰²ã€‚MAGICé€šè¿‡å¤åˆ¶ç¼–ç å’Œè§£ç åˆ†æ”¯å®ç°ï¼ŒåŸºäºnnU-Netçš„Uå½¢ä¸»å¹²ä¿ç•™å•ä¸ªæ¨¡å‹çš„åŠŸèƒ½ã€‚ä»æ¨¡æ‹ŸCTï¼ˆSim-CTï¼‰ã€ä½åœºMR-Linacå’Œå¿ƒè„CTè¡€ç®¡é€ å½±ï¼ˆCCTAï¼‰æ¨¡æ€ä¸­æ‰‹åŠ¨ç•Œå®šäºŒåä¸ªå¿ƒè„å­ç»“æ„ï¼ˆå¿ƒè„ã€å¿ƒæˆ¿ã€å¤§è¡€ç®¡ï¼ˆGVsï¼‰ã€ç“£è†œã€å† çŠ¶åŠ¨è„‰ï¼ˆCAsï¼‰å’Œä¼ å¯¼èŠ‚ç‚¹ï¼‰ï¼Œç”¨äºè®­ç»ƒï¼ˆn&#x3D;76ï¼‰ã€éªŒè¯ï¼ˆn&#x3D;15ï¼‰å’Œæµ‹è¯•ï¼ˆn&#x3D;30ï¼‰MAGICã€‚ç­‰æ•ˆè®­ç»ƒäº†åäºŒä¸ªå¯¹æ¯”æ¨¡å‹ï¼ˆä¸‰ä¸ªæ¨¡æ€ä¸­çš„å››ä¸ªåˆ†å‰²å°ç»„ï¼‰ã€‚æ‰€æœ‰æ–¹æ³•å‡æ¯”è¾ƒäº†è®­ç»ƒæ•ˆç‡å’Œå‚ç…§è½®å»“ï¼Œä½¿ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å’ŒåŒä¾§Wilcoxonç¬¦å·ç§©æ£€éªŒï¼ˆé˜ˆå€¼p&lt;0.05ï¼‰ã€‚Sim-CTçš„å¹³å‡DSCå¾—åˆ†ä¸º0.75ï¼ˆ0.16ï¼‰ï¼ŒMR-Linacä¸º0.68ï¼ˆ0.21ï¼‰ï¼ŒCCTAä¸º0.80ï¼ˆ0.16ï¼‰ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒMAGICçš„è¡¨ç°ä¼˜äºå¯¹æ¯”æ¨¡å‹ï¼Œä¸”å·®å¼‚åœ¨ç»Ÿè®¡å­¦ä¸Šæœ‰é™ã€‚MAGICæä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”å‡†ç¡®çš„åˆ†å‰²è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆè½»å·§ä¸”èƒ½å¤Ÿåœ¨å•ä¸ªæ¨¡å‹ä¸­åˆ†å‰²å¤šç§æ¨¡æ€å’Œé‡å ç»“æ„ã€‚æ­¤å¤–ï¼ŒMAGICé€šè¿‡ç®€åŒ–è®¡ç®—è¦æ±‚å¹¶æä¾›å‰æ‰€æœªæœ‰çš„ä¸´åºŠçµæ´»æ€§ï¼Œä»è€Œå®ç°äº†ä¸´åºŠå®æ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10797v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶å¼•å…¥å¹¶éªŒè¯äº†ä¸€ç§è·¨æ¨¡æ€çš„å¿ƒè„äºšç»“æ„åˆ†å‰²æ–¹æ³•â€”â€”Modality-AGnostic Image Cascadeï¼ˆMAGICï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å•ä¸€æ¨¡å‹å®ç°å¤šæ¨¡æ€å¿ƒè„äºšç»“æ„åˆ†å‰²ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œåˆ†å‰²å‡†ç¡®æ€§ï¼Œæœ‰åŠ©äºå‡å°‘è¾å°„ç–—æ³•ä¸­å¿ƒè„ç–¾ç—…çš„é£é™©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè„äºšç»“æ„åœ¨èƒ¸éƒ¨æ”¾å°„æ²»ç–—è®¡åˆ’ä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼Œæœ‰åŠ©äºå‡å°‘è¾å°„å¼•å‘çš„å¿ƒè„ç–¾ç—…é£é™©ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å¿ƒè„äºšç»“æ„åˆ†å‰²ä¸­å…·æœ‰æ•ˆç‡ï¼Œä½†ç¼ºä¹è·¨ä¸åŒæ¨¡æ€å’Œé‡å ç»“æ„çš„é€šç”¨æ€§ã€‚</li>
<li>MAGICæ–¹æ³•é€šè¿‡å•ä¸€æ¨¡å‹å®ç°å¤šæ¨¡æ€å¿ƒè„äºšç»“æ„åˆ†å‰²ï¼Œæé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>MAGICé€šè¿‡å¼•å…¥å¤åˆ¶ç¼–ç å’Œè§£ç åˆ†æ”¯çš„nnU-NetåŸºUå‹ä¸»å¹²ç½‘ç»œå®ç°ã€‚</li>
<li>åœ¨æ¨¡æ‹ŸCTã€ä½åœºMR-Linacå’Œå¿ƒè„CTè¡€ç®¡é€ å½±ï¼ˆCCTAï¼‰ä¸‰ç§æ¨¡æ€ä¸‹å¯¹MAGICè¿›è¡Œäº†è®­ç»ƒå’Œæµ‹è¯•ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒMAGICåœ¨å¤šæ•°æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ï¼Œå¹¶ä¸”åœ¨ç»Ÿè®¡ä¸Šæ— æ˜¾è‘—å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1463df2a1550cdc6eacf19cc69224596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1524f83e54a5d6bb10c14f94f32f5758.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Stroke-based-Cyclic-Amplifier-Image-Super-Resolution-at-Arbitrary-Ultra-Large-Scales"><a href="#Stroke-based-Cyclic-Amplifier-Image-Super-Resolution-at-Arbitrary-Ultra-Large-Scales" class="headerlink" title="Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary   Ultra-Large Scales"></a>Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary   Ultra-Large Scales</h2><p><strong>Authors:Wenhao Guo, Peng Lu, Xujun Peng, Zhaoran Zhao, Sheng Li</strong></p>
<p>Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience a significant performance decline when the upsampling factor exceeds the range covered by the training data, introducing substantial blurring. To address this issue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for ultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier, which decomposes the image into a series of strokes represented as vector graphics for magnification. Then, the detail completion module also restores missing details, ensuring high-fidelity image reconstruction. Our cyclic strategy achieves ultra-large upsampling by iteratively refining details with this unified SbCA model, trained only once for all, while keeping sub-scales within the training range. Our approach effectively addresses the distribution drift issue and eliminates artifacts, noise and blurring, producing high-quality, high-resolution super-resolved images. Experimental validations on both synthetic and real-world datasets demonstrate that our approach significantly outperforms existing methods in ultra-large upsampling tasks (e.g. $\times100$), delivering visual quality far superior to state-of-the-art techniques. </p>
<blockquote>
<p>å…ˆå‰çš„ä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆASISRï¼‰æ–¹æ³•ï¼Œåœ¨æ”¾å¤§å€æ•°è¶…è¿‡è®­ç»ƒæ•°æ®è¦†ç›–çš„èŒƒå›´æ—¶ï¼Œå¸¸å¸¸ä¼šé‡åˆ°æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œå¯¼è‡´å›¾åƒå‡ºç°ä¸¥é‡çš„æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹è¶…å¤§å‹æ”¾å¤§ä»»åŠ¡çš„ç»Ÿä¸€æ¨¡å‹â€”â€”åŸºäºæçº¿çš„å¾ªç¯æ”¾å¤§å™¨ï¼ˆSbCAï¼‰ã€‚SbCAçš„å…³é”®æ˜¯æçº¿å‘é‡æ”¾å¤§å™¨ï¼Œå®ƒå°†å›¾åƒåˆ†è§£æˆä¸€ç³»åˆ—æçº¿çŸ¢é‡å›¾å½¢æ¥è¿›è¡Œæ”¾å¤§ã€‚ç„¶åï¼Œç»†èŠ‚å®Œæˆæ¨¡å—æ¢å¤ç¼ºå¤±çš„ç»†èŠ‚ï¼Œç¡®ä¿é«˜ä¿çœŸåº¦çš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬çš„å¾ªç¯ç­–ç•¥é€šè¿‡è¿­ä»£ç»†åŒ–ç»†èŠ‚æ¥å®ç°è¶…å¤§å‹æ”¾å¤§ï¼Œåªéœ€è®­ç»ƒä¸€æ¬¡è¿™ä¸ªç»Ÿä¸€çš„SbCAæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå­å°ºåº¦åœ¨è®­ç»ƒèŒƒå›´å†…ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†åˆ†å¸ƒæ¼‚ç§»é—®é¢˜ï¼Œæ¶ˆé™¤äº†ä¼ªå½±ã€å™ªå£°å’Œæ¨¡ç³Šï¼Œç”Ÿæˆé«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡çš„è¶…åˆ†è¾¨ç‡å›¾åƒã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¶…å¤§å‹æ”¾å¤§ä»»åŠ¡ï¼ˆä¾‹å¦‚Ã—100ï¼‰ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†è¿œè¶…æœ€æ–°æŠ€æœ¯çš„è§†è§‰è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10774v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ¢é‡æ”¾å¤§çš„ç»Ÿä¸€æ¨¡å‹Stroke-based Cyclic Amplifierï¼ˆSbCAï¼‰ï¼Œç”¨äºè¶…åˆ†è¾¨ç‡æ”¾å¤§ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯è¶…å¤§å‹æ”¾å¤§ä»»åŠ¡ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåˆ†è§£å›¾åƒå¹¶ç”ŸæˆçŸ¢é‡å›¾å½¢è¿›è¡Œæ”¾å¤§ï¼ŒåŒæ—¶ä¿®å¤ç¼ºå¤±çš„ç»†èŠ‚ï¼Œä¿è¯é«˜ä¿çœŸåº¦çš„å›¾åƒé‡å»ºã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†åˆæˆæ•°æ®é›†å’Œå®é™…æ•°æ®é›†æ—¶è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ˜¯åœ¨è¶…å¤§å€ç‡ä¸Šé‡‡æ ·ä»»åŠ¡ä¸­è¿œè¶…ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡è§†è§‰è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SbCAæ¨¡å‹è§£å†³äº†ä¹‹å‰æ–¹æ³•åœ¨å¤„ç†è¶…è¿‡è®­ç»ƒæ•°æ®èŒƒå›´çš„ä¸Šé‡‡æ ·å› å­æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>SbCAæ¨¡å‹é‡‡ç”¨çŸ¢é‡æ”¾å¤§çš„æ–¹å¼åˆ†è§£å›¾åƒï¼Œå¹¶é‡‡ç”¨ç»†èŠ‚è¡¥å…¨æ¨¡å—æ¢å¤ç¼ºå¤±ç»†èŠ‚ã€‚</li>
<li>å¾ªç¯ç­–ç•¥é€šè¿‡è¿­ä»£ä¼˜åŒ–ç»†èŠ‚å®ç°äº†è¶…å¤§å‹ä¸Šé‡‡æ ·ä»»åŠ¡çš„é«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºã€‚</li>
<li>SbCAæ¨¡å‹ä»…è¿›è¡Œä¸€æ¬¡è®­ç»ƒï¼Œä¾¿å¯ä»¥é€‚åº”å„ç§å°ºåº¦çš„æ”¾å¤§ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹è§£å†³äº†åˆ†å¸ƒæ¼‚ç§»é—®é¢˜ï¼Œæ¶ˆé™¤äº†ä¼ªå½±ã€å™ªå£°å’Œæ¨¡ç³Šç°è±¡ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºSbCAæ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…å¤§å€ç‡ä¸Šé‡‡æ ·ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd569ee24cfaa7d4716c4e2d2e54e823.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce368df2f3203d9db42e9103740a6ab9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-562fab3b2c6dcd2ed8ce005302bc5bb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fac00f895d18700fea26dc3c42c7c566.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ConStyX-Content-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation"><a href="#ConStyX-Content-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="ConStyX: Content Style Augmentation for Generalizable Medical Image   Segmentation"></a>ConStyX: Content Style Augmentation for Generalizable Medical Image   Segmentation</h2><p><strong>Authors:Xi Chen, Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane</strong></p>
<p>Medical images are usually collected from multiple domains, leading to domain shifts that impair the performance of medical image segmentation models. Domain Generalization (DG) aims to address this issue by training a robust model with strong generalizability. Recently, numerous domain randomization-based DG methods have been proposed. However, these methods suffer from the following limitations: 1) constrained efficiency of domain randomization due to their exclusive dependence on image style perturbation, and 2) neglect of the adverse effects of over-augmented images on model training. To address these issues, we propose a novel domain randomization-based DG method, called content style augmentation (ConStyX), for generalizable medical image segmentation. Specifically, ConStyX 1) augments the content and style of training data, allowing the augmented training data to better cover a wider range of data domains, and 2) leverages well-augmented features while mitigating the negative effects of over-augmented features during model training. Extensive experiments across multiple domains demonstrate that our ConStyX achieves superior generalization performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jwxsp1/ConStyX">https://github.com/jwxsp1/ConStyX</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒé€šå¸¸æ¥è‡ªå¤šä¸ªé¢†åŸŸï¼Œå¯¼è‡´é¢†åŸŸåç§»ï¼Œä»è€Œå½±å“åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚é¢†åŸŸæ³›åŒ–ï¼ˆDGï¼‰æ—¨åœ¨é€šè¿‡è®­ç»ƒå…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æœ€è¿‘ï¼Œå·²ç»æå‡ºäº†è®¸å¤šåŸºäºé¢†åŸŸéšæœºåŒ–çš„DGæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š1ï¼‰ç”±äºå®ƒä»¬å¯¹å›¾åƒé£æ ¼æ‰°åŠ¨çš„ä¸“å±ä¾èµ–æ€§ï¼Œé¢†åŸŸéšæœºåŒ–çš„æ•ˆç‡å—é™ï¼›2ï¼‰å¿½è§†äº†è¿‡åº¦å¢å¼ºå›¾åƒå¯¹æ¨¡å‹è®­ç»ƒçš„è´Ÿé¢å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºé¢†åŸŸéšæœºåŒ–çš„DGæ–¹æ³•ï¼Œç§°ä¸ºå†…å®¹é£æ ¼å¢å¼ºï¼ˆConStyXï¼‰ï¼Œç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼ŒConStyX 1ï¼‰å¢å¼ºè®­ç»ƒæ•°æ®çš„å†…å®¹å’Œé£æ ¼ï¼Œä½¿å¢å¼ºåçš„è®­ç»ƒæ•°æ®æ›´å¥½åœ°è¦†ç›–æ›´å¹¿æ³›çš„æ•°æ®é¢†åŸŸï¼›2ï¼‰åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨å¢å¼ºè‰¯å¥½çš„ç‰¹å¾ï¼ŒåŒæ—¶å‡è½»è¿‡åº¦å¢å¼ºç‰¹å¾çš„è´Ÿé¢å½±å“ã€‚åœ¨å¤šä¸ªé¢†åŸŸçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ConStyXå®ç°äº†å“è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jwxsp1/ConStyX%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jwxsp1/ConStyXè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10675v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒå¤šæºåŸŸé‡‡é›†å¯¼è‡´çš„åŸŸåç§»ä¼šå½±å“åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºé€šè¿‡åŸŸæ³›åŒ–ï¼ˆDomain Generalizationï¼ŒDGï¼‰è®­ç»ƒå…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹ã€‚å°½ç®¡ç°æœ‰è®¸å¤šåŸºäºåŸŸéšæœºåŒ–çš„DGæ–¹æ³•ï¼Œä½†å®ƒä»¬å­˜åœ¨å±€é™æ€§ï¼š1ï¼‰ä¾èµ–äºå›¾åƒé£æ ¼æ‰°åŠ¨ï¼Œæ•ˆç‡å—é™ï¼›2ï¼‰å¿½è§†è¿‡åº¦å¢å¼ºå›¾åƒå¯¹æ¨¡å‹è®­ç»ƒçš„è´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹åŸºäºåŸŸéšæœºåŒ–çš„DGæ–¹æ³•â€”â€”å†…å®¹é£æ ¼å¢å¼ºï¼ˆConStyXï¼‰ï¼Œç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚ConStyXæ—¢èƒ½å¢å¼ºè®­ç»ƒæ•°æ®çš„å†…å®¹å’Œé£æ ¼ï¼Œä½¿å¢å¼ºæ•°æ®æ›´å¥½åœ°è¦†ç›–æ›´å¹¿æ³›çš„æ•°æ®åŸŸï¼Œåˆèƒ½åœ¨æ¨¡å‹è®­ç»ƒä¸­åˆ©ç”¨è‰¯å¥½å¢å¼ºçš„ç‰¹å¾ï¼ŒåŒæ—¶å‡è½»è¿‡åº¦å¢å¼ºç‰¹å¾çš„è´Ÿé¢å½±å“ã€‚åœ¨å¤šä¸ªåŸŸä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒConStyXå®ç°äº†ä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå¤šæºåŸŸé‡‡é›†ä¼šå¯¼è‡´åŸŸåç§»ï¼Œå½±å“åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åŸŸæ³›åŒ–ï¼ˆDGï¼‰æ˜¯è®­ç»ƒå…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹ä»¥è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰çš„åŸºäºåŸŸéšæœºåŒ–çš„DGæ–¹æ³•å­˜åœ¨æ•ˆç‡å—é™å’Œå¿½è§†è¿‡åº¦å¢å¼ºå›¾åƒçš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹åŸºäºåŸŸéšæœºåŒ–çš„DGæ–¹æ³•â€”â€”å†…å®¹é£æ ¼å¢å¼ºï¼ˆConStyXï¼‰ã€‚</li>
<li>ConStyXé€šè¿‡å¢å¼ºè®­ç»ƒæ•°æ®çš„å†…å®¹å’Œé£æ ¼ï¼Œä½¿å¢å¼ºæ•°æ®è¦†ç›–æ›´å¹¿æ³›çš„æ•°æ®åŸŸã€‚</li>
<li>ConStyXåœ¨æ¨¡å‹è®­ç»ƒä¸­åˆ©ç”¨è‰¯å¥½å¢å¼ºçš„ç‰¹å¾ï¼Œå¹¶å‡è½»è¿‡åº¦å¢å¼ºç‰¹å¾çš„è´Ÿé¢å½±å“ã€‚</li>
<li>åœ¨å¤šä¸ªåŸŸä¸Šçš„å®éªŒè¯æ˜ï¼ŒConStyXå®ç°äº†ä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10675">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5e8b31a572dd0a87c89b596ddfd8a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edef52a631d6b25d40d30088df628e05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf38d640afb53220ab6712c520a4b3d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis"><a href="#PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis" class="headerlink" title="PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image   Analysis"></a>PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image   Analysis</h2><p><strong>Authors:Marzieh Oghbaie, Teresa AraÃºjoa, Hrvoje BogunoviÄ‡</strong></p>
<p>Background and Objective: Prototype-based methods improve interpretability by learning fine-grained part-prototypes; however, their visualization in the input pixel space is not always consistent with human-understandable biomarkers. In addition, well-known prototype-based approaches typically learn extremely granular prototypes that are less interpretable in medical imaging, where both the presence and extent of biomarkers and lesions are critical.   Methods: To address these challenges, we propose PiPViT (Patch-based Visual Interpretable Prototypes), an inherently interpretable prototypical model for image recognition. Leveraging a vision transformer (ViT), PiPViT captures long-range dependencies among patches to learn robust, human-interpretable prototypes that approximate lesion extent only using image-level labels. Additionally, PiPViT benefits from contrastive learning and multi-resolution input processing, which enables effective localization of biomarkers across scales.   Results: We evaluated PiPViT on retinal OCT image classification across four datasets, where it achieved competitive quantitative performance compared to state-of-the-art methods while delivering more meaningful explanations. Moreover, quantitative evaluation on a hold-out test set confirms that the learned prototypes are semantically and clinically relevant. We believe PiPViT can transparently explain its decisions and assist clinicians in understanding diagnostic outcomes. Github page: <a target="_blank" rel="noopener" href="https://github.com/marziehoghbaie/PiPViT">https://github.com/marziehoghbaie/PiPViT</a> </p>
<blockquote>
<p>èƒŒæ™¯ä¸ç›®æ ‡ï¼šåŸºäºåŸå‹çš„æ–¹æ³•é€šè¿‡å­¦ä¹ ç²¾ç»†ç²’åº¦çš„éƒ¨åˆ†åŸå‹æ¥æé«˜å¯è§£é‡Šæ€§ï¼Œä½†æ˜¯å®ƒä»¬åœ¨è¾“å…¥åƒç´ ç©ºé—´çš„å¯è§†åŒ–å¹¶ä¸æ€»æ˜¯ä¸å¯è¢«äººç†è§£çš„ç”Ÿç‰©æ ‡å¿—ç‰©ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œä¼—æ‰€å‘¨çŸ¥çš„åŸºäºåŸå‹çš„æ–¹æ³•é€šå¸¸ä¼šå­¦ä¹ éå¸¸ç²¾ç»†çš„åŸå‹ï¼Œåœ¨åŒ»å­¦æˆåƒä¸­ä¸å¤ªå®¹æ˜“è§£é‡Šï¼Œå…¶ä¸­ç”Ÿç‰©æ ‡å¿—ç‰©å’Œç—…å˜çš„å­˜åœ¨å’Œç¨‹åº¦éƒ½è‡³å…³é‡è¦ã€‚æ–¹æ³•ï¼šä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PiPViTï¼ˆåŸºäºè¡¥ä¸çš„è§†è§‰å¯è§£é‡ŠåŸå‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾åƒè¯†åˆ«çš„å›ºæœ‰å¯è§£é‡ŠåŸå‹æ¨¡å‹ã€‚å€ŸåŠ©è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼ŒPiPViTæ•è·è¡¥ä¸ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–æ€§ï¼Œå­¦ä¹ ç¨³å¥çš„ã€å¯è¢«äººè§£é‡Šçš„åŸå‹ï¼Œä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾æ¥è¿‘ä¼¼ç—…å˜ç¨‹åº¦ã€‚æ­¤å¤–ï¼ŒPiPViTå—ç›Šäºå¯¹æ¯”å­¦ä¹ å’Œå¤šåˆ†è¾¨ç‡è¾“å…¥å¤„ç†ï¼Œè¿™èƒ½å¤Ÿå®ç°è·¨å°ºåº¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©æœ‰æ•ˆå®šä½ã€‚ç»“æœï¼šæˆ‘ä»¬åœ¨å››ä¸ªæ•°æ®é›†ä¸Šå¯¹PiPViTè¿›è¡Œäº†è§†ç½‘è†œOCTå›¾åƒåˆ†ç±»è¯„ä¼°ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„å®šé‡æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´æœ‰æ„ä¹‰çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œåœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šçš„å®šé‡è¯„ä¼°è¯å®ï¼Œæ‰€å­¦ä¹ çš„åŸå‹åœ¨è¯­ä¹‰å’Œä¸´åºŠä¸Šéƒ½å…·æœ‰ç›¸å…³æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡PiPViTèƒ½å¤Ÿé€æ˜åœ°è§£é‡Šå…¶å†³ç­–ï¼Œå¹¶å¸®åŠ©ä¸´åºŠåŒ»ç”Ÿç†è§£è¯Šæ–­ç»“æœã€‚GitHubé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/marziehoghbaie/PiPViT">https://github.com/marziehoghbaie/PiPViT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10669v1">PDF</a> </p>
<p><strong>Summary</strong><br>    PiPViTæ¨¡å‹é€šè¿‡æ•æ‰å›¾åƒè¡¥ä¸é—´çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œå­¦ä¹ ç¨³å¥ã€å¯è§£é‡Šçš„åŸå‹ï¼Œä»¥å›¾åƒçº§æ ‡ç­¾è¿‘ä¼¼ç—…å˜ç¨‹åº¦ï¼Œè§£å†³åŸå‹å¯è§†åŒ–ä¸äººç†è§£ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨è§†ç½‘è†œOCTå›¾åƒåˆ†ç±»ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æä¾›æœ‰æ„ä¹‰çš„è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PiPViTæ¨¡å‹åˆ©ç”¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰å­¦ä¹ ç¨³å¥ã€å¯è§£é‡Šçš„åŸå‹ï¼Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ•æ‰è¡¥ä¸é—´çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œæé«˜åŸå‹åœ¨åŒ»å­¦å›¾åƒä¸­çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>PiPViTåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’Œå¤šåˆ†è¾¨ç‡è¾“å…¥å¤„ç†ï¼Œæœ‰æ•ˆå®šä½ä¸åŒå°ºåº¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
<li>æ¨¡å‹åœ¨è§†ç½‘è†œOCTå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰ç«äº‰åŠ›çš„å®šé‡æ€§èƒ½ã€‚</li>
<li>å­¦ä¹ åˆ°çš„åŸå‹å…·æœ‰è¯­ä¹‰å’Œä¸´åºŠç›¸å…³æ€§ï¼Œæœ‰åŠ©äºä¸´åºŠåŒ»ç”Ÿç†è§£è¯Šæ–­ç»“æœã€‚</li>
<li>PiPViTæ¨¡å‹æä¾›é€æ˜çš„å†³ç­–è§£é‡Šï¼Œæœ‰åŠ©äºå¢å¼ºåŒ»å­¦å›¾åƒåˆ†æçš„ä¿¡ä»»åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c7d3f63f58be7d2fa39a1f3f5c2400f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22c307ff2862f361125a11c7c90838b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e77e404dc185bcbe1d351602dcb3826d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0b64cacedf2e77b764f7d8df895575b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models"><a href="#Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models" class="headerlink" title="Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent   Diffusion Models"></a>Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent   Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Ilias Stogiannidis, Junyu Yan, Alison Q. Oâ€™Neil, Sotirios A. Tsaftaris</strong></p>
<p>Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨è‡ªç„¶ï¼ˆRGBï¼‰å›¾åƒé¢†åŸŸï¼Œè¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥é€‚åº”å„ç§è§†è§‰è¯­è¨€ä¸‹æ¸¸ä»»åŠ¡ï¼Œå‡ ä¹ä¸éœ€è¦ç›‘ç£ã€‚ç›¸åï¼Œåœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä»ç„¶ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ•°æ®å¯ç”¨æ€§çš„é™åˆ¶ï¼ˆä¾‹å¦‚ï¼Œç”±äºéšç§æ‹…å¿§ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨èƒ¸éƒ¨Xå°„çº¿æ¨¡å¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜æ ‡å‡†çš„æ–‡æœ¬æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿˜æ²¡æœ‰å­¦ä¼šå°†è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„ä¸´åºŠç›¸å…³ä¿¡æ¯ä¸ç»™å®šæ‰«æçš„ç›¸åº”åŒºåŸŸå¯¹é½ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¾®è°ƒæ¡†æ¶ï¼Œä»¥æ”¹è¿›é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å¤šæ¨¡å¼å¯¹é½ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆåœ°ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚çŸ­è¯­å®šä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†ï¼ˆMS-CXRï¼‰ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨éåˆ†å¸ƒæ•°æ®ï¼ˆVinDr-CXRï¼‰ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10633v1">PDF</a> 14 pages, 6 figures</p>
<p><strong>Summary</strong><br>     è¿‘æœŸç ”ç©¶æ˜¾ç¤ºï¼Œæ½œæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸï¼Œè¯¥æ¨¡å‹å¯è½»æ¾é€‚åº”å¤šç§è§†è§‰è¯­è¨€ä¸‹æ¸¸ä»»åŠ¡ï¼Œå‡ ä¹æ— éœ€ç›‘ç£ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œç”±äºæ•°æ®æœ‰é™ï¼ˆä¾‹å¦‚ï¼Œéšç§æ‹…å¿§ï¼‰ï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ½œæ‰©æ•£æ¨¡å‹ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºèƒ¸éƒ¨Xå°„çº¿æ¨¡æ€ï¼Œé¦–å…ˆè¯æ˜æ ‡å‡†çš„æ–‡æœ¬æ¡ä»¶æ½œæ‰©æ•£æ¨¡å‹å°šæœªå­¦ä¼šå°†è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„ä¸´åºŠç›¸å…³ä¿¡æ¯ä¸ç»™å®šæ‰«æçš„åŒºåŸŸå¯¹é½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§å¾®è°ƒæ¡†æ¶ï¼Œä»¥æ”¹è¿›é¢„è®­ç»ƒæ¨¡å‹çš„å¤šæ¨¡å¼å¯¹é½ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚çŸ­è¯­å®šä½ã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆMS-CXRï¼‰ï¼Œå¹¶åœ¨éåˆ†å¸ƒæ•°æ®ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼ˆVinDr-CXRï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆæˆä¸­è¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸã€‚</li>
<li>æ½œæ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨ç›¸å¯¹è¾ƒå°‘ï¼Œä¸»è¦ç”±äºæ•°æ®æœ‰é™ï¼ˆå¦‚éšç§æ‹…å¿§ï¼‰ã€‚</li>
<li>é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿æ¨¡æ€ï¼Œæ ‡å‡†çš„æ–‡æœ¬æ¡ä»¶æ½œæ‰©æ•£æ¨¡å‹å°šæœªå­¦ä¼šå°†ä¸´åºŠç›¸å…³ä¿¡æ¯ä¸æ‰«æåŒºåŸŸå¯¹é½ã€‚</li>
<li>æå‡ºä¸€ç§å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºæ”¹è¿›é¢„è®­ç»ƒæ¨¡å‹çš„å¤šæ¨¡å¼å¯¹é½ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚çŸ­è¯­å®šä½ï¼‰ä¸­é«˜æ•ˆåº”ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†MS-CXRä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27b80e05982ab3ad87e589c3677a0e91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78406b024805e620118e7f4b5250b47b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a21d355d7538d5e2962e0af3cd2c021.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semantic-Localization-Guiding-Segment-Anything-Model-For-Reference-Remote-Sensing-Image-Segmentation"><a href="#Semantic-Localization-Guiding-Segment-Anything-Model-For-Reference-Remote-Sensing-Image-Segmentation" class="headerlink" title="Semantic Localization Guiding Segment Anything Model For Reference   Remote Sensing Image Segmentation"></a>Semantic Localization Guiding Segment Anything Model For Reference   Remote Sensing Image Segmentation</h2><p><strong>Authors:Shuyang Li, Shuang Wang, Zhuangzhuang Sun, Jing Xiao</strong></p>
<p>The Reference Remote Sensing Image Segmentation (RRSIS) task generates segmentation masks for specified objects in images based on textual descriptions, which has attracted widespread attention and research interest. Current RRSIS methods rely on multi-modal fusion backbones and semantic segmentation heads but face challenges like dense annotation requirements and complex scene interpretation. To address these issues, we propose a framework named \textit{prompt-generated semantic localization guiding Segment Anything Model}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse localization and fine segmentation. In coarse localization stage, a visual grounding network roughly locates the text-described object. In fine segmentation stage, the coordinates from the first stage guide the Segment Anything Model (SAM), enhanced by a clustering-based foreground point generator and a mask boundary iterative optimization strategy for precise segmentation. Notably, the second stage can be train-free, significantly reducing the annotation data burden for the RRSIS task. Additionally, decomposing the RRSIS task into two stages allows for focusing on specific region segmentation, avoiding interference from complex scenes.We further contribute a high-quality, multi-category manually annotated dataset. Experimental validation on two datasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant performance improvements and surpasses existing state-of-the-art models.Our code will be made publicly available. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒå‚è€ƒåˆ†å‰²ï¼ˆRRSISï¼‰ä»»åŠ¡åŸºäºæ–‡æœ¬æè¿°ä¸ºå›¾åƒä¸­çš„æŒ‡å®šå¯¹è±¡ç”Ÿæˆåˆ†å‰²æ©è†œï¼Œè¿™å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨å’Œç ”ç©¶å…´è¶£ã€‚å½“å‰çš„RRSISæ–¹æ³•ä¾èµ–äºå¤šæ¨¡æ€èåˆä¸»å¹²å’Œè¯­ä¹‰åˆ†å‰²å¤´ï¼Œä½†é¢ä¸´ç€å¯†é›†æ³¨é‡Šè¦æ±‚å’Œå¤æ‚åœºæ™¯è§£é‡Šç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œæç¤ºç”Ÿæˆè¯­ä¹‰å®šä½å¼•å¯¼åˆ†å‰²ä»»ä½•æ¨¡å‹â€ï¼ˆPSLG-SAMï¼‰çš„æ¡†æ¶ï¼Œå®ƒå°†RRSISä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç²—ç•¥å®šä½å’Œç²¾ç»†åˆ†å‰²ã€‚åœ¨ç²—ç•¥å®šä½é˜¶æ®µï¼Œè§†è§‰å®šä½ç½‘ç»œå¤§è‡´å®šä½æ–‡æœ¬æè¿°çš„å¯¹è±¡ã€‚åœ¨ç²¾ç»†åˆ†å‰²é˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µçš„åæ ‡å¼•å¯¼åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œé€šè¿‡åŸºäºèšç±»çš„å‰æ™¯ç‚¹ç”Ÿæˆå™¨å’Œç”¨äºç²¾ç¡®åˆ†å‰²çš„æ©è†œè¾¹ç•Œè¿­ä»£ä¼˜åŒ–ç­–ç•¥è¿›è¡Œå¢å¼ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¬¬äºŒé˜¶æ®µå¯ä»¥æ— è®­ç»ƒåœ°è¿è¡Œï¼Œæ˜¾è‘—å‡å°‘äº†RRSISä»»åŠ¡çš„æ ‡æ³¨æ•°æ®è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œå°†RRSISä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µå…è®¸ä¸“æ³¨äºç‰¹å®šåŒºåŸŸåˆ†å‰²ï¼Œé¿å…å¤æ‚åœºæ™¯çš„å¹²æ‰°ã€‚æˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡ã€å¤šç±»åˆ«çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ï¼ˆRRSIS-Då’ŒRRSIS-Mï¼‰ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒPSLG-SAMå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›å¹¶è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10503v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬æè¿°çš„å¯¹è±¡ç”Ÿæˆåˆ†å‰²æ©è†œæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºPSLG-SAMçš„æ¡†æ¶ï¼Œåˆ†ä¸ºç²—å®šä½å’Œç²¾ç»†åˆ†å‰²ä¸¤ä¸ªé˜¶æ®µã€‚ç²—å®šä½é˜¶æ®µé€šè¿‡è§†è§‰å®šä½ç½‘ç»œç²—ç•¥å®šä½æ–‡æœ¬æè¿°çš„å¯¹è±¡ï¼Œç²¾ç»†åˆ†å‰²é˜¶æ®µåˆ©ç”¨ç¬¬ä¸€é˜¶æ®µåæ ‡æŒ‡å¯¼åˆ†å‰²ä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œé€šè¿‡èšç±»ç”Ÿæˆå‰æ™¯ç‚¹å’Œæ©è†œè¾¹ç•Œä¼˜åŒ–ç­–ç•¥å®ç°ç²¾ç¡®åˆ†å‰²ã€‚è¯¥æ¡†æ¶å¯å‡å°‘æ ‡æ³¨æ•°æ®è´Ÿæ‹…ï¼Œå¹¶èƒ½åœ¨å¤æ‚åœºæ™¯ä¸­é¿å…å¹²æ‰°ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†PSLG-SAMçš„æ€§èƒ½ï¼Œå¹¶å…¬å¼€äº†ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RRSISä»»åŠ¡éœ€æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒä¸­æŒ‡å®šå¯¹è±¡çš„åˆ†å‰²æ©è†œï¼Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–äºå¤šæ¨¡æ€èåˆå’Œè¯­ä¹‰åˆ†å‰²æŠ€æœ¯ï¼Œé¢ä¸´å¯†é›†æ ‡æ³¨å’Œå¤æ‚åœºæ™¯è§£è¯»çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºPSLG-SAMæ¡†æ¶ï¼Œåˆ†ä¸ºç²—å®šä½å’Œç²¾ç»†åˆ†å‰²ä¸¤ä¸ªé˜¶æ®µæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ç²—å®šä½é˜¶æ®µé€šè¿‡è§†è§‰å®šä½ç½‘ç»œå®šä½å¯¹è±¡ï¼Œç²¾ç»†åˆ†å‰²é˜¶æ®µåˆ©ç”¨åæ ‡æŒ‡å¯¼SAMæ¨¡å‹è¿›è¡Œç²¾ç¡®åˆ†å‰²ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¯æ— éœ€è®­ç»ƒï¼Œæ˜¾è‘—å‡å°‘æ ‡æ³¨æ•°æ®è´Ÿæ‹…ã€‚</li>
<li>åˆ†è§£ä»»åŠ¡æœ‰åŠ©äºèšç„¦ç‰¹å®šåŒºåŸŸåˆ†å‰²ï¼Œé¿å…å¤æ‚åœºæ™¯çš„å¹²æ‰°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eebe391a792decbad342d9b3b2cc05ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb3f733376141149cd74534c5d22872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f02b4d80361d91d0f8e740bfe34143fe.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GeoCAD-Local-Geometry-Controllable-CAD-Generation"><a href="#GeoCAD-Local-Geometry-Controllable-CAD-Generation" class="headerlink" title="GeoCAD: Local Geometry-Controllable CAD Generation"></a>GeoCAD: Local Geometry-Controllable CAD Generation</h2><p><strong>Authors:Zhanwei Zhang, Kaiyuan Liu, Junjie Liu, Wenxiao Wang, Binbin Lin, Liang Xie, Chen Shen, Deng Cai</strong></p>
<p>Local geometry-controllable computer-aided design (CAD) generation aims to modify local parts of CAD models automatically, enhancing design efficiency. It also ensures that the shapes of newly generated local parts follow user-specific geometric instructions (e.g., an isosceles right triangle or a rectangle with one corner cut off). However, existing methods encounter challenges in achieving this goal. Specifically, they either lack the ability to follow textual instructions or are unable to focus on the local parts. To address this limitation, we introduce GeoCAD, a user-friendly and local geometry-controllable CAD generation method. Specifically, we first propose a complementary captioning strategy to generate geometric instructions for local parts. This strategy involves vertex-based and VLLM-based captioning for systematically annotating simple and complex parts, respectively. In this way, we caption $\sim$221k different local parts in total. In the training stage, given a CAD model, we randomly mask a local part. Then, using its geometric instruction and the remaining parts as input, we prompt large language models (LLMs) to predict the masked part. During inference, users can specify any local part for modification while adhering to a variety of predefined geometric instructions. Extensive experiments demonstrate the effectiveness of GeoCAD in generation quality, validity and text-to-CAD consistency. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/Zhanwei-Z/GeoCAD">https://github.com/Zhanwei-Z/GeoCAD</a>. </p>
<blockquote>
<p>å±€éƒ¨å‡ ä½•å¯æ§è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆæ—¨åœ¨è‡ªåŠ¨ä¿®æ”¹CADæ¨¡å‹çš„éƒ¨åˆ†ï¼Œæé«˜è®¾è®¡æ•ˆç‡ã€‚å®ƒè¿˜å¯ä»¥ç¡®ä¿æ–°ç”Ÿæˆçš„å±€éƒ¨éƒ¨ä»¶çš„å½¢çŠ¶ç¬¦åˆç”¨æˆ·ç‰¹å®šçš„å‡ ä½•æŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼Œç­‰è…°ç›´è§’ä¸‰è§’å½¢æˆ–åˆ‡æ‰ä¸€ä¸ªè§’çš„çŸ©å½¢ï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å®ç°è¿™ä¸€ç›®æ ‡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬è¦ä¹ˆç¼ºä¹éµå¾ªæ–‡æœ¬æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œè¦ä¹ˆæ— æ³•ä¸“æ³¨äºå±€éƒ¨éƒ¨ä»¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†GeoCADï¼Œè¿™æ˜¯ä¸€ç§ç”¨æˆ·å‹å¥½ä¸”å±€éƒ¨å‡ ä½•å¯æ§çš„CADç”Ÿæˆæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§è¡¥å……çš„æ ‡æ³¨ç­–ç•¥ï¼Œä¸ºå±€éƒ¨éƒ¨ä»¶ç”Ÿæˆå‡ ä½•æŒ‡ä»¤ã€‚è¯¥ç­–ç•¥åŒ…æ‹¬åŸºäºé¡¶ç‚¹å’ŒåŸºäºVLLMçš„æ ‡æ³¨ï¼Œåˆ†åˆ«ç”¨äºç³»ç»Ÿåœ°æ³¨é‡Šç®€å•å’Œå¤æ‚éƒ¨ä»¶ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬æ€»å…±æ ‡æ³¨äº†çº¦22.1ä¸‡ä¸ªä¸åŒçš„å±€éƒ¨éƒ¨ä»¶ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œç»™å®šä¸€ä¸ªCADæ¨¡å‹ï¼Œæˆ‘ä»¬éšæœºé®æŒ¡ä¸€ä¸ªå±€éƒ¨éƒ¨ä»¶ã€‚ç„¶åï¼Œåˆ©ç”¨å…¶å‡ ä½•æŒ‡ä»¤å’Œå‰©ä½™éƒ¨ä»¶ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥é¢„æµ‹è¢«é®æŒ¡çš„éƒ¨åˆ†ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·å¯ä»¥æŒ‡å®šä»»ä½•å±€éƒ¨éƒ¨ä»¶è¿›è¡Œä¿®æ”¹ï¼ŒåŒæ—¶éµå¾ªå¤šç§é¢„å®šä¹‰çš„å‡ ä½•æŒ‡ä»¤ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGeoCADåœ¨ç”Ÿæˆè´¨é‡ã€æœ‰æ•ˆæ€§å’Œæ–‡æœ¬åˆ°CADçš„ä¸€è‡´æ€§æ–¹é¢éƒ½éå¸¸æœ‰æ•ˆã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zhanwei-Z/GeoCAD%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Zhanwei-Z/GeoCADä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10337v1">PDF</a> 18 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå±€éƒ¨å‡ ä½•æ§åˆ¶çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆæ–¹æ³•æ—¨åœ¨è‡ªåŠ¨ä¿®æ”¹CADæ¨¡å‹çš„éƒ¨åˆ†å†…å®¹ï¼Œæé«˜è®¾è®¡æ•ˆç‡ï¼Œå¹¶ç¡®ä¿æ–°ç”Ÿæˆçš„å±€éƒ¨éƒ¨åˆ†ç¬¦åˆç”¨æˆ·ç‰¹å®šçš„å‡ ä½•æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å®ç°è¿™ä¸€ç›®æ ‡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†GeoCADï¼Œä¸€ç§ç”¨æˆ·å‹å¥½ã€å±€éƒ¨å‡ ä½•å¯æ§çš„CADç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§äº’è¡¥çš„æ ‡æ³¨ç­–ç•¥ï¼Œä¸ºå±€éƒ¨éƒ¨åˆ†ç”Ÿæˆå‡ ä½•æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡é¡¶ç‚¹åŸºç¡€å’ŒVLLMåŸºç¡€çš„æ ‡æ³¨æ–¹å¼ç³»ç»Ÿåœ°æ³¨é‡Šç®€å•å’Œå¤æ‚çš„éƒ¨åˆ†ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬éšæœºé®æŒ¡CADæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œç„¶åä½¿ç”¨å…¶å‡ ä½•æŒ‡ä»¤å’Œå‰©ä½™éƒ¨åˆ†ä½œä¸ºè¾“å…¥ï¼Œæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œç”¨æˆ·å¯ä»¥æŒ‡å®šä»»ä½•éƒ¨åˆ†è¿›è¡Œä¿®æ”¹ï¼ŒåŒæ—¶éµå¾ªå¤šç§é¢„å®šä¹‰çš„å‡ ä½•æŒ‡ä»¤ã€‚å®éªŒè¡¨æ˜ï¼ŒGeoCADåœ¨ç”Ÿæˆè´¨é‡ã€æœ‰æ•ˆæ€§å’Œæ–‡æœ¬åˆ°CADçš„ä¸€è‡´æ€§æ–¹é¢éƒ½éå¸¸æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GeoCADæ˜¯ä¸€ç§ç”¨æˆ·å‹å¥½ã€å±€éƒ¨å‡ ä½•å¯æ§çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>å®ƒé€šè¿‡ä¸€ç§äº’è¡¥çš„æ ‡æ³¨ç­–ç•¥ç”Ÿæˆå±€éƒ¨å‡ ä½•æŒ‡ä»¤ï¼ŒåŒ…æ‹¬é¡¶ç‚¹åŸºç¡€å’ŒVLLMåŸºç¡€çš„æ ‡æ³¨æ–¹å¼ã€‚</li>
<li>GeoCADèƒ½å¤Ÿå¤„ç†å¤æ‚çš„CADæ¨¡å‹ï¼Œå¹¶å…è®¸ç”¨æˆ·æŒ‡å®šä»»ä½•éƒ¨åˆ†è¿›è¡Œä¿®æ”¹ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸ç”¨æˆ·éµå¾ªå¤šç§é¢„å®šä¹‰çš„å‡ ä½•æŒ‡ä»¤ï¼Œæé«˜äº†è®¾è®¡æ•ˆç‡å’Œçµæ´»æ€§ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒGeoCADåœ¨ç”Ÿæˆè´¨é‡ã€æœ‰æ•ˆæ€§å’Œæ–‡æœ¬åˆ°CADçš„ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>GeoCADçš„ä»£ç å°†åœ¨[ç½‘å€]ä¸Šå…¬å¼€ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³ç°æœ‰CADè®¾è®¡å·¥å…·ä¸­çš„å±€é™æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2394e4a593868e9468e7d1fbb932fed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86932a17276667bf0d4fba7a0ba7012.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a2973fb73395d528d6e2aff4dc8aae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80b4dcc1ee23f9beb622aaf5a7810679.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SWDL-Stratum-Wise-Difference-Learning-with-Deep-Laplacian-Pyramid-for-Semi-Supervised-3D-Intracranial-Hemorrhage-Segmentation"><a href="#SWDL-Stratum-Wise-Difference-Learning-with-Deep-Laplacian-Pyramid-for-Semi-Supervised-3D-Intracranial-Hemorrhage-Segmentation" class="headerlink" title="SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for   Semi-Supervised 3D Intracranial Hemorrhage Segmentation"></a>SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for   Semi-Supervised 3D Intracranial Hemorrhage Segmentation</h2><p><strong>Authors:Cheng Wang, Siqi Chen, Donghua Mi, Yang Chen, Yudong Zhang, Yinsheng Li</strong></p>
<p>Recent advances in medical imaging have established deep learning-based segmentation as the predominant approach, though it typically requires large amounts of manually annotated data. However, obtaining annotations for intracranial hemorrhage (ICH) remains particularly challenging due to the tedious and costly labeling process. Semi-supervised learning (SSL) has emerged as a promising solution to address the scarcity of labeled data, especially in volumetric medical image segmentation. Unlike conventional SSL methods that primarily focus on high-confidence pseudo-labels or consistency regularization, we propose SWDL-Net, a novel SSL framework that exploits the complementary advantages of Laplacian pyramid and deep convolutional upsampling. The Laplacian pyramid excels at edge sharpening, while deep convolutions enhance detail precision through flexible feature mapping. Our framework achieves superior segmentation of lesion details and boundaries through a difference learning mechanism that effectively integrates these complementary approaches. Extensive experiments on a 271-case ICH dataset and public benchmarks demonstrate that SWDL-Net outperforms current state-of-the-art methods in scenarios with only 2% labeled data. Additional evaluations on the publicly available Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data further confirm the superiority of our approach. Code and data have been released at <a target="_blank" rel="noopener" href="https://github.com/SIAT-CT-LAB/SWDL">https://github.com/SIAT-CT-LAB/SWDL</a>. </p>
<blockquote>
<p>è¿‘æœŸåŒ»å­¦æˆåƒæŠ€æœ¯çš„è¿›å±•å·²ç»ç¡®ç«‹äº†åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æ–¹æ³•ä½œä¸ºä¸»æµæ–¹æ³•ï¼Œå°½ç®¡é€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨è¿‡ç¨‹çš„ç¹çå’Œæˆæœ¬é«˜æ˜‚ï¼Œå¯¹é¢…å†…å‡ºè¡€ï¼ˆICHï¼‰çš„æ ‡æ³¨è·å–ä»ç„¶ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å·²ç»æˆä¸ºè§£å†³æ ‡è®°æ•°æ®ç¨€ç¼ºçš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ã€‚ä¸åŒäºä¸»è¦å…³æ³¨é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾æˆ–ä¸€è‡´æ€§æ­£åˆ™åŒ–çš„ä¼ ç»ŸSSLæ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†SWDL-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„SSLæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”å’Œæ·±åº¦å·ç§¯ä¸Šé‡‡æ ·çš„äº’è¡¥ä¼˜åŠ¿ã€‚æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”æ“…é•¿è¾¹ç¼˜é”åŒ–ï¼Œè€Œæ·±åº¦å·ç§¯é€šè¿‡çµæ´»çš„ç‰¹å¾æ˜ å°„å¢å¼ºäº†ç»†èŠ‚ç²¾åº¦ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å·®å¼‚å­¦ä¹ æœºåˆ¶å®ç°äº†ç—…å˜ç»†èŠ‚å’Œè¾¹ç•Œçš„ä¼˜è´¨åˆ†å‰²ï¼Œæœ‰æ•ˆåœ°æ•´åˆäº†è¿™äº›äº’è¡¥æ–¹æ³•ã€‚åœ¨271ä¾‹ICHæ•°æ®é›†å’Œå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ä»…æœ‰2%æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒSWDL-Netçš„æ€§èƒ½è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨å…¬å¼€å¯ç”¨çš„Brain Hemorrhage Segmentation Datasetï¼ˆBHSDï¼‰æ•°æ®é›†ä¸Šä½¿ç”¨5%æ ‡è®°æ•°æ®çš„è¿›ä¸€æ­¥è¯„ä¼°ä¹Ÿè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/SIAT-CT-LAB/SWDL%E3%80%82">https://github.com/SIAT-CT-LAB/SWDLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10325v1">PDF</a> 11 pages, 4 figures, 6 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶SWDL-Netã€‚è¯¥æ¡†æ¶ç»“åˆäº†Laplaciané‡‘å­—å¡”å’Œæ·±åº¦å·ç§¯ä¸Šé‡‡æ ·çš„ä¼˜åŠ¿ï¼Œåœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†é¢…å†…å‡ºè¡€å›¾åƒçš„ç²¾å‡†åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWDL-Netåœ¨ä»…æœ‰2%æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å·²åœ¨å…¬å¼€æ•°æ®é›†BHSDä¸Šè¿›è¡ŒéªŒè¯ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸»è¦æ–¹æ³•ï¼Œä½†ä»éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ã€‚</li>
<li>è·å¾—é¢…å†…å‡ºè¡€ï¼ˆICHï¼‰çš„æ ‡æ³¨æ•°æ®ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ ‡æ³¨è¿‡ç¨‹æ—¢ç¹çåˆæ˜‚è´µã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ˜¯è§£å†³æ ‡è®°æ•°æ®ç¨€ç¼ºé—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>SWDL-Netæ˜¯ä¸€ç§æ–°å‹çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†Laplaciané‡‘å­—å¡”å’Œæ·±åº¦å·ç§¯ä¸Šé‡‡æ ·çš„ä¼˜åŠ¿ã€‚</li>
<li>Laplaciané‡‘å­—å¡”æ“…é•¿è¾¹ç¼˜é”åŒ–ï¼Œè€Œæ·±åº¦å·ç§¯é€šè¿‡çµæ´»çš„ç‰¹å¾æ˜ å°„å¢å¼ºäº†ç»†èŠ‚ç²¾åº¦ã€‚</li>
<li>SWDL-Neté€šè¿‡å·®å¼‚å­¦ä¹ æœºåˆ¶å®ç°äº†ç—…å˜ç»†èŠ‚å’Œè¾¹ç•Œçš„ç²¾å‡†åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b0570b86163f0822f97cc2c980eb0c2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff54c37b4286180519be31af01cdbc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2e3a4a851966bfca6c10996e56e22a0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DUN-SRE-Deep-Unrolling-Network-with-Spatiotemporal-Rotation-Equivariance-for-Dynamic-MRI-Reconstruction"><a href="#DUN-SRE-Deep-Unrolling-Network-with-Spatiotemporal-Rotation-Equivariance-for-Dynamic-MRI-Reconstruction" class="headerlink" title="DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation   Equivariance for Dynamic MRI Reconstruction"></a>DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation   Equivariance for Dynamic MRI Reconstruction</h2><p><strong>Authors:Yuliang Zhu, Jing Cheng, Qi Xie, Zhuo-Xu Cui, Qingyong Zhu, Yuanyuan Liu, Xin Liu, Jianfeng Ren, Chengbo Wang, Dong Liang</strong></p>
<p>Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries, including spatial rotation symmetry within individual frames and temporal symmetry along the time dimension. Explicit incorporation of these symmetry priors in the reconstruction model can significantly improve image quality, especially under aggressive undersampling scenarios. Recently, Equivariant convolutional neural network (ECNN) has shown great promise in exploiting spatial symmetry priors. However, existing ECNNs critically fail to model temporal symmetry, arguably the most universal and informative structural prior in dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for Dynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance through a (2+1)D equivariant convolutional architecture. In particular, it integrates both the data consistency and proximal mapping module into a unified deep unrolling framework. This architecture ensures rigorous propagation of spatiotemporal rotation symmetry constraints throughout the reconstruction process, enabling more physically accurate modeling of cardiac motion dynamics in cine MRI. In addition, a high-fidelity group filter parameterization mechanism is developed to maintain representation precision while enforcing symmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets demonstrate that DUN-SRE achieves state-of-the-art performance, particularly in preserving rotation-symmetric structures, offering strong generalization capability to a broad range of dynamic MRI reconstruction tasks. </p>
<blockquote>
<p>åŠ¨æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¡¨ç°å‡ºå˜æ¢å¯¹ç§°æ€§ï¼ŒåŒ…æ‹¬å•ä¸ªå¸§å†…çš„ç©ºé—´æ—‹è½¬å¯¹ç§°æ€§å’Œæ—¶é—´ç»´åº¦ä¸Šçš„æ—¶é—´å¯¹ç§°æ€§ã€‚åœ¨é‡å»ºæ¨¡å‹ä¸­æ˜ç¡®èå…¥è¿™äº›å¯¹ç§°å…ˆéªŒçŸ¥è¯†å¯ä»¥æ˜¾è‘—æé«˜å›¾åƒè´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¿€çƒˆçš„æ¬ é‡‡æ ·æƒ…å†µä¸‹ã€‚æœ€è¿‘ï¼Œç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆECNNï¼‰åœ¨åˆ©ç”¨ç©ºé—´å¯¹ç§°å…ˆéªŒæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ECNNsæœªèƒ½å¯¹æ—¶é—´å¯¹ç§°æ€§è¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ—¶é—´å¯¹ç§°æ€§æ— ç–‘æ˜¯åŠ¨æ€MRIé‡å»ºä¸­æœ€é€šç”¨ä¸”ä¿¡æ¯æœ€ä¸°å¯Œçš„ç»“æ„å…ˆéªŒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å…·æœ‰æ—¶ç©ºæ—‹è½¬ç­‰å˜æ€§çš„æ·±åº¦å±•å¼€ç½‘ç»œï¼ˆDUN-SREï¼‰ï¼Œç”¨äºåŠ¨æ€MRIé‡å»ºã€‚DUN-SREé€šè¿‡ï¼ˆ2+1ï¼‰Dç­‰å˜å·ç§¯æ¶æ„å»ºç«‹æ—¶ç©ºç­‰å˜æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒå°†æ•°æ®ä¸€è‡´æ€§å’Œè¿‘ç«¯æ˜ å°„æ¨¡å—é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ·±åº¦å±•å¼€æ¡†æ¶ä¸­ã€‚è¯¥æ¶æ„ç¡®ä¿äº†æ—¶ç©ºæ—‹è½¬å¯¹ç§°çº¦æŸåœ¨æ•´ä¸ªé‡å»ºè¿‡ç¨‹ä¸­çš„ä¸¥æ ¼ä¼ æ’­ï¼Œå®ç°å¯¹ç”µå½±MRIä¸­å¿ƒè„è¿åŠ¨åŠ¨æ€çš„æ›´åŠ ç‰©ç†å‡†ç¡®çš„å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§é«˜ä¿çœŸç»„æ»¤æ³¢å™¨å‚æ•°åŒ–æœºåˆ¶ï¼Œä»¥ä¿æŒè¡¨ç¤ºç²¾åº¦çš„åŒæ—¶å®æ–½å¯¹ç§°çº¦æŸã€‚åœ¨å¿ƒè„ç”µå½±MRIæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDUN-SREè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ä¿æŒæ—‹è½¬å¯¹ç§°ç»“æ„æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯¹å¹¿æ³›çš„åŠ¨æ€MRIé‡å»ºä»»åŠ¡å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10309v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŠ¨æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å…·æœ‰è½¬æ¢å¯¹ç§°æ€§ï¼ŒåŒ…æ‹¬ä¸ªä½“å¸§å†…çš„ç©ºé—´æ—‹è½¬å¯¹ç§°æ€§å’Œæ—¶é—´ç»´åº¦ä¸Šçš„æ—¶é—´å¯¹ç§°æ€§ç­‰ã€‚åœ¨é‡å»ºæ¨¡å‹ä¸­æ˜¾å¼èå…¥è¿™äº›å¯¹ç§°å…ˆéªŒå¯ä»¥æ˜¾è‘—æé«˜å›¾åƒè´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¿€è¿›æ¬ é‡‡æ ·åœºæ™¯ä¸­ã€‚é’ˆå¯¹ç°æœ‰ç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆECNNï¼‰æ— æ³•å»ºæ¨¡æ—¶é—´å¯¹ç§°æ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æ—¶ç©ºæ—‹è½¬ç­‰å˜æ€§çš„æ·±åº¦å±•å¼€ç½‘ç»œï¼ˆDUN-SREï¼‰ç”¨äºåŠ¨æ€MRIé‡å»ºã€‚è¯¥ç½‘ç»œé€šè¿‡ï¼ˆ2+1ï¼‰Dç­‰å˜å·ç§¯æ¶æ„å»ºç«‹æ—¶ç©ºç­‰å˜æ€§ï¼Œå°†æ•°æ®ä¸€è‡´æ€§å’Œè¿‘ç«¯æ˜ å°„æ¨¡å—é›†æˆåˆ°ç»Ÿä¸€çš„æ·±åº¦å±•å¼€æ¡†æ¶ä¸­ã€‚è¯¥æ¶æ„ç¡®ä¿äº†æ—¶ç©ºæ—‹è½¬å¯¹ç§°çº¦æŸåœ¨æ•´ä¸ªé‡å»ºè¿‡ç¨‹ä¸­çš„ä¸¥æ ¼ä¼ æ’­ï¼Œèƒ½å¤Ÿå¯¹å¿ƒè„è¿åŠ¨åŠ¨æ€è¿›è¡Œæ›´ç‰©ç†å‡†ç¡®çš„å»ºæ¨¡ã€‚å®éªŒè¯æ˜ï¼ŒDUN-SREåœ¨ä¿ç•™æ—‹è½¬å¯¹ç§°ç»“æ„æ–¹é¢è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œå¯¹å¹¿æ³›çš„åŠ¨æ€MRIé‡å»ºä»»åŠ¡å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€MRIå…·æœ‰è½¬æ¢å¯¹ç§°æ€§ï¼ŒåŒ…æ‹¬ç©ºé—´æ—‹è½¬å¯¹ç§°æ€§å’Œæ—¶é—´å¯¹ç§°æ€§ã€‚</li>
<li>èå…¥å¯¹ç§°å…ˆéªŒèƒ½æå‡å›¾åƒè´¨é‡ï¼Œå°¤å…¶åœ¨æ¿€è¿›æ¬ é‡‡æ ·æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰çš„ç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆECNNï¼‰æ— æ³•æœ‰æ•ˆå»ºæ¨¡åŠ¨æ€MRIçš„æ—¶é—´å¯¹ç§°æ€§ã€‚</li>
<li>æå‡ºçš„Deep Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE)èƒ½å»ºç«‹æ—¶ç©ºç­‰å˜æ€§ã€‚</li>
<li>DUN-SREé€šè¿‡é›†æˆæ•°æ®ä¸€è‡´æ€§å’Œè¿‘ç«¯æ˜ å°„æ¨¡å—ï¼Œç¡®ä¿äº†æ—¶ç©ºæ—‹è½¬å¯¹ç§°çº¦æŸçš„ä¸¥æ ¼ä¼ æ’­ã€‚</li>
<li>DUN-SREåœ¨å¿ƒè„MRIä¸­è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œèƒ½æ›´ç‰©ç†å‡†ç¡®åœ°å»ºæ¨¡å¿ƒè„è¿åŠ¨åŠ¨æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55dcaaf558074e0fa3a3a9897ce3f5ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5e90096ad024b649cccb8991cb832a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a41710e4fc9cfb62116fd5d7f27b8d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e04bca9b47e510165b581e2f8a0eeff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83e5f941e698e5dd04252795285e89a1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Aware-Deep-Learning-for-Automated-Skin-Cancer-Classification-A-Comprehensive-Evaluation"><a href="#Uncertainty-Aware-Deep-Learning-for-Automated-Skin-Cancer-Classification-A-Comprehensive-Evaluation" class="headerlink" title="Uncertainty-Aware Deep Learning for Automated Skin Cancer   Classification: A Comprehensive Evaluation"></a>Uncertainty-Aware Deep Learning for Automated Skin Cancer   Classification: A Comprehensive Evaluation</h2><p><strong>Authors:Hamzeh Asgharnezhad, Pegah Tabarisaadi, Abbas Khosravi, Roohallah Alizadehsani, U. Rajendra Acharya</strong></p>
<p>Accurate and reliable skin cancer diagnosis is critical for early treatment and improved patient outcomes. Deep learning (DL) models have shown promise in automating skin cancer classification, but their performance can be limited by data scarcity and a lack of uncertainty awareness. In this study, we present a comprehensive evaluation of DL-based skin lesion classification using transfer learning and uncertainty quantification (UQ) on the HAM10000 dataset. In the first phase, we benchmarked several pre-trained feature extractors-including Contrastive Language-Image Pretraining (CLIP) variants, Residual Network-50 (ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual Geometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range of traditional classifiers such as Support Vector Machine (SVM), eXtreme Gradient Boosting (XGBoost), and logistic regression. Our results show that CLIP-based vision transformers, particularly LAION CLIP ViT-H&#x2F;14 with SVM, deliver the highest classification performance. In the second phase, we incorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte Carlo Dropout (EMCD) to assess not only prediction accuracy but also the reliability of model outputs. We evaluated these models using uncertainty-aware metrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen), uncertainty specificity(USpe), and uncertainty precision(UPre). The results demonstrate that ensemble methods offer a good trade-off between accuracy and uncertainty handling, while EMCD is more sensitive to uncertain predictions. This study highlights the importance of integrating UQ into DL-based medical diagnosis to enhance both performance and trustworthiness in real-world clinical applications. </p>
<blockquote>
<p>å‡†ç¡®å¯é çš„çš®è‚¤ç™Œè¯Šæ–­å¯¹äºæ—©æœŸæ²»ç–—å’Œæ”¹å–„æ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–çš®è‚¤ç™Œåˆ†ç±»æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¯èƒ½ä¼šå—åˆ°æ•°æ®ç¨€ç¼ºå’Œç¼ºä¹ä¸ç¡®å®šæ€§æ„è¯†çš„å½±å“ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„çš®è‚¤ç—…å˜åˆ†ç±»è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œé‡‡ç”¨è¿ç§»å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰åœ¨HAM10000æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¯¹æ¯”äº†å‡ ç§é¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å˜ä½“ã€æ®‹å·®ç½‘ç»œ50ï¼ˆResNet50ï¼‰ã€å¯†é›†è¿æ¥å·ç§¯ç½‘ç»œï¼ˆDenseNet121ï¼‰ã€è§†è§‰å‡ ä½•ç»„ç½‘ç»œï¼ˆVGG16ï¼‰å’ŒEfficientNet-V2-Largeï¼Œç»“åˆä¸€ç³»åˆ—ä¼ ç»Ÿåˆ†ç±»å™¨ï¼Œå¦‚æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€æç«¯æ¢¯åº¦æå‡ï¼ˆXGBoostï¼‰å’Œé€»è¾‘å›å½’ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºCLIPçš„è§†è§‰å˜å‹å™¨ï¼Œç‰¹åˆ«æ˜¯LAION CLIP ViT-H&#x2F;14ä¸SVMçš„ç»“åˆï¼Œå…·æœ‰æœ€é«˜çš„åˆ†ç±»æ€§èƒ½ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è’™ç‰¹å¡æ´›Dropoutï¼ˆMCDï¼‰ã€é›†æˆæ–¹æ³•å’Œé›†æˆè’™ç‰¹å¡æ´›Dropoutï¼ˆEMCDï¼‰æ¥å¼•å…¥ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä»¥è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„é¢„æµ‹å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŒ‡æ ‡ï¼Œå¦‚ä¸ç¡®å®šæ€§å‡†ç¡®åº¦ï¼ˆUAccï¼‰ã€ä¸ç¡®å®šæ€§æ•æ„Ÿæ€§ï¼ˆUSenï¼‰ã€ä¸ç¡®å®šæ€§ç‰¹å¼‚æ€§ï¼ˆUSpeï¼‰å’Œä¸ç¡®å®šæ€§ç²¾ç¡®åº¦ï¼ˆUPreï¼‰æ¥è¯„ä¼°è¿™äº›æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œé›†æˆæ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§å¤„ç†ä¹‹é—´æä¾›äº†è‰¯å¥½çš„æƒè¡¡ï¼Œè€ŒEMCDå¯¹ä¸ç¡®å®šçš„é¢„æµ‹æ›´ä¸ºæ•æ„Ÿã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å°†ä¸ç¡®å®šæ€§é‡åŒ–æ•´åˆåˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦è¯Šæ–­ä¸­çš„é‡è¦æ€§ï¼Œä»¥æé«˜ç°å®ä¸´åºŠåº”ç”¨ä¸­æ€§èƒ½å’Œå¯ä¿¡åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10302v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çš®è‚¤ç™Œåˆ†ç±»è‡ªåŠ¨åŒ–æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å—é™äºæ•°æ®ç¨€ç¼ºå’Œç¼ºä¹ä¸ç¡®å®šæ€§æ„è¯†ã€‚æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†åŸºäºæ·±åº¦å­¦ä¹ çš„çš®è‚¤ç—…å˜åˆ†ç±»ï¼Œé‡‡ç”¨è¿ç§»å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰åœ¨HAM10000æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åŸºå‡†æµ‹è¯•äº†å¤šç§é¢„è®­ç»ƒç‰¹å¾æå–å™¨ä¸ä¸€ç³»åˆ—ä¼ ç»Ÿåˆ†ç±»å™¨çš„ç»„åˆï¼Œå‘ç°åŸºäºCLIPçš„è§†è§‰å˜å‹å™¨è¡¨ç°æœ€ä½³ã€‚ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨è’™ç‰¹å¡æ´›dropoutã€é›†æˆæ–¹æ³•å’Œé›†æˆè’™ç‰¹å¡æ´›dropoutæ¥è¯„ä¼°æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§å’Œè¾“å‡ºå¯é æ€§ï¼Œç»“æœè¡¨æ˜é›†æˆæ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§å¤„ç†ä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†åœ¨åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦è¯Šæ–­ä¸­èå…¥ä¸ç¡®å®šæ€§é‡åŒ–çš„é‡è¦æ€§ï¼Œä»¥æé«˜å®é™…ä¸´åºŠåº”ç”¨çš„æ€§èƒ½å’Œå¯ä¿¡åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çš®è‚¤ç™Œè‡ªåŠ¨åˆ†ç±»ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œä¸ç¡®å®šæ€§æ„è¯†ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†è¿ç§»å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>åœ¨å¤šç§é¢„è®­ç»ƒç‰¹å¾æå–å™¨ä¸ä¼ ç»Ÿåˆ†ç±»å™¨çš„ç»„åˆä¸­ï¼ŒåŸºäºCLIPçš„è§†è§‰å˜å‹å™¨è¡¨ç°æœ€ä½³ã€‚</li>
<li>è’™ç‰¹å¡æ´›dropoutã€é›†æˆæ–¹æ³•å’Œé›†æˆè’™ç‰¹å¡æ´›dropoutç­‰æ–¹æ³•è¢«ç”¨äºè¯„ä¼°æ¨¡å‹çš„ä¸ç¡®å®šæ€§å’Œå¯é æ€§ã€‚</li>
<li>é›†æˆæ–¹æ³•åœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>èå…¥ä¸ç¡®å®šæ€§é‡åŒ–å¯¹äºæé«˜æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ€§èƒ½å’Œå¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55c0a1f5d673342a6ea29fc4c274b6ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd9467ed64ecb27fd214f9983c4f4f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33882f121e5ea702e53adc27fdfdfdca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-360f0c3179ad241ea839a7185da6dc2d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Conditional-diffusion-models-for-guided-anomaly-detection-in-brain-images-using-fluid-driven-anomaly-randomization"><a href="#Conditional-diffusion-models-for-guided-anomaly-detection-in-brain-images-using-fluid-driven-anomaly-randomization" class="headerlink" title="Conditional diffusion models for guided anomaly detection in brain   images using fluid-driven anomaly randomization"></a>Conditional diffusion models for guided anomaly detection in brain   images using fluid-driven anomaly randomization</h2><p><strong>Authors:Ana Lawry Aguila, Peirong Liu, Oula Puonti, Juan Eugenio Iglesias</strong></p>
<p>Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our modelâ€™s ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased&#x2F;healthy images. </p>
<blockquote>
<p>ç›‘ç£æœºå™¨å­¦ä¹ å·²åœ¨è„‘MRIç—…ç†æ£€æµ‹ä¸­å®ç°äº†é«˜ç²¾åº¦ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦æ¥è‡ªæ‚£ç—…è€…çš„è®­ç»ƒæ•°æ®ï¼Œè¿™åœ¨ç½•è§ç–¾ç—…çš„æƒ…å†µä¸‹å¯èƒ½å¹¶ä¸å®¹æ˜“è·å¾—ã€‚åŸºäºé‡å»ºçš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå·²åœ¨åŒ»ç–—é¢†åŸŸå—åˆ°æ¬¢è¿ï¼Œå› ä¸ºå®ƒåªéœ€è¦åœ¨å¥åº·å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ç‰¹å®šç–¾ç—…çš„åºå¤§äººç¾¤çš„éœ€æ±‚ã€‚è¿™äº›æ–¹æ³•å‡è®¾åœ¨å¸¸è§„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ— æ³•å‡†ç¡®è¡¨ç¤ºæˆ–é‡å»ºå¼‚å¸¸ç°è±¡ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾ç»å¸¸åœ¨ä¸æ¨¡å‹æ— æ³•é‡å»ºå¥åº·ç»„ç»‡æˆ–å‡†ç¡®é‡å»ºå¼‚å¸¸åŒºåŸŸçš„æ¨¡å‹ä¸­å‡ºç°å¤±æ•ˆæƒ…å†µï¼Œå³æ— æ³•æ¶ˆé™¤å¼‚å¸¸ç°è±¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”¨äºå¼‚å¸¸æ£€æµ‹å’Œå¥åº·å›¾åƒé‡å»ºçš„æ–°å‹æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œé€‚ç”¨äºè„‘MRIã€‚æˆ‘ä»¬çš„å¼±ç›‘ç£æ–¹æ³•å°†åˆæˆç”Ÿæˆçš„ä¼ªç—…ç†å›¾åƒé›†æˆåˆ°å»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œä»¥æ›´å¥½åœ°æŒ‡å¯¼å¥åº·å›¾åƒçš„é‡å»ºã€‚ä¸ºäº†ç”Ÿæˆè¿™äº›ä¼ªç—…ç†å›¾åƒï¼Œæˆ‘ä»¬åº”ç”¨æµä½“é©±åŠ¨çš„å¼‚å¸¸éšæœºåŒ–æ¥å¢å¼ºè¾…åŠ©æ•°æ®é›†ä¸­çš„çœŸå®ç—…ç†åˆ†å‰²å›¾ï¼Œä»¥ç¡®ä¿åˆæˆå¼‚å¸¸ç°è±¡æ—¢çœŸå®åˆè§£å‰–ç»“æ„è¿è´¯ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆå¼‚å¸¸æ•°æ®é›†å’ŒATLASæ•°æ®é›†ä¸­çš„çœŸå®ç—…ç†æƒ…å†µæ¥è¯„ä¼°æˆ‘ä»¬æ¨¡å‹æ£€æµ‹ç—…ç†çš„èƒ½åŠ›ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å¦‚ä¸‹ï¼šï¼ˆiï¼‰å§‹ç»ˆä¼˜äºå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ä»¥åŠæœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„æ½œåœ¨æ‰©æ•£ï¼›ï¼ˆiiï¼‰åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¶…è¶Šäº†ä½¿ç”¨é…å¯¹ç–¾ç—…&#x2F;å¥åº·å›¾åƒçš„ç›‘ç£ä¿®å¤æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10233v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œç”¨äºåœ¨è„‘éƒ¨MRIä¸­è¿›è¡Œå¼‚å¸¸æ£€æµ‹å’Œå¥åº·å›¾åƒé‡å»ºã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¼±ç›‘ç£æ–¹æ³•ï¼Œé€šè¿‡åˆæˆç”Ÿæˆçš„ä¼ªç—…ç†å›¾åƒæ¥æ›´å¥½åœ°å¼•å¯¼å¥åº·å›¾åƒçš„é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç—…ç†æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›‘ç£æœºå™¨å­¦ä¹ åœ¨è„‘éƒ¨MRIç—…ç†æ£€æµ‹ä¸­è¡¨ç°å‡ºå‡†ç¡®æ€§ï¼Œä½†éœ€è¦ç–¾ç—…æ‚£è€…çš„è®­ç»ƒæ•°æ®ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¸æ˜“è·å¾—ï¼Œå¦‚ç½•è§ç–¾ç—…ã€‚</li>
<li>é‡å»ºå‹æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå·²åœ¨åŒ»å­¦é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬ä»…éœ€è¦ä½¿ç”¨å¥åº·å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ç‰¹å®šç–¾ç—…å¤§è§„æ¨¡é˜Ÿåˆ—çš„éœ€æ±‚ã€‚</li>
<li>å½“å‰æ–¹æ³•çš„ä¸€ä¸ªå‡è®¾æ˜¯ï¼Œè®­ç»ƒæœ‰ç´ æ¨¡å‹æ— æ³•å‡†ç¡®è¡¨ç¤ºæˆ–é‡å»ºå¼‚å¸¸ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾ç»å¸¸å¤±è´¥ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•é‡å»ºå¥åº·ç»„ç»‡æˆ–å‡†ç¡®é‡å»ºå¼‚å¸¸åŒºåŸŸã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹å’Œå¥åº·å›¾åƒé‡å»ºã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨å¼±ç›‘ç£æ–¹æ³•ï¼Œç»“åˆåˆæˆç”Ÿæˆçš„ä¼ªç—…ç†å›¾åƒæ¥æ”¹è¿›å¥åº·å›¾åƒçš„é‡å»ºè¿‡ç¨‹ã€‚</li>
<li>ä¸ºç”Ÿæˆè¿™äº›ä¼ªç—…ç†å›¾åƒï¼Œç ”ç©¶å›¢é˜Ÿåº”ç”¨æµä½“é©±åŠ¨å¼‚å¸¸éšæœºåŒ–æŠ€æœ¯ï¼Œå¢å¼ºæ¥è‡ªè¾…åŠ©æ•°æ®é›†çš„çœŸå®ç—…ç†åˆ†å‰²å›¾ï¼Œç¡®ä¿åˆæˆå¼‚å¸¸æ—¢ç°å®åˆè§£å‰–è¿è´¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f05fa6420a3004beb80f4cbf15b042c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08d7e9fec1db5ed9ed98c8b405e62b03.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Rethinking-Brain-Tumor-Segmentation-from-the-Frequency-Domain-Perspective"><a href="#Rethinking-Brain-Tumor-Segmentation-from-the-Frequency-Domain-Perspective" class="headerlink" title="Rethinking Brain Tumor Segmentation from the Frequency Domain   Perspective"></a>Rethinking Brain Tumor Segmentation from the Frequency Domain   Perspective</h2><p><strong>Authors:Minye Shao, Zeyu Wang, Haoran Duan, Yawen Huang, Bing Zhai, Shizheng Wang, Yang Long, Yefeng Zheng</strong></p>
<p>Precise segmentation of brain tumors, particularly contrast-enhancing regions visible in post-contrast MRI (areas highlighted by contrast agent injection), is crucial for accurate clinical diagnosis and treatment planning but remains challenging. However, current methods exhibit notable performance degradation in segmenting these enhancing brain tumor areas, largely due to insufficient consideration of MRI-specific tumor features such as complex textures and directional variations. To address this, we propose the Harmonized Frequency Fusion Network (HFF-Net), which rethinks brain tumor segmentation from a frequency-domain perspective. To comprehensively characterize tumor regions, we develop a Frequency Domain Decomposition (FDD) module that separates MRI images into low-frequency components, capturing smooth tumor contours and high-frequency components, highlighting detailed textures and directional edges. To further enhance sensitivity to tumor boundaries, we introduce an Adaptive Laplacian Convolution (ALC) module that adaptively emphasizes critical high-frequency details using dynamically updated convolution kernels. To effectively fuse tumor features across multiple scales, we design a Frequency Domain Cross-Attention (FDCA) integrating semantic, positional, and slice-specific information. We further validate and interpret frequency-domain improvements through visualization, theoretical reasoning, and experimental analyses. Extensive experiments on four public datasets demonstrate that HFF-Net achieves an average relative improvement of 4.48% (ranging from 2.39% to 7.72%) in the mean Dice scores across the three major subregions, and an average relative improvement of 7.33% (ranging from 5.96% to 8.64%) in the segmentation of contrast-enhancing tumor regions, while maintaining favorable computational efficiency and clinical applicability. Code: <a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/HFF">https://github.com/VinyehShaw/HFF</a>. </p>
<blockquote>
<p>ç²¾ç¡®åˆ†å‰²è„‘è‚¿ç˜¤ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹æ¯”å¢å¼ºMRIä¸­å¯è§åˆ°çš„å¯¹æ¯”å¢å¼ºåŒºåŸŸï¼ˆé€šè¿‡é€ å½±å‰‚æ³¨å°„çªå‡ºæ˜¾ç¤ºï¼‰ï¼Œå¯¹äºå‡†ç¡®çš„ä¸´åºŠè¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•åœ¨åˆ†å‰²è¿™äº›å¢å¼ºè„‘è‚¿ç˜¤åŒºåŸŸæ—¶çš„æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæœªèƒ½å……åˆ†è€ƒè™‘MRIç‰¹å®šçš„è‚¿ç˜¤ç‰¹å¾ï¼Œå¦‚å¤æ‚çš„çº¹ç†å’Œæ–¹å‘å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å’Œè°é¢‘ç‡èåˆç½‘ç»œï¼ˆHFF-Netï¼‰ï¼Œä»é¢‘ç‡åŸŸçš„è§’åº¦é‡æ–°æ€è€ƒè„‘è‚¿ç˜¤çš„åˆ†å‰²é—®é¢˜ã€‚ä¸ºäº†å…¨é¢åˆ»ç”»è‚¿ç˜¤åŒºåŸŸï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé¢‘ç‡åŸŸåˆ†è§£ï¼ˆFDDï¼‰æ¨¡å—ï¼Œå°†MRIå›¾åƒåˆ†è§£ä¸ºä½é¢‘æˆåˆ†ï¼Œæ•æ‰å¹³æ»‘çš„è‚¿ç˜¤è½®å»“å’Œé«˜é¢‘æˆåˆ†ï¼Œçªå‡ºè¯¦ç»†çš„çº¹ç†å’Œæ–¹å‘è¾¹ç¼˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯¹è‚¿ç˜¤è¾¹ç•Œçš„æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”æ‹‰æ™®æ‹‰æ–¯å·ç§¯ï¼ˆALCï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡åŠ¨æ€æ›´æ–°çš„å·ç§¯æ ¸è‡ªé€‚åº”åœ°çªå‡ºå…³é”®çš„é«˜é¢‘ç»†èŠ‚ã€‚ä¸ºäº†æœ‰æ•ˆåœ°èåˆè·¨å¤šä¸ªå°ºåº¦çš„è‚¿ç˜¤ç‰¹å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé¢‘ç‡åŸŸäº¤å‰æ³¨æ„ï¼ˆFDCAï¼‰æœºåˆ¶ï¼Œèåˆäº†è¯­ä¹‰ã€ä½ç½®å’Œåˆ‡ç‰‡ç‰¹å®šçš„ä¿¡æ¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å¯è§†åŒ–ã€ç†è®ºæ¨ç†å’Œå®éªŒåˆ†æéªŒè¯äº†é¢‘ç‡åŸŸæ”¹è¿›çš„æ•ˆæœã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHFF-Netåœ¨ä¸‰ä¸ªä¸»è¦äºšåŒºåŸŸçš„å¹³å‡Diceå¾—åˆ†å¹³å‡æé«˜äº†4.48%ï¼ˆèŒƒå›´ä»2.39%åˆ°7.72%ï¼‰ï¼Œåœ¨å¯¹æ¯”å¢å¼ºè‚¿ç˜¤åŒºåŸŸçš„åˆ†å‰²ä¸Šå¹³å‡æé«˜äº†7.33%ï¼ˆèŒƒå›´ä»5.96%åˆ°8.64%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œä¸´åºŠé€‚ç”¨æ€§ã€‚ä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/HFF%E3%80%82">https://github.com/VinyehShaw/HFFã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10142v1">PDF</a> Accepted by IEEE Transactions on Medical Imaging</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHFF-Netçš„ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œç”¨äºä»é¢‘ç‡åŸŸè§’åº¦é‡æ–°æ€è€ƒè„‘è‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚é€šè¿‡åˆ†è§£MRIå›¾åƒçš„ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ä»¥åŠé‡‡ç”¨è‡ªé€‚åº”æ‹‰æ™®æ‹‰æ–¯å·ç§¯æ¨¡å—ï¼Œè¯¥æ–¹æ³•èƒ½æ›´å…¨é¢åœ°åˆ»ç”»è‚¿ç˜¤åŒºåŸŸï¼Œå¹¶å¼ºè°ƒè‚¿ç˜¤è¾¹ç•Œçš„æ•æ„Ÿæ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡é¢‘ç‡åŸŸäº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆå¤šå°ºåº¦è‚¿ç˜¤ç‰¹å¾ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç²¾ç¡®åˆ†å‰²MRIä¸­çš„å¯¹æ¯”å¢å¼ºè„‘è‚¿ç˜¤åŒºåŸŸå¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨åˆ†å‰²è¿™äº›å¢å¼ºè„‘è‚¿ç˜¤åŒºåŸŸæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæœªèƒ½å……åˆ†è€ƒè™‘MRIç‰¹å®šçš„è‚¿ç˜¤ç‰¹å¾ã€‚</li>
<li>HFF-Netæ¡†æ¶ä»é¢‘ç‡åŸŸè§’åº¦é‡æ–°æ€è€ƒè„‘è‚¿ç˜¤åˆ†å‰²ï¼Œå¹¶è®¾è®¡äº†ä¸€ç³»åˆ—æ¨¡å—æ¥å¤„ç†MRIå›¾åƒçš„ä¸åŒé¢‘ç‡æˆåˆ†ã€‚</li>
<li>FDDæ¨¡å—å°†MRIå›¾åƒåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œä»¥å…¨é¢åˆ»ç”»è‚¿ç˜¤åŒºåŸŸå¹¶å¼ºè°ƒè¯¦ç»†çš„çº¹ç†å’Œæ–¹å‘è¾¹ç¼˜ã€‚</li>
<li>ALCæ¨¡å—é€šè¿‡è‡ªé€‚åº”å¼ºè°ƒå…³é”®çš„é«˜é¢‘ç»†èŠ‚æ¥æé«˜å¯¹è‚¿ç˜¤è¾¹ç•Œçš„æ•æ„Ÿæ€§ã€‚</li>
<li>FDCAæ¨¡å—é€šè¿‡èåˆå¤šå°ºåº¦çš„è‚¿ç˜¤ç‰¹å¾æ¥æé«˜é¢‘ç‡åŸŸçš„æ”¹è¿›æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4cbf05a7a7b2fd30a3dd0c7072684e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39562c9e37b86fbbfe7705810d2fe3f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6946600d5dd96071e2a33fc271419dcf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CINeMA-Conditional-Implicit-Neural-Multi-Modal-Atlas-for-a-Spatio-Temporal-Representation-of-the-Perinatal-Brain"><a href="#CINeMA-Conditional-Implicit-Neural-Multi-Modal-Atlas-for-a-Spatio-Temporal-Representation-of-the-Perinatal-Brain" class="headerlink" title="CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a   Spatio-Temporal Representation of the Perinatal Brain"></a>CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a   Spatio-Temporal Representation of the Perinatal Brain</h2><p><strong>Authors:Maik Dannecker, Vasiliki Sideri-Lampretsa, Sophie Starck, Angeline Mihailov, Mathieu Milh, Nadine Girard, Guillaume Auzias, Daniel Rueckert</strong></p>
<p>Magnetic resonance imaging of fetal and neonatal brains reveals rapid neurodevelopment marked by substantial anatomical changes unfolding within days. Studying this critical stage of the developing human brain, therefore, requires accurate brain models-referred to as atlases-of high spatial and temporal resolution. To meet these demands, established traditional atlases and recently proposed deep learning-based methods rely on large and comprehensive datasets. This poses a major challenge for studying brains in the presence of pathologies for which data remains scarce. We address this limitation with CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for creating high-resolution, spatio-temporal, multimodal brain atlases, suitable for low-data settings. Unlike established methods, CINeMA operates in latent space, avoiding compute-intensive image registration and reducing atlas construction times from days to minutes. Furthermore, it enables flexible conditioning on anatomical features including GA, birth age, and pathologies like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA supports downstream tasks such as tissue segmentation and age prediction whereas its generative properties enable synthetic data creation and anatomically informed data augmentation. Surpassing state-of-the-art methods in accuracy, efficiency, and versatility, CINeMA represents a powerful tool for advancing brain research. We release the code and atlases at <a target="_blank" rel="noopener" href="https://github.com/m-dannecker/CINeMA">https://github.com/m-dannecker/CINeMA</a>. </p>
<blockquote>
<p>èƒå„¿å’Œæ–°ç”Ÿå„¿å¤§è„‘çš„ç£å…±æŒ¯æˆåƒæ˜¾ç¤ºï¼Œç¥ç»å‘è‚²è¿…é€Ÿï¼Œå‡ å¤©å†…è§£å‰–ç»“æ„ä¼šå‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚å› æ­¤ï¼Œç ”ç©¶äººç±»å¤§è„‘å‘è‚²çš„è¿™ä¸ªå…³é”®é˜¶æ®µéœ€è¦å‡†ç¡®çš„å¤§è„‘æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹è¢«ç§°ä¸ºå›¾è°±ï¼Œéœ€è¦å…·æœ‰é«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ã€‚ä¸ºäº†æ»¡è¶³è¿™äº›è¦æ±‚ï¼Œå·²å»ºç«‹çš„ä¼ ç»Ÿå›¾è°±å’Œæœ€è¿‘æå‡ºçš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•éƒ½ä¾èµ–äºå¤§å‹ä¸”å…¨é¢çš„æ•°æ®é›†ã€‚è¿™å¯¹äºåœ¨å­˜åœ¨ç—…ç†æƒ…å†µä¸‹ç ”ç©¶å¤§è„‘æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œæ­¤ç±»æ•°æ®ä»ç„¶ç¨€ç¼ºã€‚æˆ‘ä»¬é€šè¿‡CINeMAï¼ˆæ¡ä»¶éšç¥ç»å¤šæ¨¡æ€å›¾è°±ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œè¯¥æ¡†æ¶é€‚ç”¨äºåˆ›å»ºé«˜åˆ†è¾¨ç‡ã€æ—¶ç©ºã€å¤šæ¨¡æ€çš„å¤§è„‘å›¾è°±ï¼Œé€‚ç”¨äºæ•°æ®ç¨€ç¼ºçš„è®¾ç½®ã€‚ä¸åŒäºå·²å»ºç«‹çš„æ–¹æ³•ï¼ŒCINeMAåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œé¿å…äº†è®¡ç®—å¯†é›†å‹çš„å›¾åƒé…å‡†ï¼Œå¹¶å°†å›¾è°±æ„å»ºæ—¶é—´ä»å‡ å¤©ç¼©çŸ­åˆ°å‡ åˆ†é’Ÿã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥æ ¹æ®è§£å‰–ç‰¹å¾è¿›è¡Œçµæ´»è°ƒæ•´ï¼ŒåŒ…æ‹¬èƒé¾„ã€å‡ºç”Ÿå¹´é¾„ä»¥åŠè„‘å®¤æ‰©å¤§å’Œèƒ¼èƒä½“å‘è‚²ä¸è‰¯ç­‰ç—…ç†ç‰¹å¾ã€‚CINeMAæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚ç»„ç»‡åˆ†å‰²å’Œå¹´é¾„é¢„æµ‹ï¼Œè€Œå…¶ç”Ÿæˆå±æ€§åˆ™å¯å®ç°åˆæˆæ•°æ®çš„åˆ›å»ºå’Œè§£å‰–ä¿¡æ¯çš„æ•°æ®å¢å¼ºã€‚åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€šç”¨æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒCINeMAæ˜¯æ¨è¿›å¤§è„‘ç ”ç©¶çš„æœ‰åŠ›å·¥å…·ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/m-dannecker/CINeMA">https://github.com/m-dannecker/CINeMA</a>å‘å¸ƒä»£ç å’Œå›¾è°±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09668v1">PDF</a> Work currently under revision for IEEE TMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨èƒå„¿å’Œæ–°ç”Ÿå„¿å¤§è„‘å‘è‚²çš„ç¥ç»å‘å±•é˜¶æ®µä¸­ï¼Œåˆ©ç”¨ç£å…±æŒ¯æˆåƒæ­ç¤ºå…¶è¿…é€Ÿå˜åŒ–å’Œæ˜æ˜¾çš„è§£å‰–ç»“æ„å˜åŒ–ã€‚ä¸ºç ”ç©¶è¿™ä¸€è¿‡ç¨‹éœ€è¦é«˜æ—¶ç©ºåˆ†è¾¨ç‡çš„å¤§è„‘æ¨¡å‹â€”â€”å›¾è°±ã€‚ä¼ ç»Ÿå›¾è°±å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡ç»¼åˆæ•°æ®é›†ï¼Œè¿™ç»™ç ”ç©¶å­˜åœ¨ç—…ç†æƒ…å†µçš„å¤§è„‘å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CINeMAï¼ˆæ¡ä»¶éšç¥ç»å¤šæ¨¡æ€å›¾è°±ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€‚ç”¨äºä½æ•°æ®ç¯å¢ƒä¸‹çš„é«˜æ—¶ç©ºåˆ†è¾¨ç‡å¤šæ¨¡æ€å¤§è„‘å›¾è°±åˆ›å»ºã€‚CINeMAåœ¨æ½œåœ¨ç©ºé—´æ“ä½œï¼Œé¿å…äº†è®¡ç®—å¯†é›†å‹çš„å›¾åƒé…å‡†ï¼Œå°†å›¾è°±æ„å»ºæ—¶é—´ä»å‡ å¤©ç¼©çŸ­åˆ°å‡ åˆ†é’Ÿã€‚åŒæ—¶æ”¯æŒçµæ´»è°ƒèŠ‚è§£å‰–ç‰¹å¾ï¼Œå¦‚èƒé¾„ã€å‡ºç”Ÿå¹´é¾„ã€è„‘ç§¯æ°´ç­‰ç–¾ç—…ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡å¦‚ç»„ç»‡åˆ†å‰²å’Œå¹´é¾„é¢„æµ‹æä¾›æ”¯æŒã€‚æ­¤å¤–ï¼ŒCINeMAè¿˜å…·æœ‰ç”Ÿæˆæ€§ç‰¹ç‚¹ï¼Œå¯åˆ›å»ºåˆæˆæ•°æ®å¹¶å¢å¼ºè§£å‰–å­¦ä¿¡æ¯ã€‚å…¶åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œçµæ´»æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæˆä¸ºæ¨åŠ¨å¤§è„‘ç ”ç©¶çš„æœ‰åŠ›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>èƒå„¿å’Œæ–°ç”Ÿå„¿å¤§è„‘å‘è‚²è¿‡ç¨‹ä¸­å­˜åœ¨å¿«é€Ÿç¥ç»å‘å±•å’Œè§£å‰–ç»“æ„å˜åŒ–ã€‚</li>
<li>éœ€è¦é«˜æ—¶ç©ºåˆ†è¾¨ç‡çš„å¤§è„‘æ¨¡å‹å›¾è°±æ¥ç ”ç©¶è¿™ä¸€è¿‡ç¨‹ã€‚</li>
<li>ä¼ ç»Ÿå›¾è°±å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡ç»¼åˆæ•°æ®é›†ï¼Œç»™ç ”ç©¶å­˜åœ¨ç—…ç†æƒ…å†µçš„å¤§è„‘å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>CINeMAæ¡†æ¶é€‚ç”¨äºä½æ•°æ®ç¯å¢ƒä¸‹çš„é«˜æ—¶ç©ºåˆ†è¾¨ç‡å¤šæ¨¡æ€å¤§è„‘å›¾è°±åˆ›å»ºã€‚</li>
<li>CINeMAåœ¨æ½œåœ¨ç©ºé—´æ“ä½œï¼Œé¿å…äº†è®¡ç®—å¯†é›†å‹çš„å›¾åƒé…å‡†ï¼Œç¼©çŸ­å›¾è°±æ„å»ºæ—¶é—´ã€‚</li>
<li>CINeMAæ”¯æŒçµæ´»è°ƒèŠ‚è§£å‰–ç‰¹å¾ï¼ŒåŒ…æ‹¬èƒé¾„ã€å‡ºç”Ÿå¹´é¾„å’ŒæŸäº›ç—…ç†æƒ…å†µã€‚</li>
<li>CINeMAå¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡å¦‚ç»„ç»‡åˆ†å‰²å’Œå¹´é¾„é¢„æµ‹ï¼Œå¹¶å…·æœ‰ç”Ÿæˆæ€§ç‰¹ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f1166a186fface9229094ccd22d53ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f6cbb3d4c0494121d57a34c7fd94f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b68246de40b036d0271ba1849dc0365b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bbf001b2481b9ff88500a7ab199fcd7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Determining-the-origin-of-the-X-ray-emission-in-blazars-through-multiwavelength-polarization"><a href="#Determining-the-origin-of-the-X-ray-emission-in-blazars-through-multiwavelength-polarization" class="headerlink" title="Determining the origin of the X-ray emission in blazars through   multiwavelength polarization"></a>Determining the origin of the X-ray emission in blazars through   multiwavelength polarization</h2><p><strong>Authors:Ioannis Liodakis, Haocheng Zhang, Stella Boula, Riccardo Middei, Jorge Otero-Santos, Dmitry Blinov, IvÃ¡n Agudo, Markus BÃ¶ttcher, Chien-Ting Chen, Steven R. Ehlert, Svetlana G. Jorstad, Philip Kaaret, Henric Krawczynski, Abel L. Peirson, Roger W. Romani, Fabrizio Tavecchio, Martin C. Weisskopf, Pouya M. Kouch, Elina Lindfors, Kari Nilsson, Callum McCall, Helen E. Jermak, Iain A. Steele, Ioannis Myserlis, Mark Gurwell, Garrett K. Keating, Ramprasad Rao, Sincheol Kang, Sang-Sung Lee, Sanghyun Kim, Whee Yeon Cheong, Hyeon-Woo Jeong, Emmanouil Angelakis, Alexander Kraus, Francisco JosÃ© Aceituno, Giacomo Bonnoli, VÃ­ctor Casanova, Juan Escudero, Beatriz AgÃ­s-GonzÃ¡lez, Daniel Morcuende, Alfredo Sota, Rumen Bachev, Tatiana S. Grishina, Evgenia N. Kopatskaya, Elena G. Larionova, Daria A. Morozova, Sergey S. Savchenko, Ekaterina V. Shishkina, Ivan S. Troitskiy, Yulia V. Troitskaya, Andrey A. Vasilyev</strong></p>
<p>The origin of the high-energy emission in astrophysical jets from black holes is a highly debated issue. This is particularly true for jets from supermassive black holes that are among the most powerful particle accelerators in the Universe. So far, the addition of new observations and new messengers have only managed to create more questions than answers. However, the newly available X-ray polarization observations promise to finally distinguish between emission models. We use extensive multiwavelength and polarization campaigns as well as state-of-the-art polarized spectral energy distribution models to attack this problem by focusing on two X-ray polarization observations of blazar BL Lacertae in flaring and quiescent $\gamma$-ray states. We find that regardless of the jet composition and underlying emission model, inverse-Compton scattering from relativistic electrons dominates at X-ray energies. </p>
<blockquote>
<p>é»‘æ´å¼•èµ·çš„å¤©ä½“ç‰©ç†å–·æµçš„é«˜èƒ½å‘å°„èµ·æºæ˜¯ä¸€ä¸ªå¤‡å—äº‰è®®çš„é—®é¢˜ã€‚è¿™åœ¨è¶…å¤§å‹é»‘æ´å¼•å‘çš„å–·æµä¸­å°¤ä¸ºçªå‡ºï¼Œè¶…å¤§å‹é»‘æ´æ˜¯å®‡å®™ä¸­æœ€ä¸ºå¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ä¹‹ä¸€ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œæ–°è§‚æµ‹å’Œæ–°ä¿¡ä½¿çš„å¢åŠ è™½ç„¶å¼•å‘äº†æ›´å¤šçš„é—®é¢˜ï¼Œä½†å°šæœªå¾—åˆ°æ˜ç¡®çš„ç­”æ¡ˆã€‚ç„¶è€Œï¼Œæœ€æ–°å¯ç”¨çš„Xå°„çº¿åæŒ¯è§‚æµ‹æœ‰æœ›åœ¨å‘å°„æ¨¡å‹ä¹‹é—´åšå‡ºæœ€ç»ˆåŒºåˆ†ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å¤šæ³¢é•¿å’ŒåæŒ¯æ´»åŠ¨ä»¥åŠæœ€å…ˆè¿›çš„åæŒ¯è°±èƒ½é‡åˆ†å¸ƒæ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨BL Lacertaeè€€æ–‘å’Œé™æ­¢Î³å°„çº¿æ€çš„ä¸¤ä¸ªXå°„çº¿åæŒ¯è§‚æµ‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ— è®ºå–·æµæˆåˆ†å’Œæ½œåœ¨å‘å°„æ¨¡å‹å¦‚ä½•ï¼Œç›¸å¯¹è®ºæ€§ç”µå­çš„é€†åº·æ™®é¡¿æ•£å°„åœ¨Xå°„çº¿èƒ½é‡æ®µå ä¸»å¯¼åœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13603v2">PDF</a> 10 pages, 15 figures, 4 Tables, published in A&amp;A</p>
<p><strong>Summary</strong><br>     å¤©æ–‡ç‰©ç†å­¦ä¸­çš„é»‘æ´å°„æµé«˜èƒ½é‡å‘å°„èµ·æºé—®é¢˜å¤‡å—äº‰è®®ï¼Œå°¤å…¶æ˜¯è¶…å¤§è´¨é‡é»‘æ´çš„å°„æµï¼Œå®ƒä»¬æ˜¯å®‡å®™ä¸­æœ€ä¸ºå¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ä¹‹ä¸€ã€‚æ–°çš„è§‚æµ‹å’Œè®¯æ¯çš„å¢åŠ å¹¶æœªç»™å‡ºæ˜ç¡®ç­”æ¡ˆï¼Œä½†æ–°å¯ç”¨çš„Xå°„çº¿åæŒ¯è§‚æµ‹å¯èƒ½æœ‰åŠ©äºåŒºåˆ†å‘å°„æ¨¡å‹ã€‚æœ¬ç ”ç©¶é€šè¿‡å…¨é¢ä½¿ç”¨å¤šæ³¢é•¿å’ŒåæŒ¯æ´»åŠ¨åŠæœ€æ–°çš„åæŒ¯è°±èƒ½é‡åˆ†å¸ƒæ¨¡å‹ï¼Œé‡ç‚¹å¯¹è€€æ–‘BL Lacertaeçš„ä¸¤ç§Xå°„çº¿åæŒ¯è§‚æµ‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°åœ¨Xå°„çº¿èƒ½é‡ä¸‹ï¼Œæ— è®ºå°„æµç»„æˆå’Œåº•å±‚å‘å°„æ¨¡å‹å¦‚ä½•ï¼Œç›¸å¯¹è®ºæ€§ç”µå­çš„é€†åº·æ™®é¡¿æ•£å°„å ä¸»å¯¼åœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é»‘æ´å°„æµçš„é«˜èƒ½é‡å‘å°„èµ·æºæ˜¯ä¸€ä¸ªå¤‡å—äº‰è®®çš„é—®é¢˜ã€‚</li>
<li>è¶…å¤§è´¨é‡é»‘æ´çš„å°„æµæ˜¯å®‡å®™ä¸­æœ€ä¸ºå¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ä¹‹ä¸€ã€‚</li>
<li>ç›®å‰çš„æ–°è§‚æµ‹å’Œè®¯æ¯å¹¶æœªç»™å‡ºå…³äºé«˜èƒ½é‡å‘å°„èµ·æºçš„æ˜ç¡®ç­”æ¡ˆã€‚</li>
<li>Xå°„çº¿åæŒ¯è§‚æµ‹æœ‰åŠ©äºåŒºåˆ†ä¸åŒçš„å‘å°„æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†å¤šæ³¢é•¿å’ŒåæŒ¯æ´»åŠ¨ä»¥åŠåæŒ¯è°±èƒ½é‡åˆ†å¸ƒæ¨¡å‹è¿›è¡Œåˆ†æã€‚</li>
<li>ç ”ç©¶é‡ç‚¹æ¯”è¾ƒäº†è€€æ–‘BL Lacertaeåœ¨ä¸¤ç§ä¸åŒçŠ¶æ€ä¸‹çš„Xå°„çº¿åæŒ¯è§‚æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e96198cdc3e0bad6ab1c06de53483607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff7be42c3c7845309ed30f4fa863d058.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7892e1fb2e6261656d01fd118b2ebd24.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Detection-Prospects-of-Electromagnetic-Signatures-from-OGLE-2011-BLG-0462"><a href="#Detection-Prospects-of-Electromagnetic-Signatures-from-OGLE-2011-BLG-0462" class="headerlink" title="Detection Prospects of Electromagnetic Signatures from   OGLE-2011-BLG-0462"></a>Detection Prospects of Electromagnetic Signatures from   OGLE-2011-BLG-0462</h2><p><strong>Authors:Shigeo S. Kimura, Lena Murchikova, Kailash C. Sahu</strong></p>
<p>Stellar-mass isolated black holes (IsoBHs) wandering in interstellar medium (ISM) are expected to be abundant in our Galaxy. Recently, an IsoBH, OGLE-2011-BLG-0462, was unambiguously discovered using astrometric microlensing. We examine prospects for detecting electromagnetic signatures from an accretion flow surrounding the IsoBH. The accretion rate onto the IsoBH should be highly sub-Eddington, which leads to formation of a hot accretion flow. In this paper, we evaluate the detectability of electromagnetic signals from the hot accretion flows in two accretion states: magnetically arrested disk (MAD) and classical radiatively inefficient accretion flows (RIAFs). For the MAD scenario, we find that the optical, infrared, and X-ray signals can be detectable by the current best facilities, such as HST, JWST, and Chandra, if the IsoBH is in a warm neutral medium. In contrast, for the classical RIAF scenario, the optical and X-ray emissions are weaker than MAD scenario, leading to unobservable signals for a typical parameter set. Future follow-up observations of OGLE-2011-BLG-0462 will provide a good test for theories of accretion processes. </p>
<blockquote>
<p>æ˜Ÿé™…ä»‹è´¨ä¸­æ¼«æ¸¸çš„æ’æ˜Ÿè´¨é‡å­¤ç«‹é»‘æ´ï¼ˆIsoBHsï¼‰åœ¨æˆ‘ä»¬çš„é“¶æ²³ç³»ä¸­é¢„è®¡éå¸¸ä¸°å¯Œã€‚æœ€è¿‘ï¼Œä½¿ç”¨å¤©ä½“æµ‹é‡å¾®é€é•œæŠ€æœ¯æ˜ç¡®åœ°å‘ç°äº†IsoBHï¼Œå³OGLE-2011-BLG-0462ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ£€æµ‹ç¯ç»•IsoBHçš„å¸ç§¯æµç”µç£ç‰¹å¾çš„å¯èƒ½æ€§ã€‚IsoBHä¸Šçš„å¸ç§¯ç‡åº”è¯¥é«˜åº¦ä½äºçˆ±ä¸é¡¿æé™ï¼Œå¯¼è‡´å½¢æˆçƒ­å¸ç§¯æµã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§å¸ç§¯çŠ¶æ€ä¸‹çƒ­å¸ç§¯æµçš„ç”µç£ä¿¡å·çš„å¯æ£€æµ‹æ€§ï¼šç£åœæ»ç›˜ï¼ˆMADï¼‰å’Œç»å…¸è¾å°„æ•ˆç‡ä½ä¸‹çš„å¸ç§¯æµï¼ˆRIAFsï¼‰ã€‚å¯¹äºMADæƒ…å†µï¼Œæˆ‘ä»¬å‘ç°å¦‚æœIsoBHå¤„äºæ¸©æš–ä¸­æ€§ä»‹è´¨ä¸­ï¼Œé€šè¿‡ç›®å‰æœ€å¥½çš„è®¾æ–½ï¼Œå¦‚HSTã€JWSTå’ŒChandraï¼Œå¯ä»¥æ£€æµ‹åˆ°å…‰å­¦ã€çº¢å¤–å’ŒXå°„çº¿ä¿¡å·ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¯¹äºç»å…¸çš„RIAFæƒ…å†µï¼Œå…‰å­¦å’ŒXå°„çº¿å‘å°„æ¯”MADæƒ…å†µå¼±ï¼Œå¯¼è‡´å¯¹äºå…¸å‹çš„å‚æ•°é›†æ¥è¯´ä¿¡å·æ— æ³•è§‚æµ‹ã€‚å¯¹OGLE-2011-BLG-0462çš„æœªæ¥åç»­è§‚æµ‹å°†ä¸ºå¸ç§¯è¿‡ç¨‹çš„ç†è®ºæä¾›è‰¯å¥½çš„æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01172v3">PDF</a> 10 pages, 2 figures, 1 table, accepted for publication in ApJ</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ’æ˜Ÿè´¨é‡å­¤ç«‹é»‘æ´ï¼ˆIsoBHï¼‰åœ¨æ˜Ÿé™…ä»‹è´¨ï¼ˆISMï¼‰ä¸­çš„ç”µç£ä¿¡å·æ¢æµ‹å¯èƒ½æ€§ã€‚ç ”ç©¶è®¤ä¸ºï¼Œå¯¹äºå­¤ç«‹é»‘æ´ï¼Œå…¶å¸ç§¯ç‡åº”è¯¥é«˜åº¦ä½äºçˆ±ä¸é¡¿æé™ï¼Œå½¢æˆçƒ­å¸ç§¯æµã€‚æ–‡ç« è¯„ä¼°äº†ä¸¤ç§å¸ç§¯çŠ¶æ€ä¸‹çƒ­å¸ç§¯æµçš„ç”µç£ä¿¡å·æ¢æµ‹æ€§ï¼Œå³ç£åœæ»ç›˜ï¼ˆMADï¼‰å’Œç»å…¸è¾å°„ä½æ•ˆå¸ç§¯æµï¼ˆRIAFsï¼‰ã€‚å¯¹äºMADæƒ…æ™¯ï¼Œè‹¥å­¤ç«‹é»‘æ´å¤„äºæ¸©æš–ä¸­æ€§ä»‹è´¨ä¸­ï¼Œå…‰å­¦ã€çº¢å¤–å’ŒXå°„çº¿ä¿¡å·å¯è¢«å“ˆå‹ƒæœ›è¿œé•œã€è©¹å§†æ–¯éŸ¦ä¼¯ç©ºé—´æœ›è¿œé•œå’Œé’±å¾·æ‹‰ç­‰ç°æœ‰æœ€ä½³è®¾æ–½æ¢æµ‹åˆ°ã€‚ç„¶è€Œï¼Œå¯¹äºç»å…¸RIAFæƒ…æ™¯ï¼Œå…‰å­¦å’ŒXå°„çº¿å‘å°„è¾ƒMADæƒ…æ™¯æ›´å¼±ï¼Œå¯¹äºå…¸å‹å‚æ•°é›†æ¥è¯´ï¼Œä¿¡å·æ— æ³•è§‚æµ‹ã€‚å¯¹OGLE-2011-BLG-0462çš„åç»­è§‚æµ‹å°†ä¸ºå¸ç§¯è¿‡ç¨‹çš„ç†è®ºæä¾›è‰¯å¥½çš„æµ‹è¯•æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ’æ˜Ÿè´¨é‡å­¤ç«‹é»‘æ´ï¼ˆIsoBHï¼‰åœ¨é“¶æ²³ç³»ä¸­é¢„è®¡æ•°é‡ä¸°å¯Œï¼Œæœ€è¿‘é€šè¿‡å¤©ä½“æµ‹é‡å¾®é€é•œæ³•å‘ç°äº†å…¶ä¸­ä¸€ä¸ªå®ä¾‹ï¼Œåä¸ºOGLE-2011-BLG-0462ã€‚</li>
<li>IsoBHçš„å¸ç§¯ç‡é«˜åº¦ä½äºçˆ±ä¸é¡¿æé™ï¼Œå½¢æˆçƒ­å¸ç§¯æµã€‚</li>
<li>è¯„ä¼°äº†ä¸¤ç§å¸ç§¯çŠ¶æ€ä¸‹çš„ç”µç£ä¿¡å·æ¢æµ‹æ€§ï¼šç£åœæ»ç›˜ï¼ˆMADï¼‰å’Œç»å…¸è¾å°„ä½æ•ˆå¸ç§¯æµï¼ˆRIAFsï¼‰ã€‚</li>
<li>åœ¨MADæƒ…æ™¯ä¸‹ï¼Œè‹¥IsoBHå¤„äºæ¸©æš–ä¸­æ€§ä»‹è´¨ä¸­ï¼Œå…‰å­¦ã€çº¢å¤–å’ŒXå°„çº¿ä¿¡å·å¯èƒ½è¢«ç°æœ‰è®¾æ–½æ¢æµ‹åˆ°ã€‚</li>
<li>ç»å…¸RIAFæƒ…æ™¯ä¸‹çš„å…‰å­¦å’ŒXå°„çº¿å‘å°„è¾ƒMADæƒ…æ™¯æ›´å¼±ï¼Œå¯èƒ½éš¾ä»¥è§‚æµ‹ã€‚</li>
<li>OGLE-2011-BLG-0462çš„åç»­è§‚æµ‹å°†å¯¹å¸ç§¯è¿‡ç¨‹çš„ç†è®ºæä¾›é‡è¦çš„éªŒè¯æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ad74db7c8055517b625186135c68634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-337fd2b2ba440130bb40c9f7528b344b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54653f5767a8ea25d097b3815307e586.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a2cc3b7cc2fda0cd3f8269c99c960717.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  Enhancing Medical Dialogue Generation through Knowledge Refinement and   Dynamic Prompt Adjustment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-afb3e0f9e81b089497a900201e0aa3c3.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  SceneCompleter Dense 3D Scene Completion for Generative Novel View   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
