<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  SVAD From Single Image to 3D Avatar via Synthetic Data Generation with   Video Diffusion and Data Augmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e120d180d577e724d91f96249920b3eb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-10-æ›´æ–°"><a href="#2025-05-10-æ›´æ–°" class="headerlink" title="2025-05-10 æ›´æ–°"></a>2025-05-10 æ›´æ–°</h1><h2 id="SVAD-From-Single-Image-to-3D-Avatar-via-Synthetic-Data-Generation-with-Video-Diffusion-and-Data-Augmentation"><a href="#SVAD-From-Single-Image-to-3D-Avatar-via-Synthetic-Data-Generation-with-Video-Diffusion-and-Data-Augmentation" class="headerlink" title="SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with   Video Diffusion and Data Augmentation"></a>SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with   Video Diffusion and Data Augmentation</h2><p><strong>Authors:Yonwoo Choi</strong></p>
<p>Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input. </p>
<blockquote>
<p>åˆ›å»ºé«˜è´¨é‡çš„å¯åŠ¨ç”»ä¸‰ç»´äººç±»è§’è‰²ï¼ˆAvatarï¼‰ä»å•å¼ å›¾åƒä¸­ä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºä»å•ä¸€è§†è§’é‡å»ºå®Œæ•´çš„ä¸‰ç»´ä¿¡æ¯å­˜åœ¨å›ºæœ‰çš„éš¾åº¦ã€‚å½“å‰çš„æ–¹æ³•å­˜åœ¨ä¸€ä¸ªæ˜æ˜¾çš„å±€é™æ€§ï¼šä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰æŠ€æœ¯è™½ç„¶èƒ½äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼Œä½†éœ€è¦å¤šä¸ªè§†è§’æˆ–è§†é¢‘åºåˆ—ï¼Œè€Œè§†é¢‘æ‰©æ•£æ¨¡å‹è™½ç„¶å¯ä»¥ä»å•ä¸ªå›¾åƒç”ŸæˆåŠ¨ç”»ï¼Œä½†åœ¨ä¿æŒä¸€è‡´æ€§å’Œèº«ä»½è¯†åˆ«æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•SVADï¼Œå®ƒç»“åˆäº†ç°æœ‰æŠ€æœ¯çš„ä¼˜åŠ¿æ¥è§£å†³è¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è§†é¢‘æ‰©æ•£ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡èº«ä»½è¯†åˆ«å’Œå›¾åƒæ¢å¤æ¨¡å—å¯¹å…¶è¿›è¡Œå¢å¼ºï¼Œå¹¶åˆ©ç”¨è¿™äº›ç»è¿‡æ”¹è¿›çš„æ•°æ®æ¥è®­ç»ƒä¸‰ç»´é«˜æ–¯å¹³é“ºè§’è‰²ã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼ŒSVADåœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§å’Œç²¾ç»†ç»†èŠ‚æ–¹é¢ä¼˜äºå½“å‰å•å›¾åƒæ–¹æ³•ï¼Œå¹¶åœ¨æ–°å‹å§¿åŠ¿å’Œè§†è§’ä¹‹é—´å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†å®æ—¶æ¸²æŸ“åŠŸèƒ½ã€‚é€šè¿‡æˆ‘ä»¬çš„æ•°æ®å¢å¼ºæµç¨‹ï¼Œæˆ‘ä»¬å…‹æœäº†ä¼ ç»Ÿä¸‰ç»´é«˜æ–¯å¹³é“ºæ–¹æ³•é€šå¸¸å¯¹å¯†é›†çš„å•çœ¼æˆ–å¤šè§†è§’è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§å¯¹æ¯”æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡æœ‰æ•ˆåœ°ç»“åˆæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸ä¸‰ç»´é«˜æ–¯å¹³é“ºçš„é«˜è´¨é‡ç»“æœå’Œæ¸²æŸ“æ•ˆç‡ï¼Œæˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†ä¸€ç§ä»å•å¼ å›¾åƒè¾“å…¥ç”Ÿæˆé«˜è´¨é‡è§’è‰²ï¼ˆAvatarï¼‰çš„æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05475v1">PDF</a> Accepted by CVPR 2025 SyntaGen Workshop, Project Page:   <a target="_blank" rel="noopener" href="https://yc4ny.github.io/SVAD/">https://yc4ny.github.io/SVAD/</a></p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºå•å¼ å›¾åƒåˆ›å»ºé«˜è´¨é‡çš„åŠ¨æ€ä¸‰ç»´äººç±»è§’è‰²ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼šä¸‰ç»´é«˜æ–¯è´´ç‰‡æ³•ï¼ˆ3DGSï¼‰è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡ç»“æœä½†éœ€è¦å¤šè§†è§’æˆ–è§†é¢‘åºåˆ—ï¼Œè€Œè§†é¢‘æ‰©æ•£æ¨¡å‹è™½èƒ½ä»å•å¼ å›¾åƒç”ŸæˆåŠ¨ç”»ä½†ç¼ºä¹ä¸€è‡´æ€§å’Œèº«ä»½ä¿ç•™ã€‚æˆ‘ä»¬æå‡ºSVADæ–¹æ³•ï¼Œç»“åˆç°æœ‰æŠ€æœ¯çš„ä¼˜åŠ¿è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è§†é¢‘æ‰©æ•£ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡èº«ä»½ä¿ç•™å’Œå›¾åƒæ¢å¤æ¨¡å—è¿›è¡Œå¢å¼ºï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒä¸‰ç»´é«˜æ–¯è´´ç‰‡è§’è‰²ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒSVADåœ¨ç»´æŒèº«ä»½ä¸€è‡´æ€§å’Œç»†èŠ‚æ–¹é¢ä¼˜äºå•å›¾åƒæ–¹æ³•ï¼Œåœ¨æ–°å§¿æ€å’Œè§†è§’ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å…·å¤‡å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®å¢å¼ºæµç¨‹å…‹æœäº†ä¼ ç»Ÿä¸‰ç»´é«˜æ–¯è´´ç‰‡æ–¹æ³•é€šå¸¸éœ€è¦çš„å¯†é›†å•ç›®æˆ–å¤šè§†è§’è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚å¯¹æ¯”åŸºçº¿æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰åˆ›å»ºé«˜è´¨é‡çš„ä¸‰ç»´äººç±»è§’è‰²ä»å•å¼ å›¾åƒä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å…‹æœå®Œæ•´ä¸‰ç»´ä¿¡æ¯ä»å•ä¸€è§†è§’é‡å»ºçš„å›°éš¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚ä¸‰ç»´é«˜æ–¯è´´ç‰‡æ³•ï¼ˆ3DGSï¼‰å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>SVADæ–¹æ³•ç»“åˆè§†é¢‘æ‰©æ•£å’Œç°æœ‰æŠ€æœ¯ä¼˜ç‚¹ï¼Œç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®å¹¶é€šè¿‡èº«ä»½ä¿ç•™å’Œå›¾åƒæ¢å¤æ¨¡å—å¢å¼ºæ•°æ®è´¨é‡ã€‚</li>
<li>SVADåœ¨ç»´æŒèº«ä»½ä¸€è‡´æ€§å’Œç»†èŠ‚æ–¹é¢ä¼˜äºå•å›¾åƒæ–¹æ³•ï¼Œè¡¨ç°ä¼˜å¼‚äºæ–°å§¿æ€å’Œè§†è§’çš„æ¸²æŸ“ã€‚</li>
<li>SVADå…·å¤‡å®æ—¶æ¸²æŸ“èƒ½åŠ›ï¼Œæé«˜ç”¨æˆ·ä½“éªŒå’Œæ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºæµç¨‹ï¼ŒSVADå…‹æœäº†ä¼ ç»Ÿä¸‰ç»´é«˜æ–¯è´´ç‰‡æ–¹æ³•å¯¹å¯†é›†è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2be95e70c3de02aa94a853d1c54a26c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61886fdbd9be482750452cca8b4fca6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c439904c9da32ca25d8c35b71402be1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a0f7f547c5c0d5070c43eda88d51f67.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="3D-Scene-Generation-A-Survey"><a href="#3D-Scene-Generation-A-Survey" class="headerlink" title="3D Scene Generation: A Survey"></a>3D Scene Generation: A Survey</h2><p><strong>Authors:Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu</strong></p>
<p>3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: <a target="_blank" rel="noopener" href="https://github.com/hzxie/Awesome-3D-Scene-Generation">https://github.com/hzxie/Awesome-3D-Scene-Generation</a>. </p>
<blockquote>
<p>ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ—¨åœ¨åˆæˆå…·æœ‰ç©ºé—´ç»“æ„ã€è¯­ä¹‰å’Œé€¼çœŸçš„ç¯å¢ƒï¼Œç”¨äºæ²‰æµ¸å¼åª’ä½“ã€æœºå™¨äººæŠ€æœ¯ã€è‡ªåŠ¨é©¾é©¶å’Œäººå·¥æ™ºèƒ½ç­‰åº”ç”¨ã€‚æ—©æœŸåŸºäºç¨‹åºè§„åˆ™çš„æ–¹æ³•è™½ç„¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œä½†å¤šæ ·æ€§æœ‰é™ã€‚æœ€è¿‘æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆä¾‹å¦‚GANsï¼Œæ‰©æ•£æ¨¡å‹ï¼‰å’Œä¸‰ç»´è¡¨ç¤ºï¼ˆä¾‹å¦‚NeRFï¼Œä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼‰çš„æœ€æ–°è¿›å±•å·²ç»èƒ½å¤Ÿå­¦ä¹ çœŸå®åœºæ™¯åˆ†å¸ƒï¼Œæé«˜äº†é€¼çœŸåº¦ã€å¤šæ ·æ€§å’Œè§†å›¾ä¸€è‡´æ€§ã€‚æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹ç­‰è¿›å±•é€šè¿‡é‡æ–°æ„å»ºç”Ÿæˆé—®é¢˜ä¸ºå›¾åƒæˆ–è§†é¢‘åˆæˆé—®é¢˜ï¼Œä»è€Œè¿æ¥äº†ä¸‰ç»´åœºæ™¯åˆæˆå’Œé€¼çœŸåº¦ã€‚è¿™ç¯‡ç»¼è¿°æä¾›äº†å¯¹æœ€æ–°æ–¹æ³•çš„ç³»ç»Ÿæ¦‚è¿°ï¼Œå°†å®ƒä»¬ç»„ç»‡æˆå››ç§èŒƒå¼ï¼šç¨‹åºç”Ÿæˆã€åŸºäºç¥ç»çš„ä¸‰ç»´ç”Ÿæˆã€åŸºäºå›¾åƒçš„ç”Ÿæˆå’ŒåŸºäºè§†é¢‘çš„ç”Ÿæˆã€‚æˆ‘ä»¬åˆ†æäº†å®ƒä»¬çš„æŠ€æœ¯åŸºç¡€ã€æƒè¡¡å’Œä»£è¡¨æ€§ç»“æœï¼Œå¹¶å›é¡¾äº†å¸¸ç”¨çš„æ•°æ®é›†ã€è¯„ä¼°åè®®å’Œä¸‹æ¸¸åº”ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†ç”Ÿæˆèƒ½åŠ›ã€ä¸‰ç»´è¡¨ç¤ºã€æ•°æ®å’Œæ³¨é‡Šä»¥åŠè¯„ä¼°æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†å…·æœ‰å‰æ™¯çš„æ–¹å‘ï¼ŒåŒ…æ‹¬æ›´é«˜çš„é€¼çœŸåº¦ã€ç‰©ç†æ„ŸçŸ¥å’Œäº¤äº’å¼ç”Ÿæˆä»¥åŠç»Ÿä¸€çš„æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹ã€‚è¿™ç¯‡ç»¼è¿°æ•´ç†äº†ä¸‰ç»´åœºæ™¯ç”Ÿæˆçš„æœ€æ–°è¿›å±•ï¼Œå¹¶å¼ºè°ƒäº†ç”Ÿæˆäººå·¥æ™ºèƒ½ã€ä¸‰ç»´è§†è§‰å’Œèº«ä½“æ™ºèƒ½äº¤å‰é¢†åŸŸçš„å…·æœ‰å‰æ™¯çš„æ–¹å‘ã€‚è¦äº†è§£æœ€æ–°è¿›å±•ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/hzxie/Awesome-3D-Scene-Generation%E3%80%82">https://github.com/hzxie/Awesome-3D-Scene-Generationã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05474v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/hzxie/Awesome-3D-Scene-Generation">https://github.com/hzxie/Awesome-3D-Scene-Generation</a></p>
<p><strong>æ‘˜è¦</strong><br>    è¯¥æ–‡ç»¼è¿°äº†æœ€æ–°çš„ä¸‰ç»´åœºæ™¯ç”ŸæˆæŠ€æœ¯ï¼Œä»‹ç»äº†å››ç§èŒƒå¼ï¼šç¨‹åºç”Ÿæˆã€åŸºäºç¥ç»çš„ä¸‰ç»´ç”Ÿæˆã€åŸºäºå›¾åƒç”Ÿæˆå’ŒåŸºäºè§†é¢‘ç”Ÿæˆã€‚åˆ†æäº†å®ƒä»¬çš„æŠ€æœ¯åŸºç¡€ã€æƒè¡¡å’Œä»£è¡¨æ€§æˆæœï¼Œå¹¶è®¨è®ºäº†æ•°æ®é›†ã€è¯„ä¼°åè®®å’Œä¸‹æ¸¸åº”ç”¨ã€‚æ–‡ç« æ€»ç»“äº†å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”Ÿæˆèƒ½åŠ›ã€ä¸‰ç»´è¡¨å¾ã€æ•°æ®å’Œæ³¨é‡Šä»¥åŠè¯„ä¼°ï¼Œå¹¶æŒ‡å‡ºäº†é«˜ä¿çœŸã€ç‰©ç†æ„ŸçŸ¥äº¤äº’ç”Ÿæˆå’Œç»Ÿä¸€æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹ç­‰æœ‰å¸Œæœ›çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>3Dåœºæ™¯ç”Ÿæˆæ—¨åœ¨åˆæˆå…·æœ‰ç©ºé—´ç»“æ„ã€è¯­ä¹‰æ„ä¹‰å’Œé€¼çœŸåº¦çš„ç¯å¢ƒï¼Œç”¨äºæ²‰æµ¸å¼åª’ä½“ã€æœºå™¨äººæŠ€æœ¯ã€è‡ªåŠ¨é©¾é©¶å’ŒåµŒå…¥å¼äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚</li>
<li>æ—©æœŸçš„æ–¹æ³•åŸºäºç¨‹åºè§„åˆ™ï¼Œè™½ç„¶æä¾›äº†å¯æ‰©å±•æ€§ï¼Œä½†å¤šæ ·æ€§æœ‰é™ã€‚</li>
<li>æœ€è¿‘çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANsã€æ‰©æ•£æ¨¡å‹ï¼‰å’Œ3Dè¡¨ç¤ºï¼ˆå¦‚NeRFã€3Dé«˜æ–¯ï¼‰çš„è¿›æ­¥ï¼Œä½¿å¾—å­¦ä¹ çœŸå®åœºæ™¯åˆ†å¸ƒæˆä¸ºå¯èƒ½ï¼Œæé«˜äº†é€¼çœŸåº¦ã€å¤šæ ·æ€§å’Œè§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç­‰æ–°æŠ€æœ¯å°†3Dåœºæ™¯åˆæˆä¸é€¼çœŸåº¦ç›¸ç»“åˆï¼Œé€šè¿‡å°†ç”Ÿæˆé—®é¢˜é‡æ–°æ„å»ºä¸ºå›¾åƒæˆ–è§†é¢‘åˆæˆé—®é¢˜æ¥å®ç°ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†å½“å‰çš„ç ”ç©¶ç°çŠ¶ï¼ŒæŒ‡å‡ºäº†å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”Ÿæˆèƒ½åŠ›ã€3Dè¡¨ç¤ºã€æ•°æ®å’Œæ³¨é‡Šä»¥åŠè¯„ä¼°ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†æœ‰å¸Œæœ›çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æ›´é«˜é€¼çœŸåº¦ã€ç‰©ç†æ„ŸçŸ¥å’Œäº¤äº’å¼ç”Ÿæˆï¼Œä»¥åŠç»Ÿä¸€çš„æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c14f07200e7b00574de4cbff9d1fb309.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-860fdec74d557d4e5fdbe5f7828924c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91c13532cdfbbf395edcd0c5d25da980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb32e3058d6aeb29684431d549e7777.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DiffusionSfM-Predicting-Structure-and-Motion-via-Ray-Origin-and-Endpoint-Diffusion"><a href="#DiffusionSfM-Predicting-Structure-and-Motion-via-Ray-Origin-and-Endpoint-Diffusion" class="headerlink" title="DiffusionSfM: Predicting Structure and Motion via Ray Origin and   Endpoint Diffusion"></a>DiffusionSfM: Predicting Structure and Motion via Ray Origin and   Endpoint Diffusion</h2><p><strong>Authors:Qitao Zhao, Amy Lin, Jeff Tan, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani</strong></p>
<p>Current Structure-from-Motion (SfM) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. In contrast, we propose a data-driven multi-view reasoning approach that directly infers 3D scene geometry and camera poses from multi-view images. Our framework, DiffusionSfM, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. To address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. We empirically validate DiffusionSfM on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty. </p>
<blockquote>
<p>å½“å‰çš„ç»“æ„åŒ–è¿åŠ¨ï¼ˆSfMï¼‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼Œç»“åˆå­¦ä¹ æˆ–å‡ ä½•é…å¯¹æ¨ç†ï¼Œéšåè¿›è¡Œå…¨å±€ä¼˜åŒ–æ­¥éª¤ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„å¤šè§†è§’æ¨ç†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä»å¤šè§†è§’å›¾åƒæ¨æ–­3Dåœºæ™¯å‡ ä½•å’Œç›¸æœºå§¿æ€ã€‚æˆ‘ä»¬çš„æ¡†æ¶ï¼ŒDiffusionSfMï¼Œå°†åœºæ™¯å‡ ä½•å’Œç›¸æœºå‚æ•°åŒ–ä¸ºå…¨å±€æ¡†æ¶ä¸­çš„åƒç´ çº§å°„çº¿èµ·ç‚¹å’Œç»ˆç‚¹ï¼Œå¹¶é‡‡ç”¨åŸºäºå˜å‹å™¨çš„å»å™ªæ‰©æ•£æ¨¡å‹ä»å¤šè§†è§’è¾“å…¥è¿›è¡Œé¢„æµ‹ã€‚ä¸ºäº†è§£å†³åœ¨è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶é¢ä¸´çš„å®é™…æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ•°æ®ç¼ºå¤±å’Œæ— ç•Œåœºæ™¯åæ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸šæœºåˆ¶ä»¥ç¡®ä¿ç¨³å¥å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå®è¯éªŒè¯äº†DiffusionSfMï¼Œè¡¨æ˜å…¶åœ¨å»ºæ¨¡ä¸ç¡®å®šæ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶è¶…è¶Šäº†ç»å…¸çš„å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05473v1">PDF</a> CVPR 2025. Project website: <a target="_blank" rel="noopener" href="https://qitaozhao.github.io/DiffusionSfM">https://qitaozhao.github.io/DiffusionSfM</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„å¤šè§†è§’æ¨ç†æ–¹æ³•ï¼Œç›´æ¥ç”±å¤šè§†è§’å›¾åƒæ¨æ–­ä¸‰ç»´åœºæ™¯å‡ ä½•å’Œç›¸æœºå§¿æ€ã€‚æ–°æ–¹æ³•é‡‡ç”¨æ‰©æ•£SfMæ¡†æ¶ï¼Œå°†åœºæ™¯å‡ ä½•å’Œç›¸æœºå‚æ•°åŒ–ä¸ºå…¨å±€åæ ‡ç³»ä¸­çš„åƒç´ çº§å°„çº¿èµ·ç‚¹å’Œç»ˆç‚¹ï¼Œå¹¶ä½¿ç”¨åŸºäºå˜å‹å™¨çš„å»å™ªæ‰©æ•£æ¨¡å‹ä»å¤šè§†è§’è¾“å…¥è¿›è¡Œé¢„æµ‹ã€‚ä¸ºè§£å†³è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶é¢ä¸´çš„ç¼ºå¤±æ•°æ®å’Œæ— é™åœºæ™¯åæ ‡é—®é¢˜ï¼Œå¼•å…¥äº†ä¸“é—¨æœºåˆ¶ä»¥ç¡®ä¿ç¨³å¥å­¦ä¹ ã€‚ç»éªŒè¯ï¼ŒDiffusionSfMåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œå¹¶èƒ½è‡ªç„¶åœ°å»ºæ¨¡ä¸ç¡®å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£SfMæå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†è§’æ¨ç†æ–¹æ³•ï¼Œç›´æ¥æ¨æ–­ä¸‰ç»´åœºæ™¯å‡ ä½•å’Œç›¸æœºå§¿æ€ã€‚</li>
<li>æ–¹æ³•ç»“åˆåƒç´ çº§å°„çº¿èµ·ç‚¹å’Œç»ˆç‚¹å‚æ•°åŒ–åœºæ™¯å‡ ä½•å’Œç›¸æœºï¼Œåœ¨å…¨å±€åæ ‡ç³»ä¸­è¿›è¡Œè¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨åŸºäºå˜å‹å™¨çš„å»å™ªæ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>å¼•å…¥ä¸“é—¨æœºåˆ¶è§£å†³è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶çš„ç¼ºå¤±æ•°æ®å’Œæ— é™åœºæ™¯åæ ‡é—®é¢˜ã€‚</li>
<li>æ‰©æ•£SfMåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£SfMèƒ½è‡ªç„¶åœ°å»ºæ¨¡ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8d91fb415b3b74e84212c0181cc8a15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d40210279a53c1dbda4a4b37a0cb372.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c39ad80a5d8d98b921ba6fb08b209668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bc15129309378b79ed27fae2aeb68ae.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mogao-An-Omni-Foundation-Model-for-Interleaved-Multi-Modal-Generation"><a href="#Mogao-An-Omni-Foundation-Model-for-Interleaved-Multi-Modal-Generation" class="headerlink" title="Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation"></a>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</h2><p><strong>Authors:Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang</strong></p>
<p>Recent progress in unified models for image understanding and generation has been impressive, yet most approaches remain limited to single-modal generation conditioned on multiple modalities. In this paper, we present Mogao, a unified framework that advances this paradigm by enabling interleaved multi-modal generation through a causal approach. Mogao integrates a set of key technical improvements in architecture design, including a deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, which allow it to harness the strengths of both autoregressive models for text generation and diffusion models for high-quality image synthesis. These practical improvements also make Mogao particularly effective to process interleaved sequences of text and images arbitrarily. To further unlock the potential of unified models, we introduce an efficient training strategy on a large-scale, in-house dataset specifically curated for joint text and image generation. Extensive experiments show that Mogao not only achieves state-of-the-art performance in multi-modal understanding and text-to-image generation, but also excels in producing high-quality, coherent interleaved outputs. Its emergent capabilities in zero-shot image editing and compositional generation highlight Mogao as a practical omni-modal foundation model, paving the way for future development and scaling the unified multi-modal systems. </p>
<blockquote>
<p>å…³äºå›¾åƒç†è§£å’Œç”Ÿæˆçš„ç»¼åˆæ¨¡å‹æ–¹é¢çš„æœ€æ–°è¿›å±•ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•ä»ç„¶å±€é™äºå•ä¸€æ¨¡æ€çš„ç”Ÿæˆæ¡ä»¶ä¸‹ï¼Œæ ¹æ®å¤šä¸ªæ¨¡æ€çš„æ¡ä»¶ç”Ÿæˆå†…å®¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Mogaoï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨è¿›è¿™ä¸€èŒƒå¼çš„ä¸€ä½“åŒ–æ¡†æ¶ï¼Œå®ƒé€šè¿‡å› æœæ–¹æ³•å®ç°äº†äº¤æ›¿å¤šæ¨¡æ€ç”Ÿæˆã€‚Mogaoé›†æˆäº†æ¶æ„è®¾è®¡ä¸­çš„ä¸€ç³»åˆ—å…³é”®æŠ€æœ¯æ”¹è¿›ï¼ŒåŒ…æ‹¬æ·±åº¦èåˆè®¾è®¡ã€åŒè§†è§‰ç¼–ç å™¨ã€äº¤æ›¿æ—‹è½¬ä½ç½®åµŒå…¥å’Œæ— æ¨¡æ€åˆ†ç±»å™¨å¼•å¯¼ç­‰ï¼Œè¿™å…è®¸å®ƒå……åˆ†åˆ©ç”¨è‡ªå›å½’æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆå’Œæ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒåˆæˆæ–¹é¢çš„ä¼˜åŠ¿ã€‚è¿™äº›å®é™…çš„æ”¹è¿›ä¹Ÿä½¿å¾—Mogaoåœ¨å¤„ç†ä»»æ„æ–‡æœ¬å’Œå›¾åƒçš„äº¤æ›¿åºåˆ—æ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚ä¸ºäº†è§£é”ç»¼åˆæ¨¡å‹çš„æ½œåŠ›ï¼Œæˆ‘ä»¬åœ¨ä¸“é—¨ä¸ºè”åˆæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆè®¾è®¡çš„å¤§è§„æ¨¡å†…éƒ¨æ•°æ®é›†ä¸Šå¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMogaoä¸ä»…åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸”åœ¨ç”Ÿæˆé«˜è´¨é‡è¿è´¯çš„äº¤æ›¿è¾“å‡ºæ–¹é¢ä¹Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®ƒåœ¨é›¶æ ·æœ¬å›¾åƒç¼–è¾‘å’Œç»„åˆç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›çªæ˜¾äº†Mogaoä½œä¸ºä¸€ä¸ªå®ç”¨çš„å…¨æ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œä¸ºæœªæ¥çš„å‘å±•å’Œæ‰©å±•ç»Ÿä¸€å¤šæ¨¡æ€ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05472v1">PDF</a> Mogao Technical Report</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€ç”Ÿæˆæ˜¯å›¾åƒç†è§£å’Œç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªçƒ­é—¨è¯é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMogaoçš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ä¸€ç³»åˆ—å…³é”®æŠ€æœ¯æ”¹è¿›ï¼Œå¦‚æ·±åº¦èåˆè®¾è®¡ã€åŒé‡è§†è§‰ç¼–ç å™¨ã€äº¤æ›¿æ—‹è½¬ä½ç½®åµŒå…¥å’Œå¤šæ¨¡æ€æ— åˆ†ç±»å™¨å¼•å¯¼ç­‰ï¼Œå®ç°äº†å¤šæ¨¡æ€ç”Ÿæˆã€‚Mogaoç»“åˆäº†è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œç”¨äºé«˜è´¨é‡å›¾åƒåˆæˆã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§å¤§è§„æ¨¡å†…å»ºæ•°æ®é›†çš„åŸ¹è®­ç­–ç•¥ï¼Œä¸ºç»Ÿä¸€æ¨¡å‹æä¾›äº†æ›´å¤šæ½œåŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒMogaoåœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶èƒ½äº§ç”Ÿé«˜è´¨é‡è¿è´¯çš„äº¤é”™è¾“å‡ºã€‚å…¶é›¶æ ·æœ¬å›¾åƒç¼–è¾‘å’Œç»„åˆç”Ÿæˆèƒ½åŠ›ä½¿å…¶æˆä¸ºå®ç”¨çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mogaoæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå®ç°äº†å¤šæ¨¡æ€ç”Ÿæˆï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬å’Œå›¾åƒçš„äº¤é”™åºåˆ—ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¸€ç³»åˆ—å…³é”®æŠ€æœ¯æ”¹è¿›ï¼Œå¦‚æ·±åº¦èåˆè®¾è®¡ã€åŒé‡è§†è§‰ç¼–ç å™¨ç­‰å®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>Mogaoç»“åˆäº†è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œç”¨äºé«˜è´¨é‡å›¾åƒåˆæˆã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å¤§è§„æ¨¡å†…å»ºæ•°æ®é›†çš„åŸ¹è®­ç­–ç•¥ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Mogaoåœ¨å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡è¿è´¯çš„äº¤é”™è¾“å‡ºï¼Œå¹¶å…·æœ‰é›¶æ ·æœ¬å›¾åƒç¼–è¾‘å’Œç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4177900d62bd136ee63ac3008a32d67e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2fa425f14bfc87b9696e4db9ef6de6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e120d180d577e724d91f96249920b3eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-Quantization-A-Review"><a href="#Diffusion-Model-Quantization-A-Review" class="headerlink" title="Diffusion Model Quantization: A Review"></a>Diffusion Model Quantization: A Review</h2><p><strong>Authors:Qian Zeng, Chenggong Hu, Mingli Song, Jie Song</strong></p>
<p>Recent success of large text-to-image models has empirically underscored the exceptional performance of diffusion models in generative tasks. To facilitate their efficient deployment on resource-constrained edge devices, model quantization has emerged as a pivotal technique for both compression and acceleration. This survey offers a thorough review of the latest advancements in diffusion model quantization, encapsulating and analyzing the current state of the art in this rapidly advancing domain. First, we provide an overview of the key challenges encountered in the quantization of diffusion models, including those based on U-Net architectures and Diffusion Transformers (DiT). We then present a comprehensive taxonomy of prevalent quantization techniques, engaging in an in-depth discussion of their underlying principles. Subsequently, we perform a meticulous analysis of representative diffusion model quantization schemes from both qualitative and quantitative perspectives. From a quantitative standpoint, we rigorously benchmark a variety of methods using widely recognized datasets, delivering an extensive evaluation of the most recent and impactful research in the field. From a qualitative standpoint, we categorize and synthesize the effects of quantization errors, elucidating these impacts through both visual analysis and trajectory examination. In conclusion, we outline prospective avenues for future research, proposing novel directions for the quantization of generative models in practical applications. The list of related papers, corresponding codes, pre-trained models and comparison results are publicly available at the survey project homepage <a target="_blank" rel="noopener" href="https://github.com/TaylorJocelyn/Diffusion-Model-Quantization">https://github.com/TaylorJocelyn/Diffusion-Model-Quantization</a>. </p>
<blockquote>
<p>å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æœ€æ–°æˆåŠŸç»éªŒå®è¯äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚ä¸ºäº†åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šæœ‰æ•ˆéƒ¨ç½²è¿™äº›æ¨¡å‹ï¼Œæ¨¡å‹é‡åŒ–ä½œä¸ºå‹ç¼©å’ŒåŠ é€Ÿçš„å…³é”®æŠ€æœ¯åº”è¿è€Œç”Ÿã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†æ‰©æ•£æ¨¡å‹é‡åŒ–çš„æœ€æ–°è¿›å±•ï¼Œåˆ†æå’Œè¯„ä¼°äº†è¿™ä¸ªå¿«é€Ÿå‘å±•é¢†åŸŸçš„å½“å‰æœ€æ–°çŠ¶æ€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ¦‚è¿°äº†æ‰©æ•£æ¨¡å‹é‡åŒ–æ‰€é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŸºäºU-Netæ¶æ„å’Œæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„æŒ‘æˆ˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å…¨é¢ä»‹ç»äº†æµè¡Œçš„é‡åŒ–æŠ€æœ¯ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†å®ƒä»¬çš„åŸºæœ¬åŸç†ã€‚éšåï¼Œæˆ‘ä»¬ä»å®šæ€§å’Œå®šé‡ä¸¤ä¸ªè§’åº¦å¯¹ä»£è¡¨æ€§çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ¡ˆè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚ä»å®šé‡è§’åº¦çœ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¬è®¤çš„æ•°æ®é›†å¯¹å„ç§æ–¹æ³•è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œå…¨é¢è¯„ä»·äº†è¯¥é¢†åŸŸæœ€æ–°ã€æœ€æœ‰å½±å“åŠ›çš„ç ”ç©¶ã€‚ä»å®šæ€§è§’åº¦çœ‹ï¼Œæˆ‘ä»¬å¯¹é‡åŒ–è¯¯å·®çš„å½±å“è¿›è¡Œäº†åˆ†ç±»å’Œå½’çº³ï¼Œå¹¶é€šè¿‡è§†è§‰åˆ†æå’Œè½¨è¿¹æ£€æŸ¥é˜æ˜äº†è¿™äº›å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„å‰æ™¯ï¼Œä¸ºå®é™…åº”ç”¨ä¸­ç”Ÿæˆæ¨¡å‹çš„é‡åŒ–æå‡ºäº†æ–°é¢–çš„ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³è®ºæ–‡åˆ—è¡¨ã€ç›¸åº”ä»£ç ã€é¢„è®­ç»ƒæ¨¡å‹å’Œæ¯”è¾ƒç»“æœå¯åœ¨è°ƒæŸ¥é¡¹ç›®ä¸»é¡µ<a target="_blank" rel="noopener" href="https://github.com/TaylorJocelyn/Diffusion-Model-Quantization%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TaylorJocelyn/Diffusion-Model-Quantizationä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05215v1">PDF</a> 40 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†æ‰©æ•£æ¨¡å‹é‡åŒ–æŠ€æœ¯çš„æœ€æ–°è¿›å±•ä¸æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬U-Netæ¶æ„å’ŒDiffusion Transformersï¼ˆDiTï¼‰ä¸­çš„æŒ‘æˆ˜ã€‚æ–‡ç« æ¦‚è¿°äº†å…³é”®é‡åŒ–æŠ€æœ¯çš„åŸç†å’Œæ€§èƒ½è¯„ä¼°ï¼ŒåŒæ—¶ä»å®šæ€§å’Œå®šé‡ä¸¤ä¸ªè§’åº¦åˆ†æäº†ä»£è¡¨æ€§çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å±•æœ›äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºå®é™…åº”ç”¨ä¸­ç”Ÿæˆæ¨¡å‹çš„é‡åŒ–æä¾›äº†æ–°æ€è·¯ã€‚ç›¸å…³èµ„æºå·²å…¬å¼€åœ¨è°ƒæŸ¥é¡¹ç›®ä¸»é¡µä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…¶é‡åŒ–æŠ€æœ¯æ˜¯å®ç°åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆéƒ¨ç½²çš„å…³é”®ã€‚</li>
<li>é¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬U-Netæ¶æ„å’ŒDiffusion Transformersï¼ˆDiTï¼‰çš„é‡åŒ–éš¾é¢˜ã€‚</li>
<li>æµè¡Œçš„é‡åŒ–æŠ€æœ¯å¾—åˆ°äº†å…¨é¢çš„å®¡æŸ¥å’Œåˆ†æï¼Œå±•ç¤ºäº†å½“å‰è¯¥é¢†åŸŸçš„æœ€æ–°å‘å±•ã€‚</li>
<li>ä»å®šé‡å’Œå®šæ€§ä¸¤ä¸ªè§’åº¦å¯¹ä»£è¡¨æ€§çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ¡ˆè¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li>
<li>é€šè¿‡å¹¿æ³›è®¤å¯çš„æ•°æ®é›†å¯¹å¤šç§æ–¹æ³•è¿›è¡Œäº†ä¸¥æ ¼åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†æœ€æ–°ç ”ç©¶æˆæœã€‚</li>
<li>é‡åŒ–è¯¯å·®çš„å½±å“é€šè¿‡è§†è§‰åˆ†æå’Œè½¨è¿¹æ£€æŸ¥å¾—åˆ°äº†è¯¦ç»†é˜è¿°å’Œåˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a7eba87e5ac7111bf0a24f671167579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b94db4e59c280d7395b325100a4562b0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EAM-Enhancing-Anything-with-Diffusion-Transformers-for-Blind-Super-Resolution"><a href="#EAM-Enhancing-Anything-with-Diffusion-Transformers-for-Blind-Super-Resolution" class="headerlink" title="EAM: Enhancing Anything with Diffusion Transformers for Blind   Super-Resolution"></a>EAM: Enhancing Anything with Diffusion Transformers for Blind   Super-Resolution</h2><p><strong>Authors:Haizhen Xie, Kunpeng Du, Qiangyu Yan, Sen Lu, Jianhong Han, Hanting Chen, Hailin Hu, Jie Hu</strong></p>
<p>Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality. </p>
<blockquote>
<p>åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹æ¥æŒ‡å¯¼ç›²è¶…åˆ†è¾¨ç‡ï¼ˆBSRï¼‰å·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»è¦æ–¹æ³•ã€‚è™½ç„¶T2Iæ¨¡å‹ä¼ ç»Ÿä¸Šä¾èµ–äºU-Netæ¶æ„ï¼Œä½†æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æ›´é«˜çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¢å¼ºä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆEAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„BSRæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨DiTå¹¶è¶…è¶Šäº†ä¹‹å‰çš„U-NetåŸºäºçš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹å—Î¨-DiTï¼Œå®ƒæœ‰æ•ˆåœ°æŒ‡å¯¼DiTè¿›è¡Œå›¾åƒæ¢å¤ã€‚æ­¤å—é‡‡ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨å€¼ä½œä¸ºå¯åˆ†ç¦»çš„æµç¨‹æ³¨å…¥æ§åˆ¶ï¼Œå½¢æˆä¸€ä¸ªä¸‰æµæ¶æ„ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒDiTä¸­çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨T2Iæ¨¡å‹çš„å…ˆéªŒæŒ‡å¯¼èƒ½åŠ›å¹¶å¢å¼ºå…¶åœ¨BSRä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¸è¿›å¼é®æŒ¡å›¾åƒå»ºæ¨¡ç­–ç•¥ï¼Œè¿™ä¹Ÿé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸»é¢˜æ„ŸçŸ¥çš„æç¤ºç”Ÿæˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ä¸­é‡‡ç”¨å¥å£®çš„å¤šæ¨¡å¼æ¨¡å‹ã€‚è¯¥ç­–ç•¥è‡ªåŠ¨è¯†åˆ«å…³é”®å›¾åƒåŒºåŸŸï¼Œæä¾›è¯¦ç»†æè¿°ï¼Œå¹¶ä¼˜åŒ–T2Iæ‰©æ•£å…ˆéªŒçš„åˆ©ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒEAMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡æ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05209v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹æŒ‡å¯¼ç›²è¶…åˆ†è¾¨ç‡ï¼ˆBSRï¼‰å·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»æµæ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„BSRæ–¹æ³•â€”â€”å¢å¼ºå‹ä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆEAMï¼‰ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å¹¶è¶…è¶Šäº†ä¼ ç»Ÿçš„U-Netæ–¹æ³•ã€‚EAMå¼•å…¥äº†ä¸€ä¸ªåä¸ºÎ¨-DiTçš„æ–°æ¨¡å—ï¼Œå®ƒæœ‰æ•ˆåœ°å¼•å¯¼DiTè¿›è¡Œå›¾åƒæ¢å¤ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ¸è¿›å¼é®æ©å›¾åƒå»ºæ¨¡ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨T2Iæ¨¡å‹çš„å…ˆéªŒæŒ‡å¯¼èƒ½åŠ›ï¼Œå¹¶å¢å¼ºå…¶æ¨å¹¿æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºä¸»ä½“æ„ŸçŸ¥çš„æç¤ºç”Ÿæˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é‡‡ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒEAMåœ¨å¤šæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EAMæ¨¡å‹æ˜¯ä¸€ç§æ–°å‹çš„ç›²è¶…åˆ†è¾¨ç‡ï¼ˆBSRï¼‰æ–¹æ³•ï¼ŒåŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>EAMåˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰æŠ€æœ¯è¶…è¶Šä¼ ç»Ÿçš„U-Netæ¨¡å‹ï¼Œæé«˜äº†å›¾åƒæ¢å¤çš„æ€§èƒ½ã€‚</li>
<li>Î¨-DiTæ¨¡å—çš„å¼•å…¥æœ‰æ•ˆåœ°å¼•å¯¼äº†DiTè¿›è¡Œå›¾åƒæ¢å¤è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨äº†æ¸è¿›å¼é®æ©å›¾åƒå»ºæ¨¡ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„å…ˆéªŒæŒ‡å¯¼èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¸»ä½“æ„ŸçŸ¥çš„æç¤ºç”Ÿæˆç­–ç•¥ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æ¨¡å‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>EAMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœè¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de8b74f5037c54def645b6c23767c627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caee6a667b678014880e0e98e8c867ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff44f97272cfc8f63e21f2f00bca19bc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Research-on-Anomaly-Detection-Methods-Based-on-Diffusion-Models"><a href="#Research-on-Anomaly-Detection-Methods-Based-on-Diffusion-Models" class="headerlink" title="Research on Anomaly Detection Methods Based on Diffusion Models"></a>Research on Anomaly Detection Methods Based on Diffusion Models</h2><p><strong>Authors:Yi Chen</strong></p>
<p>Anomaly detection is a fundamental task in machine learning and data mining, with significant applications in cybersecurity, industrial fault diagnosis, and clinical disease monitoring. Traditional methods, such as statistical modeling and machine learning-based approaches, often face challenges in handling complex, high-dimensional data distributions. In this study, we explore the potential of diffusion models for anomaly detection, proposing a novel framework that leverages the strengths of diffusion probabilistic models (DPMs) to effectively identify anomalies in both image and audio data. The proposed method models the distribution of normal data through a diffusion process and reconstructs input data via reverse diffusion, using a combination of reconstruction errors and semantic discrepancies as anomaly indicators. To enhance the frameworkâ€™s performance, we introduce multi-scale feature extraction, attention mechanisms, and wavelet-domain representations, enabling the model to capture fine-grained structures and global dependencies in the data. Extensive experiments on benchmark datasets, including MVTec AD and UrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly detection techniques, achieving superior accuracy and robustness across diverse data modalities. This research highlights the effectiveness of diffusion models in anomaly detection and provides a robust and efficient solution for real-world applications. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹æ˜¯æœºå™¨å­¦ä¹ å’Œæ•°æ®æŒ–æ˜ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œåœ¨ç½‘ç»œå®‰å…¨ã€å·¥ä¸šæ•…éšœè¯Šæ–­å’Œä¸´åºŠç–¾ç—…ç›‘æµ‹ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œå¦‚ç»Ÿè®¡å»ºæ¨¡å’ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†å¤æ‚çš„é«˜ç»´æ•°æ®åˆ†å¸ƒæ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆåœ°è¯†åˆ«å›¾åƒå’ŒéŸ³é¢‘æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰©æ•£è¿‡ç¨‹å¯¹æ­£å¸¸æ•°æ®çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡åå‘æ‰©æ•£é‡å»ºè¾“å…¥æ•°æ®ï¼Œä½¿ç”¨é‡å»ºè¯¯å·®å’Œè¯­ä¹‰å·®å¼‚ä½œä¸ºå¼‚å¸¸æŒ‡æ ‡ã€‚ä¸ºäº†æé«˜æ¡†æ¶çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦ç‰¹å¾æå–ã€æ³¨æ„æœºåˆ¶å’Œæ³¢åŸŸè¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ•°æ®çš„ç²¾ç»†ç»“æ„å’Œå…¨å±€ä¾èµ–æ€§ã€‚åœ¨MVTec ADå’ŒUrbanSound8Kç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ï¼Œåœ¨å¤šç§æ•°æ®æ¨¡æ€ä¸Šå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶çªå‡ºäº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†ç¨³å¥é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05137v1">PDF</a> 6 pages, 3 table</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰ä¼˜åŠ¿çš„æ–°æ¡†æ¶ï¼Œæœ‰æ•ˆè¯†åˆ«å›¾åƒå’ŒéŸ³é¢‘æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ã€‚è¯¥ç ”ç©¶é€šè¿‡æ‰©æ•£è¿‡ç¨‹å¯¹æ­£å¸¸æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡åå‘æ‰©æ•£é‡å»ºè¾“å…¥æ•°æ®ï¼Œä½¿ç”¨é‡å»ºè¯¯å·®å’Œè¯­ä¹‰å·®å¼‚ä½œä¸ºå¼‚å¸¸æŒ‡æ ‡ã€‚ä¸ºæé«˜æ¡†æ¶æ€§èƒ½ï¼Œå¼•å…¥äº†å¤šå°ºåº¦ç‰¹å¾æå–ã€æ³¨æ„æœºåˆ¶å’Œå°æ³¢åŸŸè¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­çš„ç²¾ç»†ç»“æ„å’Œå…¨å±€ä¾èµ–æ€§ã€‚åœ¨MVTec ADå’ŒUrbanSound8Kç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ï¼Œåœ¨å¤šç§æ•°æ®æ¨¡æ€ä¸Šå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥ç ”ç©¶çªå‡ºäº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†ç¨³å¥é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>è¦ç‚¹é€Ÿè®°</strong></p>
<ol>
<li>å¼‚å¸¸æ£€æµ‹æ˜¯æœºå™¨å­¦ä¹ å’Œæ•°æ®æŒ–æ˜ä¸­çš„åŸºæœ¬ä»»åŠ¡ï¼Œåœ¨ç½‘ç»œå®‰å…¨ã€å·¥ä¸šæ•…éšœè¯Šæ–­å’Œä¸´åºŠç–¾ç—…ç›‘æµ‹ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚ç»Ÿè®¡å»ºæ¨¡å’Œæœºå™¨å­¦ä¹ åœ¨å¤„ç†å¤æ‚ã€é«˜ç»´æ•°æ®åˆ†å¸ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰é€šè¿‡æ‰©æ•£è¿‡ç¨‹å¯¹æ­£å¸¸æ•°æ®å»ºæ¨¡ï¼Œå¹¶é€šè¿‡åå‘æ‰©æ•£é‡å»ºæ•°æ®ï¼Œä½¿ç”¨é‡å»ºè¯¯å·®å’Œè¯­ä¹‰å·®å¼‚ä½œä¸ºå¼‚å¸¸æŒ‡æ ‡ã€‚</li>
<li>å¼•å…¥å¤šå°ºåº¦ç‰¹å¾æå–ã€æ³¨æ„æœºåˆ¶å’Œå°æ³¢åŸŸè¡¨ç¤ºï¼Œæé«˜æ¡†æ¶æ€§èƒ½ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯ï¼Œå…·æœ‰é«˜åº¦çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ea637a19b79ffa658ac2083ed443b98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15559a35bf3ae85327523546f15dbc54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-854830f5a4ce18f13d83636ead240884.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44128853c2ebb52540a0a0d18c9422d0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MDAA-Diff-CT-Guided-Multi-Dose-Adaptive-Attention-Diffusion-Model-for-PET-Denoising"><a href="#MDAA-Diff-CT-Guided-Multi-Dose-Adaptive-Attention-Diffusion-Model-for-PET-Denoising" class="headerlink" title="MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for   PET Denoising"></a>MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for   PET Denoising</h2><p><strong>Authors:Xiaolong Niu, Zanting Ye, Xu Han, Yanchao Huang, Hao Sun, Hubing Wu, Lijun Lu</strong></p>
<p>Acquiring high-quality Positron Emission Tomography (PET) images requires administering high-dose radiotracers, which increases radiation exposure risks. Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a potential solution. However, previous studies have primarily focused on single low-dose PET denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from CT images. In this work, we propose a novel CT-Guided Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for multi-dose PET denoising. Our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. Specifically, this approach incorporates a CT-Guided High-frequency Wavelet Attention (HWA) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from CT images. These extracted features are then incorporated into PET imaging through an adaptive weighted fusion mechanism to enhance edge details. Additionally, we propose the Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets demonstrate that MDAA-Diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. Our code is publicly available. </p>
<blockquote>
<p>è·å–é«˜è´¨é‡çš„å‘å°„æ­£ç”µå­æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒéœ€è¦æ³¨å°„é«˜å‰‚é‡æ”¾å°„æ€§ç¤ºè¸ªå‰‚ï¼Œè¿™å¢åŠ äº†è¾å°„æš´éœ²çš„é£é™©ã€‚ä»ä½å‰‚é‡PETï¼ˆLPETï¼‰ç”Ÿæˆæ ‡å‡†å‰‚é‡PETï¼ˆSPETï¼‰å·²æˆä¸ºä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€ä½å‰‚é‡PETå»å™ªä¸Šï¼Œå¿½è§†äº†ä¸¤ä¸ªå…³é”®å› ç´ ï¼šç”±æ‚£è€…é—´å·®å¼‚å¼•èµ·çš„å‰‚é‡ååº”å·®å¼‚ï¼Œä»¥åŠä»CTå›¾åƒä¸­å¾—å‡ºçš„äº’è¡¥è§£å‰–çº¦æŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„CTå¼•å¯¼å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ï¼Œç”¨äºå¤šå‰‚é‡PETå»å™ªã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è§£å‰–æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³é€‚åº”ï¼Œä»¥åœ¨ä½å‰‚é‡æ¡ä»¶ä¸‹å®ç°å“è¶Šçš„å»å™ªæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨CTå¼•å¯¼çš„é«˜é¢‘å°æ³¢æ³¨æ„åŠ›ï¼ˆHWAï¼‰æ¨¡å—ï¼Œåˆ©ç”¨å°æ³¢å˜æ¢ä»CTå›¾åƒä¸­åˆ†ç¦»å‡ºé«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ã€‚ç„¶åï¼Œé€šè¿‡è‡ªé€‚åº”åŠ æƒèåˆæœºåˆ¶å°†è¿™äº›æå–çš„ç‰¹å¾èå…¥åˆ°PETæˆåƒä¸­ï¼Œä»¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆDAAï¼‰æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§å‰‚é‡è°ƒèŠ‚å¢å¼ºæœºåˆ¶ï¼ŒåŠ¨æ€åœ°å°†å‰‚é‡æ°´å¹³é›†æˆåˆ°é€šé“ç©ºé—´æ³¨æ„åŠ›æƒé‡è®¡ç®—ä¸­ã€‚åœ¨18F-FDGå’Œ68Ga-FAPIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMDAA-Diffåœ¨é™ä½å‰‚é‡æ¡ä»¶ä¸‹ä¿å­˜è¯Šæ–­è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05112v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹PETæˆåƒä¸­é«˜å‰‚é‡å¸¦æ¥çš„è¾å°„é£é™©é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºä¸€ç§æ–°å‹CTå¼•å¯¼çš„å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ã€‚è¯¥æ¨¡å‹ç»“åˆè§£å‰–å­¦æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³è‡ªé€‚åº”æŠ€æœ¯ï¼Œåœ¨ä½å‰‚é‡æ¡ä»¶ä¸‹å®ç°å‡ºè‰²çš„å»å™ªæ€§èƒ½ã€‚å®ƒé€šè¿‡å°æ³¢å˜æ¢æå–CTå›¾åƒçš„é«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ï¼Œå¹¶èå…¥PETæˆåƒä¸­å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚åŒæ—¶ï¼Œå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—èƒ½å¤Ÿæ ¹æ®å‰‚é‡æ°´å¹³åŠ¨æ€è°ƒæ•´é€šé“ç©ºé—´æ³¨æ„åŠ›æƒé‡è®¡ç®—ã€‚å®éªŒè¯æ˜ï¼ŒMDAA-Diffåœ¨å‡å°‘å‰‚é‡æ¡ä»¶ä¸‹èƒ½ä¿ç•™è¯Šæ–­è´¨é‡ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>PETæˆåƒä¸­éœ€è¦é«˜å‰‚é‡æ”¾å°„çº¿è¿½è¸ªå‰‚ä»¥æé«˜å›¾åƒè´¨é‡ï¼Œä½†å¢åŠ äº†è¾å°„æš´éœ²é£é™©ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹CTå¼•å¯¼çš„å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ï¼Œç”¨äºä»ä½å‰‚é‡PETç”Ÿæˆæ ‡å‡†å‰‚é‡PETï¼ˆSPETï¼‰ã€‚</li>
<li>MDAA-Diffæ¨¡å‹ç»“åˆäº†è§£å‰–å­¦æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³è‡ªé€‚åº”æŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨å°æ³¢å˜æ¢æå–CTå›¾åƒçš„é«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”åŠ æƒèåˆæœºåˆ¶èå…¥PETæˆåƒä¸­ï¼Œä»¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚</li>
<li>å‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—èƒ½æ ¹æ®å‰‚é‡æ°´å¹³åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æƒé‡è®¡ç®—ã€‚</li>
<li>å®éªŒè¯æ˜MDAA-Diffåœ¨å‡å°‘å‰‚é‡æ¡ä»¶ä¸‹èƒ½ä¿ç•™è¯Šæ–­è´¨é‡ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05112">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81b998f2c4b1b41ce719dfb3e39628be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72fb0ab82d2eb4551957a3c0446571de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c8b8e643f982798fdc7076b26c61567.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MDE-Edit-Masked-Dual-Editing-for-Multi-Object-Image-Editing-via-Diffusion-Models"><a href="#MDE-Edit-Masked-Dual-Editing-for-Multi-Object-Image-Editing-via-Diffusion-Models" class="headerlink" title="MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via   Diffusion Models"></a>MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via   Diffusion Models</h2><p><strong>Authors:Hongyang Zhu, Haipeng Liu, Bo Fu, Yang Wang</strong></p>
<p>Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where color or texture changes fail to align with intended regions due to cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color bleeding into non-target areas). Existing methods struggle with these challenges: approaches relying on global cross-attention mechanisms suffer from attention dilution and spatial interference between objects, while mask-based methods fail to bind attributes to geometrically accurate regions due to feature entanglement in multi-object scenarios. To address these limitations, we propose a training-free, inference-stage optimization approach that enables precise localized image manipulation in complex multi-object scenes, named MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention with segmentation masks for precise object positioning, and Color Consistency Loss (CCL) amplifies target attribute attention within masks while suppressing leakage to adjacent regions. This dual-loss design ensures localized and coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality, offering a robust solution for complex multi-object image manipulation tasks. </p>
<blockquote>
<p>å¤šå¯¹è±¡ç¼–è¾‘æ—¨åœ¨åœ¨å¤æ‚çš„åœºæ™¯ä¸­ä¿®æ”¹å¤šä¸ªå¯¹è±¡æˆ–åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒç»“æ„è¿è´¯æ€§ã€‚åœ¨æ¶‰åŠé‡å æˆ–äº¤äº’å¯¹è±¡çš„åœºæ™¯ä¸­ï¼Œæ­¤ä»»åŠ¡é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼š(1)ç”±äºæ³¨æ„åŠ›é”™ä½å¯¼è‡´ç›®æ ‡å¯¹è±¡å®šä½ä¸å‡†ç¡®ï¼Œä»è€Œå¯¼è‡´ç¼–è¾‘ä¸å®Œæ•´æˆ–é”™ä½ï¼›(2)å±æ€§å¯¹è±¡ä¸åŒ¹é…ï¼Œç”±äºè·¨æ³¨æ„åŠ›æ³„æ¼ï¼Œé¢œè‰²æˆ–çº¹ç†å˜åŒ–æœªèƒ½ä¸æ„å›¾åŒºåŸŸå¯¹é½ï¼Œä»è€Œäº§ç”Ÿè¯­ä¹‰å†²çªï¼ˆä¾‹å¦‚ï¼Œé¢œè‰²æ¸—å…¥éç›®æ ‡åŒºåŸŸï¼‰ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼šä¾èµ–å…¨å±€è·¨æ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•å—åˆ°æ³¨æ„åŠ›ç¨€é‡Šå’Œå¯¹è±¡é—´ç©ºé—´å¹²æ‰°çš„å½±å“ï¼Œè€ŒåŸºäºæ©è†œçš„æ–¹æ³•ç”±äºåœ¨å¤šå¯¹è±¡åœºæ™¯ä¸­çš„ç‰¹å¾çº ç¼ è€Œæ— æ³•å°†å±æ€§ç»‘å®šåˆ°å‡ ä½•ç²¾ç¡®çš„åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œä¼˜åŒ–çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­å®ç°ç²¾ç¡®çš„å®šä½å›¾åƒæ“ä½œï¼Œåä¸ºMDE-Editã€‚MDE-Edité€šè¿‡ä¸¤ä¸ªå…³é”®æŸå¤±ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°æ½œåœ¨ç‰¹å¾ï¼šå¯¹è±¡å¯¹é½æŸå¤±ï¼ˆOALï¼‰å°†å¤šå±‚è·¨æ³¨æ„åŠ›ä¸åˆ†å‰²æ©è†œå¯¹é½ï¼Œä»¥å®ç°ç²¾ç¡®çš„å¯¹è±¡å®šä½ï¼›é¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼ˆCCLï¼‰åœ¨æ©è†œå†…æ”¾å¤§ç›®æ ‡å±æ€§æ³¨æ„åŠ›ï¼ŒåŒæ—¶æŠ‘åˆ¶æ³„æ¼åˆ°ç›¸é‚»åŒºåŸŸã€‚è¿™ç§åŒé‡æŸå¤±è®¾è®¡ç¡®ä¿äº†å±€éƒ¨åŒ–å’Œè¿è´¯çš„å¤šå¯¹è±¡ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMDE-Editåœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œä¸ºå¤æ‚çš„å¤šå¯¹è±¡å›¾åƒæ“ä½œä»»åŠ¡æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05101v1">PDF</a> 9 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åœ¨å¤æ‚å¤šå¯¹è±¡åœºæ™¯ä¸­è¿›è¡Œç²¾ç¡®å±€éƒ¨å›¾åƒç¼–è¾‘çš„æ–¹æ³•MDE-Editï¼Œæ—¨åœ¨è§£å†³å¤šå¯¹è±¡ç¼–è¾‘ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚ç›®æ ‡å¯¹è±¡å®šä½ä¸å‡†ç¡®ã€å±æ€§å¯¹è±¡ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨å¯¹è±¡å¯¹é½æŸå¤±ï¼ˆOALï¼‰å’Œé¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼ˆCCLï¼‰æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°æ½œåœ¨ç‰¹å¾ï¼Œå®ç°ç²¾å‡†çš„å¯¹è±¡å®šä½åŠå±æ€§ç¼–è¾‘ï¼ŒåŒæ—¶æŠ‘åˆ¶å¯¹ç›¸é‚»åŒºåŸŸçš„å¹²æ‰°ã€‚å®éªŒè¯æ˜ï¼ŒMDE-Editåœ¨ç¼–è¾‘å‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤æ‚å¤šå¯¹è±¡å›¾åƒæ“ä½œä»»åŠ¡æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå¯¹è±¡ç¼–è¾‘é¢ä¸´ç›®æ ‡å¯¹è±¡å®šä½ä¸å‡†ç¡®å’Œå±æ€§å¯¹è±¡ä¸åŒ¹é…ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å…¨å±€äº¤å‰æ³¨æ„æœºåˆ¶é¢ä¸´æ³¨æ„åŠ›åˆ†æ•£å’Œç©ºé—´å¹²æ‰°é—®é¢˜ï¼Œè€ŒåŸºäºæ©è†œçš„æ–¹æ³•åœ¨å¤æ‚å¤šå¯¹è±¡åœºæ™¯ä¸­ç‰¹å¾çº ç¼ ã€‚</li>
<li>MDE-Edité€šè¿‡ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°æ½œåœ¨ç‰¹å¾æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>MDE-Edité‡‡ç”¨å¯¹è±¡å¯¹é½æŸå¤±ï¼ˆOALï¼‰è¿›è¡Œç²¾å‡†å¯¹è±¡å®šä½ã€‚</li>
<li>é¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼ˆCCLï¼‰ç”¨äºæ”¾å¤§ç›®æ ‡å±æ€§æ³¨æ„åŠ›ï¼ŒåŒæ—¶æŠ‘åˆ¶ç›¸é‚»åŒºåŸŸçš„æ³„æ¼ã€‚</li>
<li>åŒæŸå¤±è®¾è®¡ç¡®ä¿å±€éƒ¨åŒ–å’Œè¿è´¯çš„å¤šå¯¹è±¡ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-17cae62e6ccddbc3795cfabde33d1899.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16db5fcad470b536854fa97bf47bf00f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a7a57f0a3a4ea5f7324a07b80a84f28.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ItDPDM-Information-Theoretic-Discrete-Poisson-Diffusion-Model"><a href="#ItDPDM-Information-Theoretic-Discrete-Poisson-Diffusion-Model" class="headerlink" title="ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model"></a>ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model</h2><p><strong>Authors:Sagnik Bhattacharya, Abhiram R. Gorle, Ahmed Mohsin, Ahsan Bilal, Connor Ding, Amit Kumar Singh Yadav, Tsachy Weissman</strong></p>
<p>Existing methods for generative modeling of discrete data, such as symbolic music tokens, face two primary challenges: (1) they either embed discrete inputs into continuous state-spaces or (2) rely on variational losses that only approximate the true negative log-likelihood. Previous efforts have individually targeted these limitations. While information-theoretic Gaussian diffusion models alleviate the suboptimality of variational losses, they still perform modeling in continuous domains. In this work, we introduce the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which simultaneously addresses both limitations by directly operating in a discrete state-space via a Poisson diffusion process inspired by photon arrival processes in camera sensors. We introduce a novel Poisson Reconstruction Loss (PRL) and derive an exact relationship between PRL and the true negative log-likelihood, thereby eliminating the need for approximate evidence lower bounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the CIFAR-10 image benchmark demonstrate that ItDPDM delivers significant improvements, reducing test NLL by up to 80% compared to prior baselines, while also achieving faster convergence. </p>
<blockquote>
<p>ç°æœ‰é’ˆå¯¹ç¦»æ•£æ•°æ®ï¼ˆå¦‚ç¬¦å·éŸ³ä¹æ ‡è®°ï¼‰ç”Ÿæˆå»ºæ¨¡çš„æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å®ƒä»¬è¦ä¹ˆå°†ç¦»æ•£è¾“å…¥åµŒå…¥åˆ°è¿ç»­çŠ¶æ€ç©ºé—´ä¸­ï¼Œè¦ä¹ˆï¼ˆ2ï¼‰ä¾èµ–äºä»…è¿‘ä¼¼çœŸå®è´Ÿå¯¹æ•°ä¼¼ç„¶çš„å˜åŒ–æŸå¤±ã€‚ä¹‹å‰çš„åŠªåŠ›å·²ç»åˆ†åˆ«é’ˆå¯¹è¿™äº›å±€é™æ€§è¿›è¡Œäº†æ”¹è¿›ã€‚è™½ç„¶ä¿¡æ¯ç†è®ºé«˜æ–¯æ‰©æ•£æ¨¡å‹ç¼“è§£äº†å˜åŒ–æŸå¤±çš„æ¬¡ä¼˜æ€§ï¼Œä½†å®ƒä»¬ä»ç„¶åœ¨è¿ç»­åŸŸä¸­è¿›è¡Œå»ºæ¨¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¿¡æ¯ç†è®ºç¦»æ•£æ³Šæ¾æ‰©æ•£æ¨¡å‹ï¼ˆItDPDMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡å—ç›¸æœºä¼ æ„Ÿå™¨ä¸­å…‰å­åˆ°è¾¾è¿‡ç¨‹å¯å‘çš„æ³Šæ¾æ‰©æ•£è¿‡ç¨‹ï¼Œç›´æ¥åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è§£å†³è¿™ä¸¤ä¸ªå±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ³Šæ¾é‡å»ºæŸå¤±ï¼ˆPRLï¼‰ï¼Œå¹¶æ¨å¯¼äº†PRLå’ŒçœŸå®è´Ÿå¯¹æ•°ä¼¼ç„¶ä¹‹é—´çš„ç²¾ç¡®å…³ç³»ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹è¿‘ä¼¼è¯æ®ä¸‹ç•Œçš„éœ€æ±‚ã€‚åœ¨Lakh MIDIç¬¦å·éŸ³ä¹æ•°æ®é›†å’ŒCIFAR-10å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒItDPDMæä¾›äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸å…ˆå‰åŸºå‡†ç›¸æ¯”ï¼Œæµ‹è¯•NLLé™ä½äº†é«˜è¾¾80%ï¼ŒåŒæ—¶å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05082v1">PDF</a> Pre-print</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¿¡æ¯ç†è®ºç¦»æ•£æ³Šæ¾æ‰©æ•£æ¨¡å‹ï¼ˆItDPDMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ³Šæ¾æ‰©æ•£è¿‡ç¨‹ç›´æ¥åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œè§£å†³äº†å½“å‰ç”Ÿæˆç¦»æ•£æ•°æ®å»ºæ¨¡çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ItDPDMå¼•å…¥äº†æ–°å‹çš„æ³Šæ¾é‡å»ºæŸå¤±ï¼ˆPRLï¼‰ï¼Œå¹¶ä¸çœŸæ­£çš„è´Ÿå¯¹æ•°ä¼¼ç„¶å»ºç«‹äº†ç²¾ç¡®å…³ç³»ï¼Œæ— éœ€ä½¿ç”¨è¿‘ä¼¼çš„è¯æ®ä¸‹ç•Œã€‚åœ¨Lakh MIDIç¬¦å·éŸ³ä¹æ•°æ®é›†å’ŒCIFAR-10å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒItDPDMåœ¨æµ‹è¯•NLLä¸Šè¾ƒå…ˆå‰åŸºçº¿é™ä½äº†é«˜è¾¾80%ï¼ŒåŒæ—¶å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ItDPDMè§£å†³äº†ç”Ÿæˆç¦»æ•£æ•°æ®å»ºæ¨¡çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåµŒå…¥ç¦»æ•£è¾“å…¥åˆ°è¿ç»­çŠ¶æ€ç©ºé—´å’Œä½¿ç”¨å˜åˆ†æŸå¤±æ¥è¿‘ä¼¼çœŸå®è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚</li>
<li>ItDPDMé€šè¿‡æ³Šæ¾æ‰©æ•£è¿‡ç¨‹ç›´æ¥åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå—åˆ°ç›¸æœºä¼ æ„Ÿå™¨ä¸­å…‰å­åˆ°è¾¾è¿‡ç¨‹çš„å¯å‘ã€‚</li>
<li>ItDPDMå¼•å…¥äº†æ–°å‹çš„æ³Šæ¾é‡å»ºæŸå¤±ï¼ˆPRLï¼‰ã€‚</li>
<li>PRLå’ŒçœŸå®è´Ÿå¯¹æ•°ä¼¼ç„¶ä¹‹é—´å»ºç«‹äº†ç²¾ç¡®å…³ç³»ï¼Œæ— éœ€ä½¿ç”¨è¿‘ä¼¼çš„è¯æ®ä¸‹ç•Œã€‚</li>
<li>åœ¨Lakh MIDIç¬¦å·éŸ³ä¹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒItDPDMåœ¨æµ‹è¯•NLLä¸Šè¾ƒå…ˆå‰æ–¹æ³•é™ä½äº†æ˜¾è‘—æ¯”ä¾‹ã€‚</li>
<li>åœ¨CIFAR-10å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šï¼ŒItDPDMå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>ItDPDMçš„ä¼˜å¼‚æ€§èƒ½è¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-03e3687961af0f6f0a40ee202d9d8d3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-557eddbc1ad773b32ce262735680dd1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b523116f94103192dae6bbb17240e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c861688623ec2e008878ac47d1d2df6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14e74a7fe39681e308d94dead5c4f8dc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SOAP-Style-Omniscient-Animatable-Portraits"><a href="#SOAP-Style-Omniscient-Animatable-Portraits" class="headerlink" title="SOAP: Style-Omniscient Animatable Portraits"></a>SOAP: Style-Omniscient Animatable Portraits</h2><p><strong>Authors:Tingting Liao, Yujian Zheng, Adilbek Karmanov, Liwen Hu, Leyang Jin, Yuliang Xiu, Hao Li</strong></p>
<p>Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at <a target="_blank" rel="noopener" href="https://github.com/TingtingLiao/soap">https://github.com/TingtingLiao/soap</a>. </p>
<blockquote>
<p>ä»å•å¼ å›¾ç‰‡åˆ›å»ºå¯åŠ¨ç”»çš„3Då¤´åƒä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºæ¶‰åŠåˆ°é£æ ¼ï¼ˆå†™å®ã€å¡é€šã€åŠ¨æ¼«ï¼‰çš„å±€é™ï¼Œä»¥åŠå¤„ç†é…ä»¶å’Œå‘å‹ç­‰éš¾åº¦ã€‚è™½ç„¶3Dæ‰©æ•£æ¨¡å‹æ¨åŠ¨äº†å•è§†å›¾é‡å»ºåœ¨ä¸€èˆ¬ç‰©ä½“ä¸Šçš„åº”ç”¨ï¼Œä½†è¾“å‡ºå¾€å¾€ç¼ºä¹åŠ¨ç”»æ§åˆ¶æˆ–ç”±äºé¢†åŸŸå·®è·è€Œäº§ç”Ÿä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºäº†SOAPï¼Œè¿™æ˜¯ä¸€ä¸ªé£æ ¼å…¨é¢çš„æ¡†æ¶ï¼Œå¯ä»¥ä»ä»»ä½•è‚–åƒç”Ÿæˆè£…é…ã€æ‹“æ‰‘ä¸€è‡´çš„å¤´åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†åœ¨24K 3Då¤´åƒä¸Šè®­ç»ƒçš„å¤šå…ƒè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªé€‚åº”ä¼˜åŒ–ç®¡é“æ¥å˜å½¢FLAMEç½‘æ ¼ï¼ŒåŒæ—¶é€šè¿‡å¯å¾®åˆ†æ¸²æŸ“ä¿æŒæ‹“æ‰‘å’Œè£…é…ã€‚å¾—åˆ°çš„çº¹ç†å¤´åƒæ”¯æŒåŸºäºFACsçš„åŠ¨ç”»ï¼Œå¯ä»¥ä¸çœ¼çƒå’Œç‰™é½¿é›†æˆï¼Œå¹¶ä¿ç•™å¦‚è¾«å­å¤´å‘æˆ–é…ä»¶ç­‰ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å•è§†å›¾å¤´éƒ¨å»ºæ¨¡å’ŒåŸºäºæ‰©æ•£çš„å›¾åƒåˆ°3Dç”Ÿæˆæ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯ä¾›ç ”ç©¶ç›®çš„åœ¨<a target="_blank" rel="noopener" href="https://github.com/TingtingLiao/soap%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/TingtingLiao/soapä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05022v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå•å¹…å›¾åƒåˆ›å»ºå¯åŠ¨ç”»çš„3Då¤´åƒé¢ä¸´é£æ ¼å¤šæ ·æ€§å’Œé…ä»¶å¤„ç†éš¾é¢˜ã€‚æœ€æ–°3Dæ‰©æ•£æ¨¡å‹è™½æå‡äº†ä¸€èˆ¬ç‰©ä½“çš„å•è§†è§’é‡å»ºæ•ˆæœï¼Œä½†åœ¨åˆ›å»ºåŠ¨ç”»æ§åˆ¶å’Œå…‹æœåŸŸå·®å¼‚æ—¶ä»æœ‰å±€é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†SOAPæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯åŠ¨çš„ã€ç»“æ„ä¸€è‡´çš„å¤´åƒï¼Œé€‚ç”¨äºå„ç§è‚–åƒå›¾åƒã€‚é€šè¿‡åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ä¸24K 3Då¤´åƒæ•°æ®åº“è®­ç»ƒï¼Œç»“åˆè‡ªé€‚åº”ä¼˜åŒ–ç®¡é“å’Œå¯å¾®åˆ†æ¸²æŸ“æŠ€æœ¯ï¼Œå®ç°FLAMEç½‘æ ¼å˜å½¢å¹¶ä¿æŒæ‹“æ‰‘ç»“æ„ã€‚ç”Ÿæˆçš„çº¹ç†å¤´åƒæ”¯æŒåŸºäºé¢éƒ¨åŠ¨ä½œç¼–ç ç³»ç»Ÿçš„åŠ¨ç”»ï¼Œèƒ½èå…¥çœ¼çƒå’Œç‰™é½¿ç»†èŠ‚ï¼Œå¹¶ä¿ç•™å¦‚è¾«å­æˆ–é…ä»¶ç­‰ç»†èŠ‚ã€‚å®éªŒè¯æ˜ï¼Œæ­¤æ–¹æ³•åœ¨å•è§†è§’å¤´éƒ¨å»ºæ¨¡å’ŒåŸºäºæ‰©æ•£çš„å›¾åƒåˆ°3Dè½¬æ¢é¢†åŸŸå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å…¬å¼€å‘å¸ƒä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SOAPæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šç§é£æ ¼çš„3Då¤´åƒï¼Œè§£å†³äº†å•å›¾åƒåˆ›å»ºåŠ¨ç”»3Då¤´åƒçš„é£æ ¼å¤šæ ·æ€§éš¾é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œæé«˜äº†å¯¹ä¸€èˆ¬ç‰©ä½“çš„å•è§†è§’é‡å»ºæ•ˆæœã€‚</li>
<li>SOAPé€šè¿‡è‡ªé€‚åº”ä¼˜åŒ–ç®¡é“å’Œå¯å¾®åˆ†æ¸²æŸ“æŠ€æœ¯ï¼Œå®ç°äº†FLAMEç½‘æ ¼å˜å½¢å¹¶ä¿æŒæ‹“æ‰‘ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>ç”Ÿæˆçš„å¤´åƒæ”¯æŒåŸºäºé¢éƒ¨åŠ¨ä½œç¼–ç ç³»ç»Ÿçš„åŠ¨ç”»ï¼Œèƒ½è‡ªç„¶èå…¥çœ¼çƒå’Œç‰™é½¿ç»†èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä¿ç•™å¤´åƒçš„ç²¾ç»†ç»†èŠ‚ï¼Œå¦‚è¾«å­ã€é…ä»¶ç­‰ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSOAPåœ¨å•è§†è§’å¤´éƒ¨å»ºæ¨¡å’Œå›¾åƒåˆ°3Dè½¬æ¢æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b92a2b25d552b2c28033396289900825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76b20f025518360822289d874cbff63b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b262498308bd8785597b700929fe292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a7c65246b71ada9321aaa6714a790ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372930e4147d616dd7c0e52b01890b29.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GlyphMastero-A-Glyph-Encoder-for-High-Fidelity-Scene-Text-Editing"><a href="#GlyphMastero-A-Glyph-Encoder-for-High-Fidelity-Scene-Text-Editing" class="headerlink" title="GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing"></a>GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing</h2><p><strong>Authors:Tong Wang, Ting Liu, Xiaochao Qu, Chengjing Wu, Luoqi Liu, Xiaolin Hu</strong></p>
<p>Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region Fr&#39;echet inception distance by 53.28%. </p>
<blockquote>
<p>åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ˜¯å›¾åƒç¼–è¾‘çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒè¦æ±‚åœ¨ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬æ—¶ä¿æŒä¸å‘¨å›´ç¯å¢ƒçš„é£æ ¼ä¸€è‡´æ€§å’Œè§†è§‰è¿è´¯æ€§ã€‚è™½ç„¶åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å±•ç°å‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿæ‰­æ›²æˆ–æ— æ³•è¯†åˆ«çš„å­—ç¬¦ï¼Œç‰¹åˆ«æ˜¯å½“å¤„ç†ä¸­æ–‡ç­‰å¤æ‚å­—ç¬¦æ—¶ã€‚åœ¨è¿™æ ·çš„ç³»ç»Ÿä¸­ï¼Œå­—ç¬¦ç”±å¤æ‚çš„ç¬”ç”»æ¨¡å¼å’Œç©ºé—´å…³ç³»ç»„æˆï¼Œå¿…é¡»ç²¾ç¡®ä¿æŒã€‚æˆ‘ä»¬æå‡ºäº†GlyphMasteroï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨çš„å­—å½¢ç¼–ç å™¨ï¼Œæ—¨åœ¨å¼•å¯¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ä»¥ç¬”ç”»çº§ç²¾åº¦ç”Ÿæˆæ–‡æœ¬ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œç°æœ‰æ–¹æ³•è™½ç„¶ä½¿ç”¨é¢„è®­ç»ƒçš„OCRæ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä½†æœªèƒ½æ•æ‰æ–‡æœ¬ç»“æ„çš„å±‚æ¬¡æ€§â€”â€”ä»å•ä¸ªç¬”ç”»åˆ°ç¬”ç”»çº§äº¤äº’å†åˆ°æ•´ä½“å­—ç¬¦çº§ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„å­—å½¢ç¼–ç å™¨é€šè¿‡æ–°å‹å­—å½¢æ³¨æ„æ¨¡å—æ˜ç¡®åœ°å»ºæ¨¡å¹¶æ•æ‰å±€éƒ¨çº§åˆ«å•ä¸ªå­—ç¬¦ä¸å…¨å±€çº§åˆ«æ–‡æœ¬è¡Œä¹‹é—´çš„è·¨çº§äº¤äº’ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼Œä»¥åœ¨å…¨å±€çº§åˆ«èåˆå¤šå°ºåº¦OCRä¸»å¹²ç‰¹å¾ã€‚é€šè¿‡è¿™äº›è·¨çº§åˆ«å’Œå¤šå°ºåº¦èåˆï¼Œæˆ‘ä»¬è·å¾—äº†æ›´è¯¦ç»†çš„å­—å½¢æ„ŸçŸ¥æŒ‡å¯¼ï¼Œå®ç°å¯¹åœºæ™¯æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¥å­å‡†ç¡®æ€§æ–¹é¢æ¯”æœ€æ–°çš„å¤šè¯­è¨€åœºæ™¯æ–‡æœ¬ç¼–è¾‘åŸºçº¿æé«˜äº†18.02%ï¼ŒåŒæ—¶é™ä½äº†æ–‡æœ¬åŒºåŸŸFrâ€™echet inceptionè·ç¦»53.28%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04915v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœºæ™¯æ–‡æœ¬ç¼–è¾‘çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•GlyphMasteroã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸“é—¨çš„å­—å½¢ç¼–ç å™¨æŒ‡å¯¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç¬”ç”»çº§åˆ«çš„ç²¾åº¦ç”Ÿæˆæ–‡æœ¬ã€‚å…¶å…³é”®æ´å¯ŸåŠ›æ˜¯ç°æœ‰æ–¹æ³•æœªèƒ½æ•æ‰æ–‡æœ¬çš„å±‚æ¬¡ç»“æ„ï¼Œå¦‚ä»å•ä¸ªç¬”ç”»åˆ°ç¬”ç”»çº§äº¤äº’å†åˆ°æ•´ä½“å­—ç¬¦çº§ç»“æ„ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒGlyphMasteroé€šè¿‡å…¶æ–°é¢–çš„å­—å½¢æ³¨æ„åŠ›æ¨¡å—æ˜¾å¼åœ°å»ºæ¨¡å’Œæ•æ‰å±€éƒ¨çº§åˆ«çš„å•ä¸ªå­—ç¬¦å’Œå…¨å±€çº§åˆ«çš„æ–‡æœ¬è¡Œä¹‹é—´çš„è·¨çº§åˆ«äº¤äº’ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¸€ä¸ªç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼Œä»¥åœ¨å…¨å±€å±‚é¢ä¸Šèåˆå¤šå°ºåº¦OCRä¸»å¹²ç‰¹å¾ã€‚é€šè¿‡è¿™äº›è·¨çº§åˆ«å’Œå¤šå°ºåº¦çš„èåˆï¼ŒGlyphMasteroè·å¾—äº†æ›´è¯¦ç»†çš„å­—å½¢æ„ŸçŸ¥æŒ‡å¯¼ï¼Œå®ç°å¯¹åœºæ™¯æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶ã€‚è¯¥æ–¹æ³•åœ¨å¥å­å‡†ç¡®åº¦ä¸Šè¾ƒå½“å‰æœ€ä½³çš„å¤šè¯­è¨€åœºæ™¯æ–‡æœ¬ç¼–è¾‘åŸºçº¿æé«˜äº†18.02%ï¼ŒåŒæ—¶é™ä½äº†æ–‡æœ¬åŒºåŸŸçš„FrÃ©chet inceptionè·ç¦»è¾¾53.28%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ˜¯å›¾åƒç¼–è¾‘çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œæ—¨åœ¨ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬ï¼ŒåŒæ—¶ä¿æŒé£æ ¼ä¸€è‡´æ€§å’Œä¸å‘¨å›´ç¯å¢ƒçš„è§†è§‰è¿è´¯æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚å­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰æ—¶ç”Ÿæˆç»“æœå¸¸å‡ºç°å¤±çœŸæˆ–æ— æ³•è¯†åˆ«ã€‚</li>
<li>GlyphMasteroæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸“é—¨å­—å½¢ç¼–ç å™¨ï¼Œæ—¨åœ¨ä»¥ç¬”ç”»çº§åˆ«çš„ç²¾åº¦ç”Ÿæˆæ–‡æœ¬ã€‚</li>
<li>GlyphMasteroé€šè¿‡å­—å½¢æ³¨æ„åŠ›æ¨¡å—æ˜¾å¼åœ°å»ºæ¨¡å’Œæ•æ‰æ–‡æœ¬çš„å±‚æ¬¡ç»“æ„ã€‚</li>
<li>GlyphMasteroå®ç°äº†ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼Œèåˆå¤šå°ºåº¦OCRä¸»å¹²ç‰¹å¾ï¼Œä»¥åœ¨å…¨å±€å±‚é¢ä¸Šè·å¾—æ›´è¯¦ç»†çš„å­—å½¢æ„ŸçŸ¥æŒ‡å¯¼ã€‚</li>
<li>GlyphMasteroåœ¨å¥å­å‡†ç¡®åº¦ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c426e36100e6beb95559ace0535be29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-089823cc73ffb61602762fcf671a7125.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25e3d802befc4edd044fedad3f13d66c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-419f1c512cc4beedba8b625e93f9aa6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6afa3be4a975cf6af12dfe3b80bc79e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="D-CODA-Diffusion-for-Coordinated-Dual-Arm-Data-Augmentation"><a href="#D-CODA-Diffusion-for-Coordinated-Dual-Arm-Data-Augmentation" class="headerlink" title="D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation"></a>D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation</h2><p><strong>Authors:I-Chun Arthur Liu, Jason Chen, Gaurav Sukhatme, Daniel Seita</strong></p>
<p>Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our project website is at: <a target="_blank" rel="noopener" href="https://dcodaaug.github.io/D-CODA/">https://dcodaaug.github.io/D-CODA/</a>. </p>
<blockquote>
<p>å­¦ä¹ åŒæ‰‹åè°ƒæ“ä½œæ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒå…·æœ‰é«˜ç»´åº¦å’Œéœ€è¦ä¸¤åªæ‰‹è‡‚ç´§å¯†åè°ƒçš„ç‰¹ç‚¹ã€‚çœ¼åœ¨æ‰‹ä¸Šçš„æ¨¡ä»¿å­¦ä¹ ä½¿ç”¨æ‰‹è…•å®‰è£…çš„ç›¸æœºï¼Œé€šè¿‡ä¸“æ³¨äºä»»åŠ¡ç›¸å…³è§†è§’æ¥ç®€åŒ–æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œæ”¶é›†å„ç§ç¤ºèŒƒä»ç„¶æˆæœ¬é«˜æ˜‚ï¼Œè¿™æ¿€å‘äº†å¯¹å¯æ‰©å±•æ•°æ®å¢å¼ºçš„éœ€æ±‚ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»åœ¨å•è‡‚è®¾ç½®ä¸­è€ƒè™‘äº†è§†è§‰å¢å¼ºï¼Œä½†å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°åŒæ‰‹æ“ä½œéœ€è¦ç”Ÿæˆä¸¤åªæ‰‹è‡‚çš„è§†è§’ä¸€è‡´çš„è§‚å¯Ÿç»“æœï¼Œå¹¶äº§ç”Ÿæ—¢æœ‰æ•ˆåˆå¯è¡Œçš„ç›¸åº”åŠ¨ä½œæ ‡ç­¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºçœ¼åœ¨æ‰‹ä¸Šçš„åŒæ‰‹æ¨¡ä»¿å­¦ä¹ çš„æ•°æ®å¢å¼ºæ–¹æ³•â€”â€”æ‰©æ•£åè°ƒåŒè‡‚æ•°æ®å¢å¼ºï¼ˆD-CODAï¼‰ã€‚è¿™æ˜¯ä¸€ç§ç¦»çº¿æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè®­ç»ƒæ‰©æ•£æ¨¡å‹åˆæˆæ–°é¢–ã€è§†è§’ä¸€è‡´çš„ä¸¤è‡‚æ‰‹è…•ç›¸æœºå›¾åƒï¼ŒåŒæ—¶ç”Ÿæˆå…³èŠ‚ç©ºé—´åŠ¨ä½œæ ‡ç­¾ã€‚å®ƒé‡‡ç”¨çº¦æŸä¼˜åŒ–ï¼Œç¡®ä¿æ¶‰åŠå¤¹æŒå™¨ä¸ç‰©ä½“æ¥è§¦ç‚¹çš„å¢å¼ºçŠ¶æ€ç¬¦åˆåŒæ‰‹åè°ƒçš„çº¦æŸæ¡ä»¶ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ¨¡æ‹Ÿä»»åŠ¡å’Œä¸‰ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šè¯„ä¼°äº†D-CODAã€‚åœ¨2250æ¬¡æ¨¡æ‹Ÿè¯•éªŒå’Œ300æ¬¡çœŸå®è¯•éªŒçš„ç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨åŸºçº¿æµ‹è¯•å’Œæ¶ˆèæµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾ç¤ºå‡ºåœ¨çœ¼åœ¨æ‰‹ä¸Šçš„åŒæ‰‹æ“ä½œä¸­çš„å¯æ‰©å±•æ•°æ®å¢å¼ºçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://dcodaaug.github.io/D-CODA%E3%80%82">https://dcodaaug.github.io/D-CODA/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04860v1">PDF</a> </p>
<p><strong>Summary</strong><br>     çœ¼æ‰‹ååŒæ¨¡ä»¿å­¦ä¹ é€šè¿‡æ‰‹è…•å®‰è£…çš„ç›¸æœºç®€åŒ–æ„ŸçŸ¥è¿‡ç¨‹ï¼Œèšç„¦äºä»»åŠ¡ç›¸å…³è§†è§’ã€‚ä½†æ”¶é›†å¤šæ ·åŒ–æ¼”ç¤ºæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¯æ‰©å±•çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚é’ˆå¯¹çœ¼æ‰‹ååŒåŒè‡‚æ“ä½œçš„æ•°æ®å¢å¼ºé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºDiffusion for COordinated Dual-arm Data Augmentationï¼ˆD-CODAï¼‰çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•è®­ç»ƒæ‰©æ•£æ¨¡å‹åˆæˆæ–°é¢–ã€è§†è§’ä¸€è‡´çš„åŒè‡‚æ‰‹è…•ç›¸æœºå›¾åƒï¼ŒåŒæ—¶ç”Ÿæˆå…³èŠ‚ç©ºé—´åŠ¨ä½œæ ‡ç­¾ã€‚é‡‡ç”¨çº¦æŸä¼˜åŒ–ç¡®ä¿å¢å¼ºçŠ¶æ€ä¸­çš„å¤¹æŒå™¨ä¸ç‰©ä½“æ¥è§¦ç¬¦åˆåŒè‡‚ååŒæ“ä½œçš„çº¦æŸæ¡ä»¶ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒD-CODAè¡¨ç°ä¼˜äºåŸºå‡†æ–¹æ³•å’Œæ¶ˆèå®éªŒï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çœ¼æ‰‹ååŒåŒè‡‚æ“ä½œä¸­çš„æ•°æ®å¢å¼ºçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœ¼æ‰‹æ¨¡ä»¿å­¦ä¹ é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬é«˜ç»´åº¦å’ŒåŒè‡‚ç´§å¯†åè°ƒçš„éœ€æ±‚ã€‚</li>
<li>æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¯æ‰©å±•çš„æ•°æ®å¢å¼ºæŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>D-CODAæ–¹æ³•é’ˆå¯¹çœ¼æ‰‹ååŒåŒè‡‚æ“ä½œçš„æ•°æ®å¢å¼ºè¿›è¡Œå®šåˆ¶ã€‚</li>
<li>D-CODAè®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥åˆæˆæ–°é¢–ã€è§†è§’ä¸€è‡´çš„åŒè‡‚å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åŒæ—¶ç”Ÿæˆå…³èŠ‚ç©ºé—´åŠ¨ä½œæ ‡ç­¾ï¼Œç¡®ä¿åŠ¨ä½œçš„æœ‰æ•ˆæ€§å’Œå¯è¡Œæ€§ã€‚</li>
<li>é€šè¿‡çº¦æŸä¼˜åŒ–ï¼Œç¡®ä¿åˆæˆçš„æ•°æ®ä¸åŒè‡‚ååŒæ“ä½œçš„å®é™…çº¦æŸä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-735e249f48585a79ba5cde0974f3eba7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-244454e08cb96525a2a117d897f1269f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b484891874fda465562389793877999a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-996181a13e95ec6bbda6909cb4fc3ad2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DGSolver-Diffusion-Generalist-Solver-with-Universal-Posterior-Sampling-for-Image-Restoration"><a href="#DGSolver-Diffusion-Generalist-Solver-with-Universal-Posterior-Sampling-for-Image-Restoration" class="headerlink" title="DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling   for Image Restoration"></a>DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling   for Image Restoration</h2><p><strong>Authors:Hebaixu Wang, Jing Zhang, Haonan Guo, Di Wang, Jiayi Ma, Bo Du</strong></p>
<p>Diffusion models have achieved remarkable progress in universal image restoration. While existing methods speed up inference by reducing sampling steps, substantial step intervals often introduce cumulative errors. Moreover, they struggle to balance the commonality of degradation representations and restoration quality. To address these challenges, we introduce \textbf{DGSolver}, a diffusion generalist solver with universal posterior sampling. We first derive the exact ordinary differential equations for generalist diffusion models and tailor high-order solvers with a queue-based accelerated sampling strategy to improve both accuracy and efficiency. We then integrate universal posterior sampling to better approximate manifold-constrained gradients, yielding a more accurate noise estimation and correcting errors in inverse inference. Extensive experiments show that DGSolver outperforms state-of-the-art methods in restoration accuracy, stability, and scalability, both qualitatively and quantitatively. Code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/DGSolver">https://github.com/MiliLab/DGSolver</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨é€šç”¨å›¾åƒä¿®å¤æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šè¿‡å‡å°‘é‡‡æ ·æ­¥éª¤æ¥åŠ é€Ÿæ¨ç†ï¼Œä½†è¾ƒå¤§çš„æ­¥éª¤é—´éš”é€šå¸¸ä¼šå¼•å…¥ç´¯ç§¯è¯¯å·®ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¾ˆéš¾åœ¨é€€åŒ–è¡¨ç¤ºçš„å…±åŒæ€§å’Œä¿®å¤è´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>DGSolver</strong>ï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰é€šç”¨åéªŒé‡‡æ ·çš„æ‰©æ•£é€šç”¨æ±‚è§£å™¨ã€‚æˆ‘ä»¬é¦–å…ˆä¸ºé€šç”¨æ‰©æ•£æ¨¡å‹æ¨å¯¼å‡ºç²¾ç¡®çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼Œå¹¶ä½¿ç”¨åŸºäºé˜Ÿåˆ—çš„åŠ é€Ÿé‡‡æ ·ç­–ç•¥å®šåˆ¶é«˜é˜¶æ±‚è§£å™¨ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬èå…¥é€šç”¨åéªŒé‡‡æ ·ï¼Œä»¥æ›´å¥½åœ°è¿‘ä¼¼æµå½¢çº¦æŸæ¢¯åº¦ï¼Œä»è€Œå¾—åˆ°æ›´å‡†ç¡®çš„å™ªå£°ä¼°è®¡å¹¶çº æ­£åå‘æ¨ç†ä¸­çš„é”™è¯¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å®šæ€§è¿˜æ˜¯å®šé‡ä¸Šï¼ŒDGSolveråœ¨ä¿®å¤å‡†ç¡®æ€§ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiliLab/DGSolver%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/MiliLab/DGSolverä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21487v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹é€šç”¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæ¢å¤ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºDGSolverçš„é€šç”¨æ‰©æ•£æ±‚è§£å™¨ï¼Œé‡‡ç”¨ç²¾ç¡®å¸¸å¾®åˆ†æ–¹ç¨‹ã€é«˜é˜¶æ±‚è§£å™¨å’ŒåŸºäºé˜Ÿåˆ—çš„åŠ é€Ÿé‡‡æ ·ç­–ç•¥ï¼Œæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚åŒæ—¶å¼•å…¥é€šç”¨åé‡‡æ ·ä»¥æ›´å¥½åœ°è¿‘ä¼¼æµå½¢çº¦æŸæ¢¯åº¦ï¼Œæ”¹å–„å™ªå£°ä¼°è®¡å’Œåå‘æ¨ç†ä¸­çš„è¯¯å·®ã€‚å®éªŒè¯æ˜ï¼ŒDGSolveråœ¨æ¢å¤å‡†ç¡®æ€§ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é€šç”¨å›¾åƒæ¢å¤ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å‡å°‘é‡‡æ ·æ­¥éª¤æ¥åŠ é€Ÿæ¨ç†ï¼Œä½†ä¼šå¯¼è‡´ç´¯ç§¯è¯¯å·®ã€‚</li>
<li>æå‡ºäº†åä¸ºDGSolverçš„é€šç”¨æ‰©æ•£æ±‚è§£å™¨ï¼Œé‡‡ç”¨ç²¾ç¡®å¸¸å¾®åˆ†æ–¹ç¨‹å’Œé«˜é˜¶æ±‚è§£å™¨ã€‚</li>
<li>DGSolverä½¿ç”¨åŸºäºé˜Ÿåˆ—çš„åŠ é€Ÿé‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>å¼•å…¥é€šç”¨åé‡‡æ ·ä»¥æ›´å¥½åœ°è¿‘ä¼¼æµå½¢çº¦æŸæ¢¯åº¦ã€‚</li>
<li>DGSolveræ”¹å–„äº†å™ªå£°ä¼°è®¡å’Œåå‘æ¨ç†ä¸­çš„è¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d15af065403b9cab07a3360e4b13a9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72f4d7613a4663d2dc7006cc35ba08e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7defe55add8baabc0eafec95e88b12d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8577738c701f00cdf6e825c271f94fdc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Nexus-Gen-A-Unified-Model-for-Image-Understanding-Generation-and-Editing"><a href="#Nexus-Gen-A-Unified-Model-for-Image-Understanding-Generation-and-Editing" class="headerlink" title="Nexus-Gen: A Unified Model for Image Understanding, Generation, and   Editing"></a>Nexus-Gen: A Unified Model for Image Understanding, Generation, and   Editing</h2><p><strong>Authors:Hong Zhang, Zhongjie Duan, Xingjun Wang, Yuze Zhao, Weiyi Lu, Zhipeng Di, Yixuan Xu, Yingda Chen, Yu Zhang</strong></p>
<p>Unified multimodal large language models (MLLMs) aim to integrate multimodal understanding and generation abilities through a single framework. Despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. To align the embedding space of the LLM and diffusion model, we conduct a dual-phase alignment training process. (1) The autoregressive LLM learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. During training the LLM, we identified a critical discrepancy between the autoregressive paradigmâ€™s training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. To avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. Through dual-phase training, Nexus-Gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. All models, datasets, and codes are published at <a target="_blank" rel="noopener" href="https://github.com/modelscope/Nexus-Gen.git">https://github.com/modelscope/Nexus-Gen.git</a> to facilitate further advancements across the field. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¨åœ¨é€šè¿‡å•ä¸€æ¡†æ¶æ•´åˆå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰å¤šåŠŸèƒ½æ€§ï¼Œä½†ç°æœ‰çš„å¼€æºç»Ÿä¸€æ¨¡å‹ä¸é¢†åŸŸç‰¹å®šæ¶æ„ä¹‹é—´å­˜åœ¨æ€§èƒ½å·®è·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Nexus-Genï¼Œè¿™æ˜¯ä¸€æ¬¾å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­è¨€æ¨ç†èƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆèƒ½åŠ›ç›¸ç»“åˆçš„ç»Ÿä¸€æ¨¡å‹ã€‚ä¸ºäº†å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„åµŒå…¥ç©ºé—´ï¼Œæˆ‘ä»¬è¿›è¡Œäº†åŒé˜¶æ®µå¯¹é½è®­ç»ƒè¿‡ç¨‹ã€‚(1) è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ æ ¹æ®å¤šæ¨¡æ€è¾“å…¥é¢„æµ‹å›¾åƒåµŒå…¥ï¼Œè€Œ(2)è§†è§‰è§£ç å™¨åˆ™è®­ç»ƒä»è¿™äº›åµŒå…¥é‡å»ºé«˜ä¿çœŸå›¾åƒã€‚åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°è‡ªå›å½’èŒƒå¼çš„è®­ç»ƒé˜¶æ®µå’Œæ¨ç†é˜¶æ®µä¹‹é—´å­˜åœ¨å…³é”®å·®å¼‚ï¼Œè¿ç»­åµŒå…¥ç©ºé—´ä¸­çš„è¯¯å·®ç´¯ç§¯ä¼šä¸¥é‡é™ä½ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†é¿å…è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é¢„å¡«å……è‡ªå›å½’ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç”¨ä½ç½®åµŒå…¥çš„ç‰¹æ®Šä»¤ç‰Œé¢„å¡«å……è¾“å…¥åºåˆ—ï¼Œè€Œä¸æ˜¯è¿ç»­åµŒå…¥ã€‚é€šè¿‡åŒé˜¶æ®µè®­ç»ƒï¼ŒNexus-Genå·²å…·å¤‡å…¨é¢è§£å†³å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡çš„ç»¼åˆèƒ½åŠ›ã€‚æ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç éƒ½å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/modelscope/Nexus-Gen.git%E4%B8%8A%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%91%E5%B1%95%E3%80%82">https://github.com/modelscope/Nexus-Gen.gitä¸Šï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21356v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ—¨åœ¨é€šè¿‡å•ä¸€æ¡†æ¶æ•´åˆå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç°æœ‰å¼€æºç»Ÿä¸€æ¨¡å‹åœ¨é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ¶æ„æ–¹é¢å­˜åœ¨æ€§èƒ½å·®è·ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Nexus-Genï¼Œä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­è¨€æ¨ç†èƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆèƒ½åŠ›ç›¸ç»“åˆçš„ç»Ÿä¸€æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡åŒé˜¶æ®µå¯¹é½è®­ç»ƒè¿‡ç¨‹æ¥å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„åµŒå…¥ç©ºé—´ï¼šä¸€æ˜¯è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ åŸºäºå¤šæ¨¡æ€è¾“å…¥çš„å›¾åƒåµŒå…¥é¢„æµ‹ï¼ŒäºŒæ˜¯è§†è§‰è§£ç å™¨ä»è¿™äº›åµŒå…¥ä¸­é‡å»ºé«˜è´¨é‡å›¾åƒã€‚åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å‘ç°è‡ªå›å½’èŒƒå¼åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¹‹é—´å­˜åœ¨å…³é”®å·®å¼‚ï¼Œè¿ç»­åµŒå…¥ç©ºé—´ä¸­çš„è¯¯å·®ç´¯ç§¯ä¸¥é‡é™ä½äº†ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†é¿å…è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é¢„å¡«å……è‡ªå›å½’ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç”¨ä½ç½®åµŒå…¥çš„ç‰¹æ®Šä»¤ç‰Œé¢„å¡«å……è¾“å…¥åºåˆ—ï¼Œè€Œä¸æ˜¯è¿ç»­åµŒå…¥ã€‚é€šè¿‡åŒé˜¶æ®µè®­ç»ƒï¼ŒNexus-Genå·²å…·å¤‡å…¨é¢è§£å†³å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡çš„ç»¼åˆèƒ½åŠ›ã€‚æ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç å‡å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/modelscope/Nexus-Gen.git%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%91%E5%B1%95%E3%80%82">https://github.com/modelscope/Nexus-Gen.gitï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¼€æºç»Ÿä¸€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæ–¹é¢å­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
<li>Nexus-Genæ—¨åœ¨ç¼©å°æ€§èƒ½å·®è·ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åŒé˜¶æ®µå¯¹é½è®­ç»ƒè¿‡ç¨‹å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„åµŒå…¥ç©ºé—´ã€‚</li>
<li>åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶å‘ç°äº†è‡ªå›å½’èŒƒå¼çš„è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå·®å¼‚ã€‚</li>
<li>å¼•å…¥é¢„å¡«å……è‡ªå›å½’ç­–ç•¥ä»¥é¿å…è¿ç»­åµŒå…¥ç©ºé—´ä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9bf9cf08491c096050be2ac5f42aa0de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e38ca9a369d5342fcb3eaac097539f92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f870bede37548c9a3885c4ff0fbd97ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ce13c70f0d32254591fab13dabe9ae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09a436404eaa50247abe30ab1918b1b8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DejAIvu-Identifying-and-Explaining-AI-Art-on-the-Web-in-Real-Time-with-Saliency-Maps"><a href="#DejAIvu-Identifying-and-Explaining-AI-Art-on-the-Web-in-Real-Time-with-Saliency-Maps" class="headerlink" title="DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with   Saliency Maps"></a>DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with   Saliency Maps</h2><p><strong>Authors:Jocelyn Dzuong</strong></p>
<p>The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at <a target="_blank" rel="noopener" href="https://github.com/Noodulz/dejAIvu">https://github.com/Noodulz/dejAIvu</a>. </p>
<blockquote>
<p>è¿‘æœŸå…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ¿€å¢ï¼Œå¯¼è‡´ç½‘ä¸Šå„ä¸ªé¢†åŸŸAIç”Ÿæˆçš„å›¾åƒæ•°é‡æƒŠäººåœ°å¢é•¿ã€‚è™½ç„¶è¿™äº›æŠ€æœ¯å¸¦æ¥äº†æ°‘ä¸»åŒ–è‰ºæœ¯åˆ›ä½œç­‰å¥½å¤„ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†è™šå‡ä¿¡æ¯ã€æ•°å­—ä¼ªé€ å’Œèº«ä»½éªŒè¯çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåª’ä½“å’Œè¥é”€ä¸­æœªç»æˆæƒä½¿ç”¨AIç”Ÿæˆçš„å›¾åƒå¼•å‘äº†åœ¨çº¿ç¤¾åŒºçš„å¼ºçƒˆåå¯¹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DejAIvuï¼Œè¿™æ˜¯ä¸€æ¬¾Chromeç½‘é¡µæ‰©å±•ç¨‹åºï¼Œç»“åˆå®æ—¶AIç”Ÿæˆå›¾åƒæ£€æµ‹ä¸ç”¨æˆ·æµè§ˆç½‘é¡µæ—¶çš„åŸºäºæ˜¾è‘—æ€§çš„è§£é‡Šæ€§ã€‚DejAIvuä½¿ç”¨ä¼˜åŒ–çš„ONNXæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ†æç½‘ç«™ä¸Šçš„å›¾åƒï¼ˆå¦‚Google Imagesï¼‰ï¼Œé€šè¿‡æ¨¡å‹æ¨ç†è¯†åˆ«AIç”Ÿæˆçš„å†…å®¹ï¼Œå¹¶é€šè¿‡è¦†ç›–æ˜¾è‘—æ€§çƒ­å›¾æ¥çªå‡ºAIç›¸å…³ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†é«˜æ•ˆçš„æµè§ˆå™¨å†…æ¨ç†ã€åŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§åˆ†æå’Œæ— ç¼ç”¨æˆ·ä½“éªŒï¼Œç¡®ä¿AIæ£€æµ‹æ—¢é€æ˜åˆæ˜“è§£é‡Šã€‚æˆ‘ä»¬è¿˜å¯¹DejAIvuè¿›è¡Œäº†è·¨å¤šä¸ªé¢„è®­ç»ƒæ¶æ„å’ŒåŸºå‡†æ•°æ®é›†çš„è¯„ä¼°ï¼Œè¯æ˜äº†å…¶é«˜å‡†ç¡®æ€§å’Œä½å»¶è¿Ÿæ€§ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºAIå›¾åƒè´£ä»»åˆ¶çš„å®ç”¨ä¸”å¯éƒ¨ç½²çš„å·¥å…·ã€‚è¯¥ç³»ç»Ÿçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Noodulz/dejAIvu%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Noodulz/dejAIvuæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08821v2">PDF</a> 5 pages, 3 figures. Accepted to IJCAI 2025 Demo Track. Revised   version will be uploaded soon</p>
<p><strong>Summary</strong>ï¼š<br>æœ€è¿‘å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å¦‚æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å…´èµ·ï¼Œå¯¼è‡´ç½‘ä¸Šå„é¢†åŸŸAIç”Ÿæˆçš„å›¾åƒæ¿€å¢ã€‚è™½ç„¶è¿™äº›æŠ€æœ¯å¸¦æ¥äº†æ°‘ä¸»åŒ–è‰ºæœ¯åˆ›ä½œç­‰å¥½å¤„ï¼Œä½†ä¹Ÿå¸¦æ¥äº†è™šå‡ä¿¡æ¯ã€æ•°å­—ä¼ªé€ å’Œèº«ä»½è®¤è¯ç­‰æŒ‘æˆ˜ã€‚æœªç»æˆæƒä½¿ç”¨AIç”Ÿæˆçš„å›¾åƒåœ¨åª’ä½“å’Œè¥é”€ä¸­å¼•å‘äº†åœ¨çº¿ç¤¾åŒºçš„å¼ºçƒˆåå¯¹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DejAIvuï¼Œè¿™æ˜¯ä¸€æ¬¾Chromeç½‘é¡µæ‰©å±•ç¨‹åºï¼Œç»“åˆäº†å®æ—¶AIç”Ÿæˆçš„å›¾åƒæ£€æµ‹ä¸ç”¨æˆ·æµè§ˆç½‘é¡µæ—¶çš„æ˜¾è‘—æ€§è§£é‡Šã€‚å®ƒé€šè¿‡ä¼˜åŒ–åçš„æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†æç½‘ç«™ä¸Šçš„å›¾åƒï¼Œè¯†åˆ«AIç”Ÿæˆçš„å†…å®¹ï¼Œå¹¶å åŠ æ˜¾è‘—æ€§çƒ­åº¦å›¾æ¥çªå‡ºæ˜¾ç¤ºAIç›¸å…³çš„ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†AIæ£€æµ‹çš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ï¼ŒåŒæ—¶å®ç°äº†é«˜æ•ˆçš„æµè§ˆå™¨å†…æ¨ç†ã€åŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§åˆ†æå’Œæ— ç¼ç”¨æˆ·ä½“éªŒã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†DejAIvuåœ¨å¤šæ¶æ„é¢„è®­ç»ƒæ¨¡å‹å’ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§å’Œä½å»¶è¿Ÿæ€§ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºAIå›¾åƒè´£ä»»åˆ¶çš„å®ç”¨å’Œå¯éƒ¨ç½²å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å¦‚æ‰©æ•£æ¨¡å‹å’ŒGANså¯¼è‡´ç½‘ä¸ŠAIç”Ÿæˆçš„å›¾åƒæ¿€å¢ã€‚</li>
<li>AIç”ŸæˆæŠ€æœ¯è™½æœ‰åŠ©äºæ°‘ä¸»åŒ–è‰ºæœ¯åˆ›ä½œï¼Œä½†ä¹Ÿå¸¦æ¥è™šå‡ä¿¡æ¯ã€æ•°å­—ä¼ªé€ ç­‰é—®é¢˜ã€‚</li>
<li>æœªç»æˆæƒä½¿ç”¨AIç”Ÿæˆçš„å›¾åƒåœ¨åª’ä½“å’Œè¥é”€ä¸­å¼•å‘ç¤¾åŒºåå¯¹ã€‚</li>
<li>DejAIvuæ˜¯ä¸€æ¬¾Chromeç½‘é¡µæ‰©å±•ç¨‹åºï¼Œèƒ½å®æ—¶æ£€æµ‹AIç”Ÿæˆçš„å›¾åƒå¹¶æ˜¾ç¤ºæ˜¾è‘—æ€§çƒ­åº¦å›¾ã€‚</li>
<li>DejAIvuç»“åˆäº†å®æ—¶AIå›¾åƒæ£€æµ‹ã€æ˜¾è‘—æ€§åˆ†æå’Œæ— ç¼ç”¨æˆ·ä½“éªŒã€‚</li>
<li>DejAIvuåœ¨å¤šæ¶æ„é¢„è®­ç»ƒæ¨¡å‹å’ŒåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§å’Œä½å»¶è¿Ÿæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9d00e9057659c1c96f4134c1da35239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-016b7f2175e761325c065ff02ebecf95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-977467f71d770b9062d079a36fd06fb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdddce1b9a31973ca461a799838706b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddb6544746b73eea53e65d1936e47a5d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SceneCraft-Layout-Guided-3D-Scene-Generation"><a href="#SceneCraft-Layout-Guided-3D-Scene-Generation" class="headerlink" title="SceneCraft: Layout-Guided 3D Scene Generation"></a>SceneCraft: Layout-Guided 3D Scene Generation</h2><p><strong>Authors:Xiuyu Yang, Yunze Man, Jun-Kun Chen, Yu-Xiong Wang</strong></p>
<p>The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. Code and more results are available at: <a target="_blank" rel="noopener" href="https://orangesodahub.github.io/SceneCraft">https://orangesodahub.github.io/SceneCraft</a> </p>
<blockquote>
<p>ä½¿ç”¨ä¼ ç»Ÿçš„3Då»ºæ¨¡å·¥å…·åˆ›å»ºç¬¦åˆç”¨æˆ·è§„æ ¼çš„å¤æ‚3Dåœºæ™¯ä¸€ç›´æ˜¯ä¸€é¡¹ç¹çä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡ä¸€äº›å…ˆè¿›çš„æ–¹æ³•å·²ç»å®ç°äº†è‡ªåŠ¨æ–‡æœ¬åˆ°3Dçš„ç”Ÿæˆï¼Œä½†å®ƒä»¬é€šå¸¸ä»…é™äºè§„æ¨¡è¾ƒå°çš„åœºæ™¯ï¼Œå¯¹å½¢çŠ¶å’Œçº¹ç†çš„æ§åˆ¶æœ‰é™ã€‚æˆ‘ä»¬æ¨å‡ºäº†SceneCraftï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆè¯¦ç»†å®¤å†…åœºæ™¯çš„æ–°æ–¹æ³•ï¼Œå®ƒç¬¦åˆç”¨æˆ·æä¾›çš„æ–‡æœ¬æè¿°å’Œç©ºé—´å¸ƒå±€åå¥½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŸºäºæ¸²æŸ“çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†3Dè¯­ä¹‰å¸ƒå±€è½¬æ¢ä¸ºå¤šè§†è§’çš„2Dä»£ç†åœ°å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰å’Œæ·±åº¦çº¦æŸçš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆå¤šè§†è§’å›¾åƒï¼Œç”¨äºå­¦ä¹ ä½œä¸ºæœ€ç»ˆåœºæ™¯è¡¨ç¤ºçš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚æˆ‘ä»¬ä¸å—å…¨æ™¯å›¾åƒç”Ÿæˆçš„çº¦æŸï¼Œåœ¨æ”¯æŒå¤æ‚å®¤å†…ç©ºé—´ç”Ÿæˆæ–¹é¢è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œä¸ä»…é™äºå•ä¸ªæˆ¿é—´ï¼Œç”šè‡³å¯ä»¥ç”Ÿæˆæ•´ä¸ªå¤šå§å®¤å…¬å¯“çš„ä¸è§„åˆ™å½¢çŠ¶å’Œå¸ƒå±€ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¤šæ ·çº¹ç†ã€ä¸€è‡´å‡ ä½•ç»“æ„å’Œé€¼çœŸè§†è§‰è´¨é‡çš„å¤æ‚å®¤å†…åœºæ™¯æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’Œæ›´å¤šç»“æœå¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://orangesodahub.github.io/SceneCraft">SceneCraft</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09049v3">PDF</a> NeurIPS 2024. Code: <a target="_blank" rel="noopener" href="https://github.com/OrangeSodahub/SceneCraft">https://github.com/OrangeSodahub/SceneCraft</a>   Project Page: <a target="_blank" rel="noopener" href="https://orangesodahub.github.io/SceneCraft">https://orangesodahub.github.io/SceneCraft</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬æè¿°å’Œç”¨æˆ·æä¾›çš„ç©ºé—´å¸ƒå±€åå¥½ï¼ŒSceneCraftæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„å®¤å†…åœºæ™¯ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæ¸²æŸ“çš„æŠ€æœ¯ï¼Œå°†3Dè¯­ä¹‰å¸ƒå±€è½¬æ¢ä¸ºå¤šè§†è§’çš„2Dä»£ç†åœ°å›¾ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰å’Œæ·±åº¦æ¡ä»¶ä¸‹çš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†è§’å›¾åƒã€‚æœ€ç»ˆåœºæ™¯è¡¨ç¤ºä¸ºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚è¯¥æ–¹æ³•æ”¯æŒå¤æ‚çš„å®¤å†…ç©ºé—´ç”Ÿæˆï¼ŒåŒ…æ‹¬è¶…è¿‡å•æˆ¿é—´çš„ç©ºé—´ï¼Œå¦‚å¤æ‚çš„å¤šå§å®¤å…¬å¯“ï¼Œå…·æœ‰ä¸è§„åˆ™å½¢çŠ¶å’Œå¸ƒå±€ã€‚åœ¨çº¹ç†å¤šæ ·ã€å‡ ä½•ä¸€è‡´å’Œè§†è§‰çœŸå®æ„Ÿæ–¹é¢ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SceneCraftæ˜¯ä¸€ç§åŸºäºæ–‡æœ¬æè¿°å’Œç”¨æˆ·åå¥½ç”Ÿæˆè¯¦ç»†å®¤å†…åœºæ™¯çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨æ¸²æŸ“æŠ€æœ¯å°†3Dè¯­ä¹‰å¸ƒå±€è½¬æ¢ä¸ºå¤šè§†è§’çš„2Dä»£ç†åœ°å›¾ã€‚</li>
<li>SceneCraftè®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰å’Œæ·±åº¦æ¡ä»¶ä¸‹çš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†è§’å›¾åƒã€‚</li>
<li>æœ€ç»ˆåœºæ™¯è¡¨ç¤ºä¸ºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒå¤æ‚çš„å®¤å†…ç©ºé—´ç”Ÿæˆï¼ŒåŒ…æ‹¬è¶…è¿‡å•æˆ¿é—´çš„ç©ºé—´ï¼Œå…·æœ‰ä¸è§„åˆ™å½¢çŠ¶å’Œå¸ƒå±€ã€‚</li>
<li>SceneCraftåœ¨çº¹ç†å¤šæ ·ã€å‡ ä½•ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ„Ÿæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e7b7b0d686f9ea5b71864efd3aac2b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c031872cac09fec49b802c608bdb672d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71aad8655af6b5a69a21d25b3e6f4d08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a27d5fae3e418fb37c2acd61c3d371d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Lexicon3D-Probing-Visual-Foundation-Models-for-Complex-3D-Scene-Understanding"><a href="#Lexicon3D-Probing-Visual-Foundation-Models-for-Complex-3D-Scene-Understanding" class="headerlink" title="Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene   Understanding"></a>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene   Understanding</h2><p><strong>Authors:Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liang-Yan Gui, Yu-Xiong Wang</strong></p>
<p>Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks. Code: <a target="_blank" rel="noopener" href="https://github.com/YunzeMan/Lexicon3D">https://github.com/YunzeMan/Lexicon3D</a> </p>
<blockquote>
<p>å¤æ‚çš„ä¸‰ç»´åœºæ™¯ç†è§£å·²ç»è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œåœºæ™¯ç¼–ç ç­–ç•¥åœ¨å…¶ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå¯¹äºå„ç§åœºæ™¯çš„æœ€ä½³åœºæ™¯ç¼–ç ç­–ç•¥ä»ä¸æ¸…æ¥šï¼Œå°¤å…¶æ˜¯ä¸åŸºäºå›¾åƒçš„å¯¹åº”ç‰©ç›¸æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„ç ”ç©¶ï¼Œæ¢è®¨äº†ç”¨äºä¸‰ç»´åœºæ™¯ç†è§£çš„å„ç§è§†è§‰ç¼–ç æ¨¡å‹ï¼Œè¯†åˆ«äº†ä¸åŒåœºæ™¯ç¼–ç æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†ä¸ƒç§è§†è§‰åŸºç¡€ç¼–ç å™¨ï¼ŒåŒ…æ‹¬åŸºäºå›¾åƒã€åŸºäºè§†é¢‘å’Œä¸‰ç»´åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä»»åŠ¡ä¸­è¯„ä¼°äº†è¿™äº›æ¨¡å‹ï¼šè§†è§‰è¯­è¨€åœºæ™¯æ¨ç†ã€è§†è§‰å®šä½ã€åˆ†å‰²å’Œæ³¨å†Œï¼Œæ¯ä¸ªä»»åŠ¡éƒ½ä¾§é‡äºåœºæ™¯ç†è§£çš„ä¸åŒæ–¹é¢ã€‚æˆ‘ä»¬çš„è¯„ä¼°å¾—å‡ºäº†é‡è¦å‘ç°ï¼šDINOv2è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè§†é¢‘æ¨¡å‹åœ¨å¯¹è±¡çº§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ‰©æ•£æ¨¡å‹æœ‰åˆ©äºå‡ ä½•ä»»åŠ¡ï¼Œè€Œè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ„å¤–çš„å±€é™æ€§ã€‚è¿™äº›è§è§£æŒ‘æˆ˜äº†ä¸€äº›ä¼ ç»Ÿç†è§£ï¼Œä¸ºåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥åœ¨è§†è§‰è¯­è¨€å’Œåœºæ™¯ç†è§£ä»»åŠ¡ä¸­éœ€è¦æ›´çµæ´»çš„ç¼–ç å™¨é€‰æ‹©ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/YunzeMan/Lexicon3D">https://github.com/YunzeMan/Lexicon3D</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03757v3">PDF</a> NeurIPS 2024. Project page: <a target="_blank" rel="noopener" href="https://yunzeman.github.io/lexicon3d">https://yunzeman.github.io/lexicon3d</a>   Github: <a target="_blank" rel="noopener" href="https://github.com/YunzeMan/Lexicon3D">https://github.com/YunzeMan/Lexicon3D</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨ä¸‰ç»´åœºæ™¯ç†è§£ä¸­çš„è§†è§‰ç¼–ç ç­–ç•¥ã€‚é’ˆå¯¹æ­¤ï¼Œä½œè€…å¯¹ä¸ƒç§è§†è§‰ç¼–ç æ¨¡å‹è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œå¹¶åœ¨å››ä¸ªä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼šè§†è§‰è¯­è¨€åœºæ™¯æ¨ç†ã€è§†è§‰å®šä½ã€åˆ†å‰²å’Œæ³¨å†Œã€‚ç ”ç©¶å‘ç°DINOv2è¡¨ç°ä¼˜å¼‚ï¼Œè§†é¢‘æ¨¡å‹åœ¨å¯¹è±¡çº§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å‡ ä½•ä»»åŠ¡ä¸Šæœ‰æ‰€åŠ©ç›Šï¼Œè€Œè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ„å¤–å±€é™ã€‚è¿™äº›è§è§£ä¸ºåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹å’Œæœªæ¥è§†è§‰è¯­è¨€åŠåœºæ™¯ç†è§£ä»»åŠ¡çš„ç¼–ç å™¨é€‰æ‹©æä¾›äº†æ–°çš„è§†è§’å’ŒæŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è§†è§‰ç¼–ç æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„ä¸‰ç»´åœºæ™¯ç†è§£ç ”ç©¶ã€‚</li>
<li>åœ¨è§†è§‰è¯­è¨€åœºæ™¯æ¨ç†ã€è§†è§‰å®šä½ã€åˆ†å‰²å’Œæ³¨å†Œç­‰å››ä¸ªä»»åŠ¡ä¸­è¯„ä¼°äº†å¤šç§è§†è§‰ç¼–ç æ¨¡å‹ã€‚</li>
<li>å‘ç°DINOv2åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è§†é¢‘æ¨¡å‹åœ¨å¯¹è±¡çº§ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯¹å‡ ä½•ä»»åŠ¡æœ‰ç§¯æå½±å“ã€‚</li>
<li>è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸­å­˜åœ¨å±€é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a90c6c73174a12cbf6943196b8e03290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf49df498324fddfddc0e6328f829ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2023745bb51a4cdc1981571c89dfbde8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5220023cdd8c5c9896a66bfbaca03b96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cf6641a325af2612ed5d2d70dab6226.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e442931c371dfa78f83c79cc10e70a4e.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  Significant reflection and absorption effects in the X-ray emission of   the Intermediate Polar IGR J17195-4100
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-860fdec74d557d4e5fdbe5f7828924c4.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  3D Scene Generation A Survey
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18884.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
