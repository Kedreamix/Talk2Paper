<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-05-10  Benchmarking Ophthalmology Foundation Models for Clinically Significant   Age Macular Degeneration Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-bd010ac558ed289bc849f7bf15c26b11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-10-更新"><a href="#2025-05-10-更新" class="headerlink" title="2025-05-10 更新"></a>2025-05-10 更新</h1><h2 id="Benchmarking-Ophthalmology-Foundation-Models-for-Clinically-Significant-Age-Macular-Degeneration-Detection"><a href="#Benchmarking-Ophthalmology-Foundation-Models-for-Clinically-Significant-Age-Macular-Degeneration-Detection" class="headerlink" title="Benchmarking Ophthalmology Foundation Models for Clinically Significant   Age Macular Degeneration Detection"></a>Benchmarking Ophthalmology Foundation Models for Clinically Significant   Age Macular Degeneration Detection</h2><p><strong>Authors:Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar</strong></p>
<p>Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n&#x3D;587) of DFIs with AMD labels from Brazil. </p>
<blockquote>
<p>自监督学习（SSL）使得视觉Transformer（ViT）能够从大规模的自然图像数据集中学习鲁棒性表示，增强了其在不同领域的泛化能力。在视网膜成像中，预训练在自然或眼科数据上的基础模型已经显示出潜力，但领域内预训练的好处仍不确定。为了研究这一点，我们在包含总计7万张专家标注图像的7个数字眼底图像（DFI）数据集上，对六个SSL预训练的ViT进行了基准测试，用于中度至晚期年龄相关性黄斑病变（AMD）的识别任务。我们的结果表明，在自然图像上预训练的iBOT具有最高的跨分布泛化能力，AUC值在0.80至0.97之间，优于特定领域的模型（AUC值在0.78至0.96之间）和未经预训练的基线ViT-L（AUC值在0.68至0.91之间）。这些发现凸显了基础模型在改进AMD识别方面的价值，并挑战了认为领域内预训练是必要的假设。此外，我们发布了BRAMD，这是一个开放访问的眼底图像数据集，包含来自巴西的AMD标签图像共计587张。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05291v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>在视网膜成像领域，自监督学习（SSL）赋能Vision Transformers（ViTs）从大规模自然图像数据集中学习稳健表示，提升其在不同领域的泛化能力。本研究对比了六种SSL预训练ViTs在眼底图像（DFI）数据集中识别年龄相关性黄斑病变（AMD）的表现。结果显示，以自然图像预训练的iBOT模型表现出最佳泛化能力，领域相关模型的性能次之，无预训练的基础模型性能最低。这表明在AMD识别方面，基础模型具有改善潜力，挑战了领域特定预训练的必要假设。同时，本研究公开了巴西AMD标记眼底图像数据集BRAMD。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督学习使Vision Transformers能从大规模自然图像数据集中学习稳健表示，提升其在视网膜成像中的泛化能力。</li>
<li>在眼底图像数据集中识别年龄相关性黄斑病变的研究中，iBOT预训练模型表现出最佳泛化能力。</li>
<li>与领域特定预训练的模型相比，iBOT预训练模型性能更优，这挑战了领域特定预训练的必要假设。</li>
<li>公开了巴西的眼底图像数据集BRAMD，为未来的研究提供数据支持。</li>
<li>研究表明，基础模型在改善AMD识别方面具有潜力。</li>
<li>研究结果强调了预训练在提升模型性能中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05291">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b245454123b152d60525ad92933e9f33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68741de23d21e9d4dad004d22b15d06b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8192d69baa4700648077845d9e99c26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0807d42c6ed63bc81380acd094ea026.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-facd00a4ec60fc0008689a87d4cff094.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f09d21aa63fcb8123fdf58a54e7ad52.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models"><a href="#Biomed-DPT-Dual-Modality-Prompt-Tuning-for-Biomedical-Vision-Language-Models" class="headerlink" title="Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language   Models"></a>Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language   Models</h2><p><strong>Authors:Wei Peng, Kang Liu, Jianchen Hu, Meng Zhang</strong></p>
<p>Prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (VLMs) to the biomedical image classification tasks in few shot scenarios. However, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. In this work, we propose Biomed-DPT, a knowledge-enhanced dual modality prompt tuning technique. In designing the text prompt, Biomed-DPT constructs a dual prompt including the template-driven clinical prompts and the large language model (LLM)-driven domain-adapted prompts, then extracts the clinical knowledge from the domain-adapted prompts through the knowledge distillation technique. In designing the vision prompt, Biomed-DPT introduces the zero vector as a soft prompt to leverage attention re-weighting so that the focus on non-diagnostic regions and the recognition of non-critical pathological features are avoided. Biomed-DPT achieves an average classification accuracy of 66.14% across 11 biomedical image datasets covering 9 modalities and 10 organs, with performance reaching 78.06% in base classes and 75.97% in novel classes, surpassing the Context Optimization (CoOp) method by 6.20%, 3.78%, and 8.04%, respectively. Our code are available at \underline{<a target="_blank" rel="noopener" href="https://github.com/Kanyooo/Biomed-DPT%7D">https://github.com/Kanyooo/Biomed-DPT}</a>. </p>
<blockquote>
<p>提示学习是在小样本场景中，将预训练的视觉语言模型（VLMs）适应生物医学图像分类任务的最有效范式之一。然而，当前大多数提示学习方法仅使用文本提示，而忽略了生物医学图像中的特定结构（如复杂的解剖结构和微妙的病理特征）。在我们的工作中，我们提出了生物医学领域知识增强的双模态提示调整技术——Biomed-DPT。在设计文本提示时，Biomed-DPT构建了一个双提示，包括模板驱动的临床提示和大语言模型（LLM）驱动的域适配提示，然后通过知识蒸馏技术从域适配提示中提取临床知识。在设计视觉提示时，Biomed-DPT引入了零向量作为软提示，以利用注意力重新加权，从而避免关注非诊断区域和识别非关键病理特征。Biomed-DPT在涵盖9种模态和10个器官的11个生物医学图像数据集上取得了平均分类准确率66.14%的成绩，其中基础类的性能达到78.06%，新型类的性能达到75.97%，分别超过了CoOp方法6.20%、3.78%和8.04%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Kanyooo/Biomed-DPT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Kanyooo/Biomed-DPT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05189v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种知识增强的双模态提示调整技术（Biomed-DPT），该技术针对生物医学图像分类任务进行预训练视语言模型（VLMs）的快速适应。通过设计文本提示和视觉提示，Biomed-DPT构建了一种双提示系统，包括模板驱动的临床提示和大型语言模型（LLM）驱动的领域自适应提示，并通过知识蒸馏技术提取领域自适应提示中的临床知识。同时，引入零向量作为软提示，避免对非诊断区域的关注和对非关键病理特征的识别。在跨越多个数据集的实验中，Biomed-DPT的分类准确度达到平均66.14%，并且在基准类和新型类中分别达到了78.06%和75.97%，超过了CoOp方法的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前大多数提示学习方法仅依赖文本提示，忽略了生物医学图像中的特定结构。</li>
<li>Biomed-DPT是一种知识增强的双模态提示调整技术，旨在解决生物医学图像分类任务中的快速适应问题。</li>
<li>该技术通过构建双提示系统（包括临床提示和领域自适应提示）来优化文本提示设计。</li>
<li>通过知识蒸馏技术从领域自适应提示中提取临床知识。</li>
<li>引入零向量作为视觉软提示，避免对非诊断区域的关注和对非关键病理特征的识别。</li>
<li>实验结果显示，Biomed-DPT的分类准确度在多个生物医学图像数据集上平均达到66.14%，并且在基准类和新型类中都表现出优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05189">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-85409a86524f5f5f989f6009ea4b9f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc03b81920a476b80dab0b27c27c8e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c13e0c6a1dd4c8db9deaed54a6ccb7c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a14e463700270160e6c604d25130173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ddaec8a2136730f9e1f705f0568b385.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd010ac558ed289bc849f7bf15c26b11.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hyb-KAN-ViT-Hybrid-Kolmogorov-Arnold-Networks-Augmented-Vision-Transformer"><a href="#Hyb-KAN-ViT-Hybrid-Kolmogorov-Arnold-Networks-Augmented-Vision-Transformer" class="headerlink" title="Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision   Transformer"></a>Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision   Transformer</h2><p><strong>Authors:Sainath Dey, Mitul Goswami, Jashika Sethi, Prasant Kumar Pattnaik</strong></p>
<p>This study addresses the inherent limitations of Multi-Layer Perceptrons (MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold Network (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates wavelet-based spectral decomposition and spline-optimized activation functions, prior work has failed to focus on the prebuilt modularity of the ViT architecture and integration of edge detection capabilities of Wavelet functions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces MLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging orthogonal wavelet transforms for multi-resolution feature extraction. These modules are systematically integrated in ViT encoder layers and classification heads to enhance spatial-frequency modeling while mitigating computational bottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object Detection and Instance Segmentation), and ADE20K (Semantic Segmentation) demonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies validate the efficacy of wavelet-driven spectral priors in segmentation and spline-based efficiency in detection tasks. The framework establishes a new paradigm for balancing parameter efficiency and multi-scale representation in vision architectures. </p>
<blockquote>
<p>本研究通过引入混合Kolmogorov-Arnold网络（KAN）-ViT（Hyb-KAN ViT）这一新型框架，解决了多层感知器（MLP）在视觉转换器（ViT）中的固有局限性。该框架结合了基于小波谱分解和样条优化激活函数。先前的工作未能关注ViT架构的预构建模块性和小波功能的边缘检测能力的集成。我们提出了两个关键模块：Efficient-KAN（Eff-KAN），用样条函数替换MLP层；Wavelet-KAN（Wav-KAN），利用正交小波变换进行多分辨率特征提取。这些模块系统地集成在ViT编码器层和分类头中，以增强空间频率建模，同时缓解计算瓶颈。在ImageNet-1K（图像识别）、COCO（目标检测和实例分割）和ADE20K（语义分割）上的实验表明，Hyb-KAN ViT具有最先进的性能。消融研究验证了基于小波驱动的谱先验在分割中的有效性以及基于样条的检测任务效率。该框架为在视觉架构中平衡参数效率和多尺度表示建立了新范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04740v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Hybrid Kolmogorov-Arnold Network（KAN）-ViT（Hyb-KAN ViT）的新型框架，解决了多层感知器（MLP）在视觉转换器（ViT）中的固有局限性。该框架结合了基于小波谱分解和样条优化激活函数。它提出了两个关键模块：Efficient-KAN（Eff-KAN）和Wavelet-KAN（Wav-KAN）。前者用样条函数替代MLP层，后者利用正交小波变换进行多分辨率特征提取。这些模块被系统地集成到ViT编码层和分类头中，以增强空间频率建模并缓解计算瓶颈。在ImageNet-1K（图像识别）、COCO（对象检测和实例分割）和ADE20K（语义分割）上的实验表明，Hyb-KAN ViT具有最先进的性能。消融研究验证了以波为驱动谱先验在分割中的有效性以及基于样条的效率在检测任务中的应用。该框架为平衡参数效率和多尺度表示在视觉架构中建立了新的范例。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了Hybrid Kolmogorov-Arnold Network (KAN)-ViT框架，旨在解决多层感知器（MLP）在视觉转换器（ViT）中的局限性。</li>
<li>提出了Efficient-KAN和Wavelet-KAN两个关键模块，前者用样条函数替代MLP层以提高效率，后者利用小波变换进行多分辨率特征提取。</li>
<li>将这些模块系统地集成到ViT编码层和分类头中，以增强空间频率建模，并缓解计算瓶颈。</li>
<li>在多个数据集上的实验表明Hyb-KAN ViT具有最先进的性能，包括ImageNet-1K、COCO和ADE20K。</li>
<li>消融研究验证了以波为驱动谱先验在分割任务中的有效性以及基于样条的激活函数在检测任务中的效率。</li>
<li>该框架为平衡参数效率和多尺度表示在视觉架构中提供了新范例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04740">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f03a828ae6c37c2d871cdeb96618097.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8afb7eff808fab8e83b2569b255b83b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bdae75667d9d276316b36494bc1b6b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0657555f2f225fae0f83f5d14deb8b00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b19acd3e099d022bfd80f1f94668b1b2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Rethinking-Boundary-Detection-in-Deep-Learning-Based-Medical-Image-Segmentation"><a href="#Rethinking-Boundary-Detection-in-Deep-Learning-Based-Medical-Image-Segmentation" class="headerlink" title="Rethinking Boundary Detection in Deep Learning-Based Medical Image   Segmentation"></a>Rethinking Boundary Detection in Deep Learning-Based Medical Image   Segmentation</h2><p><strong>Authors:Yi Lin, Dong Zhang, Xiao Fang, Yufan Chen, Kwang-Ting Cheng, Hao Chen</strong></p>
<p>Medical image segmentation is a pivotal task within the realms of medical image analysis and computer vision. While current methods have shown promise in accurately segmenting major regions of interest, the precise segmentation of boundary areas remains challenging. In this study, we propose a novel network architecture named CTO, which combines Convolutional Neural Networks (CNNs), Vision Transformer (ViT) models, and explicit edge detection operators to tackle this challenge. CTO surpasses existing methods in terms of segmentation accuracy and strikes a better balance between accuracy and efficiency, without the need for additional data inputs or label injections. Specifically, CTO adheres to the canonical encoder-decoder network paradigm, with a dual-stream encoder network comprising a mainstream CNN stream for capturing local features and an auxiliary StitchViT stream for integrating long-range dependencies. Furthermore, to enhance the model’s ability to learn boundary areas, we introduce a boundary-guided decoder network that employs binary boundary masks generated by dedicated edge detection operators to provide explicit guidance during the decoding process. We validate the performance of CTO through extensive experiments conducted on seven challenging medical image segmentation datasets, namely ISIC 2016, PH2, ISIC 2018, CoNIC, LiTS17, and BTCV. Our experimental results unequivocally demonstrate that CTO achieves state-of-the-art accuracy on these datasets while maintaining competitive model complexity. The codes have been released at: <a target="_blank" rel="noopener" href="https://github.com/xiaofang007/CTO">https://github.com/xiaofang007/CTO</a>. </p>
<blockquote>
<p>医学图像分割是医学图像分析和计算机视觉领域中的一项关键任务。虽然当前方法在准确分割主要感兴趣区域方面显示出潜力，但边界区域的精确分割仍然具有挑战性。本研究提出了一种名为CTO的新型网络架构，它结合了卷积神经网络（CNN）、视觉转换器（ViT）模型和明确的边缘检测算子，以应对这一挑战。CTO在分割精度上超越了现有方法，并在精度和效率之间达到了更好的平衡，无需额外的数据输入或标签注入。具体来说，CTO遵循典型的编码器-解码器网络范式，具有双流编码器网络，包括用于捕获局部特征的主流CNN流和用于集成远程依赖的辅助StitchViT流。此外，为了提高模型学习边界区域的能力，我们引入了一个边界引导解码器网络，该网络使用由专用边缘检测算子生成的二进制边界掩膜在解码过程中提供明确指导。我们在七个具有挑战性的医学图像分割数据集上进行了广泛实验，验证了CTO的性能，这些数据集分别是ISIC 2016、PH2、ISIC 2018、CoNIC、LiTS17和BTCV。我们的实验结果明确证明，CTO在这些数据集上实现了最先进的准确性，同时保持了竞争性的模型复杂度。代码已发布在：[<a target="_blank" rel="noopener" href="https://github.com/xiaofang007/CTO%E3%80%82]">https://github.com/xiaofang007/CTO。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04652v1">PDF</a> Accepted by Medical Image Analysis</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型网络架构CTO，融合了卷积神经网络（CNN）、视觉转换器（ViT）模型和边缘检测算子，以应对医疗图像分割中边界区域精确分割的挑战。CTO在分割精度上超越了现有方法，并在精度和效率之间达到了更好的平衡，无需额外的数据输入或标签注入。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗图像分割是医学图像分析和计算机视觉中的关键任务。</li>
<li>现有方法在精确分割边界区域时面临挑战。</li>
<li>本研究提出了一种新型网络架构CTO，结合了CNN、ViT模型和边缘检测算子来应对这一挑战。</li>
<li>CTO超越了现有方法的分割精度，并在精度和效率之间取得了平衡。</li>
<li>CTO采用典型的编码器-解码器网络范式，具有双流编码器网络，包括主流CNN流和辅助StitchViT流。</li>
<li>为提高模型对边界区域的学习能力，引入了边界引导解码器网络，采用专用边缘检测算子生成的二进制边界掩膜为解码过程提供明确指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ad1843bcb2bfc5cebfd1b7d52c72f0fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba3464f4eec9332272a3e07a9cd6d120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77cc6a2bf4b8e84be2d393e6442e2104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca45fb0cad8a82dc687e997b025d76e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-327b43dbc5d647fccf99d964853e6b81.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MambaNUT-Nighttime-UAV-Tracking-via-Mamba-based-Adaptive-Curriculum-Learning"><a href="#MambaNUT-Nighttime-UAV-Tracking-via-Mamba-based-Adaptive-Curriculum-Learning" class="headerlink" title="MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum   Learning"></a>MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum   Learning</h2><p><strong>Authors:You Wu, Xiangyang Yang, Xucheng Wang, Hengzhou Ye, Dan Zeng, Shuiwang Li</strong></p>
<p>Harnessing low-light enhancement and domain adaptation, nighttime UAV tracking has made substantial strides. However, over-reliance on image enhancement, limited high-quality nighttime data, and a lack of integration between daytime and nighttime trackers hinder the development of an end-to-end trainable framework. Additionally, current ViT-based trackers demand heavy computational resources due to their reliance on the self-attention mechanism. In this paper, we propose a novel pure Mamba-based tracking framework (MambaNUT) that employs a state space model with linear complexity as its backbone, incorporating a single-stream architecture that integrates feature learning and template-search coupling within Vision Mamba. We introduce an adaptive curriculum learning (ACL) approach that dynamically adjusts sampling strategies and loss weights, thereby improving the model’s ability of generalization. Our ACL is composed of two levels of curriculum schedulers: (1) sampling scheduler that transforms the data distribution from imbalanced to balanced, as well as from easier (daytime) to harder (nighttime) samples; (2) loss scheduler that dynamically assigns weights based on the size of the training data and IoU of individual instances. Exhaustive experiments on multiple nighttime UAV tracking benchmarks demonstrate that the proposed MambaNUT achieves state-of-the-art performance while requiring lower computational costs. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/wuyou3474/MambaNUT">https://github.com/wuyou3474/MambaNUT</a>. </p>
<blockquote>
<p>利用低光增强和领域自适应技术，夜间无人机跟踪已经取得了重大进展。然而，对图像增强的过度依赖、高质量夜间数据的有限性以及日间和夜间跟踪器之间缺乏整合，阻碍了端到端可训练框架的发展。此外，当前的基于ViT的跟踪器由于依赖于自注意力机制，需要大量的计算资源。在本文中，我们提出了一种基于纯Mamba的跟踪框架（MambaNUT），它采用具有线性复杂度的状态空间模型作为骨干，并采用了单流架构，该架构在Vision Mamba内集成了特征学习和模板搜索耦合。我们引入了一种自适应课程学习（ACL）方法，该方法可以动态调整采样策略和损失权重，从而提高模型的泛化能力。我们的ACL由两级课程调度器组成：（1）采样调度器，它改变数据分布，从不平衡到平衡，从简单的（日间）样本到复杂的（夜间）样本；（2）损失调度器，它根据训练数据的大小和各个实例的IoU动态分配权重。在多个夜间无人机跟踪基准测试上的大量实验表明，所提出的MambaNUT达到了最先进的性能，同时计算成本较低。代码将在<a target="_blank" rel="noopener" href="https://github.com/wuyou3474/MambaNUT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/wuyou3474/MambaNUT上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00626v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于Mamba的纯跟踪框架（MambaNUT），采用线性复杂度的状态空间模型作为骨干。该框架结合了特征学习与模板搜索耦合的单流架构，并引入了自适应课程学习方法（ACL）以动态调整采样策略和损失权重，从而提高模型的泛化能力。实验证明，MambaNUT在夜间无人机跟踪方面达到了最新技术水平，并降低了计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>夜间无人机跟踪领域取得了显著进展，但仍面临图像增强过度依赖、高质量夜间数据有限以及日夜跟踪器融合不足的问题。</li>
<li>提出了基于Mamba的纯跟踪框架（MambaNUT），结合特征学习与模板搜索耦合的单流架构。</li>
<li>MambaNUT采用状态空间模型作为骨干，具有线性复杂度，降低了计算资源的消耗。</li>
<li>引入了自适应课程学习方法（ACL），动态调整采样策略和损失权重，提高了模型的泛化能力。</li>
<li>ACL包含两个层次的课程调度器：采样调度器和损失调度器，分别对数据的分布和损失权重进行动态调整。</li>
<li>实验证明，MambaNUT在多个夜间无人机跟踪基准测试中达到了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd13ec40030f8b56e87df14f76794694.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1e3aa283c1e6091e0b81c4b3b07db80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-037b80f29793a9347d71e3c3e0c90d61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b37629b0d5437c966aab83fafb97a7e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c307eca7ff56a567e04c62f5670b76.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Evaluating-Deep-Learning-Models-for-Breast-Cancer-Classification-A-Comparative-Study"><a href="#Evaluating-Deep-Learning-Models-for-Breast-Cancer-Classification-A-Comparative-Study" class="headerlink" title="Evaluating Deep Learning Models for Breast Cancer Classification: A   Comparative Study"></a>Evaluating Deep Learning Models for Breast Cancer Classification: A   Comparative Study</h2><p><strong>Authors:Sania Eskandari, Ali Eslamian, Nusrat Munia, Amjad Alqarni, Qiang Cheng</strong></p>
<p>This study evaluates the effectiveness of deep learning models in classifying histopathological images for early and accurate detection of breast cancer. Eight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision Transformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and SqueezeNet, were compared using a dataset of 277,524 image patches. The Vision Transformer (ViT) model, with its attention-based mechanisms, achieved the highest validation accuracy of 94%, outperforming conventional CNNs. The study demonstrates the potential of advanced machine learning methods to enhance precision and efficiency in breast cancer diagnosis in clinical settings. </p>
<blockquote>
<p>本研究评估了深度学习模型在分类病理图像以早期和准确检测乳腺癌方面的有效性。使用包含277524个图像块的数据集，对比了ResNet-50、DenseNet-121、ResNeXt-50、Vision Transformer（ViT）、GoogLeNet（Inception v3）、EfficientNet、MobileNet和SqueezeNet等八种先进模型。其中，采用注意力机制的Vision Transformer（ViT）模型取得了最高的验证准确率，达到了94%，并优于传统的卷积神经网络（CNNs）。该研究证明了先进的机器学习方法在提高乳腺癌诊断的准确性和效率方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16859v2">PDF</a> 4 pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong><br>     本研究评估了深度学习模型在分类病理图像以进行乳腺癌早期准确检测方面的有效性。研究比较了ResNet-50、DenseNet-121等八种先进模型，使用包含277,524个图像补丁的数据集进行测试。其中，Vision Transformer（ViT）模型凭借注意力机制实现了最高验证准确率94%，超过了传统CNN。此研究展示了先进机器学习方法在增强乳腺癌诊断精度和效率方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究使用深度学习模型进行病理图像分类，旨在早期准确检测乳腺癌。</li>
<li>评估了八种先进模型（ResNet-50等），通过对比验证模型效果。</li>
<li>Vision Transformer（ViT）模型因具备注意力机制，取得了最高的验证准确率94%。</li>
<li>Vision Transformer模型的性能优于传统的卷积神经网络（CNN）。</li>
<li>该研究为临床环境中的乳腺癌诊断提供了更为精确和高效的机器学习方法的潜力证据。</li>
<li>数据集包含大量的图像补丁（277,524个），有助于模型的训练和验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bfeba4bf8172ab70ba1a0b89323253ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b476f4e9aaf20f144c75a608572c3b03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef9a33f05d22e19d1d4002a7fb56f12e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28dea6e3f041edd6758be36e31d49330.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0d6169567c1dc4abeac209b7307d9785.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-05-10  MonoCoP Chain-of-Prediction for Monocular 3D Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9b9e4f3b6b85ab15d2695e8184141a49.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-10  Latte Transfering LLMs` Latent-level Knowledge for Few-shot Tabular   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
