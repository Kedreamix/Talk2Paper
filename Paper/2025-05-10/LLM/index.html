<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  Generating Physically Stable and Buildable LEGO Designs from Text">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-251d137fe019c1623a1a5a4d2d700ef2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    61 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-10-æ›´æ–°"><a href="#2025-05-10-æ›´æ–°" class="headerlink" title="2025-05-10 æ›´æ–°"></a>2025-05-10 æ›´æ–°</h1><h2 id="Generating-Physically-Stable-and-Buildable-LEGO-Designs-from-Text"><a href="#Generating-Physically-Stable-and-Buildable-LEGO-Designs-from-Text" class="headerlink" title="Generating Physically Stable and Buildable LEGO Designs from Text"></a>Generating Physically Stable and Buildable LEGO Designs from Text</h2><p><strong>Authors:Ava Pun, Kangle Deng, Ruixuan Liu, Deva Ramanan, Changliu Liu, Jun-Yan Zhu</strong></p>
<p>We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: <a target="_blank" rel="noopener" href="https://avalovelace1.github.io/LegoGPT/">https://avalovelace1.github.io/LegoGPT/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºLegoGPTï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆç‰©ç†ç¨³å®šçš„ä¹é«˜ç§¯æœ¨æ¨¡å‹çš„é¦–åˆ›æ–¹æ³•ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¹é«˜è®¾è®¡æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç‰©ç†ç¨³å®šçš„ä¹é«˜è®¾è®¡åŠå…¶ç›¸å…³æè¿°ï¼Œå¹¶è®­ç»ƒäº†ä¸€ç§è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¥é¢„æµ‹è¦æ·»åŠ çš„ä¸‹ä¸€ä¸ªç§¯æœ¨ã€‚ä¸ºæé«˜æ‰€å¾—è®¾è®¡çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬åœ¨è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨äº†é«˜æ•ˆçš„åˆç†æ€§æ£€æŸ¥å’Œç‰©ç†æ„ŸçŸ¥å›æ»šï¼Œåˆ©ç”¨ç‰©ç†å®šå¾‹å’Œè£…é…çº¦æŸæ¥å‰”é™¤ä¸å¯è¡Œçš„ä»¤ç‰Œé¢„æµ‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLegoGPTèƒ½å¤Ÿç”Ÿæˆç¨³å®šã€å¤šæ ·ä¸”ç¾è§‚çš„ä¹é«˜è®¾è®¡ï¼Œä¸è¾“å…¥çš„æ–‡æœ¬æç¤ºç´§å¯†å¯¹åº”ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§åŸºäºæ–‡æœ¬çš„ä¹é«˜çº¹ç†ç”Ÿæˆæ–¹æ³•ï¼Œä»¥ç”Ÿæˆå½©è‰²å’Œæœ‰çº¹ç†çš„è®¾è®¡ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„è®¾è®¡å¯ä»¥è¢«äººç±»æ‰‹åŠ¨ç»„è£…ï¼Œä¹Ÿå¯ä»¥è¢«æœºæ¢°è‡‚è‡ªåŠ¨ç»„è£…ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†æ–°çš„æ•°æ®é›†StableText2Legoï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡47000ä¸ªä¹é«˜ç»“æ„ï¼Œä»£è¡¨è¶…è¿‡28000ä¸ªç‹¬ç‰¹çš„3Då¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡éƒ½æœ‰è¯¦ç»†çš„æè¿°ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯ä»¥åœ¨é¡¹ç›®ç½‘ç«™ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://avalovelace1.github.io/LegoGPT/%E3%80%82">https://avalovelace1.github.io/LegoGPT/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05469v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://avalovelace1.github.io/LegoGPT/">https://avalovelace1.github.io/LegoGPT/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LegoGPTï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆç‰©ç†ç¨³å®šçš„ä¹é«˜ç§¯æœ¨æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„ç‰©ç†ç¨³å®šä¹é«˜è®¾è®¡æ•°æ®é›†åŠå…¶ç›¸å…³æè¿°ï¼Œè®­ç»ƒäº†è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¦æ·»åŠ çš„ç§¯æœ¨ã€‚ä¸ºæé«˜è®¾è®¡ç¨³å®šæ€§ï¼Œé‡‡ç”¨æœ‰æ•ˆçš„æœ‰æ•ˆæ€§æ£€æŸ¥å’Œç‰©ç†æ„ŸçŸ¥å›æ»šè¿›è¡Œè‡ªå›å½’æ¨ç†ï¼Œåˆ©ç”¨ç‰©ç†å®šå¾‹å’Œè£…é…çº¦æŸå‰”é™¤ä¸å¯è¡Œçš„ä»¤ç‰Œé¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒLegoGPTèƒ½ç”Ÿæˆç¨³å®šã€å¤šæ ·ã€ç¾è§‚çš„ä¹é«˜è®¾è®¡ï¼Œç´§å¯†ç¬¦åˆè¾“å…¥æ–‡æœ¬æç¤ºã€‚è¿˜å¼€å‘äº†ä¸€ç§åŸºäºæ–‡æœ¬çš„ä¹é«˜çº¹ç†æ–¹æ³•ï¼Œä»¥ç”Ÿæˆå¸¦é¢œè‰²çš„çº¹ç†è®¾è®¡ã€‚æœ€åé‡Šæ”¾äº†æ–°æ•°æ®é›†StableText2Legoï¼ŒåŒ…å«è¶…è¿‡4.7ä¸‡ä¸ªä¹é«˜ä¸‰ç»´ç‰©ä½“çš„ç‹¬ç‰¹ç»“æ„å’Œè¯¦ç»†æè¿°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LegoGPTæ˜¯é¦–ä¸ªé€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆç‰©ç†ç¨³å®šçš„ä¹é«˜ç§¯æœ¨æ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†æ¥è®­ç»ƒè‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°æ–‡æœ¬åˆ°ä¹é«˜è®¾è®¡çš„è½¬æ¢ã€‚</li>
<li>é‡‡ç”¨æœ‰æ•ˆæ€§æ£€æŸ¥å’Œç‰©ç†æ„ŸçŸ¥å›æ»šæ¥æé«˜è®¾è®¡ç¨³å®šæ€§ã€‚</li>
<li>LegoGPTèƒ½ç”Ÿæˆä¸æ–‡æœ¬æç¤ºç´§å¯†ç¬¦åˆçš„ç¨³å®šã€å¤šæ ·ã€ç¾è§‚çš„ä¹é«˜è®¾è®¡ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åŸºäºæ–‡æœ¬çš„ä¹é«˜çº¹ç†æ–¹æ³•ï¼Œä¸ºè®¾è®¡æ·»åŠ é¢œè‰²å’Œçº¹ç†ã€‚</li>
<li>é‡Šæ”¾äº†æ–°æ•°æ®é›†StableText2Legoï¼ŒåŒ…å«å¤§é‡ç‹¬ç‰¹çš„ä¹é«˜ç»“æ„å’Œè¯¦ç»†æè¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-251d137fe019c1623a1a5a4d2d700ef2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8d4db7c6df32df9a36e23b4788fd777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2170758fc643ffea7975417c965d5ea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78f9f6c5b7c85b15279fc77982d7010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d721143f79962f5f665fd5946c450a4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Bring-Reason-to-Vision-Understanding-Perception-and-Reasoning-through-Model-Merging"><a href="#Bring-Reason-to-Vision-Understanding-Perception-and-Reasoning-through-Model-Merging" class="headerlink" title="Bring Reason to Vision: Understanding Perception and Reasoning through   Model Merging"></a>Bring Reason to Vision: Understanding Perception and Reasoning through   Model Merging</h2><p><strong>Authors:Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, Junxian He</strong></p>
<p>Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†ç­‰ä¸€èˆ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›å¦‚ä½•ç»“åˆä»¥åŠå®ƒä»¬å¦‚ä½•å…±åŒå‘æŒ¥ä½œç”¨ï¼Œå…¶æœºåˆ¶å°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢é€šè¿‡æ¨¡å‹åˆå¹¶æ¥ç»„åˆæ„ŸçŸ¥å’Œæ¨ç†ï¼Œè¯¥åˆå¹¶è¿æ¥äº†ä¸åŒæ¨¡å‹çš„å‚æ•°ã€‚ä¸ä»¥å¾€ç»å¸¸å…³æ³¨åŒç±»æ¨¡å‹åˆå¹¶çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€æ¨¡å‹åˆå¹¶çš„æ–¹æ³•ï¼Œä½¿LLMsçš„æ¨ç†èƒ½åŠ›èƒ½å¤Ÿèå…¥VLMsä¸­ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§æˆåŠŸçš„æ–¹æ³•ï¼Œèƒ½ä»¥æ— è®­ç»ƒçš„æ–¹å¼å°†LLMsçš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°VLMsä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨åˆå¹¶çš„æ¨¡å‹æ¥äº†è§£æ„ŸçŸ¥å’Œæ¨ç†çš„å†…åœ¨æœºåˆ¶ä»¥åŠåˆå¹¶å¯¹å…¶çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°æ„ŸçŸ¥èƒ½åŠ›ä¸»è¦ç¼–ç åœ¨æ¨¡å‹çš„å‰å‡ å±‚ï¼Œè€Œæ¨ç†åˆ™å¤§å¤šç”±ä¸­é—´åˆ°åé¢çš„å±‚æ¬¡æ‰€ä¿ƒè¿›ã€‚åˆå¹¶åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰€æœ‰å±‚æ¬¡éƒ½å¼€å§‹å¯¹æ¨ç†åšå‡ºè´¡çŒ®ï¼Œè€Œæ„ŸçŸ¥èƒ½åŠ›çš„å±‚æ¬¡åˆ†å¸ƒåŸºæœ¬ä¿æŒä¸å˜ã€‚è¿™äº›è§‚å¯Ÿç»“æœæ­ç¤ºäº†æ¨¡å‹åˆå¹¶ä½œä¸ºå¤šæ¨¡æ€é›†æˆå’Œè§£é‡Šå·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05464v1">PDF</a> ICML 2025. Our code is publicly available at   <a target="_blank" rel="noopener" href="https://github.com/shiqichen17/VLM_Merging">https://github.com/shiqichen17/VLM_Merging</a></p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†ç­‰ä¸€èˆ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›å¦‚ä½•ç»“åˆä»¥åŠè´¡çŒ®æœºåˆ¶å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶é€šè¿‡æ¨¡å‹åˆå¹¶æ¢ç´¢äº†æ„ŸçŸ¥å’Œæ¨ç†çš„ç»“åˆæ–¹å¼ï¼Œè¯¥æ–¹å¼è¿æ¥äº†ä¸åŒæ¨¡å‹çš„å‚æ•°ã€‚ä¸ä¹‹å‰ç»å¸¸ä¸“æ³¨äºåˆå¹¶åŒç±»æ¨¡å‹çš„æ–¹æ¡ˆä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€æ¨¡å‹åˆå¹¶æ–¹æ³•ï¼Œä½¿å¾—å°†LLMçš„æ¨ç†èƒ½åŠ›èå…¥VLMsæˆä¸ºå¯èƒ½ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§æˆåŠŸçš„æ–¹å¼ï¼Œä»¥æ— è®­ç»ƒçš„æ–¹å¼å°†LLMçš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°VLMsä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨åˆå¹¶åçš„æ¨¡å‹æ¥äº†è§£æ„ŸçŸ¥å’Œæ¨ç†çš„å†…åœ¨æœºåˆ¶ä»¥åŠåˆå¹¶å¯¹å…¶çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°æ„ŸçŸ¥èƒ½åŠ›ä¸»è¦ç¼–ç åœ¨æ¨¡å‹çš„æ—©æœŸå±‚ä¸­ï¼Œè€Œæ¨ç†ä¸»è¦ç”±ä¸­åæœŸå±‚ä¿ƒè¿›ã€‚åˆå¹¶åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰€æœ‰å±‚éƒ½å¼€å§‹å¯¹æ¨ç†åšå‡ºè´¡çŒ®ï¼Œè€Œæ„ŸçŸ¥èƒ½åŠ›çš„å±‚åˆ†å¸ƒå˜åŒ–ä¸å¤§ã€‚è¿™äº›è§‚å¯Ÿæ­ç¤ºäº†æ¨¡å‹åˆå¹¶ä½œä¸ºå¤šæ¨¡æ€é›†æˆå’Œè§£é‡Šçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åˆå¹¶æ˜¯è¿æ¥ä¸åŒæ¨¡å‹å‚æ•°ä»¥å®ç°æ„ŸçŸ¥å’Œæ¨ç†ç»“åˆçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ä¸åŒç±»æ¨¡å‹åˆå¹¶ä¸åŒï¼Œæœ¬ç ”ç©¶æå‡ºè·¨æ¨¡æ€æ¨¡å‹åˆå¹¶æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åˆå¹¶æˆåŠŸå°†LLMçš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°VLMsä¸Šï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>æ„ŸçŸ¥èƒ½åŠ›ä¸»è¦ä½äºæ¨¡å‹çš„æ—©æœŸå±‚ï¼Œè€Œæ¨ç†èƒ½åŠ›ä¸»è¦åœ¨ä¸­åæœŸå±‚ã€‚</li>
<li>åˆå¹¶åï¼Œæ‰€æœ‰å±‚éƒ½å¯¹æ¨ç†åšå‡ºè´¡çŒ®ï¼Œè€Œæ„ŸçŸ¥èƒ½åŠ›çš„å±‚åˆ†å¸ƒä¿æŒç¨³å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4de5a3473a7206f292f526f756e08579.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2e1e586449520be8152e52ef2c2a4d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae00fe2dc9a6b56c4f88cad8e7539d53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8af089e4e123f464297932d72ef1ec56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc78149bb84fa27b2418cc9696cf5f79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c911444722fcfaad04548918231c8423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f8976e505d4acc8629ad15852390629.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UKElectionNarratives-A-Dataset-of-Misleading-Narratives-Surrounding-Recent-UK-General-Elections"><a href="#UKElectionNarratives-A-Dataset-of-Misleading-Narratives-Surrounding-Recent-UK-General-Elections" class="headerlink" title="UKElectionNarratives: A Dataset of Misleading Narratives Surrounding   Recent UK General Elections"></a>UKElectionNarratives: A Dataset of Misleading Narratives Surrounding   Recent UK General Elections</h2><p><strong>Authors:Fatima Haouari, Carolina Scarton, NicolÃ² Faggiani, Nikolaos Nikolaidis, Bonka Kotseva, Ibrahim Abu Farha, Jens Linge, Kalina Bontcheva</strong></p>
<p>Misleading narratives play a crucial role in shaping public opinion during elections, as they can influence how voters perceive candidates and political parties. This entails the need to detect these narratives accurately. To address this, we introduce the first taxonomy of common misleading narratives that circulated during recent elections in Europe. Based on this taxonomy, we construct and analyse UKElectionNarratives: the first dataset of human-annotated misleading narratives which circulated during the UK General Elections in 2019 and 2024. We also benchmark Pre-trained and Large Language Models (focusing on GPT-4o), studying their effectiveness in detecting election-related misleading narratives. Finally, we discuss potential use cases and make recommendations for future research directions using the proposed codebook and dataset. </p>
<blockquote>
<p>åœ¨é€‰ä¸¾ä¸­ï¼Œè¯¯å¯¼æ€§å™äº‹å¯¹å¡‘é€ å…¬ä¼—æ„è§èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå› ä¸ºå®ƒä»¬ä¼šå½±å“é€‰æ°‘å¯¹å€™é€‰äººå’Œæ”¿å…šçš„çš„çœ‹æ³•ã€‚è¿™éœ€è¦æˆ‘ä»¬å‡†ç¡®æ£€æµ‹è¿™äº›å™äº‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†æœ€è¿‘åœ¨æ¬§æ´²é€‰ä¸¾ä¸­æµä¼ çš„å¸¸è§è¯¯å¯¼æ€§å™äº‹çš„ç¬¬ä¸€ä¸ªåˆ†ç±»ã€‚åŸºäºè¿™ä¸€åˆ†ç±»ï¼Œæˆ‘ä»¬æ„å»ºå¹¶åˆ†æäº†UKElectionNarrativesï¼šè¿™æ˜¯é¦–ä¸ªç»äººå·¥æ ‡æ³¨çš„è¯¯å¯¼æ€§å™äº‹æ•°æ®é›†ï¼ŒåŒ…å«äº†2019å¹´å’Œ2024å¹´è‹±å›½å¤§é€‰ä¸­æµä¼ çš„è¯¯å¯¼æ€§å™äº‹ã€‚æˆ‘ä»¬è¿˜å¯¹é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä»¥GPT-4oä¸ºé‡ç‚¹ï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å…¶åœ¨æ£€æµ‹ä¸é€‰ä¸¾ç›¸å…³çš„è¯¯å¯¼æ€§å™äº‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ½œåœ¨çš„ä½¿ç”¨æ¡ˆä¾‹ï¼Œå¹¶ä½¿ç”¨æ‹Ÿå®šçš„ç¼–ç è¡¨å’Œæ•°æ®é›†ä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æå‡ºå»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05459v1">PDF</a> This work was accepted at the International AAAI Conference on Web   and Social Media (ICWSM 2025)</p>
<p><strong>Summary</strong></p>
<p>é€‰ä¸¾æœŸé—´ï¼Œè¯¯å¯¼æ€§å™äº‹å¯¹å¡‘é€ å…¬ä¼—æ„è§å…·æœ‰å…³é”®ä½œç”¨ï¼Œå½±å“é€‰æ°‘å¯¹å€™é€‰äººå’Œæ”¿æ²»çš„çœ‹æ³•ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ¬§æ´²æœ€æ–°é€‰ä¸¾ä¸­å¸¸è§è¯¯å¯¼æ€§å™äº‹çš„åˆ†ç±»ä½“ç³»ã€‚åŸºäºè¿™ä¸€åˆ†ç±»ä½“ç³»ï¼Œæˆ‘ä»¬æ„å»ºäº†é¦–ä¸ªæ ‡æ³¨è¯¯å¯¼æ€§å™äº‹çš„æ•°æ®é›†UKElectionNarrativesï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬åœ¨æœ€è¿‘ä¸¤å±Šè‹±å›½å¤§é€‰ä¸­ä¼ æ’­çš„é€‰ä¸¾è¯¯å¯¼æ€§å™äº‹å†…å®¹ã€‚åŒæ—¶æˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ˆé‡ç‚¹å…³æ³¨GPT-4oï¼‰ï¼Œè¯„ä¼°å…¶æ£€æµ‹é€‰ä¸¾ç›¸å…³è¯¯å¯¼æ€§å™äº‹çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ½œåœ¨çš„åº”ç”¨åœºæ™¯ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å»ºè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¯å¯¼æ€§å™äº‹åœ¨é€‰ä¸¾ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå¡‘é€ å…¬ä¼—å¯¹å€™é€‰äººå’Œæ”¿æ²»çš„çœ‹æ³•ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†æ¬§æ´²æœ€æ–°é€‰ä¸¾ä¸­å¸¸è§è¯¯å¯¼æ€§å™äº‹çš„åˆ†ç±»ä½“ç³»ã€‚</li>
<li>åŸºäºåˆ†ç±»ä½“ç³»ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«ä¸¤å±Šè‹±å›½å¤§é€‰ä¸­ä¼ æ’­é€‰ä¸¾è¯¯å¯¼æ€§å™äº‹çš„æ•°æ®é›†UKElectionNarrativesã€‚</li>
<li>å¯¹é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å…¶æ£€æµ‹é€‰ä¸¾ç›¸å…³è¯¯å¯¼æ€§å™äº‹çš„èƒ½åŠ›ã€‚</li>
<li>æ½œåœ¨åº”ç”¨åœºæ™¯åŒ…æ‹¬æ”¿æ²»å®£ä¼ ã€å…¬ä¼—æ•™è‚²å’Œç¤¾ä¼šèˆ†è®ºç ”ç©¶ç­‰ã€‚</li>
<li>ç ”ç©¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘å’Œå»ºè®®ï¼Œå¦‚å¼€å‘æ›´æœ‰æ•ˆçš„æ£€æµ‹å·¥å…·å’Œæ–¹æ³•ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a13890bd5620beb53029765bc3ac7ae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe08a14f677d222a3e7b1dd69b74d785.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca0bd7299cac4a1804e162bac8f441a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72d4e600378d5daab0d4d7c5fb428af0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1df7c8142bbddc1ac66b870dee43e072.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Ultra-FineWeb-Efficient-Data-Filtering-and-Verification-for-High-Quality-LLM-Training-Data"><a href="#Ultra-FineWeb-Efficient-Data-Filtering-and-Verification-for-High-Quality-LLM-Training-Data" class="headerlink" title="Ultra-FineWeb: Efficient Data Filtering and Verification for   High-Quality LLM Training Data"></a>Ultra-FineWeb: Efficient Data Filtering and Verification for   High-Quality LLM Training Data</h2><p><strong>Authors:Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, Xu Han, Zhiyuan Liu</strong></p>
<p>Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ•°æ®è´¨é‡å·²æˆä¸ºæé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ä»¥æ¨¡å‹é©±åŠ¨çš„æ•°æ®è¿‡æ»¤å·²æˆä¸ºè·å–é«˜è´¨é‡æ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç¼ºä¹æœ‰æ•ˆçš„æ•°æ®éªŒè¯ç­–ç•¥ï¼Œéš¾ä»¥å¯¹æ•°æ®è´¨é‡æä¾›åŠæ—¶åé¦ˆï¼›ï¼ˆ2ï¼‰ç”¨äºè®­ç»ƒåˆ†ç±»å™¨çš„ç§å­æ•°æ®é€‰æ‹©ç¼ºä¹æ˜ç¡®æ ‡å‡†ï¼Œé«˜åº¦ä¾èµ–äººä¸ºç»éªŒï¼Œå¼•å…¥äº†ä¸€å®šç¨‹åº¦çš„ä¸»è§‚æ€§ã€‚ä¸ºäº†è§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é«˜æ•ˆéªŒè¯ç­–ç•¥ï¼Œèƒ½å¤Ÿä»¥æœ€ä½çš„è®¡ç®—æˆæœ¬å¿«é€Ÿè¯„ä¼°æ•°æ®å¯¹LLMè®­ç»ƒçš„å½±å“ã€‚ä¸ºäº†è§£å†³ç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å‡è®¾é«˜è´¨é‡ç§å­æ•°æ®å¯¹LLMè®­ç»ƒæœ‰ç›Šï¼Œå¹¶ç»“åˆæå‡ºçš„éªŒè¯ç­–ç•¥ï¼Œä¼˜åŒ–äº†æ­£è´Ÿæ ·æœ¬çš„é€‰æ‹©ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®è¿‡æ»¤æµç¨‹ã€‚è¯¥æµç¨‹ä¸ä»…æé«˜äº†è¿‡æ»¤æ•ˆç‡ã€åˆ†ç±»å™¨è´¨é‡å’Œç¨³å¥æ€§ï¼Œè¿˜æ˜¾è‘—é™ä½äº†å®éªŒå’Œæ¨ç†æˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆåœ°è¿‡æ»¤é«˜è´¨é‡æ•°æ®ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºfastTextçš„è½»é‡çº§åˆ†ç±»å™¨ï¼Œå¹¶å°†è¿‡æ»¤æµç¨‹æˆåŠŸåº”ç”¨äºä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„é¢„è®­ç»ƒè¯­æ–™åº“FineWebå’Œä¸­æ–‡FineWebæ•°æ®é›†ï¼Œä»è€Œåˆ›å»ºäº†æ›´é«˜è´¨é‡çš„Ultra-FineWebæ•°æ®é›†ã€‚Ultra-FineWebåŒ…å«çº¦1ä¸‡äº¿ä¸ªè‹±æ–‡æ ‡è®°å’Œ120äº¿ä¸ªä¸­æ–‡æ ‡è®°ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨Ultra-FineWebè®­ç»ƒçš„LLMæ€§èƒ½å¾—åˆ°æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æµç¨‹åœ¨æé«˜æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05427v1">PDF</a> The datasets are available on   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/openbmb/UltraFineWeb">https://huggingface.co/datasets/openbmb/UltraFineWeb</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ•°æ®è´¨é‡å·²æˆä¸ºæé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚æ¨¡å‹é©±åŠ¨çš„æ•°æ®è¿‡æ»¤æ–¹æ³•é€æ¸æˆä¸ºè·å–é«˜è´¨é‡æ•°æ®çš„ä¸»è¦æ‰‹æ®µï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¼ºä¹æœ‰æ•ˆçš„æ•°æ®éªŒè¯ç­–ç•¥ï¼Œéš¾ä»¥å¯¹æ•°æ®å®‰å…¨åŠæ—¶æä¾›åé¦ˆï¼›äºŒæ˜¯ç§å­æ•°æ®çš„é€‰æ‹©ç¼ºä¹æ˜ç¡®æ ‡å‡†ï¼Œé«˜åº¦ä¾èµ–äººä¸ºç»éªŒï¼Œå¼•å…¥ä¸»è§‚æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§é«˜æ•ˆéªŒè¯ç­–ç•¥ï¼Œä»¥ä½æˆæœ¬è®¡ç®—å®ç°å¯¹LLMè®­ç»ƒæ•°æ®å½±å“çš„å¿«é€Ÿè¯„ä¼°ã€‚æˆ‘ä»¬å‡è®¾é«˜è´¨é‡ç§å­æ•°æ®å¯¹LLMè®­ç»ƒæœ‰ç›Šï¼Œç»“åˆéªŒè¯ç­–ç•¥ä¼˜åŒ–æ­£è´Ÿæ ·æœ¬é€‰æ‹©ï¼Œå¹¶æå‡ºé«˜æ•ˆæ•°æ®è¿‡æ»¤æµç¨‹ã€‚æ­¤æµç¨‹ä¸ä»…æé«˜è¿‡æ»¤æ•ˆç‡ã€åˆ†ç±»å™¨è´¨é‡å’Œç¨³å¥æ€§ï¼Œè¿˜æ˜¾è‘—é™ä½å®éªŒå’Œæ¨ç†æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºfastTextçš„è½»é‡çº§åˆ†ç±»å™¨è¿›è¡Œé«˜æ•ˆé«˜è´¨é‡æ•°æ®è¿‡æ»¤ï¼Œå¹¶å°†è¿‡æ»¤æµç¨‹æˆåŠŸåº”ç”¨äºå¹¿æ³›ä½¿ç”¨çš„é¢„è®­ç»ƒè¯­æ–™åº“FineWebå’Œä¸­æ–‡FineWebæ•°æ®é›†ï¼Œåˆ›å»ºå‡ºæ›´é«˜è´¨é‡çš„Ultra-FineWebæ•°æ®é›†ã€‚å«æœ‰çº¦ä¸€ä¸‡äº¿è‹±æ–‡ä»¤ç‰Œå’Œä¸€ç™¾äºŒåäº¿ä¸­æ–‡ä»¤ç‰Œçš„Ultra-FineWebæ•°æ®é›†å®è¯ç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šï¼ŒåŸºäºUltra-FineWebè®­ç»ƒçš„LLMæ€§èƒ½æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æˆ‘ä»¬æµç¨‹åœ¨æé«˜æ•°æ®è´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®è´¨é‡å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æ¨¡å‹é©±åŠ¨çš„æ•°æ®è¿‡æ»¤æ˜¯è·å–é«˜è´¨é‡æ•°æ®çš„ä¸»è¦æ‰‹æ®µï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹é«˜æ•ˆæ•°æ®éªŒè¯ç­–ç•¥ï¼Œéš¾ä»¥å¯¹æ•°æ®å®‰å…¨åŠæ—¶æä¾›åé¦ˆã€‚</li>
<li>ç§å­æ•°æ®é€‰æ‹©ç¼ºä¹æ˜ç¡®æ ‡å‡†ï¼Œä¾èµ–äººä¸ºç»éªŒï¼Œå¼•å…¥ä¸»è§‚æ€§ã€‚</li>
<li>æå‡ºé«˜æ•ˆéªŒè¯ç­–ç•¥ï¼Œèƒ½å¿«é€Ÿè¯„ä¼°æ•°æ®å¯¹LLMè®­ç»ƒçš„å½±å“ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>ç»“åˆéªŒè¯ç­–ç•¥ä¼˜åŒ–æ­£è´Ÿæ ·æœ¬é€‰æ‹©ï¼Œæå‡è¿‡æ»¤æ•ˆç‡ã€åˆ†ç±»å™¨è´¨é‡å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d02d40f1e25580ebaf308d64da973fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cacde3beee15c2dcfa799e30dc4f3e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c1adb6b0d53b85344978e594a2c0ee3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-105ee030a6746c773d76a92d97b75ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9bfd3264a72b234fc5195fcebda798e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TransProQA-an-LLM-based-literary-Translation-evaluation-metric-with-Professional-Question-Answering"><a href="#TransProQA-an-LLM-based-literary-Translation-evaluation-metric-with-Professional-Question-Answering" class="headerlink" title="TransProQA: an LLM-based literary Translation evaluation metric with   Professional Question Answering"></a>TransProQA: an LLM-based literary Translation evaluation metric with   Professional Question Answering</h2><p><strong>Authors:Ran Zhang, Wei Zhao, Lieve Macken, Steffen Eger</strong></p>
<p>The impact of Large Language Models (LLMs) has extended into literary domains. However, existing evaluation metrics prioritize mechanical accuracy over artistic expression and tend to overrate machine translation (MT) as being superior to experienced professional human translation. In the long run, this bias could result in a permanent decline in translation quality and cultural authenticity. In response to the urgent need for a specialized literary evaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based question-answering (QA) framework designed specifically for literary translation evaluation. TransProQA uniquely integrates insights from professional literary translators and researchers, focusing on critical elements in literary quality assessment such as literary devices, cultural understanding, and authorial voice. Our extensive evaluation shows that while literary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially outperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ and Kendallâ€™s tau) and surpassing the best state-of-the-art (SOTA) metrics by over 15 points in adequacy assessments. Incorporating professional translator insights as weights further improves performance, highlighting the value of translator inputs. Notably, TransProQA approaches human-level evaluation performance comparable to trained linguistic annotators. It demonstrates broad applicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b, indicating its potential as an accessible and training-free literary evaluation metric and a valuable tool for evaluating texts that require local processing due to copyright or ethical considerations. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å½±å“å·²ç»æ‰©å±•åˆ°æ–‡å­¦é¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡æ›´æ³¨é‡æœºæ¢°å‡†ç¡®æ€§è€Œéè‰ºæœ¯è¡¨è¾¾ï¼Œå¹¶å€¾å‘äºè®¤ä¸ºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä¼˜äºç»éªŒä¸°å¯Œçš„ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚ä»é•¿è¿œæ¥çœ‹ï¼Œè¿™ç§åè§å¯èƒ½å¯¼è‡´ç¿»è¯‘è´¨é‡å’Œæ–‡åŒ–çœŸå®æ€§çš„æ°¸ä¹…ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹å¯¹ä¸“ç”¨æ–‡å­¦è¯„ä¼°æŒ‡æ ‡çš„è¿«åˆ‡éœ€æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†TransProQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ã€æ— å‚è€ƒçš„ã€åŸºäºLLMçš„é—®ç­”ï¼ˆQAï¼‰æ¡†æ¶ï¼Œä¸“ä¸ºæ–‡å­¦ç¿»è¯‘è¯„ä¼°è€Œè®¾è®¡ã€‚TransProQAç‹¬ç‰¹åœ°ç»“åˆäº†ä¸“ä¸šæ–‡å­¦ç¿»è¯‘è€…å’Œç ”ç©¶äººå‘˜çš„è§è§£ï¼Œä¸“æ³¨äºæ–‡å­¦è´¨é‡è¯„ä¼°çš„å…³é”®è¦ç´ ï¼Œå¦‚æ–‡å­¦æ‰‹æ³•ã€æ–‡åŒ–ç†è§£å’Œä½œè€…å£°éŸ³ã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè™½ç„¶ç»è¿‡æ–‡å­¦å¾®è°ƒçš„XCOMET-XLåªå¸¦æ¥è½»å¾®çš„æå‡ï¼Œä½†TransProQAåœ¨ç°æœ‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨ç›¸å…³æ€§ï¼ˆACC-EQå’ŒKendallçš„tauï¼‰ä¸Šå®ç°äº†é«˜è¾¾0.07çš„å¢ç›Šï¼Œå¹¶åœ¨å……åˆ†æ€§è¯„ä¼°ä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„æŒ‡æ ‡è¶…è¿‡15åˆ†ã€‚é€šè¿‡å°†ä¸“ä¸šç¿»è¯‘äººå‘˜çš„è§è§£ä½œä¸ºæƒé‡çº³å…¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œçªæ˜¾äº†ç¿»è¯‘äººå‘˜è¾“å…¥çš„ä»·å€¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTransProQAçš„è¯„ä»·æ€§èƒ½æ¥è¿‘äººç±»æ°´å¹³ï¼Œä¸å—è¿‡è®­ç»ƒçš„è¯­è¨€å­¦æ³¨é‡Šè€…ç›¸å½“ã€‚å®ƒè¯æ˜äº†å¯¹å¼€æºæ¨¡å‹ï¼ˆå¦‚LLaMA 3.3-70bå’ŒQwen 2.5-32bï¼‰çš„å¹¿æ³›åº”ç”¨èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºå¯è®¿é—®ä¸”æ— éœ€è®­ç»ƒå³å¯ä½¿ç”¨çš„æ–‡å­¦è¯„ä¼°æŒ‡æ ‡ä»¥åŠç”¨äºè¯„ä¼°å› ç‰ˆæƒæˆ–ä¼¦ç†è€ƒè™‘éœ€è¦æœ¬åœ°å¤„ç†çš„æ–‡æœ¬çš„æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05423v1">PDF</a> WIP</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡å­¦é¢†åŸŸäº§ç”Ÿäº†å½±å“ï¼Œä½†ç°æœ‰è¯„ä¼°æŒ‡æ ‡åé‡æœºæ¢°å‡†ç¡®æ€§è€Œå¿½è§†è‰ºæœ¯è¡¨è¾¾ï¼Œå€¾å‘äºè®¤ä¸ºæœºå™¨ç¿»è¯‘ä¼˜äºç»éªŒä¸°å¯Œçš„ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚è¿™å¯èƒ½å¯¼è‡´ç¿»è¯‘è´¨é‡å’Œæ–‡åŒ–çœŸå®æ€§æ°¸ä¹…ä¸‹é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ã€åŸºäºLLMçš„é—®ç­”æ¡†æ¶â€”â€”TransProQAï¼Œä¸“é—¨ç”¨äºæ–‡å­¦ç¿»è¯‘è¯„ä¼°ã€‚å®ƒç»“åˆäº†ä¸“ä¸šæ–‡å­¦ç¿»è¯‘äººå‘˜å’Œç ”ç©¶äººå‘˜çš„è§è§£ï¼Œä¾§é‡äºæ–‡å­¦è´¨é‡è¯„ä¼°çš„å…³é”®è¦ç´ ï¼Œå¦‚æ–‡å­¦æ‰‹æ³•ã€æ–‡åŒ–ç†è§£å’Œä½œè€…å£°éŸ³ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒTransProQAæ˜¾è‘—ä¼˜äºå½“å‰æŒ‡æ ‡ï¼Œä¸äººç±»æ°´å¹³è¯„ä¼°æ€§èƒ½ç›¸è¿‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯¹æ–‡å­¦é¢†åŸŸäº§ç”Ÿå½±å“ï¼Œä½†ç°æœ‰è¯„ä¼°æŒ‡æ ‡åé‡æœºæ¢°å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰æŒ‡æ ‡å€¾å‘äºè®¤ä¸ºæœºå™¨ç¿»è¯‘ä¼˜äºä¸“ä¸šäººå·¥ç¿»è¯‘ï¼Œå¯èƒ½å½±å“ç¿»è¯‘è´¨é‡å’Œæ–‡åŒ–çœŸå®æ€§ã€‚</li>
<li>æ¨å‡ºä¸“é—¨ç”¨äºæ–‡å­¦ç¿»è¯‘è¯„ä¼°çš„æ–°å‹LLMé—®ç­”æ¡†æ¶â€”â€”TransProQAã€‚</li>
<li>TransProQAç»“åˆä¸“ä¸šæ–‡å­¦ç¿»è¯‘äººå‘˜å’Œç ”ç©¶äººå‘˜çš„è§è§£ã€‚</li>
<li>TransProQAä¾§é‡äºæ–‡å­¦è´¨é‡è¯„ä¼°çš„å…³é”®è¦ç´ ï¼Œå¦‚æ–‡å­¦æ‰‹æ³•ã€æ–‡åŒ–ç†è§£å’Œä½œè€…å£°éŸ³ã€‚</li>
<li>TransProQAæ˜¾è‘—ä¼˜äºå½“å‰è¯„ä¼°æŒ‡æ ‡ï¼Œä¸äººç±»æ°´å¹³è¯„ä¼°æ€§èƒ½ç›¸è¿‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-515f75a7328ad6d6a80f8734bc7566e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b89964f4475589d275b2431094bfd9f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0438a130540900b17f79c2c3db0d3887.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ICon-In-Context-Contribution-for-Automatic-Data-Selection"><a href="#ICon-In-Context-Contribution-for-Automatic-Data-Selection" class="headerlink" title="ICon: In-Context Contribution for Automatic Data Selection"></a>ICon: In-Context Contribution for Automatic Data Selection</h2><p><strong>Authors:Yixin Yang, Qingxiu Dong, Linli Yao, Fangwei Zhu, Zhifang Sui</strong></p>
<p>Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICon offers a computationally efficient alternative to gradient-based methods and reduces human inductive bias inherent in heuristic-based approaches. ICon comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by ICon, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones. </p>
<blockquote>
<p>æ•°æ®é€‰æ‹©å¯¹äºæŒ‡ä»¤å¾®è°ƒåœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½å’Œé™ä½è®­ç»ƒæˆæœ¬æ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨é€‰æ‹©æ–¹æ³•è¦ä¹ˆä¾èµ–äºè®¡ç®—æ˜‚è´µçš„åŸºäºæ¢¯åº¦çš„åº¦é‡ï¼Œè¦ä¹ˆä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„å¯å‘å¼è§„åˆ™ï¼Œè¿™å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®çš„å†…åœ¨å±æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¯­å¢ƒå­¦ä¹ çš„è´¡çŒ®æµ‹é‡ï¼ˆIConï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ— éœ€æ¢¯åº¦çš„æµ‹é‡æ–¹æ³•ï¼Œåˆ©ç”¨è¯­å¢ƒå­¦ä¹ ï¼ˆCLï¼‰çš„éšæ€§å¾®è°ƒç‰¹æ€§æ¥æµ‹é‡æ ·æœ¬è´¡çŒ®ï¼Œæ— éœ€è¿›è¡Œæ¢¯åº¦è®¡ç®—æˆ–æ‰‹åŠ¨æŒ‡æ ‡å·¥ç¨‹ã€‚IConä¸ºåŸºäºæ¢¯åº¦çš„æ–¹æ³•æä¾›äº†è®¡ç®—æ•ˆç‡æ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å‡å°‘äº†å¯å‘å¼æ–¹æ³•ä¸­å›ºæœ‰çš„äººç±»å½’çº³åè§ã€‚IConåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼Œé€šè¿‡è¯„ä¼°è¯­å¢ƒå­¦ä¹ ä¸‹çš„æ€§èƒ½å˜åŒ–æ¥è¯†åˆ«é«˜è´¡çŒ®æ•°æ®ã€‚åœ¨ä¸‰ä¸ªLLMã€è·¨è¶Š12ä¸ªåŸºå‡†æµ‹è¯•å’Œ5ä¸ªé…å¯¹è¯„ä¼°é›†çš„å¤§é‡å®éªŒè¯æ˜äº†IConçš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œåœ¨LLaMA 3.1-8Bä¸Šï¼Œä½¿ç”¨IConé€‰æ‹©çš„15%æ•°æ®è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹æ¯”ä½¿ç”¨å…¨æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹é«˜å‡º5.42ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶è¶…è¿‡äº†å¹¿æ³›ä½¿ç”¨çš„é€‰æ‹©æ–¹æ³•çš„æœ€ä½³æ€§èƒ½2.06ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†IConé€‰æ‹©çš„é«˜è´¡çŒ®æ ·æœ¬ï¼Œæ˜¾ç¤ºå‡ºä»»åŠ¡å¤šæ ·æ€§å’Œé€‚å½“çš„éš¾åº¦æ°´å¹³ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å›°éš¾çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05327v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹æ—¶ä»£ï¼Œæ•°æ®é€‰æ‹©å°¤ä¸ºé‡è¦ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–é€‰æ‹©æ–¹æ³•è¦ä¹ˆä¾èµ–è®¡ç®—æˆæœ¬é«˜æ˜‚çš„åŸºäºæ¢¯åº¦çš„åº¦é‡æ–¹æ³•ï¼Œè¦ä¹ˆä¾èµ–äºäººå·¥è®¾è®¡çš„å¯å‘å¼è§„åˆ™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®çš„å†…åœ¨å±æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„è´¡çŒ®åº¦é‡æ–¹æ³•ï¼ˆIConï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ— éœ€æ¢¯åº¦çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„éšå¼å¾®è°ƒç‰¹æ€§æ¥åº¦é‡æ ·æœ¬è´¡çŒ®ï¼Œæ— éœ€æ¢¯åº¦è®¡ç®—æˆ–æ‰‹åŠ¨æŒ‡æ ‡å·¥ç¨‹ã€‚IConä¸ºåŸºäºæ¢¯åº¦çš„æ–¹æ³•æä¾›äº†è®¡ç®—æ•ˆç‡æ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å‡å°‘äº†å¯å‘å¼æ–¹æ³•ä¸­å›ºæœ‰çš„äººç±»å½’çº³åè§ã€‚é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼ŒIConåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨LLaMA3.1-8Bä¸Šï¼Œä½¿ç”¨IConé€‰æ‹©çš„15%æ•°æ®è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½è¶…è¶Šäº†å…¨æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶è¶…è¿‡äº†å…¶ä»–å¸¸ç”¨é€‰æ‹©æ–¹æ³•çš„æœ€ä¼˜æ€§èƒ½ã€‚åˆ†ææ˜¾ç¤ºï¼ŒIConé€‰æ‹©çš„é«˜è´¡çŒ®æ ·æœ¬å…·æœ‰å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œé€‚å½“çš„éš¾åº¦çº§åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é€‰æ‹©åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æå‡å’Œè®­ç»ƒæˆæœ¬é™ä½æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ•°æ®é€‰æ‹©æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œæ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®å†…åœ¨å±æ€§ç­‰é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„è´¡çŒ®åº¦é‡æ–¹æ³•ï¼ˆIConï¼‰ã€‚</li>
<li>IConåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„éšå¼å¾®è°ƒç‰¹æ€§ï¼Œæ— éœ€æ¢¯åº¦è®¡ç®—æˆ–å¤æ‚çš„æ‰‹å·¥æŒ‡æ ‡å·¥ç¨‹ã€‚</li>
<li>IConæä¾›äº†è®¡ç®—æ•ˆç‡æ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å‡å°‘äº†å¯å‘å¼æ–¹æ³•ä¸­çš„äººç±»å½’çº³åè§ã€‚</li>
<li>åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†IConçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9542fd7760518bf49f669c421666416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c605c1cdea5d7402aa2c0116ddbce510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea59fd9607b401cbbfc1fa28f236c96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcb0cfc9765ded3732ed7407accc0372.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HEXGEN-TEXT2SQL-Optimizing-LLM-Inference-Request-Scheduling-for-Agentic-Text-to-SQL-Workflow"><a href="#HEXGEN-TEXT2SQL-Optimizing-LLM-Inference-Request-Scheduling-for-Agentic-Text-to-SQL-Workflow" class="headerlink" title="HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic   Text-to-SQL Workflow"></a>HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic   Text-to-SQL Workflow</h2><p><strong>Authors:You Peng, Youhe Jiang, Chen Wang, Binhang Yuan</strong></p>
<p>Recent advances in leveraging the agentic paradigm of large language models (LLMs) utilization have significantly enhanced Text-to-SQL capabilities, enabling users without specialized database expertise to query data intuitively. However, deploying these agentic LLM-based Text-to-SQL systems in production poses substantial challenges due to their inherently multi-stage workflows, stringent latency constraints, and potentially heterogeneous GPU infrastructure in enterprise environments. Current LLM serving frameworks lack effective mechanisms for handling interdependent inference tasks, dynamic latency variability, and resource heterogeneity, leading to suboptimal performance and frequent service-level objective (SLO) violations. In this paper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to schedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on heterogeneous GPU clusters that handle multi-tenant end-to-end queries. HEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global workload-balanced task dispatching and local adaptive urgency-guided prioritization, guided by a systematic analysis of agentic Text-to-SQL workflows. Additionally, we propose a lightweight simulation-based method for tuning critical scheduling hyperparameters, further enhancing robustness and adaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks demonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art LLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines by up to 1.67$\times$ (average: 1.41$\times$) and improves system throughput by up to 1.75$\times$ (average: 1.65$\times$) compared to vLLM under diverse, realistic workload conditions. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Relaxed-System-Lab/Hexgen-Flow">https://github.com/Relaxed-System-Lab/Hexgen-Flow</a>. </p>
<blockquote>
<p>è¿‘æœŸåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†èŒƒå¼çš„è¿›æ­¥æå¤§åœ°å¢å¼ºäº†æ–‡æœ¬åˆ°SQLçš„èƒ½åŠ›ï¼Œä½¿å¾—æ²¡æœ‰ä¸“ä¸šæ•°æ®åº“çŸ¥è¯†çš„ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°æŸ¥è¯¢æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²è¿™äº›åŸºäºLLMçš„æ–‡æœ¬åˆ°SQLç³»ç»Ÿå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬æœ¬è´¨ä¸Šå…·æœ‰å¤šé˜¶æ®µå·¥ä½œæµç¨‹ã€ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶å’Œæ½œåœ¨çš„ä¼ä¸šç¯å¢ƒä¸­çš„å¼‚æ„GPUåŸºç¡€è®¾æ–½ã€‚å½“å‰çš„LLMæœåŠ¡æ¡†æ¶ç¼ºä¹å¤„ç†ç›¸äº’ä¾èµ–çš„æ¨ç†ä»»åŠ¡ã€åŠ¨æ€å»¶è¿Ÿå¯å˜æ€§ä»¥åŠèµ„æºå¼‚æ„æ€§çš„æœ‰æ•ˆæœºåˆ¶ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œé¢‘ç¹çš„æœåŠ¡çº§åˆ«ç›®æ ‡ï¼ˆSLOï¼‰è¿è§„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†èŒƒå¼åœ¨Text-to-SQLèƒ½åŠ›æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œä½¿å¾—æ²¡æœ‰ä¸“ä¸šæ•°æ®åº“çŸ¥è¯†çš„ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°æŸ¥è¯¢æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²è¿™äº›åŸºäºLLMçš„Text-to-SQLç³»ç»Ÿé¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å¤šé˜¶æ®µå·¥ä½œæµç¨‹ã€ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶å’Œæ½œåœ¨çš„ä¼ä¸šç¯å¢ƒä¸­çš„å¼‚æ„GPUåŸºç¡€è®¾æ–½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶HEXGEN-TEXT2SQLï¼Œä¸“ä¸ºåœ¨å¼‚æ„GPUé›†ç¾¤ä¸Šè°ƒåº¦å’Œæ‰§è¡ŒåŸºäºLLMçš„å¤šé˜¶æ®µText-to-SQLå·¥ä½œæµç¨‹è€Œè®¾è®¡ï¼Œè¯¥æ¡†æ¶å¤„ç†ç«¯åˆ°ç«¯çš„å¤šç§Ÿæˆ·æŸ¥è¯¢ã€‚HEXGEN-TEXT2SQLå¼•å…¥äº†ä¸€ç§å±‚æ¬¡è°ƒåº¦æ–¹æ³•ï¼Œç»“åˆå…¨å±€è´Ÿè½½å¹³è¡¡çš„ä»»åŠ¡åˆ†é…å’Œæœ¬åœ°è‡ªé€‚åº”ç´§æ€¥å¼•å¯¼ä¼˜å…ˆæ’åºï¼Œå¹¶å¯¹agentic Text-to-SQLå·¥ä½œæµç¨‹è¿›è¡Œç³»ç»Ÿåˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºè½»é‡çº§æ¨¡æ‹Ÿçš„æ–¹æ³•ï¼Œç”¨äºè°ƒæ•´å…³é”®è°ƒåº¦è¶…å‚æ•°ï¼Œè¿›ä¸€æ­¥æé«˜ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚åœ¨ç°å®çš„Text-to-SQLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHEXGEN-TEXT2SQLçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„LLMæœåŠ¡æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsçš„ä»£ç†èŒƒå¼åœ¨Text-to-SQLèƒ½åŠ›æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½¿å¾—éä¸“ä¸šæ•°æ®åº“ç”¨æˆ·èƒ½æ›´ç›´è§‚åœ°æŸ¥è¯¢æ•°æ®ã€‚</li>
<li>åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²åŸºäºLLMçš„Text-to-SQLç³»ç»Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¤šé˜¶æ®µæµç¨‹ã€å»¶è¿Ÿé™åˆ¶å’Œå¼‚æ„GPUåŸºç¡€è®¾æ–½ã€‚</li>
<li>HEXGEN-TEXT2SQLæ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºåœ¨å¼‚æ„GPUé›†ç¾¤ä¸Šæ‰§è¡ŒåŸºäºLLMçš„å¤šé˜¶æ®µText-to-SQLå·¥ä½œæµç¨‹çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>HEXGEN-TEXT2SQLé‡‡ç”¨å±‚æ¬¡è°ƒåº¦æ–¹æ³•ï¼Œç»“åˆå…¨å±€è´Ÿè½½å¹³è¡¡å’Œæœ¬åœ°è‡ªé€‚åº”ç´§æ€¥å¼•å¯¼ä¼˜å…ˆæ’åºã€‚</li>
<li>HEXGEN-TEXT2SQLé€šè¿‡è½»é‡çº§æ¨¡æ‹Ÿæ–¹æ³•è°ƒæ•´è°ƒåº¦è¶…å‚æ•°ï¼Œæé«˜ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>åœ¨ç°å®çš„Text-to-SQLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHEXGEN-TEXT2SQLç›¸è¾ƒäºå…¶ä»–LLMæœåŠ¡æ¡†æ¶è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>HEXGEN-TEXT2SQLèƒ½é™ä½å»¶è¿ŸæœŸé™ï¼Œå¹¶æé«˜ç³»ç»Ÿååé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b8cab20959956365dd9a68443716228.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a356b529c97af9875e6357fb1c73de36.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PADriver-Towards-Personalized-Autonomous-Driving"><a href="#PADriver-Towards-Personalized-Autonomous-Driving" class="headerlink" title="PADriver: Towards Personalized Autonomous Driving"></a>PADriver: Towards Personalized Autonomous Driving</h2><p><strong>Authors:Genghua Kou, Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Ziheng Zhang, Osamu Yoshie, Tiancai Wang, Ying Li, Xiangyu Zhang</strong></p>
<p>In this paper, we propose PADriver, a novel closed-loop framework for personalized autonomous driving (PAD). Built upon Multi-modal Large Language Model (MLLM), PADriver takes streaming frames and personalized textual prompts as inputs. It autoaggressively performs scene understanding, danger level estimation and action decision. The predicted danger level reflects the risk of the potential action and provides an explicit reference for the final action, which corresponds to the preset personalized prompt. Moreover, we construct a closed-loop benchmark named PAD-Highway based on Highway-Env simulator to comprehensively evaluate the decision performance under traffic rules. The dataset contains 250 hours videos with high-quality annotation to facilitate the development of PAD behavior analysis. Experimental results on the constructed benchmark show that PADriver outperforms state-of-the-art approaches on different evaluation metrics, and enables various driving modes. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PADriverï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä¸ªæ€§åŒ–è‡ªåŠ¨é©¾é©¶ï¼ˆPADï¼‰çš„æ–°å‹é—­ç¯æ¡†æ¶ã€‚åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼ŒPADriverä»¥æµå¼å¸§å’Œä¸ªæ€§åŒ–æ–‡æœ¬æç¤ºä¸ºè¾“å…¥ã€‚å®ƒè‡ªåŠ¨æ‰§è¡Œåœºæ™¯ç†è§£ã€å±é™©çº§åˆ«ä¼°è®¡å’ŒåŠ¨ä½œå†³ç­–ã€‚é¢„æµ‹çš„å±é™©çº§åˆ«åæ˜ äº†æ½œåœ¨åŠ¨ä½œçš„é£é™©ï¼Œå¹¶ä¸ºæœ€ç»ˆåŠ¨ä½œæä¾›äº†æ˜ç¡®çš„å‚è€ƒï¼Œè¿™ä¸é¢„è®¾çš„ä¸ªæ€§åŒ–æç¤ºç›¸å¯¹åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºHighway-Envæ¨¡æ‹Ÿå™¨æ„å»ºäº†ä¸€ä¸ªåä¸ºPAD-Highwayçš„é—­ç¯åŸºå‡†æµ‹è¯•ï¼Œä»¥å…¨é¢è¯„ä¼°äº¤é€šè§„åˆ™ä¸‹çš„å†³ç­–æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«250å°æ—¶çš„é«˜è´¨é‡æ³¨é‡Šè§†é¢‘ï¼Œæœ‰åŠ©äºPADè¡Œä¸ºåˆ†æçš„å¼€å‘ã€‚åœ¨æ„å»ºçš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPADriveråœ¨ä¸åŒè¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œå¹¶å¯å®ç°å¤šç§é©¾é©¶æ¨¡å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05240v1">PDF</a> </p>
<p><strong>Summary</strong><br>PADè®ºæ–‡æå‡ºä¸€ä¸ªæ–°é¢–çš„é—­ç¯æ¡†æ¶PADriverï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å®ç°ä¸ªæ€§åŒ–è‡ªåŠ¨é©¾é©¶ï¼ˆPADï¼‰ã€‚PADriveræ¥å—æµå¼å¸§å’Œä¸ªæ€§åŒ–æ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥ï¼Œè¿›è¡Œåœºæ™¯ç†è§£ã€å±é™©ç­‰çº§ä¼°è®¡å’ŒåŠ¨ä½œå†³ç­–ã€‚é€šè¿‡æ„å»ºPAD-HighwayåŸºå‡†æµ‹è¯•å¹³å°è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜äº†PADriveråœ¨ä¸åŒè¯„ä»·æŒ‡æ ‡ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œå¹¶å¯å®ç°å¤šç§é©¾é©¶æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PADriveræ˜¯åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ä¸ªæ€§åŒ–è‡ªåŠ¨é©¾é©¶é—­ç¯æ¡†æ¶ã€‚</li>
<li>å®ƒæ¥å—æµå¼å¸§å’Œä¸ªæ€§åŒ–æ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥ï¼Œè¿›è¡Œåœºæ™¯ç†è§£ã€‚</li>
<li>PADriverèƒ½è¿›è¡Œå±é™©ç­‰çº§ä¼°è®¡å’ŒåŠ¨ä½œå†³ç­–ï¼Œå…¶ä¸­å±é™©ç­‰çº§åæ˜ äº†æ½œåœ¨åŠ¨ä½œçš„é£é™©å¹¶ä¸ºæœ€ç»ˆåŠ¨ä½œæä¾›æ˜ç¡®å‚è€ƒã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåä¸ºPAD-Highwayçš„é—­ç¯åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŸºäºHighway-Envæ¨¡æ‹Ÿå™¨è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚</li>
<li>PADriveråŒ…å«é«˜è´¨é‡æ ‡æ³¨çš„250å°æ—¶è§†é¢‘æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›PADè¡Œä¸ºåˆ†æçš„å‘å±•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPADriveråœ¨ä¸åŒè¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f67e424050575284fb9afbddf971897e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-419ce6e284f14132d3e917a118e7050e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dbc2ad4edda66d2ac5737e026c9d18ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d6a87696c22c041020c9d6fc30e757.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Latte-Transfering-LLMs-Latent-level-Knowledge-for-Few-shot-Tabular-Learning"><a href="#Latte-Transfering-LLMs-Latent-level-Knowledge-for-Few-shot-Tabular-Learning" class="headerlink" title="Latte: Transfering LLMs&#96; Latent-level Knowledge for Few-shot Tabular   Learning"></a>Latte: Transfering LLMs&#96; Latent-level Knowledge for Few-shot Tabular   Learning</h2><p><strong>Authors:Ruxue Shi, Hengrui Gu, Hangting Ye, Yiwei Dai, Xu Shen, Xin Wang</strong></p>
<p>Few-shot tabular learning, in which machine learning models are trained with a limited amount of labeled data, provides a cost-effective approach to addressing real-world challenges. The advent of Large Language Models (LLMs) has sparked interest in leveraging their pre-trained knowledge for few-shot tabular learning. Despite promising results, existing approaches either rely on test-time knowledge extraction, which introduces undesirable latency, or text-level knowledge, which leads to unreliable feature engineering. To overcome these limitations, we propose Latte, a training-time knowledge extraction framework that transfers the latent prior knowledge within LLMs to optimize a more generalized downstream model. Latte enables general knowledge-guided downstream tabular learning, facilitating the weighted fusion of information across different feature values while reducing the risk of overfitting to limited labeled data. Furthermore, Latte is compatible with existing unsupervised pre-training paradigms and effectively utilizes available unlabeled samples to overcome the performance limitations imposed by an extremely small labeled dataset. Extensive experiments on various few-shot tabular learning benchmarks demonstrate the superior performance of Latte, establishing it as a state-of-the-art approach in this domain </p>
<blockquote>
<p>å°æ ·æœ¬è¡¨æ ¼å­¦ä¹ é€šè¿‡ä½¿ç”¨æœ‰é™é‡çš„æ ‡è®°æ•°æ®è¿›è¡Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒï¼Œä¸ºè§£å†³ç°å®ä¸–ç•ŒæŒ‘æˆ˜æä¾›äº†å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œå¦‚ä½•åˆ©ç”¨å…¶é¢„è®­ç»ƒçŸ¥è¯†æ¥è¿›è¡Œå°æ ·æœ¬è¡¨æ ¼å­¦ä¹ å¼•èµ·äº†äººä»¬çš„å…´è¶£ã€‚å°½ç®¡å·²æœ‰ä¸€äº›ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºæµ‹è¯•æ—¶çš„çŸ¥è¯†æå–ï¼Œè¿™å¼•å…¥äº†ä¸å¿…è¦çš„å»¶è¿Ÿï¼Œè¦ä¹ˆä¾èµ–äºæ–‡æœ¬çº§åˆ«çš„çŸ¥è¯†ï¼Œè¿™å¯¼è‡´äº†ä¸å¯é çš„ç‰¹å¾å·¥ç¨‹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Latteï¼Œè¿™æ˜¯ä¸€ä¸ªè®­ç»ƒæ—¶çš„çŸ¥è¯†æå–æ¡†æ¶ï¼Œå®ƒå°†LLMä¸­çš„æ½œåœ¨å…ˆéªŒçŸ¥è¯†è½¬ç§»å‡ºæ¥ï¼Œä»¥ä¼˜åŒ–æ›´é€šç”¨çš„ä¸‹æ¸¸æ¨¡å‹ã€‚Latteèƒ½å¤Ÿå®ç°é€šç”¨çŸ¥è¯†å¼•å¯¼çš„ä¸‹æ¸¸è¡¨æ ¼å­¦ä¹ ï¼Œä¿ƒè¿›ä¸åŒç‰¹å¾å€¼ä¹‹é—´ä¿¡æ¯çš„åŠ æƒèåˆï¼ŒåŒæ—¶é™ä½å¯¹æœ‰é™æ ‡è®°æ•°æ®è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼ŒLatteä¸ç°æœ‰çš„æ— ç›‘ç£é¢„è®­ç»ƒèŒƒå¼å…¼å®¹ï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨å¯ç”¨çš„æœªæ ‡è®°æ ·æœ¬ï¼Œä»¥å…‹æœç”±æå°æ ‡è®°æ•°æ®é›†å¸¦æ¥çš„æ€§èƒ½é™åˆ¶ã€‚åœ¨å„ç§å°æ ·æœ¬è¡¨æ ¼å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†Latteçš„å“è¶Šæ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºè¯¥é¢†åŸŸçš„æœ€æ–°å‰æ²¿æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05237v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå°‘é‡æ ‡æ³¨æ•°æ®çš„æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼ˆå³ä½æ ·æœ¬è¡¨å­¦ä¹ ï¼‰ä¸ºåº”å¯¹ç°å®æŒ‘æˆ˜æä¾›äº†ç»æµé«˜æ•ˆçš„æ–¹æ³•ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æ¿€å‘äº†äººä»¬å°†å…¶é¢„è®­ç»ƒçŸ¥è¯†ç”¨äºä½æ ·æœ¬è¡¨å­¦ä¹ çš„å…´è¶£ã€‚å°½ç®¡å·²æœ‰æ–¹æ³•å–å¾—æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬è¦ä¹ˆä¾èµ–äºæµ‹è¯•æ—¶çš„çŸ¥è¯†æå–ï¼Œè¿™å¼•å…¥äº†ä¸å¿…è¦çš„å»¶è¿Ÿï¼Œè¦ä¹ˆä¾èµ–äºæ–‡æœ¬çº§åˆ«çš„çŸ¥è¯†ï¼Œè¿™å¯¼è‡´ç‰¹å¾å·¥ç¨‹ä¸å¯é ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Latteæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªè®­ç»ƒæ—¶çš„çŸ¥è¯†æå–æ¡†æ¶ï¼Œç”¨äºå°†LLMä¸­çš„æ½œåœ¨å…ˆéªŒçŸ¥è¯†è½¬ç§»åˆ°ä¸‹æ¸¸æ¨¡å‹ä»¥è¿›è¡Œä¼˜åŒ–ã€‚Latteä½¿é€šç”¨çš„çŸ¥è¯†å¼•å¯¼ä¸‹æ¸¸è¡¨å­¦ä¹ æˆä¸ºå¯èƒ½ï¼Œä¿ƒè¿›ä¸åŒç‰¹å¾å€¼ä¹‹é—´ä¿¡æ¯çš„åŠ æƒèåˆï¼ŒåŒæ—¶é™ä½å¯¹æœ‰é™æ ‡æ³¨æ•°æ®è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼ŒLatteä¸ç°æœ‰çš„æ— ç›‘ç£é¢„è®­ç»ƒèŒƒå¼å…¼å®¹ï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨å¯ç”¨çš„æœªæ ‡æ³¨æ ·æœ¬ï¼Œä»¥å…‹æœç”±æå°çš„æ ‡æ³¨æ•°æ®é›†å¸¦æ¥çš„æ€§èƒ½é™åˆ¶ã€‚åœ¨å„ç§ä½æ ·æœ¬è¡¨å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLatteçš„æ€§èƒ½å¤„äºé¢†å…ˆåœ°ä½ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„æœ€å‰æ²¿æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½æ ·æœ¬è¡¨å­¦ä¹ æ˜¯ä¸€ç§åˆ©ç”¨æœ‰é™æ ‡æ³¨æ•°æ®çš„æœ‰æ•ˆæ–¹æ³•æ¥è§£å†³ç°å®æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒçŸ¥è¯†åœ¨ä½æ ·æœ¬è¡¨å­¦ä¹ ä¸­å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æµ‹è¯•æ—¶çŸ¥è¯†æå–å¸¦æ¥çš„å»¶è¿Ÿå’Œä¾èµ–æ–‡æœ¬çº§åˆ«çŸ¥è¯†çš„ç‰¹å¾å·¥ç¨‹ä¸å¯é é—®é¢˜ã€‚</li>
<li>Latteæ¡†æ¶æ˜¯ä¸€ç§è®­ç»ƒæ—¶çš„çŸ¥è¯†æå–æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¸‹æ¸¸æ¨¡å‹å¹¶é™ä½è¿‡åº¦æ‹Ÿåˆé£é™©ã€‚</li>
<li>Latteä¿ƒè¿›ä¸åŒç‰¹å¾å€¼ä¹‹é—´çš„ä¿¡æ¯åŠ æƒèåˆï¼Œå®ç°æ›´é€šç”¨çš„çŸ¥è¯†å¼•å¯¼ä¸‹æ¸¸è¡¨å­¦ä¹ ã€‚</li>
<li>Latteä¸æ— ç›‘ç£é¢„è®­ç»ƒèŒƒå¼å…¼å®¹ï¼Œèƒ½æœ‰æ•ˆåˆ©ç”¨æœªæ ‡æ³¨æ ·æœ¬ä»¥å¢å¼ºæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a860ab2e82b0131f407a6f3912174523.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cae4672510d6a0e0615bd91e8f274ac3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-320de0c4d5fc208f15b6b708f3d02c83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-342424ea6700154eb92c9dee696b6cd9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="QualBench-Benchmarking-Chinese-LLMs-with-Localized-Professional-Qualifications-for-Vertical-Domain-Evaluation"><a href="#QualBench-Benchmarking-Chinese-LLMs-with-Localized-Professional-Qualifications-for-Vertical-Domain-Evaluation" class="headerlink" title="QualBench: Benchmarking Chinese LLMs with Localized Professional   Qualifications for Vertical Domain Evaluation"></a>QualBench: Benchmarking Chinese LLMs with Localized Professional   Qualifications for Vertical Domain Evaluation</h2><p><strong>Authors:Mengze Hong, Wailing Ng, Di Jiang, Chen Jason Zhang</strong></p>
<p>The rapid advancement of Chinese large language models (LLMs) underscores the need for domain-specific evaluations to ensure reliable applications. However, existing benchmarks often lack coverage in vertical domains and offer limited insights into the Chinese working context. Leveraging qualification exams as a unified framework for human expertise evaluation, we introduce QualBench, the first multi-domain Chinese QA benchmark dedicated to localized assessment of Chinese LLMs. The dataset includes over 17,000 questions across six vertical domains, with data selections grounded in 24 Chinese qualifications to closely align with national policies and working standards. Through comprehensive evaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with Chinese LLMs consistently surpassing non-Chinese models, highlighting the importance of localized domain knowledge in meeting qualification requirements. The best performance of 75.26% reveals the current gaps in domain coverage within model capabilities. Furthermore, we present the failure of LLM collaboration with crowdsourcing mechanisms and suggest the opportunities for multi-domain RAG knowledge enhancement and vertical domain LLM training with Federated Learning. </p>
<blockquote>
<p>ä¸­æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¼ºè°ƒäº†è¿›è¡Œä¸“ä¸šé¢†åŸŸè¯„ä¼°çš„å¿…è¦æ€§ï¼Œä»¥ç¡®ä¿å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ç¼ºä¹å‚ç›´é¢†åŸŸçš„è¦†ç›–ï¼Œå¯¹äºä¸­æ–‡å·¥ä½œç¯å¢ƒä¸‹çš„æ·±å…¥æ´å¯Ÿä¹Ÿæœ‰é™ã€‚æˆ‘ä»¬å€ŸåŠ©èµ„æ ¼è€ƒè¯•ä½œä¸ºäººç±»ä¸“ä¸šçŸ¥è¯†è¯„ä¼°çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¼•å…¥äº†QualBenchï¼Œè¿™æ˜¯é¦–ä¸ªå¤šé¢†åŸŸçš„ä¸­æ–‡é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œè‡´åŠ›äºå¯¹ä¸­æ–‡LLMè¿›è¡Œæœ¬åœ°åŒ–è¯„ä¼°ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡17000ä¸ªè·¨è¶Šå…­ä¸ªå‚ç›´é¢†åŸŸçš„é—®é¢˜ï¼Œæ•°æ®é€‰æ‹©åŸºäº24é¡¹ä¸­æ–‡èµ„æ ¼è®¤è¯ï¼Œä¸å›½å®¶æ”¿ç­–å’Œå·¥ä½œæ ‡å‡†ç´§å¯†å¯¹é½ã€‚é€šè¿‡ç»¼åˆè¯„ä¼°ï¼ŒQwen2.5æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æ›´å…ˆè¿›çš„GPT-4oï¼Œä¸­æ–‡LLMæŒç»­è¶…è¶Šéä¸­æ–‡æ¨¡å‹ï¼Œçªæ˜¾äº†ç¬¦åˆèµ„æ ¼è¦æ±‚çš„æœ¬åœ°åŒ–é¢†åŸŸçŸ¥è¯†çš„é‡è¦æ€§ã€‚æœ€ä½³æ€§èƒ½ä¸º75.26%ï¼Œæ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›åœ¨é¢†åŸŸè¦†ç›–æ–¹é¢çš„å½“å‰å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†LLMä¸ä¼—åŒ…æœºåˆ¶åˆä½œçš„å¤±è´¥ï¼Œå¹¶æå‡ºäº†é€šè¿‡è”åˆå­¦ä¹ è¿›è¡Œå¤šé¢†åŸŸRAGçŸ¥è¯†å¢å¼ºå’Œå‚ç›´é¢†åŸŸLLMè®­ç»ƒçš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05225v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸­å›½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å‡¸æ˜¾äº†é¢†åŸŸç‰¹å®šè¯„ä¼°çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿å…¶åœ¨å„ç§åº”ç”¨åœºæ™¯ä¸­çš„å¯é æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ç¼ºä¹å‚ç›´é¢†åŸŸçš„è¦†ç›–ï¼Œå¯¹äºä¸­æ–‡å·¥ä½œç¯å¢ƒçš„æ´å¯Ÿä¹Ÿååˆ†æœ‰é™ã€‚æœ¬ç ”ç©¶åˆ©ç”¨èµ„æ ¼è€ƒè¯•ä½œä¸ºäººç±»ä¸“ä¸šè¯„ä¼°çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ¨å‡ºQualBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ä¸­æ–‡LLMçš„è·¨é¢†åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ã€‚æ•°æ®é›†åŒ…å«è¶…è¿‡17ï¼Œ000ä¸ªè·¨è¶Šå…­ä¸ªå‚ç›´é¢†åŸŸçš„é—®é¢˜ï¼Œæ•°æ®é€‰æ‹©åŸºäº24é¡¹ä¸­å›½èµ„æ ¼è®¤è¯ï¼Œä¸å›½å®¶æ”¿ç­–å’Œè¡Œä¸šæ ‡å‡†ç´§å¯†å¯¹é½ã€‚é€šè¿‡å¯¹Qwen2.5æ¨¡å‹çš„å…¨é¢è¯„ä¼°ï¼Œå‘ç°å…¶è¡¨ç°ä¼˜äºæ›´å…ˆè¿›çš„GPT-4oæ¨¡å‹ï¼Œä¸”ä¸­æ–‡LLMåœ¨åˆæ ¼è¦æ±‚æ–¹é¢æŒç»­è¶…è¶Šéä¸­æ–‡æ¨¡å‹ï¼Œçªæ˜¾æœ¬åœ°åŒ–é¢†åŸŸçŸ¥è¯†çš„é‡è¦æ€§ã€‚æœ€ä½³æ€§èƒ½ä¸º75.26%ï¼Œæ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›åœ¨é¢†åŸŸè¦†ç›–æ–¹é¢çš„å½“å‰å·®è·ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†LLMä¸ä¼—åŒ…æœºåˆ¶çš„åä½œå¤±è´¥åŸå› ï¼Œå¹¶æå‡ºäº†åˆ©ç”¨è”é‚¦å­¦ä¹ è¿›è¡Œå¤šé¢†åŸŸRAGçŸ¥è¯†å¢å¼ºå’Œå‚ç›´é¢†åŸŸLLMè®­ç»ƒçš„æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­å›½LLMçš„å¿«é€Ÿå‘å±•éœ€è¦é¢†åŸŸç‰¹å®šçš„è¯„ä¼°ä»¥ç¡®ä¿å…¶åœ¨ä¸åŒé¢†åŸŸä¸­çš„å¯é åº”ç”¨ã€‚</li>
<li>ç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨å‚ç›´é¢†åŸŸçš„è¦†ç›–æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¹äºä¸­æ–‡å·¥ä½œç¯å¢ƒçš„æ´å¯Ÿæœ‰é™ã€‚</li>
<li>QualBenchæ˜¯é¦–ä¸ªè·¨é¢†åŸŸçš„ä¸­æ–‡é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMçš„æœ¬åœ°åŒ–èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šä¸ªå‚ç›´é¢†åŸŸçš„é—®é¢˜ï¼Œæ•°æ®é€‰æ‹©åŸºäºä¸­å›½èµ„æ ¼è®¤è¯ï¼Œä¸å›½å®¶æ”¿ç­–å’Œè¡Œä¸šæ ‡å‡†å¯¹é½ã€‚</li>
<li>Qwen2.5æ¨¡å‹åœ¨è¡¨ç°ä¸Šä¼˜äºGPT-4oç­‰æ›´å…ˆè¿›çš„æ¨¡å‹ï¼Œçªæ˜¾æœ¬åœ°åŒ–é¢†åŸŸçŸ¥è¯†çš„é‡è¦æ€§ã€‚</li>
<li>ä¸­æ–‡LLMåœ¨åˆæ ¼è¦æ±‚æ–¹é¢æŒç»­è¶…è¶Šéä¸­æ–‡æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-142b8343054b7cbd4707366bbf7a1498.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8368b3f6a67f2ea5d2c73af32055a8f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf1e205fa9a796dbd770cf767b1fc9dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60b8ba8dd7f0c5bd5e6af7c79d59df55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-743ab29ea475f3a95cfe4a160bf30729.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lay-Your-Scene-Natural-Scene-Layout-Generation-with-Diffusion-Transformers"><a href="#Lay-Your-Scene-Natural-Scene-Layout-Generation-with-Diffusion-Transformers" class="headerlink" title="Lay-Your-Scene: Natural Scene Layout Generation with Diffusion   Transformers"></a>Lay-Your-Scene: Natural Scene Layout Generation with Diffusion   Transformers</h2><p><strong>Authors:Divyansh Srivastava, Xiang Zhang, He Wen, Chenru Wen, Zhuowen Tu</strong></p>
<p>We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Lay-Your-Sceneï¼ˆç®€ç§°LayouSynï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè‡ªç„¶åœºæ™¯çš„æ–°å‹æ–‡æœ¬åˆ°å¸ƒå±€ç”Ÿæˆç®¡é“ã€‚å…ˆå‰çš„åœºæ™¯å¸ƒå±€ç”Ÿæˆæ–¹æ³•è¦ä¹ˆæ˜¯å°é—­è¯æ±‡è¡¨ï¼Œè¦ä¹ˆä½¿ç”¨ä¸“æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼€æ”¾è¯æ±‡è¡¨ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†å…¶å»ºæ¨¡èƒ½åŠ›å’Œåœ¨å¯æ§å›¾åƒç”Ÿæˆä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨è½»å‹å¼€æºè¯­è¨€æ¨¡å‹ä»æ–‡æœ¬æç¤ºä¸­è·å¾—åœºæ™¯å…ƒç´ ï¼Œä»¥åŠä¸€ç§ä»¥å¼€æ”¾è¯æ±‡è¡¨æ–¹å¼è®­ç»ƒçš„æ–°å‹æ–¹é¢æ„ŸçŸ¥æ‰©æ•£Transformeræ¶æ„ï¼Œç”¨äºæ¡ä»¶å¸ƒå±€ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLayouSynä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºé—´å’Œæ•°å€¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†LayouSynçš„ä¸¤ä¸ªåº”ç”¨ç¨‹åºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²—ç•¥åˆå§‹åŒ–å¯ä»¥æ— ç¼åœ°ä¸æˆ‘ä»¬çš„æ–¹æ³•ç›¸ç»“åˆï¼Œä»¥å–å¾—æ›´å¥½çš„ç»“æœã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç”¨äºå‘å›¾åƒæ·»åŠ å¯¹è±¡çš„ç®¡é“ï¼Œè¿™ä½“ç°äº†LayouSynåœ¨å›¾åƒç¼–è¾‘åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04718v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LayouSynæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬åˆ°è‡ªç„¶åœºæ™¯å¸ƒå±€ç”Ÿæˆç®¡é“ï¼Œå®ƒä½¿ç”¨è½»é‡çº§å¼€æºè¯­è¨€æ¨¡å‹è·å–åœºæ™¯å…ƒç´ å’Œæ–°å‹é¢å‘æ–¹é¢çš„æ‰©æ•£Transformeræ¶æ„è¿›è¡Œå¼€æ”¾å¼è¯æ±‡è¡¨æ¡ä»¶ä¸‹çš„å¸ƒå±€ç”Ÿæˆã€‚è¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜çš„ç©ºé—´å’Œæ•°å€¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•å¹¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼ŒLayouSynè¿˜æä¾›äº†ä¸¤ä¸ªåº”ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²—ç•¥åˆå§‹åŒ–ç›¸ç»“åˆå®ç°æ›´å¥½çš„ç»“æœï¼Œä»¥åŠåœ¨å›¾åƒç¼–è¾‘åº”ç”¨ä¸­æ·»åŠ å¯¹è±¡çš„ç®¡é“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LayouSynæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬åˆ°è‡ªç„¶åœºæ™¯å¸ƒå±€ç”Ÿæˆç®¡é“ã€‚</li>
<li>å®ƒå…‹æœäº†ä»¥å¾€åœºæ™¯å¸ƒå±€ç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ï¼Œä½¿ç”¨è½»é‡çº§å¼€æºè¯­è¨€æ¨¡å‹è·å–åœºæ™¯å…ƒç´ ã€‚</li>
<li>LayouSyné‡‡ç”¨å¼€æ”¾å¼è¯æ±‡è¡¨æ–¹å¼è®­ç»ƒï¼Œå¢å¼ºäº†å…¶å»ºæ¨¡èƒ½åŠ›å’Œæ›´å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
<li>åœ¨ç©ºé—´å’Œæ•°å€¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLayouSynè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>LayouSynå¯ä»¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²—ç•¥åˆå§‹åŒ–ç›¸ç»“åˆï¼Œå®ç°æ›´å¥½çš„ç»“æœã€‚</li>
<li>LayouSynæä¾›äº†åœ¨å›¾åƒç¼–è¾‘åº”ç”¨ä¸­æ·»åŠ å¯¹è±¡çš„ç®¡é“ï¼Œå±•ç¤ºäº†å…¶åœ¨å›¾åƒç¼–è¾‘ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4082f06bdece543ee4c02ce51a624903.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51d169deddaf63e529efc661ae62345f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b914fc2f7a69b26afd7537a4ef5c6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd858c5da0db9eece639018cb5440f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b82248d04b9cbbd1e765776c08a8e66b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2f6dedeffbc4d437b7524f579b2282b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation"><a href="#SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation" class="headerlink" title="SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation"></a>SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation</h2><p><strong>Authors:Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song</strong></p>
<p>Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan â€“ a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics. </p>
<blockquote>
<p>åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡ã€åŠ¨æ€ç¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„é«˜è®¡ç®—æˆæœ¬å’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰é™é€‚åº”æ€§é˜»ç¢äº†å®ƒä»¬åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶éƒ¨ç½²ã€‚æˆ‘ä»¬æå‡ºäº†SmallPlanâ€”â€”ä¸€ä¸ªåˆ©ç”¨LLMä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ–°å‹æ¡†æ¶ã€‚åœ¨SmallPlanä¸­ï¼ŒSLMæä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—ï¼Œä»¥åœ¨åœºæ™¯å›¾ä¸­è¿›è¡Œå¯¼èˆªï¼Œè¿™äº›åœºæ™¯å›¾ç´§å‡‘åœ°ä»£è¡¨å…¨å°ºå¯¸3Dåœºæ™¯ã€‚SLMä»¥æ¨¡æ‹Ÿé©±åŠ¨çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡LLMæŒ‡å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œäº¤æ›¿è®­ç»ƒã€‚è¿™ç§ç­–ç•¥ä¸ä»…ä½¿SLMèƒ½å¤ŸæˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜ä½¿å®ƒä»¬æ„è¯†åˆ°æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ç»è¿‡å¾®è°ƒçš„SLMåœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢çš„è¡¨ç°ä¸GPT-4oç­‰å¤§å‹æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”ä¸å­˜åœ¨å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆçš„é—®é¢˜ã€‚SmallPlanèµ„æºæ•ˆç‡é«˜ï¼Œéå¸¸é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ï¼Œæœ‰åŠ©äºæ¨åŠ¨å®ç”¨å‹è‡ªä¸»æœºå™¨äººçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00831v3">PDF</a> Paper is under review</p>
<p><strong>Summary</strong></p>
<p>é«˜æ•ˆè·¯å¾„è§„åˆ’åœ¨æœºå™¨äººé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡åŠ¨æ€ç¯å¢ƒä¸­ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶SmallPlanï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè®­ç»ƒè½»é‡çº§å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ•™å¸ˆæ¨¡å‹ï¼Œç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡ã€‚SmallPlanä¸­çš„SLMèƒ½å¤Ÿåœ¨ç´§å‡‘è¡¨ç¤ºå…¨è§„æ¨¡3Dåœºæ™¯çš„åœºæ™¯å›¾ä¸­æä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—è¿›è¡Œå¯¼èˆªã€‚SLMé€šè¿‡æ¨¡æ‹Ÿè®­ç»ƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼çš„ç›‘ç£å’Œå¼ºåŒ–è®­ç»ƒç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–¹æ³•ä¸ä»…ä½¿SLMèƒ½å¤ŸæˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜èƒ½ä½¿å…¶äº†è§£æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹åœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢çš„è¡¨ç°ä¸GPTç­‰å¤§å‹æ¨¡å‹ç›¸å½“ï¼Œä¸”ä¸å­˜åœ¨å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚SmallPlanå…·æœ‰èµ„æºé«˜æ•ˆæ€§ï¼Œéå¸¸é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä¸ºå®é™…çš„è‡ªä¸»æœºå™¨äººæŠ€æœ¯æä¾›äº†æ¨åŠ¨åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨äººè·¯å¾„è§„åˆ’ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬å’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰é™é€‚åº”æ€§é™åˆ¶äº†å…¶åœ¨å®æ—¶éƒ¨ç½²ä¸­çš„åº”ç”¨ã€‚</li>
<li>SmallPlanæ¡†æ¶æå‡ºåˆ©ç”¨LLMä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒè½»é‡çº§å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼Œç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡ã€‚</li>
<li>SLMèƒ½å¤Ÿåœ¨ç´§å‡‘è¡¨ç¤ºå…¨è§„æ¨¡3Dåœºæ™¯çš„åœºæ™¯å›¾ä¸­æä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—è¿›è¡Œå¯¼èˆªã€‚</li>
<li>SLMçš„è®­ç»ƒç»“åˆäº†æ¨¡æ‹Ÿè®­ç»ƒã€å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼çš„ç›‘ç£å’Œå¼ºåŒ–è®­ç»ƒã€‚</li>
<li>è®­ç»ƒåçš„SLMä¸ä»…æˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜è€ƒè™‘æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„SLMåœ¨åºåˆ—è·¯å¾„è§„åˆ’ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä¸å¤§å‹æ¨¡å‹å¦‚GPT-4ç›¸å½“ï¼Œä¸”ä¸å­˜åœ¨å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-25f8367d9c81f38282b4ddd803eab39a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d4bfd2544253742e0f454092cee4cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8128331a572e8ccf69601b9baaf08878.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Generating-Symbolic-World-Models-via-Test-time-Scaling-of-Large-Language-Models"><a href="#Generating-Symbolic-World-Models-via-Test-time-Scaling-of-Large-Language-Models" class="headerlink" title="Generating Symbolic World Models via Test-time Scaling of Large Language   Models"></a>Generating Symbolic World Models via Test-time Scaling of Large Language   Models</h2><p><strong>Authors:Zhouliang Yu, Yuhuan Yuan, Tim Z. Xiao, Fuxiang Frank Xia, Jie Fu, Ge Zhang, Ge Lin, Weiyang Liu</strong></p>
<p>Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domains, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks. </p>
<blockquote>
<p>è§£å†³å¤æ‚çš„è§„åˆ’é—®é¢˜éœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾å¼åœ°å»ºæ¨¡çŠ¶æ€è½¬æ¢ï¼Œä»¥é¿å…è¿åè§„åˆ™ã€éµå®ˆçº¦æŸå¹¶ç¡®ä¿æœ€ä¼˜æ€§ï¼Œè¿™æ˜¯ä¸€é¡¹å› è‡ªç„¶è¯­è¨€å›ºæœ‰çš„æ¨¡ç³Šæ€§è€Œå—åˆ°é˜»ç¢çš„ä»»åŠ¡ã€‚ä¸ºäº†å…‹æœè¿™ç§æ¨¡ç³Šæ€§ï¼Œè§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰è¢«ç”¨ä½œä¸€ç§è§„åˆ’æŠ½è±¡ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®å’Œæ­£å¼çš„çŠ¶æ€æè¿°ã€‚ä½¿ç”¨PDDLï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹ï¼Œå…¶ä¸­ç»å…¸çš„æœç´¢ç®—æ³•ï¼ˆå¦‚A*ï¼‰å¯ä»¥æ— ç¼åœ°åº”ç”¨äºå¯»æ‰¾æœ€ä¼˜è®¡åˆ’ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘PDDLè®­ç»ƒæ•°æ®ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥ç”ŸæˆPDDLé¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºæ‰©å¤§å¤§å‹è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶é—´è®¡ç®—è§„æ¨¡ï¼Œä»¥å¢å¼ºå…¶PDDLæ¨ç†èƒ½åŠ›ï¼Œä»è€Œå®ç°é«˜è´¨é‡PDDLé¢†åŸŸçš„ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç®—æ³•ï¼Œè¯¥ç®—æ³•é¦–å…ˆé‡‡ç”¨â€œæœ€ä½³Né€‰â€é‡‡æ ·æ–¹æ³•æé«˜åˆå§‹è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼Œç„¶åä»¥ç²¾ç»†çš„æ–¹å¼é€šè¿‡å£å¤´åŒ–çš„æœºå™¨å­¦ä¹ å®Œå–„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”ŸæˆPDDLé¢†åŸŸæ–¹é¢è¿œè¶…o1-miniï¼Œåœ¨ä¸¤ä¸ªä»»åŠ¡ï¼ˆå³ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆPDDLé¢†åŸŸæˆ–PDDLé—®é¢˜ï¼‰ä¸Šçš„æˆåŠŸç‡è¶…è¿‡5,ä¸”ä¸æ— éœ€é¢å¤–è®­ç»ƒå³å¯å®Œæˆè¿™é¡¹å·¥ä½œç›¸æ¯”æœ‰æ‰€ä¸Šå‡ã€‚åˆ©ç”¨PDDLä½œä¸ºçŠ¶æ€æŠ½è±¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å‡ ä¹æ‰€æœ‰ç«èµ›çº§åˆ«çš„è§„åˆ’ä»»åŠ¡ä¸Šä¼˜äºç›®å‰æœ€å…ˆè¿›çš„æ–¹è‡´å¤–å¼ºç§è¯´èƒœå…¶å®ƒèƒœæœ€ä¸ºä¸Šä¹˜çš„ç§æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04728v2">PDF</a> Accepted by TMLR2025 (32 pages, 6 figures)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è§£å†³å¤æ‚è§„åˆ’é—®é¢˜éœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾å¼å»ºæ¨¡çŠ¶æ€è½¬æ¢ä»¥éµå®ˆè§„åˆ™ã€é¿å…è¿è§„å¹¶è¾¾åˆ°æœ€ä¼˜çŠ¶æ€ã€‚ç”±äºè‡ªç„¶è¯­è¨€å›ºæœ‰çš„æ­§ä¹‰æ€§ï¼Œè¿™ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚ä¸ºå…‹æœè¿™ä¸€éš¾é¢˜ï¼Œåˆ©ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ä½œä¸ºè§„åˆ’æŠ½è±¡ï¼Œå®ç°ç²¾ç¡®å’Œæ­£å¼çš„çŠ¶æ€æè¿°ã€‚é€šè¿‡PDDLï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹ï¼Œåœ¨è¯¥æ¨¡å‹ä¸­å¯æ— ç¼åº”ç”¨ç»å…¸æœç´¢ç®—æ³•ï¼ˆå¦‚A *ç®—æ³•ï¼‰ä»¥æ‰¾åˆ°æœ€ä¼˜è®¡åˆ’ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘PDDLè®­ç»ƒæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨LLMç”ŸæˆPDDLé¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºæ‰©å¤§LLMçš„æµ‹è¯•æ—¶é—´è®¡ç®—ä»¥å¢å¼ºå…¶PDDLæ¨ç†èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„PDDLé¢†åŸŸã€‚é€šè¿‡é‡‡ç”¨ç®€å•æœ‰æ•ˆçš„ç®—æ³•ï¼Œé¦–å…ˆä½¿ç”¨æœ€ä½³Né‡‡æ ·æ–¹æ³•æé«˜åˆå§‹è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼Œç„¶åé€šè¿‡ç²¾ç»†åŒ–çš„æœºå™¨å­¦ä¹ è¿›ä¸€æ­¥ç»†åŒ–è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”ŸæˆPDDLé¢†åŸŸæ–¹é¢è¿œè¶…o1-miniåŸºå‡†çº¿ï¼Œåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡è¶…è¿‡50%ï¼Œå¹¶ä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚å€ŸåŠ©PDDLä½œä¸ºçŠ¶æ€æŠ½è±¡å·¥å…·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡ ä¹åœ¨æ‰€æœ‰ç«èµ›çº§åˆ«çš„è§„åˆ’ä»»åŠ¡ä¸Šéƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§£å†³å¤æ‚è§„åˆ’é—®é¢˜éœ€è¦LLMæ˜¾å¼å»ºæ¨¡çŠ¶æ€è½¬æ¢ã€‚</li>
<li>è‡ªç„¶è¯­è¨€çš„æ­§ä¹‰æ€§æ˜¯LLMé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>PDDLä½œä¸ºè§„åˆ’æŠ½è±¡ï¼Œå¯å®ç°ç²¾ç¡®å’Œæ­£å¼çš„çŠ¶æ€æè¿°ã€‚</li>
<li>åˆ©ç”¨PDDLç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹å¯åº”ç”¨ç»å…¸æœç´¢ç®—æ³•æ‰¾åˆ°æœ€ä¼˜è®¡åˆ’ã€‚</li>
<li>å½“å‰LLMé¢ä¸´ç¼ºä¹PDDLè®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ‰©å¤§LLMæµ‹è¯•æ—¶é—´è®¡ç®—ä»¥æé«˜å…¶PDDLæ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bf41e13075ce1b5be750e80b617756a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9675dbc07d776d598498e735ec452a05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82977139f30b83292f861973d2bb96a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eb4e8ac90f0627e0a6ab37f0e73425a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant"><a href="#Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant" class="headerlink" title="Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant"></a>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant</h2><p><strong>Authors:Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon</strong></p>
<p>Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a modelâ€™s inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in coding and STEM tasks, though reasoning may sometimes improve. </p>
<blockquote>
<p>é‡åŒ–æŠ€æœ¯ä½œä¸ºéƒ¨ç½²å¤§å°è¯­è¨€æ¨¡å‹çš„ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œå·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œä»…é™äºå›°æƒ‘åº¦æˆ–åŸºæœ¬çŸ¥è¯†ä»»åŠ¡ï¼Œç¼ºä¹å¯¹åƒLlama-3.3è¿™æ ·çš„æœ€æ–°æ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æŒ‡ä»¤å¾®è°ƒæ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹å‚æ•°èŒƒå›´ä»1Båˆ°405Bï¼Œåº”ç”¨å››ç§é‡åŒ–æ–¹æ³•ï¼Œæ¶‰åŠ13ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰é‡åŒ–æ¨¡å‹é€šå¸¸è¶…è¿‡è¾ƒå°çš„FP16åŸºå‡†æµ‹è¯•ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤å’Œå¹»è§‰æ£€æµ‹æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ï¼›ï¼ˆ2ï¼‰FP8åœ¨å„é¡¹ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°ä¸ºæœ€ç¨³å¥çš„é€‰æ‹©ï¼ŒAWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢å€¾å‘äºä¼˜äºGPTQï¼›ï¼ˆ3ï¼‰è¾ƒå°æ¨¡å‹åœ¨4ä½é‡åŒ–æ—¶å¯èƒ½ä¼šé­å—ä¸¥é‡çš„ç²¾åº¦ä¸‹é™ï¼Œè€Œ70Bè§„æ¨¡æ¨¡å‹åˆ™èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼›ï¼ˆ4ï¼‰å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¹¶éæ‰€æœ‰å›°éš¾ä»»åŠ¡éƒ½ä¼šé­å—æœ€å¤§çš„ç²¾åº¦æŸå¤±ï¼Œè¿™è¡¨æ˜é‡åŒ–ä¼šæ”¾å¤§æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¸ä»»åŠ¡éš¾åº¦ç›¸å…³ï¼›ï¼ˆ5ï¼‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­ï¼ˆMT-Benchï¼‰çªæ˜¾å‡ºåœ¨ç¼–ç å’ŒSTEMä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°½ç®¡æœ‰æ—¶æ¨ç†èƒ½åŠ›å¯èƒ½ä¼šæé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11055v3">PDF</a> Accepted in IJCAI 2025, 21 pages, 2 figure</p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d34c6b4b68fff046c726eb8d1d414f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c6953c0fd643e48a949e3db7843ca22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f17fa72f0fc37134b5f42fdaff94fab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6748fa4dc153505de77db20a110c062f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Enhancing-Differential-Testing-With-LLMs-For-Testing-Deep-Learning-Libraries"><a href="#Enhancing-Differential-Testing-With-LLMs-For-Testing-Deep-Learning-Libraries" class="headerlink" title="Enhancing Differential Testing With LLMs For Testing Deep Learning   Libraries"></a>Enhancing Differential Testing With LLMs For Testing Deep Learning   Libraries</h2><p><strong>Authors:Meiziniu Li, Dongze Li, Jianmeng Liu, Jialun Cao, Yongqiang Tian, Shing-Chi Cheung</strong></p>
<p>Differential testing offers a promising strategy to alleviate the test oracle problem by comparing the test results between alternative implementations. However, existing differential testing techniques for deep learning (DL) libraries are limited by the key challenges of finding alternative implementations (called counterparts) for a given API and subsequently generating diverse test inputs. To address the two challenges, this paper introduces DLLens, an LLM-enhanced differential testing technique for DL libraries. To address the first challenge, DLLens incorporates an LLM-based counterpart synthesis workflow, with the insight that the counterpart of a given DL library APIâ€™s computation could be successfully synthesized through certain composition and adaptation of the APIs from another DL library. To address the second challenge, DLLens incorporates a static analysis technique that extracts the path constraints from the implementations of a given API and its counterpart to guide diverse test input generation. The extraction is facilitated by LLMâ€™s knowledge of the concerned DL library and its upstream libraries.   We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our evaluation shows that DLLens synthesizes counterparts for 1.84 times as many APIs as those found by state-of-the-art techniques on these libraries. Moreover, under the same time budget, DLLens covers 7.23% more branches and detects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly sampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and PyTorch libraries. Among them, 59 are confirmed by developers, including 46 confirmed as previously unknown bugs, and 10 of these previously unknown bugs have been fixed in the latest version of TensorFlow and PyTorch. </p>
<blockquote>
<p>å·®åˆ†æµ‹è¯•é€šè¿‡æ¯”è¾ƒä¸åŒå®ç°çš„æµ‹è¯•ç»“æœä¸ºè§£å†³æµ‹è¯•é¢„è¨€é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åº“çš„å·®åˆ†æµ‹è¯•æŠ€æœ¯åœ¨å¯»æ‰¾ç»™å®šAPIçš„æ›¿ä»£å®ç°ï¼ˆç§°ä¸ºå¯¹åº”ç‰©ï¼‰ä»¥åŠéšåç”Ÿæˆå¤šæ ·åŒ–çš„æµ‹è¯•è¾“å…¥æ–¹é¢å­˜åœ¨å…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†DLLensï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºDLåº“çš„å¢å¼ºå‹LLMå·®åˆ†æµ‹è¯•æŠ€æœ¯ã€‚ä¸ºäº†è§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼ŒDLLensé‡‡ç”¨äº†åŸºäºLLMçš„å¯¹åº”ç‰©åˆæˆæµç¨‹ï¼Œå…¶è§è§£æ˜¯ï¼Œç»™å®šDLåº“APIçš„è®¡ç®—çš„å¯¹åº”ç‰©å¯ä»¥é€šè¿‡å…¶ä»–DLåº“APIçš„æŸç§ç»„åˆå’Œé€‚åº”æ¥æˆåŠŸåˆæˆã€‚ä¸ºäº†è§£å†³ç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼ŒDLLensé‡‡ç”¨äº†ä¸€ç§é™æ€åˆ†ææŠ€æœ¯ï¼Œå®ƒä»ç»™å®šAPIåŠå…¶å¯¹åº”ç‰©çš„å®ç°ä¸­æå–è·¯å¾„çº¦æŸï¼Œä»¥æŒ‡å¯¼å¤šæ ·åŒ–çš„æµ‹è¯•è¾“å…¥ç”Ÿæˆã€‚æå–å·¥ä½œå¾—ç›ŠäºLLMå¯¹æœ‰å…³DLåº“åŠå…¶ä¸Šæ¸¸åº“çš„çŸ¥è¯†ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæµè¡Œçš„DLåº“TensorFlowå’ŒPyTorchä¸Šè¯„ä¼°äº†DLLensã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒDLLensåˆæˆçš„å¯¹åº”ç‰©æ˜¯ç°æœ‰æŠ€æœ¯åœ¨è¿™äº›åº“ä¸­æ‰¾åˆ°çš„APIçš„1.84å€ã€‚æ­¤å¤–ï¼Œåœ¨ç›¸åŒçš„æ—¶é—´é¢„ç®—ä¸‹ï¼ŒDLLensè¦†ç›–çš„åˆ†æ”¯æ›´å¤šï¼ˆå¤šå‡º7.23%ï¼‰ï¼Œå¹¶ä¸”åœ¨200ä¸ªéšæœºé‡‡æ ·çš„APIä¸Šæ£€æµ‹åˆ°çš„é”™è¯¯æ˜¯ç°æœ‰æŠ€æœ¯çš„1.88å€ã€‚DLLenså·²æˆåŠŸæ£€æµ‹åˆ°TensorFlowå’ŒPyTorchåº“ä¸­çš„71ä¸ªé”™è¯¯ã€‚å…¶ä¸­ï¼Œå¼€å‘è€…ç¡®è®¤çš„59ä¸ªé”™è¯¯ä¸­ï¼Œæœ‰46ä¸ªæ˜¯ä»¥å‰æœªçŸ¥çš„é”™è¯¯ï¼Œå…¶ä¸­10ä¸ªå·²åœ¨TensorFlowå’ŒPyTorchçš„æœ€æ–°ç‰ˆæœ¬ä¸­ä¿®å¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07944v2">PDF</a> This work has been accepted by ACM TOSEM. Manuscript under final   preparation</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDLLensçš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åº“å·®å¼‚æµ‹è¯•æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³æµ‹è¯•é¢„è¨€é—®é¢˜ã€‚DLLensç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè§£å†³ç°æœ‰å·®å¼‚æµ‹è¯•æŠ€æœ¯çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šå¯»æ‰¾ç»™å®šAPIçš„æ›¿ä»£å®ç°ï¼ˆç§°ä¸ºå¯¹åº”ç‰©ï¼‰ä»¥åŠéšåç”Ÿæˆå¤šæ ·åŒ–çš„æµ‹è¯•è¾“å…¥ã€‚ä¸ºè§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼ŒDLLensé‡‡ç”¨åŸºäºLLMçš„å¯¹åº”ç‰©åˆæˆæµç¨‹ï¼Œé€šè¿‡åˆæˆå…¶ä»–DLåº“APIçš„ç»„åˆå’Œé€‚åº”æ¥æˆåŠŸåˆæˆç»™å®šDLåº“APIçš„å¯¹åº”ç‰©ã€‚å¯¹äºç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼ŒDLLensé‡‡ç”¨é™æ€åˆ†ææŠ€æœ¯ï¼Œä»ç»™å®šAPIåŠå…¶å¯¹åº”ç‰©çš„å®ç°ä¸­æå–è·¯å¾„çº¦æŸï¼Œä»¥æŒ‡å¯¼å¤šæ ·åŒ–çš„æµ‹è¯•è¾“å…¥ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨TensorFlowå’ŒPyTorchè¿™ä¸¤ä¸ªæµè¡Œçš„DLåº“ä¸Šè¯„ä¼°äº†DLLensã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDLLensåˆæˆçš„å¯¹åº”ç‰©æ˜¯ç°æœ‰æŠ€æœ¯åœ¨è¿™äº›åº“ä¸Šæ‰¾åˆ°çš„APIçš„1.84å€ã€‚æ­¤å¤–ï¼Œåœ¨ç›¸åŒçš„æ—¶é—´é¢„ç®—ä¸‹ï¼ŒDLLensè¦†ç›–çš„åˆ†æ”¯å¢åŠ äº†7.23%ï¼Œæ£€æµ‹åˆ°çš„é”™è¯¯æ˜¯ç°æœ‰æŠ€æœ¯çš„1.88å€ã€‚DLLensæˆåŠŸæ£€æµ‹åˆ°TensorFlowå’ŒPyTorchåº“ä¸­çš„71ä¸ªé”™è¯¯ï¼Œå…¶ä¸­59ä¸ªå·²å¾—åˆ°å¼€å‘è€…ç¡®è®¤ï¼ŒåŒ…æ‹¬46ä¸ªä¹‹å‰æœªçŸ¥çš„é”™è¯¯ã€‚å…¶ä¸­10ä¸ªä¹‹å‰æœªçŸ¥é”™è¯¯å·²åœ¨TensorFlowå’ŒPyTorchçš„æœ€æ–°ç‰ˆæœ¬ä¸­å¾—åˆ°ä¿®å¤ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å·®åˆ†æµ‹è¯•æ˜¯ä¸€ç§è§£å†³æµ‹è¯•é¢„è¨€é—®é¢˜çš„æœ‰å‰é€”çš„ç­–ç•¥ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒå®ç°çš„ç»“æœæ¥æ£€æµ‹é”™è¯¯ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åº“çš„å·®å¼‚æµ‹è¯•æŠ€æœ¯é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¯»æ‰¾ç»™å®šAPIçš„æ›¿ä»£å®ç°ï¼ˆå¯¹åº”ç‰©ï¼‰å’Œç”Ÿæˆå¤šæ ·åŒ–çš„æµ‹è¯•è¾“å…¥ã€‚</li>
<li>DLLensæ˜¯ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·®å¼‚æµ‹è¯•æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>DLLensé€šè¿‡ç»“åˆLLMï¼Œè§£å†³äº†å¯¹åº”ç‰©çš„åˆæˆé—®é¢˜ï¼Œå¹¶å¯ä»¥é€šè¿‡é™æ€åˆ†ææŠ€æœ¯ç”Ÿæˆå¤šæ ·åŒ–çš„æµ‹è¯•è¾“å…¥ã€‚</li>
<li>åœ¨TensorFlowå’ŒPyTorchä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDLLensåœ¨åˆæˆå¯¹åº”ç‰©ã€è¦†ç›–åˆ†æ”¯å’Œæ£€æµ‹é”™è¯¯æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>DLLensæˆåŠŸæ£€æµ‹åˆ°71ä¸ªé”™è¯¯ï¼Œå…¶ä¸­åŒ…æ‹¬ä¹‹å‰æœªçŸ¥çš„46ä¸ªé”™è¯¯ï¼Œå·²æœ‰éƒ¨åˆ†å¾—åˆ°å¼€å‘è€…ç¡®è®¤å¹¶åœ¨æœ€æ–°ç‰ˆæœ¬ä¸­å¾—åˆ°ä¿®å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59afb5ad9d7007f05c9cba3ea10e6f2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17d17a6fddda24ba7c7be9f39bbe5bdb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6cd858a7d58c219ebbe442bd4dc8c874.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  HEXGEN-TEXT2SQL Optimizing LLM Inference Request Scheduling for Agentic   Text-to-SQL Workflow
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-17a73e965d07f8fa2171bfd6ebc521ee.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  Bring Reason to Vision Understanding Perception and Reasoning through   Model Merging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
