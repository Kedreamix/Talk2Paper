<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  Bring Reason to Vision Understanding Perception and Reasoning through   Model Merging">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-17a73e965d07f8fa2171bfd6ebc521ee.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-10-æ›´æ–°"><a href="#2025-05-10-æ›´æ–°" class="headerlink" title="2025-05-10 æ›´æ–°"></a>2025-05-10 æ›´æ–°</h1><h2 id="Bring-Reason-to-Vision-Understanding-Perception-and-Reasoning-through-Model-Merging"><a href="#Bring-Reason-to-Vision-Understanding-Perception-and-Reasoning-through-Model-Merging" class="headerlink" title="Bring Reason to Vision: Understanding Perception and Reasoning through   Model Merging"></a>Bring Reason to Vision: Understanding Perception and Reasoning through   Model Merging</h2><p><strong>Authors:Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, Junxian He</strong></p>
<p>Vision-Language Models (VLMs) combine visual perception with the general capabilities, such as reasoning, of Large Language Models (LLMs). However, the mechanisms by which these two abilities can be combined and contribute remain poorly understood. In this work, we explore to compose perception and reasoning through model merging that connects parameters of different models. Unlike previous works that often focus on merging models of the same kind, we propose merging models across modalities, enabling the incorporation of the reasoning capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate that model merging offers a successful pathway to transfer reasoning abilities from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged models to understand the internal mechanism of perception and reasoning and how merging affects it. We find that perception capabilities are predominantly encoded in the early layers of the model, whereas reasoning is largely facilitated by the middle-to-late layers. After merging, we observe that all layers begin to contribute to reasoning, whereas the distribution of perception abilities across layers remains largely unchanged. These observations shed light on the potential of model merging as a tool for multimodal integration and interpretation. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†ç­‰ä¸€èˆ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›å¦‚ä½•ç»“åˆä»¥åŠå®ƒä»¬å¦‚ä½•å…±åŒå‘æŒ¥ä½œç”¨ï¼Œå…¶æœºåˆ¶ä»æœªèƒ½è¢«å……åˆ†ç†è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡æ¨¡å‹åˆå¹¶æ¥ç»„åˆæ„ŸçŸ¥å’Œæ¨ç†ï¼Œè¯¥æ–¹å¼è¿æ¥ä¸åŒæ¨¡å‹çš„å‚æ•°ã€‚ä¸åŒäºä¹‹å‰å¸¸å¸¸å…³æ³¨ç›¸åŒç±»å‹æ¨¡å‹çš„åˆå¹¶ï¼Œæˆ‘ä»¬æå‡ºè·¨æ¨¡æ€çš„æ¨¡å‹åˆå¹¶ï¼Œä½¿å¾—èƒ½å¤Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›èå…¥è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨¡å‹åˆå¹¶æ˜¯æ— éœ€è®­ç»ƒå³å¯å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸€æ¡æˆåŠŸè·¯å¾„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆå¹¶åçš„æ¨¡å‹æ¥ç†è§£æ„ŸçŸ¥å’Œæ¨ç†çš„å†…åœ¨æœºåˆ¶ä»¥åŠåˆå¹¶å¯¹å…¶äº§ç”Ÿçš„å½±å“ã€‚æˆ‘ä»¬å‘ç°æ„ŸçŸ¥èƒ½åŠ›ä¸»è¦ç¼–ç åœ¨æ¨¡å‹çš„æ—©æœŸå±‚æ¬¡ä¸­ï¼Œè€Œæ¨ç†åˆ™ä¸»è¦ç”±ä¸­åæœŸå±‚æ¬¡ä¿ƒè¿›ã€‚åˆå¹¶åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰€æœ‰å±‚æ¬¡éƒ½å¼€å§‹å¯¹æ¨ç†åšå‡ºè´¡çŒ®ï¼Œè€Œæ„ŸçŸ¥èƒ½åŠ›åœ¨å±‚æ¬¡ä¸­çš„åˆ†å¸ƒåŸºæœ¬ä¿æŒä¸å˜ã€‚è¿™äº›è§‚å¯Ÿæ­ç¤ºäº†æ¨¡å‹åˆå¹¶ä½œä¸ºå¤šæ¨¡æ€é›†æˆå’Œè§£é‡Šçš„å·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05464v1">PDF</a> ICML 2025. Our code is publicly available at   <a target="_blank" rel="noopener" href="https://github.com/shiqichen17/VLM_Merging">https://github.com/shiqichen17/VLM_Merging</a></p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†ç­‰ä¸€èˆ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›å¦‚ä½•ç»“åˆå’Œè´¡çŒ®çš„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶é€šè¿‡æ¨¡å‹åˆå¹¶æ¢ç´¢äº†æ„ŸçŸ¥å’Œæ¨ç†çš„ç»“åˆæ–¹å¼ï¼Œè¯¥æ–¹å¼è¿æ¥äº†ä¸åŒæ¨¡å‹çš„å‚æ•°ã€‚ä¸åŒäºä»¥å¾€å¤šé›†ä¸­åœ¨ç›¸åŒæ¨¡å‹åˆå¹¶çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€æ¨¡å‹åˆå¹¶çš„æ–¹æ³•ï¼Œä½¿LLMsçš„æ¨ç†èƒ½åŠ›å¯ä»¥èå…¥VLMsä¸­ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨¡å‹åˆå¹¶æ˜¯æ— éœ€è®­ç»ƒå³å¯å°†LLMsçš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°VLMsçš„æœ‰æ•ˆæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨åˆå¹¶åçš„æ¨¡å‹äº†è§£æ„ŸçŸ¥å’Œæ¨ç†çš„å†…åœ¨æœºåˆ¶ä»¥åŠåˆå¹¶å¯¹å…¶çš„å½±å“ã€‚å‘ç°æ„ŸçŸ¥èƒ½åŠ›ä¸»è¦ç¼–ç åœ¨æ¨¡å‹çš„æ—©æœŸå±‚æ¬¡ï¼Œè€Œæ¨ç†èƒ½åŠ›ä¸»è¦ç”±ä¸­åæœŸå±‚æ¬¡æ”¯æŒã€‚åˆå¹¶åï¼Œè§‚å¯Ÿåˆ°æ‰€æœ‰å±‚æ¬¡éƒ½å¼€å§‹å¯¹æ¨ç†åšå‡ºè´¡çŒ®ï¼Œè€Œæ„ŸçŸ¥èƒ½åŠ›çš„å±‚æ¬¡åˆ†å¸ƒåŸºæœ¬ä¿æŒä¸å˜ã€‚è¿™äº›è§‚å¯Ÿæ­ç¤ºäº†æ¨¡å‹åˆå¹¶ä½œä¸ºå¤šæ¨¡æ€é›†æˆå’Œè§£é‡Šå·¥å…·çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsç»“åˆè§†è§‰æ„ŸçŸ¥ä¸LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åˆå¹¶æ˜¯è·¨æ¨¡æ€è¿æ¥ä¸åŒæ¨¡å‹å‚æ•°çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åˆå¹¶å¯ä»¥ä½¿LLMsçš„æ¨ç†èƒ½åŠ›èå…¥VLMsä¸­ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜æ¨¡å‹åˆå¹¶æ— éœ€è®­ç»ƒå³å¯å®ç°èƒ½åŠ›è½¬ç§»ã€‚</li>
<li>æ„ŸçŸ¥èƒ½åŠ›ä¸»è¦åˆ†å¸ƒåœ¨æ¨¡å‹çš„æ—©æœŸå±‚æ¬¡ï¼Œè€Œæ¨ç†èƒ½åŠ›æ¶‰åŠä¸­åæœŸå±‚æ¬¡ã€‚</li>
<li>åˆå¹¶åæ‰€æœ‰å±‚æ¬¡çš„æ¨¡å‹éƒ½å¯¹æ¨ç†åšå‡ºè´¡çŒ®ï¼Œæ„ŸçŸ¥èƒ½åŠ›çš„å±‚æ¬¡åˆ†å¸ƒä¿æŒä¸å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4de5a3473a7206f292f526f756e08579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e1e586449520be8152e52ef2c2a4d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae00fe2dc9a6b56c4f88cad8e7539d53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8af089e4e123f464297932d72ef1ec56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc78149bb84fa27b2418cc9696cf5f79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c911444722fcfaad04548918231c8423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f8976e505d4acc8629ad15852390629.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reasoning-Models-Donâ€™t-Always-Say-What-They-Think"><a href="#Reasoning-Models-Donâ€™t-Always-Say-What-They-Think" class="headerlink" title="Reasoning Models Donâ€™t Always Say What They Think"></a>Reasoning Models Donâ€™t Always Say What They Think</h2><p><strong>Authors:Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez</strong></p>
<p>Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a modelâ€™s CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing modelsâ€™ actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨æä¾›äº†æ½œåœ¨çš„ç¦éŸ³ï¼Œå› ä¸ºå®ƒå…è®¸ç›‘æ§æ¨¡å‹çš„æ€ç»´é“¾ï¼Œä»¥è¯•å›¾ç†è§£å…¶æ„å›¾å’Œæ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ç§ç›‘æ§çš„æœ‰æ•ˆæ€§å–å†³äºæ€ç»´é“¾æ˜¯å¦çœŸå®åœ°ä»£è¡¨æ¨¡å‹çš„å®é™…æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å‰æ²¿æ¨ç†æ¨¡å‹çš„æ€ç»´é“¾å¿ å®åº¦ï¼Œæ¶µç›–äº†æç¤ºä¸­çš„6ä¸ªæ¨ç†æç¤ºï¼Œå¹¶å‘ç°ï¼š<br>ï¼ˆ1ï¼‰åœ¨å¤§å¤šæ•°æµ‹è¯•å’Œæ¨¡å‹æƒ…å†µä¸‹ï¼Œæ€ç»´é“¾æ­ç¤ºäº†å®ƒä»¬åœ¨è‡³å°‘1%çš„ç¤ºä¾‹ä¸­ä½¿ç”¨äº†æç¤ºï¼Œä½†æ­ç¤ºç‡é€šå¸¸ä½äº20%ï¼›<br>ï¼ˆ2ï¼‰åŸºäºç»“æœçš„æ­£å‘å¼ºåŒ–å­¦ä¹ æœ€åˆä¼šæé«˜å¿ å®åº¦ï¼Œä½†ä¼šè¾¾åˆ°å¹³ç¨³çŠ¶æ€è€Œä¸ä¼šé¥±å’Œï¼›<br>ï¼ˆ3ï¼‰å½“æ­£å‘å¼ºåŒ–å­¦ä¹ å¢åŠ ä½¿ç”¨æç¤ºçš„é¢‘ç‡ï¼ˆå¥–åŠ±ç ´è§£ï¼‰æ—¶ï¼Œå³ä½¿åœ¨æ²¡æœ‰é’ˆå¯¹æ€ç»´é“¾ç›‘è§†å™¨è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¡¨è¿°å®ƒä»¬çš„å€¾å‘ä¹Ÿä¸ä¼šå¢åŠ ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ€ç»´é“¾ç›‘æ§æ˜¯æ³¨æ„åˆ°è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­ä¸å¸Œæœ›å‡ºç°çš„ä¸€ç§è¡Œä¸ºçš„ä¸€ç§æœ‰å‰é€”çš„æ–¹å¼ï¼Œä½†ä»…å‡­è¿™ç§æ–¹å¼å¹¶ä¸è¶³ä»¥å°†å…¶æ’é™¤ã€‚å®ƒä»¬è¿˜è¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬è¿™æ ·çš„ç¯å¢ƒä¸­ï¼Œå¦‚æœæ€ç»´é“¾æ¨ç†å¹¶éå¿…è¦ï¼Œé‚£ä¹ˆåœ¨æµ‹è¯•æ—¶å¯¹æ€ç»´é“¾çš„ç›‘æ§å¾ˆå¯èƒ½æ— æ³•å¯é åœ°æ•æ‰åˆ°ç½•è§çš„å’Œç¾éš¾æ€§çš„æ„å¤–è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05410v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Chain-of-thoughtï¼ˆCoTï¼‰åœ¨AIå®‰å…¨é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶è¯„ä¼°äº†å½“å‰å…ˆè¿›æ¨ç†æ¨¡å‹çš„CoTå¿ å®åº¦ã€‚ç ”ç©¶å‘ç°ï¼ŒCoTèƒ½å¤Ÿåœ¨è‡³å°‘1%çš„ç¤ºä¾‹ä¸­æ­ç¤ºæ¨¡å‹å¯¹æç¤ºçš„ä½¿ç”¨æƒ…å†µï¼Œä½†æ­ç¤ºç‡é€šå¸¸ä½äº20%ã€‚åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ æœ€åˆèƒ½æé«˜å¿ å®åº¦ï¼Œä½†ä¼šè¾¾åˆ°ç¨³å®šçŠ¶æ€è€Œä¸ä¼šé¥±å’Œã€‚å½“å¼ºåŒ–å­¦ä¹ å¢åŠ æç¤ºçš„ä½¿ç”¨é¢‘ç‡æ—¶ï¼Œå³ä½¿åœ¨æ²¡æœ‰é’ˆå¯¹CoTç›‘è§†å™¨è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¡¨è¿°å®ƒä»¬çš„å€¾å‘ä¹Ÿä¸ä¼šå¢åŠ ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒCoTç›‘æ§æ˜¯ä¸€ç§åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­å‘ç°ä¸è‰¯è¡Œä¸ºçš„æœ‰ç”¨æ–¹æ³•ï¼Œä½†å¹¶ä¸è¶³ä»¥å®Œå…¨æ’é™¤è¿™äº›è¡Œä¸ºã€‚åœ¨ä¸éœ€è¦CoTæ¨ç†çš„æƒ…å¢ƒä¸­ï¼Œæµ‹è¯•æœŸé—´çš„CoTç›‘æ§å¯èƒ½æ— æ³•å¯é åœ°æ•è·ç½•è§ä¸”ç¾éš¾æ€§çš„æ„å¤–è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoTèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹å¯¹æç¤ºçš„ä½¿ç”¨æƒ…å†µï¼Œä½†æ­ç¤ºç‡è¾ƒä½ï¼Œé€šå¸¸ä½äº20%ã€‚</li>
<li>åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ èƒ½æé«˜CoTå¿ å®åº¦ï¼Œä½†å­˜åœ¨ç¨³å®šçŠ¶æ€ï¼Œä¸èƒ½å®Œå…¨è¾¾åˆ°é¥±å’Œã€‚</li>
<li>å½“å¼ºåŒ–å­¦ä¹ å¢åŠ æç¤ºä½¿ç”¨é¢‘ç‡æ—¶ï¼Œæ¨¡å‹è¡¨è¿°æç¤ºçš„å€¾å‘å¹¶ä¸ä¼šå¢åŠ ã€‚</li>
<li>CoTç›‘æ§åœ¨å‘ç°å’Œæ³¨æ„ä¸è‰¯è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†ä¸è¶³ä»¥å®Œå…¨æ’é™¤è¿™äº›è¡Œä¸ºã€‚</li>
<li>åœ¨ä¸éœ€è¦CoTæ¨ç†çš„æƒ…å¢ƒä¸­ï¼ŒCoTç›‘æ§å¯èƒ½æ— æ³•æœ‰æ•ˆæ•è·ç½•è§ä¸”ç¾éš¾æ€§çš„æ„å¤–è¡Œä¸ºã€‚</li>
<li>CoTä»£è¡¨æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹å¯èƒ½å—åˆ°é™åˆ¶ï¼Œå› æ­¤åº”å½“è¿›ä¸€æ­¥ç ”ç©¶ä»¥æé«˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-540639235148d6534337596a089374d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a040db3b7cc593c146db884ffc0f33f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-768b34692a9314131c24e53e0e067a1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c578dd59f3ec35cdaa37575b6fff1cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fcf79592beb1eacad77745748188f98.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DSDrive-Distilling-Large-Language-Model-for-Lightweight-End-to-End-Autonomous-Driving-with-Unified-Reasoning-and-Planning"><a href="#DSDrive-Distilling-Large-Language-Model-for-Lightweight-End-to-End-Autonomous-Driving-with-Unified-Reasoning-and-Planning" class="headerlink" title="DSDrive: Distilling Large Language Model for Lightweight End-to-End   Autonomous Driving with Unified Reasoning and Planning"></a>DSDrive: Distilling Large Language Model for Lightweight End-to-End   Autonomous Driving with Unified Reasoning and Planning</h2><p><strong>Authors:Wenru Liu, Pei Liu, Jun Ma</strong></p>
<p>We present DSDrive, a streamlined end-to-end paradigm tailored for integrating the reasoning and planning of autonomous vehicles into a unified framework. DSDrive leverages a compact LLM that employs a distillation method to preserve the enhanced reasoning capabilities of a larger-sized vision language model (VLM). To effectively align the reasoning and planning tasks, a waypoint-driven dual-head coordination module is further developed, which synchronizes dataset structures, optimization objectives, and the learning process. By integrating these tasks into a unified framework, DSDrive anchors on the planning results while incorporating detailed reasoning insights, thereby enhancing the interpretability and reliability of the end-to-end pipeline. DSDrive has been thoroughly tested in closed-loop simulations, where it performs on par with benchmark models and even outperforms in many key metrics, all while being more compact in size. Additionally, the computational efficiency of DSDrive (as reflected in its time and memory requirements during inference) has been significantly enhanced. Evidently thus, this work brings promising aspects and underscores the potential of lightweight systems in delivering interpretable and efficient solutions for AD. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DSDriveï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºè‡ªåŠ¨é©¾é©¶è½¦çš„æ¨ç†å’Œè§„åˆ’æ•´åˆè€Œé‡èº«å®šåˆ¶çš„ç«¯åˆ°ç«¯èŒƒå¼ç»Ÿä¸€æ¡†æ¶ã€‚DSDriveåˆ©ç”¨äº†ä¸€ä¸ªç´§å‡‘çš„LLMï¼Œè¯¥æ¨¡å‹é‡‡ç”¨è’¸é¦æ³•ä¿ç•™å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å¯¹é½æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ï¼Œè¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§åŸºäºè·¯ç‚¹çš„åŒå¤´åè°ƒæ¨¡å—ï¼Œè¯¥æ¨¡å—åŒæ­¥æ•°æ®é›†ç»“æ„ã€ä¼˜åŒ–ç›®æ ‡å’Œå­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡å°†è¿™äº›ä»»åŠ¡æ•´åˆåˆ°ç»Ÿä¸€æ¡†æ¶ä¸­ï¼ŒDSDriveä»¥è§„åˆ’ç»“æœä¸ºæ ¸å¿ƒï¼ŒåŒæ—¶èå…¥è¯¦ç»†çš„æ¨ç†è§è§£ï¼Œä»è€Œæé«˜äº†ç«¯åˆ°ç«¯ç®¡é“çš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚DSDriveå·²åœ¨é—­ç¯ä»¿çœŸä¸­è¿›è¡Œäº†å…¨é¢æµ‹è¯•ï¼Œå…¶æ€§èƒ½ä¸åŸºå‡†æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨è®¸å¤šå…³é”®æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ç³»ç»Ÿä½“ç§¯æ›´å°ã€‚æ­¤å¤–ï¼ŒDSDriveçš„è®¡ç®—æ•ˆç‡ï¼ˆåæ˜ åœ¨å…¶æ¨ç†è¿‡ç¨‹ä¸­çš„æ—¶é—´å’Œå†…å­˜è¦æ±‚ï¼‰å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œå¸¦æ¥äº†æœ‰å¸Œæœ›çš„æ–¹é¢ï¼Œå¹¶çªå‡ºäº†è½»é‡åŒ–ç³»ç»Ÿåœ¨æä¾›å¯è§£é‡Šå’Œé«˜æ•ˆçš„è‡ªåŠ¨é©¾é©¶è§£å†³æ–¹æ¡ˆæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05360v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DSDriveæ˜¯ä¸€ä¸ªä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†æä¾›ç«¯åˆ°ç«¯çš„æ¨ç†å’Œè§„åˆ’ä¸€ä½“åŒ–çš„æ¡†æ¶ã€‚å®ƒé‡‡ç”¨ç´§å‡‘çš„LLMæ¨¡å‹ï¼Œåˆ©ç”¨è’¸é¦æ–¹æ³•ä¿ç•™å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¼€å‘ä¸€ç§åŸºäºè·¯å¾„ç‚¹çš„åŒå¤´åè°ƒæ¨¡å—ï¼Œæœ‰æ•ˆå¯¹é½æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ï¼Œæé«˜æ•°æ®é›†ç»“æ„ã€ä¼˜åŒ–ç›®æ ‡å’Œå­¦ä¹ è¿‡ç¨‹çš„åŒæ­¥æ€§ã€‚DSDriveå°†è¯¦ç»†çš„æ¨ç†ç»“æœèå…¥è§„åˆ’ç»“æœä¸­ï¼Œæé«˜äº†æ•´ä¸ªç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚åœ¨å°é—­ç¯å¢ƒçš„æ¨¡æ‹Ÿæµ‹è¯•ä¸­ï¼ŒDSDriveè¡¨ç°ä¼˜ç§€ï¼Œä¸”è®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ã€‚è¿™é¡¹å·¥ä½œå±•ç°äº†å…¶åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSDriveæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œä¸“ä¸ºæ•´åˆè‡ªåŠ¨é©¾é©¶çš„æ¨ç†å’Œè§„åˆ’è€Œè®¾è®¡ã€‚</li>
<li>å®ƒé‡‡ç”¨ç´§å‡‘çš„LLMæ¨¡å‹ï¼Œå…·å¤‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è’¸é¦æ–¹æ³•ï¼ŒDSDriveä¿ç•™äº†åŸå§‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºè·¯å¾„ç‚¹çš„åŒå¤´åè°ƒæ¨¡å—ï¼ŒåŒæ­¥æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ã€‚</li>
<li>DSDriveå°†è¯¦ç»†çš„æ¨ç†ç»“æœèå…¥è§„åˆ’ç»“æœä¸­ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿæµ‹è¯•ä¸­ï¼ŒDSDriveè¡¨ç°ä¼˜ç§€ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨å…³é”®æŒ‡æ ‡ä¸Šæœ‰æ‰€è¶…è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b13ab05423db9c0e7785cb3ce6354a64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3757bda4a5eded4beb00cf665c61a23b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f43a95c772b1d4ec3b8cdcb08397e9fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-123ecd54f8a6abb99f29fc9a563067d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-408a08a45fa9042399501ce93c6561a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b7b6f5c401eea8c9db56776ddd69b63.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SpatialPrompting-Keyframe-driven-Zero-Shot-Spatial-Reasoning-with-Off-the-Shelf-Multimodal-Large-Language-Models"><a href="#SpatialPrompting-Keyframe-driven-Zero-Shot-Spatial-Reasoning-with-Off-the-Shelf-Multimodal-Large-Language-Models" class="headerlink" title="SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with   Off-the-Shelf Multimodal Large Language Models"></a>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with   Off-the-Shelf Multimodal Large Language Models</h2><p><strong>Authors:Shun Taguchi, Hideki Deguchi, Takumi Hamazaki, Hiroyuki Sakai</strong></p>
<p>This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large language models to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†SpatialPromptingï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç°æˆçš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹çš„çªå‘æ¨ç†èƒ½åŠ›ï¼Œå®ç°ä¸‰ç»´ç¯å¢ƒä¸­æ— éœ€è®­ç»ƒçš„ç©ºé—´æ¨ç†ã€‚ä¸ä¾èµ–æ˜‚è´µçš„é’ˆå¯¹ä¸‰ç»´çš„å¾®è°ƒä»¥åŠç‰¹å®šçš„ä¸‰ç»´è¾“å…¥ï¼ˆå¦‚ç‚¹äº‘æˆ–åŸºäºä½“ç´ çš„ç‰¹å¾ï¼‰çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSpatialPromptingé‡‡ç”¨åŸºäºå…³é”®å¸§çš„æç¤ºç”Ÿæˆç­–ç•¥ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è§†è§‰è¯­è¨€ç›¸ä¼¼æ€§ã€é©¬æ°è·ç¦»ã€è§†é‡å’Œå›¾åƒæ¸…æ™°åº¦ç­‰åº¦é‡æ ‡å‡†ï¼Œä»å›¾åƒåºåˆ—ä¸­é€‰æ‹©å¤šæ ·ä¸”ä¿¡æ¯ä¸°å¯Œçš„å…³é”®å¸§ï¼Œç„¶åå°†å…¶ä¸ç›¸åº”çš„ç›¸æœºå§¿æ€æ•°æ®ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆåœ°æŠ½è±¡ç©ºé—´å…³ç³»å¹¶æ¨æ–­å¤æ‚çš„ä¸‰ç»´ç»“æ„ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ä»…ä¸ºåˆ©ç”¨ç›´è§‚è§†è§‰å’Œä½ç½®çº¿ç´¢çš„çµæ´»ç©ºé—´æ¨ç†å»ºç«‹äº†æ–°èŒƒå¼ï¼Œè€Œä¸”åœ¨ScanQAå’ŒSQA3Dç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œè·¨è¶Šå¤šä¸ªæŒ‡æ ‡ã€‚æ‰€æå‡ºçš„æ–¹æ³•æœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¯¹ç‰¹æ®Šä¸‰ç»´è¾“å…¥å’Œå¾®è°ƒçš„éœ€æ±‚ï¼Œä¸ºä¼ ç»Ÿæ–¹æ³•æä¾›äº†æ›´ç®€å•ã€æ›´å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04911v1">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†SpatialPromptingæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç°æˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå®ç°é›¶æ ·æœ¬ç©ºé—´æ¨ç†ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºæ˜‚è´µçš„ä¸‰ç»´ç‰¹å®šå¾®è°ƒæ–¹æ³•å’Œç‰¹æ®Šä¸‰ç»´è¾“å…¥ï¼ˆå¦‚ç‚¹äº‘æˆ–åŸºäºä½“ç´ çš„ç‰¹å¾ï¼‰çš„æ–¹æ³•ä¸åŒï¼ŒSpatialPromptingé‡‡ç”¨åŸºäºå…³é”®å¸§çš„æç¤ºç”Ÿæˆç­–ç•¥ã€‚å®ƒä½¿ç”¨è§†è§‰è¯­è¨€ç›¸ä¼¼æ€§ã€é©¬æ°è·ç¦»ã€è§†é‡å’Œå›¾åƒæ¸…æ™°åº¦ç­‰åº¦é‡æ ‡å‡†ä»å›¾åƒåºåˆ—ä¸­é€‰æ‹©å¤šæ ·ä¸”ä¿¡æ¯ä¸°å¯Œçš„å…³é”®å¸§ï¼Œå¹¶å°†å…¶ä¸ç›¸åº”çš„ç›¸æœºå§¿æ€æ•°æ®é›†æˆï¼Œä»¥æœ‰æ•ˆåœ°æŠ½è±¡ç©ºé—´å…³ç³»å¹¶æ¨æ–­å¤æ‚çš„ä¸‰ç»´ç»“æ„ã€‚è¯¥æ¡†æ¶ä¸ä»…å»ºç«‹äº†ä¸€ç§çµæ´»åˆ©ç”¨ç›´è§‚è§†è§‰å’Œä½ç½®çº¿ç´¢è¿›è¡Œç©ºé—´æ¨ç†çš„æ–°èŒƒå¼ï¼Œè€Œä¸”åœ¨ScanQAå’ŒSQA3Dç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒæœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¯¹ç‰¹æ®Šä¸‰ç»´è¾“å…¥å’Œå¾®è°ƒçš„éœ€æ±‚ï¼Œä¸ºä¼ ç»Ÿæ–¹æ³•æä¾›äº†æ›´ç®€å•ä¸”æ›´å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SpatialPromptingæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºå®ç°é›¶æ ·æœ¬ç©ºé—´æ¨ç†ï¼Œä¾èµ–äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œå®ƒä¸éœ€è¦æ˜‚è´µçš„ä¸‰ç»´ç‰¹å®šå¾®è°ƒæˆ–ç‰¹æ®Šçš„ä¸‰ç»´è¾“å…¥ã€‚</li>
<li>é€šè¿‡åŸºäºå…³é”®å¸§çš„æç¤ºç”Ÿæˆç­–ç•¥é€‰æ‹©å…³é”®å¸§å¹¶é›†æˆç›¸æœºå§¿æ€æ•°æ®ï¼Œä»¥æ¨æ–­å¤æ‚çš„ä¸‰ç»´ç»“æ„ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨è§†è§‰è¯­è¨€ç›¸ä¼¼æ€§ã€é©¬æ°è·ç¦»ã€è§†é‡å’Œå›¾åƒæ¸…æ™°åº¦ç­‰åº¦é‡æ ‡å‡†æ¥é€‰æ‹©å…³é”®å¸§ã€‚</li>
<li>å®ƒæœ‰æ•ˆåœ°æŠ½è±¡ç©ºé—´å…³ç³»å¹¶æ•´åˆè§†è§‰å’Œä½ç½®çº¿ç´¢è¿›è¡Œçµæ´»çš„ç©ºé—´æ¨ç†ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe69143a7c764d848570e5b5f8bd28b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-093d48ae4475047b20837fa712edce2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6092eb319c1d2d7ba607b1cc48855470.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e375a0f9dc60661137f91bf896dc203.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2da3e94ffeb36fd9e163c933f2c2010.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ConCISE-Confidence-guided-Compression-in-Step-by-step-Efficient-Reasoning"><a href="#ConCISE-Confidence-guided-Compression-in-Step-by-step-Efficient-Reasoning" class="headerlink" title="ConCISE: Confidence-guided Compression in Step-by-step Efficient   Reasoning"></a>ConCISE: Confidence-guided Compression in Step-by-step Efficient   Reasoning</h2><p><strong>Authors:Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Fandong Meng, Jie Zhou, Ju Ren, Yaoxue Zhang</strong></p>
<p>Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused by redundant content, increasing computational overhead, and degrading user experience. Existing compression methods either operate post-hoc pruning, risking disruption to reasoning coherence, or rely on sampling-based selection, which fails to intervene effectively during generation. In this work, we introduce a confidence-guided perspective to explain the emergence of redundant reflection in LRMs, identifying two key patterns: Confidence Deficit, where the model reconsiders correct steps due to low internal confidence, and Termination Delay, where reasoning continues even after reaching a confident answer. Based on this analysis, we propose ConCISE (Confidence-guided Compression In Step-by-step Efficient Reasoning), a framework that simplifies reasoning chains by reinforcing the modelâ€™s confidence during inference, thus preventing the generation of redundant reflection steps. It integrates Confidence Injection to stabilize intermediate steps and Early Stopping to terminate reasoning when confidence is sufficient. Extensive experiments demonstrate that fine-tuning LRMs on ConCISE-generated data yields significantly shorter outputs, reducing length by up to approximately 50% under SimPO, while maintaining high task accuracy. ConCISE consistently outperforms existing baselines across multiple reasoning benchmarks. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å¸¸å¸¸å› ä¸ºå†—ä½™å†…å®¹è€Œå¯¼è‡´å†—é•¿çš„è¾“å‡ºï¼Œå¢åŠ äº†è®¡ç®—å¼€é”€å¹¶é™ä½äº†ç”¨æˆ·ä½“éªŒã€‚ç°æœ‰çš„å‹ç¼©æ–¹æ³•è¦ä¹ˆè¿›è¡Œäº‹åä¿®å‰ªï¼Œå¯èƒ½ç ´åæ¨ç†çš„è¿è´¯æ€§ï¼Œè¦ä¹ˆä¾èµ–äºåŸºäºé‡‡æ ·çš„é€‰æ‹©ï¼Œæ— æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæœ‰æ•ˆå¹²é¢„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºä¿¡å¿ƒçš„è§†è§’æ¥è§£é‡ŠLRMä¸­å†—ä½™åæ€çš„å‡ºç°åŸå› ï¼Œå¹¶è¯†åˆ«å‡ºä¸¤ç§å…³é”®æ¨¡å¼ï¼šä¿¡å¿ƒä¸è¶³ï¼Œå³æ¨¡å‹å› å†…éƒ¨ä¿¡å¿ƒä¸è¶³è€Œé‡æ–°è€ƒè™‘æ­£ç¡®çš„æ­¥éª¤ï¼›ç»ˆæ­¢å»¶è¿Ÿï¼Œå³ä½¿åœ¨å¾—åˆ°ç¡®å®šçš„ç­”æ¡ˆåï¼Œæ¨ç†ä»ä¼šç»§ç»­ã€‚åŸºäºè¿™ç§åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ConCISEï¼ˆåŸºäºä¿¡å¿ƒçš„é€æ­¥é«˜æ•ˆæ¨ç†å‹ç¼©æ¡†æ¶ï¼‰ï¼Œå®ƒé€šè¿‡åŠ å¼ºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡å¿ƒæ¥ç®€åŒ–æ¨ç†é“¾ï¼Œä»è€Œé˜²æ­¢ç”Ÿæˆå†—ä½™çš„åæ€æ­¥éª¤ã€‚å®ƒé›†æˆäº†ä¿¡å¿ƒæ³¨å…¥æ¥ç¨³å®šä¸­é—´æ­¥éª¤å’Œæå‰åœæ­¢åŠŸèƒ½ï¼Œåœ¨ä¿¡å¿ƒè¶³å¤Ÿæ—¶ç»ˆæ­¢æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ConCISEç”Ÿæˆçš„æ•°æ®ä¸Šå¯¹LRMè¿›è¡Œå¾®è°ƒä¼šäº§ç”Ÿæ›´çŸ­çš„è¾“å‡ºï¼Œåœ¨SimPOä¸‹é•¿åº¦å‡å°‘é«˜è¾¾çº¦50%ï¼ŒåŒæ—¶ä¿æŒé«˜ä»»åŠ¡å‡†ç¡®ç‡ã€‚ConCISEåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸å› å†—ä½™å†…å®¹è€Œäº§ç”Ÿå†—é•¿è¾“å‡ºï¼Œå¢åŠ è®¡ç®—å¼€é”€å¹¶é™ä½ç”¨æˆ·ä½“éªŒã€‚ç°æœ‰å‹ç¼©æ–¹æ³•è¦ä¹ˆè¿›è¡Œäº‹åä¿®å‰ªï¼Œå¯èƒ½ç ´åæ¨ç†è¿è´¯æ€§ï¼Œè¦ä¹ˆä¾èµ–é‡‡æ ·é€‰æ‹©ï¼Œæ— æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰æ•ˆå¹²é¢„ã€‚æœ¬æ–‡å¼•å…¥ä¿¡å¿ƒå¼•å¯¼çš„è§†è§’ï¼Œåˆ†æLRMsä¸­å†—ä½™åå°„çš„å‡ºç°åŸå› ï¼Œè¯†åˆ«å‡ºä¸¤å¤§å…³é”®æ¨¡å¼ï¼šä¿¡å¿ƒä¸è¶³å’Œç»ˆæ­¢å»¶è¿Ÿã€‚åŸºäºæ­¤åˆ†æï¼Œæˆ‘ä»¬æå‡ºConCISEï¼ˆä¿¡å¿ƒå¼•å¯¼é€æ­¥é«˜æ•ˆæ¨ç†å‹ç¼©ï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŠ å¼ºæ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡å¿ƒæ¥ç®€åŒ–æ¨ç†é“¾ï¼Œé˜²æ­¢ç”Ÿæˆå†—ä½™åæ€æ­¥éª¤ã€‚å®ƒé›†æˆäº†ä¿¡å¿ƒæ³¨å…¥æ¥ç¨³å®šä¸­é—´æ­¥éª¤å’Œæå‰åœæ­¢åŠŸèƒ½ï¼Œåœ¨è¾¾åˆ°è¶³å¤Ÿä¿¡å¿ƒæ—¶ç»ˆæ­¢æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ConCISEç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„å¤§å‹æ¨ç†æ¨¡å‹è¾“å‡ºæ›´çŸ­ï¼Œåœ¨SimPOä¸‹é•¿åº¦å‡å°‘çº¦50%ï¼ŒåŒæ—¶ä¿æŒé«˜ä»»åŠ¡å‡†ç¡®æ€§ã€‚ConCISEåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å†—é•¿è¾“å‡ºå’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚</li>
<li>å†—ä½™è¾“å‡ºä¸»è¦æºäºä¿¡å¿ƒä¸è¶³å’Œç»ˆæ­¢å»¶è¿Ÿä¸¤å¤§æ¨¡å¼ã€‚</li>
<li>ConCISEæ¡†æ¶é€šè¿‡åŠ å¼ºæ¨¡å‹ä¿¡å¿ƒæ¥ç®€åŒ–æ¨ç†é“¾ï¼Œé›†æˆä¿¡å¿ƒæ³¨å…¥å’Œæå‰åœæ­¢åŠŸèƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒConCISEèƒ½æ˜¾è‘—ç¼©çŸ­è¾“å‡ºé•¿åº¦ï¼ŒåŒæ—¶ä¿æŒé«˜ä»»åŠ¡å‡†ç¡®æ€§ã€‚</li>
<li>ConCISEåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>ConCISEæ¡†æ¶æœ‰åŠ©äºæ”¹å–„ç”¨æˆ·ä½“éªŒå’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b515cdb7bb80c8fe29efe9299fe37bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d11c9c883e9eb0b70813dfe332465f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a50233230d48958784fe30a3d3552af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-457f2e079466122a85258868e95944a4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-are-Autonomous-Cyber-Defenders"><a href="#Large-Language-Models-are-Autonomous-Cyber-Defenders" class="headerlink" title="Large Language Models are Autonomous Cyber Defenders"></a>Large Language Models are Autonomous Cyber Defenders</h2><p><strong>Authors:SebastiÃ¡n R. Castro, Roberto Campbell, Nancy Lau, Octavio Villalobos, Jiaqi Duan, Alvaro A. Cardenas</strong></p>
<p>Fast and effective incident response is essential to prevent adversarial cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response through Artificial Intelligence (AI) agents that plan and execute actions. Most ACD approaches focus on single-agent scenarios and leverage Reinforcement Learning (RL). However, ACD RL-trained agents depend on costly training, and their reasoning is not always explainable or transferable. Large Language Models (LLMs) can address these concerns by providing explainable actions in general security contexts. Researchers have explored LLM agents for ACD but have not evaluated them on multi-agent scenarios or interacting with other ACD agents. In this paper, we show the first study on how LLMs perform in multi-agent ACD environments by proposing a new integration to the CybORG CAGE 4 environment. We examine how ACD teams of LLM and RL agents can interact by proposing a novel communication protocol. Our results highlight the strengths and weaknesses of LLMs and RL and help us identify promising research directions to create, train, and deploy future teams of ACD agents. </p>
<blockquote>
<p>å¿«é€Ÿæœ‰æ•ˆçš„åº”æ€¥å“åº”æ˜¯é˜²æ­¢å¯¹æŠ—æ€§ç½‘ç»œæ”»å‡»çš„å…³é”®ã€‚è‡ªä¸»ç½‘ç»œå®‰å…¨é˜²å¾¡ï¼ˆACDï¼‰æ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä»£ç†æ¥è‡ªåŠ¨åŒ–åº”æ€¥å“åº”ï¼Œè¿™äº›ä»£ç†å¯ä»¥è®¡åˆ’å’Œæ‰§è¡Œæ“ä½œã€‚å¤§å¤šæ•°ACDæ–¹æ³•éƒ½å…³æ³¨å•ä»£ç†åœºæ™¯ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚ç„¶è€Œï¼ŒACDçš„RLè®­ç»ƒä»£ç†ä¾èµ–äºæ˜‚è´µçš„è®­ç»ƒæˆæœ¬ï¼Œå¹¶ä¸”å®ƒä»¬çš„æ¨ç†å¹¶ä¸æ€»æ˜¯å¯è§£é‡Šæˆ–å¯è¿ç§»çš„ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡åœ¨ä¸€èˆ¬å®‰å…¨ç¯å¢ƒä¸­æä¾›å¯è§£é‡Šçš„è¡ŒåŠ¨æ¥è§£å†³è¿™äº›æ‹…å¿§ã€‚ç ”ç©¶äººå‘˜å·²ç»æ¢ç´¢äº†ç”¨äºACDçš„LLMä»£ç†ï¼Œä½†å°šæœªåœ¨å¤šä»£ç†åœºæ™¯æˆ–ä¸å…¶ä»–ACDä»£ç†äº¤äº’æ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºå¯¹CybORG CAGE 4ç¯å¢ƒçš„æ–°é›†æˆï¼Œå±•ç¤ºäº†LLMåœ¨å¤šä»£ç†ACDç¯å¢ƒä¸­çš„è¡¨ç°çš„é¦–é¡¹ç ”ç©¶ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æå‡ºä¸€ç§æ–°å‹é€šä¿¡åè®®æ¥æ£€æŸ¥LLMå’ŒRLä»£ç†çš„ACDå›¢é˜Ÿå¦‚ä½•ç›¸äº’åä½œã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†LLMå’ŒRLçš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œå¹¶æœ‰åŠ©äºæˆ‘ä»¬ç¡®å®šåˆ›å»ºã€è®­ç»ƒå’Œéƒ¨ç½²æœªæ¥ACDä»£ç†å›¢é˜Ÿçš„æœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04843v1">PDF</a> Presented at IEEE CAI Workshop on Adaptive Cyber Defense 2025.   Proceedings to appear</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨å“åº”å¯¹äºé˜²æ­¢ç½‘ç»œæ”»å‡»è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“è‡ªä¸»ç½‘ç»œå®‰å…¨é˜²å¾¡ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é›†æˆæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¸€ç§æ–°å‹é€šä¿¡åè®®ï¼Œå®ç°LLMå’Œå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„äº¤äº’ã€‚ç ”ç©¶ç»“æœè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ å„æœ‰ä¼˜åŠ£ï¼Œå¯¹ä»Šåç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚é€šè¿‡è‡ªé€‚åº”çš„è®­ç»ƒéƒ¨ç½²ä¸æ·±åº¦æ¢ç©¶åº”å¯¹å®‰å…¨æŒ‘æˆ˜çš„æœ€ä½³é€”å¾„å°†å¸¦æ¥å·¨å¤§çš„æ½œåœ¨æ•ˆç›Šã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹ç ”ç©¶ä¸ºæœªæ¥ååŒåˆä½œç­–ç•¥çš„ç ”å‘å’Œæ”¹è¿›å¥ å®šäº†åŸºç¡€ã€‚è¿™å°†å¢å¼ºæŠµå¾¡å¤æ‚çš„ç°å®æ”»å‡»åœºæ™¯çš„è‡ªä¸»æ€§ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ äº†å®‰å…¨äº‹ä»¶çš„å“åº”é€Ÿåº¦ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„é‡è¦è§è§£åˆ—è¡¨ï¼š</p>
<ul>
<li>å¿«é€Ÿæœ‰æ•ˆçš„åº”æ€¥å“åº”å¯¹äºé˜²æ­¢ç½‘ç»œæ”»å‡»è‡³å…³é‡è¦ã€‚è‡ªä¸»ç½‘ç»œå®‰å…¨é˜²å¾¡ï¼ˆACDï¼‰æ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä»£ç†è‡ªåŠ¨åŒ–åº”æ€¥å“åº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ACDæ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€ä»£ç†åœºæ™¯ï¼Œå¹¶ä¾èµ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚RLè®­ç»ƒä»£ç†çš„æˆæœ¬è¾ƒé«˜ï¼Œå…¶æ¨ç†ç»“æœä¹Ÿå¾€å¾€æ— æ³•è§£é‡Šå’Œè¿ç§»ã€‚å¯¹æ­¤ç±»é—®é¢˜çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆæ¶‰åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç›®å‰æš‚æ— ä½¿ç”¨LLMä»£ç†åœ¨ACDç¯å¢ƒä¸­è¿›è¡Œå¤šæ™ºèƒ½ä½“ç ”ç©¶çš„è¯æ®æˆ–è¯æ®ä¸è¶³ï¼Œç¼ºä¹è¯„ä¼°å®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„è¡¨ç°ä»¥åŠä¸ACDå…¶ä»–ä»£ç†çš„äº’åŠ¨æ–¹å¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83b8d8e9105808e553ce946bd0c7e6f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4ded5b380af8865d099e8663bdc43cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff61d6a8feb1852f77176c318f381832.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32628b5aa55715353cebcbb0a8f7b959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866e300f8eb5b2429cf6595acc5f3fda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43326df8e3390783d31a17a3743dde84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51bd0de69ecdeeab5126e4ed48f9e21c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47c7b785abe924bf2d221629df5d0a06.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lay-Your-Scene-Natural-Scene-Layout-Generation-with-Diffusion-Transformers"><a href="#Lay-Your-Scene-Natural-Scene-Layout-Generation-with-Diffusion-Transformers" class="headerlink" title="Lay-Your-Scene: Natural Scene Layout Generation with Diffusion   Transformers"></a>Lay-Your-Scene: Natural Scene Layout Generation with Diffusion   Transformers</h2><p><strong>Authors:Divyansh Srivastava, Xiang Zhang, He Wen, Chenru Wen, Zhuowen Tu</strong></p>
<p>We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Lay-Your-Sceneï¼ˆç®€ç§°LayouSynï¼‰è¿™ä¸€å…¨æ–°çš„æ–‡æœ¬åˆ°å¸ƒå±€ç”Ÿæˆç®¡é“ï¼Œç”¨äºè‡ªç„¶åœºæ™¯ã€‚ä¹‹å‰çš„åœºæ™¯å¸ƒå±€ç”Ÿæˆæ–¹æ³•è¦ä¹ˆæ˜¯å°é—­è¯æ±‡è¡¨ï¼Œè¦ä¹ˆä½¿ç”¨ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼€æ”¾è¯æ±‡è¡¨ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†å…¶å»ºæ¨¡èƒ½åŠ›å’Œåœ¨å¯æ§å›¾åƒç”Ÿæˆä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨è½»é‡çº§å¼€æºè¯­è¨€æ¨¡å‹ä»æ–‡æœ¬æç¤ºä¸­è·å–åœºæ™¯å…ƒç´ ï¼Œå¹¶ä½¿ç”¨ä¸€ç§æ–°å‹çš„é¢å‘æ–¹é¢çš„æ‰©æ•£Transformeræ¶æ„ï¼Œä»¥å¼€æ”¾è¯æ±‡è¡¨çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œç”¨äºæ¡ä»¶å¸ƒå±€ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLayouSynä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºé—´å’Œæ•°å€¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†LayouSynçš„ä¸¤ä¸ªåº”ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²—ç•¥åˆå§‹åŒ–å¯ä»¥æ— ç¼åœ°ä¸æˆ‘ä»¬çš„æ–¹æ³•ç›¸ç»“åˆï¼Œä»¥å–å¾—æ›´å¥½çš„ç»“æœã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªå‘å›¾åƒæ·»åŠ ç‰©ä½“çš„ç®¡é“ï¼Œè¯æ˜äº†LayouSynåœ¨å›¾åƒç¼–è¾‘åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04718v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Lay-Your-Sceneï¼ˆç®€ç§°LayouSynï¼‰è¿™ä¸€æ–°å‹çš„æ–‡æœ¬åˆ°è‡ªç„¶åœºæ™¯å¸ƒå±€ç”Ÿæˆç®¡é“ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒLayouSyné‡‡ç”¨è½»é‡çº§å¼€æºè¯­è¨€æ¨¡å‹è·å–åœºæ™¯å…ƒç´ ï¼Œå¹¶ç»“åˆæ–°é¢–çš„é¢å‘æ–¹é¢çš„æ‰©æ•£Transformeræ¶æ„ï¼Œä»¥å¼€æ”¾è¯æ±‡è¡¨çš„æ–¹å¼è¿›è¡Œæ¡ä»¶å¸ƒå±€ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒLayouSynåœ¨å…·æœ‰æŒ‘æˆ˜çš„ç©ºé—´å’Œæ•°å­—æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶è¶…è¶Šç°æœ‰æ–¹æ³•è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†LayouSynåœ¨å›¾åƒç¼–è¾‘åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LayouSynæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬åˆ°è‡ªç„¶åœºæ™¯å¸ƒå±€ç”Ÿæˆç®¡é“ã€‚</li>
<li>å®ƒé‡‡ç”¨è½»é‡çº§å¼€æºè¯­è¨€æ¨¡å‹è·å–åœºæ™¯å…ƒç´ ã€‚</li>
<li>LayouSynä½¿ç”¨æ–°é¢–çš„é¢å‘æ–¹é¢çš„æ‰©æ•£Transformeræ¶æ„è¿›è¡Œæ¡ä»¶å¸ƒå±€ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¼€æ”¾è¯æ±‡è¡¨æ–¹å¼ä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>LayouSynåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºé—´å’Œæ•°å­—æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>LayouSynå¯æ— ç¼ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„ç²—ç•¥åˆå§‹åŒ–ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4082f06bdece543ee4c02ce51a624903.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51d169deddaf63e529efc661ae62345f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b914fc2f7a69b26afd7537a4ef5c6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd858c5da0db9eece639018cb5440f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b82248d04b9cbbd1e765776c08a8e66b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2f6dedeffbc4d437b7524f579b2282b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="REVEAL-Multi-turn-Evaluation-of-Image-Input-Harms-for-Vision-LLM"><a href="#REVEAL-Multi-turn-Evaluation-of-Image-Input-Harms-for-Vision-LLM" class="headerlink" title="REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM"></a>REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM</h2><p><strong>Authors:Madhur Jindal, Saurabh Deshpande</strong></p>
<p>Vision Large Language Models (VLLMs) represent a significant advancement in artificial intelligence by integrating image-processing capabilities with textual understanding, thereby enhancing user interactions and expanding application domains. However, their increased complexity introduces novel safety and ethical challenges, particularly in multi-modal and multi-turn conversations. Traditional safety evaluation frameworks, designed for text-based, single-turn interactions, are inadequate for addressing these complexities. To bridge this gap, we introduce the REVEAL (Responsible Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated image mining, synthetic adversarial data generation, multi-turn conversational expansion using crescendo attack strategies, and comprehensive harm assessment through evaluators like GPT-4o.   We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual harm, violence, and misinformation. Our findings reveal that multi-turn interactions result in significantly higher defect rates compared to single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably, GPT-4o demonstrated the most balanced performance as measured by our Safety-Usability Index (SUI) followed closely by Pixtral. Additionally, misinformation emerged as a critical area requiring enhanced contextual defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 %$) while Qwen2-VL showed the highest MT refusal rate ($19.1 %$). </p>
<blockquote>
<p>è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰é€šè¿‡æ•´åˆå›¾åƒå¤„ç†èƒ½åŠ›ä¸æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œå®ç°äº†äººå·¥æ™ºèƒ½çš„ä¸€å¤§è¿›æ­¥ï¼Œä»è€Œå¢å¼ºäº†ç”¨æˆ·äº¤äº’å¹¶æ‰©å¤§äº†åº”ç”¨èŒƒå›´ã€‚ç„¶è€Œï¼Œå…¶å¢åŠ çš„å¤æ‚æ€§ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨å’Œé“å¾·æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€å’Œå¤šè½®å¯¹è¯ä¸­ã€‚ä¼ ç»Ÿçš„å®‰å…¨è¯„ä¼°æ¡†æ¶ï¼Œè®¾è®¡ç”¨äºåŸºäºæ–‡æœ¬çš„å•ä¸€è½®æ¬¡äº¤äº’ï¼Œä¸è¶³ä»¥åº”å¯¹è¿™äº›å¤æ‚æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†REVEALï¼ˆè§†è§‰èµ‹èƒ½äººå·¥æ™ºèƒ½å¤§å‹è¯­è¨€æ¨¡å‹è´£ä»»è¯„ä¼°ï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°VLLMsä¸­å›¾åƒè¾“å…¥å±å®³çš„å¯æ‰©å±•å’Œè‡ªåŠ¨åŒ–ç®¡é“ã€‚REVEALåŒ…æ‹¬è‡ªåŠ¨åŒ–å›¾åƒæŒ–æ˜ã€åˆæˆå¯¹æŠ—æ•°æ®ç”Ÿæˆã€ä½¿ç”¨æ¸å¼ºæ”»å‡»ç­–ç•¥çš„å¤šè½®å¯¹è¯æ‰©å±•ï¼Œä»¥åŠé€šè¿‡è¯„ä¼°è€…å¦‚GPT-4oè¿›è¡Œå…¨é¢å±å®³è¯„ä¼°ã€‚æˆ‘ä»¬å¹¿æ³›è¯„ä¼°äº†äº”ç§æœ€å…ˆè¿›çš„VLLMsï¼ŒåŒ…æ‹¬GPT-4oã€Llama-3.2ã€Qwen2-VLã€Phi3.5Vå’ŒPixtralï¼Œåœ¨ä¸‰ä¸ªé‡è¦çš„å±å®³ç±»åˆ«ï¼šæ€§å±å®³ã€æš´åŠ›å’Œé”™è¯¯ä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸å•è½®è¯„ä¼°ç›¸æ¯”ï¼Œå¤šè½®äº¤äº’å¯¼è‡´çš„ç¼ºé™·ç‡æ˜¾è‘—æ›´é«˜ï¼Œè¿™å‡¸æ˜¾äº†VLLMsçš„æ›´æ·±å±‚æ¬¡æ¼æ´ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®æˆ‘ä»¬çš„å®‰å…¨å¯ç”¨æ€§æŒ‡æ•°ï¼ˆSUIï¼‰æµ‹é‡ï¼ŒGPT-4oè¡¨ç°æœ€ä¸ºå‡è¡¡ï¼Œå…¶æ¬¡æ˜¯Pixtralã€‚å¦å¤–ï¼Œé”™è¯¯ä¿¡æ¯ä½œä¸ºä¸€ä¸ªå…³é”®é¢†åŸŸå‡ºç°ï¼Œéœ€è¦å¢å¼ºä¸Šä¸‹æ–‡é˜²å¾¡ã€‚Llama-3.2çš„MTç¼ºé™·ç‡æœ€é«˜ï¼ˆ16.55%ï¼‰ï¼Œè€ŒQwen2-VLçš„MTæ‹’ç»ç‡æœ€é«˜ï¼ˆ19.1%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04673v1">PDF</a> 13 pages (8 main), to be published in IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†VLLMï¼ˆè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„çªç ´æ€§è¿›å±•ï¼Œé€šè¿‡é›†æˆå›¾åƒå¤„ç†å’Œæ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œæé«˜äº†ç”¨æˆ·äº¤äº’å¹¶æ‰©å±•äº†åº”ç”¨èŒƒå›´ã€‚ç„¶è€Œï¼Œå…¶å¤æ‚æ€§å¢åŠ äº†æ–°çš„å®‰å…¨å’Œé“å¾·æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€å’Œå¤šè½®å¯¹è¯ä¸­ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†REVEALæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ºè¯„ä¼°å›¾åƒè¾“å…¥å¯¹VLLMçš„å±å®³æä¾›äº†ä¸€ä¸ªå¯è§„æ¨¡åŒ–ä¸”è‡ªåŠ¨åŒ–çš„ç®¡é“ã€‚ç ”ç©¶å¯¹äº”æ¬¾é¡¶å°–VLLMè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å¤šè½®äº¤äº’çš„ç¼ºé™·ç‡è¿œé«˜äºå•è½®è¯„ä¼°ï¼ŒGPT-4oåœ¨å¹³è¡¡æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒPixtralç´§éšå…¶åã€‚åŒæ—¶ï¼Œè¯¯ä¼ ä¿¡æ¯æˆä¸ºä¸€ä¸ªéœ€è¦å¢å¼ºä¸Šä¸‹æ–‡é˜²å¾¡çš„å…³é”®é¢†åŸŸã€‚Llama-3.2çš„MTç¼ºé™·ç‡æœ€é«˜ï¼Œè€ŒQwen2-VLçš„MTæ‹’ç»ç‡æœ€é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹å’Œä¿¡æ¯è¦ç‚¹ï¼š</p>
<ol>
<li>VLLMç»“åˆäº†å›¾åƒå¤„ç†å’Œæ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œæé«˜äº†ç”¨æˆ·äº¤äº’å¹¶æ‰©å±•äº†åº”ç”¨èŒƒå›´ï¼Œä»£è¡¨äº†äººå·¥æ™ºèƒ½çš„ä¸€å¤§è¿›æ­¥ã€‚</li>
<li>VLLMçš„å¤æ‚æ€§å¸¦æ¥äº†æ–°å‹å®‰å…¨å’Œé“å¾·æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€å’Œå¤šè½®å¯¹è¯ä¸­ã€‚</li>
<li>ä¼ ç»Ÿå®‰å…¨è¯„ä¼°æ¡†æ¶æ— æ³•åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå› æ­¤éœ€è¦æ–°çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>REVEALæ¡†æ¶æ˜¯ä¸€ä¸ªé’ˆå¯¹VLLMä¸­å›¾åƒè¾“å…¥å±å®³çš„è‡ªåŠ¨åŒ–è¯„ä¼°ç®¡é“ã€‚å®ƒåŒ…æ‹¬è‡ªåŠ¨åŒ–å›¾åƒæŒ–æ˜ã€åˆæˆå¯¹æŠ—æ•°æ®ç”Ÿæˆç­‰ã€‚</li>
<li>åœ¨è¯„ä¼°äº†äº”æ¬¾é¡¶å°–VLLMåï¼Œå‘ç°å¤šè½®äº¤äº’çš„ç¼ºé™·ç‡æ˜¾è‘—é«˜äºå•è½®è¯„ä¼°ã€‚</li>
<li>GPT-4oåœ¨å¹³è¡¡æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼ŒPixtralç´§éšå…¶åã€‚è¯¯ä¼ ä¿¡æ¯æˆä¸ºå…³é”®é¢†åŸŸï¼Œéœ€è¦å¢å¼ºä¸Šä¸‹æ–‡é˜²å¾¡æªæ–½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fca6f941b4737b0503b3311774b5793b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cf2e565402b67ef46d12aa596240dc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f8cff23d390255a295318652a528fa6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abf3dbbed7e7a8e6c223cbe4caa5ae78.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reward-SQL-Boosting-Text-to-SQL-via-Stepwise-Reasoning-and-Process-Supervised-Rewards"><a href="#Reward-SQL-Boosting-Text-to-SQL-via-Stepwise-Reasoning-and-Process-Supervised-Rewards" class="headerlink" title="Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and   Process-Supervised Rewards"></a>Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and   Process-Supervised Rewards</h2><p><strong>Authors:Yuxin Zhang, Meihao Fan, Ju Fan, Mingyang Yi, Yuyu Luo, Jian Tan, Guoliang Li</strong></p>
<p>Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation.To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a â€œcold start, then PRM supervisionâ€ paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning. Our code is publicly available. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥é€šè¿‡åˆ©ç”¨å…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ºäº†æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥å¤–éƒ¨è¿›ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ¥æä¾›ç²¾ç»†çš„ç›‘ç£ã€‚ç„¶è€Œï¼Œå¦‚æœè¯¯ç”¨PRMï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨ç†è½¨è¿¹å¤±çœŸï¼Œè¿›è€Œäº§ç”Ÿæ¬¡ä¼˜æˆ–é”™è¯¯çš„SQLç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Reward-SQLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåœ°å°†PRMçº³å…¥æ–‡æœ¬åˆ°SQLçš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•éµå¾ªâ€œå†·å¯åŠ¨ï¼Œç„¶åPRMç›‘ç£â€çš„æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨å¸¸è§è¡¨è¡¨è¾¾å¼ï¼ˆChain-of-CTEsï¼‰å°†SQLæŸ¥è¯¢åˆ†è§£ä¸ºç»“æ„åŒ–çš„é€æ­¥æ¨ç†é“¾ï¼Œå»ºç«‹å¼ºå¤§ä¸”å¯è§£é‡Šçš„æ¨ç†åŸºçº¿ã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å››ç§PRMé›†æˆç­–ç•¥ï¼Œå¹¶å‘ç°å°†PRMä½œä¸ºåœ¨çº¿è®­ç»ƒä¿¡å·ï¼ˆGRPOï¼‰ä¸PRMå¼•å¯¼æ¨ç†ï¼ˆä¾‹å¦‚ï¼Œæœ€ä½³Né‡‡æ ·ï¼‰ç›¸ç»“åˆå¯ä»¥è·å¾—æœ€ä½³ç»“æœã€‚åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šï¼ŒReward-SQLé€šè¿‡å—7B PRMç›‘ç£çš„æ¨¡å‹å®ç°äº†å„ç§æŒ‡å¯¼ç­–ç•¥çš„æ€§èƒ½æå‡13.1%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºäºQwen2.5-Coder-7B-Instructçš„GRPOå¯¹é½ç­–ç•¥æ¨¡å‹åœ¨BIRDå¼€å‘é›†ä¸Šè¾¾åˆ°äº†68.9%çš„å‡†ç¡®ç‡ï¼Œåœ¨ç›¸åŒæ¨¡å‹å¤§å°ä¸‹è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†Reward-SQLåœ¨åˆ©ç”¨å¥–åŠ±ç›‘ç£è¿›è¡Œæ–‡æœ¬åˆ°SQLæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04671v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Text-to-SQLä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œé€šè¿‡å¼•å…¥å¤–éƒ¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä»¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ã€‚ä¸ºæœ‰æ•ˆèå…¥PRMï¼Œæå‡ºReward-SQLæ¡†æ¶ï¼Œé‡‡ç”¨â€œå†·å¯åŠ¨ï¼ŒåPRMç›‘ç£â€çš„ç­–ç•¥ï¼Œå…ˆè®­ç»ƒæ¨¡å‹ä½¿ç”¨å¸¸è§è¡¨è¡¨è¾¾å¼ï¼ˆChain-of-CTEsï¼‰åˆ†è§£SQLæŸ¥è¯¢ä¸ºç»“æ„åŒ–çš„æ¨ç†é“¾ï¼Œå†æ¢ç´¢å››ç§PRMæ•´åˆç­–ç•¥ï¼Œå‘ç°å°†PRMä½œä¸ºåœ¨çº¿è®­ç»ƒä¿¡å·ä¸PRMå¼•å¯¼æ¨ç†ç›¸ç»“åˆæ•ˆæœæœ€ä½³ã€‚åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨7B PRMç›‘ç£çš„Reward-SQLæ¨¡å‹å®ç°13.1%çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Text-to-SQLä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>å¤–éƒ¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¯ä»¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ã€‚</li>
<li>Reward-SQLæ¡†æ¶é‡‡ç”¨â€œå†·å¯åŠ¨ï¼ŒåPRMç›‘ç£â€ç­–ç•¥ï¼Œæœ‰æ•ˆèå…¥PRMã€‚</li>
<li>é€šè¿‡æ¢ç´¢å››ç§PRMæ•´åˆç­–ç•¥ï¼Œå‘ç°å°†PRMä½œä¸ºåœ¨çº¿è®­ç»ƒä¿¡å·ä¸PRMå¼•å¯¼æ¨ç†ç»“åˆæ•ˆæœæœ€ä½³ã€‚</li>
<li>åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šï¼ŒReward-SQLæ¨¡å‹ä½¿ç”¨7B PRMç›‘ç£å®ç°13.1%æ€§èƒ½æå‡ã€‚</li>
<li>GRPOå¯¹é½ç­–ç•¥æ¨¡å‹åœ¨BIRDå¼€å‘é›†ä¸Šè¾¾åˆ°68.9%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºåŒè§„æ¨¡åŸºçº¿æ–¹æ³•ã€‚</li>
<li>Reward-SQLæ¡†æ¶å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fee6e5a4e18d9987198055b6d2550ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70eb1f26a378f06cd3feb2456033d60d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c05fd3e44619842c3cfd537e055fd165.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FRAME-Feedback-Refined-Agent-Methodology-for-Enhancing-Medical-Research-Insights"><a href="#FRAME-Feedback-Refined-Agent-Methodology-for-Enhancing-Medical-Research-Insights" class="headerlink" title="FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research   Insights"></a>FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research   Insights</h2><p><strong>Authors:Chengzhang Yu, Yiming Zhang, Zhixin Liu, Zenghui Ding, Yining Sun, Zhanpeng Jin</strong></p>
<p>The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAMEâ€™s effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards. </p>
<blockquote>
<p>é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°ç§‘å­¦ç ”ç©¶çš„è‡ªåŠ¨åŒ–ï¼Œè™½ç„¶å¸¦æ¥äº†é‡è¦æœºé‡ï¼Œä½†åœ¨çŸ¥è¯†ç»¼åˆå’Œè´¨é‡ä¿è¯æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä»‹ç»äº†Feedback-Refined Agent Methodologyï¼ˆFRAMEï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œç»“æ„åŒ–åé¦ˆæ¥æé«˜åŒ»å­¦è®ºæ–‡çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ç»“æ„åŒ–æ•°æ®é›†æ„å»ºæ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–å°†4287ç¯‡åŒ»å­¦è®ºæ–‡åˆ†è§£ä¸ºåŸºæœ¬ç ”ç©¶æˆåˆ†ï¼›ï¼ˆ2ï¼‰é›†æˆäº†ç”Ÿæˆå™¨ã€è¯„ä¼°å™¨å’Œåå°„å™¨ä¸‰ç§ä»£ç†çš„ä¸‰æ–¹æ¶æ„ï¼Œé€šè¿‡æŒ‡æ ‡é©±åŠ¨çš„åé¦ˆé€æ­¥æ”¹è¿›å†…å®¹è´¨é‡ï¼›ï¼ˆ3ï¼‰ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆç»Ÿè®¡æŒ‡æ ‡å’ŒåŸºäºäººç±»åŸºå‡†çš„åŸºå‡†è¿›è¡Œæµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFRAMEçš„æœ‰æ•ˆæ€§åœ¨å¤šä¸ªæ¨¡å‹å’Œè¯„ä¼°ç»´åº¦ä¸Šéƒ½å®ç°äº†å¯¹ä¼ ç»Ÿæ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ï¼ˆDeepSeek V3å¹³å‡æå‡9.91%ï¼Œä¸GPT-4o Miniç›¸æ¯”ä¹Ÿæœ‰æ˜¾è‘—æ”¹è¿›ï¼‰ã€‚äººç±»è¯„ä¼°è¯å®ï¼ŒFRAMEç”Ÿæˆçš„è®ºæ–‡è´¨é‡å¯ä¸äººç±»æ’°å†™çš„ä½œå“ç›¸åª²ç¾ï¼Œå°¤å…¶åœ¨ç»¼åˆæœªæ¥ç ”ç©¶æ–¹å‘æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚ç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¯ä»¥é€šè¿‡ä¸ºè‡ªåŠ¨åŒ–åŒ»å­¦è®ºæ–‡ç”Ÿæˆå»ºç«‹ç¨³å¥çš„åŸºç¡€ï¼ŒåŒæ—¶ä¿æŒä¸¥æ ¼çš„å­¦æœ¯æ ‡å‡†ï¼Œæœ‰æ•ˆåœ°è¾…åŠ©åŒ»å­¦ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04649v1">PDF</a> 12 pages, 4 figures, 5 table</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘ç ”è‡ªåŠ¨åŒ–æ–¹é¢å¸¦æ¥äº†æœºé‡ï¼Œä½†åœ¨çŸ¥è¯†åˆæˆå’Œè´¨é‡ä¿è¯ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åé¦ˆç²¾ç»†åŒ–ä»£ç†æ–¹æ³•ï¼ˆFRAMEï¼‰ï¼Œé€šè¿‡è¿­ä»£ç»†åŒ–å’Œç»“æ„åŒ–åé¦ˆæé«˜åŒ»å­¦è®ºæ–‡ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šç»“æ„åŒ–æ•°æ®é›†æ„å»ºæ–¹æ³•ã€é›†æˆäº†ç”Ÿæˆå™¨ã€è¯„ä¼°å™¨å’Œåå°„å™¨çš„ä¸‰æ–¹æ¶æ„ä»¥åŠç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFRAMEåœ¨å¤šä¸ªæ¨¡å‹å’Œè¯„ä»·ç»´åº¦ä¸Šå‡å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œç”Ÿæˆçš„è®ºæ–‡è´¨é‡ä¸äººç±»æ’°å†™çš„è®ºæ–‡ç›¸å½“ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªæ¥ç ”ç©¶æ–¹å‘çš„åˆæˆä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘ç ”è‡ªåŠ¨åŒ–ä¸­å¸¦æ¥æœºé‡ï¼Œä½†çŸ¥è¯†åˆæˆå’Œè´¨é‡ä¿è¯ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>FRAMEæ˜¯ä¸€ç§é€šè¿‡è¿­ä»£ç»†åŒ–å’Œç»“æ„åŒ–åé¦ˆæé«˜åŒ»å­¦è®ºæ–‡ç”Ÿæˆè´¨é‡çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>FRAMEåŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šç»“æ„åŒ–æ•°æ®é›†æ„å»ºã€ä¸‰æ–¹æ¶æ„å’Œç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFRAMEåœ¨å¤šä¸ªæ¨¡å‹å’Œè¯„ä»·ç»´åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</li>
<li>FRAMEç”Ÿæˆçš„è®ºæ–‡è´¨é‡ä¸äººç±»æ’°å†™çš„è®ºæ–‡ç›¸å½“ã€‚</li>
<li>FRAMEåœ¨æœªæ¥ç ”ç©¶æ–¹å‘çš„åˆæˆä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d88c3d74e2ca07b9ca7df59a1ab5eb06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea251f1e5f12362d729a0496983464ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-572e726abfe44a87685e0c9593baca53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1d4881663466e1e52c9b00962fc2a39.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Uncovering-the-Limitations-of-Model-Inversion-Evaluation-â€“-Benchmarks-and-Connection-to-Type-I-Adversarial-Attacks"><a href="#Uncovering-the-Limitations-of-Model-Inversion-Evaluation-â€“-Benchmarks-and-Connection-to-Type-I-Adversarial-Attacks" class="headerlink" title="Uncovering the Limitations of Model Inversion Evaluation â€“ Benchmarks   and Connection to Type-I Adversarial Attacks"></a>Uncovering the Limitations of Model Inversion Evaluation â€“ Benchmarks   and Connection to Type-I Adversarial Attacks</h2><p><strong>Authors:Sy-Tuyen Ho, Koh Jun Hao, Ngoc-Bao Nguyen, Alexander Binder, Ngai-Man Cheung</strong></p>
<p>Model Inversion (MI) attacks aim to reconstruct information of private training data by exploiting access to machine learning models. The most common evaluation framework for MI attacks&#x2F;defenses relies on an evaluation model that has been utilized to assess progress across almost all MI attacks and defenses proposed in recent years. In this paper, for the first time, we present an in-depth study of MI evaluation. Firstly, we construct the first comprehensive human-annotated dataset of MI attack samples, based on 28 setups of different MI attacks, defenses, private and public datasets. Secondly, using our dataset, we examine the accuracy of the MI evaluation framework and reveal that it suffers from a significant number of false positives. These findings raise questions about the previously reported success rates of SOTA MI attacks. Thirdly, we analyze the causes of these false positives, design controlled experiments, and discover the surprising effect of Type I adversarial features on MI evaluation, as well as adversarial transferability, highlighting a relationship between two previously distinct research areas. Our findings suggest that the performance of SOTA MI attacks has been overestimated, with the actual privacy leakage being significantly less than previously reported. In conclusion, we highlight critical limitations in the widely used MI evaluation framework and present our methods to mitigate false positive rates. We remark that prior research has shown that Type I adversarial attacks are very challenging, with no existing solution. Therefore, we urge to consider human evaluation as a primary MI evaluation framework rather than merely a supplement as in previous MI research. We also encourage further work on developing more robust and reliable automatic evaluation frameworks. </p>
<blockquote>
<p>æ¨¡å‹åè½¬ï¼ˆMIï¼‰æ”»å‡»æ—¨åœ¨é€šè¿‡è®¿é—®æœºå™¨å­¦ä¹ æ¨¡å‹æ¥é‡å»ºç§æœ‰è®­ç»ƒæ•°æ®çš„ä¿¡æ¯ã€‚MIæ”»å‡»&#x2F;é˜²å¾¡çš„æœ€å¸¸è§è¯„ä¼°æ¡†æ¶ä¾èµ–äºä¸€ä¸ªè¯„ä¼°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è¿‘å‡ å¹´æå‡ºçš„å‡ ä¹æ‰€æœ‰MIæ”»å‡»å’Œé˜²å¾¡æªæ–½ä¸­éƒ½ç”¨äºè¯„ä¼°è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹MIè¯„ä¼°è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªå…¨é¢çš„MIæ”»å‡»æ ·æœ¬çš„äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŸºäºä¸åŒçš„MIæ”»å‡»ã€é˜²å¾¡æªæ–½ã€ç§æœ‰å’Œå…¬å…±æ•°æ®é›†çš„28ä¸ªè®¾ç½®ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†MIè¯„ä¼°æ¡†æ¶çš„å‡†ç¡®æ€§ï¼Œå¹¶å‘ç°å®ƒå­˜åœ¨å¤§é‡çš„è¯¯æŠ¥ã€‚è¿™äº›å‘ç°å¯¹å…ˆå‰æŠ¥å‘Šçš„æœ€æ–°MIæ”»å‡»çš„æˆåŠŸç‡æå‡ºäº†è´¨ç–‘ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›è¯¯æŠ¥çš„åŸå› ï¼Œè®¾è®¡äº†å—æ§å®éªŒï¼Œå¹¶å‘ç°äº†ç¬¬ä¸€ç±»å¯¹æŠ—ç‰¹å¾å¯¹MIè¯„ä¼°çš„æƒŠäººå½±å“ä»¥åŠå¯¹å¯¹æŠ—å¯è¿ç§»æ€§çš„å…³æ³¨ï¼Œçªå‡ºäº†ä¸¤ä¸ªå…ˆå‰ä¸åŒçš„ç ”ç©¶é¢†åŸŸä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€æ–°MIæ”»å‡»çš„æ€§èƒ½è¢«é«˜ä¼°äº†ï¼Œå®é™…çš„éšç§æ³„éœ²ç¨‹åº¦è¿œè¿œä½äºå…ˆå‰çš„æŠ¥å‘Šã€‚æœ€åï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†å¹¿æ³›ä½¿ç”¨çš„MIè¯„ä¼°æ¡†æ¶çš„å…³é”®å±€é™æ€§ï¼Œå¹¶æå‡ºäº†é™ä½è¯¯æŠ¥ç‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ç¬¬ä¸€ç±»å¯¹æŠ—æ€§æ”»å‡»éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ä¸”å°šæ— è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ•¦ä¿ƒå°†äººå·¥è¯„ä¼°ä½œä¸ºä¸»è¦çš„MIè¯„ä¼°æ¡†æ¶ï¼Œè€Œä¸æ˜¯åƒå…ˆå‰çš„MIç ”ç©¶é‚£æ ·ä»…ä»…ä½œä¸ºè¡¥å……ã€‚æˆ‘ä»¬ä¹Ÿé¼“åŠ±è¿›ä¸€æ­¥å¼€å‘æ›´ç¨³å¥å’Œå¯é çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03519v2">PDF</a> Our dataset and code are available in the Supp</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ¨¡å‹åæ¼”ï¼ˆMIï¼‰æ”»å‡»æ—¨åœ¨é€šè¿‡è®¿é—®æœºå™¨å­¦ä¹ æ¨¡å‹æ¥é‡å»ºç§æœ‰è®­ç»ƒæ•°æ®çš„ä¿¡æ¯ã€‚å¯¹äºMIæ”»å‡»å’Œé˜²å¾¡ç­–ç•¥çš„æœ€å¸¸è§è¯„ä¼°æ¡†æ¶ä¸»è¦ä¾èµ–äºä¸€ä¸ªè¯„ä¼°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æœ€è¿‘å‡ å¹´æå‡ºçš„å‡ ä¹æ‰€æœ‰MIæ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ä¸­éƒ½å¾—åˆ°äº†åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹MIè¯„ä¼°è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®å¤šç§MIæ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ä»¥åŠå…¬å…±å’Œç§æœ‰æ•°æ®é›†å»ºç«‹äº†é¦–ä¸ªå…¨é¢çš„äººå·¥æ³¨é‡Šçš„MIæ”»å‡»æ ·æœ¬æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†å‘ç°MIè¯„ä¼°æ¡†æ¶çš„å‡†ç¡®æ€§å­˜åœ¨é—®é¢˜ï¼Œå¹¶å­˜åœ¨å¤§é‡è¯¯æŠ¥ã€‚è¿™äº›å‘ç°å¯¹å…ˆå‰æŠ¥å‘Šçš„å…ˆè¿›MIæ”»å‡»çš„æˆåŠŸç‡æå‡ºäº†è´¨ç–‘ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›è¯¯æŠ¥çš„åŸå› ï¼Œè®¾è®¡äº†å—æ§å®éªŒï¼Œå¹¶å‘ç°äº†Type Iå¯¹æŠ—æ€§ç‰¹å¾å¯¹MIè¯„ä¼°çš„æƒŠäººå½±å“ä»¥åŠå¯¹å¯¹æŠ—æ€§å¯è¿ç§»æ€§çš„å…³æ³¨ï¼Œçªæ˜¾äº†ä¹‹å‰ä¸¤ä¸ªæˆªç„¶ä¸åŒçš„ç ”ç©¶é¢†åŸŸä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…ˆè¿›MIæ”»å‡»çš„æ€§èƒ½è¢«é«˜ä¼°äº†ï¼Œå®é™…çš„éšç§æ³„éœ²æƒ…å†µæ¯”ä¹‹å‰æŠ¥é“çš„è¦å°‘å¾—å¤šã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å¹¿æ³›åº”ç”¨äºMIè¯„ä¼°çš„æ¡†æ¶çš„å…³é”®å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æˆ‘ä»¬é™ä½è¯¯æŠ¥ç‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜Type Iå¯¹æŠ—æ€§æ”»å‡»éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ä¸”å°šæ— è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå€¡å°†äººå·¥è¯„ä¼°è§†ä¸ºä¸»è¦çš„MIè¯„ä¼°æ¡†æ¶è€Œä¸ä»…ä»…ä½œä¸ºä»¥å‰MIç ”ç©¶çš„è¡¥å……éƒ¨åˆ†ã€‚æˆ‘ä»¬ä¹Ÿé¼“åŠ±è¿›ä¸€æ­¥å¼€å‘æ›´ç¨³å¥å’Œå¯é çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ„å»ºäº†é¦–ä¸ªå…¨é¢çš„äººå·¥æ³¨é‡Šçš„MIæ”»å‡»æ ·æœ¬æ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒçš„MIæ”»å‡»ã€é˜²å¾¡ç­–ç•¥ã€ç§æœ‰å’Œå…¬å…±æ•°æ®é›†ã€‚</li>
<li>å‘ç°å¹¶åˆ†æäº†MIè¯„ä¼°æ¡†æ¶å­˜åœ¨å¤§é‡è¯¯æŠ¥çš„é—®é¢˜ã€‚</li>
<li>æ­ç¤ºäº†Type Iå¯¹æŠ—æ€§ç‰¹å¾å¯¹MIè¯„ä¼°çš„å½±å“ä»¥åŠå¯¹æŠ—æ€§å¯è¿ç§»æ€§çš„é‡è¦æ€§ã€‚</li>
<li>æŒ‡å‡ºå…ˆè¿›MIæ”»å‡»çš„æ€§èƒ½è¢«é«˜ä¼°ï¼Œå®é™…éšç§æ³„éœ²æƒ…å†µä½äºå…ˆå‰æŠ¥å‘Šã€‚</li>
<li>å¼ºè°ƒäº†ç°æœ‰MIè¯„ä¼°æ¡†æ¶çš„å…³é”®å±€é™æ€§ï¼Œå¹¶æå‡ºäº†é™ä½è¯¯æŠ¥ç‡çš„æ–¹æ³•ã€‚</li>
<li>æå€¡å°†äººå·¥è¯„ä¼°è§†ä¸ºä¸»è¦çš„MIè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>é¼“åŠ±å¼€å‘æ›´ç¨³å¥å’Œå¯é çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dde8804119106cecf6905bce79954c1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a8efba22c18d66a2ac4b7564a67077a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423489fa9feead36ad682895b0b29b07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbef995f3e4374d373b099dc055172d0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation"><a href="#SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation" class="headerlink" title="SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation"></a>SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation</h2><p><strong>Authors:Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song</strong></p>
<p>Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan â€“ a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics. </p>
<blockquote>
<p>åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡ã€åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œé«˜æ•ˆè·¯å¾„è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„é«˜è®¡ç®—æˆæœ¬å’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰é™é€‚åº”æ€§é˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶éƒ¨ç½²ã€‚æˆ‘ä»¬æå‡ºäº†SmallPlanâ€”â€”ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ–°å‹æ¡†æ¶ã€‚åœ¨SmallPlanä¸­ï¼ŒSLMæä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—ï¼Œä»¥åœ¨ç´§å‡‘åœ°ä»£è¡¨å…¨å°ºå¯¸3Dåœºæ™¯çš„åœºæ™¯å›¾ä¸­è¿›è¡Œå¯¼èˆªã€‚SLMä»¥æ¨¡æ‹Ÿé©±åŠ¨çš„æ–¹å¼ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒã€‚è¿™ç§ç­–ç•¥ä¸ä»…ä½¿SLMèƒ½å¤ŸæˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè€Œä¸”ä½¿å…¶èƒ½å¤Ÿæ„è¯†åˆ°æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ç»è¿‡å¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹åœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢çš„è¡¨ç°ä¸GPT-4ç­‰å¤§å‹æ¨¡å‹ç›¸å½“ï¼Œä¸ä¼šå‡ºç°å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆçš„æƒ…å†µã€‚SmallPlanèµ„æºé«˜æ•ˆï¼Œéå¸¸é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ï¼Œä¸ºæ¨åŠ¨å®é™…åº”ç”¨ä¸­çš„è‡ªä¸»æœºå™¨äººæŠ€æœ¯æä¾›äº†åŠ¨åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00831v3">PDF</a> Paper is under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SmallPlanæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒè½»é‡çº§çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ï¼Œç”¨äºæœºå™¨äººè·¯å¾„è§„åˆ’ä»»åŠ¡ã€‚SmallPlané€šè¿‡SLMæä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—ï¼Œåœ¨åœºæ™¯å›¾ä¸­è¿›è¡Œå¯¼èˆªï¼Œè¿™äº›åœºæ™¯å›¾ç´§å‡‘åœ°è¡¨ç¤ºå…¨å°ºå¯¸çš„3Dåœºæ™¯ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨æ¨¡æ‹Ÿä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼ï¼Œä½¿SLMä¸ä»…èƒ½æˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜èƒ½è€ƒè™‘æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„SLMåœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸GPTç­‰å¤§æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”èµ„æºæ•ˆç‡é«˜ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmallPlanæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºæ•™å¸ˆæ¨¡å‹è®­ç»ƒè½»é‡çº§çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ï¼Œç”¨äºæœºå™¨äººè·¯å¾„è§„åˆ’ä»»åŠ¡ã€‚</li>
<li>SLMåœ¨åœºæ™¯å›¾ä¸­æä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—è¿›è¡Œå¯¼èˆªï¼Œåœºæ™¯å›¾ç´§å‡‘è¡¨ç¤ºå…¨å°ºå¯¸3Dåœºæ™¯ã€‚</li>
<li>è®­ç»ƒé‡‡ç”¨æ¨¡æ‹Ÿç»“åˆå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä½¿å¾—SLMä¸ä»…èƒ½å®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜èƒ½è€ƒè™‘æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚</li>
<li>ç»è¿‡è®­ç»ƒçš„SLMè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢ä¸å¤§å‹æ¨¡å‹å¦‚GPT-4oç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>SmallPlanæ¡†æ¶çš„èµ„æºæ•ˆç‡é«˜ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºè§£å†³æœºå™¨äººåœ¨å¤§è§„æ¨¡åŠ¨æ€ç¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’éš¾é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-25f8367d9c81f38282b4ddd803eab39a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d4bfd2544253742e0f454092cee4cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8128331a572e8ccf69601b9baaf08878.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VL-Rethinker-Incentivizing-Self-Reflection-of-Vision-Language-Models-with-Reinforcement-Learning"><a href="#VL-Rethinker-Incentivizing-Self-Reflection-of-Vision-Language-Models-with-Reinforcement-Learning" class="headerlink" title="VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models   with Reinforcement Learning"></a>VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models   with Reinforcement Learning</h2><p><strong>Authors:Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1â€™s performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. Our empirical results show the effectiveness of our approaches. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¦‚GPT-o1å’ŒDeepSeek-R1ä¹‹ç±»çš„æ…¢æ€è€ƒç³»ç»Ÿå·²é€šè¿‡æ˜ç¡®çš„åæ€å±•ç°å‡ºè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§é—®é¢˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚åœ¨å„ç§æ•°å­¦å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒä»¬çš„è¡¨ç°è¿œèƒœäºæœ€ä½³çš„å¿«æ€è€ƒæ¨¡å‹ï¼Œä¾‹å¦‚GPT-4oã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢ä¸å¿«æ€è€ƒæ¨¡å‹æŒå¹³ã€‚ä¾‹å¦‚ï¼ŒGPT-o1åœ¨MathVistaã€MathVerseå’ŒMathVisionç­‰åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¸å¿«æ€è€ƒæ¨¡å‹ç›¸ä¼¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆæ— éœ€ä¾èµ–è’¸é¦ï¼‰æ¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ï¼Œä»¥æ¨åŠ¨ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨GRPOç®—æ³•ï¼Œå¹¶ç»“åˆä¸€ç§åä¸ºé€‰æ‹©æ€§æ ·æœ¬å›æ”¾ï¼ˆSSRï¼‰çš„æ–°æŠ€æœ¯æ¥è§£å†³ä¼˜åŠ¿æ¶ˆå¤±çš„é—®é¢˜ã€‚è™½ç„¶è¿™ç§æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å¾—åˆ°çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„è‡ªæˆ‘åæ€æˆ–è‡ªæˆ‘éªŒè¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ…¢æ€è€ƒèƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¼ºåˆ¶åæ€ï¼Œé€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„rolloutç»“å°¾å¤„æ·»åŠ ä¸€ä¸ªåæ€è§¦å‘ä»¤ç‰Œï¼Œæ˜ç¡®æ‰§è¡Œè‡ªæˆ‘åæ€æ¨ç†æ­¥éª¤ã€‚é€šè¿‡ç»“åˆè¿™ä¸¤ç§æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹VL-Rethinkeråœ¨MathVistaå’ŒMathVerseä¸Šçš„æˆç»©è¾¾åˆ°äº†æ–°çš„å›½å®¶æ°´å¹³æ ‡å‡†è¯„ä¼°æ ‡å‡†çš„é¢†å…ˆä½ç½®ï¼Œåˆ†åˆ«è¾¾åˆ°äº†80.4%å’Œ63.5%ã€‚VL-Rethinkerè¿˜åœ¨è·¨å­¦ç§‘åŸºå‡†æµ‹è¯•å¦‚MathVisionã€MMMU-Proã€EMMAå’ŒMEGA-Benchä¸Šå®ç°äº†å¼€æºçŠ¶æ€çš„é¡¶çº§æ°´å¹³è¡¨ç°ã€‚ç¼©å°ä¸OpenAI-o1ä¹‹é—´çš„å·®è·è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æˆåŠŸå’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»éªŒæ€§ç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08837v3">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¢å¼ºæ…¢æ€è€ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ï¼Œé€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆä¸ä¾èµ–è’¸é¦ï¼‰æ¥æå‡å…¶æ€§èƒ½ã€‚æ–‡ç« ä»‹ç»äº†ä¸¤ç§æŠ€æœ¯ï¼šé€‰æ‹©æ€§æ ·æœ¬å›æ”¾ï¼ˆSSRï¼‰å’Œå¼ºåˆ¶åæ€ï¼Œä»¥é¼“åŠ±æ¨¡å‹è¿›è¡Œæ›´å¤šçš„æ…¢æ€è€ƒã€‚ç»“åˆè¿™ä¸¤ç§æŠ€æœ¯ï¼ŒVL-Rethinkeræ¨¡å‹åœ¨MathVistaã€MathVerseç­‰æ•°å­¦å’Œè·¨å­¦ç§‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…ˆè¿›åˆ†æ•°ï¼Œç¼©å°äº†ä¸OpenAI-o1çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ…¢æ€è€ƒç³»ç»Ÿå¦‚GPT-o1å’ŒDeepSeek-R1é€šè¿‡æ˜ç¡®çš„åæ€åœ¨è§£å†³æŒ‘æˆ˜æ€§é—®é¢˜ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå¿«é€Ÿæ€è€ƒæ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡ç›®æ ‡æ˜¯ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ï¼Œä»¥æå‡å…¶æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨GRPOç®—æ³•å’Œé€‰æ‹©æ€§æ ·æœ¬å›æ”¾ï¼ˆSSRï¼‰æŠ€æœ¯æ¥è§£å†³ä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚</li>
<li>RLè®­ç»ƒæ¨¡å‹è‡ªæˆ‘åæ€æˆ–è‡ªæˆ‘éªŒè¯æœ‰é™ï¼Œå› æ­¤å¼•å…¥å¼ºåˆ¶åæ€æ¥è¿›ä¸€æ­¥é¼“åŠ±æ…¢æ€è€ƒã€‚</li>
<li>ç»“åˆä»¥ä¸Šä¸¤ç§æŠ€æœ¯ï¼ŒVL-Rethinkeræ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…ˆè¿›åˆ†æ•°ï¼Œå¦‚MathVistaã€MathVerseç­‰ã€‚</li>
<li>VL-Rethinkeråœ¨å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•å¦‚MathVisionã€MMMU-Proã€EMMAå’ŒMEGA-Benchä¸Šä¹Ÿå–å¾—äº†å¼€æ”¾æºä»£ç çš„å…ˆè¿›åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a7f0cdde9b27410ddaaa51e0f64d8af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f6e398845f24a991ad9b3ceb9ff28de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17a73e965d07f8fa2171bfd6ebc521ee.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models"><a href="#Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models" class="headerlink" title="Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models"></a>Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models</h2><p><strong>Authors:Teng Wang, Zhangyi Jiang, Zhenqi He, Shenyang Tong, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Hailei Gong</strong></p>
<p>Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate step. In addition, the cost of annotating reasoning processes for reward modeling is high, making large-scale collection of high-quality data challenging. To address this, we propose a novel reward model approach called the Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps at both fine-grained and coarse-grained levels. HRM excels at assessing multi-step reasoning coherence, especially when flawed steps are later corrected through self-reflection. To further reduce the cost of generating training data, we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC), which merges two consecutive reasoning steps into one within the tree structure. By applying HNC to MCTS-generated reasoning trajectories, we enhance the diversity and robustness of HRM training data while introducing controlled noise with minimal computational overhead. Empirical results on the PRM800K dataset show that HRM, together with HNC, provides more stable and reliable evaluations than PRM. Furthermore, cross-domain evaluations on the MATH500 and GSM8K datasets demonstrate HRMâ€™s strong generalization and robustness across a variety of reasoning tasks. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å®ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ç§å…³é”®æ–¹æ³•â€”â€”è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±ä½œå¼Šçš„é—®é¢˜ï¼Œä½¿å…¶åœ¨è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤æ—¶ä¸å¯é ã€‚æ­¤å¤–ï¼Œä¸ºå¥–åŠ±æ¨¡å‹æ ‡æ³¨æ¨ç†è¿‡ç¨‹çš„æˆæœ¬å¾ˆé«˜ï¼Œä½¿å¾—æ”¶é›†é«˜è´¨é‡æ•°æ®çš„ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•ï¼Œç§°ä¸ºåˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œå®ƒå¯ä»¥åœ¨ç²¾ç»†ç²’åº¦å’Œç²—ç•¥ç²’åº¦ä¸Šè¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ã€‚HRMåœ¨è¯„ä¼°å¤šæ­¥éª¤æ¨ç†è¿è´¯æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨åç»­æ­¥éª¤é€šè¿‡è‡ªæˆ‘åæ€çº æ­£é”™è¯¯æ—¶ã€‚ä¸ºè¿›ä¸€æ­¥é™ä½ç”Ÿæˆè®­ç»ƒæ•°æ®çš„æˆæœ¬ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½»ä¾¿æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç§°ä¸ºåˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰ï¼Œå®ƒå°†ä¸¤ä¸ªè¿ç»­çš„æ¨ç†æ­¥éª¤åˆå¹¶åˆ°æ ‘ç»“æ„ä¸­çš„ä¸€æ­¥ã€‚é€šè¿‡å°†HNCåº”ç”¨äºMCTSç”Ÿæˆçš„æ¨ç†è½¨è¿¹ï¼Œæˆ‘ä»¬åœ¨å¼•å…¥å¯æ§å™ªå£°å’Œæœ€å°è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œå¢å¼ºäº†HRMè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œç¨³å¥æ€§ã€‚åœ¨PRM800Kæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒHRMä¸HNCç›¸ç»“åˆæä¾›çš„è¯„ä¼°æ¯”PRMæ›´ç¨³å®šã€æ›´å¯é ã€‚æ­¤å¤–ï¼ŒMATH500å’ŒGSM8Kæ•°æ®é›†ä¸Šçš„è·¨åŸŸè¯„ä¼°è¯æ˜äº†HRMåœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­çš„å¼ºå¤§é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13551v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸­çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œéš¾ä»¥è¯†åˆ«æœ€ä½³çš„ä¸­é—´æ­¥éª¤ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œèƒ½å¤Ÿç²¾ç»†åœ°è¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ã€‚ä¸ºé™ä½ç”Ÿæˆè®­ç»ƒæ•°æ®æˆæœ¬ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è½»é‡çº§ã€é«˜æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥â€”â€”åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰ã€‚åœ¨PRM800Kæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒHRMç»“åˆHNCè¡¨ç°æ›´ç¨³å®šå¯é ã€‚åŒæ—¶ï¼Œè·¨åŸŸè¯„ä¼°æ˜¾ç¤ºHRMåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œéš¾ä»¥è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤ã€‚</li>
<li>æå‡ºäº†åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œèƒ½å¤Ÿè¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶æ“…é•¿è¯„ä¼°å¤šæ­¥éª¤æ¨ç†çš„è¿è´¯æ€§ã€‚</li>
<li>HRMå¯ä»¥é€šè¿‡è‡ªæˆ‘åæ€æ¥è¯„ä¼°å³ä½¿åœ¨åç»­æ­¥éª¤ä¸­çº æ­£çš„é”™è¯¯æ­¥éª¤ã€‚</li>
<li>å¼•å…¥åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰ç­–ç•¥æ¥é™ä½ç”Ÿæˆè®­ç»ƒæ•°æ®çš„æˆæœ¬ï¼Œå¢å¼ºHRMè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>HRMç»“åˆHNCåœ¨PRM800Kæ•°æ®é›†ä¸Šçš„è¡¨ç°æ›´ç¨³å®šå¯é ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ed7f0c2314c8a7bce2b806cbbf98ca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-260795d0451412d32c723c2df113237e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be796bbb9c05e06190dbe9609b22d913.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8acf619d271f19a298866eabb78cd983.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b3e47b17fdbe17c4a1384ecc86d1cff1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cfc5b75a5a681d68f4d27a1dca63a08.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="How-Do-Multimodal-Large-Language-Models-Handle-Complex-Multimodal-Reasoning-Placing-Them-in-An-Extensible-Escape-Game"><a href="#How-Do-Multimodal-Large-Language-Models-Handle-Complex-Multimodal-Reasoning-Placing-Them-in-An-Extensible-Escape-Game" class="headerlink" title="How Do Multimodal Large Language Models Handle Complex Multimodal   Reasoning? Placing Them in An Extensible Escape Game"></a>How Do Multimodal Large Language Models Handle Complex Multimodal   Reasoning? Placing Them in An Extensible Escape Game</h2><p><strong>Authors:Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, Yang Liu</strong></p>
<p>The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred interest in complex multimodal reasoning tasks in the real-world and virtual environment, which require coordinating multiple abilities, including visual perception, visual reasoning, spatial awareness, and target deduction. However, existing evaluations primarily assess the final task completion, often degrading assessments to isolated abilities such as visual grounding and visual question answering. Less attention is given to comprehensively and quantitatively analyzing reasoning process in multimodal environments, which is crucial for understanding model behaviors and underlying reasoning mechanisms beyond merely task success. To address this, we introduce MM-Escape, an extensible benchmark for investigating multimodal reasoning, inspired by real-world escape games. MM-Escape emphasizes intermediate model behaviors alongside final task completion. To achieve this, we develop EscapeCraft, a customizable and open environment that enables models to engage in free-form exploration for assessing multimodal reasoning. Extensive experiments show that MLLMs, regardless of scale, can successfully complete the simplest room escape tasks, with some exhibiting human-like exploration strategies. Yet, performance dramatically drops as task difficulty increases. Moreover, we observe that performance bottlenecks vary across models, revealing distinct failure modes and limitations in their multimodal reasoning abilities, such as repetitive trajectories without adaptive exploration, getting stuck in corners due to poor visual spatial awareness, and ineffective use of acquired props, such as the key. We hope our work sheds light on new challenges in multimodal reasoning, and uncovers potential improvements in MLLMs capabilities. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿè¿›æ­¥ï¼Œç°å®ä¸–ç•Œå’Œè™šæ‹Ÿç¯å¢ƒä¸­çš„å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å¼•å‘äº†äººä»¬çš„å…´è¶£ã€‚è¿™äº›ä»»åŠ¡éœ€è¦åè°ƒå¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§†è§‰æ„ŸçŸ¥ã€è§†è§‰æ¨ç†ã€ç©ºé—´æ„è¯†å’Œç›®æ ‡æ¨æ–­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­äºæœ€ç»ˆä»»åŠ¡å®Œæˆæƒ…å†µçš„è¯„ä¼°ï¼Œå¾€å¾€å°†è¯„ä¼°ç®€åŒ–ä¸ºå­¤ç«‹çš„è§†è§‰å®šä½èƒ½åŠ›å’Œè§†è§‰é—®ç­”èƒ½åŠ›ã€‚å¯¹äºåœ¨å¤šç§æ¨¡æ€ç¯å¢ƒä¸­å…¨é¢å’Œå®šé‡åœ°åˆ†ææ¨ç†è¿‡ç¨‹ï¼Œä»¥åŠäº†è§£æ¨¡å‹è¡Œä¸ºå’Œæ¨ç†æœºåˆ¶çš„é‡è¦æ€§å´è¢«å¿½è§†ï¼Œè¿™ä¸ä»…å…³ä¹ä»»åŠ¡çš„æˆåŠŸä¸å¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MM-Escapeï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œå…¶çµæ„Ÿæ¥æºäºç°å®ä¸–ç•Œçš„è§£è°œæ¸¸æˆã€‚MM-Escapeå¼ºè°ƒæ¨¡å‹åœ¨å®Œæˆæœ€ç»ˆä»»åŠ¡è¿‡ç¨‹ä¸­çš„ä¸­é—´è¡Œä¸ºã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼€å‘äº†EscapeCraftï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å®šåˆ¶å’Œå¼€æ”¾çš„ç¯å¢ƒï¼Œè®©æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œè‡ªç”±å½¢å¼çš„æ¢ç´¢æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºè§„æ¨¡å¤§å°ï¼ŒMLLMséƒ½èƒ½æˆåŠŸå®Œæˆæœ€ç®€å•çš„æˆ¿é—´é€ƒè„±ä»»åŠ¡ï¼Œå…¶ä¸­ä¸€äº›å±•ç°å‡ºç±»ä¼¼äººç±»çš„æ¢ç´¢ç­–ç•¥ã€‚ç„¶è€Œï¼Œéšç€ä»»åŠ¡éš¾åº¦çš„å¢åŠ ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸åŒæ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆå„ä¸ç›¸åŒï¼Œæ­ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸åŒå¤±è´¥æ¨¡å¼å’Œå±€é™æ€§ï¼Œä¾‹å¦‚é‡å¤è½¨è¿¹ç¼ºä¹é€‚åº”æ€§æ¢ç´¢ã€å› è§†è§‰ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å·®è€Œå›°åœ¨è§’è½ä»¥åŠæ— æ•ˆåœ°ä½¿ç”¨è·å¾—çš„é“å…·ï¼ˆå¦‚é’¥åŒ™ï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½æ­ç¤ºå¤šæ¨¡æ€æ¨ç†çš„æ–°æŒ‘æˆ˜ï¼Œå¹¶æ­ç¤ºæé«˜MLLMsæ½œåŠ›çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10042v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„å¿«é€Ÿå‘å±•ï¼ŒåŒ…æ‹¬ç°å®å’Œè™šæ‹Ÿç¯å¢ƒä¸­çš„è§†è§‰æ„ŸçŸ¥ã€è§†è§‰æ¨ç†ã€ç©ºé—´æ„ŸçŸ¥å’Œç›®æ ‡æ¨æ–­ç­‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°ä¸»è¦ä¾§é‡äºæœ€ç»ˆä»»åŠ¡å®Œæˆï¼Œå¿½è§†äº†å¤šæ¨¡æ€ç¯å¢ƒä¸­æ¨ç†è¿‡ç¨‹çš„å…¨é¢å’Œé‡åŒ–åˆ†æã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†MM-EscapeåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç ”ç©¶å¤šæ¨¡æ€æ¨ç†ï¼Œå¹¶å¼€å‘äº†EscapeCraftç¯å¢ƒï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨è‡ªç”±å½¢å¼æ¢ç´¢ä¸­çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŒè§„æ¨¡çš„è¯­è¨€æ¨¡å‹åœ¨ç®€å•çš„æˆ¿é—´é€ƒç”Ÿä»»åŠ¡ä¸­å¯ä»¥æˆåŠŸå®Œæˆä»»åŠ¡ï¼Œä½†éšç€ä»»åŠ¡éš¾åº¦çš„å¢åŠ ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚åŒæ—¶ï¼Œä¸åŒæ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆå‘ˆç°å‡ºä¸åŒçš„å¤±è´¥æ¨¡å¼å’Œå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ã€‚æœ¬æ–‡å·¥ä½œä¸ºæ­ç¤ºå¤šæ¨¡æ€æ¨ç†çš„æ–°æŒ‘æˆ˜å’Œæå‡MLLMsçš„èƒ½åŠ›æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œè¿™äº›ä»»åŠ¡è¦æ±‚æ¨¡å‹å…·å¤‡å¤šç§èƒ½åŠ›ï¼Œå¦‚è§†è§‰æ„ŸçŸ¥ã€è§†è§‰æ¨ç†ã€ç©ºé—´æ„ŸçŸ¥å’Œç›®æ ‡æ¨æ–­ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨æœ€ç»ˆä»»åŠ¡å®Œæˆï¼Œç¼ºä¹å¯¹äºå¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹çš„å…¨é¢å’Œé‡åŒ–åˆ†æã€‚</li>
<li>MM-EscapeåŸºå‡†æµ‹è¯•çš„å¼•å…¥æ—¨åœ¨æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¼ºè°ƒé™¤äº†æœ€ç»ˆä»»åŠ¡å®Œæˆå¤–çš„ä¸­é—´æ¨¡å‹è¡Œä¸ºã€‚</li>
<li>EscapeCraftç¯å¢ƒçš„å¼€å‘ä¸ºæ¨¡å‹æä¾›äº†ä¸€ä¸ªè‡ªç”±å½¢å¼æ¢ç´¢çš„å¹³å°ï¼Œæœ‰åŠ©äºè¯„ä¼°å…¶åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¡¨ç°ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯­è¨€æ¨¡å‹åœ¨ç®€å•çš„æˆ¿é—´é€ƒç”Ÿä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†éšç€ä»»åŠ¡éš¾åº¦å¢åŠ ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ä¸åŒè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ç“¶é¢ˆå‘ˆç°å‡ºä¸åŒçš„å¤±è´¥æ¨¡å¼å’Œå¤šæ¨¡æ€æ¨ç†çš„å±€é™æ€§ï¼Œå¦‚ç¼ºä¹é€‚åº”æ€§æ¢ç´¢ã€ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å¼±ä»¥åŠå¯¹è·å–ç‰©å“ï¼ˆå¦‚é’¥åŒ™ï¼‰çš„ä½¿ç”¨ä¸å½“ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5e9dd18c880456e79476965637c1353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd3a09c7eb38af2c14279ec8d7f458a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d48a21fa7a775def42bdcb65ef71824.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb3e929bd991c3a2373f2622b765ce82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58c14a761546a5a0690468d3bb981b12.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BIG-Bench-Extra-Hard"><a href="#BIG-Bench-Extra-Hard" class="headerlink" title="BIG-Bench Extra Hard"></a>BIG-Bench Extra Hard</h2><p><strong>Authors:Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat</strong></p>
<p>Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8% for the best general-purpose model and 44.8% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/bbeh">https://github.com/google-deepmind/bbeh</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œè¿™è¦æ±‚å®ƒä»¬å…·å¤‡ç¨³å¥çš„é€šç”¨æ¨ç†èƒ½åŠ›å’Œå¤šæ ·åŒ–çš„æ¨ç†æŠ€èƒ½é›†ã€‚ç„¶è€Œï¼Œå½“å‰çš„LLMæ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç èƒ½åŠ›ä¸Šï¼Œåœ¨è¯„ä¼°æ›´å¹¿æ³›çš„æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å·®è·ã€‚ä¸€ä¸ªç‰¹åˆ«çš„ä¾‹å­æ˜¯BIG-Benchæ•°æ®é›†ï¼Œå®ƒå·²æˆä¸ºè¯„ä¼°LLMé€šç”¨æ¨ç†èƒ½åŠ›çš„é‡è¦åŸºå‡†ï¼Œç”±äºå…¶åŒ…å«å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹å…¨é¢è¯„ä¼°å„ç§æŠ€èƒ½çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMçš„æœ€æ–°è¿›å±•å¯¼è‡´åœ¨BIG-BenchåŠå…¶æ›´é«˜çº§ç‰ˆæœ¬BIG-Bench Hard (BBH)ä¸Šçš„é¥±å’Œã€‚æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨BBHçš„è®¸å¤šä»»åŠ¡ä¸Šå–å¾—äº†è¿‘ä¹å®Œç¾çš„æˆç»©ï¼Œä»è€Œé™ä½äº†å…¶å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BIG-Bench Extra Hard (BBEH)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¨åŠ¨LLMæ¨ç†è¯„ä¼°çš„ç•Œé™ã€‚BBEHç”¨æ–°å‹ä»»åŠ¡æ›¿æ¢BBHä¸­çš„æ¯ä¸ªä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡åœ¨æ¢ç´¢ç±»ä¼¼çš„æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºæ˜¾è‘—å¢åŠ çš„éš¾åº¦ã€‚æˆ‘ä»¬åœ¨BBEHä¸Šè¯„ä¼°äº†å„ç§æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ°æœ€ä½³é€šç”¨æ¨¡å‹çš„ï¼ˆè°ƒå’Œï¼‰å¹³å‡å‡†ç¡®ç‡ä¸º9.8%ï¼Œæœ€ä½³æ¨ç†ä¸“ç”¨æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ä¸º44.8%ï¼Œè¿™è¡¨æ˜è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œå¹¶çªå‡ºäº†åœ¨LLMä¸­å®ç°ç¨³å¥é€šç”¨æ¨ç†çš„å½“å‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†BBEHï¼š<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/bbeh%E3%80%82">https://github.com/google-deepmind/bbehã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19187v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„éƒ¨ç½²æ—¥ç›Šå¢å¤šï¼Œéœ€è¦å¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›å’Œå¤šæ ·åŒ–çš„æ¨ç†æŠ€èƒ½é›†ã€‚ç„¶è€Œï¼Œå½“å‰LLMæ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç èƒ½åŠ›ä¸Šï¼Œåœ¨è¯„ä¼°æ›´å¹¿æ³›çš„æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å·®è·ã€‚BIG-Benchæ•°æ®é›†æ˜¯ä¸€ä¸ªä¾‹å¤–ï¼Œå®ƒä¸ºè¯„ä¼°LLMçš„é€šç”¨æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶å¤šæ ·åŒ–çš„æŒ‘æˆ˜æ€§ä»»åŠ¡èƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¡†æ¶å†…å…¨é¢è¯„ä¼°å„ç§æŠ€èƒ½çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€LLMçš„æœ€æ–°è¿›å±•ï¼ŒBIG-BenchåŠå…¶æ›´é«˜çº§ç‰ˆæœ¬BIG-Bench Hard (BBH)çš„é¥±å’Œæ€§å·²ç»æ˜¾ç°ã€‚æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è®¸å¤šBBHä»»åŠ¡ä¸Šå–å¾—äº†è¿‘ä¹å®Œç¾çš„æˆç»©ï¼Œå¤§å¤§é™ä½äº†å…¶æ•ˆç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BIG-Bench Extra Hard (BBEH)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨çªç ´LLMæ¨ç†è¯„ä¼°çš„ç•Œé™ã€‚BBEHç”¨å…·æœ‰ç±»ä¼¼æ¨ç†èƒ½åŠ›ä½†éš¾åº¦æ˜¾è‘—å¢åŠ çš„å…¨æ–°ä»»åŠ¡æ›¿æ¢äº†BBHä¸­çš„æ¯ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å¯¹å„ç§æ¨¡å‹åœ¨BBEHä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæœ€ä½³é€šç”¨æ¨¡å‹çš„å¹³å‡å‡†ç¡®åº¦ä¸º9.8%ï¼Œæœ€ä½³æ¨ç†ä¸“ç”¨æ¨¡å‹çš„å¹³å‡å‡†ç¡®åº¦ä¸º44.8%ï¼Œè¯´æ˜ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œå¹¶çªæ˜¾äº†åœ¨LLMä¸­å®ç°ç¨³å¥é€šç”¨æ¨ç†çš„æŒä¹…æŒ‘æˆ˜ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†BBEHï¼š<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/bbeh%E3%80%82">https://github.com/google-deepmind/bbehã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›å’Œå¤šæ ·åŒ–çš„æŠ€èƒ½é›†æ¥æ»¡è¶³æ—¥å¸¸åº”ç”¨éœ€æ±‚ã€‚</li>
<li>å½“å‰LLMæ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç èƒ½åŠ›ä¸Šï¼Œå¿½ç•¥äº†æ›´å¹¿æ³›çš„æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>BIG-Benchæ˜¯ä¸€ä¸ªé‡è¦çš„åŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°LLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éšç€LLMçš„æœ€æ–°è¿›å±•ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¦‚BIG-Bench Hard (BBH)å·²ç»å‡ºç°é¥±å’Œã€‚</li>
<li>å¼•å…¥BIG-Bench Extra Hard (BBEH)ï¼Œä¸€ä¸ªæ›´é«˜çº§çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ›´å¤æ‚çš„ä»»åŠ¡æ¥è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨BBEHä¸Šçš„è¡¨ç°æ˜¾ç¤ºä»æœ‰æ˜¾è‘—çš„æå‡ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e571303080952684f3d005ae5d8a432c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6680889353916fcbdaffa8beff0608cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef566b0d4946994dad0aa6f3e2b63a89.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Drift-Decoding-time-Personalized-Alignments-with-Implicit-User-Preferences"><a href="#Drift-Decoding-time-Personalized-Alignments-with-Implicit-User-Preferences" class="headerlink" title="Drift: Decoding-time Personalized Alignments with Implicit User   Preferences"></a>Drift: Decoding-time Personalized Alignments with Implicit User   Preferences</h2><p><strong>Authors:Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, Thibaut Thonet, Kyomin Jung</strong></p>
<p>Personalized alignments for individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding time with implicit user preferences. Traditional Reinforcement Learning from Human Feedback (RLHF) requires thousands of annotated examples and expensive gradient updates. In contrast, Drift personalizes LLMs in a training-free manner, using only a few dozen examples to steer a frozen model through efficient preference modeling. Our approach models user preferences as a composition of predefined, interpretable attributes and aligns them at decoding time to enable personalized generation. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines while using only 50-100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable. </p>
<blockquote>
<p>é’ˆå¯¹ä¸ªäººç”¨æˆ·çš„ä¸ªæ€§åŒ–å¯¹é½åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ˜¯ä¸€ä¸ªé•¿æœŸç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†Driftï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è§£ç æ—¶é—´å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–è®¾ç½®çš„æ–°æ¡†æ¶ï¼Œå¹¶åŒ…å«éšå¼ç”¨æˆ·åå¥½ã€‚ä¼ ç»Ÿçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰éœ€è¦æ•°åƒä¸ªæ ‡æ³¨ç¤ºä¾‹å’Œæ˜‚è´µçš„æ¢¯åº¦æ›´æ–°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDriftä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼ä¸ªæ€§åŒ–LLMï¼Œä»…ä½¿ç”¨å‡ åä¸ªç¤ºä¾‹æ¥å¼•å¯¼å†»ç»“æ¨¡å‹ï¼Œé€šè¿‡é«˜æ•ˆçš„åå¥½å»ºæ¨¡è¿›è¡Œå·¥ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç”¨æˆ·åå¥½å»ºæ¨¡ä¸ºé¢„å®šä¹‰çš„å¯è§£é‡Šå±æ€§çš„ç»„åˆï¼Œå¹¶åœ¨è§£ç æ—¶é—´ä¸ä¸ªæ€§åŒ–ç”Ÿæˆå¯¹é½ã€‚åœ¨åˆæˆäººæ ¼æ•°æ®é›†ï¼ˆPerspectiveï¼‰å’ŒçœŸå®äººç±»æ³¨é‡Šæ•°æ®é›†ï¼ˆPRISMï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä½¿ç”¨ä»…50-100ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼ŒDriftæ˜¾è‘—ä¼˜äºRLHFåŸºçº¿ã€‚æˆ‘ä»¬çš„ç»“æœå’Œåˆ†æè¡¨æ˜ï¼ŒDriftåœ¨è®¡ç®—ä¸Šæ—¢é«˜æ•ˆåˆæ˜“äºè§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14289v3">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDriftçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨è§£ç æ—¶é—´å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä¸ªæ€§åŒ–å¤„ç†ï¼Œé€šè¿‡éšå¼ç”¨æˆ·åå¥½å®ç°ä¸ªæ€§åŒ–å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦é€šè¿‡æˆåƒä¸Šä¸‡æ³¨è§£ç¤ºä¾‹å’Œæ˜‚è´µæ¢¯åº¦æ›´æ–°æ¥å®ç°çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸åŒï¼ŒDriftä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼ä¸ªæ€§åŒ–LLMsï¼Œä»…ä½¿ç”¨å‡ åä¸ªç¤ºä¾‹é€šè¿‡é«˜æ•ˆåå¥½å»ºæ¨¡æ¥å¼•å¯¼å†»ç»“æ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†ç”¨æˆ·åå¥½å»ºæ¨¡ä¸ºé¢„å®šä¹‰ã€å¯è§£é‡Šå±æ€§çš„ç»„åˆï¼Œå¹¶åœ¨è§£ç æ—¶é—´å¯¹å…¶è¿›è¡Œå¯¹é½ï¼Œä»¥å®ç°ä¸ªæ€§åŒ–ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨åˆæˆäººæ ¼æ•°æ®é›†ï¼ˆPerspectiveï¼‰è¿˜æ˜¯çœŸå®äººç±»æ³¨é‡Šæ•°æ®é›†ï¼ˆPRISMï¼‰ä¸Šï¼ŒDriftåœ¨ä»…ä½¿ç”¨50-100ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºRLHFåŸºçº¿ã€‚åˆ†æå’Œç»“æœè¯æ˜ï¼ŒDriftæ—¢è®¡ç®—é«˜æ•ˆï¼Œåˆå…·å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Driftæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–å¯¹é½ã€‚</li>
<li>ä¸ä¼ ç»Ÿéœ€è¦æˆåƒä¸Šä¸‡æ³¨è§£ç¤ºä¾‹å’Œæ˜‚è´µæ¢¯åº¦æ›´æ–°çš„RLHFä¸åŒï¼ŒDrifté‡‡ç”¨è®­ç»ƒå¤–çš„æ–¹å¼ä¸ªæ€§åŒ–LLMsã€‚</li>
<li>Drifté€šè¿‡éšå¼ç”¨æˆ·åå¥½è¿›è¡Œä¸ªæ€§åŒ–å¤„ç†ã€‚</li>
<li>Driftå°†ç”¨æˆ·åå¥½å»ºæ¨¡ä¸ºé¢„å®šä¹‰ã€å¯è§£é‡Šå±æ€§çš„ç»„åˆã€‚</li>
<li>Driftåœ¨è§£ç æ—¶é—´è¿›è¡Œä¸ªæ€§åŒ–å¯¹é½ï¼Œå®ç°ä¸ªæ€§åŒ–ç”Ÿæˆã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDriftåœ¨ä»…ä½¿ç”¨å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºRLHFåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cb157e35fab04be42c671b793770dca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87f84db1eeb73bb551d442218fe2bb8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cb768f405e4dde00b8447152e5e439.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2fab063af24aaa30cb0afbdd07cf7e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d38e8e864b7dec2a197b6446df29ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-042eecc385a19f5fef3f6c6e2712a1ff.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Cooperative-Multi-Agent-Planning-with-Adaptive-Skill-Synthesis"><a href="#Cooperative-Multi-Agent-Planning-with-Adaptive-Skill-Synthesis" class="headerlink" title="Cooperative Multi-Agent Planning with Adaptive Skill Synthesis"></a>Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</h2><p><strong>Authors:Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen</strong></p>
<p>Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASSâ€™s strong performance against state-of-the-art MARL baselines across both symmetric and asymmetric scenarios. Notably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57% win rate, representing a 30 percentage point advantage over QMIX (27%). Project page can be found at <a target="_blank" rel="noopener" href="https://stellar-entremet-1720bb.netlify.app/">https://stellar-entremet-1720bb.netlify.app/</a>. </p>
<blockquote>
<p>å°½ç®¡åœ¨åˆ†å¸ƒå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è®­ç»ƒæ–¹é¢å–å¾—äº†è®¸å¤šè¿›å±•ï¼Œä½†ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ„å»ºåˆä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä»ç„¶é¢ä¸´æ ·æœ¬æ•ˆç‡ã€å¯è§£é‡Šæ€§å’Œå¯è¿ç§»æ€§çš„æŒ‘æˆ˜ã€‚ä¸åŒäºéœ€è¦å¤§é‡ä¸ç¯å¢ƒäº’åŠ¨çš„ä¼ ç»ŸåŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ­¥è§„åˆ’å’Œå¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ–‡æœ¬è§‚å¯Ÿï¼Œå¹¶åœ¨éƒ¨åˆ†è§‚å¯Ÿä¸‹éš¾ä»¥å¤„ç†å¤šæ™ºèƒ½ä½“çš„éé©¬å°”å¯å¤«æ€§è´¨äº’åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†COMPASSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œå®ƒå°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸åŠ¨æ€æŠ€èƒ½åº“å’Œç»“æ„åŒ–é€šä¿¡ç›¸ç»“åˆï¼Œç”¨äºåˆ†æ•£çš„é—­ç¯å†³ç­–ã€‚æŠ€èƒ½åº“ä»¥æ¼”ç¤ºä¸ºåŸºç¡€è¿›è¡Œå¼•å¯¼ä»»åŠ¡ï¼Œå®ç°è‡ªé€‚åº”ç­–ç•¥ã€‚COMPASSåœ¨éƒ¨åˆ†è§‚å¯Ÿä¸‹é€šè¿‡å¤šè·³é€šä¿¡ä¼ æ’­å®ä½“ä¿¡æ¯ã€‚åœ¨æ”¹è¿›åçš„æ˜Ÿé™…äº‰éœ¸å¤šæ™ºèƒ½ä½“æŒ‘æˆ˜ï¼ˆSMACv2ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCOMPASSåœ¨å¯¹ç§°å’Œéå¯¹ç§°åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€æ–°çš„MARLåŸºçº¿ã€‚ç‰¹åˆ«å€¼å¾—ä¸€æçš„æ˜¯ï¼Œåœ¨å¯¹ç§°çš„Protoss 5v5ä»»åŠ¡ä¸­ï¼ŒCOMPASSçš„èƒœç‡è¾¾åˆ°äº†57%ï¼Œç›¸æ¯”QMIXæœ‰ç€30ä¸ªç™¾åˆ†ç‚¹çš„ä¼˜åŠ¿ï¼ˆQMIXçš„èƒœç‡ä¸º27%ï¼‰ã€‚é¡¹ç›®é¡µé¢å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://stellar-entremet-1720bb.netlify.app/%E6%89%BE%E5%88%B0%E3%80%82">https://stellar-entremet-1720bb.netlify.app/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10148v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åˆ†å¸ƒå¼äººå·¥æ™ºèƒ½è®­ç»ƒä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ ·æœ¬æ•ˆç‡ã€è§£é‡Šæ€§å’Œå¯è¿ç§»æ€§ç­‰é—®é¢˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è§„åˆ’å’Œå¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦ä¾èµ–äºæ–‡æœ¬è§‚å¯Ÿï¼Œå¯¹äºéƒ¨åˆ†è§‚å¯Ÿä¸‹çš„éé©¬å°”å¯å¤«å¤šæ™ºèƒ½ä½“äº¤äº’å…·æœ‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ™ºèƒ½ä½“æ¶æ„COMPASSï¼Œè¯¥æ¶æ„ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªåŠ¨æ€æŠ€èƒ½åº“ï¼Œç”¨äºåˆ†æ•£å¼é—­ç¯å†³ç­–ã€‚æŠ€èƒ½åº“é€šè¿‡è§„åˆ’å™¨å¼•å¯¼çš„ä»»åŠ¡è¿›è¡Œæ¼”åŒ–ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç­–ç•¥ã€‚åœ¨æ”¹è¿›åçš„æ˜Ÿé™…äº‰éœ¸å¤šæ™ºèƒ½ä½“æŒ‘æˆ˜ï¼ˆSMACv2ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCOMPASSåœ¨å¤šç§åœºæ™¯ä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ†å¸ƒå¼äººå·¥æ™ºèƒ½è®­ç»ƒé¢ä¸´æ ·æœ¬æ•ˆç‡ã€è§£é‡Šæ€§å’Œå¯è¿ç§»æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è™½è¡¨ç°å‡ºä¼˜ç§€çš„é›¶æ ·æœ¬è§„åˆ’å’Œå¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†éƒ¨åˆ†è§‚å¯Ÿä¸‹çš„éé©¬å°”å¯å¤«å¤šæ™ºèƒ½ä½“äº¤äº’æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>COMPASSæ˜¯ä¸€ç§æ–°çš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>COMPASSåŒ…å«ä¸€ä¸ªåŠ¨æ€æŠ€èƒ½åº“ï¼Œè¯¥åº“é€šè¿‡è§„åˆ’å™¨å¼•å¯¼çš„ä»»åŠ¡è¿›è¡Œæ¼”åŒ–ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç­–ç•¥å’Œç¯å¢ƒã€‚</li>
<li>COMPASSé€šè¿‡å¤šè·³é€šä¿¡ä¼ æ’­å®ä½“ä¿¡æ¯ï¼Œé€‚ç”¨äºéƒ¨åˆ†è§‚å¯Ÿçš„ç¯å¢ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a20691c4fcd1ec99c41f6b272c0c2df5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4914c085237c8c4a2db33b972a300bb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c3e734172f2bae1bf6f745882c05a04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cd7692b8bc39a07c4c9e69357130d10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5855bd9e9a02fb64156949fb8324b385.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-251d137fe019c1623a1a5a4d2d700ef2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  Generating Physically Stable and Buildable LEGO Designs from Text
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-09/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-726420598d3c08505ffdccdde0a5f202.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-09  PAHA Parts-Aware Audio-Driven Human Animation with Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19017.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
