<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  Significant reflection and absorption effects in the X-ray emission of   the Intermediate Polar IGR J17195-4100">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e442931c371dfa78f83c79cc10e70a4e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-10-æ›´æ–°"><a href="#2025-05-10-æ›´æ–°" class="headerlink" title="2025-05-10 æ›´æ–°"></a>2025-05-10 æ›´æ–°</h1><h2 id="Significant-reflection-and-absorption-effects-in-the-X-ray-emission-of-the-Intermediate-Polar-IGR-J17195-4100"><a href="#Significant-reflection-and-absorption-effects-in-the-X-ray-emission-of-the-Intermediate-Polar-IGR-J17195-4100" class="headerlink" title="Significant reflection and absorption effects in the X-ray emission of   the Intermediate Polar IGR J17195-4100"></a>Significant reflection and absorption effects in the X-ray emission of   the Intermediate Polar IGR J17195-4100</h2><p><strong>Authors:Elif Åafak, ÅÃ¶len Balman, Gloria Sala</strong></p>
<p>X-ray emission is from shock-heated plasma in magnetic cataclysmic variables, and is processed by absorption and scattering before reaching the observer. We investigate these effects in the X-ray emission of the Intermediate Polar IGR J17195-4100 by carrying out the X-ray spectral analysis, spin modulation and hardness ratio. We present high-sensitivity broadband X-ray spectral analysis by combining NuSTAR and XMM-Newton observations in the 0.3-78.0 keV energy range. The X-ray spectral analysis is performed using six composite models, including the angle-dependent reflection model (reflect), multi-temperature plasma emission models (CEVMKL, MKCFLOW), and photoionised and&#x2F;or neutral partially covering absorption models (zxipcf, pcfabs) within XSPEC. We examine also the spin modulation in four energy ranges and the hardness ratio, to determine the absorption and scattering effects. We find that the spectrum is best modelled with a reflection amplitude ($\Omega$) of 0.58 $^{+0.38}<em>{-0.26}$, an ionisation parameter, log($\xi$), of 1.46$^{+0.44}</em>{-0.23}$ with an equivalent hydrogen column density of 3.09$^{+2.26}<em>{-0.68}$ $\times$ 10$^{22}$ cm$^{-2}$, a neutral absorber, and a multi-temperature plasma temperature of 27.14$^{+2.0}</em>{-2.13}$ keV. In addition, we detect effects of electron scattering in the NuSTAR band, leading to modulation amplitude of about a steady 9$%$ that increases up to 15$%$ after 20 keV. We stress that these effects significantly affect the X-ray emission of intermediate polars and should be considered to obtain a good representation of the intrinsic spectrum. </p>
<blockquote>
<p>Xå°„çº¿å‘å°„æ¥æºäºç£ç¾å˜å˜é‡çš„å†²å‡»åŠ çƒ­ç­‰ç¦»å­ä½“ï¼Œåœ¨åˆ°è¾¾è§‚å¯Ÿè€…ä¹‹å‰ç»è¿‡å¸æ”¶å’Œæ•£å°„å¤„ç†ã€‚æˆ‘ä»¬é€šè¿‡è¿›è¡ŒXå°„çº¿å…‰è°±åˆ†æã€è‡ªè½¬è°ƒåˆ¶å’Œç¡¬åº¦æ¯”æ¥ç ”ç©¶ä¸­é—´åæIGR J17195-4100çš„Xå°„çº¿å‘å°„ä¸­çš„è¿™äº›æ•ˆåº”ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆNuSTARå’ŒXMM-Newtonåœ¨0.3-78.0 keVèƒ½é‡èŒƒå›´å†…çš„è§‚æµ‹ç»“æœï¼Œè¿›è¡Œäº†é«˜çµæ•åº¦å®½å¸¦Xå°„çº¿å…‰è°±åˆ†æã€‚Xå°„çº¿å…‰è°±åˆ†æä½¿ç”¨äº†å…­ç§å¤åˆæ¨¡å‹ï¼ŒåŒ…æ‹¬è§’åº¦ä¾èµ–åå°„æ¨¡å‹ï¼ˆreflectï¼‰ã€å¤šæ¸©åº¦ç­‰ç¦»å­ä½“å‘å°„æ¨¡å‹ï¼ˆCEVMKLã€MKCFLOWï¼‰ï¼Œä»¥åŠå…‰ç¦»å­åŒ–å’Œ&#x2F;æˆ–éƒ¨åˆ†ä¸­æ€§è¦†ç›–å¸æ”¶æ¨¡å‹ï¼ˆzxipcfã€pcfabsï¼‰ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†å››ä¸ªèƒ½é‡èŒƒå›´å†…çš„è‡ªè½¬è°ƒåˆ¶å’Œç¡¬åº¦æ¯”ï¼Œä»¥ç¡®å®šå¸æ”¶å’Œæ•£å°„æ•ˆåº”ã€‚æˆ‘ä»¬å‘ç°ï¼Œå…‰è°±çš„æœ€ä½³æ¨¡å‹åå°„å¹…åº¦ï¼ˆÎ©ï¼‰ä¸º0.58 $^{+0.38}<em>{-0.26}$ï¼Œç”µç¦»å‚æ•°logï¼ˆÎ¾ï¼‰ä¸º1.46^{+0.44}</em>{-0.23}ï¼Œç­‰æ•ˆæ°¢æŸ±å¯†åº¦ä¸º3.09^{+2.26}<em>{-0.68} Ã— 10^{22} cm^{-2}ï¼Œä¸­æ€§å¸æ”¶ä½“å’Œå¤šæ¸©åº¦ç­‰ç¦»å­ä½“æ¸©åº¦ä¸º27.14^{+2.0}</em>{-2.13} keVã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨NuSTARæ³¢æ®µæ£€æµ‹åˆ°äº†ç”µå­æ•£å°„æ•ˆåº”ï¼Œå¯¼è‡´è°ƒåˆ¶å¹…åº¦çº¦ä¸ºç¨³å®šçš„9%ï¼Œåœ¨20 keVåå¢åŠ åˆ°15%ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œè¿™äº›æ•ˆåº”å¯¹ä¸­é—´åæçš„Xå°„çº¿å‘å°„äº§ç”Ÿäº†æ˜¾è‘—å½±å“ï¼Œåº”è¯¥è€ƒè™‘è¿™äº›å› ç´ ä»¥è·å¾—å†…åœ¨è°±çš„è‰¯å¥½è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05400v1">PDF</a> 10 Pages, 4 Figures, and 3 Tables, accepted to be published in the   Astronomy &amp; Astrophysics as it stands</p>
<p><strong>Summary</strong><br>    ç£ç¾å˜å˜é‡ä¸­çš„Xå°„çº¿å‘å°„æºäºç­‰ç¦»å­ä½“å†²å‡»åŠ çƒ­ï¼Œç»è¿‡å¸æ”¶å’Œæ•£å°„ååˆ°è¾¾è§‚æµ‹è€…ã€‚ç ”ç©¶å°ç»„é€šè¿‡å¯¹ä¸­é—´ææ€§çš„IGR J17195-4100çš„Xå°„çº¿å‘å°„è¿›è¡Œå…‰è°±åˆ†æã€è‡ªè½¬è°ƒåˆ¶å’Œç¡¬åº¦æ¯”æ¥ç ”ç©¶è¿™äº›å½±å“ã€‚ç ”ç©¶é‡‡ç”¨äº†é«˜çµæ•åº¦å®½å¸¦Xå°„çº¿å…‰è°±åˆ†æï¼Œç»“åˆäº†NuSTARå’ŒXMM-Newtonåœ¨0.3-78.0 keVèƒ½æ®µçš„è§‚æµ‹æ•°æ®ã€‚å‘ç°Xå°„çº¿å…‰è°±ä¸åå°„å¹…åº¦ã€ç¦»å­åŒ–å‚æ•°ã€ç­‰æ•ˆæ°¢æŸ±å¯†åº¦ã€ä¸­æ€§å¸æ”¶ä½“å’Œå¤šæ¸©åº¦ç­‰ç¦»å­ä½“æ¸©åº¦ç­‰å¤šä¸ªæ¨¡å‹å‚æ•°æœ‰å…³ã€‚æ­¤å¤–ï¼Œè¿˜æ£€æµ‹åˆ°ç”µå­æ•£å°„çš„å½±å“ï¼Œå¯¼è‡´è°ƒåˆ¶å¹…åº¦åœ¨ç¨³å®šçŠ¶æ€ä¸‹çš„9%å¢åŠ è‡³è¶…è¿‡20keVæ—¶çš„çº¦15%ã€‚è¿™äº›å½±å“å¯¹ä¸­é—´ææ€§çš„Xå°„çº¿å‘å°„è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Xå°„çº¿å‘å°„æ¥æºäºç£ç¾å˜å˜é‡ä¸­çš„ç­‰ç¦»å­ä½“å†²å‡»åŠ çƒ­ï¼Œç»è¿‡å¸æ”¶å’Œæ•£å°„ååˆ°è¾¾è§‚æµ‹è€…ã€‚</li>
<li>é€šè¿‡å…‰è°±åˆ†æã€è‡ªè½¬è°ƒåˆ¶å’Œç¡¬åº¦æ¯”å¯¹ä¸­é—´ææ€§çš„IGR J17195-4100çš„Xå°„çº¿å‘å°„è¿›è¡Œäº†ç ”ç©¶ã€‚</li>
<li>é«˜çµæ•åº¦å®½å¸¦Xå°„çº¿å…‰è°±åˆ†æç»“åˆäº†NuSTARå’ŒXMM-Newtonåœ¨0.3-78.0 keVèƒ½æ®µçš„è§‚æµ‹æ•°æ®ã€‚</li>
<li>Xå°„çº¿å…‰è°±ä¸åå°„å¹…åº¦ã€ç¦»å­åŒ–å‚æ•°ã€ç­‰æ•ˆæ°¢æŸ±å¯†åº¦ç­‰å‚æ•°æœ‰å…³ã€‚</li>
<li>å‘ç°äº†ä¸­æ€§å¸æ”¶ä½“å’Œå¤šæ¸©åº¦ç­‰ç¦»å­ä½“æ¸©åº¦çš„å½±å“ã€‚</li>
<li>ç”µå­æ•£å°„çš„å½±å“å¯¼è‡´è°ƒåˆ¶å¹…åº¦å¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-555d03b75656e4604f2ea3f44cc48add.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c5c74eb61b7cd2f83b3b360c7d099d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a330e442b9217613ed9deaee3b5a5d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39c39f6ccff14c49d1a7becef35274f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed4b9b8cdba9146578dcc54dcf014109.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MDAA-Diff-CT-Guided-Multi-Dose-Adaptive-Attention-Diffusion-Model-for-PET-Denoising"><a href="#MDAA-Diff-CT-Guided-Multi-Dose-Adaptive-Attention-Diffusion-Model-for-PET-Denoising" class="headerlink" title="MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for   PET Denoising"></a>MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for   PET Denoising</h2><p><strong>Authors:Xiaolong Niu, Zanting Ye, Xu Han, Yanchao Huang, Hao Sun, Hubing Wu, Lijun Lu</strong></p>
<p>Acquiring high-quality Positron Emission Tomography (PET) images requires administering high-dose radiotracers, which increases radiation exposure risks. Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a potential solution. However, previous studies have primarily focused on single low-dose PET denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from CT images. In this work, we propose a novel CT-Guided Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for multi-dose PET denoising. Our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. Specifically, this approach incorporates a CT-Guided High-frequency Wavelet Attention (HWA) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from CT images. These extracted features are then incorporated into PET imaging through an adaptive weighted fusion mechanism to enhance edge details. Additionally, we propose the Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets demonstrate that MDAA-Diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. Our code is publicly available. </p>
<blockquote>
<p>è·å–é«˜è´¨é‡çš„æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒéœ€è¦æ³¨å°„é«˜å‰‚é‡æ”¾å°„æ€§ç¤ºè¸ªå‰‚ï¼Œè¿™å¢åŠ äº†è¾å°„æš´éœ²é£é™©ã€‚ä»ä½å‰‚é‡PETï¼ˆLPETï¼‰ç”Ÿæˆæ ‡å‡†å‰‚é‡PETï¼ˆSPETï¼‰å·²æˆä¸ºä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€ä½å‰‚é‡PETå»å™ªä¸Šï¼Œå¿½è§†äº†ç”±æ‚£è€…é—´å·®å¼‚å¼•èµ·çš„å‰‚é‡ååº”å·®å¼‚ï¼Œä»¥åŠä»CTå›¾åƒä¸­è·å¾—çš„äº’è¡¥è§£å‰–çº¦æŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„CTå¼•å¯¼å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰è¿›è¡Œå¤šå‰‚é‡PETå»å™ªã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è§£å‰–æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³é€‚åº”ï¼Œä»¥åœ¨ä½å‰‚é‡æ¡ä»¶ä¸‹å®ç°å“è¶Šçš„å»å™ªæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨CTå¼•å¯¼é«˜é¢‘å°æ³¢æ³¨æ„åŠ›ï¼ˆHWAï¼‰æ¨¡å—ï¼Œåˆ©ç”¨å°æ³¢å˜æ¢ä»CTå›¾åƒä¸­åˆ†ç¦»å‡ºé«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ã€‚è¿™äº›æå–çš„ç‰¹å¾ç„¶åé€šè¿‡è‡ªé€‚åº”åŠ æƒèåˆæœºåˆ¶èå…¥PETæˆåƒï¼Œä»¥å¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆDAAï¼‰æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§å‰‚é‡è°ƒèŠ‚å¢å¼ºæœºåˆ¶ï¼ŒåŠ¨æ€åœ°å°†å‰‚é‡æ°´å¹³èå…¥é€šé“ç©ºé—´æ³¨æ„åŠ›æƒé‡è®¡ç®—ã€‚åœ¨18F-FDGå’Œ68Ga-FAPIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMDAA-Diffåœ¨é™ä½å‰‚é‡æ¡ä»¶ä¸‹ä¿æŒè¯Šæ–­è´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05112v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½å‰‚é‡PETå›¾åƒçš„å»å™ªé—®é¢˜ã€‚æå‡ºäº†ä¸€ç§æ–°å‹çš„CTå¼•å¯¼å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§£å‰–å­¦æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³è‡ªé€‚åº”ï¼Œåœ¨ä½å‰‚é‡æ¡ä»¶ä¸‹å®ç°äº†å“è¶Šçš„å»å™ªæ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨CTå¼•å¯¼çš„é«˜é¢‘å°æ³¢æ³¨æ„åŠ›ï¼ˆHWAï¼‰æ¨¡å—å’Œå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆDAAï¼‰æ¨¡å—ï¼Œèƒ½æœ‰æ•ˆå¤„ç†ä¸åŒæ‚£è€…ä¹‹é—´çš„å‰‚é‡å“åº”å·®å¼‚ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯åœ¨é™ä½å‰‚é‡æ¡ä»¶ä¸‹èƒ½å¤Ÿä¿æŒè¯Šæ–­è´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„CTå¼•å¯¼å¤šå‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆMDAA-Diffï¼‰ï¼Œç”¨äºå¤„ç†ä½å‰‚é‡PETå›¾åƒçš„å»å™ªé—®é¢˜ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†è§£å‰–å­¦æŒ‡å¯¼å’Œå‰‚é‡æ°´å¹³è‡ªé€‚åº”ï¼Œä»¥æé«˜ä½å‰‚é‡æ¡ä»¶ä¸‹çš„å»å™ªæ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨CTå¼•å¯¼çš„é«˜é¢‘å°æ³¢æ³¨æ„åŠ›ï¼ˆHWAï¼‰æ¨¡å—ï¼Œé€šè¿‡å°æ³¢å˜æ¢æå–CTå›¾åƒçš„é«˜é¢‘è§£å‰–è¾¹ç•Œç‰¹å¾ï¼Œå¹¶å°†å…¶èå…¥PETæˆåƒä¸­ï¼Œå¢å¼ºè¾¹ç¼˜ç»†èŠ‚ã€‚</li>
<li>æå‡ºäº†å‰‚é‡è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆDAAï¼‰æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§æ ¹æ®å‰‚é‡æ¡ä»¶å¢å¼ºçš„æœºåˆ¶ï¼ŒåŠ¨æ€åœ°å°†å‰‚é‡æ°´å¹³çº³å…¥é€šé“ç©ºé—´æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—ä¸­ã€‚</li>
<li>é€šè¿‡å¯¹18F-FDGå’Œ68Ga-FAPIæ•°æ®é›†çš„å¤§é‡å®éªŒï¼Œè¯æ˜MDAA-Diffåœ¨é™ä½å‰‚é‡æ¡ä»¶ä¸‹èƒ½å¤Ÿä¿æŒè¯Šæ–­è´¨é‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹å…¬å¼€å¯ç”¨ï¼Œä¸ºåŒ»å­¦å›¾åƒå»å™ªæä¾›äº†æœ‰åŠ›å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05112">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81b998f2c4b1b41ce719dfb3e39628be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72fb0ab82d2eb4551957a3c0446571de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c8b8e643f982798fdc7076b26c61567.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Automated-Thoracolumbar-Stump-Rib-Detection-and-Analysis-in-a-Large-CT-Cohort"><a href="#Automated-Thoracolumbar-Stump-Rib-Detection-and-Analysis-in-a-Large-CT-Cohort" class="headerlink" title="Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT   Cohort"></a>Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT   Cohort</h2><p><strong>Authors:Hendrik MÃ¶ller, Hanna SchÃ¶n, Alina Dima, Benjamin Keinert-Weth, Robert Graf, Matan Atad, Johannes Paetzold, Friederike Jungmann, Rickmer Braren, Florian Kofler, Bjoern Menze, Daniel Rueckert, Jan S. Kirschke</strong></p>
<p>Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar transitional vertebrae or enumeration anomalies. While some studies manually assess these anomalies and describe the ribs qualitatively, this study aims to automate thoracolumbar stump rib detection and analyze their morphology quantitatively. To this end, we train a high-resolution deep-learning model for rib segmentation and show significant improvements compared to existing models (Dice score 0.997 vs. 0.779, p-value &lt; 0.01). In addition, we use an iterative algorithm and piece-wise linear interpolation to assess the length of the ribs, showing a success rate of 98.2%. When analyzing morphological features, we show that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs -13.8 +- 2.5, p-value &lt; 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1, p-value &lt; 0.01), and are oriented more downwards and sideways within the first centimeters in contrast to full-length ribs. We show that with partially visible ribs, these features can achieve an F1-score of 0.84 in differentiating stump ribs from regular ones. We publish the model weights and masks for public use. </p>
<blockquote>
<p>èƒ¸è…°æ¤æ®‹ç«¯è‚‹éª¨æ˜¯èƒ¸è…°æ¤è¿‡æ¸¡æ¤æˆ–åˆ—ä¸¾å¼‚å¸¸çš„é‡è¦æŒ‡å¾ä¹‹ä¸€ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶æ‰‹åŠ¨è¯„ä¼°è¿™äº›å¼‚å¸¸å¹¶å¯¹å…¶è¿›è¡Œå®šæ€§æè¿°ï¼Œä½†æœ¬ç ”ç©¶æ—¨åœ¨è‡ªåŠ¨æ£€æµ‹èƒ¸è…°æ¤æ®‹ç«¯è‚‹éª¨å¹¶å¯¹å…¶å½¢æ€è¿›è¡Œå®šé‡åˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªé«˜åˆ†è¾¨ç‡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è¿›è¡Œè‚‹éª¨åˆ†å‰²ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¸ç°æœ‰æ¨¡å‹çš„æ˜¾è‘—æ”¹è¿›ï¼ˆDiceå¾—åˆ†ä¸º0.997ä¸0.779ï¼Œpå€¼&lt;0.01ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿­ä»£ç®—æ³•å’Œåˆ†æ®µçº¿æ€§æ’å€¼æ¥è¯„ä¼°è‚‹éª¨çš„é•¿åº¦ï¼ŒæˆåŠŸç‡è¾¾åˆ°98.2%ã€‚åœ¨åˆ†æå½¢æ€ç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å‘ç°æ®‹ç«¯è‚‹éª¨åœ¨æ¤ä½“ä¸Šæ›´å‘åï¼ˆ-19.2 +- 3.8 vs -13.8 +- 2.5ï¼Œpå€¼&lt;0.01ï¼‰ï¼Œæ›´è–„ï¼ˆ260.6 +- 103.4 vs. 563.6 +- 127.1ï¼Œpå€¼&lt;0.01ï¼‰ï¼Œå¹¶ä¸”åœ¨å‰å‡ å˜ç±³å†…ç›¸æ¯”æ­£å¸¸å…¨é•¿çš„è‚‹éª¨ï¼Œå®ƒä»¬çš„æ–¹å‘æ›´åå‘äºå‘ä¸‹å’Œä¾§å‘ã€‚æˆ‘ä»¬è¯æ˜äº†åˆ©ç”¨éƒ¨åˆ†å¯è§çš„è‚‹éª¨ç‰¹å¾ï¼Œå¯ä»¥å°†æ®‹ç«¯è‚‹éª¨ä¸æ­£å¸¸è‚‹éª¨åŒºåˆ†å¼€æ¥ï¼Œå¹¶è¾¾åˆ°F1åˆ†æ•°ä¸º0.84ã€‚æˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹æƒé‡å’Œæ©è†œä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05004v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ—¨åœ¨è‡ªåŠ¨åŒ–æ£€æµ‹èƒ¸è…°æ¤æ®‹ç«¯è‚‹éª¨ï¼Œå¹¶å¯¹å…¶å½¢æ€è¿›è¡Œå®šé‡åˆ†æå’Œè¯„ä¼°ã€‚ç ”ç©¶è®­ç»ƒäº†é«˜åˆ†è¾¨ç‡æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè‚‹éª¨åˆ†å‰²ï¼Œå¹¶å±•ç°å‡ºç›¸è¾ƒäºç°æœ‰æ¨¡å‹æ˜¾è‘—çš„æ”¹è¿›ï¼ˆDiceå¾—åˆ†0.997å¯¹æ¯”0.779ï¼Œpå€¼å°äº0.01ï¼‰ã€‚é€šè¿‡è¿­ä»£ç®—æ³•å’Œåˆ†æ®µçº¿æ€§æ’å€¼è¯„ä¼°è‚‹éª¨é•¿åº¦ï¼ŒæˆåŠŸç‡è¾¾98.2%ã€‚åˆ†æå½¢æ€å­¦ç‰¹å¾æ˜¾ç¤ºï¼Œæ®‹ç«¯è‚‹éª¨åœ¨æ¤ä½“ä¸Šæ›´å‘åï¼ˆ-19.2 Â± 3.8 å¯¹æ¯” -13.8 Â± 2.5ï¼Œpå€¼å°äº0.01ï¼‰ï¼Œæ›´è–„ï¼ˆ260.6 Â± 103.4 å¯¹æ¯” 563.6 Â± 127.1ï¼Œpå€¼å°äº0.01ï¼‰ï¼Œå¹¶ä¸”åœ¨å‰å‡ å˜ç±³å†…å‘ä¸‹å’Œä¾§å‘å€¾æ–œä¸å…¨é•¿è‚‹éª¨ç›¸æ¯”ã€‚è¿™äº›ç‰¹å¾å¯ç”¨äºåŒºåˆ†æ®‹ç«¯è‚‹éª¨å’Œæ­£å¸¸è‚‹éª¨ï¼Œéƒ¨åˆ†å¯è§è‚‹éª¨çš„F1åˆ†æ•°å¯è¾¾0.84ã€‚å…¬å¼€ä½¿ç”¨æ¨¡å‹æƒé‡å’Œæ©è†œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨è‡ªåŠ¨åŒ–æ£€æµ‹èƒ¸è…°æ¤æ®‹ç«¯è‚‹éª¨å¹¶è¿›è¡Œå½¢æ€å®šé‡è¯„ä¼°ã€‚</li>
<li>é«˜åˆ†è¾¨ç‡æ·±åº¦å­¦ä¹ æ¨¡å‹ç”¨äºè‚‹éª¨åˆ†å‰²ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>é€šè¿‡è¿­ä»£ç®—æ³•å’Œåˆ†æ®µçº¿æ€§æ’å€¼å‡†ç¡®è¯„ä¼°è‚‹éª¨é•¿åº¦ã€‚</li>
<li>æ®‹ç«¯è‚‹éª¨åœ¨æ¤ä½“ä½ç½®ã€åšåº¦ã€æ–¹å‘æ€§ä¸å…¨é•¿è‚‹éª¨æœ‰æ˜¾è‘—ä¸åŒã€‚</li>
<li>è¿™äº›å·®å¼‚å¯ç”¨äºåŒºåˆ†æ®‹ç«¯è‚‹éª¨å’Œæ­£å¸¸è‚‹éª¨ã€‚</li>
<li>éƒ¨åˆ†å¯è§è‚‹éª¨çš„è¯†åˆ«å‡†ç¡®ç‡è¾ƒé«˜ï¼ˆF1åˆ†æ•°0.84ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee3bec5d6347939828bf95c8616b6374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59b4a5789cbd0c719c392009e956cb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06a55da194a46395768084e3475f596b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ViCTr-Vital-Consistency-Transfer-for-Pathology-Aware-Image-Synthesis"><a href="#ViCTr-Vital-Consistency-Transfer-for-Pathology-Aware-Image-Synthesis" class="headerlink" title="ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis"></a>ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis</h2><p><strong>Authors:Onkar Susladkar, Gayatri Deshmukh, Yalcin Tur, Ulas Bagci</strong></p>
<p>Synthesizing medical images remains challenging due to limited annotated pathological data, modality domain gaps, and the complexity of representing diffuse pathologies such as liver cirrhosis. Existing methods often struggle to maintain anatomical fidelity while accurately modeling pathological features, frequently relying on priors derived from natural images or inefficient multi-step sampling. In this work, we introduce ViCTr (Vital Consistency Transfer), a novel two-stage framework that combines a rectified flow trajectory with a Tweedie-corrected diffusion process to achieve high-fidelity, pathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k dataset using Elastic Weight Consolidation (EWC) to preserve critical anatomical structures. We then fine-tune the model adversarially with Low-Rank Adaptation (LoRA) modules for precise control over pathology severity. By reformulating Tweedieâ€™s formula within a linear trajectory framework, ViCTr supports one-step sampling, reducing inference from 50 steps to just 4, without sacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and CirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art performance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for cirrhosis synthesis 28% lower than existing approaches and improving nnUNet segmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews indicate that ViCTr-generated liver cirrhosis MRIs are clinically indistinguishable from real scans. To our knowledge, ViCTr is the first method to provide fine-grained, pathology-aware MRI synthesis with graded severity control, closing a critical gap in AI-driven medical imaging research. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆæˆä»ç„¶æ˜¯ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œç”±äºæœ‰é™çš„æ ‡æ³¨ç—…ç†æ•°æ®ã€æ¨¡æ€åŸŸå·®å¼‚ä»¥åŠè¡¨ç¤ºå¼¥æ•£æ€§ç—…ç†ï¼ˆå¦‚è‚ç¡¬åŒ–ï¼‰çš„å¤æ‚æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ä¿æŒè§£å‰–ç»“æ„çœŸå®æ€§çš„åŒæ—¶å‡†ç¡®å»ºæ¨¡ç—…ç†ç‰¹å¾ï¼Œç»å¸¸ä¾èµ–äºä»è‡ªç„¶å›¾åƒä¸­å¾—å‡ºçš„å…ˆéªŒçŸ¥è¯†æˆ–ä½æ•ˆçš„å¤šæ­¥é‡‡æ ·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViCTrï¼ˆå…³é”®ä¸€è‡´æ€§è½¬ç§»ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆæ ¡æ­£æµè½¨è¿¹å’ŒTweedieæ ¡æ­£æ‰©æ•£è¿‡ç¨‹çš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œä»¥å®ç°é«˜ä¿çœŸã€ç—…ç†æ„ŸçŸ¥çš„å›¾åƒåˆæˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ATLAS-8kæ•°æ®é›†ä¸Šé¢„è®­ç»ƒViCTrï¼Œä½¿ç”¨å¼¹æ€§æƒé‡æ•´åˆï¼ˆEWCï¼‰æ¥ä¿ç•™å…³é”®çš„è§£å‰–ç»“æ„ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—å¯¹æ¨¡å‹è¿›è¡Œå¯¹æŠ—å¾®è°ƒï¼Œä»¥ç²¾ç¡®æ§åˆ¶ç—…ç†ä¸¥é‡ç¨‹åº¦ã€‚ViCTré€šè¿‡åœ¨çº¿æ€§è½¨è¿¹æ¡†æ¶å†…é‡æ–°åˆ¶å®šTweedieå…¬å¼ï¼Œæ”¯æŒä¸€æ­¥é‡‡æ ·ï¼Œå°†æ¨ç†ä»50æ­¥å‡å°‘åˆ°ä»…4æ­¥ï¼Œè€Œä¸æŸå¤±è§£å‰–ç»“æ„çœŸå®æ€§ã€‚æˆ‘ä»¬åœ¨BTCVï¼ˆCTï¼‰ã€AMOSï¼ˆMRIï¼‰å’ŒCirrMRI600+ï¼ˆè‚ç¡¬åŒ–ï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†ViCTrã€‚ç»“æœè¡¨æ˜ï¼Œå…¶æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè‚ç¡¬åŒ–åˆæˆçš„åŒ»ç–—Frechet Inceptionè·ç¦»ï¼ˆMFIDï¼‰è¾¾åˆ°äº†17.01ï¼Œæ¯”ç°æœ‰æ–¹æ³•é™ä½äº†28%ï¼Œåœ¨ä½¿ç”¨æ•°æ®å¢å¼ºæ—¶ï¼ŒnnUNetåˆ†å‰²æé«˜äº†3.8%çš„mDSCã€‚æ”¾å°„ç§‘åŒ»ç”Ÿè¯„å®¡è¡¨æ˜ï¼ŒViCTrç”Ÿæˆçš„è‚ç¡¬åŒ–MRIå›¾åƒä¸çœŸå®æ‰«æå›¾åƒåœ¨ä¸´åºŠä¸Šæ— æ³•åŒºåˆ†ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒViCTræ˜¯ç¬¬ä¸€ç§æä¾›ç²¾ç»†ã€ç—…ç†æ„ŸçŸ¥çš„MRIåˆæˆæ–¹æ³•ï¼Œå…·æœ‰åˆ†çº§ä¸¥é‡ç¨‹åº¦æ§åˆ¶åŠŸèƒ½ï¼Œå¡«è¡¥äº†äººå·¥æ™ºèƒ½é©±åŠ¨åŒ»å­¦æˆåƒç ”ç©¶ä¸­çš„ä¸€ä¸ªå…³é”®ç©ºç™½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04963v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºViCTrçš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºåˆæˆåŒ»å­¦å›¾åƒã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ ¡æ­£æµè½¨è¿¹å’ŒTweedieæ ¡æ­£æ‰©æ•£è¿‡ç¨‹ï¼Œå®ç°äº†é«˜ä¿çœŸã€ç—…ç†æ„ŸçŸ¥çš„å›¾åƒåˆæˆã€‚é€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒï¼ŒViCTrèƒ½å¤Ÿåœ¨ä¿æŒè§£å‰–ç»“æ„çš„åŒæ—¶ï¼Œç²¾ç¡®æ§åˆ¶ç—…ç†ä¸¥é‡ç¨‹åº¦ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒViCTråœ¨åˆæˆè‚è„ç¡¬åŒ–å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå®ç°äº†åŒ»ç–—é¢†åŸŸæœ€å…ˆè¿›çš„æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViCTræ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆæˆã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†æ ¡æ­£æµè½¨è¿¹å’ŒTweedieæ ¡æ­£æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥å®ç°é«˜ä¿çœŸå’Œç—…ç†æ„ŸçŸ¥çš„å›¾åƒåˆæˆã€‚</li>
<li>é€šè¿‡åœ¨ATLAS-8kæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒViCTrèƒ½å¤Ÿä¿æŒå…³é”®çš„è§£å‰–ç»“æ„ã€‚</li>
<li>ä½¿ç”¨Low-Rank Adaptationï¼ˆLoRAï¼‰æ¨¡å—å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ç²¾ç¡®æ§åˆ¶ç—…ç†ä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>ViCTrå°†æ¨ç†æ­¥éª¤ä»50æ­¥å‡å°‘åˆ°ä»…4æ­¥ï¼ŒåŒæ—¶ä¿æŒè§£å‰–ç»“æ„çš„ç°å®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒViCTråœ¨åˆæˆè‚è„ç¡¬åŒ–å›¾åƒæ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49d0fd691cf4bfbfbd254ff672338fcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04a13805fc2980b5e75da8cb8631ff91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3170986f0200d03bb16e59920b84aa16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1278141434220a86e1038d58e597dd92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eff173d049e07fae0c65b28fc139f40.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MoRe-3DGSMR-Motion-resolved-reconstruction-framework-for-free-breathing-pulmonary-MRI-based-on-3D-Gaussian-representation"><a href="#MoRe-3DGSMR-Motion-resolved-reconstruction-framework-for-free-breathing-pulmonary-MRI-based-on-3D-Gaussian-representation" class="headerlink" title="MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing   pulmonary MRI based on 3D Gaussian representation"></a>MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing   pulmonary MRI based on 3D Gaussian representation</h2><p><strong>Authors:Tengya Peng, Ruyi Zha, Qing Zou</strong></p>
<p>This study presents an unsupervised, motion-resolved reconstruction framework for high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI), utilizing a three-dimensional Gaussian representation (3DGS). The proposed method leverages 3DGS to address the challenges of motion-resolved 3D isotropic pulmonary MRI reconstruction by enabling data smoothing between voxels for continuous spatial representation. Pulmonary MRI data acquisition is performed using a golden-angle radial sampling trajectory, with respiratory motion signals extracted from the center of k-space in each radial spoke. Based on the estimated motion signal, the k-space data is sorted into multiple respiratory phases. A 3DGS framework is then applied to reconstruct a reference image volume from the first motion state. Subsequently, a patient-specific convolutional neural network is trained to estimate the deformation vector fields (DVFs), which are used to generate the remaining motion states through spatial transformation of the reference volume. The proposed reconstruction pipeline is evaluated on six datasets from six subjects and bench-marked against three state-of-the-art reconstruction methods. The experimental findings demonstrate that the proposed reconstruction framework effectively reconstructs high-resolution, motion-resolved pulmonary MR images. Compared with existing approaches, it achieves superior image quality, reflected by higher signal-to-noise ratio and contrast-to-noise ratio. The proposed unsupervised 3DGS-based reconstruction method enables accurate motion-resolved pulmonary MRI with isotropic spatial resolution. Its superior performance in image quality metrics over state-of-the-art methods highlights its potential as a robust solution for clinical pulmonary MR imaging. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼ˆ3DGSï¼‰çš„æ— ç›‘ç£ã€è¿åŠ¨è§£æé‡å»ºæ¡†æ¶ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡ã€è‡ªç”±å‘¼å¸çš„è‚ºéƒ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨3DGSè§£å†³è¿åŠ¨è§£æä¸‰ç»´åŒå‘è‚ºéƒ¨MRIé‡å»ºçš„æŒ‘æˆ˜ï¼Œé€šè¿‡åœ¨ä½“ç´ ä¹‹é—´è¿›è¡Œæ•°æ®å¹³æ»‘å¤„ç†ï¼Œå®ç°è¿ç»­çš„ç©ºé—´è¡¨ç¤ºã€‚è‚ºéƒ¨MRIæ•°æ®é‡‡ç”¨é»„é‡‘è§’å¾„å‘é‡‡æ ·è½¨è¿¹é‡‡é›†ï¼Œä»æ¯ä¸ªå¾„å‘å°„çº¿çš„kç©ºé—´ä¸­å¿ƒæå–å‘¼å¸è¿åŠ¨ä¿¡å·ã€‚åŸºäºä¼°è®¡çš„è¿åŠ¨ä¿¡å·ï¼Œå°†kç©ºé—´æ•°æ®æŒ‰å¤šä¸ªå‘¼å¸é˜¶æ®µè¿›è¡Œæ’åºã€‚ç„¶ååº”ç”¨ä¸€ä¸ªåŸºäºå›¾åƒç»“æ„çš„ä¸‰ç»´é«˜è¡¨ç¤ºæ³•é‡å»ºå‡ºä¸€ä¸ªåŸºå‡†å›¾åƒä½“ç§¯çš„ç¬¬ä¸€è¿åŠ¨çŠ¶æ€ã€‚æ¥ç€è®­ç»ƒç—…äººç‰¹å®šçš„å·ç§¯ç¥ç»ç½‘ç»œæ¥ä¼°è®¡å˜å½¢å‘é‡åœºï¼ˆDVFsï¼‰ï¼Œå¹¶é€šè¿‡å¯¹å‚è€ƒä½“ç§¯çš„ç©ºé—´å˜æ¢ç”Ÿæˆå‰©ä½™çš„è¿åŠ¨çŠ¶æ€ã€‚è¯¥é‡å»ºæµç¨‹åœ¨å…­ä¸ªä¸åŒä¸»é¢˜çš„å…­ç»„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ä¸‰ç§æœ€æ–°é‡å»ºæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é‡å»ºæ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°é‡å»ºå‡ºé«˜åˆ†è¾¨ç‡çš„è¿åŠ¨è§£æè‚ºéƒ¨MRå›¾åƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å›¾åƒè´¨é‡å’Œä¿¡å·å™ªå£°æ¯”ä»¥åŠå¯¹æ¯”å™ªå£°æ¯”æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„ä¼˜è¶Šæ€§ã€‚è¿™ç§åŸºäºæ— ç›‘ç£çš„3DGSé‡å»ºæ–¹æ³•èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„è¿åŠ¨è§£æè‚ºéƒ¨MRIï¼Œå…·æœ‰åŒå‘ç©ºé—´åˆ†è¾¨ç‡ã€‚å…¶åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šçš„å“è¶Šæ€§èƒ½çªå‡ºè¡¨æ˜äº†å…¶åœ¨ä¸´åºŠè‚ºéƒ¨MRIä¸­çš„ç¨³å¥è§£å†³æ–¹æ¡ˆæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04959v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„ã€åŸºäºè¿åŠ¨è§£æçš„é‡å»ºæ¡†æ¶ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡çš„è‡ªç”±å‘¼å¸è‚ºéƒ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼ˆ3DGSï¼‰æŠ€æœ¯ï¼Œè§£å†³äº†è¿åŠ¨è§£æçš„ä¸‰ç»´ç«‹ä½“è‚ºéƒ¨MRIé‡å»ºé—®é¢˜ã€‚é€šè¿‡ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼Œæ•°æ®åœ¨ä½“ç´ é—´å¾—ä»¥å¹³æ»‘å¤„ç†ï¼Œå®ç°è¿ç»­çš„ç©ºé—´è¡¨ç¤ºã€‚è¯¥ç ”ç©¶ä½¿ç”¨é»„é‡‘è§’å¾„å‘é‡‡æ ·è½¨è¿¹è¿›è¡Œè‚ºéƒ¨MRIæ•°æ®é‡‡é›†ï¼Œä»æ¯ä¸ªå¾„å‘å°„çº¿çš„kç©ºé—´ä¸­å¿ƒæå–å‘¼å¸è¿åŠ¨ä¿¡å·ã€‚åŸºäºä¼°è®¡çš„è¿åŠ¨ä¿¡å·ï¼Œå°†kç©ºé—´æ•°æ®æŒ‰å¤šä¸ªå‘¼å¸é˜¶æ®µè¿›è¡Œæ’åºã€‚ç„¶åï¼Œä½¿ç”¨3DGSæ¡†æ¶é‡å»ºå‚è€ƒå›¾åƒä½“ç§¯çš„ç¬¬ä¸€ä¸ªè¿åŠ¨çŠ¶æ€ã€‚éšåï¼Œè®­ç»ƒæ‚£è€…ç‰¹å®šçš„å·ç§¯ç¥ç»ç½‘ç»œæ¥ä¼°è®¡å˜å½¢çŸ¢é‡åœºï¼ˆDVFsï¼‰ï¼Œç”¨äºé€šè¿‡å‚è€ƒä½“ç§¯çš„ç©ºé—´å˜æ¢ç”Ÿæˆå…¶ä½™çš„è¿åŠ¨çŠ¶æ€ã€‚è¯¥é‡å»ºæµç¨‹åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ä¸‰ç§æœ€å…ˆè¿›çš„é‡å»ºæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é‡å»ºæ¡†æ¶æœ‰æ•ˆåœ°é‡å»ºäº†é«˜åˆ†è¾¨ç‡çš„è¿åŠ¨è§£æè‚ºéƒ¨MRå›¾åƒã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å›¾åƒè´¨é‡å’Œä¿¡å·å™ªå£°æ¯”ä»¥åŠå¯¹æ¯”å™ªå£°æ¯”æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚è¿™é¡¹æ— ç›‘ç£çš„åŸºäºä¸‰ç»´é«˜æ–¯è¡¨ç¤ºçš„é‡å»ºæ–¹æ³•èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„è¿åŠ¨è§£æè‚ºéƒ¨MRIï¼Œå…·æœ‰å„å‘åŒæ€§çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚å…¶åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šçš„å“è¶Šæ€§èƒ½çªæ˜¾äº†å…¶åœ¨ä¸´åºŠè‚ºéƒ¨MRIä¸­çš„ç¨³å¥è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„é‡å»ºæ¡†æ¶ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡çš„è‡ªç”±å‘¼å¸è‚ºéƒ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€‚</li>
<li>é‡‡ç”¨äº†ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼ˆ3DGSï¼‰æŠ€æœ¯ï¼Œä»¥è§£å†³è¿åŠ¨è§£æçš„ä¸‰ç»´ç«‹ä½“è‚ºéƒ¨MRIé‡å»ºçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡é»„é‡‘è§’å¾„å‘é‡‡æ ·è½¨è¿¹é‡‡é›†æ•°æ®ï¼Œå¹¶ä»kç©ºé—´çš„ä¸­å¿ƒæå–å‘¼å¸è¿åŠ¨ä¿¡å·ã€‚</li>
<li>æå‡ºäº†åŸºäºä¼°è®¡çš„è¿åŠ¨ä¿¡å·çš„kç©ºé—´æ•°æ®æ’åºæ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨æ‚£è€…ç‰¹å®šçš„å·ç§¯ç¥ç»ç½‘ç»œä¼°è®¡å˜å½¢çŸ¢é‡åœºï¼ˆDVFsï¼‰ï¼Œç”¨äºç”Ÿæˆè¿åŠ¨çŠ¶æ€ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é«˜å›¾åƒè´¨é‡ï¼Œè¡¨ç°åœ¨è¾ƒé«˜çš„ä¿¡å·å™ªå£°æ¯”å’Œå¯¹æ¯”å™ªå£°æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e662333de0d65b3180582a4191c517c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FF-PNet-A-Pyramid-Network-Based-on-Feature-and-Field-for-Brain-Image-Registration"><a href="#FF-PNet-A-Pyramid-Network-Based-on-Feature-and-Field-for-Brain-Image-Registration" class="headerlink" title="FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image   Registration"></a>FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image   Registration</h2><p><strong>Authors:Ying Zhang, Shuai Guo, Chenxi Sun, Yuchen Zhu, Jinhai Xiang</strong></p>
<p>In recent years, deformable medical image registration techniques have made significant progress. However, existing models still lack efficiency in parallel extraction of coarse and fine-grained features. To address this, we construct a new pyramid registration network based on feature and deformation field (FF-PNet). For coarse-grained feature extraction, we design a Residual Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a Residual Deformation Field Fusion Module (RDFFM). Through the parallel operation of these two modules, the model can effectively handle complex image deformations. It is worth emphasizing that the encoding stage of FF-PNet only employs traditional convolutional neural networks without any attention mechanisms or multilayer perceptrons, yet it still achieves remarkable improvements in registration accuracy, fully demonstrating the superior feature decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on the LPBA and OASIS datasets. The results show our network consistently outperforms popular methods in metrics like the Dice Similarity Coefficient. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¯å˜å½¢åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨ç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾çš„å¹¶è¡Œæå–æ–¹é¢ä»ç„¶å­˜åœ¨æ•ˆç‡ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºç‰¹å¾å’Œå˜å½¢åœºï¼ˆFF-PNetï¼‰çš„æ–°é‡‘å­—å¡”é…å‡†ç½‘ç»œã€‚å¯¹äºç²—ç²’åº¦ç‰¹å¾æå–ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ®‹å·®ç‰¹å¾èåˆæ¨¡å—ï¼ˆRFFMï¼‰ï¼Œå¯¹äºç»†ç²’åº¦å›¾åƒå˜å½¢ï¼Œæˆ‘ä»¬æå‡ºäº†æ®‹å·®å˜å½¢åœºèåˆæ¨¡å—ï¼ˆRDFFMï¼‰ã€‚è¿™ä¸¤ä¸ªæ¨¡å—çš„å¹¶è¡Œæ“ä½œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„å›¾åƒå˜å½¢ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒFF-PNetçš„ç¼–ç é˜¶æ®µä»…é‡‡ç”¨ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œæ²¡æœ‰ä»»ä½•æ³¨æ„åŠ›æœºåˆ¶æˆ–å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œä½†åœ¨é…å‡†ç²¾åº¦ä¸Šä»å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå……åˆ†å±•ç¤ºäº†RFFMå’ŒRDFFMçš„ä¼˜ç§€ç‰¹å¾è§£ç èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨LPBAå’ŒOASISæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç½‘ç»œåœ¨Diceç›¸ä¼¼ç³»æ•°ç­‰æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºæµè¡Œçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04938v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯è¿‘å¹´æ¥å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾å¹¶è¡Œæå–æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºç‰¹å¾å’Œå˜å½¢åœºçš„æ–°é¢–é‡‘å­—å¡”é…å‡†ç½‘ç»œï¼ˆFF-PNetï¼‰ã€‚ä¸ºç²—ç²’åº¦ç‰¹å¾æå–è®¾è®¡äº†æ®‹å·®ç‰¹å¾èåˆæ¨¡å—ï¼ˆRFFMï¼‰ï¼Œä¸ºç»†ç²’åº¦å›¾åƒå˜å½¢æå‡ºäº†æ®‹å·®å˜å½¢åœºèåˆæ¨¡å—ï¼ˆRDFFMï¼‰ã€‚è¿™ä¸¤ä¸ªæ¨¡å—çš„å¹¶è¡Œæ“ä½œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„å›¾åƒå˜å½¢ã€‚è¯¥ç½‘ç»œåœ¨ç¼–ç é˜¶æ®µä»…ä½¿ç”¨ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œæœªé‡‡ç”¨ä»»ä½•æ³¨æ„åŠ›æœºåˆ¶æˆ–å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œä½†åœ¨é…å‡†ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå……åˆ†å±•ç¤ºäº†RFFMå’ŒRDFFMçš„ä¼˜ç§€ç‰¹å¾è§£ç èƒ½åŠ›ã€‚åœ¨LPBAå’ŒOASISæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç½‘ç»œåœ¨Diceç›¸ä¼¼ç³»æ•°ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºæµè¡Œçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯è™½æœ‰æ‰€è¿›æ­¥ï¼Œä½†åœ¨ç²—ç²’åº¦å’Œç»†ç²’åº¦ç‰¹å¾çš„å¹¶è¡Œæå–æ•ˆç‡ä¸Šä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>æå‡ºäº†åŸºäºç‰¹å¾å’Œå˜å½¢åœºçš„æ–°é¢–é‡‘å­—å¡”é…å‡†ç½‘ç»œï¼ˆFF-PNetï¼‰ã€‚</li>
<li>è®¾è®¡äº†æ®‹å·®ç‰¹å¾èåˆæ¨¡å—ï¼ˆRFFMï¼‰ç”¨äºç²—ç²’åº¦ç‰¹å¾æå–ã€‚</li>
<li>æå‡ºäº†æ®‹å·®å˜å½¢åœºèåˆæ¨¡å—ï¼ˆRDFFMï¼‰ç”¨äºå¤„ç†ç»†ç²’åº¦å›¾åƒå˜å½¢ã€‚</li>
<li>FF-PNetåœ¨ç¼–ç é˜¶æ®µä»…ä½¿ç”¨ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–å¤šå±‚æ„ŸçŸ¥å™¨ã€‚</li>
<li>FF-PNetåœ¨é…å‡†ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå±•ç¤ºäº†RFFMå’ŒRDFFMçš„ä¼˜ç§€ç‰¹å¾è§£ç èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f859646a09b77dfbb18ce07f0ed168b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b58ccadecce4db9ec0324ccdad7e2683.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-166168e9743b24b2b80c31a76aacdcb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f83bfcdb21e54a5f2a804222d900af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6c3d0a0cc6ac1a610e482e677a11b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-490fe5b9b7afabeb045701f9e380c554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b3332208edece132c1230792e09355.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Integrated-Image-Reconstruction-and-Target-Recognition-based-on-Deep-Learning-Technique"><a href="#Integrated-Image-Reconstruction-and-Target-Recognition-based-on-Deep-Learning-Technique" class="headerlink" title="Integrated Image Reconstruction and Target Recognition based on Deep   Learning Technique"></a>Integrated Image Reconstruction and Target Recognition based on Deep   Learning Technique</h2><p><strong>Authors:Cien Zhang, Jiaming Zhang, Jiajun He, Okan Yurduseven</strong></p>
<p>Computational microwave imaging (CMI) has gained attention as an alternative technique for conventional microwave imaging techniques, addressing their limitations such as hardware-intensive physical layer and slow data collection acquisition speed to name a few. Despite these advantages, CMI still encounters notable computational bottlenecks, especially during the image reconstruction stage. In this setting, both image recovery and object classification present significant processing demands. To address these challenges, our previous work introduced ClassiGAN, which is a generative deep learning model designed to simultaneously reconstruct images and classify targets using only back-scattered signals. In this study, we build upon that framework by incorporating attention gate modules into ClassiGAN. These modules are intended to refine feature extraction and improve the identification of relevant information. By dynamically focusing on important features and suppressing irrelevant ones, the attention mechanism enhances the overall model performance. The proposed architecture, named Att-ClassiGAN, significantly reduces the reconstruction time compared to traditional CMI approaches. Furthermore, it outperforms current advanced methods, delivering improved Normalized Mean Squared Error (NMSE), higher Structural Similarity Index (SSIM), and better classification outcomes for the reconstructed targets. </p>
<blockquote>
<p>è®¡ç®—å¾®æ³¢æˆåƒï¼ˆCMIï¼‰ä½œä¸ºä¸€ç§æ›¿ä»£ä¼ ç»Ÿå¾®æ³¢æˆåƒçš„æŠ€æœ¯ï¼Œå¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚å®ƒå…‹æœäº†ä¼ ç»Ÿå¾®æ³¢æˆåƒæŠ€æœ¯çš„ä¸€äº›å±€é™æ€§ï¼Œå¦‚ç¡¬ä»¶å¯†é›†çš„ç‰©ç†å±‚å’Œç¼“æ…¢çš„æ•°æ®é‡‡é›†é€Ÿåº¦ç­‰ã€‚å°½ç®¡å…·æœ‰è¿™äº›ä¼˜åŠ¿ï¼Œä½†CMIä»ç„¶é¢ä¸´æ˜¾è‘—çš„è®¡ç®—ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒé‡å»ºé˜¶æ®µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›¾åƒæ¢å¤å’Œå¯¹è±¡åˆ†ç±»éƒ½å‘ˆç°å‡ºå·¨å¤§çš„å¤„ç†éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œå¼•å…¥äº†ClassiGANï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå¼æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨ä»…ä½¿ç”¨åå°„ä¿¡å·åŒæ—¶é‡å»ºå›¾åƒå’Œåˆ†ç±»ç›®æ ‡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨ClassiGANä¸­èå…¥äº†æ³¨æ„åŠ›é—¨æ¨¡å—ï¼Œä»¥æ­¤ä¸ºåŸºç¡€æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ã€‚è¿™äº›æ¨¡å—æ—¨åœ¨æ”¹è¿›ç‰¹å¾æå–å¹¶æé«˜å¯¹ç›¸å…³ä¿¡æ¯è¯†åˆ«çš„èƒ½åŠ›ã€‚é€šè¿‡åŠ¨æ€å…³æ³¨é‡è¦ç‰¹å¾å¹¶æŠ‘åˆ¶ä¸ç›¸å…³ç‰¹å¾ï¼Œæ³¨æ„åŠ›æœºåˆ¶æé«˜äº†æ•´ä½“æ¨¡å‹æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ¶æ„ï¼ˆç§°ä¸ºAtt-ClassiGANï¼‰ä¸ä¼ ç»ŸCMIæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†é‡å»ºæ—¶é—´ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ”¹è¿›äº†å½’ä¸€åŒ–å‡æ–¹è¯¯å·®ï¼ˆNMSEï¼‰ã€æé«˜äº†ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰ä»¥åŠä¸ºé‡å»ºç›®æ ‡æä¾›äº†æ›´å¥½çš„åˆ†ç±»ç»“æœæ–¹é¢ï¼Œè¡¨ç°å‡ºè¶…è¶Šå½“å‰å…ˆè¿›æ–¹æ³•çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04836v1">PDF</a> Submitted to The 2025 15th IEEE International Conference on Signal   Processing, Communications and Computing (ICSPCC 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®¡ç®—å¾®æ³¢æˆåƒï¼ˆCMIï¼‰ä½œä¸ºä¸€ç§æ›¿ä»£ä¼ ç»Ÿå¾®æ³¢æˆåƒæŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œè™½ç„¶å…·æœ‰è§£å†³ç¡¬ä»¶å¯†é›†çš„ç‰©ç†å±‚å’Œæ…¢æ•°æ®é‡‡é›†è·å–é€Ÿåº¦ç­‰é™åˆ¶çš„ä¼˜åŠ¿ï¼Œä½†åœ¨å›¾åƒé‡å»ºé˜¶æ®µä»é¢ä¸´é‡è¦çš„è®¡ç®—ç“¶é¢ˆã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶è€…åœ¨å…ˆå‰å·¥ä½œçš„åŸºç¡€ä¸Šå¼•å…¥äº†ClassiGANï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶é‡å»ºå›¾åƒå¹¶åˆ†ç±»ç›®æ ‡ï¼Œä»…ä½¿ç”¨åå‘æ•£å°„ä¿¡å·ã€‚æœ¬ç ”ç©¶åœ¨ClassiGANæ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œèå…¥äº†æ³¨æ„åŠ›é—¨æ¨¡å—ï¼Œæ—¨åœ¨ä¼˜åŒ–ç‰¹å¾æå–ï¼Œæé«˜ç›¸å…³ä¿¡æ¯çš„è¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡åŠ¨æ€å…³æ³¨é‡è¦ç‰¹å¾å¹¶æŠ‘åˆ¶ä¸ç›¸å…³ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚æå‡ºçš„æ¶æ„Att-ClassiGANä¸ä¼ ç»ŸCMIæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†é‡å»ºæ—¶é—´ï¼Œå¹¶ä¸”åœ¨æ”¹è¿›å½’ä¸€åŒ–å‡æ–¹è¯¯å·®ï¼ˆNMSEï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰å’Œé‡å»ºç›®æ ‡çš„åˆ†ç±»ç»“æœæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—å¾®æ³¢æˆåƒï¼ˆCMIï¼‰ä½œä¸ºä¼ ç»Ÿå¾®æ³¢æˆåƒæŠ€æœ¯çš„æ›¿ä»£æ–¹æ³•ï¼Œå…·æœ‰è§£å†³ç¡¬ä»¶å¯†é›†å’Œæ…¢æ•°æ®é‡‡é›†ç­‰é™åˆ¶çš„ä¼˜åŠ¿ã€‚</li>
<li>CMIåœ¨å›¾åƒé‡å»ºé˜¶æ®µä»é¢ä¸´è®¡ç®—ç“¶é¢ˆï¼Œå›¾åƒæ¢å¤å’Œç›®æ ‡åˆ†ç±»å­˜åœ¨é‡å¤§å¤„ç†éœ€æ±‚ã€‚</li>
<li>ClassiGANæ˜¯ä¸€ç§ç”Ÿæˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶é‡å»ºå›¾åƒå’Œåˆ†ç±»ç›®æ ‡ï¼Œä½¿ç”¨èƒŒæ•£å°„ä¿¡å·ã€‚</li>
<li>ç ”ç©¶åœ¨ClassiGANæ¡†æ¶ä¸­èå…¥æ³¨æ„åŠ›é—¨æ¨¡å—ï¼Œä»¥æé«˜ç‰¹å¾æå–å’Œç›¸å…³ä¿¡æ¯è¯†åˆ«çš„æ€§èƒ½ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡åŠ¨æ€å…³æ³¨é‡è¦ç‰¹å¾å¹¶æŠ‘åˆ¶ä¸ç›¸å…³ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹æ€»ä½“æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„Att-ClassiGANæ¶æ„ä¸ä¼ ç»ŸCMIæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†é‡å»ºæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f02a4ca42e385f0523ee904aa5b724d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a353ba4ca611e8db95fb20cda3394ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1364661176d0279e0743f273c5774301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c321a7be249d6b5978258a00bb59498f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bae3ba578979218bfcc1bcf745c678f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84d8471eb3efaffc1bcb0adf503c16c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-220eb98e4608be40e55beff3aae3f3ca.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="False-Promises-in-Medical-Imaging-AI-Assessing-Validity-of-Outperformance-Claims"><a href="#False-Promises-in-Medical-Imaging-AI-Assessing-Validity-of-Outperformance-Claims" class="headerlink" title="False Promises in Medical Imaging AI? Assessing Validity of   Outperformance Claims"></a>False Promises in Medical Imaging AI? Assessing Validity of   Outperformance Claims</h2><p><strong>Authors:Evangelia Christodoulou, Annika Reinke, Pascaline AndrÃ¨, Patrick Godau, Piotr Kalinowski, Rola Houhou, Selen Erkan, Carole H. Sudre, Ninon Burgos, SofiÃ¨ne Boutaj, Sophie Loizillon, MaÃ«lys Solal, Veronika Cheplygina, Charles Heitz, Michal Kozubek, Michela Antonelli, Nicola Rieke, Antoine Gilson, Leon D. Mayer, Minu D. Tizabi, M. Jorge Cardoso, Amber Simpson, Annette Kopp-Schneider, GaÃ«l Varoquaux, Olivier Colliot, Lena Maier-Hein</strong></p>
<p>Performance comparisons are fundamental in medical imaging Artificial Intelligence (AI) research, often driving claims of superiority based on relative improvements in common performance metrics. However, such claims frequently rely solely on empirical mean performance. In this paper, we investigate whether newly proposed methods genuinely outperform the state of the art by analyzing a representative cohort of medical imaging papers. We quantify the probability of false claims based on a Bayesian approach that leverages reported results alongside empirically estimated model congruence to estimate whether the relative ranking of methods is likely to have occurred by chance. According to our results, the majority (&gt;80%) of papers claims outperformance when introducing a new method. Our analysis further revealed a high probability (&gt;5%) of false outperformance claims in 86% of classification papers and 53% of segmentation papers. These findings highlight a critical flaw in current benchmarking practices: claims of outperformance in medical imaging AI are frequently unsubstantiated, posing a risk of misdirecting future research efforts. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å½±åƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç ”ç©¶ä¸­ï¼Œæ€§èƒ½å¯¹æ¯”æ˜¯æ ¹æœ¬ï¼Œé€šå¸¸åŸºäºé€šç”¨æ€§èƒ½æŒ‡æ ‡ä¸Šçš„ç›¸å¯¹æ”¹è¿›æ¥æå‡ºä¼˜è¶Šæ€§ä¸»å¼ ã€‚ç„¶è€Œï¼Œæ­¤ç±»ä¸»å¼ å¾€å¾€ä»…ä¾èµ–äºç»éªŒå¹³å‡æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æå…·æœ‰ä»£è¡¨æ€§çš„åŒ»å­¦å½±åƒè®ºæ–‡ï¼Œæ¢ç©¶æ–°æå‡ºçš„æ–¹æ³•æ˜¯å¦çœŸæ­£ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬åŸºäºè´å¶æ–¯æ–¹æ³•é‡åŒ–é”™è¯¯ä¸»å¼ çš„æ¦‚ç‡ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æŠ¥å‘Šç»“æœå’Œå®è¯ä¼°è®¡çš„æ¨¡å‹ä¸€è‡´æ€§æ¥ä¼°è®¡æ–¹æ³•çš„ç›¸å¯¹æ’åæ˜¯å¦å¯èƒ½å¶ç„¶å‘ç”Ÿã€‚æ ¹æ®æˆ‘ä»¬çš„ç»“æœï¼Œåœ¨å¼•å…¥æ–°æ–¹æ³•æ—¶ï¼Œå¤§å¤šæ•°ï¼ˆ&gt; 80ï¼…ï¼‰çš„è®ºæ–‡ä¸»å¼ æ€§èƒ½ä¼˜è¶Šã€‚æˆ‘ä»¬çš„åˆ†æè¿˜æ˜¾ç¤ºï¼Œåˆ†ç±»è®ºæ–‡ä¸­æœ‰é«˜è¾¾86ï¼…å’Œåˆ†å‰²è®ºæ–‡ä¸­æœ‰é«˜è¾¾53ï¼…å­˜åœ¨é«˜æ¦‚ç‡ï¼ˆ&gt; 5ï¼…ï¼‰çš„è™šå‡æ€§èƒ½ä¼˜è¶Šä¸»å¼ ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰åŸºå‡†æµ‹è¯•å®è·µä¸­çš„ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šåŒ»å­¦å½±åƒAIçš„ä¼˜è¶Šæ€§èƒ½ä¸»å¼ å¾€å¾€ç¼ºä¹ä¾æ®ï¼Œå­˜åœ¨è¯¯å¯¼æœªæ¥ç ”ç©¶åŠªåŠ›çš„é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04720v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è°ƒæŸ¥äº†åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½é¢†åŸŸæ–°æå‡ºçš„æ–¹æ³•æ˜¯å¦çœŸæ­£è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚é€šè¿‡åŸºäºè´å¶æ–¯æ–¹æ³•çš„ç»Ÿè®¡åˆ†æï¼Œå‘ç°å¤§å¤šæ•°è®ºæ–‡åœ¨å¼•å…¥æ–°æ–¹æ³•æ—¶å£°ç§°æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†å­˜åœ¨è¾ƒé«˜çš„è™šå‡å£°ç§°æ¦‚ç‡ã€‚åˆ†ç±»è®ºæ–‡ä¸­çš„è™šå‡å£°ç§°æ¦‚ç‡æ›´é«˜ã€‚è¿™æ­ç¤ºäº†å½“å‰åŸºå‡†æµ‹è¯•å®è·µä¸­çš„ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šåŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½æ€§èƒ½æå‡çš„å£°ç§°å¾€å¾€ç¼ºä¹ä¾æ®ï¼Œå¯èƒ½å¯¼è‡´æœªæ¥ç ”ç©¶å·¥ä½œçš„è¯¯å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒAIç ”ç©¶é¢†åŸŸï¼Œæ€§èƒ½æ¯”è¾ƒæ˜¯åŸºç¡€ç ”ç©¶çš„æ ¸å¿ƒï¼Œä½†åŸºäºç»éªŒå‡å€¼æ€§èƒ½çš„ç›¸å¯¹æ”¹è¿›æ¥å£°ç§°ä¼˜è¶Šæ€§å¯èƒ½å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æ–°æ–¹æ³•è¶…è¶Šç°æœ‰æŠ€æœ¯çš„å£°ç§°åœ¨å¤šæ•°åŒ»å­¦æˆåƒè®ºæ–‡ä¸­æ™®éå­˜åœ¨ã€‚</li>
<li>åŸºäºè´å¶æ–¯æ–¹æ³•çš„ç»Ÿè®¡åˆ†ææ­ç¤ºäº†è™šå‡æ€§èƒ½æå‡å£°ç§°çš„é«˜æ¦‚ç‡ã€‚</li>
<li>åˆ†ç±»è®ºæ–‡ä¸­çš„è™šå‡å£°ç§°æ¦‚ç‡é«˜äºåˆ†å‰²è®ºæ–‡ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•å®è·µå­˜åœ¨å…³é”®ç¼ºé™·ï¼Œå³æ€§èƒ½æå‡å£°ç§°ç¼ºä¹å®è´¨ä¾æ®ã€‚</li>
<li>è™šå‡å£°ç§°å¯èƒ½å¯¼è‡´æœªæ¥ç ”ç©¶å·¥ä½œçš„è¯¯å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f288523934bd249c03a34d2ef73dce41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffce1d9b2f766da41fdb17106bbd7648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427c99ba7a4a0f0ca81b05d4447955b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-299d0006ea21f9e30a5b34691daa3534.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1690ec30bfe428b2d45d7f6a2b72b27b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-927c2f85705ad1786cd676578edc30d8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Advancing-3D-Medical-Image-Segmentation-Unleashing-the-Potential-of-Planarian-Neural-Networks-in-Artificial-Intelligence"><a href="#Advancing-3D-Medical-Image-Segmentation-Unleashing-the-Potential-of-Planarian-Neural-Networks-in-Artificial-Intelligence" class="headerlink" title="Advancing 3D Medical Image Segmentation: Unleashing the Potential of   Planarian Neural Networks in Artificial Intelligence"></a>Advancing 3D Medical Image Segmentation: Unleashing the Potential of   Planarian Neural Networks in Artificial Intelligence</h2><p><strong>Authors:Ziyuan Huang, Kevin Huggins, Srikar Bellur</strong></p>
<p>Our study presents PNN-UNet as a method for constructing deep neural networks that replicate the planarian neural network (PNN) structure in the context of 3D medical image data. Planarians typically have a cerebral structure comprising two neural cords, where the cerebrum acts as a coordinator, and the neural cords serve slightly different purposes within the organismâ€™s neurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a Wide-UNet as the nerve cords, with a densely connected autoencoder performing the role of the brain. This distinct architecture offers advantages over both monolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D MRI hippocampus dataset, with and without data augmentation, demonstrate that PNN-UNet outperforms the baseline UNet and several other UNet variants in image segmentation. </p>
<blockquote>
<p>æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†PNN-UNetæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ„å»ºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œåœ¨3DåŒ»å­¦å›¾åƒæ•°æ®èƒŒæ™¯ä¸‹å¤åˆ¶äº†å¹³æ¿è™«ç¥ç»ç½‘ç»œï¼ˆPNNï¼‰ç»“æ„ã€‚å¹³æ¿è™«çš„å¤§è„‘ç»“æ„é€šå¸¸åŒ…æ‹¬ä¸¤æ¡ç¥ç»ç´¢ï¼Œå…¶ä¸­å¤§è„‘èµ·åˆ°åè°ƒä½œç”¨ï¼Œè€Œç¥ç»ç´¢åœ¨ç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­èµ·åˆ°ç•¥å¾®ä¸åŒçš„ä½œç”¨ã€‚å› æ­¤ï¼ŒPNN-UNetç”±Deep-UNetå’ŒWide-UNetä½œä¸ºç¥ç»ç´¢ç»„æˆï¼Œå¯†é›†è¿æ¥çš„è‡ªåŠ¨ç¼–ç å™¨åˆ™å……å½“å¤§è„‘çš„è§’è‰²ã€‚è¿™ç§ç‹¬ç‰¹çš„æ¶æ„ç›¸è¾ƒäºå•ä¸€ï¼ˆUNetï¼‰å’Œç½‘ç»œæ¨¡å—ï¼ˆEnsemble-UNetï¼‰å…·æœ‰ä¼˜åŠ¿ã€‚æˆ‘ä»¬åœ¨å¸¦æœ‰å’Œä¸å¸¦æœ‰æ•°æ®å¢å¼ºçš„3D MRIæµ·é©¬ä½“æ•°æ®é›†ä¸Šçš„ç»“æœè¯æ˜ï¼ŒPNN-UNetåœ¨å›¾åƒåˆ†å‰²æ–¹é¢çš„æ€§èƒ½ä¼˜äºåŸºå‡†UNetå’Œå…¶ä»–å‡ ä¸ªUNetå˜ä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04664v1">PDF</a> 36 pages, 8 figures, 21 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†PNN-UNetæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨äºæ„å»ºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œä»¥æ¨¡æ‹Ÿå¹³é¢åŠ¨ç‰©ç¥ç»ç½‘ç»œï¼ˆPNNï¼‰ç»“æ„å¤„ç†3DåŒ»å­¦å›¾åƒæ•°æ®ã€‚PNN-UNetåŒ…æ‹¬Deep-UNetå’ŒWide-UNetä½œä¸ºç¥ç»ç´¢ï¼Œå¹¶é…å¤‡ä¸€ä¸ªå¯†é›†è¿æ¥çš„è‡ªåŠ¨ç¼–ç å™¨ä½œä¸ºå¤§è„‘ã€‚è¿™ç§ç‹¬ç‰¹æ¶æ„åœ¨å›¾åƒåˆ†å‰²æ–¹é¢ä¼˜äºå•ä¸€UNetå’Œæ¨¡å—åŒ–ç½‘ç»œEnsemble-UNetï¼Œå¯¹3D MRIæµ·é©¬ä½“æ•°æ®é›†çš„ç»“æœéªŒè¯äº†PNN-UNetçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PNN-UNetæ˜¯ä¸€ç§ç»“åˆäº†Deep-UNetå’ŒWide-UNetç»“æ„çš„æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚</li>
<li>PNN-UNetè®¾è®¡çµæ„Ÿæ¥æºäºå¹³é¢åŠ¨ç‰©çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œå…·æœ‰ç‹¬ç‰¹çš„æ¶æ„ã€‚</li>
<li>PNN-UNetåœ¨å¤„ç†3DåŒ»å­¦å›¾åƒæ•°æ®æ—¶è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åœ¨3D MRIæµ·é©¬ä½“æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†PNN-UNetåœ¨å›¾åƒåˆ†å‰²æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>PNN-UNetç›¸æ¯”ä¼ ç»ŸUNetå’Œå…¶ä»–UNetå˜ä½“æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®å¢å¼ºåœ¨PNN-UNetçš„æ€§èƒ½æå‡ä¸­å¯èƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d09b01ced9a7c915cfe0a735ae27eaa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-256a6c2cdbd48cc3f4aebf66f7ccc748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84542f2cd688426070fbf9c88e202b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73965a3181621e3d2c629405aa48734a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b3b8e9cd16125b9f092cda61e6c61f2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Cross-organ-all-in-one-parallel-compressed-sensing-magnetic-resonance-imaging"><a href="#Cross-organ-all-in-one-parallel-compressed-sensing-magnetic-resonance-imaging" class="headerlink" title="Cross-organ all-in-one parallel compressed sensing magnetic resonance   imaging"></a>Cross-organ all-in-one parallel compressed sensing magnetic resonance   imaging</h2><p><strong>Authors:Baoshun Shi, Zheng Liu, Xin Meng, Yan Yang</strong></p>
<p>Recent advances in deep learning-based parallel compressed sensing magnetic resonance imaging (p-CSMRI) have significantly improved reconstruction quality. However, current p-CSMRI methods often require training separate deep neural network (DNN) for each organ due to anatomical variations, creating a barrier to developing generalized medical image reconstruction systems. To address this, we propose CAPNet (cross-organ all-in-one deep unfolding p-CSMRI network), a unified framework that implements a p-CSMRI iterative algorithm via three specialized modules: auxiliary variable module, prior module, and data consistency module. Recognizing that p-CSMRI systems often employ varying sampling ratios for different organs, resulting in organ-specific artifact patterns, we introduce an artifact generation submodule, which extracts and integrates artifact features into the data consistency module to enhance the discriminative capability of the overall network. For the prior module, we design an organ structure-prompt generation submodule that leverages structural features extracted from the segment anything model (SAM) to create cross-organ prompts. These prompts are strategically incorporated into the prior module through an organ structure-aware Mamba submodule. Comprehensive evaluations on a cross-organ dataset confirm that CAPNet achieves state-of-the-art reconstruction performance across multiple anatomical structures using a single unified model. Our code will be published at <a target="_blank" rel="noopener" href="https://github.com/shibaoshun/CAPNet">https://github.com/shibaoshun/CAPNet</a>. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å¹¶è¡Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒï¼ˆp-CSMRIï¼‰çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡ã€‚ç„¶è€Œï¼Œç”±äºè§£å‰–ç»“æ„çš„å˜åŒ–ï¼Œå½“å‰çš„p-CSMRIæ–¹æ³•é€šå¸¸éœ€è¦ä¸ºæ¯ä¸ªå™¨å®˜è®­ç»ƒå•ç‹¬çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œè¿™æˆä¸ºå¼€å‘é€šç”¨åŒ»å­¦å›¾åƒé‡å»ºç³»ç»Ÿçš„éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAPNetï¼ˆè·¨å™¨å®˜å…¨åˆä¸€æ·±åº¦å±•å¼€p-CSMRIç½‘ç»œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªä¸“ç”¨æ¨¡å—å®ç°p-CSMRIè¿­ä»£ç®—æ³•ï¼šè¾…åŠ©å˜é‡æ¨¡å—ã€å…ˆéªŒæ¨¡å—å’Œæ•°æ®ä¸€è‡´æ€§æ¨¡å—ã€‚æˆ‘ä»¬è®¤è¯†åˆ°p-CSMRIç³»ç»Ÿé€šå¸¸å¯¹ä¸åŒå™¨å®˜é‡‡ç”¨ä¸åŒçš„é‡‡æ ·æ¯”ç‡ï¼Œå¯¼è‡´å™¨å®˜ç‰¹å®šçš„ä¼ªå½±æ¨¡å¼ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¼ªå½±ç”Ÿæˆå­æ¨¡å—ï¼Œè¯¥æ¨¡å—æå–å¹¶å°†ä¼ªå½±ç‰¹å¾é›†æˆåˆ°æ•°æ®ä¸€è‡´æ€§æ¨¡å—ä¸­ï¼Œä»¥å¢å¼ºæ•´ä½“ç½‘ç»œçš„åˆ¤åˆ«èƒ½åŠ›ã€‚å¯¹äºå…ˆéªŒæ¨¡å—ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå™¨å®˜ç»“æ„æç¤ºç”Ÿæˆå­æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ä»åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ä¸­æå–çš„ç»“æ„ç‰¹å¾æ¥åˆ›å»ºè·¨å™¨å®˜æç¤ºã€‚è¿™äº›æç¤ºé€šè¿‡å™¨å®˜ç»“æ„æ„ŸçŸ¥çš„Mambaå­æ¨¡å—æˆ˜ç•¥æ€§åœ°èå…¥å…ˆéªŒæ¨¡å—ã€‚åœ¨è·¨å™¨å®˜æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¯å®ï¼ŒCAPNetä½¿ç”¨å•ä¸ªç»Ÿä¸€æ¨¡å‹åœ¨å¤šä¸ªè§£å‰–ç»“æ„ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨[<a target="_blank" rel="noopener" href="https://github.com/shibaoshun/CAPNet%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82]">https://github.com/shibaoshun/CAPNetä¸Šå‘å¸ƒã€‚]</a>(<a target="_blank" rel="noopener" href="https://github.com/shibaoshun/CAPNet%E4%B8%8A%E5%8F%91%E5%B8%A%E8%AF%A5%E7%A8%8B%E5%BA%8F%E5%9C%B0%E5%9D%80%E4%BA%8E%E5%90%8E%EF%BC%89%E3%80%82%E8%BF%99%E4%B8%AA%E6%8F%90%E8%AE%AE%E5%B8%8C%E6%9C%9B%E9%80%9A%E8%BF%87%E5%9C%A8%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF%E4%B8%8D%E8%B6%B3%E7%9A%84%E6%83%85%E5%A2%83%E4%B8%AD%E8%A7%A3%E5%86%B3%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E7%9A%84%E9%97%AE%E9%A2%98%E6%9D%A5%E5%AE%9E%E7%8E%B0%E8%B6%85%E8%B6%8A%E5%8E%9F%E6%9C%89%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%BE%B9%E7%95%8C%E7%9A%84%E9%87%8D%E5%A4%A7%E8%BF%9B%E5%B1%95%E3%80%82%E8%BF%99%E6%98%AF%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F%E7%9A%84%E6%A0%B8%E5%BF%83%E9%83%A8%E5%88%86%E7%9B%AE%E5%89%8D%E5%8F%91%E5%B1%95%E8%87%B3%E8%83%BD%E5%A4%A7%E5%A4%A7%E6%8F%90%E5%8D%87%E6%80%A7%E8%83%BD%E7%9A%84%E5%AE%9E%E9%99%85%E9%9C%80%E8%A6%81%E9%97%AE%E9%A2%98%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E7%9A%84%E4%B8%80%E4%B8%AA%E6%98%BE%E8%91%97%E4%BE%8B%E5%AD%90%E3%80%82%E6%88%91%E4%BB%AC%E7%9B%B8%E4%BF%A1%E6%88%91%E4%BB%AC%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%B0%86%E6%8E%A8%E5%8A%A8%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%87%8D%E5%BB%BA%E9%A2%86%E5%9F%9F%E7%9A%84%E5%8F%91%E5%B1%95%EF%BC%8C%E5%B9%B6%E4%BF%83%E8%BF%9B%E5%85%B6%E5%9C%A8%E4%B8%B4%E5%BA%8A%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E3%80%82">https://github.com/shibaoshun/CAPNet%E4%B8%8A%E5%8F%91%E5%B8%Aè¯¥ç¨‹åºåœ°å€äºåï¼‰ã€‚è¿™ä¸ªæè®®å¸Œæœ›é€šè¿‡åœ¨å…ˆéªŒä¿¡æ¯ä¸è¶³çš„æƒ…å¢ƒä¸­è§£å†³å®é™…åº”ç”¨çš„é—®é¢˜æ¥å®ç°è¶…è¶ŠåŸæœ‰å›¾åƒè´¨é‡è¾¹ç•Œçš„é‡å¤§è¿›å±•ã€‚è¿™æ˜¯ç”Ÿç‰©åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ ¸å¿ƒéƒ¨åˆ†ç›®å‰å‘å±•è‡³èƒ½å¤§å¤§æå‡æ€§èƒ½çš„å®é™…éœ€è¦é—®é¢˜çš„é‡è¦æ€§çš„ä¸€ä¸ªæ˜¾è‘—ä¾‹å­ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„å·¥ä½œå°†æ¨åŠ¨åŒ»å­¦å›¾åƒé‡å»ºé¢†åŸŸçš„å‘å±•ï¼Œå¹¶ä¿ƒè¿›å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04658v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å¹¶è¡Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒï¼ˆp-CSMRIï¼‰æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡ã€‚ç„¶è€Œï¼Œç”±äºå™¨å®˜è§£å‰–ç»“æ„å·®å¼‚ï¼Œå½“å‰p-CSMRIæ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªå™¨å®˜è®­ç»ƒå•ç‹¬çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œè¿™é˜»ç¢äº†é€šç”¨åŒ»ç–—å›¾åƒé‡å»ºç³»ç»Ÿçš„å¼€å‘ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºä¸€ç§ç»Ÿä¸€çš„è·¨å™¨å®˜p-CSMRIç½‘ç»œCAPNetï¼Œé€šè¿‡è¾…åŠ©å˜é‡æ¨¡å—ã€å…ˆéªŒæ¨¡å—å’Œæ•°æ®ä¸€è‡´æ€§æ¨¡å—ä¸‰ä¸ªä¸“ä¸šæ¨¡å—å®ç°p-CSMRIè¿­ä»£ç®—æ³•ã€‚CAPNetå¼•å…¥ç¼ºé™·ç”Ÿæˆå­æ¨¡å—ï¼Œæå–å’Œæ•´åˆä¸åŒå™¨å®˜çš„ç¼ºé™·ç‰¹å¾ï¼Œå¢å¼ºç½‘ç»œçš„æ•´ä½“åˆ¤åˆ«èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè®¾è®¡å™¨å®˜ç»“æ„æç¤ºç”Ÿæˆå­æ¨¡å—ï¼Œåˆ©ç”¨ä»åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ä¸­æå–çš„ç»“æ„ç‰¹å¾åˆ›å»ºè·¨å™¨å®˜æç¤ºã€‚é€šè¿‡è·¨å™¨å®˜æ•°æ®é›†çš„ç»¼åˆè¯„ä¼°ï¼Œè¯æ˜CAPNetä½¿ç”¨å•ä¸€ç»Ÿä¸€æ¨¡å‹å³å¯åœ¨å¤šä¸ªè§£å‰–ç»“æ„ä¸Šå®ç°æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°p-CSMRIè¿›å±•æé«˜äº†é‡å»ºè´¨é‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€ä¸ºæ¯ä¸ªå™¨å®˜è®­ç»ƒå•ç‹¬çš„DNNï¼Œé˜»ç¢é€šç”¨åŒ»ç–—å›¾åƒé‡å»ºç³»ç»Ÿå¼€å‘ã€‚</li>
<li>CAPNetæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è·¨å™¨å®˜p-CSMRIç½‘ç»œï¼Œé€šè¿‡ä¸‰ä¸ªä¸“ä¸šæ¨¡å—å®ç°p-CSMRIè¿­ä»£ç®—æ³•ã€‚</li>
<li>CAPNetå¼•å…¥ç¼ºé™·ç”Ÿæˆå­æ¨¡å—ï¼Œå¢å¼ºç½‘ç»œåˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡å™¨å®˜ç»“æ„æç¤ºç”Ÿæˆå­æ¨¡å—ï¼Œåˆ©ç”¨SAMæå–çš„ç»“æ„ç‰¹å¾ã€‚</li>
<li>CAPNetåœ¨å¤šä¸ªè§£å‰–ç»“æ„ä¸Šå®ç°æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½ã€‚</li>
<li>ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/shibaoshun/CAPNet%E3%80%82">https://github.com/shibaoshun/CAPNetã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b40225281faa741b864b89b89e610b0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8bc1cc90387f81fdd3a7169dcba9065.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb7e1b221caae0325baac8e0be389f5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e45742e689f722ccdf57fa81062c97d9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Rethinking-Boundary-Detection-in-Deep-Learning-Based-Medical-Image-Segmentation"><a href="#Rethinking-Boundary-Detection-in-Deep-Learning-Based-Medical-Image-Segmentation" class="headerlink" title="Rethinking Boundary Detection in Deep Learning-Based Medical Image   Segmentation"></a>Rethinking Boundary Detection in Deep Learning-Based Medical Image   Segmentation</h2><p><strong>Authors:Yi Lin, Dong Zhang, Xiao Fang, Yufan Chen, Kwang-Ting Cheng, Hao Chen</strong></p>
<p>Medical image segmentation is a pivotal task within the realms of medical image analysis and computer vision. While current methods have shown promise in accurately segmenting major regions of interest, the precise segmentation of boundary areas remains challenging. In this study, we propose a novel network architecture named CTO, which combines Convolutional Neural Networks (CNNs), Vision Transformer (ViT) models, and explicit edge detection operators to tackle this challenge. CTO surpasses existing methods in terms of segmentation accuracy and strikes a better balance between accuracy and efficiency, without the need for additional data inputs or label injections. Specifically, CTO adheres to the canonical encoder-decoder network paradigm, with a dual-stream encoder network comprising a mainstream CNN stream for capturing local features and an auxiliary StitchViT stream for integrating long-range dependencies. Furthermore, to enhance the modelâ€™s ability to learn boundary areas, we introduce a boundary-guided decoder network that employs binary boundary masks generated by dedicated edge detection operators to provide explicit guidance during the decoding process. We validate the performance of CTO through extensive experiments conducted on seven challenging medical image segmentation datasets, namely ISIC 2016, PH2, ISIC 2018, CoNIC, LiTS17, and BTCV. Our experimental results unequivocally demonstrate that CTO achieves state-of-the-art accuracy on these datasets while maintaining competitive model complexity. The codes have been released at: <a target="_blank" rel="noopener" href="https://github.com/xiaofang007/CTO">https://github.com/xiaofang007/CTO</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æå’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚è™½ç„¶å½“å‰æ–¹æ³•åœ¨ä¸»è¦åŒºåŸŸçš„åˆ†å‰²æ–¹é¢æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å¯¹è¾¹ç•ŒåŒºåŸŸçš„ç²¾ç¡®åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œæ¶æ„CTOï¼Œè¯¥æ¶æ„ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€è§†è§‰Transformerï¼ˆViTï¼‰æ¨¡å‹å’Œæ˜ç¡®çš„è¾¹ç¼˜æ£€æµ‹ç®—å­ï¼Œä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚CTOåœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®è¾“å…¥æˆ–æ ‡ç­¾æ³¨å…¥ã€‚å…·ä½“æ¥è¯´ï¼ŒCTOéµå¾ªå…¸å‹çš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œèŒƒå¼ï¼Œå…·æœ‰åŒç¼–ç å™¨ç½‘ç»œæµï¼Œä¸»æµæ˜¯CNNæµï¼Œç”¨äºæ•æ‰å±€éƒ¨ç‰¹å¾ï¼Œè¾…åŠ©æµæ˜¯StitchViTæµï¼Œç”¨äºæ•´åˆé•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ¨¡å‹å¯¹è¾¹ç•ŒåŒºåŸŸçš„å­¦ä¹ èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¾¹ç•Œå¼•å¯¼è§£ç å™¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œä½¿ç”¨ç”±ä¸“ç”¨è¾¹ç¼˜æ£€æµ‹ç®—å­ç”Ÿæˆçš„äºŒè¿›åˆ¶è¾¹ç•Œæ©è†œï¼Œä¸ºè§£ç è¿‡ç¨‹æä¾›æ˜ç¡®æŒ‡å¯¼ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸ŠéªŒè¯äº†CTOçš„æ€§èƒ½ï¼Œå³ISIC 2016ã€PH2ã€ISIC 2018ã€CoNICã€LiTS17å’ŒBTCVã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜ç¡®è¯æ˜ï¼ŒCTOåœ¨è¿™äº›æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ¨¡å‹å¤æ‚åº¦ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/xiaofang007/CTO%E3%80%82">https://github.com/xiaofang007/CTOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04652v1">PDF</a> Accepted by Medical Image Analysis</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œæ¶æ„CTOï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹å’Œè¾¹ç¼˜æ£€æµ‹ç®—å­ï¼Œæ—¨åœ¨è§£å†³è¾¹ç•ŒåŒºåŸŸç²¾ç¡®åˆ†å‰²çš„æŒ‘æˆ˜ã€‚CTOåœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°æ›´å¥½çš„å¹³è¡¡ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®è¾“å…¥æˆ–æ ‡ç­¾æ³¨å…¥ã€‚CTOéµå¾ªå…¸å‹çš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œèŒƒå¼ï¼Œå…·æœ‰åŒæµç¼–ç å™¨ç½‘ç»œï¼ŒåŒ…æ‹¬ä¸»æµCNNæµç”¨äºæ•æ‰å±€éƒ¨ç‰¹å¾å’Œè¾…åŠ©StitchViTæµç”¨äºæ•´åˆé•¿ç¨‹ä¾èµ–å…³ç³»ã€‚ä¸ºå¢å¼ºæ¨¡å‹å¯¹è¾¹ç•ŒåŒºåŸŸçš„å­¦ä¹ èƒ½åŠ›ï¼Œå¼•å…¥è¾¹ç•Œå¼•å¯¼è§£ç å™¨ç½‘ç»œï¼Œé‡‡ç”¨ç”±ä¸“ç”¨è¾¹ç¼˜æ£€æµ‹ç®—å­ç”Ÿæˆçš„äºŒè¿›åˆ¶è¾¹ç•Œæ©è†œï¼Œä¸ºè§£ç è¿‡ç¨‹æä¾›æ˜ç¡®æŒ‡å¯¼ã€‚åœ¨ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCTOå–å¾—äº†æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§çš„æ¨¡å‹å¤æ‚åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æå’Œè®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®åˆ†å‰²è¾¹ç•ŒåŒºåŸŸæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œæ¶æ„CTOï¼Œç»“åˆCNNã€ViTæ¨¡å‹å’Œè¾¹ç¼˜æ£€æµ‹ç®—å­ï¼Œä»¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>CTOè¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„åˆ†å‰²ç²¾åº¦ï¼Œå¹¶åœ¨ç²¾åº¦å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æ›´å¥½çš„å¹³è¡¡ã€‚</li>
<li>CTOéµå¾ªå…¸å‹çš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œèŒƒå¼ï¼Œå…·æœ‰åŒæµç¼–ç å™¨ç½‘ç»œå’Œè¾¹ç•Œå¼•å¯¼è§£ç å™¨ç½‘ç»œã€‚</li>
<li>å®éªŒåœ¨ä¸ƒä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¯æ˜äº†CTOçš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>CTOçš„ä»£ç å·²å‘å¸ƒåœ¨ç›¸å…³ä»“åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad1843bcb2bfc5cebfd1b7d52c72f0fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba3464f4eec9332272a3e07a9cd6d120.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77cc6a2bf4b8e84be2d393e6442e2104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca45fb0cad8a82dc687e997b025d76e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-327b43dbc5d647fccf99d964853e6b81.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MAISY-Motion-Aware-Image-SYnthesis-for-Medical-Image-Motion-Correction"><a href="#MAISY-Motion-Aware-Image-SYnthesis-for-Medical-Image-Motion-Correction" class="headerlink" title="MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction"></a>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</h2><p><strong>Authors:Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim</strong></p>
<p>Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%. </p>
<blockquote>
<p>æ‚£è€…åœ¨åŒ»å­¦å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­çš„è¿åŠ¨ä¼šå¯¼è‡´å›¾åƒæ¨¡ç³Šã€å‡ºç°é¬¼å½±å’Œå™¨å®˜æ‰­æ›²ï¼Œè¿™ä½¿å¾—å›¾åƒè§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰æœ€å…ˆè¿›çš„ç®—æ³•ä½¿ç”¨åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ï¼Œå®ƒä»¬èƒ½å¤Ÿé€šè¿‡ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰æŸå¤±å­¦ä¹ å—æŸå›¾åƒä¸å…¶çœŸå®å€¼ä¹‹é—´çš„æ˜ å°„ï¼Œä»è€Œæœ‰æ•ˆåœ°ç”Ÿæˆæ— è¿åŠ¨å›¾åƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä»¥ä¸‹å±€é™æ€§ï¼šï¼ˆiï¼‰å®ƒä»¬ä¸»è¦å…³æ³¨å…¨å±€ç»“æ„ç‰¹å¾ï¼Œå› æ­¤å¿½ç•¥äº†é€šå¸¸æºå¸¦å…³é”®ç—…ç†ä¿¡æ¯çš„å±€éƒ¨ç‰¹å¾ï¼›ï¼ˆiiï¼‰SSIMæŸå¤±å‡½æ•°åœ¨å¤„ç†åƒç´ å¼ºåº¦ã€äº®åº¦å› ç´ å’Œæ–¹å·®å„å¼‚çš„å›¾åƒæ—¶é‡åˆ°å›°éš¾ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¿åŠ¨æ„ŸçŸ¥å›¾åƒåˆæˆï¼ˆMAISYï¼‰ï¼Œå®ƒé¦–å…ˆè¡¨å¾è¿åŠ¨ï¼Œç„¶ååˆ©ç”¨è¿åŠ¨è¿›è¡Œæ ¡æ­£ï¼šé€šè¿‡ï¼ˆaï¼‰åˆ©ç”¨åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰çš„åŸºç¡€æ¨¡å‹ï¼ŒåŠ¨æ€å­¦ä¹ è§£å‰–è¾¹ç•Œå¤„çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›ç©ºé—´æ¨¡å¼æ˜¯è¿åŠ¨ä¼ªå½±æœ€æ˜æ˜¾çš„ä½ç½®ï¼›ï¼ˆbï¼‰å¼•å…¥æ–¹å·®é€‰æ‹©æ€§SSIMï¼ˆVS-SSIMï¼‰æŸå¤±ï¼Œè¯¥æŸå¤±èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¼ºè°ƒå…·æœ‰é«˜åƒç´ æ–¹å·®çš„åŒºåŸŸï¼Œåœ¨ä¼ªå½±æ ¡æ­£è¿‡ç¨‹ä¸­ä¿ç•™å…³é”®çš„è§£å‰–ç»†èŠ‚ã€‚å¯¹èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŒç±»æ¨¡å‹ï¼Œå³°å€¼ä¿¡å·ä¸å™ªå£°æ¯”ï¼ˆPSNRï¼‰æé«˜äº†40%ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰æé«˜äº†10%ï¼ŒDiceç³»æ•°æé«˜äº†16%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04105v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒè·å–è¿‡ç¨‹ä¸­æ‚£è€…è¿åŠ¨å¼•èµ·çš„å›¾åƒæ¨¡ç³Šã€é¬¼å½±å’Œå™¨å®˜æ‰­æ›²é—®é¢˜ï¼Œä½¿å¾—å›¾åƒè§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰æœ€å…ˆè¿›çš„ç®—æ³•ä¸»è¦å…³æ³¨å…¨å±€ç»“æ„ç‰¹å¾ï¼Œå¿½ç•¥äº†æºå¸¦é‡è¦ç—…ç†ä¿¡æ¯çš„å±€éƒ¨ç‰¹å¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†Motion-Aware Image SYnthesisï¼ˆMAISYï¼‰æ¨¡å‹ï¼Œé€šè¿‡åŠ¨æ€å­¦ä¹ ç©ºé—´æ¨¡å¼å¹¶å¯¹é«˜åƒç´ æ–¹å·®çš„ç©ºé—´åŒºåŸŸè¿›è¡Œè‡ªé€‚åº”å¼ºè°ƒï¼Œä»¥çº æ­£è¿åŠ¨å¼•èµ·çš„ä¼ªå½±å¹¶ä¿ç•™å…³é”®è§£å‰–ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‚£è€…è¿åŠ¨åœ¨åŒ»å­¦å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­ä¼šå¯¼è‡´å›¾åƒæ¨¡ç³Šã€é¬¼å½±å’Œå™¨å®˜æ‰­æ›²ï¼Œå¢åŠ äº†å›¾åƒè§£è¯»çš„éš¾åº¦ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„ç®—æ³•ä½¿ç”¨GANå’ŒSSIMæŸå¤±æ¥ç”Ÿæˆæ— è¿åŠ¨å›¾åƒï¼Œä½†ä»å­˜åœ¨å¯¹å±€éƒ¨ç‰¹å¾å’Œåƒç´ å¼ºåº¦å˜åŒ–çš„å¤„ç†ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>MAISYæ¨¡å‹åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰åŠ¨æ€å­¦ä¹ ç©ºé—´æ¨¡å¼ï¼Œé’ˆå¯¹è¿åŠ¨ä¼ªå½±æœ€æ˜æ˜¾çš„è§£å‰–è¾¹ç•Œè¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>MAISYæ¨¡å‹å¼•å…¥äº†Variance-Selective SSIMï¼ˆVS-SSIMï¼‰æŸå¤±ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”å¼ºè°ƒé«˜åƒç´ æ–¹å·®çš„ç©ºé—´åŒºåŸŸï¼Œä»¥ä¿ç•™å…³é”®è§£å‰–ç»†èŠ‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMAISYæ¨¡å‹åœ¨èƒ¸éƒ¨å’Œå¤´éƒ¨CTæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>MAISYæ¨¡å‹é€šè¿‡çº æ­£è¿åŠ¨ä¼ªå½±ï¼Œæé«˜äº†å›¾åƒè´¨é‡ï¼Œè¡¨ç°ä¸ºPeak Signal-to-Noise Ratioï¼ˆPSNRï¼‰å¢åŠ 40%ï¼ŒStructural Similarity Index Measureï¼ˆSSIMï¼‰å¢åŠ 10%ï¼ŒDiceç³»æ•°å¢åŠ 16%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ad56f32c9dc60dd7bad97809a6d3e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1cfcf6463d8b74bad08fcbe4efb97f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec9bc57c218e1014e2b1609ce0922f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="IntelliCardiac-An-Intelligent-Platform-for-Cardiac-Image-Segmentation-and-Classification"><a href="#IntelliCardiac-An-Intelligent-Platform-for-Cardiac-Image-Segmentation-and-Classification" class="headerlink" title="IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation   and Classification"></a>IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation   and Classification</h2><p><strong>Authors:Ting Yu Tsai, An Yu, Meghana Spurthi Maadugundu, Ishrat Jahan Mohima, Umme Habiba Barsha, Mei-Hwa F. Chen, Balakrishnan Prabhakaran, Ming-Ching Chang</strong></p>
<p>Precise and effective processing of cardiac imaging data is critical for the identification and management of the cardiovascular diseases. We introduce IntelliCardiac, a comprehensive, web-based medical image processing platform for the automatic segmentation of 4D cardiac images and disease classification, utilizing an AI model trained on the publicly accessible ACDC dataset. The system, intended for patients, cardiologists, and healthcare professionals, offers an intuitive interface and uses deep learning models to identify essential heart structures and categorize cardiac diseases. The system supports analysis of both the right and left ventricles as well as myocardium, and then classifies patientâ€™s cardiac images into five diagnostic categories: dilated cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right ventricular abnormality, and no disease. IntelliCardiac combines a deep learning-based segmentation model with a two-step classification pipeline. The segmentation module gains an overall accuracy of 92.6%. The classification module, trained on characteristics taken from segmented heart structures, achieves 98% accuracy in five categories. These results exceed the performance of the existing state-of-the-art methods that integrate both segmentation and classification models. IntelliCardiac, which supports real-time visualization, workflow integration, and AI-assisted diagnostics, has great potential as a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis. </p>
<blockquote>
<p>ç²¾ç¡®ä¸”æœ‰æ•ˆåœ°å¤„ç†å¿ƒè„æˆåƒæ•°æ®å¯¹äºå¿ƒè¡€ç®¡ç–¾ç—…çš„è¯†åˆ«å’Œç®¡ç†è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ¨å‡ºIntelliCardiacï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºäºç½‘é¡µçš„åŒ»å­¦å›¾åƒå¤„ç†å¹³å°ï¼Œç”¨äºè‡ªåŠ¨åˆ†å‰²4Då¿ƒè„å›¾åƒå’Œç–¾ç—…åˆ†ç±»ã€‚æˆ‘ä»¬åˆ©ç”¨åœ¨å…¬å…±å¯è®¿é—®çš„ACDCæ•°æ®é›†ä¸Šè®­ç»ƒçš„AIæ¨¡å‹å»ºç«‹æ­¤ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé¢å‘æ‚£è€…ã€å¿ƒè„ç—…ä¸“å®¶å’ŒåŒ»ç–—ä¿å¥ä¸“ä¸šäººå‘˜ï¼Œæä¾›ç›´è§‚ç•Œé¢ï¼Œå¹¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è¯†åˆ«å¿ƒè„å…³é”®ç»“æ„å¹¶å¯¹å¿ƒè„ç—…è¿›è¡Œåˆ†ç±»ã€‚è¯¥ç³»ç»Ÿæ”¯æŒå³å¿ƒå®¤å’Œå·¦å¿ƒå®¤ä»¥åŠå¿ƒè‚Œçš„åˆ†æï¼Œç„¶åå°†æ‚£è€…çš„å¿ƒè„å›¾åƒåˆ†ç±»ä¸ºäº”ç§è¯Šæ–­ç±»åˆ«ï¼šæ‰©å¼ å‹å¿ƒè‚Œç—…ã€å¿ƒè‚Œæ¢—æ­»ã€è‚¥åšå‹å¿ƒè‚Œç—…ã€å³å¿ƒå®¤å¼‚å¸¸å’Œæ— ç–¾ç—…ã€‚IntelliCardiacå°†åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æ¨¡å‹ä¸ä¸¤æ­¥åˆ†ç±»æµç¨‹ç›¸ç»“åˆã€‚åˆ†å‰²æ¨¡å—çš„æ•´ä½“å‡†ç¡®åº¦è¾¾åˆ°92.6%ã€‚åˆ†ç±»æ¨¡å—æ ¹æ®åˆ†å‰²çš„å¿ƒè„ç»“æ„ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œåœ¨äº”ä¸ªç±»åˆ«ä¸­è¾¾åˆ°98%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„é›†æˆåˆ†å‰²å’Œåˆ†ç±»æ¨¡å‹çš„æ–¹æ³•çš„æ€§èƒ½ã€‚IntelliCardiacæ”¯æŒå®æ—¶å¯è§†åŒ–ã€å·¥ä½œæµç¨‹é›†æˆå’Œäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ï¼Œä½œä¸ºå¿ƒè„æˆåƒå’Œè¯Šæ–­çš„ä¸´åºŠå†³ç­–æ”¯æŒå·¥å…·ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03838v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†IntelliCardiacè¿™ä¸€åŸºäºç½‘é¡µçš„åŒ»å­¦å›¾åƒå¤„ç†å¹³å°ï¼Œè¯¥å¹³å°å¯å¯¹å¿ƒè„4Då›¾åƒè¿›è¡Œè‡ªåŠ¨åˆ†å‰²å¹¶å¯¹ç–¾ç—…è¿›è¡Œåˆ†ç±»ã€‚å®ƒé‡‡ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯å¯¹å¿ƒè„å…³é”®ç»“æ„è¿›è¡Œè¯†åˆ«ï¼Œå¹¶å¯¹äº”ç§å¸¸è§çš„å¿ƒè„ç–¾ç—…è¿›è¡Œåˆ†ç±»ã€‚æ­¤å¹³å°å¯å®ç°å®æ—¶å¯è§†åŒ–ã€å·¥ä½œæµç¨‹é›†æˆå’Œäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IntelliCardiacæ˜¯ä¸€ä¸ªåŸºäºç½‘é¡µçš„åŒ»å­¦å›¾åƒå¤„ç†å¹³å°ï¼Œä¸“é—¨ç”¨äºå¤„ç†å¿ƒè„æˆåƒæ•°æ®ã€‚</li>
<li>å¹³å°å¯å®ç°å¿ƒè„4Då›¾åƒçš„è‡ªåŠ¨åˆ†å‰²å’Œç–¾ç—…åˆ†ç±»ã€‚</li>
<li>åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¯†åˆ«å¿ƒè„å…³é”®ç»“æ„ã€‚</li>
<li>å¹³å°æ”¯æŒå¯¹å³å¿ƒå®¤å’Œå·¦å¿ƒå®¤çš„åˆ†æï¼Œä»¥åŠå¿ƒè‚Œçš„åˆ†æã€‚</li>
<li>å¯å°†æ‚£è€…çš„å¿ƒè„å›¾åƒåˆ†ç±»ä¸ºäº”ç§è¯Šæ–­ç±»åˆ«ï¼šæ‰©å¼ å‹å¿ƒè‚Œç—…ã€å¿ƒè‚Œæ¢—æ­»ã€è‚¥åšå‹å¿ƒè‚Œç—…ã€å³å¿ƒå®¤å¼‚å¸¸å’Œæ— ç–¾ç—…ã€‚</li>
<li>IntelliCardiacçš„åˆ†å‰²æ¨¡å‹æ€»ä½“å‡†ç¡®åº¦ä¸º92.6%ï¼Œåˆ†ç±»æ¨¡å‹å‡†ç¡®åº¦ä¸º98%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3fbf95c65a7eb91411fafeb1db30f3de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a15755d537f500447dcf3fcc3c146cac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3608cd25951f0b2fc10788ff462a7257.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-315f557b431ddefa20dd9d90ee04b425.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-752e3cd3412f80d43099d556cbf0ef54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56bfddbf7c70202ecd1577dbf4f0384f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5db69e6b4914d0f89deeefa779e0db63.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Advances-in-Automated-Fetal-Brain-MRI-Segmentation-and-Biometry-Insights-from-the-FeTA-2024-Challenge"><a href="#Advances-in-Automated-Fetal-Brain-MRI-Segmentation-and-Biometry-Insights-from-the-FeTA-2024-Challenge" class="headerlink" title="Advances in Automated Fetal Brain MRI Segmentation and Biometry:   Insights from the FeTA 2024 Challenge"></a>Advances in Automated Fetal Brain MRI Segmentation and Biometry:   Insights from the FeTA 2024 Challenge</h2><p><strong>Authors:Vladyslav Zalevskyi, Thomas Sanchez, Misha Kaandorp, Margaux Roulet, Diego Fajardo-Rojas, Liu Li, Jana Hutter, Hongwei Bran Li, Matthew Barkovich, Hui Ji, Luca Wilhelmi, Aline DÃ¤ndliker, CÃ©line Steger, MÃ©riam Koob, Yvan Gomez, Anton JakovÄiÄ‡, Melita KlaiÄ‡, Ana AdÅ¾iÄ‡, Pavel MarkoviÄ‡, Gracia GrabariÄ‡, Milan Rados, Jordina Aviles Verdera, Gregor Kasprian, Gregor Dovjak, Raphael Gaubert-RachmÃ¼hl, Maurice Aschwanden, Qi Zeng, Davood Karimi, Denis Peruzzo, Tommaso Ciceri, Giorgio Longari, Rachika E. Hamadache, Amina Bouzid, Xavier LladÃ³, Simone Chiarella, Gerard MartÃ­-Juan, Miguel Ãngel GonzÃ¡lez Ballester, Marco Castellaro, Marco Pinamonti, Valentina Visani, Robin Cremese, KeÃ¯n Sam, Fleur Gaudfernau, Param Ahir, Mehul Parikh, Maximilian Zenk, Michael Baumgartner, Klaus Maier-Hein, Li Tianhong, Yang Hong, Zhao Longfei, Domen Preloznik, Å½iga Å piclin, Jae Won Choi, Muyang Li, Jia Fu, Guotai Wang, Jingwen Jiang, Lyuyang Tong, Bo Du, Andrea Gondova, Sungmin You, Kiho Im, Abdul Qayyum, Moona Mazher, Steven A Niederer, Andras Jakab, Roxane Licandro, Kelly Payette, Meritxell Bach Cuadra</strong></p>
<p>Accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. The FeTA Challenge 2024 advanced automated fetal brain MRI analysis by introducing biometry prediction as a new task alongside tissue segmentation. For the first time, our diverse multi-centric test set included data from a new low-field (0.55T) MRI dataset. Evaluation metrics were also expanded to include the topology-specific Euler characteristic difference (ED). Sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. However, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. The ED metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. Seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. Domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. Other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. Overall, FeTA 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain MRI, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable AI tools. </p>
<blockquote>
<p>å‡†ç¡®å¯¹èƒå„¿è„‘ç»„ç»‡è¿›è¡Œåˆ†å‰²å’Œç”Ÿç‰©æµ‹å®šåˆ†ææ˜¯ç ”ç©¶èƒå„¿åœ¨æ¯ä½“å†…è„‘å‘è‚²çš„é‡è¦å‰æã€‚FeTA Challenge 2024é€šè¿‡å¼•å…¥ç”Ÿç‰©æµ‹å®šé¢„æµ‹ä½œä¸ºæ–°ä»»åŠ¡ï¼Œæ¨åŠ¨äº†èƒå„¿è„‘éƒ¨è‡ªåŠ¨åŒ–MRIåˆ†æçš„å‘å±•ï¼ŒåŒæ—¶è¾…ä»¥ç»„ç»‡åˆ†å‰²ã€‚æˆ‘ä»¬çš„å¤šæ ·åŒ–å¤šä¸­å¿ƒæµ‹è¯•é›†é¦–æ¬¡åŒ…å«æ¥è‡ªæ–°ä½åœºï¼ˆ0.55Tï¼‰MRIæ•°æ®é›†çš„æ•°æ®ã€‚è¯„ä¼°æŒ‡æ ‡ä¹Ÿæ‰©å±•åˆ°åŒ…æ‹¬æ‹“æ‰‘ç‰¹å®šçš„æ¬§æ‹‰ç‰¹å¾å·®å¼‚ï¼ˆEDï¼‰ã€‚æœ‰åå…­æ”¯é˜Ÿä¼æäº¤äº†åˆ†å‰²æ–¹æ³•ï¼Œå…¶ä¸­å¤§å¤šæ•°åœ¨é«˜åœºå’Œä½åœºæ‰«æä¸­è¡¨ç°ä¸€è‡´ã€‚ç„¶è€Œï¼Œçºµå‘è¶‹åŠ¿è¡¨æ˜ï¼Œåˆ†å‰²ç²¾åº¦å¯èƒ½å·²è¾¾åˆ°ä¸€ä¸ªç“¶é¢ˆï¼Œç»“æœç°åœ¨å·²æ¥è¿‘ä¸åŒè¯„ä¼°äººå‘˜ä¹‹é—´çš„å·®å¼‚ã€‚EDæŒ‡æ ‡å‘ç°äº†ä¼ ç»ŸæŒ‡æ ‡æœªèƒ½æ•è·åˆ°çš„æ‹“æ‰‘å·®å¼‚ï¼ŒåŒæ—¶ä½åœºæ•°æ®é›†è·å¾—äº†æœ€é«˜çš„åˆ†å‰²å¾—åˆ†ï¼Œçªæ˜¾äº†å½“ä¸é«˜è´¨é‡é‡å»ºç›¸ç»“åˆæ—¶ï¼Œç»æµå®æƒ çš„æˆåƒç³»ç»Ÿçš„æ½œåŠ›ã€‚æœ‰ä¸ƒæ”¯é˜Ÿä¼å‚ä¸äº†ç”Ÿç‰©æµ‹å®šä»»åŠ¡ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•æœªèƒ½è¶…è¶Šä¸€ä¸ªç®€å•åŸºçº¿ï¼Œè¯¥åŸºçº¿ä»…æ ¹æ®èƒé¾„è¿›è¡Œé¢„æµ‹æµ‹é‡ï¼Œè¿™çªæ˜¾äº†ä»…ä»å›¾åƒæ•°æ®ä¸­æå–å¯é ç”Ÿç‰©æµ‹å®šä¼°è®¡çš„æŒ‘æˆ˜æ€§ã€‚åŸŸåç§»åˆ†æç¡®å®šäº†å›¾åƒè´¨é‡æ˜¯å½±å“æ¨¡å‹æ³›åŒ–çš„æœ€é‡è¦å› ç´ ï¼Œè¶…åˆ†è¾¨ç‡æµç¨‹ä¹Ÿå‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚å…¶ä»–å› ç´ ï¼Œå¦‚èƒé¾„ã€ç—…ç†å’Œé‡‡é›†åœ°ç‚¹ï¼Œè™½ç„¶å½±å“è¾ƒå°ä½†ä»å¯è¡¡é‡ã€‚æ€»ä½“è€Œè¨€ï¼ŒFeTA 2024ä¸ºå¤šç±»åˆ†å‰²å’Œèƒå„¿è„‘éƒ¨MRIä¸­çš„ç”Ÿç‰©æµ‹å®šä¼°è®¡æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™çªæ˜¾äº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ã€æ”¹è¿›æ‹“æ‰‘è¯„ä¼°å’Œæ›´å¤§çš„æ•°æ®é›†å¤šæ ·æ€§çš„éœ€æ±‚ï¼Œä»¥å®ç°ä¸´åºŠç¨³å¥å’Œå¯æ¨å¹¿çš„äººå·¥æ™ºèƒ½å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02784v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†FeTA Challenge 2024åœ¨èƒå„¿è„‘éƒ¨MRIåˆ†ææ–¹é¢çš„è¿›å±•ï¼ŒåŒ…æ‹¬ç»„ç»‡åˆ†å‰²å’Œç”Ÿç‰©è®¡é‡é¢„æµ‹ä¸¤é¡¹ä»»åŠ¡ã€‚ç ”ç©¶ä½¿ç”¨äº†å¤šä¸­å¿ƒæµ‹è¯•é›†ï¼Œå¹¶å¼•å…¥äº†æ‹“æ‰‘ç‰¹æ€§å·®å¼‚ä½œä¸ºæ–°çš„è¯„ä¼°æŒ‡æ ‡ã€‚å°½ç®¡å¤§å¤šæ•°å›¢é˜Ÿåœ¨åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”Ÿç‰©è®¡é‡é¢„æµ‹ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç ”ç©¶å¼ºè°ƒäº†æ•°æ®ä¸­å¿ƒæ–¹æ³•ã€æ›´å…ˆè¿›çš„æ‹“æ‰‘è¯„ä¼°å’Œæ›´ä¸°å¯Œçš„æ•°æ®é›†å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FeTA Challenge 2024æ‰©å±•äº†èƒå„¿è„‘éƒ¨MRIåˆ†æï¼Œå¼•å…¥ç”Ÿç‰©è®¡é‡é¢„æµ‹ä½œä¸ºæ–°ä»»åŠ¡ï¼ŒåŒæ—¶ä½¿ç”¨å¤šä¸­å¿ƒæµ‹è¯•é›†å’Œæ–°çš„æ‹“æ‰‘ç‰¹æ€§å·®å¼‚è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>å¤§å¤šæ•°å›¢é˜Ÿåœ¨åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ç¨³å®šï¼Œä½†ç”Ÿç‰©è®¡é‡é¢„æµ‹ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç®€å•çš„åŸºçº¿æ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ä½åœºMRIæ•°æ®é›†è·å¾—æœ€é«˜åˆ†å‰²åˆ†æ•°ï¼Œå‡¸æ˜¾å‡ºé«˜è´¨é‡é‡å»ºä¸ä»·æ ¼åˆç†çš„æˆåƒç³»ç»Ÿç»“åˆçš„é‡è¦æ€§ã€‚</li>
<li>åŸŸè½¬ç§»åˆ†æè¡¨æ˜å›¾åƒè´¨é‡æ˜¯å½±å“æ¨¡å‹æ³›åŒ–çš„æœ€é‡è¦å› ç´ ï¼Œè¶…åˆ†è¾¨ç‡ç®¡é“ä¹Ÿå‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</li>
<li>å…¶ä»–å› ç´ å¦‚å¦Šå¨ å‘¨æ•°ã€ç—…ç†æƒ…å†µå’Œé‡‡é›†åœ°ç‚¹å¯¹æ¨¡å‹æ€§èƒ½ä¹Ÿæœ‰ä¸€å®šå½±å“ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ•°æ®ä¸­å¿ƒæ–¹æ³•çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ”¹è¿›æ‹“æ‰‘è¯„ä¼°å’Œå¢åŠ æ•°æ®é›†å¤šæ ·æ€§ä»¥å®ç°ä¸´åºŠç¨³å¥å’Œå¯æ¨å¹¿çš„äººå·¥æ™ºèƒ½å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-75afc9e2747463cf710f8f5f87a58bab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832bc4077b2ac604381d11d72513ff93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33490215f6013f90bf3c62bc0035a8e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MaskAttn-UNet-A-Mask-Attention-Driven-Framework-for-Universal-Low-Resolution-Image-Segmentation"><a href="#MaskAttn-UNet-A-Mask-Attention-Driven-Framework-for-Universal-Low-Resolution-Image-Segmentation" class="headerlink" title="MaskAttn-UNet: A Mask Attention-Driven Framework for Universal   Low-Resolution Image Segmentation"></a>MaskAttn-UNet: A Mask Attention-Driven Framework for Universal   Low-Resolution Image Segmentation</h2><p><strong>Authors:Anzhe Cheng, Chenzhong Yin, Yu Chang, Heng Ping, Shixuan Li, Shahin Nazarian, Paul Bogdan</strong></p>
<p>Low-resolution image segmentation is crucial in real-world applications such as robotics, augmented reality, and large-scale scene understanding, where high-resolution data is often unavailable due to computational constraints. To address this challenge, we propose MaskAttn-UNet, a novel segmentation framework that enhances the traditional U-Net architecture via a mask attention mechanism. Our model selectively emphasizes important regions while suppressing irrelevant backgrounds, thereby improving segmentation accuracy in cluttered and complex scenes. Unlike conventional U-Net variants, MaskAttn-UNet effectively balances local feature extraction with broader contextual awareness, making it particularly well-suited for low-resolution inputs. We evaluate our approach on three benchmark datasets with input images rescaled to 128x128 and demonstrate competitive performance across semantic, instance, and panoptic segmentation tasks. Our results show that MaskAttn-UNet achieves accuracy comparable to state-of-the-art methods at significantly lower computational cost than transformer-based models, making it an efficient and scalable solution for low-resolution segmentation in resource-constrained scenarios. </p>
<blockquote>
<p>ä½åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä¾‹å¦‚æœºå™¨äººæŠ€æœ¯ã€å¢å¼ºç°å®å’Œå¤§è§„æ¨¡åœºæ™¯ç†è§£ç­‰ã€‚åœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œç”±äºè®¡ç®—èµ„æºé™åˆ¶ï¼Œå¾€å¾€æ— æ³•è·å¾—é«˜åˆ†è¾¨ç‡æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MaskAttn-UNetï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºä¼ ç»ŸU-Netæ¶æ„çš„æ–°å‹åˆ†å‰²æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨æ©è†œæ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°çªå‡ºé‡è¦åŒºåŸŸï¼ŒåŒæ—¶æŠ‘åˆ¶æ— å…³èƒŒæ™¯ï¼Œä»è€Œæé«˜å¤æ‚å’Œæ··ä¹±åœºæ™¯çš„åˆ†å‰²ç²¾åº¦ã€‚ä¸åŒäºä¼ ç»Ÿçš„U-Netå˜ä½“ï¼ŒMaskAttn-UNetæœ‰æ•ˆåœ°å¹³è¡¡äº†å±€éƒ¨ç‰¹å¾æå–å’Œæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡æ„è¯†ï¼Œä½¿å…¶ç‰¹åˆ«é€‚åˆä½åˆ†è¾¨ç‡è¾“å…¥ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå°†è¾“å…¥å›¾åƒç¼©æ”¾åˆ°128x128ï¼Œå¹¶åœ¨è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šå±•ç¤ºäº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMaskAttn-UNetçš„å‡†ç¡®åº¦ä¸æœ€æ–°æ–¹æ³•ç›¸å½“ï¼Œè€Œä¸”ä¸åŸºäºå˜å‹å™¨çš„æ¨¡å‹ç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ï¼Œå› æ­¤åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸­ï¼Œå®ƒæ˜¯ä½åˆ†è¾¨ç‡åˆ†å‰²çš„é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10686v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä½åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²åœ¨æœºå™¨äººã€å¢å¼ºç°å®å’Œå¤§è§„æ¨¡åœºæ™¯ç†è§£ç­‰å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†MaskAttn-UNetæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ©è†œæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºä¼ ç»ŸU-Netæ¶æ„ï¼Œé€‰æ‹©æ€§å¼ºè°ƒé‡è¦åŒºåŸŸå¹¶æŠ‘åˆ¶æ— å…³èƒŒæ™¯ï¼Œæé«˜å¤æ‚åœºæ™¯ä¸­çš„åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå¯¹è¾“å…¥å›¾åƒè¿›è¡Œ128x128ç¼©æ”¾ï¼Œå®ç°è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡çš„ç«äº‰åŠ›è¡¨ç°ï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œé€‚ç”¨äºèµ„æºå—é™åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²åœ¨æœºå™¨äººã€å¢å¼ºç°å®å’Œå¤§è§„æ¨¡åœºæ™¯ç†è§£ä¸­å…·æœ‰é‡è¦æ€§ã€‚</li>
<li>MaskAttn-UNetæ¨¡å‹æ˜¯åŸºäºä¼ ç»ŸU-Netæ¶æ„çš„æ”¹è¿›ï¼Œå¼•å…¥æ©è†œæ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>MaskAttn-UNetæ¨¡å‹èƒ½é€‰æ‹©æ€§å¼ºè°ƒé‡è¦åŒºåŸŸï¼ŒæŠ‘åˆ¶æ— å…³èƒŒæ™¯ï¼Œæé«˜å¤æ‚åœºæ™¯åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>MaskAttn-UNetæ¨¡å‹åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œé€‚ç”¨äºå¤šç§åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>MaskAttn-UNetæ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¼©æ”¾è‡³128x128ï¼Œå…·æœ‰è®¡ç®—æˆæœ¬ä½çš„ä¼˜åŠ¿ã€‚</li>
<li>MaskAttn-UNetæ¨¡å‹çš„æ€§èƒ½ä¸å½“å‰å…ˆè¿›æ–¹æ³•ç›¸å½“ï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6be664ce6778978c4db3283f4d0bec71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e442931c371dfa78f83c79cc10e70a4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57edf0d7ef2918362c9b606a99aa850d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-146c0a5900a966cfdc0af7540d880283.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9effb8ffbd24fd9d1525209c79ec2f4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Quaternionic-Reweighted-Amplitude-Flow-for-Phase-Retrieval-in-Image-Reconstruction"><a href="#Quaternionic-Reweighted-Amplitude-Flow-for-Phase-Retrieval-in-Image-Reconstruction" class="headerlink" title="Quaternionic Reweighted Amplitude Flow for Phase Retrieval in Image   Reconstruction"></a>Quaternionic Reweighted Amplitude Flow for Phase Retrieval in Image   Reconstruction</h2><p><strong>Authors:Ren Hu, Pan Lian</strong></p>
<p>Quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. In this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. Specifically, we propose the Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted QRAF algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow (QPAF) algorithm, which has linear convergence. Extensive numerical experiments on both synthetic data and real images, demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches. </p>
<blockquote>
<p>å››å…ƒä¿¡å·å¤„ç†æŠ€æœ¯æä¾›äº†å¼ºå¤§çš„å·¥å…·ï¼Œèƒ½å¤Ÿé€šè¿‡å››å…ƒä»£æ•°ä¿ç•™ä¿¡å·ç»´åº¦ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œä»è€Œæœ‰æ•ˆåœ°ç®¡ç†å½©è‰²ä¿¡å·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†å››å…ƒç›¸ä½æ¢å¤é—®é¢˜ï¼Œç³»ç»Ÿåœ°å¼€å‘äº†åŸºäºæŒ¯å¹…æ¨¡å‹çš„æ–°å‹ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†å››å…ƒé‡åŠ æƒæŒ¯å¹…æµï¼ˆQRAFï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•æœ‰ä¸‰ç§å˜ä½“ï¼šå¢é‡å‹ã€åŠ é€Ÿå‹å’Œé€‚åº”å‹QRAFç®—æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å…·æœ‰çº¿æ€§æ”¶æ•›æ€§çš„å››å…ƒæ‰°åŠ¨æŒ¯å¹…æµï¼ˆQPAFï¼‰ç®—æ³•ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®å›¾åƒä¸Šçš„å¤§é‡æ•°å€¼å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ¢å¤æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02180v2">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå››å…ƒæ•°ä¿¡å·å¤„ç†çš„æ–°å‹ç®—æ³•ï¼Œé’ˆå¯¹å››å…ƒæ•°ç›¸ä½æ¢å¤é—®é¢˜æå‡ºäº†Quaternionic Reweighted Amplitude Flowï¼ˆQRAFï¼‰ç®—æ³•åŠå…¶å¢é‡ã€åŠ é€Ÿå’Œè‡ªé€‚åº”ç‰ˆæœ¬ã€‚åŒæ—¶å¼•å…¥äº†å…·æœ‰çº¿æ€§æ”¶æ•›æ€§çš„Quaternionic Perturbed Amplitude Flowï¼ˆQPAFï¼‰ç®—æ³•ï¼Œå®éªŒè¡¨æ˜è¿™äº›æ–¹æ³•åœ¨æ¢å¤æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å››å…ƒæ•°ä¿¡å·å¤„ç†é€šè¿‡ä¿ç•™ä¿¡å·ç»´åº¦ä¹‹é—´çš„å†…åœ¨ç›¸å…³æ€§ï¼Œä¸ºæœ‰æ•ˆç®¡ç†å½©è‰²ä¿¡å·æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚</li>
<li>æœ¬æ–‡è§£å†³äº†å››å…ƒæ•°ç›¸ä½æ¢å¤é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæŒ¯å¹…æ¨¡å‹çš„å…¨æ–°ç®—æ³•ã€‚</li>
<li>ä»‹ç»äº†Quaternionic Reweighted Amplitude Flow (QRAF) ç®—æ³•åŠå…¶å¢é‡ã€åŠ é€Ÿå’Œè‡ªé€‚åº”ç‰ˆæœ¬ï¼Œè¿™äº›ç‰ˆæœ¬å¢å¼ºäº†ç®—æ³•çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†Quaternionic Perturbed Amplitude Flow (QPAF) ç®—æ³•ï¼Œå…·æœ‰çº¿æ€§æ”¶æ•›æ€§ã€‚</li>
<li>é€šè¿‡å¤§é‡çš„æ•°å€¼å®éªŒï¼Œåœ¨åˆæˆæ•°æ®å’ŒçœŸå®å›¾åƒä¸ŠéªŒè¯äº†æ‰€ææ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯åœ¨æ¢å¤æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¿™äº›ç®—æ³•å¯¹äºå¤„ç†å½©è‰²ä¿¡å·å…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼å’Œåº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58333b6a3dea1dd8b1b7d4dc4f629cda.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations"><a href="#Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations" class="headerlink" title="Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations"></a>Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations</h2><p><strong>Authors:Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen</strong></p>
<p>Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6% increase in success rates for complex real-world dexterous manipulation tasks. Project page at <a target="_blank" rel="noopener" href="https://video-prediction-policy.github.io/">https://video-prediction-policy.github.io</a> </p>
<blockquote>
<p>è§†è§‰è¡¨ç¤ºåœ¨å¼€å‘é€šç”¨æœºå™¨äººç­–ç•¥ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¹‹å‰çš„è§†è§‰ç¼–ç å™¨é€šå¸¸é€šè¿‡å•å›¾åƒé‡å»ºæˆ–ä¸¤å›¾åƒå¯¹æ¯”å­¦ä¹ è¿›è¡Œé¢„è®­ç»ƒï¼Œå®ƒä»¬æ›´å€¾å‘äºæ•æ‰é™æ€ä¿¡æ¯ï¼Œå¾€å¾€ä¼šå¿½ç•¥å¯¹å®ä½“ä»»åŠ¡è‡³å…³é‡è¦çš„åŠ¨æ€æ–¹é¢ã€‚æœ€è¿‘ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰å±•ç¤ºäº†é¢„æµ‹æœªæ¥å¸§çš„èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†å¯¹ç‰©ç†ä¸–ç•Œçš„å¼ºçƒˆç†è§£ã€‚æˆ‘ä»¬å‡è®¾VDMså¤©ç”Ÿå°±èƒ½äº§ç”ŸåŒ…å«å½“å‰é™æ€ä¿¡æ¯å’Œé¢„æµ‹æœªæ¥åŠ¨æ€çš„è§†è§‰è¡¨ç¤ºï¼Œä»è€Œä¸ºæœºå™¨äººåŠ¨ä½œå­¦ä¹ æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºäº†è§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œå®ƒå­¦ä¹ åŸºäºVDMså†…éƒ¨é¢„æµ‹æœªæ¥è¡¨ç¤ºçš„éšå¼é€†åŠ¨åŠ›å­¦æ¨¡å‹ã€‚ä¸ºäº†é¢„æµ‹æ›´ç²¾ç¡®çš„æœªæ¥ï¼Œæˆ‘ä»¬åœ¨æœºå™¨äººæ•°æ®é›†å’Œäº’è”ç½‘äººç±»æ“ä½œæ•°æ®ä¸Šå¯¹é¢„è®­ç»ƒçš„è§†é¢‘åŸºç¡€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨å®éªŒä¸­ï¼ŒVPPåœ¨Calvin ABC-Dæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸Šç›¸å¯¹äºä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³å®ç°äº†18.6%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨å¤æ‚çš„ç°å®ä¸–ç•Œçµå·§æ“ä½œä»»åŠ¡ä¸ŠæˆåŠŸç‡æé«˜äº†3.è§†æŠ•å½±é¡µé¢è§ï¼š[<a target="_blank" rel="noopener" href="https://video-prediction-policy.github/">https://video-prediction-policy.github</a> io]ç°åœ¨æä¾›ä¸Šè¿°è®ºæ–‡çš„ç¿»è¯‘ä»¥ä¾›å®¡æ ¸ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ä¿®æ”¹æ„è§æˆ–å»ºè®®ï¼Œè¯·éšæ—¶å‘ŠçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14803v2">PDF</a> ICML 2025 Spotlight Paper. The first two authors contribute equally</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¡¨ç¤ºåœ¨å¼€å‘é€šç”¨æœºå™¨äººç­–ç•¥ä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚ä¼ ç»Ÿè§†è§‰ç¼–ç å™¨ä¸»è¦æ•æ‰é™æ€ä¿¡æ¯ï¼Œå¿½ç•¥äº†åŠ¨æ€æ–¹é¢ï¼Œè¿™å¯¹äºå®ä½“ä»»åŠ¡è‡³å…³é‡è¦ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰èƒ½é¢„æµ‹æœªæ¥å¸§ï¼Œå±•ç°äº†å¼ºå¤§çš„ç‰©ç†ä¸–ç•Œç†è§£åŠ›ã€‚æˆ‘ä»¬å‡è®¾VDMsäº§ç”Ÿçš„è§†è§‰è¡¨ç¤ºåŒ…å«å½“å‰é™æ€ä¿¡æ¯å’Œé¢„æµ‹çš„æœªæ¥åŠ¨æ€ï¼Œä¸ºæœºå™¨äººåŠ¨ä½œå­¦ä¹ æä¾›å®è´µæŒ‡å¯¼ã€‚åŸºäºæ­¤å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºè§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œå®ƒå­¦ä¹ åŸºäºVDMså†…é¢„æµ‹æœªæ¥è¡¨ç¤ºçš„éšé€†åŠ¨åŠ›å­¦æ¨¡å‹ã€‚åœ¨æœºå™¨äººæ•°æ®é›†å’Œäº’è”ç½‘äººç±»æ“ä½œæ•°æ®ä¸Šå¾®è°ƒé¢„è®­ç»ƒè§†é¢‘åŸºç¡€æ¨¡å‹ä»¥é¢„æµ‹æ›´ç²¾ç¡®çš„æœªæ¥ã€‚å®éªŒä¸­ï¼ŒVPPåœ¨Calvin ABC-Dæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸Šç›¸å¯¹äºä¹‹å‰çš„æœ€ä¼˜ç­–ç•¥å®ç°äº†18.6%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œç²¾ç»†æ“ä½œä»»åŠ¡ä¸ŠæˆåŠŸç‡æé«˜äº†31.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¡¨ç¤ºå¯¹äºæœºå™¨äººç­–ç•¥çš„å­¦ä¹ è‡³å…³é‡è¦ï¼Œæ¶µç›–é™æ€å’ŒåŠ¨æ€ä¿¡æ¯ã€‚</li>
<li>ä¼ ç»Ÿè§†è§‰ç¼–ç å™¨ä¸»è¦æ•æ‰é™æ€ä¿¡æ¯ï¼Œå¯èƒ½å¿½ç•¥åŠ¨æ€æ–¹é¢ï¼Œå½±å“æœºå™¨äººä»»åŠ¡æ‰§è¡Œã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰èƒ½é¢„æµ‹æœªæ¥å¸§ï¼Œå±•ç°å‡ºå¯¹ç‰©ç†ä¸–ç•Œçš„å¼ºçƒˆç†è§£ã€‚</li>
<li>VDMsäº§ç”Ÿçš„è§†è§‰è¡¨ç¤ºåŒ…å«å½“å‰å’Œæœªæ¥çš„åŠ¨æ€ä¿¡æ¯ï¼Œå¯¹æœºå™¨äººåŠ¨ä½œå­¦ä¹ æœ‰æŒ‡å¯¼ä»·å€¼ã€‚</li>
<li>æå‡ºçš„è§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰åŸºäºé¢„æµ‹çš„æœªæ¥è¡¨ç¤ºå­¦ä¹ éšé€†åŠ¨åŠ›å­¦æ¨¡å‹ã€‚</li>
<li>å¯¹é¢„è®­ç»ƒè§†é¢‘åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜å¯¹æœªæ¥é¢„æµ‹çš„ç²¾ç¡®åº¦ã€‚</li>
<li>VPPåœ¨æ³›åŒ–æ€§èƒ½å’Œå¤æ‚æœºå™¨äººä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28eed16301fb31ab198f640220812916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bafd87c9205eff72bbcf5ca44016234f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-638c74e920c73ec3c3ff707756825979.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CAD-NeRF-Learning-NeRFs-from-Uncalibrated-Few-view-Images-by-CAD-Model-Retrieval"><a href="#CAD-NeRF-Learning-NeRFs-from-Uncalibrated-Few-view-Images-by-CAD-Model-Retrieval" class="headerlink" title="CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model   Retrieval"></a>CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model   Retrieval</h2><p><strong>Authors:Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu</strong></p>
<p>Reconstructing from multi-view images is a longstanding problem in 3D vision, where neural radiance fields (NeRFs) have shown great potential and get realistic rendered images of novel views. Currently, most NeRF methods either require accurate camera poses or a large number of input images, or even both. Reconstructing NeRF from few-view images without poses is challenging and highly ill-posed. To address this problem, we propose CAD-NeRF, a method reconstructed from less than 10 images without any known poses. Specifically, we build a mini library of several CAD models from ShapeNet and render them from many random views. Given sparse-view input images, we run a model and pose retrieval from the library, to get a model with similar shapes, serving as the density supervision and pose initializations. Here we propose a multi-view pose retrieval method to avoid pose conflicts among views, which is a new and unseen problem in uncalibrated NeRF methods. Then, the geometry of the object is trained by the CAD guidance. The deformation of the density field and camera poses are optimized jointly. Then texture and density are trained and fine-tuned as well. All training phases are in self-supervised manners. Comprehensive evaluations of synthetic and real images show that CAD-NeRF successfully learns accurate densities with a large deformation from retrieved CAD models, showing the generalization abilities. </p>
<blockquote>
<p>ä»å¤šè§†è§’å›¾åƒé‡å»ºæ˜¯3Dè§†è§‰ä¸­çš„ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„é—®é¢˜ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰åœ¨æ­¤å±•ç¤ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶èƒ½å‘ˆç°çœŸå®æ„Ÿçš„æ–°è§†è§’æ¸²æŸ“å›¾åƒã€‚ç›®å‰ï¼Œå¤§å¤šæ•°NeRFæ–¹æ³•éƒ½éœ€è¦å‡†ç¡®çš„ç›¸æœºå§¿æ€æˆ–å¤§é‡è¾“å…¥å›¾åƒï¼Œç”šè‡³ä¸¤è€…éƒ½éœ€è¦ã€‚ä»å°‘æ•°è§†è§’å›¾åƒæ— å§¿æ€åœ°é‡å»ºNeRFæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œé«˜åº¦ä¸é€‚å®šçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-NeRFæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»ä¸åˆ°10å¼ çš„å›¾åƒä¸­è¿›è¡Œé‡å»ºï¼Œè€Œæ— éœ€ä»»ä½•å·²çŸ¥çš„å§¿æ€ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬ä»ShapeNetæ„å»ºäº†å‡ ä¸ªCADæ¨¡å‹çš„è¿·ä½ åº“ï¼Œå¹¶ä»è®¸å¤šéšæœºè§†è§’è¿›è¡Œæ¸²æŸ“ã€‚ç»™å®šç¨€ç–è§†è§’çš„è¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬ä»åº“ä¸­è¿è¡Œæ¨¡å‹å’Œå§¿æ€æ£€ç´¢ï¼Œä»¥è·å¾—å…·æœ‰ç›¸ä¼¼å½¢çŠ¶çš„æ¨¡å‹ï¼Œä½œä¸ºå¯†åº¦ç›‘ç£å’Œå§¿æ€åˆå§‹åŒ–çš„åŸºç¡€ã€‚è¿™é‡Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè§†è§’å§¿æ€æ£€ç´¢æ–¹æ³•ï¼Œä»¥é¿å…å„è§†è§’é—´çš„å§¿æ€å†²çªï¼Œè¿™æ˜¯åœ¨æœªæ ¡å‡†çš„NeRFæ–¹æ³•ä¸­çš„æ–°ä¸”æœªè§çš„é—®é¢˜ã€‚ç„¶åï¼Œé€šè¿‡CADæŒ‡å¯¼è®­ç»ƒç‰©ä½“çš„å‡ ä½•å½¢çŠ¶ã€‚å¯†åº¦åœºçš„å˜å½¢å’Œç›¸æœºå§¿æ€ä¼šè¿›è¡Œè”åˆä¼˜åŒ–ã€‚ç„¶åè®­ç»ƒå’Œå¾®è°ƒçº¹ç†å’Œå¯†åº¦ã€‚æ‰€æœ‰çš„è®­ç»ƒé˜¶æ®µéƒ½æ˜¯è‡ªç›‘ç£çš„ã€‚å¯¹åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒCAD-NeRFæˆåŠŸåœ°ä»æ£€ç´¢åˆ°çš„CADæ¨¡å‹ä¸­å­¦ä¹ åˆ°å‡†ç¡®çš„å¯†åº¦ï¼Œå¹¶å±•ç¤ºäº†æ³›åŒ–èƒ½åŠ›ï¼Œå³ä½¿å­˜åœ¨å¤§çš„å˜å½¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02979v2">PDF</a> The article has been accepted by Frontiers of Computer Science (FCS)</p>
<p><strong>Summary</strong><br>     åŸºäºå°‘é‡å›¾åƒï¼ˆå°‘äº10å¼ ï¼‰ä¸”æ— å§¿æ€ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œé‡å»ºNeRFæ¨¡å‹æ˜¯å›°éš¾çš„ä¸”é«˜åº¦ä¸é€‚å®šçš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-NeRFæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ShapeNetä¸­çš„å¤šä¸ªCADæ¨¡å‹åº“è¿›è¡Œé‡å»ºï¼Œå¹¶ä»åº“ä¸­æ£€ç´¢æ¨¡å‹å’Œå§¿æ€ã€‚æˆ‘ä»¬æå‡ºäº†å¤šè§†è§’å§¿æ€æ£€ç´¢æ–¹æ³•æ¥é¿å…è§†è§’é—´çš„å§¿æ€å†²çªã€‚è®­ç»ƒé˜¶æ®µé‡‡ç”¨è‡ªæˆ‘ç›‘ç£æ–¹å¼ï¼Œé€šè¿‡CADæŒ‡å¯¼è®­ç»ƒç‰©ä½“å‡ ä½•å½¢çŠ¶ï¼Œè”åˆä¼˜åŒ–å¯†åº¦åœºçš„å˜å½¢å’Œç›¸æœºå§¿æ€ï¼Œå†è®­ç»ƒå’Œä¼˜åŒ–çº¹ç†å’Œå¯†åº¦ã€‚åœ¨åˆæˆå’ŒçœŸå®å›¾åƒä¸Šçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒCAD-NeRFæˆåŠŸå­¦ä¹ åˆ°äº†å‡†ç¡®çš„å¯†åº¦ä¿¡æ¯ï¼Œå¹¶å±•ç¤ºäº†ä»æ£€ç´¢çš„CADæ¨¡å‹ä¸­çš„å¤§å˜å½¢èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CAD-NeRFå¯ä»¥åœ¨ç¼ºå°‘ç›¸æœºå§¿æ€å’Œä»…ä½¿ç”¨å°‘é‡å›¾åƒçš„æƒ…å†µä¸‹è¿›è¡Œé‡å»ºã€‚</li>
<li>åˆ©ç”¨ShapeNetä¸­çš„CADæ¨¡å‹åº“è¿›è¡Œæ¨¡å‹æ£€ç´¢ï¼Œä¸ºé‡å»ºæä¾›å¯†åº¦ç›‘ç£å’Œå§¿æ€åˆå§‹åŒ–ã€‚</li>
<li>æå‡ºå¤šè§†è§’å§¿æ€æ£€ç´¢æ–¹æ³•æ¥è§£å†³æœªæ ¡å‡†NeRFæ–¹æ³•ä¸­çš„æ–°ä¸”æœªè§çš„é—®é¢˜â€”â€”å§¿æ€å†²çªã€‚</li>
<li>è®­ç»ƒé˜¶æ®µé‡‡ç”¨è‡ªæˆ‘ç›‘ç£æ–¹å¼ï¼Œé€šè¿‡CADæŒ‡å¯¼è®­ç»ƒç‰©ä½“å‡ ä½•å½¢çŠ¶ã€‚</li>
<li>è”åˆä¼˜åŒ–å¯†åº¦åœºçš„å˜å½¢å’Œç›¸æœºå§¿æ€ï¼Œå†è®­ç»ƒå’Œä¼˜åŒ–çº¹ç†å’Œå¯†åº¦ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®å›¾åƒä¸Šçš„è¯„ä¼°è¡¨æ˜CAD-NeRFå…·æœ‰å‡†ç¡®å­¦ä¹ å¯†åº¦ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f705d6602b7141a228521c2ff4965af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f77ddece07dedaa5525cbccdd5f45954.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca823a07d0cb58a25307c7105bbd81c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e3d5a7de62575000354d4d4394b745b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2cf8da9f09e5f8b99f4a46b9befba9d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evaluating-Deep-Learning-Models-for-Breast-Cancer-Classification-A-Comparative-Study"><a href="#Evaluating-Deep-Learning-Models-for-Breast-Cancer-Classification-A-Comparative-Study" class="headerlink" title="Evaluating Deep Learning Models for Breast Cancer Classification: A   Comparative Study"></a>Evaluating Deep Learning Models for Breast Cancer Classification: A   Comparative Study</h2><p><strong>Authors:Sania Eskandari, Ali Eslamian, Nusrat Munia, Amjad Alqarni, Qiang Cheng</strong></p>
<p>This study evaluates the effectiveness of deep learning models in classifying histopathological images for early and accurate detection of breast cancer. Eight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision Transformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and SqueezeNet, were compared using a dataset of 277,524 image patches. The Vision Transformer (ViT) model, with its attention-based mechanisms, achieved the highest validation accuracy of 94%, outperforming conventional CNNs. The study demonstrates the potential of advanced machine learning methods to enhance precision and efficiency in breast cancer diagnosis in clinical settings. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ç—…ç†å›¾åƒä»¥æ—©æœŸå’Œå‡†ç¡®æ£€æµ‹ä¹³è…ºç™Œæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ä½¿ç”¨åŒ…å«ResNet-50ã€DenseNet-121ã€ResNeXt-50ã€Vision Transformerï¼ˆViTï¼‰ã€GoogLeNetï¼ˆInception v3ï¼‰ã€EfficientNetã€MobileNetå’ŒSqueezeNetåœ¨å†…çš„å…«ç§å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚é€šè¿‡ä½¿ç”¨å«æœ‰277ï¼Œ524ä¸ªå›¾åƒå—çš„æ•°æ®é›†ï¼Œé€šè¿‡æµ‹è¯•å¾—å‡ºVision Transformerï¼ˆViTï¼‰æ¨¡å‹ä½¿ç”¨å…¶åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶è·å¾—äº†æœ€é«˜çš„éªŒè¯ç²¾åº¦ï¼Œè¾¾åˆ°94%ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¢å¼ºä¹³è…ºç™Œè¯Šæ–­çš„ç²¾ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16859v2">PDF</a> 4 pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶è¯„ä¼°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ç—…ç†å›¾åƒä»¥æ—©æœŸå’Œå‡†ç¡®æ£€æµ‹ä¹³è…ºç™Œæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ¯”è¾ƒäº†ResNet-50ã€DenseNet-121ã€ResNeXt-50ã€Vision Transformerï¼ˆViTï¼‰ã€GoogLeNetï¼ˆInception v3ï¼‰ã€EfficientNetã€MobileNetå’ŒSqueezeNetç­‰å…«ç§å…ˆè¿›æ¨¡å‹ã€‚ä½¿ç”¨åŒ…å«277,524ä¸ªå›¾åƒè¡¥ä¸çš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼Œå…¶ä¸­åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹éªŒè¯å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°94%ï¼Œä¼˜äºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚ç ”ç©¶è¯æ˜äº†å…ˆè¿›æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¢å¼ºä¹³è…ºç™Œè¯Šæ–­çš„ç²¾ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è¯„ä¼°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¹³è…ºç™Œç—…ç†å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¯¹æ¯”äº†å…«ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬ResNetã€DenseNetã€ResNeXtã€Vision Transformerã€GoogLeNetã€EfficientNetã€MobileNetå’ŒSqueezeNetã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åœ¨éªŒè¯å‡†ç¡®ç‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°94%ã€‚</li>
<li>ViTæ¨¡å‹åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¼˜äºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚</li>
<li>ç ”ç©¶æ•°æ®é›†åŒ…å«å¤§é‡çš„å›¾åƒè¡¥ä¸ï¼Œå…±è®¡277,524ä¸ªã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¹³è…ºç™Œè¯Šæ–­ä¸­æœ‰æœ›æé«˜ç²¾ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfeba4bf8172ab70ba1a0b89323253ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b476f4e9aaf20f144c75a608572c3b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef9a33f05d22e19d1d4002a7fb56f12e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28dea6e3f041edd6758be36e31d49330.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RaDialog-A-Large-Vision-Language-Model-for-Radiology-Report-Generation-and-Conversational-Assistance"><a href="#RaDialog-A-Large-Vision-Language-Model-for-Radiology-Report-Generation-and-Conversational-Assistance" class="headerlink" title="RaDialog: A Large Vision-Language Model for Radiology Report Generation   and Conversational Assistance"></a>RaDialog: A Large Vision-Language Model for Radiology Report Generation   and Conversational Assistance</h2><p><strong>Authors:Chantal Pellegrini, Ege Ã–zsoy, Benjamin Busam, Nassir Navab, Matthias Keicher</strong></p>
<p>Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: <a target="_blank" rel="noopener" href="https://github.com/ChantalMP/RaDialog">https://github.com/ChantalMP/RaDialog</a>. </p>
<blockquote>
<p>èƒ½å¤Ÿé’ˆå¯¹ç»™å®šçš„åŒ»å­¦å›¾åƒç”Ÿæˆå¹¶è®¨è®ºä¸´åºŠä¸Šæ­£ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šçš„å¯¹è°ˆå¼äººå·¥æ™ºèƒ½å·¥å…·å…·æœ‰æ”¹å˜æ”¾å°„å­¦çš„æ½œåŠ›ã€‚è¿™æ ·çš„äººæœºäº¤äº’æ”¾å°„å­¦åŠ©ç†å¯ä»¥ä¿ƒè¿›åä½œè¯Šæ–­è¿‡ç¨‹ï¼Œä»è€ŒèŠ‚çœæ—¶é—´å¹¶æé«˜æŠ¥å‘Šè´¨é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†RaDialogï¼Œè¿™æ˜¯é¦–ä¸ªç»è¿‡å…¨é¢è¯„ä¼°ä¸”é¢å‘å…¬ä¼—çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¯ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œäº¤äº’å¼å¯¹è¯ã€‚RaDialogæœ‰æ•ˆåœ°æ•´åˆäº†è§†è§‰å›¾åƒç‰¹å¾ã€ç»“æ„åŒ–ç—…ç†æ£€æŸ¥ç»“æœä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒæ—¶ä½¿ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ä½¿å…¶é€‚åº”ä¸“ä¸šé¢†åŸŸã€‚ä¸ºäº†ä¿æŒåº•å±‚LLMçš„å¯¹è¯èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºèƒ¸éƒ¨Xå°„çº¿æ”¾å°„å­¦ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªç»¼åˆçš„ã€åŠè‡ªåŠ¨æ ‡æ³¨çš„ã€ä»¥å›¾åƒä¸ºåŸºç¡€çš„æŒ‡ä»¤æ•°æ®é›†ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†å…ˆè¿›çš„ä¸´åºŠæ­£ç¡®æ€§ï¼Œå¹¶åœ¨çº æ­£æŠ¥å‘Šå’Œå›ç­”é—®é¢˜ç­‰äº¤äº’ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œè¿™ä½œä¸ºè¿ˆå‘ä¸´åºŠå¯¹è¯ç³»ç»Ÿçš„åŸºç¡€ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨githubä¸Šè·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/ChantalMP/RaDialog">https://github.com/ChantalMP/RaDialog</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.18681v3">PDF</a> Accepted for publication at MIDL 2025</p>
<p><strong>Summary</strong></p>
<p>å¯¹è¯å¼äººå·¥æ™ºèƒ½å·¥å…·å…·æœ‰ç”Ÿæˆå’Œè®¨è®ºç‰¹å®šåŒ»å­¦å›¾åƒçš„ä¸´åºŠæ­£ç¡®æ”¾å°„å­¦æŠ¥å‘Šçš„èƒ½åŠ›ï¼Œå¯ä¸ºæ”¾å°„å­¦å¸¦æ¥å˜é©ã€‚æ­¤ç±»äººç±»å‚ä¸çš„æ”¾å°„å­¦åŠ©ç†å¯æ¨åŠ¨åä½œè¯Šæ–­è¿‡ç¨‹ï¼Œä»è€ŒèŠ‚çœæ—¶é—´å¹¶æé«˜æŠ¥å‘Šè´¨é‡ã€‚ä¸ºæ­¤ç›®æ ‡ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RaDialogï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°å¹¶å…¬å¼€å‘å¸ƒçš„ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œäº¤äº’å¼å¯¹è¯çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚RaDialogæœ‰æ•ˆåœ°æ•´åˆäº†è§†è§‰å›¾åƒç‰¹å¾ã€ç»“æ„åŒ–ç—…ç†å‘ç°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒæ—¶é‡‡ç”¨é«˜æ•ˆçš„å‚æ•°å¾®è°ƒç­–ç•¥ä½¿å…¶é€‚åº”ä¸“ä¸šé¢†åŸŸã€‚ä¸ºäº†ä¿æŒåº•å±‚LLMçš„å¯¹è¯èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºèƒ¸éƒ¨Xå°„çº¿æ”¾å°„å­¦ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªç»¼åˆçš„ã€åŠè‡ªåŠ¨æ ‡è®°çš„å›¾åƒåŸºç¡€æŒ‡ä»¤æ•°æ®é›†ã€‚é€šè¿‡æ­¤æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†ä¸´åºŠæ­£ç¡®æ€§çš„æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨çº æ­£æŠ¥å‘Šå’Œå›ç­”é—®é¢˜ç­‰äº¤äº’ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œè¿™å¯ä½œä¸ºä¸´åºŠå¯¹è¯ç³»ç»Ÿçš„åŸºç¡€ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è¯å¼AIå·¥å…·åœ¨æ”¾å°„å­¦é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå’Œè®¨è®ºä¸´åºŠæ­£ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚</li>
<li>RaDialogæ˜¯é¦–ä¸ªç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œäº¤äº’å¼å¯¹è¯çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>RaDialogé›†æˆäº†è§†è§‰å›¾åƒç‰¹å¾ã€ç»“æ„åŒ–ç—…ç†å‘ç°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>RaDialogé€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥é€‚åº”ä¸“ä¸šé¢†åŸŸã€‚</li>
<li>ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæå‡ºä¸€ä¸ªç»¼åˆçš„ã€åŠè‡ªåŠ¨æ ‡è®°çš„å›¾åƒåŸºç¡€æŒ‡ä»¤æ•°æ®é›†ï¼Œä¸“æ³¨äºèƒ¸éƒ¨Xå°„çº¿æ”¾å°„å­¦ä»»åŠ¡ã€‚</li>
<li>RaDialogåœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†ä¸´åºŠæ­£ç¡®æ€§çš„æœ€æ–°æ°´å¹³ã€‚</li>
<li>RaDialogåœ¨äº¤äº’ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚æŠ¥å‘Šçº æ­£å’Œé—®ç­”ï¼Œä¸ºä¸´åºŠå¯¹è¯ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.18681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c67953fb3b0e2f1ed0806c96c0afa0e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-484bbce7b88c0bf78758654f4e38d995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef1e303568d005365b64d19aeef82a8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02fa338933e55bb882f5a168855e5bb9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-515e0bc66b06ad458007d03845ae17a8.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  FlexSpeech Towards Stable, Controllable and Expressive Text-to-Speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-10/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e120d180d577e724d91f96249920b3eb.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-10  SVAD From Single Image to 3D Avatar via Synthetic Data Generation with   Video Diffusion and Data Augmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
