<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Terra Explorable Native 3D World Model with Point Latents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-84212c18eac2ed370448fac84aeb1a36')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Terra-Explorable-Native-3D-World-Model-with-Point-Latents"><a href="#Terra-Explorable-Native-3D-World-Model-with-Point-Latents" class="headerlink" title="Terra: Explorable Native 3D World Model with Point Latents"></a>Terra: Explorable Native 3D World Model with Point Latents</h2><p><strong>Authors:Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu</strong></p>
<p>World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency. </p>
<blockquote>
<p>ä¸–ç•Œæ¨¡å‹å› å…¶å¯¹çœŸå®ä¸–ç•Œçš„å…¨é¢å»ºæ¨¡è€Œè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ç„¶ä¾èµ–äºåƒç´ å¯¹é½è¡¨ç¤ºä½œä¸ºä¸–ç•Œæ¼”åŒ–çš„åŸºç¡€ï¼Œå¿½ç•¥äº†ç‰©ç†ä¸–ç•Œçš„å›ºæœ‰ä¸‰ç»´ç‰¹æ€§ã€‚è¿™å¯èƒ½ä¼šç ´åä¸‰ç»´ä¸€è‡´æ€§å¹¶é™ä½ä¸–ç•Œæ¨¡å‹çš„å»ºæ¨¡æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Terraï¼Œä¸€ç§å†…åœ¨çš„ä¸‰ç»´ä¸–ç•Œæ¨¡å‹ï¼Œå®ƒä»¥å†…åœ¨çš„ä¸‰ç»´æ½œåœ¨ç©ºé—´è¡¨ç¤ºå¹¶ç”Ÿæˆå¯æ¢ç´¢çš„ç¯å¢ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç‚¹é«˜æ–¯å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆP2G-VAEï¼‰ï¼Œå®ƒå°†ä¸‰ç»´è¾“å…¥ç¼–ç ä¸ºæ½œåœ¨ç‚¹è¡¨ç¤ºï¼Œç„¶åå°†å…¶è§£ç ä¸ºä¸‰ç»´é«˜æ–¯åŸºæœ¬ä½“ä»¥è”åˆå»ºæ¨¡å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥ç¨€ç–ç‚¹æµåŒ¹é…ç½‘ç»œï¼ˆSPFlowï¼‰æ¥ç”Ÿæˆæ½œåœ¨ç‚¹è¡¨ç¤ºï¼Œè¯¥ç½‘ç»œå¯ä»¥åŒæ—¶å»é™¤ç‚¹æ½œå˜é‡ä½ç½®å’Œç‰¹å¾ä¸­çš„å™ªå£°ã€‚æˆ‘ä»¬çš„Terraé€šè¿‡å†…åœ¨çš„ä¸‰ç»´è¡¨ç¤ºå’Œç»“æ„å®ç°äº†ç²¾ç¡®çš„å¤šè§†è§’ä¸€è‡´æ€§ï¼Œå¹¶æ”¯æŒä»…é€šè¿‡å•æ¬¡ç”Ÿæˆè¿‡ç¨‹ä»ä»»ä½•è§†è§’è¿›è¡Œçµæ´»çš„æ¸²æŸ“ã€‚æ­¤å¤–ï¼ŒTerraé€šè¿‡ç‚¹æ½œåœ¨ç©ºé—´ä¸­çš„æ¸è¿›ç”Ÿæˆå®ç°äº†å¯æ¢ç´¢çš„ä¸–ç•Œå»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨ScanNet v2çš„æŒ‘æˆ˜æ€§å®¤å†…åœºæ™¯ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚Terraåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰è¾ƒé«˜çš„ä¸‰ç»´ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14977v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://huang-yh.github.io/terra/">https://huang-yh.github.io/terra/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTerraçš„æœ¬åœ°3Dä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å†…åœ¨çš„3Dæ½œåœ¨ç©ºé—´ä¸­è¡¨ç¤ºå’Œç”Ÿæˆå¯æ¢ç´¢çš„ç¯å¢ƒã€‚é€šè¿‡å¼•å…¥P2G-VAEï¼ˆç‚¹è‡³é«˜æ–¯å˜åˆ†è‡ªç¼–ç å™¨ï¼‰å’ŒSPFlowï¼ˆç¨€ç–ç‚¹æµåŒ¹é…ç½‘ç»œï¼‰ï¼Œå®ç°äº†åœ¨3Dæ½œåœ¨ç©ºé—´ä¸­çš„å‡ ä½•å’Œå¤–è§‚è”åˆå»ºæ¨¡ï¼Œä»¥åŠçµæ´»æ¸²æŸ“å’Œæ¸è¿›å¼ç”Ÿæˆã€‚Terraåœ¨ScanNet v2çš„å®¤å†…åœºæ™¯å®éªŒä¸Šå–å¾—äº†å‡ºè‰²çš„é‡å»ºå’Œç”Ÿæˆæ€§èƒ½ï¼Œå±•ç°äº†é«˜åº¦çš„3Dä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸–ç•Œæ¨¡å‹è¶Šæ¥è¶Šå—å…³æ³¨ï¼Œä½†éœ€è¦è§£å†³å¯¹ç°å®ä¸–ç•Œå…¨é¢å»ºæ¨¡çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ç‰©ç†ä¸–ç•Œçš„å†…åœ¨ä¸‰ç»´æ€§è´¨ï¼Œå¯èƒ½å‰Šå¼±æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æå‡ºçš„Terraæ¨¡å‹æ˜¯ä¸€ç§æœ¬åœ°ä¸‰ç»´ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å†…åœ¨çš„ä¸‰ç»´æ½œåœ¨ç©ºé—´ä¸­è¡¨ç¤ºå’Œç”Ÿæˆå¯æ¢ç´¢çš„ç¯å¢ƒã€‚</li>
<li>P2G-VAEç”¨äºå°†ä¸‰ç»´è¾“å…¥ç¼–ç ä¸ºæ½œåœ¨ç‚¹è¡¨ç¤ºï¼Œç„¶åè§£ç ä¸ºä¸‰ç»´é«˜æ–¯åŸå§‹ç‚¹ä»¥è”åˆå»ºæ¨¡å‡ ä½•å’Œå¤–è§‚ã€‚</li>
<li>SPFlowç½‘ç»œç”¨äºç”Ÿæˆæ½œåœ¨ç‚¹è¡¨ç¤ºï¼Œå¯ä»¥åŒæ—¶å¯¹ç‚¹æ½œä¼ä½ç½®çš„å™ªå£°è¿›è¡Œå»å™ªå’Œç‰¹å¾å¤„ç†ã€‚</li>
<li>Terraå®ç°äº†å¤šè§†è§’ä¸€è‡´æ€§ï¼Œå…·æœ‰æœ¬åœ°ä¸‰ç»´è¡¨ç¤ºå’Œæ¶æ„ï¼Œæ”¯æŒä»ä»»ä½•è§†è§’çµæ´»æ¸²æŸ“ï¼Œåªéœ€è¿›è¡Œä¸€æ¬¡ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡åœ¨ç‚¹æ½œä¼ç©ºé—´ä¸­çš„æ¸è¿›ç”Ÿæˆï¼ŒTerraå®ç°äº†å¯æ¢ç´¢çš„ä¸–ç•Œå»ºæ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55c1f04b89a66fdf50e5ce75d9783f91" align="middle">
<img src="https://picx.zhimg.com/v2-8d5c789c2967d57ec55f5c146fd4c130" align="middle">
<img src="https://picx.zhimg.com/v2-3645d84e8a65c119921fb83987121ee4" align="middle">
<img src="https://picx.zhimg.com/v2-4a2b22ca89682c34256297adce0f44e3" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Leveraging-Learned-Image-Prior-for-3D-Gaussian-Compression"><a href="#Leveraging-Learned-Image-Prior-for-3D-Gaussian-Compression" class="headerlink" title="Leveraging Learned Image Prior for 3D Gaussian Compression"></a>Leveraging Learned Image Prior for 3D Gaussian Compression</h2><p><strong>Authors:Seungjoo Shin, Jaesik Park, Sunghyun Cho</strong></p>
<p>Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage. </p>
<blockquote>
<p>é’ˆå¯¹ä¸‰ç»´é«˜æ–¯æ ·æ¡ï¼ˆ3DGSï¼‰çš„å‹ç¼©æŠ€æœ¯æœ€è¿‘åœ¨å‡å°‘ä¸‰ç»´é«˜æ–¯æ•°æ®çš„å­˜å‚¨å¼€é”€æ–¹é¢å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœã€‚å°½ç®¡å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å­˜å‚¨ç¼©å‡ï¼Œä½†ç”±äºç¼ºä¹å­¦ä¹ å…ˆéªŒçŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸‰ç»´GSå‹ç¼©ä»»åŠ¡çš„é€Ÿç‡å¤±çœŸæƒè¡¡æ–¹é¢çš„è¿›ä¸€æ­¥è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„3DGSå‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å­¦ä¹ åˆ°çš„å›¾åƒå…ˆéªŒçš„å¼ºå¤§è¡¨å¾èƒ½åŠ›æ¥æ¢å¤å‹ç¼©å¼•èµ·çš„è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬çš„ä¿®å¤ç½‘ç»œå»ºç«‹åœ¨æœ€åˆå‹ç¼©çš„é«˜æ–¯æ•°æ®ä¹‹ä¸Šï¼Œæœ‰æ•ˆåœ°å¯¹é€€åŒ–é«˜æ–¯å’ŒåŸå§‹é«˜æ–¯ä¹‹é—´çš„å›¾åƒç©ºé—´ä¸­çš„å‹ç¼©ä¼ªå½±è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†æé«˜é€Ÿç‡å¤±çœŸæ€§èƒ½ï¼Œæˆ‘ä»¬å°†ç²—ç•¥çš„æ¸²æŸ“æ®‹å·®ä½œä¸ºä¾§é¢ä¿¡æ¯æä¾›ç»™ä¿®å¤ç½‘ç»œã€‚é€šè¿‡åˆ©ç”¨æ¢å¤å›¾åƒçš„ç›‘ç£ä¿¡æ¯ï¼Œå¯¹å‹ç¼©çš„é«˜æ–¯æ•°æ®è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°äº†é«˜åº¦ç´§å‡‘çš„è¡¨ç¤ºå¹¶å¢å¼ºäº†æ¸²æŸ“æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ—¨åœ¨ä¸ç°æœ‰çš„é«˜æ–¯å‹ç¼©æ–¹æ³•å…¼å®¹ï¼Œä½¿å…¶åœ¨ä¸åŒçš„åŸºçº¿æ ‡å‡†ä¸‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨é€Ÿç‡å¤±çœŸæ€§èƒ½å’Œæ¸²æŸ“è´¨é‡æ–¹é¢å‡ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„3DGSå‹ç¼©æ–¹æ³•ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„å­˜å‚¨ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14705v1">PDF</a> Accepted to ICCV 2025 Workshop on ECLR</p>
<p><strong>æ‘˜è¦</strong><br>     åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå…ˆéªŒæŠ€æœ¯ï¼Œä¼˜åŒ–äº†ä¸‰ç»´é«˜æ–¯çº¹ç†é›†ï¼ˆGaussian Splatting, GSï¼‰å‹ç¼©çš„ç®—æ³•ï¼Œé€šè¿‡å¯¹å·²å‹ç¼©çš„é«˜æ–¯æ•°æ®é›†è¿›è¡Œå»ºæ¨¡å’Œåˆ†æå‹ç¼©æ‰€äº§ç”Ÿçš„å›¾åƒå¤±çœŸï¼Œè¿›è€Œæå‡äº†å‹ç¼©å­˜å‚¨çš„æ€§èƒ½å’Œæ¸²æŸ“è´¨é‡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å›¾åƒæ®‹å·®ä¿¡æ¯ï¼Œå¼ºåŒ–äº†å‹ç¼©è¿‡ç¨‹ä¸­çš„ç‡å¤±çœŸæ€§èƒ½ï¼ŒåŒæ—¶å…¼å®¹ç°æœ‰çš„é«˜æ–¯å‹ç¼©æ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å­˜å‚¨éœ€æ±‚å’Œæ¸²æŸ“è´¨é‡ä¸Šå‡ä¼˜äºå½“å‰ä¸»æµçš„ä¸‰ç»´é«˜æ–¯çº¹ç†é›†å‹ç¼©æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>å¼•å…¥æ·±åº¦å­¦ä¹ å›¾åƒå…ˆéªŒæŠ€æœ¯ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯çº¹ç†é›†ï¼ˆGSï¼‰å‹ç¼©ç®—æ³•ã€‚</li>
<li>é€šè¿‡åˆ†æå·²å‹ç¼©çš„é«˜æ–¯æ•°æ®é›†ï¼Œæé«˜å‹ç¼©å­˜å‚¨çš„æ€§èƒ½å’Œæ¸²æŸ“è´¨é‡ã€‚</li>
<li>è®¾è®¡ä¸€ä¸ªåŸºäºå·²å‹ç¼©é«˜æ–¯æ•°æ®çš„æ¢å¤ç½‘ç»œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½æœ‰æ•ˆå¤„ç†å‹ç¼©äº§ç”Ÿçš„å›¾åƒå¤±çœŸé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å›¾åƒæ®‹å·®ä¿¡æ¯å¢å¼ºç‡å¤±çœŸæ€§èƒ½ã€‚</li>
<li>å…¼å®¹ç°æœ‰é«˜æ–¯å‹ç¼©æ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>å®éªŒéªŒè¯è¯¥æ¡†æ¶åœ¨å­˜å‚¨éœ€æ±‚å’Œæ¸²æŸ“è´¨é‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe2ba2b24d5849bf9fd94fc0ab1a48e2" align="middle">
<img src="https://picx.zhimg.com/v2-3ced1e4b1f2e01179cd5accde8f6f270" align="middle">
<img src="https://picx.zhimg.com/v2-b9eb1a075706aa5cdfc43c89f3b25fe6" align="middle">
<img src="https://picx.zhimg.com/v2-17e7e6732e70fd6e3c65ae0c386ab69c" align="middle">
<img src="https://picx.zhimg.com/v2-88a76c5cff23e4928f5c4f2fbc56e4c6" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GauSSmart-Enhanced-3D-Reconstruction-through-2D-Foundation-Models-and-Geometric-Filtering"><a href="#GauSSmart-Enhanced-3D-Reconstruction-through-2D-Foundation-Models-and-Geometric-Filtering" class="headerlink" title="GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and   Geometric Filtering"></a>GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and   Geometric Filtering</h2><p><strong>Authors:Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang</strong></p>
<p>Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.   In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.   We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone. </p>
<blockquote>
<p>åœºæ™¯é‡å»ºå·²æˆä¸ºè®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œé«˜æ–¯è´´å›¾ç­‰æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶é«˜æ–¯è´´å›¾åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨ç¨€ç–è¦†ç›–çš„åŒºåŸŸæ•æ‰ç»†å¾®ç»†èŠ‚æˆ–ä¿æŒçœŸå®æ„Ÿæ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¨€ç–3Dè®­ç»ƒæ•°æ®çš„å›ºæœ‰å±€é™æ€§æ‰€è‡´ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GauSSmartï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ··åˆæ–¹æ³•ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°è¿æ¥2DåŸºç¡€æ¨¡å‹å’Œ3Dé«˜æ–¯è´´å›¾é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†æˆç†Ÿçš„2Dè®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬å‡¸è¿‡æ»¤å™¨å’Œæ¥è‡ªDINOç­‰åŸºç¡€æ¨¡å‹çš„è¯­ä¹‰ç‰¹å¾ç›‘ç£ï¼Œä»¥å¢å¼ºåŸºäºé«˜æ–¯çš„åœºæ™¯é‡å»ºã€‚é€šè¿‡åˆ©ç”¨2Dåˆ†å‰²å…ˆéªŒçŸ¥è¯†å’Œé«˜ç»´ç‰¹å¾åµŒå…¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŒ‡å¯¼é«˜æ–¯è´´å›¾çš„ç¨ å¯†åŒ–å’Œç»†åŒ–ï¼Œæ”¹è¿›äº†æ¬ ä»£è¡¨åŒºåŸŸçš„è¦†ç›–ï¼Œå¹¶ä¿ç•™äº†å¤æ‚ç»“æ„ç»†èŠ‚ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒGauSSmartåœ¨å¤§å¤šæ•°è¯„ä¼°åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºç°æœ‰é«˜æ–¯è´´å›¾ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†æ··åˆ2D-3Dæ–¹æ³•çš„å·¨å¤§æ½œåŠ›ï¼Œçªå‡ºäº†å¦‚ä½•å°†2DåŸºç¡€æ¨¡å‹ä¸3Dé‡å»ºç®¡é“ç›¸ç»“åˆï¼Œå…‹æœå•ä¸€æ–¹æ³•çš„å›ºæœ‰å±€é™æ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14270v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGauSSmartçš„æ··åˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°ç»“åˆäº†äºŒç»´åŸºç¡€æ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯Splattingé‡å»ºæŠ€æœ¯ï¼Œè§£å†³äº†åœºæ™¯é‡å»ºä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡é›†æˆæˆç†Ÿçš„äºŒç»´è®¡ç®—æœºè§†è§‰æŠ€æœ¯å’Œä»åŸºç¡€æ¨¡å‹ï¼ˆå¦‚DINOï¼‰è·å¾—çš„è¯­ä¹‰ç‰¹å¾ç›‘ç£ï¼ŒGauSSmartèƒ½å¤Ÿæå‡åŸºäºé«˜æ–¯æ–¹æ³•çš„åœºæ™¯é‡å»ºæ•ˆæœã€‚è¯¥æ–¹æ³•åˆ©ç”¨äºŒç»´åˆ†å‰²å…ˆéªŒå’Œé«˜ç»´ç‰¹å¾åµŒå…¥ï¼ŒæŒ‡å¯¼é«˜æ–¯æ–‘ç‚¹çš„å¯†é›†åŒ–å’Œç²¾ç»†åŒ–ï¼Œæ”¹è¿›äº†æ¬ ä»£è¡¨åŒºåŸŸçš„è¦†ç›–æƒ…å†µï¼Œå¹¶ä¿ç•™äº†ç²¾ç»†çš„ç»“æ„ç»†èŠ‚ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒGauSSmartåœ¨å¤§å¤šæ•°è¯„ä¼°åœºæ™¯ä¸­å‡è¡¨ç°å‡ºè¶…è¶Šä¼ ç»Ÿé«˜æ–¯Splattingçš„æ€§èƒ½ã€‚ç»“æœå±•ç¤ºäº†æ··åˆäºŒç»´-ä¸‰ç»´æ–¹æ³•çš„å·¨å¤§æ½œåŠ›ï¼Œçªæ˜¾äº†å°†äºŒç»´åŸºç¡€æ¨¡å‹ä¸ä¸‰ç»´é‡å»ºæµç¨‹ç›¸ç»“åˆå¦‚ä½•å…‹æœå•ä¸€æ–¹æ³•çš„å›ºæœ‰å±€é™æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Gaussian Splattingåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»†èŠ‚æ•æ‰å’Œç¨€ç–åŒºåŸŸçš„ç°å®æ€§ç»´æŒæ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸»è¦æ˜¯ç”±äºç¨€ç–ä¸‰ç»´è®­ç»ƒæ•°æ®çš„å›ºæœ‰å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆæ–¹æ³•GauSSmartï¼Œç»“åˆäº†äºŒç»´åŸºç¡€æ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯Splattingé‡å»ºæŠ€æœ¯ã€‚</li>
<li>GauSSmarté›†æˆäº†æˆç†Ÿçš„äºŒç»´è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬å‡¸è¿‡æ»¤å’Œæ¥è‡ªåŸºç¡€æ¨¡å‹ï¼ˆå¦‚DINOï¼‰çš„è¯­ä¹‰ç‰¹å¾ç›‘ç£ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨äºŒç»´åˆ†å‰²å…ˆéªŒå’Œé«˜ç»´ç‰¹å¾åµŒå…¥ï¼ŒGauSSmartæ”¹è¿›äº†é«˜æ–¯æ–‘ç‚¹çš„å¯†é›†åŒ–å’Œç²¾ç»†åŒ–è¿‡ç¨‹ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒGauSSmartåœ¨å¤šæ•°è¯„ä¼°åœºæ™¯ä¸­è¶…è¶Šäº†ä¼ ç»Ÿçš„Gaussian Splattingã€‚</li>
<li>ç»“æœçªæ˜¾äº†æ··åˆäºŒç»´-ä¸‰ç»´æ–¹æ³•çš„æ½œåŠ›ï¼Œæ˜¾ç¤ºå‡ºç»“åˆäºŒç»´åŸºç¡€æ¨¡å‹ä¸ä¸‰ç»´é‡å»ºæµç¨‹èƒ½å¤Ÿå…‹æœå•ä¸€æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcb9dac7594853a6fa456e166ab0cf23" align="middle">
<img src="https://picx.zhimg.com/v2-ce10a941529e531a97f079066de5d6d9" align="middle">
<img src="https://picx.zhimg.com/v2-5410135cd5e523629df20d961f072fa8" align="middle">
<img src="https://picx.zhimg.com/v2-cf51a1426d76308cf72aa9b8477ee678" align="middle">
<img src="https://picx.zhimg.com/v2-a091cc0ae86436254de50778d513e4f1" align="middle">
<img src="https://picx.zhimg.com/v2-cfaf455274dbb883378f7edb6c713edf" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Virtually-Being-Customizing-Camera-Controllable-Video-Diffusion-Models-with-Multi-View-Performance-Captures"><a href="#Virtually-Being-Customizing-Camera-Controllable-Video-Diffusion-Models-with-Multi-View-Performance-Captures" class="headerlink" title="Virtually Being: Customizing Camera-Controllable Video Diffusion Models   with Multi-View Performance Captures"></a>Virtually Being: Customizing Camera-Controllable Video Diffusion Models   with Multi-View Performance Captures</h2><p><strong>Authors:Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent TaÅŸel, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu</strong></p>
<p>We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: <a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/Virtually-Being">https://eyeline-labs.github.io/Virtually-Being</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡æ–°é¢–çš„æ•°æ®å®šåˆ¶ç®¡é“ï¼Œå®ç°äº†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šè§†è§’è§’è‰²ä¸€è‡´æ€§å’Œ3Dç›¸æœºæ§åˆ¶ã€‚æˆ‘ä»¬ä½¿ç”¨4Dé«˜æ–¯å±•å¸ƒï¼ˆ4DGSï¼‰æŠ€æœ¯é‡æ–°æ¸²æŸ“çš„å½•åˆ¶ä½“ç§¯æ•è·æ€§èƒ½æ•°æ®æ¥è®­ç»ƒè§’è‰²ä¸€è‡´æ€§ç»„ä»¶ï¼Œè¯¥æ•°æ®é€šè¿‡è§†é¢‘é‡ç…§æ˜æ¨¡å‹è·å¾—çš„å…‰çº¿å˜åŒ–å¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨è¿™äº›æ•°æ®ä¸Šå¾®è°ƒäº†æœ€å…ˆè¿›çš„å¼€æºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥æä¾›å¼ºå¤§çš„å¤šè§†è§’èº«ä»½ä¿ç•™ã€ç²¾ç¡®çš„ç›¸æœºæ§åˆ¶å’Œç¯å…‰é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜æ”¯æŒè™šæ‹Ÿç”Ÿäº§çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸¤ç§æ–¹æ³•çš„å¤šä¸»é¢˜ç”Ÿæˆï¼šè”åˆè®­ç»ƒå’Œå™ªå£°æ··åˆï¼Œåè€…èƒ½å¤Ÿåœ¨æ¨ç†æ—¶é—´æœ‰æ•ˆåœ°ç»„åˆç‹¬ç«‹å®šåˆ¶æ¨¡å‹ï¼›å®ƒè¿˜èƒ½å®ç°åœºæ™¯å’Œç°å®ç”Ÿæ´»è§†é¢‘çš„å®šåˆ¶ï¼Œä»¥åŠåœ¨å®šåˆ¶è¿‡ç¨‹ä¸­çš„åŠ¨ä½œå’Œç©ºé—´å¸ƒå±€æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè§†é¢‘è´¨é‡æœ‰æ‰€æé«˜ï¼Œä¸ªæ€§åŒ–ç²¾åº¦æ›´é«˜ï¼Œç›¸æœºæ§åˆ¶å’Œç¯å…‰é€‚åº”æ€§å¢å¼ºï¼Œæ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆåœ¨è™šæ‹Ÿç”Ÿäº§ä¸­çš„èåˆã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://eyeline-labs.github.io/Virtually-Being%E6%9F%A5%E7%9C%8B%E3%80%82">https://eyeline-labs.github.io/Virtually-BeingæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14179v1">PDF</a> Accepted to SIGGRAPH Asia 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ–°å‹å®šåˆ¶æ•°æ®ç®¡é“å®ç°äº†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šè§†è§’è§’è‰²ä¸€è‡´æ€§åŠ3Dç›¸æœºæ§åˆ¶ã€‚é€šè¿‡é‡‡ç”¨4Dé«˜æ–¯æº…å°„æŠ€æœ¯å½•åˆ¶ä½“ç§¯æ•è·æ€§èƒ½å¹¶é‡æ–°æ¸²æŸ“ï¼Œç»“åˆè§†é¢‘é‡ç…§æ˜æ¨¡å‹è·å¾—çš„å…‰ç…§å˜åŒ–ï¼Œè®­ç»ƒè§’è‰²ä¸€è‡´æ€§ç»„ä»¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šå¯¹å¼€æºè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°å¼ºå¤§çš„å¤šè§†è§’èº«ä»½ä¿ç•™ã€ç²¾ç¡®çš„ç›¸æœºæ§åˆ¶å’Œå…‰ç…§é€‚åº”æ€§ã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒè™šæ‹Ÿç”Ÿäº§çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ä½¿ç”¨è”åˆè®­ç»ƒå’Œå™ªå£°æ··åˆä¸¤ç§æ–¹æ³•è¿›è¡Œå¤šä¸»é¢˜ç”Ÿæˆï¼Œå®ç°äº†ç‹¬ç«‹å®šåˆ¶æ¨¡å‹çš„é«˜æ•ˆç»„åˆï¼›å®ç°äº†åœºæ™¯å’Œç°å®ç”Ÿæ´»è§†é¢‘çš„å®šåˆ¶ï¼Œä»¥åŠåœ¨å®šåˆ¶è¿‡ç¨‹ä¸­çš„è¿åŠ¨æ§åˆ¶å’Œç©ºé—´å¸ƒå±€æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æé«˜äº†è§†é¢‘è´¨é‡ã€ä¸ªæ€§åŒ–ç²¾åº¦ä»¥åŠç›¸æœºæ§åˆ¶å’Œå…‰ç…§é€‚åº”æ€§ï¼Œæ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆåœ¨è™šæ‹Ÿç”Ÿäº§ä¸­çš„æ•´åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ç°äº†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šè§†è§’è§’è‰²ä¸€è‡´æ€§åŠ3Dç›¸æœºæ§åˆ¶ã€‚</li>
<li>é€šè¿‡4Dé«˜æ–¯æº…å°„æŠ€æœ¯å’Œè§†é¢‘é‡ç…§æ˜æ¨¡å‹ï¼Œè®­ç»ƒè§’è‰²ä¸€è‡´æ€§ç»„ä»¶ã€‚</li>
<li>å¯¹å¼€æºè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°å¤šè§†è§’èº«ä»½ä¿ç•™ã€ç›¸æœºæ§åˆ¶å’Œå…‰ç…§é€‚åº”æ€§ã€‚</li>
<li>æ¡†æ¶æ”¯æŒè™šæ‹Ÿç”Ÿäº§çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤šä¸»é¢˜ç”Ÿæˆã€åœºæ™¯å’Œç°å®ç”Ÿæ´»è§†é¢‘çš„å®šåˆ¶ã€‚</li>
<li>é‡‡ç”¨äº†è”åˆè®­ç»ƒå’Œå™ªå£°æ··åˆä¸¤ç§æ–¹æ³•ï¼Œå®ç°äº†ç‹¬ç«‹å®šåˆ¶æ¨¡å‹çš„é«˜æ•ˆç»„åˆã€‚</li>
<li>æ¡†æ¶æ”¯æŒåœ¨å®šåˆ¶è¿‡ç¨‹ä¸­çš„è¿åŠ¨æ§åˆ¶å’Œç©ºé—´å¸ƒå±€æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f2e1c9b6d07c1c8a87adbafc04307fb" align="middle">
<img src="https://picx.zhimg.com/v2-4900dcb953b2cb479750947ce103695d" align="middle">
<img src="https://picx.zhimg.com/v2-21a76f6fd63c80dd8389a67e6bf52e7a" align="middle">
<img src="https://picx.zhimg.com/v2-ca448ac7b784020d10286130e5cb4d23" align="middle">
<img src="https://picx.zhimg.com/v2-92a92cde3a2251be1daec5310c58ada5" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Instant-Skinned-Gaussian-Avatars-for-Web-Mobile-and-VR-Applications"><a href="#Instant-Skinned-Gaussian-Avatars-for-Web-Mobile-and-VR-Applications" class="headerlink" title="Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications"></a>Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications</h2><p><strong>Authors:Naruya Kondo, Yuto Asano, Yoichi Ochiai</strong></p>
<p>We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website: <a target="_blank" rel="noopener" href="https://sites.google.com/view/gaussian-vrm">https://sites.google.com/view/gaussian-vrm</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†å³æ—¶çš®è‚¤åŒ–é«˜æ–¯åŒ–èº«ï¼ˆInstant Skinned Gaussian Avatarsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®æ—¶è·¨å¹³å°çš„3DåŒ–èº«ç³»ç»Ÿã€‚å°½ç®¡å·²ç»æœ‰è®¸å¤šå…³äºé«˜æ–¯è’™çš®åŠ¨ç”»çš„æ–¹æ³•è¢«æå‡ºï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ç›¸æœºé˜µåˆ—ã€é•¿æ—¶é—´çš„é¢„å¤„ç†æˆ–é«˜ç«¯GPUã€‚ä¸€äº›æ–¹æ³•è¯•å›¾å°†é«˜æ–¯è’™çš®è½¬æ¢ä¸ºåŸºäºç½‘æ ¼çš„è¡¨ç¤ºå½¢å¼ï¼Œä»¥å®ç°è½»é‡çº§æ€§èƒ½ï¼Œä½†ç‰ºç‰²äº†è§†è§‰ä¿çœŸåº¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡åˆ©ç”¨å¹¶è¡Œå¹³é“ºå¤„ç†æ¥é«˜æ•ˆåœ°å®ç°é«˜æ–¯è’™çš®åŠ¨ç”»ï¼Œèƒ½å¤Ÿå®æ—¶è·Ÿè¸ªåº•å±‚è’™çš®ç½‘æ ¼ï¼ŒåŒæ—¶ä¿æŒé«˜è§†è§‰ä¿çœŸåº¦ã€‚ä»åŸºäºæ™ºèƒ½æ‰‹æœºçš„3Dæ‰«æåˆ°è®¾å¤‡ä¸Šçš„é¢„å¤„ç†ï¼Œæ•´ä¸ªè¿‡ç¨‹ä»…éœ€çº¦äº”åˆ†é’Ÿï¼Œå…¶ä¸­åŒ–èº«ç”Ÿæˆæ­¥éª¤æœ¬èº«ä»…éœ€çº¦30ç§’å³å¯å®Œæˆã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå…è®¸ç”¨æˆ·ç«‹å³å°†è‡ªå·±çš„çœŸå®å¤–è²Œè½¬åŒ–ä¸º3DåŒ–èº«ï¼Œä½¿å…¶æˆä¸ºæ— ç¼é›†æˆç¤¾äº¤åª’ä½“å’Œå…ƒå®‡å®™åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚ç½‘ç«™åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/gaussian-vrm">https://sites.google.com/view/gaussian-vrm</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13978v1">PDF</a> Accepted to SUI 2025 Demo Track</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å³æ—¶è’™çš®é«˜æ–¯åŒ–èº«ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå®æ—¶è·¨å¹³å°çš„3DåŒ–èº«ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ©ç”¨å¹³è¡Œç‰‡å…ƒå¤„ç†ï¼Œå®ç°äº†é«˜æ–¯è’™çš®çš„é«˜æ•ˆåŠ¨ç”»åŒ–ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶åŠ¨æ€è·Ÿéšè’™çš®ç½‘æ ¼çš„åŒæ—¶ä¿æŒé«˜è§†è§‰ä¿çœŸåº¦ã€‚ä»åŸºäºæ™ºèƒ½æ‰‹æœºçš„3Dæ‰«æåˆ°è®¾å¤‡ä¸Šçš„é¢„å¤„ç†ï¼Œæ•´ä¸ªè¿‡ç¨‹ä»…éœ€çº¦äº”åˆ†é’Ÿï¼Œå…¶ä¸­åŒ–èº«ç”Ÿæˆæ­¥éª¤ä»…éœ€çº¦ä¸‰åç§’ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨æˆ·èƒ½å¤Ÿç«‹å³å°†å…¶çœŸå®å¤–è²Œè½¬åŒ–ä¸º3DåŒ–èº«ï¼Œéå¸¸é€‚åˆä¸ç¤¾äº¤åª’ä½“å’Œå…ƒå®‡å®™åº”ç”¨ç¨‹åºæ— ç¼é›†æˆã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>ä»‹ç»äº†å³æ—¶è’™çš®é«˜æ–¯åŒ–èº«ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ›å»ºå®æ—¶è·¨å¹³å°3DåŒ–èº«çš„ç³»ç»Ÿã€‚</li>
<li>å¾ˆå¤šæ–¹æ³•å°è¯•å¯¹é«˜æ–¯è’™çš®è¿›è¡ŒåŠ¨ç”»åŒ–ï¼Œä½†éœ€è¦ç›¸æœºé˜µåˆ—ã€é•¿æ—¶é—´çš„é¢„å¤„ç†æˆ–é«˜ç«¯GPUã€‚</li>
<li>ä¸€äº›æ–¹æ³•è¯•å›¾å°†é«˜æ–¯è’™çš®è½¬æ¢ä¸ºåŸºäºç½‘æ ¼çš„è¡¨ç¤ºå½¢å¼ï¼Œä»¥å®ç°è½»é‡çº§æ€§èƒ½ï¼Œä½†ç‰ºç‰²äº†è§†è§‰ä¿çœŸåº¦ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡åˆ©ç”¨å¹³è¡Œç‰‡å…ƒå¤„ç†ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜è§†è§‰ä¿çœŸåº¦çš„åŒæ—¶ï¼Œå®æ—¶åŠ¨æ€è·Ÿéšè’™çš®ç½‘æ ¼ã€‚</li>
<li>è¯¥ç³»ç»Ÿçš„æ•´ä¸ªæµç¨‹ä»åŸºäºæ™ºèƒ½æ‰‹æœºçš„3Dæ‰«æåˆ°è®¾å¤‡ä¸Šçš„é¢„å¤„ç†ä»…éœ€äº”åˆ†é’Ÿå®Œæˆã€‚</li>
<li>åŒ–èº«ç”Ÿæˆæ­¥éª¤å¿«é€Ÿï¼Œä»…çº¦ä¸‰åç§’å³å¯å®Œæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cf5742be8160b3509248c4eda33c488" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FlashWorld-High-quality-3D-Scene-Generation-within-Seconds"><a href="#FlashWorld-High-quality-3D-Scene-Generation-within-Seconds" class="headerlink" title="FlashWorld: High-quality 3D Scene Generation within Seconds"></a>FlashWorld: High-quality 3D Scene Generation within Seconds</h2><p><strong>Authors:Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, Liujuan Cao</strong></p>
<p>We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100$\times$ faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the modelâ€™s generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FlashWorldï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’å†…ä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆ3Dåœºæ™¯ã€‚ç›¸è¾ƒäºå…ˆå‰çš„å·¥ä½œï¼Œå…¶é€Ÿåº¦æé«˜äº†10~100å€ï¼ŒåŒæ—¶æ‹¥æœ‰å“è¶Šçš„æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‘’å¼ƒäº†ä¼ ç»Ÿçš„é¢å‘å¤šè§†è§’ï¼ˆMV-orientedï¼‰çš„èŒƒå¼ï¼Œè¯¥èŒƒå¼ç”Ÿæˆå¤šè§†è§’å›¾åƒç”¨äºåç»­çš„ä¸‰ç»´é‡å»ºï¼Œè½¬è€Œé‡‡ç”¨é¢å‘3Dçš„æ–¹æ³•ï¼Œå…¶ä¸­æ¨¡å‹åœ¨ç”Ÿæˆå¤šè§†è§’æ—¶ç›´æ¥äº§ç”Ÿä¸‰ç»´é«˜æ–¯è¡¨ç¤ºã€‚åœ¨ä¿éšœä¸‰ç»´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œé¢å‘3Dçš„æ–¹æ³•é€šå¸¸è§†è§‰è´¨é‡è¾ƒå·®ã€‚FlashWorldåŒ…æ‹¬åŒæ¨¡å¼é¢„è®­ç»ƒé˜¶æ®µå’Œè·¨æ¨¡å¼åè®­ç»ƒé˜¶æ®µï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†ä¸¤ç§èŒƒå¼çš„ä¼˜ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å€ŸåŠ©è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œé¦–å…ˆé¢„è®­ç»ƒä¸€ä¸ªåŒæ¨¡å¼å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒæ—¶æ”¯æŒé¢å‘å¤šè§†è§’å’Œé¢å‘3Dçš„ç”Ÿæˆæ¨¡å¼ã€‚ä¸ºäº†ç¼©å°é¢å‘3Dç”Ÿæˆçš„å“è´¨å·®è·ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è·¨æ¨¡å¼åè®­ç»ƒè’¸é¦ï¼Œé€šè¿‡åŒ¹é…ä¸€è‡´é¢å‘3Dæ¨¡å¼çš„åˆ†å¸ƒåˆ°é«˜è´¨é‡é¢å‘å¤šè§†è§’æ¨¡å¼æ¥æå‡å…¶è§†è§‰è´¨é‡ã€‚è¿™ä¸ä»…æé«˜äº†è§†è§‰è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸‰ç»´ä¸€è‡´æ€§ï¼Œè¿˜å‡å°‘äº†æ¨ç†æ‰€éœ€çš„å»å™ªæ­¥éª¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç­–ç•¥ï¼Œåˆ©ç”¨å¤§é‡çš„å•è§†è§’å›¾åƒå’Œæ–‡æœ¬æç¤ºåœ¨æ­¤è¿‡ç¨‹ä¸­æ¥æé«˜æ¨¡å‹å¯¹ç¦»ç¾¤è¾“å…¥çš„ä¸€èˆ¬åŒ–èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13678v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://imlixinyang.github.io/FlashWorld-Project-Page/">https://imlixinyang.github.io/FlashWorld-Project-Page/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåä¸ºFlashWorldçš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»å•ä¸€å›¾åƒæˆ–æ–‡æœ¬æç¤ºå¿«é€Ÿç”Ÿæˆ3Dåœºæ™¯ï¼Œè¾ƒä¹‹å‰çš„æ–¹æ³•å¿«10~100å€ï¼ŒåŒæ—¶æ‹¥æœ‰å“è¶Šæ¸²æŸ“è´¨é‡ã€‚å®ƒæ‘’å¼ƒäº†ä¼ ç»Ÿçš„ä»¥å¤šè§†è§’ä¸ºä¸»ï¼ˆMV-orientedï¼‰çš„æ–¹æ³•ï¼Œé‡‡ç”¨ç›´æ¥ç”Ÿæˆ3Dé«˜æ–¯è¡¨ç¤ºçš„3Då¯¼å‘æ–¹æ³•ã€‚ä¸ºæé«˜è§†è§‰è´¨é‡å’Œä¿æŒ3Dä¸€è‡´æ€§ï¼ŒFlashWorldé‡‡ç”¨åŒæ¨¡å¼é¢„è®­ç»ƒåŠè·¨æ¨¡å¼åè®­ç»ƒçš„æ–¹å¼ï¼Œç»“åˆä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ã€‚é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿›è¡ŒåŒæ¨¡å¼å¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒã€‚ä¸ºç¼©å°3Då¯¼å‘ç”Ÿæˆçš„å“è´¨å·®è·ï¼Œæå‡ºè·¨æ¨¡å¼åè®­ç»ƒè’¸é¦æ–¹æ³•ï¼ŒåŒ¹é…ä¸€è‡´æ€§çš„3Då¯¼å‘æ¨¡å¼ä¸é«˜å“è´¨çš„MV-orientedæ¨¡å¼çš„åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºåˆ©ç”¨å¤§é‡å•è§†è§’å›¾åƒå’Œæ–‡æœ¬æç¤ºå¢å¼ºæ¨¡å‹å¯¹å¼‚å¸¸è¾“å…¥çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å“è¶Šä¸”é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlashWorldæ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¿«é€Ÿä»å•ä¸€å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆ3Dåœºæ™¯ï¼Œé€Ÿåº¦è¾ƒä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—æå‡ã€‚</li>
<li>æ‘’å¼ƒä¼ ç»Ÿä»¥å¤šè§†è§’ä¸ºä¸»çš„æ–¹æ³•ï¼Œç›´æ¥ç”Ÿæˆ3Dé«˜æ–¯è¡¨ç¤ºï¼Œæé«˜æ•ˆç‡å’Œæ¸²æŸ“è´¨é‡ã€‚</li>
<li>é‡‡ç”¨åŒæ¨¡å¼é¢„è®­ç»ƒå’Œè·¨æ¨¡å¼åè®­ç»ƒçš„æ–¹å¼ï¼Œç»“åˆä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæå‡è§†è§‰è´¨é‡å’Œä¿æŒ3Dä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æå‡ºè·¨æ¨¡å¼åè®­ç»ƒè’¸é¦æ–¹æ³•ï¼ŒåŒ¹é…ä¸åŒæ¨¡å¼çš„åˆ†å¸ƒï¼Œè¿›ä¸€æ­¥æé«˜3Då¯¼å‘ç”Ÿæˆçš„å“è´¨ã€‚</li>
<li>åˆ©ç”¨å¤§é‡å•è§†è§’å›¾åƒå’Œæ–‡æœ¬æç¤ºï¼Œå¢å¼ºæ¨¡å‹å¯¹å¼‚å¸¸è¾“å…¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b8648f29ff4a7bcb56061e6ac0ed6ec" align="middle">
<img src="https://picx.zhimg.com/v2-fa80452a267f2ee0d95772aec293bfec" align="middle">
<img src="https://picx.zhimg.com/v2-2d53d318021376249b9b3f6dc1fe49db" align="middle">
<img src="https://picx.zhimg.com/v2-4c01b4e27a21b244bf374ce629303cba" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator"><a href="#VIST3A-Text-to-3D-by-Stitching-a-Multi-view-Reconstruction-Network-to-a-Video-Generator" class="headerlink" title="VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a   Video Generator"></a>VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a   Video Generator</h2><p><strong>Authors:Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Truong, Federico Tombari, Konrad Schindler</strong></p>
<p>The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as â€œgeneratorâ€ with the geometric abilities of a recent (feedforward) 3D reconstruction system as â€œdecoderâ€. We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation. </p>
<blockquote>
<p>å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆå’Œ3Dé‡å»ºæ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œä¸ºæ–‡æœ¬åˆ°3Dç”Ÿæˆçš„è½¬æ¢å¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚ç›´è§‚åœ°è®²ï¼Œå¦‚æœèƒ½å¤Ÿç»“åˆç°ä»£æ½œåœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸æœ€è¿‘çš„ï¼ˆå‰é¦ˆï¼‰3Dé‡å»ºç³»ç»Ÿçš„å‡ ä½•èƒ½åŠ›ï¼Œå°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¼ºå¤§çš„3Dåœºæ™¯ç”Ÿæˆå™¨ã€‚æˆ‘ä»¬ä»‹ç»äº†VIST3Aï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œæ­£å¥½å®ç°äº†è¿™ä¸€ç›®æ ‡ï¼Œè§£å†³äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¿…é¡»ä»¥ä¿ç•™å…¶æƒé‡ä¸­ç¼–ç çš„ä¸°å¯ŒçŸ¥è¯†çš„æ–¹å¼å°†è¿™ä¸¤ä¸ªç»„ä»¶ç»“åˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬é‡æ–°ç ”ç©¶äº†æ¨¡å‹æ‹¼æ¥ï¼Œå³ç¡®å®š3Dè§£ç å™¨ä¸­ä¸æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå™¨äº§ç”Ÿçš„æ½œåœ¨è¡¨ç¤ºæœ€ä½³åŒ¹é…çš„å±‚ï¼Œå¹¶å°†ä¸¤éƒ¨åˆ†æ‹¼æ¥åœ¨ä¸€èµ·ã€‚è¯¥æ“ä½œåªéœ€è¦ä¸€ä¸ªå°å‹æ•°æ®é›†ï¼Œè€Œæ— éœ€ä»»ä½•æ ‡ç­¾ã€‚å…¶æ¬¡ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå™¨å¿…é¡»ä¸æ‹¼æ¥çš„3Dè§£ç å™¨å¯¹é½ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„æ½œåœ¨å‘é‡èƒ½å¤Ÿè§£ç ä¸ºä¸€è‡´ä¸”æ„ŸçŸ¥ä¸Šä»¤äººä¿¡æœçš„3Dåœºæ™¯å‡ ä½•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç›´æ¥å¥–åŠ±å¾®è°ƒè¿™ä¸€æµè¡Œçš„ä¸äººç±»åå¥½å¯¹é½çš„æŠ€æœ¯ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„è§†é¢‘ç”Ÿæˆå™¨å’Œ3Dé‡å»ºæ¨¡å‹å¯¹æå‡ºçš„VIST3Aæ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚æ‰€æœ‰æµ‹è¯•é…å¯¹éƒ½æ˜¾è‘—æ”¹è¿›äº†å…ˆå‰è¾“å‡ºé«˜æ–¯splatçš„æ–‡æœ¬åˆ°3Dæ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡é€‰æ‹©é€‚åˆçš„3DåŸºç¡€æ¨¡å‹ï¼ŒVIST3Aè¿˜å¯ä»¥å®ç°é«˜è´¨é‡æ–‡æœ¬åˆ°ç‚¹äº‘å›¾çš„ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13454v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gohyojun15.github.io/VIST3A/">https://gohyojun15.github.io/VIST3A/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ç»“åˆæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸ä¸‰ç»´é‡å»ºç³»ç»Ÿçš„ä¼˜åŠ¿ï¼Œæå‡ºä¸€ç§é€šç”¨æ¡†æ¶VIST3Aï¼Œè§£å†³äº†ä¸¤å¤§æŒ‘æˆ˜ã€‚é¦–å…ˆæ˜¯é€šè¿‡æ¨¡å‹æ‹¼æ¥æŠ€æœ¯å°†ä¸¤ä¸ªç»„ä»¶ç»“åˆï¼Œæ— éœ€å¤§é‡æ•°æ®é›†å’Œæ ‡ç­¾ã€‚å…¶æ¬¡æ˜¯è°ƒæ•´æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸æ‹¼æ¥åçš„ä¸‰ç»´è§£ç å™¨å¯¹é½ï¼Œç¡®ä¿ç”Ÿæˆçš„æ½œåœ¨å˜é‡å¯è§£ç æˆè¿è´¯ã€æ„ŸçŸ¥ä¸Šä»¤äººä¿¡æœçš„ä¸‰ç»´åœºæ™¯å‡ ä½•ã€‚å®éªŒè¯æ˜ï¼ŒVIST3Aæ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°ä¸‰ç»´æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIST3Aæ¡†æ¶ç»“åˆäº†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸ä¸‰ç»´é‡å»ºç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡æ¨¡å‹æ‹¼æ¥æŠ€æœ¯ï¼Œå®ç°äº†ä¸¤ä¸ªç»„ä»¶çš„ç´§å¯†ç»“åˆï¼Œæ— éœ€å¤§é‡æ•°æ®é›†å’Œæ ‡ç­¾ã€‚</li>
<li>è°ƒæ•´æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä»¥ç¡®ä¿ç”Ÿæˆçš„æ½œåœ¨å˜é‡èƒ½å¤Ÿè¢«è§£ç æˆè¿è´¯çš„ä¸‰ç»´åœºæ™¯å‡ ä½•ã€‚</li>
<li>VIST3Aæ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°ä¸‰ç»´æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥é€‰æ‹©é€‚åˆçš„ä¸‰ç»´åŸºç¡€æ¨¡å‹ï¼Œå®ç°é«˜è´¨é‡çš„æ–‡æœ¬åˆ°ç‚¹äº‘æ˜ å°„ç”Ÿæˆã€‚</li>
<li>æå‡ºçš„æ¡†æ¶è§£å†³äº†æ–‡æœ¬ä¸ä¸‰ç»´é‡å»ºä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b562f6a43e9675ae3c9408aa2cd254ce" align="middle">
<img src="https://picx.zhimg.com/v2-72e2908c1e25798c13cd5f894f0d01e3" align="middle">
<img src="https://picx.zhimg.com/v2-c1c126c07190c02d41546d1b704df491" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-2D-Priors-and-SDF-Guidance-for-Dynamic-Urban-Scene-Rendering"><a href="#Leveraging-2D-Priors-and-SDF-Guidance-for-Dynamic-Urban-Scene-Rendering" class="headerlink" title="Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering"></a>Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering</h2><p><strong>Authors:Siddharth Tourani, Jayaram Reddy, Akash Kumbar, Satyajit Tourani, Nishant Goyal, Madhava Krishna, N. Dinesh Reddy, Muhammad Haris Khan</strong></p>
<p>Dynamic scene rendering and reconstruction play a crucial role in computer vision and augmented reality. Recent methods based on 3D Gaussian Splatting (3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban scenes they require both camera and LiDAR data, ground-truth 3D segmentations and motion data in the form of tracklets or pre-defined object templates such as SMPL. In this work, we explore whether a combination of 2D object agnostic priors in the form of depth and point tracking coupled with a signed distance function (SDF) representation for dynamic objects can be used to relax some of these requirements. We present a novel approach that integrates Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust object representation by harnessing the strengths of both methods. Our unified optimization framework enhances the geometric accuracy of 3D Gaussian splatting and improves deformation modeling within the SDF, resulting in a more adaptable and precise representation. We demonstrate that our method achieves state-of-the-art performance in rendering metrics even without LiDAR data on urban scenes. When incorporating LiDAR, our approach improved further in reconstructing and generating novel views across diverse object categories, without ground-truth 3D motion annotation. Additionally, our method enables various scene editing tasks, including scene decomposition, and scene composition. </p>
<blockquote>
<p>åŠ¨æ€åœºæ™¯æ¸²æŸ“å’Œé‡å»ºåœ¨è®¡ç®—æœºè§†è§‰å’Œå¢å¼ºç°å®é¢†åŸŸèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ€è¿‘åŸºäºä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰çš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®åœ°å¯¹åŠ¨æ€åŸå¸‚åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œä½†å®ƒä»¬éœ€è¦ç›¸æœºå’Œæ¿€å…‰é›·è¾¾æ•°æ®ã€çœŸå®çš„ä¸‰ç»´åˆ†å‰²ä»¥åŠè½¨è¿¹æˆ–é¢„å®šä¹‰å¯¹è±¡æ¨¡æ¿ï¼ˆå¦‚SMPLï¼‰å½¢å¼çš„è¿åŠ¨æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ·±åº¦ä¸ç‚¹è·Ÿè¸ªçš„äºŒç»´å¯¹è±¡æ— å…ˆéªŒçŸ¥è¯†ï¼Œç»“åˆåŠ¨æ€å¯¹è±¡çš„ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰è¡¨ç¤ºæ˜¯å¦å¯ä»¥å‡è½»ä¸Šè¿°ä¸€äº›è¦æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFsï¼‰ä¸ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡åˆ©ç”¨è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹æ¥åˆ›å»ºæ›´ç¨³å¥çš„å¯¹è±¡è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶æé«˜äº†ä¸‰ç»´é«˜æ–¯æ‹¼è´´çš„å‡ ä½•ç²¾åº¦ï¼Œå¹¶æ”¹å–„äº†SDFå†…çš„å˜å½¢å»ºæ¨¡ï¼Œä»è€Œå¾—åˆ°äº†æ›´çµæ´»å’Œç²¾ç¡®çš„å¯¹è±¡è¡¨ç¤ºã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå³ä½¿åœ¨åŸå¸‚åœºæ™¯ä¸­ä¸ä½¿ç”¨æ¿€å…‰é›·è¾¾æ•°æ®ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å½“ç»“åˆæ¿€å…‰é›·è¾¾æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä½¿ç”¨çœŸå®ä¸‰ç»´è¿åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œè¿›ä¸€æ­¥æ”¹è¿›äº†ä¸åŒå¯¹è±¡ç±»åˆ«çš„é‡å»ºå’Œæ–°å‹è§†å›¾çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ‰§è¡Œå„ç§åœºæ™¯ç¼–è¾‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬åœºæ™¯åˆ†è§£å’Œåœºæ™¯ç»„åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13381v1">PDF</a> Accepted at ICCV-2025, project page: <a target="_blank" rel="noopener" href="https://dynamic-ugsdf.github.io/">https://dynamic-ugsdf.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯å±•å¸ƒï¼ˆ3DGSï¼‰çš„æ–¹æ³•å¯¹äºåŠ¨æ€åœºæ™¯çš„æ¸²æŸ“å’Œé‡å»ºè‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢ç´¢å°†æ·±åº¦ä¸ç‚¹è·Ÿè¸ªçš„äºŒç»´å¯¹è±¡æ— å…³å…ˆéªŒä¸åŠ¨æ€å¯¹è±¡çš„ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰è¡¨ç¤ºç›¸ç»“åˆï¼Œä»¥å‡è½»å¯¹æŸäº›éœ€æ±‚çš„è¦æ±‚ã€‚é€šè¿‡å°†SDFä¸3DGSç»“åˆï¼Œåˆ›å»ºæ›´ç¨³å¥çš„å¯¹è±¡è¡¨ç¤ºï¼Œæé«˜å‡ ä½•ç²¾åº¦å’Œå˜å½¢å»ºæ¨¡ï¼Œæ— éœ€æ¿€å…‰é›·è¾¾æ•°æ®å³å¯å®ç°åŸå¸‚åœºæ™¯çš„å…ˆè¿›æ¸²æŸ“æ€§èƒ½ã€‚ç»“åˆæ¿€å…‰é›·è¾¾æ—¶ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºå’Œç”Ÿæˆä¸åŒå¯¹è±¡ç±»åˆ«çš„è§†å›¾æ–¹é¢è¿›ä¸€æ­¥æ”¹è¿›ï¼Œæ— éœ€åœ°é¢çœŸå®ä¸‰ç»´è¿åŠ¨æ³¨é‡Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯å®ç°å„ç§åœºæ™¯ç¼–è¾‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬åœºæ™¯åˆ†è§£å’Œåœºæ™¯ç»„åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSåœ¨åŠ¨æ€åœºæ™¯æ¸²æŸ“å’Œé‡å»ºä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>äºŒç»´å¯¹è±¡æ— å…³å…ˆéªŒï¼ˆæ·±åº¦ä¸ç‚¹è·Ÿè¸ªï¼‰ä¸åŠ¨æ€å¯¹è±¡çš„ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰è¡¨ç¤ºç›¸ç»“åˆï¼Œå¯å‡è½»å¯¹ç‰¹å®šæ•°æ®å’Œæ–¹æ³•çš„è¦æ±‚ã€‚</li>
<li>ç»“åˆSDFä¸3DGSåˆ›å»ºæ›´ç¨³å¥çš„å¯¹è±¡è¡¨ç¤ºã€‚</li>
<li>ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶æé«˜å‡ ä½•ç²¾åº¦å’Œå˜å½¢å»ºæ¨¡æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åŸå¸‚åœºæ™¯æ¸²æŸ“æ–¹é¢è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œå³ä½¿ä¸ä½¿ç”¨æ¿€å…‰é›·è¾¾æ•°æ®ã€‚</li>
<li>ç»“åˆæ¿€å…‰é›·è¾¾æ•°æ®æ—¶ï¼Œæ–¹æ³•åœ¨é‡å»ºå’Œç”Ÿæˆä¸åŒå¯¹è±¡ç±»åˆ«çš„è§†å›¾æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f81a6349ab5452ec2498d4bd858a1586" align="middle">
<img src="https://picx.zhimg.com/v2-6cf883318bebccfb6d23a931a4fed168" align="middle">
<img src="https://picx.zhimg.com/v2-8f5dc8592a6f4d8af766b73feef75e0e" align="middle">
<img src="https://picx.zhimg.com/v2-83ffe7cfeb5b22c3bf34e04d5194d195" align="middle">
<img src="https://picx.zhimg.com/v2-ac22bc01d092a3e5af8f0f5ad4b036ca" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="STT-GS-Sample-Then-Transmit-Edge-Gaussian-Splatting-with-Joint-Client-Selection-and-Power-Control"><a href="#STT-GS-Sample-Then-Transmit-Edge-Gaussian-Splatting-with-Joint-Client-Selection-and-Power-Control" class="headerlink" title="STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client   Selection and Power Control"></a>STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client   Selection and Power Control</h2><p><strong>Authors:Zhen Li, Xibin Jin, Guoliang Li, Shuai Wang, Miaowen Wen, Huseyin Arslan, Derrick Wing Kwan Ng, Chengzhong Xu</strong></p>
<p>Edge Gaussian splatting (EGS), which aggregates data from distributed clients and trains a global GS model at the edge server, is an emerging paradigm for scene reconstruction. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clientsâ€™ images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments unveil that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. It is found that the GS-oriented objective can be accurately predicted with low sampling ratios (e.g.,10%), and our method achieves an excellent tradeoff between view contributions and communication costs. </p>
<blockquote>
<p>è¾¹ç¼˜é«˜æ–¯æ‹¼æ¥ï¼ˆEGSï¼‰æ˜¯ä¸€ç§æ–°å…´çš„åœºæ™¯é‡å»ºèŒƒå¼ï¼Œå®ƒä»åˆ†å¸ƒå¼å®¢æˆ·ç«¯èšåˆæ•°æ®å¹¶åœ¨è¾¹ç¼˜æœåŠ¡å™¨è®­ç»ƒå…¨å±€GSæ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„è¾¹ç¼˜èµ„æºç®¡ç†æ–¹æ³•ä¸åŒï¼ŒEGSæ—¨åœ¨æœ€å¤§åŒ–GSè´¨é‡ï¼Œä½¿å¾—ç°æœ‰æ–¹æ³•ä¸é€‚ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡åˆ¶å®šäº†ä¸€ä¸ªæ–°çš„é¢å‘GSçš„ç›®æ ‡å‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤ŸåŒºåˆ†ä¸åŒå®¢æˆ·ç«¯çš„å¼‚æ„è§†å›¾è´¡çŒ®ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¯¥å‡½æ•°éœ€è¦å®¢æˆ·ç«¯çš„å›¾åƒï¼Œè¿™å¯¼è‡´äº†ä¸€ä¸ªå› æœå›°å¢ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§é‡‡æ ·åä¼ è¾“çš„EGSï¼ˆæˆ–ç®€ç§°STT-GSï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é¦–å…ˆä»æ¯ä¸ªå®¢æˆ·ç«¯é‡‡æ ·ä¸€éƒ¨åˆ†å›¾åƒä½œä¸ºè¯•ç‚¹æ•°æ®è¿›è¡ŒæŸå¤±é¢„æµ‹ã€‚åŸºäºç¬¬ä¸€é˜¶æ®µçš„è¯„ä¼°ï¼Œç„¶åå°†é€šä¿¡èµ„æºä¼˜å…ˆåˆ†é…ç»™æ›´æœ‰ä»·å€¼çš„å®¢æˆ·ç«¯ã€‚ä¸ºäº†å®ç°æœ‰æ•ˆçš„é‡‡æ ·ï¼Œæå‡ºäº†ä¸€ç§ç‰¹å¾åŸŸèšç±»ï¼ˆFDCï¼‰æ–¹æ¡ˆæ¥é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ•°æ®ï¼Œå¹¶é‡‡ç”¨ä¼ è¾“æ—¶é—´æœ€å°åŒ–ï¼ˆPTTMï¼‰æ¥å‡å°‘è¯•ç‚¹å¼€é”€ã€‚éšåï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè”åˆå®¢æˆ·ç«¯é€‰æ‹©å’ŒåŠŸç‡æ§åˆ¶ï¼ˆJCSPCï¼‰æ¡†æ¶ï¼Œä»¥åœ¨é€šä¿¡èµ„æºçº¦æŸä¸‹æœ€å¤§åŒ–é¢å‘GSçš„å‡½æ•°ã€‚å°½ç®¡è¯¥é—®é¢˜å…·æœ‰éå‡¸æ€§ï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæƒ©ç½šäº¤æ›¿ä¸»è¦æœ€å°åŒ–ï¼ˆPAMMï¼‰ç®—æ³•çš„ä½å¤æ‚åº¦æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ä½é‡‡æ ·ç‡ï¼ˆä¾‹å¦‚10%ï¼‰å³å¯å‡†ç¡®é¢„æµ‹é¢å‘GSçš„ç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†å›¾è´¡çŒ®å’Œé€šä¿¡æˆæœ¬ä¹‹é—´å®ç°äº†å‡ºè‰²çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13186v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è¾¹ç¼˜æœåŠ¡å™¨èšåˆæ•°æ®å¹¶è®­ç»ƒå…¨å±€é«˜æ–¯å–·å°„æ¨¡å‹ï¼ˆGSæ¨¡å‹ï¼‰çš„è¾¹ç¼˜é«˜æ–¯å–·å°„ï¼ˆEGSï¼‰æŠ€æœ¯ä¸ºåœºæ™¯é‡å»ºæä¾›äº†ä¸€ç§æ–°å…´èŒƒå¼ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•æ— æ³•æœ€å¤§åŒ–GSè´¨é‡çš„é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªé¢å‘GSçš„ç›®æ ‡å‡½æ•°ï¼Œå¹¶è€ƒè™‘äº†ä¸åŒå®¢æˆ·çš„å¼‚æ„è§†å›¾è´¡çŒ®ã€‚ä¸ºè¯„ä¼°è¯¥å‡½æ•°éœ€è¦å®¢æˆ·å›¾åƒï¼Œå¯¼è‡´å› æœå›°å¢ƒã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†é‡‡æ ·åä¼ è¾“çš„EGSç­–ç•¥ï¼ˆSTT-GSï¼‰ï¼Œé¦–å…ˆä»æ¯ä¸ªå®¢æˆ·ä¸­é‡‡æ ·éƒ¨åˆ†å›¾åƒä½œä¸ºè¯•ç‚¹æ•°æ®è¿›è¡ŒæŸå¤±é¢„æµ‹ã€‚åŸºäºç¬¬ä¸€é˜¶æ®µè¯„ä¼°ï¼Œä¼˜å…ˆä¸ºæ›´æœ‰ä»·å€¼çš„å®¢æˆ·æä¾›é€šä¿¡èµ„æºã€‚ä¸ºå®ç°é«˜æ•ˆé‡‡æ ·ï¼Œæå‡ºäº†ç‰¹å¾åŸŸèšç±»æ–¹æ¡ˆå¹¶é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ•°æ®ï¼Œå¹¶é‡‡ç”¨ä¼ è¾“æ—¶é—´æœ€å°åŒ–æ¥å‡å°‘è¯•ç‚¹å¼€é”€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼€å‘äº†è”åˆå®¢æˆ·é€‰æ‹©å’ŒåŠŸç‡æ§åˆ¶æ¡†æ¶ï¼Œä»¥åœ¨é€šä¿¡èµ„æºé™åˆ¶ä¸‹æœ€å¤§åŒ–é¢å‘GSçš„å‡½æ•°ã€‚å°½ç®¡é—®é¢˜å…·æœ‰éå‡¸æ€§ï¼Œä½†åŸºäºæƒ©ç½šäº¤æ›¿ä¸»è¦æœ€å°åŒ–ç®—æ³•æå‡ºäº†ä½å¤æ‚åº¦è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨çœŸå®æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥åœ¨ä½é‡‡æ ·ç‡ä¸‹å‡†ç¡®é¢„æµ‹é¢å‘GSçš„ç›®æ ‡ï¼Œå¹¶åœ¨è§†å›¾è´¡çŒ®å’Œé€šä¿¡æˆæœ¬ä¹‹é—´å–å¾—äº†å‡ºè‰²çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EGSæŠ€æœ¯æ˜¯ä¸€ç§æ–°å…´çš„åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡è¾¹ç¼˜æœåŠ¡å™¨èšåˆæ•°æ®å¹¶è®­ç»ƒå…¨å±€GSæ¨¡å‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•æœ€å¤§åŒ–GSè´¨é‡çš„é—®é¢˜ï¼Œå› æ­¤æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªé¢å‘GSçš„ç›®æ ‡å‡½æ•°ã€‚</li>
<li>è¯„ä¼°ç›®æ ‡å‡½æ•°éœ€è¦å®¢æˆ·å›¾åƒï¼Œå¯¼è‡´å› æœå›°å¢ƒï¼Œä¸ºæ­¤æå‡ºäº†STT-GSç­–ç•¥è¿›è¡Œé‡‡æ ·åä¼ è¾“ã€‚</li>
<li>ä¸ºå®ç°é«˜æ•ˆé‡‡æ ·ï¼Œé‡‡ç”¨ç‰¹å¾åŸŸèšç±»æ–¹æ¡ˆå¹¶é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ•°æ®ã€‚</li>
<li>è”åˆå®¢æˆ·é€‰æ‹©å’ŒåŠŸç‡æ§åˆ¶æ¡†æ¶è¢«å¼€å‘ï¼Œä»¥åœ¨é€šä¿¡èµ„æºé™åˆ¶ä¸‹æœ€å¤§åŒ–é¢å‘GSçš„å‡½æ•°ã€‚</li>
<li>æ‰€æå‡ºçš„è§£å†³æ–¹æ¡ˆåŸºäºæƒ©ç½šäº¤æ›¿ä¸»è¦æœ€å°åŒ–ç®—æ³•ï¼Œå…·æœ‰ä½å¤æ‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-caeb569986dc7bbd86c9a94484cb3e01" align="middle">
<img src="https://picx.zhimg.com/v2-2bb1d03fddc1204611faf01af78bf100" align="middle">
<img src="https://picx.zhimg.com/v2-6d96c525a4b979acc4df84bb4bf0dc11" align="middle">
<img src="https://picx.zhimg.com/v2-2ea69069cf1358308639d87703f8ad38" align="middle">
<img src="https://picx.zhimg.com/v2-cc5f4b4d6eb308f97cd899c17b486c40" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SimULi-Real-Time-LiDAR-and-Camera-Simulation-with-Unscented-Transforms"><a href="#SimULi-Real-Time-LiDAR-and-Camera-Simulation-with-Unscented-Transforms" class="headerlink" title="SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms"></a>SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms</h2><p><strong>Authors:Haithem Turki, Qi Wu, Xin Kang, Janick Martinez Esturo, Shengyu Huang, Ruilong Li, Zan Gojcic, Riccardo de Lutio</strong></p>
<p>Rigorous testing of autonomous robots, such as self-driving vehicles, is essential to ensure their safety in real-world deployments. This requires building high-fidelity simulators to test scenarios beyond those that can be safely or exhaustively collected in the real-world. Existing neural rendering methods based on NeRF and 3DGS hold promise but suffer from low rendering speeds or can only render pinhole camera models, hindering their suitability to applications that commonly require high-distortion lenses and LiDAR data. Multi-sensor simulation poses additional challenges as existing methods handle cross-sensor inconsistencies by favoring the quality of one modality at the expense of others. To overcome these limitations, we propose SimULi, the first method capable of rendering arbitrary camera models and LiDAR data in real-time. Our method extends 3DGUT, which natively supports complex camera models, with LiDAR support, via an automated tiling strategy for arbitrary spinning LiDAR models and ray-based culling. To address cross-sensor inconsistencies, we design a factorized 3D Gaussian representation and anchoring strategy that reduces mean camera and depth error by up to 40% compared to existing methods. SimULi renders 10-20x faster than ray tracing approaches and 1.5-10x faster than prior rasterization-based work (and handles a wider range of camera models). When evaluated on two widely benchmarked autonomous driving datasets, SimULi matches or exceeds the fidelity of existing state-of-the-art methods across numerous camera and LiDAR metrics. </p>
<blockquote>
<p>å¯¹è‡ªåŠ¨é©¾é©¶è½¦è¾†ç­‰è‡ªä¸»æœºå™¨äººçš„ä¸¥æ ¼æµ‹è¯•æ˜¯ç¡®ä¿å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­å®‰å…¨æ€§çš„å…³é”®ã€‚è¿™éœ€è¦ä½¿ç”¨é«˜ä¿çœŸæ¨¡æ‹Ÿå™¨æ¥æµ‹è¯•è¶…å‡ºé‚£äº›åœ¨ç°å®ä¸–ç•Œä¸­å¯ä»¥å®‰å…¨æˆ–è¯¦å°½æ”¶é›†çš„åœºæ™¯ã€‚åŸºäºNeRFå’Œ3DGSçš„ç°æœ‰ç¥ç»æ¸²æŸ“æ–¹æ³•è™½ç„¶å¾ˆæœ‰å‰æ™¯ï¼Œä½†å®ƒä»¬å­˜åœ¨æ¸²æŸ“é€Ÿåº¦æ…¢æˆ–åªèƒ½å‘ˆç°é’ˆå­”ç›¸æœºæ¨¡å‹çš„å±€é™æ€§ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨å¯¹é«˜å¤±çœŸé•œå¤´å’Œæ¿€å…‰é›·è¾¾æ•°æ®é€šå¸¸æœ‰æ‰€è¦æ±‚çš„åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚å¤šä¼ æ„Ÿå™¨ä»¿çœŸå¸¦æ¥äº†é¢å¤–çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•é€šè¿‡ç‰ºç‰²å…¶ä»–æ¨¡æ€çš„è´¨é‡æ¥ä¼˜å…ˆè€ƒè™‘æŸä¸€æ¨¡æ€çš„è·¨ä¼ æ„Ÿå™¨ä¸ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SimULiï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿå®æ—¶å‘ˆç°ä»»æ„ç›¸æœºæ¨¡å‹å’Œæ¿€å…‰é›·è¾¾æ•°æ®çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•äº†3DGUTï¼Œå®ƒåŸç”Ÿæ”¯æŒå¤æ‚çš„ç›¸æœºæ¨¡å‹ï¼Œé€šè¿‡è‡ªåŠ¨å¹³é“ºç­–ç•¥å’ŒåŸºäºå°„çº¿çš„å‰”é™¤æŠ€æœ¯æ¥æ”¯æŒæ¿€å…‰é›·è¾¾æ•°æ®ã€‚ä¸ºäº†è§£å†³è·¨ä¼ æ„Ÿå™¨çš„ä¸ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†è§£çš„3Dé«˜æ–¯è¡¨ç¤ºå’Œé”šå®šç­–ç•¥ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå°†æ‘„åƒæœºå’Œæ·±åº¦è¯¯å·®å¹³å‡å‡å°‘äº†é«˜è¾¾40%ã€‚SimULiçš„æ¸²æŸ“é€Ÿåº¦æ˜¯å…‰çº¿è¿½è¸ªæ–¹æ³•çš„10-20å€ï¼Œæ˜¯å…ˆå‰åŸºäºå…‰æ …åŒ–çš„å·¥ä½œçš„1.5-10å€ï¼ˆå¹¶ä¸”æ”¯æŒæ›´å¹¿æ³›çš„ç›¸æœºæ¨¡å‹ï¼‰ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼ŒSimULiåœ¨å¤šä¸ªç›¸æœºå’Œæ¿€å…‰é›·è¾¾æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†ç°æœ‰æœ€æ–°æ–¹æ³•çš„ä¿çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12901v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/sil/projects/simuli">https://research.nvidia.com/labs/sil/projects/simuli</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†è‡ªä¸»æœºå™¨äººä¸¥æ ¼æµ‹è¯•çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºé«˜ä¿çœŸæ¨¡æ‹Ÿå™¨åœ¨æµ‹è¯•è¶…è¶Šç°å®ä¸–ç•Œå®‰å…¨æˆ–è¯¦å°½æ”¶é›†çš„åœºæ™¯ä¸­çš„å…³é”®ä½œç”¨ã€‚ç°æœ‰åŸºäºNeRFå’Œ3DGSçš„ç¥ç»æ¸²æŸ“æ–¹æ³•å­˜åœ¨æ¸²æŸ“é€Ÿåº¦æ…¢æˆ–åªèƒ½æ¸²æŸ“é’ˆå­”ç›¸æœºæ¨¡å‹çš„å±€é™æ€§ï¼Œéš¾ä»¥æ»¡è¶³é«˜å¤±çœŸé•œå¤´å’ŒLiDARæ•°æ®çš„åº”ç”¨éœ€æ±‚ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SimULiæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®æ—¶æ¸²æŸ“ä»»æ„ç›¸æœºæ¨¡å‹å’ŒLiDARæ•°æ®ï¼Œå¹¶æ‰©å±•äº†æ”¯æŒå¤æ‚ç›¸æœºæ¨¡å‹çš„3DGUTï¼Œé€šè¿‡è‡ªåŠ¨åŒ–å¹³é“ºç­–ç•¥å’ŒåŸºäºå°„çº¿çš„å‰”é™¤æŠ€æœ¯æ¥æ”¯æŒLiDARã€‚ä¸ºè§£å†³è·¨ä¼ æ„Ÿå™¨çš„ä¸ä¸€è‡´æ€§ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§åˆ†è§£çš„3Dé«˜æ–¯è¡¨ç¤ºå’Œé”šå®šç­–ç•¥ï¼Œå°†ç›¸æœºå’Œæ·±åº¦è¯¯å·®å‡å°‘äº†é«˜è¾¾40%ã€‚SimULiçš„æ¸²æŸ“é€Ÿåº¦æ¯”å…‰çº¿è¿½è¸ªæ–¹æ³•å¿«10-20å€ï¼Œæ¯”å…ˆå‰çš„åŸºäºå…‰æ …åŒ–çš„å·¥ä½œå¿«1.5-10å€ï¼Œå¹¶ä¸”æ”¯æŒæ›´å¹¿æ³›çš„ç›¸æœºæ¨¡å‹ã€‚åœ¨ä¸¤ä¸ªå¹¿æ³›è¯„ä¼°çš„è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šï¼ŒSimULiåœ¨å¤šä¸ªç›¸æœºå’ŒLiDARæŒ‡æ ‡ä¸Šçš„é€¼çœŸåº¦è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»æœºå™¨äººçš„ä¸¥æ ¼æµ‹è¯•å¯¹äºç¡®ä¿å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­çš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚</li>
<li>é«˜ä¿çœŸæ¨¡æ‹Ÿå™¨èƒ½å¤Ÿæµ‹è¯•è¶…è¶Šç°å®ä¸–ç•Œä¸­å®‰å…¨æˆ–è¯¦å°½æ”¶é›†çš„åœºæ™¯ã€‚</li>
<li>ç°æœ‰ç¥ç»æ¸²æŸ“æ–¹æ³•å­˜åœ¨æ¸²æŸ“é€Ÿåº¦æ…¢æˆ–åªèƒ½æ¸²æŸ“ç‰¹å®šç›¸æœºæ¨¡å‹çš„å±€é™æ€§ã€‚</li>
<li>SimULiæ–¹æ³•èƒ½å¤Ÿå®æ—¶æ¸²æŸ“ä»»æ„ç›¸æœºæ¨¡å‹å’ŒLiDARæ•°æ®ï¼Œæ‰©å±•äº†3DGUTã€‚</li>
<li>SimULié€šè¿‡è‡ªåŠ¨åŒ–ç­–ç•¥æ”¯æŒLiDARæ•°æ®ï¼Œè§£å†³è·¨ä¼ æ„Ÿå™¨çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>SimULié‡‡ç”¨åˆ†è§£çš„3Dé«˜æ–¯è¡¨ç¤ºå’Œé”šå®šç­–ç•¥ï¼Œæé«˜äº†ç›¸æœºå’Œæ·±åº¦è¯¯å·®çš„å‡å°‘ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd0fb0faea61889d3f905ca7ef865ab4" align="middle">
<img src="https://picx.zhimg.com/v2-e2208d2f33539ae1e0d54979b8efd2df" align="middle">
<img src="https://picx.zhimg.com/v2-e3e312ab31efe9ec09b6bc03bdc03e21" align="middle">
<img src="https://picx.zhimg.com/v2-b39ddb7dfd24f0036d0aae934865142f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BSGS-Bi-stage-3D-Gaussian-Splatting-for-Camera-Motion-Deblurring"><a href="#BSGS-Bi-stage-3D-Gaussian-Splatting-for-Camera-Motion-Deblurring" class="headerlink" title="BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring"></a>BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring</h2><p><strong>Authors:An Zhao, Piaopiao Yu, Zhe Zhu, Mingqiang Wei</strong></p>
<p>3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene reconstruction.However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion blur.To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur distortions.To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both stages.Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts. </p>
<blockquote>
<p>3Dé«˜æ–¯æ‘Šé“ºæŠ€æœ¯åœ¨3Dåœºæ™¯é‡å»ºä¸­å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»ç”±äºç›¸æœºè¿åŠ¨é€ æˆçš„è¿åŠ¨æ¨¡ç³Šå›¾åƒé‡å»ºé«˜è´¨é‡3Dåœºæ™¯æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäº3DGSçš„å»æ¨¡ç³Šæ–¹æ³•çš„æ€§èƒ½ç”±äºå…¶å›ºæœ‰æœºåˆ¶è€Œå—åˆ°é™åˆ¶ï¼Œä¾‹å¦‚æåº¦ä¾èµ–äºç›¸æœºå§¿æ€çš„å‡†ç¡®æ€§ï¼Œä»¥åŠæ— æ³•æœ‰æ•ˆæ§åˆ¶ç”±è¿åŠ¨æ¨¡ç³Šå¼•èµ·çš„é”™è¯¯é«˜æ–¯åŸºæœ¬å½¢æ€å¯†å®åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”åŒé˜¶æ®µ3Dé«˜æ–¯æ‘Šé“ºï¼Œä»¥å‡†ç¡®åœ°ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒé‡å»º3Dåœºæ™¯ã€‚BSGSåŒ…å«ä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œç›¸æœºå§¿æ€ä¼˜åŒ–å¤§è‡´ä¼˜åŒ–ç›¸æœºå§¿æ€ï¼Œä»¥å‡å°‘è¿åŠ¨å¼•èµ·çš„å¤±çœŸã€‚å…¶æ¬¡ï¼Œåœ¨å›ºå®šçš„ç²—ç•¥ç›¸æœºå§¿æ€ä¸‹ï¼Œå…¨å±€åˆšæ€§å˜æ¢è¿›ä¸€æ­¥æ ¡æ­£è¿åŠ¨å¼•èµ·çš„æ¨¡ç³Šå¤±çœŸã€‚ä¸ºäº†ç¼“è§£å¤šå­å¸§æ¢¯åº¦å†²çªï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å­å¸§æ¢¯åº¦èšåˆç­–ç•¥æ¥ä¼˜åŒ–è¿™ä¸¤ä¸ªé˜¶æ®µã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ—¶ç©ºåŒé˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œä»¥åŠ¨æ€è°ƒæ•´åŸºæœ¬å¯†å®åŒ–é˜ˆå€¼ï¼Œé˜²æ­¢æ¨¡ç³ŠåŒºåŸŸè¿‡æ—©äº§ç”Ÿå˜ˆæ‚çš„é«˜æ–¯ã€‚ç»¼åˆå®éªŒéªŒè¯äº†æ‰€æå‡ºçš„å»æ¨¡ç³Šæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12493v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯æç”»ï¼ˆ3DGSï¼‰æŠ€æœ¯çš„é«˜ç²¾åº¦é‡å»ºç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å› ç›¸æœºè¿åŠ¨å¼•èµ·çš„å›¾åƒè¿åŠ¨æ¨¡ç³Šé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°å‹çš„åŒé˜¶æ®µä¸‰ç»´é«˜æ–¯æç”»æ¡†æ¶ï¼ˆBSGSï¼‰ï¼Œç”¨äºä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å‡†ç¡®é‡å»ºä¸‰ç»´åœºæ™¯ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¸ºç›¸æœºå§¿æ€ä¼˜åŒ–ï¼Œç”¨äºå‡å°‘è¿åŠ¨å¼•èµ·çš„ç•¸å˜ï¼›ç¬¬äºŒé˜¶æ®µä¸ºå…¨å±€åˆšä½“å˜æ¢ï¼Œç”¨äºè¿›ä¸€æ­¥ä¿®æ­£è¿åŠ¨å¼•èµ·çš„æ¨¡ç³Šç•¸å˜ã€‚é€šè¿‡å­å¸§æ¢¯åº¦èšåˆç­–ç•¥ç¼“è§£å¤šå­å¸§æ¢¯åº¦å†²çªï¼Œå¹¶é‡‡ç”¨æ—¶ç©ºåŒé˜¶æ®µä¼˜åŒ–ç­–ç•¥åŠ¨æ€è°ƒæ•´åŸºæœ¬ç²’å­å¯†åº¦é˜ˆå€¼ï¼Œé¿å…æ¨¡ç³ŠåŒºåŸŸè¿‡æ—©äº§ç”Ÿå™ªå£°é«˜æ–¯ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºå»æ¨¡ç³Šæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨è¡Œä¸šå†…é¢†å…ˆçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹åŸºäº3DGSçš„å»æ¨¡ç³ŠæŠ€æœ¯ï¼Œå¤„ç†ç›¸æœºè¿åŠ¨å¯¼è‡´çš„å›¾åƒè¿åŠ¨æ¨¡ç³Šæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>æ–°æ¡†æ¶åä¸ºBi-Stage 3D Gaussian Splattingï¼ˆBSGSï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>BSGSåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µä¼˜åŒ–ç›¸æœºå§¿æ€ä»¥å‡å°‘è¿åŠ¨ç•¸å˜ï¼Œç¬¬äºŒé˜¶æ®µè¿›ä¸€æ­¥ä¿®æ­£è¿åŠ¨æ¨¡ç³Šã€‚</li>
<li>æå‡ºå­å¸§æ¢¯åº¦èšåˆç­–ç•¥ä»¥è§£å†³å¤šå­å¸§æ¢¯åº¦å†²çªé—®é¢˜ã€‚</li>
<li>å¼•å…¥æ—¶ç©ºåŒé˜¶æ®µä¼˜åŒ–ç­–ç•¥ä»¥åŠ¨æ€è°ƒæ•´åŸºæœ¬ç²’å­å¯†åº¦é˜ˆå€¼ï¼Œé˜²æ­¢ç”Ÿæˆè¿‡æ—©çš„å™ªå£°é«˜æ–¯ã€‚</li>
<li>ç»¼åˆå®éªŒéªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c592e37d90ebc09a0bf03a2b3d89fb84" align="middle">
<img src="https://picx.zhimg.com/v2-bfaa43df0ea1d592c7405b461280e21f" align="middle">
<img src="https://picx.zhimg.com/v2-3c1a34b4177120989896bc4a02365e9e" align="middle">
<img src="https://picx.zhimg.com/v2-90aa79d3a5256eb400a99e9940b5469d" align="middle">
<img src="https://picx.zhimg.com/v2-d954819182be554e2ce082c078efcfa3" align="middle">
<img src="https://picx.zhimg.com/v2-ff3b90d6cc0faa0ceff9aaf0f1a986cf" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Hybrid-Gaussian-Splatting-for-Novel-Urban-View-Synthesis"><a href="#Hybrid-Gaussian-Splatting-for-Novel-Urban-View-Synthesis" class="headerlink" title="Hybrid Gaussian Splatting for Novel Urban View Synthesis"></a>Hybrid Gaussian Splatting for Novel Urban View Synthesis</h2><p><strong>Authors:Mohamed Omran, Farhad Zanjani, Davide Abati, Jens Petersen, Amirhossein Habibian</strong></p>
<p>This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall. </p>
<blockquote>
<p>æœ¬æ–‡æè¿°äº†Qualcomm AI Researchå›¢é˜Ÿåœ¨ICCV 2025ä¸¾åŠçš„RealADSim Workshopä¸Šé’ˆå¯¹RealADSim-NVSæŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æŒ‘æˆ˜æ¶‰åŠè¡—é“åœºæ™¯çš„æ–°è§†è§’åˆæˆï¼Œå‚èµ›è€…éœ€æ ¹æ®æŸäº›è®­ç»ƒè¿‡ç¨‹ä¸­çš„è½¦è½½æ‘„åƒå¤´æ‹æ‘„çš„ç”»é¢ï¼Œç”Ÿæˆä»ä¸åŒè·¯å¾„ï¼ˆå¦‚ä¸åŒçš„è¡—é“è½¦é“æˆ–è½¦è¾†æ–¹å‘ï¼‰è§‚å¯Ÿåˆ°çš„åŒä¸€åŸå¸‚ç¯å¢ƒçš„æ¸²æŸ“å›¾åƒã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå—åˆ°åœºæ™¯ç”Ÿæˆæ··åˆæ–¹æ³•å’Œç”Ÿæˆæ¨¡æ‹Ÿå™¨èåˆé«˜æ–¯æ¶‚æŠ¹å’Œæ‰©æ•£æ¨¡å‹çš„å¯å‘ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¯¹åœºæ™¯è¿›è¡Œ3Dé‡å»ºï¼Œå¹¶ä»ç›®æ ‡æ‘„åƒå¤´è§’åº¦å‘ˆç°æ–°é¢–è§†è§’çš„æ¸²æŸ“å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“ç”¨çš„å•æ­¥æ‰©æ•£æ¨¡å‹å¢å¼ºæ‰€å¾—ç”»é¢ã€‚æˆ‘ä»¬è®¨è®ºäº†é«˜æ–¯åŸå§‹æ•°æ®çš„åˆå§‹åŒ–é€‰æ‹©ä»¥åŠå¯¹å¢å¼ºæ¨¡å‹çš„å¾®è°ƒåŠå…¶è®­ç»ƒæ•°æ®é›†çš„æ„å»ºã€‚æˆ‘ä»¬æŠ¥å‘Šäº†æ¨¡å‹è®¾è®¡çš„æ€§èƒ½ï¼Œå¹¶æ ¹æ®å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œçº¿æ€§æ„ŸçŸ¥å›¾åƒä¿çœŸåº¦ï¼ˆLPIPSï¼‰ç­‰æŒ‡æ ‡è¯„ä¼°å…¶ç»„ä»¶å¯¹æ–°é¢–è§†è§’è´¨é‡çš„å½±å“ã€‚åœ¨å…¬å¼€æ’è¡Œæ¦œæŠ¥å‘Šæµ‹è¯•ç»“æœä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆè·å¾—äº†ç»¼åˆå¾—åˆ†0.432ï¼Œæ€»ä½“æ’åç¬¬äºŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12308v1">PDF</a> ICCV 2025 RealADSim Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Qualcomm AI Researchå›¢é˜Ÿåœ¨ICCV 2025ä¸¾åŠçš„RealADSim-NVSæŒ‘æˆ˜èµ›ä¸Šçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æŒ‘æˆ˜è¦æ±‚å‚èµ›è€…é’ˆå¯¹è¡—é“åœºæ™¯çš„æ–°è§†è§’åˆæˆä»»åŠ¡ï¼Œåˆ©ç”¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»è½¦è½½è§†è§’æ•æ‰çš„å¸§ï¼Œç”ŸæˆåŒä¸€åŸå¸‚ç¯å¢ƒä»ä¸åŒè§†è§’ï¼ˆå¦‚ä¸åŒè¡—é“æˆ–ä¸åŒè¡Œè½¦æ–¹å‘ï¼‰çš„æ¸²æŸ“å›¾åƒã€‚è¯¥æ–¹æ¡ˆç»“åˆäº†åœºæ™¯ç”Ÿæˆå’Œç”Ÿæˆå¼æ¨¡æ‹Ÿå™¨çš„æ··åˆæ–¹æ³•ï¼ŒåŒ…æ‹¬é«˜æ–¯å–·å°„å’Œæ‰©æ•£æ¨¡å‹ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¿›è¡Œåœºæ™¯çš„ä¸‰ç»´é‡å»ºå¹¶ä»ç›®æ ‡ç›¸æœºè§’åº¦æ¸²æŸ“æ–°è§†è§’ï¼›æ¥ç€ä½¿ç”¨ä¸“ç”¨çš„å•æ­¥æ‰©æ•£æ¨¡å‹å¢å¼ºç”Ÿæˆçš„å¸§ã€‚æ–‡ç« è®¨è®ºäº†é«˜æ–¯åŸå§‹æ•°æ®çš„åˆå§‹è®¾å®šä»¥åŠå¢å¼ºæ¨¡å‹çš„å¾®è°ƒä¸è®­ç»ƒæ•°æ®é›†çš„æ„å»ºã€‚æœ€åï¼Œæ ¹æ®PSNRã€SSIMå’ŒLPIPSç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨å…¬å¼€æ’è¡Œæ¦œä¸Šè·å¾—æµ‹è¯•ç»“æœï¼Œè¯¥æ–¹æ¡ˆæ€»ä½“æ’åç¬¬äºŒï¼Œç»¼åˆå¾—åˆ†ä¸º0.432ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Qualcomm AI Researchå›¢é˜Ÿå‚åŠ äº†RealADSim-NVSæŒ‘æˆ˜èµ›ï¼ŒæŒ‘æˆ˜å†…å®¹ä¸ºè¡—é“åœºæ™¯çš„æ–°è§†è§’åˆæˆã€‚</li>
<li>è§£å†³æ–¹æ¡ˆç»“åˆäº†åœºæ™¯ç”Ÿæˆå’Œç”Ÿæˆå¼æ¨¡æ‹Ÿå™¨çš„æ··åˆæ–¹æ³•ï¼ŒåŒ…æ‹¬é«˜æ–¯å–·å°„å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è§£å†³æ–¹æ¡ˆåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåœºæ™¯ä¸‰ç»´é‡å»ºå’Œæ–°è§†è§’æ¸²æŸ“ï¼Œä»¥åŠä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„å¸§å¢å¼ºã€‚</li>
<li>å›¢é˜Ÿè®¨è®ºäº†é«˜æ–¯åŸå§‹æ•°æ®çš„åˆå§‹è®¾å®šä»¥åŠå¢å¼ºæ¨¡å‹çš„å¾®è°ƒä¸è®­ç»ƒæ•°æ®é›†çš„æ„å»ºã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨ICCV 2025å…¬å¼€æ’è¡Œæ¦œçš„æµ‹è¯•ä¸­å–å¾—äº†ç¬¬äºŒåçš„æˆç»©ï¼Œç»¼åˆå¾—åˆ†ä¸º0.432ã€‚</li>
<li>å›¢é˜Ÿé€šè¿‡PSNRã€SSIMå’ŒLPIPSç­‰æŒ‡æ ‡è¯„ä¼°äº†å…¶æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67cc311ade2dcf4b2b9504540028bdc2" align="middle">
<img src="https://picx.zhimg.com/v2-71696bc10de8853520c2522ea98c25c1" align="middle">
<img src="https://picx.zhimg.com/v2-f827f07552a4cb3d82b64ecbe3333888" align="middle">
<img src="https://picx.zhimg.com/v2-936e7fa52dd29b2d1d2028ca204d5610" align="middle">
<img src="https://picx.zhimg.com/v2-586246358de4f2a61d129377b402548b" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UniGS-Unified-Geometry-Aware-Gaussian-Splatting-for-Multimodal-Rendering"><a href="#UniGS-Unified-Geometry-Aware-Gaussian-Splatting-for-Multimodal-Rendering" class="headerlink" title="UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal   Rendering"></a>UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal   Rendering</h2><p><strong>Authors:Yusen Xie, Zhenmin Huang, Jianhao Jiao, Dimitrios Kanoulas, Jun Ma</strong></p>
<p>In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†UniGSï¼Œè¿™æ˜¯ä¸€ç§åŸºäº3Dé«˜æ–¯æ‹¼æ¥çš„é«˜ä¿çœŸå¤šæ¨¡æ€3Dé‡å»ºçš„ç»Ÿä¸€åœ°å›¾è¡¨ç¤ºå’Œå¯åŒºåˆ†æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸€ä¸ªCUDAåŠ é€Ÿçš„å…‰æ …åŒ–ç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤ŸåŒæ—¶å‘ˆç°é€¼çœŸçš„RGBå›¾åƒã€å‡ ä½•å‡†ç¡®çš„æ·±åº¦å›¾ã€ä¸€è‡´çš„é¢æ³•å‘é‡å’Œè¯­ä¹‰å¯¹æ•°å‡ ç‡ã€‚æˆ‘ä»¬é‡æ–°è®¾è®¡äº†å…‰æ …åŒ–ï¼Œé€šè¿‡å¯åŒºåˆ†çš„å°„çº¿ä¸æ¤­åœ†ä½“ç›¸äº¤æ¥å‘ˆç°æ·±åº¦ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é«˜æ–¯ä¸­å¿ƒï¼Œé€šè¿‡è§£ææ·±åº¦æ¢¯åº¦æœ‰æ•ˆåœ°ä¼˜åŒ–æ—‹è½¬å’Œç¼©æ”¾å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºé¢æ³•çº¿æ¸²æŸ“æ¨å¯¼äº†è§£ææ¢¯åº¦å…¬å¼ï¼Œç¡®ä¿é‡å»ºçš„3Dåœºæ™¯ä¹‹é—´çš„å‡ ä½•ä¸€è‡´æ€§ã€‚ä¸ºäº†æé«˜è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„å±æ€§ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡å¯åŒºåˆ†çš„ä¿®å‰ªå¯¹è´¡çŒ®æœ€å°çš„é«˜æ–¯è¿›è¡Œä¿®å‰ªã€‚å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰æ¨¡æ€ä¸‹é‡å»ºç²¾åº¦å‡å¤„äºæœ€æ–°æ°´å¹³ï¼ŒéªŒè¯äº†æˆ‘ä»¬å‡ ä½•æ„ŸçŸ¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å’Œå¤šæ¨¡æ€æŸ¥çœ‹å™¨å°†åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12174v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäº3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„ç»Ÿä¸€åœ°å›¾è¡¨ç¤ºå’Œå¯åˆ†åŒ–æ¡†æ¶UniGSï¼Œç”¨äºé«˜ä¿çœŸåº¦å¤šæ¨¡æ€ä¸‰ç»´é‡å»ºã€‚è¯¥æ¡†æ¶é›†æˆCUDAåŠ é€Ÿçš„å…‰æ …åŒ–ç®¡é“ï¼Œå¯åŒæ—¶æ¸²æŸ“é€¼çœŸçš„RGBå›¾åƒã€å‡ ä½•å‡†ç¡®çš„æ·±åº¦å›¾ã€ä¸€è‡´çš„é¢æœæ–¹å‘å’Œè¯­ä¹‰å¯¹æ•°ã€‚é€šè¿‡å¯å¾®åˆ†çš„å°„çº¿ä¸æ¤­çƒäº¤ç‚¹è®¾è®¡å…‰æ …åŒ–æ·±åº¦æ¸²æŸ“è€Œéä½¿ç”¨é«˜æ–¯ä¸­å¿ƒï¼Œå®ç°äº†æ—‹è½¬å’Œç¼©æ”¾å±æ€§çš„æœ‰æ•ˆä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæ¨å¯¼äº†è¡¨é¢æ­£å¸¸æ¸²æŸ“çš„åˆ†ææ¢¯åº¦å…¬å¼ï¼Œç¡®ä¿é‡å»ºçš„ä¸‰ç»´åœºæ™¯å‡ ä½•ä¸€è‡´æ€§ã€‚ä¸ºæé«˜è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ï¼Œå¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ çš„å±æ€§ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯å®ç°é«˜æ–¯çš„æœ€å°è´¡çŒ®çš„å¯å¾®åˆ†è£å‰ªã€‚å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰æ¨¡æ€ä¸‹çš„é‡å»ºç²¾åº¦å‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„å‡ ä½•æ„ŸçŸ¥èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åä¸ºUniGSçš„ç»Ÿä¸€åœ°å›¾è¡¨ç¤ºå’Œå¯åˆ†åŒ–æ¡†æ¶ï¼Œç”¨äºé«˜ä¿çœŸåº¦å¤šæ¨¡æ€ä¸‰ç»´é‡å»ºã€‚</li>
<li>é›†æˆCUDAåŠ é€Ÿçš„å…‰æ …åŒ–ç®¡é“ï¼Œå®ç°å¤šç§æ¨¡æ€çš„åŒæ—¶æ¸²æŸ“ï¼ŒåŒ…æ‹¬RGBå›¾åƒã€æ·±åº¦å›¾ã€é¢æœæ–¹å‘å’Œè¯­ä¹‰å¯¹æ•°ã€‚</li>
<li>é€šè¿‡å¯å¾®åˆ†çš„è®¾è®¡å®ç°æ·±åº¦æ¸²æŸ“ï¼Œä¼˜åŒ–æ—‹è½¬å’Œç¼©æ”¾å±æ€§ã€‚</li>
<li>æ¨å¯¼äº†è¡¨é¢æ­£å¸¸æ¸²æŸ“çš„åˆ†ææ¢¯åº¦å…¬å¼ï¼Œå¢å¼ºä¸‰ç»´é‡å»ºçš„å‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥å¯å­¦ä¹ çš„å±æ€§ï¼Œæé«˜è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ï¼ŒåŒæ—¶å®ç°é«˜æ–¯çš„æœ€å°è´¡çŒ®çš„å¯å¾®åˆ†è£å‰ªã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ¡†æ¶åœ¨æ‰€æœ‰æ¨¡æ€ä¸‹çš„é‡å»ºç²¾åº¦è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b6445d0af807d780c21c09efc7fa402" align="middle">
<img src="https://picx.zhimg.com/v2-c3cb60da545bf02152ccc5a6afcd840f" align="middle">
<img src="https://picx.zhimg.com/v2-0c53a25eae2d78a4e9b0fde3b8b8ec28" align="middle">
<img src="https://picx.zhimg.com/v2-17f46d0de888f3cc8f18acff6db72533" align="middle">
<img src="https://picx.zhimg.com/v2-e926846cfd45ad51d5a45109cad4ffc1" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="G4Splat-Geometry-Guided-Gaussian-Splatting-with-Generative-Prior"><a href="#G4Splat-Geometry-Guided-Gaussian-Splatting-with-Generative-Prior" class="headerlink" title="G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior"></a>G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior</h2><p><strong>Authors:Junfeng Ni, Yixin Chen, Zhifei Yang, Yu Liu, Ruijie Lu, Song-Chun Zhu, Siyuan Huang</strong></p>
<p>Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape-appearance ambiguities and degraded scene geometry. In this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. We first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. Furthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion. Extensive experiments on Replica, ScanNet++, and DeepBlending show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions. Moreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. The project page is available at <a target="_blank" rel="noopener" href="https://dali-jack.github.io/g4splat-web/">https://dali-jack.github.io/g4splat-web/</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒè¿›è¡Œ3Dåœºæ™¯é‡å»ºæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ã€‚é¦–å…ˆï¼Œç”±äºç¼ºä¹å¯é çš„å‡ ä½•ç›‘ç£ï¼Œå³ä½¿åœ¨è§‚å¯ŸåŒºåŸŸï¼Œå®ƒä»¬ä¹Ÿéš¾ä»¥äº§ç”Ÿé«˜è´¨é‡çš„é‡å»ºï¼Œæ›´ä¸ç”¨è¯´åœ¨æœªè§‚å¯Ÿåˆ°çš„åŒºåŸŸã€‚å…¶æ¬¡ï¼Œä»–ä»¬ç¼ºä¹æœ‰æ•ˆçš„æœºåˆ¶æ¥ç¼“è§£ç”Ÿæˆå›¾åƒä¸­çš„å¤šè§†å›¾ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´å½¢çŠ¶å¤–è§‚æ¨¡ç³Šå’Œåœºæ™¯å‡ ä½•é€€åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºç²¾ç¡®å‡ ä½•æ˜¯åˆ©ç”¨ç”Ÿæˆæ¨¡å‹å¢å¼º3Dåœºæ™¯é‡å»ºçš„åŸºæœ¬å‰æã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºåˆ©ç”¨å¹³é¢ç»“æ„çš„æ™®éæ€§æ¥æ¨å¯¼å‡†ç¡®çš„åº¦é‡å°ºåº¦æ·±åº¦å›¾ï¼Œä¸ºè§‚æµ‹åŒºåŸŸå’Œæœªè§‚æµ‹åŒºåŸŸæä¾›å¯é çš„ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è¿™ç§å‡ ä½•æŒ‡å¯¼èå…¥ç”Ÿæˆç®¡é“ä¸­ï¼Œä»¥æé«˜å¯è§æ€§æ©æ¨¡ä¼°è®¡ã€å¼•å¯¼æ–°è§†å›¾é€‰æ‹©ï¼Œå¹¶åœ¨ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œç»˜ç”»æ—¶å¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°å‡†ç¡®ä¸”ä¸€è‡´çš„åœºæ™¯å®Œæˆã€‚åœ¨Replicaã€ScanNet++å’ŒDeepBlendingä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä½•å’Œå¤–è§‚é‡å»ºæ–¹é¢å§‹ç»ˆè¶…è¶Šç°æœ‰åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§‚å¯ŸåŒºåŸŸã€‚è€Œä¸”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤©ç„¶æ”¯æŒå•è§†å›¾è¾“å…¥å’Œæ— å§¿åŠ¿è§†é¢‘ï¼Œåœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸­å…·æœ‰å¼ºå¤§çš„é€šç”¨æ€§å’Œå®é™…åº”ç”¨æ€§ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://dali-jack.github.io/g4splat-web/%E6%89%BE%E5%88%B0%E3%80%82">https://dali-jack.github.io/g4splat-web/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12099v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://dali-jack.github.io/g4splat-web/">https://dali-jack.github.io/g4splat-web/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºäº†ç°æœ‰åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„3Dåœºæ™¯é‡å»ºæ–¹æ³•å­˜åœ¨å‡ ä½•ç›‘ç£ä¸è¶³å’Œå¤šè§†è§’ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨å¹³é¢ç»“æ„æ¥æ¨å¯¼å‡†ç¡®çš„åº¦é‡å°ºåº¦æ·±åº¦å›¾ï¼Œä¸ºè§‚æµ‹å’Œæœªè§‚æµ‹åŒºåŸŸæä¾›å¯é çš„ç›‘ç£ã€‚åŒæ—¶ï¼Œå°†æ­¤å‡ ä½•æŒ‡å¯¼èå…¥ç”Ÿæˆç®¡é“ï¼Œæ”¹è¿›å¯è§æ€§æ©æ¨¡ä¼°è®¡ã€å¼•å¯¼æ–°è§†è§’é€‰æ‹©ï¼Œå¹¶åœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å¢å¼ºå¤šè§†è§’ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°å‡†ç¡®ä¸”ä¸€è‡´çš„åœºæ™¯å®Œæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä½•å’Œå¤–è§‚é‡å»ºæ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå°¤å…¶é€‚ç”¨äºæœªè§‚æµ‹åŒºåŸŸçš„é‡å»ºã€‚è¯¥é¡¹ç›®æ”¯æŒå•è§†å›¾è¾“å…¥å’Œæ— å§¿æ€è§†é¢‘ï¼Œå…·æœ‰è¾ƒå¼ºçš„å®¤å†…å¤–å®é™…åº”ç”¨æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰3Dåœºæ™¯é‡å»ºæ–¹æ³•å­˜åœ¨å‡ ä½•ç›‘ç£ä¸è¶³å’Œå¤šè§†è§’ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºåˆ©ç”¨å¹³é¢ç»“æ„æ¨å¯¼å‡†ç¡®åº¦é‡å°ºåº¦æ·±åº¦å›¾ï¼Œä¸ºè§‚æµ‹å’Œæœªè§‚æµ‹åŒºåŸŸæä¾›å¯é ç›‘ç£ã€‚</li>
<li>å°†å‡ ä½•æŒ‡å¯¼èå…¥ç”Ÿæˆç®¡é“ï¼Œæ”¹è¿›å¯è§æ€§æ©æ¨¡ä¼°è®¡ã€æ–°è§†è§’é€‰æ‹©åŠè§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>æ–¹æ³•å®ç°å‡†ç¡®ä¸”ä¸€è‡´çš„åœºæ™¯å®Œæˆï¼Œåœ¨å‡ ä½•å’Œå¤–è§‚é‡å»ºæ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>æ”¯æŒå•è§†å›¾è¾“å…¥å’Œæ— å§¿æ€è§†é¢‘ï¼Œå…·æœ‰å®¤å†…å¤–å®é™…åº”ç”¨æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒï¼Œåœ¨Replicaã€ScanNet++å’ŒDeepBlendingæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e38d3a8dd109b40212b8072ebe40f795" align="middle">
<img src="https://picx.zhimg.com/v2-28a673640120782b5e9303c3c14a2676" align="middle">
<img src="https://picx.zhimg.com/v2-07245680344d157f73839524c5239b36" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GS-Verse-Mesh-based-Gaussian-Splatting-for-Physics-aware-Interaction-in-Virtual-Reality"><a href="#GS-Verse-Mesh-based-Gaussian-Splatting-for-Physics-aware-Interaction-in-Virtual-Reality" class="headerlink" title="GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in   Virtual Reality"></a>GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in   Virtual Reality</h2><p><strong>Authors:Anastasiya Pechko, Piotr Borycki, Joanna WaczyÅ„ska, Daniel Barczyk, Agata SzymaÅ„ska, SÅ‚awomir Tadeja, PrzemysÅ‚aw Spurek</strong></p>
<p>As the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce \our{} (\textbf{G}aussian \textbf{S}platting for \textbf{V}irtual \textbf{E}nvironment \textbf{R}endering and \textbf{S}cene \textbf{E}diting), a novel method designed to overcome these challenges by directly integrating an objectâ€™s mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, \our{} facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods. </p>
<blockquote>
<p>éšç€å¯¹æ²‰æµ¸å¼3Då†…å®¹çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œå¯¹ç›´è§‚é«˜æ•ˆäº¤äº’æ–¹æ³•çš„éœ€æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚å½“å‰åœ¨è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ä¸­ç‰©ç†æ“ä½œ3Då†…å®¹çš„æŠ€å·§å¸¸å¸¸é¢ä¸´é‡å¤§å±€é™ï¼ŒåŒ…æ‹¬ä¾èµ–å·¥ç¨‹å¯†é›†å‹è¿‡ç¨‹å’Œç®€åŒ–å‡ ä½•è¡¨ç¤ºï¼ˆä¾‹å¦‚å››é¢ä½“ç¬¼å­ï¼‰ï¼Œè¿™å¯èƒ½ä¼šæŸå®³è§†è§‰ä¿çœŸåº¦å’Œç‰©ç†å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆé«˜æ–¯æ‹¼è´´æ¸²æŸ“æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡ç›´æ¥å°†å¯¹è±¡çš„ç½‘æ ¼ä¸é«˜æ–¯æ‹¼è´´ï¼ˆGSï¼‰è¡¨ç¤ºé›†æˆæ¥å…‹æœè¿™äº›æŒ‘æˆ˜çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿è¡¨é¢è¿‘ä¼¼æ›´åŠ ç²¾ç¡®ï¼Œä»è€Œå®ç°é«˜åº¦é€¼çœŸçš„å˜å½¢å’Œäº¤äº’ã€‚é€šè¿‡åˆ©ç”¨ç°æœ‰çš„3Dç½‘æ ¼èµ„äº§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†å†…å®¹å¤ç”¨å’Œå¼€å‘å·¥ä½œæµç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿè®¾è®¡ä¸ºç‰©ç†å¼•æ“æ— å…³æ€§ï¼Œä¸ºå¼€å‘è€…æä¾›äº†å¼ºå¤§çš„éƒ¨ç½²çµæ´»æ€§ã€‚è¿™ç§é€šç”¨æ¶æ„æä¾›äº†ä¸€ç§é«˜åº¦é€¼çœŸã€çµæ´»ç›´è§‚çš„äº¤äº’å¼3Dæ“ä½œæ–¹å¼ã€‚æˆ‘ä»¬é€šè¿‡ä¸€é¡¹æ¶‰åŠ18åå‚ä¸è€…çš„æ¯”è¾ƒæ€§ç”¨æˆ·ç ”ç©¶ï¼Œå¯¹æˆ‘ä»¬çš„æ–¹æ³•ä¸å½“å‰å…ˆè¿›çš„è™šæ‹Ÿç°å®ä¸GSç›¸ç»“åˆçš„æŠ€æœ¯è¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰©ç†æ„ŸçŸ¥æ‹‰ä¼¸æ“ä½œä¸­è¡¨ç°ä¼˜å¼‚å¹¶å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œå¹¶ä¸”åœ¨æ‰­æ›²å’Œæ‘‡æ™ƒç­‰å…¶ä»–åŸºäºç‰©ç†çš„æ“ä½œä¸­ä¹Ÿæ›´åŠ ä¸€è‡´ã€‚å¯¹å„ç§äº¤äº’å’Œåœºæ™¯çš„è¿›ä¸€æ­¥è¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆè¡¨ç°å‡ºé«˜è€Œå¯é çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºä½œä¸ºç°æœ‰æ–¹æ³•çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11878v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºourï¼ˆé«˜æ–¯å¹³é“ºç”¨äºè™šæ‹Ÿç°å®ç¯å¢ƒæ¸²æŸ“å’Œåœºæ™¯ç¼–è¾‘ï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å½“å‰è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ä¸­ç‰©ç†äº¤äº’æŠ€æœ¯çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥æ•´åˆå¯¹è±¡ç½‘æ ¼ä¸é«˜æ–¯å¹³é“ºï¼ˆGSï¼‰è¡¨ç¤ºï¼Œæé«˜äº†è¡¨é¢è¿‘ä¼¼ç²¾åº¦ï¼Œå®ç°äº†é«˜åº¦é€¼çœŸçš„å˜å½¢å’Œäº¤äº’ã€‚Ouræ–¹æ³•è¿˜æ”¯æŒæ— ç¼å†…å®¹å¤ç”¨å’Œç®€åŒ–å¼€å‘æµç¨‹ï¼Œåˆ©ç”¨ç°æœ‰3Dç½‘æ ¼èµ„äº§è¿›è¡Œæ¸²æŸ“åœºæ™¯ç¼–è¾‘ï¼ŒåŒæ—¶å…·æœ‰ç‰©ç†å¼•æ“æ— å…³æ€§ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•å…·æœ‰ç°å®æ€§å¼ºã€é€‚åº”æ€§å¼ºå’Œç›´è§‚äº¤äº’ç­‰ç‰¹ç‚¹ï¼Œå¹¶åœ¨ç”¨æˆ·ç ”ç©¶ä¸­éªŒè¯äº†å…¶åœ¨ç‰©ç†æ„ŸçŸ¥æ‹‰ä¼¸æ“ä½œä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰VRä¸­ç‰©ç†äº¤äº’æ–¹æ³•çš„å±€é™æ€§åŒ…æ‹¬å·¥ç¨‹å¯†é›†è¿‡ç¨‹å’Œç®€åŒ–å‡ ä½•è¡¨ç¤ºï¼Œå¯èƒ½å½±å“è§†è§‰çœŸå®æ€§å’Œç‰©ç†å‡†ç¡®æ€§ã€‚</li>
<li>Ouræ–¹æ³•æ•´åˆå¯¹è±¡ç½‘æ ¼ä¸é«˜æ–¯å¹³é“ºè¡¨ç¤ºï¼Œæé«˜è¡¨é¢è¿‘ä¼¼ç²¾åº¦å¹¶å®ç°é«˜åº¦é€¼çœŸçš„å˜å½¢å’Œäº¤äº’ã€‚</li>
<li>Ouræ–¹æ³•æ”¯æŒæ— ç¼å†…å®¹å¤ç”¨å’Œç®€åŒ–å¼€å‘æµç¨‹ï¼Œåˆ©ç”¨ç°æœ‰3Dç½‘æ ¼èµ„äº§è¿›è¡Œæ¸²æŸ“åœºæ™¯ç¼–è¾‘ã€‚</li>
<li>Ourç³»ç»Ÿå…·æœ‰ç‰©ç†å¼•æ“æ— å…³æ€§ä¼˜åŠ¿ï¼Œä¸ºå¼€å‘è€…æä¾›çµæ´»çš„éƒ¨ç½²é€‰æ‹©ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯ï¼Œåœ¨ç‰©ç†æ„ŸçŸ¥æ‹‰ä¼¸æ“ä½œä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>Ouræ–¹æ³•åœ¨å¤šç§äº¤äº’å’Œåœºæ™¯ä¸‹çš„è¯„ä»·è¡¨ç°å‡ºé«˜å¯é æ€§å’Œæ€§èƒ½ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-190bcb1f6fd6bca9ab99371e9631c602" align="middle">
<img src="https://picx.zhimg.com/v2-312d97ea9f9a5908e6d197e9b013dffb" align="middle">
<img src="https://picx.zhimg.com/v2-f7706e0f34fcbe982937b8e330b5a9ed" align="middle">
<img src="https://picx.zhimg.com/v2-daa4639dcfaaf399a0781da2598c5ff1" align="middle">
<img src="https://picx.zhimg.com/v2-eed3cc71e67fcc69ad316f87c78d1b74" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Ev4DGS-Novel-view-Rendering-of-Non-Rigid-Objects-from-Monocular-Event-Streams"><a href="#Ev4DGS-Novel-view-Rendering-of-Non-Rigid-Objects-from-Monocular-Event-Streams" class="headerlink" title="Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event   Streams"></a>Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event   Streams</h2><p><strong>Authors:Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik</strong></p>
<p>Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. This paper sheds light on this challenging open question and introduces Ev4DGS, i.e., the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or greyscale images) from monocular event streams. Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. We will release our models and the datasets used in the evaluation for research purposes; see the project webpage: <a target="_blank" rel="noopener" href="https://4dqv.mpi-inf.mpg.de/Ev4DGS/">https://4dqv.mpi-inf.mpg.de/Ev4DGS/</a>. </p>
<blockquote>
<p>ç›¸æ¯”äºåŒæ­¥è¿è¡Œçš„RGBç›¸æœºï¼Œäº‹ä»¶ç›¸æœºå¯¹äºæ–°å‹è§†å›¾æ¸²æŸ“å…·æœ‰å¤šç§ä¼˜åŠ¿ã€‚æ”¯æŒåˆšç¡¬åœºæ™¯çš„åŸºäºäº‹ä»¶çš„é«˜æ•ˆæŠ€æœ¯å·²ç»åœ¨æ–‡çŒ®ä¸­å¾—åˆ°äº†å±•ç¤ºã€‚ç„¶è€Œï¼Œåœ¨éåˆšä½“å¯¹è±¡çš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•è¿˜éœ€è¦ç¨€ç–RGBè¾“å…¥ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½æ˜¯ä¸€ä¸ªé‡è¦çš„é™åˆ¶ï¼›ç›®å‰å°šä¸æ¸…æ¥šæ˜¯å¦ä»…èƒ½ä»äº‹ä»¶æµä¸­å­¦ä¹ ç±»ä¼¼çš„æ¨¡å‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾é—®é¢˜ï¼Œå¹¶ä»‹ç»äº†Ev4DGSï¼Œå³é¦–ä¸ªä»å•çœ¼äº‹ä»¶æµåœ¨éåˆšä½“å˜å½¢å¯¹è±¡çš„æ˜¾å¼è§‚æµ‹ç©ºé—´ï¼ˆå³RGBæˆ–ç°åº¦å›¾åƒï¼‰ä¸­è¿›è¡Œæ–°å‹è§†å›¾æ¸²æŸ“çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢å›å½’å¯å˜å½¢çš„3Dé«˜æ–¯å–·å°„è¡¨ç¤ºï¼š1ï¼‰ä¸ä¼°è®¡æ¨¡å‹çš„è¾“å‡ºä¸äºŒç»´äº‹ä»¶è§‚æµ‹ç©ºé—´ç›¸å…³çš„æŸå¤±ï¼›2ï¼‰ä»äº‹ä»¶ç”Ÿæˆçš„äºŒè¿›åˆ¶è’™ç‰ˆè®­ç»ƒçš„ç²—ç•¥ä¸‰ç»´å˜å½¢æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„åˆæˆæ•°æ®ä»¥åŠæ–°è®°å½•çš„çœŸå®æ•°æ®é›†ä¸Šçš„éåˆšä½“å¯¹è±¡è¿›è¡Œäº†å®éªŒæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜Ev4DGSçš„æœ‰æ•ˆæ€§ä»¥åŠå…¶ç›¸è¾ƒäºå¤šä¸ªå¯åœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­åº”ç”¨çš„ç®€å•åŸºå‡†çº¿çš„å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¸ºäº†ç ”ç©¶ç›®çš„å‘å¸ƒæˆ‘ä»¬åœ¨è¯„ä¼°ä¸­ä½¿ç”¨çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼›è¯¦è§é¡¹ç›®ç½‘é¡µï¼š<a target="_blank" rel="noopener" href="https://4dqv.mpi-inf.mpg.de/Ev4DGS/%E3%80%82">https://4dqv.mpi-inf.mpg.de/Ev4DGS/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11717v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äº‹ä»¶ç›¸æœºå¯¹äºæ–°å‹è§†å›¾æ¸²æŸ“å…·æœ‰å¤šç§ä¼˜åŠ¿ï¼Œç›¸æ¯”åŒæ­¥æ“ä½œçš„RGBç›¸æœºï¼ŒåŸºäºäº‹ä»¶çš„æ–¹æ³•åœ¨éåˆšæ€§åœºæ™¯å¤„ç†ä¸Šå±•ç¤ºäº†æ½œåŠ›ã€‚æœ¬æ–‡è§£å†³éåˆšæ€§ç‰©ä½“è§†å›¾æ¸²æŸ“çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæå‡ºäº†Ev4DGSæ–¹æ³•ï¼Œå³ä»å•ç›®äº‹ä»¶æµä¸­æ¸²æŸ“éåˆšæ€§å˜å½¢ç‰©ä½“çš„æ˜ç¡®è§‚å¯Ÿç©ºé—´ï¼ˆå¦‚RGBæˆ–ç°åº¦å›¾åƒï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡1ï¼‰ä¸ä¼°è®¡æ¨¡å‹è¾“å‡ºå’ŒäºŒç»´äº‹ä»¶è§‚å¯Ÿç©ºé—´ç›¸å…³çš„æŸå¤±ï¼Œä»¥åŠ2ï¼‰ä»äº‹ä»¶ç”Ÿæˆçš„äºŒè¿›åˆ¶è’™ç‰ˆè®­ç»ƒçš„ç²—ç•¥ä¸‰ç»´å˜å½¢æ¨¡å‹ï¼Œå›å½’å¯å˜å½¢ä¸‰ç»´é«˜æ–¯æ‹¼è´´è¡¨ç¤ºã€‚åœ¨ç°æœ‰åˆæˆæ•°æ®å’Œæ–°å½•åˆ¶çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒæ¯”è¾ƒè¡¨æ˜Ev4DGSçš„æœ‰æ•ˆæ€§åŠå…¶ç›¸è¾ƒäºå¤šä¸ªåŸºçº¿æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äº‹ä»¶ç›¸æœºåœ¨æ–°å‹è§†å›¾æ¸²æŸ“æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œç›¸è¾ƒäºRGBç›¸æœºï¼Œå…¶åœ¨å¤„ç†éåˆšæ€§åœºæ™¯æ—¶å±•ç¤ºæ›´å¤§çš„æ½œåŠ›ã€‚</li>
<li>æœ¬æ–‡è§£å†³äº†ä»äº‹ä»¶æµä¸­æ¸²æŸ“éåˆšæ€§ç‰©ä½“çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæå‡ºäº†Ev4DGSæ–¹æ³•ã€‚</li>
<li>Ev4DGSæ–¹æ³•é€šè¿‡å›å½’å¯å˜å½¢ä¸‰ç»´é«˜æ–¯æ‹¼è´´è¡¨ç¤ºï¼Œåˆ©ç”¨ä¸ä¼°è®¡æ¨¡å‹è¾“å‡ºå’ŒäºŒç»´äº‹ä»¶è§‚å¯Ÿç©ºé—´ç›¸å…³çš„æŸå¤±ä»¥åŠç²—ç•¥ä¸‰ç»´å˜å½¢æ¨¡å‹æ¥å®ç°æ¸²æŸ“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜Ev4DGSçš„æœ‰æ•ˆæ€§ï¼Œä¸”å…¶åœ¨æ€§èƒ½ä¸Šä¼˜äºå¤šä¸ªåŸºçº¿æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee87a2b519bd507927a45bf40a3c43a8" align="middle">
<img src="https://picx.zhimg.com/v2-62856f53097c70b0f1f8f8681d254076" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Phys2Real-Fusing-VLM-Priors-with-Interactive-Online-Adaptation-for-Uncertainty-Aware-Sim-to-Real-Manipulation"><a href="#Phys2Real-Fusing-VLM-Priors-with-Interactive-Online-Adaptation-for-Uncertainty-Aware-Sim-to-Real-Manipulation" class="headerlink" title="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for   Uncertainty-Aware Sim-to-Real Manipulation"></a>Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for   Uncertainty-Aware Sim-to-Real Manipulation</h2><p><strong>Authors:Maggie Wang, Stephen Tian, Aiden Swann, Ola Shorinwa, Jiajun Wu, Mac Schwager</strong></p>
<p>Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: <a target="_blank" rel="noopener" href="https://phys2real.github.io/">https://phys2real.github.io/</a> . </p>
<blockquote>
<p>å­¦ä¹ æœºå™¨äººæ“æ§ç­–ç•¥ç›´æ¥åœ¨ç°å®ä¸–ç•Œä¸­å¯èƒ½ä¼šèŠ±è´¹é«˜æ˜‚ä¸”è€—æ—¶ã€‚è™½ç„¶ä»¿çœŸä¸­è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†æœ‰æ•ˆçš„ä»¿çœŸåˆ°ç°å®çš„è½¬ç§»ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦ç²¾ç¡®åŠ¨åŠ›å­¦çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Phys2Realï¼Œè¿™æ˜¯ä¸€ä¸ªä»ç°å®åˆ°ä»¿çœŸå†åˆ°ç°å®çš„RLç®¡é“ï¼Œå®ƒç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨æ–­çš„ç‰©ç†å‚æ•°ä¼°è®¡å’Œé€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥èåˆè¿›è¡Œçš„äº¤äº’é€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰ä½¿ç”¨3Dé«˜æ–¯é£æº…çš„é«˜ç²¾åº¦å‡ ä½•é‡å»ºï¼Œï¼ˆ2ï¼‰VLMæ¨æ–­ç‰©ç†å‚æ•°çš„å‰å‘åˆ†å¸ƒï¼Œä»¥åŠï¼ˆ3ï¼‰åœ¨çº¿ç‰©ç†å‚æ•°ä¼°è®¡ä»äº¤äº’æ•°æ®ä¸­è·å–ã€‚Phys2Realæ ¹æ®å¯è§£é‡Šçš„ç‰©ç†å‚æ•°è°ƒæ•´ç­–ç•¥ï¼Œé€šè¿‡åŸºäºé›†åˆçš„ä¸ç¡®å®šæ€§é‡åŒ–åœ¨çº¿ä¼°è®¡ä¿®æ­£VLMé¢„æµ‹ã€‚åœ¨å¤„ç†å¹³é¢æ¨åŠ¨ä»»åŠ¡ä¸­ï¼Œå¦‚åœ¨é‡å¿ƒï¼ˆCoMï¼‰å˜åŒ–çš„Tå½¢å—å’Œç¦»ä¸­å¿ƒè´¨é‡åˆ†å¸ƒçš„é”¤å­çš„æƒ…å†µä¸‹ï¼ŒPhys2Realç›¸å¯¹äºåŸŸéšæœºåŸºçº¿å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼šåº•éƒ¨åŠ é‡çš„Tå½¢å—æˆåŠŸç‡ä¸º100%å¯¹æ¯”79%ï¼Œé¡¶éƒ¨åŠ é‡çš„Tå½¢å—ä»57%æå‡è‡³23%ï¼Œé”¤å­æ¨åŠ¨ä»»åŠ¡å¹³å‡å®Œæˆæ—¶é—´ç¼©çŸ­äº†15%ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒVLMå’Œäº¤äº’ä¿¡æ¯çš„ç»“åˆå¯¹äºæˆåŠŸè‡³å…³é‡è¦ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://phys2real.github.io/%E3%80%82">https://phys2real.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11689v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹æ¨æ–­çš„ç‰©ç†å‚æ•°ä¼°è®¡ä¸äº¤äº’é€‚åº”çš„ä»¿çœŸåˆ°çœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œç­–ç•¥å­¦ä¹ æ–¹æ³•Phys2Realã€‚é€šè¿‡é«˜ä¿çœŸå‡ ä½•é‡å»ºã€VLMæ¨æ–­ç‰©ç†å‚æ•°å…ˆéªŒåˆ†å¸ƒä»¥åŠåœ¨çº¿ç‰©ç†å‚æ•°ä¼°è®¡ç­‰æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨å¹³é¢æ¨åŠ¨ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨æ–­çš„ç‰©ç†å‚æ•°ä¼°è®¡ä¸äº¤äº’é€‚åº”ï¼Œå½¢æˆäº†ä¸€ç§ç‹¬ç‰¹çš„Real-to-Sim-to-Real RLç®¡é“ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé«˜ä¿çœŸå‡ ä½•é‡å»ºã€VLMæ¨æ–­ç‰©ç†å‚æ•°å…ˆéªŒåˆ†å¸ƒä»¥åŠåœ¨çº¿ç‰©ç†å‚æ•°ä¼°è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ¡ä»¶ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿ç”¨å¯è§£é‡Šçš„ç‰©ç†å‚æ•°ï¼Œé€šè¿‡åŸºäºé›†åˆçš„ä¸ç¡®å®šæ€§é‡åŒ–åœ¨çº¿ä¼°è®¡æ¥å®Œå–„VLMé¢„æµ‹ã€‚</li>
<li>åœ¨å¹³é¢æ¨åŠ¨ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”é¢†åŸŸéšæœºåŒ–åŸºçº¿ï¼ŒPhys2Realåœ¨å…·æœ‰ä¸åŒè´¨å¿ƒï¼ˆCoMï¼‰çš„Tå—å’Œå…·æœ‰åå¿ƒè´¨é‡åˆ†å¸ƒçš„é”¤å­ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æˆåŠŸç‡æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åº•éƒ¨åŠ é‡çš„Tå—ä»»åŠ¡ä¸Šè¾¾åˆ°äº†100%çš„æˆåŠŸç‡ï¼Œç›¸æ¯”åŸºçº¿æå‡äº†21%ï¼›åœ¨é¡¶éƒ¨åŠ é‡çš„Tå—ä»»åŠ¡ä¸Šæå‡äº†34%ï¼›åœ¨é”¤å­æ¨åŠ¨ä»»åŠ¡ä¸Šå¹³å‡ä»»åŠ¡å®Œæˆæ—¶é—´ç¼©çŸ­äº†15%ã€‚</li>
<li>æ¶ˆèç ”ç©¶æŒ‡å‡ºï¼ŒVLMå’Œäº¤äº’ä¿¡æ¯çš„ç»“åˆå¯¹äºæˆåŠŸè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e756d3fa36da7fb54b1521c00e4d4fde" align="middle">
<img src="https://picx.zhimg.com/v2-ca71034a208fdcb4e65b83ebb70bfb8c" align="middle">
<img src="https://picx.zhimg.com/v2-55edeb82af1642163747f4140acee90c" align="middle">
<img src="https://picx.zhimg.com/v2-0b39a236ac63965f902bda9ce13c48c3" align="middle">
<img src="https://picx.zhimg.com/v2-c88327ef9d70d69f18e6297fc777f703" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VA-GS-Enhancing-the-Geometric-Representation-of-Gaussian-Splatting-via-View-Alignment"><a href="#VA-GS-Enhancing-the-Geometric-Representation-of-Gaussian-Splatting-via-View-Alignment" class="headerlink" title="VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via   View Alignment"></a>VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via   View Alignment</h2><p><strong>Authors:Qing Li, Huifang Feng, Xun Gong, Yu-Shen Liu</strong></p>
<p>3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/LeoQLi/VA-GS">https://github.com/LeoQLi/VA-GS</a>. </p>
<blockquote>
<p>3Dé«˜æ–¯æ··åˆæŠ€æœ¯è¿‘æœŸè¢«è¯æ˜æ˜¯ä¸€ç§é«˜æ•ˆçš„é«˜è´¨é‡å®æ—¶åˆæˆæ–°è§†è§’çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå…¶åœ¨ç²¾ç¡®è¡¨é¢é‡å»ºæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç”±äºé«˜æ–¯æ··åˆå…·æœ‰ç¦»æ•£å’Œéç»“æ„åŒ–çš„ç‰¹æ€§ï¼Œä»…åŸºäºå›¾åƒæ¸²æŸ“æŸå¤±çš„ç›‘ç£é€šå¸¸ä¼šå¯¼è‡´å‡ ä½•ä¸å‡†ç¡®å’Œå¤šè§†è§’å¯¹é½ä¸ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è§†è§’å¯¹é½ï¼ˆVAï¼‰å¢å¼º3Dé«˜æ–¯æ··åˆå‡ ä½•è¡¨ç¤ºçš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è¾¹ç¼˜æ„ŸçŸ¥å›¾åƒçº¿ç´¢çº³å…¥æ¸²æŸ“æŸå¤±ï¼Œä»¥æé«˜è¡¨é¢è¾¹ç•Œçš„æç»˜ã€‚ä¸ºäº†å¼ºåˆ¶è·¨è§†è§’çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯è§åº¦æ„ŸçŸ¥çš„å…‰åº¦å¯¹é½æŸå¤±ï¼Œè¯¥æŸå¤±å¯¹é®æŒ¡è¿›è¡Œå»ºæ¨¡å¹¶é¼“åŠ±é«˜æ–¯ä¹‹é—´çš„å‡†ç¡®ç©ºé—´å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡è½»ç”±å…‰ç…§å˜åŒ–å¼•èµ·çš„æ­§ä¹‰ï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºæ³•çº¿çš„çº¦æŸæ¥ç»†åŒ–é«˜æ–¯çš„ç©ºé—´æ–¹å‘å¹¶æ”¹å–„å±€éƒ¨è¡¨é¢ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨æ·±åº¦å›¾åƒç‰¹å¾åµŒå…¥æ¥å¼ºåˆ¶æ‰§è¡Œè·¨è§†è§’çš„ä¸€è‡´æ€§ï¼Œåœ¨å¤šç§è§†è§’å’Œå…‰ç…§æ¡ä»¶ä¸‹å¢å¼ºäº†å­¦ä¹ å‡ ä½•çš„ç¨³å¥æ€§ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¡¨é¢é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LeoQLi/VA-GS%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LeoQLi/VA-GSå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11473v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸‰ç»´é«˜æ–¯èåˆæŠ€æœ¯ä¸­çš„å‡ ä½•é‡å»ºé—®é¢˜ã€‚ä¸ºè§£å†³ä¼ ç»Ÿç›‘ç£æ–¹æ³•ä¸­çš„å‡ ä½•ä¸ç²¾ç¡®ä¸å¤šè§†è§’å¯¹é½ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæå‡ºäº†ç»“åˆè¾¹ç¼˜æ„ŸçŸ¥å›¾åƒçº¿ç´¢æ”¹è¿›è¡¨é¢è¾¹ç•Œåˆ’åˆ†çš„æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥å¯è§åº¦æ„ŸçŸ¥çš„å…‰åº¦å¯¹é½æŸå¤±æ¨¡å‹é®æŒ¡ç°è±¡ï¼ŒåŠ å¼ºé«˜æ–¯ä¹‹é—´çš„ç©ºé—´å…³ç³»å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œåˆ©ç”¨åŸºäºæ³•çº¿çš„çº¦æŸä¼˜åŒ–é«˜æ–¯çš„ç©ºé—´æ–¹å‘ä¼°è®¡ï¼Œå¹¶æé«˜äº†å±€éƒ¨è¡¨é¢é‡å»ºè´¨é‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ·±åº¦å›¾åƒç‰¹å¾åµŒå…¥å¢å¼ºè·¨è§†è§’çš„ä¸€è‡´æ€§ï¼Œæé«˜åœ¨ä¸åŒè§†è§’å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„å‡ ä½•å­¦ä¹ ç¨³å¥æ€§ã€‚å®éªŒç»“æœåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´é«˜æ–¯èåˆæŠ€æœ¯å¯¹äºé«˜è´¨é‡å®æ—¶æ–°é¢–è§†è§’åˆæˆå…·æœ‰é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†å…¶å‡†ç¡®çš„è¡¨é¢é‡å»ºèƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†å‘æŒ¥ã€‚</li>
<li>ä¼ ç»Ÿçš„åŸºäºå›¾åƒæ¸²æŸ“æŸå¤±çš„ç›‘ç£æ–¹æ³•ä¼šå¯¼è‡´å‡ ä½•ä¸å‡†ç¡®å’Œå¤šè§†è§’å¯¹é½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>æå‡ºç»“åˆè¾¹ç¼˜æ„ŸçŸ¥å›¾åƒçº¿ç´¢ï¼Œæ”¹å–„è¡¨é¢è¾¹ç•Œçš„åˆ’åˆ†ï¼Œä»è€Œæé«˜å‡ ä½•è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥å¯è§åº¦æ„ŸçŸ¥çš„å…‰åº¦å¯¹é½æŸå¤±æ¨¡å‹ï¼Œå¼ºåŒ–é«˜æ–¯ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œå¤„ç†é®æŒ¡ç°è±¡ã€‚</li>
<li>åˆ©ç”¨åŸºäºæ³•çº¿çš„çº¦æŸä¼˜åŒ–é«˜æ–¯çš„ç©ºé—´æ–¹å‘ä¼°è®¡ï¼Œæé«˜å±€éƒ¨è¡¨é¢é‡å»ºè´¨é‡ã€‚</li>
<li>é€šè¿‡æ·±åº¦å›¾åƒç‰¹å¾åµŒå…¥å¢å¼ºè·¨è§†è§’çš„ä¸€è‡´æ€§ï¼Œæé«˜å‡ ä½•å­¦ä¹ çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b902a931aa0f414c506aee182a08954" align="middle">
<img src="https://picx.zhimg.com/v2-a01fcac5c5a0156498f424cafe25485f" align="middle">
<img src="https://picx.zhimg.com/v2-28bfcb76a4cf06bb855e89c01d0c64e3" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MaterialRefGS-Reflective-Gaussian-Splatting-with-Multi-view-Consistent-Material-Inference"><a href="#MaterialRefGS-Reflective-Gaussian-Splatting-with-Multi-view-Consistent-Material-Inference" class="headerlink" title="MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent   Material Inference"></a>MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent   Material Inference</h2><p><strong>Authors:Wenyuan Zhang, Jimin Tang, Weiqi Zhang, Yi Fang, Yu-Shen Liu, Zhizhong Han</strong></p>
<p>Modeling reflections from 2D images is essential for photorealistic rendering and novel view synthesis. Recent approaches enhance Gaussian primitives with reflection-related material attributes to enable physically based rendering (PBR) with Gaussian Splatting. However, the material inference often lacks sufficient constraints, especially under limited environment modeling, resulting in illumination aliasing and reduced generalization. In this work, we revisit the problem from a multi-view perspective and show that multi-view consistent material inference with more physically-based environment modeling is key to learning accurate reflections with Gaussian Splatting. To this end, we enforce 2D Gaussians to produce multi-view consistent material maps during deferred shading. We also track photometric variations across views to identify highly reflective regions, which serve as strong priors for reflection strength terms. To handle indirect illumination caused by inter-object occlusions, we further introduce an environment modeling strategy through ray tracing with 2DGS, enabling photorealistic rendering of indirect radiance. Experiments on widely used benchmarks show that our method faithfully recovers both illumination and geometry, achieving state-of-the-art rendering quality in novel views synthesis. </p>
<blockquote>
<p>ä»äºŒç»´å›¾åƒä¸­å¯¹åå°„è¿›è¡Œå»ºæ¨¡å¯¹äºå®ç°é€¼çœŸçš„æ¸²æŸ“å’Œæ–°å‹è§†å›¾åˆæˆè‡³å…³é‡è¦ã€‚æœ€è¿‘çš„æ–¹æ³•ä½¿ç”¨ä¸åå°„ç›¸å…³çš„ææ–™å±æ€§å¢å¼ºé«˜æ–¯åŸºæœ¬å½¢æ€ï¼Œä»¥å®ç°åŸºäºé«˜æ–¯å–·å°„çš„ç‰©ç†æ¸²æŸ“ï¼ˆPBRï¼‰ã€‚ç„¶è€Œï¼Œææ–™æ¨æ–­é€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„çº¦æŸï¼Œç‰¹åˆ«æ˜¯åœ¨ç¯å¢ƒå»ºæ¨¡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¯¼è‡´ç…§æ˜æ··æ·†å’Œæ³›åŒ–å‡å°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å¤šè§’åº¦é‡æ–°å®¡è§†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¡¨æ˜ä½¿ç”¨æ›´å¤šåŸºäºç‰©ç†çš„ç¯å¢ƒå»ºæ¨¡çš„å¤šè§†è§’ä¸€è‡´ææ–™æ¨æ–­æ˜¯ä½¿ç”¨é«˜æ–¯å–·å°„å­¦ä¹ å‡†ç¡®åå°„çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨å»¶è¿Ÿç€è‰²æœŸé—´å¼ºåˆ¶äºŒç»´é«˜æ–¯ç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„ææ–™å›¾ã€‚æˆ‘ä»¬è¿˜è·Ÿè¸ªè§†å›¾ä¹‹é—´çš„å…‰åº¦å˜åŒ–ï¼Œä»¥è¯†åˆ«é«˜åå°„åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸä½œä¸ºåå°„å¼ºåº¦é¡¹çš„å¼ºå…ˆéªŒã€‚ä¸ºäº†å¤„ç†ç”±å¯¹è±¡é—´é®æŒ¡é€ æˆçš„é—´æ¥ç…§æ˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ç»“åˆäºŒç»´é«˜æ–¯å–·å°„çš„å°„çº¿è¿½è¸ªå¼•å…¥ç¯å¢ƒå»ºæ¨¡ç­–ç•¥ï¼Œå®ç°é—´æ¥è¾å°„çš„é€¼çœŸæ¸²æŸ“ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç…§æ˜å’Œå‡ ä½•ç»“æ„æ¢å¤ä¸Šå¿ å®åº¦é«˜ï¼Œå¹¶åœ¨æ–°è§†è§’åˆæˆä¸­è¾¾åˆ°äº†å‰æ²¿çš„æ¸²æŸ“è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11387v1">PDF</a> Accepted by NeurIPS 2025. Project Page:   <a target="_blank" rel="noopener" href="https://wen-yuan-zhang.github.io/MaterialRefGS">https://wen-yuan-zhang.github.io/MaterialRefGS</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºäºŒç»´å›¾åƒå»ºæ¨¡åå°„å¯¹çœŸå®æ„Ÿæ¸²æŸ“å’Œæ–°é¢–è§†è§’åˆæˆçš„é‡è¦æ€§ã€‚é’ˆå¯¹ç°æœ‰é«˜æ–¯åŸºå…ƒæ–¹æ³•åœ¨åå°„ç›¸å…³ææ–™å±æ€§æ¨æ–­ä¸Šçš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¯å¢ƒå»ºæ¨¡å—é™æƒ…å†µä¸‹çš„çº¦æŸä¸è¶³é—®é¢˜ï¼Œæœ¬æ–‡ä»å¤šè§†è§’è§†è§’é‡æ–°å®¡è§†é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºé‡‡ç”¨æ›´å…·ç‰©ç†åŸºç¡€çš„ç¯å¢ƒå»ºæ¨¡è¿›è¡Œå¤šè§†è§’ä¸€è‡´æ€§ææ–™æ¨æ–­æ˜¯ä½¿ç”¨é«˜æ–¯æ‘Šå¼€æŠ€æœ¯çš„å…³é”®ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæœ¬æ–‡åœ¨å»¶è¿Ÿç€è‰²è¿‡ç¨‹ä¸­å¼ºåˆ¶äºŒç»´é«˜æ–¯ç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„ææ–™å›¾ï¼Œå¹¶è·Ÿè¸ªè·¨è§†å›¾çš„å…‰åº¦å˜åŒ–ä»¥è¯†åˆ«é«˜åå°„åŒºåŸŸï¼Œä½œä¸ºåå°„å¼ºåº¦é¡¹çš„å¼ºå…ˆéªŒã€‚ä¸ºè§£å†³ç”±å¯¹è±¡é—´é®æŒ¡å¼•èµ·çš„é—´æ¥ç…§æ˜é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡ç»“åˆäºŒç»´é«˜æ–¯åˆ†è£‚å’Œå…‰çº¿è¿½è¸ªæŠ€æœ¯è¿›ä¸€æ­¥å¼•å…¥ç¯å¢ƒå»ºæ¨¡ç­–ç•¥ï¼Œä»¥å®ç°é—´æ¥è¾å°„çš„é€¼çœŸæ¸²æŸ“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ç…§æ˜å’Œå‡ ä½•æ¢å¤æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå®ç°äº†æ–°é¢–è§†è§’åˆæˆçš„æœ€æ–°æ¸²æŸ“è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜æ–¯åŸºå…ƒé€šè¿‡åŠ å…¥åå°„ç›¸å…³ææ–™å±æ€§å¢å¼ºäº†ç‰©ç†åŸºç¡€æ¸²æŸ“èƒ½åŠ›ã€‚</li>
<li>ç¯å¢ƒå»ºæ¨¡é™åˆ¶ä¸‹çš„ææ–™æ¨æ–­ç¼ºä¹è¶³å¤Ÿçº¦æŸï¼Œå¯¼è‡´ç…§æ˜æ··æ·†å’Œæ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚</li>
<li>æå‡ºä»å¤šè§†è§’ä¸€è‡´æ€§çš„è§’åº¦è¿›è¡Œææ–™æ¨æ–­ï¼Œä»¥å¢å¼ºç‰©ç†åŸºç¡€çš„ç¯å¢ƒå»ºæ¨¡ã€‚</li>
<li>é€šè¿‡å¼ºåˆ¶äºŒç»´é«˜æ–¯ç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„ææ–™å›¾æ¥ä¼˜åŒ–å»¶è¿Ÿç€è‰²è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡è·Ÿè¸ªè·¨è§†å›¾çš„å…‰åº¦å˜åŒ–æ¥è¯†åˆ«é«˜åå°„åŒºåŸŸï¼Œä¸ºåå°„å¼ºåº¦æä¾›å¼ºå…ˆéªŒã€‚</li>
<li>ç»“åˆäºŒç»´é«˜æ–¯åˆ†è£‚å’Œå…‰çº¿è¿½è¸ªæŠ€æœ¯æ¥è§£å†³é—´æ¥ç…§æ˜é—®é¢˜ï¼Œå®ç°é—´æ¥è¾å°„çš„é€¼çœŸæ¸²æŸ“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca09617c2ade03f4feee1354457944b3" align="middle">
<img src="https://picx.zhimg.com/v2-2303660fd99be8f64b7bea2e6cbcf9ac" align="middle">
<img src="https://picx.zhimg.com/v2-d9c38ef1d73491b07e00c3b959631ae6" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-3D-Gaussian-Human-Avatar-Compression-A-Prior-Guided-Framework"><a href="#Towards-Efficient-3D-Gaussian-Human-Avatar-Compression-A-Prior-Guided-Framework" class="headerlink" title="Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided   Framework"></a>Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided   Framework</h2><p><strong>Authors:Shanzhi Yin, Bolin Chen, Xinju Wu, Ru-Ling Liao, Jie Chen, Shiqi Wang, Yan Ye</strong></p>
<p>This paper proposes an efficient 3D avatar coding framework that leverages compact human priors and canonical-to-target transformation to enable high-quality 3D human avatar video compression at ultra-low bit rates. The framework begins by training a canonical Gaussian avatar using articulated splatting in a network-free manner, which serves as the foundation for avatar appearance modeling. Simultaneously, a human-prior template is employed to capture temporal body movements through compact parametric representations. This decomposition of appearance and temporal evolution minimizes redundancy, enabling efficient compression: the canonical avatar is shared across the sequence, requiring compression only once, while the temporal parameters, consisting of just 94 parameters per frame, are transmitted with minimal bit-rate. For each frame, the target human avatar is generated by deforming canonical avatar via Linear Blend Skinning transformation, facilitating temporal coherent video reconstruction and novel view synthesis. Experimental results demonstrate that the proposed method significantly outperforms conventional 2D&#x2F;3D codecs and existing learnable dynamic 3D Gaussian splatting compression method in terms of rate-distortion performance on mainstream multi-view human video datasets, paving the way for seamless immersive multimedia experiences in meta-verse applications. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„3DåŒ–èº«ç¼–ç æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç´§å‡‘çš„äººä½“å…ˆéªŒçŸ¥è¯†å’Œè§„èŒƒåˆ°ç›®æ ‡çš„è½¬æ¢ï¼Œä»¥å®ç°è¶…é«˜å‹ç¼©ç‡çš„ä¼˜è´¨3Däººç±»åŒ–èº«è§†é¢‘å‹ç¼©ã€‚è¯¥æ¡†æ¶é¦–å…ˆä»¥æ— ç½‘ç»œçš„æ–¹å¼ä½¿ç”¨å…³èŠ‚æº…æ³¼æ³•è®­ç»ƒè§„èŒƒé«˜æ–¯åŒ–èº«ï¼Œä½œä¸ºåŒ–èº«å¤–è§‚å»ºæ¨¡çš„åŸºç¡€ã€‚åŒæ—¶ï¼Œåˆ©ç”¨äººä½“å…ˆéªŒæ¨¡æ¿é€šè¿‡ç´§å‡‘çš„å‚æ•°è¡¨ç¤ºæ¥æ•æ‰èº«ä½“åŠ¨ä½œçš„ä¸´æ—¶å˜åŒ–ã€‚è¿™ç§å¤–è§‚å’Œä¸´æ—¶æ¼”å˜çš„åˆ†è§£æœ€å°åŒ–å†—ä½™ï¼Œå®ç°äº†é«˜æ•ˆçš„å‹ç¼©ï¼šè§„èŒƒåŒ–èº«åœ¨æ•´ä¸ªåºåˆ—ä¸­å…±äº«ï¼Œåªéœ€å‹ç¼©ä¸€æ¬¡ï¼Œè€Œä¸´æ—¶å‚æ•°ï¼ˆæ¯å¸§ä»…åŒ…å«94ä¸ªå‚æ•°ï¼‰åˆ™ä»¥æä½çš„æ¯”ç‰¹ç‡ä¼ è¾“ã€‚é’ˆå¯¹æ¯ä¸€å¸§ï¼Œç›®æ ‡äººç±»åŒ–èº«æ˜¯é€šè¿‡çº¿æ€§æ··åˆè’™çš®å˜æ¢å¯¹è§„èŒƒåŒ–èº«è¿›è¡Œå˜å½¢è€Œç”Ÿæˆçš„ï¼Œè¿™ä¿ƒè¿›äº†æ—¶é—´è¿è´¯çš„è§†é¢‘é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸»æµçš„å¤šè§†è§’äººç±»è§†é¢‘æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨é€Ÿç‡å¤±çœŸæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„2D&#x2F;3Dç¼–è§£ç å™¨å’Œç°æœ‰çš„å¯å­¦ä¹ åŠ¨æ€3Dé«˜æ–¯æº…æ³¼å‹ç¼©æ–¹æ³•ï¼Œä¸ºå…ƒå®‡å®™åº”ç”¨ä¸­çš„æ— ç¼æ²‰æµ¸å¼å¤šåª’ä½“ä½“éªŒé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10492v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„3DåŒ–èº«ç¼–ç æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç´§å‡‘çš„äººç±»å…ˆéªŒçŸ¥è¯†å’Œè§„èŒƒåˆ°ç›®æ ‡çš„è½¬æ¢ï¼Œå®ç°äº†è¶…ä½æ¯”ç‰¹ç‡ä¸‹çš„é«˜è´¨é‡3Däººç±»åŒ–èº«è§†é¢‘å‹ç¼©ã€‚è¯¥æ¡†æ¶é€šè¿‡æ— ç½‘ç»œçš„æ–¹å¼ä½¿ç”¨å…³èŠ‚ç‚¹è´´ç‰‡æŠ€æœ¯è®­ç»ƒè§„èŒƒé«˜æ–¯åŒ–èº«ï¼Œä½œä¸ºåŒ–èº«å¤–è§‚æ¨¡å‹çš„åŸºç¡€ã€‚åŒæ—¶ï¼Œåˆ©ç”¨äººç±»å…ˆéªŒæ¨¡æ¿æ•è·èº«ä½“è¿åŠ¨çš„ä¸´æ—¶å˜åŒ–ï¼Œé€šè¿‡ç´§å‡‘çš„å‚æ•°è¡¨ç¤ºå‘ˆç°ã€‚å¤–è§‚å’Œä¸´æ—¶æ¼”å˜çš„åˆ†è§£æœ€å°åŒ–å†—ä½™ï¼Œå®ç°äº†é«˜æ•ˆçš„å‹ç¼©ï¼šè§„èŒƒåŒ–èº«åœ¨åºåˆ—ä¸­å…±äº«ï¼Œåªéœ€å‹ç¼©ä¸€æ¬¡ï¼Œè€Œæ¯å¸§çš„ä¸´æ—¶å‚æ•°ä»…ç”±94ä¸ªå‚æ•°ç»„æˆï¼Œä»¥æœ€ä½çš„æ¯”ç‰¹ç‡ä¼ è¾“ã€‚ç›®æ ‡äººç±»åŒ–èº«æ˜¯é€šè¿‡çº¿æ€§æ··åˆè’™çš®å˜æ¢å˜å½¢è§„èŒƒåŒ–èº«ç”Ÿæˆçš„ï¼Œæœ‰åˆ©äºä¸´æ—¶è¿è´¯çš„è§†é¢‘é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»æµçš„å¤šè§†è§’äººç±»è§†é¢‘æ•°æ®é›†ä¸Šçš„é€Ÿç‡å¤±çœŸæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„2D&#x2F;3Dç¼–è§£ç å™¨å’Œç°æœ‰çš„å¯å­¦ä¹ åŠ¨æ€3Dé«˜æ–¯è´´ç‰‡å‹ç¼©æ–¹æ³•ï¼Œä¸ºå…ƒå®‡å®™åº”ç”¨ä¸­çš„æ— ç¼æ²‰æµ¸å¼å¤šåª’ä½“ä½“éªŒé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„3DåŒ–èº«ç¼–ç æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆå‹ç¼©é«˜è´¨é‡3Däººç±»åŒ–èº«è§†é¢‘ã€‚</li>
<li>åˆ©ç”¨è§„èŒƒé«˜æ–¯åŒ–èº«ä½œä¸ºå¤–è§‚æ¨¡å‹åŸºç¡€ï¼Œé€šè¿‡æ— ç½‘ç»œæ–¹å¼è®­ç»ƒã€‚</li>
<li>é‡‡ç”¨äººç±»å…ˆéªŒæ¨¡æ¿æ•æ‰èº«ä½“è¿åŠ¨çš„ä¸´æ—¶å˜åŒ–ï¼Œä»¥ç´§å‡‘çš„å‚æ•°å½¢å¼è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡åˆ†è§£å¤–è§‚å’Œä¸´æ—¶æ¼”å˜å®ç°é«˜æ•ˆå‹ç¼©ã€‚è§„èŒƒåŒ–èº«åœ¨æ•´ä¸ªåºåˆ—ä¸­å…±äº«ä¸”åªéœ€å‹ç¼©ä¸€æ¬¡ã€‚æ¯å¸§çš„ä¸´æ—¶å‚æ•°æ•°é‡æœ‰é™ï¼ˆä»…94ä¸ªå‚æ•°ï¼‰ã€‚</li>
<li>ç›®æ ‡åŒ–èº«é€šè¿‡çº¿æ€§æ··åˆè’™çš®å˜æ¢ä»è§„èŒƒåŒ–èº«å˜å½¢è€Œæ¥ï¼Œæ”¯æŒè¿è´¯çš„è§†é¢‘é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•çš„é€Ÿç‡å¤±çœŸæ€§èƒ½ä¼˜äºä¼ ç»Ÿç¼–è§£ç å™¨å’Œç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c121e29951958cfdeac071fc41520449" align="middle">
<img src="https://picx.zhimg.com/v2-d129393c374091520bda02fcadf48cb5" align="middle">
<img src="https://picx.zhimg.com/v2-1181f67f7d0c19ecde70e116f1ace6eb" align="middle">
<img src="https://picx.zhimg.com/v2-84212c18eac2ed370448fac84aeb1a36" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be184f978b42f0ba861eeded4fd503ca" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  GauSSmart Enhanced 3D Reconstruction through 2D Foundation Models and   Geometric Filtering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e93f49726b817261f4cfd661a066c550" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
