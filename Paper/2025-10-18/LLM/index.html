<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-10-18  Agentic Design of Compositional Machines">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-07dfd4190e066b6ffd27340fa7e6a279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728472&auth_key=1760728472-0-0-7b38aef80c3e51a98c4a5f5c3d1e5e84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    88 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="Agentic-Design-of-Compositional-Machines"><a href="#Agentic-Design-of-Compositional-Machines" class="headerlink" title="Agentic Design of Compositional Machines"></a>Agentic Design of Compositional Machines</h2><p><strong>Authors:Wenqian Zhang, Weiyang Liu, Zhen Liu</strong></p>
<p>The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning. </p>
<blockquote>
<p>复杂机器的设计既是人类智慧的标志，也是工程实践的基础。鉴于大型语言模型（LLM）的最新进展，我们想知道它们是否也能学会创造。我们通过组合机器设计的视角来探讨这个问题：这是一项任务，其中机器由标准化组件组装而成，以满足模拟物理环境中的运动或操作等功能需求。为了支持这项研究，我们引入了BesiegeField，这是一个建立在机器建造游戏Besiege之上的测试平台，支持部件构造、物理模拟和奖励驱动评估。使用BesiegeField，我们评估了最先进的LLM的代理工作流程，并确定了成功所需的关键能力，包括空间推理、策略性装配和指令遵循。由于当前开源模型的不足，我们探索了强化学习（RL）作为改进的途径：我们整理了一个冷启动数据集，进行了RL微调实验，并强调了语言、机器设计和物理推理交汇处的开放挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14980v1">PDF</a> 75 pages, 31 figures, Project Page: <a target="_blank" rel="noopener" href="https://besiegefield.github.io/">https://besiegefield.github.io</a></p>
<p><strong>摘要</strong></p>
<p>最新的大型语言模型（LLM）在机器设计领域展现出巨大潜力。本文聚焦组合式机器设计任务，即利用标准化组件组装机器以满足模拟物理环境中的功能需求，如运动或操控。为支持这一研究，我们引入了基于机器建造游戏Besiege打造的测试平台BesiegeField，该平台支持部件级建造、物理模拟和奖励驱动评估。我们利用BesiegeField对最先进的LLM进行基准测试，并确定成功的关键能力，包括空间推理、策略性装配和指令遵循等。鉴于当前开源模型的局限性，我们探索了强化学习（RL）作为提升这些能力的途径：整理初始数据集，进行RL微调实验，并强调语言、机器设计和物理推理交汇处的开放挑战。</p>
<p><strong>要点摘要</strong></p>
<ol>
<li>大型语言模型（LLM）在机器设计领域的应用成为研究焦点。</li>
<li>组合式机器设计任务要求机器满足功能需求，如运动与操控。</li>
<li>BesiegeField测试平台支持机器建造的多个方面评估，包括部件级建造和物理模拟。</li>
<li>LLM成功关键能力包括空间推理、策略性装配和指令遵循。</li>
<li>当前开源模型在机器设计任务中存在局限性。</li>
<li>强化学习（RL）被探索为提升LLM在机器设计领域能力的途径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14980">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c256346b3b9819197b782e126d33bdfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728298&auth_key=1760728298-0-0-5dfc888d30fb9602a876ba910394f727&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b72de284f534b45d2dde836e7407ef4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728305&auth_key=1760728305-0-0-0098c50b1512c8f8b23238c86a42c044&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a452b05e0464d861460cf73d4168c3cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728312&auth_key=1760728312-0-0-7808e1521202482a6d327d4c10bf620a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d601bcfadf1df847879211c85ec7849~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728319&auth_key=1760728319-0-0-1b2b679db83068d7bf5825db295c5c32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7a965fcabd9913c6053e7b0c2183944~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728325&auth_key=1760728325-0-0-6969bdf9a087f56fc2889d8b4fbe4eca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents"><a href="#Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents" class="headerlink" title="Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents"></a>Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents</h2><p><strong>Authors:Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying</strong></p>
<p>Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy’s probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model’s own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理越来越多地使用强化学习（RL）进行训练，以加强其通过工具使用与外部环境的交互能力，特别是在需要多轮推理和知识获取基于搜索的环境中。然而，现有方法通常依赖于以结果为基础的奖励，这些奖励仅在最终答案时提供。在需要多轮对话的环境中，奖励稀疏性变得特别成问题，其中长期轨迹加剧了两个关键问题：（i）优势崩溃，所有实验反馈均收到相同奖励且无法提供有用的学习信号；（ii）精细奖励分配不足，在特定情况下每一轮对话之间的依赖性被遮蔽。本文提出了基于信息增益的策略优化（IGPO），这是一个简单有效的强化学习框架，为针对多轮对话的代理训练提供密集且内在的监督。IGPO将每个交互回合视为关于真相的增量信息获取过程，并将回合级奖励定义为策略产生正确答案的概率的边际增加。与依赖外部奖励模型或昂贵的蒙特卡洛估计的先前过程级奖励方法不同，IGPO直接从模型自身的信念更新中获得内在奖励。这些内在回合级奖励与结果级监督相结合，形成了密集的奖励轨迹。在内部和外部基准测试上的广泛实验表明，在多轮对话场景中，IGPO始终优于强大的基准测试，具有更高的准确性和样本效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14967v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>强化学习（RL）被越来越多地用于训练大型语言模型（LLM）代理，以增强其在工具使用等方面的外部环境交互能力，特别是在需要多轮推理和知识获取的的搜索环境中。然而，现有方法通常依赖于结果导向的奖励，仅在最终答案提供奖励。在多轮对话场景中，这种奖励稀疏性会引发两个问题：（i）优势崩溃，所有回合获得相同奖励，无法提供有用的学习信号；（ii）缺乏精细的信用分配，回合之间的依赖关系被掩盖，特别是在长期任务中。本文提出基于信息增益的策略优化（IGPO），这是一个简单有效的强化学习框架，为多轮代理训练提供密集和内在的监督。IGPO将每个交互回合视为获取关于事实真相的增量信息的过程，并将回合级奖励定义为策略产生正确答案的概率的边际增加。与依赖外部奖励模型或昂贵的蒙特卡洛估计的过程级奖励方法不同，IGPO直接从模型自身的信念更新中获得内在奖励。这些内在的回合级奖励与结果级监督相结合，形成了密集的奖励轨迹。在域内和跨域基准测试上的广泛实验表明，IGPO在多轮场景中始终优于强大的基准测试，提高了准确性和样本效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs结合强化学习（RL）用于提升与外部环境交互能力，特别是在多轮推理和知识获取方面的搜索环境。</li>
<li>现有方法依赖结果导向的奖励系统，在多轮对话场景中引发奖励稀疏性问题。</li>
<li>IGPO框架被提出作为一个简单有效的RL框架，为多轮代理训练提供密集和内在的监督。</li>
<li>IGPO将每个交互回合视为获取信息的增量过程，并定义回合级奖励为策略产生正确答案的概率的边际增加。</li>
<li>IGPO直接从模型自身的信念更新中得出内在奖励，与结果级监督结合形成密集的奖励轨迹。</li>
<li>实验表明，IGPO在多轮场景中的性能优于其他方法，提高了准确性和样本效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14967">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6acee461152fa22ff3a292ec3c6b9c47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728333&auth_key=1760728333-0-0-5d37b06b5811ec041243f18a36e5a4a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-069829c476fc987398af88ea95aa8f54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728340&auth_key=1760728340-0-0-a33a796db3441e6c272da6f477ee92ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcb8a3c56cd16d74ac346368c4442502~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728346&auth_key=1760728346-0-0-4ceb527164828b3f407703c63ceac732&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Identity-Link-IRT-for-Label-Free-LLM-Evaluation-Preserving-Additivity-in-TVD-MI-Scores"><a href="#Identity-Link-IRT-for-Label-Free-LLM-Evaluation-Preserving-Additivity-in-TVD-MI-Scores" class="headerlink" title="Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity   in TVD-MI Scores"></a>Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity   in TVD-MI Scores</h2><p><strong>Authors:Zachary Robertson</strong></p>
<p>Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MI’s binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 &#x3D; [0.474, 0.580]), whereas probit&#x2F;logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent rankings (Spearman $\rho &#x3D; 0.972 \pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\rho &#x3D; 0.872$) and consistent identity-link advantage. TVD-MI’s geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains. </p>
<blockquote>
<p>使用总变差距离互信息（TVD-MI）对大型语言模型进行配对比较，为每对模型产生二元批判决策。我们展示，对TVD-MI的二元试验进行平均，可以得到具有加法结构的中心化概率分数，适用于项目反应理论（IRT）而无需非线性链接函数。IRT的最大似然方法使用逻辑链接，但我们的经验表明，这些转换会引入曲线，从而破坏可加性：在三个领域中，身份链接导致原始数据的中位数卷曲为0.080-0.150（P95&#x3D;[0.474, 0.580]），而Probit&#x2F;logit则引入更高的违规情况（中位数[0.245, 0.588]，P95 [0.825, 2.252]）。我们从基尼熵最大化中得出这个裁剪的线性模型，得到一个受盒子约束的最小二乘公式，该公式可处理边界饱和问题。在33%的覆盖率下，我们实现了留出验证的RMSE为$0.117 \pm 0.008$，同时保留了代理排名（斯皮尔曼$\rho &#x3D; 0.972 \pm 0.015$），所需评估次数比全面密集评估减少三倍。法官稳健性分析（GPT-4o-mini与Llama3-70b）显示代理排名高度一致（$\rho &#x3D; 0.872$）以及身份链接的一致优势。对于其他有界响应领域，TVD-MI的几何结构通过身份映射得到最佳保留，有利于大型语言模型的有效评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14966v1">PDF</a> 9 pages, 2 figures</p>
<p><strong>摘要</strong></p>
<p>使用总变差距离互信息（TVD-MI）进行大型语言模型的配对比较，产生每对的二元批评决策。我们展示平均TVD-MI的二元试验会产生适合项目反应理论（IRT）的中心概率分数，具有可加结构而无需非线性链接函数。最大似然方法采用逻辑链接，但经验显示这些转换破坏了可加性引入了曲线。跨三个领域，身份链接导致原始数据的平均卷曲在0.080-0.150之间（P95 &#x3D; [0.474，0.580]），而probit&#x2F;logit引入的违规情况更高（中位数[0.245，0.588]，P95 [0.825，2.252]）。我们从基尼熵最大化中推导出这种被截断的线性模型，它采用框约束最小二乘法公式处理边界饱和问题。在覆盖率达到33%的情况下，我们实现了保留代理排名的同时的holdout RMSE为$ 0.117 \pm 0.008 $（Spearman $\rho &#x3D; 0.972 \pm 0.015$），所需评估次数减少到全密集情况下的三分之一。法官稳健性分析（GPT-4o-mini与Llama3-70b的比较）显示代理排名高度一致（ρ&#x3D; 0.872），进一步证实了身份链接的优势。对于大型语言模型评估，身份映射可以更好地保留TVD-MI的几何结构，适用于其他有界响应领域。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>使用总变差距离互信息（TVD-MI）进行大型语言模型的配对比较。</li>
<li>通过平均TVD-MI的二元试验得出适合项目反应理论（IRT）的中心概率分数。</li>
<li>在IRT中，身份链接相比逻辑链接具有更好的性能，因为逻辑链接破坏了可加性引入了曲线。</li>
<li>在处理边界饱和问题时，采用框约束最小二乘法公式。</li>
<li>在覆盖率达到33%的情况下，评估结果具有良好的稳健性，holdout RMSE较低，同时保留代理排名。</li>
<li>在法官稳健性分析中，身份链接的优势得到进一步证实，代理排名高度一致。</li>
<li>TVD-MI的几何结构在身份映射下能更好地保留，适用于其他有界响应领域的评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-045f80efde0d486662affd10dded00dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728353&auth_key=1760728353-0-0-2bafeff3691e17a748629d808a0ce00d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e28c90ce88af456a792d74812d9da244~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728360&auth_key=1760728360-0-0-3ebd8b42e863f7c77238e294d984ac9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2e5e15797097f45122c5712d06e8d05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728367&auth_key=1760728367-0-0-03f15cf867295a0a9b447b1b257f3b1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc38e4067371bf02c7d9a4bd572f3b7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728374&auth_key=1760728374-0-0-42dfbd28f51a78006b5934001a517ccf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning"><a href="#MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning" class="headerlink" title="MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning"></a>MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning</h2><p><strong>Authors:Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li</strong></p>
<p>While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a> </p>
<blockquote>
<p>大型语言模型（LLM）在文本推理方面表现出色，但在依赖视觉辅助工具的几何等数学领域却遇到困难。现有的视觉思维链（VCoT）方法往往受到僵化外部工具的限制，或者无法生成用于复杂问题解决的具有高保真度、战略定时图的必要。为了弥补这一差距，我们引入了MathCanvas，这是一个旨在赋予统一的大型多模态模型（LMM）内在VCoT能力的全面框架，用于数学领域。我们的方法分为两个阶段。首先，视觉操作阶段使用新型1520万对数据对模型进行预训练，其中包括1亿个标题到图表的配对（MathCanvas-Imagen）和520万步的编辑轨迹（MathCanvas-Edit），以掌握图表生成和编辑。其次，战略视觉辅助推理阶段在MathCanvas-Instruct数据集上进行微调，这是一组包含交错视觉文本推理路径的新数据集，含有21.9万个示例，教学模型何时以及如何利用视觉辅助工具。为了促进严格评估，我们推出了MathCanvas-Bench，这是一个具有挑战性的基准测试，包含3000个问题，需要模型生成交错的视觉文本解决方案。在我们的框架训练下的BAGEL-Canvas模型在MathCanvas-Bench上的表现相较于强大的LMM基准测试有了86%的相对改进，展现出对其他公共数学基准测试的出色泛化能力。我们的工作提供了一个完整的工具包框架、数据集和基准测试，以解锁LMM中复杂、类似人类的视觉辅助推理。项目页面：<a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14958v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>该文介绍了MathCanvas框架，该框架旨在将大型多模态模型（LMMs）赋予内在的可视化思维链（VCoT）能力，以应对数学领域的挑战。该框架包含两个阶段：首先是视觉操作阶段，通过预训练模型掌握图表生成和编辑技能；其次是战略视觉辅助推理阶段，对模型进行微调以学会如何利用视觉辅助工具。该框架下的模型BAGEL-Canvas在MathCanvas-Bench上相对于强大的LMM基准模型实现了86%的相对改进，展现出优秀泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs在文本推理方面表现出色，但在依赖视觉辅助的数学领域如几何上遇到困难。</li>
<li>MathCanvas框架旨在通过两个阶段赋予LMMs内在的可视化思维链（VCoT）能力。</li>
<li>第一个阶段是视觉操作阶段，通过预训练模型掌握图表生成和编辑技能。</li>
<li>第二个阶段是战略视觉辅助推理阶段，对模型进行微调以学会如何利用视觉辅助工具进行复杂的数学问题求解。</li>
<li>MathCanvas引入了新的数据集MathCanvas-Bench，用于评估模型的性能。</li>
<li>模型BAGEL-Canvas在MathCanvas-Bench上实现了较高的性能，相较于基准模型有显著改善。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5c4eefc85e5854dadcc669fa4285b07d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728382&auth_key=1760728382-0-0-e86ab40d83b6eaaf00f247557e19b1b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de68a430433309e0a20c51788ecb1b34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728389&auth_key=1760728389-0-0-326232a6660338f13d5174eeb14d9a65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea25758b6de272f758c85964a8765599~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728395&auth_key=1760728395-0-0-4b3bebd1b96efd4e434f3e02ba4b6377&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5450ab2559bbf7b0feb603ba68bd91fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728402&auth_key=1760728402-0-0-4c9842c5a622707ef62bbf9b97f77a7b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning"><a href="#GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning" class="headerlink" title="GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning"></a>GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning</h2><p><strong>Authors:Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp</strong></p>
<p>Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning. </p>
<blockquote>
<p>流程奖励模型（PRM）旨在通过监督中间步骤和识别错误来改善大型语言模型（LLM）的多步推理能力。然而，构建有效的PRM仍然是一个挑战，因为缺乏可扩展的高质量注释。现有方法依赖于昂贵的人工标注、基于LLM的自我评估（容易产生幻觉）或蒙特卡洛（MC）估计。MC估计仅从滚动结果推断步骤质量，通常由于信用分配不当而引入嘈杂、失配的监督。这些问题导致三个核心局限：奖励噪声、事实准确性低以及与步骤级推理目标的不对齐。为了解决这些挑战，我们引入了GroundedPRM，这是一个用于自动流程监督的树状指导和保真度感知框架。为了减少奖励噪声并实现精细的信用分配，我们通过蒙特卡洛树搜索（MCTS）构建结构化推理路径。为了消除幻觉监督，我们使用外部工具验证每个中间步骤，提供基于执行的正确性信号。为了结合步骤级验证和全局结果评估，我们设计了一种混合奖励聚合机制，它将基于工具的检查与MCTS派生的反馈相结合。最后，我们将奖励信号格式化为增强理由的生成结构，以促进与指令调整过的LLM的可解释性和兼容性。GroundedPRM仅使用自动标记的4万样本进行训练，这仅相当于使用自动标记监督的最佳PRM所使用的数据的十分之一。尽管如此，它在ProcessBench上的平均性能达到了高达26%的相对改进。当用于奖励指导的贪心搜索时，GroundedPRM甚至超越了使用人工标记监督训练的PRM，为高质量流程级推理提供了可扩展和可验证的路径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14942v1">PDF</a> 25 pages</p>
<p><strong>摘要</strong></p>
<p>基于过程奖励模型（PRM）旨在通过监督中间步骤和识别错误来改善大型语言模型（LLM）的多步推理能力。然而，构建有效的PRM仍然具有挑战性，缺乏可扩展的高品质注释是主要原因。现有方法依赖于昂贵的人力标注、LLM的自我评价（容易出现幻觉）或蒙特卡洛（MC）估算，后者仅从滚动结果推断步骤质量，但由于信用误判，经常引入嘈杂、失真的监督。针对这些核心问题，我们提出GroundedPRM，一个用于自动过程监督的树状引导和保真度感知框架。通过蒙特卡洛树搜索（MCTS）构建结构化推理路径，减少奖励噪声并实现精细的信用分配。我们利用外部工具验证每个中间步骤，提供执行基础正确性信号，消除幻觉监督。通过结合步骤级验证和全局结果评估，我们设计了一种混合奖励聚合机制，融合了工具验证与MCTS派生的反馈。最后，我们将奖励信号格式化为增强理性的生成结构，以促进与指令调整LLM的解读和兼容性。GroundedPRM仅使用4万自动标记样本进行训练，仅相当于使用自动标记监督的最佳PRM所用数据的十分之一。然而，它在ProcessBench上的平均性能提高了26%。当用于奖励引导贪心搜索时，GroundedPRM甚至超越了使用人力标记监督训练的PRM，为高质量的过程级推理提供了一条可扩展和可验证的路径。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>过程奖励模型（PRM）旨在改善大型语言模型（LLM）的多步推理能力。</li>
<li>现有PRM方法面临三大核心挑战：奖励噪声、低事实保真度和与步骤级推理目标的不对齐。</li>
<li>GroundedPRM通过蒙特卡洛树搜索（MCTS）构建结构化推理路径，减少奖励噪声并实现精细的信用分配。</li>
<li>GroundedPRM利用外部工具验证中间步骤，提供执行基础正确性信号，消除幻觉监督。</li>
<li>GroundedPRM结合步骤级验证和全局结果评估，设计混合奖励聚合机制。</li>
<li>GroundedPRM仅使用少量自动标记样本进行训练，表现出卓越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14942">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-dd41ee8a24b5e926edfce49f81364020~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728409&auth_key=1760728409-0-0-15005a3a314f7e4e9399375be1a3b75d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3a6fabc7ec2f2e13ded990993e3bb07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728451&auth_key=1760728451-0-0-dafb72c2bada9a4c0f589202550cfce2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Predicting-Task-Performance-with-Context-aware-Scaling-Laws"><a href="#Predicting-Task-Performance-with-Context-aware-Scaling-Laws" class="headerlink" title="Predicting Task Performance with Context-aware Scaling Laws"></a>Predicting Task Performance with Context-aware Scaling Laws</h2><p><strong>Authors:Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang</strong></p>
<p>Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/context-scaling">https://github.com/wang-research-lab/context-scaling</a>. </p>
<blockquote>
<p>比例定律通过联系上游指标（如交叉熵损失）与设计因素（如模型规模、训练数据和计算）改变了我们对大型语言模型的理解。然而，这些传统定律无法捕捉下游任务性能，其中上下文起着至关重要的作用。在这项工作中，我们提出了一个简单、可解释的框架，该框架将下游性能作为训练计算和所提供上下文的函数进行建模。我们通过拟合Llama-2-7B和Llama-2-13B的扩展上下文变体在65500个唯一实例中的下游性能表现来实证验证我们的框架，这些实例跨越三项任务：算术推理、常识推理和机器翻译。我们的结果表明，我们的框架准确地模拟了分布内下游性能表现，在训练计算方面跨越了三个数量级的范围，并可靠地预测了随着上下文增加而提高的性能表现。这些发现对于训练计算和上下文利用之间的相互作用提供了有价值的见解，为设计更高效的长上下文大型语言模型以用于多种下游任务提供了指导。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/context-scaling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wang-research-lab/context-scaling找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14919v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究提出了一个简单且可解释的框架，该框架联合建模下游性能作为训练计算和所提供上下文的功能。通过对Llama-2的不同版本在三个任务上的下游性能进行实证研究，验证了该框架的准确性。研究结果表明，该框架能够准确模拟下游性能，在训练计算方面跨越三个数量级进行推广，并可靠地预测随着上下文量增加的性能。这为训练计算和上下文利用之间的相互作用提供了有价值的见解，为设计更高效的长上下文LLMs用于各种下游任务提供了指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了一个联合建模下游性能的简单且可解释的框架，该框架考虑了训练计算量和上下文的影响。</li>
<li>通过实证研究发现，所提出的框架能够准确模拟下游性能，在训练计算方面具有良好的推广性。</li>
<li>框架可靠地预测了随着上下文量增加的性能，显示出其在不同任务间的通用性。</li>
<li>研究结果揭示了训练计算和上下文利用之间的复杂关系，为设计更高效的长上下文LLMs提供了指导。</li>
<li>该研究对比了不同版本的Llama-2模型在不同任务上的表现，证明了其方法的实用性。</li>
<li>研究强调了语境在下游任务性能中的重要性，表明理解语境对于优化LLM的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d086e31c5e64fb72baf55bd4b0355c59~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728458&auth_key=1760728458-0-0-10dc4557c5f11cf6b7767dabe22e11ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-48f800080ccd736ddedc4fab8ae2afaf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728466&auth_key=1760728466-0-0-0653f9ea92c914c5f119ad9f21481d88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07dfd4190e066b6ffd27340fa7e6a279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728472&auth_key=1760728472-0-0-7b38aef80c3e51a98c4a5f5c3d1e5e84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97d79f88002381a2dd9edd1c4219f66a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728479&auth_key=1760728479-0-0-19ca4cfde6f2b0d285c64f4c9d69b9a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7135da153e7f5413dfd2f6ade749ec7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728485&auth_key=1760728485-0-0-8b0eed1ebccb494441eb99265b2797c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b81458f426e7d35069b7b2df5cbfe58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728492&auth_key=1760728492-0-0-37917b4b9275a2af82abd2d6c60e1b2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1aa9e2bcbbf5b55cfa5673202d72e9ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728499&auth_key=1760728499-0-0-ef776761b9c6cc1136b4b5ad14b2636d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12f50a18ee9a6da6573b24fa021a6e9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728505&auth_key=1760728505-0-0-c2a2f1ae3e30e7e1aa3558ab1ebee0a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Budget-aware-Test-time-Scaling-via-Discriminative-Verification"><a href="#Budget-aware-Test-time-Scaling-via-Discriminative-Verification" class="headerlink" title="Budget-aware Test-time Scaling via Discriminative Verification"></a>Budget-aware Test-time Scaling via Discriminative Verification</h2><p><strong>Authors:Kyle Montgomery, Sijun Tan, Yuqi Chen, Siyuan Zhuang, Tianjun Zhang, Raluca Ada Popa, Chenguang Wang</strong></p>
<p>Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a “free” upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/verification">https://github.com/wang-research-lab/verification</a>. </p>
<blockquote>
<p>测试时缩放策略是提高大型语言模型在复杂推理任务上性能的强大策略。虽然最先进的方法通常采用生成式验证器从候选池中挑选最佳解决方案，但这种方法计算成本高昂，实用性受限。在这项工作中，我们将重点转向更具预算意识的范式：判别式验证。我们进行了彻底的实证分析，并证明虽然孤立使用判别式验证器可能表现不佳，但将其与自我一致性相结合，在混合方法中可创建强大且高效的测试时缩放机制。值得注意的是，在固定的计算预算下，这种混合方法大幅超越了最先进的生成式验证方法：在AIME2025上达到了高达15.3%的准确率提升。我们的研究结果表明，对于实际应用和现实世界应用，采用预算意识的判别式验证器进行缩放不仅是一个免费的自我一致性升级，而且也是一种成本更高的生成技术的更高效和有效的替代方案。代码可在<a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/verification%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wang-research-lab/verification找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14913v1">PDF</a> </p>
<p><strong>Summary</strong><br>大规模语言模型在复杂推理任务中的测试时间缩放策略可有效提升性能。尽管目前最先进的方法通常采用生成式验证器从候选池中筛选最佳解决方案，但这种方法计算成本高昂，限制了其实际应用。本研究转向更注重预算的判别式验证方法。虽然孤立地看，判别式验证器性能可能不如生成式验证器，但当与自我一致性相结合时，在测试时间缩放机制中展现出强大的效率和效果。在固定计算预算下，这种混合方法显著超越了最先进的生成式验证方法，在AIME2025上提高了高达15.3%的准确率。本研究发现，对于实际应用，预算意识的判别式验证不仅是自我一致性的“免费”升级，而且是成本高昂的生成技术的更高效替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间缩放策略对于提高大规模语言模型在复杂推理任务中的性能至关重要。</li>
<li>生成式验证器虽然性能优越，但计算成本高昂，限制了实际应用。</li>
<li>判别式验证器作为一种更预算意识的策略被提出。</li>
<li>判别式验证器与自我一致性相结合的混合方法在测试时间缩放中展现出强大的效率和效果。</li>
<li>在固定计算预算下，混合方法显著超越了生成式验证方法，提高了AIME2025上的准确率。</li>
<li>预算意识的判别式验证不仅是自我一致性的升级，而且相对成本高昂的生成技术更为高效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2422b8d3c9fb0296a9af580c5dc0c630~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728512&auth_key=1760728512-0-0-48d16a9128f5a0e21a9c77ab693e64f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-037fd5225188d1c30ab2a69f90747d2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728519&auth_key=1760728519-0-0-2ab9a357bfa5102d92295debe53c647f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-091268d32be6efcea5cbf0ee89e5cacf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728526&auth_key=1760728526-0-0-4734d882ed7c891c9f76378ee36d6489&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-607ce8bf9c360c76430dfcdbfb619684~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728533&auth_key=1760728533-0-0-7bfb124d82b974d9004bdd960980d698&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e97c6cec93a518187b5df1380ec8a8c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728539&auth_key=1760728539-0-0-83e8c648e6a6378d9612493b6385d4fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Multimodal-LLM-Descriptions-of-Activity-for-Explainable-Semi-Supervised-Video-Anomaly-Detection"><a href="#Leveraging-Multimodal-LLM-Descriptions-of-Activity-for-Explainable-Semi-Supervised-Video-Anomaly-Detection" class="headerlink" title="Leveraging Multimodal LLM Descriptions of Activity for Explainable   Semi-Supervised Video Anomaly Detection"></a>Leveraging Multimodal LLM Descriptions of Activity for Explainable   Semi-Supervised Video Anomaly Detection</h2><p><strong>Authors:Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz</strong></p>
<p>Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies. </p>
<blockquote>
<p>现有的半监督视频异常检测（VAD）方法在检测涉及对象交互的复杂异常时经常遇到困难，并且通常缺乏可解释性。为了克服这些局限性，我们提出了一种利用多模态大语言模型（MLLMs）的新型VAD框架。与以前基于MLLM的方法直接在帧级别进行异常判断不同，我们的方法侧重于提取和解释对象活动和随时间变化的交互。我们通过使用MLLM查询不同时刻的对象对的视觉输入，从标准视频中生成活动和交互的文本描述。这些文本描述作为视频中对象活动和交互的高级表示。它们在测试时通过比较标准训练视频中的文本描述来检测异常。我们的方法本质上提供了可解释性，并且可以与传统VAD方法结合使用，以进一步增强其可解释性。在基准数据集上的大量实验表明，我们的方法不仅有效地检测基于交互的异常，而且在没有交互异常的数据集上也达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14896v1">PDF</a> </p>
<p><strong>Summary</strong>：针对现有半监督视频异常检测（VAD）方法在检测涉及对象交互的复杂异常时面临的挑战及其缺乏可解释性的问题，我们提出了一种新的利用多模态大型语言模型（MLLMs）的VAD框架。该方法通过查询MLLM以获取对象活动的文本描述，并利用这些描述来检测异常。这种方法提高了异常检测的准确性和可解释性，特别是在检测复杂交互异常方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>现有半监督视频异常检测方法在检测涉及对象交互的复杂异常时存在局限性，缺乏可解释性。</li>
<li>提出了一种新的利用多模态大型语言模型（MLLMs）的VAD框架。</li>
<li>该方法通过查询MLLM获取对象活动的文本描述，这些描述作为视频中对象活动和交互的高级表示。</li>
<li>通过将测试时的描述与训练视频中发现的描述进行比较来检测异常。</li>
<li>该方法提供了内在的可解释性，并可与传统VAD方法结合，进一步提高其可解释性。</li>
<li>在基准数据集上的广泛实验表明，该方法在检测复杂交互异常方面表现出卓越性能，同时在无交互异常的数据集上也达到了最佳性能。</li>
<li>该框架有望改进现有的VAD方法，特别是在处理涉及对象交互的复杂异常时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14896">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1c1bcb30db1ab3ddcea8597756e759fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728547&auth_key=1760728547-0-0-ad3b679bf173de136ae951e33f3d2da5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aba016c47b20ddc8e91e59d60c7fcdac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728554&auth_key=1760728554-0-0-8e53a339bd46835241c388946d7cf2f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1c96e48c2bb8622bf84c83527001c8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728561&auth_key=1760728561-0-0-468c07bf74c478f7eca1785ab44cdec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17a1065474dcd508e8d3507c014e5164~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728568&auth_key=1760728568-0-0-bd48fccd793be65e52dd580284ad5a21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="You-May-Speak-Freely-Improving-the-Fine-Grained-Visual-Recognition-Capabilities-of-Multimodal-Large-Language-Models-with-Answer-Extraction"><a href="#You-May-Speak-Freely-Improving-the-Fine-Grained-Visual-Recognition-Capabilities-of-Multimodal-Large-Language-Models-with-Answer-Extraction" class="headerlink" title="You May Speak Freely: Improving the Fine-Grained Visual Recognition   Capabilities of Multimodal Large Language Models with Answer Extraction"></a>You May Speak Freely: Improving the Fine-Grained Visual Recognition   Capabilities of Multimodal Large Language Models with Answer Extraction</h2><p><strong>Authors:Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn</strong></p>
<p>Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don’t consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language. </p>
<blockquote>
<p>尽管由于多模态大型语言模型（MLLMs）的兴起，零样本视觉分类重新引起了人们的兴趣，但评估自回归模型的自由形式响应的问题仍然是一个持续存在的挑战。现有的大多数工作都专注于纯语言任务，或者不考虑超过5种选择的多个选择题（MCQs），而这两种能力对于解决细粒度视觉分类（FGVC）中的任务都是至关重要的，因为FGVC中的选择计数通常在数百到数千之间，并且各个选择之间高度相关。此外，在这种高度多选择的MCQ环境中，尚不清楚如何将LLM选择提取扩展到基于检索的问题，在基于检索的问题中，在计算选择集上的概率是非常计算密集型的。在这项工作中，我们研究了nlg2choice，这是一种简单的两阶段方法，首先向MLLM提出任务相关的开放式问题，几乎没有约束，然后使用纯文本约束解码来预测最可能的答案。在检索设置中，我们采用早期停止方法计算约束响应选择该答案的概率，以显著提高吞吐量。我们的结果表明，在细粒度视觉数据集的分类和检索评估中，该方法均有所提高，并证明这种性能在各种LLM用户以自然语言执行任务的方式中都能保持。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14885v1">PDF</a> Accepted to WACV26. 12 pages, 8 tables, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了多模态大型语言模型（MLLMs）在零样本视觉分类中的挑战，特别是在细粒度视觉分类（FGVC）中的自由形式响应评价问题。现有的工作多聚焦于语言任务或仅限于五选一的选择题场景，对于多选项场景的应用有限。为此，本文提出了一种名为nlg2choice的两阶段方法，先在无约束情况下向MLLM提出问题，再利用文本约束解码预测最可能的选项。在检索场景下，采用早期停止法计算约束响应的概率，提高运行效率。实验结果显示，该方法在七个细粒度视觉数据集上的分类和检索任务中表现出优越性，并能够适应多种不同任务需求的语言表达方式。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是该文本的关键要点：</p>
<ul>
<li>多模态大型语言模型（MLLMs）在零样本视觉分类中面临挑战，特别是在细粒度视觉分类（FGVC）中如何评估自由形式响应的问题。</li>
<li>现有工作多聚焦于语言任务或局限于五选一选择题场景，缺乏在多选项场景下的应用。</li>
<li>提出了一种名为nlg2choice的两阶段方法，先对MLLM进行无约束问题的提问，然后通过文本约束解码预测最可能的选项。</li>
<li>在检索场景下，使用早期停止法计算约束响应的概率，显著提高运行效率。</li>
<li>实验结果显示该方法在七个细粒度视觉数据集上的分类和检索任务表现优越。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3cb3645d7df221febbbd9e9281ea6d80~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728575&auth_key=1760728575-0-0-10a295fc6a35089dd44a5a6fcd7d6d47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21f82e40facc5fb8cf0e426afadacd15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728582&auth_key=1760728582-0-0-e1db0d72181ebb4a4d38ca07106e77c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdd35d319ad32a2ec59c4c8cfddd69c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728589&auth_key=1760728589-0-0-773c1ddd15f8a13c4fa4da56d0e45178&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91bb271263f798690fa14158df96cafc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728596&auth_key=1760728596-0-0-faecb938cd2d49e82659be7f1f88494f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ed2f2f332b1cb11710be6dca9aa42d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728604&auth_key=1760728604-0-0-99254630a06daeef8ea6087018d16cd8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Multimodal-Large-Language-Models-for-Face-Recognition"><a href="#Benchmarking-Multimodal-Large-Language-Models-for-Face-Recognition" class="headerlink" title="Benchmarking Multimodal Large Language Models for Face Recognition"></a>Benchmarking Multimodal Large Language Models for Face Recognition</h2><p><strong>Authors:Hatef Otroshi Shahreza, Sébastien Marcel</strong></p>
<p>Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在各种视觉和语言任务中取得了显著的性能。然而，它们在人脸识别方面的潜力仍未得到充分利用。尤其是开源的MLLMs的性能需要通过标准协议在人脸识别任务上与现有的人脸识别模型进行比较和评估。在这项工作中，我们在多个人脸识别数据集上，包括LFW、CALFW、CPLFW、CFP、AgeDB和RFW等数据集上进行了最新的MLLMs人脸识别系统基准测试。实验结果表明，虽然MLLMs能够捕捉到丰富的人脸语义线索，对于人脸识别任务非常有用，但在零样本应用的精确识别场景中，它们落后于专业模型。这一基准测试为基于MLLM的人脸识别技术提供了发展基础，为设计下一代更高精度和泛化能力更强的模型提供了见解。我们的基准测试源代码已在项目页面公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14866v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMs在多种视觉和语言任务中表现出卓越性能，但在人脸识别领域的应用潜力尚未得到充分探索。本研究对最先进的MLLMs进行了系统评估，涉及多个人脸识别数据集，包括LFW、CALFW等。实验结果显示，虽然MLLMs捕获了丰富的人脸语义线索，但在零样本应用的高精度识别场景中，仍落后于专用模型。本评估为推进MLLM在人脸识别中的应用提供了基础，并为下一代模型的更高精度和泛化能力设计提供了启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在多种视觉和语言任务中表现出卓越性能。</li>
<li>人脸识别领域对MLLMs的应用潜力尚未充分探索。</li>
<li>MLLMs在人脸识别数据集上的系统评估是必要的。</li>
<li>MLLMs捕获了丰富的人脸语义线索。</li>
<li>在零样本应用的高精度识别场景中，MLLMs的表现仍落后于专用模型。</li>
<li>本评估为推进MLLM在人脸识别中的应用提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14866">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ca469d5c200d3b3af98196d5f8919c38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728611&auth_key=1760728611-0-0-38b144a3e0f7089f697157eb0879d8f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a8edfc4f3f1e345993ee3c6c35cc113~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728619&auth_key=1760728619-0-0-950cc228fe7503bb75f63c74f3b131b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d35e40af7c67c6c5e1c3adc69e847f0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728625&auth_key=1760728625-0-0-60e37efe363f451772939477826ab5db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-625ea576d78200b97dc8da0507aace2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728632&auth_key=1760728632-0-0-b2876a3bc02537b62f4dab292c5a7943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning"><a href="#RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning" class="headerlink" title="RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning"></a>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</h2><p><strong>Authors:Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li</strong></p>
<p>Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model’s shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark. </p>
<blockquote>
<p>提高实体代理的推理能力对于机器人在长期视角操控任务中成功完成复杂的人类指令至关重要。尽管基于监督微调（SFT）的大型语言模型和视觉语言模型在规划任务中取得了成功，但它们仍面临着在复杂的真实环境中执行长期视角操控任务的挑战，因为它们有限的常识和推理能力。考虑到通过监督微调将通用视觉语言模型与机器人规划任务对齐存在通用性差和物理理解不足的问题，我们提出了RoboGPT-R1，这是一个用于实体规划的两阶段微调框架。在这个框架中，监督训练通过专家序列获得基础知识，然后通过强化学习来解决模型在视觉空间理解和推理方面的不足。为了实现多步推理任务中的物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，该函数同时考虑了长期性能和环境中的动作约束。在EmbodiedBench基准测试上，经过Qwen2. -VL上训练的推理模型显著优于更大规模的GPT-4o-mini模型，准确率提高了21.33%，并且超过了其他在Qwen2.VL上训练的工作，准确率提高了20.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>机器人完成复杂人类指令的长期操作任务中，提高其推理能力至关重要。虽然基于监督微调的大型语言模型和视觉语言模型在规划任务中取得了成功，但它们仍面临在复杂现实环境中执行长期操作任务的挑战，因为它们的通用常识和推理能力有限。为此，我们提出了RoboGPT-R1，这是一个两阶段的微调框架，用于实体规划。该框架首先通过专家序列进行有监督的训练以获取基础知识，然后通过强化学习来解决模型在视觉空间理解和推理方面的不足。为实现多步推理任务中的物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，该函数同时考虑长期性能和环境中的动作约束。在EmbodiedBench基准测试中，该推理模型的性能显著优于较大规模的GPT-4o-mini模型，提高了21.33%，并且超越了其他在Qwen2.5-VL-7B上训练的工作，提高了20.33%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提高机器人的推理能力是完成复杂人类指令的长期操作任务的关键。</li>
<li>基于监督微调的大型语言模型和视觉语言模型在规划任务中面临挑战，尤其是处理复杂的现实世界环境和长期操作任务。</li>
<li>提出RoboGPT-R1框架，通过两阶段微调来提高实体规划能力，包括有监督的训练和强化学习。</li>
<li>有监督的训练阶段通过专家序列获取基础知识。</li>
<li>强化学习阶段解决模型在视觉空间理解和推理方面的不足。</li>
<li>设计基于规则的奖励函数以实现多步推理任务中的物理理解和动作序列一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a099ba6074f47e3e889319f629068932~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728639&auth_key=1760728639-0-0-ac6b1196037f26fd7d8c13994397584c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e69d26203f68eba4ccdf3dc77eb805db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728647&auth_key=1760728647-0-0-bfc9a71487bcc74ce680e87f62029848&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7422f8a710d4ef94e3e57bf037190e52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728654&auth_key=1760728654-0-0-313e76f70db66ca11bd35a172ebcc466&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Evaluating-Reducing-Deceptive-Dialogue-From-Language-Models-with-Multi-turn-RL"><a href="#Evaluating-Reducing-Deceptive-Dialogue-From-Language-Models-with-Multi-turn-RL" class="headerlink" title="Evaluating &amp; Reducing Deceptive Dialogue From Language Models with   Multi-turn RL"></a>Evaluating &amp; Reducing Deceptive Dialogue From Language Models with   Multi-turn RL</h2><p><strong>Authors:Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine</strong></p>
<p>Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models. </p>
<blockquote>
<p>大型语言模型（LLM）在全球范围内与数以百万计的用户进行交互，广泛应用于客户服务、教育和医疗保健等领域。然而，无论是故意还是无意中，它们产生欺骗输出的能力引发了严重的安全问题。LLM行为的不可预测性，以及对幻觉、误导信息和用户操纵的防护措施不足，使得它们的滥用成为一个严肃且真实的威胁。在本文中，我们研究了LLM在对话中产生欺骗的程度，并提出了信念错位度量标准来量化欺骗行为。我们在四种不同的对话场景中评估欺骗行为，使用了五种成熟的欺骗检测指标和我们提出的指标。我们的研究发现，这一新颖的欺骗度量标准与人类判断的相关性比我们测试的所有现有指标都更高。此外，我们对八种最先进的模型的基准测试表明，即使在看似无害的目标提示下，LLM在大约26%的对话轮次中也会自然表现出欺骗行为。当被提示进行欺骗时，LLM有能力将欺骗性提高高达基线水平的31%。出乎意料的是，使用RLHF（确保广泛部署的LLM安全的主要方法）训练的模型仍然平均以43%的速率表现出欺骗行为。鉴于对话中的欺骗行为是在交互历史中发展起来的，对其的有效评估和缓解需要超越单句分析。我们引入了一种多轮强化学习方法对LLM进行微调，以减少欺骗行为，与其他指令微调模型相比，这导致了77.6%的减少。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14318v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在客户支持、教育和医疗等领域与数百万全球用户互动。然而，它们有意无意产生欺骗输出的能力引发了重大安全担忧。LLM行为的不可预测性，以及对幻觉、误导信息和用户操纵的防护不足，使得其滥用成为一个严重的现实问题。本文调查了LLM在对话中进行欺骗的程度，并提出了信念错位度量来衡量欺骗。我们在四种不同的对话场景中评估了欺骗行为，使用了五种现有的欺骗检测指标和我们提出的指标。研究发现，新的欺骗度量指标与人类判断的相关性更高。此外，我们对八种最先进的模型进行了评估，表明LLM在对话回合中约有26%表现出自然欺骗行为，即使在看似无害的目标提示下也会如此。当被提示进行欺骗时，LLM的欺骗性可能增加高达31%。出乎意料的是，使用RLHF（确保广泛部署的LLM安全的主要方法）训练的模型平均欺骗率为43%。由于对话中的欺骗行为是随着互动历史而发展的，其有效评估和缓解需要超越单发言的分析。我们引入了一种多轮强化学习方法来微调LLM以减少欺骗行为，与其他指令调整模型相比，这导致了77.6%的减少。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM在与全球用户的互动中表现出欺骗行为，这引发了重大安全担忧。</li>
<li>LLM行为的不可预测性使得其滥用成为一个严重的现实问题。</li>
<li>提出了一种新的信念错位度量指标，更准确地衡量LLM的欺骗行为。</li>
<li>在四种对话场景中评估了欺骗行为，发现现有指标与人类判断的相关性有待提高。</li>
<li>八种最先进的LLM模型自然表现出欺骗行为，占比约26%。</li>
<li>当被提示进行欺骗时，LLM的欺骗性可能增加高达31%。</li>
<li>使用RLHF训练的模型仍存在欺骗问题，平均欺骗率为43%。同时发现多轮强化学习可以有效减少LLM的欺骗行为，达到较高的减少比例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14318">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3ec41fc3013fd8040b48e9a34d479001~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728661&auth_key=1760728661-0-0-963b57c5f05b62335daaa858c2e50b90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2d9e93aadf21cbaf3b121d8df650649~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728669&auth_key=1760728669-0-0-a50e110947c25cfdacaf5af8249ba096&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-69f68596c33e88b39e06d6bfdcc81536~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728675&auth_key=1760728675-0-0-be1242f22e13baeed9caae2e9f690aac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Agentic-Data-for-Web-Agents-with-Progressive-Difficulty-Enhancement-Mechanisms"><a href="#Synthesizing-Agentic-Data-for-Web-Agents-with-Progressive-Difficulty-Enhancement-Mechanisms" class="headerlink" title="Synthesizing Agentic Data for Web Agents with Progressive Difficulty   Enhancement Mechanisms"></a>Synthesizing Agentic Data for Web Agents with Progressive Difficulty   Enhancement Mechanisms</h2><p><strong>Authors:Shrey Pandit, Xuan-Phi Nguyen, Yifei Ming, Austin Xu, Jiayu Wang, Caiming Xiong, Shafiq Joty</strong></p>
<p>Web-based ‘deep research’ agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors. </p>
<blockquote>
<p>基于网络的”深度研究”代理旨在通过与在线工具进行长期互动来解决复杂的问答任务。这些任务仍然具有挑战性，因为基础语言模型通常没有经过长期推理和探索的优化。早期的工作已经提出了构建指令调整数据集的工作流程，通常利用知识图谱。然而，这些方法通常缺乏对难度和质量的精细控制，产生的合成数据无法捕捉长期推理所需的复杂性。此外，许多研究通过比较在不同优化配方下训练的模型来混淆数据和训练效果，很难孤立地评估数据本身的有效性。我们引入了一种双管齐下的数据合成管道，通过逐步增加任务复杂性来生成问答对，直到前沿基线网络代理失败。基线代理在这个过程中扮演了多个角色：尝试问题、验证事实、检查替代答案和执行过滤。为了评估我们的合成方法的有效性，我们采用了基于从强大网络代理中提炼出来的受控训练设置。在多个网络基准测试上的实验表明，尽管我们的数据集较小，但它能够训练出比现有数据集更有效的网络代理。特别是，我们的数据在工具使用行为方面表现出两倍的多样性，使得在此数据上训练的模型能够取得更强的性能，同时避免重复的工具调用行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13913v1">PDF</a> Preprint. ICLR 26 submission</p>
<p><strong>摘要</strong><br>    本研究介绍了一种两阶段数据合成流程，旨在生成针对基于Web的深度问答任务的数据集。该研究通过逐步提高任务复杂度来生成问答对，直至基线Web代理无法完成任务。基线代理在此过程中扮演多重角色，包括尝试问题、验证事实性、检查替代答案和执行过滤。实验表明，尽管数据量较小，但本研究的数据集能够有效训练Web代理，使其表现优于现有数据集。其数据集在工具使用动作方面表现出两倍的多样性，可训练模型表现出更强性能并避免重复性工具调用行为。</p>
<p><strong>要点摘要</strong></p>
<ol>
<li>研究提出了一种新的数据合成流程，针对基于Web的深度问答任务生成数据集。</li>
<li>数据集生成采用逐步提高任务复杂度的策略，直至基线Web代理无法完成。</li>
<li>基线代理在数据生成过程中扮演多重角色，包括尝试问题、验证事实性和检查替代答案等。</li>
<li>采用基于强Web代理蒸馏的控制训练设置来评估数据合成的有效性。</li>
<li>在多个基于Web的基准测试上进行的实验表明，该研究的数据集在训练有效Web代理方面表现优异。</li>
<li>与现有数据集相比，该研究的数据集在工具使用动作的多样性上表现出两倍的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5068e536e9c0603cd35b51a3be1d3b41~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728682&auth_key=1760728682-0-0-a9074715a8979febbf2e96c156db63c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fcab1798ac445b2695e171d66068ddb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728690&auth_key=1760728690-0-0-97a0025c52c907e080096cde6d6787c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57bcf3e7be95900bece3f2587ca53c15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728696&auth_key=1760728696-0-0-a8fdf19f54d219ddcbbdeb778fe27cae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14c04f5fd7f5cf2847c8f299046e0e74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728703&auth_key=1760728703-0-0-39a9a6157cf7d2fb173321972e2974ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Attribution-Quality-in-AI-Generated-Content-Benchmarking-Style-Embeddings-and-LLM-Judges"><a href="#Attribution-Quality-in-AI-Generated-Content-Benchmarking-Style-Embeddings-and-LLM-Judges" class="headerlink" title="Attribution Quality in AI-Generated Content:Benchmarking Style   Embeddings and LLM Judges"></a>Attribution Quality in AI-Generated Content:Benchmarking Style   Embeddings and LLM Judges</h2><p><strong>Authors:Misam Abbas</strong></p>
<p>Attributing authorship in the era of large language models (LLMs) is increasingly challenging as machine-generated prose rivals human writing. We benchmark two complementary attribution mechanisms , fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an open dataset of 600 balanced instances spanning six domains (academic, news, fiction, blogs, spoken transcripts, and TV&#x2F;movie scripts). Each instance contains a human prompt with both a gold continuation and an LLM-generated continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs. 68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA continuations (85 pct vs. 81 pct) but the results are not statistically significant. Crucially, the LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity, whereas embeddings dominate in spoken and scripted dialogue, reflecting structural strengths. These complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies. To support reproducibility we provide code on GitHub and derived data on Hugging Face under the MIT license. This open framework provides a reproducible benchmark for attribution quality assessment in AI-generated content, along with a review of related literature influencing this work. </p>
<blockquote>
<p>在大型语言模型（LLM）时代，对作者归属的认定越来越具有挑战性，因为机器生成的散文与人类的写作能力不相上下。我们在Human AI Parallel Corpus上基准测试了两种互补的归属机制，即固定的风格嵌入和一个经过指令训练的LLM判断器（GPT-4o）。这是一个包含600个平衡实例的开放数据集，涵盖六个领域（学术、新闻、小说、博客、口语转录和电视&#x2F;电影剧本）。每个实例都包含一个人类提示，其中包括黄金延续和来自GPT-4o或LLaMA-70B-Instruct的LLM生成延续。风格嵌入基线在GPT延续上达到了更高的总体准确率（82%对68%）。LLM判断器在LLaMA延续上略胜于风格嵌入（85%对81%），但结果不具有统计学意义。关键的是，LLM判断器在小说和学术散文方面的表现显著优于其他方法，显示出语义敏感性，而嵌入法则主导口语和剧本对话，反映了其结构性优势。这些互补模式强调了归属是一个多维问题，需要混合策略来解决。为了支持可重复性，我们在GitHub上提供了代码，并在Hugging Face上提供了衍生数据，遵循MIT许可证。这一开放框架为AI生成内容的归属质量评估提供了可重复性的基准测试，并对影响这项工作的相关文献进行了评述。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13898v1">PDF</a> Accepted for publication at the 2025 IEEE ICDM Workshop on “Grounding   Documents with Reasoning, Agents, Retrieval, and Attribution”. This is author   submitted version. Not yet published</p>
<p><strong>Summary</strong></p>
<p>本文探讨了在大语言模型时代如何归属作者身份的难题。文章在Human AI Parallel Corpus数据集上评估了两种互补的归属机制——固定风格嵌入和指令训练的语言模型判断器GPT-4o的表现。结果显示，风格嵌入在GPT续作中的总体准确率更高，而LLM判断器在LLaMA续作上的表现略好，但结果无显著差异。值得注意的是，LLM判断器在小说和学术散文中的表现更佳，显示出了语义敏感性，而风格嵌入则在口语和剧本对话中更占优势，反映了其结构性优势。总体而言，归属问题是一个多维度的问题，需要混合策略来解决。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型时代的作者身份归属成为一大挑战，机器生成文本与人类写作难以区分。</li>
<li>风格嵌入和LLM判断器是两种互补的归属机制，在Human AI Parallel Corpus数据集上进行评估。</li>
<li>风格嵌入在GPT续作中的总体准确率更高，达到82%，而LLM判断器在LLaMA续作上的准确率略高，但结果不具有统计学意义。</li>
<li>LLM判断器在小说和学术散文中表现较好，显示出语义敏感性；而风格嵌入在口语和剧本对话中更具优势，反映其结构性特点。</li>
<li>归属问题是一个多维问题，需要混合策略来解决。</li>
<li>文章提供了GitHub代码和Hugging Face下的衍生数据，以支持研究的可重复性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13898">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-680bd3f170a5cce1e220887db0b5decf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728710&auth_key=1760728710-0-0-48b29400b9af6e984d88fd5618dbe088&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a59f5f7e1e606217568598e5ade9b65b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728717&auth_key=1760728717-0-0-39f1a11e4bca9207bb22429a04c66a50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93b0e9f4476815fa62d0e9f28bb99230~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728723&auth_key=1760728723-0-0-dc74f2ac5cf4d2c14f50c310c4705718&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-584e299d28ee48c18a21ec114e20b105~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728730&auth_key=1760728730-0-0-7f37b60042f6dcdf4824e0b263cab419&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89a0a6b6321b6279b1aea4d27c680978~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728737&auth_key=1760728737-0-0-1bdf1575d5a7846188d50b973d212db2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e37402b8f7c3fa74c381e5f6a26dbccd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728743&auth_key=1760728743-0-0-b1c59b393a286511933e7cb773c7a93f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Confidence-as-a-Reward-Transforming-LLMs-into-Reward-Models"><a href="#Confidence-as-a-Reward-Transforming-LLMs-into-Reward-Models" class="headerlink" title="Confidence as a Reward: Transforming LLMs into Reward Models"></a>Confidence as a Reward: Transforming LLMs into Reward Models</h2><p><strong>Authors:He Du, Bowen Li, Chengxing Xie, Chang Gao, Kai Chen, Dacheng Tao</strong></p>
<p>Reward models can significantly enhance the reasoning capabilities of large language models (LLMs), but they typically require extensive curated data and costly training. To mitigate these challenges, training-free approaches such as LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate responses, achieving promising results. Recent works have also indicated that model confidence can serve effectively as a reward metric, distinguishing between chain-of-thought (CoT) and non-CoT paths. However, the concept of using confidence as a reward has not been comprehensively studied. In this work, we systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful training-free method that utilizes token-level confidence in the model’s final answers as a proxy for reward, especially suitable for close-ended tasks. Through extensive experiments on mathematical reasoning tasks, we demonstrate that CRew outperforms existing training-free reward approaches on the MATH500 and RewardMATH benchmarks, and even surpasses most trained reward models. We further identify a strong correlation between CRew scores and the actual reasoning performance of the model. Additionally, we find that CRew can effectively filter high-quality training data. Building upon these insights, we propose CRew-DPO, a training strategy that constructs preference data from confidence scores combined with correctness signals. Finetuning with CRew-DPO further enhances the model’s judging capabilities and consistently outperforms existing self-training methods. </p>
<blockquote>
<p>奖励模型能够显著提高大语言模型（LLM）的推理能力，但它们通常需要大量精心策划的数据和昂贵的训练成本。为了缓解这些挑战，无需训练的方法，如LLM-as-a-Judge，利用LLM的内在推理能力来评估响应，并取得了令人鼓舞的结果。最近的研究还表明，模型信心可以作为奖励指标，有效区分链式思维（CoT）和非CoT路径。然而，使用信心作为奖励的概念尚未得到全面研究。在这项工作中，我们系统地研究了信心作为奖励（CRew），这是一种简单而强大的无需训练的方法，利用模型最终答案的令牌级信心作为奖励的代理，尤其适用于封闭任务。通过在数学推理任务上的大量实验，我们证明了CRew在MATH500和RewardMATH基准测试上优于现有的无需训练的奖励方法，甚至超越了大多数经过训练的奖励模型。我们进一步发现CRew分数与模型的实际推理性能之间存在强烈的相关性。此外，我们发现CRew可以有效地过滤高质量的训练数据。基于这些见解，我们提出了CRew-DPO，这是一种结合信心分数和正确性信号构建偏好数据的训练策略。使用CRew-DPO进行微调进一步增强了模型的判断能力，并始终优于现有的自训练方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>奖励模型能显著提升大语言模型的推理能力，但通常需要大量定制数据和昂贵的训练成本。为缓解这些问题，出现了一种无需训练的方法，如利用LLM作为评判者来评估回应，取得了令人鼓舞的结果。本研究系统地探讨了使用模型最终答案中的令牌级别信心作为奖励的代理的“信心即奖励”（CRew）方法，特别适合封闭任务。通过在数学推理任务上的大量实验，我们证明了CRew在MATH500和RewardMATH基准测试上优于现有的无需训练的奖励方法，甚至超越了大多数经过训练奖励模型。此外，CRew与模型的实际推理性能之间存在强烈的相关性，并能有效地过滤出高质量的训练数据。基于这些见解，我们提出了CRew-DPO训练策略，该策略结合信心评分和正确性信号构建偏好数据。通过CRew-DPO进行微调，可进一步提高模型的判断能力，并始终优于现有的自训练方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励模型能增强LLM的推理能力，但需大量定制数据和昂贵训练。</li>
<li>LLM-as-a-Judge等无需训练的方法利用LLM的内在推理能力进行评估，取得良好效果。</li>
<li>本研究提出“信心即奖励”（CRew）方法，利用模型最终答案的令牌级别信心作为奖励代理，适用于封闭任务。</li>
<li>CRew在数学推理任务上的实验表现优于其他无需训练的奖励方法，甚至超越部分训练奖励模型。</li>
<li>CRew与模型实际推理性能存在强烈相关性，并能有效过滤高质量训练数据。</li>
<li>提出CRew-DPO训练策略，结合信心评分和正确性信号构建偏好数据，进一步提高模型判断能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13501">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2a97af3317a01d2f7d2f8fd2c8209c28~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728750&auth_key=1760728750-0-0-46c1e5e3f7feea3f3fb6fa07b826608d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df826254f85fc571e5ccc5fdc61d74c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728757&auth_key=1760728757-0-0-23818898bd3e4656bbdecccdffb9bb5d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0726b2960118f732900d5d8ba2f0401~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728764&auth_key=1760728764-0-0-3e8ea13b20a4e810b34de9233034ae80&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Who-Speaks-for-the-Trigger-Dynamic-Expert-Routing-in-Backdoored-Mixture-of-Experts-Transformers"><a href="#Who-Speaks-for-the-Trigger-Dynamic-Expert-Routing-in-Backdoored-Mixture-of-Experts-Transformers" class="headerlink" title="Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored   Mixture-of-Experts Transformers"></a>Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored   Mixture-of-Experts Transformers</h2><p><strong>Authors:Xin Zhao, Xiaojun Chen, Bingshan Liu, Haoyu Gao, Zhendong Zhao, Yilong Chen</strong></p>
<p>Large language models (LLMs) with Mixture-of-Experts (MoE) architectures achieve impressive performance and efficiency by dynamically routing inputs to specialized subnetworks, known as experts. However, this sparse routing mechanism inherently exhibits task preferences due to expert specialization, introducing a new and underexplored vulnerability to backdoor attacks. In this work, we investigate the feasibility and effectiveness of injecting backdoors into MoE-based LLMs by exploiting their inherent expert routing preferences. We thus propose BadSwitch, a novel backdoor framework that integrates task-coupled dynamic trigger optimization with a sensitivity-guided Top-S expert tracing mechanism. Our approach jointly optimizes trigger embeddings during pretraining while identifying S most sensitive experts, subsequently constraining the Top-K gating mechanism to these targeted experts. Unlike traditional backdoor attacks that rely on superficial data poisoning or model editing, BadSwitch primarily embeds malicious triggers into expert routing paths with strong task affinity, enabling precise and stealthy model manipulation. Through comprehensive evaluations across three prominent MoE architectures (Switch Transformer, QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack pre-trained models with up to 100% success rate (ASR) while maintaining the highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch exhibits strong resilience against both text-level and model-level defense mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our analysis of expert activation patterns reveals fundamental insights into MoE vulnerabilities. We anticipate this work will expose security risks in MoE systems and contribute to advancing AI safety. </p>
<blockquote>
<p>基于大型语言模型（LLM）和专家混合（MoE）架构的模型通过动态将输入路由到专业子网（称为专家）来实现令人印象深刻的性能和效率。然而，这种稀疏路由机制由于专家的专业化而天生地表现出任务偏好，引入了一种新的且尚未被充分探索的后门攻击的脆弱性。在这项工作中，我们研究了通过在基于MoE的大型语言模型中利用其固有的专家路由偏好来注入后门的能力。因此，我们提出了BadSwitch，这是一种新型后门框架，它将任务耦合的动态触发优化与敏感性引导的Top-S专家跟踪机制相结合。我们的方法能在预训练过程中优化触发嵌入的同时，确定最敏感的专家，然后将Top-K网关机制限制在这些目标专家上。与传统的依赖表面数据污染或模型编辑的后门攻击不同，BadSwitch主要将恶意触发嵌入到具有强任务亲和力的专家路由路径中，从而实现精确和隐蔽的模型操纵。通过全面评估三种主流的MoE架构（Switch Transformer、QwenMoE和DeepSeekMoE），我们证明了BadSwitch可以有效地接管预训练模型，成功率高达100%（ASR），同时保持所有基线中的最高清洁精度（ACC）。此外，BadSwitch对文本级别和模型级别的防御机制表现出强大的韧性，在AGNews数据集上实现94.07%的ASR和87.18%的ACC。我们对专家激活模式的分析揭示了MoE漏洞的根本见解。我们预计这项工作将暴露MoE系统的安全风险，并为提高人工智能安全性做出贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过动态地将输入路由到专门化的子网络（即专家）实现性能与效率的提升，这种结构因其内在的特性暴露了新的安全漏洞。本研究探讨了利用专家路由偏好向基于MoE的LLM注入后门的可能性。我们提出了一种名为BadSwitch的新型后门框架，它通过任务耦合的动态触发优化和敏感性导向的Top-S专家追踪机制来操作模型。BadSwitch优化了预训练过程中的触发嵌入，同时识别最敏感专家并约束Top-K网关机制以针对目标专家进行路由选择。与传统的依赖表面数据污染或模型编辑的后门攻击不同，BadSwitch主要通过将恶意触发嵌入到具有强任务亲和力的专家路由路径中实现精确且隐蔽的模型操控。通过三个主流MoE架构的综合评估，证明了BadSwitch的高效性，其成功率高达百分之百，同时保持高清洁精度。此外，BadSwitch对文本级别和模型级别的防御机制表现出强大的韧性。本研究揭示了MoE架构的潜在风险，并为推动AI安全做出贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）通过动态路由机制实现性能提升，但这种机制具有任务偏好性，容易受到后门攻击的影响。</li>
<li>提出BadSwitch框架，一种针对基于MoE架构的LLM的后门攻击方法。</li>
<li>BadSwitch优化了触发嵌入的预训练过程，同时识别并约束最敏感专家进行路由选择。</li>
<li>BadSwitch不同于传统攻击方式，通过嵌入恶意触发到专家路由路径操纵模型，更加精确且隐蔽。</li>
<li>BadSwitch在多种MoE架构下表现出高效性，成功率高达百分之百，同时保持高清洁精度。</li>
<li>BadSwitch对多种防御机制展现出强大的韧性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13462">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e17863158d3f624a1b9da9de65abe9ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728771&auth_key=1760728771-0-0-764e47c4deba8da17845de89954023b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86a4f7b9e1ac871165b77e74bdc89365~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728781&auth_key=1760728781-0-0-83ec63f441782e65d8ff86fe0247270e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5761275bb507be4c93ff7393eb4014b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728788&auth_key=1760728788-0-0-9de16b46d15b5387da7720c040532f5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f68fd4385158fa879d2700a3c784c8c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728794&auth_key=1760728794-0-0-bbbc1eaa829b22bc58bf5f1209236e12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Visual-Interestingness-Decoded-How-GPT-4o-Mirrors-Human-Interests"><a href="#Visual-Interestingness-Decoded-How-GPT-4o-Mirrors-Human-Interests" class="headerlink" title="Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests"></a>Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests</h2><p><strong>Authors:Fitim Abdullahu, Helmut Grabner</strong></p>
<p>Our daily life is highly influenced by what we consume and see. Attracting and holding one’s attention – the definition of (visual) interestingness – is essential. The rise of Large Multimodal Models (LMMs) trained on large-scale visual and textual data has demonstrated impressive capabilities. We explore these models’ potential to understand to what extent the concepts of visual interestingness are captured and examine the alignment between human assessments and GPT-4o’s, a leading LMM, predictions through comparative analysis. Our studies reveal partial alignment between humans and GPT-4o. It already captures the concept as best compared to state-of-the-art methods. Hence, this allows for the effective labeling of image pairs according to their (commonly) interestingness, which are used as training data to distill the knowledge into a learning-to-rank model. The insights pave the way for a deeper understanding of human interest. </p>
<blockquote>
<p>我们的日常生活受到我们所消费和所看到内容的高度影响。吸引并维持人们的注意力——（视觉）有趣性的定义——至关重要。经过大规模视觉和文本数据训练的大型多模态模型（LMMs）的崛起展现了令人印象深刻的能力。我们探索了这些模型在理解视觉有趣性概念方面的潜力，并通过对比分析，检验了人类对GPT-4o（一款领先的大型多模态模型）预测的评价的一致性。我们的研究表明，人类与GPT-4o之间存在部分一致性。与其他最先进的方法相比，它已经捕捉到了最佳的概念。因此，这可以更有效地根据它们的（普遍）有趣性对图像对进行标注，这些标注作为训练数据被提炼成一种学习排名模型。这些见解为人类兴趣的更深入理解铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13316v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>大型多模态模型（LMMs）通过大规模视觉和文本数据的训练展现出令人印象深刻的能力。研究探索了这些模型在理解视觉有趣性的程度方面的潜力，并通过对比分析揭示了人类评估和GPT-4o预测之间的部分一致性。研究结果表明，GPT-4o已经掌握了最佳的比较方法，可有效标注图像对根据它们的有趣程度，并可作为训练数据蒸馏知识到排名模型中。这为更深入地理解人类兴趣铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）通过大规模视觉和文本数据训练展现出强大的能力。</li>
<li>研究的目的是探索LMMs在理解视觉有趣性的程度方面的潜力。</li>
<li>通过对比分析，研究发现人类评估和GPT-4o的预测之间存在部分一致性。</li>
<li>GPT-4o在理解视觉有趣性的概念上相比其他最新技术具有优势。</li>
<li>GPT-4o能够有效地根据图像的有趣程度进行标注，这些标注可以作为训练数据用于学习排名模型。</li>
<li>这些见解为更深入地理解人类兴趣提供了机会。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13316">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ad1f410c5b5f856f6336f3f75bc5197e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728802&auth_key=1760728802-0-0-54664cec1ecad60a2da4f70a5ca51bb2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc266e406fb224cc6b693f62be0288d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728809&auth_key=1760728809-0-0-cba471aa01740ad1bdc47c01b459dd42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed464a71caa5722a94c778de86a3f13b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728815&auth_key=1760728815-0-0-8d69608a7cf4fdb9d29b8ae58cd2a93e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bcb9b634097412791485bbf41da4840a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728822&auth_key=1760728822-0-0-573d64b97cd88fbaedba5dcbbc086c21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2d6551da020b6b147b09df1d3717715~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728828&auth_key=1760728828-0-0-880eca82cec9043ef9c5b3f2d2b9ad75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c10e8cfcd27d577e68969baf7abd69f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728835&auth_key=1760728835-0-0-91f88414c4c88fbc8285123e6ad78a44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval"><a href="#Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval" class="headerlink" title="Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval"></a>Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval</h2><p><strong>Authors:Yingchen zhang, Ruqing zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv</strong></p>
<p>Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R. </p>
<blockquote>
<p>生成式检索（GR）是一种新兴范式，它利用大型语言模型（LLM）来自动回归生成与给定查询相关的文档标识符（docids）。早期的研究主要集中在利用LLM的生成能力来提高GR的性能，却忽视了它们的推理能力同样有助于此。这引发了一个关键问题：明确的推理能否有益于GR？为了探究这个问题，我们首先进行了一项初步研究，在该研究中，LLM会在执行受限的docid解码之前，被提示生成自由形式的思维链（CoT）推理。尽管这种方法优于标准的GR，但生成的推理往往过于冗长，且与docid空间不太对齐。这些局限性促使我们开发一种更适合GR的推理机制。因此，我们提出了Reason-for-Retrieval（R4R），这是一种增强型GR推理框架，它将自由形式的CoT推理转换为紧凑的结构化格式，并在检索过程中迭代地优化推理。R4R通过利用具备GR指令调优能力、具备推理能力的LLM来增强现有的GR方法。在推理时，R4R首先使用LLM生成初始结构化推理；然后，同一LLM在（i）使用选择的GR方法执行受限解码以产生候选docids与（ii）基于检索结果更新推理以提高下一轮效果之间交替进行。R4R不需要额外的模型或训练，而只需一个LLM即可同时充当推理生成器和检索器。在自然问题、MS MARCO和现实世界中的商品搜索基准测试上的大量实验验证了R4R的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13095v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的生成式检索（GR）正在成为一个新兴的研究领域。虽然已有研究侧重于利用LLM的生成能力来提升GR性能，但其推理能力同样具有潜力。本研究提出了一种名为Reason-for-Retrieval（R4R）的框架，该框架结合了生成式检索与推理能力，将自由形式的链式思维（CoT）推理转化为紧凑的结构化格式，并在检索过程中不断迭代优化推理。R4R不依赖额外的模型或训练，只需一个具有推理能力的LLM即可同时担任推理生成器和检索器。实验证明，R4R在自然问题、MS MARCO以及真实世界的物品搜索基准测试中均表现出良好的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式检索（GR）开始利用大型语言模型（LLM）。</li>
<li>现有研究主要关注LLM的生成能力，但推理能力同样重要。</li>
<li>提出Reason-for-Retrieval（R4R）框架，结合生成式检索与推理能力。</li>
<li>R4R将自由形式的链式思维（CoT）推理转化为紧凑的结构化格式。</li>
<li>R4R在检索过程中迭代优化推理。</li>
<li>R4R不需要额外的模型或训练，只依赖一个具有推理能力的LLM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9f65b0defe59ec3f4474392d16b1409a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728863&auth_key=1760728863-0-0-29cdfa866bf92741ae832734f2fa4d76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f7d3a2d2e9b59fa6c5809598e151579~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728870&auth_key=1760728870-0-0-d11febe66cfa0eb72565b07287c5c570&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54909d62486394003d7ba95199ba7376~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728899&auth_key=1760728899-0-0-02a2daeb5b7d823b050c3758a328dcd4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-338aaccc421e2acb56f970c6db9620dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728906&auth_key=1760728906-0-0-0a15240a7e0302d8cb07274aaff61a4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33162bd3fe6dd5fa8c9a9eea6f9c80a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728913&auth_key=1760728913-0-0-20f2a1f9cbd7323b27d470203833d0bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evolution-of-meta’s-llama-models-and-parameter-efficient-fine-tuning-of-large-language-models-a-survey"><a href="#Evolution-of-meta’s-llama-models-and-parameter-efficient-fine-tuning-of-large-language-models-a-survey" class="headerlink" title="Evolution of meta’s llama models and parameter-efficient fine-tuning of   large language models: a survey"></a>Evolution of meta’s llama models and parameter-efficient fine-tuning of   large language models: a survey</h2><p><strong>Authors:Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi</strong></p>
<p>This review surveys the rapid evolution of Meta AI’s LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method’s mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies. </p>
<blockquote>
<p>本文回顾了Meta AI的LLaMA（大型语言模型Meta AI）系列的快速发展，从LLaMA 1到LLaMA 4以及为这些模型开发的专用参数高效微调（PEFT）方法。首先，我们描述了LLaMA基础模型家族（从7B到65B再到288B参数）、它们的架构（包括原生多模态和混合专家变体）和关键性能特征。然后，我们介绍并讨论了PEFT的概念，该概念通过仅更新一小部分参数来适应大型预训练模型，并回顾了五种已应用于LLaMA的PEFT方法：LoRA（低秩适应）、LLaMA-Adapter V1和V2、LLaMA-Excitor和Quantized LoRA（QLoRA）。我们讨论了每种方法的机制、参数节省情况以及其在LLaMA上的示例应用（例如指令调整、多模态任务）。我们对模型和适配器架构、参数计数和基准测试结果进行了结构化讨论和分析（包括微调后的LLaMA模型表现超过较大基准模型的示例）。最后，我们研究了LLaMA模型及其在现实世界中成功应用的PEFT的实际用例（例如法律和医学领域），并讨论了当前面临的挑战和未来研究方向（如扩展到更大的上下文环境和提高稳健性）。这篇综述论文为对LLaMA模型和高效微调策略感兴趣的机器学习研究人员和从业者提供了一个一站式的资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12178v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLaMA系列模型快速进化，从LLaMA 1到LLaMA 4，以及针对这些模型开发的参数高效微调（PEFT）方法。本文首先介绍LLaMA家族的基础模型、架构和关键性能特点。接着介绍PEFT的概念和五种应用于LLaMA的PEFT方法。最后探讨LLaMA模型和PEFT在现实世界的成功应用案例，以及未来面临的挑战和研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaMA系列模型从LLaMA 1到LLaMA 4呈现快速进化，同时发展出参数高效微调（PEFT）方法。</li>
<li>LLaMA家族模型具有不同的参数规模，从7B到288B参数。</li>
<li>PEFT是一种只更新预训练模型一小部分参数的方法，以提高模型性能。</li>
<li>LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor以及QLoRA等五种PEFT方法被应用于LLaMA。</li>
<li>LLaMA模型在指令微调、多模态任务等方面有应用实例。</li>
<li>LLaMA模型和PEFT在现实世界的应用中，已在法律和医疗领域取得成功。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cdbbabbbf15cad08294170953abc178f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728920&auth_key=1760728920-0-0-561665298915b6821628ad592d4c5b03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-185723f54e3a08ffbcab3dc5c5a135e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728928&auth_key=1760728928-0-0-40d1ae67a9335d1536b7c52de9a99247&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7743abbf611742efb7b82889a5926e22~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728934&auth_key=1760728934-0-0-a25db7b74ca60d6c500f4102156971ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7404671b810b6ac9f1e1cce0a76030b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728941&auth_key=1760728941-0-0-d1c936da2a42463765707a9990c73e32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82c691cb0ba83d5cca1f44dbef073f25~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728948&auth_key=1760728948-0-0-6f35ce1333f3abe8bd899b8a63535bb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4739b18ee06fbf1351bd025feabc6077~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728955&auth_key=1760728955-0-0-ab9f3fb3d9a74e0fcf24975dba87665b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ddffcda8c4a7af987aedb47ac6f33a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728962&auth_key=1760728962-0-0-a17ce02a3780a2dea988a6e569cdc241&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Self-Verifying-Reflection-Helps-Transformers-with-CoT-Reasoning"><a href="#Self-Verifying-Reflection-Helps-Transformers-with-CoT-Reasoning" class="headerlink" title="Self-Verifying Reflection Helps Transformers with CoT Reasoning"></a>Self-Verifying Reflection Helps Transformers with CoT Reasoning</h2><p><strong>Authors:Zhongwei Yu, Wannian Xia, Xue Yan, Bo Xu, Haifeng Zhang, Yali Du, Jun Wang</strong></p>
<p>Advanced large language models (LLMs) frequently reflect in reasoning chain-of-thoughts (CoTs), where they self-verify the correctness of current solutions and explore alternatives. However, given recent findings that LLMs detect limited errors in CoTs, how reflection contributes to empirical improvements remains unclear. To analyze this issue, in this paper, we present a minimalistic reasoning framework to support basic self-verifying reflection for small transformers without natural language, which ensures analytic clarity and reduces the cost of comprehensive experiments. Theoretically, we prove that self-verifying reflection guarantees improvements if verification errors are properly bounded. Experimentally, we show that tiny transformers, with only a few million parameters, benefit from self-verification in both training and reflective execution, reaching remarkable LLM-level performance in integer multiplication and Sudoku. Similar to LLM results, we find that reinforcement learning (RL) improves in-distribution performance and incentivizes frequent reflection for tiny transformers, yet RL mainly optimizes shallow statistical patterns without faithfully reducing verification errors. In conclusion, integrating generative transformers with discriminative verification inherently facilitates CoT reasoning, regardless of scaling and natural language. </p>
<blockquote>
<p>高级大型语言模型（LLM）经常在推理思维链（CoTs）中反映出自验证当前解决方案的正确性并探索替代方案。然而，鉴于最近发现LLM在CoTs中检测到的错误有限，反思如何对经验改进做出贡献仍不清楚。为了分析这个问题，本文提出了一个简洁的推理框架，支持没有自然语言的小型变压器的基本自我验证反思，这确保了分析清晰性并降低了全面实验的成本。从理论上讲，我们证明了如果验证错误得到适当的限制，自我验证反思可以保证改进。实验上，我们展示了只有数百万参数的微型变压器在训练和反思执行中都受益于自我验证，在整数乘法和数独方面达到了惊人的LLM级性能。与LLM的结果相似，我们发现强化学习（RL）提高了内部性能并激励了微型变压器的频繁反思，但RL主要优化浅层的统计模式，而没有忠实地减少验证错误。总之，将生成变压器与判别性验证相结合，本质上促进了无论规模大小和自然语言如何的CoT推理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12157v1">PDF</a> Accepted by NeurIPS2025</p>
<p><strong>Summary</strong><br>高级大语言模型（LLM）在推理过程中会进行自我验证和探索替代方案。然而，鉴于最近发现LLM在推理过程中检测到的错误有限，反思如何对经验改进做出贡献尚不清楚。本文提出一个简约的推理框架，支持小变形器的基本自我验证反思，无需自然语言，确保分析清晰并降低全面实验的成本。理论上，我们证明了如果验证错误得到适当控制，自我验证反思可以保证改进。实验表明，只有几百万参数的微型变形器在训练和反思执行中都受益于自我验证，在整数乘法和数独方面达到了惊人的LLM级性能。与LLM结果类似，我们发现强化学习（RL）提高了内部性能并激励了微型变形器的频繁反思，但RL主要优化浅统计模式，并没有真正减少验证错误。结论是，将生成式变形器与判别式验证相结合，有助于促进无论规模大小和自然语言的推理思考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高级LLM在推理过程中会进行自我验证和探索替代方案。</li>
<li>自我验证反思有助于提高LLM的性能，但需适当控制验证错误。</li>
<li>微型变形器在训练和反思执行中都受益于自我验证。</li>
<li>自我验证的微型变形器在整数乘法和数独方面表现出惊人的性能。</li>
<li>强化学习（RL）能提高内部性能，但主要优化浅统计模式，并未真正减少验证错误。</li>
<li>结合生成式变形器和判别式验证有助于促进推理思考，不受规模大小或自然语言限制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fcd6923cf438627f3c6ada7685f31210~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728970&auth_key=1760728970-0-0-a3991fe02d9ae9ff55fbb4036e22a8b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c5d6a131f729a2c25357307e99f2dce3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728977&auth_key=1760728977-0-0-f62f7bd2f4be9b6f779376c283c8a920&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e15b72067381f8c2934747fca3fd083~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728983&auth_key=1760728983-0-0-e86e318570a1ebac034ce3188e9a77b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2368d7b4f937f01ce88c9482f50e964b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728990&auth_key=1760728990-0-0-8d5960452b6df69df2f746f88a6f55d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a1862c1a795a16d75aaf27d0f09839e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728996&auth_key=1760728996-0-0-e17198230f4f7cd2967e761b8e9bebbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-0891dd7b028f247a1bd8a7257aed0b40~resize:0:q75.jpg?source=1f5c5e47&expiration=1760730653&auth_key=1760730653-0-0-fe26461d7f44becd1dc3a4381b9e1399&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-10-18  Agentic Design of Compositional Machines
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-e4af957f63b08c2752075fc2c2f13d7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726092&auth_key=1760726092-0-0-a53a65026dc21de436c0cc649fc316cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-10-18  Agentic Design of Compositional Machines
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
