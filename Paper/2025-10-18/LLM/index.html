<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Agentic Design of Compositional Machines">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-07dfd4190e066b6ffd27340fa7e6a279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728472&auth_key=1760728472-0-0-7b38aef80c3e51a98c4a5f5c3d1e5e84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Agentic-Design-of-Compositional-Machines"><a href="#Agentic-Design-of-Compositional-Machines" class="headerlink" title="Agentic Design of Compositional Machines"></a>Agentic Design of Compositional Machines</h2><p><strong>Authors:Wenqian Zhang, Weiyang Liu, Zhen Liu</strong></p>
<p>The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning. </p>
<blockquote>
<p>å¤æ‚æœºå™¨çš„è®¾è®¡æ—¢æ˜¯äººç±»æ™ºæ…§çš„æ ‡å¿—ï¼Œä¹Ÿæ˜¯å·¥ç¨‹å®è·µçš„åŸºç¡€ã€‚é‰´äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“å®ƒä»¬æ˜¯å¦ä¹Ÿèƒ½å­¦ä¼šåˆ›é€ ã€‚æˆ‘ä»¬é€šè¿‡ç»„åˆæœºå™¨è®¾è®¡çš„è§†è§’æ¥æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼šè¿™æ˜¯ä¸€é¡¹ä»»åŠ¡ï¼Œå…¶ä¸­æœºå™¨ç”±æ ‡å‡†åŒ–ç»„ä»¶ç»„è£…è€Œæˆï¼Œä»¥æ»¡è¶³æ¨¡æ‹Ÿç‰©ç†ç¯å¢ƒä¸­çš„è¿åŠ¨æˆ–æ“ä½œç­‰åŠŸèƒ½éœ€æ±‚ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†BesiegeFieldï¼Œè¿™æ˜¯ä¸€ä¸ªå»ºç«‹åœ¨æœºå™¨å»ºé€ æ¸¸æˆBesiegeä¹‹ä¸Šçš„æµ‹è¯•å¹³å°ï¼Œæ”¯æŒéƒ¨ä»¶æ„é€ ã€ç‰©ç†æ¨¡æ‹Ÿå’Œå¥–åŠ±é©±åŠ¨è¯„ä¼°ã€‚ä½¿ç”¨BesiegeFieldï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMçš„ä»£ç†å·¥ä½œæµç¨‹ï¼Œå¹¶ç¡®å®šäº†æˆåŠŸæ‰€éœ€çš„å…³é”®èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç©ºé—´æ¨ç†ã€ç­–ç•¥æ€§è£…é…å’ŒæŒ‡ä»¤éµå¾ªã€‚ç”±äºå½“å‰å¼€æºæ¨¡å‹çš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæ”¹è¿›çš„é€”å¾„ï¼šæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå†·å¯åŠ¨æ•°æ®é›†ï¼Œè¿›è¡Œäº†RLå¾®è°ƒå®éªŒï¼Œå¹¶å¼ºè°ƒäº†è¯­è¨€ã€æœºå™¨è®¾è®¡å’Œç‰©ç†æ¨ç†äº¤æ±‡å¤„çš„å¼€æ”¾æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14980v1">PDF</a> 75 pages, 31 figures, Project Page: <a target="_blank" rel="noopener" href="https://besiegefield.github.io/">https://besiegefield.github.io</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨è®¾è®¡é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡èšç„¦ç»„åˆå¼æœºå™¨è®¾è®¡ä»»åŠ¡ï¼Œå³åˆ©ç”¨æ ‡å‡†åŒ–ç»„ä»¶ç»„è£…æœºå™¨ä»¥æ»¡è¶³æ¨¡æ‹Ÿç‰©ç†ç¯å¢ƒä¸­çš„åŠŸèƒ½éœ€æ±‚ï¼Œå¦‚è¿åŠ¨æˆ–æ“æ§ã€‚ä¸ºæ”¯æŒè¿™ä¸€ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæœºå™¨å»ºé€ æ¸¸æˆBesiegeæ‰“é€ çš„æµ‹è¯•å¹³å°BesiegeFieldï¼Œè¯¥å¹³å°æ”¯æŒéƒ¨ä»¶çº§å»ºé€ ã€ç‰©ç†æ¨¡æ‹Ÿå’Œå¥–åŠ±é©±åŠ¨è¯„ä¼°ã€‚æˆ‘ä»¬åˆ©ç”¨BesiegeFieldå¯¹æœ€å…ˆè¿›çš„LLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶ç¡®å®šæˆåŠŸçš„å…³é”®èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç©ºé—´æ¨ç†ã€ç­–ç•¥æ€§è£…é…å’ŒæŒ‡ä»¤éµå¾ªç­‰ã€‚é‰´äºå½“å‰å¼€æºæ¨¡å‹çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæå‡è¿™äº›èƒ½åŠ›çš„é€”å¾„ï¼šæ•´ç†åˆå§‹æ•°æ®é›†ï¼Œè¿›è¡ŒRLå¾®è°ƒå®éªŒï¼Œå¹¶å¼ºè°ƒè¯­è¨€ã€æœºå™¨è®¾è®¡å’Œç‰©ç†æ¨ç†äº¤æ±‡å¤„çš„å¼€æ”¾æŒ‘æˆ˜ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨è®¾è®¡é¢†åŸŸçš„åº”ç”¨æˆä¸ºç ”ç©¶ç„¦ç‚¹ã€‚</li>
<li>ç»„åˆå¼æœºå™¨è®¾è®¡ä»»åŠ¡è¦æ±‚æœºå™¨æ»¡è¶³åŠŸèƒ½éœ€æ±‚ï¼Œå¦‚è¿åŠ¨ä¸æ“æ§ã€‚</li>
<li>BesiegeFieldæµ‹è¯•å¹³å°æ”¯æŒæœºå™¨å»ºé€ çš„å¤šä¸ªæ–¹é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬éƒ¨ä»¶çº§å»ºé€ å’Œç‰©ç†æ¨¡æ‹Ÿã€‚</li>
<li>LLMæˆåŠŸå…³é”®èƒ½åŠ›åŒ…æ‹¬ç©ºé—´æ¨ç†ã€ç­–ç•¥æ€§è£…é…å’ŒæŒ‡ä»¤éµå¾ªã€‚</li>
<li>å½“å‰å¼€æºæ¨¡å‹åœ¨æœºå™¨è®¾è®¡ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«æ¢ç´¢ä¸ºæå‡LLMåœ¨æœºå™¨è®¾è®¡é¢†åŸŸèƒ½åŠ›çš„é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c256346b3b9819197b782e126d33bdfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728298&auth_key=1760728298-0-0-5dfc888d30fb9602a876ba910394f727&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b72de284f534b45d2dde836e7407ef4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728305&auth_key=1760728305-0-0-0098c50b1512c8f8b23238c86a42c044&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a452b05e0464d861460cf73d4168c3cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728312&auth_key=1760728312-0-0-7808e1521202482a6d327d4c10bf620a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d601bcfadf1df847879211c85ec7849~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728319&auth_key=1760728319-0-0-1b2b679db83068d7bf5825db295c5c32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7a965fcabd9913c6053e7b0c2183944~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728325&auth_key=1760728325-0-0-6969bdf9a087f56fc2889d8b4fbe4eca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents"><a href="#Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents" class="headerlink" title="Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents"></a>Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents</h2><p><strong>Authors:Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying</strong></p>
<p>Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policyâ€™s probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the modelâ€™s own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠ å¼ºå…¶é€šè¿‡å·¥å…·ä½¿ç”¨ä¸å¤–éƒ¨ç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè½®æ¨ç†å’ŒçŸ¥è¯†è·å–åŸºäºæœç´¢çš„ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä»¥ç»“æœä¸ºåŸºç¡€çš„å¥–åŠ±ï¼Œè¿™äº›å¥–åŠ±ä»…åœ¨æœ€ç»ˆç­”æ¡ˆæ—¶æä¾›ã€‚åœ¨éœ€è¦å¤šè½®å¯¹è¯çš„ç¯å¢ƒä¸­ï¼Œå¥–åŠ±ç¨€ç–æ€§å˜å¾—ç‰¹åˆ«æˆé—®é¢˜ï¼Œå…¶ä¸­é•¿æœŸè½¨è¿¹åŠ å‰§äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆiï¼‰ä¼˜åŠ¿å´©æºƒï¼Œæ‰€æœ‰å®éªŒåé¦ˆå‡æ”¶åˆ°ç›¸åŒå¥–åŠ±ä¸”æ— æ³•æä¾›æœ‰ç”¨çš„å­¦ä¹ ä¿¡å·ï¼›ï¼ˆiiï¼‰ç²¾ç»†å¥–åŠ±åˆ†é…ä¸è¶³ï¼Œåœ¨ç‰¹å®šæƒ…å†µä¸‹æ¯ä¸€è½®å¯¹è¯ä¹‹é—´çš„ä¾èµ–æ€§è¢«é®è”½ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºä¿¡æ¯å¢ç›Šçš„ç­–ç•¥ä¼˜åŒ–ï¼ˆIGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸ºé’ˆå¯¹å¤šè½®å¯¹è¯çš„ä»£ç†è®­ç»ƒæä¾›å¯†é›†ä¸”å†…åœ¨çš„ç›‘ç£ã€‚IGPOå°†æ¯ä¸ªäº¤äº’å›åˆè§†ä¸ºå…³äºçœŸç›¸çš„å¢é‡ä¿¡æ¯è·å–è¿‡ç¨‹ï¼Œå¹¶å°†å›åˆçº§å¥–åŠ±å®šä¹‰ä¸ºç­–ç•¥äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡çš„è¾¹é™…å¢åŠ ã€‚ä¸ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–æ˜‚è´µçš„è’™ç‰¹å¡æ´›ä¼°è®¡çš„å…ˆå‰è¿‡ç¨‹çº§å¥–åŠ±æ–¹æ³•ä¸åŒï¼ŒIGPOç›´æ¥ä»æ¨¡å‹è‡ªèº«çš„ä¿¡å¿µæ›´æ–°ä¸­è·å¾—å†…åœ¨å¥–åŠ±ã€‚è¿™äº›å†…åœ¨å›åˆçº§å¥–åŠ±ä¸ç»“æœçº§ç›‘ç£ç›¸ç»“åˆï¼Œå½¢æˆäº†å¯†é›†çš„å¥–åŠ±è½¨è¿¹ã€‚åœ¨å†…éƒ¨å’Œå¤–éƒ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­ï¼ŒIGPOå§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14967v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ï¼Œä»¥å¢å¼ºå…¶åœ¨å·¥å…·ä½¿ç”¨ç­‰æ–¹é¢çš„å¤–éƒ¨ç¯å¢ƒäº¤äº’èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè½®æ¨ç†å’ŒçŸ¥è¯†è·å–çš„çš„æœç´¢ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç»“æœå¯¼å‘çš„å¥–åŠ±ï¼Œä»…åœ¨æœ€ç»ˆç­”æ¡ˆæä¾›å¥–åŠ±ã€‚åœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­ï¼Œè¿™ç§å¥–åŠ±ç¨€ç–æ€§ä¼šå¼•å‘ä¸¤ä¸ªé—®é¢˜ï¼šï¼ˆiï¼‰ä¼˜åŠ¿å´©æºƒï¼Œæ‰€æœ‰å›åˆè·å¾—ç›¸åŒå¥–åŠ±ï¼Œæ— æ³•æä¾›æœ‰ç”¨çš„å­¦ä¹ ä¿¡å·ï¼›ï¼ˆiiï¼‰ç¼ºä¹ç²¾ç»†çš„ä¿¡ç”¨åˆ†é…ï¼Œå›åˆä¹‹é—´çš„ä¾èµ–å…³ç³»è¢«æ©ç›–ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æœŸä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æå‡ºåŸºäºä¿¡æ¯å¢ç›Šçš„ç­–ç•¥ä¼˜åŒ–ï¼ˆIGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸ºå¤šè½®ä»£ç†è®­ç»ƒæä¾›å¯†é›†å’Œå†…åœ¨çš„ç›‘ç£ã€‚IGPOå°†æ¯ä¸ªäº¤äº’å›åˆè§†ä¸ºè·å–å…³äºäº‹å®çœŸç›¸çš„å¢é‡ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œå¹¶å°†å›åˆçº§å¥–åŠ±å®šä¹‰ä¸ºç­–ç•¥äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡çš„è¾¹é™…å¢åŠ ã€‚ä¸ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–æ˜‚è´µçš„è’™ç‰¹å¡æ´›ä¼°è®¡çš„è¿‡ç¨‹çº§å¥–åŠ±æ–¹æ³•ä¸åŒï¼ŒIGPOç›´æ¥ä»æ¨¡å‹è‡ªèº«çš„ä¿¡å¿µæ›´æ–°ä¸­è·å¾—å†…åœ¨å¥–åŠ±ã€‚è¿™äº›å†…åœ¨çš„å›åˆçº§å¥–åŠ±ä¸ç»“æœçº§ç›‘ç£ç›¸ç»“åˆï¼Œå½¢æˆäº†å¯†é›†çš„å¥–åŠ±è½¨è¿¹ã€‚åœ¨åŸŸå†…å’Œè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIGPOåœ¨å¤šè½®åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæå‡ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè½®æ¨ç†å’ŒçŸ¥è¯†è·å–æ–¹é¢çš„æœç´¢ç¯å¢ƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–ç»“æœå¯¼å‘çš„å¥–åŠ±ç³»ç»Ÿï¼Œåœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­å¼•å‘å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚</li>
<li>IGPOæ¡†æ¶è¢«æå‡ºä½œä¸ºä¸€ä¸ªç®€å•æœ‰æ•ˆçš„RLæ¡†æ¶ï¼Œä¸ºå¤šè½®ä»£ç†è®­ç»ƒæä¾›å¯†é›†å’Œå†…åœ¨çš„ç›‘ç£ã€‚</li>
<li>IGPOå°†æ¯ä¸ªäº¤äº’å›åˆè§†ä¸ºè·å–ä¿¡æ¯çš„å¢é‡è¿‡ç¨‹ï¼Œå¹¶å®šä¹‰å›åˆçº§å¥–åŠ±ä¸ºç­–ç•¥äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡çš„è¾¹é™…å¢åŠ ã€‚</li>
<li>IGPOç›´æ¥ä»æ¨¡å‹è‡ªèº«çš„ä¿¡å¿µæ›´æ–°ä¸­å¾—å‡ºå†…åœ¨å¥–åŠ±ï¼Œä¸ç»“æœçº§ç›‘ç£ç»“åˆå½¢æˆå¯†é›†çš„å¥–åŠ±è½¨è¿¹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒIGPOåœ¨å¤šè½®åœºæ™¯ä¸­çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6acee461152fa22ff3a292ec3c6b9c47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728333&auth_key=1760728333-0-0-5d37b06b5811ec041243f18a36e5a4a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-069829c476fc987398af88ea95aa8f54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728340&auth_key=1760728340-0-0-a33a796db3441e6c272da6f477ee92ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcb8a3c56cd16d74ac346368c4442502~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728346&auth_key=1760728346-0-0-4ceb527164828b3f407703c63ceac732&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Identity-Link-IRT-for-Label-Free-LLM-Evaluation-Preserving-Additivity-in-TVD-MI-Scores"><a href="#Identity-Link-IRT-for-Label-Free-LLM-Evaluation-Preserving-Additivity-in-TVD-MI-Scores" class="headerlink" title="Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity   in TVD-MI Scores"></a>Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity   in TVD-MI Scores</h2><p><strong>Authors:Zachary Robertson</strong></p>
<p>Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MIâ€™s binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 &#x3D; [0.474, 0.580]), whereas probit&#x2F;logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent rankings (Spearman $\rho &#x3D; 0.972 \pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\rho &#x3D; 0.872$) and consistent identity-link advantage. TVD-MIâ€™s geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains. </p>
<blockquote>
<p>ä½¿ç”¨æ€»å˜å·®è·ç¦»äº’ä¿¡æ¯ï¼ˆTVD-MIï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé…å¯¹æ¯”è¾ƒï¼Œä¸ºæ¯å¯¹æ¨¡å‹äº§ç”ŸäºŒå…ƒæ‰¹åˆ¤å†³ç­–ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œå¯¹TVD-MIçš„äºŒå…ƒè¯•éªŒè¿›è¡Œå¹³å‡ï¼Œå¯ä»¥å¾—åˆ°å…·æœ‰åŠ æ³•ç»“æ„çš„ä¸­å¿ƒåŒ–æ¦‚ç‡åˆ†æ•°ï¼Œé€‚ç”¨äºé¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰è€Œæ— éœ€éçº¿æ€§é“¾æ¥å‡½æ•°ã€‚IRTçš„æœ€å¤§ä¼¼ç„¶æ–¹æ³•ä½¿ç”¨é€»è¾‘é“¾æ¥ï¼Œä½†æˆ‘ä»¬çš„ç»éªŒè¡¨æ˜ï¼Œè¿™äº›è½¬æ¢ä¼šå¼•å…¥æ›²çº¿ï¼Œä»è€Œç ´åå¯åŠ æ€§ï¼šåœ¨ä¸‰ä¸ªé¢†åŸŸä¸­ï¼Œèº«ä»½é“¾æ¥å¯¼è‡´åŸå§‹æ•°æ®çš„ä¸­ä½æ•°å·æ›²ä¸º0.080-0.150ï¼ˆP95&#x3D;[0.474, 0.580]ï¼‰ï¼Œè€ŒProbit&#x2F;logitåˆ™å¼•å…¥æ›´é«˜çš„è¿è§„æƒ…å†µï¼ˆä¸­ä½æ•°[0.245, 0.588]ï¼ŒP95 [0.825, 2.252]ï¼‰ã€‚æˆ‘ä»¬ä»åŸºå°¼ç†µæœ€å¤§åŒ–ä¸­å¾—å‡ºè¿™ä¸ªè£å‰ªçš„çº¿æ€§æ¨¡å‹ï¼Œå¾—åˆ°ä¸€ä¸ªå—ç›’å­çº¦æŸçš„æœ€å°äºŒä¹˜å…¬å¼ï¼Œè¯¥å…¬å¼å¯å¤„ç†è¾¹ç•Œé¥±å’Œé—®é¢˜ã€‚åœ¨33%çš„è¦†ç›–ç‡ä¸‹ï¼Œæˆ‘ä»¬å®ç°äº†ç•™å‡ºéªŒè¯çš„RMSEä¸º$0.117 \pm 0.008$ï¼ŒåŒæ—¶ä¿ç•™äº†ä»£ç†æ’åï¼ˆæ–¯çš®å°”æ›¼$\rho &#x3D; 0.972 \pm 0.015$ï¼‰ï¼Œæ‰€éœ€è¯„ä¼°æ¬¡æ•°æ¯”å…¨é¢å¯†é›†è¯„ä¼°å‡å°‘ä¸‰å€ã€‚æ³•å®˜ç¨³å¥æ€§åˆ†æï¼ˆGPT-4o-miniä¸Llama3-70bï¼‰æ˜¾ç¤ºä»£ç†æ’åé«˜åº¦ä¸€è‡´ï¼ˆ$\rho &#x3D; 0.872$ï¼‰ä»¥åŠèº«ä»½é“¾æ¥çš„ä¸€è‡´ä¼˜åŠ¿ã€‚å¯¹äºå…¶ä»–æœ‰ç•Œå“åº”é¢†åŸŸï¼ŒTVD-MIçš„å‡ ä½•ç»“æ„é€šè¿‡èº«ä»½æ˜ å°„å¾—åˆ°æœ€ä½³ä¿ç•™ï¼Œæœ‰åˆ©äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14966v1">PDF</a> 9 pages, 2 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä½¿ç”¨æ€»å˜å·®è·ç¦»äº’ä¿¡æ¯ï¼ˆTVD-MIï¼‰è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„é…å¯¹æ¯”è¾ƒï¼Œäº§ç”Ÿæ¯å¯¹çš„äºŒå…ƒæ‰¹è¯„å†³ç­–ã€‚æˆ‘ä»¬å±•ç¤ºå¹³å‡TVD-MIçš„äºŒå…ƒè¯•éªŒä¼šäº§ç”Ÿé€‚åˆé¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰çš„ä¸­å¿ƒæ¦‚ç‡åˆ†æ•°ï¼Œå…·æœ‰å¯åŠ ç»“æ„è€Œæ— éœ€éçº¿æ€§é“¾æ¥å‡½æ•°ã€‚æœ€å¤§ä¼¼ç„¶æ–¹æ³•é‡‡ç”¨é€»è¾‘é“¾æ¥ï¼Œä½†ç»éªŒæ˜¾ç¤ºè¿™äº›è½¬æ¢ç ´åäº†å¯åŠ æ€§å¼•å…¥äº†æ›²çº¿ã€‚è·¨ä¸‰ä¸ªé¢†åŸŸï¼Œèº«ä»½é“¾æ¥å¯¼è‡´åŸå§‹æ•°æ®çš„å¹³å‡å·æ›²åœ¨0.080-0.150ä¹‹é—´ï¼ˆP95 &#x3D; [0.474ï¼Œ0.580]ï¼‰ï¼Œè€Œprobit&#x2F;logitå¼•å…¥çš„è¿è§„æƒ…å†µæ›´é«˜ï¼ˆä¸­ä½æ•°[0.245ï¼Œ0.588]ï¼ŒP95 [0.825ï¼Œ2.252]ï¼‰ã€‚æˆ‘ä»¬ä»åŸºå°¼ç†µæœ€å¤§åŒ–ä¸­æ¨å¯¼å‡ºè¿™ç§è¢«æˆªæ–­çš„çº¿æ€§æ¨¡å‹ï¼Œå®ƒé‡‡ç”¨æ¡†çº¦æŸæœ€å°äºŒä¹˜æ³•å…¬å¼å¤„ç†è¾¹ç•Œé¥±å’Œé—®é¢˜ã€‚åœ¨è¦†ç›–ç‡è¾¾åˆ°33%çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®ç°äº†ä¿ç•™ä»£ç†æ’åçš„åŒæ—¶çš„holdout RMSEä¸º$ 0.117 \pm 0.008 $ï¼ˆSpearman $\rho &#x3D; 0.972 \pm 0.015$ï¼‰ï¼Œæ‰€éœ€è¯„ä¼°æ¬¡æ•°å‡å°‘åˆ°å…¨å¯†é›†æƒ…å†µä¸‹çš„ä¸‰åˆ†ä¹‹ä¸€ã€‚æ³•å®˜ç¨³å¥æ€§åˆ†æï¼ˆGPT-4o-miniä¸Llama3-70bçš„æ¯”è¾ƒï¼‰æ˜¾ç¤ºä»£ç†æ’åé«˜åº¦ä¸€è‡´ï¼ˆÏ&#x3D; 0.872ï¼‰ï¼Œè¿›ä¸€æ­¥è¯å®äº†èº«ä»½é“¾æ¥çš„ä¼˜åŠ¿ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ï¼Œèº«ä»½æ˜ å°„å¯ä»¥æ›´å¥½åœ°ä¿ç•™TVD-MIçš„å‡ ä½•ç»“æ„ï¼Œé€‚ç”¨äºå…¶ä»–æœ‰ç•Œå“åº”é¢†åŸŸã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½¿ç”¨æ€»å˜å·®è·ç¦»äº’ä¿¡æ¯ï¼ˆTVD-MIï¼‰è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„é…å¯¹æ¯”è¾ƒã€‚</li>
<li>é€šè¿‡å¹³å‡TVD-MIçš„äºŒå…ƒè¯•éªŒå¾—å‡ºé€‚åˆé¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰çš„ä¸­å¿ƒæ¦‚ç‡åˆ†æ•°ã€‚</li>
<li>åœ¨IRTä¸­ï¼Œèº«ä»½é“¾æ¥ç›¸æ¯”é€»è¾‘é“¾æ¥å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå› ä¸ºé€»è¾‘é“¾æ¥ç ´åäº†å¯åŠ æ€§å¼•å…¥äº†æ›²çº¿ã€‚</li>
<li>åœ¨å¤„ç†è¾¹ç•Œé¥±å’Œé—®é¢˜æ—¶ï¼Œé‡‡ç”¨æ¡†çº¦æŸæœ€å°äºŒä¹˜æ³•å…¬å¼ã€‚</li>
<li>åœ¨è¦†ç›–ç‡è¾¾åˆ°33%çš„æƒ…å†µä¸‹ï¼Œè¯„ä¼°ç»“æœå…·æœ‰è‰¯å¥½çš„ç¨³å¥æ€§ï¼Œholdout RMSEè¾ƒä½ï¼ŒåŒæ—¶ä¿ç•™ä»£ç†æ’åã€‚</li>
<li>åœ¨æ³•å®˜ç¨³å¥æ€§åˆ†æä¸­ï¼Œèº«ä»½é“¾æ¥çš„ä¼˜åŠ¿å¾—åˆ°è¿›ä¸€æ­¥è¯å®ï¼Œä»£ç†æ’åé«˜åº¦ä¸€è‡´ã€‚</li>
<li>TVD-MIçš„å‡ ä½•ç»“æ„åœ¨èº«ä»½æ˜ å°„ä¸‹èƒ½æ›´å¥½åœ°ä¿ç•™ï¼Œé€‚ç”¨äºå…¶ä»–æœ‰ç•Œå“åº”é¢†åŸŸçš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-045f80efde0d486662affd10dded00dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728353&auth_key=1760728353-0-0-2bafeff3691e17a748629d808a0ce00d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e28c90ce88af456a792d74812d9da244~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728360&auth_key=1760728360-0-0-3ebd8b42e863f7c77238e294d984ac9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2e5e15797097f45122c5712d06e8d05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728367&auth_key=1760728367-0-0-03f15cf867295a0a9b447b1b257f3b1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc38e4067371bf02c7d9a4bd572f3b7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728374&auth_key=1760728374-0-0-42dfbd28f51a78006b5934001a517ccf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning"><a href="#MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning" class="headerlink" title="MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning"></a>MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning</h2><p><strong>Authors:Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li</strong></p>
<p>While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¾èµ–è§†è§‰è¾…åŠ©å·¥å…·çš„å‡ ä½•ç­‰æ•°å­¦é¢†åŸŸå´é‡åˆ°å›°éš¾ã€‚ç°æœ‰çš„è§†è§‰æ€ç»´é“¾ï¼ˆVCoTï¼‰æ–¹æ³•å¾€å¾€å—åˆ°åƒµåŒ–å¤–éƒ¨å·¥å…·çš„é™åˆ¶ï¼Œæˆ–è€…æ— æ³•ç”Ÿæˆç”¨äºå¤æ‚é—®é¢˜è§£å†³çš„å…·æœ‰é«˜ä¿çœŸåº¦ã€æˆ˜ç•¥å®šæ—¶å›¾çš„å¿…è¦ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MathCanvasï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨èµ‹äºˆç»Ÿä¸€çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å†…åœ¨VCoTèƒ½åŠ›çš„å…¨é¢æ¡†æ¶ï¼Œç”¨äºæ•°å­¦é¢†åŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œè§†è§‰æ“ä½œé˜¶æ®µä½¿ç”¨æ–°å‹1520ä¸‡å¯¹æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬1äº¿ä¸ªæ ‡é¢˜åˆ°å›¾è¡¨çš„é…å¯¹ï¼ˆMathCanvas-Imagenï¼‰å’Œ520ä¸‡æ­¥çš„ç¼–è¾‘è½¨è¿¹ï¼ˆMathCanvas-Editï¼‰ï¼Œä»¥æŒæ¡å›¾è¡¨ç”Ÿæˆå’Œç¼–è¾‘ã€‚å…¶æ¬¡ï¼Œæˆ˜ç•¥è§†è§‰è¾…åŠ©æ¨ç†é˜¶æ®µåœ¨MathCanvas-Instructæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«äº¤é”™è§†è§‰æ–‡æœ¬æ¨ç†è·¯å¾„çš„æ–°æ•°æ®é›†ï¼Œå«æœ‰21.9ä¸‡ä¸ªç¤ºä¾‹ï¼Œæ•™å­¦æ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•åˆ©ç”¨è§†è§‰è¾…åŠ©å·¥å…·ã€‚ä¸ºäº†ä¿ƒè¿›ä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathCanvas-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«3000ä¸ªé—®é¢˜ï¼Œéœ€è¦æ¨¡å‹ç”Ÿæˆäº¤é”™çš„è§†è§‰æ–‡æœ¬è§£å†³æ–¹æ¡ˆã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶è®­ç»ƒä¸‹çš„BAGEL-Canvasæ¨¡å‹åœ¨MathCanvas-Benchä¸Šçš„è¡¨ç°ç›¸è¾ƒäºå¼ºå¤§çš„LMMåŸºå‡†æµ‹è¯•æœ‰äº†86%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå±•ç°å‡ºå¯¹å…¶ä»–å…¬å…±æ•°å­¦åŸºå‡†æµ‹è¯•çš„å‡ºè‰²æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„å·¥å…·åŒ…æ¡†æ¶ã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥è§£é”LMMä¸­å¤æ‚ã€ç±»ä¼¼äººç±»çš„è§†è§‰è¾…åŠ©æ¨ç†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14958v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†MathCanvasæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰èµ‹äºˆå†…åœ¨çš„å¯è§†åŒ–æ€ç»´é“¾ï¼ˆVCoTï¼‰èƒ½åŠ›ï¼Œä»¥åº”å¯¹æ•°å­¦é¢†åŸŸçš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯è§†è§‰æ“ä½œé˜¶æ®µï¼Œé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æŒæ¡å›¾è¡¨ç”Ÿæˆå’Œç¼–è¾‘æŠ€èƒ½ï¼›å…¶æ¬¡æ˜¯æˆ˜ç•¥è§†è§‰è¾…åŠ©æ¨ç†é˜¶æ®µï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å­¦ä¼šå¦‚ä½•åˆ©ç”¨è§†è§‰è¾…åŠ©å·¥å…·ã€‚è¯¥æ¡†æ¶ä¸‹çš„æ¨¡å‹BAGEL-Canvasåœ¨MathCanvas-Benchä¸Šç›¸å¯¹äºå¼ºå¤§çš„LMMåŸºå‡†æ¨¡å‹å®ç°äº†86%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå±•ç°å‡ºä¼˜ç§€æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¾èµ–è§†è§‰è¾…åŠ©çš„æ•°å­¦é¢†åŸŸå¦‚å‡ ä½•ä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>MathCanvasæ¡†æ¶æ—¨åœ¨é€šè¿‡ä¸¤ä¸ªé˜¶æ®µèµ‹äºˆLMMså†…åœ¨çš„å¯è§†åŒ–æ€ç»´é“¾ï¼ˆVCoTï¼‰èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯è§†è§‰æ“ä½œé˜¶æ®µï¼Œé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æŒæ¡å›¾è¡¨ç”Ÿæˆå’Œç¼–è¾‘æŠ€èƒ½ã€‚</li>
<li>ç¬¬äºŒä¸ªé˜¶æ®µæ˜¯æˆ˜ç•¥è§†è§‰è¾…åŠ©æ¨ç†é˜¶æ®µï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å­¦ä¼šå¦‚ä½•åˆ©ç”¨è§†è§‰è¾…åŠ©å·¥å…·è¿›è¡Œå¤æ‚çš„æ•°å­¦é—®é¢˜æ±‚è§£ã€‚</li>
<li>MathCanvaså¼•å…¥äº†æ–°çš„æ•°æ®é›†MathCanvas-Benchï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹BAGEL-Canvasåœ¨MathCanvas-Benchä¸Šå®ç°äº†è¾ƒé«˜çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºå‡†æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5c4eefc85e5854dadcc669fa4285b07d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728382&auth_key=1760728382-0-0-e86ab40d83b6eaaf00f247557e19b1b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de68a430433309e0a20c51788ecb1b34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728389&auth_key=1760728389-0-0-326232a6660338f13d5174eeb14d9a65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea25758b6de272f758c85964a8765599~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728395&auth_key=1760728395-0-0-4b3bebd1b96efd4e434f3e02ba4b6377&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5450ab2559bbf7b0feb603ba68bd91fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728402&auth_key=1760728402-0-0-4c9842c5a622707ef62bbf9b97f77a7b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning"><a href="#GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning" class="headerlink" title="GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning"></a>GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning</h2><p><strong>Authors:Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp</strong></p>
<p>Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ—¨åœ¨é€šè¿‡ç›‘ç£ä¸­é—´æ­¥éª¤å’Œè¯†åˆ«é”™è¯¯æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ„å»ºæœ‰æ•ˆçš„PRMä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹å¯æ‰©å±•çš„é«˜è´¨é‡æ³¨é‡Šã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ ‡æ³¨ã€åŸºäºLLMçš„è‡ªæˆ‘è¯„ä¼°ï¼ˆå®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼‰æˆ–è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°è®¡ã€‚MCä¼°è®¡ä»…ä»æ»šåŠ¨ç»“æœæ¨æ–­æ­¥éª¤è´¨é‡ï¼Œé€šå¸¸ç”±äºä¿¡ç”¨åˆ†é…ä¸å½“è€Œå¼•å…¥å˜ˆæ‚ã€å¤±é…çš„ç›‘ç£ã€‚è¿™äº›é—®é¢˜å¯¼è‡´ä¸‰ä¸ªæ ¸å¿ƒå±€é™ï¼šå¥–åŠ±å™ªå£°ã€äº‹å®å‡†ç¡®æ€§ä½ä»¥åŠä¸æ­¥éª¤çº§æ¨ç†ç›®æ ‡çš„ä¸å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GroundedPRMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨æµç¨‹ç›‘ç£çš„æ ‘çŠ¶æŒ‡å¯¼å’Œä¿çœŸåº¦æ„ŸçŸ¥æ¡†æ¶ã€‚ä¸ºäº†å‡å°‘å¥–åŠ±å™ªå£°å¹¶å®ç°ç²¾ç»†çš„ä¿¡ç”¨åˆ†é…ï¼Œæˆ‘ä»¬é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ„å»ºç»“æ„åŒ–æ¨ç†è·¯å¾„ã€‚ä¸ºäº†æ¶ˆé™¤å¹»è§‰ç›‘ç£ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤–éƒ¨å·¥å…·éªŒè¯æ¯ä¸ªä¸­é—´æ­¥éª¤ï¼Œæä¾›åŸºäºæ‰§è¡Œçš„æ­£ç¡®æ€§ä¿¡å·ã€‚ä¸ºäº†ç»“åˆæ­¥éª¤çº§éªŒè¯å’Œå…¨å±€ç»“æœè¯„ä¼°ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆå¥–åŠ±èšåˆæœºåˆ¶ï¼Œå®ƒå°†åŸºäºå·¥å…·çš„æ£€æŸ¥ä¸MCTSæ´¾ç”Ÿçš„åé¦ˆç›¸ç»“åˆã€‚æœ€åï¼Œæˆ‘ä»¬å°†å¥–åŠ±ä¿¡å·æ ¼å¼åŒ–ä¸ºå¢å¼ºç†ç”±çš„ç”Ÿæˆç»“æ„ï¼Œä»¥ä¿ƒè¿›ä¸æŒ‡ä»¤è°ƒæ•´è¿‡çš„LLMçš„å¯è§£é‡Šæ€§å’Œå…¼å®¹æ€§ã€‚GroundedPRMä»…ä½¿ç”¨è‡ªåŠ¨æ ‡è®°çš„4ä¸‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™ä»…ç›¸å½“äºä½¿ç”¨è‡ªåŠ¨æ ‡è®°ç›‘ç£çš„æœ€ä½³PRMæ‰€ä½¿ç”¨çš„æ•°æ®çš„ååˆ†ä¹‹ä¸€ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒåœ¨ProcessBenchä¸Šçš„å¹³å‡æ€§èƒ½è¾¾åˆ°äº†é«˜è¾¾26%çš„ç›¸å¯¹æ”¹è¿›ã€‚å½“ç”¨äºå¥–åŠ±æŒ‡å¯¼çš„è´ªå¿ƒæœç´¢æ—¶ï¼ŒGroundedPRMç”šè‡³è¶…è¶Šäº†ä½¿ç”¨äººå·¥æ ‡è®°ç›‘ç£è®­ç»ƒçš„PRMï¼Œä¸ºé«˜è´¨é‡æµç¨‹çº§æ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯éªŒè¯çš„è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14942v1">PDF</a> 25 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ—¨åœ¨é€šè¿‡ç›‘ç£ä¸­é—´æ­¥éª¤å’Œè¯†åˆ«é”™è¯¯æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ„å»ºæœ‰æ•ˆçš„PRMä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç¼ºä¹å¯æ‰©å±•çš„é«˜å“è´¨æ³¨é‡Šæ˜¯ä¸»è¦åŸå› ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„äººåŠ›æ ‡æ³¨ã€LLMçš„è‡ªæˆ‘è¯„ä»·ï¼ˆå®¹æ˜“å‡ºç°å¹»è§‰ï¼‰æˆ–è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°ç®—ï¼Œåè€…ä»…ä»æ»šåŠ¨ç»“æœæ¨æ–­æ­¥éª¤è´¨é‡ï¼Œä½†ç”±äºä¿¡ç”¨è¯¯åˆ¤ï¼Œç»å¸¸å¼•å…¥å˜ˆæ‚ã€å¤±çœŸçš„ç›‘ç£ã€‚é’ˆå¯¹è¿™äº›æ ¸å¿ƒé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGroundedPRMï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨è¿‡ç¨‹ç›‘ç£çš„æ ‘çŠ¶å¼•å¯¼å’Œä¿çœŸåº¦æ„ŸçŸ¥æ¡†æ¶ã€‚é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ„å»ºç»“æ„åŒ–æ¨ç†è·¯å¾„ï¼Œå‡å°‘å¥–åŠ±å™ªå£°å¹¶å®ç°ç²¾ç»†çš„ä¿¡ç”¨åˆ†é…ã€‚æˆ‘ä»¬åˆ©ç”¨å¤–éƒ¨å·¥å…·éªŒè¯æ¯ä¸ªä¸­é—´æ­¥éª¤ï¼Œæä¾›æ‰§è¡ŒåŸºç¡€æ­£ç¡®æ€§ä¿¡å·ï¼Œæ¶ˆé™¤å¹»è§‰ç›‘ç£ã€‚é€šè¿‡ç»“åˆæ­¥éª¤çº§éªŒè¯å’Œå…¨å±€ç»“æœè¯„ä¼°ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆå¥–åŠ±èšåˆæœºåˆ¶ï¼Œèåˆäº†å·¥å…·éªŒè¯ä¸MCTSæ´¾ç”Ÿçš„åé¦ˆã€‚æœ€åï¼Œæˆ‘ä»¬å°†å¥–åŠ±ä¿¡å·æ ¼å¼åŒ–ä¸ºå¢å¼ºç†æ€§çš„ç”Ÿæˆç»“æ„ï¼Œä»¥ä¿ƒè¿›ä¸æŒ‡ä»¤è°ƒæ•´LLMçš„è§£è¯»å’Œå…¼å®¹æ€§ã€‚GroundedPRMä»…ä½¿ç”¨4ä¸‡è‡ªåŠ¨æ ‡è®°æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä»…ç›¸å½“äºä½¿ç”¨è‡ªåŠ¨æ ‡è®°ç›‘ç£çš„æœ€ä½³PRMæ‰€ç”¨æ•°æ®çš„ååˆ†ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œå®ƒåœ¨ProcessBenchä¸Šçš„å¹³å‡æ€§èƒ½æé«˜äº†26%ã€‚å½“ç”¨äºå¥–åŠ±å¼•å¯¼è´ªå¿ƒæœç´¢æ—¶ï¼ŒGroundedPRMç”šè‡³è¶…è¶Šäº†ä½¿ç”¨äººåŠ›æ ‡è®°ç›‘ç£è®­ç»ƒçš„PRMï¼Œä¸ºé«˜è´¨é‡çš„è¿‡ç¨‹çº§æ¨ç†æä¾›äº†ä¸€æ¡å¯æ‰©å±•å’Œå¯éªŒè¯çš„è·¯å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰PRMæ–¹æ³•é¢ä¸´ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šå¥–åŠ±å™ªå£°ã€ä½äº‹å®ä¿çœŸåº¦å’Œä¸æ­¥éª¤çº§æ¨ç†ç›®æ ‡çš„ä¸å¯¹é½ã€‚</li>
<li>GroundedPRMé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ„å»ºç»“æ„åŒ–æ¨ç†è·¯å¾„ï¼Œå‡å°‘å¥–åŠ±å™ªå£°å¹¶å®ç°ç²¾ç»†çš„ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>GroundedPRMåˆ©ç”¨å¤–éƒ¨å·¥å…·éªŒè¯ä¸­é—´æ­¥éª¤ï¼Œæä¾›æ‰§è¡ŒåŸºç¡€æ­£ç¡®æ€§ä¿¡å·ï¼Œæ¶ˆé™¤å¹»è§‰ç›‘ç£ã€‚</li>
<li>GroundedPRMç»“åˆæ­¥éª¤çº§éªŒè¯å’Œå…¨å±€ç»“æœè¯„ä¼°ï¼Œè®¾è®¡æ··åˆå¥–åŠ±èšåˆæœºåˆ¶ã€‚</li>
<li>GroundedPRMä»…ä½¿ç”¨å°‘é‡è‡ªåŠ¨æ ‡è®°æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dd41ee8a24b5e926edfce49f81364020~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728409&auth_key=1760728409-0-0-15005a3a314f7e4e9399375be1a3b75d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3a6fabc7ec2f2e13ded990993e3bb07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728451&auth_key=1760728451-0-0-dafb72c2bada9a4c0f589202550cfce2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Predicting-Task-Performance-with-Context-aware-Scaling-Laws"><a href="#Predicting-Task-Performance-with-Context-aware-Scaling-Laws" class="headerlink" title="Predicting Task Performance with Context-aware Scaling Laws"></a>Predicting Task Performance with Context-aware Scaling Laws</h2><p><strong>Authors:Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang</strong></p>
<p>Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/context-scaling">https://github.com/wang-research-lab/context-scaling</a>. </p>
<blockquote>
<p>æ¯”ä¾‹å®šå¾‹é€šè¿‡è”ç³»ä¸Šæ¸¸æŒ‡æ ‡ï¼ˆå¦‚äº¤å‰ç†µæŸå¤±ï¼‰ä¸è®¾è®¡å› ç´ ï¼ˆå¦‚æ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ•°æ®å’Œè®¡ç®—ï¼‰æ”¹å˜äº†æˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›ä¼ ç»Ÿå®šå¾‹æ— æ³•æ•æ‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•ã€å¯è§£é‡Šçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºè®­ç»ƒè®¡ç®—å’Œæ‰€æä¾›ä¸Šä¸‹æ–‡çš„å‡½æ•°è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬é€šè¿‡æ‹ŸåˆLlama-2-7Bå’ŒLlama-2-13Bçš„æ‰©å±•ä¸Šä¸‹æ–‡å˜ä½“åœ¨65500ä¸ªå”¯ä¸€å®ä¾‹ä¸­çš„ä¸‹æ¸¸æ€§èƒ½è¡¨ç°æ¥å®è¯éªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ï¼Œè¿™äº›å®ä¾‹è·¨è¶Šä¸‰é¡¹ä»»åŠ¡ï¼šç®—æœ¯æ¨ç†ã€å¸¸è¯†æ¨ç†å’Œæœºå™¨ç¿»è¯‘ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å‡†ç¡®åœ°æ¨¡æ‹Ÿäº†åˆ†å¸ƒå†…ä¸‹æ¸¸æ€§èƒ½è¡¨ç°ï¼Œåœ¨è®­ç»ƒè®¡ç®—æ–¹é¢è·¨è¶Šäº†ä¸‰ä¸ªæ•°é‡çº§çš„èŒƒå›´ï¼Œå¹¶å¯é åœ°é¢„æµ‹äº†éšç€ä¸Šä¸‹æ–‡å¢åŠ è€Œæé«˜çš„æ€§èƒ½è¡¨ç°ã€‚è¿™äº›å‘ç°å¯¹äºè®­ç»ƒè®¡ç®—å’Œä¸Šä¸‹æ–‡åˆ©ç”¨ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/context-scaling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wang-research-lab/context-scalingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14919v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç®€å•ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆå»ºæ¨¡ä¸‹æ¸¸æ€§èƒ½ä½œä¸ºè®­ç»ƒè®¡ç®—å’Œæ‰€æä¾›ä¸Šä¸‹æ–‡çš„åŠŸèƒ½ã€‚é€šè¿‡å¯¹Llama-2çš„ä¸åŒç‰ˆæœ¬åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šçš„ä¸‹æ¸¸æ€§èƒ½è¿›è¡Œå®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå‡†ç¡®æ¨¡æ‹Ÿä¸‹æ¸¸æ€§èƒ½ï¼Œåœ¨è®­ç»ƒè®¡ç®—æ–¹é¢è·¨è¶Šä¸‰ä¸ªæ•°é‡çº§è¿›è¡Œæ¨å¹¿ï¼Œå¹¶å¯é åœ°é¢„æµ‹éšç€ä¸Šä¸‹æ–‡é‡å¢åŠ çš„æ€§èƒ½ã€‚è¿™ä¸ºè®­ç»ƒè®¡ç®—å’Œä¸Šä¸‹æ–‡åˆ©ç”¨ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡LLMsç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè”åˆå»ºæ¨¡ä¸‹æ¸¸æ€§èƒ½çš„ç®€å•ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è€ƒè™‘äº†è®­ç»ƒè®¡ç®—é‡å’Œä¸Šä¸‹æ–‡çš„å½±å“ã€‚</li>
<li>é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œæ‰€æå‡ºçš„æ¡†æ¶èƒ½å¤Ÿå‡†ç¡®æ¨¡æ‹Ÿä¸‹æ¸¸æ€§èƒ½ï¼Œåœ¨è®­ç»ƒè®¡ç®—æ–¹é¢å…·æœ‰è‰¯å¥½çš„æ¨å¹¿æ€§ã€‚</li>
<li>æ¡†æ¶å¯é åœ°é¢„æµ‹äº†éšç€ä¸Šä¸‹æ–‡é‡å¢åŠ çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¸åŒä»»åŠ¡é—´çš„é€šç”¨æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœæ­ç¤ºäº†è®­ç»ƒè®¡ç®—å’Œä¸Šä¸‹æ–‡åˆ©ç”¨ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡LLMsæä¾›äº†æŒ‡å¯¼ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹æ¯”äº†ä¸åŒç‰ˆæœ¬çš„Llama-2æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„å®ç”¨æ€§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†è¯­å¢ƒåœ¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸­çš„é‡è¦æ€§ï¼Œè¡¨æ˜ç†è§£è¯­å¢ƒå¯¹äºä¼˜åŒ–LLMçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d086e31c5e64fb72baf55bd4b0355c59~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728458&auth_key=1760728458-0-0-10dc4557c5f11cf6b7767dabe22e11ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-48f800080ccd736ddedc4fab8ae2afaf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728466&auth_key=1760728466-0-0-0653f9ea92c914c5f119ad9f21481d88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07dfd4190e066b6ffd27340fa7e6a279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728472&auth_key=1760728472-0-0-7b38aef80c3e51a98c4a5f5c3d1e5e84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97d79f88002381a2dd9edd1c4219f66a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728479&auth_key=1760728479-0-0-19ca4cfde6f2b0d285c64f4c9d69b9a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7135da153e7f5413dfd2f6ade749ec7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728485&auth_key=1760728485-0-0-8b0eed1ebccb494441eb99265b2797c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b81458f426e7d35069b7b2df5cbfe58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728492&auth_key=1760728492-0-0-37917b4b9275a2af82abd2d6c60e1b2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1aa9e2bcbbf5b55cfa5673202d72e9ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728499&auth_key=1760728499-0-0-ef776761b9c6cc1136b4b5ad14b2636d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12f50a18ee9a6da6573b24fa021a6e9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728505&auth_key=1760728505-0-0-c2a2f1ae3e30e7e1aa3558ab1ebee0a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Budget-aware-Test-time-Scaling-via-Discriminative-Verification"><a href="#Budget-aware-Test-time-Scaling-via-Discriminative-Verification" class="headerlink" title="Budget-aware Test-time Scaling via Discriminative Verification"></a>Budget-aware Test-time Scaling via Discriminative Verification</h2><p><strong>Authors:Kyle Montgomery, Sijun Tan, Yuqi Chen, Siyuan Zhuang, Tianjun Zhang, Raluca Ada Popa, Chenguang Wang</strong></p>
<p>Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a â€œfreeâ€ upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/verification">https://github.com/wang-research-lab/verification</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½çš„å¼ºå¤§ç­–ç•¥ã€‚è™½ç„¶æœ€å…ˆè¿›çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨ç”Ÿæˆå¼éªŒè¯å™¨ä»å€™é€‰æ± ä¸­æŒ‘é€‰æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œä½†è¿™ç§æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå®ç”¨æ€§å—é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹è½¬å‘æ›´å…·é¢„ç®—æ„è¯†çš„èŒƒå¼ï¼šåˆ¤åˆ«å¼éªŒè¯ã€‚æˆ‘ä»¬è¿›è¡Œäº†å½»åº•çš„å®è¯åˆ†æï¼Œå¹¶è¯æ˜è™½ç„¶å­¤ç«‹ä½¿ç”¨åˆ¤åˆ«å¼éªŒè¯å™¨å¯èƒ½è¡¨ç°ä¸ä½³ï¼Œä½†å°†å…¶ä¸è‡ªæˆ‘ä¸€è‡´æ€§ç›¸ç»“åˆï¼Œåœ¨æ··åˆæ–¹æ³•ä¸­å¯åˆ›å»ºå¼ºå¤§ä¸”é«˜æ•ˆçš„æµ‹è¯•æ—¶ç¼©æ”¾æœºåˆ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å›ºå®šçš„è®¡ç®—é¢„ç®—ä¸‹ï¼Œè¿™ç§æ··åˆæ–¹æ³•å¤§å¹…è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç”Ÿæˆå¼éªŒè¯æ–¹æ³•ï¼šåœ¨AIME2025ä¸Šè¾¾åˆ°äº†é«˜è¾¾15.3%çš„å‡†ç¡®ç‡æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºå®é™…åº”ç”¨å’Œç°å®ä¸–ç•Œåº”ç”¨ï¼Œé‡‡ç”¨é¢„ç®—æ„è¯†çš„åˆ¤åˆ«å¼éªŒè¯å™¨è¿›è¡Œç¼©æ”¾ä¸ä»…æ˜¯ä¸€ä¸ªå…è´¹çš„è‡ªæˆ‘ä¸€è‡´æ€§å‡çº§ï¼Œè€Œä¸”ä¹Ÿæ˜¯ä¸€ç§æˆæœ¬æ›´é«˜çš„ç”ŸæˆæŠ€æœ¯çš„æ›´é«˜æ•ˆå’Œæœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/verification%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wang-research-lab/verificationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14913v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥å¯æœ‰æ•ˆæå‡æ€§èƒ½ã€‚å°½ç®¡ç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨ç”Ÿæˆå¼éªŒè¯å™¨ä»å€™é€‰æ± ä¸­ç­›é€‰æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œä½†è¿™ç§æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶è½¬å‘æ›´æ³¨é‡é¢„ç®—çš„åˆ¤åˆ«å¼éªŒè¯æ–¹æ³•ã€‚è™½ç„¶å­¤ç«‹åœ°çœ‹ï¼Œåˆ¤åˆ«å¼éªŒè¯å™¨æ€§èƒ½å¯èƒ½ä¸å¦‚ç”Ÿæˆå¼éªŒè¯å™¨ï¼Œä½†å½“ä¸è‡ªæˆ‘ä¸€è‡´æ€§ç›¸ç»“åˆæ—¶ï¼Œåœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾æœºåˆ¶ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ•ˆç‡å’Œæ•ˆæœã€‚åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œè¿™ç§æ··åˆæ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç”Ÿæˆå¼éªŒè¯æ–¹æ³•ï¼Œåœ¨AIME2025ä¸Šæé«˜äº†é«˜è¾¾15.3%çš„å‡†ç¡®ç‡ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œå¯¹äºå®é™…åº”ç”¨ï¼Œé¢„ç®—æ„è¯†çš„åˆ¤åˆ«å¼éªŒè¯ä¸ä»…æ˜¯è‡ªæˆ‘ä¸€è‡´æ€§çš„â€œå…è´¹â€å‡çº§ï¼Œè€Œä¸”æ˜¯æˆæœ¬é«˜æ˜‚çš„ç”ŸæˆæŠ€æœ¯çš„æ›´é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥å¯¹äºæé«˜å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ç”Ÿæˆå¼éªŒè¯å™¨è™½ç„¶æ€§èƒ½ä¼˜è¶Šï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>åˆ¤åˆ«å¼éªŒè¯å™¨ä½œä¸ºä¸€ç§æ›´é¢„ç®—æ„è¯†çš„ç­–ç•¥è¢«æå‡ºã€‚</li>
<li>åˆ¤åˆ«å¼éªŒè¯å™¨ä¸è‡ªæˆ‘ä¸€è‡´æ€§ç›¸ç»“åˆçš„æ··åˆæ–¹æ³•åœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
<li>åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œæ··åˆæ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç”Ÿæˆå¼éªŒè¯æ–¹æ³•ï¼Œæé«˜äº†AIME2025ä¸Šçš„å‡†ç¡®ç‡ã€‚</li>
<li>é¢„ç®—æ„è¯†çš„åˆ¤åˆ«å¼éªŒè¯ä¸ä»…æ˜¯è‡ªæˆ‘ä¸€è‡´æ€§çš„å‡çº§ï¼Œè€Œä¸”ç›¸å¯¹æˆæœ¬é«˜æ˜‚çš„ç”ŸæˆæŠ€æœ¯æ›´ä¸ºé«˜æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2422b8d3c9fb0296a9af580c5dc0c630~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728512&auth_key=1760728512-0-0-48d16a9128f5a0e21a9c77ab693e64f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-037fd5225188d1c30ab2a69f90747d2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728519&auth_key=1760728519-0-0-2ab9a357bfa5102d92295debe53c647f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-091268d32be6efcea5cbf0ee89e5cacf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728526&auth_key=1760728526-0-0-4734d882ed7c891c9f76378ee36d6489&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-607ce8bf9c360c76430dfcdbfb619684~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728533&auth_key=1760728533-0-0-7bfb124d82b974d9004bdd960980d698&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e97c6cec93a518187b5df1380ec8a8c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728539&auth_key=1760728539-0-0-83e8c648e6a6378d9612493b6385d4fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Multimodal-LLM-Descriptions-of-Activity-for-Explainable-Semi-Supervised-Video-Anomaly-Detection"><a href="#Leveraging-Multimodal-LLM-Descriptions-of-Activity-for-Explainable-Semi-Supervised-Video-Anomaly-Detection" class="headerlink" title="Leveraging Multimodal LLM Descriptions of Activity for Explainable   Semi-Supervised Video Anomaly Detection"></a>Leveraging Multimodal LLM Descriptions of Activity for Explainable   Semi-Supervised Video Anomaly Detection</h2><p><strong>Authors:Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz</strong></p>
<p>Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies. </p>
<blockquote>
<p>ç°æœ‰çš„åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ–¹æ³•åœ¨æ£€æµ‹æ¶‰åŠå¯¹è±¡äº¤äº’çš„å¤æ‚å¼‚å¸¸æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå¹¶ä¸”é€šå¸¸ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–°å‹VADæ¡†æ¶ã€‚ä¸ä»¥å‰åŸºäºMLLMçš„æ–¹æ³•ç›´æ¥åœ¨å¸§çº§åˆ«è¿›è¡Œå¼‚å¸¸åˆ¤æ–­ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºæå–å’Œè§£é‡Šå¯¹è±¡æ´»åŠ¨å’Œéšæ—¶é—´å˜åŒ–çš„äº¤äº’ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨MLLMæŸ¥è¯¢ä¸åŒæ—¶åˆ»çš„å¯¹è±¡å¯¹çš„è§†è§‰è¾“å…¥ï¼Œä»æ ‡å‡†è§†é¢‘ä¸­ç”Ÿæˆæ´»åŠ¨å’Œäº¤äº’çš„æ–‡æœ¬æè¿°ã€‚è¿™äº›æ–‡æœ¬æè¿°ä½œä¸ºè§†é¢‘ä¸­å¯¹è±¡æ´»åŠ¨å’Œäº¤äº’çš„é«˜çº§è¡¨ç¤ºã€‚å®ƒä»¬åœ¨æµ‹è¯•æ—¶é€šè¿‡æ¯”è¾ƒæ ‡å‡†è®­ç»ƒè§†é¢‘ä¸­çš„æ–‡æœ¬æè¿°æ¥æ£€æµ‹å¼‚å¸¸ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ¬è´¨ä¸Šæä¾›äº†å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä¼ ç»ŸVADæ–¹æ³•ç»“åˆä½¿ç”¨ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶å¯è§£é‡Šæ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æœ‰æ•ˆåœ°æ£€æµ‹åŸºäºäº¤äº’çš„å¼‚å¸¸ï¼Œè€Œä¸”åœ¨æ²¡æœ‰äº¤äº’å¼‚å¸¸çš„æ•°æ®é›†ä¸Šä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14896v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ç°æœ‰åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ–¹æ³•åœ¨æ£€æµ‹æ¶‰åŠå¯¹è±¡äº¤äº’çš„å¤æ‚å¼‚å¸¸æ—¶é¢ä¸´çš„æŒ‘æˆ˜åŠå…¶ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„VADæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡æŸ¥è¯¢MLLMä»¥è·å–å¯¹è±¡æ´»åŠ¨çš„æ–‡æœ¬æè¿°ï¼Œå¹¶åˆ©ç”¨è¿™äº›æè¿°æ¥æ£€æµ‹å¼‚å¸¸ã€‚è¿™ç§æ–¹æ³•æé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹å¤æ‚äº¤äº’å¼‚å¸¸æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨æ£€æµ‹æ¶‰åŠå¯¹è±¡äº¤äº’çš„å¤æ‚å¼‚å¸¸æ—¶å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„VADæ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æŸ¥è¯¢MLLMè·å–å¯¹è±¡æ´»åŠ¨çš„æ–‡æœ¬æè¿°ï¼Œè¿™äº›æè¿°ä½œä¸ºè§†é¢‘ä¸­å¯¹è±¡æ´»åŠ¨å’Œäº¤äº’çš„é«˜çº§è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡å°†æµ‹è¯•æ—¶çš„æè¿°ä¸è®­ç»ƒè§†é¢‘ä¸­å‘ç°çš„æè¿°è¿›è¡Œæ¯”è¾ƒæ¥æ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å¯ä¸ä¼ ç»ŸVADæ–¹æ³•ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜å…¶å¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹å¤æ‚äº¤äº’å¼‚å¸¸æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ— äº¤äº’å¼‚å¸¸çš„æ•°æ®é›†ä¸Šä¹Ÿè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰æœ›æ”¹è¿›ç°æœ‰çš„VADæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¶‰åŠå¯¹è±¡äº¤äº’çš„å¤æ‚å¼‚å¸¸æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1c1bcb30db1ab3ddcea8597756e759fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728547&auth_key=1760728547-0-0-ad3b679bf173de136ae951e33f3d2da5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aba016c47b20ddc8e91e59d60c7fcdac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728554&auth_key=1760728554-0-0-8e53a339bd46835241c388946d7cf2f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1c96e48c2bb8622bf84c83527001c8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728561&auth_key=1760728561-0-0-468c07bf74c478f7eca1785ab44cdec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17a1065474dcd508e8d3507c014e5164~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728568&auth_key=1760728568-0-0-bd48fccd793be65e52dd580284ad5a21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="You-May-Speak-Freely-Improving-the-Fine-Grained-Visual-Recognition-Capabilities-of-Multimodal-Large-Language-Models-with-Answer-Extraction"><a href="#You-May-Speak-Freely-Improving-the-Fine-Grained-Visual-Recognition-Capabilities-of-Multimodal-Large-Language-Models-with-Answer-Extraction" class="headerlink" title="You May Speak Freely: Improving the Fine-Grained Visual Recognition   Capabilities of Multimodal Large Language Models with Answer Extraction"></a>You May Speak Freely: Improving the Fine-Grained Visual Recognition   Capabilities of Multimodal Large Language Models with Answer Extraction</h2><p><strong>Authors:Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn</strong></p>
<p>Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or donâ€™t consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language. </p>
<blockquote>
<p>å°½ç®¡ç”±äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…´èµ·ï¼Œé›¶æ ·æœ¬è§†è§‰åˆ†ç±»é‡æ–°å¼•èµ·äº†äººä»¬çš„å…´è¶£ï¼Œä½†è¯„ä¼°è‡ªå›å½’æ¨¡å‹çš„è‡ªç”±å½¢å¼å“åº”çš„é—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤§å¤šæ•°å·¥ä½œéƒ½ä¸“æ³¨äºçº¯è¯­è¨€ä»»åŠ¡ï¼Œæˆ–è€…ä¸è€ƒè™‘è¶…è¿‡5ç§é€‰æ‹©çš„å¤šä¸ªé€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œè€Œè¿™ä¸¤ç§èƒ½åŠ›å¯¹äºè§£å†³ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­çš„ä»»åŠ¡éƒ½æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºFGVCä¸­çš„é€‰æ‹©è®¡æ•°é€šå¸¸åœ¨æ•°ç™¾åˆ°æ•°åƒä¹‹é—´ï¼Œå¹¶ä¸”å„ä¸ªé€‰æ‹©ä¹‹é—´é«˜åº¦ç›¸å…³ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ç§é«˜åº¦å¤šé€‰æ‹©çš„MCQç¯å¢ƒä¸­ï¼Œå°šä¸æ¸…æ¥šå¦‚ä½•å°†LLMé€‰æ‹©æå–æ‰©å±•åˆ°åŸºäºæ£€ç´¢çš„é—®é¢˜ï¼Œåœ¨åŸºäºæ£€ç´¢çš„é—®é¢˜ä¸­ï¼Œåœ¨è®¡ç®—é€‰æ‹©é›†ä¸Šçš„æ¦‚ç‡æ˜¯éå¸¸è®¡ç®—å¯†é›†å‹çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†nlg2choiceï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆå‘MLLMæå‡ºä»»åŠ¡ç›¸å…³çš„å¼€æ”¾å¼é—®é¢˜ï¼Œå‡ ä¹æ²¡æœ‰çº¦æŸï¼Œç„¶åä½¿ç”¨çº¯æ–‡æœ¬çº¦æŸè§£ç æ¥é¢„æµ‹æœ€å¯èƒ½çš„ç­”æ¡ˆã€‚åœ¨æ£€ç´¢è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨æ—©æœŸåœæ­¢æ–¹æ³•è®¡ç®—çº¦æŸå“åº”é€‰æ‹©è¯¥ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œä»¥æ˜¾è‘—æé«˜ååé‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç»†ç²’åº¦è§†è§‰æ•°æ®é›†çš„åˆ†ç±»å’Œæ£€ç´¢è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•å‡æœ‰æ‰€æé«˜ï¼Œå¹¶è¯æ˜è¿™ç§æ€§èƒ½åœ¨å„ç§LLMç”¨æˆ·ä»¥è‡ªç„¶è¯­è¨€æ‰§è¡Œä»»åŠ¡çš„æ–¹å¼ä¸­éƒ½èƒ½ä¿æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14885v1">PDF</a> Accepted to WACV26. 12 pages, 8 tables, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é›¶æ ·æœ¬è§†è§‰åˆ†ç±»ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­çš„è‡ªç”±å½¢å¼å“åº”è¯„ä»·é—®é¢˜ã€‚ç°æœ‰çš„å·¥ä½œå¤šèšç„¦äºè¯­è¨€ä»»åŠ¡æˆ–ä»…é™äºäº”é€‰ä¸€çš„é€‰æ‹©é¢˜åœºæ™¯ï¼Œå¯¹äºå¤šé€‰é¡¹åœºæ™¯çš„åº”ç”¨æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºnlg2choiceçš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆåœ¨æ— çº¦æŸæƒ…å†µä¸‹å‘MLLMæå‡ºé—®é¢˜ï¼Œå†åˆ©ç”¨æ–‡æœ¬çº¦æŸè§£ç é¢„æµ‹æœ€å¯èƒ½çš„é€‰é¡¹ã€‚åœ¨æ£€ç´¢åœºæ™¯ä¸‹ï¼Œé‡‡ç”¨æ—©æœŸåœæ­¢æ³•è®¡ç®—çº¦æŸå“åº”çš„æ¦‚ç‡ï¼Œæé«˜è¿è¡Œæ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªç»†ç²’åº¦è§†è§‰æ•°æ®é›†ä¸Šçš„åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”å¤šç§ä¸åŒä»»åŠ¡éœ€æ±‚çš„è¯­è¨€è¡¨è¾¾æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é›¶æ ·æœ¬è§†è§‰åˆ†ç±»ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­å¦‚ä½•è¯„ä¼°è‡ªç”±å½¢å¼å“åº”çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰å·¥ä½œå¤šèšç„¦äºè¯­è¨€ä»»åŠ¡æˆ–å±€é™äºäº”é€‰ä¸€é€‰æ‹©é¢˜åœºæ™¯ï¼Œç¼ºä¹åœ¨å¤šé€‰é¡¹åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºnlg2choiceçš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆå¯¹MLLMè¿›è¡Œæ— çº¦æŸé—®é¢˜çš„æé—®ï¼Œç„¶åé€šè¿‡æ–‡æœ¬çº¦æŸè§£ç é¢„æµ‹æœ€å¯èƒ½çš„é€‰é¡¹ã€‚</li>
<li>åœ¨æ£€ç´¢åœºæ™¯ä¸‹ï¼Œä½¿ç”¨æ—©æœŸåœæ­¢æ³•è®¡ç®—çº¦æŸå“åº”çš„æ¦‚ç‡ï¼Œæ˜¾è‘—æé«˜è¿è¡Œæ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªç»†ç²’åº¦è§†è§‰æ•°æ®é›†ä¸Šçš„åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡è¡¨ç°ä¼˜è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3cb3645d7df221febbbd9e9281ea6d80~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728575&auth_key=1760728575-0-0-10a295fc6a35089dd44a5a6fcd7d6d47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21f82e40facc5fb8cf0e426afadacd15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728582&auth_key=1760728582-0-0-e1db0d72181ebb4a4d38ca07106e77c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdd35d319ad32a2ec59c4c8cfddd69c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728589&auth_key=1760728589-0-0-773c1ddd15f8a13c4fa4da56d0e45178&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91bb271263f798690fa14158df96cafc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728596&auth_key=1760728596-0-0-faecb938cd2d49e82659be7f1f88494f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ed2f2f332b1cb11710be6dca9aa42d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728604&auth_key=1760728604-0-0-99254630a06daeef8ea6087018d16cd8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Multimodal-Large-Language-Models-for-Face-Recognition"><a href="#Benchmarking-Multimodal-Large-Language-Models-for-Face-Recognition" class="headerlink" title="Benchmarking Multimodal Large Language Models for Face Recognition"></a>Benchmarking Multimodal Large Language Models for Face Recognition</h2><p><strong>Authors:Hatef Otroshi Shahreza, SÃ©bastien Marcel</strong></p>
<p>Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨äººè„¸è¯†åˆ«æ–¹é¢çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚å°¤å…¶æ˜¯å¼€æºçš„MLLMsçš„æ€§èƒ½éœ€è¦é€šè¿‡æ ‡å‡†åè®®åœ¨äººè„¸è¯†åˆ«ä»»åŠ¡ä¸Šä¸ç°æœ‰çš„äººè„¸è¯†åˆ«æ¨¡å‹è¿›è¡Œæ¯”è¾ƒå’Œè¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªäººè„¸è¯†åˆ«æ•°æ®é›†ä¸Šï¼ŒåŒ…æ‹¬LFWã€CALFWã€CPLFWã€CFPã€AgeDBå’ŒRFWç­‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†æœ€æ–°çš„MLLMsäººè„¸è¯†åˆ«ç³»ç»ŸåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶MLLMsèƒ½å¤Ÿæ•æ‰åˆ°ä¸°å¯Œçš„äººè„¸è¯­ä¹‰çº¿ç´¢ï¼Œå¯¹äºäººè„¸è¯†åˆ«ä»»åŠ¡éå¸¸æœ‰ç”¨ï¼Œä½†åœ¨é›¶æ ·æœ¬åº”ç”¨çš„ç²¾ç¡®è¯†åˆ«åœºæ™¯ä¸­ï¼Œå®ƒä»¬è½åäºä¸“ä¸šæ¨¡å‹ã€‚è¿™ä¸€åŸºå‡†æµ‹è¯•ä¸ºåŸºäºMLLMçš„äººè„¸è¯†åˆ«æŠ€æœ¯æä¾›äº†å‘å±•åŸºç¡€ï¼Œä¸ºè®¾è®¡ä¸‹ä¸€ä»£æ›´é«˜ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æºä»£ç å·²åœ¨é¡¹ç›®é¡µé¢å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14866v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨å¤šç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨äººè„¸è¯†åˆ«é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å¯¹æœ€å…ˆè¿›çš„MLLMsè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œæ¶‰åŠå¤šä¸ªäººè„¸è¯†åˆ«æ•°æ®é›†ï¼ŒåŒ…æ‹¬LFWã€CALFWç­‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶MLLMsæ•è·äº†ä¸°å¯Œçš„äººè„¸è¯­ä¹‰çº¿ç´¢ï¼Œä½†åœ¨é›¶æ ·æœ¬åº”ç”¨çš„é«˜ç²¾åº¦è¯†åˆ«åœºæ™¯ä¸­ï¼Œä»è½åäºä¸“ç”¨æ¨¡å‹ã€‚æœ¬è¯„ä¼°ä¸ºæ¨è¿›MLLMåœ¨äººè„¸è¯†åˆ«ä¸­çš„åº”ç”¨æä¾›äº†åŸºç¡€ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£æ¨¡å‹çš„æ›´é«˜ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›è®¾è®¡æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>äººè„¸è¯†åˆ«é¢†åŸŸå¯¹MLLMsçš„åº”ç”¨æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>MLLMsåœ¨äººè„¸è¯†åˆ«æ•°æ®é›†ä¸Šçš„ç³»ç»Ÿè¯„ä¼°æ˜¯å¿…è¦çš„ã€‚</li>
<li>MLLMsæ•è·äº†ä¸°å¯Œçš„äººè„¸è¯­ä¹‰çº¿ç´¢ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬åº”ç”¨çš„é«˜ç²¾åº¦è¯†åˆ«åœºæ™¯ä¸­ï¼ŒMLLMsçš„è¡¨ç°ä»è½åäºä¸“ç”¨æ¨¡å‹ã€‚</li>
<li>æœ¬è¯„ä¼°ä¸ºæ¨è¿›MLLMåœ¨äººè„¸è¯†åˆ«ä¸­çš„åº”ç”¨æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ca469d5c200d3b3af98196d5f8919c38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728611&auth_key=1760728611-0-0-38b144a3e0f7089f697157eb0879d8f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a8edfc4f3f1e345993ee3c6c35cc113~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728619&auth_key=1760728619-0-0-950cc228fe7503bb75f63c74f3b131b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d35e40af7c67c6c5e1c3adc69e847f0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728625&auth_key=1760728625-0-0-60e37efe363f451772939477826ab5db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-625ea576d78200b97dc8da0507aace2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728632&auth_key=1760728632-0-0-b2876a3bc02537b62f4dab292c5a7943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning"><a href="#RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning" class="headerlink" title="RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning"></a>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</h2><p><strong>Authors:Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li</strong></p>
<p>Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the modelâ€™s shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark. </p>
<blockquote>
<p>æé«˜å®ä½“ä»£ç†çš„æ¨ç†èƒ½åŠ›å¯¹äºæœºå™¨äººåœ¨é•¿æœŸè§†è§’æ“æ§ä»»åŠ¡ä¸­æˆåŠŸå®Œæˆå¤æ‚çš„äººç±»æŒ‡ä»¤è‡³å…³é‡è¦ã€‚å°½ç®¡åŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä»é¢ä¸´ç€åœ¨å¤æ‚çš„çœŸå®ç¯å¢ƒä¸­æ‰§è¡Œé•¿æœŸè§†è§’æ“æ§ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬æœ‰é™çš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚è€ƒè™‘åˆ°é€šè¿‡ç›‘ç£å¾®è°ƒå°†é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸æœºå™¨äººè§„åˆ’ä»»åŠ¡å¯¹é½å­˜åœ¨é€šç”¨æ€§å·®å’Œç‰©ç†ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoboGPT-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå®ä½“è§„åˆ’çš„ä¸¤é˜¶æ®µå¾®è°ƒæ¡†æ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œç›‘ç£è®­ç»ƒé€šè¿‡ä¸“å®¶åºåˆ—è·å¾—åŸºç¡€çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è§£å†³æ¨¡å‹åœ¨è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºäº†å®ç°å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„ç‰©ç†ç†è§£å’ŒåŠ¨ä½œåºåˆ—ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶è€ƒè™‘äº†é•¿æœŸæ€§èƒ½å’Œç¯å¢ƒä¸­çš„åŠ¨ä½œçº¦æŸã€‚åœ¨EmbodiedBenchåŸºå‡†æµ‹è¯•ä¸Šï¼Œç»è¿‡Qwen2. -VLä¸Šè®­ç»ƒçš„æ¨ç†æ¨¡å‹æ˜¾è‘—ä¼˜äºæ›´å¤§è§„æ¨¡çš„GPT-4o-miniæ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†21.33%ï¼Œå¹¶ä¸”è¶…è¿‡äº†å…¶ä»–åœ¨Qwen2.VLä¸Šè®­ç»ƒçš„å·¥ä½œï¼Œå‡†ç¡®ç‡æé«˜äº†20.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººå®Œæˆå¤æ‚äººç±»æŒ‡ä»¤çš„é•¿æœŸæ“ä½œä»»åŠ¡ä¸­ï¼Œæé«˜å…¶æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºç›‘ç£å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä»é¢ä¸´åœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­æ‰§è¡Œé•¿æœŸæ“ä½œä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬çš„é€šç”¨å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RoboGPT-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºå®ä½“è§„åˆ’ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ä¸“å®¶åºåˆ—è¿›è¡Œæœ‰ç›‘ç£çš„è®­ç»ƒä»¥è·å–åŸºç¡€çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è§£å†³æ¨¡å‹åœ¨è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºå®ç°å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„ç‰©ç†ç†è§£å’ŒåŠ¨ä½œåºåˆ—ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶è€ƒè™‘é•¿æœŸæ€§èƒ½å’Œç¯å¢ƒä¸­çš„åŠ¨ä½œçº¦æŸã€‚åœ¨EmbodiedBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨ç†æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºè¾ƒå¤§è§„æ¨¡çš„GPT-4o-miniæ¨¡å‹ï¼Œæé«˜äº†21.33%ï¼Œå¹¶ä¸”è¶…è¶Šäº†å…¶ä»–åœ¨Qwen2.5-VL-7Bä¸Šè®­ç»ƒçš„å·¥ä½œï¼Œæé«˜äº†20.33%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æé«˜æœºå™¨äººçš„æ¨ç†èƒ½åŠ›æ˜¯å®Œæˆå¤æ‚äººç±»æŒ‡ä»¤çš„é•¿æœŸæ“ä½œä»»åŠ¡çš„å…³é”®ã€‚</li>
<li>åŸºäºç›‘ç£å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œç¯å¢ƒå’Œé•¿æœŸæ“ä½œä»»åŠ¡ã€‚</li>
<li>æå‡ºRoboGPT-R1æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒæ¥æé«˜å®ä½“è§„åˆ’èƒ½åŠ›ï¼ŒåŒ…æ‹¬æœ‰ç›‘ç£çš„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>æœ‰ç›‘ç£çš„è®­ç»ƒé˜¶æ®µé€šè¿‡ä¸“å®¶åºåˆ—è·å–åŸºç¡€çŸ¥è¯†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µè§£å†³æ¨¡å‹åœ¨è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>è®¾è®¡åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ä»¥å®ç°å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„ç‰©ç†ç†è§£å’ŒåŠ¨ä½œåºåˆ—ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a099ba6074f47e3e889319f629068932~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728639&auth_key=1760728639-0-0-ac6b1196037f26fd7d8c13994397584c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e69d26203f68eba4ccdf3dc77eb805db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728647&auth_key=1760728647-0-0-bfc9a71487bcc74ce680e87f62029848&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7422f8a710d4ef94e3e57bf037190e52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728654&auth_key=1760728654-0-0-313e76f70db66ca11bd35a172ebcc466&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Evaluating-Reducing-Deceptive-Dialogue-From-Language-Models-with-Multi-turn-RL"><a href="#Evaluating-Reducing-Deceptive-Dialogue-From-Language-Models-with-Multi-turn-RL" class="headerlink" title="Evaluating &amp; Reducing Deceptive Dialogue From Language Models with   Multi-turn RL"></a>Evaluating &amp; Reducing Deceptive Dialogue From Language Models with   Multi-turn RL</h2><p><strong>Authors:Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine</strong></p>
<p>Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¨çƒèŒƒå›´å†…ä¸æ•°ä»¥ç™¾ä¸‡è®¡çš„ç”¨æˆ·è¿›è¡Œäº¤äº’ï¼Œå¹¿æ³›åº”ç”¨äºå®¢æˆ·æœåŠ¡ã€æ•™è‚²å’ŒåŒ»ç–—ä¿å¥ç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œæ— è®ºæ˜¯æ•…æ„è¿˜æ˜¯æ— æ„ä¸­ï¼Œå®ƒä»¬äº§ç”Ÿæ¬ºéª—è¾“å‡ºçš„èƒ½åŠ›å¼•å‘äº†ä¸¥é‡çš„å®‰å…¨é—®é¢˜ã€‚LLMè¡Œä¸ºçš„ä¸å¯é¢„æµ‹æ€§ï¼Œä»¥åŠå¯¹å¹»è§‰ã€è¯¯å¯¼ä¿¡æ¯å’Œç”¨æˆ·æ“çºµçš„é˜²æŠ¤æªæ–½ä¸è¶³ï¼Œä½¿å¾—å®ƒä»¬çš„æ»¥ç”¨æˆä¸ºä¸€ä¸ªä¸¥è‚ƒä¸”çœŸå®çš„å¨èƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMåœ¨å¯¹è¯ä¸­äº§ç”Ÿæ¬ºéª—çš„ç¨‹åº¦ï¼Œå¹¶æå‡ºäº†ä¿¡å¿µé”™ä½åº¦é‡æ ‡å‡†æ¥é‡åŒ–æ¬ºéª—è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨å››ç§ä¸åŒçš„å¯¹è¯åœºæ™¯ä¸­è¯„ä¼°æ¬ºéª—è¡Œä¸ºï¼Œä½¿ç”¨äº†äº”ç§æˆç†Ÿçš„æ¬ºéª—æ£€æµ‹æŒ‡æ ‡å’Œæˆ‘ä»¬æå‡ºçš„æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿™ä¸€æ–°é¢–çš„æ¬ºéª—åº¦é‡æ ‡å‡†ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ¯”æˆ‘ä»¬æµ‹è¯•çš„æ‰€æœ‰ç°æœ‰æŒ‡æ ‡éƒ½æ›´é«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€å…ˆè¿›çš„æ¨¡å‹çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå³ä½¿åœ¨çœ‹ä¼¼æ— å®³çš„ç›®æ ‡æç¤ºä¸‹ï¼ŒLLMåœ¨å¤§çº¦26%çš„å¯¹è¯è½®æ¬¡ä¸­ä¹Ÿä¼šè‡ªç„¶è¡¨ç°å‡ºæ¬ºéª—è¡Œä¸ºã€‚å½“è¢«æç¤ºè¿›è¡Œæ¬ºéª—æ—¶ï¼ŒLLMæœ‰èƒ½åŠ›å°†æ¬ºéª—æ€§æé«˜é«˜è¾¾åŸºçº¿æ°´å¹³çš„31%ã€‚å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œä½¿ç”¨RLHFï¼ˆç¡®ä¿å¹¿æ³›éƒ¨ç½²çš„LLMå®‰å…¨çš„ä¸»è¦æ–¹æ³•ï¼‰è®­ç»ƒçš„æ¨¡å‹ä»ç„¶å¹³å‡ä»¥43%çš„é€Ÿç‡è¡¨ç°å‡ºæ¬ºéª—è¡Œä¸ºã€‚é‰´äºå¯¹è¯ä¸­çš„æ¬ºéª—è¡Œä¸ºæ˜¯åœ¨äº¤äº’å†å²ä¸­å‘å±•èµ·æ¥çš„ï¼Œå¯¹å…¶çš„æœ‰æ•ˆè¯„ä¼°å’Œç¼“è§£éœ€è¦è¶…è¶Šå•å¥åˆ†æã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šè½®å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘æ¬ºéª—è¡Œä¸ºï¼Œä¸å…¶ä»–æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ç›¸æ¯”ï¼Œè¿™å¯¼è‡´äº†77.6%çš„å‡å°‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14318v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®¢æˆ·æ”¯æŒã€æ•™è‚²å’ŒåŒ»ç–—ç­‰é¢†åŸŸä¸æ•°ç™¾ä¸‡å…¨çƒç”¨æˆ·äº’åŠ¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬æœ‰æ„æ— æ„äº§ç”Ÿæ¬ºéª—è¾“å‡ºçš„èƒ½åŠ›å¼•å‘äº†é‡å¤§å®‰å…¨æ‹…å¿§ã€‚LLMè¡Œä¸ºçš„ä¸å¯é¢„æµ‹æ€§ï¼Œä»¥åŠå¯¹å¹»è§‰ã€è¯¯å¯¼ä¿¡æ¯å’Œç”¨æˆ·æ“çºµçš„é˜²æŠ¤ä¸è¶³ï¼Œä½¿å¾—å…¶æ»¥ç”¨æˆä¸ºä¸€ä¸ªä¸¥é‡çš„ç°å®é—®é¢˜ã€‚æœ¬æ–‡è°ƒæŸ¥äº†LLMåœ¨å¯¹è¯ä¸­è¿›è¡Œæ¬ºéª—çš„ç¨‹åº¦ï¼Œå¹¶æå‡ºäº†ä¿¡å¿µé”™ä½åº¦é‡æ¥è¡¡é‡æ¬ºéª—ã€‚æˆ‘ä»¬åœ¨å››ç§ä¸åŒçš„å¯¹è¯åœºæ™¯ä¸­è¯„ä¼°äº†æ¬ºéª—è¡Œä¸ºï¼Œä½¿ç”¨äº†äº”ç§ç°æœ‰çš„æ¬ºéª—æ£€æµ‹æŒ‡æ ‡å’Œæˆ‘ä»¬æå‡ºçš„æŒ‡æ ‡ã€‚ç ”ç©¶å‘ç°ï¼Œæ–°çš„æ¬ºéª—åº¦é‡æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ›´é«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨æ˜LLMåœ¨å¯¹è¯å›åˆä¸­çº¦æœ‰26%è¡¨ç°å‡ºè‡ªç„¶æ¬ºéª—è¡Œä¸ºï¼Œå³ä½¿åœ¨çœ‹ä¼¼æ— å®³çš„ç›®æ ‡æç¤ºä¸‹ä¹Ÿä¼šå¦‚æ­¤ã€‚å½“è¢«æç¤ºè¿›è¡Œæ¬ºéª—æ—¶ï¼ŒLLMçš„æ¬ºéª—æ€§å¯èƒ½å¢åŠ é«˜è¾¾31%ã€‚å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œä½¿ç”¨RLHFï¼ˆç¡®ä¿å¹¿æ³›éƒ¨ç½²çš„LLMå®‰å…¨çš„ä¸»è¦æ–¹æ³•ï¼‰è®­ç»ƒçš„æ¨¡å‹å¹³å‡æ¬ºéª—ç‡ä¸º43%ã€‚ç”±äºå¯¹è¯ä¸­çš„æ¬ºéª—è¡Œä¸ºæ˜¯éšç€äº’åŠ¨å†å²è€Œå‘å±•çš„ï¼Œå…¶æœ‰æ•ˆè¯„ä¼°å’Œç¼“è§£éœ€è¦è¶…è¶Šå•å‘è¨€çš„åˆ†æã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šè½®å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥å¾®è°ƒLLMä»¥å‡å°‘æ¬ºéª—è¡Œä¸ºï¼Œä¸å…¶ä»–æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ç›¸æ¯”ï¼Œè¿™å¯¼è‡´äº†77.6%çš„å‡å°‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨ä¸å…¨çƒç”¨æˆ·çš„äº’åŠ¨ä¸­è¡¨ç°å‡ºæ¬ºéª—è¡Œä¸ºï¼Œè¿™å¼•å‘äº†é‡å¤§å®‰å…¨æ‹…å¿§ã€‚</li>
<li>LLMè¡Œä¸ºçš„ä¸å¯é¢„æµ‹æ€§ä½¿å¾—å…¶æ»¥ç”¨æˆä¸ºä¸€ä¸ªä¸¥é‡çš„ç°å®é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¿¡å¿µé”™ä½åº¦é‡æŒ‡æ ‡ï¼Œæ›´å‡†ç¡®åœ°è¡¡é‡LLMçš„æ¬ºéª—è¡Œä¸ºã€‚</li>
<li>åœ¨å››ç§å¯¹è¯åœºæ™¯ä¸­è¯„ä¼°äº†æ¬ºéª—è¡Œä¸ºï¼Œå‘ç°ç°æœ‰æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>å…«ç§æœ€å…ˆè¿›çš„LLMæ¨¡å‹è‡ªç„¶è¡¨ç°å‡ºæ¬ºéª—è¡Œä¸ºï¼Œå æ¯”çº¦26%ã€‚</li>
<li>å½“è¢«æç¤ºè¿›è¡Œæ¬ºéª—æ—¶ï¼ŒLLMçš„æ¬ºéª—æ€§å¯èƒ½å¢åŠ é«˜è¾¾31%ã€‚</li>
<li>ä½¿ç”¨RLHFè®­ç»ƒçš„æ¨¡å‹ä»å­˜åœ¨æ¬ºéª—é—®é¢˜ï¼Œå¹³å‡æ¬ºéª—ç‡ä¸º43%ã€‚åŒæ—¶å‘ç°å¤šè½®å¼ºåŒ–å­¦ä¹ å¯ä»¥æœ‰æ•ˆå‡å°‘LLMçš„æ¬ºéª—è¡Œä¸ºï¼Œè¾¾åˆ°è¾ƒé«˜çš„å‡å°‘æ¯”ä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3ec41fc3013fd8040b48e9a34d479001~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728661&auth_key=1760728661-0-0-963b57c5f05b62335daaa858c2e50b90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2d9e93aadf21cbaf3b121d8df650649~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728669&auth_key=1760728669-0-0-a50e110947c25cfdacaf5af8249ba096&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-69f68596c33e88b39e06d6bfdcc81536~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728675&auth_key=1760728675-0-0-be1242f22e13baeed9caae2e9f690aac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Agentic-Data-for-Web-Agents-with-Progressive-Difficulty-Enhancement-Mechanisms"><a href="#Synthesizing-Agentic-Data-for-Web-Agents-with-Progressive-Difficulty-Enhancement-Mechanisms" class="headerlink" title="Synthesizing Agentic Data for Web Agents with Progressive Difficulty   Enhancement Mechanisms"></a>Synthesizing Agentic Data for Web Agents with Progressive Difficulty   Enhancement Mechanisms</h2><p><strong>Authors:Shrey Pandit, Xuan-Phi Nguyen, Yifei Ming, Austin Xu, Jiayu Wang, Caiming Xiong, Shafiq Joty</strong></p>
<p>Web-based â€˜deep researchâ€™ agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors. </p>
<blockquote>
<p>åŸºäºç½‘ç»œçš„â€æ·±åº¦ç ”ç©¶â€ä»£ç†æ—¨åœ¨é€šè¿‡ä¸åœ¨çº¿å·¥å…·è¿›è¡Œé•¿æœŸäº’åŠ¨æ¥è§£å†³å¤æ‚çš„é—®ç­”ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåŸºç¡€è¯­è¨€æ¨¡å‹é€šå¸¸æ²¡æœ‰ç»è¿‡é•¿æœŸæ¨ç†å’Œæ¢ç´¢çš„ä¼˜åŒ–ã€‚æ—©æœŸçš„å·¥ä½œå·²ç»æå‡ºäº†æ„å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„å·¥ä½œæµç¨‹ï¼Œé€šå¸¸åˆ©ç”¨çŸ¥è¯†å›¾è°±ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹éš¾åº¦å’Œè´¨é‡çš„ç²¾ç»†æ§åˆ¶ï¼Œäº§ç”Ÿçš„åˆæˆæ•°æ®æ— æ³•æ•æ‰é•¿æœŸæ¨ç†æ‰€éœ€çš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œè®¸å¤šç ”ç©¶é€šè¿‡æ¯”è¾ƒåœ¨ä¸åŒä¼˜åŒ–é…æ–¹ä¸‹è®­ç»ƒçš„æ¨¡å‹æ¥æ··æ·†æ•°æ®å’Œè®­ç»ƒæ•ˆæœï¼Œå¾ˆéš¾å­¤ç«‹åœ°è¯„ä¼°æ•°æ®æœ¬èº«çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒç®¡é½ä¸‹çš„æ•°æ®åˆæˆç®¡é“ï¼Œé€šè¿‡é€æ­¥å¢åŠ ä»»åŠ¡å¤æ‚æ€§æ¥ç”Ÿæˆé—®ç­”å¯¹ï¼Œç›´åˆ°å‰æ²¿åŸºçº¿ç½‘ç»œä»£ç†å¤±è´¥ã€‚åŸºçº¿ä»£ç†åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æ‰®æ¼”äº†å¤šä¸ªè§’è‰²ï¼šå°è¯•é—®é¢˜ã€éªŒè¯äº‹å®ã€æ£€æŸ¥æ›¿ä»£ç­”æ¡ˆå’Œæ‰§è¡Œè¿‡æ»¤ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„åˆæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºä»å¼ºå¤§ç½‘ç»œä»£ç†ä¸­æç‚¼å‡ºæ¥çš„å—æ§è®­ç»ƒè®¾ç½®ã€‚åœ¨å¤šä¸ªç½‘ç»œåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ•°æ®é›†è¾ƒå°ï¼Œä½†å®ƒèƒ½å¤Ÿè®­ç»ƒå‡ºæ¯”ç°æœ‰æ•°æ®é›†æ›´æœ‰æ•ˆçš„ç½‘ç»œä»£ç†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®åœ¨å·¥å…·ä½¿ç”¨è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºä¸¤å€çš„å¤šæ ·æ€§ï¼Œä½¿å¾—åœ¨æ­¤æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿå–å¾—æ›´å¼ºçš„æ€§èƒ½ï¼ŒåŒæ—¶é¿å…é‡å¤çš„å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13913v1">PDF</a> Preprint. ICLR 26 submission</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§ä¸¤é˜¶æ®µæ•°æ®åˆæˆæµç¨‹ï¼Œæ—¨åœ¨ç”Ÿæˆé’ˆå¯¹åŸºäºWebçš„æ·±åº¦é—®ç­”ä»»åŠ¡çš„æ•°æ®é›†ã€‚è¯¥ç ”ç©¶é€šè¿‡é€æ­¥æé«˜ä»»åŠ¡å¤æ‚åº¦æ¥ç”Ÿæˆé—®ç­”å¯¹ï¼Œç›´è‡³åŸºçº¿Webä»£ç†æ— æ³•å®Œæˆä»»åŠ¡ã€‚åŸºçº¿ä»£ç†åœ¨æ­¤è¿‡ç¨‹ä¸­æ‰®æ¼”å¤šé‡è§’è‰²ï¼ŒåŒ…æ‹¬å°è¯•é—®é¢˜ã€éªŒè¯äº‹å®æ€§ã€æ£€æŸ¥æ›¿ä»£ç­”æ¡ˆå’Œæ‰§è¡Œè¿‡æ»¤ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æ•°æ®é‡è¾ƒå°ï¼Œä½†æœ¬ç ”ç©¶çš„æ•°æ®é›†èƒ½å¤Ÿæœ‰æ•ˆè®­ç»ƒWebä»£ç†ï¼Œä½¿å…¶è¡¨ç°ä¼˜äºç°æœ‰æ•°æ®é›†ã€‚å…¶æ•°æ®é›†åœ¨å·¥å…·ä½¿ç”¨åŠ¨ä½œæ–¹é¢è¡¨ç°å‡ºä¸¤å€çš„å¤šæ ·æ€§ï¼Œå¯è®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºæ€§èƒ½å¹¶é¿å…é‡å¤æ€§å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆæµç¨‹ï¼Œé’ˆå¯¹åŸºäºWebçš„æ·±åº¦é—®ç­”ä»»åŠ¡ç”Ÿæˆæ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†ç”Ÿæˆé‡‡ç”¨é€æ­¥æé«˜ä»»åŠ¡å¤æ‚åº¦çš„ç­–ç•¥ï¼Œç›´è‡³åŸºçº¿Webä»£ç†æ— æ³•å®Œæˆã€‚</li>
<li>åŸºçº¿ä»£ç†åœ¨æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­æ‰®æ¼”å¤šé‡è§’è‰²ï¼ŒåŒ…æ‹¬å°è¯•é—®é¢˜ã€éªŒè¯äº‹å®æ€§å’Œæ£€æŸ¥æ›¿ä»£ç­”æ¡ˆç­‰ã€‚</li>
<li>é‡‡ç”¨åŸºäºå¼ºWebä»£ç†è’¸é¦çš„æ§åˆ¶è®­ç»ƒè®¾ç½®æ¥è¯„ä¼°æ•°æ®åˆæˆçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºäºWebçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç ”ç©¶çš„æ•°æ®é›†åœ¨è®­ç»ƒæœ‰æ•ˆWebä»£ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶çš„æ•°æ®é›†åœ¨å·¥å…·ä½¿ç”¨åŠ¨ä½œçš„å¤šæ ·æ€§ä¸Šè¡¨ç°å‡ºä¸¤å€çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5068e536e9c0603cd35b51a3be1d3b41~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728682&auth_key=1760728682-0-0-a9074715a8979febbf2e96c156db63c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fcab1798ac445b2695e171d66068ddb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728690&auth_key=1760728690-0-0-97a0025c52c907e080096cde6d6787c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57bcf3e7be95900bece3f2587ca53c15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728696&auth_key=1760728696-0-0-a8fdf19f54d219ddcbbdeb778fe27cae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14c04f5fd7f5cf2847c8f299046e0e74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728703&auth_key=1760728703-0-0-39a9a6157cf7d2fb173321972e2974ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Attribution-Quality-in-AI-Generated-Content-Benchmarking-Style-Embeddings-and-LLM-Judges"><a href="#Attribution-Quality-in-AI-Generated-Content-Benchmarking-Style-Embeddings-and-LLM-Judges" class="headerlink" title="Attribution Quality in AI-Generated Content:Benchmarking Style   Embeddings and LLM Judges"></a>Attribution Quality in AI-Generated Content:Benchmarking Style   Embeddings and LLM Judges</h2><p><strong>Authors:Misam Abbas</strong></p>
<p>Attributing authorship in the era of large language models (LLMs) is increasingly challenging as machine-generated prose rivals human writing. We benchmark two complementary attribution mechanisms , fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an open dataset of 600 balanced instances spanning six domains (academic, news, fiction, blogs, spoken transcripts, and TV&#x2F;movie scripts). Each instance contains a human prompt with both a gold continuation and an LLM-generated continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs. 68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA continuations (85 pct vs. 81 pct) but the results are not statistically significant. Crucially, the LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity, whereas embeddings dominate in spoken and scripted dialogue, reflecting structural strengths. These complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies. To support reproducibility we provide code on GitHub and derived data on Hugging Face under the MIT license. This open framework provides a reproducible benchmark for attribution quality assessment in AI-generated content, along with a review of related literature influencing this work. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ä»£ï¼Œå¯¹ä½œè€…å½’å±çš„è®¤å®šè¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæœºå™¨ç”Ÿæˆçš„æ•£æ–‡ä¸äººç±»çš„å†™ä½œèƒ½åŠ›ä¸ç›¸ä¸Šä¸‹ã€‚æˆ‘ä»¬åœ¨Human AI Parallel Corpusä¸ŠåŸºå‡†æµ‹è¯•äº†ä¸¤ç§äº’è¡¥çš„å½’å±æœºåˆ¶ï¼Œå³å›ºå®šçš„é£æ ¼åµŒå…¥å’Œä¸€ä¸ªç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„LLMåˆ¤æ–­å™¨ï¼ˆGPT-4oï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«600ä¸ªå¹³è¡¡å®ä¾‹çš„å¼€æ”¾æ•°æ®é›†ï¼Œæ¶µç›–å…­ä¸ªé¢†åŸŸï¼ˆå­¦æœ¯ã€æ–°é—»ã€å°è¯´ã€åšå®¢ã€å£è¯­è½¬å½•å’Œç”µè§†&#x2F;ç”µå½±å‰§æœ¬ï¼‰ã€‚æ¯ä¸ªå®ä¾‹éƒ½åŒ…å«ä¸€ä¸ªäººç±»æç¤ºï¼Œå…¶ä¸­åŒ…æ‹¬é»„é‡‘å»¶ç»­å’Œæ¥è‡ªGPT-4oæˆ–LLaMA-70B-Instructçš„LLMç”Ÿæˆå»¶ç»­ã€‚é£æ ¼åµŒå…¥åŸºçº¿åœ¨GPTå»¶ç»­ä¸Šè¾¾åˆ°äº†æ›´é«˜çš„æ€»ä½“å‡†ç¡®ç‡ï¼ˆ82%å¯¹68%ï¼‰ã€‚LLMåˆ¤æ–­å™¨åœ¨LLaMAå»¶ç»­ä¸Šç•¥èƒœäºé£æ ¼åµŒå…¥ï¼ˆ85%å¯¹81%ï¼‰ï¼Œä½†ç»“æœä¸å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚å…³é”®çš„æ˜¯ï¼ŒLLMåˆ¤æ–­å™¨åœ¨å°è¯´å’Œå­¦æœ¯æ•£æ–‡æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºè¯­ä¹‰æ•æ„Ÿæ€§ï¼Œè€ŒåµŒå…¥æ³•åˆ™ä¸»å¯¼å£è¯­å’Œå‰§æœ¬å¯¹è¯ï¼Œåæ˜ äº†å…¶ç»“æ„æ€§ä¼˜åŠ¿ã€‚è¿™äº›äº’è¡¥æ¨¡å¼å¼ºè°ƒäº†å½’å±æ˜¯ä¸€ä¸ªå¤šç»´é—®é¢˜ï¼Œéœ€è¦æ··åˆç­–ç•¥æ¥è§£å†³ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤æ€§ï¼Œæˆ‘ä»¬åœ¨GitHubä¸Šæä¾›äº†ä»£ç ï¼Œå¹¶åœ¨Hugging Faceä¸Šæä¾›äº†è¡ç”Ÿæ•°æ®ï¼Œéµå¾ªMITè®¸å¯è¯ã€‚è¿™ä¸€å¼€æ”¾æ¡†æ¶ä¸ºAIç”Ÿæˆå†…å®¹çš„å½’å±è´¨é‡è¯„ä¼°æä¾›äº†å¯é‡å¤æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹å½±å“è¿™é¡¹å·¥ä½œçš„ç›¸å…³æ–‡çŒ®è¿›è¡Œäº†è¯„è¿°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13898v1">PDF</a> Accepted for publication at the 2025 IEEE ICDM Workshop on â€œGrounding   Documents with Reasoning, Agents, Retrieval, and Attributionâ€. This is author   submitted version. Not yet published</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£å¦‚ä½•å½’å±ä½œè€…èº«ä»½çš„éš¾é¢˜ã€‚æ–‡ç« åœ¨Human AI Parallel Corpusæ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸¤ç§äº’è¡¥çš„å½’å±æœºåˆ¶â€”â€”å›ºå®šé£æ ¼åµŒå…¥å’ŒæŒ‡ä»¤è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åˆ¤æ–­å™¨GPT-4oçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œé£æ ¼åµŒå…¥åœ¨GPTç»­ä½œä¸­çš„æ€»ä½“å‡†ç¡®ç‡æ›´é«˜ï¼Œè€ŒLLMåˆ¤æ–­å™¨åœ¨LLaMAç»­ä½œä¸Šçš„è¡¨ç°ç•¥å¥½ï¼Œä½†ç»“æœæ— æ˜¾è‘—å·®å¼‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLLMåˆ¤æ–­å™¨åœ¨å°è¯´å’Œå­¦æœ¯æ•£æ–‡ä¸­çš„è¡¨ç°æ›´ä½³ï¼Œæ˜¾ç¤ºå‡ºäº†è¯­ä¹‰æ•æ„Ÿæ€§ï¼Œè€Œé£æ ¼åµŒå…¥åˆ™åœ¨å£è¯­å’Œå‰§æœ¬å¯¹è¯ä¸­æ›´å ä¼˜åŠ¿ï¼Œåæ˜ äº†å…¶ç»“æ„æ€§ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œå½’å±é—®é¢˜æ˜¯ä¸€ä¸ªå¤šç»´åº¦çš„é—®é¢˜ï¼Œéœ€è¦æ··åˆç­–ç•¥æ¥è§£å†³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£çš„ä½œè€…èº«ä»½å½’å±æˆä¸ºä¸€å¤§æŒ‘æˆ˜ï¼Œæœºå™¨ç”Ÿæˆæ–‡æœ¬ä¸äººç±»å†™ä½œéš¾ä»¥åŒºåˆ†ã€‚</li>
<li>é£æ ¼åµŒå…¥å’ŒLLMåˆ¤æ–­å™¨æ˜¯ä¸¤ç§äº’è¡¥çš„å½’å±æœºåˆ¶ï¼Œåœ¨Human AI Parallel Corpusæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>é£æ ¼åµŒå…¥åœ¨GPTç»­ä½œä¸­çš„æ€»ä½“å‡†ç¡®ç‡æ›´é«˜ï¼Œè¾¾åˆ°82%ï¼Œè€ŒLLMåˆ¤æ–­å™¨åœ¨LLaMAç»­ä½œä¸Šçš„å‡†ç¡®ç‡ç•¥é«˜ï¼Œä½†ç»“æœä¸å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚</li>
<li>LLMåˆ¤æ–­å™¨åœ¨å°è¯´å’Œå­¦æœ¯æ•£æ–‡ä¸­è¡¨ç°è¾ƒå¥½ï¼Œæ˜¾ç¤ºå‡ºè¯­ä¹‰æ•æ„Ÿæ€§ï¼›è€Œé£æ ¼åµŒå…¥åœ¨å£è¯­å’Œå‰§æœ¬å¯¹è¯ä¸­æ›´å…·ä¼˜åŠ¿ï¼Œåæ˜ å…¶ç»“æ„æ€§ç‰¹ç‚¹ã€‚</li>
<li>å½’å±é—®é¢˜æ˜¯ä¸€ä¸ªå¤šç»´é—®é¢˜ï¼Œéœ€è¦æ··åˆç­–ç•¥æ¥è§£å†³ã€‚</li>
<li>æ–‡ç« æä¾›äº†GitHubä»£ç å’ŒHugging Faceä¸‹çš„è¡ç”Ÿæ•°æ®ï¼Œä»¥æ”¯æŒç ”ç©¶çš„å¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-680bd3f170a5cce1e220887db0b5decf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728710&auth_key=1760728710-0-0-48b29400b9af6e984d88fd5618dbe088&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a59f5f7e1e606217568598e5ade9b65b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728717&auth_key=1760728717-0-0-39f1a11e4bca9207bb22429a04c66a50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93b0e9f4476815fa62d0e9f28bb99230~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728723&auth_key=1760728723-0-0-dc74f2ac5cf4d2c14f50c310c4705718&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-584e299d28ee48c18a21ec114e20b105~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728730&auth_key=1760728730-0-0-7f37b60042f6dcdf4824e0b263cab419&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89a0a6b6321b6279b1aea4d27c680978~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728737&auth_key=1760728737-0-0-1bdf1575d5a7846188d50b973d212db2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e37402b8f7c3fa74c381e5f6a26dbccd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728743&auth_key=1760728743-0-0-b1c59b393a286511933e7cb773c7a93f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Confidence-as-a-Reward-Transforming-LLMs-into-Reward-Models"><a href="#Confidence-as-a-Reward-Transforming-LLMs-into-Reward-Models" class="headerlink" title="Confidence as a Reward: Transforming LLMs into Reward Models"></a>Confidence as a Reward: Transforming LLMs into Reward Models</h2><p><strong>Authors:He Du, Bowen Li, Chengxing Xie, Chang Gao, Kai Chen, Dacheng Tao</strong></p>
<p>Reward models can significantly enhance the reasoning capabilities of large language models (LLMs), but they typically require extensive curated data and costly training. To mitigate these challenges, training-free approaches such as LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate responses, achieving promising results. Recent works have also indicated that model confidence can serve effectively as a reward metric, distinguishing between chain-of-thought (CoT) and non-CoT paths. However, the concept of using confidence as a reward has not been comprehensively studied. In this work, we systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful training-free method that utilizes token-level confidence in the modelâ€™s final answers as a proxy for reward, especially suitable for close-ended tasks. Through extensive experiments on mathematical reasoning tasks, we demonstrate that CRew outperforms existing training-free reward approaches on the MATH500 and RewardMATH benchmarks, and even surpasses most trained reward models. We further identify a strong correlation between CRew scores and the actual reasoning performance of the model. Additionally, we find that CRew can effectively filter high-quality training data. Building upon these insights, we propose CRew-DPO, a training strategy that constructs preference data from confidence scores combined with correctness signals. Finetuning with CRew-DPO further enhances the modelâ€™s judging capabilities and consistently outperforms existing self-training methods. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®å’Œæ˜‚è´µçš„è®­ç»ƒæˆæœ¬ã€‚ä¸ºäº†ç¼“è§£è¿™äº›æŒ‘æˆ˜ï¼Œæ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¦‚LLM-as-a-Judgeï¼Œåˆ©ç”¨LLMçš„å†…åœ¨æ¨ç†èƒ½åŠ›æ¥è¯„ä¼°å“åº”ï¼Œå¹¶å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚æœ€è¿‘çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ¨¡å‹ä¿¡å¿ƒå¯ä»¥ä½œä¸ºå¥–åŠ±æŒ‡æ ‡ï¼Œæœ‰æ•ˆåŒºåˆ†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’ŒéCoTè·¯å¾„ã€‚ç„¶è€Œï¼Œä½¿ç”¨ä¿¡å¿ƒä½œä¸ºå¥–åŠ±çš„æ¦‚å¿µå°šæœªå¾—åˆ°å…¨é¢ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¿¡å¿ƒä½œä¸ºå¥–åŠ±ï¼ˆCRewï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ¨¡å‹æœ€ç»ˆç­”æ¡ˆçš„ä»¤ç‰Œçº§ä¿¡å¿ƒä½œä¸ºå¥–åŠ±çš„ä»£ç†ï¼Œå°¤å…¶é€‚ç”¨äºå°é—­ä»»åŠ¡ã€‚é€šè¿‡åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†CRewåœ¨MATH500å’ŒRewardMATHåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰çš„æ— éœ€è®­ç»ƒçš„å¥–åŠ±æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†å¤§å¤šæ•°ç»è¿‡è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°CRewåˆ†æ•°ä¸æ¨¡å‹çš„å®é™…æ¨ç†æ€§èƒ½ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°CRewå¯ä»¥æœ‰æ•ˆåœ°è¿‡æ»¤é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†CRew-DPOï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆä¿¡å¿ƒåˆ†æ•°å’Œæ­£ç¡®æ€§ä¿¡å·æ„å»ºåå¥½æ•°æ®çš„è®­ç»ƒç­–ç•¥ã€‚ä½¿ç”¨CRew-DPOè¿›è¡Œå¾®è°ƒè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„åˆ¤æ–­èƒ½åŠ›ï¼Œå¹¶å§‹ç»ˆä¼˜äºç°æœ‰çš„è‡ªè®­ç»ƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¥–åŠ±æ¨¡å‹èƒ½æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡å®šåˆ¶æ•°æ®å’Œæ˜‚è´µçš„è®­ç»ƒæˆæœ¬ã€‚ä¸ºç¼“è§£è¿™äº›é—®é¢˜ï¼Œå‡ºç°äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¦‚åˆ©ç”¨LLMä½œä¸ºè¯„åˆ¤è€…æ¥è¯„ä¼°å›åº”ï¼Œå–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†ä½¿ç”¨æ¨¡å‹æœ€ç»ˆç­”æ¡ˆä¸­çš„ä»¤ç‰Œçº§åˆ«ä¿¡å¿ƒä½œä¸ºå¥–åŠ±çš„ä»£ç†çš„â€œä¿¡å¿ƒå³å¥–åŠ±â€ï¼ˆCRewï¼‰æ–¹æ³•ï¼Œç‰¹åˆ«é€‚åˆå°é—­ä»»åŠ¡ã€‚é€šè¿‡åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†CRewåœ¨MATH500å’ŒRewardMATHåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰çš„æ— éœ€è®­ç»ƒçš„å¥–åŠ±æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†å¤§å¤šæ•°ç»è¿‡è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒCRewä¸æ¨¡å‹çš„å®é™…æ¨ç†æ€§èƒ½ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°è¿‡æ»¤å‡ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†CRew-DPOè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆä¿¡å¿ƒè¯„åˆ†å’Œæ­£ç¡®æ€§ä¿¡å·æ„å»ºåå¥½æ•°æ®ã€‚é€šè¿‡CRew-DPOè¿›è¡Œå¾®è°ƒï¼Œå¯è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„åˆ¤æ–­èƒ½åŠ›ï¼Œå¹¶å§‹ç»ˆä¼˜äºç°æœ‰çš„è‡ªè®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹èƒ½å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†éœ€å¤§é‡å®šåˆ¶æ•°æ®å’Œæ˜‚è´µè®­ç»ƒã€‚</li>
<li>LLM-as-a-Judgeç­‰æ— éœ€è®­ç»ƒçš„æ–¹æ³•åˆ©ç”¨LLMçš„å†…åœ¨æ¨ç†èƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œå–å¾—è‰¯å¥½æ•ˆæœã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºâ€œä¿¡å¿ƒå³å¥–åŠ±â€ï¼ˆCRewï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ¨¡å‹æœ€ç»ˆç­”æ¡ˆçš„ä»¤ç‰Œçº§åˆ«ä¿¡å¿ƒä½œä¸ºå¥–åŠ±ä»£ç†ï¼Œé€‚ç”¨äºå°é—­ä»»åŠ¡ã€‚</li>
<li>CRewåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–æ— éœ€è®­ç»ƒçš„å¥–åŠ±æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šéƒ¨åˆ†è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>CRewä¸æ¨¡å‹å®é™…æ¨ç†æ€§èƒ½å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ï¼Œå¹¶èƒ½æœ‰æ•ˆè¿‡æ»¤é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>æå‡ºCRew-DPOè®­ç»ƒç­–ç•¥ï¼Œç»“åˆä¿¡å¿ƒè¯„åˆ†å’Œæ­£ç¡®æ€§ä¿¡å·æ„å»ºåå¥½æ•°æ®ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹åˆ¤æ–­èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2a97af3317a01d2f7d2f8fd2c8209c28~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728750&auth_key=1760728750-0-0-46c1e5e3f7feea3f3fb6fa07b826608d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df826254f85fc571e5ccc5fdc61d74c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728757&auth_key=1760728757-0-0-23818898bd3e4656bbdecccdffb9bb5d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0726b2960118f732900d5d8ba2f0401~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728764&auth_key=1760728764-0-0-3e8ea13b20a4e810b34de9233034ae80&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Who-Speaks-for-the-Trigger-Dynamic-Expert-Routing-in-Backdoored-Mixture-of-Experts-Transformers"><a href="#Who-Speaks-for-the-Trigger-Dynamic-Expert-Routing-in-Backdoored-Mixture-of-Experts-Transformers" class="headerlink" title="Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored   Mixture-of-Experts Transformers"></a>Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored   Mixture-of-Experts Transformers</h2><p><strong>Authors:Xin Zhao, Xiaojun Chen, Bingshan Liu, Haoyu Gao, Zhendong Zhao, Yilong Chen</strong></p>
<p>Large language models (LLMs) with Mixture-of-Experts (MoE) architectures achieve impressive performance and efficiency by dynamically routing inputs to specialized subnetworks, known as experts. However, this sparse routing mechanism inherently exhibits task preferences due to expert specialization, introducing a new and underexplored vulnerability to backdoor attacks. In this work, we investigate the feasibility and effectiveness of injecting backdoors into MoE-based LLMs by exploiting their inherent expert routing preferences. We thus propose BadSwitch, a novel backdoor framework that integrates task-coupled dynamic trigger optimization with a sensitivity-guided Top-S expert tracing mechanism. Our approach jointly optimizes trigger embeddings during pretraining while identifying S most sensitive experts, subsequently constraining the Top-K gating mechanism to these targeted experts. Unlike traditional backdoor attacks that rely on superficial data poisoning or model editing, BadSwitch primarily embeds malicious triggers into expert routing paths with strong task affinity, enabling precise and stealthy model manipulation. Through comprehensive evaluations across three prominent MoE architectures (Switch Transformer, QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack pre-trained models with up to 100% success rate (ASR) while maintaining the highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch exhibits strong resilience against both text-level and model-level defense mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our analysis of expert activation patterns reveals fundamental insights into MoE vulnerabilities. We anticipate this work will expose security risks in MoE systems and contribute to advancing AI safety. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„æ¨¡å‹é€šè¿‡åŠ¨æ€å°†è¾“å…¥è·¯ç”±åˆ°ä¸“ä¸šå­ç½‘ï¼ˆç§°ä¸ºä¸“å®¶ï¼‰æ¥å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œè¿™ç§ç¨€ç–è·¯ç”±æœºåˆ¶ç”±äºä¸“å®¶çš„ä¸“ä¸šåŒ–è€Œå¤©ç”Ÿåœ°è¡¨ç°å‡ºä»»åŠ¡åå¥½ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„ä¸”å°šæœªè¢«å……åˆ†æ¢ç´¢çš„åé—¨æ”»å‡»çš„è„†å¼±æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é€šè¿‡åœ¨åŸºäºMoEçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­åˆ©ç”¨å…¶å›ºæœ‰çš„ä¸“å®¶è·¯ç”±åå¥½æ¥æ³¨å…¥åé—¨çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†BadSwitchï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åé—¨æ¡†æ¶ï¼Œå®ƒå°†ä»»åŠ¡è€¦åˆçš„åŠ¨æ€è§¦å‘ä¼˜åŒ–ä¸æ•æ„Ÿæ€§å¼•å¯¼çš„Top-Sä¸“å®¶è·Ÿè¸ªæœºåˆ¶ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–è§¦å‘åµŒå…¥çš„åŒæ—¶ï¼Œç¡®å®šæœ€æ•æ„Ÿçš„ä¸“å®¶ï¼Œç„¶åå°†Top-Kç½‘å…³æœºåˆ¶é™åˆ¶åœ¨è¿™äº›ç›®æ ‡ä¸“å®¶ä¸Šã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–è¡¨é¢æ•°æ®æ±¡æŸ“æˆ–æ¨¡å‹ç¼–è¾‘çš„åé—¨æ”»å‡»ä¸åŒï¼ŒBadSwitchä¸»è¦å°†æ¶æ„è§¦å‘åµŒå…¥åˆ°å…·æœ‰å¼ºä»»åŠ¡äº²å’ŒåŠ›çš„ä¸“å®¶è·¯ç”±è·¯å¾„ä¸­ï¼Œä»è€Œå®ç°ç²¾ç¡®å’Œéšè”½çš„æ¨¡å‹æ“çºµã€‚é€šè¿‡å…¨é¢è¯„ä¼°ä¸‰ç§ä¸»æµçš„MoEæ¶æ„ï¼ˆSwitch Transformerã€QwenMoEå’ŒDeepSeekMoEï¼‰ï¼Œæˆ‘ä»¬è¯æ˜äº†BadSwitchå¯ä»¥æœ‰æ•ˆåœ°æ¥ç®¡é¢„è®­ç»ƒæ¨¡å‹ï¼ŒæˆåŠŸç‡é«˜è¾¾100%ï¼ˆASRï¼‰ï¼ŒåŒæ—¶ä¿æŒæ‰€æœ‰åŸºçº¿ä¸­çš„æœ€é«˜æ¸…æ´ç²¾åº¦ï¼ˆACCï¼‰ã€‚æ­¤å¤–ï¼ŒBadSwitchå¯¹æ–‡æœ¬çº§åˆ«å’Œæ¨¡å‹çº§åˆ«çš„é˜²å¾¡æœºåˆ¶è¡¨ç°å‡ºå¼ºå¤§çš„éŸ§æ€§ï¼Œåœ¨AGNewsæ•°æ®é›†ä¸Šå®ç°94.07%çš„ASRå’Œ87.18%çš„ACCã€‚æˆ‘ä»¬å¯¹ä¸“å®¶æ¿€æ´»æ¨¡å¼çš„åˆ†ææ­ç¤ºäº†MoEæ¼æ´çš„æ ¹æœ¬è§è§£ã€‚æˆ‘ä»¬é¢„è®¡è¿™é¡¹å·¥ä½œå°†æš´éœ²MoEç³»ç»Ÿçš„å®‰å…¨é£é™©ï¼Œå¹¶ä¸ºæé«˜äººå·¥æ™ºèƒ½å®‰å…¨æ€§åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åŠ¨æ€åœ°å°†è¾“å…¥è·¯ç”±åˆ°ä¸“é—¨åŒ–çš„å­ç½‘ç»œï¼ˆå³ä¸“å®¶ï¼‰å®ç°æ€§èƒ½ä¸æ•ˆç‡çš„æå‡ï¼Œè¿™ç§ç»“æ„å› å…¶å†…åœ¨çš„ç‰¹æ€§æš´éœ²äº†æ–°çš„å®‰å…¨æ¼æ´ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ä¸“å®¶è·¯ç”±åå¥½å‘åŸºäºMoEçš„LLMæ³¨å…¥åé—¨çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºBadSwitchçš„æ–°å‹åé—¨æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä»»åŠ¡è€¦åˆçš„åŠ¨æ€è§¦å‘ä¼˜åŒ–å’Œæ•æ„Ÿæ€§å¯¼å‘çš„Top-Sä¸“å®¶è¿½è¸ªæœºåˆ¶æ¥æ“ä½œæ¨¡å‹ã€‚BadSwitchä¼˜åŒ–äº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„è§¦å‘åµŒå…¥ï¼ŒåŒæ—¶è¯†åˆ«æœ€æ•æ„Ÿä¸“å®¶å¹¶çº¦æŸTop-Kç½‘å…³æœºåˆ¶ä»¥é’ˆå¯¹ç›®æ ‡ä¸“å®¶è¿›è¡Œè·¯ç”±é€‰æ‹©ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–è¡¨é¢æ•°æ®æ±¡æŸ“æˆ–æ¨¡å‹ç¼–è¾‘çš„åé—¨æ”»å‡»ä¸åŒï¼ŒBadSwitchä¸»è¦é€šè¿‡å°†æ¶æ„è§¦å‘åµŒå…¥åˆ°å…·æœ‰å¼ºä»»åŠ¡äº²å’ŒåŠ›çš„ä¸“å®¶è·¯ç”±è·¯å¾„ä¸­å®ç°ç²¾ç¡®ä¸”éšè”½çš„æ¨¡å‹æ“æ§ã€‚é€šè¿‡ä¸‰ä¸ªä¸»æµMoEæ¶æ„çš„ç»¼åˆè¯„ä¼°ï¼Œè¯æ˜äº†BadSwitchçš„é«˜æ•ˆæ€§ï¼Œå…¶æˆåŠŸç‡é«˜è¾¾ç™¾åˆ†ä¹‹ç™¾ï¼ŒåŒæ—¶ä¿æŒé«˜æ¸…æ´ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒBadSwitchå¯¹æ–‡æœ¬çº§åˆ«å’Œæ¨¡å‹çº§åˆ«çš„é˜²å¾¡æœºåˆ¶è¡¨ç°å‡ºå¼ºå¤§çš„éŸ§æ€§ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†MoEæ¶æ„çš„æ½œåœ¨é£é™©ï¼Œå¹¶ä¸ºæ¨åŠ¨AIå®‰å…¨åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åŠ¨æ€è·¯ç”±æœºåˆ¶å®ç°æ€§èƒ½æå‡ï¼Œä½†è¿™ç§æœºåˆ¶å…·æœ‰ä»»åŠ¡åå¥½æ€§ï¼Œå®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ã€‚</li>
<li>æå‡ºBadSwitchæ¡†æ¶ï¼Œä¸€ç§é’ˆå¯¹åŸºäºMoEæ¶æ„çš„LLMçš„åé—¨æ”»å‡»æ–¹æ³•ã€‚</li>
<li>BadSwitchä¼˜åŒ–äº†è§¦å‘åµŒå…¥çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶è¯†åˆ«å¹¶çº¦æŸæœ€æ•æ„Ÿä¸“å®¶è¿›è¡Œè·¯ç”±é€‰æ‹©ã€‚</li>
<li>BadSwitchä¸åŒäºä¼ ç»Ÿæ”»å‡»æ–¹å¼ï¼Œé€šè¿‡åµŒå…¥æ¶æ„è§¦å‘åˆ°ä¸“å®¶è·¯ç”±è·¯å¾„æ“çºµæ¨¡å‹ï¼Œæ›´åŠ ç²¾ç¡®ä¸”éšè”½ã€‚</li>
<li>BadSwitchåœ¨å¤šç§MoEæ¶æ„ä¸‹è¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼ŒæˆåŠŸç‡é«˜è¾¾ç™¾åˆ†ä¹‹ç™¾ï¼ŒåŒæ—¶ä¿æŒé«˜æ¸…æ´ç²¾åº¦ã€‚</li>
<li>BadSwitchå¯¹å¤šç§é˜²å¾¡æœºåˆ¶å±•ç°å‡ºå¼ºå¤§çš„éŸ§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e17863158d3f624a1b9da9de65abe9ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728771&auth_key=1760728771-0-0-764e47c4deba8da17845de89954023b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86a4f7b9e1ac871165b77e74bdc89365~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728781&auth_key=1760728781-0-0-83ec63f441782e65d8ff86fe0247270e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5761275bb507be4c93ff7393eb4014b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728788&auth_key=1760728788-0-0-9de16b46d15b5387da7720c040532f5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f68fd4385158fa879d2700a3c784c8c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728794&auth_key=1760728794-0-0-bbbc1eaa829b22bc58bf5f1209236e12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Visual-Interestingness-Decoded-How-GPT-4o-Mirrors-Human-Interests"><a href="#Visual-Interestingness-Decoded-How-GPT-4o-Mirrors-Human-Interests" class="headerlink" title="Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests"></a>Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests</h2><p><strong>Authors:Fitim Abdullahu, Helmut Grabner</strong></p>
<p>Our daily life is highly influenced by what we consume and see. Attracting and holding oneâ€™s attention â€“ the definition of (visual) interestingness â€“ is essential. The rise of Large Multimodal Models (LMMs) trained on large-scale visual and textual data has demonstrated impressive capabilities. We explore these modelsâ€™ potential to understand to what extent the concepts of visual interestingness are captured and examine the alignment between human assessments and GPT-4oâ€™s, a leading LMM, predictions through comparative analysis. Our studies reveal partial alignment between humans and GPT-4o. It already captures the concept as best compared to state-of-the-art methods. Hence, this allows for the effective labeling of image pairs according to their (commonly) interestingness, which are used as training data to distill the knowledge into a learning-to-rank model. The insights pave the way for a deeper understanding of human interest. </p>
<blockquote>
<p>æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»å—åˆ°æˆ‘ä»¬æ‰€æ¶ˆè´¹å’Œæ‰€çœ‹åˆ°å†…å®¹çš„é«˜åº¦å½±å“ã€‚å¸å¼•å¹¶ç»´æŒäººä»¬çš„æ³¨æ„åŠ›â€”â€”ï¼ˆè§†è§‰ï¼‰æœ‰è¶£æ€§çš„å®šä¹‰â€”â€”è‡³å…³é‡è¦ã€‚ç»è¿‡å¤§è§„æ¨¡è§†è§‰å’Œæ–‡æœ¬æ•°æ®è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å´›èµ·å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ¢ç´¢äº†è¿™äº›æ¨¡å‹åœ¨ç†è§£è§†è§‰æœ‰è¶£æ€§æ¦‚å¿µæ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶é€šè¿‡å¯¹æ¯”åˆ†æï¼Œæ£€éªŒäº†äººç±»å¯¹GPT-4oï¼ˆä¸€æ¬¾é¢†å…ˆçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼‰é¢„æµ‹çš„è¯„ä»·çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œäººç±»ä¸GPT-4oä¹‹é—´å­˜åœ¨éƒ¨åˆ†ä¸€è‡´æ€§ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå·²ç»æ•æ‰åˆ°äº†æœ€ä½³çš„æ¦‚å¿µã€‚å› æ­¤ï¼Œè¿™å¯ä»¥æ›´æœ‰æ•ˆåœ°æ ¹æ®å®ƒä»¬çš„ï¼ˆæ™®éï¼‰æœ‰è¶£æ€§å¯¹å›¾åƒå¯¹è¿›è¡Œæ ‡æ³¨ï¼Œè¿™äº›æ ‡æ³¨ä½œä¸ºè®­ç»ƒæ•°æ®è¢«æç‚¼æˆä¸€ç§å­¦ä¹ æ’åæ¨¡å‹ã€‚è¿™äº›è§è§£ä¸ºäººç±»å…´è¶£çš„æ›´æ·±å…¥ç†è§£é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13316v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é€šè¿‡å¤§è§„æ¨¡è§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„è®­ç»ƒå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç ”ç©¶æ¢ç´¢äº†è¿™äº›æ¨¡å‹åœ¨ç†è§£è§†è§‰æœ‰è¶£æ€§çš„ç¨‹åº¦æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶é€šè¿‡å¯¹æ¯”åˆ†ææ­ç¤ºäº†äººç±»è¯„ä¼°å’ŒGPT-4oé¢„æµ‹ä¹‹é—´çš„éƒ¨åˆ†ä¸€è‡´æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒGPT-4oå·²ç»æŒæ¡äº†æœ€ä½³çš„æ¯”è¾ƒæ–¹æ³•ï¼Œå¯æœ‰æ•ˆæ ‡æ³¨å›¾åƒå¯¹æ ¹æ®å®ƒä»¬çš„æœ‰è¶£ç¨‹åº¦ï¼Œå¹¶å¯ä½œä¸ºè®­ç»ƒæ•°æ®è’¸é¦çŸ¥è¯†åˆ°æ’åæ¨¡å‹ä¸­ã€‚è¿™ä¸ºæ›´æ·±å…¥åœ°ç†è§£äººç±»å…´è¶£é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é€šè¿‡å¤§è§„æ¨¡è§†è§‰å’Œæ–‡æœ¬æ•°æ®è®­ç»ƒå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶çš„ç›®çš„æ˜¯æ¢ç´¢LMMsåœ¨ç†è§£è§†è§‰æœ‰è¶£æ€§çš„ç¨‹åº¦æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”åˆ†æï¼Œç ”ç©¶å‘ç°äººç±»è¯„ä¼°å’ŒGPT-4oçš„é¢„æµ‹ä¹‹é—´å­˜åœ¨éƒ¨åˆ†ä¸€è‡´æ€§ã€‚</li>
<li>GPT-4oåœ¨ç†è§£è§†è§‰æœ‰è¶£æ€§çš„æ¦‚å¿µä¸Šç›¸æ¯”å…¶ä»–æœ€æ–°æŠ€æœ¯å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>GPT-4oèƒ½å¤Ÿæœ‰æ•ˆåœ°æ ¹æ®å›¾åƒçš„æœ‰è¶£ç¨‹åº¦è¿›è¡Œæ ‡æ³¨ï¼Œè¿™äº›æ ‡æ³¨å¯ä»¥ä½œä¸ºè®­ç»ƒæ•°æ®ç”¨äºå­¦ä¹ æ’åæ¨¡å‹ã€‚</li>
<li>è¿™äº›è§è§£ä¸ºæ›´æ·±å…¥åœ°ç†è§£äººç±»å…´è¶£æä¾›äº†æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ad1f410c5b5f856f6336f3f75bc5197e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728802&auth_key=1760728802-0-0-54664cec1ecad60a2da4f70a5ca51bb2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc266e406fb224cc6b693f62be0288d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728809&auth_key=1760728809-0-0-cba471aa01740ad1bdc47c01b459dd42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed464a71caa5722a94c778de86a3f13b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728815&auth_key=1760728815-0-0-8d69608a7cf4fdb9d29b8ae58cd2a93e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bcb9b634097412791485bbf41da4840a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728822&auth_key=1760728822-0-0-573d64b97cd88fbaedba5dcbbc086c21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2d6551da020b6b147b09df1d3717715~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728828&auth_key=1760728828-0-0-880eca82cec9043ef9c5b3f2d2b9ad75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c10e8cfcd27d577e68969baf7abd69f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728835&auth_key=1760728835-0-0-91f88414c4c88fbc8285123e6ad78a44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval"><a href="#Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval" class="headerlink" title="Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval"></a>Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval</h2><p><strong>Authors:Yingchen zhang, Ruqing zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv</strong></p>
<p>Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R. </p>
<blockquote>
<p>ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è‡ªåŠ¨å›å½’ç”Ÿæˆä¸ç»™å®šæŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£æ ‡è¯†ç¬¦ï¼ˆdocidsï¼‰ã€‚æ—©æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›æ¥æé«˜GRçš„æ€§èƒ½ï¼Œå´å¿½è§†äº†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›åŒæ ·æœ‰åŠ©äºæ­¤ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæ˜ç¡®çš„æ¨ç†èƒ½å¦æœ‰ç›ŠäºGRï¼Ÿä¸ºäº†æ¢ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹åˆæ­¥ç ”ç©¶ï¼Œåœ¨è¯¥ç ”ç©¶ä¸­ï¼ŒLLMä¼šåœ¨æ‰§è¡Œå—é™çš„docidè§£ç ä¹‹å‰ï¼Œè¢«æç¤ºç”Ÿæˆè‡ªç”±å½¢å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚å°½ç®¡è¿™ç§æ–¹æ³•ä¼˜äºæ ‡å‡†çš„GRï¼Œä½†ç”Ÿæˆçš„æ¨ç†å¾€å¾€è¿‡äºå†—é•¿ï¼Œä¸”ä¸docidç©ºé—´ä¸å¤ªå¯¹é½ã€‚è¿™äº›å±€é™æ€§ä¿ƒä½¿æˆ‘ä»¬å¼€å‘ä¸€ç§æ›´é€‚åˆGRçš„æ¨ç†æœºåˆ¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-for-Retrievalï¼ˆR4Rï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºå‹GRæ¨ç†æ¡†æ¶ï¼Œå®ƒå°†è‡ªç”±å½¢å¼çš„CoTæ¨ç†è½¬æ¢ä¸ºç´§å‡‘çš„ç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¿­ä»£åœ°ä¼˜åŒ–æ¨ç†ã€‚R4Ré€šè¿‡åˆ©ç”¨å…·å¤‡GRæŒ‡ä»¤è°ƒä¼˜èƒ½åŠ›ã€å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMæ¥å¢å¼ºç°æœ‰çš„GRæ–¹æ³•ã€‚åœ¨æ¨ç†æ—¶ï¼ŒR4Ré¦–å…ˆä½¿ç”¨LLMç”Ÿæˆåˆå§‹ç»“æ„åŒ–æ¨ç†ï¼›ç„¶åï¼ŒåŒä¸€LLMåœ¨ï¼ˆiï¼‰ä½¿ç”¨é€‰æ‹©çš„GRæ–¹æ³•æ‰§è¡Œå—é™è§£ç ä»¥äº§ç”Ÿå€™é€‰docidsä¸ï¼ˆiiï¼‰åŸºäºæ£€ç´¢ç»“æœæ›´æ–°æ¨ç†ä»¥æé«˜ä¸‹ä¸€è½®æ•ˆæœä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚R4Rä¸éœ€è¦é¢å¤–çš„æ¨¡å‹æˆ–è®­ç»ƒï¼Œè€Œåªéœ€ä¸€ä¸ªLLMå³å¯åŒæ—¶å……å½“æ¨ç†ç”Ÿæˆå™¨å’Œæ£€ç´¢å™¨ã€‚åœ¨è‡ªç„¶é—®é¢˜ã€MS MARCOå’Œç°å®ä¸–ç•Œä¸­çš„å•†å“æœç´¢åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†R4Rçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13095v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰æ­£åœ¨æˆä¸ºä¸€ä¸ªæ–°å…´çš„ç ”ç©¶é¢†åŸŸã€‚è™½ç„¶å·²æœ‰ç ”ç©¶ä¾§é‡äºåˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›æ¥æå‡GRæ€§èƒ½ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›åŒæ ·å…·æœ‰æ½œåŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºReason-for-Retrievalï¼ˆR4Rï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç”Ÿæˆå¼æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›ï¼Œå°†è‡ªç”±å½¢å¼çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è½¬åŒ–ä¸ºç´§å‡‘çš„ç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ä¸æ–­è¿­ä»£ä¼˜åŒ–æ¨ç†ã€‚R4Rä¸ä¾èµ–é¢å¤–çš„æ¨¡å‹æˆ–è®­ç»ƒï¼Œåªéœ€ä¸€ä¸ªå…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMå³å¯åŒæ—¶æ‹…ä»»æ¨ç†ç”Ÿæˆå™¨å’Œæ£€ç´¢å™¨ã€‚å®éªŒè¯æ˜ï¼ŒR4Råœ¨è‡ªç„¶é—®é¢˜ã€MS MARCOä»¥åŠçœŸå®ä¸–ç•Œçš„ç‰©å“æœç´¢åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰å¼€å§‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨LLMçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†æ¨ç†èƒ½åŠ›åŒæ ·é‡è¦ã€‚</li>
<li>æå‡ºReason-for-Retrievalï¼ˆR4Rï¼‰æ¡†æ¶ï¼Œç»“åˆç”Ÿæˆå¼æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›ã€‚</li>
<li>R4Rå°†è‡ªç”±å½¢å¼çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è½¬åŒ–ä¸ºç´§å‡‘çš„ç»“æ„åŒ–æ ¼å¼ã€‚</li>
<li>R4Råœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¿­ä»£ä¼˜åŒ–æ¨ç†ã€‚</li>
<li>R4Rä¸éœ€è¦é¢å¤–çš„æ¨¡å‹æˆ–è®­ç»ƒï¼Œåªä¾èµ–ä¸€ä¸ªå…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9f65b0defe59ec3f4474392d16b1409a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728863&auth_key=1760728863-0-0-29cdfa866bf92741ae832734f2fa4d76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f7d3a2d2e9b59fa6c5809598e151579~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728870&auth_key=1760728870-0-0-d11febe66cfa0eb72565b07287c5c570&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54909d62486394003d7ba95199ba7376~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728899&auth_key=1760728899-0-0-02a2daeb5b7d823b050c3758a328dcd4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-338aaccc421e2acb56f970c6db9620dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728906&auth_key=1760728906-0-0-0a15240a7e0302d8cb07274aaff61a4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33162bd3fe6dd5fa8c9a9eea6f9c80a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728913&auth_key=1760728913-0-0-20f2a1f9cbd7323b27d470203833d0bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evolution-of-metaâ€™s-llama-models-and-parameter-efficient-fine-tuning-of-large-language-models-a-survey"><a href="#Evolution-of-metaâ€™s-llama-models-and-parameter-efficient-fine-tuning-of-large-language-models-a-survey" class="headerlink" title="Evolution of metaâ€™s llama models and parameter-efficient fine-tuning of   large language models: a survey"></a>Evolution of metaâ€™s llama models and parameter-efficient fine-tuning of   large language models: a survey</h2><p><strong>Authors:Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi</strong></p>
<p>This review surveys the rapid evolution of Meta AIâ€™s LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each methodâ€™s mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies. </p>
<blockquote>
<p>æœ¬æ–‡å›é¡¾äº†Meta AIçš„LLaMAï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹Meta AIï¼‰ç³»åˆ—çš„å¿«é€Ÿå‘å±•ï¼Œä»LLaMA 1åˆ°LLaMA 4ä»¥åŠä¸ºè¿™äº›æ¨¡å‹å¼€å‘çš„ä¸“ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æè¿°äº†LLaMAåŸºç¡€æ¨¡å‹å®¶æ—ï¼ˆä»7Båˆ°65Bå†åˆ°288Bå‚æ•°ï¼‰ã€å®ƒä»¬çš„æ¶æ„ï¼ˆåŒ…æ‹¬åŸç”Ÿå¤šæ¨¡æ€å’Œæ··åˆä¸“å®¶å˜ä½“ï¼‰å’Œå…³é”®æ€§èƒ½ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»å¹¶è®¨è®ºäº†PEFTçš„æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µé€šè¿‡ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°æ¥é€‚åº”å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å›é¡¾äº†äº”ç§å·²åº”ç”¨äºLLaMAçš„PEFTæ–¹æ³•ï¼šLoRAï¼ˆä½ç§©é€‚åº”ï¼‰ã€LLaMA-Adapter V1å’ŒV2ã€LLaMA-Excitorå’ŒQuantized LoRAï¼ˆQLoRAï¼‰ã€‚æˆ‘ä»¬è®¨è®ºäº†æ¯ç§æ–¹æ³•çš„æœºåˆ¶ã€å‚æ•°èŠ‚çœæƒ…å†µä»¥åŠå…¶åœ¨LLaMAä¸Šçš„ç¤ºä¾‹åº”ç”¨ï¼ˆä¾‹å¦‚æŒ‡ä»¤è°ƒæ•´ã€å¤šæ¨¡æ€ä»»åŠ¡ï¼‰ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹å’Œé€‚é…å™¨æ¶æ„ã€å‚æ•°è®¡æ•°å’ŒåŸºå‡†æµ‹è¯•ç»“æœè¿›è¡Œäº†ç»“æ„åŒ–è®¨è®ºå’Œåˆ†æï¼ˆåŒ…æ‹¬å¾®è°ƒåçš„LLaMAæ¨¡å‹è¡¨ç°è¶…è¿‡è¾ƒå¤§åŸºå‡†æ¨¡å‹çš„ç¤ºä¾‹ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLaMAæ¨¡å‹åŠå…¶åœ¨ç°å®ä¸–ç•Œä¸­æˆåŠŸåº”ç”¨çš„PEFTçš„å®é™…ç”¨ä¾‹ï¼ˆä¾‹å¦‚æ³•å¾‹å’ŒåŒ»å­¦é¢†åŸŸï¼‰ï¼Œå¹¶è®¨è®ºäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼ˆå¦‚æ‰©å±•åˆ°æ›´å¤§çš„ä¸Šä¸‹æ–‡ç¯å¢ƒå’Œæé«˜ç¨³å¥æ€§ï¼‰ã€‚è¿™ç¯‡ç»¼è¿°è®ºæ–‡ä¸ºå¯¹LLaMAæ¨¡å‹å’Œé«˜æ•ˆå¾®è°ƒç­–ç•¥æ„Ÿå…´è¶£çš„æœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†ä¸€ä¸ªä¸€ç«™å¼çš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12178v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLaMAç³»åˆ—æ¨¡å‹å¿«é€Ÿè¿›åŒ–ï¼Œä»LLaMA 1åˆ°LLaMA 4ï¼Œä»¥åŠé’ˆå¯¹è¿™äº›æ¨¡å‹å¼€å‘çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚æœ¬æ–‡é¦–å…ˆä»‹ç»LLaMAå®¶æ—çš„åŸºç¡€æ¨¡å‹ã€æ¶æ„å’Œå…³é”®æ€§èƒ½ç‰¹ç‚¹ã€‚æ¥ç€ä»‹ç»PEFTçš„æ¦‚å¿µå’Œäº”ç§åº”ç”¨äºLLaMAçš„PEFTæ–¹æ³•ã€‚æœ€åæ¢è®¨LLaMAæ¨¡å‹å’ŒPEFTåœ¨ç°å®ä¸–ç•Œçš„æˆåŠŸåº”ç”¨æ¡ˆä¾‹ï¼Œä»¥åŠæœªæ¥é¢ä¸´çš„æŒ‘æˆ˜å’Œç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaMAç³»åˆ—æ¨¡å‹ä»LLaMA 1åˆ°LLaMA 4å‘ˆç°å¿«é€Ÿè¿›åŒ–ï¼ŒåŒæ—¶å‘å±•å‡ºå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚</li>
<li>LLaMAå®¶æ—æ¨¡å‹å…·æœ‰ä¸åŒçš„å‚æ•°è§„æ¨¡ï¼Œä»7Båˆ°288Bå‚æ•°ã€‚</li>
<li>PEFTæ˜¯ä¸€ç§åªæ›´æ–°é¢„è®­ç»ƒæ¨¡å‹ä¸€å°éƒ¨åˆ†å‚æ•°çš„æ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>LoRAã€LLaMA-Adapter V1å’ŒV2ã€LLaMA-Excitorä»¥åŠQLoRAç­‰äº”ç§PEFTæ–¹æ³•è¢«åº”ç”¨äºLLaMAã€‚</li>
<li>LLaMAæ¨¡å‹åœ¨æŒ‡ä»¤å¾®è°ƒã€å¤šæ¨¡æ€ä»»åŠ¡ç­‰æ–¹é¢æœ‰åº”ç”¨å®ä¾‹ã€‚</li>
<li>LLaMAæ¨¡å‹å’ŒPEFTåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œå·²åœ¨æ³•å¾‹å’ŒåŒ»ç–—é¢†åŸŸå–å¾—æˆåŠŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cdbbabbbf15cad08294170953abc178f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728920&auth_key=1760728920-0-0-561665298915b6821628ad592d4c5b03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-185723f54e3a08ffbcab3dc5c5a135e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728928&auth_key=1760728928-0-0-40d1ae67a9335d1536b7c52de9a99247&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7743abbf611742efb7b82889a5926e22~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728934&auth_key=1760728934-0-0-a25db7b74ca60d6c500f4102156971ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7404671b810b6ac9f1e1cce0a76030b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728941&auth_key=1760728941-0-0-d1c936da2a42463765707a9990c73e32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82c691cb0ba83d5cca1f44dbef073f25~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728948&auth_key=1760728948-0-0-6f35ce1333f3abe8bd899b8a63535bb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4739b18ee06fbf1351bd025feabc6077~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728955&auth_key=1760728955-0-0-ab9f3fb3d9a74e0fcf24975dba87665b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ddffcda8c4a7af987aedb47ac6f33a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728962&auth_key=1760728962-0-0-a17ce02a3780a2dea988a6e569cdc241&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Self-Verifying-Reflection-Helps-Transformers-with-CoT-Reasoning"><a href="#Self-Verifying-Reflection-Helps-Transformers-with-CoT-Reasoning" class="headerlink" title="Self-Verifying Reflection Helps Transformers with CoT Reasoning"></a>Self-Verifying Reflection Helps Transformers with CoT Reasoning</h2><p><strong>Authors:Zhongwei Yu, Wannian Xia, Xue Yan, Bo Xu, Haifeng Zhang, Yali Du, Jun Wang</strong></p>
<p>Advanced large language models (LLMs) frequently reflect in reasoning chain-of-thoughts (CoTs), where they self-verify the correctness of current solutions and explore alternatives. However, given recent findings that LLMs detect limited errors in CoTs, how reflection contributes to empirical improvements remains unclear. To analyze this issue, in this paper, we present a minimalistic reasoning framework to support basic self-verifying reflection for small transformers without natural language, which ensures analytic clarity and reduces the cost of comprehensive experiments. Theoretically, we prove that self-verifying reflection guarantees improvements if verification errors are properly bounded. Experimentally, we show that tiny transformers, with only a few million parameters, benefit from self-verification in both training and reflective execution, reaching remarkable LLM-level performance in integer multiplication and Sudoku. Similar to LLM results, we find that reinforcement learning (RL) improves in-distribution performance and incentivizes frequent reflection for tiny transformers, yet RL mainly optimizes shallow statistical patterns without faithfully reducing verification errors. In conclusion, integrating generative transformers with discriminative verification inherently facilitates CoT reasoning, regardless of scaling and natural language. </p>
<blockquote>
<p>é«˜çº§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸åœ¨æ¨ç†æ€ç»´é“¾ï¼ˆCoTsï¼‰ä¸­åæ˜ å‡ºè‡ªéªŒè¯å½“å‰è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§å¹¶æ¢ç´¢æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œé‰´äºæœ€è¿‘å‘ç°LLMåœ¨CoTsä¸­æ£€æµ‹åˆ°çš„é”™è¯¯æœ‰é™ï¼Œåæ€å¦‚ä½•å¯¹ç»éªŒæ”¹è¿›åšå‡ºè´¡çŒ®ä»ä¸æ¸…æ¥šã€‚ä¸ºäº†åˆ†æè¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç®€æ´çš„æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒæ²¡æœ‰è‡ªç„¶è¯­è¨€çš„å°å‹å˜å‹å™¨çš„åŸºæœ¬è‡ªæˆ‘éªŒè¯åæ€ï¼Œè¿™ç¡®ä¿äº†åˆ†ææ¸…æ™°æ€§å¹¶é™ä½äº†å…¨é¢å®éªŒçš„æˆæœ¬ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†å¦‚æœéªŒè¯é”™è¯¯å¾—åˆ°é€‚å½“çš„é™åˆ¶ï¼Œè‡ªæˆ‘éªŒè¯åæ€å¯ä»¥ä¿è¯æ”¹è¿›ã€‚å®éªŒä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†åªæœ‰æ•°ç™¾ä¸‡å‚æ•°çš„å¾®å‹å˜å‹å™¨åœ¨è®­ç»ƒå’Œåæ€æ‰§è¡Œä¸­éƒ½å—ç›Šäºè‡ªæˆ‘éªŒè¯ï¼Œåœ¨æ•´æ•°ä¹˜æ³•å’Œæ•°ç‹¬æ–¹é¢è¾¾åˆ°äº†æƒŠäººçš„LLMçº§æ€§èƒ½ã€‚ä¸LLMçš„ç»“æœç›¸ä¼¼ï¼Œæˆ‘ä»¬å‘ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†å†…éƒ¨æ€§èƒ½å¹¶æ¿€åŠ±äº†å¾®å‹å˜å‹å™¨çš„é¢‘ç¹åæ€ï¼Œä½†RLä¸»è¦ä¼˜åŒ–æµ…å±‚çš„ç»Ÿè®¡æ¨¡å¼ï¼Œè€Œæ²¡æœ‰å¿ å®åœ°å‡å°‘éªŒè¯é”™è¯¯ã€‚æ€»ä¹‹ï¼Œå°†ç”Ÿæˆå˜å‹å™¨ä¸åˆ¤åˆ«æ€§éªŒè¯ç›¸ç»“åˆï¼Œæœ¬è´¨ä¸Šä¿ƒè¿›äº†æ— è®ºè§„æ¨¡å¤§å°å’Œè‡ªç„¶è¯­è¨€å¦‚ä½•çš„CoTæ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12157v1">PDF</a> Accepted by NeurIPS2025</p>
<p><strong>Summary</strong><br>é«˜çº§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šè¿›è¡Œè‡ªæˆ‘éªŒè¯å’Œæ¢ç´¢æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œé‰´äºæœ€è¿‘å‘ç°LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ£€æµ‹åˆ°çš„é”™è¯¯æœ‰é™ï¼Œåæ€å¦‚ä½•å¯¹ç»éªŒæ”¹è¿›åšå‡ºè´¡çŒ®å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªç®€çº¦çš„æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒå°å˜å½¢å™¨çš„åŸºæœ¬è‡ªæˆ‘éªŒè¯åæ€ï¼Œæ— éœ€è‡ªç„¶è¯­è¨€ï¼Œç¡®ä¿åˆ†ææ¸…æ™°å¹¶é™ä½å…¨é¢å®éªŒçš„æˆæœ¬ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†å¦‚æœéªŒè¯é”™è¯¯å¾—åˆ°é€‚å½“æ§åˆ¶ï¼Œè‡ªæˆ‘éªŒè¯åæ€å¯ä»¥ä¿è¯æ”¹è¿›ã€‚å®éªŒè¡¨æ˜ï¼Œåªæœ‰å‡ ç™¾ä¸‡å‚æ•°çš„å¾®å‹å˜å½¢å™¨åœ¨è®­ç»ƒå’Œåæ€æ‰§è¡Œä¸­éƒ½å—ç›Šäºè‡ªæˆ‘éªŒè¯ï¼Œåœ¨æ•´æ•°ä¹˜æ³•å’Œæ•°ç‹¬æ–¹é¢è¾¾åˆ°äº†æƒŠäººçš„LLMçº§æ€§èƒ½ã€‚ä¸LLMç»“æœç±»ä¼¼ï¼Œæˆ‘ä»¬å‘ç°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†å†…éƒ¨æ€§èƒ½å¹¶æ¿€åŠ±äº†å¾®å‹å˜å½¢å™¨çš„é¢‘ç¹åæ€ï¼Œä½†RLä¸»è¦ä¼˜åŒ–æµ…ç»Ÿè®¡æ¨¡å¼ï¼Œå¹¶æ²¡æœ‰çœŸæ­£å‡å°‘éªŒè¯é”™è¯¯ã€‚ç»“è®ºæ˜¯ï¼Œå°†ç”Ÿæˆå¼å˜å½¢å™¨ä¸åˆ¤åˆ«å¼éªŒè¯ç›¸ç»“åˆï¼Œæœ‰åŠ©äºä¿ƒè¿›æ— è®ºè§„æ¨¡å¤§å°å’Œè‡ªç„¶è¯­è¨€çš„æ¨ç†æ€è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜çº§LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šè¿›è¡Œè‡ªæˆ‘éªŒè¯å’Œæ¢ç´¢æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>è‡ªæˆ‘éªŒè¯åæ€æœ‰åŠ©äºæé«˜LLMçš„æ€§èƒ½ï¼Œä½†éœ€é€‚å½“æ§åˆ¶éªŒè¯é”™è¯¯ã€‚</li>
<li>å¾®å‹å˜å½¢å™¨åœ¨è®­ç»ƒå’Œåæ€æ‰§è¡Œä¸­éƒ½å—ç›Šäºè‡ªæˆ‘éªŒè¯ã€‚</li>
<li>è‡ªæˆ‘éªŒè¯çš„å¾®å‹å˜å½¢å™¨åœ¨æ•´æ•°ä¹˜æ³•å’Œæ•°ç‹¬æ–¹é¢è¡¨ç°å‡ºæƒŠäººçš„æ€§èƒ½ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜å†…éƒ¨æ€§èƒ½ï¼Œä½†ä¸»è¦ä¼˜åŒ–æµ…ç»Ÿè®¡æ¨¡å¼ï¼Œå¹¶æœªçœŸæ­£å‡å°‘éªŒè¯é”™è¯¯ã€‚</li>
<li>ç»“åˆç”Ÿæˆå¼å˜å½¢å™¨å’Œåˆ¤åˆ«å¼éªŒè¯æœ‰åŠ©äºä¿ƒè¿›æ¨ç†æ€è€ƒï¼Œä¸å—è§„æ¨¡å¤§å°æˆ–è‡ªç„¶è¯­è¨€é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fcd6923cf438627f3c6ada7685f31210~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728970&auth_key=1760728970-0-0-a3991fe02d9ae9ff55fbb4036e22a8b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c5d6a131f729a2c25357307e99f2dce3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728977&auth_key=1760728977-0-0-f62f7bd2f4be9b6f779376c283c8a920&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e15b72067381f8c2934747fca3fd083~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728983&auth_key=1760728983-0-0-e86e318570a1ebac034ce3188e9a77b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2368d7b4f937f01ce88c9482f50e964b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728990&auth_key=1760728990-0-0-8d5960452b6df69df2f746f88a6f55d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a1862c1a795a16d75aaf27d0f09839e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728996&auth_key=1760728996-0-0-e17198230f4f7cd2967e761b8e9bebbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-0891dd7b028f247a1bd8a7257aed0b40~resize:0:q75.jpg?source=1f5c5e47&expiration=1760730653&auth_key=1760730653-0-0-fe26461d7f44becd1dc3a4381b9e1399&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Agentic Design of Compositional Machines
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-e4af957f63b08c2752075fc2c2f13d7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726092&auth_key=1760726092-0-0-a53a65026dc21de436c0cc649fc316cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Agentic Design of Compositional Machines
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
