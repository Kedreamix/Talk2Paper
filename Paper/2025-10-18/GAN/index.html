<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e36e96c097e6fedff5bc7e1ec57dfc00')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    40 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection"><a href="#A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection" class="headerlink" title="A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection"></a>A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection</h2><p><strong>Authors:Shivangi Yadav, Arun Ross</strong></p>
<p>An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method. </p>
<blockquote>
<p>è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿå¯èƒ½ä¼šå—åˆ°è¡¨ç°æ”»å‡»ï¼ˆPAsï¼‰çš„å¨èƒï¼Œæ”»å‡»è€…ä½¿ç”¨äººé€ çœ¼ç›ã€æ‰“å°çš„çœ¼éƒ¨å›¾åƒæˆ–ç¾å®¹éšå½¢çœ¼é•œç­‰ä¼ªé€ ç‰©å“æ¥æ¬ºéª—ç³»ç»Ÿã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå·²ç»å¼€å‘äº†å‡ ç§è¡¨ç°æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºæ„é€ å’Œæˆåƒè¡¨ç°æ”»å‡»å­˜åœ¨éšæ€§çš„å›°éš¾ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°è™¹è†œPADæŠ€æœ¯çš„æ•°æ®é›†ååˆ†åŒ®ä¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šåŸŸå›¾åƒè½¬æ¢æ‰©æ•£é£æ ¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆMID-StyleGANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç”Ÿæˆåˆæˆçœ¼éƒ¨å›¾åƒçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚çœŸå®çœ¼éƒ¨å›¾åƒã€æ‰“å°çš„çœ¼éƒ¨å›¾åƒå’Œç¾å®¹éšå½¢çœ¼é•œï¼‰æ•æ‰è¡¨ç°æ”»å‡»å’ŒçœŸå®ç‰¹å¾ã€‚MID-StyleGANç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ï¼Œç”ŸæˆçœŸå®ä¸”å¤šæ ·åŒ–çš„åˆæˆæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤šåŸŸæ¶æ„ï¼Œèƒ½å¤Ÿåœ¨çœŸå®çœ¼éƒ¨å›¾åƒå’Œä¸åŒè¡¨ç°æ”»å‡»é¢†åŸŸä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚è¯¥æ¨¡å‹é‡‡ç”¨é’ˆå¯¹çœ¼éƒ¨æ•°æ®çš„è‡ªé€‚åº”æŸå¤±å‡½æ•°ï¼Œä»¥ä¿æŒé¢†åŸŸä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆçœ¼éƒ¨å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä½¿ç”¨ç”Ÿæˆçš„æ•°æ®æ˜¾è‘—æé«˜äº†PADç³»ç»Ÿçš„æ€§èƒ½ï¼Œä¸ºè§£å†³è™¹è†œå’Œçœ¼éƒ¨ç”Ÿç‰©è¯†åˆ«ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œåœ¨LivDet2020æ•°æ®é›†ä¸Šï¼Œåœ¨1%çš„è¯¯æ£€ç‡ä¸‹ï¼ŒçœŸå®æ£€æµ‹ç‡ä»93.41%æé«˜åˆ°äº†98.72%ï¼Œå±•ç¤ºäº†æ‰€æå‡ºæ–¹æ³•çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14314v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­å­˜åœ¨çš„æ¼”ç¤ºæ”»å‡»ï¼ˆPAsï¼‰é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¡†æ¶MID-StyleGANã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸç”Ÿæˆé€¼çœŸçš„è™¹è†œå›¾åƒæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒMID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆè™¹è†œå›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè§£å†³äº†è™¹è†œç”Ÿç‰©è¯†åˆ«ä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¼”ç¤ºæ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿæ˜“å—åˆ°æ¼”ç¤ºæ”»å‡»ï¼ˆPAsï¼‰çš„å½±å“ï¼Œéœ€è¦ä½¿ç”¨æ¼”ç¤ºæ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰æ–¹æ³•è¿›è¡Œé˜²èŒƒã€‚</li>
<li>å½“å‰ç¼ºä¹ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è™¹è†œPADæŠ€æœ¯çš„æ•°æ®é›†ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å›¾åƒç”Ÿæˆæ¡†æ¶MID-StyleGANï¼Œç”¨äºç”Ÿæˆåˆæˆè™¹è†œå›¾åƒæ•°æ®ã€‚</li>
<li>MID-StyleGANç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸç”Ÿæˆé€¼çœŸçš„è™¹è†œå›¾åƒæ•°æ®ã€‚</li>
<li>MID-StyleGANé‡‡ç”¨è‡ªé€‚åº”æŸå¤±å‡½æ•°æ¥ä¿æŒé¢†åŸŸä¸€è‡´æ€§ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>å®éªŒè¯æ˜MID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆè™¹è†œå›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80a4fdafcc32495b50c09a446a994bfb" align="middle">
<img src="https://picx.zhimg.com/v2-6056cd86ebec8885b5424fb61832431d" align="middle">
<img src="https://picx.zhimg.com/v2-43c738f3cd0db04b00bbb1720790275e" align="middle">
<img src="https://picx.zhimg.com/v2-122efb3575dd31fe74dde03e03b33246" align="middle">
<img src="https://picx.zhimg.com/v2-fb6859920f9fd33d336e66f021b91ed8" align="middle">
<img src="https://picx.zhimg.com/v2-20972dcc99576616073b9315b35c5966" align="middle">
<img src="https://picx.zhimg.com/v2-9aa34d8e9eb70a66a0a99f36cdcd4fd4" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LOTA-Bit-Planes-Guided-AI-Generated-Image-Detection"><a href="#LOTA-Bit-Planes-Guided-AI-Generated-Image-Detection" class="headerlink" title="LOTA: Bit-Planes Guided AI-Generated Image Detection"></a>LOTA: Bit-Planes Guided AI-Generated Image Detection</h2><p><strong>Authors:Hongsong Wang, Renxi Cheng, Yang Zhang, Chaolei Han, Jie Gui</strong></p>
<p>The rapid advancement of GAN and Diffusion models makes it more difficult to distinguish AI-generated images from real ones. Recent studies often use image-based reconstruction errors as an important feature for determining whether an image is AI-generated. However, these approaches typically incur high computational costs and also fail to capture intrinsic noisy features present in the raw images. To solve these problems, we innovatively refine error extraction by using bit-plane-based image processing, as lower bit planes indeed represent noise patterns in images. We introduce an effective bit-planes guided noisy image generation and exploit various image normalization strategies, including scaling and thresholding. Then, to amplify the noise signal for easier AI-generated image detection, we design a maximum gradient patch selection that applies multi-directional gradients to compute the noise score and selects the region with the highest score. Finally, we propose a lightweight and effective classification head and explore two different structures: noise-based classifier and noise-guided classifier. Extensive experiments on the GenImage benchmark demonstrate the outstanding performance of our method, which achieves an average accuracy of \textbf{98.9%} (\textbf{11.9}%~$\uparrow$) and shows excellent cross-generator generalization capability. Particularly, our method achieves an accuracy of over 98.2% from GAN to Diffusion and over 99.2% from Diffusion to GAN. Moreover, it performs error extraction at the millisecond level, nearly a hundred times faster than existing methods. The code is at <a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA">https://github.com/hongsong-wang/LOTA</a>. </p>
<blockquote>
<p>GANå’Œæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—åŒºåˆ†äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒå’ŒçœŸå®å›¾åƒå˜å¾—æ›´åŠ å›°éš¾ã€‚æœ€è¿‘çš„ç ”ç©¶ç»å¸¸ä½¿ç”¨åŸºäºå›¾åƒçš„é‡å»ºè¯¯å·®ä½œä¸ºåˆ¤æ–­å›¾åƒæ˜¯å¦ç”±äººå·¥æ™ºèƒ½ç”Ÿæˆçš„é‡è¦ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œå¹¶ä¸”æ— æ³•æ•æ‰åˆ°åŸå§‹å›¾åƒä¸­å­˜åœ¨çš„å†…åœ¨å™ªå£°ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°é€šè¿‡åŸºäºä½å¹³é¢çš„å›¾åƒå¤„ç†æ¥ä¼˜åŒ–è¯¯å·®æå–ï¼Œå› ä¸ºä½ä½å¹³é¢ç¡®å®ä»£è¡¨äº†å›¾åƒä¸­çš„å™ªå£°æ¨¡å¼ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„ä½å¹³é¢å¼•å¯¼å™ªå£°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå¹¶é‡‡ç”¨äº†å„ç§å›¾åƒå½’ä¸€åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬ç¼©æ”¾å’Œé˜ˆå€¼åŒ–ã€‚ç„¶åï¼Œä¸ºäº†æ”¾å¤§å™ªå£°ä¿¡å·ï¼Œä¾¿äºæ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœ€å¤§æ¢¯åº¦è¡¥ä¸é€‰æ‹©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åº”ç”¨å¤šæ–¹å‘æ¢¯åº¦æ¥è®¡ç®—å™ªå£°åˆ†æ•°ï¼Œå¹¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„åŒºåŸŸã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§ä¸”æœ‰æ•ˆçš„åˆ†ç±»å¤´ï¼Œå¹¶æ¢ç´¢äº†ä¸¤ç§ä¸åŒç»“æ„ï¼šåŸºäºå™ªå£°çš„åˆ†ç±»å™¨å’Œå™ªå£°å¼•å¯¼çš„åˆ†ç±»å™¨ã€‚åœ¨GenImageåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„å‡ºè‰²æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†98.9%ï¼ˆä¸Šå‡äº†11.9%ï¼‰ï¼Œå¹¶æ˜¾ç¤ºå‡ºå‡ºè‰²çš„è·¨ç”Ÿæˆå™¨æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä»GANåˆ°æ‰©æ•£æ¨¡å‹çš„å‡†ç¡®ç‡è¶…è¿‡98.2%ï¼Œä»æ‰©æ•£æ¨¡å‹åˆ°GANçš„å‡†ç¡®ç‡è¶…è¿‡99.2%ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ¯«ç§’çº§åˆ«æ‰§è¡Œè¯¯å·®æå–ï¼Œå‡ ä¹æ˜¯ç°æœ‰æ–¹æ³•çš„100å€å¿«ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA%E3%80%82">https://github.com/hongsong-wang/LOTAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14230v1">PDF</a> Published in the ICCV2025, COde is   <a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA">https://github.com/hongsong-wang/LOTA</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨ä½å¹³é¢å¤„ç†æŠ€æœ¯å¯¹å›¾åƒè¿›è¡Œå™ªå£°å¤„ç†çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å™ªå£°å¼•å¯¼çš„å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œå®ç°äº†å¯¹AIç”Ÿæˆå›¾åƒçš„é«˜æ•ˆæ£€æµ‹ã€‚é‡‡ç”¨æœ€å¤§æ¢¯åº¦å—é€‰æ‹©ç®—æ³•ï¼Œå¢å¼ºå™ªå£°ä¿¡å·ï¼Œå¹¶è®¾è®¡äº†è½»é‡çº§çš„åˆ†ç±»å™¨å¤´ã€‚åœ¨GenImageåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¹³å‡å‡†ç¡®ç‡é«˜è¾¾98.9%çš„ä¼˜å¼‚è¡¨ç°ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„è·¨ç”Ÿæˆå™¨æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨GANå’ŒDiffusionæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç ”ç©¶AIç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒçš„åŒºåˆ†å˜å¾—æ›´åŠ å›°éš¾ã€‚</li>
<li>å½“å‰ç ”ç©¶å¸¸é‡‡ç”¨åŸºäºå›¾åƒé‡å»ºè¯¯å·®çš„æ–¹æ³•æ¥åˆ¤æ–­å›¾åƒæ˜¯å¦ç”±AIç”Ÿæˆï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œæ— æ³•æ•æ‰å›¾åƒå†…åœ¨å™ªå£°ç‰¹å¾çš„é—®é¢˜ã€‚</li>
<li>åˆ›æ–°æ€§åœ°ä½¿ç”¨ä½å¹³é¢å¤„ç†æŠ€æœ¯æ¥ä¼˜åŒ–è¯¯å·®æå–ï¼Œåˆ©ç”¨ä½é˜¶ä½å¹³é¢ä»£è¡¨å›¾åƒä¸­çš„å™ªå£°æ¨¡å¼ã€‚</li>
<li>å¼•å…¥å™ªå£°å¼•å¯¼çš„å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨å¤šç§å›¾åƒå½’ä¸€åŒ–ç­–ç•¥ã€‚</li>
<li>é€šè¿‡æœ€å¤§æ¢¯åº¦å—é€‰æ‹©ç®—æ³•æ”¾å¤§å™ªå£°ä¿¡å·ï¼Œä¾¿äºæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>è®¾è®¡äº†è½»é‡çº§çš„åˆ†ç±»å™¨å¤´ï¼Œå¹¶æ¢ç´¢äº†ä¸¤ç§ä¸åŒç»“æ„ï¼šåŸºäºå™ªå£°çš„åˆ†ç±»å™¨å’Œå™ªå£°å¼•å¯¼çš„åˆ†ç±»å™¨ã€‚</li>
<li>åœ¨GenImageåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¹³å‡å‡†ç¡®ç‡é«˜è¾¾98.9%çš„ä¼˜å¼‚è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„è·¨ç”Ÿæˆå™¨æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä»GANåˆ°Diffusionå’Œä»Diffusionåˆ°GANçš„å‡†ç¡®ç‡åˆ†åˆ«è¶…è¿‡98.2%å’Œ99.2%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4b90f46d84dc0793bab0a7629f5d391" align="middle">
<img src="https://picx.zhimg.com/v2-88e4463b350a13dfd876c873c2229e66" align="middle">
<img src="https://picx.zhimg.com/v2-e36e96c097e6fedff5bc7e1ec57dfc00" align="middle">
<img src="https://picx.zhimg.com/v2-f7f5f134905652cfc5d37d57c217ee61" align="middle">
<img src="https://picx.zhimg.com/v2-7239b3a08036f28d666d0f89f51ecac0" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-Selective-Guided-Diffusion-Model-for-Old-Photo-Face-Restoration"><a href="#Self-Supervised-Selective-Guided-Diffusion-Model-for-Old-Photo-Face-Restoration" class="headerlink" title="Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face   Restoration"></a>Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face   Restoration</h2><p><strong>Authors:Wenjie Li, Xiangyi Wang, Heng Guo, Guangwei Gao, Zhanyu Ma</strong></p>
<p>Old-photo face restoration poses significant challenges due to compounded degradations such as breakage, fading, and severe blur. Existing pre-trained diffusion-guided methods either rely on explicit degradation priors or global statistical guidance, which struggle with localized artifacts or face color. We propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages pseudo-reference faces generated by a pre-trained diffusion model under weak guidance. These pseudo-labels exhibit structurally aligned contours and natural colors, enabling region-specific restoration via staged supervision: structural guidance applied throughout the denoising process and color refinement in later steps, aligned with the coarse-to-fine nature of diffusion. By incorporating face parsing maps and scratch masks, our method selectively restores breakage regions while avoiding identity mismatch. We further construct VintageFace, a 300-image benchmark of real old face photos with varying degradation levels. SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual quality, fidelity, and regional controllability. Code link: <a target="_blank" rel="noopener" href="https://github.com/PRIS-CV/SSDiff">https://github.com/PRIS-CV/SSDiff</a>. </p>
<blockquote>
<p>æ—§ç…§ç‰‡äººè„¸ä¿®å¤ç”±äºæ–­è£‚ã€è¤ªè‰²å’Œä¸¥é‡æ¨¡ç³Šç­‰å¤åˆé€€åŒ–è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„é¢„è®­ç»ƒæ‰©æ•£å¼•å¯¼æ–¹æ³•è¦ä¹ˆä¾èµ–äºæ˜ç¡®çš„é€€åŒ–å…ˆéªŒï¼Œè¦ä¹ˆä¾èµ–äºå…¨å±€ç»Ÿè®¡å¼•å¯¼ï¼Œå®ƒä»¬åœ¨å¤„ç†å±€éƒ¨ä¼ªå½±æˆ–äººè„¸é¢œè‰²æ—¶é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªç›‘ç£é€‰æ‹©æ€§å¼•å¯¼æ‰©æ•£ï¼ˆSSDiffï¼‰ï¼Œå®ƒåˆ©ç”¨å¼±å¼•å¯¼ä¸‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ä¼ªå‚è€ƒäººè„¸ã€‚è¿™äº›ä¼ªæ ‡ç­¾å…·æœ‰ç»“æ„å¯¹é½çš„è½®å»“å’Œè‡ªç„¶é¢œè‰²ï¼Œé€šè¿‡åˆ†é˜¶æ®µç›‘ç£å®ç°ç‰¹å®šåŒºåŸŸä¿®å¤ï¼šå»å™ªè¿‡ç¨‹ä¸­åº”ç”¨ç»“æ„æŒ‡å¯¼ï¼Œåç»­æ­¥éª¤è¿›è¡Œé¢œè‰²ç»†åŒ–ï¼Œä¸æ‰©æ•£çš„ç”±ç²—åˆ°ç»†ç‰¹ç‚¹ç›¸å¯¹åº”ã€‚é€šè¿‡ç»“åˆäººè„¸è§£æå›¾å’Œåˆ’ç—•æ©è†œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé€‰æ‹©æ€§ä¿®å¤æ–­è£‚åŒºåŸŸï¼ŒåŒæ—¶é¿å…èº«ä»½ä¸åŒ¹é…ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†VintageFaceï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸åŒé€€åŒ–ç¨‹åº¦çœŸå®æ—§ç…§ç‰‡äººè„¸çš„300å¼ å›¾åƒåŸºå‡†æµ‹è¯•é›†ã€‚SSDiffåœ¨æ„ŸçŸ¥è´¨é‡ã€ä¿çœŸåº¦å’ŒåŒºåŸŸå¯æ§æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºGANå’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚ä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/PRIS-CV/SSDiff%E3%80%82">https://github.com/PRIS-CV/SSDiffã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12114v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è€ç…§ç‰‡äººè„¸ä¿®å¤çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ–­è£‚ã€è¤ªè‰²å’Œä¸¥é‡æ¨¡ç³Šç­‰é—®é¢˜ã€‚ç°æœ‰çš„é¢„è®­ç»ƒæ‰©æ•£å¼•å¯¼æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä¾èµ–æ˜ç¡®çš„é€€åŒ–å…ˆéªŒæˆ–å…¨å±€ç»Ÿè®¡æŒ‡å¯¼ï¼Œéš¾ä»¥å¤„ç†å±€éƒ¨ä¼ªå½±æˆ–äººè„¸é¢œè‰²é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§è‡ªç›‘ç£é€‰æ‹©æ€§å¼•å¯¼æ‰©æ•£ï¼ˆSSDiffï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨å¼±å¼•å¯¼ä¸‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„äººè„¸ä¼ªå‚è€ƒå›¾åƒã€‚ä¼ªæ ‡ç­¾å…·æœ‰ç»“æ„å¯¹é½çš„è½®å»“å’Œè‡ªç„¶é¢œè‰²ï¼Œé€šè¿‡åˆ†é˜¶æ®µç›‘ç£å®ç°ç‰¹å®šåŒºåŸŸä¿®å¤ï¼šå»å™ªè¿‡ç¨‹ä¸­åº”ç”¨ç»“æ„æŒ‡å¯¼ï¼ŒåæœŸæ­¥éª¤è¿›è¡Œé¢œè‰²ä¼˜åŒ–ï¼Œä¸æ‰©æ•£çš„ç”±ç²—åˆ°ç»†ç‰¹æ€§ç›¸ç¬¦ã€‚ç»“åˆäººè„¸è§£æå›¾å’Œåˆ’ç—•æ©è†œï¼Œè¯¥æ–¹æ³•èƒ½é€‰æ‹©æ€§æ¢å¤æ–­è£‚åŒºåŸŸï¼ŒåŒæ—¶é¿å…èº«ä»½ä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†VintageFaceæ•°æ®é›†ï¼ŒåŒ…å«300å¼ ä¸åŒé€€åŒ–ç¨‹åº¦çš„è€ç…§ç‰‡äººè„¸å›¾åƒã€‚SSDiffåœ¨æ„ŸçŸ¥è´¨é‡ã€ä¿çœŸåº¦å’ŒåŒºåŸŸå¯æ§æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºGANå’Œæ‰©æ•£çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è€ç…§ç‰‡äººè„¸ä¿®å¤é¢ä¸´æ–­è£‚ã€è¤ªè‰²å’Œæ¨¡ç³Šç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ˜ç¡®çš„é€€åŒ–å…ˆéªŒæˆ–å…¨å±€ç»Ÿè®¡æŒ‡å¯¼ï¼Œéš¾ä»¥å¤„ç†å±€éƒ¨ä¼ªå½±å’Œé¢œè‰²é—®é¢˜ã€‚</li>
<li>æå‡ºSSDiffæ–¹æ³•ï¼Œåˆ©ç”¨ä¼ªå‚è€ƒå›¾åƒå®ç°è‡ªç›‘ç£é€‰æ‹©æ€§å¼•å¯¼æ‰©æ•£ã€‚</li>
<li>ä¼ªæ ‡ç­¾å…·æœ‰ç»“æ„å¯¹é½çš„è½®å»“å’Œè‡ªç„¶é¢œè‰²ï¼Œé€šè¿‡åˆ†é˜¶æ®µç›‘ç£å®ç°ç‰¹å®šåŒºåŸŸä¿®å¤ã€‚</li>
<li>ç»“åˆäººè„¸è§£æå›¾å’Œåˆ’ç—•æ©è†œï¼Œèƒ½é€‰æ‹©æ€§æ¢å¤æ–­è£‚åŒºåŸŸï¼Œé¿å…èº«ä»½ä¸åŒ¹é…ã€‚</li>
<li>æ„å»ºäº†VintageFaceæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§é€€åŒ–ç¨‹åº¦çš„è€ç…§ç‰‡äººè„¸å›¾åƒã€‚</li>
<li>SSDiffåœ¨æ„ŸçŸ¥è´¨é‡ã€ä¿çœŸåº¦å’ŒåŒºåŸŸå¯æ§æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4820a76657be3c9c6c0fd8f614b143c" align="middle">
<img src="https://picx.zhimg.com/v2-61356183ea71dee10088769cbc2f7377" align="middle">
<img src="https://picx.zhimg.com/v2-30882c8bb6dba2709b5e3099f0224500" align="middle">
<img src="https://picx.zhimg.com/v2-43a14c6a443683c01d5dbd83820598e4" align="middle">
<img src="https://picx.zhimg.com/v2-b53509e8562497ff7770b04ce8cfc0fb" align="middle">
<img src="https://picx.zhimg.com/v2-a4490650f51e265940739da3253e871b" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DISC-GAN-Disentangling-Style-and-Content-for-Cluster-Specific-Synthetic-Underwater-Image-Generation"><a href="#DISC-GAN-Disentangling-Style-and-Content-for-Cluster-Specific-Synthetic-Underwater-Image-Generation" class="headerlink" title="DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic   Underwater Image Generation"></a>DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic   Underwater Image Generation</h2><p><strong>Authors:Sneha Varur, Anirudh R Hanchinamani, Tarun S Bagewadi, Uma Mudenagudi, Chaitra D Desai, Sujata C, Padmashree Desai, Sumit Meharwade</strong></p>
<p>In this paper, we propose a novel framework, Disentangled Style-Content GAN (DISC-GAN), which integrates style-content disentanglement with a cluster-specific training strategy towards photorealistic underwater image synthesis. The quality of synthetic underwater images is challenged by optical due to phenomena such as color attenuation and turbidity. These phenomena are represented by distinct stylistic variations across different waterbodies, such as changes in tint and haze. While generative models are well-suited to capture complex patterns, they often lack the ability to model the non-uniform conditions of diverse underwater environments. To address these challenges, we employ K-means clustering to partition a dataset into style-specific domains. We use separate encoders to get latent spaces for style and content; we further integrate these latent representations via Adaptive Instance Normalization (AdaIN) and decode the result to produce the final synthetic image. The model is trained independently on each style cluster to preserve domain-specific characteristics. Our framework demonstrates state-of-the-art performance, obtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak Signal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance (FID) of 13.3728. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå³è§£çº ç¼ é£æ ¼å†…å®¹GANï¼ˆDISC-GANï¼‰ï¼Œå®ƒå°†é£æ ¼å†…å®¹è§£çº ç¼ ä¸é’ˆå¯¹ç‰¹å®šé›†ç¾¤çš„è®­ç»ƒç­–ç•¥ç›¸ç»“åˆï¼Œä»¥å®ç°é€¼çœŸçš„æ°´ä¸‹å›¾åƒåˆæˆã€‚ç”±äºé¢œè‰²è¡°å‡å’Œæµ‘æµŠç­‰ç°è±¡ï¼Œåˆæˆæ°´ä¸‹å›¾åƒçš„è´¨é‡é¢ä¸´å…‰å­¦æŒ‘æˆ˜ã€‚è¿™äº›ç°è±¡è¡¨ç°ä¸ºä¸åŒæ°´ä½“ä¹‹é—´ç‹¬ç‰¹çš„é£æ ¼å˜åŒ–ï¼Œä¾‹å¦‚è‰²è°ƒå’Œé›¾éœ¾çš„å˜åŒ–ã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹éå¸¸é€‚åˆæ•æ‰å¤æ‚æ¨¡å¼ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹å¯¹ä¸åŒæ°´ä¸‹ç¯å¢ƒéå‡åŒ€æ¡ä»¶çš„å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨K-meansèšç±»å°†æ•°æ®é›†åˆ’åˆ†ä¸ºç‰¹å®šé£æ ¼çš„é¢†åŸŸã€‚æˆ‘ä»¬ä½¿ç”¨å•ç‹¬çš„ç¼–ç å™¨è·å–é£æ ¼å’Œå†…å®¹çš„æ½œåœ¨ç©ºé—´ï¼›æˆ‘ä»¬é€šè¿‡è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ï¼ˆAdaINï¼‰è¿›ä¸€æ­¥æ•´åˆè¿™äº›æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶è§£ç ç»“æœä»¥äº§ç”Ÿæœ€ç»ˆçš„åˆæˆå›¾åƒã€‚è¯¥æ¨¡å‹æ˜¯åœ¨æ¯ä¸ªé£æ ¼é›†ç¾¤ä¸Šç‹¬ç«‹è®­ç»ƒçš„ï¼Œä»¥ä¿ç•™ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ¡†æ¶å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè·å¾—äº†ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰ä¸º0.9012ã€å¹³å‡å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º32.5118åˆ†è´å’Œå¼—é›·æ­‡ç‰¹å…¥å£è·ç¦»ï¼ˆFIDï¼‰ä¸º13.3728çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”è§£çº ç¼ é£æ ¼å†…å®¹GANï¼ˆDISC-GANï¼‰ï¼Œå®ƒå°†é£æ ¼å†…å®¹è§£çº ç¼ ä¸ç‰¹å®šé›†ç¾¤çš„è®­ç»ƒç­–ç•¥ç›¸ç»“åˆï¼Œä»¥å®ç°é€¼çœŸçš„æ°´ä¸‹å›¾åƒåˆæˆã€‚é’ˆå¯¹æ°´ä¸‹å›¾åƒåˆæˆä¸­å› å…‰å­¦ç°è±¡ï¼ˆå¦‚é¢œè‰²è¡°å‡å’Œæµ‘æµŠåº¦ï¼‰å¯¼è‡´çš„æŒ‘æˆ˜ï¼ŒDISC-GANé€šè¿‡K-meansèšç±»å°†æ•°æ®é›†åˆ†æˆç‰¹å®šé£æ ¼çš„é¢†åŸŸï¼Œå¹¶åˆ†åˆ«è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»¥æ¨¡æ‹Ÿä¸åŒçš„æ°´ä¸‹ç¯å¢ƒã€‚è¯¥æ¡†æ¶é‡‡ç”¨å•ç‹¬çš„ç¼–ç å™¨è·å–é£æ ¼å’Œå†…å®¹çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ï¼ˆAdaINï¼‰æ•´åˆè¿™äº›æ½œåœ¨è¡¨ç¤ºï¼Œä»¥äº§ç”Ÿæœ€ç»ˆåˆæˆå›¾åƒã€‚è¯¥æ¡†æ¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DISC-GANæ¡†æ¶ç»“åˆäº†é£æ ¼å†…å®¹è§£çº ç¼ å’Œç‰¹å®šé›†ç¾¤çš„è®­ç»ƒç­–ç•¥ï¼Œç”¨äºæ°´ä¸‹å›¾åƒåˆæˆã€‚</li>
<li>æ°´ä¸‹å›¾åƒåˆæˆé¢ä¸´å…‰å­¦ç°è±¡ï¼ˆå¦‚é¢œè‰²è¡°å‡å’Œæµ‘æµŠåº¦ï¼‰çš„æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨K-meansèšç±»å°†æ•°æ®é›†åˆ†æˆç‰¹å®šé£æ ¼çš„é¢†åŸŸï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å•ç‹¬çš„ç¼–ç å™¨è·å–é£æ ¼å’Œå†…å®¹çš„æ½œåœ¨ç©ºé—´ã€‚</li>
<li>é€šè¿‡AdaINæ•´åˆæ½œåœ¨è¡¨ç¤ºï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„åˆæˆå›¾åƒã€‚</li>
<li>æ¨¡å‹åœ¨æ¯ä¸ªé£æ ¼é›†ç¾¤ä¸Šè¿›è¡Œç‹¬ç«‹è®­ç»ƒï¼Œä»¥ä¿ç•™ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d157f4cdefd4f2b515cca641fc27e7db" align="middle">
<img src="https://picx.zhimg.com/v2-7a8b96ede5120c5ddf504aa38b9be228" align="middle">
<img src="https://picx.zhimg.com/v2-360660fe45fa250fe373f79f2feaae46" align="middle">
<img src="https://picx.zhimg.com/v2-22e7a23d5084472a2b82dca68d5527a8" align="middle">
<img src="https://picx.zhimg.com/v2-048419971a067f41490a35904a1477f7" align="middle">
<img src="https://picx.zhimg.com/v2-61a178cd322a1af88cb8c376ed507dd0" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-1st-Solution-for-CARE-Liver-Task-Challenge-2025-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation"><a href="#The-1st-Solution-for-CARE-Liver-Task-Challenge-2025-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation" class="headerlink" title="The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware   Semi-Supervised Segmentation with Domain Generalization and Test-Time   Adaptation"></a>The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware   Semi-Supervised Segmentation with Domain Generalization and Test-Time   Adaptation</h2><p><strong>Authors:Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang</strong></p>
<p>Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions. </p>
<blockquote>
<p>ä»å¯¹æ¯”å¢å¼ºMRIè¿›è¡Œå‡†ç¡®çš„è‚è„åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨æ•°æ®æœ‰é™ã€å¢å¼ºåè®®å¼‚è´¨ä»¥åŠæ‰«æä»ªå’Œæœºæ„ä¹‹é—´çš„é¢†åŸŸå·®å¼‚è¾ƒå¤§ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¡†æ¶åœ¨é¢†åŸŸæ¨å¹¿æ–¹é¢å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†å…¶åº”ç”¨å¹¶ä¸ç›´æ¥ã€‚ä¾‹å¦‚ï¼ŒPix2Pixéœ€è¦è¿›è¡Œå›¾åƒæ³¨å†Œï¼Œè€Œå¾ªç¯GANæ— æ³•æ— ç¼é›†æˆåˆ°åˆ†å‰²ç®¡é“ä¸­ã€‚åŒæ—¶ï¼Œè¿™äº›æ–¹æ³•æœ€åˆæ˜¯ç”¨äºå¤„ç†è·¨æ¨¡æ€åœºæ™¯çš„ï¼Œé€šå¸¸ä¼šå¼•å…¥ç»“æ„å¤±çœŸå¹¶é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¿™åœ¨æˆ‘ä»¬è¿™ç§å•æ¨¡æ€åœºæ™¯ä¸­å¯èƒ½ä¼šå¸¦æ¥ç¼ºç‚¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoSSeg-TTAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹GED4ï¼ˆé’†å¡æŸå¢å¼ºçš„è‚èƒ†æœŸMRIï¼‰æ¨¡æ€çš„ç´§å‡‘åˆ†å‰²æ¡†æ¶ï¼Œå®ƒåŸºäºnnU-Netv2å¹¶å¢å¼ºäº†ä¸€ç§åŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆæ¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°ä½“ç§¯æ•°æ®ã€‚ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å—ï¼Œç»“åˆåŸºäºéšæœºç›´æ–¹å›¾çš„é£æ ¼å¤–è§‚è½¬ç§»å‡½æ•°å’Œä¸€ä¸ªå¯è®­ç»ƒçš„å¯¹æ¯”æ„ŸçŸ¥ç½‘ç»œï¼Œä¸°å¯Œäº†é¢†åŸŸå¤šæ ·æ€§å¹¶å‡è½»äº†è·¨ä¸­å¿ƒå·®å¼‚æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¿ç»­æµ‹è¯•æ—¶é—´é€‚åº”ç­–ç•¥ï¼Œä»¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºnnU-Netv2åŸºçº¿ï¼Œåœ¨Diceå¾—åˆ†å’ŒHausdorffè·ç¦»æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼ŒåŒæ—¶åœ¨ä½æ³¨é‡Šæ¡ä»¶ä¸‹å¯¹æœªè§é¢†åŸŸå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04243v2">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong><br>åœ¨å¢å¼ºMRIä¸­å¯¹è‚è„è¿›è¡Œå‡†ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ä½†ç”±äºæ ‡æ³¨æ•°æ®æœ‰é™ã€å¢å¼ºåè®®å¼‚è´¨æ€§å’Œè·¨æ‰«æä»ªå’Œæœºæ„åŸŸåç§»ç­‰é—®é¢˜ï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿå›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¡†æ¶åœ¨åŸŸæ¨å¹¿æ–¹é¢å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†åº”ç”¨å¹¶ä¸ç›´æ¥ã€‚ä¾‹å¦‚ï¼ŒPix2Pixéœ€è¦å›¾åƒé…å‡†ï¼Œcycle-GANæ— æ³•æ— ç¼é›†æˆåˆ°åˆ†å‰²ç®¡é“ä¸­ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoSSeg-TTAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹GED4æ¨¡æ€çš„ç´§å‡‘åˆ†å‰²æ¡†æ¶ï¼ŒåŸºäºnnU-Netv2æ„å»ºï¼Œå¹¶é€šè¿‡åŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆåˆ©ç”¨å¤§é‡æœªæ ‡è®°ä½“ç§¯è¿›è¡Œå¢å¼ºã€‚åŸŸé€‚åº”æ¨¡å—é€šè¿‡éšæœºç›´æ–¹å›¾æ ·å¼çš„é£æ ¼è½¬æ¢åŠŸèƒ½å’Œå¯è®­ç»ƒçš„å¯¹æ¯”ç½‘ç»œï¼Œä¸°å¯Œäº†åŸŸå¤šæ ·æ€§å¹¶å‡è½»äº†è·¨ä¸­å¿ƒå·®å¼‚ã€‚æ­¤å¤–ï¼Œåœ¨æµ‹è¯•é˜¶æ®µé‡‡ç”¨äº†è¿ç»­æµ‹è¯•æ—¶é—´é€‚åº”ç­–ç•¥ï¼Œä»¥æé«˜æ¨ç†é˜¶æ®µçš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æœ‰é™æ ‡æ³¨æ¡ä»¶ä¸‹å§‹ç»ˆä¼˜äºnnU-Netv2åŸºçº¿ï¼Œå®ç°äº†æ›´é«˜çš„Diceåˆ†æ•°å’ŒHausdorffè·ç¦»ï¼Œå¹¶åœ¨æœªè§åŸŸçš„æƒ…å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚è„åœ¨å¢å¼ºMRIä¸­çš„å‡†ç¡®åˆ†å‰²å¯¹è¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¡†æ¶åœ¨åŸŸé€‚åº”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨äºè‚è„åˆ†å‰²ã€‚</li>
<li>æå‡ºçš„CoSSeg-TTAæ¡†æ¶åŸºäºnnU-Netv2æ„å»ºï¼Œæ—¨åœ¨è§£å†³æœ‰é™æ ‡æ³¨æ•°æ®ã€å¢å¼ºåè®®å¼‚è´¨æ€§å’Œè·¨æœºæ„åŸŸåç§»ç­‰é—®é¢˜ã€‚</li>
<li>CoSSeg-TTAåˆ©ç”¨åŠç›‘ç£å­¦ä¹ æ–¹æ³•å’Œå‡å€¼æ•™å¸ˆæ–¹æ¡ˆä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®ã€‚</li>
<li>åŸŸé€‚åº”æ¨¡å—é€šè¿‡éšæœºç›´æ–¹å›¾æ ·å¼è½¬æ¢ä¸°å¯ŒåŸŸå¤šæ ·æ€§ï¼Œå¹¶å‡è½»è·¨ä¸­å¿ƒå·®å¼‚ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨è¿ç»­æµ‹è¯•æ—¶é—´é€‚åº”ç­–ç•¥ï¼Œæé«˜æ¨ç†é˜¶æ®µçš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3d7bfd5e60b514a48bb97b3ef7c2171" align="middle">
<img src="https://picx.zhimg.com/v2-fd93b6d3ead1a4ebf9ee116bf4377edf" align="middle">
<img src="https://picx.zhimg.com/v2-4759b31fe7fc13d067ccf4222b0acdc1" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective"><a href="#AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective" class="headerlink" title="AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective"></a>AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective</h2><p><strong>Authors:Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</strong></p>
<p>Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. In addition, to ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text&#x2F;audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized Phoneme-Frame Causal Attention Mask to support multi-step parallel acceleration. Extensive experiments conducted on both Chinese (CMLR) and English (HDTF) datasets demonstrate that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution. </p>
<blockquote>
<p>å¤´éƒ¨åŠ¨ç”»ä¸»è¦å…³æ³¨ä»éŸ³é¢‘è¾“å…¥ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨è§†é¢‘ã€‚ç»§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¹‹åï¼Œæ‰©æ•£æ¨¡å‹ç”±äºå…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºä¸»æµã€‚ç„¶è€Œï¼Œæ‰©æ•£è¿‡ç¨‹æœ¬èº«çš„å›ºæœ‰å±€é™æ€§å¸¸å¸¸å¯¼è‡´å¸§é—´é—ªçƒå’Œæ¨ç†ç¼“æ…¢ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨å®é™…éƒ¨ç½²ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AvatarSyncï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºéŸ³ç´ è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•å¼ å‚è€ƒå›¾åƒç”Ÿæˆé€¼çœŸä¸”å¯æ§çš„å¤´éƒ¨åŠ¨ç”»ï¼Œç›´æ¥ç”±æ–‡æœ¬æˆ–éŸ³é¢‘è¾“å…¥é©±åŠ¨ã€‚ä¸ºäº†å‡è½»é—ªçƒå¹¶ç¡®ä¿è¿ç»­æ€§ï¼ŒAvatarSyncåˆ©ç”¨äº†ä¸€ä¸ªè‡ªå›å½’ç®¡é“ï¼Œå¢å¼ºäº†æ—¶é—´å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿å¯æ§æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†éŸ³ç´ ï¼ˆå³è¯­éŸ³çš„åŸºæœ¬å•ä½ï¼‰ï¼Œæ„å»ºäº†ä»æ–‡æœ¬&#x2F;éŸ³é¢‘åˆ°éŸ³ç´ çš„å¤šå¯¹ä¸€æ˜ å°„ï¼Œå®ç°äº†ç²¾ç¡®çš„éŸ³ç´ åˆ°è§†è§‰çš„å¯¹é½ã€‚å¦å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œå°†è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨æ€è§£è€¦ï¼Œå¹¶èå…¥å®šåˆ¶çš„éŸ³ç´ å¸§å› æœæ³¨æ„åŠ›æ©ç ï¼Œä»¥æ”¯æŒå¤šæ­¥å¹¶è¡ŒåŠ é€Ÿã€‚åœ¨ä¸­æ–‡ï¼ˆCMLRï¼‰å’Œè‹±æ–‡ï¼ˆHDTFï¼‰æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„å¤´éƒ¨åŠ¨ç”»æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12052v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ‰©æ•£æ¨¡å‹å·²æˆä¸ºä¸»æµè¯´è¯äººå¤´åŠ¨ç”»çš„ä¸»æµæ–¹æ³•ï¼Œä½†å…¶å›ºæœ‰çš„æ‰©æ•£è¿‡ç¨‹é™åˆ¶å…¶å®é™…åº”ç”¨ï¼Œå¦‚å¸§é—´é—ªçƒå’Œæ¨ç†é€Ÿåº¦æ…¢ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºAvatarSyncï¼Œè¿™æ˜¯ä¸€ç§åŸºäºéŸ³ç´ è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶ï¼Œå¯ä»å•å¼ å‚è€ƒå›¾åƒç”Ÿæˆé€¼çœŸä¸”å¯æ§çš„è¯´è¯äººå¤´åŠ¨ç”»ï¼Œç”±æ–‡æœ¬æˆ–éŸ³é¢‘è¾“å…¥é©±åŠ¨ã€‚ä¸ºå‡å°‘é—ªçƒå¹¶ç¡®ä¿è¿ç»­æ€§ï¼ŒAvatarSyncé‡‡ç”¨å¢å¼ºæ—¶é—´å»ºæ¨¡çš„è‡ªå›å½’ç®¡é“ã€‚é€šè¿‡å¼•å…¥éŸ³ç´ å¹¶å»ºç«‹æ–‡æœ¬&#x2F;éŸ³é¢‘åˆ°éŸ³ç´ çš„å¤šå¯¹ä¸€æ˜ å°„ï¼Œç¡®ä¿å¯æ§æ€§ã€‚æ­¤å¤–ï¼Œä¸ºåŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œå°†è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨æ€è§£è€¦ï¼Œå¹¶èå…¥å®šåˆ¶çš„éŸ³ç´ å¸§å› æœæ³¨æ„åŠ›æ©ç ä»¥æ”¯æŒå¤šæ­¥å¹¶è¡ŒåŠ é€Ÿã€‚åœ¨ä¸­æ–‡ï¼ˆCMLRï¼‰å’Œè‹±æ–‡ï¼ˆHDTFï¼‰æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰è¯´è¯äººå¤´åŠ¨ç”»æ–¹æ³•ï¼Œæä¾›å¯ä¼¸ç¼©å’Œå¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯äººå¤´åŠ¨ç”»é¢†åŸŸæ­£è½¬å‘åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œå› å…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å­˜åœ¨å¸§é—´é—ªçƒå’Œæ¨ç†é€Ÿåº¦æ…¢ç­‰å†…åœ¨é™åˆ¶ã€‚</li>
<li>AvatarSyncæ˜¯ä¸€ä¸ªè‡ªå›å½’æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å‚è€ƒå›¾åƒç”Ÿæˆé€¼çœŸä¸”å¯æ§çš„è¯´è¯äººå¤´åŠ¨ç”»ï¼Œç”±æ–‡æœ¬æˆ–éŸ³é¢‘é©±åŠ¨ã€‚</li>
<li>ä¸ºå‡å°‘é—ªçƒå¹¶ç¡®ä¿è¿ç»­æ€§ï¼ŒAvatarSyncé‡‡ç”¨å¢å¼ºæ—¶é—´å»ºæ¨¡çš„è‡ªå›å½’ç®¡é“ã€‚</li>
<li>é€šè¿‡å¼•å…¥éŸ³ç´ å¹¶å»ºç«‹å¤šå¯¹ä¸€æ˜ å°„ï¼Œæé«˜åŠ¨ç”»çš„ç²¾ç¡®æ€§å’Œå¯æ§æ€§ã€‚</li>
<li>ä¸ºåŠ é€Ÿæ¨ç†ï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥å’Œå®šåˆ¶çš„éŸ³ç´ å¸§å› æœæ³¨æ„åŠ›æ©ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-048d641b03e67540af17e882e55f70c6" align="middle">
<img src="https://picx.zhimg.com/v2-8077d61ade754d02cc81cb658becfc0d" align="middle">
<img src="https://picx.zhimg.com/v2-9b6c5da1fb33bc9098fcecb7a7597568" align="middle">
<img src="https://picx.zhimg.com/v2-07e6ea6fa8caed71571d12ebb34a9ea0" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StegOT-Trade-offs-in-Steganography-via-Optimal-Transport"><a href="#StegOT-Trade-offs-in-Steganography-via-Optimal-Transport" class="headerlink" title="StegOT: Trade-offs in Steganography via Optimal Transport"></a>StegOT: Trade-offs in Steganography via Optimal Transport</h2><p><strong>Authors:Chengde Lin, Xuezhu Gong, Shuxue Ding, Mingzhe Yang, Xijun Lu, Chengjun Mo</strong></p>
<p>Image hiding is often referred to as steganography, which aims to hide a secret image in a cover image of the same resolution. Many steganography models are based on genera-tive adversarial networks (GANs) and variational autoencoders (VAEs). However, most existing models suffer from mode collapse. Mode collapse will lead to an information imbalance between the cover and secret images in the stego image and further affect the subsequent extraction. To address these challenges, this paper proposes StegOT, an autoencoder-based steganography model incorporating optimal transport theory. We designed the multiple channel optimal transport (MCOT) module to transform the feature distribution, which exhibits multiple peaks, into a single peak to achieve the trade-off of information. Experiments demonstrate that we not only achieve a trade-off between the cover and secret images but also enhance the quality of both the stego and recovery images. The source code will be released on <a target="_blank" rel="noopener" href="https://github.com/Rss1124/StegOT">https://github.com/Rss1124/StegOT</a>. </p>
<blockquote>
<p>å›¾åƒéšè—é€šå¸¸è¢«ç§°ä¸ºéšå†™æœ¯ï¼ˆsteganographyï¼‰ï¼Œæ—¨åœ¨å°†ä¸€ä¸ªç§˜å¯†å›¾åƒéšè—åœ¨ç›¸åŒåˆ†è¾¨ç‡çš„è½½ä½“å›¾åƒä¸­ã€‚è®¸å¤šéšå†™æœ¯æ¨¡å‹åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ¨¡å‹éƒ½å­˜åœ¨æ¨¡å¼å´©æºƒçš„é—®é¢˜ã€‚æ¨¡å¼å´©æºƒä¼šå¯¼è‡´è½½ä½“å›¾åƒå’Œç§˜å¯†å›¾åƒä¹‹é—´çš„ä¿¡æ¯ä¸å¹³è¡¡ï¼Œè¿›è€Œå½±å“åç»­çš„æå–è¿‡ç¨‹ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†StegOTæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„éšå†™æœ¯æ¨¡å‹ï¼Œç»“åˆäº†æœ€ä¼˜ä¼ è¾“ç†è®ºã€‚æˆ‘ä»¬è®¾è®¡äº†å¤šé€šé“æœ€ä¼˜ä¼ è¾“ï¼ˆMCOTï¼‰æ¨¡å—æ¥è½¬æ¢ç‰¹å¾åˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒè¡¨ç°å‡ºå¤šä¸ªå³°å€¼ï¼Œå°†å…¶è½¬åŒ–ä¸ºå•å³°åˆ†å¸ƒä»¥å®ç°ä¿¡æ¯çš„æƒè¡¡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬ä¸ä»…åœ¨è½½ä½“å›¾åƒå’Œç§˜å¯†å›¾åƒä¹‹é—´å®ç°äº†æƒè¡¡ï¼Œè¿˜æé«˜äº†éšå†™å›¾åƒå’Œæ¢å¤å›¾åƒçš„è´¨é‡ã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Rss1124/StegOT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Rss1124/StegOTä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11178v2">PDF</a> Accepted by IEEE International Conference on Multimedia and Expo   (ICME 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè‡ªç¼–ç å™¨çš„éšå†™æœ¯æ¨¡å‹StegOTï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æœ€ä¼˜ä¼ è¾“ç†è®ºä»¥è§£å†³éšå†™æœ¯ä¸­å¸¸è§çš„æ¨¡å¼å´©æºƒé—®é¢˜ã€‚é€šè¿‡è®¾è®¡å¤šé€šé“æœ€ä¼˜ä¼ è¾“ï¼ˆMCOTï¼‰æ¨¡å—ï¼Œå°†ç‰¹å¾åˆ†å¸ƒçš„å¤šå³°è½¬åŒ–ä¸ºå•å³°ï¼Œå®ç°äº†ä¿¡æ¯å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…å®ç°äº†è¦†ç›–å›¾åƒä¸ç§˜å¯†å›¾åƒä¹‹é—´çš„å¹³è¡¡ï¼Œè¿˜æé«˜äº†éšå†™å’Œæ¢å¤å›¾åƒçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StegOTæ˜¯ä¸€ä¸ªåŸºäºè‡ªç¼–ç å™¨çš„éšå†™æœ¯æ¨¡å‹ï¼Œç»“åˆäº†æœ€ä¼˜ä¼ è¾“ç†è®ºã€‚</li>
<li>MCOTæ¨¡å—èƒ½å°†ç‰¹å¾åˆ†å¸ƒçš„å¤šå³°è½¬åŒ–ä¸ºå•å³°ï¼Œä»¥å®ç°ä¿¡æ¯å¹³è¡¡ã€‚</li>
<li>StegOTå¯ä»¥è§£å†³éšå†™æœ¯ä¸­å¸¸è§çš„æ¨¡å¼å´©æºƒé—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜StegOTèƒ½å¤Ÿå¹³è¡¡è¦†ç›–å›¾åƒå’Œç§˜å¯†å›¾åƒä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>StegOTèƒ½æé«˜éšå†™å’Œæ¢å¤å›¾åƒçš„è´¨é‡ã€‚</li>
<li>StegOTçš„æºä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Rss1124/StegOT%E3%80%82">https://github.com/Rss1124/StegOTã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b87567e6c647b59ba6f71952bba7cd9c" align="middle">
<img src="https://picx.zhimg.com/v2-b68efca11c9731a543d5338b94d82548" align="middle">
<img src="https://picx.zhimg.com/v2-06c91dad9f84cfa4af3259eaf46693eb" align="middle">
<img src="https://picx.zhimg.com/v2-a85913a0b01110b08e77a0cfdd2cd1e1" align="middle">
<img src="https://picx.zhimg.com/v2-c3ced8d5775e8e2a71db9d38006b8ecf" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PanoLAM-Large-Avatar-Model-for-Gaussian-Full-Head-Synthesis-from-One-shot-Unposed-Image"><a href="#PanoLAM-Large-Avatar-Model-for-Gaussian-Full-Head-Synthesis-from-One-shot-Unposed-Image" class="headerlink" title="PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from   One-shot Unposed Image"></a>PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from   One-shot Unposed Image</h2><p><strong>Authors:Peng Li, Yisheng He, Yingdong Hu, Yuan Dong, Weihao Yuan, Yuan Liu, Siyu Zhu, Gang Cheng, Zilong Dong, Yike Guo</strong></p>
<p>We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work. Project page at: <a target="_blank" rel="noopener" href="https://panolam.github.io/">https://panolam.github.io/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå•ä¸€æœªå®šä½å›¾åƒçš„é«˜æ–¯å…¨å¤´åˆæˆå‰é¦ˆæ¡†æ¶ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–äºè€—æ—¶GANåæ¼”å’Œæµ‹è¯•æ—¶ä¼˜åŒ–çš„å·¥ä½œï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­æ ¹æ®å•ä¸€æœªå®šä½å›¾åƒé‡å»ºé«˜æ–¯å…¨å¤´æ¨¡å‹ã€‚è¿™å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°å¿«é€Ÿé‡å»ºå’Œæ¸²æŸ“ã€‚ä¸ºäº†ç¼“è§£å¤§è§„æ¨¡3Då¤´éƒ¨èµ„äº§ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºè®­ç»ƒå¥½çš„3D GANsçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå¹¶ä¸”åªç”¨åˆæˆæ•°æ®æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¡†æ¶ã€‚ä¸ºäº†è¿›è¡Œé«˜æ•ˆçš„é«˜ä¿çœŸç”Ÿæˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»ç²—åˆ°ç»†çš„é«˜æ–¯å¤´éƒ¨ç”Ÿæˆç®¡é“ï¼Œå…¶ä¸­FLAMEæ¨¡å‹çš„ç¨€ç–ç‚¹ä¸å›¾åƒç‰¹å¾é€šè¿‡transformerå—è¿›è¡Œç‰¹å¾æå–å’Œç²—ç•¥å½¢çŠ¶é‡å»ºï¼Œç„¶åè¿›è¡Œå¯†é›†åŒ–ä»¥å®ç°é«˜ä¿çœŸé‡å»ºã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„3D GANsä¸­çš„å…ˆéªŒçŸ¥è¯†è¿›è¡Œæœ‰æ•ˆçš„é‡å»ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒåˆ†æ”¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°èšåˆç»“æ„åŒ–çš„çƒé¢triplaneç‰¹å¾å’Œéç»“æ„åŒ–çš„ç‚¹åŸºç‰¹å¾ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„é«˜æ–¯å¤´éƒ¨é‡å»ºã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶ç›¸è¾ƒäºç°æœ‰å·¥ä½œçš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š<a target="_blank" rel="noopener" href="https://panolam.github.io/">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07552v2">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‰é¦ˆç½‘ç»œçš„é«˜æ–¯å…¨å¤´åˆæˆæ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ æœªå®šä½å›¾åƒä¸­é‡å»ºé«˜æ–¯å…¨å¤´æ¨¡å‹ã€‚æ¡†æ¶æ— éœ€è€—è´¹æ—¶é—´çš„GANåæ¼”å’Œæµ‹è¯•æ—¶é—´ä¼˜åŒ–ï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿé‡å»ºå’Œæ¸²æŸ“ã€‚ä¸ºè§£å†³ç¼ºä¹å¤§è§„æ¨¡3Då¤´éƒ¨èµ„äº§çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨è®­ç»ƒå¥½çš„3D GANsç”Ÿæˆå¤§è§„æ¨¡åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ä¸ºæé«˜ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ï¼Œå¼•å…¥ä»ç²—åˆ°ç»†çš„é«˜æ–¯å¤´éƒ¨ç”Ÿæˆç®¡é“ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„3D GANsä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œæå‡ºåŒåˆ†æ”¯æ¡†æ¶è¿›è¡Œæ›´æœ‰æ•ˆçš„å¤´éƒ¨é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç ”ç©¶æå‡ºçš„æ¡†æ¶ç›¸å¯¹äºç°æœ‰å·¥ä½œæ›´æœ‰æ•ˆã€‚è¯¦æƒ…å‚è§é¡¹ç›®é¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§åŸºäºå‰é¦ˆç½‘ç»œçš„é«˜æ–¯å…¨å¤´åˆæˆæ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ æœªå®šä½å›¾åƒé‡å»ºæ¨¡å‹ã€‚</li>
<li>æ¡†æ¶æ— éœ€æ—¶é—´æ¶ˆè€—çš„åæ¼”å’Œä¼˜åŒ–è¿‡ç¨‹ï¼Œå®ç°å¿«é€Ÿé‡å»ºå’Œæ¸²æŸ“ã€‚</li>
<li>åˆ©ç”¨è®­ç»ƒå¥½çš„3D GANsç”Ÿæˆå¤§è§„æ¨¡åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè§£å†³ç¼ºä¹çœŸå®æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥ä»ç²—åˆ°ç»†çš„é«˜æ–¯å¤´éƒ¨ç”Ÿæˆç®¡é“ï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„3D GANsä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ä¸ªåŒåˆ†æ”¯æ¡†æ¶ï¼Œç”¨äºæ›´é«˜æ•ˆçš„å¤´éƒ¨é‡å»ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76b9fff782f0c2a1f0c924e4b6a27722" align="middle">
<img src="https://picx.zhimg.com/v2-e93f49726b817261f4cfd661a066c550" align="middle">
<img src="https://picx.zhimg.com/v2-834b82e71db97b338ecdc3d4a451bf6f" align="middle">
<img src="https://picx.zhimg.com/v2-11875b038386f7955b0beb30acb6e0ec" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SQ-GAN-Semantic-Image-Communications-Using-Masked-Vector-Quantization"><a href="#SQ-GAN-Semantic-Image-Communications-Using-Masked-Vector-Quantization" class="headerlink" title="SQ-GAN: Semantic Image Communications Using Masked Vector Quantization"></a>SQ-GAN: Semantic Image Communications Using Masked Vector Quantization</h2><p><strong>Authors:Francesco Pezone, Sergio Barbarossa, Giuseppe Caire</strong></p>
<p>This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic&#x2F;task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­ä¹‰æ©ç å‘é‡é‡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆSQ-GANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå°†è¯­ä¹‰é©±åŠ¨çš„å›¾åƒç¼–ç å’Œå‘é‡é‡åŒ–ç›¸ç»“åˆï¼Œé’ˆå¯¹è¯­ä¹‰&#x2F;ä»»åŠ¡å¯¼å‘é€šä¿¡ä¼˜åŒ–å›¾åƒå‹ç¼©ã€‚è¯¥æ–¹æ³•ä»…å¯¹æºç¼–ç èµ·ä½œç”¨ï¼Œå¹¶å®Œå…¨ç¬¦åˆæ—§ç³»ç»Ÿã€‚è¯­ä¹‰æ˜¯é€šè¿‡å›¾åƒè®¡ç®—å¾—åˆ°çš„ï¼Œä½¿ç”¨ç°æˆçš„è½¯ä»¶è®¡ç®—å…¶è¯­ä¹‰åˆ†å‰²å›¾ã€‚æ–°å¼€å‘çš„è¯­ä¹‰æ¡ä»¶è‡ªé€‚åº”æ©æ¨¡æ¨¡å—ï¼ˆSAMMï¼‰ä¼šé€‰æ‹©æ€§åœ°å¯¹å›¾åƒä¸­çš„è¯­ä¹‰ç›¸å…³ç‰¹å¾è¿›è¡Œç¼–ç ã€‚ä¸åŒè¯­ä¹‰ç±»åˆ«çš„ç›¸å…³æ€§æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œé€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥é€‚å½“çš„æƒé‡ï¼Œåœ¨è®­ç»ƒé˜¶æ®µå°†å…¶çº³å…¥è€ƒè™‘ã€‚SQ-GANåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„å›¾åƒå‹ç¼©æ–¹æ¡ˆï¼ŒåŒ…æ‹¬JPEG2000ã€BPGå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œåœ¨æä½å‹ç¼©ç‡ä¸‹ï¼Œå¯¹é‡å»ºå›¾åƒçš„æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰åˆ†å‰²å‡†ç¡®æ€§éƒ½æœ‰æ˜¾è‘—çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09520v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2502.01675</p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºSQ-GANçš„æ–°å‹è¯­ä¹‰è’™ç‰ˆçŸ¢é‡é‡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚è¿™ä¸ªæ–¹æ³•å°†è¯­ä¹‰é©±åŠ¨å›¾åƒç¼–ç å’ŒçŸ¢é‡é‡åŒ–ç›¸ç»“åˆï¼Œæ—¨åœ¨ä¼˜åŒ–è¯­ä¹‰&#x2F;ä»»åŠ¡å¯¼å‘é€šä¿¡çš„å›¾åƒå‹ç¼©æ•ˆæœã€‚è¯¥æ–¹æ³•åªä½œç”¨äºæºç¼–ç ï¼Œå…¼å®¹ç°æœ‰ç³»ç»Ÿã€‚åˆ©ç”¨å¸‚å”®è½¯ä»¶è®¡ç®—å›¾åƒçš„è¯­ä¹‰åˆ†å‰²å›¾æ¥æå–è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥è¯­ä¹‰æ¡ä»¶è‡ªé€‚åº”è’™ç‰ˆæ¨¡å—ï¼ˆSAMMï¼‰ï¼Œé€‰æ‹©æ€§ç¼–ç å›¾åƒä¸­çš„è¯­ä¹‰ç›¸å…³ç‰¹å¾ã€‚SQ-GANåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„å›¾åƒå‹ç¼©æ–¹æ¡ˆï¼ŒåŒ…æ‹¬JPEG2000ã€BPGå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡å»ºå›¾åƒçš„æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰åˆ†å‰²å‡†ç¡®æ€§æ–¹é¢ï¼Œå³ä½¿åœ¨æä½çš„å‹ç¼©ç‡ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SQ-GANæ˜¯ä¸€ä¸ªæ–°å‹çš„å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œç»“åˆäº†è¯­ä¹‰é©±åŠ¨å›¾åƒç¼–ç å’ŒçŸ¢é‡é‡åŒ–æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸»è¦é’ˆå¯¹æºç¼–ç è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶å…¼å®¹ç°æœ‰ç³»ç»Ÿã€‚</li>
<li>SQ-GANé€šè¿‡è®¡ç®—å›¾åƒçš„è¯­ä¹‰åˆ†å‰²å›¾æ¥æå–è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>SAMMæ¨¡å—ç”¨äºé€‰æ‹©æ€§ç¼–ç å›¾åƒä¸­çš„è¯­ä¹‰ç›¸å…³ç‰¹å¾ï¼Œè€ƒè™‘äº†ä¸åŒè¯­ä¹‰ç±»åˆ«çš„ä»»åŠ¡ç‰¹å¼‚æ€§ã€‚</li>
<li>SQ-GANåœ¨å¤šç§å›¾åƒå‹ç¼©æ–¹æ¡ˆä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰åˆ†å‰²å‡†ç¡®æ€§æ–¹é¢çš„ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>SQ-GANåœ¨æä½å‹ç¼©ç‡ä¸‹ä¾ç„¶èƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0509c42e728ca4f51fb81bf18e29d8" align="middle">
<img src="https://picx.zhimg.com/v2-5bd873fcaf9dafbfda591ae7dc6c79b4" align="middle">
<img src="https://picx.zhimg.com/v2-5c4b5eb621962733d9ade98dd3494ce2" align="middle">
<img src="https://picx.zhimg.com/v2-47cd7ec746f62aa1c10c3bdf738c00f1" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RATLIP-Generative-Adversarial-CLIP-Text-to-Image-Synthesis-Based-on-Recurrent-Affine-Transformations"><a href="#RATLIP-Generative-Adversarial-CLIP-Text-to-Image-Synthesis-Based-on-Recurrent-Affine-Transformations" class="headerlink" title="RATLIP: Generative Adversarial CLIP Text-to-Image Synthesis Based on   Recurrent Affine Transformations"></a>RATLIP: Generative Adversarial CLIP Text-to-Image Synthesis Based on   Recurrent Affine Transformations</h2><p><strong>Authors:Chengde Lin, Xijun Lu, Guangxi Chen</strong></p>
<p>Synthesizing high-quality photorealistic images with textual descriptions as a condition is very challenging. Generative Adversarial Networks (GANs), the classical model for this task, frequently suffer from low consistency between image and text descriptions and insufficient richness in synthesized images. Recently, conditional affine transformations (CAT), such as conditional batch normalization and instance normalization, have been applied to different layers of GAN to control content synthesis in images. CAT is a multi-layer perceptron that independently predicts data based on batch statistics between neighboring layers, with global textual information unavailable to other layers. To address this issue, we first model CAT and a recurrent neural network (RAT) to ensure that different layers can access global information. We then introduce shuffle attention between RAT to mitigate the characteristic of information forgetting in recurrent neural networks. Moreover, both our generator and discriminator utilize the powerful pre-trained model, Clip, which has been extensively employed for establishing associations between text and images through the learning of multimodal representations in latent space. The discriminator utilizes CLIPâ€™s ability to comprehend complex scenes to accurately assess the quality of the generated images. Extensive experiments have been conducted on the CUB, Oxford, and CelebA-tiny datasets to demonstrate the superiority of the proposed model over current state-of-the-art models. The code is <a target="_blank" rel="noopener" href="https://github.com/OxygenLu/RATLIP">https://github.com/OxygenLu/RATLIP</a>. </p>
<blockquote>
<p>å°†æ–‡æœ¬æè¿°ä½œä¸ºæ¡ä»¶åˆæˆé«˜è´¨é‡çš„ç…§ç‰‡çº§å›¾åƒæ˜¯ä¸€é¡¹éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æ˜¯è¿™é¡¹ä»»åŠ¡çš„ç»å…¸æ¨¡å‹ï¼Œä½†å®ƒä»¬åœ¨å›¾åƒå’Œæ–‡æœ¬æè¿°ä¹‹é—´çš„ä¸€è‡´æ€§è¾ƒä½ï¼Œä¸”åˆæˆå›¾åƒçš„ä¸°å¯Œæ€§ä¸è¶³ã€‚æœ€è¿‘ï¼Œæ¡ä»¶ä»¿å°„å˜æ¢ï¼ˆCATï¼Œå¦‚æ¡ä»¶æ‰¹é‡å½’ä¸€åŒ–å’Œå®ä¾‹å½’ä¸€åŒ–ï¼‰å·²åº”ç”¨äºGANçš„ä¸åŒå±‚ï¼Œä»¥æ§åˆ¶å›¾åƒä¸­çš„å†…å®¹åˆæˆã€‚CATæ˜¯ä¸€ç§å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå®ƒåŸºäºç›¸é‚»å±‚ä¹‹é—´çš„æ‰¹é‡ç»Ÿè®¡æ•°æ®ç‹¬ç«‹é¢„æµ‹æ•°æ®ï¼Œè€Œå…¶ä»–å±‚æ— æ³•è·å–å…¨å±€æ–‡æœ¬ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹CATå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRATï¼‰æ¨¡å‹ï¼Œä»¥ç¡®ä¿ä¸åŒå±‚å¯ä»¥è®¿é—®å…¨å±€ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨RATä¹‹é—´å¼•å…¥shuffleæ³¨æ„åŠ›ï¼Œä»¥å‡è½»å¾ªç¯ç¥ç»ç½‘ç»œä¸­ä¿¡æ¯é—å¿˜çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨éƒ½ä½¿ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹Clipã€‚Clipå·²è¢«å¹¿æ³›ç”¨äºé€šè¿‡å»ºç«‹æ½œåœ¨ç©ºé—´ä¸­çš„å¤šæ¨¡å¼è¡¨ç¤ºæ¥å»ºç«‹æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„å…³è”ã€‚åˆ¤åˆ«å™¨åˆ©ç”¨CLIPç†è§£å¤æ‚åœºæ™¯çš„èƒ½åŠ›æ¥å‡†ç¡®è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚åœ¨CUBã€Oxfordå’ŒCelebA-tinyæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä»¥è¯æ˜æ‰€æå‡ºçš„æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/OxygenLu/RATLIP%E3%80%82">https://github.com/OxygenLu/RATLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.08114v2">PDF</a> Accepted by 2024 IEEE International Conference on Systems, Man, and   Cybernetics(SMC)</p>
<p><strong>Summary</strong><br>     åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—æ€§ç½‘ç»œï¼ˆGANsï¼‰åˆæˆé«˜è´¨é‡é€¼çœŸçš„å›¾åƒå¹¶ä»¥æ–‡æœ¬æè¿°ä½œä¸ºæ¡ä»¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œæ¡ä»¶ä»¿å°„å˜æ¢ï¼ˆCATï¼‰è¢«åº”ç”¨äºGANçš„ä¸åŒå±‚ä»¥æ§åˆ¶å›¾åƒå†…å®¹çš„åˆæˆã€‚ä¸ºè§£å†³ä¸åŒå±‚æ— æ³•è®¿é—®å…¨å±€ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ç»“åˆæ¡ä»¶ä»¿å°„å˜æ¢å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRATï¼‰çš„æ¨¡å‹ï¼Œå¹¶å¼•å…¥shuffle attentionæ¥ç¼“è§£å¾ªç¯ç¥ç»ç½‘ç»œä¸­ä¿¡æ¯é—å¿˜çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨éƒ½åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹Clipï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¥å»ºç«‹æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„è”ç³»ã€‚åˆ¤åˆ«å™¨åˆ©ç”¨CLIPç†è§£å¤æ‚åœºæ™¯çš„èƒ½åŠ›æ¥å‡†ç¡®è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚åœ¨CUBã€Oxfordå’ŒCelebA-tinyæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANåœ¨åˆæˆé«˜è´¨é‡é€¼çœŸçš„å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä¸æ–‡æœ¬æè¿°çš„ä¸€è‡´æ€§é—®é¢˜å’Œå›¾åƒä¸°å¯Œæ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æ¡ä»¶ä»¿å°„å˜æ¢ï¼ˆCATï¼‰è¢«åº”ç”¨äºGANçš„ä¸åŒå±‚ä»¥æ”¹è¿›å›¾åƒå†…å®¹çš„åˆæˆã€‚</li>
<li>ä¸ºè§£å†³å…¨å±€ä¿¡æ¯è®¿é—®é—®é¢˜ï¼Œç»“åˆäº†CATå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRATï¼‰ï¼Œå¹¶å¼•å…¥shuffle attentionæ¥ç¼“è§£ä¿¡æ¯é—å¿˜ã€‚</li>
<li>ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨éƒ½åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹Clipï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¥å»ºç«‹æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„è”ç³»ã€‚</li>
<li>åˆ¤åˆ«å™¨åˆ©ç”¨CLIPç†è§£å¤æ‚åœºæ™¯çš„èƒ½åŠ›è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.08114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0bd47c2a492a445c0bbee3f7ff2b19be" align="middle">
<img src="https://picx.zhimg.com/v2-0d0e293b55b331a467fea321423276fc" align="middle">
<img src="https://picx.zhimg.com/v2-73c1f95f7e9e2475b9fef246f5aee46d" align="middle">
<img src="https://picx.zhimg.com/v2-9e6a1d5a08e291703c7dbc04ae9b4e21" align="middle">
<img src="https://picx.zhimg.com/v2-eb4e66b77ea52606afa120d29a6187e2" align="middle">
<img src="https://picx.zhimg.com/v2-c9c755f78c727f3b528404a64410172c" align="middle">
<img src="https://picx.zhimg.com/v2-08a6787b39663368be16585dfc47299f" align="middle">
<img src="https://picx.zhimg.com/v2-0de4b30b79423a03e7f61c72f40bf444" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e93f49726b817261f4cfd661a066c550" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-221339e4fe9daa95e0bac391bcfb220f" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  PIA Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic   Analysis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
