<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="元宇宙/虚拟人">
    <meta name="description" content="元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-10-18  Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>元宇宙/虚拟人 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2509.07552v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">元宇宙/虚拟人</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                <span class="chip bg-color">元宇宙/虚拟人</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                元宇宙/虚拟人
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="Instant-Skinned-Gaussian-Avatars-for-Web-Mobile-and-VR-Applications"><a href="#Instant-Skinned-Gaussian-Avatars-for-Web-Mobile-and-VR-Applications" class="headerlink" title="Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications"></a>Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications</h2><p><strong>Authors:Naruya Kondo, Yuto Asano, Yoichi Ochiai</strong></p>
<p>We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website: <a target="_blank" rel="noopener" href="https://sites.google.com/view/gaussian-vrm">https://sites.google.com/view/gaussian-vrm</a> </p>
<blockquote>
<p>我们推出了即时皮肤化高斯化身（Instant Skinned Gaussian Avatars），这是一个实时跨平台的3D化身系统。虽然有许多方法已经提出用于动画高斯拼贴（Gaussian Splatting），但它们通常需要相机阵列、长时间的预处理或高端GPU。一些方法试图将高斯拼贴转换为基于网格的表示形式，以实现轻量级性能，但牺牲了视觉保真度。相比之下，我们的系统通过利用并行拼贴处理来有效地动画化高斯拼贴，能够实时追踪底层蒙皮网格，同时保持高视觉保真度。从基于智能手机的3D扫描到设备上的预处理，整个过程仅需大约五分钟，其中化身生成步骤本身仅需约30秒即可完成。我们的系统使用户能够立即将他们在现实世界中的外观转化为3D化身，使其成为与社交媒体和元宇宙应用程序无缝集成的理想选择。网站：<a target="_blank" rel="noopener" href="https://sites.google.com/view/gaussian-vrm/">https://sites.google.com/view/gaussian-vrm/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13978v1">PDF</a> Accepted to SUI 2025 Demo Track</p>
<p><strong>Summary</strong><br>实时跨平台三维阿凡达系统——即时蒙皮高斯阿凡达呈现。该系统通过利用并行贴片处理，实时动态跟随蒙皮网格，同时保持高视觉保真度，高效动画高斯拼贴。整个流程从手机3D扫描到设备预处理只需约五分钟，其中阿凡达生成步骤只需约30秒。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了即时蒙皮高斯阿凡达系统，实现实时跨平台的三维阿凡达呈现。</li>
<li>利用并行贴片处理，高效动画化高斯拼贴。</li>
<li>系统可实时动态跟随蒙皮网格，保持高视觉保真度。</li>
<li>流程简洁快速，从手机3D扫描到设备预处理只需五分钟。</li>
<li>阿凡达生成步骤快速，仅需约30秒。</li>
<li>系统适用于社交媒体和元宇宙应用的无缝集成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13978">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.13978v1/page_0_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HRM-2Avatar-High-Fidelity-Real-Time-Mobile-Avatars-from-Monocular-Phone-Scans"><a href="#HRM-2Avatar-High-Fidelity-Real-Time-Mobile-Avatars-from-Monocular-Phone-Scans" class="headerlink" title="HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone   Scans"></a>HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone   Scans</h2><p><strong>Authors:Chao Shi, Shenghao Jia, Jinhui Liu, Yong Zhang, Liangchao Zhu, Zhonglei Yang, Jinze Ma, Chaoyue Niu, Chengfei Lv</strong></p>
<p>We present HRM$^2$Avatar, a framework for creating high-fidelity avatars from monocular phone scans, which can be rendered and animated in real time on mobile devices. Monocular capture with smartphones provides a low-cost alternative to studio-grade multi-camera rigs, making avatar digitization accessible to non-expert users. Reconstructing high-fidelity avatars from single-view video sequences poses challenges due to limited visual and geometric data. To address these limitations, at the data level, our method leverages two types of data captured with smartphones: static pose sequences for texture reconstruction and dynamic motion sequences for learning pose-dependent deformations and lighting changes. At the representation level, we employ a lightweight yet expressive representation to reconstruct high-fidelity digital humans from sparse monocular data. We extract garment meshes from monocular data to model clothing deformations effectively, and attach illumination-aware Gaussians to the mesh surface, enabling high-fidelity rendering and capturing pose-dependent lighting. This representation efficiently learns high-resolution and dynamic information from monocular data, enabling the creation of detailed avatars. At the rendering level, real-time performance is critical for animating high-fidelity avatars in AR&#x2F;VR, social gaming, and on-device creation. Our GPU-driven rendering pipeline delivers 120 FPS on mobile devices and 90 FPS on standalone VR devices at 2K resolution, over $2.7\times$ faster than representative mobile-engine baselines. Experiments show that HRM$^2$Avatar delivers superior visual realism and real-time interactivity, outperforming state-of-the-art monocular methods. </p>
<blockquote>
<p>我们推出了HRM$^2$Avatar框架，该框架可以通过单目手机扫描创建高保真虚拟形象，并可在移动设备上实时呈现和动画。单目捕捉与智能手机相结合，为专业级的多相机拍摄提供了低成本替代方案，使虚拟形象数字化对非专业用户也变得触手可及。从单目视频序列重建高保真虚拟形象，由于视觉和几何数据的局限性，这带来了挑战。为了克服这些局限，在数据层面，我们的方法利用智能手机捕获的两种数据：静态姿势序列用于纹理重建，动态运动序列用于学习姿势相关的变形和光照变化。在表示层面，我们采用了一种轻便而富有表现力的表示方法，从稀疏的单目数据中重建高保真数字人类。我们从单目数据中提取服装网格，以有效地对服装变形进行建模，并将光照感知高斯附加到网格表面，从而实现高保真渲染和捕捉姿势相关的光照。这种表示法可以有效地从单目数据中学习高分辨率和动态信息，从而创建详细的虚拟形象。在渲染层面，对于在AR&#x2F;VR、社交游戏和设备端创建中动画化高保真虚拟形象而言，实时性能至关重要。我们的GPU驱动渲染管道在移动设备上实现了120 FPS的帧率，在独立VR设备上的帧率为90 FPS（在2K分辨率下），比典型的移动引擎基线快$2.7\times$以上。实验表明，HRM$^2$Avatar在视觉真实感和实时交互方面均优于最先进单目方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13587v1">PDF</a> SIGGRAPH Asia 2025, Project Page:   <a target="_blank" rel="noopener" href="https://acennr-engine.github.io/HRM2Avatar">https://acennr-engine.github.io/HRM2Avatar</a></p>
<p><strong>摘要</strong></p>
<p>HRM$^2$Avatar框架实现通过单目手机扫描创建高保真虚拟人，可在移动设备上实时渲染和动画。单目捕捉智能手机提供了一种低成本选择，替代了工作室级别的多相机装置，使虚拟人数字化对非专业用户更加便捷。从单视角视频序列重建高保真虚拟人面临视觉和几何数据有限的挑战。为应对这些局限，我们的方法利用智能手机捕获的两种类型的数据：静态姿势序列用于纹理重建和动态运动序列用于学习姿势相关的变形和光照变化。在表示层面，我们采用简洁而富有表现力的表示方法，从稀疏的单目数据中重建高保真数字人类。我们从单目数据中提取服装网格以有效地模拟服装变形，并将照明感知高斯附加到网格表面，以实现高保真渲染和捕捉姿势相关的照明。这种表示方法能够高效地从单目数据中学习高分辨率和动态信息，从而创建详细的虚拟人。在渲染层面，对于在AR&#x2F;VR、社交游戏和实时创作中进行高保真虚拟人动画来说，实时性能至关重要。我们的GPU驱动的渲染管道在移动设备上实现了120帧&#x2F;秒的帧率，在独立VR设备上的2K分辨率下实现了90帧&#x2F;秒的帧率，比代表性的移动引擎基线提高了$2.7\times$的速度。实验表明，HRM$^2$Avatar在视觉真实感和实时交互性方面均优于现有技术最先进的单目方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>HRM$^2$Avatar框架允许通过低成本的单目手机扫描创建高保真虚拟人。</li>
<li>利用智能手机捕获的静态和动态数据，以应对单目视频序列中视觉和几何数据的局限性。</li>
<li>高效的GPU驱动渲染技术确保了高质量的实时渲染和动画性能。</li>
<li>通过提取服装网格和添加照明感知高斯来增强虚拟人的逼真度。</li>
<li>HRM$^2$Avatar实现了超越现有技术的视觉真实感和实时交互性能。</li>
<li>该方法在移动设备和独立VR设备上都表现出优异的性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13587">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.13587v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.13587v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.13587v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MVP4D-Multi-View-Portrait-Video-Diffusion-for-Animatable-4D-Avatars"><a href="#MVP4D-Multi-View-Portrait-Video-Diffusion-for-Animatable-4D-Avatars" class="headerlink" title="MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars"></a>MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars</h2><p><strong>Authors:Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell</strong></p>
<p>Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods. </p>
<blockquote>
<p>数字人类化身旨在模拟虚拟环境中人类的动态外观，为游戏、电影、虚拟现实等领域提供沉浸式体验。然而，创建和驱动逼真人类化身的传统过程既昂贵又耗时，需要大型相机捕捉设备和专业3D艺术家的大量手动工作。随着强大的图像和视频生成模型的出现，最近的方法能够从单个随意捕获的目标对象参考图像自动渲染逼真的动画化身。虽然这些技术大大降低了创建化身的障碍并提供了引人注目的逼真效果，但它们缺乏多视图信息或明确的3D表示所提供的约束。因此，当从与参考图像偏离较大的视角进行渲染时，图像质量和逼真度会降低。在这里，我们建立了一个基于单个参考图像和目标表情生成数字人类多角度视频的视频模型。我们的模型MVP4D基于最先进的预训练视频扩散模型，可以同时从目标对象周围最多360度的视角生成数百帧。我们展示了如何将该模型的输出提炼成可以在实时渲染的4D化身。与以前的方法相比，我们的方法显著提高了生成的化身在逼真度、时间一致性和3D一致性方面的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12785v1">PDF</a> 18 pages, 12 figures</p>
<p><strong>Summary</strong><br>数字人像是模拟虚拟环境中人类动态外观的技术。传统创建方式成本高昂且耗时，依赖大型相机捕捉设备和专业3D艺术家。最新的方法可通过单个随意拍摄的参考图像自动渲染逼真动画人像，降低了门槛。但缺乏多角度信息或明确的3D表现，导致从与参考图像角度偏差较大的视角渲染时，图像质量和逼真度下降。本文建立了一个基于单参考图像和目标表情的动画人视频生成模型MVP4D，可从多角度生成数百帧视频。与以往方法相比，显著提高了生成人偶的逼真度、时间连贯性和3D一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数字人像是模拟虚拟环境中的人类动态外观的技术，已广泛应用于游戏、电影和虚拟现实等领域。</li>
<li>传统创建高清数字人偶的方式需要昂贵的设备和大量的专业3D艺术家参与，耗费时间和成本。</li>
<li>最新方法能通过单一参考图像自动渲染逼真动画人像，大大降低了创建门槛。</li>
<li>当前技术缺乏多角度信息或明确的3D表现，导致在偏离参考视角时图像质量下降。</li>
<li>MVP4D模型解决了上述问题，能够基于单参考图像和目标表情生成多角度的视频。</li>
<li>MVP4D模型生成的数字人偶具有高度的逼真度、时间连贯性和3D一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.12785v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.12785v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.12785v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.12785v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.12785v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.12785v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-3D-Gaussian-Human-Avatar-Compression-A-Prior-Guided-Framework"><a href="#Towards-Efficient-3D-Gaussian-Human-Avatar-Compression-A-Prior-Guided-Framework" class="headerlink" title="Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided   Framework"></a>Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided   Framework</h2><p><strong>Authors:Shanzhi Yin, Bolin Chen, Xinju Wu, Ru-Ling Liao, Jie Chen, Shiqi Wang, Yan Ye</strong></p>
<p>This paper proposes an efficient 3D avatar coding framework that leverages compact human priors and canonical-to-target transformation to enable high-quality 3D human avatar video compression at ultra-low bit rates. The framework begins by training a canonical Gaussian avatar using articulated splatting in a network-free manner, which serves as the foundation for avatar appearance modeling. Simultaneously, a human-prior template is employed to capture temporal body movements through compact parametric representations. This decomposition of appearance and temporal evolution minimizes redundancy, enabling efficient compression: the canonical avatar is shared across the sequence, requiring compression only once, while the temporal parameters, consisting of just 94 parameters per frame, are transmitted with minimal bit-rate. For each frame, the target human avatar is generated by deforming canonical avatar via Linear Blend Skinning transformation, facilitating temporal coherent video reconstruction and novel view synthesis. Experimental results demonstrate that the proposed method significantly outperforms conventional 2D&#x2F;3D codecs and existing learnable dynamic 3D Gaussian splatting compression method in terms of rate-distortion performance on mainstream multi-view human video datasets, paving the way for seamless immersive multimedia experiences in meta-verse applications. </p>
<blockquote>
<p>本文提出了一种高效的3D化身编码框架，该框架利用紧凑的人类先验知识和标准到目标的转换，以实现在超低比特率下的高质量3D人类化身视频压缩。该框架首先以无网络的方式使用关节贴图训练一个标准高斯化身，作为化身外观建模的基础。同时，利用人类先验模板通过紧凑的参数表示来捕获身体运动的时序。这种外观和时序演变的分解最小化了冗余，实现了有效的压缩：标准化身在整个序列中共享，只需压缩一次，而每帧仅包含94个参数的时序参数以极低的比特率传输。对于每一帧，目标人类化身是通过线性混合蒙皮变换对标准化身进行变形而生成的，这有助于实现时序一致的视频重建和新视角的合成。实验结果表明，该方法在主流的多视角人类视频数据集上，在速率失真性能上显著优于传统的2D&#x2F;3D编解码器和现有的可学习动态3D高斯贴图压缩方法，为元宇宙应用中的无缝沉浸式多媒体体验铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10492v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>高效的三维阿凡达编码框架，利用紧凑的人类先验知识和规范到目标转换，实现超低比特率下的高质量三维阿凡达视频压缩。该框架以无网络方式使用关节拼接技术训练规范高斯阿凡达，作为阿凡达外观建模的基础。同时，利用人体先验模板以紧凑的参数表示捕获身体运动的时序变化。外观和时序演变的分解最小化了冗余，实现了高效压缩：规范阿凡达可在整个序列中共享，只需压缩一次，而每帧仅包含94个参数的临时参数以极低的比特率传输。通过线性混合蒙皮变换使规范阿凡达变形为目标阿凡达，可实现时序连贯的视频重建和新颖视图合成。实验结果表明，该方法在主流的多视角人类视频数据集上，相对于传统的二维&#x2F;三维编解码器和现有的可学习动态三维高斯拼接压缩方法，在速率失真性能上有着显著的优势，为元宇宙应用中的无缝沉浸式多媒体体验铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了高效的三维阿凡达编码框架，实现了超低比特率下的高质量视频压缩。</li>
<li>利用规范高斯阿凡达作为外观建模基础，通过无网络方式的关节拼接技术实现训练。</li>
<li>采用人体先验模板捕获身体运动的时序变化，以紧凑的参数表示提高效率。</li>
<li>分解外观和时序演变，最小化冗余，规范阿凡达可共享并只需一次压缩。</li>
<li>每帧的临时参数仅包含94个参数，以极低的比特率传输。</li>
<li>通过线性混合蒙皮变换实现目标阿凡达的生成，支持时序连贯的视频重建和新颖视图合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.10492v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.10492v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.10492v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2510.10492v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PanoLAM-Large-Avatar-Model-for-Gaussian-Full-Head-Synthesis-from-One-shot-Unposed-Image"><a href="#PanoLAM-Large-Avatar-Model-for-Gaussian-Full-Head-Synthesis-from-One-shot-Unposed-Image" class="headerlink" title="PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from   One-shot Unposed Image"></a>PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from   One-shot Unposed Image</h2><p><strong>Authors:Peng Li, Yisheng He, Yingdong Hu, Yuan Dong, Weihao Yuan, Yuan Liu, Siyu Zhu, Gang Cheng, Zilong Dong, Yike Guo</strong></p>
<p>We present a feed-forward framework for Gaussian full-head synthesis from a single unposed image. Unlike previous work that relies on time-consuming GAN inversion and test-time optimization, our framework can reconstruct the Gaussian full-head model given a single unposed image in a single forward pass. This enables fast reconstruction and rendering during inference. To mitigate the lack of large-scale 3D head assets, we propose a large-scale synthetic dataset from trained 3D GANs and train our framework using only synthetic data. For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian head generation pipeline, where sparse points from the FLAME model interact with the image features by transformer blocks for feature extraction and coarse shape reconstruction, which are then densified for high-fidelity reconstruction. To fully leverage the prior knowledge residing in pretrained 3D GANs for effective reconstruction, we propose a dual-branch framework that effectively aggregates the structured spherical triplane feature and unstructured point-based features for more effective Gaussian head reconstruction. Experimental results show the effectiveness of our framework towards existing work. Project page at: <a target="_blank" rel="noopener" href="https://panolam.github.io/">https://panolam.github.io/</a>. </p>
<blockquote>
<p>我们提出了一种基于单一未姿态图像的高斯全头合成前馈框架。不同于之前依赖于耗时性的GAN反演和测试时优化的工作，我们的框架能够在单次前向传递中根据单一的未姿态图像重建高斯全头模型。这可以在推理过程中实现快速重建和渲染。为了缓解大规模三维头部资产缺乏的问题，我们从训练好的三维GAN中提出了大规模合成数据集，并且只使用合成数据来训练我们的框架。为了进行有效的精细高品质生成，我们引入了从粗到细的高斯头部生成管道，其中来自FLAME模型的稀疏点与图像特征通过变换器块进行特征提取和粗略形状重建，然后进行密集化以实现高保真重建。为了充分利用预训练三维GAN中的先验知识进行有效重建，我们提出了一个双分支框架，该框架可以有效地聚合结构化球面三角平面特征和非结构化点基特征，以进行更有效的高斯头部重建。实验结果证明我们的框架相比现有工作更有效。项目页面地址为：<a target="_blank" rel="noopener" href="https://panolam.github.io/">https://panolam.github.io/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07552v2">PDF</a> </p>
<p><strong>Summary</strong><br>基于单张未摆姿势的图像，我们提出了一种前馈框架，用于高斯全头合成。该框架能够在单个前向传递中从单个未摆姿势的图像重建高斯全头模型，从而实现了快速的重建和渲染。为了解决缺乏大规模3D头部资产的问题，我们提出了一个基于训练好的3D GANs的大规模合成数据集，并使用仅合成数据进行框架训练。为了进行高效的高保真生成，我们引入了从粗到细的高斯头部生成管道，其中FLAME模型的稀疏点与图像特征通过变压器块进行特征提取和粗略形状重建，然后进行密集化以实现高保真重建。为了充分利用预训练的3D GANs中的先验知识进行有效的重建，我们提出了一个双分支框架，该框架有效地聚合了结构化的球面triplane特征和无结构的点基特征，以实现更有效的高斯头部重建。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一个前馈框架，能够从单张未摆姿势的图像进行高斯全头合成，实现快速重建和渲染。</li>
<li>利用训练好的3D GANs创建大规模合成数据集以解决缺乏真实3D头部资产的问题。</li>
<li>引入从粗到细的高斯头部生成管道，通过特征提取和粗略形状重建，实现高保真生成。</li>
<li>利用FLAME模型的稀疏点与图像特征的交互进行高效特征提取和形状重建。</li>
<li>提出双分支框架，聚合结构化的球面triplane特征和无结构的点基特征，实现更有效的高斯头部重建。</li>
<li>实验结果证明该框架相较于现有工作更加有效。</li>
<li>项目页面提供了更多详细信息：<a target="_blank" rel="noopener" href="https://panolam.github.io/">https://panolam.github.io/。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07552">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2509.07552v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2509.07552v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2509.07552v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2509.07552v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Generative-Head-Mounted-Camera-Captures-for-Photorealistic-Avatars"><a href="#Generative-Head-Mounted-Camera-Captures-for-Photorealistic-Avatars" class="headerlink" title="Generative Head-Mounted Camera Captures for Photorealistic Avatars"></a>Generative Head-Mounted Camera Captures for Photorealistic Avatars</h2><p><strong>Authors:Shaojie Bai, Seunghyeon Seo, Yida Wang, Chenghui Li, Owen Wang, Te-Li Wang, Tianyang Ma, Jason Saragih, Shih-En Wei, Nojun Kwak, Hyung Jun Kim</strong></p>
<p>Enabling photorealistic avatar animations in virtual and augmented reality (VR&#x2F;AR) has been challenging because of the difficulty of obtaining ground truth state of faces. It is physically impossible to obtain synchronized images from head-mounted cameras (HMC) sensing input, which has partial observations in infrared (IR), and an array of outside-in dome cameras, which have full observations that match avatars’ appearance. Prior works relying on analysis-by-synthesis methods could generate accurate ground truth, but suffer from imperfect disentanglement between expression and style in their personalized training. The reliance of extensive paired captures (HMC and dome) for the same subject makes it operationally expensive to collect large-scale datasets, which cannot be reused for different HMC viewpoints and lighting. In this work, we propose a novel generative approach, Generative HMC (GenHMC), that leverages large unpaired HMC captures, which are much easier to collect, to directly generate high-quality synthetic HMC images given any conditioning avatar state from dome captures. We show that our method is able to properly disentangle the input conditioning signal that specifies facial expression and viewpoint, from facial appearance, leading to more accurate ground truth. Furthermore, our method can generalize to unseen identities, removing the reliance on the paired captures. We demonstrate these breakthroughs by both evaluating synthetic HMC images and universal face encoders trained from these new HMC-avatar correspondences, which achieve better data efficiency and state-of-the-art accuracy. </p>
<blockquote>
<p>在虚拟和增强现实（VR&#x2F;AR）中实现逼真的人物动画一直具有挑战性，因为获取面部真实状态的信息很困难。头显相机（HMC）捕捉的输入同步图像与红外部分观测数据以及外部全景相机阵列的全面观测数据相匹配，以生成逼真的面部动画，但直接从这两者获得同步图像在物理上是不可能的。先前的工作通过合成分析方法可以生成准确的真实数据，但在个性化训练中，表情和风格的分离并不完美。对同一主题的大量配对捕捉（HMC和全景相机）使大规模数据集的收集变得操作昂贵，并且不能在不同的HMC观点和照明下重复使用。在这项工作中，我们提出了一种新型生成方法，即生成式HMC（GenHMC），它利用更容易收集的配对HMC捕获的大量非配对数据，根据全景捕获的任何条件虚拟人物状态直接生成高质量的合成HMC图像。我们证明，我们的方法能够适当地分离输入条件信号，该信号指定面部表情和视点，与面部外观无关，从而得到更准确的真实数据。此外，我们的方法可以推广到未见过的身份，不再依赖于配对捕获。我们通过评估合成HMC图像和从这些新的HMC-虚拟人物对应关系训练的通用面部编码器来证明这些突破，这些编码器实现了更好的数据效率和最先进的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05620v2">PDF</a> SIGGRAPH Asia 2025 (ACM Transactions on Graphics (TOG)). Project   page: <a target="_blank" rel="noopener" href="https://shawn615.github.io/genhmc/">https://shawn615.github.io/genhmc/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型生成方法——生成式HMC（GenHMC），该方法利用大量未配对的HMC捕捉数据，直接生成高质量合成HMC图像，给定任何条件化avatar状态。此方法能够准确地将面部表情和视角的输入条件信号与面部外观分离，生成更准确的地面真实数据。此外，该方法可推广至未见过的身份，无需配对捕捉。通过评估合成HMC图像和基于新HMC-avatar对应关系的通用面部编码器，展现了该方法在数据效率和准确性方面的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式HMC（GenHMC）方法利用大量未配对的HMC捕捉数据，简化大规模数据集收集过程。</li>
<li>GenHMC可以直接生成高质量合成HMC图像，基于任意条件化avatar状态。</li>
<li>GenHMC能够准确分离面部表情和视角的条件信号与面部外观，提高地面真实数据的准确性。</li>
<li>GenHMC方法可推广至未见过的身份，无需配对捕捉，增强了方法的实用性。</li>
<li>合成HMC图像的评价证明了GenHMC方法的有效性。</li>
<li>基于新HMC-avatar对应关系的通用面部编码器展示了在数据效率和准确性方面的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05620">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2507.05620v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2507.05620v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2507.05620v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2507.05620v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AsynFusion-Towards-Asynchronous-Latent-Consistency-Models-for-Decoupled-Whole-Body-Audio-Driven-Avatars"><a href="#AsynFusion-Towards-Asynchronous-Latent-Consistency-Models-for-Decoupled-Whole-Body-Audio-Driven-Avatars" class="headerlink" title="AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled   Whole-Body Audio-Driven Avatars"></a>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled   Whole-Body Audio-Driven Avatars</h2><p><strong>Authors:Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, Zhaoxin Fan, Wenjun Wu, Xuelong Li</strong></p>
<p>Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>全身音频驱动的角色姿态和表情生成是创建逼真数字人和增强交互式虚拟代理人能力的一项关键任务，在虚拟现实、数字娱乐和远程通信等领域有广泛应用。现有方法通常独立生成音频驱动的面部表情和动作，这引入了一个重要的局限性：面部表情和动作元素之间缺乏无缝协调，导致动画效果不够自然和连贯。为了解决这一局限性，我们提出了AsynFusion，这是一个利用扩散变压器实现和谐表情和动作合成的新框架。该方法建立在双分支DiT架构之上，能够实现面部表情和动作的并行生成。在模型中，我们引入了一个合作同步模块，以促进两种模式之间的双向特征交互，以及一种异步LCM采样策略，以减少计算开销同时保持高质量输出。大量实验表明，AsynFusion在生成实时同步全身动画方面达到了最新技术水平，在定量和定性评估中均优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15058v2">PDF</a> 15pages, conference</p>
<p><strong>Summary</strong><br>全息音频驱动的全身动作捕捉技术在创建逼真数字人类和增强交互式虚拟代理能力方面扮演重要角色，广泛应用于虚拟现实、数字娱乐和远程通信等领域。现有方法常常独立生成音频驱动的面部表情和动作，导致面部表情和动作之间缺乏无缝协调，动画效果不自然连贯。我们提出AsynFusion框架，利用扩散变压器实现和谐的表情和动作合成。该方法基于双分支DiT架构，实现面部表情和动作的并行生成。模型内引入协同同步模块促进两种模态之间的双向特征交互，以及异步LCM采样策略降低计算开销同时保持高质量输出。实验证明，AsynFusion在生成实时同步全身动画方面达到最佳性能，在定量和定性评估上均超越现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全息音频驱动的全身动作捕捉技术对于创建数字人类和增强虚拟代理能力至关重要，广泛应用于多个领域。</li>
<li>现有方法独立生成音频驱动的面部表情和动作，导致两者缺乏协调。</li>
<li>AsynFusion框架利用扩散变压器实现和谐的表情和动作合成。</li>
<li>AsynFusion基于双分支DiT架构，实现面部表情和动作的并行生成。</li>
<li>协同同步模块促进面部表情和动作两种模态之间的双向特征交互。</li>
<li>异步LCM采样策略在降低计算开销的同时保持高质量输出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15058">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2505.15058v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2505.15058v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_元宇宙_虚拟人/2505.15058v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                    <span class="chip bg-color">元宇宙/虚拟人</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/3DGS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_3DGS/2510.10492v1/page_5_0.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-10-18  Terra Explorable Native 3D World Model with Point Latents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/GAN/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_GAN/2510.14230v1/page_2_0.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-10-18  A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
