<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Shot2Tactic-Caption Multi-Scale Captioning of Badminton Videos for   Tactical Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-06e62343c630cc627d1133022c4bb17a')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Shot2Tactic-Caption-Multi-Scale-Captioning-of-Badminton-Videos-for-Tactical-Understanding"><a href="#Shot2Tactic-Caption-Multi-Scale-Captioning-of-Badminton-Videos-for-Tactical-Understanding" class="headerlink" title="Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for   Tactical Understanding"></a>Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for   Tactical Understanding</h2><p><strong>Authors:Ning Ding, Keisuke Fujii, Toru Tamaki</strong></p>
<p>Tactical understanding in badminton involves interpreting not only individual actions but also how tactics are dynamically executed over time. In this paper, we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and temporal multi-scale video captioning in badminton, capable of generating shot-level captions that describe individual actions and tactic-level captions that capture how these actions unfold over time within a tactical execution. We also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning dataset containing 5,494 shot captions and 544 tactic captions. Shot2Tactic-Caption adopts a dual-branch design, with both branches including a visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based decoder to generate shot and tactic captions. To support tactic captioning, we additionally introduce a Tactic Unit Detector that identifies valid tactic units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic captioning, we further incorporate a shot-wise prompt-guided mechanism, where the predicted tactic type and state are embedded as prompts and injected into the decoder via cross-attention. The shot-wise prompt-guided mechanism enables our system not only to describe successfully executed tactics but also to capture tactical executions that are temporarily interrupted and later resumed. Experimental results demonstrate the effectiveness of our framework in generating both shot and tactic captions. Ablation studies show that the ResNet50-based spatio-temporal encoder outperforms other variants, and that shot-wise prompt structuring leads to more coherent and accurate tactic captioning. </p>
<blockquote>
<p>ç¾½æ¯›çƒæˆ˜æœ¯ç†è§£ä¸ä»…æ¶‰åŠå¯¹å•ä¸ªåŠ¨ä½œçš„è§£è¯»ï¼Œè¿˜æ¶‰åŠå¦‚ä½•åŠ¨æ€æ‰§è¡Œæˆ˜æœ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶<strong>Shot2Tactic-Caption</strong>ï¼Œç”¨äºç¾½æ¯›çƒè¯­ä¹‰å’Œæ—¶é—´å¤šå°ºåº¦è§†é¢‘æè¿°ç”Ÿæˆï¼Œèƒ½å¤Ÿç”Ÿæˆæè¿°å•ä¸ªåŠ¨ä½œçš„é•œå¤´çº§æè¿°å’Œæ•æ‰æˆ˜æœ¯æ‰§è¡Œè¿‡ç¨‹ä¸­è¿™äº›åŠ¨ä½œéšæ—¶é—´å±•å¼€çš„æˆ˜æœ¯çº§æè¿°ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Shot2Tactic-Captionæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«5494ä¸ªé•œå¤´æè¿°å’Œ544ä¸ªæˆ˜æœ¯æè¿°çš„ç¾½æ¯›çƒæè¿°æ•°æ®é›†ã€‚Shot2Tactic-Captioné‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡ï¼Œä¸¤ä¸ªåˆ†æ”¯éƒ½åŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€æ—¶ç©ºTransformerç¼–ç å™¨å’ŒåŸºäºTransformerçš„è§£ç å™¨ï¼Œä»¥ç”Ÿæˆé•œå¤´å’Œæˆ˜æœ¯æè¿°ã€‚ä¸ºäº†æ”¯æŒæˆ˜æœ¯æè¿°ç”Ÿæˆï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæˆ˜æœ¯å•å…ƒæ£€æµ‹å™¨ï¼Œç”¨äºè¯†åˆ«æœ‰æ•ˆçš„æˆ˜æœ¯å•å…ƒã€æˆ˜æœ¯ç±»å‹å’Œæˆ˜æœ¯çŠ¶æ€ï¼ˆä¾‹å¦‚ä¸­æ–­ã€æ¢å¤ï¼‰ã€‚å¯¹äºæˆ˜æœ¯æè¿°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨äº†ä¸€ç§åŸºäºé•œå¤´çš„æç¤ºå¼•å¯¼æœºåˆ¶ï¼Œå°†é¢„æµ‹çš„æˆ˜æœ¯ç±»å‹å’ŒçŠ¶æ€åµŒå…¥ä¸ºæç¤ºï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥è§£ç å™¨ã€‚åŸºäºé•œå¤´çš„æç¤ºå¼•å¯¼æœºåˆ¶ä½¿æˆ‘ä»¬çš„ç³»ç»Ÿä¸ä»…èƒ½å¤Ÿæè¿°æˆåŠŸæ‰§è¡Œçš„æˆ˜æœ¯ï¼Œè¿˜èƒ½å¤Ÿæ•æ‰æš‚æ—¶ä¸­æ–­å¹¶éšåæ¢å¤çš„æˆ˜æœ¯æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ç”Ÿæˆé•œå¤´å’Œæˆ˜æœ¯æè¿°æ–¹é¢éƒ½éå¸¸æœ‰æ•ˆã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºResNet50çš„æ—¶ç©ºç¼–ç å™¨ä¼˜äºå…¶ä»–å˜ä½“ï¼ŒåŸºäºé•œå¤´çš„æç¤ºç»“æ„å¯¼è‡´æ›´è¿è´¯å’Œå‡†ç¡®çš„æˆ˜æœ¯æè¿°ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14617v1">PDF</a> 9 pages, 3 figures. Accepted to ACM MMSports 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç¾½æ¯›çƒè¿åŠ¨ä¸­ï¼Œæˆ˜æœ¯ç†è§£ä¸ä»…åŒ…æ‹¬è§£è¯»ä¸ªåˆ«åŠ¨ä½œï¼Œè¿˜åŒ…æ‹¬ç†è§£æˆ˜æœ¯éšæ—¶é—´å¦‚ä½•åŠ¨æ€æ‰§è¡Œã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶Shot2Tactic-Captionï¼Œè¯¥æ¡†æ¶å¯ä»¥ç”Ÿæˆæè¿°ä¸ªåˆ«åŠ¨ä½œçš„shot-levelå­—å¹•ä»¥åŠæ•æ‰è¿™äº›åŠ¨ä½œåœ¨æˆ˜æœ¯æ‰§è¡Œä¸­éšæ—¶é—´å±•å¼€çš„tactic-levelå­—å¹•ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†Shot2Tactic-Captionæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«ç¾½æ¯›çƒè§†é¢‘å­—å¹•çš„æ•°æ®é›†ï¼ŒåŒ…å«5494ä¸ªshotå­—å¹•å’Œ544ä¸ªæˆ˜æœ¯å­—å¹•ã€‚Shot2Tactic-Captioné‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡ï¼ŒåŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€æ—¶ç©ºTransformerç¼–ç å™¨å’ŒåŸºäºTransformerçš„è§£ç å™¨æ¥ç”Ÿæˆæˆ˜æœ¯å­—å¹•ã€‚ä¸ºäº†æ”¯æŒæˆ˜æœ¯å­—å¹•ç”Ÿæˆï¼Œå¼•å…¥äº†æˆ˜æœ¯å•å…ƒæ£€æµ‹å™¨ï¼Œç”¨äºè¯†åˆ«æœ‰æ•ˆçš„æˆ˜æœ¯å•å…ƒã€æˆ˜æœ¯ç±»å‹å’Œæˆ˜æœ¯çŠ¶æ€ï¼ˆå¦‚ä¸­æ–­ã€æ¢å¤ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆæˆ˜æœ¯å­—å¹•æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºResNet50çš„æ—¶ç©ºç¼–ç å™¨ä¼˜äºå…¶ä»–å˜ä½“ï¼Œè€ŒåŸºäºæç¤ºç»“æ„çš„ç­–ç•¥æœ‰åŠ©äºæ›´è¿è´¯å’Œå‡†ç¡®çš„æˆ˜æœ¯å­—å¹•ç”Ÿæˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Shot2Tactic-Captionæ¡†æ¶å¯ä»¥ç”Ÿæˆæè¿°ç¾½æ¯›çƒæ¯”èµ›ä¸­ä¸ªåˆ«åŠ¨ä½œçš„shot-levelå­—å¹•ä»¥åŠæˆ˜æœ¯æ‰§è¡Œçš„tactic-levelå­—å¹•ã€‚</li>
<li>å¼•å…¥Shot2Tactic-Captionæ•°æ®é›†ï¼ŒåŒ…å«ç¾½æ¯›çƒè§†é¢‘å­—å¹•ã€‚</li>
<li>Shot2Tactic-Captioné‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡æ¥å¤„ç†è§†é¢‘å†…å®¹ã€‚</li>
<li>å¼•å…¥äº†æˆ˜æœ¯å•å…ƒæ£€æµ‹å™¨ä»¥æ”¯æŒæˆ˜æœ¯å­—å¹•çš„ç”Ÿæˆã€‚</li>
<li>æˆ˜æœ¯å­—å¹•ç”Ÿæˆä¸­èå…¥äº†åŸºäºæç¤ºç»“æ„çš„ç­–ç•¥ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66b61f4005e5a0f30b389908f01b20f7" align="middle">
<img src="https://picx.zhimg.com/v2-cc851b3970255d8d08e5bb534b6a9895" align="middle">
<img src="https://picx.zhimg.com/v2-093453b3abf0d04d3af545645e25b7b2" align="middle">
<img src="https://picx.zhimg.com/v2-e7c50bc6dc3b5cc598e41de27e6b8fdd" align="middle">
<img src="https://picx.zhimg.com/v2-1f72d2b80af655e3e12394fab5f0109a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vgent-Graph-based-Retrieval-Reasoning-Augmented-Generation-For-Long-Video-Understanding"><a href="#Vgent-Graph-based-Retrieval-Reasoning-Augmented-Generation-For-Long-Video-Understanding" class="headerlink" title="Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long   Video Understanding"></a>Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long   Video Understanding</h2><p><strong>Authors:Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny</strong></p>
<p>Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel graph-based retrieval-reasoning-augmented generation framework to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of $3.0%\sim 5.4%$ over base models on MLVU, and outperformed state-of-the-art video RAG methods by $8.6%$. Our code is publicly available at <a target="_blank" rel="noopener" href="https://xiaoqian-shen.github.io/Vgent">https://xiaoqian-shen.github.io/Vgent</a>. </p>
<blockquote>
<p>ç†è§£å’Œæ¨ç†é•¿è§†é¢‘å¯¹å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºå¤„ç†è¶…å‡ºä¸Šä¸‹æ–‡çª—å£çš„å¯†é›†è§†é¢‘ä»¤ç‰Œå’Œä¿ç•™é•¿æœŸåºåˆ—ä¿¡æ¯çš„éš¾åº¦ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é•¿ä¸Šä¸‹æ–‡æ–¹é¢å·²æ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå°†RAGåº”ç”¨äºé•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´ä¾èµ–å…³ç³»ä¸­æ–­å’ŒåŒ…å«æ— å…³ä¿¡æ¯ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å‡†ç¡®æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Vgentï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºå›¾çš„æ£€ç´¢æ¨ç†å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LVLMså¯¹é•¿è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆiï¼‰å®ƒé€šè¿‡ç»“æ„å›¾è¡¨ç¤ºè§†é¢‘ï¼Œä¿ç•™è·¨è§†é¢‘å‰ªè¾‘çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥æé«˜æ£€ç´¢æ•ˆæœã€‚ï¼ˆiiï¼‰å®ƒå¼•å…¥äº†ä¸€ä¸ªä¸­é—´æ¨ç†æ­¥éª¤æ¥ç¼“è§£LVLMsçš„æ¨ç†å±€é™æ€§ï¼Œè¯¥æ­¥éª¤åˆ©ç”¨ç»“æ„åŒ–éªŒè¯æ¥å‡å°‘æ£€ç´¢å™ªå£°ï¼Œä¿ƒè¿›è·¨å‰ªè¾‘çš„ç›¸å…³ä¿¡æ¯çš„æ˜¾å¼èšåˆï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå¯¹å¼€æºLVLMså…¨é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MLVUä¸Šçš„æ€»ä½“æ€§èƒ½æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†$ 3.0%\sim 5.4%$ï¼Œå¹¶ä¸”ä¼˜äºæœ€æ–°çš„è§†é¢‘RAGæ–¹æ³•$ 8.6%$ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://xiaoqian-shen.github.io/Vgent%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://xiaoqian-shen.github.io/Vgentå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14032v1">PDF</a> NeurIPS 2025 (Spotlight). Webpage at   <a target="_blank" rel="noopener" href="https://xiaoqian-shen.github.io/Vgent">https://xiaoqian-shen.github.io/Vgent</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤„ç†é•¿è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶Vgentï¼Œç”¨äºå¢å¼ºå¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚Vgenté€šè¿‡ç»“æ„åŒ–å›¾è¡¨ç¤ºè§†é¢‘ï¼Œå¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤æ¥å‡å°‘LVLMsçš„æ¨ç†é™åˆ¶ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–éªŒè¯å‡å°‘æ£€ç´¢å™ªå£°ï¼Œä¿ƒè¿›è·¨ç‰‡æ®µç›¸å…³ä¿¡æ¯çš„æ˜¾å¼èšåˆï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”ã€‚åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒVgentçš„æ€»ä½“æ€§èƒ½æé«˜äº†3.0%~5.4%ï¼Œå¹¶è¶…è¶Šäº†æœ€æ–°çš„è§†é¢‘RAGæ–¹æ³•ï¼Œæé«˜äº†8.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¤„ç†å¤§é‡è§†é¢‘ä»¤ç‰Œã€ä¿æŒé•¿æœŸåºåˆ—ä¿¡æ¯ç­‰ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬æ–¹é¢å·²æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ï¼Œä½†åº”ç”¨äºé•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä¸­æ–­çš„æ—¶ç©ºä¾èµ–å’ŒåŒ…å«æ— å…³ä¿¡æ¯ã€‚</li>
<li>Vgentæ˜¯ä¸€ä¸ªåŸºäºå›¾çš„æ£€ç´¢æ¨ç†å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LVLMsåœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>Vgentçš„ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šé€šè¿‡ç»“æ„åŒ–å›¾è¡¨ç¤ºè§†é¢‘ï¼Œæ”¹å–„æ£€ç´¢æ•ˆæœï¼›å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œç¼“è§£LVLMsçš„æ¨ç†é™åˆ¶ã€‚</li>
<li>Vgenté€šè¿‡ç»“æ„åŒ–éªŒè¯å‡å°‘æ£€ç´¢å™ªå£°ï¼Œä¿ƒè¿›è·¨ç‰‡æ®µç›¸å…³ä¿¡æ¯çš„æ˜¾å¼èšåˆï¼Œå®ç°æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”ã€‚</li>
<li>åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒVgentçš„æ€»ä½“æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>Vgentçš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://xiaoqian-shen.github.io/Vgent%E3%80%82">https://xiaoqian-shen.github.io/Vgentã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23b6039abf71cc8261509f877d8e025d" align="middle">
<img src="https://picx.zhimg.com/v2-494aff1ca6314e6b9acbc48171a2cb73" align="middle">
<img src="https://picx.zhimg.com/v2-99a6e8ab9a80845e5c6deb41fe3a9aba" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VideoLucy-Deep-Memory-Backtracking-for-Long-Video-Understanding"><a href="#VideoLucy-Deep-Memory-Backtracking-for-Long-Video-Understanding" class="headerlink" title="VideoLucy: Deep Memory Backtracking for Long Video Understanding"></a>VideoLucy: Deep Memory Backtracking for Long Video Understanding</h2><p><strong>Authors:Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao</strong></p>
<p>Recent studies have shown that agent-based systems leveraging large language models (LLMs) for key information retrieval and integration have emerged as a promising approach for long video understanding. However, these systems face two major challenges. First, they typically perform modeling and reasoning on individual frames, struggling to capture the temporal context of consecutive frames. Second, to reduce the cost of dense frame-level captioning, they adopt sparse frame sampling, which risks discarding crucial information. To overcome these limitations, we propose VideoLucy, a deep memory backtracking framework for long video understanding. Inspired by the human recollection process from coarse to fine, VideoLucy employs a hierarchical memory structure with progressive granularity. This structure explicitly defines the detail level and temporal scope of memory at different hierarchical depths. Through an agent-based iterative backtracking mechanism, VideoLucy systematically mines video-wide, question-relevant deep memories until sufficient information is gathered to provide a confident answer. This design enables effective temporal understanding of consecutive frames while preserving critical details. In addition, we introduce EgoMem, a new benchmark for long video understanding. EgoMem is designed to comprehensively evaluate a modelâ€™s ability to understand complex events that unfold over time and capture fine-grained details in extremely long videos. Extensive experiments demonstrate the superiority of VideoLucy. Built on open-source models, VideoLucy significantly outperforms state-of-the-art methods on multiple long video understanding benchmarks, achieving performance even surpassing the latest proprietary models such as GPT-4o. Our code and dataset will be made publicly at <a target="_blank" rel="noopener" href="https://videolucy.github.io/">https://videolucy.github.io</a> </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå…³é”®ä¿¡æ¯æ£€ç´¢å’Œæ•´åˆçš„åŸºäºä»£ç†çš„ç³»ç»Ÿå·²æˆä¸ºé•¿è§†é¢‘ç†è§£çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒä»¬é€šå¸¸å¯¹å•ä¸ªå¸§è¿›è¡Œå»ºæ¨¡å’Œæ¨ç†ï¼Œéš¾ä»¥æ•æ‰è¿ç»­å¸§çš„æ—¶é—´ä¸Šä¸‹æ–‡ã€‚å…¶æ¬¡ï¼Œä¸ºäº†é™ä½å¯†é›†å¸§çº§å­—å¹•çš„æˆæœ¬ï¼Œå®ƒä»¬é‡‡ç”¨ç¨€ç–å¸§é‡‡æ ·ï¼Œè¿™æœ‰å¯èƒ½ä¸¢å¤±å…³é”®ä¿¡æ¯ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VideoLucyï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé•¿è§†é¢‘ç†è§£çš„æ·±åº¦è®°å¿†å›æº¯æ¡†æ¶ã€‚VideoLucyå—åˆ°äººç±»ä»ç²—ç•¥åˆ°ç²¾ç»†çš„å›å¿†è¿‡ç¨‹çš„å¯å‘ï¼Œé‡‡ç”¨åˆ†å±‚è®°å¿†ç»“æ„ï¼Œå…·æœ‰æ¸è¿›çš„ç²’åº¦ã€‚è¿™ç§ç»“æ„åœ¨ä¸åŒå±‚æ¬¡æ·±åº¦æ˜ç¡®å®šä¹‰äº†è®°å¿†çš„ç»†èŠ‚æ°´å¹³å’Œæ—¶é—´èŒƒå›´ã€‚é€šè¿‡åŸºäºä»£ç†çš„è¿­ä»£å›æº¯æœºåˆ¶ï¼ŒVideoLucyç³»ç»Ÿåœ°æŒ–æ˜ä¸è§†é¢‘ç›¸å…³çš„æ·±åº¦è®°å¿†ï¼Œç›´åˆ°æ”¶é›†è¶³å¤Ÿçš„ä¿¡æ¯ä»¥æä¾›ç¡®ä¿¡çš„ç­”æ¡ˆã€‚è¿™ç§è®¾è®¡å®ç°äº†å¯¹è¿ç»­å¸§çš„æœ‰æ•ˆæ—¶é—´ç†è§£ï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†EgoMemï¼Œä¸€ä¸ªç”¨äºé•¿è§†é¢‘ç†è§£çš„æ–°åŸºå‡†æµ‹è¯•ã€‚EgoMemæ—¨åœ¨å…¨é¢è¯„ä¼°æ¨¡å‹ç†è§£éšæ—¶é—´å±•å¼€å¤æ‚äº‹ä»¶çš„èƒ½åŠ›ï¼Œå¹¶åœ¨æé•¿çš„è§†é¢‘ä¸­æ•æ‰ç»†å¾®çš„ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¯æ˜äº†VideoLucyçš„ä¼˜è¶Šæ€§ã€‚å»ºç«‹åœ¨å¼€æºæ¨¡å‹ä¹‹ä¸Šï¼ŒVideoLucyåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†ä¸€äº›æœ€æ–°çš„ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-4oã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://videolucy.github.ioå…¬å¼€./">https://videolucy.github.ioå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12422v1">PDF</a> NeurIPS-2025 Accepted Paper</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„agentç³»ç»Ÿå·²é€æ¸æˆä¸ºé•¿è§†é¢‘ç†è§£çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¼ºä¹å¯¹è¿ç»­å¸§çš„ä¸´æ—¶ä¸Šä¸‹æ–‡æ•æ‰èƒ½åŠ›ï¼ŒäºŒæ˜¯ä¸ºäº†é™ä½å¯†é›†å¸§çº§æ ‡æ³¨çš„æˆæœ¬è€Œé‡‡ç”¨ç¨€ç–å¸§é‡‡æ ·ï¼Œå¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯çš„ä¸¢å¤±ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VideoLucyï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé•¿è§†é¢‘ç†è§£çš„æ·±åº¦è®°å¿†å›æº¯æ¡†æ¶ã€‚å®ƒé‡‡ç”¨åˆ†å±‚è®°å¿†ç»“æ„ï¼Œé€šè¿‡agentçš„è¿­ä»£å›æº¯æœºåˆ¶ï¼Œç³»ç»Ÿåœ°æŒ–æ˜ä¸é—®é¢˜ç›¸å…³çš„æ·±å±‚è®°å¿†ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„ä¸´æ—¶ç†è§£å¹¶ä¿ç•™å…³é”®ç»†èŠ‚ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†EgoMemåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ç†è§£å¤æ‚äº‹ä»¶å’Œæ•æ‰é•¿æ—¶é—´è§†é¢‘ä¸­çš„ç²¾ç»†ç»†èŠ‚æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜VideoLucyæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„agentç³»ç»Ÿä¸ºé•¿è§†é¢‘ç†è§£æä¾›äº†æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ç›®å‰ç³»ç»Ÿé¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯ç¼ºä¹è¿ç»­å¸§çš„ä¸´æ—¶ä¸Šä¸‹æ–‡ç†è§£å’Œç¨€ç–å¸§é‡‡æ ·å¯¼è‡´çš„å…³é”®ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>VideoLucyæ¡†æ¶é€šè¿‡æ·±åº¦è®°å¿†å›æº¯è§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œå®ç°äº†æœ‰æ•ˆçš„ä¸´æ—¶ç†è§£å¹¶ä¿ç•™å…³é”®ç»†èŠ‚ã€‚</li>
<li>VideoLucyé‡‡ç”¨åˆ†å±‚è®°å¿†ç»“æ„å’Œagentè¿­ä»£å›æº¯æœºåˆ¶ï¼ŒæŒ–æ˜ä¸é—®é¢˜ç›¸å…³çš„æ·±å±‚è®°å¿†ã€‚</li>
<li>VideoLucyåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶Šäº†æœ€æ–°çš„ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-4oã€‚</li>
<li>æˆ‘ä»¬å¼•å…¥äº†EgoMemåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç†è§£å¤æ‚äº‹ä»¶å’Œæ•æ‰é•¿æ—¶é—´è§†é¢‘ä¸­çš„ç²¾ç»†ç»†èŠ‚çš„èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a309baf2b6d6824fcc5d8d0111c5df4e" align="middle">
<img src="https://picx.zhimg.com/v2-8377b175bc3cd79f873792cb0f782b3e" align="middle">
<img src="https://picx.zhimg.com/v2-d54dc300c00b8a4b0311cbafaf145df7" align="middle">
<img src="https://picx.zhimg.com/v2-f816f019ae2dc5e387a65c01a7667e2c" align="middle">
<img src="https://picx.zhimg.com/v2-39d6eafdb91bc0e43be4acd9a214d23b" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="State-Space-Prompting-via-Gathering-and-Spreading-Spatio-Temporal-Information-for-Video-Understanding"><a href="#State-Space-Prompting-via-Gathering-and-Spreading-Spatio-Temporal-Information-for-Video-Understanding" class="headerlink" title="State Space Prompting via Gathering and Spreading Spatio-Temporal   Information for Video Understanding"></a>State Space Prompting via Gathering and Spreading Spatio-Temporal   Information for Video Understanding</h2><p><strong>Authors:Jiahuan Zhou, Kai Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua</strong></p>
<p>Recently, pre-trained state space models have shown great potential for video classification, which sequentially compresses visual tokens in videos with linear complexity, thereby improving the processing efficiency of video data while maintaining high performance. To apply powerful pre-trained models to downstream tasks, prompt learning is proposed to achieve efficient downstream task adaptation with only a small number of fine-tuned parameters. However, the sequentially compressed visual prompt tokens fail to capture the spatial and temporal contextual information in the video, thus limiting the effective propagation of spatial information within a video frame and temporal information between frames in the state compression model and the extraction of discriminative information. To tackle the above issue, we proposed a State Space Prompting (SSP) method for video understanding, which combines intra-frame and inter-frame prompts to aggregate and propagate key spatiotemporal information in the video. Specifically, an Intra-Frame Gathering (IFG) module is designed to aggregate spatial key information within each frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread discriminative spatio-temporal information across different frames. By adaptively balancing and compressing key spatio-temporal information within and between frames, our SSP effectively propagates discriminative information in videos in a complementary manner. Extensive experiments on four video benchmark datasets verify that our SSP significantly outperforms existing SOTA methods by 2.76% on average while reducing the overhead of fine-tuning parameters. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé¢„è®­ç»ƒçš„çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨è§†é¢‘åˆ†ç±»æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œè¯¥æ¨¡å‹ä»¥çº¿æ€§å¤æ‚åº¦å¯¹è§†é¢‘ä¸­çš„è§†è§‰æ ‡è®°è¿›è¡Œé¡ºåºå‹ç¼©ï¼Œä»è€Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æé«˜äº†è§†é¢‘æ•°æ®çš„å¤„ç†æ•ˆç‡ã€‚ä¸ºäº†å°†å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œæå‡ºäº†æç¤ºå­¦ä¹ ï¼ˆprompt learningï¼‰ï¼Œé€šè¿‡å¾®è°ƒå°‘é‡å‚æ•°å³å¯å®ç°é«˜æ•ˆçš„ä¸‹æ¸¸ä»»åŠ¡é€‚åº”ã€‚ç„¶è€Œï¼Œé¡ºåºå‹ç¼©çš„è§†è§‰æç¤ºæ ‡è®°æœªèƒ½æ•è·è§†é¢‘ä¸­çš„ç©ºé—´å’Œæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†çŠ¶æ€å‹ç¼©æ¨¡å‹ä¸­è§†é¢‘å¸§å†…çš„ç©ºé—´ä¿¡æ¯å’Œå¸§ä¹‹é—´çš„æ—¶é—´ä¿¡æ¯çš„æœ‰æ•ˆä¼ æ’­ä»¥åŠåˆ¤åˆ«ä¿¡æ¯çš„æå–ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè§†é¢‘ç†è§£çš„çŠ¶æ€ç©ºé—´æç¤ºï¼ˆSSPï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¸§å†…å’Œå¸§é—´æç¤ºï¼Œä»¥èšé›†å’Œä¼ æ’­è§†é¢‘ä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¸§å†…èšé›†ï¼ˆIFGï¼‰æ¨¡å—æ¥èšé›†æ¯ä¸€å¸§å†…çš„å…³é”®ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªå¸§é—´æ‰©æ•£ï¼ˆIFSï¼‰æ¨¡å—ï¼Œä»¥å°†åˆ¤åˆ«æ€§çš„æ—¶ç©ºä¿¡æ¯ä¼ æ’­åˆ°ä¸åŒçš„å¸§ã€‚é€šè¿‡è‡ªé€‚åº”åœ°å¹³è¡¡å’Œå‹ç¼©å¸§å†…å’Œå¸§é—´çš„å…³é”®æ—¶ç©ºä¿¡æ¯ï¼Œæˆ‘ä»¬çš„SSPä»¥äº’è¡¥çš„æ–¹å¼æœ‰æ•ˆåœ°ä¼ æ’­äº†è§†é¢‘ä¸­çš„åˆ¤åˆ«ä¿¡æ¯ã€‚åœ¨å››ä¸ªè§†é¢‘åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SSPå¹³å‡æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•é«˜å‡º2.76%ï¼ŒåŒæ—¶é™ä½äº†å¾®è°ƒå‚æ•°çš„å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12160v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒçŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨è§†é¢‘åˆ†ç±»ä¸­å±•ç°å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡çº¿æ€§å¤æ‚åº¦å‹ç¼©è§†è§‰ä»¤ç‰Œæé«˜è§†é¢‘æ•°æ®å¤„ç†æ•ˆç‡åŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚ä¸ºå°†å¼ºå¤§é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œæå‡ºæç¤ºå­¦ä¹ æ³•ï¼Œä»…éœ€å°‘é‡å‚æ•°å¾®è°ƒå³å¯å®ç°é«˜æ•ˆä¸‹æ¸¸ä»»åŠ¡é€‚åº”ã€‚ç„¶è€Œï¼Œé¡ºåºå‹ç¼©çš„è§†è§‰æç¤ºä»¤ç‰Œæ— æ³•æ•è·è§†é¢‘ä¸­çš„ç©ºé—´å’Œæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé™åˆ¶äº†çŠ¶æ€å‹ç¼©æ¨¡å‹ä¸­ç©ºé—´ä¿¡æ¯åœ¨è§†é¢‘å¸§å†…çš„æœ‰æ•ˆä¼ æ’­ä»¥åŠæ—¶é—´ä¿¡æ¯åœ¨ä¸åŒå¸§ä¹‹é—´çš„ä¼ æ’­ï¼Œä»¥åŠåˆ¤åˆ«ä¿¡æ¯çš„æå–ã€‚ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç”¨äºè§†é¢‘ç†è§£çš„çŠ¶æ€ç©ºé—´æç¤ºï¼ˆSSPï¼‰æ–¹æ³•ï¼Œç»“åˆå¸§å†…å’Œå¸§é—´æç¤ºï¼Œèšé›†å’Œä¼ æ’­è§†é¢‘ä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ã€‚é€šè¿‡è‡ªé€‚åº”å¹³è¡¡å’Œå‹ç¼©å¸§å†…å’Œå¸§é—´çš„å…³é”®æ—¶ç©ºä¿¡æ¯ï¼ŒSSPä»¥äº’è¡¥çš„æ–¹å¼æœ‰æ•ˆä¼ æ’­è§†é¢‘ä¸­çš„åˆ¤åˆ«ä¿¡æ¯ã€‚åœ¨å››ä¸ªè§†é¢‘åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„SSPæ–¹æ³•å¹³å‡æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•é«˜å‡º2.76%ï¼ŒåŒæ—¶é™ä½äº†å¾®è°ƒå‚æ•°çš„å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒçŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨è§†é¢‘åˆ†ç±»ä¸­å…·æœ‰é«˜æ•ˆä¸é«˜æ€§èƒ½çš„ä¼˜åŠ¿ã€‚</li>
<li>é¡ºåºå‹ç¼©è§†è§‰ä»¤ç‰Œå­˜åœ¨æ— æ³•æ•è·è§†é¢‘æ—¶ç©ºä¸Šä¸‹æ–‡ä¿¡æ¯çš„å±€é™ã€‚</li>
<li>çŠ¶æ€ç©ºé—´æç¤ºï¼ˆSSPï¼‰æ–¹æ³•ç»“åˆå¸§å†…å’Œå¸§é—´æç¤ºæ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>SSPé€šè¿‡è‡ªé€‚åº”å¹³è¡¡å’Œå‹ç¼©å…³é”®æ—¶ç©ºä¿¡æ¯ï¼Œæœ‰æ•ˆä¼ æ’­è§†é¢‘ä¸­çš„åˆ¤åˆ«ä¿¡æ¯ã€‚</li>
<li>åœ¨å¤šä¸ªè§†é¢‘æ•°æ®é›†ä¸Šï¼ŒSSPæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ã€‚</li>
<li>SSPæ–¹æ³•åœ¨æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½äº†å¾®è°ƒå‚æ•°çš„å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef4a4827fdd4ef1493a24d416bfe74e2" align="middle">
<img src="https://picx.zhimg.com/v2-252ef57f98f0ef927a19ce5ae943c21d" align="middle">
<img src="https://picx.zhimg.com/v2-b42bd3e7e86a3b473bf3cfeea81ec558" align="middle">
<img src="https://picx.zhimg.com/v2-d79395ac05813235e8e4bebd9ec8f344" align="middle">
<img src="https://picx.zhimg.com/v2-43832a5e7347724fbe378c17a856fa2e" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LikePhys-Evaluating-Intuitive-Physics-Understanding-in-Video-Diffusion-Models-via-Likelihood-Preference"><a href="#LikePhys-Evaluating-Intuitive-Physics-Understanding-in-Video-Diffusion-Models-via-Likelihood-Preference" class="headerlink" title="LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion   Models via Likelihood Preference"></a>LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion   Models via Likelihood Preference</h2><p><strong>Authors:Jianhao Yuan, Fabio Pizzati, Francesco Pinto, Lars Kunze, Ivan Laptev, Paul Newman, Philip Torr, Daniele De Martini</strong></p>
<p>Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale. </p>
<blockquote>
<p>åœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç›´è§‰ç‰©ç†ç†è§£åœ¨æ„å»ºé€šç”¨ç‰©ç†å¯è¡Œä¸–ç•Œæ¨¡æ‹Ÿå™¨æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºå¾ˆéš¾å°†ç‰©ç†æ­£ç¡®æ€§ä»è§†è§‰å¤–è§‚ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå‡†ç¡®è¯„ä¼°è¿™ç§èƒ½åŠ›ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†LikePhysï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åŒºåˆ†ç‰©ç†ä¸Šæœ‰æ•ˆå’Œä¸å¯èƒ½çš„è§†é¢‘ï¼Œä½¿ç”¨å»å™ªç›®æ ‡ä½œä¸ºåŸºäºELBOçš„ä¼¼ç„¶æ€§æ›¿ä»£ç‰©ï¼Œåœ¨æœ‰æ•ˆ-æ— æ•ˆå¯¹çš„æ•°æ®é›†ä¸Šè¯„ä¼°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç›´è§‰ç‰©ç†ã€‚é€šè¿‡åœ¨æˆ‘ä»¬æ„å»ºçš„æ¶µç›–å››ä¸ªç‰©ç†é¢†åŸŸçš„åäºŒä¸ªåœºæ™¯åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬æ˜¾ç¤ºæˆ‘ä»¬çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å¯è¡Œæ€§åå¥½è¯¯å·®ï¼ˆPPEï¼‰ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ï¼Œä¼˜äºæœ€æ–°çš„è¯„ä¼°åŸºçº¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç›´è§‰ç‰©ç†ç†è§£è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†æ¨¡å‹è®¾è®¡å’Œæ¨ç†è®¾ç½®å¦‚ä½•å½±å“ç›´è§‰ç‰©ç†ç†è§£ï¼Œå¹¶çªå‡ºäº†ä¸åŒç‰©ç†å®šå¾‹é¢†åŸŸç‰¹å®šçš„èƒ½åŠ›å·®å¼‚ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰æ¨¡å‹åœ¨å¤æ‚å’Œæ··æ²ŒåŠ¨åŠ›å­¦æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†éšç€æ¨¡å‹èƒ½åŠ›å’Œæ¨ç†è®¾ç½®çš„æ‰©å¤§ï¼Œå¯¹ç‰©ç†çŸ¥è¯†çš„ç†è§£æœ‰æ˜æ˜¾çš„æ”¹è¿›è¶‹åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11512v1">PDF</a> 22 pages, 9 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ç›´è§‰ç‰©ç†å­¦ç†è§£åœ¨æ„å»ºé€šç”¨ç‰©ç†æ¨¡æ‹Ÿä¸–ç•Œä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†ç”±äºåŒºåˆ†ç‰©ç†æ­£ç¡®æ€§ä»è§†è§‰è¡¨ç°çš„éš¾åº¦ï¼Œå‡†ç¡®è¯„ä¼°è¿™æ ·çš„èƒ½åŠ›ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†LikePhysè¿™ä¸€æ— è®­ç»ƒè¯„ä¼°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åŒºåˆ†ç‰©ç†æœ‰æ•ˆå’Œæ— æ•ˆçš„è§†é¢‘æ¥è¯„ä¼°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç›´è§‰ç‰©ç†å­¦ï¼Œä½¿ç”¨é™å™ªç›®æ ‡ä½œä¸ºåŸºäºELBOçš„ä¼¼ç„¶æ€§æ›¿ä»£ç‰©åœ¨ä¸€ç»„æœ‰æ•ˆä¸æ— æ•ˆçš„è§†é¢‘é…å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡åœ¨æ„å»ºçš„æ¶µç›–å››ä¸ªç‰©ç†é¢†åŸŸçš„åäºŒä¸ªåœºæ™¯åŸºå‡†æµ‹è¯•ä¸­æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å¯ä¿¡åº¦åå¥½è¯¯å·®ï¼ˆPPEï¼‰ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è¯„ä¼°åŸºçº¿ã€‚ç„¶åæˆ‘ä»¬å¯¹å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç›´è§‰ç‰©ç†å­¦ç†è§£è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†æ¨¡å‹è®¾è®¡å’Œæ¨ç†è®¾ç½®å¦‚ä½•å½±å“ç›´è§‰ç‰©ç†å­¦ç†è§£ï¼Œå¹¶çªå‡ºäº†ä¸åŒç‰©ç†å®šå¾‹é¢†åŸŸçš„èƒ½åŠ›å·®å¼‚ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å’Œæ··æ²ŒåŠ¨åŠ›å­¦æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†éšç€æ¨¡å‹èƒ½åŠ›å’Œæ¨ç†è®¾ç½®çš„æ‰©å¤§ï¼Œç‰©ç†å­¦ç†è§£çš„è¶‹åŠ¿æ­£åœ¨æ˜æ˜¾æ”¹å–„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ç›´è§‰ç‰©ç†å­¦ç†è§£å¯¹äºæ„å»ºé€šç”¨ç‰©ç†æ¨¡æ‹Ÿä¸–ç•Œè‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºLikePhysçš„æ— è®­ç»ƒè¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡åŒºåˆ†ç‰©ç†æœ‰æ•ˆå’Œæ— æ•ˆçš„è§†é¢‘æ¥è¯„ä¼°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç›´è§‰ç‰©ç†å­¦ç†è§£ã€‚</li>
<li>LikePhysä½¿ç”¨é™å™ªç›®æ ‡ä½œä¸ºåŸºäºELBOçš„ä¼¼ç„¶æ€§æ›¿ä»£ç‰©åœ¨ä¸€ç»„æœ‰æ•ˆä¸æ— æ•ˆçš„è§†é¢‘é…å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å¼•å…¥çš„è¯„ä»·æŒ‡æ ‡â€”â€”å¯ä¿¡åº¦åå¥½è¯¯å·®ï¼ˆPPEï¼‰ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è¯„ä¼°åŸºçº¿ã€‚</li>
<li>å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç›´è§‰ç‰©ç†å­¦ç†è§£æ–¹é¢è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œè¡¨æ˜å°½ç®¡å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†æ¨¡å‹åœ¨ç‰©ç†å­¦ç†è§£æ–¹é¢æ­£åœ¨æ”¹å–„ã€‚</li>
<li>æ¨¡å‹è®¾è®¡å’Œæ¨ç†è®¾ç½®å¯¹ç›´è§‰ç‰©ç†å­¦ç†è§£æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc4548a0b5673f32f8dc5184ac16603f" align="middle">
<img src="https://picx.zhimg.com/v2-b611e1de1d65b0efeb97198cbb0eb878" align="middle">
<img src="https://picx.zhimg.com/v2-8fb350aaa869c054b82542c646a2764a" align="middle">
<img src="https://picx.zhimg.com/v2-81083db14101c33370cc0670b32cc580" align="middle">
<img src="https://picx.zhimg.com/v2-1b0bf67467d03ae8b22b30c53c540a22" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams"><a href="#StreamingVLM-Real-Time-Understanding-for-Infinite-Video-Streams" class="headerlink" title="StreamingVLM: Real-Time Understanding for Infinite Video Streams"></a>StreamingVLM: Real-Time Understanding for Infinite Video Streams</h2><p><strong>Authors:Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han</strong></p>
<p>Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/streaming-vlm">https://github.com/mit-han-lab/streaming-vlm</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èƒ½å¤Ÿä¸ºå®æ—¶åŠ©ç†å’Œè‡ªä¸»ä»£ç†æä¾›å¼ºå¤§çš„æ”¯æŒï¼Œä½†å®ƒä»¬é¢ä¸´ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼šåœ¨æ— é™æ¥è¿‘äºæ— ç©·çš„è§†é¢‘æµä¸­ç†è§£å†…å®¹ï¼ŒåŒæ—¶ä¸ä¼šå¢åŠ å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨ã€‚å¯¹æ•´æ®µè§†é¢‘è¿›è¡Œå…¨é¢å…³æ³¨å¤„ç†ä¼šå¯¼è‡´è®¡ç®—æˆæœ¬å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œå¹¶ä¸”åœ¨å¤„ç†é•¿è§†é¢‘æ—¶è¡¨ç°ä¸ä½³ã€‚åŒæ—¶ï¼Œç®€å•çš„æ»‘åŠ¨çª—å£æ–¹æ³•ä¹Ÿå­˜åœ¨ç¼ºé™·ï¼Œå› ä¸ºå®ƒä»¬è¦ä¹ˆç ´åè¿è´¯æ€§ï¼Œè¦ä¹ˆç”±äºå†—ä½™çš„é‡æ–°è®¡ç®—è€Œé­å—é«˜å»¶è¿Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†StreamingVLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“ä¸ºå®æ—¶ã€ç¨³å®šåœ°ç†è§£æ— é™è§†è§‰è¾“å…¥è€Œè®¾è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†è®­ç»ƒä¸æµå¼æ¨ç†ç›¸ç»“åˆã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é‡ç”¨æ³¨æ„åŠ›æ±‡èšçš„çŠ¶æ€ã€æœ€è¿‘çš„è§†è§‰ä»¤ç‰ŒçŸ­æœŸçª—å£ä»¥åŠæœ€è¿‘çš„æ–‡æœ¬ä»¤ç‰Œé•¿æœŸçª—å£æ¥ä¿æŒç´§å‡‘çš„KVç¼“å­˜ã€‚è¿™ç§æµå¼èƒ½åŠ›æ˜¯é€šè¿‡ä¸€ç§ç®€å•çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç­–ç•¥çŒè¾“çš„ï¼Œè¯¥ç­–ç•¥å¯¹é‡å çš„çŸ­æœŸè§†é¢‘å—åº”ç”¨å…¨æ³¨æ„åŠ›ï¼Œè¿™æœ‰æ•ˆåœ°æ¨¡ä»¿äº†æ¨ç†æ—¶çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè€Œæ— éœ€åœ¨ç¦æ­¢çš„è¿‡é•¿ä¸Šä¸‹æ–‡ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºç«‹äº†Inf-Streams-EvalåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¹³å‡è¶…è¿‡ä¸¤å°æ—¶çš„è§†é¢‘çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå®ƒè¦æ±‚å¸§å’Œæ–‡æœ¬ä¹‹é—´å¯†é›†çš„æ¯ç§’å¯¹é½ã€‚åœ¨Inf-Streams-EvalåŸºå‡†æµ‹è¯•ä¸­ï¼ŒStreamingVLMä»¥66.18%çš„èƒœç‡æˆ˜èƒœäº†GPT-4O miniï¼Œå¹¶åœ¨å•ä¸ªNVIDIA H100ä¸Šå®ç°äº†ç¨³å®šå®æ—¶çš„é«˜è¾¾8 FPSçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„SFTç­–ç•¥è¿˜æé«˜äº†é€šç”¨çš„VQAèƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•ç‰¹å®šçš„VQAå¾®è°ƒï¼Œåœ¨LongVideoBenchä¸Šçš„æ€§èƒ½æé«˜äº†+4.30ï¼Œåœ¨OVOBench Realtimeä¸Šçš„æ€§èƒ½æé«˜äº†+5.96ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/streaming-vlm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mit-han-lab/streaming-vlmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09608v1">PDF</a> The first two authors contributed equally to this work</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§ä¸ºå®æ—¶æ— é™è§†è§‰è¾“å…¥ç†è§£è€Œè®¾è®¡çš„StreamingVLMæ¨¡å‹ã€‚è¯¥æ¨¡å‹æ‹¥æœ‰å®æ—¶ç¨³å®šçš„ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡ç»´æŠ¤ä¸€ä¸ªç´§å‡‘çš„KVç¼“å­˜å¹¶é‡ç”¨æ³¨æ„åŠ›çŠ¶æ€ï¼Œå®ç°äº†å¯¹è¿‘æ— é™è§†é¢‘æµçš„ç†è§£ï¼Œè€Œæ— éœ€æ‰¿å—ä¸æ–­å¢é•¿çš„å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨å‹åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒStreamingVLMåœ¨Inf-Streams-EvalåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œä¸”å…¶ç›‘ç£å¾®è°ƒç­–ç•¥ä¸ä»…æå‡äº†æ¨¡å‹çš„å®æ—¶æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†é€šç”¨è§†è§‰é—®ç­”èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamingVLMæ¨¡å‹ä¸“ä¸ºå®æ—¶ç†è§£æ— é™è§†è§‰è¾“å…¥è®¾è®¡ï¼Œè§£å†³äº†è§†é¢‘æµå¤„ç†ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç»´æŠ¤KVç¼“å­˜å’Œé‡ç”¨æ³¨æ„åŠ›çŠ¶æ€ï¼Œå®ç°äº†å¯¹è§†é¢‘æµçš„å®æ—¶ç¨³å®šç†è§£ã€‚</li>
<li>StreamingVLMé€šè¿‡ç®€å•çš„ç›‘ç£å¾®è°ƒç­–ç•¥ï¼Œæ¨¡ä»¿æ¨ç†æ—¶çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œæ— éœ€åœ¨è¿‡äºå†—é•¿çš„ä¸Šä¸‹æ–‡ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¨¡å‹åœ¨Inf-Streams-EvalåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå±•ç¤ºäº†é«˜æ•ˆå®æ—¶çš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨å•NVIDIA H100ä¸Šä»¥æœ€é«˜è¾¾8å¸§æ¯ç§’çš„é€Ÿåº¦ç»´æŒç¨³å®šè¿è¡Œã€‚</li>
<li>ç›‘ç£å¾®è°ƒç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„é€šç”¨è§†è§‰é—®ç­”èƒ½åŠ›ï¼Œåœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f42ee154511d7702dd2b37a07a6697d2" align="middle">
<img src="https://picx.zhimg.com/v2-0b6e0b08377795c8a2311f591b4cb658" align="middle">
<img src="https://picx.zhimg.com/v2-9219ac14fd8cb5c49bbacaba83b8e402" align="middle">
<img src="https://picx.zhimg.com/v2-525a31e82781e2a21d587701555418e2" align="middle">
<img src="https://picx.zhimg.com/v2-709f453093bdb58c2e6490838a9a8248" align="middle">
<img src="https://picx.zhimg.com/v2-7a37590aa7c130f041d49872bd9a7316" align="middle">
<img src="https://picx.zhimg.com/v2-de2ee86ffbe89554e2bef887ffcb174a" align="middle">
<img src="https://picx.zhimg.com/v2-17a2db01de7d048f29c52f6aff236a17" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MomentSeg-Moment-Centric-Sampling-for-Enhanced-Video-Pixel-Understanding"><a href="#MomentSeg-Moment-Centric-Sampling-for-Enhanced-Video-Pixel-Understanding" class="headerlink" title="MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel   Understanding"></a>MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel   Understanding</h2><p><strong>Authors:Ming Dai, Sen Yang, Boqiang Duan, Wankou Yang, Jingdong Wang</strong></p>
<p>Referring Video Object Segmentation (RefVOS) seeks to segment target objects in videos guided by natural language descriptions, demanding both temporal reasoning and fine-grained visual comprehension. Existing sampling strategies for LLM-based approaches typically rely on either handcrafted heuristics or external keyframe models. The former often overlooks essential temporal cues, while the latter increases system complexity. To address this, we propose a unified framework that jointly optimizes Temporal Sentence Grounding (TSG) and RefVOS, naturally incorporating key moment grounding capability. During training, we introduce a novel TSG paradigm that employs a dedicated \texttt{[FIND]} token for key moment identification through temporal token similarity matching, thereby avoiding the need for external timestamp encodings. For inference, we design a Moment-Centric Sampling (MCS) strategy that densely samples informative moments while sparsely sampling non-essential frames, preserving both motion details and global context. To further enhance tracking stability, we develop Bidirectional Anchor-updated Propagation (BAP), which leverages the most relevant moment as start point for high-quality mask initialization and dynamically updates at sampled points to mitigate accumulated errors. Code and model will be available at: <a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/MomentSeg">https://github.com/Dmmm1997/MomentSeg</a> </p>
<blockquote>
<p>æŒ‡ä»£è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRefVOSï¼‰æ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼è§†é¢‘ä¸­çš„ç›®æ ‡å¯¹è±¡åˆ†å‰²ï¼Œè¿™éœ€è¦æ—¶åºæ¨ç†å’Œç²¾ç»†ç²’åº¦çš„è§†è§‰ç†è§£ã€‚åŸºäºLLMçš„ç°æœ‰é‡‡æ ·ç­–ç•¥é€šå¸¸ä¾èµ–äºæ‰‹å·¥å¯å‘å¼æ–¹æ³•æˆ–å¤–éƒ¨å…³é”®å¸§æ¨¡å‹ã€‚å‰è€…å¾€å¾€ä¼šå¿½ç•¥é‡è¦çš„æ—¶åºçº¿ç´¢ï¼Œè€Œåè€…å¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆä¼˜åŒ–æ—¶åºå¥å­å®šä½ï¼ˆTSGï¼‰å’ŒRefVOSï¼Œè‡ªç„¶åœ°èå…¥äº†å…³é”®æ—¶åˆ»å®šä½èƒ½åŠ›ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„TSGèŒƒå¼ï¼Œå®ƒé‡‡ç”¨ä¸“ç”¨çš„â€œ[FIND]â€ä»¤ç‰Œé€šè¿‡æ—¶åºä»¤ç‰Œç›¸ä¼¼æ€§åŒ¹é…è¿›è¡Œå…³é”®æ—¶åˆ»è¯†åˆ«ï¼Œä»è€Œé¿å…äº†å¤–éƒ¨æ—¶é—´æˆ³ç¼–ç çš„éœ€æ±‚ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä»¥æ—¶åˆ»ä¸ºä¸­å¿ƒçš„é‡‡æ ·ï¼ˆMCSï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯¹ä¿¡æ¯ä¸°å¯Œçš„æ—¶åˆ»è¿›è¡Œå¯†é›†é‡‡æ ·ï¼Œè€Œå¯¹éå…³é”®å¸§è¿›è¡Œç¨€ç–é‡‡æ ·ï¼Œä»è€Œä¿ç•™äº†è¿åŠ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è·Ÿè¸ªç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†åŒå‘é”šç‚¹æ›´æ–°ä¼ æ’­ï¼ˆBAPï¼‰ï¼Œå®ƒåˆ©ç”¨æœ€ç›¸å…³çš„æ—¶åˆ»ä½œä¸ºé«˜è´¨é‡æ©ç çš„èµ·ç‚¹ï¼Œå¹¶åœ¨é‡‡æ ·ç‚¹åŠ¨æ€æ›´æ–°ï¼Œä»¥å‡è½»ç´¯ç§¯è¯¯å·®ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ä»¥ä¸‹ç½‘å€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/MomentSeg">https://github.com/Dmmm1997/MomentSeg</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09274v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘å¯¹è±¡åˆ†å‰²æŠ€æœ¯ï¼ˆRefVOSï¼‰æ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å¯¹è§†é¢‘ä¸­çš„ç›®æ ‡å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œè¿™éœ€è¦æ—¶é—´å’Œç²¾ç»†çš„è§†è§‰ç†è§£ã€‚ç°æœ‰é‡‡æ ·ç­–ç•¥é€šå¸¸ä¾èµ–äºæ‰‹å·¥å¯å‘å¼æˆ–å¤–éƒ¨å…³é”®å¸§æ¨¡å‹ï¼Œå‰è€…å¿½ç•¥äº†é‡è¦çš„æ—¶é—´çº¿ç´¢ï¼Œåè€…å¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªè”åˆä¼˜åŒ–æ—¶é—´å¥å­å®šä½ï¼ˆTSGï¼‰å’ŒRefVOSçš„ç»Ÿä¸€æ¡†æ¶ï¼Œè‡ªç„¶èå…¥å…³é”®æ—¶åˆ»å®šä½èƒ½åŠ›ã€‚è®­ç»ƒæ—¶å¼•å…¥TSGèŒƒå¼ï¼Œé‡‡ç”¨ä¸“é—¨çš„[FIND]æ ‡è®°è¿›è¡Œå…³é”®æ—¶åˆ»è¯†åˆ«ï¼Œé€šè¿‡æ—¶é—´ä»¤ç‰Œç›¸ä¼¼æ€§åŒ¹é…ï¼Œæ— éœ€å¤–éƒ¨æ—¶é—´æˆ³ç¼–ç ã€‚æ¨ç†æ—¶è®¾è®¡Moment-Centricé‡‡æ ·ç­–ç•¥ï¼ˆMCSï¼‰ï¼Œå¯¹ä¿¡æ¯ä¸°å¯Œçš„æ—¶åˆ»å¯†é›†é‡‡æ ·ï¼Œå¯¹ä¸é‡è¦å¸§ç¨€ç–é‡‡æ ·ï¼Œå…¼é¡¾è¿åŠ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚è¿˜å¼€å‘åŒå‘é”šç‚¹æ›´æ–°ä¼ æ’­ï¼ˆBAPï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨æœ€ç›¸å…³çš„æ—¶åˆ»ä½œä¸ºé«˜è´¨é‡æ©ç çš„èµ·ç‚¹ï¼Œå¹¶åœ¨é‡‡æ ·ç‚¹åŠ¨æ€æ›´æ–°ï¼Œå‡å°‘ç´¯ç§¯è¯¯å·®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RefVOSæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å®ç°è§†é¢‘ä¸­çš„ç›®æ ‡å¯¹è±¡åˆ†å‰²ï¼Œéœ€è¦æ—¶é—´å’Œç²¾ç»†çš„è§†è§‰ç†è§£ã€‚</li>
<li>ç°æœ‰é‡‡æ ·ç­–ç•¥å­˜åœ¨ä¾èµ–æ‰‹å·¥å¯å‘å¼æˆ–å¤–éƒ¨æ¨¡å‹çš„é—®é¢˜ï¼Œå¯èƒ½å¿½ç•¥é‡è¦æ—¶é—´çº¿ç´¢å¹¶å¢åŠ ç³»ç»Ÿå¤æ‚æ€§ã€‚</li>
<li>å¼•å…¥ç»Ÿä¸€æ¡†æ¶è”åˆä¼˜åŒ–TSGå’ŒRefVOSï¼Œè‡ªç„¶èå…¥å…³é”®æ—¶åˆ»å®šä½èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨TSGèŒƒå¼å’Œ[FIND]æ ‡è®°è¿›è¡Œå…³é”®æ—¶åˆ»è¯†åˆ«ï¼Œé€šè¿‡æ—¶é—´ä»¤ç‰Œç›¸ä¼¼æ€§åŒ¹é…ï¼Œæ— éœ€å¤–éƒ¨ç¼–ç ã€‚</li>
<li>è®¾è®¡MCSç­–ç•¥å¯¹ä¿¡æ¯ä¸°å¯Œçš„æ—¶åˆ»å¯†é›†é‡‡æ ·ï¼Œå¯¹ä¸é‡è¦å¸§ç¨€ç–é‡‡æ ·ã€‚</li>
<li>å¼€å‘BAPæŠ€æœ¯åˆ©ç”¨æœ€ç›¸å…³æ—¶åˆ»ä½œä¸ºé«˜è´¨é‡æ©ç çš„èµ·ç‚¹ï¼Œå¹¶åœ¨é‡‡æ ·ç‚¹åŠ¨æ€æ›´æ–°ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨æé«˜è·Ÿè¸ªç¨³å®šæ€§å¹¶å‡å°‘ç´¯ç§¯è¯¯å·®ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨æŒ‡å®šé“¾æ¥ä¸Šå…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09274">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9239ab30619c1d5a5e3aaf734de6057" align="middle">
<img src="https://picx.zhimg.com/v2-a2e7867e4070d3b55a4648faa022198c" align="middle">
<img src="https://picx.zhimg.com/v2-c4312a93ed82a18f618120f38d2da6d0" align="middle">
<img src="https://picx.zhimg.com/v2-6e13ca4378ea2e6142c2d5203dfa815d" align="middle">
<img src="https://picx.zhimg.com/v2-db51409e92b30fbbe98fe3e4d348ed4c" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NeMo-Needle-in-a-Montage-for-Video-Language-Understanding"><a href="#NeMo-Needle-in-a-Montage-for-Video-Language-Understanding" class="headerlink" title="NeMo: Needle in a Montage for Video-Language Understanding"></a>NeMo: Needle in a Montage for Video-Language Understanding</h2><p><strong>Authors:Zi-Yuan Hu, Shuo Liang, Duo Zheng, Yanyang Li, Yeyao Tao, Shijia Huang, Wei Feng, Jia Qin, Jianguang Yu, Jing Huang, Meng Fang, Yin Li, Liwei Wang</strong></p>
<p>Recent advances in video large language models (VideoLLMs) call for new evaluation protocols and benchmarks for complex temporal reasoning in video-language understanding. Inspired by the needle in a haystack test widely used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed to assess VideoLLMsâ€™ critical reasoning capabilities, including long-context recall and temporal grounding. To generate video question answering data for our task, we develop a scalable automated data generation pipeline that facilitates high-quality data synthesis. Built upon the proposed pipeline, we present NeMoBench, a video-language benchmark centered on our task. Specifically, our full set of NeMoBench features 31,378 automatically generated question-answer (QA) pairs from 13,486 videos with various durations ranging from seconds to hours. Experiments demonstrate that our pipeline can reliably and automatically generate high-quality evaluation data, enabling NeMoBench to be continuously updated with the latest videos. We evaluate 20 state-of-the-art models on our benchmark, providing extensive results and key insights into their capabilities and limitations. Our project page is available at: <a target="_blank" rel="noopener" href="https://lavi-lab.github.io/NeMoBench">https://lavi-lab.github.io/NeMoBench</a>. </p>
<blockquote>
<p>å…³äºè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œè¦æ±‚å¯¹è§†é¢‘è¯­è¨€ç†è§£ä¸­çš„å¤æ‚æ—¶é—´æ¨ç†é‡‡ç”¨æ–°çš„è¯„ä¼°åè®®å’ŒåŸºå‡†æµ‹è¯•ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨çš„â€œå¯»æ‰¾ä¸€æ ¹é’ˆåœ¨å¤§æµ·é‡Œâ€æµ‹è¯•çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åä¸ºâ€œè’™å¤ªå¥‡ä¸­çš„é’ˆâ€ï¼ˆNeMoï¼‰çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°VideoLLMçš„å…³é”®æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬é•¿ä¸Šä¸‹æ–‡å›å¿†å’Œæ—¶é—´å®šä½ã€‚ä¸ºäº†ä¸ºæˆ‘ä»¬çš„ä»»åŠ¡ç”Ÿæˆè§†é¢‘é—®ç­”æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¾¿äºé«˜è´¨é‡æ•°æ®çš„åˆæˆã€‚åŸºäºæå‡ºçš„ç®¡é“ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä»¥æˆ‘ä»¬çš„ä»»åŠ¡ä¸ºä¸­å¿ƒçš„NeMoBenchè§†é¢‘è¯­è¨€åŸºå‡†æµ‹è¯•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„NeMoBenchå…¨å¥—åŠŸèƒ½åŒ…å«æ¥è‡ª13486ä¸ªè§†é¢‘çš„è‡ªåŠ¨ç”Ÿæˆçš„31378ä¸ªé—®ç­”å¯¹ï¼Œè§†é¢‘æ—¶é•¿ä»å‡ ç§’åˆ°å‡ å°æ—¶ä¸ç­‰ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“èƒ½å¤Ÿå¯é åœ°è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è¯„ä¼°æ•°æ®ï¼Œä½¿NeMoBenchèƒ½å¤Ÿä¸æ–­æ›´æ–°ä¸ºæœ€æ–°çš„è§†é¢‘ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†20ä¸ªæœ€å…ˆè¿›æ¨¡å‹ï¼Œæä¾›äº†å…³äºå…¶èƒ½åŠ›å’Œå±€é™æ€§çš„å¹¿æ³›ç»“æœå’Œå…³é”®è§è§£ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://lavi-lab.github.io/NeMoBench%E3%80%82">https://lavi-lab.github.io/NeMoBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24563v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰çš„æ–°ä»»åŠ¡â€œé’ˆå°–åœ¨è’™å¤ªå¥‡ä¸­çš„ä»»åŠ¡â€ï¼ˆNeMoï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘è¯­è¨€ç†è§£çš„å¤æ‚æ—¶åºæ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬é•¿ä¸Šä¸‹æ–‡å›å¿†å’Œæ—¶åºå®šä½ã€‚ä¸ºæ­¤ä»»åŠ¡ï¼Œå¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“ï¼Œå¹¶åŸºäºè¯¥ç®¡é“æ„å»ºäº†NeMoBenchè§†é¢‘è¯­è¨€åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä»æ—¶é•¿ä»å‡ ç§’åˆ°å‡ å°æ—¶ä¸ç­‰çš„13,486ä¸ªè§†é¢‘ä¸­è‡ªåŠ¨ç”Ÿæˆçš„31,378ä¸ªé—®ç­”å¯¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿå¯é åœ°è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è¯„ä¼°æ•°æ®ï¼Œä½¿å¾—NeMoBenchèƒ½å¤Ÿä¸æ–­æ›´æ–°ä»¥åŒ…å«æœ€æ–°è§†é¢‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹å½“å‰å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°å’Œç»“æœåˆ†æã€‚é¡¹ç›®è¯¦æƒ…å¯é€šè¿‡é“¾æ¥æŸ¥çœ‹ï¼š[é“¾æ¥åœ°å€]ï¼ˆå…·ä½“é“¾æ¥è¯·æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¤§è¯­è¨€æ¨¡å‹éœ€è¦æ–°çš„è¯„ä¼°åè®®å’ŒåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å¤æ‚æ—¶åºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥â€œé’ˆå°–åœ¨è’™å¤ªå¥‡ä¸­çš„ä»»åŠ¡â€ï¼ˆNeMoï¼‰ä»¥è¯„ä¼°è§†é¢‘è¯­è¨€ç†è§£çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘é—®ç­”æ•°æ®ã€‚</li>
<li>åŸºäºè¯¥ç®¡é“æ„å»ºäº†NeMoBenchè§†é¢‘è¯­è¨€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤§é‡è‡ªåŠ¨ç”Ÿæˆçš„é—®ç­”å¯¹ã€‚</li>
<li>è¯¥ç®¡é“èƒ½å¤Ÿå¯é åœ°è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è¯„ä¼°æ•°æ®ï¼Œä½¿NeMoBenchå¯ä¸æ–­æ›´æ–°ã€‚</li>
<li>åœ¨NeMoBenchä¸Šè¯„ä¼°äº†å¤šä¸ªå…ˆè¿›æ¨¡å‹ï¼Œå¹¶æä¾›äº†å¹¿æ³›çš„å®éªŒç»“æœå’Œåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-932f7110217ed5fb827364bf20071bf5" align="middle">
<img src="https://picx.zhimg.com/v2-1db36c46cc277e4b4efd7f1cca1e713c" align="middle">
<img src="https://picx.zhimg.com/v2-81c2d4608d28500704515efb4518c087" align="middle">
<img src="https://picx.zhimg.com/v2-71cf16d1dde298e5eb5e1e50fe9b8619" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="In-the-Eye-of-MLLM-Benchmarking-Egocentric-Video-Intent-Understanding-with-Gaze-Guided-Prompting"><a href="#In-the-Eye-of-MLLM-Benchmarking-Egocentric-Video-Intent-Understanding-with-Gaze-Guided-Prompting" class="headerlink" title="In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding   with Gaze-Guided Prompting"></a>In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding   with Gaze-Guided Prompting</h2><p><strong>Authors:Taiying Peng, Jiacheng Hua, Miao Liu, Feng Lu</strong></p>
<p>The emergence of advanced multimodal large language models (MLLMs) has significantly enhanced AI assistantsâ€™ ability to process complex information across modalities. Recently, egocentric videos, by directly capturing user focus, actions, and context in an unified coordinate, offer an exciting opportunity to enable proactive and personalized AI user experiences with MLLMs. However, existing benchmarks overlook the crucial role of gaze as an indicator of user intent. To address this gap, we introduce EgoGazeVQA, an egocentric gaze-guided video question answering benchmark that leverages gaze information to improve the understanding of longer daily-life videos. EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by human annotators. Our experiments reveal that existing MLLMs struggle to accurately interpret user intentions. In contrast, our gaze-guided intent prompting methods significantly enhance performance by integrating spatial, temporal, and intent-related cues. We further conduct experiments on gaze-related fine-tuning and analyze how gaze estimation accuracy impacts prompting effectiveness. These results underscore the value of gaze for more personalized and effective AI assistants in egocentric settings. Project page: <a target="_blank" rel="noopener" href="https://taiyi98.github.io/projects/EgoGazeVQA">https://taiyi98.github.io/projects/EgoGazeVQA</a> </p>
<blockquote>
<p>å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°ï¼Œæ˜¾è‘—å¢å¼ºäº†AIåŠ©ç†å¤„ç†è·¨æ¨¡æ€å¤æ‚ä¿¡æ¯çš„èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘é€šè¿‡ç›´æ¥æ•æ‰ç”¨æˆ·çš„å…³æ³¨ç‚¹ã€åŠ¨ä½œå’Œä¸Šä¸‹æ–‡ç»Ÿä¸€åæ ‡ï¼Œä¸ºåˆ©ç”¨MLLMså®ç°ä¸»åŠ¨ä¸ªæ€§åŒ–çš„AIç”¨æˆ·ä½“éªŒæä¾›äº†æ¿€åŠ¨äººå¿ƒçš„æœºä¼šã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¿½è§†äº†çœ¼ç¥ä½œä¸ºç”¨æˆ·æ„å›¾æŒ‡æ ‡çš„å…³é”®ä½œç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoGazeVQAï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥çœ¼ç¥ä¸ºä¸­å¿ƒçš„è‡ªæˆ‘æ³¨è§†å¼•å¯¼è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œå®ƒåˆ©ç”¨çœ¼ç¥ä¿¡æ¯æ¥æé«˜å¯¹æ—¥å¸¸ç”Ÿæ´»è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚EgoGazeVQAç”±MLLMsç”Ÿæˆçš„åŸºäºçœ¼ç¥çš„QAå¯¹ç»„æˆï¼Œå¹¶ç”±äººç±»æ³¨é‡Šè€…è¿›è¡Œå®Œå–„ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„MLLMsåœ¨å‡†ç¡®è§£é‡Šç”¨æˆ·æ„å›¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„çœ¼ç¥å¼•å¯¼æ„å›¾æç¤ºæ–¹æ³•é€šè¿‡æ•´åˆç©ºé—´ã€æ—¶é—´å’Œæ„å›¾ç›¸å…³çš„çº¿ç´¢ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…³äºçœ¼ç¥ç›¸å…³çš„å¾®è°ƒå®éªŒï¼Œå¹¶åˆ†æäº†çœ¼ç¥ä¼°è®¡ç²¾åº¦å¯¹æç¤ºæ•ˆæœçš„å½±å“ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†çœ¼ç¥åœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç¯å¢ƒä¸­ä¸ºæ›´åŠ ä¸ªæ€§åŒ–å’Œé«˜æ•ˆçš„AIåŠ©ç†æä¾›çš„ä»·å€¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://taiyi98.github.io/projects/EgoGazeVQA">https://taiyi98.github.io/projects/EgoGazeVQA</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07447v2">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong><br>     å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°ï¼Œæ˜¾è‘—å¢å¼ºäº†AIåŠ©ç†å¤„ç†è·¨æ¨¡æ€å¤æ‚ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºäº†æä¾›æ›´ä¸»åŠ¨ã€ä¸ªæ€§åŒ–çš„AIç”¨æˆ·ä½“éªŒï¼Œå¼•å…¥ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¿½è§†äº†è§†çº¿ä½œä¸ºç”¨æˆ·æ„å›¾æŒ‡æ ‡çš„é‡è¦ä½œç”¨ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EgoGazeVQAåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•é€šè¿‡è§†çº¿ä¿¡æ¯æ¥å¢å¼ºå¯¹æ—¥å¸¸é•¿è§†é¢‘çš„ç†è§£ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„MLLMsåœ¨å‡†ç¡®è§£é‡Šç”¨æˆ·æ„å›¾æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œæˆ‘ä»¬çš„è§†çº¿å¼•å¯¼æç¤ºæ–¹æ³•é€šè¿‡æ•´åˆç©ºé—´ã€æ—¶é—´å’Œæ„å›¾ç›¸å…³çº¿ç´¢æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…³äºè§†çº¿ç›¸å…³çš„å¾®è°ƒå®éªŒï¼Œå¹¶åˆ†æäº†è§†çº¿ä¼°è®¡ç²¾åº¦å¯¹æç¤ºæ•ˆæœçš„å½±å“ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç¯å¢ƒä¸­ï¼Œè§†çº¿å¯¹äºæ›´ä¸ªæ€§åŒ–ã€æœ‰æ•ˆçš„AIåŠ©ç†çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æé«˜äº†AIåŠ©ç†å¤„ç†è·¨æ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘èƒ½æä¾›æ›´ä¸»åŠ¨ã€ä¸ªæ€§åŒ–çš„AIç”¨æˆ·ä½“éªŒã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¿½ç•¥äº†è§†çº¿ä½œä¸ºç”¨æˆ·æ„å›¾çš„é‡è¦æŒ‡æ ‡ã€‚</li>
<li>å¼•å…¥EgoGazeVQAåŸºå‡†æµ‹è¯•ï¼Œç»“åˆè§†çº¿ä¿¡æ¯ï¼Œæé«˜æ—¥å¸¸é•¿è§†é¢‘çš„ç†è§£ã€‚</li>
<li>MLLMsåœ¨å‡†ç¡®è§£é‡Šç”¨æˆ·æ„å›¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>è§†çº¿å¼•å¯¼æç¤ºæ–¹æ³•é€šè¿‡æ•´åˆç©ºé—´ã€æ—¶é—´å’Œæ„å›¾ç›¸å…³çº¿ç´¢æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3796f8b7b8c425a6e0922b6cebb1ffb2" align="middle">
<img src="https://picx.zhimg.com/v2-351f8cd7599be05e9881fdd6e218c0eb" align="middle">
<img src="https://picx.zhimg.com/v2-85645f6025e56284627e3c432f6d8c7e" align="middle">
<img src="https://picx.zhimg.com/v2-1077c9172deeaef389b7483296fed781" align="middle">
<img src="https://picx.zhimg.com/v2-5de0b3ce62be53b6b168acafc4aae114" align="middle">
<img src="https://picx.zhimg.com/v2-8592ff2a0e111864c5e04821acca1302" align="middle">
<img src="https://picx.zhimg.com/v2-59e86e7b8ac22dc11a7164c634c38877" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="StreamAgent-Towards-Anticipatory-Agents-for-Streaming-Video-Understanding"><a href="#StreamAgent-Towards-Anticipatory-Agents-for-Streaming-Video-Understanding" class="headerlink" title="StreamAgent: Towards Anticipatory Agents for Streaming Video   Understanding"></a>StreamAgent: Towards Anticipatory Agents for Streaming Video   Understanding</h2><p><strong>Authors:Haolin Yang, Feilong Tang, Lingxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak</strong></p>
<p>Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸï¼Œå®æ—¶æµåª’ä½“è§†é¢‘ç†è§£é¢ä¸´ä¼ ç»Ÿç¦»çº¿è§†é¢‘å¤„ç†ä¹‹å¤–çš„æŒ‘æˆ˜ã€‚è¿™è¦æ±‚åŸºäºåŠ¨æ€å˜åŒ–çš„è§†è§‰å†…å®¹è¿›è¡ŒæŒç»­æ„ŸçŸ¥ã€ä¸»åŠ¨å†³ç­–å’Œå“åº”äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºäº¤æ›¿æ„ŸçŸ¥ååº”æˆ–å¼‚æ­¥è§¦å‘ï¼Œç¼ºä¹ä»»åŠ¡é©±åŠ¨çš„è§„åˆ’å’Œæœªæ¥é¢„æœŸï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®æ—¶æµåª’ä½“ä¸­çš„å“åº”èƒ½åŠ›å’Œä¸»åŠ¨å†³ç­–åˆ¶å®šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§StreamAgentï¼Œå®ƒèƒ½å¤Ÿé¢„æµ‹æœªæ¥ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„é¢„æœŸæ—¶é—´é—´éš”å’Œç©ºé—´åŒºåŸŸï¼Œä»¥å®ç°ä¸»åŠ¨å’Œç›®æ ‡é©±åŠ¨çš„å“åº”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æç¤ºé¢„æµ‹ä»£ç†æ•´åˆé—®é¢˜è¯­ä¹‰å’Œå†å²è§‚å¯Ÿï¼Œä»¥é¢„æµ‹å…³é”®äº‹ä»¶çš„æ—¶é—´è¿›å±•ï¼Œå°†å½“å‰è§‚å¯Ÿä¸é¢„æœŸçš„æœªæ¥è¯æ®å¯¹é½ï¼Œå¹¶éšåè°ƒæ•´æ„ŸçŸ¥åŠ¨ä½œï¼ˆä¾‹å¦‚ï¼Œå…³æ³¨ä»»åŠ¡ç›¸å…³åŒºåŸŸæˆ–åœ¨åç»­å¸§ä¸­æŒç»­è·Ÿè¸ªï¼‰ã€‚ä¸ºäº†å®ç°é«˜æ•ˆæ¨ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æµå¼KV-ç¼“å­˜å†…å­˜æœºåˆ¶ï¼Œæ„å»ºåˆ†å±‚å†…å­˜ç»“æ„ï¼Œå®ç°ç›¸å…³ä»¤ç‰Œçš„é€‰æ‹©æ€§å¬å›ï¼Œä»è€Œåœ¨å‡å°‘å­˜å‚¨æ‰€æœ‰ä»¤ç‰Œçš„ä¼ ç»ŸKV-ç¼“å­˜å¼€é”€çš„åŒæ—¶ï¼Œå®ç°é«˜æ•ˆè¯­ä¹‰æ£€ç´¢ã€‚åœ¨æµåª’ä½“å’Œé•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å“åº”å‡†ç¡®æ€§å’Œå®æ—¶æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†å…¶åœ¨çœŸå®ä¸–ç•Œæµåª’ä½“åœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01875v3">PDF</a> </p>
<p><strong>Summary</strong><br>å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½ç›‘æ§ç­‰é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è¶…è¶Šä¼ ç»Ÿçš„ç¦»çº¿è§†é¢‘å¤„ç†ï¼Œå®ç°è¿ç»­æ„ŸçŸ¥ã€ä¸»åŠ¨å†³ç­–å’ŒåŸºäºåŠ¨æ€è§†è§‰å†…å®¹çš„å“åº”äº¤äº’ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–æ„ŸçŸ¥ååº”æˆ–å¼‚æ­¥è§¦å‘ï¼Œç¼ºä¹ä»»åŠ¡é©±åŠ¨çš„è§„åˆ’å’Œæœªæ¥é¢„æµ‹ï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶æµåª’ä½“ä¸­çš„å“åº”æ€§å’Œä¸»åŠ¨å†³ç­–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºStreamAgentï¼Œå¯é¢„æµ‹æœªæ¥ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„æ—¶ç©ºé—´éš”åŒºåŸŸï¼Œå®ç°ç›®æ ‡é©±åŠ¨çš„å“åº”ã€‚é€šè¿‡æ•´åˆé—®é¢˜è¯­ä¹‰å’Œå†å²è§‚å¯Ÿï¼Œä¿ƒä½¿é¢„æµ‹ä»£ç†é¢„æµ‹å…³é”®äº‹ä»¶çš„æ—¶é—´è¿›å±•ï¼Œå°†å½“å‰è§‚å¯Ÿä¸é¢„æœŸçš„æœªæ¥è¯æ®å¯¹é½ï¼Œå¹¶è°ƒæ•´æ„ŸçŸ¥åŠ¨ä½œã€‚ä¸ºæå‡æ¨ç†æ•ˆç‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æµå¼KVç¼“å­˜æœºåˆ¶ï¼Œæ„å»ºåˆ†å±‚å†…å­˜ç»“æ„ï¼Œå®ç°ç›¸å…³ä»¤ç‰Œçš„é€‰æ‹©æ€§å›å¿†ï¼Œä»è€Œåœ¨å‡å°‘å­˜å‚¨æ‰€æœ‰ä»¤ç‰Œçš„ä¼ ç»ŸKVç¼“å­˜å¼€é”€çš„åŒæ—¶ï¼Œå®ç°é«˜æ•ˆè¯­ä¹‰æ£€ç´¢ã€‚åœ¨æµåª’ä½“å’Œé•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å“åº”å‡†ç¡®æ€§å’Œå®æ—¶æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡¸æ˜¾å…¶åœ¨çœŸå®æµåª’ä½“åœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶æµåª’ä½“è§†é¢‘ç†è§£åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½ç›‘æ§é¢†åŸŸå…·æœ‰æŒ‘æˆ˜ï¼Œéœ€è¶…è¶Šä¼ ç»Ÿç¦»çº¿è§†é¢‘å¤„ç†ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ„ŸçŸ¥ååº”æˆ–å¼‚æ­¥è§¦å‘ï¼Œç¼ºä¹ä»»åŠ¡é©±åŠ¨çš„è§„åˆ’å’Œæœªæ¥é¢„æµ‹ã€‚</li>
<li>æå‡ºStreamAgentï¼Œèƒ½é¢„æµ‹æœªæ¥ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„æ—¶ç©ºé—´éš”åŒºåŸŸï¼Œå®ç°ä¸»åŠ¨å†³ç­–ã€‚</li>
<li>é€šè¿‡æ•´åˆé—®é¢˜è¯­ä¹‰å’Œå†å²è§‚å¯Ÿï¼Œæé«˜é¢„æµ‹ä»£ç†çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡æµå¼KVç¼“å­˜æœºåˆ¶ï¼Œæé«˜è¯­ä¹‰æ£€ç´¢æ•ˆç‡ï¼Œå‡å°‘å­˜å‚¨å¼€é”€ã€‚</li>
<li>æ–¹æ³•åœ¨å“åº”å‡†ç¡®æ€§å’Œå®æ—¶æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01875">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f013fe7146e3fb78902619fef112eef" align="middle">
<img src="https://picx.zhimg.com/v2-105653dc05b713b6cc31bb7b27a35676" align="middle">
<img src="https://picx.zhimg.com/v2-c40900d9b6866c23f69d1d0da3841a2b" align="middle">
<img src="https://picx.zhimg.com/v2-ce3bf9eb2f809a9e5734c04f22a8f2c3" align="middle">
<img src="https://picx.zhimg.com/v2-e80d3f34fb55854ae4617fc6b17d98c4" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Role-of-Video-Generation-in-Enhancing-Data-Limited-Action-Understanding"><a href="#The-Role-of-Video-Generation-in-Enhancing-Data-Limited-Action-Understanding" class="headerlink" title="The Role of Video Generation in Enhancing Data-Limited Action   Understanding"></a>The Role of Video Generation in Enhancing Data-Limited Action   Understanding</h2><p><strong>Authors:Wei Li, Dezhao Luo, Dongbao Yang, Zhenhang Li, Weiping Wang, Yu Zhou</strong></p>
<p>Video action understanding tasks in real-world scenarios always suffer data limitations. In this paper, we address the data-limited action understanding problem by bridging data scarcity. We propose a novel method that employs a text-to-video diffusion transformer to generate annotated data for model training. This paradigm enables the generation of realistic annotated data on an infinite scale without human intervention. We proposed the information enhancement strategy and the uncertainty-based label smoothing tailored to generate sample training. Through quantitative and qualitative analysis, we observed that real samples generally contain a richer level of information than generated samples. Based on this observation, the information enhancement strategy is proposed to enhance the informative content of the generated samples from two aspects: the environments and the characters. Furthermore, we observed that some low-quality generated samples might negatively affect model training. To address this, we devised the uncertainty-based label smoothing strategy to increase the smoothing of these samples, thus reducing their impact. We demonstrate the effectiveness of the proposed method on four datasets across five tasks and achieve state-of-the-art performance for zero-shot action recognition. </p>
<blockquote>
<p>åœ¨ç°å®åœºæ™¯çš„è§†é¢‘åŠ¨ä½œç†è§£ä»»åŠ¡ä¸­ï¼Œæ•°æ®é™åˆ¶å§‹ç»ˆæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æ¥è§£å†³æ•°æ®æœ‰é™çš„åŠ¨ä½œç†è§£é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé‡‡ç”¨æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£å˜å‹å™¨æ¥ç”Ÿæˆç”¨äºæ¨¡å‹è®­ç»ƒçš„æ•°æ®æ ‡æ³¨ã€‚è¿™ç§èŒƒå¼èƒ½å¤Ÿåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ— é™è§„æ¨¡çš„ç°å®æ ‡æ³¨æ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¿¡æ¯å¢å¼ºç­–ç•¥å’ŒåŸºäºä¸ç¡®å®šæ€§çš„æ ‡ç­¾å¹³æ»‘ï¼Œä»¥å®šåˆ¶ç”Ÿæˆæ ·æœ¬è®­ç»ƒã€‚é€šè¿‡å®šé‡å’Œå®šæ€§åˆ†æï¼Œæˆ‘ä»¬å‘ç°çœŸå®æ ·æœ¬é€šå¸¸åŒ…å«æ¯”ç”Ÿæˆæ ·æœ¬æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºä¿¡æ¯å¢å¼ºç­–ç•¥ï¼Œä»ç¯å¢ƒå’Œè§’è‰²ä¸¤ä¸ªæ–¹é¢æé«˜ç”Ÿæˆæ ·æœ¬çš„ä¿¡æ¯å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ä¸€äº›ä½è´¨é‡çš„ç”Ÿæˆæ ·æœ¬å¯èƒ½ä¼šå¯¹æ¨¡å‹è®­ç»ƒäº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŸºäºä¸ç¡®å®šæ€§çš„æ ‡ç­¾å¹³æ»‘ç­–ç•¥ï¼Œä»¥å¢åŠ è¿™äº›æ ·æœ¬çš„å¹³æ»‘åº¦ï¼Œä»è€Œé™ä½å…¶å½±å“ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªä»»åŠ¡ä¸­çš„å››ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨é›¶æ ·æœ¬åŠ¨ä½œè¯†åˆ«é¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19495v2">PDF</a> IJCAI2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸­è§£å†³æ•°æ®æœ‰é™çš„é—®é¢˜çš„æ–¹æ³•ä¸»è¦æ˜¯é€šè¿‡å»ºç«‹æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè¿›è€Œæé«˜åŠ¨ä½œç†è§£æ¨¡å‹çš„æ€§èƒ½ã€‚é‡‡ç”¨ä¿¡æ¯å¢å¼ºç­–ç•¥å’ŒåŸºäºä¸ç¡®å®šæ€§çš„æ ‡ç­¾å¹³æ»‘ç­–ç•¥ï¼Œç”Ÿæˆæ›´çœŸå®ä¸°å¯Œçš„æ ·æœ¬æ•°æ®ï¼Œæå‡æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•å®ç°äº†é›¶æ¬¡åŠ¨ä½œè¯†åˆ«çš„æ€§èƒ½è¡¨ç°å¤„äºé¢†å…ˆçŠ¶æ€ã€‚è¿™ç§æ¨¡å‹å¯åœ¨ç°å®åœºæ™¯ä¸­è·å¾—åº”ç”¨ä»·å€¼ï¼Œå¹¶è§£å†³åŠ¨ä½œæ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå°±æ˜¯åˆ©ç”¨æ•°æ®ç”ŸæˆæŠ€æœ¯å’Œå¢å¼ºç­–ç•¥ä¼˜åŒ–åŠ¨ä½œè¯†åˆ«æ¨¡å‹çš„è¡¨ç°ã€‚è¿™ä¸€æ¨¡å‹å¯ç”¨äºå®é™…åœºæ™¯ä¸­è§£å†³åŠ¨ä½œæ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é’ˆå¯¹è§†é¢‘åŠ¨ä½œç†è§£ä»»åŠ¡åœ¨çœŸå®åœºæ™¯ä¸­çš„æ•°æ®é™åˆ¶é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£å˜æ¢å™¨ç”Ÿæˆè®­ç»ƒæ•°æ®çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºä¿¡æ¯å¢å¼ºç­–ç•¥ï¼Œä»ç¯å¢ƒå’Œäººç‰©ä¸¤ä¸ªæ–¹é¢æå‡ç”Ÿæˆæ ·æœ¬çš„ä¿¡æ¯ä¸°å¯Œåº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a869ac52a867974eba964cd79d1949e0" align="middle">
<img src="https://picx.zhimg.com/v2-51696f7df400c8e93732b40a0d3095f5" align="middle">
<img src="https://picx.zhimg.com/v2-1bebdea814480054ece8e4e500c9b8bb" align="middle">
<img src="https://picx.zhimg.com/v2-8af7cf1b09b871a4db7cd952a7a26b6f" align="middle">
<img src="https://picx.zhimg.com/v2-60217ce80bb81b243813491bab563f25" align="middle">
<img src="https://picx.zhimg.com/v2-a9e58b2c9f754854df65ab6fdacb2cc8" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VideoAds-for-Fast-Paced-Video-Understanding"><a href="#VideoAds-for-Fast-Paced-Video-Understanding" class="headerlink" title="VideoAds for Fast-Paced Video Understanding"></a>VideoAds for Fast-Paced Video Understanding</h2><p><strong>Authors:Zheyuan Zhang, Monica Dou, Linkai Peng, Hongyi Pan, Ulas Bagci, Boqing Gong</strong></p>
<p>Advertisement videos serve as a rich and valuable source of purpose-driven information, encompassing high-quality visual, textual, and contextual cues designed to engage viewers. They are often more complex than general videos of similar duration due to their structured narratives and rapid scene transitions, posing significant challenges to multi-modal large language models (MLLMs). In this work, we introduce VideoAds, the first dataset tailored for benchmarking the performance of MLLMs on advertisement videos. VideoAds comprises well-curated advertisement videos with complex temporal structures, accompanied by \textbf{manually} annotated diverse questions across three core tasks: visual finding, video summary, and visual reasoning. We propose a quantitative measure to compare VideoAds against existing benchmarks in terms of video complexity. Through extensive experiments, we find that Qwen2.5-VL-72B, an opensource MLLM, achieves 73.35% accuracy on VideoAds, outperforming GPT-4o (66.82%) and Gemini-1.5 Pro (69.66%); the two proprietary models especially fall behind the opensource model in video summarization and reasoning, but perform the best in visual finding. Notably, human experts easily achieve a remarkable accuracy of 94.27%. These results underscore the necessity of advancing MLLMsâ€™ temporal modeling capabilities and highlight VideoAds as a potentially pivotal benchmark for future research in understanding video that requires high FPS sampling. The dataset and evaluation code will be publicly available at <a target="_blank" rel="noopener" href="https://videoadsbenchmark.netlify.app/">https://videoadsbenchmark.netlify.app</a>. </p>
<blockquote>
<p>å¹¿å‘Šè§†é¢‘ä½œä¸ºç›®çš„é©±åŠ¨ä¿¡æ¯çš„ä¸°å¯Œä¸”å®è´µçš„æ¥æºï¼ŒåŒ…å«äº†ä¸ºå¸å¼•è§‚ä¼—è€Œè®¾è®¡çš„é«˜è´¨é‡è§†è§‰ã€æ–‡æœ¬å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ç”±äºå®ƒä»¬å…·æœ‰ç»“æ„åŒ–çš„å™äº‹å’Œå¿«é€Ÿçš„åœºæ™¯è½¬æ¢ï¼Œå¾€å¾€æ¯”ç±»ä¼¼æ—¶é•¿çš„æ™®é€šè§†é¢‘æ›´åŠ å¤æ‚ï¼Œè¿™ç»™å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VideoAdsï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¹¿å‘Šè§†é¢‘è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚VideoAdsåŒ…å«äº†å…·æœ‰å¤æ‚æ—¶é—´ç»“æ„çš„ç²¾å¿ƒæŒ‘é€‰çš„å¹¿å‘Šè§†é¢‘ï¼Œä»¥åŠé’ˆå¯¹ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡çš„å¤šæ ·åŒ–æ‰‹åŠ¨æ³¨é‡Šé—®é¢˜ï¼šè§†è§‰æŸ¥æ‰¾ã€è§†é¢‘æ‘˜è¦å’Œè§†è§‰æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®šé‡åº¦é‡æ ‡å‡†ï¼Œä»¥æ ¹æ®è§†é¢‘å¤æ‚æ€§å°†VideoAdsä¸ç°æœ‰åŸºå‡†æµ‹è¯•è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°å¼€æºMLLM Qwen2.5-VL-72Båœ¨VideoAdsä¸Šè¾¾åˆ°äº†73.35%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†GPT-4oï¼ˆ66.82%ï¼‰å’ŒGemini-1.5 Proï¼ˆ69.66%ï¼‰ï¼›è¿™ä¸¤ä¸ªä¸“æœ‰æ¨¡å‹å°¤å…¶åœ¨è§†é¢‘æ‘˜è¦å’Œæ¨ç†æ–¹é¢è½åäºå¼€æºæ¨¡å‹ï¼Œä½†åœ¨è§†è§‰æŸ¥æ‰¾æ–¹é¢è¡¨ç°æœ€ä½³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œäººç±»ä¸“å®¶å¾ˆå®¹æ˜“è¾¾åˆ°94.27%çš„æ˜¾è‘—å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æé«˜MLLMsçš„æ—¶é—´å»ºæ¨¡èƒ½åŠ›çš„å¿…è¦æ€§ï¼Œå¹¶çªå‡ºäº†VideoAdsä½œä¸ºæœªæ¥ç ”ç©¶ç†è§£è§†é¢‘çš„é«˜FPSé‡‡æ ·æ½œåŠ›çš„å…³é”®åŸºå‡†æµ‹è¯•ã€‚æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://videoadsbenchmark.netlify.appä¸Šå…¬å¼€å¯ç”¨./">https://videoadsbenchmark.netlify.appä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09282v2">PDF</a> ICCV2025</p>
<p><strong>Summary</strong>ï¼š<br>å¹¿å‘Šè§†é¢‘ä½œä¸ºç›®çš„é©±åŠ¨ä¿¡æ¯çš„ä¸°å¯Œæ¥æºï¼ŒåŒ…å«é«˜è´¨é‡è§†è§‰ã€æ–‡æœ¬å’Œä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œæ—¨åœ¨å¸å¼•è§‚ä¼—ã€‚ç”±äºå…¶ç»“æ„åŒ–çš„å™äº‹å’Œå¿«é€Ÿçš„åœºæ™¯è½¬æ¢ï¼Œå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¨å‡ºVideoAdsæ•°æ®é›†ï¼Œä¸“ä¸ºè¯„ä¼°å¹¿å‘Šè§†é¢‘ä¸ŠMLLMsæ€§èƒ½è€Œè®¾è®¡ã€‚VideoAdsåŒ…å«ç²¾å¿ƒæŒ‘é€‰çš„å¹¿å‘Šè§†é¢‘ï¼Œå…·æœ‰å¤æ‚çš„æ—¶é—´ç»“æ„ï¼Œå¹¶é…æœ‰é’ˆå¯¹ä¸‰é¡¹æ ¸å¿ƒä»»åŠ¡çš„æ‰‹å·¥æ³¨é‡Šçš„å¤šæ ·åŒ–é—®é¢˜ï¼šè§†è§‰æŸ¥æ‰¾ã€è§†é¢‘æ‘˜è¦å’Œè§†è§‰æ¨ç†ã€‚é€šè¿‡å¹¿æ³›å®éªŒå‘ç°å¼€æºMLLM Qwen2.5-VL-72Båœ¨VideoAdsä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º73.35%ï¼Œä¼˜äºGPT-4oå’ŒGemini-1.5 Proã€‚äººç±»ä¸“å®¶è½»æ¾è¾¾åˆ°94.27%çš„å‡†ç¡®ç‡ã€‚ç»“æœçªæ˜¾äº†æé«˜MLLMsæ—¶é—´å»ºæ¨¡èƒ½åŠ›çš„å¿…è¦æ€§ï¼Œå¹¶å¼ºè°ƒäº†VideoAdsä½œä¸ºæœªæ¥ç ”ç©¶çš„å…³é”®åŸºå‡†çš„é‡è¦æ€§ã€‚è¯¥æ•°æ®é›†å’Œè¯„ä¼°ä»£ç å°†å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://videoadsbenchmark.netlify.app./">https://videoadsbenchmark.netlify.appã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¹¿å‘Šè§†é¢‘æ˜¯ä¿¡æ¯ä¸°å¯Œçš„æ•°æ®æ¥æºï¼ŒåŒ…å«äº†è§†è§‰ã€æ–‡æœ¬å’Œä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œå…·æœ‰å¤æ‚çš„å™äº‹ç»“æ„å’Œåœºæ™¯è½¬æ¢ã€‚</li>
<li>VideoAdsæ•°æ®é›†æ˜¯ä¸“ä¸ºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¹¿å‘Šè§†é¢‘ä¸Šçš„æ€§èƒ½è€Œè®¾è®¡çš„ã€‚</li>
<li>VideoAdsåŒ…å«æ‰‹å·¥æ³¨é‡Šçš„å¤šæ ·åŒ–é—®é¢˜ï¼Œæ¶µç›–è§†è§‰æŸ¥æ‰¾ã€è§†é¢‘æ‘˜è¦å’Œè§†è§‰æ¨ç†ä¸‰é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚</li>
<li>å¼€æºMLLM Qwen2.5-VL-72Båœ¨VideoAdsæ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º73.35%ã€‚</li>
<li>åœ¨è§†è§‰æŸ¥æ‰¾ä»»åŠ¡ä¸Šï¼Œä¸¤ä¸ªä¸“æœ‰æ¨¡å‹è¡¨ç°æœ€å¥½ã€‚ä½†åœ¨è§†é¢‘æ‘˜è¦å’Œæ¨ç†æ–¹é¢ï¼Œå¼€æºæ¨¡å‹è¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
<li>äººç±»ä¸“å®¶åœ¨è¯„ä¼°å¹¿å‘Šè§†é¢‘ä»»åŠ¡æ—¶è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼ˆ94.27%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-272b76fe3b715ba97098052bac958979" align="middle">
<img src="https://picx.zhimg.com/v2-204ddb7f406fc8dfed421d4afe3bd5e8" align="middle">
<img src="https://picx.zhimg.com/v2-cea4cb52cd39bec1f8e670587becc273" align="middle">
<img src="https://picx.zhimg.com/v2-06e62343c630cc627d1133022c4bb17a" align="middle">
<img src="https://picx.zhimg.com/v2-edcc8e871aa9d04d1eabff91ee2f0164" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-85b3126f3e4d8c14cccbeb1047147760" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Towards Generalist Intelligence in Dentistry Vision Foundation Models   for Oral and Maxillofacial Radiology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-94d2f501aa22e6a9e80e834bef2a1c6f" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
