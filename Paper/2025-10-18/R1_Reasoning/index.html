<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-10-18  Agentic Design of Compositional Machines">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-e4af957f63b08c2752075fc2c2f13d7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726092&auth_key=1760726092-0-0-a53a65026dc21de436c0cc649fc316cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    84 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="Agentic-Design-of-Compositional-Machines"><a href="#Agentic-Design-of-Compositional-Machines" class="headerlink" title="Agentic Design of Compositional Machines"></a>Agentic Design of Compositional Machines</h2><p><strong>Authors:Wenqian Zhang, Weiyang Liu, Zhen Liu</strong></p>
<p>The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning. </p>
<blockquote>
<p>复杂机器的设计既是人类智慧的标志，也是工程实践的基础。鉴于大型语言模型（LLMs）的最新进展，我们想知道它们是否也能学会创造。我们通过对组合机械设计这一视角来探讨这个问题：这是一项任务，其中机器由标准化组件组装而成，以满足模拟物理环境中的运动或操作等功能需求。为了支持这项研究，我们引入了基于机器建造游戏Besiege构建的BesiegeField测试平台，该平台支持基于零件的建造、物理模拟和奖励驱动评估。通过BesiegeField平台，我们对最先进的LLMs进行了基准测试，并确定了成功所需的关键能力，包括空间推理、策略装配和指令执行。由于当前开源模型的不足，我们探索了强化学习（RL）作为改进的途径：我们整理了一个冷启动数据集，进行了RL微调实验，并强调了语言、机器设计和物理推理交汇处的开放挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14980v1">PDF</a> 75 pages, 31 figures, Project Page: <a target="_blank" rel="noopener" href="https://besiegefield.github.io/">https://besiegefield.github.io</a></p>
<p><strong>Summary</strong></p>
<p>复杂机器的设计既体现了人类智慧，又是工程实践的基础。借助大型语言模型（LLMs）的最新发展，我们探讨它们是否也能学习创造。本研究通过组合机器设计的视角来探讨这一问题，这是一项在模拟物理环境中，通过标准化组件来组装机器以满足行动或操作等需求的任务。为了支持这一研究，我们基于机器建造游戏Besiege建立了BesiegeField测试平台，该平台支持部件构造、物理模拟和奖励驱动评估。我们使用BesiegeField对最先进的LLMs进行基准测试，并确定成功的关键能力，包括空间推理、策略性装配和指令遵循等。由于当前开源模型的不足，我们探索了强化学习（RL）作为改进的路径：我们筛选了冷启动数据集，进行了RL微调实验，并强调了语言、机器设计和物理推理交叉领域的开放挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在机器设计方面的能力成为研究焦点。</li>
<li>BesiegeField测试平台用于支持机器构造、物理模拟和评估。</li>
<li>先进LLMs在组合机器设计任务中需具备空间推理、策略性装配和指令遵循等关键能力。</li>
<li>当前开源模型在机器设计方面存在不足，强化学习（RL）被提出为改进方向。</li>
<li>研究筛选了冷启动数据集用于RL微调实验。</li>
<li>在语言、机器设计和物理推理交叉领域存在许多开放挑战。</li>
<li>该研究为LLMs在机器设计领域的进一步发展提供了基准和研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14980">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c256346b3b9819197b782e126d33bdfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725843&auth_key=1760725843-0-0-11566775e72575ed804faa5df6c1005d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b72de284f534b45d2dde836e7407ef4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725850&auth_key=1760725850-0-0-da6aed061ecee525d1abb53cb2f78d70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a452b05e0464d861460cf73d4168c3cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725857&auth_key=1760725857-0-0-b7cb619b74f7870af63845a355edb5ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d601bcfadf1df847879211c85ec7849~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725864&auth_key=1760725864-0-0-d10ca7de67f0bd8a2861a65cba6451ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7a965fcabd9913c6053e7b0c2183944~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725871&auth_key=1760725871-0-0-132bd2dd52d7f40a16e472ea12b1fab3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents"><a href="#Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents" class="headerlink" title="Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents"></a>Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents</h2><p><strong>Authors:Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying</strong></p>
<p>Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy’s probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model’s own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理越来越多地使用强化学习（RL）进行训练，以加强其通过工具使用与外部环境的交互能力，特别是在需要多轮推理和知识获取基于搜索的环境中。然而，现有方法通常依赖于仅在最终答案上提供的结果型奖励。这种奖励稀疏性在多轮环境中变得特别成问题，长轨迹会加剧两个关键问题：（i）优势崩溃，所有滚动结果获得相同的奖励，无法提供有用的学习信号；（ii）缺乏精细的信用分配，回合之间的依赖关系被掩盖，特别是在长期任务中。在本文中，我们提出了基于信息增益的策略优化（IGPO），这是一个简单有效的强化学习框架，为多轮代理训练提供密集和内生的监督。IGPO将每个交互回合视为关于真相的增量获取过程，并将回合奖励定义为策略产生正确答案的概率的边际增加。与依赖外部奖励模型或昂贵的蒙特卡洛估计的先前过程级奖励方法不同，IGPO直接从模型自身的信念更新中得出内在奖励。这些内在回合奖励与结果级监督相结合，形成了密集的奖励轨迹。在域内和域外基准测试上的大量实验表明，IGPO在多轮场景中始终优于强大的基准测试，具有更高的准确性和样本效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14967v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>大型语言模型（LLM）基于强化学习（RL）训练的智能体在搜索等环境中使用工具的能力日益增强。然而，现有的方法主要依赖于最终答案的产出导向奖励，这在多轮推理和任务中获取知识的场景中存在问题。针对奖励稀疏的问题，本文提出了一种简单有效的强化学习框架——基于信息增益的策略优化（IGPO）。它将每轮交互视为对事实真相信息的累积过程，并将奖励定义为策略产生正确答案的概率的边际增加。与其他依赖于外部奖励模型或昂贵的蒙特卡洛估计的过程级奖励方法不同，IGPO直接从模型自身的信念更新中获得内在奖励。这些内在的轮级奖励与结果级的监督相结合，形成了密集的奖励轨迹。在内部和外部基准测试的大量实验中，IGPO在多轮场景中的表现始终优于强大的基线，实现了更高的准确性和样本效率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型结合强化学习在多轮推理和知识获取场景中表现优异。</li>
<li>现有方法主要依赖最终答案的产出导向奖励，这在多轮场景中存在问题。</li>
<li>基于信息增益的策略优化（IGPO）框架能有效解决奖励稀疏问题。</li>
<li>IGPO将每轮交互视为信息累积过程，并定义奖励为策略产生正确答案概率的边际增加。</li>
<li>IGPO从模型自身的信念更新中获得内在奖励，避免依赖外部奖励模型或蒙特卡洛估计。</li>
<li>内在轮级奖励与结果级监督结合形成密集奖励轨迹，提高学习效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14967">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6acee461152fa22ff3a292ec3c6b9c47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725881&auth_key=1760725881-0-0-4be1871661d727369525e9bdbba456f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-069829c476fc987398af88ea95aa8f54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725888&auth_key=1760725888-0-0-0f0864e6976950b3980ed3f915530d36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcb8a3c56cd16d74ac346368c4442502~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725894&auth_key=1760725894-0-0-85576916188c7701f0cf662bfe02b37b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning"><a href="#MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning" class="headerlink" title="MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning"></a>MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning</h2><p><strong>Authors:Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li</strong></p>
<p>While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a> </p>
<blockquote>
<p>虽然大型语言模型（LLM）在文本推理方面表现出色，但在几何等数学领域，它们天生依赖视觉辅助工具，因此面临挑战。现有的视觉思维链（VCoT）方法往往受到僵化外部工具的限制，或者无法生成解决复杂问题所需的忠实度高、时效战略强的图表。为了弥合这一鸿沟，我们推出了MathCanvas，这是一个旨在赋予统一大型多模态模型（LMM）内在视觉思维链能力的全面框架，用于数学领域。我们的方法分为两个阶段。首先，视觉操作阶段在一个新型1520万对数据对上对模型进行预训练，包括1亿个标题到图表的配对（MathCanvas-Imagen）和520万条逐步编辑轨迹（MathCanvas-Edit），以掌握图表生成和编辑。其次，策略性视觉辅助推理阶段对模型进行微调，使用MathCanvas-Instruct，这是一套包含混合视觉文本推理路径的新数据集，拥有多达21万条样本，向模型教授何时以及如何运用视觉辅助工具。为了方便严格的评估测试，我们推出了MathCanvas-Bench这一具有挑战性的基准测试平台，拥有需要模型生成混合视觉文本解决方案的3千个问题。我们的模型BAGEL-Canvas在该框架下经过训练后，在MathCanvas-Bench上的表现比强大的多模态模型基线高出相对改进幅度高达86%，在公共数学基准测试中展现出出色的泛化能力。我们的工作提供了完整的工具箱框架、数据集和基准测试平台，以解锁大型多模态模型中复杂、人性化的视觉辅助推理能力。项目页面链接：<a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14958v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
<p><strong>Summary</strong><br>在大型语言模型（LLM）擅长文本推理的同时，它们在依赖视觉辅助工具的几何等领域却表现不佳。现有的视觉思维链（VCoT）方法受限于外部工具的限制或无法生成用于复杂问题解决的必要的高保真度、策略性定时图表。为了弥补这一差距，我们推出了MathCanvas框架，旨在赋予统一的大型多模态模型（LMM）内在VCoT能力。该框架包含两个阶段：首先通过新颖的包含一亿五千两百万配对图像与五十二亿编辑轨迹数据的视觉操控阶段，让模型学会生成与编辑图表；然后通过精细微调数学中的解题策略阶段，让模型学会何时以及如何利用视觉辅助工具。我们引入了MathCanvas-Bench作为挑战基准测试，包含三千道需要模型生成图文结合解决方案的问题。在我们的框架下训练的BAGEL-Canvas模型，在MathCanvas-Bench上的表现比强大的LMM基准测试高出86%，并展示了在其他公共数学测试中的良好泛化能力。我们的工作提供了一个完整的大型多模态模型进行视觉辅助推理的工具包框架、数据集和基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在几何等领域表现不佳，缺乏视觉辅助能力。</li>
<li>MathCanvas框架旨在通过视觉操控和精细微调赋予大型多模态模型内在视觉思维链（VCoT）能力。</li>
<li>MathCanvas框架包含两个阶段：视觉操控阶段和战略视觉辅助推理阶段。</li>
<li>MathCanvas引入了新数据集用于训练和评估模型在视觉辅助下的数学问题解决能力。</li>
<li>BAGEL-Canvas模型在MathCanvas基准测试中表现优异，相对改进率达到了86%。</li>
<li>BAGEL-Canvas模型具有良好的泛化能力，在其他公共数学测试中也有出色表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5c4eefc85e5854dadcc669fa4285b07d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725903&auth_key=1760725903-0-0-1b723f4da0bcccf3e1debd9955155a27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de68a430433309e0a20c51788ecb1b34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725911&auth_key=1760725911-0-0-67492bdd5ccd70bb9ac98d26727a4f71&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea25758b6de272f758c85964a8765599~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725918&auth_key=1760725918-0-0-55890b866f1de9be456bc70396fff4cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5450ab2559bbf7b0feb603ba68bd91fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725925&auth_key=1760725925-0-0-ab4e05ef1165e6c06c68b555b94889e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning"><a href="#GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning" class="headerlink" title="GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning"></a>GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning</h2><p><strong>Authors:Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp</strong></p>
<p>Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning. </p>
<blockquote>
<p>流程奖励模型（PRMs）旨在通过监督中间步骤和识别错误来提高大型语言模型（LLM）中的多步推理能力。然而，由于缺乏可扩展的高质量注释，构建有效的PRM仍然是一个挑战。现有方法依赖于昂贵的人工标注、基于LLM的自我评价（容易出现幻觉）或蒙特卡洛（MC）估计。MC估计仅从滚动结果中推断步骤质量，但由于功劳分配不当，经常引入嘈杂、错位的监督。这些问题导致三个核心局限：奖励噪声、事实准确性低以及与步骤级推理目标的错位。为了解决这些挑战，我们引入了GroundedPRM，这是一个用于自动流程监督的树形引导和保真度感知框架。为了减少奖励噪声并实现精细的功劳分配，我们通过蒙特卡洛树搜索（MCTS）构建结构化推理路径。为了消除幻觉监督，我们使用外部工具验证每个中间步骤，提供基于执行的正确性信号。为了结合步骤级验证和全局结果评估，我们设计了一种混合奖励聚合机制，它将工具验证与MCTS派生的反馈结合起来。最后，我们将奖励信号格式化为增强理性、生成性的结构，以促进可解释性与指令调优的LLM的兼容性。GroundedPRM仅使用40万个自动标记样本进行训练，仅占使用自动标记监督的最佳PRM所用数据的10%。然而，它在ProcessBench上的平均性能实现了高达26%的相对改进。当用于奖励引导贪婪搜索时，GroundedPRM甚至超越了使用人工标记监督训练的PRM，为高质量流程级推理提供了可扩展和可验证的路径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14942v1">PDF</a> 25 pages</p>
<p><strong>Summary</strong><br>     进程奖励模型旨在通过监督中间步骤和识别错误来改善大型语言模型的多步推理能力。但构建有效的进程奖励模型仍然具有挑战性，主要因为缺乏可扩展的高质量注释。为应对这些挑战，提出基于树引导与保真度的框架进行自动过程监督，构建结构化推理路径并减少奖励噪声，利用蒙特卡洛树搜索进行精细化的信用分配，并使用外部工具验证每个中间步骤以消除幻觉监督。结合步骤级验证和全局结果评估的混合奖励聚合机制，融合工具验证与蒙特卡洛树搜索的反馈。该模型在仅有4万自动标记样本的训练下，实现了对最佳进程的相对改进。在ProcessBench上的平均性能提高了26%，在奖励指导的贪心搜索方面表现尤为出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>进程奖励模型旨在提高大型语言模型的多步推理能力，通过监督中间步骤和识别错误来实现。</li>
<li>当前进程奖励模型面临的主要挑战是缺乏高质量、可扩展的注释数据。</li>
<li>提出名为GroundedPRM的框架，结合蒙特卡洛树搜索构建结构化推理路径，以减少奖励噪声并实现精细化的信用分配。</li>
<li>通过使用外部工具验证中间步骤，消除幻觉监督，提高监督的准确性。</li>
<li>结合步骤级验证和全局结果评估的混合奖励聚合机制，提高奖励信号的质量和有效性。</li>
<li>通过融合工具验证与蒙特卡洛树搜索的反馈，提高了模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14942">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-dd41ee8a24b5e926edfce49f81364020~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725933&auth_key=1760725933-0-0-f0de87c0eebb7a00f8ca2146e3adf69c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3a6fabc7ec2f2e13ded990993e3bb07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725940&auth_key=1760725940-0-0-9427b1a1a68a62e5e1c1fb28ae2adba6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection"><a href="#CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection" class="headerlink" title="CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection"></a>CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection</h2><p><strong>Authors:Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim</strong></p>
<p>Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art. </p>
<blockquote>
<p>开放词汇对象检测（OVD）旨在识别并定位超出训练期间所见对象的类别。最近的方法通常利用视觉语言模型（VLM）通过图像文本对齐生成伪标签，允许检测器在没有明确监督的情况下推广到未见类别。然而，这些方法过于依赖直接的图像文本匹配，忽略了解释语义复杂场景所必需的中介推理步骤。这导致在面对拥挤或遮挡的视觉上下文时，其稳健性受到限制。在本文中，我们介绍了CoT-PL，这是一种新的框架，它将结构化的视觉思维链（CoT）推理融入伪标签过程中。CoT-PL将对象理解分解为三个可解释步骤：（1）即使对于未见对象也能进行区域感知，（2）通过零射击推理进行类别识别，（3）背景定位以分离语义复杂的对象。关键的是，第三步自然激励我们的对比背景学习（CBL），它使用预先计算好的背景线索作为负样本，以促进对象和背景特征之间的解耦。通过这种方式，CoT推理和CBL形成了一个针对拥挤或遮挡场景中的稳健伪标签的集成管道。值得注意的是，在这两种设置中，我们的新型伪标签质量分别比最佳先前技术提高了103.4%和168.4%。我们的广泛实验表明，CoT-PL在开放词汇COCO上提高了+7.7 AP50，在LVIS的新类别上提高了+2.9 mask AP，创造了新的技术记录。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14792v1">PDF</a> 28 pages, 13 Figures, 12 Tables</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为CoT-PL的新框架，用于开放词汇对象检测中的伪标签生成。该框架引入结构化视觉思维链（CoT）推理，将对象理解分解为三个可解释的步骤：区域感知、类别识别以及背景定位。通过结合对比背景学习（CBL），该框架能在拥挤或遮挡的场景中实现稳健的伪标签生成，并在新型类别伪标签质量方面实现了显著的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoT-PL框架利用结构化视觉思维链（CoT）推理进行伪标签生成。</li>
<li>CoT-PL将对象理解分解为三个步骤：区域感知、类别识别和背景定位。</li>
<li>对比背景学习（CBL）用于促进特征和背景之间的特征解耦。</li>
<li>CoT-PL在拥挤和遮挡的场景中生成伪标签时表现出稳健性。</li>
<li>相对于最佳先前技术，CoT-PL在新型类别伪标签质量方面实现了显著提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14792">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a51571a7753a8b8f1b0df0f2f6e96076~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725947&auth_key=1760725947-0-0-b3f10e1baaef5a1d5f421efc00e63913&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11ae2304570f3115aa020a2f8ca8e459~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725955&auth_key=1760725955-0-0-4b8d4665ab609855430051c356c3c04a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a7c94335e1212f32458fa6635cc6a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725961&auth_key=1760725961-0-0-d9105258af9b751f25317a0174e1e518&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a163431ddc0d286b1581671654c24eac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725969&auth_key=1760725969-0-0-a7f3dbbb1bf26de59c453ad62f38be2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pluto-A-Benchmark-for-Evaluating-Efficiency-of-LLM-generated-Hardware-Code"><a href="#Pluto-A-Benchmark-for-Evaluating-Efficiency-of-LLM-generated-Hardware-Code" class="headerlink" title="Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware   Code"></a>Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware   Code</h2><p><strong>Authors:Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda</strong></p>
<p>Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8%, delay efficiency of 65.9%, and power efficiency of 64.0% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research. </p>
<blockquote>
<p>大规模语言模型（LLM）越来越多地被用于自动化硬件设计任务，包括生成Verilog代码。虽然早期的基准测试主要集中在功能正确性上，但高效的硬件设计需要针对合成指标（如面积、延迟和功耗）进行额外的优化。现有基准测试在全面评估这些方面时存在不足：他们通常缺乏针对验证的优化基准或测试平台。为了弥补这些差距，我们提出了Pluto，一个旨在评估LLM生成Verilog设计效率的基准测试与评估框架。Pluto提供了一套包含114个问题的全面评估集，带有自我检查测试平台和多个Pareto最优参考实现。实验结果表明，最先进的LLM可以达到高达78.3%的高功能正确性（pass@1），但它们在合成效率方面仍然落后于专家设计的实现，面积效率为63.8%，延迟效率为65.9%，功率效率为64.0%（eff@1）。这凸显了需要像Pluto这样的注重效率的评估框架来推动以硬件为中心的LLM研究的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14756v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）已广泛应用于硬件设计自动化任务，包括Verilog代码生成。现有基准测试在评估合成度量（如面积、延迟和功耗）方面存在不足。为解决这些问题，我们提出Pluto，一个用于评估LLM生成Verilog设计效率的基准测试和评估框架。实验结果表明，虽然LLM在功能正确性方面表现出色，但在合成效率方面仍有待提高。这凸显了需要像Pluto这样的效率感知评估框架来推动硬件相关LLM研究的进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）现在被广泛应用于硬件设计自动化，包括Verilog代码生成。</li>
<li>现有基准测试主要关注功能正确性，但硬件设计还需要考虑合成度量，如面积、延迟和功耗的优化。</li>
<li>现有基准测试在评估这些方面存在不足，缺乏优化基准测试和验证测试平台。</li>
<li>提出Pluto基准测试和评估框架，用于评估LLM生成的Verilog设计的效率。</li>
<li>实验结果表明，LLM在功能正确性方面表现出色，但在合成效率方面仍有提升空间。</li>
<li>LLM的合成效率与专家手工实现相比仍有差距，表现在面积效率、延迟效率和功率效率方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14756">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-58f89d2fdb071159c0ce377a954e4e9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725976&auth_key=1760725976-0-0-c826efc134cf7a4a221bbeefd45b0f34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c12f97feed90ab3004571190b88ea9e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725984&auth_key=1760725984-0-0-f954a7fe5a36546468b512ad1f733118&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7ead1398dc20cfb2330b12907002a36~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725991&auth_key=1760725991-0-0-51da046db572714351f42d67b0d12683&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-483d64631c6e49ddf96541820c86a4af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725997&auth_key=1760725997-0-0-4b2c6238efd4a11c4c4209bd4c31913f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1daaa803ce713b4c337f05e7701951f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726004&auth_key=1760726004-0-0-1578dafcb7e4bb268f284277f2f45148&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models"><a href="#DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models" class="headerlink" title="DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models"></a>DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models</h2><p><strong>Authors:Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato</strong></p>
<p>Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier’s decision process without access to training data or ground-truth labels. We demonstrate DEXTER’s flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter">https://github.com/perceivelab/dexter</a>. </p>
<blockquote>
<p>理解和解释机器学习模型的行为对于构建透明和可信赖的AI系统至关重要。我们引入了DEXTER，这是一个无数据的框架，它采用扩散模型和大语言模型来生成视觉分类器的全局文本解释。DEXTER通过优化文本提示来合成能强烈激活目标分类器的类别条件图像。这些合成样本然后用于激发详细的自然语言报告，描述特定类别的决策模式和偏见。与以前的工作不同，DEXTER能够在没有访问训练数据或真实标签的情况下，用自然语言解释分类器的决策过程。我们展示了DEXTER在三项任务中的灵活性——激活最大化、切片发现和去偏以及偏见解释——每项任务都说明了其在揭示视觉分类器内部机制方面的能力。定量和定性评估，包括一项用户研究，表明DEXTER产生的输出准确且易于解释。在ImageNet、Waterbirds、CelebA和FairFaces上的实验证实，DEXTER在全局模型解释和类别层面偏见报告方面优于现有方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/perceivelab/dexter获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14741v1">PDF</a> Accepted to NeurIPS 2025 (spotlight)</p>
<p><strong>Summary</strong><br>机器学习模型的解释性对于构建透明和可信赖的AI系统至关重要。DEXTER框架通过采用扩散模型和大语言模型，生成视觉分类器的全局文本解释。DEXTER通过优化文本提示来合成能够强烈激活目标分类器的类条件图像，并利用这些合成样本引出详细的自然语言报告，描述类特定的决策模式和偏见。不同于以往的工作，DEXTER无需访问训练数据或真实标签即可生成自然语言解释分类器的决策过程。在激活最大化、切片发现与去偏以及偏见解释三个任务上的演示展示了DEXTER揭示视觉分类器内部机制的能力。定量和定性评估以及用户研究显示DEXTER产生的输出准确且易于理解。在ImageNet、Waterbirds、CelebA和FairFaces上的实验证实，DEXTER在全局模型解释和类别级偏见报告方面优于现有方法。代码公开在<a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter%E3%80%82">https://github.com/perceivelab/dexter。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DEXTER是一个数据无关的框架，用于生成视觉分类器的全局文本解释。</li>
<li>它结合了扩散模型和大语言模型来合成类条件图像并生成自然语言报告来解释分类器的决策过程。</li>
<li>DEXTER无需访问训练数据或真实标签即可工作，使其成为一种强大的工具。</li>
<li>DEXTER在多种任务上展示了其能力，包括激活最大化、切片发现与去偏以及偏见解释。</li>
<li>定量和定性评估以及用户研究证明DEXTER的输出准确且易于理解。</li>
<li>在多个数据集上的实验表明，DEXTER在全局模型解释和类别级别偏见报告方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14741">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7da9bc4c36b2a1e029838525c732fe31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726011&auth_key=1760726011-0-0-1f597b681f1d2d9ba01809e9fb03ead1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5487f02bf6fbb21fa801270a588322ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726018&auth_key=1760726018-0-0-df0b76390f0809bc70c38fc4ab9ae595&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AutoRubric-R1V-Rubric-Based-Generative-Rewards-for-Faithful-Multimodal-Reasoning"><a href="#AutoRubric-R1V-Rubric-Based-Generative-Rewards-for-Faithful-Multimodal-Reasoning" class="headerlink" title="AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal   Reasoning"></a>AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal   Reasoning</h2><p><strong>Authors:Mengzhao Jia, Zhihan Zhang, Ignacio Cases, Zheyuan Liu, Meng Jiang, Peng Qi</strong></p>
<p>Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）已经从感知任务迅速发展到复杂的多步骤推理。然而，使用可验证奖励的强化学习（RLVR）往往会导致推理错误，因为只对最终答案的正确性进行奖励。为了解决这一局限性，我们提出了AutoRubric-R1V框架，该框架通过基于自动收集的rubirc生成奖励将RLVR与过程级监督相结合。我们的关键创新在于一种可扩展的自我聚合方法，该方法从成功的轨迹中提炼出一致的推理检查点，能够在无需人工注释或更强大的教师模型的情况下构建特定问题的rubirc。通过联合利用rubirc和结果奖励，AutoRubric-R1V在六个多模态推理基准测试上达到了最新技术水平，并在专项评估中大大提高了推理的忠实度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14738v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型多模态语言模型（MLLMs）已从感知任务迅速发展到复杂的多步骤推理，然而强化学习加可验证奖励（RLVR）通常只奖励最终答案的正确性，导致出现错误推理。为解决此限制，我们提出AutoRubric-R1V框架，它结合了RLVR与基于自动收集评分标准的过程级监督来生成奖励。我们的关键创新在于采用可扩展的自我聚合方法，从成功的轨迹中提炼出一致的推理检查点，实现特定问题的评分标准构建，无需人工标注或更强大的教师模型。通过联合利用基于评分标准的奖励和结果奖励，AutoRubric-R1V在六个多模态推理基准测试中实现了最佳性能，并在专项评估中大大提高了推理的忠实度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）已扩展到复杂多步骤推理。</li>
<li>强化学习加可验证奖励（RLVR）存在只奖励最终答案正确性的局限，导致可能出现错误推理。</li>
<li>提出AutoRubric-R1V框架，结合RLVR与过程级监督，通过自动收集评分标准生成奖励。</li>
<li>AutoRubric-R1V的关键创新在于采用自我聚合方法提炼推理检查点，实现特定问题的评分标准构建，无需人工标注或强大教师模型。</li>
<li>该框架实现了在六个多模态推理基准测试中的最佳性能。</li>
<li>AutoRubric-R1V提高了在专项评估中的推理忠实度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14738">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-389ae6802a4527d0076c096ad5671dfe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726026&auth_key=1760726026-0-0-9bc35051a7d25e4b69c9d7c05ce28afa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd11cd2b9fbb25decd60857e98717d10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726034&auth_key=1760726034-0-0-c284bc43835291efe25bd92a08c1cbca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-25a280213bdf756d0bfa98baf3b90ee4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726042&auth_key=1760726042-0-0-83ccb2fb486a838d67fea3d1f53d5c9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b691f67bddc6357224212d9899de288f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726049&auth_key=1760726049-0-0-1ee72e1409ba1928f3e273c125fa361a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VTimeCoT-Thinking-by-Drawing-for-Video-Temporal-Grounding-and-Reasoning"><a href="#VTimeCoT-Thinking-by-Drawing-for-Video-Temporal-Grounding-and-Reasoning" class="headerlink" title="VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning"></a>VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</h2><p><strong>Authors:Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma</strong></p>
<p>In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: <a target="_blank" rel="noopener" href="https://vtimecot.github.io/">https://vtimecot.github.io</a> </p>
<blockquote>
<p>近年来，基于多模态大型语言模型（MLLM）的视频问答引起了广泛关注，这得益于大型语言模型的巨大进步所带来的好处。然而，这些模型在视频时间定位和推理领域存在明显缺陷，给发展有效的现实世界视频理解系统带来了挑战。受人类如何使用视频播放器与进度条进行视频理解的启发，我们推出了VTimeCoT，这是一个简单而高效、无需训练的培训框架，专为高性能视频定位和推理而设计。该框架引入了进度条的两种新型视觉工具：即插即用的进度条集成工具和高效高亮工具。此外，为了解决基于文本的链式思维（CoT）方法的局限性，我们引入了视觉时间CoT过程，该过程融合了视频和文本之间的跨模态推理。我们的方法在视频时间定位和基于推理的问题回答任务中，显著提高了在Qwen2VL-7B和GPT4o基线任务上的性能。最后，我们展示了所提出的框架实现了组合和可解释的推理过程。项目页面：<a target="_blank" rel="noopener" href="https://vtimecot.github.io/">https://vtimecot.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14672v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>多媒体模态的大型语言模型在视频问答领域受到广泛关注，但仍存在视频时空定位与推理方面的挑战。为此，本文提出一种名为VTimeCoT的训练免框架，通过利用视频播放器中的进度条工具，实现高性能的视频定位与推理。该框架引入两种新颖的视觉工具并改进传统基于文本的思考链方法，实现在视频和文本之间的跨模态推理。在Qwen2VL-7B和GPT4o基准测试中，该框架在视频时空定位与基于推理的问题回答方面表现出显著性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频问答领域基于多媒体模态的大型语言模型受到关注，但仍存在视频时空定位与推理的挑战。</li>
<li>VTimeCoT框架是一种训练免框架，能够利用视频播放器中的进度条工具实现高性能的视频定位与推理。</li>
<li>VTimeCoT框架引入两种新颖的视觉工具：进度条集成工具和高效高亮工具。</li>
<li>该框架改进了传统基于文本的思考链方法，实现跨模态推理，结合视频和文本数据。</li>
<li>在基准测试中，VTimeCoT框架在视频时空定位与基于推理的问题回答方面表现出显著性能提升。</li>
<li>VTimeCoT框架实现了组合和可解释的推理过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-803a1824e93e3aace39416e4585d658b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726056&auth_key=1760726056-0-0-eb7ad1265066346bfc9a92f56254ad5c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2cee3b97e88a7428b3a357bd74d5d3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726063&auth_key=1760726063-0-0-10756b807de8374e2d3007a6923a8a3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-991727758a3ebe7cf043f4c90362fcb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726069&auth_key=1760726069-0-0-d0c2f22bde6488cd186039a3dcf4c4d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b649e52c6a5da695701b53cf48a35d74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726076&auth_key=1760726076-0-0-08478e530a28c2409ff3b33732a3be9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MR-Rec-Synergizing-Memory-and-Reasoning-for-Personalized-Recommendation-Assistant-with-LLMs"><a href="#MR-Rec-Synergizing-Memory-and-Reasoning-for-Personalized-Recommendation-Assistant-with-LLMs" class="headerlink" title="MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation   Assistant with LLMs"></a>MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation   Assistant with LLMs</h2><p><strong>Authors:Jiani Huang, Xingchen Zou, Lianghao Xia, Qing Li</strong></p>
<p>The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification. </p>
<blockquote>
<p>将大型语言模型（LLM）应用于推荐系统面临着实现深度个性化和智能推理的关键挑战，特别是在交互式场景中。当前的方法常常受限于有限的上下文窗口和单轮推理，无法捕捉动态的用户偏好和在推荐上下文中进行主动推理。为了解决这些局限性，我们提出了MR.Rec，一个结合记忆和推理的新型LLM推荐框架。为了实现个性化，我们开发了一个全面的增强检索生成（RAG）系统，该系统能够高效地索引和检索相关的外部记忆，以增强LLM的个性化能力。此外，为了实现记忆和推理之间的协同作用，我们的RAG系统超越了传统的基于查询的检索，通过整合增强推理的记忆检索。最后，我们设计了一个强化学习框架，训练LLM自主学习有效的策略来进行记忆利用和推理优化。通过结合动态记忆检索和自适应推理，这种方法确保了更准确、具有上下文意识和高度的个性化推荐。大量实验表明，MR.Rec在多个指标上显著优于最新技术水平的基准测试，验证了其在提供智能和个性化推荐方面的有效性。论文通知发布后，我们将公开代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在推荐系统中的应用面临个性化深度与智能推理的挑战，特别是在交互式场景中。为解决这些问题，我们提出了MR.Rec框架，通过记忆与推理的协同工作来提升LLM的推荐能力。我们开发了一个全面的基于检索的生成（RAG）系统，通过有效地索引和检索相关外部记忆，增强LLM的个性化能力。同时，我们设计了一个强化学习框架，训练LLM自主学习有效的记忆利用和推理优化策略。通过结合动态记忆检索与自适应推理，确保更精准、情境感知和个性化的推荐。实验证明，MR.Rec在多个指标上显著优于现有技术，验证了其在提供智能与个性化推荐方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs在推荐系统中面临个性化与智能推理的挑战。</li>
<li>MR.Rec框架通过记忆与推理的协同工作提升LLM的推荐能力。</li>
<li>RAG系统通过检索外部记忆增强LLM的个性化能力。</li>
<li>强化学习框架用于训练LLM在记忆利用和推理优化方面的策略。</li>
<li>结合动态记忆检索与自适应推理，确保精准、情境感知和个性化的推荐。</li>
<li>MR.Rec在多个指标上显著优于现有技术。</li>
<li>将发布代码和数据以供论文通知使用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-aeb19ce6d2398ccde531923a20f210b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726084&auth_key=1760726084-0-0-ba9f50d9d77c1d4b564fa5fd85b0ac11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4af957f63b08c2752075fc2c2f13d7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726092&auth_key=1760726092-0-0-a53a65026dc21de436c0cc649fc316cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-095481680595c7dddaa766229f4835e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726099&auth_key=1760726099-0-0-38b9b3b28287d28515ac77424cebf036&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering"><a href="#Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering" class="headerlink" title="Knowledge-based Visual Question Answer with Multimodal Processing,   Retrieval and Filtering"></a>Knowledge-based Visual Question Answer with Multimodal Processing,   Retrieval and Filtering</h2><p><strong>Authors:Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye</strong></p>
<p>Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model’s reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/cqu-student/Wiki-PRF">https://github.com/cqu-student/Wiki-PRF</a> </p>
<blockquote>
<p>基于知识的视觉问答（KB-VQA）需要视觉语言模型（VLM）将视觉理解与外部知识检索相结合。尽管通过结合知识库查询的检索增强生成（RAG）在此任务上取得了重大进展，但它仍然面临着多模态查询的质量和检索结果的相关性方面的挑战。为了克服这些挑战，我们提出了一种新的三阶段方法，称为Wiki-PRF，包括处理、检索和过滤三个阶段。处理阶段动态调用视觉工具来提取用于检索的精确多模态信息。检索阶段融合了视觉和文本特征，以实现多模态知识检索。过滤阶段对检索结果进行相关性过滤和集中。为此，我们引入了一个视觉语言模型，该模型通过强化学习方式以答案准确性和格式一致性作为奖励信号进行训练。这增强了模型的推理能力、工具调用以生成准确查询以及过滤无关内容的能力。在基准数据集（E-VQA和InfoSeek）上的实验显示，在答案质量方面取得了显著改进（分别为36.0和42.8），达到了最新技术水平。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/cqu-student/Wiki-PRF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cqu-student/Wiki-PRF找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14605v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>基于知识的视觉问答（KB-VQA）需要视觉语言模型（VLMs）整合视觉理解与外部知识检索。尽管检索增强生成（RAG）在此任务上取得了显著进展，但在多媒体查询的质量和检索结果的相关性方面仍存在挑战。为此，我们提出了一种新的三阶段方法，称为Wiki-PRF，包括处理、检索和过滤三个阶段。处理阶段动态调用视觉工具提取精确的多媒体信息进行检索；检索阶段融合视觉和文本特征实现多媒体知识检索；过滤阶段对检索结果进行相关性过滤和集中。为此，我们引入了一个视觉语言模型，通过强化学习方式以答案准确性和格式一致性作为奖励信号进行训练，提高了模型的推理能力、工具调用精准查询以及过滤无关内容的能力。在E-VQA和InfoSeek等基准数据集上的实验表明，在答案质量方面取得了显著改进（分别提高了36.0和42.8），达到了业界领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KB-VQA任务需要整合视觉理解与外部知识检索，依赖视觉语言模型（VLMs）。</li>
<li>检索增强生成（RAG）在多媒体查询的质量和检索结果的相关性方面存在挑战。</li>
<li>Wiki-PRF是一种新的三阶段方法，包括处理、检索和过滤，针对以上挑战进行改进。</li>
<li>处理阶段动态调用视觉工具提取精确信息。</li>
<li>检索阶段融合视觉和文本特征进行多媒体知识检索。</li>
<li>过滤阶段对检索结果进行相关性过滤和集中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14605">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d41e079b15f1690f8bdec842f016a4e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726106&auth_key=1760726106-0-0-aca2a9f72b9739dc15ba1b56c6f1ff2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0728f7ecdf9af3788e4197cb6c8c27c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726114&auth_key=1760726114-0-0-32f8d90a8c08144df54d030177739466&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cbe9f65a1b7514bf1214943f218a444d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726121&auth_key=1760726121-0-0-07863da92e7f49519c5810e981ec7ce8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Talking-Points-Describing-and-Localizing-Pixels"><a href="#Talking-Points-Describing-and-Localizing-Pixels" class="headerlink" title="Talking Points: Describing and Localizing Pixels"></a>Talking Points: Describing and Localizing Pixels</h2><p><strong>Authors:Matan Rusanovsky, Shimon Malnick, Shai Avidan</strong></p>
<p>Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points">https://github.com/matanr/Talking_Points</a>. </p>
<blockquote>
<p>视觉语言模型在多模态理解方面取得了显著的成功。然而，这些模型仍然局限于对象级别或区域级别的定位，缺乏通过自然语言进行像素精确关键点理解的能力。我们引入了一种新颖的像素级别定位框架。该框架由两个互补的组件构成：一个点描述符，用于生成单个关键点的丰富上下文描述；一个点定位器，用于从这些描述中回归精确的像素坐标。与以往依赖于模板提示或关键点名称的方法不同，我们的方法能够生成自由形式、从粗到细的描述，将关键点置于其视觉上下文中。由于目前没有可用的数据集来训练这样的系统，我们引入了LlamaPointInPart数据集，这是一个精心策划的由2万多个图像-关键点-描述三元组合成的数据集，从场景级别的上下文到关键点的视觉特征，捕捉多尺度信息。为了实现跨类别的泛化，我们使用GRPO优化了AP-10K上的点描述符，并使用冻结的点定位器作为奖励模型，以产生最大化定位精度的描述。为了评估我们的结果，我们建立了一个新的评估协议。不同于将我们的方法产生的文本描述与真实标签进行比较，我们使用定位器来确定预测点与真实点之间的接近程度。在LlamaPointInPart数据集上的实验表明，与基线模型相比，我们的方法具有优越的性能。我们框架的双向性质将有助于未来在关键点引导的图像理解和语言引导的精确定位方面的应用。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/matanr/Talking_Points公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的像素级定位框架，该框架包含两个互补组件：点描述符和点定位器。框架可以实现通过自然语言对像素级关键点的精确理解，且生成丰富、上下文的单个关键点描述。由于缺少可用的数据集来训练此类系统，研究人员引入了LlamaPointInPart数据集，该数据集由多个视觉语言模型合成，包含超过两万张图像、关键点和描述三元组，能够捕捉从场景级上下文到关键点周围视觉特征的多尺度信息。实验结果证明，相较于基线模型，该框架在LlamaPointInPart数据集上的表现更优。代码和数据集已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种新的像素级定位框架，能实现通过自然语言进行像素精确的关键点理解。</li>
<li>包含点描述符和点定位器两个互补组件，分别负责生成关键点的丰富描述和回归精确像素坐标。</li>
<li>引入了LlamaPointInPart数据集，用于训练此类系统，包含图像、关键点和描述的三元组。</li>
<li>通过合成方法从多个视觉语言模型中捕获多尺度信息。</li>
<li>优化点描述符在AP-10K上的性能，使用冻结的点定位器作为奖励模型，以最大化定位准确性。</li>
<li>建立新的评估协议，不再比较方法产生的文本描述与地面真实情况，而是使用定位器来确定预测点与地面真实点之间的距离。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-652edad153a670e8ef96c41230c0dec3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726128&auth_key=1760726128-0-0-39e8c501e6c35aad83761d8a0b6b187f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4cf2fa892aa95fc6f20cac393650257f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726138&auth_key=1760726138-0-0-bbc288d0396c38a49041097d88f514ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40a8a230e32586222cc7cab5d40aa88b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726145&auth_key=1760726145-0-0-6298d11016d389da12a532eb48d268fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a061fcf875e5f5c403e7da25446dd445~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726152&auth_key=1760726152-0-0-73a9428296ed795e73871ec8f1549d20&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Launching-AoyuX-A-25-Year-Pseudo-Prospective-Earthquake-Forecasting-Experiment-at-the-China-Seismic-Experimental-Site"><a href="#Launching-AoyuX-A-25-Year-Pseudo-Prospective-Earthquake-Forecasting-Experiment-at-the-China-Seismic-Experimental-Site" class="headerlink" title="Launching AoyuX: A 25-Year Pseudo-Prospective Earthquake Forecasting   Experiment at the China Seismic Experimental Site"></a>Launching AoyuX: A 25-Year Pseudo-Prospective Earthquake Forecasting   Experiment at the China Seismic Experimental Site</h2><p><strong>Authors:Jiawei Li, Qingyuan Zhang, Didier Sornette</strong></p>
<p>Forecast models in statistical seismology are commonly evaluated with log-likelihood scores of the full distribution P(n) of earthquake numbers, yet heavy tails and out-of-range observations can bias model ranking. We develop a tail-aware evaluation framework that estimates cell-wise P(n) using adaptive Gaussian kernel density estimation and tests three strategies for handling out-of-range counts. Using the AoyuX platform, we perform a ~25-year month-by-month pseudo-prospective forecast experiment in the China Seismic Experimental Site (CSES), comparing Epidemic-Type Aftershock Sequence (ETAS) model with a homogeneous background (ETAS{\mu}) to a spatially heterogeneous variant (ETAS{\mu}(x,y)) across six spatial resolutions and five magnitude thresholds. Empirical probability density functions (PDFs) of counts per cell are well described by power laws with exponents a &#x3D; 1.40 +- 0.21 across all settings. Using previous theoretical results, this provides a robust estimate of the productivity exponent, {\alpha} &#x3D; 0.57 +- 0.08 using a b-value equal to 0.8, providing a valuable quantification of this key parameter in aftershock modeling. Model ranking is sensitive to how the tail of the full distribution P(n) of earthquake counts is treated: power law extrapolation is both theoretically justified and empirically the most robust. Cumulative information gain (CIG) shows that ETAS{\mu}(x,y) outperforms ETAS{\mu} in data-rich configurations, whereas in data-poor settings stochastic fluctuations dominate. A coefficient-of-variation analysis of per-window log-likelihood differences distinguishes genuine upward trends in CIG from noise-dominated fluctuations. By aligning a fat-tail-aware scoring methodology with an open testing platform, our work advances fair and statistically grounded assessment of earthquake forecasting models for the CSES and beyond. </p>
<blockquote>
<p>在统计地震学中，通常使用对数似然分数来评估地震次数全分布P(n)的预测模型，但尾部偏重和观测范围外的数据可能会影响模型排名。我们开发了一个尾部感知评估框架，该框架使用自适应高斯核密度估计来估算单元级的P(n)，并采用了三种策略来处理观测范围外的数据。我们利用AoyuX平台，在中国地震实验区（CSES）进行了长达约25个月的逐月伪前瞻性预测实验，比较了具有同质背景的流行型余震序列（ETASμ）模型与空间异质的ETASμ（x，y）模型，涉及六种空间分辨率和五种震级阈值。每个单元的计数经验概率密度函数（PDF）很好地遵循幂律，指数为a&#x3D;1.40±0.21。根据之前的理论结果，使用b值等于0.8时，这提供了生产力指数α&#x3D;0.57±0.08的稳健估计值，为余震建模中这一关键参数提供了有价值的量化指标。模型排名对地震计数全分布P(n)尾部处理方式敏感：幂律外推既有理论支持又在实践中最为稳健。累积信息增益（CIG）显示，在数据丰富的配置中，ETASμ（x，y）的表现优于ETASμ；而在数据稀缺的环境中，随机波动占据主导。窗口对数似然差异的变化系数分析将累积信息增益中的真正上升趋势与噪声主导的波动区分开来。通过采用兼顾肥尾感知评分方法和开放测试平台的方式，我们的研究为CSES及更广泛领域的地震预测模型提供了公平且统计上健全的评价依据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14407v1">PDF</a> 30 pages, 7 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>本文研究了统计地震学中的预测模型评估问题。针对地震次数全分布P(n)的log-likelihood评分，由于重尾和超出范围观测可能导致模型排名偏差，提出了一种尾部感知评估框架。利用AoyuX平台，在中国地震实验站（CSES）进行了约25年的月度伪前瞻性预测实验，对比了ETAS模型的不同版本在不同空间分辨率和震级阈值下的表现。模型排名对地震计数全分布P(n)的尾部处理方式敏感，采用幂律外推既合理又稳健。累计信息增益（CIG）显示，在数据丰富的情况下，ETASμ(x,y)优于ETASμ；而在数据稀缺的情况下，随机波动占主导。本研究将胖尾感知评分方法与开放测试平台相结合，推动了CSES和更广泛领域的地震预测模型的公平、统计基础的评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重尾和超出范围观测会影响统计地震学预测模型的评估。</li>
<li>提出了一种尾部感知评估框架，利用自适应高斯核密度估计进行细胞级的P(n)估计。</li>
<li>在中国地震实验站进行了长期实验，对比了不同版本的ETAS模型在不同空间分辨率和震级阈值下的表现。</li>
<li>模型排名对处理地震计数全分布的尾部方式敏感，采用幂律外推既合理又稳健。</li>
<li>在数据丰富的情况下，ETASμ(x,y)表现优于ETASμ；而在数据稀缺时，随机波动影响显著。</li>
<li>累积信息增益是衡量模型表现的有效指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14407">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-49cb2db5a3e34fefcfdfb9fe83b6a9d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726159&auth_key=1760726159-0-0-6a23f4c3e4ecaa5cc08a0b018bf9c712&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Hi-Agent-Hierarchical-Vision-Language-Agents-for-Mobile-Device-Control"><a href="#Hi-Agent-Hierarchical-Vision-Language-Agents-for-Mobile-Device-Control" class="headerlink" title="Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control"></a>Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control</h2><p><strong>Authors:Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi</strong></p>
<p>Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios. </p>
<blockquote>
<p>构建能够自主操作移动设备的智能代理已引起越来越多的关注。尽管视觉语言模型（VLMs）显示出潜力，但大多数现有方法依赖于直接的状态到动作映射，这缺乏结构化推理和规划，因此在面对新任务或未见的用户界面布局时泛化能力较差。我们引入了Hi-Agent，这是一种用于移动控制的可训练分层视觉语言代理，它具备高级推理模型和低级动作模型，并进行了联合优化。为了进行有效的训练，我们将多步决策重新制定为一系列单步子目标，并提出了一种前瞻性优势函数，该函数利用低级模型的执行反馈来指导高级优化。这种设计缓解了长周期任务中群体相对策略优化（GRPO）所面临的路径爆炸问题，实现了稳定且无评价的联合训练。Hi-Agent在Android-in-the-Wild（AitW）基准测试上达到了新的最高任务成功率（87.9%），在三种范式下均显著优于先前的方法：基于提示的（AppAgent: 17.7%）、基于监督的（Filtered BC: 54.5%）和基于强化学习的（DigiRL: 71.9%）。在ScreenSpot-v2基准测试上，它也展现出具有竞争力的零样本泛化能力。在更具挑战性的AndroidWorld基准测试上，Hi-Agent在较大的骨干网架构下也能有效扩展，显示出在高复杂度移动控制场景中的强大适应性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14388v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>提出了一种名为Hi-Agent的可训练分层视觉语言移动控制代理，包含高级推理模型和低级动作模型，并进行了联合优化。为解决长期任务中的路径爆炸问题，提出前瞻优势函数，利用低级模型的执行反馈引导高级优化。在Android-in-the-Wild基准测试中，Hi-Agent实现了新的最先进的任务成功率，并在其他两个基准测试中表现出卓越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Hi-Agent是一个自主操作移动设备的可训练分层视觉语言代理。</li>
<li>它包含高级推理模型和低级动作模型，并进行了联合优化。</li>
<li>提出前瞻优势函数，解决长期任务中的路径爆炸问题。</li>
<li>Hi-Agent在Android-in-the-Wild基准测试上达到了新的最先进的任务成功率。</li>
<li>与其他方法相比，Hi-Agent在三个基准测试中表现出卓越性能。</li>
<li>Hi-Agent在更复杂的AndroidWorld基准测试上也能有效地扩展，显示出在高复杂度移动控制场景中的强适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14388">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8f37274c0431bb765a91b4c47f86a3b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726167&auth_key=1760726167-0-0-8c6b1a2e2bd730b495a291dad8376544&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c6e438e670285adb03657a47cc31d29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726174&auth_key=1760726174-0-0-d0ff3395edeff21b4fad0c69e03e2609&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08ed6abfa7b780cd9a6823e811c18a61~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726181&auth_key=1760726181-0-0-b387fb33d61186492bb621706cfaa58f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab9c36e885c0f70378baa8e96a5f47c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726188&auth_key=1760726188-0-0-3cea8654e6a9f50a23b23c90252b4a78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CURE-Confidence-driven-Unified-Reasoning-Ensemble-Framework-for-Medical-Question-Answering"><a href="#CURE-Confidence-driven-Unified-Reasoning-Ensemble-Framework-for-Medical-Question-Answering" class="headerlink" title="CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical   Question Answering"></a>CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical   Question Answering</h2><p><strong>Authors:Ziad Elshaer, Essam A. Rashed</strong></p>
<p>High-performing medical Large Language Models (LLMs) typically require extensive fine-tuning with substantial computational resources, limiting accessibility for resource-constrained healthcare institutions. This study introduces a confidence-driven multi-model framework that leverages model diversity to enhance medical question answering without fine-tuning. Our framework employs a two-stage architecture: a confidence detection module assesses the primary model’s certainty, and an adaptive routing mechanism directs low-confidence queries to Helper models with complementary knowledge for collaborative reasoning. We evaluate our approach using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework achieves competitive performance, with particularly strong results in PubMedQA (95.0%) and MedMCQA (78.0%). Ablation studies confirm that confidence-aware routing combined with multi-model collaboration substantially outperforms single-model approaches and uniform reasoning strategies. This work establishes that strategic model collaboration offers a practical, computationally efficient pathway to improve medical AI systems, with significant implications for democratizing access to advanced medical AI in resource-limited settings. </p>
<blockquote>
<p>高性能医疗大语言模型（LLM）通常需要大量的微调以及大量的计算资源，这限制了资源受限的医疗机构对其的访问。本研究引入了一种基于信心的多模型框架，该框架利用模型多样性增强医疗问答功能，无需微调。我们的框架采用两阶段架构：信心检测模块评估主模型的确定性，自适应路由机制将低信心查询引导至具有补充知识的辅助模型进行协同推理。我们使用Qwen3-30B-A3B-Instruct、Phi-4 14B和Gemma 2 12B，在三个医学基准测试MedQA、MedMCQA和PubMedQA上评估了我们的方法。结果表明，我们的框架实现了具有竞争力的性能，特别是在PubMedQA（95.0%）和MedMCQA（78.0%）中取得了显著成效。消融研究证实，结合信心感知路由和多模型协作的方法显著优于单模型方法和统一推理策略。这项工作证明，战略模型协作为改进医疗AI系统提供了一条实用且计算高效的途径，对于在资源有限的环境中普及先进的医疗AI具有重大意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14353v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究提出了一种基于信心的多模型框架，该框架利用模型多样性提高医疗问答性能，无需精细调整。该框架采用两阶段架构，首先通过信心检测模块评估主要模型的确定性，然后通过自适应路由机制将低信心查询引导至具有补充知识的辅助模型进行协同推理。在三个医疗基准测试（MedQA、MedMCQA和PubMedQA）上的评估结果表明，该框架取得了具有竞争力的表现，特别是在PubMedQA（95.0%）和MedMCQA（78.0%）上表现突出。此外，消融研究证实，结合信心感知路由和多模型协同的方法显著优于单模型方法和均匀推理策略。这项研究为在资源受限环境中实现医疗人工智能系统的民主化访问提供了实用、计算高效的途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高性能医疗语言大模型通常需要大量的精细调整和计算资源，这限制了资源受限医疗机构的使用。</li>
<li>本研究提出了一种基于信心的多模型框架，用于提高医疗问答的准确度，且无需精细调整。</li>
<li>该框架采用两阶段架构，包括信心检测模块和自适应路由机制。</li>
<li>信心检测模块评估主要模型的确定性，而自适应路由机制则处理低信心查询，通过引导查询至辅助模型进行协同推理。</li>
<li>在多个医疗基准测试上的评估结果表明该框架具有竞争力，特别是在PubMedQA和MedMCQA上的表现突出。</li>
<li>消融研究证明了信心感知路由与多模型协同的显著优势，优于单模型方法和均匀推理策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14353">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d2d89f96e17bc9903dbbfaa3f4780f5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726195&auth_key=1760726195-0-0-c0f61f99558b64358a33ca9a033495a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ea1b0cbfa29fd82aa87f12695f5fe51~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726202&auth_key=1760726202-0-0-fb2ed6d03321c038efc36925f6cf3428&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MathMist-A-Parallel-Multilingual-Benchmark-Dataset-for-Mathematical-Problem-Solving-and-Reasoning"><a href="#MathMist-A-Parallel-Multilingual-Benchmark-Dataset-for-Mathematical-Problem-Solving-and-Reasoning" class="headerlink" title="MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical   Problem Solving and Reasoning"></a>MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical   Problem Solving and Reasoning</h2><p><strong>Authors:Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda</strong></p>
<p>Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned question-answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMs’ ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: <a target="_blank" rel="noopener" href="https://github.com/mahbubhimel/MathMist">https://github.com/mahbubhimel/MathMist</a> </p>
<blockquote>
<p>文本中的数学推理仍然是大型语言模型（LLM）最具挑战性的领域之一，它不仅需要语言理解，还需要结构化的逻辑推导和数值精确性。虽然最近的大型语言模型表现出强大的通用推理能力，但它们在多种语言中的数学能力仍然未得到充分探索。现有的基准测试主要集中在英语或资源丰富的狭窄语言子集上，在评估多语种和跨语言数学推理方面存在很大差距。为了解决这一问题，我们推出了MathMist，这是一个用于数学问题解决和推理的并行多语言基准测试。MathMist涵盖了超过21K个跨七种语言的对齐问答对，代表了高、中、低资源语言环境的平衡覆盖。该数据集捕捉了语言的多样性、多种问题设置和解决方案综合能力。我们系统地评估了一系列模型，包括开源的小型和中型LLM、专有系统以及面向多语言推理的模型，在零样本、思维链（CoT）和代码切换推理范式下进行评价。我们的结果表明，大型语言模型在执行跨语言的连贯和可解释数学推理方面存在持续缺陷，在低资源环境中这种退化更为明显。所有代码和数据都可在GitHub上找到：<a target="_blank" rel="noopener" href="https://github.com/mahbubhimel/MathMist">https://github.com/mahbubhimel/MathMist</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14305v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>大型语言模型在数学推理方面存在挑战，需要语言理解和结构化逻辑演绎和数值精确度。现有基准测试主要集中在英语或少数高资源语言上，评估多语言跨语言的数学推理能力存在差距。为解决此问题，我们推出了MathMist，一个用于数学问题解决和推理的平行多语言基准测试。MathMist包含超过21K个跨七种语言的问题答案对，代表高、中、低资源语言环境的平衡覆盖。我们系统地评估了一系列模型，包括开源的小型和中型LLM、专有系统和多语言推理重点模型，在零样本、思维链和代码切换推理范式下，结果揭示LLM在数学推理方面的持续缺陷，特别是在低资源环境下。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在数学推理领域面临挑战，需兼具语言理解和结构化逻辑推演及数值精确度。</li>
<li>现有基准测试主要集中在英语等少数高资源语言上，缺乏多语言和跨语言的数学推理评估。</li>
<li>MathMist是一个用于数学问题解决和推理的平行多语言基准测试，覆盖高、中、低资源语言的平衡问题答案对。</li>
<li>MathMist数据集捕捉了语言多样性、多种问题设置和解决方案综合能力。</li>
<li>对一系列模型进行的系统评估揭示了大型语言模型在数学推理方面的持续不足。</li>
<li>在低资源环境下，大型语言模型的性能显著下降。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14305">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bb8191ce2e8484da3bdf8f25cf9718eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726209&auth_key=1760726209-0-0-d0651cf22c271e222aec09f4f7d3e97d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79ab063a78d7bc66f7de557ac37b3c0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726216&auth_key=1760726216-0-0-39d68e2c25a82b9fa42fa2db3efd4ba0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ff099a9bc245e2761d04419abf5e5a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726226&auth_key=1760726226-0-0-2776678903a0bc17389e47a966e0b66d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-508bb1b513c1121386fc2ee780c8c76d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726233&auth_key=1760726233-0-0-88ee4fd22f184d7ed69b0fd718effb4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fda0695894064839c36a38e60e1fa15c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726240&auth_key=1760726240-0-0-7cb26efff813a344d06aa2feaa1a0332&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading"><a href="#AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading" class="headerlink" title="AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement   Learning Framework for Stock Trading"></a>AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement   Learning Framework for Stock Trading</h2><p><strong>Authors:Zheye Deng, Jiashu Wang</strong></p>
<p>While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/AlphaQuanter/AlphaQuanter">https://github.com/AlphaQuanter/AlphaQuanter</a> </p>
<blockquote>
<p>虽然大型语言模型（LLM）代理在自动化交易领域显示出潜力，但它们仍然面临重大局限性。显著的多代理框架通常存在效率低下、产生信号不一致以及缺乏从市场反馈中学习连贯策略所需的端到端优化等问题。为了解决这一问题，我们推出了AlphaQuanter，这是一个单代理框架，它使用强化学习（RL）来学习在透明且工具增强的决策工作流程上的动态策略，这使得单个代理能够自主地协调工具并按需主动获取信息，建立一个透明且可审核的推理过程。大量实验表明，AlphaQuanter在关键财务指标上达到了最先进的性能。此外，其可解释的推理揭示了复杂的策略，为人类交易者提供了新颖且有价值的见解。我们的数据收集和代理训练代码在：<a target="_blank" rel="noopener" href="https://github.com/AlphaQuanter/AlphaQuanter%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8">https://github.com/AlphaQuanter/AlphaQuanter公开可用</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14264v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的自动化交易虽然展现出潜力，但仍面临诸多挑战。当前主流的多智能体框架存在效率低下、信号不一致等问题，缺乏从市场反馈中学习连贯策略的能力。为解决这些问题，我们推出AlphaQuanter单一智能体框架，采用强化学习（RL）学习动态策略，通过透明的工具增强决策流程，使单一智能体能够自主协调工具并主动按需获取信息，建立透明可审计的推理过程。实验表明，AlphaQuanter在关键财务指标上取得了最新技术水平，其可解释的推理揭示了复杂策略，为人工交易者提供了有价值的新见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自动化交易领域展现出潜力，但仍面临挑战。</li>
<li>当前主流的多智能体框架存在效率低下和信号不一致的问题。</li>
<li>AlphaQuanter是一个新的单一智能体框架，采用强化学习来学习动态策略。</li>
<li>AlphaQuanter使用透明的工具增强决策流程，智能体能自主协调工具并主动获取信息。</li>
<li>AlphaQuanter实现了关键财务指标上的最新技术水平表现。</li>
<li>AlphaQuanter的推理过程透明可审计。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14264">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d16f1595f4c89ac746222115275c50f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726247&auth_key=1760726247-0-0-0939f91f1a20590194b6cb674a8ed16d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9e748b5bab5883ad81c28de240196f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726254&auth_key=1760726254-0-0-f9e88120109aa66cf43a3f3815a936b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bd56ec27b74df555f47ef1705a3c756~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726261&auth_key=1760726261-0-0-e7c445355ebd6bfa059747bd27c09b1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-553e9314c98b68e5121f2e433136f4b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726270&auth_key=1760726270-0-0-1bb25baf31b5e3a7e007ea88a0daff41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8bac441c3e7f1c7b47caa2131ef6fd1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726276&auth_key=1760726276-0-0-f47ceaf6a0800f149beb06cf807138f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reasoning-in-Space-via-Grounding-in-the-World"><a href="#Reasoning-in-Space-via-Grounding-in-the-World" class="headerlink" title="Reasoning in Space via Grounding in the World"></a>Reasoning in Space via Grounding in the World</h2><p><strong>Authors:Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, Peidong Liu</strong></p>
<p>In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance. </p>
<blockquote>
<p>在这篇论文中，我们主张3D视觉定位（grounding）是空间推理的基石，并引入基于定位的时空推理器（GS-Reasoner）来探索有效的空间表示，以缩小它们之间的差距。现有的大型语言模型（LLM）在处理三维数据时缺乏统一的三维表示能力，无法同时捕捉语义和几何信息。这一缺陷表现为定位性能不佳或对外部模块的过度依赖，最终阻碍了无缝整合定位和空间推理。为了解决这个问题，我们提出了一种简单有效的双路径池化机制，紧密对齐几何特征与语义和位置线索，构建了一个基于图像块的统一三维表示，它能涵盖所有关键信息而无需增加输入令牌的数量。借助这种全面的表示，GS-Reasoner成为首个能在无需外部模块的情况下实现完全自动定位的三维大型语言模型，同时其性能与最先进的模型相当，为三维空间推理建立了统一、自足的框架。为了进一步弥合定位和空间推理之间的差距，我们引入了基于定位的思维链（GCoT）数据集。该数据集精心挑选包含三维边界框标注的对象以及推理问题中的逐步推理路径，并将定位作为问题解决过程的核心组成部分。大量实验表明，GS-Reasoner在三维视觉定位方面取得了令人印象深刻的结果，这反过来又显著增强了其空间推理能力，达到了最先进的性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13800v2">PDF</a> 20 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出3D视觉定位是空间推理的核心，并引入GS-Reasoner来探索有效的空间表示方法，以弥合它们之间的差距。现有的3D大型语言模型缺乏统一的3D表示能力，无法同时捕捉语义和几何信息，导致定位性能不佳或过度依赖外部模块，阻碍了定位和空间推理的无缝集成。为解决这一问题，我们提出了一种简单有效的双路径池化机制，紧密对齐几何特征与语义和位置线索，构建了一个统一的基于图像补丁的3D表示，而无需增加输入标记的数量。利用这种整体表示，GS-Reasoner成为首个在定位方面完全不需要外部模块的自回归3D大型语言模型，同时在保持与最新模型性能相当的情况下，为3D空间推理建立了一个统一、自我包含的框架。为了弥合定位和空间推理之间的差距，我们还引入了GCoT数据集。该数据集精心挑选，包括用于参考推理问题的3D边界框注释和集成定位作为问题解决过程核心组件的逐步推理路径。实验表明，GS-Reasoner在3D视觉定位方面取得了令人印象深刻的结果，进而显著增强了其空间推理能力，达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D视觉定位是空间推理的核心。</li>
<li>现有3D大型语言模型缺乏统一的3D表示能力，无法捕捉语义和几何信息。</li>
<li>GS-Reasoner通过双路径池化机制构建统一的图像补丁基于3D表示。</li>
<li>GS-Reasoner在定位方面完全不需要外部模块，建立了统一、自我包含的3D空间推理框架。</li>
<li>GCoT数据集包括用于参考推理问题的3D边界框注释和逐步推理路径。</li>
<li>GS-Reasoner在视觉定位方面取得了显著成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9fc6faf1dbcc2bb917012ad84616a3d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726284&auth_key=1760726284-0-0-c2fcc51a67488bb7b1933722a90e4a04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1ab25fdebc74d19286b2a86d72f8524~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726291&auth_key=1760726291-0-0-8b26579136d87cb4b5f3c44109500551&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa2718f5a1619f04a54154ac46156809~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726298&auth_key=1760726298-0-0-e4fd117b88b440468dff002bb6a9788e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b1b180f82e97d1652b95a64c81f6de6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726304&auth_key=1760726304-0-0-0812893504313d878cd8a76b639280fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RECODE-Reasoning-Through-Code-Generation-for-Visual-Question-Answering"><a href="#RECODE-Reasoning-Through-Code-Generation-for-Visual-Question-Answering" class="headerlink" title="RECODE: Reasoning Through Code Generation for Visual Question Answering"></a>RECODE: Reasoning Through Code Generation for Visual Question Answering</h2><p><strong>Authors:Junhong Shen, Mu Cai, Bo Hu, Ameet Talwalkar, David A Ross, Cordelia Schmid, Alireza Fathi</strong></p>
<p>Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering – the process of reverse-engineering visuals into executable code – as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在处理图表、示意图等结构化视觉内容的精确推理方面存在困难，因为基于像素的感知缺乏验证机制。为了解决这一问题，我们提出利用反渲染技术——将视觉内容逆向工程为可执行代码的过程——作为一种新的可验证视觉推理模式。具体来说，我们提出了RECODE，这是一个代理框架，首先生成多个候选程序来重建输入图像。然后，它使用一个评论家来选择最忠实的重建，并迭代地优化代码。这一过程不仅将一个模糊的感知任务转变为可验证的符号问题，而且为后续的计算和逻辑推理提供了精确性。在CharXiv、ChartQA和Geometry3K等视觉推理基准测试中，RECODE显著优于不利用代码或仅用于绘制辅助线或裁剪的方法。我们的工作证明，以可执行代码为基础进行视觉感知为更准确、可验证的多模态推理提供了新的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13756v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本提出针对多模态大型语言模型在处理结构化视觉内容（如图表、图表等）时的推理能力局限问题，引入了一种名为RECODE的新框架。RECODE通过利用反渲染技术，将视觉内容转化为可执行代码进行验证，实现了从模糊感知任务到可验证符号问题的转变。RECODE在不同视觉推理基准测试中表现优异，证明了将视觉感知与可执行代码相结合的方法对于实现更准确、可验证的多模态推理具有潜力。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是该文本中的七个关键要点：</p>
<ol>
<li>多模态大型语言模型在处理结构化视觉内容（如图表和图表）时面临精确推理的挑战。这是因为像素感知缺乏验证机制。</li>
<li>RECODE框架旨在解决这一问题，它通过反渲染技术将视觉内容转化为可执行代码进行验证。</li>
<li>RECODE框架首先生成多个候选程序来重建图像，并使用评论家选择最准确的重建，然后迭代优化代码。这个过程实现了从模糊感知任务到可验证符号问题的转变。</li>
<li>RECODE在不同视觉推理基准测试中表现优异，如CharXiv、ChartQA和Geometry3K等。相较于不依赖代码或仅使用代码绘制辅助线或裁剪的方法，RECODE具有显著优势。</li>
<li>RECODE框架将视觉感知与可执行代码相结合，为更精确和可验证的多模态推理提供了新途径。这不仅解决了现有的视觉推理挑战，还为未来的研究和应用提供了可能性。例如可以在诸如自然语言描述和图像生成等任务中引入更多基于代码的操作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13756">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-39258217e9197226496dc306a6c0d900~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726386&auth_key=1760726386-0-0-a052f32ca9316d2672302a77ab68dc86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a9e1d43a16452ab9f0f3ab747c4c2a24~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726395&auth_key=1760726395-0-0-e7e8bd4f533cc89a994ee53e3ddd8e38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3221f7b17ef938f49ba0e9ad7f533f81~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726401&auth_key=1760726401-0-0-30da92a5d47cfd73338f36fc8d05e619&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfabaddd8edee136530a996ed70e7dc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726407&auth_key=1760726407-0-0-7102a96ecf44c53136ddfa19b580c741&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c626b96270281d2bd657d8c5791d47b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726413&auth_key=1760726413-0-0-c971800e5b52ab8850ca6ffbd2576c9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-Meets-Masked-Generative-Models-Mask-GRPO-for-Text-to-Image-Generation"><a href="#Reinforcement-Learning-Meets-Masked-Generative-Models-Mask-GRPO-for-Text-to-Image-Generation" class="headerlink" title="Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for   Text-to-Image Generation"></a>Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for   Text-to-Image Generation</h2><p><strong>Authors:Yifu Luo, Xinhao Hu, Keyu Fan, Haoyuan Sun, Zeyu Chen, Bo Xia, Tiantian Zhang, Yongzhe Chang, Xueqian Wang</strong></p>
<p>Reinforcement learning (RL) has garnered increasing attention in text-to-image (T2I) generation. However, most existing RL approaches are tailored to either diffusion models or autoregressive models, overlooking an important alternative: masked generative models. In this work, we propose Mask-GRPO, the first method to incorporate Group Relative Policy Optimization (GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine the transition probability, which is different from current approaches, and formulate the unmasking process as a multi-step decision-making problem. To further enhance our method, we explore several useful strategies, including removing the KL constraint, applying the reduction strategy, and filtering out low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with substantial improvements on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches. The code is available on <a target="_blank" rel="noopener" href="https://github.com/xingzhejun/Mask-GRPO">https://github.com/xingzhejun/Mask-GRPO</a> </p>
<blockquote>
<p>强化学习（RL）在文本到图像（T2I）生成中受到了越来越多的关注。然而，大多数现有的RL方法都是针对扩散模型或自回归模型设计的，忽略了一种重要的替代方法：基于掩码的生成模型。在这项工作中，我们提出了Mask-GRPO，这是第一种将基于Group Relative Policy Optimization (GRPO)的RL纳入这一被忽视范式的方法。我们的核心见解是重新定义转移概率，这与当前的方法不同，并将解掩过程制定为一个多步决策问题。为了进一步改进我们的方法，我们探索了多种有用的策略，包括去除KL约束、应用缩减策略以及过滤掉低质量的样本。通过使用Mask-GRPO，我们改进了基准模型Show-o，在标准的T2I基准测试和偏好对齐上取得了显著的改进，超越了现有的最新技术方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/xingzhejun/Mask-GRPO">https://github.com/xingzhejun/Mask-GRPO</a>上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13418v1">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习在文本到图像生成领域逐渐受到关注，但大多数强化学习方法针对扩散模型或自回归模型，忽略了掩模生成模型。本研究提出Mask-GRPO，首次将基于Group Relative Policy Optimization（GRPO）的强化学习融入此领域。其核心是重新定义了转换概率，将其表述为分步决策问题。通过移除KL约束、应用缩减策略及过滤低质量样本，提升了基础模型Show-o在标准文本到图像生成基准测试上的表现，并优于现有前沿方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在文本到图像生成中的应用逐渐受到关注。</li>
<li>现有强化学习方法主要关注扩散模型和自回归模型，忽略了掩模生成模型。</li>
<li>Mask-GRPO方法首次将Group Relative Policy Optimization（GRPO）强化学习融入文本到图像生成领域。</li>
<li>Mask-GRPO重新定义了转换概率，并将其表述为分步决策问题。</li>
<li>Mask-GRPO通过移除KL约束、应用缩减策略及过滤低质量样本，提升了模型性能。</li>
<li>Mask-GRPO改善了基础模型Show-o在标准文本到图像生成基准测试上的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bfcd8e0c2178ab6505c029345bf20d98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726421&auth_key=1760726421-0-0-64113c5421ccfcb8e340195ddf1c2047&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-15ec5f1b3336702d660b0e87ba6164f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726428&auth_key=1760726428-0-0-7ecd919e5c13f45a46b9eeffaade007f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d8ba91ac8abb3029a259de25fbcf645~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726435&auth_key=1760726435-0-0-c1082efd82b02aad711a28e1fda5d269&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d01bcccaed0ac297e42817b74875c127~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726443&auth_key=1760726443-0-0-2effe4fc7d9bd284eeb5d9b641d2fb50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-07dfd4190e066b6ffd27340fa7e6a279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728472&auth_key=1760728472-0-0-7b38aef80c3e51a98c4a5f5c3d1e5e84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-10-18  Agentic Design of Compositional Machines
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-3a927d8a3c411167e73facd194a696d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760137343&auth_key=1760137343-0-0-0e3516ade6215a562278545588981320&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-10-11  Paper2Video Automatic Video Generation from Scientific Papers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
