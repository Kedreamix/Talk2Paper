<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Agentic Design of Compositional Machines">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-e4af957f63b08c2752075fc2c2f13d7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726092&auth_key=1760726092-0-0-a53a65026dc21de436c0cc649fc316cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Agentic-Design-of-Compositional-Machines"><a href="#Agentic-Design-of-Compositional-Machines" class="headerlink" title="Agentic Design of Compositional Machines"></a>Agentic Design of Compositional Machines</h2><p><strong>Authors:Wenqian Zhang, Weiyang Liu, Zhen Liu</strong></p>
<p>The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning. </p>
<blockquote>
<p>å¤æ‚æœºå™¨çš„è®¾è®¡æ—¢æ˜¯äººç±»æ™ºæ…§çš„æ ‡å¿—ï¼Œä¹Ÿæ˜¯å·¥ç¨‹å®è·µçš„åŸºç¡€ã€‚é‰´äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“å®ƒä»¬æ˜¯å¦ä¹Ÿèƒ½å­¦ä¼šåˆ›é€ ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ç»„åˆæœºæ¢°è®¾è®¡è¿™ä¸€è§†è§’æ¥æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼šè¿™æ˜¯ä¸€é¡¹ä»»åŠ¡ï¼Œå…¶ä¸­æœºå™¨ç”±æ ‡å‡†åŒ–ç»„ä»¶ç»„è£…è€Œæˆï¼Œä»¥æ»¡è¶³æ¨¡æ‹Ÿç‰©ç†ç¯å¢ƒä¸­çš„è¿åŠ¨æˆ–æ“ä½œç­‰åŠŸèƒ½éœ€æ±‚ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæœºå™¨å»ºé€ æ¸¸æˆBesiegeæ„å»ºçš„BesiegeFieldæµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°æ”¯æŒåŸºäºé›¶ä»¶çš„å»ºé€ ã€ç‰©ç†æ¨¡æ‹Ÿå’Œå¥–åŠ±é©±åŠ¨è¯„ä¼°ã€‚é€šè¿‡BesiegeFieldå¹³å°ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶ç¡®å®šäº†æˆåŠŸæ‰€éœ€çš„å…³é”®èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç©ºé—´æ¨ç†ã€ç­–ç•¥è£…é…å’ŒæŒ‡ä»¤æ‰§è¡Œã€‚ç”±äºå½“å‰å¼€æºæ¨¡å‹çš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæ”¹è¿›çš„é€”å¾„ï¼šæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå†·å¯åŠ¨æ•°æ®é›†ï¼Œè¿›è¡Œäº†RLå¾®è°ƒå®éªŒï¼Œå¹¶å¼ºè°ƒäº†è¯­è¨€ã€æœºå™¨è®¾è®¡å’Œç‰©ç†æ¨ç†äº¤æ±‡å¤„çš„å¼€æ”¾æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14980v1">PDF</a> 75 pages, 31 figures, Project Page: <a target="_blank" rel="noopener" href="https://besiegefield.github.io/">https://besiegefield.github.io</a></p>
<p><strong>Summary</strong></p>
<p>å¤æ‚æœºå™¨çš„è®¾è®¡æ—¢ä½“ç°äº†äººç±»æ™ºæ…§ï¼Œåˆæ˜¯å·¥ç¨‹å®è·µçš„åŸºç¡€ã€‚å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°å‘å±•ï¼Œæˆ‘ä»¬æ¢è®¨å®ƒä»¬æ˜¯å¦ä¹Ÿèƒ½å­¦ä¹ åˆ›é€ ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»„åˆæœºå™¨è®¾è®¡çš„è§†è§’æ¥æ¢è®¨è¿™ä¸€é—®é¢˜ï¼Œè¿™æ˜¯ä¸€é¡¹åœ¨æ¨¡æ‹Ÿç‰©ç†ç¯å¢ƒä¸­ï¼Œé€šè¿‡æ ‡å‡†åŒ–ç»„ä»¶æ¥ç»„è£…æœºå™¨ä»¥æ»¡è¶³è¡ŒåŠ¨æˆ–æ“ä½œç­‰éœ€æ±‚çš„ä»»åŠ¡ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç ”ç©¶ï¼Œæˆ‘ä»¬åŸºäºæœºå™¨å»ºé€ æ¸¸æˆBesiegeå»ºç«‹äº†BesiegeFieldæµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°æ”¯æŒéƒ¨ä»¶æ„é€ ã€ç‰©ç†æ¨¡æ‹Ÿå’Œå¥–åŠ±é©±åŠ¨è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨BesiegeFieldå¯¹æœ€å…ˆè¿›çš„LLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶ç¡®å®šæˆåŠŸçš„å…³é”®èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç©ºé—´æ¨ç†ã€ç­–ç•¥æ€§è£…é…å’ŒæŒ‡ä»¤éµå¾ªç­‰ã€‚ç”±äºå½“å‰å¼€æºæ¨¡å‹çš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæ”¹è¿›çš„è·¯å¾„ï¼šæˆ‘ä»¬ç­›é€‰äº†å†·å¯åŠ¨æ•°æ®é›†ï¼Œè¿›è¡Œäº†RLå¾®è°ƒå®éªŒï¼Œå¹¶å¼ºè°ƒäº†è¯­è¨€ã€æœºå™¨è®¾è®¡å’Œç‰©ç†æ¨ç†äº¤å‰é¢†åŸŸçš„å¼€æ”¾æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨è®¾è®¡æ–¹é¢çš„èƒ½åŠ›æˆä¸ºç ”ç©¶ç„¦ç‚¹ã€‚</li>
<li>BesiegeFieldæµ‹è¯•å¹³å°ç”¨äºæ”¯æŒæœºå™¨æ„é€ ã€ç‰©ç†æ¨¡æ‹Ÿå’Œè¯„ä¼°ã€‚</li>
<li>å…ˆè¿›LLMsåœ¨ç»„åˆæœºå™¨è®¾è®¡ä»»åŠ¡ä¸­éœ€å…·å¤‡ç©ºé—´æ¨ç†ã€ç­–ç•¥æ€§è£…é…å’ŒæŒ‡ä»¤éµå¾ªç­‰å…³é”®èƒ½åŠ›ã€‚</li>
<li>å½“å‰å¼€æºæ¨¡å‹åœ¨æœºå™¨è®¾è®¡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«æå‡ºä¸ºæ”¹è¿›æ–¹å‘ã€‚</li>
<li>ç ”ç©¶ç­›é€‰äº†å†·å¯åŠ¨æ•°æ®é›†ç”¨äºRLå¾®è°ƒå®éªŒã€‚</li>
<li>åœ¨è¯­è¨€ã€æœºå™¨è®¾è®¡å’Œç‰©ç†æ¨ç†äº¤å‰é¢†åŸŸå­˜åœ¨è®¸å¤šå¼€æ”¾æŒ‘æˆ˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºLLMsåœ¨æœºå™¨è®¾è®¡é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†åŸºå‡†å’Œç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c256346b3b9819197b782e126d33bdfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725843&auth_key=1760725843-0-0-11566775e72575ed804faa5df6c1005d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b72de284f534b45d2dde836e7407ef4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725850&auth_key=1760725850-0-0-da6aed061ecee525d1abb53cb2f78d70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a452b05e0464d861460cf73d4168c3cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725857&auth_key=1760725857-0-0-b7cb619b74f7870af63845a355edb5ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d601bcfadf1df847879211c85ec7849~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725864&auth_key=1760725864-0-0-d10ca7de67f0bd8a2861a65cba6451ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7a965fcabd9913c6053e7b0c2183944~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725871&auth_key=1760725871-0-0-132bd2dd52d7f40a16e472ea12b1fab3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents"><a href="#Information-Gain-based-Policy-Optimization-A-Simple-and-Effective-Approach-for-Multi-Turn-LLM-Agents" class="headerlink" title="Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents"></a>Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents</h2><p><strong>Authors:Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying</strong></p>
<p>Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policyâ€™s probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the modelâ€™s own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠ å¼ºå…¶é€šè¿‡å·¥å…·ä½¿ç”¨ä¸å¤–éƒ¨ç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè½®æ¨ç†å’ŒçŸ¥è¯†è·å–åŸºäºæœç´¢çš„ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä»…åœ¨æœ€ç»ˆç­”æ¡ˆä¸Šæä¾›çš„ç»“æœå‹å¥–åŠ±ã€‚è¿™ç§å¥–åŠ±ç¨€ç–æ€§åœ¨å¤šè½®ç¯å¢ƒä¸­å˜å¾—ç‰¹åˆ«æˆé—®é¢˜ï¼Œé•¿è½¨è¿¹ä¼šåŠ å‰§ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆiï¼‰ä¼˜åŠ¿å´©æºƒï¼Œæ‰€æœ‰æ»šåŠ¨ç»“æœè·å¾—ç›¸åŒçš„å¥–åŠ±ï¼Œæ— æ³•æä¾›æœ‰ç”¨çš„å­¦ä¹ ä¿¡å·ï¼›ï¼ˆiiï¼‰ç¼ºä¹ç²¾ç»†çš„ä¿¡ç”¨åˆ†é…ï¼Œå›åˆä¹‹é—´çš„ä¾èµ–å…³ç³»è¢«æ©ç›–ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æœŸä»»åŠ¡ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¿¡æ¯å¢ç›Šçš„ç­–ç•¥ä¼˜åŒ–ï¼ˆIGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸ºå¤šè½®ä»£ç†è®­ç»ƒæä¾›å¯†é›†å’Œå†…ç”Ÿçš„ç›‘ç£ã€‚IGPOå°†æ¯ä¸ªäº¤äº’å›åˆè§†ä¸ºå…³äºçœŸç›¸çš„å¢é‡è·å–è¿‡ç¨‹ï¼Œå¹¶å°†å›åˆå¥–åŠ±å®šä¹‰ä¸ºç­–ç•¥äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡çš„è¾¹é™…å¢åŠ ã€‚ä¸ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–æ˜‚è´µçš„è’™ç‰¹å¡æ´›ä¼°è®¡çš„å…ˆå‰è¿‡ç¨‹çº§å¥–åŠ±æ–¹æ³•ä¸åŒï¼ŒIGPOç›´æ¥ä»æ¨¡å‹è‡ªèº«çš„ä¿¡å¿µæ›´æ–°ä¸­å¾—å‡ºå†…åœ¨å¥–åŠ±ã€‚è¿™äº›å†…åœ¨å›åˆå¥–åŠ±ä¸ç»“æœçº§ç›‘ç£ç›¸ç»“åˆï¼Œå½¢æˆäº†å¯†é›†çš„å¥–åŠ±è½¨è¿¹ã€‚åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIGPOåœ¨å¤šè½®åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14967v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨æœç´¢ç­‰ç¯å¢ƒä¸­ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæœ€ç»ˆç­”æ¡ˆçš„äº§å‡ºå¯¼å‘å¥–åŠ±ï¼Œè¿™åœ¨å¤šè½®æ¨ç†å’Œä»»åŠ¡ä¸­è·å–çŸ¥è¯†çš„åœºæ™¯ä¸­å­˜åœ¨é—®é¢˜ã€‚é’ˆå¯¹å¥–åŠ±ç¨€ç–çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”åŸºäºä¿¡æ¯å¢ç›Šçš„ç­–ç•¥ä¼˜åŒ–ï¼ˆIGPOï¼‰ã€‚å®ƒå°†æ¯è½®äº¤äº’è§†ä¸ºå¯¹äº‹å®çœŸç›¸ä¿¡æ¯çš„ç´¯ç§¯è¿‡ç¨‹ï¼Œå¹¶å°†å¥–åŠ±å®šä¹‰ä¸ºç­–ç•¥äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡çš„è¾¹é™…å¢åŠ ã€‚ä¸å…¶ä»–ä¾èµ–äºå¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–æ˜‚è´µçš„è’™ç‰¹å¡æ´›ä¼°è®¡çš„è¿‡ç¨‹çº§å¥–åŠ±æ–¹æ³•ä¸åŒï¼ŒIGPOç›´æ¥ä»æ¨¡å‹è‡ªèº«çš„ä¿¡å¿µæ›´æ–°ä¸­è·å¾—å†…åœ¨å¥–åŠ±ã€‚è¿™äº›å†…åœ¨çš„è½®çº§å¥–åŠ±ä¸ç»“æœçº§çš„ç›‘ç£ç›¸ç»“åˆï¼Œå½¢æˆäº†å¯†é›†çš„å¥–åŠ±è½¨è¿¹ã€‚åœ¨å†…éƒ¨å’Œå¤–éƒ¨åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒä¸­ï¼ŒIGPOåœ¨å¤šè½®åœºæ™¯ä¸­çš„è¡¨ç°å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆå¼ºåŒ–å­¦ä¹ åœ¨å¤šè½®æ¨ç†å’ŒçŸ¥è¯†è·å–åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æœ€ç»ˆç­”æ¡ˆçš„äº§å‡ºå¯¼å‘å¥–åŠ±ï¼Œè¿™åœ¨å¤šè½®åœºæ™¯ä¸­å­˜åœ¨é—®é¢˜ã€‚</li>
<li>åŸºäºä¿¡æ¯å¢ç›Šçš„ç­–ç•¥ä¼˜åŒ–ï¼ˆIGPOï¼‰æ¡†æ¶èƒ½æœ‰æ•ˆè§£å†³å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚</li>
<li>IGPOå°†æ¯è½®äº¤äº’è§†ä¸ºä¿¡æ¯ç´¯ç§¯è¿‡ç¨‹ï¼Œå¹¶å®šä¹‰å¥–åŠ±ä¸ºç­–ç•¥äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆæ¦‚ç‡çš„è¾¹é™…å¢åŠ ã€‚</li>
<li>IGPOä»æ¨¡å‹è‡ªèº«çš„ä¿¡å¿µæ›´æ–°ä¸­è·å¾—å†…åœ¨å¥–åŠ±ï¼Œé¿å…ä¾èµ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–è’™ç‰¹å¡æ´›ä¼°è®¡ã€‚</li>
<li>å†…åœ¨è½®çº§å¥–åŠ±ä¸ç»“æœçº§ç›‘ç£ç»“åˆå½¢æˆå¯†é›†å¥–åŠ±è½¨è¿¹ï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6acee461152fa22ff3a292ec3c6b9c47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725881&auth_key=1760725881-0-0-4be1871661d727369525e9bdbba456f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-069829c476fc987398af88ea95aa8f54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725888&auth_key=1760725888-0-0-0f0864e6976950b3980ed3f915530d36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcb8a3c56cd16d74ac346368c4442502~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725894&auth_key=1760725894-0-0-85576916188c7701f0cf662bfe02b37b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning"><a href="#MathCanvas-Intrinsic-Visual-Chain-of-Thought-for-Multimodal-Mathematical-Reasoning" class="headerlink" title="MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning"></a>MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning</h2><p><strong>Authors:Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li</strong></p>
<p>While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a> </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å‡ ä½•ç­‰æ•°å­¦é¢†åŸŸï¼Œå®ƒä»¬å¤©ç”Ÿä¾èµ–è§†è§‰è¾…åŠ©å·¥å…·ï¼Œå› æ­¤é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„è§†è§‰æ€ç»´é“¾ï¼ˆVCoTï¼‰æ–¹æ³•å¾€å¾€å—åˆ°åƒµåŒ–å¤–éƒ¨å·¥å…·çš„é™åˆ¶ï¼Œæˆ–è€…æ— æ³•ç”Ÿæˆè§£å†³å¤æ‚é—®é¢˜æ‰€éœ€çš„å¿ å®åº¦é«˜ã€æ—¶æ•ˆæˆ˜ç•¥å¼ºçš„å›¾è¡¨ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathCanvasï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨èµ‹äºˆç»Ÿä¸€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å†…åœ¨è§†è§‰æ€ç»´é“¾èƒ½åŠ›çš„å…¨é¢æ¡†æ¶ï¼Œç”¨äºæ•°å­¦é¢†åŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œè§†è§‰æ“ä½œé˜¶æ®µåœ¨ä¸€ä¸ªæ–°å‹1520ä¸‡å¯¹æ•°æ®å¯¹ä¸Šå¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…æ‹¬1äº¿ä¸ªæ ‡é¢˜åˆ°å›¾è¡¨çš„é…å¯¹ï¼ˆMathCanvas-Imagenï¼‰å’Œ520ä¸‡æ¡é€æ­¥ç¼–è¾‘è½¨è¿¹ï¼ˆMathCanvas-Editï¼‰ï¼Œä»¥æŒæ¡å›¾è¡¨ç”Ÿæˆå’Œç¼–è¾‘ã€‚å…¶æ¬¡ï¼Œç­–ç•¥æ€§è§†è§‰è¾…åŠ©æ¨ç†é˜¶æ®µå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨MathCanvas-Instructï¼Œè¿™æ˜¯ä¸€å¥—åŒ…å«æ··åˆè§†è§‰æ–‡æœ¬æ¨ç†è·¯å¾„çš„æ–°æ•°æ®é›†ï¼Œæ‹¥æœ‰å¤šè¾¾21ä¸‡æ¡æ ·æœ¬ï¼Œå‘æ¨¡å‹æ•™æˆä½•æ—¶ä»¥åŠå¦‚ä½•è¿ç”¨è§†è§‰è¾…åŠ©å·¥å…·ã€‚ä¸ºäº†æ–¹ä¾¿ä¸¥æ ¼çš„è¯„ä¼°æµ‹è¯•ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathCanvas-Benchè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ‹¥æœ‰éœ€è¦æ¨¡å‹ç”Ÿæˆæ··åˆè§†è§‰æ–‡æœ¬è§£å†³æ–¹æ¡ˆçš„3åƒä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹BAGEL-Canvasåœ¨è¯¥æ¡†æ¶ä¸‹ç»è¿‡è®­ç»ƒåï¼Œåœ¨MathCanvas-Benchä¸Šçš„è¡¨ç°æ¯”å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹åŸºçº¿é«˜å‡ºç›¸å¯¹æ”¹è¿›å¹…åº¦é«˜è¾¾86%ï¼Œåœ¨å…¬å…±æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†å®Œæ•´çš„å·¥å…·ç®±æ¡†æ¶ã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¹³å°ï¼Œä»¥è§£é”å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­å¤æ‚ã€äººæ€§åŒ–çš„è§†è§‰è¾…åŠ©æ¨ç†èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14958v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mathcanvas.github.io/">https://mathcanvas.github.io/</a></p>
<p><strong>Summary</strong><br>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿æ–‡æœ¬æ¨ç†çš„åŒæ—¶ï¼Œå®ƒä»¬åœ¨ä¾èµ–è§†è§‰è¾…åŠ©å·¥å…·çš„å‡ ä½•ç­‰é¢†åŸŸå´è¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„è§†è§‰æ€ç»´é“¾ï¼ˆVCoTï¼‰æ–¹æ³•å—é™äºå¤–éƒ¨å·¥å…·çš„é™åˆ¶æˆ–æ— æ³•ç”Ÿæˆç”¨äºå¤æ‚é—®é¢˜è§£å†³çš„å¿…è¦çš„é«˜ä¿çœŸåº¦ã€ç­–ç•¥æ€§å®šæ—¶å›¾è¡¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathCanvasæ¡†æ¶ï¼Œæ—¨åœ¨èµ‹äºˆç»Ÿä¸€çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å†…åœ¨VCoTèƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡æ–°é¢–çš„åŒ…å«ä¸€äº¿äº”åƒä¸¤ç™¾ä¸‡é…å¯¹å›¾åƒä¸äº”åäºŒäº¿ç¼–è¾‘è½¨è¿¹æ•°æ®çš„è§†è§‰æ“æ§é˜¶æ®µï¼Œè®©æ¨¡å‹å­¦ä¼šç”Ÿæˆä¸ç¼–è¾‘å›¾è¡¨ï¼›ç„¶åé€šè¿‡ç²¾ç»†å¾®è°ƒæ•°å­¦ä¸­çš„è§£é¢˜ç­–ç•¥é˜¶æ®µï¼Œè®©æ¨¡å‹å­¦ä¼šä½•æ—¶ä»¥åŠå¦‚ä½•åˆ©ç”¨è§†è§‰è¾…åŠ©å·¥å…·ã€‚æˆ‘ä»¬å¼•å…¥äº†MathCanvas-Benchä½œä¸ºæŒ‘æˆ˜åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸‰åƒé“éœ€è¦æ¨¡å‹ç”Ÿæˆå›¾æ–‡ç»“åˆè§£å†³æ–¹æ¡ˆçš„é—®é¢˜ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸‹è®­ç»ƒçš„BAGEL-Canvasæ¨¡å‹ï¼Œåœ¨MathCanvas-Benchä¸Šçš„è¡¨ç°æ¯”å¼ºå¤§çš„LMMåŸºå‡†æµ‹è¯•é«˜å‡º86%ï¼Œå¹¶å±•ç¤ºäº†åœ¨å…¶ä»–å…¬å…±æ•°å­¦æµ‹è¯•ä¸­çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè§†è§‰è¾…åŠ©æ¨ç†çš„å·¥å…·åŒ…æ¡†æ¶ã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‡ ä½•ç­‰é¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹è§†è§‰è¾…åŠ©èƒ½åŠ›ã€‚</li>
<li>MathCanvasæ¡†æ¶æ—¨åœ¨é€šè¿‡è§†è§‰æ“æ§å’Œç²¾ç»†å¾®è°ƒèµ‹äºˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹å†…åœ¨è§†è§‰æ€ç»´é“¾ï¼ˆVCoTï¼‰èƒ½åŠ›ã€‚</li>
<li>MathCanvasæ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šè§†è§‰æ“æ§é˜¶æ®µå’Œæˆ˜ç•¥è§†è§‰è¾…åŠ©æ¨ç†é˜¶æ®µã€‚</li>
<li>MathCanvaså¼•å…¥äº†æ–°æ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰è¾…åŠ©ä¸‹çš„æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>BAGEL-Canvasæ¨¡å‹åœ¨MathCanvasåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹æ”¹è¿›ç‡è¾¾åˆ°äº†86%ã€‚</li>
<li>BAGEL-Canvasæ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¶ä»–å…¬å…±æ•°å­¦æµ‹è¯•ä¸­ä¹Ÿæœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5c4eefc85e5854dadcc669fa4285b07d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725903&auth_key=1760725903-0-0-1b723f4da0bcccf3e1debd9955155a27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de68a430433309e0a20c51788ecb1b34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725911&auth_key=1760725911-0-0-67492bdd5ccd70bb9ac98d26727a4f71&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea25758b6de272f758c85964a8765599~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725918&auth_key=1760725918-0-0-55890b866f1de9be456bc70396fff4cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5450ab2559bbf7b0feb603ba68bd91fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725925&auth_key=1760725925-0-0-ab4e05ef1165e6c06c68b555b94889e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning"><a href="#GroundedPRM-Tree-Guided-and-Fidelity-Aware-Process-Reward-Modeling-for-Step-Level-Reasoning" class="headerlink" title="GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning"></a>GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning</h2><p><strong>Authors:Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp</strong></p>
<p>Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰æ—¨åœ¨é€šè¿‡ç›‘ç£ä¸­é—´æ­¥éª¤å’Œè¯†åˆ«é”™è¯¯æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯æ‰©å±•çš„é«˜è´¨é‡æ³¨é‡Šï¼Œæ„å»ºæœ‰æ•ˆçš„PRMä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ ‡æ³¨ã€åŸºäºLLMçš„è‡ªæˆ‘è¯„ä»·ï¼ˆå®¹æ˜“å‡ºç°å¹»è§‰ï¼‰æˆ–è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ä¼°è®¡ã€‚MCä¼°è®¡ä»…ä»æ»šåŠ¨ç»“æœä¸­æ¨æ–­æ­¥éª¤è´¨é‡ï¼Œä½†ç”±äºåŠŸåŠ³åˆ†é…ä¸å½“ï¼Œç»å¸¸å¼•å…¥å˜ˆæ‚ã€é”™ä½çš„ç›‘ç£ã€‚è¿™äº›é—®é¢˜å¯¼è‡´ä¸‰ä¸ªæ ¸å¿ƒå±€é™ï¼šå¥–åŠ±å™ªå£°ã€äº‹å®å‡†ç¡®æ€§ä½ä»¥åŠä¸æ­¥éª¤çº§æ¨ç†ç›®æ ‡çš„é”™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GroundedPRMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨æµç¨‹ç›‘ç£çš„æ ‘å½¢å¼•å¯¼å’Œä¿çœŸåº¦æ„ŸçŸ¥æ¡†æ¶ã€‚ä¸ºäº†å‡å°‘å¥–åŠ±å™ªå£°å¹¶å®ç°ç²¾ç»†çš„åŠŸåŠ³åˆ†é…ï¼Œæˆ‘ä»¬é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ„å»ºç»“æ„åŒ–æ¨ç†è·¯å¾„ã€‚ä¸ºäº†æ¶ˆé™¤å¹»è§‰ç›‘ç£ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤–éƒ¨å·¥å…·éªŒè¯æ¯ä¸ªä¸­é—´æ­¥éª¤ï¼Œæä¾›åŸºäºæ‰§è¡Œçš„æ­£ç¡®æ€§ä¿¡å·ã€‚ä¸ºäº†ç»“åˆæ­¥éª¤çº§éªŒè¯å’Œå…¨å±€ç»“æœè¯„ä¼°ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆå¥–åŠ±èšåˆæœºåˆ¶ï¼Œå®ƒå°†å·¥å…·éªŒè¯ä¸MCTSæ´¾ç”Ÿçš„åé¦ˆç»“åˆèµ·æ¥ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å¥–åŠ±ä¿¡å·æ ¼å¼åŒ–ä¸ºå¢å¼ºç†æ€§ã€ç”Ÿæˆæ€§çš„ç»“æ„ï¼Œä»¥ä¿ƒè¿›å¯è§£é‡Šæ€§ä¸æŒ‡ä»¤è°ƒä¼˜çš„LLMçš„å…¼å®¹æ€§ã€‚GroundedPRMä»…ä½¿ç”¨40ä¸‡ä¸ªè‡ªåŠ¨æ ‡è®°æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä»…å ä½¿ç”¨è‡ªåŠ¨æ ‡è®°ç›‘ç£çš„æœ€ä½³PRMæ‰€ç”¨æ•°æ®çš„10%ã€‚ç„¶è€Œï¼Œå®ƒåœ¨ProcessBenchä¸Šçš„å¹³å‡æ€§èƒ½å®ç°äº†é«˜è¾¾26%çš„ç›¸å¯¹æ”¹è¿›ã€‚å½“ç”¨äºå¥–åŠ±å¼•å¯¼è´ªå©ªæœç´¢æ—¶ï¼ŒGroundedPRMç”šè‡³è¶…è¶Šäº†ä½¿ç”¨äººå·¥æ ‡è®°ç›‘ç£è®­ç»ƒçš„PRMï¼Œä¸ºé«˜è´¨é‡æµç¨‹çº§æ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯éªŒè¯çš„è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14942v1">PDF</a> 25 pages</p>
<p><strong>Summary</strong><br>     è¿›ç¨‹å¥–åŠ±æ¨¡å‹æ—¨åœ¨é€šè¿‡ç›‘ç£ä¸­é—´æ­¥éª¤å’Œè¯†åˆ«é”™è¯¯æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ä½†æ„å»ºæœ‰æ•ˆçš„è¿›ç¨‹å¥–åŠ±æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦å› ä¸ºç¼ºä¹å¯æ‰©å±•çš„é«˜è´¨é‡æ³¨é‡Šã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºåŸºäºæ ‘å¼•å¯¼ä¸ä¿çœŸåº¦çš„æ¡†æ¶è¿›è¡Œè‡ªåŠ¨è¿‡ç¨‹ç›‘ç£ï¼Œæ„å»ºç»“æ„åŒ–æ¨ç†è·¯å¾„å¹¶å‡å°‘å¥–åŠ±å™ªå£°ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢è¿›è¡Œç²¾ç»†åŒ–çš„ä¿¡ç”¨åˆ†é…ï¼Œå¹¶ä½¿ç”¨å¤–éƒ¨å·¥å…·éªŒè¯æ¯ä¸ªä¸­é—´æ­¥éª¤ä»¥æ¶ˆé™¤å¹»è§‰ç›‘ç£ã€‚ç»“åˆæ­¥éª¤çº§éªŒè¯å’Œå…¨å±€ç»“æœè¯„ä¼°çš„æ··åˆå¥–åŠ±èšåˆæœºåˆ¶ï¼Œèåˆå·¥å…·éªŒè¯ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢çš„åé¦ˆã€‚è¯¥æ¨¡å‹åœ¨ä»…æœ‰4ä¸‡è‡ªåŠ¨æ ‡è®°æ ·æœ¬çš„è®­ç»ƒä¸‹ï¼Œå®ç°äº†å¯¹æœ€ä½³è¿›ç¨‹çš„ç›¸å¯¹æ”¹è¿›ã€‚åœ¨ProcessBenchä¸Šçš„å¹³å‡æ€§èƒ½æé«˜äº†26%ï¼Œåœ¨å¥–åŠ±æŒ‡å¯¼çš„è´ªå¿ƒæœç´¢æ–¹é¢è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿›ç¨‹å¥–åŠ±æ¨¡å‹æ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç›‘ç£ä¸­é—´æ­¥éª¤å’Œè¯†åˆ«é”™è¯¯æ¥å®ç°ã€‚</li>
<li>å½“å‰è¿›ç¨‹å¥–åŠ±æ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºä¹é«˜è´¨é‡ã€å¯æ‰©å±•çš„æ³¨é‡Šæ•°æ®ã€‚</li>
<li>æå‡ºåä¸ºGroundedPRMçš„æ¡†æ¶ï¼Œç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢æ„å»ºç»“æ„åŒ–æ¨ç†è·¯å¾„ï¼Œä»¥å‡å°‘å¥–åŠ±å™ªå£°å¹¶å®ç°ç²¾ç»†åŒ–çš„ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤–éƒ¨å·¥å…·éªŒè¯ä¸­é—´æ­¥éª¤ï¼Œæ¶ˆé™¤å¹»è§‰ç›‘ç£ï¼Œæé«˜ç›‘ç£çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆæ­¥éª¤çº§éªŒè¯å’Œå…¨å±€ç»“æœè¯„ä¼°çš„æ··åˆå¥–åŠ±èšåˆæœºåˆ¶ï¼Œæé«˜å¥–åŠ±ä¿¡å·çš„è´¨é‡å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡èåˆå·¥å…·éªŒè¯ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢çš„åé¦ˆï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dd41ee8a24b5e926edfce49f81364020~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725933&auth_key=1760725933-0-0-f0de87c0eebb7a00f8ca2146e3adf69c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3a6fabc7ec2f2e13ded990993e3bb07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725940&auth_key=1760725940-0-0-9427b1a1a68a62e5e1c1fb28ae2adba6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection"><a href="#CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection" class="headerlink" title="CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection"></a>CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection</h2><p><strong>Authors:Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim</strong></p>
<p>Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰æ—¨åœ¨è¯†åˆ«å¹¶å®šä½è¶…å‡ºè®­ç»ƒæœŸé—´æ‰€è§å¯¹è±¡çš„ç±»åˆ«ã€‚æœ€è¿‘çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡å›¾åƒæ–‡æœ¬å¯¹é½ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå…è®¸æ£€æµ‹å™¨åœ¨æ²¡æœ‰æ˜ç¡®ç›‘ç£çš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æœªè§ç±»åˆ«ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¿‡äºä¾èµ–ç›´æ¥çš„å›¾åƒæ–‡æœ¬åŒ¹é…ï¼Œå¿½ç•¥äº†è§£é‡Šè¯­ä¹‰å¤æ‚åœºæ™¯æ‰€å¿…éœ€çš„ä¸­ä»‹æ¨ç†æ­¥éª¤ã€‚è¿™å¯¼è‡´åœ¨é¢å¯¹æ‹¥æŒ¤æˆ–é®æŒ¡çš„è§†è§‰ä¸Šä¸‹æ–‡æ—¶ï¼Œå…¶ç¨³å¥æ€§å—åˆ°é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CoT-PLï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒå°†ç»“æ„åŒ–çš„è§†è§‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èå…¥ä¼ªæ ‡ç­¾è¿‡ç¨‹ä¸­ã€‚CoT-PLå°†å¯¹è±¡ç†è§£åˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šæ­¥éª¤ï¼šï¼ˆ1ï¼‰å³ä½¿å¯¹äºæœªè§å¯¹è±¡ä¹Ÿèƒ½è¿›è¡ŒåŒºåŸŸæ„ŸçŸ¥ï¼Œï¼ˆ2ï¼‰é€šè¿‡é›¶å°„å‡»æ¨ç†è¿›è¡Œç±»åˆ«è¯†åˆ«ï¼Œï¼ˆ3ï¼‰èƒŒæ™¯å®šä½ä»¥åˆ†ç¦»è¯­ä¹‰å¤æ‚çš„å¯¹è±¡ã€‚å…³é”®çš„æ˜¯ï¼Œç¬¬ä¸‰æ­¥è‡ªç„¶æ¿€åŠ±æˆ‘ä»¬çš„å¯¹æ¯”èƒŒæ™¯å­¦ä¹ ï¼ˆCBLï¼‰ï¼Œå®ƒä½¿ç”¨é¢„å…ˆè®¡ç®—å¥½çš„èƒŒæ™¯çº¿ç´¢ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œä»¥ä¿ƒè¿›å¯¹è±¡å’ŒèƒŒæ™¯ç‰¹å¾ä¹‹é—´çš„è§£è€¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCoTæ¨ç†å’ŒCBLå½¢æˆäº†ä¸€ä¸ªé’ˆå¯¹æ‹¥æŒ¤æˆ–é®æŒ¡åœºæ™¯ä¸­çš„ç¨³å¥ä¼ªæ ‡ç­¾çš„é›†æˆç®¡é“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ä¸¤ç§è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬çš„æ–°å‹ä¼ªæ ‡ç­¾è´¨é‡åˆ†åˆ«æ¯”æœ€ä½³å…ˆå‰æŠ€æœ¯æé«˜äº†103.4%å’Œ168.4%ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoT-PLåœ¨å¼€æ”¾è¯æ±‡COCOä¸Šæé«˜äº†+7.7 AP50ï¼Œåœ¨LVISçš„æ–°ç±»åˆ«ä¸Šæé«˜äº†+2.9 mask APï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯è®°å½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14792v1">PDF</a> 28 pages, 13 Figures, 12 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCoT-PLçš„æ–°æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ä¸­çš„ä¼ªæ ‡ç­¾ç”Ÿæˆã€‚è¯¥æ¡†æ¶å¼•å…¥ç»“æ„åŒ–è§†è§‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œå°†å¯¹è±¡ç†è§£åˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„æ­¥éª¤ï¼šåŒºåŸŸæ„ŸçŸ¥ã€ç±»åˆ«è¯†åˆ«ä»¥åŠèƒŒæ™¯å®šä½ã€‚é€šè¿‡ç»“åˆå¯¹æ¯”èƒŒæ™¯å­¦ä¹ ï¼ˆCBLï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨æ‹¥æŒ¤æˆ–é®æŒ¡çš„åœºæ™¯ä¸­å®ç°ç¨³å¥çš„ä¼ªæ ‡ç­¾ç”Ÿæˆï¼Œå¹¶åœ¨æ–°å‹ç±»åˆ«ä¼ªæ ‡ç­¾è´¨é‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CoT-PLæ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–è§†è§‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿›è¡Œä¼ªæ ‡ç­¾ç”Ÿæˆã€‚</li>
<li>CoT-PLå°†å¯¹è±¡ç†è§£åˆ†è§£ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šåŒºåŸŸæ„ŸçŸ¥ã€ç±»åˆ«è¯†åˆ«å’ŒèƒŒæ™¯å®šä½ã€‚</li>
<li>å¯¹æ¯”èƒŒæ™¯å­¦ä¹ ï¼ˆCBLï¼‰ç”¨äºä¿ƒè¿›ç‰¹å¾å’ŒèƒŒæ™¯ä¹‹é—´çš„ç‰¹å¾è§£è€¦ã€‚</li>
<li>CoT-PLåœ¨æ‹¥æŒ¤å’Œé®æŒ¡çš„åœºæ™¯ä¸­ç”Ÿæˆä¼ªæ ‡ç­¾æ—¶è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>ç›¸å¯¹äºæœ€ä½³å…ˆå‰æŠ€æœ¯ï¼ŒCoT-PLåœ¨æ–°å‹ç±»åˆ«ä¼ªæ ‡ç­¾è´¨é‡æ–¹é¢å®ç°äº†æ˜¾è‘—æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a51571a7753a8b8f1b0df0f2f6e96076~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725947&auth_key=1760725947-0-0-b3f10e1baaef5a1d5f421efc00e63913&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11ae2304570f3115aa020a2f8ca8e459~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725955&auth_key=1760725955-0-0-4b8d4665ab609855430051c356c3c04a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a7c94335e1212f32458fa6635cc6a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725961&auth_key=1760725961-0-0-d9105258af9b751f25317a0174e1e518&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a163431ddc0d286b1581671654c24eac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725969&auth_key=1760725969-0-0-a7f3dbbb1bf26de59c453ad62f38be2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pluto-A-Benchmark-for-Evaluating-Efficiency-of-LLM-generated-Hardware-Code"><a href="#Pluto-A-Benchmark-for-Evaluating-Efficiency-of-LLM-generated-Hardware-Code" class="headerlink" title="Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware   Code"></a>Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware   Code</h2><p><strong>Authors:Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda</strong></p>
<p>Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8%, delay efficiency of 65.9%, and power efficiency of 64.0% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºè‡ªåŠ¨åŒ–ç¡¬ä»¶è®¾è®¡ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç”ŸæˆVerilogä»£ç ã€‚è™½ç„¶æ—©æœŸçš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨åŠŸèƒ½æ­£ç¡®æ€§ä¸Šï¼Œä½†é«˜æ•ˆçš„ç¡¬ä»¶è®¾è®¡éœ€è¦é’ˆå¯¹åˆæˆæŒ‡æ ‡ï¼ˆå¦‚é¢ç§¯ã€å»¶è¿Ÿå’ŒåŠŸè€—ï¼‰è¿›è¡Œé¢å¤–çš„ä¼˜åŒ–ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨å…¨é¢è¯„ä¼°è¿™äº›æ–¹é¢æ—¶å­˜åœ¨ä¸è¶³ï¼šä»–ä»¬é€šå¸¸ç¼ºä¹é’ˆå¯¹éªŒè¯çš„ä¼˜åŒ–åŸºå‡†æˆ–æµ‹è¯•å¹³å°ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†Plutoï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMç”ŸæˆVerilogè®¾è®¡æ•ˆç‡çš„åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶ã€‚Plutoæä¾›äº†ä¸€å¥—åŒ…å«114ä¸ªé—®é¢˜çš„å…¨é¢è¯„ä¼°é›†ï¼Œå¸¦æœ‰è‡ªæˆ‘æ£€æŸ¥æµ‹è¯•å¹³å°å’Œå¤šä¸ªParetoæœ€ä¼˜å‚è€ƒå®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LLMå¯ä»¥è¾¾åˆ°é«˜è¾¾78.3%çš„é«˜åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆpass@1ï¼‰ï¼Œä½†å®ƒä»¬åœ¨åˆæˆæ•ˆç‡æ–¹é¢ä»ç„¶è½åäºä¸“å®¶è®¾è®¡çš„å®ç°ï¼Œé¢ç§¯æ•ˆç‡ä¸º63.8%ï¼Œå»¶è¿Ÿæ•ˆç‡ä¸º65.9%ï¼ŒåŠŸç‡æ•ˆç‡ä¸º64.0%ï¼ˆeff@1ï¼‰ã€‚è¿™å‡¸æ˜¾äº†éœ€è¦åƒPlutoè¿™æ ·çš„æ³¨é‡æ•ˆç‡çš„è¯„ä¼°æ¡†æ¶æ¥æ¨åŠ¨ä»¥ç¡¬ä»¶ä¸ºä¸­å¿ƒçš„LLMç ”ç©¶çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14756v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬Verilogä»£ç ç”Ÿæˆã€‚ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°åˆæˆåº¦é‡ï¼ˆå¦‚é¢ç§¯ã€å»¶è¿Ÿå’ŒåŠŸè€—ï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºPlutoï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°LLMç”ŸæˆVerilogè®¾è®¡æ•ˆç‡çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMåœ¨åŠŸèƒ½æ­£ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åˆæˆæ•ˆç‡æ–¹é¢ä»æœ‰å¾…æé«˜ã€‚è¿™å‡¸æ˜¾äº†éœ€è¦åƒPlutoè¿™æ ·çš„æ•ˆç‡æ„ŸçŸ¥è¯„ä¼°æ¡†æ¶æ¥æ¨åŠ¨ç¡¬ä»¶ç›¸å…³LLMç ”ç©¶çš„è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç°åœ¨è¢«å¹¿æ³›åº”ç”¨äºç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–ï¼ŒåŒ…æ‹¬Verilogä»£ç ç”Ÿæˆã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§ï¼Œä½†ç¡¬ä»¶è®¾è®¡è¿˜éœ€è¦è€ƒè™‘åˆæˆåº¦é‡ï¼Œå¦‚é¢ç§¯ã€å»¶è¿Ÿå’ŒåŠŸè€—çš„ä¼˜åŒ–ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è¿™äº›æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¼˜åŒ–åŸºå‡†æµ‹è¯•å’ŒéªŒè¯æµ‹è¯•å¹³å°ã€‚</li>
<li>æå‡ºPlutoåŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMç”Ÿæˆçš„Verilogè®¾è®¡çš„æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨åŠŸèƒ½æ­£ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åˆæˆæ•ˆç‡æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>LLMçš„åˆæˆæ•ˆç‡ä¸ä¸“å®¶æ‰‹å·¥å®ç°ç›¸æ¯”ä»æœ‰å·®è·ï¼Œè¡¨ç°åœ¨é¢ç§¯æ•ˆç‡ã€å»¶è¿Ÿæ•ˆç‡å’ŒåŠŸç‡æ•ˆç‡æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-58f89d2fdb071159c0ce377a954e4e9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725976&auth_key=1760725976-0-0-c826efc134cf7a4a221bbeefd45b0f34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c12f97feed90ab3004571190b88ea9e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725984&auth_key=1760725984-0-0-f954a7fe5a36546468b512ad1f733118&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7ead1398dc20cfb2330b12907002a36~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725991&auth_key=1760725991-0-0-51da046db572714351f42d67b0d12683&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-483d64631c6e49ddf96541820c86a4af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760725997&auth_key=1760725997-0-0-4b2c6238efd4a11c4c4209bd4c31913f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1daaa803ce713b4c337f05e7701951f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726004&auth_key=1760726004-0-0-1578dafcb7e4bb268f284277f2f45148&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models"><a href="#DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models" class="headerlink" title="DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models"></a>DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models</h2><p><strong>Authors:Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato</strong></p>
<p>Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifierâ€™s decision process without access to training data or ground-truth labels. We demonstrate DEXTERâ€™s flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter">https://github.com/perceivelab/dexter</a>. </p>
<blockquote>
<p>ç†è§£å’Œè§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡Œä¸ºå¯¹äºæ„å»ºé€æ˜å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†DEXTERï¼Œè¿™æ˜¯ä¸€ä¸ªæ— æ•°æ®çš„æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆè§†è§‰åˆ†ç±»å™¨çš„å…¨å±€æ–‡æœ¬è§£é‡Šã€‚DEXTERé€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥åˆæˆèƒ½å¼ºçƒˆæ¿€æ´»ç›®æ ‡åˆ†ç±»å™¨çš„ç±»åˆ«æ¡ä»¶å›¾åƒã€‚è¿™äº›åˆæˆæ ·æœ¬ç„¶åç”¨äºæ¿€å‘è¯¦ç»†çš„è‡ªç„¶è¯­è¨€æŠ¥å‘Šï¼Œæè¿°ç‰¹å®šç±»åˆ«çš„å†³ç­–æ¨¡å¼å’Œåè§ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼ŒDEXTERèƒ½å¤Ÿåœ¨æ²¡æœ‰è®¿é—®è®­ç»ƒæ•°æ®æˆ–çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œç”¨è‡ªç„¶è¯­è¨€è§£é‡Šåˆ†ç±»å™¨çš„å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†DEXTERåœ¨ä¸‰é¡¹ä»»åŠ¡ä¸­çš„çµæ´»æ€§â€”â€”æ¿€æ´»æœ€å¤§åŒ–ã€åˆ‡ç‰‡å‘ç°å’Œå»åä»¥åŠåè§è§£é‡Šâ€”â€”æ¯é¡¹ä»»åŠ¡éƒ½è¯´æ˜äº†å…¶åœ¨æ­ç¤ºè§†è§‰åˆ†ç±»å™¨å†…éƒ¨æœºåˆ¶æ–¹é¢çš„èƒ½åŠ›ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œè¡¨æ˜DEXTERäº§ç”Ÿçš„è¾“å‡ºå‡†ç¡®ä¸”æ˜“äºè§£é‡Šã€‚åœ¨ImageNetã€Waterbirdsã€CelebAå’ŒFairFacesä¸Šçš„å®éªŒè¯å®ï¼ŒDEXTERåœ¨å…¨å±€æ¨¡å‹è§£é‡Šå’Œç±»åˆ«å±‚é¢åè§æŠ¥å‘Šæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/perceivelab/dexterè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14741v1">PDF</a> Accepted to NeurIPS 2025 (spotlight)</p>
<p><strong>Summary</strong><br>æœºå™¨å­¦ä¹ æ¨¡å‹çš„è§£é‡Šæ€§å¯¹äºæ„å»ºé€æ˜å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚DEXTERæ¡†æ¶é€šè¿‡é‡‡ç”¨æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œç”Ÿæˆè§†è§‰åˆ†ç±»å™¨çš„å…¨å±€æ–‡æœ¬è§£é‡Šã€‚DEXTERé€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥åˆæˆèƒ½å¤Ÿå¼ºçƒˆæ¿€æ´»ç›®æ ‡åˆ†ç±»å™¨çš„ç±»æ¡ä»¶å›¾åƒï¼Œå¹¶åˆ©ç”¨è¿™äº›åˆæˆæ ·æœ¬å¼•å‡ºè¯¦ç»†çš„è‡ªç„¶è¯­è¨€æŠ¥å‘Šï¼Œæè¿°ç±»ç‰¹å®šçš„å†³ç­–æ¨¡å¼å’Œåè§ã€‚ä¸åŒäºä»¥å¾€çš„å·¥ä½œï¼ŒDEXTERæ— éœ€è®¿é—®è®­ç»ƒæ•°æ®æˆ–çœŸå®æ ‡ç­¾å³å¯ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šåˆ†ç±»å™¨çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨æ¿€æ´»æœ€å¤§åŒ–ã€åˆ‡ç‰‡å‘ç°ä¸å»åä»¥åŠåè§è§£é‡Šä¸‰ä¸ªä»»åŠ¡ä¸Šçš„æ¼”ç¤ºå±•ç¤ºäº†DEXTERæ­ç¤ºè§†è§‰åˆ†ç±»å™¨å†…éƒ¨æœºåˆ¶çš„èƒ½åŠ›ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°ä»¥åŠç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºDEXTERäº§ç”Ÿçš„è¾“å‡ºå‡†ç¡®ä¸”æ˜“äºç†è§£ã€‚åœ¨ImageNetã€Waterbirdsã€CelebAå’ŒFairFacesä¸Šçš„å®éªŒè¯å®ï¼ŒDEXTERåœ¨å…¨å±€æ¨¡å‹è§£é‡Šå’Œç±»åˆ«çº§åè§æŠ¥å‘Šæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter%E3%80%82">https://github.com/perceivelab/dexterã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DEXTERæ˜¯ä¸€ä¸ªæ•°æ®æ— å…³çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè§†è§‰åˆ†ç±»å™¨çš„å…¨å±€æ–‡æœ¬è§£é‡Šã€‚</li>
<li>å®ƒç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹æ¥åˆæˆç±»æ¡ä»¶å›¾åƒå¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€æŠ¥å‘Šæ¥è§£é‡Šåˆ†ç±»å™¨çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>DEXTERæ— éœ€è®¿é—®è®­ç»ƒæ•°æ®æˆ–çœŸå®æ ‡ç­¾å³å¯å·¥ä½œï¼Œä½¿å…¶æˆä¸ºä¸€ç§å¼ºå¤§çš„å·¥å…·ã€‚</li>
<li>DEXTERåœ¨å¤šç§ä»»åŠ¡ä¸Šå±•ç¤ºäº†å…¶èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¿€æ´»æœ€å¤§åŒ–ã€åˆ‡ç‰‡å‘ç°ä¸å»åä»¥åŠåè§è§£é‡Šã€‚</li>
<li>å®šé‡å’Œå®šæ€§è¯„ä¼°ä»¥åŠç”¨æˆ·ç ”ç©¶è¯æ˜DEXTERçš„è¾“å‡ºå‡†ç¡®ä¸”æ˜“äºç†è§£ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEXTERåœ¨å…¨å±€æ¨¡å‹è§£é‡Šå’Œç±»åˆ«çº§åˆ«åè§æŠ¥å‘Šæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7da9bc4c36b2a1e029838525c732fe31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726011&auth_key=1760726011-0-0-1f597b681f1d2d9ba01809e9fb03ead1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5487f02bf6fbb21fa801270a588322ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726018&auth_key=1760726018-0-0-df0b76390f0809bc70c38fc4ab9ae595&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AutoRubric-R1V-Rubric-Based-Generative-Rewards-for-Faithful-Multimodal-Reasoning"><a href="#AutoRubric-R1V-Rubric-Based-Generative-Rewards-for-Faithful-Multimodal-Reasoning" class="headerlink" title="AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal   Reasoning"></a>AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal   Reasoning</h2><p><strong>Authors:Mengzhao Jia, Zhihan Zhang, Ignacio Cases, Zheyuan Liu, Meng Jiang, Peng Qi</strong></p>
<p>Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»ä»æ„ŸçŸ¥ä»»åŠ¡è¿…é€Ÿå‘å±•åˆ°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¾€å¾€ä¼šå¯¼è‡´æ¨ç†é”™è¯¯ï¼Œå› ä¸ºåªå¯¹æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§è¿›è¡Œå¥–åŠ±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AutoRubric-R1Væ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŸºäºè‡ªåŠ¨æ”¶é›†çš„rubircç”Ÿæˆå¥–åŠ±å°†RLVRä¸è¿‡ç¨‹çº§ç›‘ç£ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åœ¨äºä¸€ç§å¯æ‰©å±•çš„è‡ªæˆ‘èšåˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»æˆåŠŸçš„è½¨è¿¹ä¸­æç‚¼å‡ºä¸€è‡´çš„æ¨ç†æ£€æŸ¥ç‚¹ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€äººå·¥æ³¨é‡Šæˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹çš„æƒ…å†µä¸‹æ„å»ºç‰¹å®šé—®é¢˜çš„rubircã€‚é€šè¿‡è”åˆåˆ©ç”¨rubircå’Œç»“æœå¥–åŠ±ï¼ŒAutoRubric-R1Våœ¨å…­ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨ä¸“é¡¹è¯„ä¼°ä¸­å¤§å¤§æé«˜äº†æ¨ç†çš„å¿ å®åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14738v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ä»æ„ŸçŸ¥ä»»åŠ¡è¿…é€Ÿå‘å±•åˆ°å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ï¼Œç„¶è€Œå¼ºåŒ–å­¦ä¹ åŠ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šå¸¸åªå¥–åŠ±æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´å‡ºç°é”™è¯¯æ¨ç†ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºAutoRubric-R1Væ¡†æ¶ï¼Œå®ƒç»“åˆäº†RLVRä¸åŸºäºè‡ªåŠ¨æ”¶é›†è¯„åˆ†æ ‡å‡†çš„è¿‡ç¨‹çº§ç›‘ç£æ¥ç”Ÿæˆå¥–åŠ±ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åœ¨äºé‡‡ç”¨å¯æ‰©å±•çš„è‡ªæˆ‘èšåˆæ–¹æ³•ï¼Œä»æˆåŠŸçš„è½¨è¿¹ä¸­æç‚¼å‡ºä¸€è‡´çš„æ¨ç†æ£€æŸ¥ç‚¹ï¼Œå®ç°ç‰¹å®šé—®é¢˜çš„è¯„åˆ†æ ‡å‡†æ„å»ºï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ã€‚é€šè¿‡è”åˆåˆ©ç”¨åŸºäºè¯„åˆ†æ ‡å‡†çš„å¥–åŠ±å’Œç»“æœå¥–åŠ±ï¼ŒAutoRubric-R1Våœ¨å…­ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨ä¸“é¡¹è¯„ä¼°ä¸­å¤§å¤§æé«˜äº†æ¨ç†çš„å¿ å®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²æ‰©å±•åˆ°å¤æ‚å¤šæ­¥éª¤æ¨ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åŠ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å­˜åœ¨åªå¥–åŠ±æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å±€é™ï¼Œå¯¼è‡´å¯èƒ½å‡ºç°é”™è¯¯æ¨ç†ã€‚</li>
<li>æå‡ºAutoRubric-R1Væ¡†æ¶ï¼Œç»“åˆRLVRä¸è¿‡ç¨‹çº§ç›‘ç£ï¼Œé€šè¿‡è‡ªåŠ¨æ”¶é›†è¯„åˆ†æ ‡å‡†ç”Ÿæˆå¥–åŠ±ã€‚</li>
<li>AutoRubric-R1Vçš„å…³é”®åˆ›æ–°åœ¨äºé‡‡ç”¨è‡ªæˆ‘èšåˆæ–¹æ³•æç‚¼æ¨ç†æ£€æŸ¥ç‚¹ï¼Œå®ç°ç‰¹å®šé—®é¢˜çš„è¯„åˆ†æ ‡å‡†æ„å»ºï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–å¼ºå¤§æ•™å¸ˆæ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†åœ¨å…­ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>AutoRubric-R1Væé«˜äº†åœ¨ä¸“é¡¹è¯„ä¼°ä¸­çš„æ¨ç†å¿ å®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-389ae6802a4527d0076c096ad5671dfe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726026&auth_key=1760726026-0-0-9bc35051a7d25e4b69c9d7c05ce28afa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd11cd2b9fbb25decd60857e98717d10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726034&auth_key=1760726034-0-0-c284bc43835291efe25bd92a08c1cbca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-25a280213bdf756d0bfa98baf3b90ee4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726042&auth_key=1760726042-0-0-83ccb2fb486a838d67fea3d1f53d5c9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b691f67bddc6357224212d9899de288f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726049&auth_key=1760726049-0-0-1ee72e1409ba1928f3e273c125fa361a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VTimeCoT-Thinking-by-Drawing-for-Video-Temporal-Grounding-and-Reasoning"><a href="#VTimeCoT-Thinking-by-Drawing-for-Video-Temporal-Grounding-and-Reasoning" class="headerlink" title="VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning"></a>VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</h2><p><strong>Authors:Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma</strong></p>
<p>In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: <a target="_blank" rel="noopener" href="https://vtimecot.github.io/">https://vtimecot.github.io</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§†é¢‘é—®ç­”å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œè¿™å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å·¨å¤§è¿›æ­¥æ‰€å¸¦æ¥çš„å¥½å¤„ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è§†é¢‘æ—¶é—´å®šä½å’Œæ¨ç†é¢†åŸŸå­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼Œç»™å‘å±•æœ‰æ•ˆçš„ç°å®ä¸–ç•Œè§†é¢‘ç†è§£ç³»ç»Ÿå¸¦æ¥äº†æŒ‘æˆ˜ã€‚å—äººç±»å¦‚ä½•ä½¿ç”¨è§†é¢‘æ’­æ”¾å™¨ä¸è¿›åº¦æ¡è¿›è¡Œè§†é¢‘ç†è§£çš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VTimeCoTï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆã€æ— éœ€è®­ç»ƒçš„åŸ¹è®­æ¡†æ¶ï¼Œä¸“ä¸ºé«˜æ€§èƒ½è§†é¢‘å®šä½å’Œæ¨ç†è€Œè®¾è®¡ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è¿›åº¦æ¡çš„ä¸¤ç§æ–°å‹è§†è§‰å·¥å…·ï¼šå³æ’å³ç”¨çš„è¿›åº¦æ¡é›†æˆå·¥å…·å’Œé«˜æ•ˆé«˜äº®å·¥å…·ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³åŸºäºæ–‡æœ¬çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰æ—¶é—´CoTè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹èåˆäº†è§†é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„è·¨æ¨¡æ€æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘æ—¶é—´å®šä½å’ŒåŸºäºæ¨ç†çš„é—®é¢˜å›ç­”ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æé«˜äº†åœ¨Qwen2VL-7Bå’ŒGPT4oåŸºçº¿ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„æ¡†æ¶å®ç°äº†ç»„åˆå’Œå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://vtimecot.github.io/">https://vtimecot.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14672v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘é—®ç­”é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ä»å­˜åœ¨è§†é¢‘æ—¶ç©ºå®šä½ä¸æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºVTimeCoTçš„è®­ç»ƒå…æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è§†é¢‘æ’­æ”¾å™¨ä¸­çš„è¿›åº¦æ¡å·¥å…·ï¼Œå®ç°é«˜æ€§èƒ½çš„è§†é¢‘å®šä½ä¸æ¨ç†ã€‚è¯¥æ¡†æ¶å¼•å…¥ä¸¤ç§æ–°é¢–çš„è§†è§‰å·¥å…·å¹¶æ”¹è¿›ä¼ ç»ŸåŸºäºæ–‡æœ¬çš„æ€è€ƒé“¾æ–¹æ³•ï¼Œå®ç°åœ¨è§†é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„è·¨æ¨¡æ€æ¨ç†ã€‚åœ¨Qwen2VL-7Bå’ŒGPT4oåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨è§†é¢‘æ—¶ç©ºå®šä½ä¸åŸºäºæ¨ç†çš„é—®é¢˜å›ç­”æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘é—®ç­”é¢†åŸŸåŸºäºå¤šåª’ä½“æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹å—åˆ°å…³æ³¨ï¼Œä½†ä»å­˜åœ¨è§†é¢‘æ—¶ç©ºå®šä½ä¸æ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>VTimeCoTæ¡†æ¶æ˜¯ä¸€ç§è®­ç»ƒå…æ¡†æ¶ï¼Œèƒ½å¤Ÿåˆ©ç”¨è§†é¢‘æ’­æ”¾å™¨ä¸­çš„è¿›åº¦æ¡å·¥å…·å®ç°é«˜æ€§èƒ½çš„è§†é¢‘å®šä½ä¸æ¨ç†ã€‚</li>
<li>VTimeCoTæ¡†æ¶å¼•å…¥ä¸¤ç§æ–°é¢–çš„è§†è§‰å·¥å…·ï¼šè¿›åº¦æ¡é›†æˆå·¥å…·å’Œé«˜æ•ˆé«˜äº®å·¥å…·ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¹è¿›äº†ä¼ ç»ŸåŸºäºæ–‡æœ¬çš„æ€è€ƒé“¾æ–¹æ³•ï¼Œå®ç°è·¨æ¨¡æ€æ¨ç†ï¼Œç»“åˆè§†é¢‘å’Œæ–‡æœ¬æ•°æ®ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVTimeCoTæ¡†æ¶åœ¨è§†é¢‘æ—¶ç©ºå®šä½ä¸åŸºäºæ¨ç†çš„é—®é¢˜å›ç­”æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>VTimeCoTæ¡†æ¶å®ç°äº†ç»„åˆå’Œå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-803a1824e93e3aace39416e4585d658b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726056&auth_key=1760726056-0-0-eb7ad1265066346bfc9a92f56254ad5c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2cee3b97e88a7428b3a357bd74d5d3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726063&auth_key=1760726063-0-0-10756b807de8374e2d3007a6923a8a3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-991727758a3ebe7cf043f4c90362fcb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726069&auth_key=1760726069-0-0-d0c2f22bde6488cd186039a3dcf4c4d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b649e52c6a5da695701b53cf48a35d74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726076&auth_key=1760726076-0-0-08478e530a28c2409ff3b33732a3be9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MR-Rec-Synergizing-Memory-and-Reasoning-for-Personalized-Recommendation-Assistant-with-LLMs"><a href="#MR-Rec-Synergizing-Memory-and-Reasoning-for-Personalized-Recommendation-Assistant-with-LLMs" class="headerlink" title="MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation   Assistant with LLMs"></a>MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation   Assistant with LLMs</h2><p><strong>Authors:Jiani Huang, Xingchen Zou, Lianghao Xia, Qing Li</strong></p>
<p>The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºæ¨èç³»ç»Ÿé¢ä¸´ç€å®ç°æ·±åº¦ä¸ªæ€§åŒ–å’Œæ™ºèƒ½æ¨ç†çš„å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨äº¤äº’å¼åœºæ™¯ä¸­ã€‚å½“å‰çš„æ–¹æ³•å¸¸å¸¸å—é™äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£å’Œå•è½®æ¨ç†ï¼Œæ— æ³•æ•æ‰åŠ¨æ€çš„ç”¨æˆ·åå¥½å’Œåœ¨æ¨èä¸Šä¸‹æ–‡ä¸­è¿›è¡Œä¸»åŠ¨æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MR.Recï¼Œä¸€ä¸ªç»“åˆè®°å¿†å’Œæ¨ç†çš„æ–°å‹LLMæ¨èæ¡†æ¶ã€‚ä¸ºäº†å®ç°ä¸ªæ€§åŒ–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆåœ°ç´¢å¼•å’Œæ£€ç´¢ç›¸å…³çš„å¤–éƒ¨è®°å¿†ï¼Œä»¥å¢å¼ºLLMçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†å®ç°è®°å¿†å’Œæ¨ç†ä¹‹é—´çš„ååŒä½œç”¨ï¼Œæˆ‘ä»¬çš„RAGç³»ç»Ÿè¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºæŸ¥è¯¢çš„æ£€ç´¢ï¼Œé€šè¿‡æ•´åˆå¢å¼ºæ¨ç†çš„è®°å¿†æ£€ç´¢ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè®­ç»ƒLLMè‡ªä¸»å­¦ä¹ æœ‰æ•ˆçš„ç­–ç•¥æ¥è¿›è¡Œè®°å¿†åˆ©ç”¨å’Œæ¨ç†ä¼˜åŒ–ã€‚é€šè¿‡ç»“åˆåŠ¨æ€è®°å¿†æ£€ç´¢å’Œè‡ªé€‚åº”æ¨ç†ï¼Œè¿™ç§æ–¹æ³•ç¡®ä¿äº†æ›´å‡†ç¡®ã€å…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†å’Œé«˜åº¦çš„ä¸ªæ€§åŒ–æ¨èã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMR.Recåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯æ°´å¹³çš„åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†å…¶åœ¨æä¾›æ™ºèƒ½å’Œä¸ªæ€§åŒ–æ¨èæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡é€šçŸ¥å‘å¸ƒåï¼Œæˆ‘ä»¬å°†å…¬å¼€ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨é¢ä¸´ä¸ªæ€§åŒ–æ·±åº¦ä¸æ™ºèƒ½æ¨ç†çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨äº¤äº’å¼åœºæ™¯ä¸­ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MR.Recæ¡†æ¶ï¼Œé€šè¿‡è®°å¿†ä¸æ¨ç†çš„ååŒå·¥ä½œæ¥æå‡LLMçš„æ¨èèƒ½åŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„åŸºäºæ£€ç´¢çš„ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œé€šè¿‡æœ‰æ•ˆåœ°ç´¢å¼•å’Œæ£€ç´¢ç›¸å…³å¤–éƒ¨è®°å¿†ï¼Œå¢å¼ºLLMçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè®­ç»ƒLLMè‡ªä¸»å­¦ä¹ æœ‰æ•ˆçš„è®°å¿†åˆ©ç”¨å’Œæ¨ç†ä¼˜åŒ–ç­–ç•¥ã€‚é€šè¿‡ç»“åˆåŠ¨æ€è®°å¿†æ£€ç´¢ä¸è‡ªé€‚åº”æ¨ç†ï¼Œç¡®ä¿æ›´ç²¾å‡†ã€æƒ…å¢ƒæ„ŸçŸ¥å’Œä¸ªæ€§åŒ–çš„æ¨èã€‚å®éªŒè¯æ˜ï¼ŒMR.Recåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶åœ¨æä¾›æ™ºèƒ½ä¸ä¸ªæ€§åŒ–æ¨èæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨æ¨èç³»ç»Ÿä¸­é¢ä¸´ä¸ªæ€§åŒ–ä¸æ™ºèƒ½æ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>MR.Recæ¡†æ¶é€šè¿‡è®°å¿†ä¸æ¨ç†çš„ååŒå·¥ä½œæå‡LLMçš„æ¨èèƒ½åŠ›ã€‚</li>
<li>RAGç³»ç»Ÿé€šè¿‡æ£€ç´¢å¤–éƒ¨è®°å¿†å¢å¼ºLLMçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶ç”¨äºè®­ç»ƒLLMåœ¨è®°å¿†åˆ©ç”¨å’Œæ¨ç†ä¼˜åŒ–æ–¹é¢çš„ç­–ç•¥ã€‚</li>
<li>ç»“åˆåŠ¨æ€è®°å¿†æ£€ç´¢ä¸è‡ªé€‚åº”æ¨ç†ï¼Œç¡®ä¿ç²¾å‡†ã€æƒ…å¢ƒæ„ŸçŸ¥å’Œä¸ªæ€§åŒ–çš„æ¨èã€‚</li>
<li>MR.Recåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>å°†å‘å¸ƒä»£ç å’Œæ•°æ®ä»¥ä¾›è®ºæ–‡é€šçŸ¥ä½¿ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-aeb19ce6d2398ccde531923a20f210b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726084&auth_key=1760726084-0-0-ba9f50d9d77c1d4b564fa5fd85b0ac11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4af957f63b08c2752075fc2c2f13d7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726092&auth_key=1760726092-0-0-a53a65026dc21de436c0cc649fc316cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-095481680595c7dddaa766229f4835e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726099&auth_key=1760726099-0-0-38b9b3b28287d28515ac77424cebf036&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering"><a href="#Knowledge-based-Visual-Question-Answer-with-Multimodal-Processing-Retrieval-and-Filtering" class="headerlink" title="Knowledge-based Visual Question Answer with Multimodal Processing,   Retrieval and Filtering"></a>Knowledge-based Visual Question Answer with Multimodal Processing,   Retrieval and Filtering</h2><p><strong>Authors:Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye</strong></p>
<p>Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the modelâ€™s reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/cqu-student/Wiki-PRF">https://github.com/cqu-student/Wiki-PRF</a> </p>
<blockquote>
<p>åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”ï¼ˆKB-VQAï¼‰éœ€è¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°†è§†è§‰ç†è§£ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ç›¸ç»“åˆã€‚å°½ç®¡é€šè¿‡ç»“åˆçŸ¥è¯†åº“æŸ¥è¯¢çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨æ­¤ä»»åŠ¡ä¸Šå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å®ƒä»ç„¶é¢ä¸´ç€å¤šæ¨¡æ€æŸ¥è¯¢çš„è´¨é‡å’Œæ£€ç´¢ç»“æœçš„ç›¸å…³æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œç§°ä¸ºWiki-PRFï¼ŒåŒ…æ‹¬å¤„ç†ã€æ£€ç´¢å’Œè¿‡æ»¤ä¸‰ä¸ªé˜¶æ®µã€‚å¤„ç†é˜¶æ®µåŠ¨æ€è°ƒç”¨è§†è§‰å·¥å…·æ¥æå–ç”¨äºæ£€ç´¢çš„ç²¾ç¡®å¤šæ¨¡æ€ä¿¡æ¯ã€‚æ£€ç´¢é˜¶æ®µèåˆäº†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œä»¥å®ç°å¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢ã€‚è¿‡æ»¤é˜¶æ®µå¯¹æ£€ç´¢ç»“æœè¿›è¡Œç›¸å…³æ€§è¿‡æ»¤å’Œé›†ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹å¼ä»¥ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ ¼å¼ä¸€è‡´æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·è¿›è¡Œè®­ç»ƒã€‚è¿™å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€å·¥å…·è°ƒç”¨ä»¥ç”Ÿæˆå‡†ç¡®æŸ¥è¯¢ä»¥åŠè¿‡æ»¤æ— å…³å†…å®¹çš„èƒ½åŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ï¼ˆE-VQAå’ŒInfoSeekï¼‰ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨ç­”æ¡ˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ˆåˆ†åˆ«ä¸º36.0å’Œ42.8ï¼‰ï¼Œè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cqu-student/Wiki-PRF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cqu-student/Wiki-PRFæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14605v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”ï¼ˆKB-VQAï¼‰éœ€è¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ•´åˆè§†è§‰ç†è§£ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ã€‚å°½ç®¡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨æ­¤ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤šåª’ä½“æŸ¥è¯¢çš„è´¨é‡å’Œæ£€ç´¢ç»“æœçš„ç›¸å…³æ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œç§°ä¸ºWiki-PRFï¼ŒåŒ…æ‹¬å¤„ç†ã€æ£€ç´¢å’Œè¿‡æ»¤ä¸‰ä¸ªé˜¶æ®µã€‚å¤„ç†é˜¶æ®µåŠ¨æ€è°ƒç”¨è§†è§‰å·¥å…·æå–ç²¾ç¡®çš„å¤šåª’ä½“ä¿¡æ¯è¿›è¡Œæ£€ç´¢ï¼›æ£€ç´¢é˜¶æ®µèåˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾å®ç°å¤šåª’ä½“çŸ¥è¯†æ£€ç´¢ï¼›è¿‡æ»¤é˜¶æ®µå¯¹æ£€ç´¢ç»“æœè¿›è¡Œç›¸å…³æ€§è¿‡æ»¤å’Œé›†ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹å¼ä»¥ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ ¼å¼ä¸€è‡´æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€å·¥å…·è°ƒç”¨ç²¾å‡†æŸ¥è¯¢ä»¥åŠè¿‡æ»¤æ— å…³å†…å®¹çš„èƒ½åŠ›ã€‚åœ¨E-VQAå’ŒInfoSeekç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç­”æ¡ˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ˆåˆ†åˆ«æé«˜äº†36.0å’Œ42.8ï¼‰ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KB-VQAä»»åŠ¡éœ€è¦æ•´åˆè§†è§‰ç†è§£ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼Œä¾èµ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨å¤šåª’ä½“æŸ¥è¯¢çš„è´¨é‡å’Œæ£€ç´¢ç»“æœçš„ç›¸å…³æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Wiki-PRFæ˜¯ä¸€ç§æ–°çš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬å¤„ç†ã€æ£€ç´¢å’Œè¿‡æ»¤ï¼Œé’ˆå¯¹ä»¥ä¸ŠæŒ‘æˆ˜è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>å¤„ç†é˜¶æ®µåŠ¨æ€è°ƒç”¨è§†è§‰å·¥å…·æå–ç²¾ç¡®ä¿¡æ¯ã€‚</li>
<li>æ£€ç´¢é˜¶æ®µèåˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œå¤šåª’ä½“çŸ¥è¯†æ£€ç´¢ã€‚</li>
<li>è¿‡æ»¤é˜¶æ®µå¯¹æ£€ç´¢ç»“æœè¿›è¡Œç›¸å…³æ€§è¿‡æ»¤å’Œé›†ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d41e079b15f1690f8bdec842f016a4e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726106&auth_key=1760726106-0-0-aca2a9f72b9739dc15ba1b56c6f1ff2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0728f7ecdf9af3788e4197cb6c8c27c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726114&auth_key=1760726114-0-0-32f8d90a8c08144df54d030177739466&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cbe9f65a1b7514bf1214943f218a444d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726121&auth_key=1760726121-0-0-07863da92e7f49519c5810e981ec7ce8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Talking-Points-Describing-and-Localizing-Pixels"><a href="#Talking-Points-Describing-and-Localizing-Pixels" class="headerlink" title="Talking Points: Describing and Localizing Pixels"></a>Talking Points: Describing and Localizing Pixels</h2><p><strong>Authors:Matan Rusanovsky, Shimon Malnick, Shai Avidan</strong></p>
<p>Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points">https://github.com/matanr/Talking_Points</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶å±€é™äºå¯¹è±¡çº§åˆ«æˆ–åŒºåŸŸçº§åˆ«çš„å®šä½ï¼Œç¼ºä¹é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œåƒç´ ç²¾ç¡®å…³é”®ç‚¹ç†è§£çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åƒç´ çº§åˆ«å®šä½æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªäº’è¡¥çš„ç»„ä»¶æ„æˆï¼šä¸€ä¸ªç‚¹æè¿°ç¬¦ï¼Œç”¨äºç”Ÿæˆå•ä¸ªå…³é”®ç‚¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡æè¿°ï¼›ä¸€ä¸ªç‚¹å®šä½å™¨ï¼Œç”¨äºä»è¿™äº›æè¿°ä¸­å›å½’ç²¾ç¡®çš„åƒç´ åæ ‡ã€‚ä¸ä»¥å¾€ä¾èµ–äºæ¨¡æ¿æç¤ºæˆ–å…³é”®ç‚¹åç§°çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆè‡ªç”±å½¢å¼ã€ä»ç²—åˆ°ç»†çš„æè¿°ï¼Œå°†å…³é”®ç‚¹ç½®äºå…¶è§†è§‰ä¸Šä¸‹æ–‡ä¸­ã€‚ç”±äºç›®å‰æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†æ¥è®­ç»ƒè¿™æ ·çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†LlamaPointInPartæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„ç”±2ä¸‡å¤šä¸ªå›¾åƒ-å…³é”®ç‚¹-æè¿°ä¸‰å…ƒç»„åˆæˆçš„æ•°æ®é›†ï¼Œä»åœºæ™¯çº§åˆ«çš„ä¸Šä¸‹æ–‡åˆ°å…³é”®ç‚¹çš„è§†è§‰ç‰¹å¾ï¼Œæ•æ‰å¤šå°ºåº¦ä¿¡æ¯ã€‚ä¸ºäº†å®ç°è·¨ç±»åˆ«çš„æ³›åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨GRPOä¼˜åŒ–äº†AP-10Kä¸Šçš„ç‚¹æè¿°ç¬¦ï¼Œå¹¶ä½¿ç”¨å†»ç»“çš„ç‚¹å®šä½å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œä»¥äº§ç”Ÿæœ€å¤§åŒ–å®šä½ç²¾åº¦çš„æè¿°ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„ç»“æœï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åè®®ã€‚ä¸åŒäºå°†æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„æ–‡æœ¬æè¿°ä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨å®šä½å™¨æ¥ç¡®å®šé¢„æµ‹ç‚¹ä¸çœŸå®ç‚¹ä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦ã€‚åœ¨LlamaPointInPartæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¡†æ¶çš„åŒå‘æ€§è´¨å°†æœ‰åŠ©äºæœªæ¥åœ¨å…³é”®ç‚¹å¼•å¯¼çš„å›¾åƒç†è§£å’Œè¯­è¨€å¼•å¯¼çš„ç²¾ç¡®å®šä½æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/matanr/Talking_Pointså…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åƒç´ çº§å®šä½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªäº’è¡¥ç»„ä»¶ï¼šç‚¹æè¿°ç¬¦å’Œç‚¹å®šä½å™¨ã€‚æ¡†æ¶å¯ä»¥å®ç°é€šè¿‡è‡ªç„¶è¯­è¨€å¯¹åƒç´ çº§å…³é”®ç‚¹çš„ç²¾ç¡®ç†è§£ï¼Œä¸”ç”Ÿæˆä¸°å¯Œã€ä¸Šä¸‹æ–‡çš„å•ä¸ªå…³é”®ç‚¹æè¿°ã€‚ç”±äºç¼ºå°‘å¯ç”¨çš„æ•°æ®é›†æ¥è®­ç»ƒæ­¤ç±»ç³»ç»Ÿï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†LlamaPointInPartæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹åˆæˆï¼ŒåŒ…å«è¶…è¿‡ä¸¤ä¸‡å¼ å›¾åƒã€å…³é”®ç‚¹å’Œæè¿°ä¸‰å…ƒç»„ï¼Œèƒ½å¤Ÿæ•æ‰ä»åœºæ™¯çº§ä¸Šä¸‹æ–‡åˆ°å…³é”®ç‚¹å‘¨å›´è§†è§‰ç‰¹å¾çš„å¤šå°ºåº¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œè¯¥æ¡†æ¶åœ¨LlamaPointInPartæ•°æ®é›†ä¸Šçš„è¡¨ç°æ›´ä¼˜ã€‚ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„åƒç´ çº§å®šä½æ¡†æ¶ï¼Œèƒ½å®ç°é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œåƒç´ ç²¾ç¡®çš„å…³é”®ç‚¹ç†è§£ã€‚</li>
<li>åŒ…å«ç‚¹æè¿°ç¬¦å’Œç‚¹å®šä½å™¨ä¸¤ä¸ªäº’è¡¥ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆå…³é”®ç‚¹çš„ä¸°å¯Œæè¿°å’Œå›å½’ç²¾ç¡®åƒç´ åæ ‡ã€‚</li>
<li>å¼•å…¥äº†LlamaPointInPartæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ­¤ç±»ç³»ç»Ÿï¼ŒåŒ…å«å›¾åƒã€å…³é”®ç‚¹å’Œæè¿°çš„ä¸‰å…ƒç»„ã€‚</li>
<li>é€šè¿‡åˆæˆæ–¹æ³•ä»å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ•è·å¤šå°ºåº¦ä¿¡æ¯ã€‚</li>
<li>ä¼˜åŒ–ç‚¹æè¿°ç¬¦åœ¨AP-10Kä¸Šçš„æ€§èƒ½ï¼Œä½¿ç”¨å†»ç»“çš„ç‚¹å®šä½å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œä»¥æœ€å¤§åŒ–å®šä½å‡†ç¡®æ€§ã€‚</li>
<li>å»ºç«‹æ–°çš„è¯„ä¼°åè®®ï¼Œä¸å†æ¯”è¾ƒæ–¹æ³•äº§ç”Ÿçš„æ–‡æœ¬æè¿°ä¸åœ°é¢çœŸå®æƒ…å†µï¼Œè€Œæ˜¯ä½¿ç”¨å®šä½å™¨æ¥ç¡®å®šé¢„æµ‹ç‚¹ä¸åœ°é¢çœŸå®ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-652edad153a670e8ef96c41230c0dec3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726128&auth_key=1760726128-0-0-39e8c501e6c35aad83761d8a0b6b187f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4cf2fa892aa95fc6f20cac393650257f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726138&auth_key=1760726138-0-0-bbc288d0396c38a49041097d88f514ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40a8a230e32586222cc7cab5d40aa88b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726145&auth_key=1760726145-0-0-6298d11016d389da12a532eb48d268fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a061fcf875e5f5c403e7da25446dd445~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726152&auth_key=1760726152-0-0-73a9428296ed795e73871ec8f1549d20&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Launching-AoyuX-A-25-Year-Pseudo-Prospective-Earthquake-Forecasting-Experiment-at-the-China-Seismic-Experimental-Site"><a href="#Launching-AoyuX-A-25-Year-Pseudo-Prospective-Earthquake-Forecasting-Experiment-at-the-China-Seismic-Experimental-Site" class="headerlink" title="Launching AoyuX: A 25-Year Pseudo-Prospective Earthquake Forecasting   Experiment at the China Seismic Experimental Site"></a>Launching AoyuX: A 25-Year Pseudo-Prospective Earthquake Forecasting   Experiment at the China Seismic Experimental Site</h2><p><strong>Authors:Jiawei Li, Qingyuan Zhang, Didier Sornette</strong></p>
<p>Forecast models in statistical seismology are commonly evaluated with log-likelihood scores of the full distribution P(n) of earthquake numbers, yet heavy tails and out-of-range observations can bias model ranking. We develop a tail-aware evaluation framework that estimates cell-wise P(n) using adaptive Gaussian kernel density estimation and tests three strategies for handling out-of-range counts. Using the AoyuX platform, we perform a ~25-year month-by-month pseudo-prospective forecast experiment in the China Seismic Experimental Site (CSES), comparing Epidemic-Type Aftershock Sequence (ETAS) model with a homogeneous background (ETAS{\mu}) to a spatially heterogeneous variant (ETAS{\mu}(x,y)) across six spatial resolutions and five magnitude thresholds. Empirical probability density functions (PDFs) of counts per cell are well described by power laws with exponents a &#x3D; 1.40 +- 0.21 across all settings. Using previous theoretical results, this provides a robust estimate of the productivity exponent, {\alpha} &#x3D; 0.57 +- 0.08 using a b-value equal to 0.8, providing a valuable quantification of this key parameter in aftershock modeling. Model ranking is sensitive to how the tail of the full distribution P(n) of earthquake counts is treated: power law extrapolation is both theoretically justified and empirically the most robust. Cumulative information gain (CIG) shows that ETAS{\mu}(x,y) outperforms ETAS{\mu} in data-rich configurations, whereas in data-poor settings stochastic fluctuations dominate. A coefficient-of-variation analysis of per-window log-likelihood differences distinguishes genuine upward trends in CIG from noise-dominated fluctuations. By aligning a fat-tail-aware scoring methodology with an open testing platform, our work advances fair and statistically grounded assessment of earthquake forecasting models for the CSES and beyond. </p>
<blockquote>
<p>åœ¨ç»Ÿè®¡åœ°éœ‡å­¦ä¸­ï¼Œé€šå¸¸ä½¿ç”¨å¯¹æ•°ä¼¼ç„¶åˆ†æ•°æ¥è¯„ä¼°åœ°éœ‡æ¬¡æ•°å…¨åˆ†å¸ƒP(n)çš„é¢„æµ‹æ¨¡å‹ï¼Œä½†å°¾éƒ¨åé‡å’Œè§‚æµ‹èŒƒå›´å¤–çš„æ•°æ®å¯èƒ½ä¼šå½±å“æ¨¡å‹æ’åã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå°¾éƒ¨æ„ŸçŸ¥è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨è‡ªé€‚åº”é«˜æ–¯æ ¸å¯†åº¦ä¼°è®¡æ¥ä¼°ç®—å•å…ƒçº§çš„P(n)ï¼Œå¹¶é‡‡ç”¨äº†ä¸‰ç§ç­–ç•¥æ¥å¤„ç†è§‚æµ‹èŒƒå›´å¤–çš„æ•°æ®ã€‚æˆ‘ä»¬åˆ©ç”¨AoyuXå¹³å°ï¼Œåœ¨ä¸­å›½åœ°éœ‡å®éªŒåŒºï¼ˆCSESï¼‰è¿›è¡Œäº†é•¿è¾¾çº¦25ä¸ªæœˆçš„é€æœˆä¼ªå‰ç»æ€§é¢„æµ‹å®éªŒï¼Œæ¯”è¾ƒäº†å…·æœ‰åŒè´¨èƒŒæ™¯çš„æµè¡Œå‹ä½™éœ‡åºåˆ—ï¼ˆETASÎ¼ï¼‰æ¨¡å‹ä¸ç©ºé—´å¼‚è´¨çš„ETASÎ¼ï¼ˆxï¼Œyï¼‰æ¨¡å‹ï¼Œæ¶‰åŠå…­ç§ç©ºé—´åˆ†è¾¨ç‡å’Œäº”ç§éœ‡çº§é˜ˆå€¼ã€‚æ¯ä¸ªå•å…ƒçš„è®¡æ•°ç»éªŒæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰å¾ˆå¥½åœ°éµå¾ªå¹‚å¾‹ï¼ŒæŒ‡æ•°ä¸ºa&#x3D;1.40Â±0.21ã€‚æ ¹æ®ä¹‹å‰çš„ç†è®ºç»“æœï¼Œä½¿ç”¨bå€¼ç­‰äº0.8æ—¶ï¼Œè¿™æä¾›äº†ç”Ÿäº§åŠ›æŒ‡æ•°Î±&#x3D;0.57Â±0.08çš„ç¨³å¥ä¼°è®¡å€¼ï¼Œä¸ºä½™éœ‡å»ºæ¨¡ä¸­è¿™ä¸€å…³é”®å‚æ•°æä¾›äº†æœ‰ä»·å€¼çš„é‡åŒ–æŒ‡æ ‡ã€‚æ¨¡å‹æ’åå¯¹åœ°éœ‡è®¡æ•°å…¨åˆ†å¸ƒP(n)å°¾éƒ¨å¤„ç†æ–¹å¼æ•æ„Ÿï¼šå¹‚å¾‹å¤–æ¨æ—¢æœ‰ç†è®ºæ”¯æŒåˆåœ¨å®è·µä¸­æœ€ä¸ºç¨³å¥ã€‚ç´¯ç§¯ä¿¡æ¯å¢ç›Šï¼ˆCIGï¼‰æ˜¾ç¤ºï¼Œåœ¨æ•°æ®ä¸°å¯Œçš„é…ç½®ä¸­ï¼ŒETASÎ¼ï¼ˆxï¼Œyï¼‰çš„è¡¨ç°ä¼˜äºETASÎ¼ï¼›è€Œåœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­ï¼Œéšæœºæ³¢åŠ¨å æ®ä¸»å¯¼ã€‚çª—å£å¯¹æ•°ä¼¼ç„¶å·®å¼‚çš„å˜åŒ–ç³»æ•°åˆ†æå°†ç´¯ç§¯ä¿¡æ¯å¢ç›Šä¸­çš„çœŸæ­£ä¸Šå‡è¶‹åŠ¿ä¸å™ªå£°ä¸»å¯¼çš„æ³¢åŠ¨åŒºåˆ†å¼€æ¥ã€‚é€šè¿‡é‡‡ç”¨å…¼é¡¾è‚¥å°¾æ„ŸçŸ¥è¯„åˆ†æ–¹æ³•å’Œå¼€æ”¾æµ‹è¯•å¹³å°çš„æ–¹å¼ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ºCSESåŠæ›´å¹¿æ³›é¢†åŸŸçš„åœ°éœ‡é¢„æµ‹æ¨¡å‹æä¾›äº†å…¬å¹³ä¸”ç»Ÿè®¡ä¸Šå¥å…¨çš„è¯„ä»·ä¾æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14407v1">PDF</a> 30 pages, 7 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç»Ÿè®¡åœ°éœ‡å­¦ä¸­çš„é¢„æµ‹æ¨¡å‹è¯„ä¼°é—®é¢˜ã€‚é’ˆå¯¹åœ°éœ‡æ¬¡æ•°å…¨åˆ†å¸ƒP(n)çš„log-likelihoodè¯„åˆ†ï¼Œç”±äºé‡å°¾å’Œè¶…å‡ºèŒƒå›´è§‚æµ‹å¯èƒ½å¯¼è‡´æ¨¡å‹æ’ååå·®ï¼Œæå‡ºäº†ä¸€ç§å°¾éƒ¨æ„ŸçŸ¥è¯„ä¼°æ¡†æ¶ã€‚åˆ©ç”¨AoyuXå¹³å°ï¼Œåœ¨ä¸­å›½åœ°éœ‡å®éªŒç«™ï¼ˆCSESï¼‰è¿›è¡Œäº†çº¦25å¹´çš„æœˆåº¦ä¼ªå‰ç»æ€§é¢„æµ‹å®éªŒï¼Œå¯¹æ¯”äº†ETASæ¨¡å‹çš„ä¸åŒç‰ˆæœ¬åœ¨ä¸åŒç©ºé—´åˆ†è¾¨ç‡å’Œéœ‡çº§é˜ˆå€¼ä¸‹çš„è¡¨ç°ã€‚æ¨¡å‹æ’åå¯¹åœ°éœ‡è®¡æ•°å…¨åˆ†å¸ƒP(n)çš„å°¾éƒ¨å¤„ç†æ–¹å¼æ•æ„Ÿï¼Œé‡‡ç”¨å¹‚å¾‹å¤–æ¨æ—¢åˆç†åˆç¨³å¥ã€‚ç´¯è®¡ä¿¡æ¯å¢ç›Šï¼ˆCIGï¼‰æ˜¾ç¤ºï¼Œåœ¨æ•°æ®ä¸°å¯Œçš„æƒ…å†µä¸‹ï¼ŒETASÎ¼(x,y)ä¼˜äºETASÎ¼ï¼›è€Œåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œéšæœºæ³¢åŠ¨å ä¸»å¯¼ã€‚æœ¬ç ”ç©¶å°†èƒ–å°¾æ„ŸçŸ¥è¯„åˆ†æ–¹æ³•ä¸å¼€æ”¾æµ‹è¯•å¹³å°ç›¸ç»“åˆï¼Œæ¨åŠ¨äº†CSESå’Œæ›´å¹¿æ³›é¢†åŸŸçš„åœ°éœ‡é¢„æµ‹æ¨¡å‹çš„å…¬å¹³ã€ç»Ÿè®¡åŸºç¡€çš„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å°¾å’Œè¶…å‡ºèŒƒå›´è§‚æµ‹ä¼šå½±å“ç»Ÿè®¡åœ°éœ‡å­¦é¢„æµ‹æ¨¡å‹çš„è¯„ä¼°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å°¾éƒ¨æ„ŸçŸ¥è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªé€‚åº”é«˜æ–¯æ ¸å¯†åº¦ä¼°è®¡è¿›è¡Œç»†èƒçº§çš„P(n)ä¼°è®¡ã€‚</li>
<li>åœ¨ä¸­å›½åœ°éœ‡å®éªŒç«™è¿›è¡Œäº†é•¿æœŸå®éªŒï¼Œå¯¹æ¯”äº†ä¸åŒç‰ˆæœ¬çš„ETASæ¨¡å‹åœ¨ä¸åŒç©ºé—´åˆ†è¾¨ç‡å’Œéœ‡çº§é˜ˆå€¼ä¸‹çš„è¡¨ç°ã€‚</li>
<li>æ¨¡å‹æ’åå¯¹å¤„ç†åœ°éœ‡è®¡æ•°å…¨åˆ†å¸ƒçš„å°¾éƒ¨æ–¹å¼æ•æ„Ÿï¼Œé‡‡ç”¨å¹‚å¾‹å¤–æ¨æ—¢åˆç†åˆç¨³å¥ã€‚</li>
<li>åœ¨æ•°æ®ä¸°å¯Œçš„æƒ…å†µä¸‹ï¼ŒETASÎ¼(x,y)è¡¨ç°ä¼˜äºETASÎ¼ï¼›è€Œåœ¨æ•°æ®ç¨€ç¼ºæ—¶ï¼Œéšæœºæ³¢åŠ¨å½±å“æ˜¾è‘—ã€‚</li>
<li>ç´¯ç§¯ä¿¡æ¯å¢ç›Šæ˜¯è¡¡é‡æ¨¡å‹è¡¨ç°çš„æœ‰æ•ˆæŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-49cb2db5a3e34fefcfdfb9fe83b6a9d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726159&auth_key=1760726159-0-0-6a23f4c3e4ecaa5cc08a0b018bf9c712&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Hi-Agent-Hierarchical-Vision-Language-Agents-for-Mobile-Device-Control"><a href="#Hi-Agent-Hierarchical-Vision-Language-Agents-for-Mobile-Device-Control" class="headerlink" title="Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control"></a>Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control</h2><p><strong>Authors:Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi</strong></p>
<p>Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios. </p>
<blockquote>
<p>æ„å»ºèƒ½å¤Ÿè‡ªä¸»æ“ä½œç§»åŠ¨è®¾å¤‡çš„æ™ºèƒ½ä»£ç†å·²å¼•èµ·è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºç›´æ¥çš„çŠ¶æ€åˆ°åŠ¨ä½œæ˜ å°„ï¼Œè¿™ç¼ºä¹ç»“æ„åŒ–æ¨ç†å’Œè§„åˆ’ï¼Œå› æ­¤åœ¨é¢å¯¹æ–°ä»»åŠ¡æˆ–æœªè§çš„ç”¨æˆ·ç•Œé¢å¸ƒå±€æ—¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æˆ‘ä»¬å¼•å…¥äº†Hi-Agentï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç§»åŠ¨æ§åˆ¶çš„å¯è®­ç»ƒåˆ†å±‚è§†è§‰è¯­è¨€ä»£ç†ï¼Œå®ƒå…·å¤‡é«˜çº§æ¨ç†æ¨¡å‹å’Œä½çº§åŠ¨ä½œæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†è”åˆä¼˜åŒ–ã€‚ä¸ºäº†è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†å¤šæ­¥å†³ç­–é‡æ–°åˆ¶å®šä¸ºä¸€ç³»åˆ—å•æ­¥å­ç›®æ ‡ï¼Œå¹¶æå‡ºäº†ä¸€ç§å‰ç»æ€§ä¼˜åŠ¿å‡½æ•°ï¼Œè¯¥å‡½æ•°åˆ©ç”¨ä½çº§æ¨¡å‹çš„æ‰§è¡Œåé¦ˆæ¥æŒ‡å¯¼é«˜çº§ä¼˜åŒ–ã€‚è¿™ç§è®¾è®¡ç¼“è§£äº†é•¿å‘¨æœŸä»»åŠ¡ä¸­ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ‰€é¢ä¸´çš„è·¯å¾„çˆ†ç‚¸é—®é¢˜ï¼Œå®ç°äº†ç¨³å®šä¸”æ— è¯„ä»·çš„è”åˆè®­ç»ƒã€‚Hi-Agentåœ¨Android-in-the-Wildï¼ˆAitWï¼‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€é«˜ä»»åŠ¡æˆåŠŸç‡ï¼ˆ87.9%ï¼‰ï¼Œåœ¨ä¸‰ç§èŒƒå¼ä¸‹å‡æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼šåŸºäºæç¤ºçš„ï¼ˆAppAgent: 17.7%ï¼‰ã€åŸºäºç›‘ç£çš„ï¼ˆFiltered BC: 54.5%ï¼‰å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ï¼ˆDigiRL: 71.9%ï¼‰ã€‚åœ¨ScreenSpot-v2åŸºå‡†æµ‹è¯•ä¸Šï¼Œå®ƒä¹Ÿå±•ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„AndroidWorldåŸºå‡†æµ‹è¯•ä¸Šï¼ŒHi-Agentåœ¨è¾ƒå¤§çš„éª¨å¹²ç½‘æ¶æ„ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆæ‰©å±•ï¼Œæ˜¾ç¤ºå‡ºåœ¨é«˜å¤æ‚åº¦ç§»åŠ¨æ§åˆ¶åœºæ™¯ä¸­çš„å¼ºå¤§é€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14388v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æå‡ºäº†ä¸€ç§åä¸ºHi-Agentçš„å¯è®­ç»ƒåˆ†å±‚è§†è§‰è¯­è¨€ç§»åŠ¨æ§åˆ¶ä»£ç†ï¼ŒåŒ…å«é«˜çº§æ¨ç†æ¨¡å‹å’Œä½çº§åŠ¨ä½œæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†è”åˆä¼˜åŒ–ã€‚ä¸ºè§£å†³é•¿æœŸä»»åŠ¡ä¸­çš„è·¯å¾„çˆ†ç‚¸é—®é¢˜ï¼Œæå‡ºå‰ç»ä¼˜åŠ¿å‡½æ•°ï¼Œåˆ©ç”¨ä½çº§æ¨¡å‹çš„æ‰§è¡Œåé¦ˆå¼•å¯¼é«˜çº§ä¼˜åŒ–ã€‚åœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHi-Agentå®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶åœ¨å…¶ä»–ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Hi-Agentæ˜¯ä¸€ä¸ªè‡ªä¸»æ“ä½œç§»åŠ¨è®¾å¤‡çš„å¯è®­ç»ƒåˆ†å±‚è§†è§‰è¯­è¨€ä»£ç†ã€‚</li>
<li>å®ƒåŒ…å«é«˜çº§æ¨ç†æ¨¡å‹å’Œä½çº§åŠ¨ä½œæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†è”åˆä¼˜åŒ–ã€‚</li>
<li>æå‡ºå‰ç»ä¼˜åŠ¿å‡½æ•°ï¼Œè§£å†³é•¿æœŸä»»åŠ¡ä¸­çš„è·¯å¾„çˆ†ç‚¸é—®é¢˜ã€‚</li>
<li>Hi-Agentåœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„ä»»åŠ¡æˆåŠŸç‡ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒHi-Agentåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Hi-Agentåœ¨æ›´å¤æ‚çš„AndroidWorldåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿèƒ½æœ‰æ•ˆåœ°æ‰©å±•ï¼Œæ˜¾ç¤ºå‡ºåœ¨é«˜å¤æ‚åº¦ç§»åŠ¨æ§åˆ¶åœºæ™¯ä¸­çš„å¼ºé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14388">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8f37274c0431bb765a91b4c47f86a3b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726167&auth_key=1760726167-0-0-8c6b1a2e2bd730b495a291dad8376544&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c6e438e670285adb03657a47cc31d29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726174&auth_key=1760726174-0-0-d0ff3395edeff21b4fad0c69e03e2609&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08ed6abfa7b780cd9a6823e811c18a61~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726181&auth_key=1760726181-0-0-b387fb33d61186492bb621706cfaa58f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab9c36e885c0f70378baa8e96a5f47c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726188&auth_key=1760726188-0-0-3cea8654e6a9f50a23b23c90252b4a78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CURE-Confidence-driven-Unified-Reasoning-Ensemble-Framework-for-Medical-Question-Answering"><a href="#CURE-Confidence-driven-Unified-Reasoning-Ensemble-Framework-for-Medical-Question-Answering" class="headerlink" title="CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical   Question Answering"></a>CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical   Question Answering</h2><p><strong>Authors:Ziad Elshaer, Essam A. Rashed</strong></p>
<p>High-performing medical Large Language Models (LLMs) typically require extensive fine-tuning with substantial computational resources, limiting accessibility for resource-constrained healthcare institutions. This study introduces a confidence-driven multi-model framework that leverages model diversity to enhance medical question answering without fine-tuning. Our framework employs a two-stage architecture: a confidence detection module assesses the primary modelâ€™s certainty, and an adaptive routing mechanism directs low-confidence queries to Helper models with complementary knowledge for collaborative reasoning. We evaluate our approach using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework achieves competitive performance, with particularly strong results in PubMedQA (95.0%) and MedMCQA (78.0%). Ablation studies confirm that confidence-aware routing combined with multi-model collaboration substantially outperforms single-model approaches and uniform reasoning strategies. This work establishes that strategic model collaboration offers a practical, computationally efficient pathway to improve medical AI systems, with significant implications for democratizing access to advanced medical AI in resource-limited settings. </p>
<blockquote>
<p>é«˜æ€§èƒ½åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸éœ€è¦å¤§é‡çš„å¾®è°ƒä»¥åŠå¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¿™é™åˆ¶äº†èµ„æºå—é™çš„åŒ»ç–—æœºæ„å¯¹å…¶çš„è®¿é—®ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„å¤šæ¨¡å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ¨¡å‹å¤šæ ·æ€§å¢å¼ºåŒ»ç–—é—®ç­”åŠŸèƒ½ï¼Œæ— éœ€å¾®è°ƒã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼šä¿¡å¿ƒæ£€æµ‹æ¨¡å—è¯„ä¼°ä¸»æ¨¡å‹çš„ç¡®å®šæ€§ï¼Œè‡ªé€‚åº”è·¯ç”±æœºåˆ¶å°†ä½ä¿¡å¿ƒæŸ¥è¯¢å¼•å¯¼è‡³å…·æœ‰è¡¥å……çŸ¥è¯†çš„è¾…åŠ©æ¨¡å‹è¿›è¡ŒååŒæ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨Qwen3-30B-A3B-Instructã€Phi-4 14Bå’ŒGemma 2 12Bï¼Œåœ¨ä¸‰ä¸ªåŒ»å­¦åŸºå‡†æµ‹è¯•MedQAã€MedMCQAå’ŒPubMedQAä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨PubMedQAï¼ˆ95.0%ï¼‰å’ŒMedMCQAï¼ˆ78.0%ï¼‰ä¸­å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œç»“åˆä¿¡å¿ƒæ„ŸçŸ¥è·¯ç”±å’Œå¤šæ¨¡å‹åä½œçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºå•æ¨¡å‹æ–¹æ³•å’Œç»Ÿä¸€æ¨ç†ç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œè¯æ˜ï¼Œæˆ˜ç•¥æ¨¡å‹åä½œä¸ºæ”¹è¿›åŒ»ç–—AIç³»ç»Ÿæä¾›äº†ä¸€æ¡å®ç”¨ä¸”è®¡ç®—é«˜æ•ˆçš„é€”å¾„ï¼Œå¯¹äºåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­æ™®åŠå…ˆè¿›çš„åŒ»ç–—AIå…·æœ‰é‡å¤§æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14353v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„å¤šæ¨¡å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ¨¡å‹å¤šæ ·æ€§æé«˜åŒ»ç–—é—®ç­”æ€§èƒ½ï¼Œæ— éœ€ç²¾ç»†è°ƒæ•´ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œé¦–å…ˆé€šè¿‡ä¿¡å¿ƒæ£€æµ‹æ¨¡å—è¯„ä¼°ä¸»è¦æ¨¡å‹çš„ç¡®å®šæ€§ï¼Œç„¶åé€šè¿‡è‡ªé€‚åº”è·¯ç”±æœºåˆ¶å°†ä½ä¿¡å¿ƒæŸ¥è¯¢å¼•å¯¼è‡³å…·æœ‰è¡¥å……çŸ¥è¯†çš„è¾…åŠ©æ¨¡å‹è¿›è¡ŒååŒæ¨ç†ã€‚åœ¨ä¸‰ä¸ªåŒ»ç–—åŸºå‡†æµ‹è¯•ï¼ˆMedQAã€MedMCQAå’ŒPubMedQAï¼‰ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨PubMedQAï¼ˆ95.0%ï¼‰å’ŒMedMCQAï¼ˆ78.0%ï¼‰ä¸Šè¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¯å®ï¼Œç»“åˆä¿¡å¿ƒæ„ŸçŸ¥è·¯ç”±å’Œå¤šæ¨¡å‹ååŒçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºå•æ¨¡å‹æ–¹æ³•å’Œå‡åŒ€æ¨ç†ç­–ç•¥ã€‚è¿™é¡¹ç ”ç©¶ä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸­å®ç°åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ°‘ä¸»åŒ–è®¿é—®æä¾›äº†å®ç”¨ã€è®¡ç®—é«˜æ•ˆçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ€§èƒ½åŒ»ç–—è¯­è¨€å¤§æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„ç²¾ç»†è°ƒæ•´å’Œè®¡ç®—èµ„æºï¼Œè¿™é™åˆ¶äº†èµ„æºå—é™åŒ»ç–—æœºæ„çš„ä½¿ç”¨ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„å¤šæ¨¡å‹æ¡†æ¶ï¼Œç”¨äºæé«˜åŒ»ç–—é—®ç­”çš„å‡†ç¡®åº¦ï¼Œä¸”æ— éœ€ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼ŒåŒ…æ‹¬ä¿¡å¿ƒæ£€æµ‹æ¨¡å—å’Œè‡ªé€‚åº”è·¯ç”±æœºåˆ¶ã€‚</li>
<li>ä¿¡å¿ƒæ£€æµ‹æ¨¡å—è¯„ä¼°ä¸»è¦æ¨¡å‹çš„ç¡®å®šæ€§ï¼Œè€Œè‡ªé€‚åº”è·¯ç”±æœºåˆ¶åˆ™å¤„ç†ä½ä¿¡å¿ƒæŸ¥è¯¢ï¼Œé€šè¿‡å¼•å¯¼æŸ¥è¯¢è‡³è¾…åŠ©æ¨¡å‹è¿›è¡ŒååŒæ¨ç†ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜è¯¥æ¡†æ¶å…·æœ‰ç«äº‰åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨PubMedQAå’ŒMedMCQAä¸Šçš„è¡¨ç°çªå‡ºã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†ä¿¡å¿ƒæ„ŸçŸ¥è·¯ç”±ä¸å¤šæ¨¡å‹ååŒçš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¼˜äºå•æ¨¡å‹æ–¹æ³•å’Œå‡åŒ€æ¨ç†ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d2d89f96e17bc9903dbbfaa3f4780f5d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726195&auth_key=1760726195-0-0-c0f61f99558b64358a33ca9a033495a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ea1b0cbfa29fd82aa87f12695f5fe51~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726202&auth_key=1760726202-0-0-fb2ed6d03321c038efc36925f6cf3428&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MathMist-A-Parallel-Multilingual-Benchmark-Dataset-for-Mathematical-Problem-Solving-and-Reasoning"><a href="#MathMist-A-Parallel-Multilingual-Benchmark-Dataset-for-Mathematical-Problem-Solving-and-Reasoning" class="headerlink" title="MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical   Problem Solving and Reasoning"></a>MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical   Problem Solving and Reasoning</h2><p><strong>Authors:Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda</strong></p>
<p>Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned question-answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMsâ€™ ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: <a target="_blank" rel="noopener" href="https://github.com/mahbubhimel/MathMist">https://github.com/mahbubhimel/MathMist</a> </p>
<blockquote>
<p>æ–‡æœ¬ä¸­çš„æ•°å­¦æ¨ç†ä»ç„¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€å…·æŒ‘æˆ˜æ€§çš„é¢†åŸŸä¹‹ä¸€ï¼Œå®ƒä¸ä»…éœ€è¦è¯­è¨€ç†è§£ï¼Œè¿˜éœ€è¦ç»“æ„åŒ–çš„é€»è¾‘æ¨å¯¼å’Œæ•°å€¼ç²¾ç¡®æ€§ã€‚è™½ç„¶æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤šç§è¯­è¨€ä¸­çš„æ•°å­¦èƒ½åŠ›ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æˆ–èµ„æºä¸°å¯Œçš„ç‹­çª„è¯­è¨€å­é›†ä¸Šï¼Œåœ¨è¯„ä¼°å¤šè¯­ç§å’Œè·¨è¯­è¨€æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨å¾ˆå¤§å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathMistï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ•°å­¦é—®é¢˜è§£å†³å’Œæ¨ç†çš„å¹¶è¡Œå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ã€‚MathMistæ¶µç›–äº†è¶…è¿‡21Kä¸ªè·¨ä¸ƒç§è¯­è¨€çš„å¯¹é½é—®ç­”å¯¹ï¼Œä»£è¡¨äº†é«˜ã€ä¸­ã€ä½èµ„æºè¯­è¨€ç¯å¢ƒçš„å¹³è¡¡è¦†ç›–ã€‚è¯¥æ•°æ®é›†æ•æ‰äº†è¯­è¨€çš„å¤šæ ·æ€§ã€å¤šç§é—®é¢˜è®¾ç½®å’Œè§£å†³æ–¹æ¡ˆç»¼åˆèƒ½åŠ›ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸€ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºçš„å°å‹å’Œä¸­å‹LLMã€ä¸“æœ‰ç³»ç»Ÿä»¥åŠé¢å‘å¤šè¯­è¨€æ¨ç†çš„æ¨¡å‹ï¼Œåœ¨é›¶æ ·æœ¬ã€æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œä»£ç åˆ‡æ¢æ¨ç†èŒƒå¼ä¸‹è¿›è¡Œè¯„ä»·ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡Œè·¨è¯­è¨€çš„è¿è´¯å’Œå¯è§£é‡Šæ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨æŒç»­ç¼ºé™·ï¼Œåœ¨ä½èµ„æºç¯å¢ƒä¸­è¿™ç§é€€åŒ–æ›´ä¸ºæ˜æ˜¾ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mahbubhimel/MathMist">https://github.com/mahbubhimel/MathMist</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14305v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦è¯­è¨€ç†è§£å’Œç»“æ„åŒ–é€»è¾‘æ¼”ç»å’Œæ•°å€¼ç²¾ç¡®åº¦ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æˆ–å°‘æ•°é«˜èµ„æºè¯­è¨€ä¸Šï¼Œè¯„ä¼°å¤šè¯­è¨€è·¨è¯­è¨€çš„æ•°å­¦æ¨ç†èƒ½åŠ›å­˜åœ¨å·®è·ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathMistï¼Œä¸€ä¸ªç”¨äºæ•°å­¦é—®é¢˜è§£å†³å’Œæ¨ç†çš„å¹³è¡Œå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ã€‚MathMiståŒ…å«è¶…è¿‡21Kä¸ªè·¨ä¸ƒç§è¯­è¨€çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œä»£è¡¨é«˜ã€ä¸­ã€ä½èµ„æºè¯­è¨€ç¯å¢ƒçš„å¹³è¡¡è¦†ç›–ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸€ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºçš„å°å‹å’Œä¸­å‹LLMã€ä¸“æœ‰ç³»ç»Ÿå’Œå¤šè¯­è¨€æ¨ç†é‡ç‚¹æ¨¡å‹ï¼Œåœ¨é›¶æ ·æœ¬ã€æ€ç»´é“¾å’Œä»£ç åˆ‡æ¢æ¨ç†èŒƒå¼ä¸‹ï¼Œç»“æœæ­ç¤ºLLMåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æŒç»­ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºç¯å¢ƒä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œéœ€å…¼å…·è¯­è¨€ç†è§£å’Œç»“æ„åŒ–é€»è¾‘æ¨æ¼”åŠæ•°å€¼ç²¾ç¡®åº¦ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ç­‰å°‘æ•°é«˜èµ„æºè¯­è¨€ä¸Šï¼Œç¼ºä¹å¤šè¯­è¨€å’Œè·¨è¯­è¨€çš„æ•°å­¦æ¨ç†è¯„ä¼°ã€‚</li>
<li>MathMistæ˜¯ä¸€ä¸ªç”¨äºæ•°å­¦é—®é¢˜è§£å†³å’Œæ¨ç†çš„å¹³è¡Œå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œè¦†ç›–é«˜ã€ä¸­ã€ä½èµ„æºè¯­è¨€çš„å¹³è¡¡é—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>MathMistæ•°æ®é›†æ•æ‰äº†è¯­è¨€å¤šæ ·æ€§ã€å¤šç§é—®é¢˜è®¾ç½®å’Œè§£å†³æ–¹æ¡ˆç»¼åˆèƒ½åŠ›ã€‚</li>
<li>å¯¹ä¸€ç³»åˆ—æ¨¡å‹è¿›è¡Œçš„ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æŒç»­ä¸è¶³ã€‚</li>
<li>åœ¨ä½èµ„æºç¯å¢ƒä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bb8191ce2e8484da3bdf8f25cf9718eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726209&auth_key=1760726209-0-0-d0651cf22c271e222aec09f4f7d3e97d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79ab063a78d7bc66f7de557ac37b3c0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726216&auth_key=1760726216-0-0-39d68e2c25a82b9fa42fa2db3efd4ba0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ff099a9bc245e2761d04419abf5e5a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726226&auth_key=1760726226-0-0-2776678903a0bc17389e47a966e0b66d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-508bb1b513c1121386fc2ee780c8c76d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726233&auth_key=1760726233-0-0-88ee4fd22f184d7ed69b0fd718effb4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fda0695894064839c36a38e60e1fa15c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726240&auth_key=1760726240-0-0-7cb26efff813a344d06aa2feaa1a0332&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading"><a href="#AlphaQuanter-An-End-to-End-Tool-Orchestrated-Agentic-Reinforcement-Learning-Framework-for-Stock-Trading" class="headerlink" title="AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement   Learning Framework for Stock Trading"></a>AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement   Learning Framework for Stock Trading</h2><p><strong>Authors:Zheye Deng, Jiashu Wang</strong></p>
<p>While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/AlphaQuanter/AlphaQuanter">https://github.com/AlphaQuanter/AlphaQuanter</a> </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–äº¤æ˜“é¢†åŸŸæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´é‡å¤§å±€é™æ€§ã€‚æ˜¾è‘—çš„å¤šä»£ç†æ¡†æ¶é€šå¸¸å­˜åœ¨æ•ˆç‡ä½ä¸‹ã€äº§ç”Ÿä¿¡å·ä¸ä¸€è‡´ä»¥åŠç¼ºä¹ä»å¸‚åœºåé¦ˆä¸­å­¦ä¹ è¿è´¯ç­–ç•¥æ‰€éœ€çš„ç«¯åˆ°ç«¯ä¼˜åŒ–ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AlphaQuanterï¼Œè¿™æ˜¯ä¸€ä¸ªå•ä»£ç†æ¡†æ¶ï¼Œå®ƒä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥å­¦ä¹ åœ¨é€æ˜ä¸”å·¥å…·å¢å¼ºçš„å†³ç­–å·¥ä½œæµç¨‹ä¸Šçš„åŠ¨æ€ç­–ç•¥ï¼Œè¿™ä½¿å¾—å•ä¸ªä»£ç†èƒ½å¤Ÿè‡ªä¸»åœ°åè°ƒå·¥å…·å¹¶æŒ‰éœ€ä¸»åŠ¨è·å–ä¿¡æ¯ï¼Œå»ºç«‹ä¸€ä¸ªé€æ˜ä¸”å¯å®¡æ ¸çš„æ¨ç†è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAlphaQuanteråœ¨å…³é”®è´¢åŠ¡æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå…¶å¯è§£é‡Šçš„æ¨ç†æ­ç¤ºäº†å¤æ‚çš„ç­–ç•¥ï¼Œä¸ºäººç±»äº¤æ˜“è€…æä¾›äº†æ–°é¢–ä¸”æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„æ•°æ®æ”¶é›†å’Œä»£ç†è®­ç»ƒä»£ç åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/AlphaQuanter/AlphaQuanter%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8">https://github.com/AlphaQuanter/AlphaQuanterå…¬å¼€å¯ç”¨</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14264v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–äº¤æ˜“è™½ç„¶å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚å½“å‰ä¸»æµçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶å­˜åœ¨æ•ˆç‡ä½ä¸‹ã€ä¿¡å·ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œç¼ºä¹ä»å¸‚åœºåé¦ˆä¸­å­¦ä¹ è¿è´¯ç­–ç•¥çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºAlphaQuanterå•ä¸€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å­¦ä¹ åŠ¨æ€ç­–ç•¥ï¼Œé€šè¿‡é€æ˜çš„å·¥å…·å¢å¼ºå†³ç­–æµç¨‹ï¼Œä½¿å•ä¸€æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»åè°ƒå·¥å…·å¹¶ä¸»åŠ¨æŒ‰éœ€è·å–ä¿¡æ¯ï¼Œå»ºç«‹é€æ˜å¯å®¡è®¡çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒAlphaQuanteråœ¨å…³é”®è´¢åŠ¡æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå…¶å¯è§£é‡Šçš„æ¨ç†æ­ç¤ºäº†å¤æ‚ç­–ç•¥ï¼Œä¸ºäººå·¥äº¤æ˜“è€…æä¾›äº†æœ‰ä»·å€¼çš„æ–°è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–äº¤æ˜“é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ä¸»æµçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œä¿¡å·ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>AlphaQuanteræ˜¯ä¸€ä¸ªæ–°çš„å•ä¸€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ åŠ¨æ€ç­–ç•¥ã€‚</li>
<li>AlphaQuanterä½¿ç”¨é€æ˜çš„å·¥å…·å¢å¼ºå†³ç­–æµç¨‹ï¼Œæ™ºèƒ½ä½“èƒ½è‡ªä¸»åè°ƒå·¥å…·å¹¶ä¸»åŠ¨è·å–ä¿¡æ¯ã€‚</li>
<li>AlphaQuanterå®ç°äº†å…³é”®è´¢åŠ¡æŒ‡æ ‡ä¸Šçš„æœ€æ–°æŠ€æœ¯æ°´å¹³è¡¨ç°ã€‚</li>
<li>AlphaQuanterçš„æ¨ç†è¿‡ç¨‹é€æ˜å¯å®¡è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d16f1595f4c89ac746222115275c50f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726247&auth_key=1760726247-0-0-0939f91f1a20590194b6cb674a8ed16d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9e748b5bab5883ad81c28de240196f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726254&auth_key=1760726254-0-0-f9e88120109aa66cf43a3f3815a936b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bd56ec27b74df555f47ef1705a3c756~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726261&auth_key=1760726261-0-0-e7c445355ebd6bfa059747bd27c09b1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-553e9314c98b68e5121f2e433136f4b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726270&auth_key=1760726270-0-0-1bb25baf31b5e3a7e007ea88a0daff41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8bac441c3e7f1c7b47caa2131ef6fd1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726276&auth_key=1760726276-0-0-f47ceaf6a0800f149beb06cf807138f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reasoning-in-Space-via-Grounding-in-the-World"><a href="#Reasoning-in-Space-via-Grounding-in-the-World" class="headerlink" title="Reasoning in Space via Grounding in the World"></a>Reasoning in Space via Grounding in the World</h2><p><strong>Authors:Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, Peidong Liu</strong></p>
<p>In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸»å¼ 3Dè§†è§‰å®šä½ï¼ˆgroundingï¼‰æ˜¯ç©ºé—´æ¨ç†çš„åŸºçŸ³ï¼Œå¹¶å¼•å…¥åŸºäºå®šä½çš„æ—¶ç©ºæ¨ç†å™¨ï¼ˆGS-Reasonerï¼‰æ¥æ¢ç´¢æœ‰æ•ˆçš„ç©ºé—´è¡¨ç¤ºï¼Œä»¥ç¼©å°å®ƒä»¬ä¹‹é—´çš„å·®è·ã€‚ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ä¸‰ç»´æ•°æ®æ—¶ç¼ºä¹ç»Ÿä¸€çš„ä¸‰ç»´è¡¨ç¤ºèƒ½åŠ›ï¼Œæ— æ³•åŒæ—¶æ•æ‰è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ã€‚è¿™ä¸€ç¼ºé™·è¡¨ç°ä¸ºå®šä½æ€§èƒ½ä¸ä½³æˆ–å¯¹å¤–éƒ¨æ¨¡å—çš„è¿‡åº¦ä¾èµ–ï¼Œæœ€ç»ˆé˜»ç¢äº†æ— ç¼æ•´åˆå®šä½å’Œç©ºé—´æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŒè·¯å¾„æ± åŒ–æœºåˆ¶ï¼Œç´§å¯†å¯¹é½å‡ ä½•ç‰¹å¾ä¸è¯­ä¹‰å’Œä½ç½®çº¿ç´¢ï¼Œæ„å»ºäº†ä¸€ä¸ªåŸºäºå›¾åƒå—çš„ç»Ÿä¸€ä¸‰ç»´è¡¨ç¤ºï¼Œå®ƒèƒ½æ¶µç›–æ‰€æœ‰å…³é”®ä¿¡æ¯è€Œæ— éœ€å¢åŠ è¾“å…¥ä»¤ç‰Œçš„æ•°é‡ã€‚å€ŸåŠ©è¿™ç§å…¨é¢çš„è¡¨ç¤ºï¼ŒGS-Reasoneræˆä¸ºé¦–ä¸ªèƒ½åœ¨æ— éœ€å¤–éƒ¨æ¨¡å—çš„æƒ…å†µä¸‹å®ç°å®Œå…¨è‡ªåŠ¨å®šä½çš„ä¸‰ç»´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶å…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ï¼Œä¸ºä¸‰ç»´ç©ºé—´æ¨ç†å»ºç«‹äº†ç»Ÿä¸€ã€è‡ªè¶³çš„æ¡†æ¶ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¼¥åˆå®šä½å’Œç©ºé—´æ¨ç†ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå®šä½çš„æ€ç»´é“¾ï¼ˆGCoTï¼‰æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ç²¾å¿ƒæŒ‘é€‰åŒ…å«ä¸‰ç»´è¾¹ç•Œæ¡†æ ‡æ³¨çš„å¯¹è±¡ä»¥åŠæ¨ç†é—®é¢˜ä¸­çš„é€æ­¥æ¨ç†è·¯å¾„ï¼Œå¹¶å°†å®šä½ä½œä¸ºé—®é¢˜è§£å†³è¿‡ç¨‹çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGS-Reasoneråœ¨ä¸‰ç»´è§†è§‰å®šä½æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œè¿™åè¿‡æ¥åˆæ˜¾è‘—å¢å¼ºäº†å…¶ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13800v2">PDF</a> 20 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡º3Dè§†è§‰å®šä½æ˜¯ç©ºé—´æ¨ç†çš„æ ¸å¿ƒï¼Œå¹¶å¼•å…¥GS-Reasoneræ¥æ¢ç´¢æœ‰æ•ˆçš„ç©ºé—´è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥å¼¥åˆå®ƒä»¬ä¹‹é—´çš„å·®è·ã€‚ç°æœ‰çš„3Då¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹ç»Ÿä¸€çš„3Dè¡¨ç¤ºèƒ½åŠ›ï¼Œæ— æ³•åŒæ—¶æ•æ‰è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œå¯¼è‡´å®šä½æ€§èƒ½ä¸ä½³æˆ–è¿‡åº¦ä¾èµ–å¤–éƒ¨æ¨¡å—ï¼Œé˜»ç¢äº†å®šä½å’Œç©ºé—´æ¨ç†çš„æ— ç¼é›†æˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŒè·¯å¾„æ± åŒ–æœºåˆ¶ï¼Œç´§å¯†å¯¹é½å‡ ä½•ç‰¹å¾ä¸è¯­ä¹‰å’Œä½ç½®çº¿ç´¢ï¼Œæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºå›¾åƒè¡¥ä¸çš„3Dè¡¨ç¤ºï¼Œè€Œæ— éœ€å¢åŠ è¾“å…¥æ ‡è®°çš„æ•°é‡ã€‚åˆ©ç”¨è¿™ç§æ•´ä½“è¡¨ç¤ºï¼ŒGS-Reasoneræˆä¸ºé¦–ä¸ªåœ¨å®šä½æ–¹é¢å®Œå…¨ä¸éœ€è¦å¤–éƒ¨æ¨¡å—çš„è‡ªå›å½’3Då¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åœ¨ä¿æŒä¸æœ€æ–°æ¨¡å‹æ€§èƒ½ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œä¸º3Dç©ºé—´æ¨ç†å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€ã€è‡ªæˆ‘åŒ…å«çš„æ¡†æ¶ã€‚ä¸ºäº†å¼¥åˆå®šä½å’Œç©ºé—´æ¨ç†ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†GCoTæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ç²¾å¿ƒæŒ‘é€‰ï¼ŒåŒ…æ‹¬ç”¨äºå‚è€ƒæ¨ç†é—®é¢˜çš„3Dè¾¹ç•Œæ¡†æ³¨é‡Šå’Œé›†æˆå®šä½ä½œä¸ºé—®é¢˜è§£å†³è¿‡ç¨‹æ ¸å¿ƒç»„ä»¶çš„é€æ­¥æ¨ç†è·¯å¾„ã€‚å®éªŒè¡¨æ˜ï¼ŒGS-Reasoneråœ¨3Dè§†è§‰å®šä½æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œè¿›è€Œæ˜¾è‘—å¢å¼ºäº†å…¶ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dè§†è§‰å®šä½æ˜¯ç©ºé—´æ¨ç†çš„æ ¸å¿ƒã€‚</li>
<li>ç°æœ‰3Då¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹ç»Ÿä¸€çš„3Dè¡¨ç¤ºèƒ½åŠ›ï¼Œæ— æ³•æ•æ‰è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ã€‚</li>
<li>GS-Reasoneré€šè¿‡åŒè·¯å¾„æ± åŒ–æœºåˆ¶æ„å»ºç»Ÿä¸€çš„å›¾åƒè¡¥ä¸åŸºäº3Dè¡¨ç¤ºã€‚</li>
<li>GS-Reasoneråœ¨å®šä½æ–¹é¢å®Œå…¨ä¸éœ€è¦å¤–éƒ¨æ¨¡å—ï¼Œå»ºç«‹äº†ç»Ÿä¸€ã€è‡ªæˆ‘åŒ…å«çš„3Dç©ºé—´æ¨ç†æ¡†æ¶ã€‚</li>
<li>GCoTæ•°æ®é›†åŒ…æ‹¬ç”¨äºå‚è€ƒæ¨ç†é—®é¢˜çš„3Dè¾¹ç•Œæ¡†æ³¨é‡Šå’Œé€æ­¥æ¨ç†è·¯å¾„ã€‚</li>
<li>GS-Reasoneråœ¨è§†è§‰å®šä½æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9fc6faf1dbcc2bb917012ad84616a3d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726284&auth_key=1760726284-0-0-c2fcc51a67488bb7b1933722a90e4a04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1ab25fdebc74d19286b2a86d72f8524~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726291&auth_key=1760726291-0-0-8b26579136d87cb4b5f3c44109500551&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa2718f5a1619f04a54154ac46156809~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726298&auth_key=1760726298-0-0-e4fd117b88b440468dff002bb6a9788e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b1b180f82e97d1652b95a64c81f6de6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726304&auth_key=1760726304-0-0-0812893504313d878cd8a76b639280fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RECODE-Reasoning-Through-Code-Generation-for-Visual-Question-Answering"><a href="#RECODE-Reasoning-Through-Code-Generation-for-Visual-Question-Answering" class="headerlink" title="RECODE: Reasoning Through Code Generation for Visual Question Answering"></a>RECODE: Reasoning Through Code Generation for Visual Question Answering</h2><p><strong>Authors:Junhong Shen, Mu Cai, Bo Hu, Ameet Talwalkar, David A Ross, Cordelia Schmid, Alireza Fathi</strong></p>
<p>Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering â€“ the process of reverse-engineering visuals into executable code â€“ as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å›¾è¡¨ã€ç¤ºæ„å›¾ç­‰ç»“æ„åŒ–è§†è§‰å†…å®¹çš„ç²¾ç¡®æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºåŸºäºåƒç´ çš„æ„ŸçŸ¥ç¼ºä¹éªŒè¯æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨åæ¸²æŸ“æŠ€æœ¯â€”â€”å°†è§†è§‰å†…å®¹é€†å‘å·¥ç¨‹ä¸ºå¯æ‰§è¡Œä»£ç çš„è¿‡ç¨‹â€”â€”ä½œä¸ºä¸€ç§æ–°çš„å¯éªŒè¯è§†è§‰æ¨ç†æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†RECODEï¼Œè¿™æ˜¯ä¸€ä¸ªä»£ç†æ¡†æ¶ï¼Œé¦–å…ˆç”Ÿæˆå¤šä¸ªå€™é€‰ç¨‹åºæ¥é‡å»ºè¾“å…¥å›¾åƒã€‚ç„¶åï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªè¯„è®ºå®¶æ¥é€‰æ‹©æœ€å¿ å®çš„é‡å»ºï¼Œå¹¶è¿­ä»£åœ°ä¼˜åŒ–ä»£ç ã€‚è¿™ä¸€è¿‡ç¨‹ä¸ä»…å°†ä¸€ä¸ªæ¨¡ç³Šçš„æ„ŸçŸ¥ä»»åŠ¡è½¬å˜ä¸ºå¯éªŒè¯çš„ç¬¦å·é—®é¢˜ï¼Œè€Œä¸”ä¸ºåç»­çš„è®¡ç®—å’Œé€»è¾‘æ¨ç†æä¾›äº†ç²¾ç¡®æ€§ã€‚åœ¨CharXivã€ChartQAå’ŒGeometry3Kç­‰è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRECODEæ˜¾è‘—ä¼˜äºä¸åˆ©ç”¨ä»£ç æˆ–ä»…ç”¨äºç»˜åˆ¶è¾…åŠ©çº¿æˆ–è£å‰ªçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œä»¥å¯æ‰§è¡Œä»£ç ä¸ºåŸºç¡€è¿›è¡Œè§†è§‰æ„ŸçŸ¥ä¸ºæ›´å‡†ç¡®ã€å¯éªŒè¯çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13756v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºé’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–è§†è§‰å†…å®¹ï¼ˆå¦‚å›¾è¡¨ã€å›¾è¡¨ç­‰ï¼‰æ—¶çš„æ¨ç†èƒ½åŠ›å±€é™é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºRECODEçš„æ–°æ¡†æ¶ã€‚RECODEé€šè¿‡åˆ©ç”¨åæ¸²æŸ“æŠ€æœ¯ï¼Œå°†è§†è§‰å†…å®¹è½¬åŒ–ä¸ºå¯æ‰§è¡Œä»£ç è¿›è¡ŒéªŒè¯ï¼Œå®ç°äº†ä»æ¨¡ç³Šæ„ŸçŸ¥ä»»åŠ¡åˆ°å¯éªŒè¯ç¬¦å·é—®é¢˜çš„è½¬å˜ã€‚RECODEåœ¨ä¸åŒè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å°†è§†è§‰æ„ŸçŸ¥ä¸å¯æ‰§è¡Œä»£ç ç›¸ç»“åˆçš„æ–¹æ³•å¯¹äºå®ç°æ›´å‡†ç¡®ã€å¯éªŒè¯çš„å¤šæ¨¡æ€æ¨ç†å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬ä¸­çš„ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–è§†è§‰å†…å®¹ï¼ˆå¦‚å›¾è¡¨å’Œå›¾è¡¨ï¼‰æ—¶é¢ä¸´ç²¾ç¡®æ¨ç†çš„æŒ‘æˆ˜ã€‚è¿™æ˜¯å› ä¸ºåƒç´ æ„ŸçŸ¥ç¼ºä¹éªŒè¯æœºåˆ¶ã€‚</li>
<li>RECODEæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒé€šè¿‡åæ¸²æŸ“æŠ€æœ¯å°†è§†è§‰å†…å®¹è½¬åŒ–ä¸ºå¯æ‰§è¡Œä»£ç è¿›è¡ŒéªŒè¯ã€‚</li>
<li>RECODEæ¡†æ¶é¦–å…ˆç”Ÿæˆå¤šä¸ªå€™é€‰ç¨‹åºæ¥é‡å»ºå›¾åƒï¼Œå¹¶ä½¿ç”¨è¯„è®ºå®¶é€‰æ‹©æœ€å‡†ç¡®çš„é‡å»ºï¼Œç„¶åè¿­ä»£ä¼˜åŒ–ä»£ç ã€‚è¿™ä¸ªè¿‡ç¨‹å®ç°äº†ä»æ¨¡ç³Šæ„ŸçŸ¥ä»»åŠ¡åˆ°å¯éªŒè¯ç¬¦å·é—®é¢˜çš„è½¬å˜ã€‚</li>
<li>RECODEåœ¨ä¸åŒè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚CharXivã€ChartQAå’ŒGeometry3Kç­‰ã€‚ç›¸è¾ƒäºä¸ä¾èµ–ä»£ç æˆ–ä»…ä½¿ç”¨ä»£ç ç»˜åˆ¶è¾…åŠ©çº¿æˆ–è£å‰ªçš„æ–¹æ³•ï¼ŒRECODEå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>RECODEæ¡†æ¶å°†è§†è§‰æ„ŸçŸ¥ä¸å¯æ‰§è¡Œä»£ç ç›¸ç»“åˆï¼Œä¸ºæ›´ç²¾ç¡®å’Œå¯éªŒè¯çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†æ–°é€”å¾„ã€‚è¿™ä¸ä»…è§£å†³äº†ç°æœ‰çš„è§†è§‰æ¨ç†æŒ‘æˆ˜ï¼Œè¿˜ä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å¯èƒ½æ€§ã€‚ä¾‹å¦‚å¯ä»¥åœ¨è¯¸å¦‚è‡ªç„¶è¯­è¨€æè¿°å’Œå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸­å¼•å…¥æ›´å¤šåŸºäºä»£ç çš„æ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-39258217e9197226496dc306a6c0d900~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726386&auth_key=1760726386-0-0-a052f32ca9316d2672302a77ab68dc86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a9e1d43a16452ab9f0f3ab747c4c2a24~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726395&auth_key=1760726395-0-0-e7e8bd4f533cc89a994ee53e3ddd8e38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3221f7b17ef938f49ba0e9ad7f533f81~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726401&auth_key=1760726401-0-0-30da92a5d47cfd73338f36fc8d05e619&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfabaddd8edee136530a996ed70e7dc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726407&auth_key=1760726407-0-0-7102a96ecf44c53136ddfa19b580c741&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c626b96270281d2bd657d8c5791d47b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726413&auth_key=1760726413-0-0-c971800e5b52ab8850ca6ffbd2576c9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-Meets-Masked-Generative-Models-Mask-GRPO-for-Text-to-Image-Generation"><a href="#Reinforcement-Learning-Meets-Masked-Generative-Models-Mask-GRPO-for-Text-to-Image-Generation" class="headerlink" title="Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for   Text-to-Image Generation"></a>Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for   Text-to-Image Generation</h2><p><strong>Authors:Yifu Luo, Xinhao Hu, Keyu Fan, Haoyuan Sun, Zeyu Chen, Bo Xia, Tiantian Zhang, Yongzhe Chang, Xueqian Wang</strong></p>
<p>Reinforcement learning (RL) has garnered increasing attention in text-to-image (T2I) generation. However, most existing RL approaches are tailored to either diffusion models or autoregressive models, overlooking an important alternative: masked generative models. In this work, we propose Mask-GRPO, the first method to incorporate Group Relative Policy Optimization (GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine the transition probability, which is different from current approaches, and formulate the unmasking process as a multi-step decision-making problem. To further enhance our method, we explore several useful strategies, including removing the KL constraint, applying the reduction strategy, and filtering out low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with substantial improvements on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches. The code is available on <a target="_blank" rel="noopener" href="https://github.com/xingzhejun/Mask-GRPO">https://github.com/xingzhejun/Mask-GRPO</a> </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆä¸­å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„RLæ–¹æ³•éƒ½æ˜¯é’ˆå¯¹æ‰©æ•£æ¨¡å‹æˆ–è‡ªå›å½’æ¨¡å‹è®¾è®¡çš„ï¼Œå¿½ç•¥äº†ä¸€ç§é‡è¦çš„æ›¿ä»£æ–¹æ³•ï¼šåŸºäºæ©ç çš„ç”Ÿæˆæ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Mask-GRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ç§å°†åŸºäºGroup Relative Policy Optimization (GRPO)çš„RLçº³å…¥è¿™ä¸€è¢«å¿½è§†èŒƒå¼çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè§è§£æ˜¯é‡æ–°å®šä¹‰è½¬ç§»æ¦‚ç‡ï¼Œè¿™ä¸å½“å‰çš„æ–¹æ³•ä¸åŒï¼Œå¹¶å°†è§£æ©è¿‡ç¨‹åˆ¶å®šä¸ºä¸€ä¸ªå¤šæ­¥å†³ç­–é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šç§æœ‰ç”¨çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬å»é™¤KLçº¦æŸã€åº”ç”¨ç¼©å‡ç­–ç•¥ä»¥åŠè¿‡æ»¤æ‰ä½è´¨é‡çš„æ ·æœ¬ã€‚é€šè¿‡ä½¿ç”¨Mask-GRPOï¼Œæˆ‘ä»¬æ”¹è¿›äº†åŸºå‡†æ¨¡å‹Show-oï¼Œåœ¨æ ‡å‡†çš„T2IåŸºå‡†æµ‹è¯•å’Œåå¥½å¯¹é½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xingzhejun/Mask-GRPO">https://github.com/xingzhejun/Mask-GRPO</a>ä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13418v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸé€æ¸å—åˆ°å…³æ³¨ï¼Œä½†å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ æ–¹æ³•é’ˆå¯¹æ‰©æ•£æ¨¡å‹æˆ–è‡ªå›å½’æ¨¡å‹ï¼Œå¿½ç•¥äº†æ©æ¨¡ç”Ÿæˆæ¨¡å‹ã€‚æœ¬ç ”ç©¶æå‡ºMask-GRPOï¼Œé¦–æ¬¡å°†åŸºäºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ èå…¥æ­¤é¢†åŸŸã€‚å…¶æ ¸å¿ƒæ˜¯é‡æ–°å®šä¹‰äº†è½¬æ¢æ¦‚ç‡ï¼Œå°†å…¶è¡¨è¿°ä¸ºåˆ†æ­¥å†³ç­–é—®é¢˜ã€‚é€šè¿‡ç§»é™¤KLçº¦æŸã€åº”ç”¨ç¼©å‡ç­–ç•¥åŠè¿‡æ»¤ä½è´¨é‡æ ·æœ¬ï¼Œæå‡äº†åŸºç¡€æ¨¡å‹Show-oåœ¨æ ‡å‡†æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¼˜äºç°æœ‰å‰æ²¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦å…³æ³¨æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ï¼Œå¿½ç•¥äº†æ©æ¨¡ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>Mask-GRPOæ–¹æ³•é¦–æ¬¡å°†Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ èå…¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸã€‚</li>
<li>Mask-GRPOé‡æ–°å®šä¹‰äº†è½¬æ¢æ¦‚ç‡ï¼Œå¹¶å°†å…¶è¡¨è¿°ä¸ºåˆ†æ­¥å†³ç­–é—®é¢˜ã€‚</li>
<li>Mask-GRPOé€šè¿‡ç§»é™¤KLçº¦æŸã€åº”ç”¨ç¼©å‡ç­–ç•¥åŠè¿‡æ»¤ä½è´¨é‡æ ·æœ¬ï¼Œæå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Mask-GRPOæ”¹å–„äº†åŸºç¡€æ¨¡å‹Show-oåœ¨æ ‡å‡†æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bfcd8e0c2178ab6505c029345bf20d98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726421&auth_key=1760726421-0-0-64113c5421ccfcb8e340195ddf1c2047&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-15ec5f1b3336702d660b0e87ba6164f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726428&auth_key=1760726428-0-0-7ecd919e5c13f45a46b9eeffaade007f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d8ba91ac8abb3029a259de25fbcf645~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726435&auth_key=1760726435-0-0-c1082efd82b02aad711a28e1fda5d269&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d01bcccaed0ac297e42817b74875c127~resize:0:q75.jpg?source=1f5c5e47&expiration=1760726443&auth_key=1760726443-0-0-2effe4fc7d9bd284eeb5d9b641d2fb50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-07dfd4190e066b6ffd27340fa7e6a279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760728472&auth_key=1760728472-0-0-7b38aef80c3e51a98c4a5f5c3d1e5e84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Agentic Design of Compositional Machines
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-11/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-3a927d8a3c411167e73facd194a696d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760137343&auth_key=1760137343-0-0-0e3516ade6215a562278545588981320&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-11  Paper2Video Automatic Video Generation from Scientific Papers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
