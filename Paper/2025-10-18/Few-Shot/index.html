<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Unifying Environment Perception and Route Choice Modeling for Trajectory   Representation Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-9ea2b7948d507d2a3f8a9514f3be86a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733406&auth_key=1760733406-0-0-c1df8e09920c2d8a6f5769cb5b41f9c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Unifying-Environment-Perception-and-Route-Choice-Modeling-for-Trajectory-Representation-Learning"><a href="#Unifying-Environment-Perception-and-Route-Choice-Modeling-for-Trajectory-Representation-Learning" class="headerlink" title="Unifying Environment Perception and Route Choice Modeling for Trajectory   Representation Learning"></a>Unifying Environment Perception and Route Choice Modeling for Trajectory   Representation Learning</h2><p><strong>Authors:Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song</strong></p>
<p>Trajectory Representation Learning (TRL) aims to encode raw trajectories into low-dimensional vectors, which can then be leveraged in various downstream tasks, including travel time estimation, location prediction, and trajectory similarity analysis. However, existing TRL methods suffer from a key oversight: treating trajectories as isolated spatio-temporal sequences, without considering the external environment and internal route choice behavior that govern their formation. To bridge this gap, we propose a novel framework that unifies comprehensive environment \textbf{P}erception and explicit \textbf{R}oute choice modeling for effective \textbf{Traj}ectory representation learning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces an Environment Perception Module to enhance the road network by capturing multi-granularity environmental semantics from surrounding POI distributions. Building on this environment-aware backbone, a Route Choice Encoder then captures the route choice behavior inherent in each trajectory by modeling its constituent road segment transitions as a sequence of decisions. These route-choice-aware representations are finally aggregated to form the global trajectory embedding. Extensive experiments on 3 real-world datasets across 5 downstream tasks validate the effectiveness and generalizability of PRTraj. Moreover, PRTraj demonstrates strong data efficiency, maintaining robust performance under few-shot scenarios. Our code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PRTraj">https://anonymous.4open.science/r/PRTraj</a>. </p>
<blockquote>
<p>è½¨è¿¹è¡¨ç¤ºå­¦ä¹ ï¼ˆTRLï¼‰æ—¨åœ¨å°†åŸå§‹è½¨è¿¹ç¼–ç ä¸ºä½ç»´å‘é‡ï¼Œç„¶åå¯ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—…è¡Œæ—¶é—´ä¼°è®¡ã€ä½ç½®é¢„æµ‹å’Œè½¨è¿¹ç›¸ä¼¼æ€§åˆ†æã€‚ç„¶è€Œï¼Œç°æœ‰çš„TRLæ–¹æ³•å­˜åœ¨ä¸€ä¸ªé‡è¦ç–å¿½ï¼šå°†è½¨è¿¹è§†ä¸ºå­¤ç«‹çš„æ—¶ç©ºåºåˆ—ï¼Œè€Œæ²¡æœ‰è€ƒè™‘æ§åˆ¶å…¶å½¢æˆçš„å¤–éƒ¨ç¯å¢ƒå’Œå†…éƒ¨è·¯çº¿é€‰æ‹©è¡Œä¸ºã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€å…¨é¢çš„ç¯å¢ƒæ„ŸçŸ¥å’Œæ˜ç¡®çš„è·¯çº¿é€‰æ‹©å»ºæ¨¡çš„æœ‰æ•ˆè½¨è¿¹è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºPRTrajã€‚å…·ä½“æ¥è¯´ï¼ŒPRTrajé¦–å…ˆå¼•å…¥ç¯å¢ƒæ„ŸçŸ¥æ¨¡å—ï¼Œé€šè¿‡æ•è·å‘¨å›´POIåˆ†å¸ƒçš„å¤šç²’åº¦ç¯å¢ƒè¯­ä¹‰æ¥å¢å¼ºé“è·¯ç½‘ç»œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè·¯çº¿é€‰æ‹©ç¼–ç å™¨æ•è·æ¯æ¡è½¨è¿¹å›ºæœ‰çš„è·¯çº¿é€‰æ‹©è¡Œä¸ºï¼Œé€šè¿‡å°†å…¶æ„æˆçš„è·¯æ®µè½¬æ¢å»ºæ¨¡ä¸ºä¸€ç³»åˆ—å†³ç­–ã€‚è¿™äº›åŸºäºè·¯çº¿é€‰æ‹©çš„è¡¨ç¤ºæœ€ç»ˆè¢«èšåˆä»¥å½¢æˆå…¨å±€è½¨è¿¹åµŒå…¥ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„äº”ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„å¹¿æ³›å®éªŒéªŒè¯äº†PRTrajçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚è€Œä¸”ï¼ŒPRTrajè¡¨ç°å‡ºå¾ˆå¼ºçš„æ•°æ®æ•ˆç‡ï¼Œåœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸‹ä¹Ÿèƒ½ä¿æŒç¨³å¥çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PRTraj%E3%80%82">https://anonymous.4open.science/r/PRTrajã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14819v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è½¨è¿¹è¡¨ç¤ºå­¦ä¹ ï¼ˆTRLï¼‰æ—¨åœ¨å°†åŸå§‹è½¨è¿¹ç¼–ç ä¸ºä½ç»´å‘é‡ï¼Œå¯åº”ç”¨äºæ—…è¡Œæ—¶é—´ä¼°è®¡ã€ä½ç½®é¢„æµ‹å’Œè½¨è¿¹ç›¸ä¼¼æ€§åˆ†æç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„TRLæ–¹æ³•å¿½ç•¥äº†è½¨è¿¹å½¢æˆèƒŒåçš„å¤–éƒ¨ç¯å¢ƒå’Œå†…éƒ¨è·¯çº¿é€‰æ‹©è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€ç¯å¢ƒæ„ŸçŸ¥å’Œè·¯çº¿é€‰æ‹©å»ºæ¨¡çš„æ–°æ¡†æ¶ï¼Œç”¨äºæœ‰æ•ˆçš„è½¨è¿¹è¡¨ç¤ºå­¦ä¹ ï¼Œç§°ä¸ºPRTrajã€‚å®éªŒè¡¨æ˜ï¼ŒPRTrajåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„äº”ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚è€Œä¸”ï¼ŒPRTrajåœ¨å°‘é‡æ•°æ®åœºæ™¯ä¸‹ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ•°æ®æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¨è¿¹è¡¨ç¤ºå­¦ä¹ ï¼ˆTRLï¼‰æ—¨åœ¨å°†è½¨è¿¹ç¼–ç ä¸ºä½ç»´å‘é‡ï¼Œç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰TRLæ–¹æ³•å¿½ç•¥è½¨è¿¹èƒŒåçš„å¤–éƒ¨ç¯å¢ƒå’Œå†…éƒ¨è·¯çº¿é€‰æ‹©è¡Œä¸ºã€‚</li>
<li>PRTrajæ¡†æ¶é€šè¿‡ç¯å¢ƒæ„ŸçŸ¥æ¨¡å—æ•æ‰å¤šç²’åº¦ç¯å¢ƒè¯­ä¹‰ï¼Œå¢å¼ºé“è·¯ç½‘ç»œã€‚</li>
<li>PRTrajé€šè¿‡è·¯çº¿é€‰æ‹©ç¼–ç å™¨å»ºæ¨¡è½¨è¿¹çš„å†…åœ¨è·¯çº¿é€‰æ‹©è¡Œä¸ºï¼Œå°†è·¯çº¿é€‰æ‹©æ„ŸçŸ¥è¡¨ç¤ºèšåˆå½¢æˆå…¨å±€è½¨è¿¹åµŒå…¥ã€‚</li>
<li>PRTrajåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„äº”ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>PRTrajåœ¨å°‘é‡æ•°æ®åœºæ™¯ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ•°æ®æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f8ba4b738ae94c3fd2fec233a93a654a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733081&auth_key=1760733081-0-0-9270663de6d0779461b293ea35dd3bc6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-47539b59c39185735dbb7ed803871346~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733088&auth_key=1760733088-0-0-d527ef3dd591146dd609ef5de82898da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-100ce34fd81ff065af7420753d464739~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733095&auth_key=1760733095-0-0-2677cfdf06572fa88242c2c34e65c669&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="David-vs-Goliath-A-comparative-study-of-different-sized-LLMs-for-code-generation-in-the-domain-of-automotive-scenario-generation"><a href="#David-vs-Goliath-A-comparative-study-of-different-sized-LLMs-for-code-generation-in-the-domain-of-automotive-scenario-generation" class="headerlink" title="David vs. Goliath: A comparative study of different-sized LLMs for code   generation in the domain of automotive scenario generation"></a>David vs. Goliath: A comparative study of different-sized LLMs for code   generation in the domain of automotive scenario generation</h2><p><strong>Authors:Philipp Bauerfeind, Amir Salarpour, David Fernandez, Pedram MohajerAnsari, Johannes Reschke, Mert D. PesÃ©</strong></p>
<p>Scenario simulation is central to testing autonomous driving systems. Scenic, a domain-specific language (DSL) for CARLA, enables precise and reproducible scenarios, but NL-to-Scenic generation with large language models (LLMs) suffers from scarce data, limited reproducibility, and inconsistent metrics. We introduce NL2Scenic, an open dataset and framework with 146 NL&#x2F;Scenic pairs, a difficulty-stratified 30-case test split, an Example Retriever, and 14 prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B&#x2F;13B&#x2F;34B), using text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics (compilation and generation), and compare them with an expert study (n&#x3D;11). EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88 percent of its expert score on local hardware. Retrieval-augmented prompting, Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a standardized, reproducible basis for evaluating Scenic code generation and indicate that mid-size open-source models are practical, cost-effective options for autonomous-driving scenario programming. </p>
<blockquote>
<p>åœºæ™¯æ¨¡æ‹Ÿæ˜¯æµ‹è¯•è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ ¸å¿ƒã€‚é’ˆå¯¹CARLAçš„åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰Scenicèƒ½å¤Ÿå®ç°ç²¾ç¡®ä¸”å¯é‡å¤çš„åœºæ™¯ï¼Œä½†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒNL-to-Scenicç”Ÿæˆé¢ä¸´ç€æ•°æ®ç¨€ç¼ºã€å¯é‡å¤æ€§æœ‰é™å’ŒæŒ‡æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†NL2Scenicï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«146ä¸ªNL&#x2F;Scenicå¯¹ã€éš¾åº¦åˆ†å±‚30ä¾‹æµ‹è¯•é›†çš„å¼€æ”¾æ•°æ®é›†å’Œæ¡†æ¶ï¼Œè¿˜åŒ…å«ç¤ºä¾‹æ£€ç´¢å™¨ä»¥åŠ14ç§æç¤ºå˜ä½“ï¼ˆZSã€FSã€CoTã€SPã€MoTï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†13ä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬å››ä¸ªä¸“æœ‰æ¨¡å‹ï¼ˆGPT-4oã€GPT-5ã€Claude-Sonnet-4ã€Gemini-2.5-proï¼‰å’Œä¹ä¸ªå¼€æºä»£ç æ¨¡å‹ï¼ˆQwen2.5Coder 0.5B-32Bï¼›CodeLlama 7B&#x2F;13B&#x2F;34Bï¼‰ï¼Œä½¿ç”¨äº†æ–‡æœ¬æŒ‡æ ‡ï¼ˆBLEUã€ChrFã€EDIT-SIMã€CrystalBLEUï¼‰å’Œæ‰§è¡ŒæŒ‡æ ‡ï¼ˆç¼–è¯‘å’Œç”Ÿæˆï¼‰ï¼Œå¹¶ä¸ä¸“å®¶ç ”ç©¶ï¼ˆn&#x3D;11ï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚EDIT-SIMä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æœ€å¥½ï¼›æˆ‘ä»¬è¿˜æå‡ºäº†EDIT-COMPï¼ˆEDIT-SIMå’Œç¼–è¯‘çš„F1å€¼ï¼‰ä½œä¸ºä¸€ä¸ªç¨³å¥çš„æ•°æ®é›†çº§åˆ«ä»£ç†ï¼Œå¯ä»¥æé«˜æ’åä¿çœŸåº¦ã€‚GPT-4oæ€»ä½“è¡¨ç°æœ€ä½³ï¼Œè€ŒQwen2.5Coder-14Båœ¨æœ¬åœ°ç¡¬ä»¶ä¸Šè¾¾åˆ°äº†ä¸“å®¶å¾—åˆ†çš„çº¦88%ã€‚æ£€ç´¢å¢å¼ºæç¤ºã€å¸¦æœ‰ç¤ºä¾‹æ£€ç´¢å™¨çš„Few-Shotï¼ˆFSERï¼‰å§‹ç»ˆå¯ä»¥æå‡è¾ƒå°æ¨¡å‹çš„è¡¨ç°ï¼Œè€Œåœ¨ä¸­ç­‰è§„æ¨¡ä¹‹åï¼Œè§„æ¨¡æ‰©å¤§æ”¶ç›Šé€’å‡ï¼ŒQwen2.5Coderåœ¨ç›¸å½“è§„æ¨¡ä¸Šè¡¨ç°ä¼˜äºCodeLlamaã€‚NL2Scenicå’ŒEDIT-COMPä¸ºè¯„ä¼°Scenicä»£ç ç”Ÿæˆæä¾›äº†æ ‡å‡†åŒ–ã€å¯é‡å¤çš„åŸºç¡€ï¼Œå¹¶è¡¨æ˜ä¸­ç­‰è§„æ¨¡çš„å¼€æºæ¨¡å‹æ˜¯è‡ªåŠ¨é©¾é©¶åœºæ™¯ç¼–ç¨‹å®ç”¨ä¸”ç»æµå®æƒ çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14115v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæµ‹è¯•ä¸­çš„åœºæ™¯æ¨¡æ‹Ÿé—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†NL2Scenicè¿™ä¸€å¼€æ”¾æ•°æ®é›†å’Œæ¡†æ¶ã€‚è¯¥æ¡†æ¶è§£å†³äº†NL-to-Scenicç”Ÿæˆä¸­çš„ä¸€äº›é—®é¢˜ï¼Œå¦‚æ•°æ®ç¨€ç¼ºã€é‡ç°æ€§å·®å’ŒæŒ‡æ ‡ä¸ä¸€è‡´ç­‰ã€‚é€šè¿‡å¼•å…¥å¤šç§æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œæœ¬æ–‡å‘ç°GPT-4oæ€§èƒ½æœ€ä½³ï¼Œè€ŒQwen2.5Coderåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°ä¼˜ç§€ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è®¨è®ºäº†æ•°æ®å¢å¼ºæç¤ºå¯¹å°å‹æ¨¡å‹æ€§èƒ½çš„æå‡ä»¥åŠæ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½çš„å…³ç³»ã€‚æ€»ä½“æ¥è¯´ï¼ŒNL2Scenicä¸ºScenicä»£ç ç”Ÿæˆæä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–ã€å¯é‡ç°çš„è¯„ä¼°åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NL2Scenicæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°Scenicä»£ç ç”Ÿæˆèƒ½åŠ›çš„å¼€æ”¾æ•°æ®é›†å’Œæ¡†æ¶ã€‚</li>
<li>NL2Scenicè§£å†³äº†NL-to-Scenicç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç¼ºã€é‡ç°æ€§å·®å’ŒæŒ‡æ ‡ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚</li>
<li>GPT-4oåœ¨æ€§èƒ½è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>Qwen2.5Coderåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°ä¼˜ç§€ï¼Œä¸ä¸“å®¶è¯„åˆ†æ¥è¿‘ã€‚</li>
<li>æ£€ç´¢å¢å¼ºæç¤ºæŠ€æœ¯æœ‰åŠ©äºæé«˜å°å‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½çš„å…³ç³»å‘ˆç°è¾¹é™…æ”¶ç›Šé€’å‡çš„è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f9ec88b449d2a1c1bf53c37dcc156cd7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733103&auth_key=1760733103-0-0-a8e10514007c1f9ddd9b3490c6134fe2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f950e4c45e29e92683eccb27bdffe761~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733109&auth_key=1760733109-0-0-fe728dbb8f18a409cacf4fd854aa1c11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2160770d4ea3f3f75ec8a9fdfea536a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733116&auth_key=1760733116-0-0-d08c14764bd8b2f92e03dc611b9ad001&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcc8e9abdc4459128bfd0ab7b091cbf9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733122&auth_key=1760733122-0-0-226f76308e21914452865a21d4d1a63a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-220a74aa78e4148397b0f4daed75d541~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733129&auth_key=1760733129-0-0-568777279f9e7497dba3246d31db5b4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Efficient-Few-Shot-Learning-in-Remote-Sensing-Fusing-Vision-and-Vision-Language-Models"><a href="#Efficient-Few-Shot-Learning-in-Remote-Sensing-Fusing-Vision-and-Vision-Language-Models" class="headerlink" title="Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and   Vision-Language Models"></a>Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and   Vision-Language Models</h2><p><strong>Authors:Jia Yun Chua, Argyrios Zolotas, Miguel Arana-Catania</strong></p>
<p>Remote sensing has become a vital tool across sectors such as urban planning, environmental monitoring, and disaster response. While the volume of data generated has increased significantly, traditional vision models are often constrained by the requirement for extensive domain-specific labelled data and their limited ability to understand the context within complex environments. Vision Language Models offer a complementary approach by integrating visual and textual data; however, their application to remote sensing remains underexplored, particularly given their generalist nature. This work investigates the combination of vision models and VLMs to enhance image analysis in remote sensing, with a focus on aircraft detection and scene understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and Gemini aims to achieve more accurate and contextually aware image interpretation. Performance is evaluated on both labelled and unlabelled remote sensing data, as well as degraded image scenarios which are crucial for remote sensing. The findings show an average MAE improvement of 48.46% across models in the accuracy of aircraft detection and counting, especially in challenging conditions, in both raw and degraded scenarios. A 6.17% improvement in CLIPScore for comprehensive understanding of remote sensing images is obtained. The proposed approach combining traditional vision models and VLMs paves the way for more advanced and efficient remote sensing image analysis, especially in few-shot learning scenarios. </p>
<blockquote>
<p>é¥æ„ŸæŠ€æœ¯å·²æˆä¸ºåŸå¸‚è§„åˆ’ã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³åº”å¯¹ç­‰é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚è™½ç„¶äº§ç”Ÿçš„æ•°æ®é‡å¤§å¹…å¢åŠ ï¼Œä½†ä¼ ç»Ÿè§†è§‰æ¨¡å‹å¾€å¾€å—é™äºéœ€è¦å¤§é‡ç‰¹å®šé¢†åŸŸçš„æ ‡è®°æ•°æ®åŠå…¶ç†è§£å¤æ‚ç¯å¢ƒä¸­ä¸Šä¸‹æ–‡çš„èƒ½åŠ›æœ‰é™ã€‚è§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®æä¾›äº†ä¸€ç§äº’è¡¥çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬åœ¨é¥æ„Ÿé¢†åŸŸçš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°å®ƒä»¬çš„é€šç”¨æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å°†è§†è§‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜é¥æ„Ÿå›¾åƒåˆ†æçš„ç²¾åº¦ã€‚å·¥ä½œé‡ç‚¹æ”¾åœ¨é£æœºæ£€æµ‹å’Œåœºæ™¯ç†è§£ä¸Šã€‚é€šè¿‡æ•´åˆYOLOæ¨¡å‹å’ŒLLaVAã€ChatGPTå’ŒGeminiç­‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°æ›´å‡†ç¡®å’Œå…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„å›¾åƒè§£è¯»ã€‚è¯„ä¼°æ€§èƒ½æ—¶ï¼Œæ—¢è€ƒè™‘äº†æ ‡è®°çš„å’Œæœªæ ‡è®°çš„é¥æ„Ÿæ•°æ®ï¼Œä¹Ÿè€ƒè™‘äº†å¯¹äºé¥æ„Ÿè‡³å…³é‡è¦çš„é€€åŒ–å›¾åƒåœºæ™¯ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåœ¨é£æœºæ£€æµ‹å’Œè®¡æ•°æ–¹é¢ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼Œæ–°æ¨¡å‹çš„å¹³å‡MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰æé«˜äº†48.46%ï¼Œå°¤å…¶æ˜¯åœ¨åŸå§‹å’Œé€€åŒ–åœºæ™¯ä¸­æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹æ›´æ˜¯å¦‚æ­¤ã€‚åœ¨é¥æ„Ÿå›¾åƒçš„ç»¼åˆç†è§£æ–¹é¢ï¼ŒCLIPScoreæé«˜äº†6.17%ã€‚æ‰€æå‡ºç»“åˆä¼ ç»Ÿè§†è§‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ä¸ºæ›´å…ˆè¿›å’Œé«˜æ•ˆçš„é¥æ„Ÿå›¾åƒåˆ†æé“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·å­¦ä¹ åœºæ™¯ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13993v1">PDF</a> 11 pages, 7 figures, 8 tables. To be published in Applied AI Letters</p>
<p><strong>Summary</strong><br>     é¥æ„ŸæŠ€æœ¯åœ¨åŸå¸‚è§„åˆ’ã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³å“åº”ç­‰é¢†åŸŸæ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚éšç€æ•°æ®é‡çš„å¢é•¿ï¼Œä¼ ç»Ÿè§†è§‰æ¨¡å‹å—é™äºå¤§é‡ç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨æ•°æ®å’Œå¤æ‚ç¯å¢ƒä¸‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ç»“åˆè§†è§‰æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»¥å¢å¼ºé¥æ„Ÿå›¾åƒåˆ†æçš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é£æœºæ£€æµ‹å’Œåœºæ™¯ç†è§£æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡æ•´åˆYOLOä¸LLaVAã€ChatGPTå’ŒGeminiç­‰VLMsï¼Œåœ¨æ ‡è®°å’Œæœªæ ‡è®°çš„é¥æ„Ÿæ•°æ®ä»¥åŠé€€åŒ–å›¾åƒåœºæ™¯ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºé£æœºæ£€æµ‹å’Œè®¡æ•°çš„å‡†ç¡®æ€§å¹³å‡æé«˜äº†48.46%ï¼Œå°¤å…¶æ˜¯åœ¨åŸå§‹å’Œé€€åŒ–åœºæ™¯ä¸­æ›´å…·æŒ‘æˆ˜æ€§ã€‚åŒæ—¶ï¼Œå¯¹é¥æ„Ÿå›¾åƒçš„ç»¼åˆç†è§£ä¹Ÿå–å¾—äº†CLIPScoreçš„6.17%çš„æå‡ã€‚ç»“åˆä¼ ç»Ÿè§†è§‰æ¨¡å‹å’ŒVLMsçš„æ–¹æ³•ä¸ºæ›´å…ˆè¿›å’Œé«˜æ•ˆçš„é¥æ„Ÿå›¾åƒåˆ†æé“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„ŸæŠ€æœ¯å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œå¦‚åŸå¸‚è§„åˆ’ã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³å“åº”ã€‚</li>
<li>ä¼ ç»Ÿè§†è§‰æ¨¡å‹å—é™äºéœ€è¦å¤§é‡ç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨æ•°æ®å’Œåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿé€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®æä¾›ä¸€ç§æ–°çš„æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†YOLOä¸LLaVAã€ChatGPTå’ŒGeminiç­‰VLMsè¿›è¡Œé¥æ„Ÿå›¾åƒåˆ†æï¼Œä¸“æ³¨äºé£æœºæ£€æµ‹å’Œåœºæ™¯ç†è§£ã€‚</li>
<li>åœ¨æ ‡è®°å’Œæœªæ ‡è®°çš„é¥æ„Ÿæ•°æ®ä»¥åŠé€€åŒ–å›¾åƒåœºæ™¯ä¸‹è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>é£æœºæ£€æµ‹å’Œè®¡æ•°çš„å‡†ç¡®æ€§å¹³å‡æé«˜äº†48.46%ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-69b53c4387fd41f10f1a198883a5218a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733136&auth_key=1760733136-0-0-768c8a0936306cb46d2b72d13e30ffc0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f4e91044e46ec7a67365e5661e63d10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733143&auth_key=1760733143-0-0-5e405eb37b73b5e81f70b4f188c57e38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc0b72fa6771635cb96cf04071c494f2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733150&auth_key=1760733150-0-0-46d396930776d2aac1993b264a7a4e29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1929543001a20b5f347bed399193ea43~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733156&auth_key=1760733156-0-0-4c147ac2145312a20c74c597701effd7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03f7700b7b38486bad5cac0a18f2384a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733163&auth_key=1760733163-0-0-611613d30bce322f7fa435c80be088e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ece416a660e0e9a9c7395e2959f2b998~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733170&auth_key=1760733170-0-0-fd2026fbcb8156d81b15ddf7847af7de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CoLoR-GAN-Continual-Few-Shot-Learning-with-Low-Rank-Adaptation-in-Generative-Adversarial-Networks"><a href="#CoLoR-GAN-Continual-Few-Shot-Learning-with-Low-Rank-Adaptation-in-Generative-Adversarial-Networks" class="headerlink" title="CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in   Generative Adversarial Networks"></a>CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in   Generative Adversarial Networks</h2><p><strong>Authors:Munsif Ali, Leonardo Rossi, Massimo Bertozzi</strong></p>
<p>Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \textcolor{red}{\textbf{\underline{c}}}ontinual few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with \textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \href{<a target="_blank" rel="noopener" href="https://github.com/munsifali11/CoLoR-GAN%7D%7BGithub">https://github.com/munsifali11/CoLoR-GAN}{Github</a>. </p>
<blockquote>
<p>åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„èƒŒæ™¯ä¸‹ï¼ŒæŒç»­å­¦ä¹ ï¼ˆCLï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å°‘é‡æ ·æœ¬ï¼ˆFSï¼‰è¿›è¡Œå­¦ä¹ çš„åœºæ™¯ä¸­ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜å°¤ä¸ºé‡è¦ã€‚å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ˆå¦‚LFS-GANï¼‰æ¯æ¬¡è®­ç»ƒè¿­ä»£éƒ½ä¼šå¼•å…¥å¤§é‡çš„æ–°æƒé‡ï¼Œè¿™åœ¨é•¿æœŸè€ƒè™‘æ—¶å˜å¾—å°¤ä¸ºæ˜¾è‘—ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†åä¸ºCoLoR-GANçš„GANsä¸­çš„æŒç»­å°‘é‡æ ·æœ¬å­¦ä¹ ï¼Œåˆ©ç”¨ä½ç§©è‡ªé€‚åº”æ¥è®¾è®¡æ¡†æ¶åŒæ—¶å¤„ç†å°‘é‡æ ·æœ¬å­¦ä¹ å’ŒæŒç»­å­¦ä¹ ã€‚é€šè¿‡åˆ©ç”¨ä½ç§©å¼ é‡æœ‰æ•ˆåœ°ä½¿æ¨¡å‹é€‚åº”ç›®æ ‡ä»»åŠ¡ï¼ŒåŒæ—¶è¿›ä¸€æ­¥å‡å°‘æ‰€éœ€çš„å‚æ•°æ•°é‡ã€‚é€šè¿‡åº”ç”¨ç®€å•çš„LoRAå®ç°ï¼Œæˆ‘ä»¬å·²ç»å–å¾—äº†ç›¸å½“ä¸é”™çš„ç»“æœã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–é€‚é…å™¨çš„å¤§å°ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹å·ç§¯å±‚çš„åµŒå¥—LoRAï¼ˆLLoRAï¼‰æŠ€æœ¯ã€‚æœ€åï¼Œè€ƒè™‘åˆ°LoRAè¶…å‚æ•°é€‰æ‹©çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€é¡¹å®è¯ç ”ç©¶ï¼Œä»¥ä¾¿è½»æ¾æ‰¾åˆ°æœ€ä½³å‚æ•°ã€‚æˆ‘ä»¬é€šè¿‡å¤šä¸ªåŸºå‡†æµ‹è¯•CLå’ŒFSä»»åŠ¡çš„å®éªŒè¯æ˜äº†CoLoR-GANçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰é«˜æ•ˆç‡ï¼Œè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ä½†å¤§å¹…å‡å°‘äº†èµ„æºæ¶ˆè€—ã€‚æºä»£ç å¯ä»¥åœ¨Githubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/munsifali11/CoLoR-GAN">https://github.com/munsifali11/CoLoR-GAN</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13869v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†é’ˆå¯¹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æŒç»­å­¦ä¹ ï¼ˆCLï¼‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ·æœ¬ï¼ˆFSï¼‰ä¸‹çš„æŒ‘æˆ˜ã€‚è®ºæ–‡æå‡ºäº†åä¸ºCoLoR-GANçš„æ¡†æ¶ï¼Œåˆ©ç”¨ä½ç§©å¼ é‡æ¥æœ‰æ•ˆåœ°é€‚åº”ç›®æ ‡ä»»åŠ¡ï¼ŒåŒæ—¶è¿›ä¸€æ­¥å‡å°‘æ‰€éœ€çš„å‚æ•°æ•°é‡ã€‚é€šè¿‡å¼•å…¥LoRA in LoRAï¼ˆLLoRAï¼‰æŠ€æœ¯ï¼Œä¼˜åŒ–äº†é€‚é…å™¨çš„å¤§å°ã€‚è®ºæ–‡è¿˜æä¾›äº†å…³äºLoRAè¶…å‚æ•°é€‰æ‹©çš„å®è¯ç ”ç©¶ï¼Œå¹¶é€šè¿‡å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡å±•ç¤ºäº†CoLoR-GANçš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†é«˜æ•ˆä¸”èµ„æºæ¶ˆè€—å¤§å¤§å‡å°‘çš„æ€§èƒ½ï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åä¸ºCoLoR-GANçš„æ¡†æ¶ï¼Œç”¨äºå¤„ç†æœ‰é™æ ·æœ¬ä¸‹çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„æŒç»­å­¦ä¹ é—®é¢˜ã€‚</li>
<li>CoLoR-GANåˆ©ç”¨ä½ç§©å¼ é‡è¿›è¡Œæ¨¡å‹é€‚åº”ï¼Œæœ‰æ•ˆå‡å°‘æ‰€éœ€çš„å‚æ•°æ•°é‡ã€‚</li>
<li>å¼•å…¥äº†LoRA in LoRAï¼ˆLLoRAï¼‰æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–é€‚é…å™¨å¤§å°ã€‚</li>
<li>è®ºæ–‡æä¾›äº†å…³äºLoRAè¶…å‚æ•°é€‰æ‹©çš„å®è¯ç ”ç©¶ï¼Œä»¥æ–¹ä¾¿æ‰¾åˆ°æœ€ä½³å‚æ•°ã€‚</li>
<li>é€šè¿‡å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡éªŒè¯äº†CoLoR-GANçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>CoLoR-GANè¾¾åˆ°äº†ç°æœ‰æœ€ä½³æ°´å¹³ï¼Œå¹¶å®ç°äº†èµ„æºæ¶ˆè€—å¤§å¤§å‡å°‘çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a72d6de1bd8b091e3790c23608b0d9ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733177&auth_key=1760733177-0-0-5d014989b69260f087f83506d3f79cfa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9570254770ff809635bf33de0be7b3fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733184&auth_key=1760733184-0-0-990645a453314dc81b54e0102645747f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ADMIT-Few-shot-Knowledge-Poisoning-Attacks-on-RAG-based-Fact-Checking"><a href="#ADMIT-Few-shot-Knowledge-Poisoning-Attacks-on-RAG-based-Fact-Checking" class="headerlink" title="ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking"></a>ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking</h2><p><strong>Authors:Yutao Wu, Xiao Liu, Yinghui Li, Yifeng Gao, Yifan Ding, Jiale Ding, Xiang Zheng, Xingjun Ma</strong></p>
<p>Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation (RAG) systems by injecting adversarial content into knowledge bases, tricking Large Language Models (LLMs) into producing attacker-controlled outputs grounded in manipulated context. Prior work highlights LLMsâ€™ susceptibility to misleading or malicious retrieved content. However, real-world fact-checking scenarios are more challenging, as credible evidence typically dominates the retrieval pool. To investigate this problem, we extend knowledge poisoning to the fact-checking setting, where retrieved context includes authentic supporting or refuting evidence. We propose \textbf{ADMIT} (\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a few-shot, semantically aligned poisoning attack that flips fact-checking decisions and induces deceptive justifications, all without access to the target LLMs, retrievers, or token-level control. Extensive experiments show that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4 cross-domain benchmarks, achieving an average attack success rate (ASR) of 86% at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining robust even in the presence of strong counter-evidence. Compared with prior state-of-the-art attacks, ADMIT improves ASR by 11.2% across all settings, exposing significant vulnerabilities in real-world RAG-based fact-checking systems. </p>
<blockquote>
<p>çŸ¥è¯†æ±¡æŸ“å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæ„æˆä¸¥é‡å¨èƒï¼Œå®ƒé€šè¿‡å‘çŸ¥è¯†åº“æ³¨å…¥å¯¹æŠ—æ€§å†…å®¹ï¼Œæ¬ºéª—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ“çºµçš„ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæ”»å‡»è€…æ§åˆ¶çš„è¾“å‡ºã€‚æ—©æœŸçš„ç ”ç©¶å¼ºè°ƒäº†LLMæ˜“å—è¯¯å¯¼æˆ–æ¶æ„æ£€ç´¢å†…å®¹çš„å½±å“ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„äº‹å®æ ¸æŸ¥åœºæ™¯æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå¯ä¿¡çš„è¯æ®é€šå¸¸ä¸»å¯¼æ£€ç´¢æ± ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†çŸ¥è¯†æ±¡æŸ“æ‰©å±•åˆ°äº‹å®æ ¸æŸ¥ç¯å¢ƒï¼Œå…¶ä¸­æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åŒ…æ‹¬çœŸå®çš„æ”¯æŒæˆ–åé©³è¯æ®ã€‚æˆ‘ä»¬æå‡ºADMITï¼ˆå¯¹æŠ—æ€§å¤šæ³¨å…¥æŠ€æœ¯ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°‘æ•°ã€è¯­ä¹‰å¯¹é½çš„æ±¡æŸ“æ”»å‡»ï¼Œèƒ½å¤Ÿé¢ è¦†äº‹å®æ ¸æŸ¥å†³ç­–å¹¶å¼•å‘æ¬ºéª—æ€§è¾©è§£ï¼Œæ— éœ€è®¿é—®ç›®æ ‡LLMã€æ£€ç´¢å™¨æˆ–ä»¤ç‰Œçº§åˆ«æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒADMITåœ¨4ä¸ªæ£€ç´¢å™¨ã€1aä¸ªLLMå’Œ4ä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„è½¬ç§»éå¸¸æœ‰æ•ˆï¼Œåœ¨æä½çš„æ±¡æŸ“ç‡ï¼ˆ0.93Ã—10^-6ï¼‰ä¸‹ï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰è¾¾åˆ°86%ï¼Œå³ä½¿åœ¨å­˜åœ¨å¼ºæœ‰åŠ›çš„åè¯çš„æƒ…å†µä¸‹ä¹Ÿä¿æŒç¨³å¥ã€‚ä¸å…ˆå‰çš„æœ€å…ˆè¿›çš„æ”»å‡»ç›¸æ¯”ï¼ŒADMITåœ¨æ‰€æœ‰è®¾ç½®ä¸­çš„ASRæé«˜äº†11.2%ï¼Œæš´éœ²äº†åŸºäºRAGçš„ç°å®ä¸–ç•Œäº‹å®æ ¸æŸ¥ç³»ç»Ÿä¸­çš„é‡å¤§æ¼æ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13842v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†çŸ¥è¯†ä¸­æ¯’å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„å¨èƒï¼Œé€šè¿‡å‘çŸ¥è¯†åº“ä¸­æ³¨å…¥å¯¹æŠ—æ€§å†…å®¹ï¼Œè¯¯å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”ŸåŸºäºæ“çºµä¸Šä¸‹æ–‡çš„è¾“å‡ºã€‚é’ˆå¯¹äº‹å®æ ¸æŸ¥åœºæ™¯ï¼Œæå‡ºADMITå¯¹æŠ—æ€§å¤šæ³¨å…¥æŠ€æœ¯ï¼Œèƒ½åœ¨æ— éœ€è®¿é—®ç›®æ ‡LLMã€æ£€ç´¢å™¨æˆ–æ§åˆ¶ä»¤ç‰Œçº§åˆ«çš„æƒ…å†µä¸‹ï¼Œç¿»è½¬äº‹å®æ ¸æŸ¥ç»“æœå¹¶äº§ç”Ÿæ¬ºéª—æ€§ç†ç”±ã€‚å®éªŒè¡¨æ˜ï¼ŒADMITåœ¨å››ä¸ªæ£€ç´¢å™¨ã€11ä¸ªLLMå’Œå››ä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰è¾¾åˆ°86%ï¼Œä¸”åœ¨å¼ºå¤§çš„åè¯å­˜åœ¨çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒç¨³å¥ã€‚ä¸å…ˆå‰çš„æœ€å…ˆè¿›çš„æ”»å‡»ç›¸æ¯”ï¼ŒADMITåœ¨æ‰€æœ‰è®¾ç½®ä¸­çš„ASRæé«˜äº†11.2%ï¼Œæš´éœ²äº†ç°å®ä¸–ç•Œä¸­åŸºäºRAGçš„äº‹å®æ ¸æŸ¥ç³»ç»Ÿçš„é‡å¤§æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†ä¸­æ¯’å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæ„æˆé‡å¤§å¨èƒï¼Œå¯é€šè¿‡æ³¨å…¥å¯¹æŠ—æ€§å†…å®¹è¯¯å¯¼LLMã€‚</li>
<li>åœ¨äº‹å®æ ¸æŸ¥åœºæ™¯ä¸­ï¼Œå­˜åœ¨å¯¹æŠ—æ€§å†…å®¹å¯èƒ½å¯¼è‡´è¯­è¨€æ¨¡å‹åœ¨çœŸå®è¯æ®ä¸å‡ä¿¡æ¯é—´åšå‡ºé”™è¯¯çš„å†³ç­–ã€‚</li>
<li>æå‡ºçš„ADMITæŠ€æœ¯èƒ½åœ¨æ— éœ€è®¿é—®ç›®æ ‡LLMã€æ£€ç´¢å™¨æˆ–æ§åˆ¶ä»¤ç‰Œçº§åˆ«çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆç¿»è½¬äº‹å®æ ¸æŸ¥ç»“æœã€‚</li>
<li>ADMITåœ¨å¤šä¸ªæ£€ç´¢å™¨å’ŒLLMä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰é«˜è¾¾86%ã€‚</li>
<li>ADMITåœ¨æä½çš„çŸ¥è¯†ä¸­æ¯’ç‡ä¸‹å³å¯å®ç°é«˜æ”»å‡»æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºRAGç³»ç»Ÿçš„æ˜¾è‘—æ¼æ´ã€‚</li>
<li>ä¸ç°æœ‰æ”»å‡»ç›¸æ¯”ï¼ŒADMITçš„æ”»å‡»æˆåŠŸç‡æé«˜äº†11.2%ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-80acfa63ac8f29408b45da1393b4ac1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733195&auth_key=1760733195-0-0-f6fdd20698972e32ca452422f866f1aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7430d6e6116daa35a39b4eeaf739ba2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733202&auth_key=1760733202-0-0-54cf6bed65c2984ef5edb9f49a55aaab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a37fa1dbf16bb89eac2a19a70d9372cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733209&auth_key=1760733209-0-0-b2e1f6ad5f00491c951d4a9a76c4b5b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Program-of-Thoughts-for-Financial-Reasoning-Leveraging-Dynamic-In-Context-Examples-and-Generative-Retrieval"><a href="#Program-of-Thoughts-for-Financial-Reasoning-Leveraging-Dynamic-In-Context-Examples-and-Generative-Retrieval" class="headerlink" title="Program of Thoughts for Financial Reasoning: Leveraging Dynamic   In-Context Examples and Generative Retrieval"></a>Program of Thoughts for Financial Reasoning: Leveraging Dynamic   In-Context Examples and Generative Retrieval</h2><p><strong>Authors:Subhendu Khatuya, Shashwat Naidu, Pawan Goyal, Niloy Ganguly</strong></p>
<p>Despite continuous advancements in the capabilities of large language models (LLMs), numerical reasoning remains a challenging area. Techniques like chain-of-thought prompting, tree-of-thought prompting, and program-of-thought prompting guide LLMs through intermediate reasoning steps. Although in-context learning with few-shot prompting has improved performance, LLMs still lag behind state-of-the-art models on financial numerical reasoning datasets such as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step framework, to enhance LLMsâ€™ capabilities in financial numerical reasoning. The first step utilizes a generative retriever to extract relevant facts from unstructured data, including both text and tables. This is followed by context-aware Program of Thought prompting with dynamic selection of in-context examples. Our model FINDER achieves a new state-of-the-art performance on both the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution accuracy improvements of 5.98% and 4.05%, respectively. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æŒç»­è¿›æ­¥ï¼Œæ•°å€¼æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜çš„é¢†åŸŸã€‚é“¾å¼æ€ç»´æç¤ºã€æ ‘çŠ¶æ€ç»´æç¤ºå’Œç¨‹åºåŒ–æ€ç»´æç¤ºç­‰æŠ€æœ¯é€šè¿‡å¼•å¯¼LLMè¿›è¡Œä¸­é—´æ¨ç†æ­¥éª¤æ¥æé«˜æ€§èƒ½ã€‚è™½ç„¶åŸºäºä¸Šä¸‹æ–‡çš„å°‘é‡æç¤ºçš„å­¦ä¹ å·²ç»æé«˜äº†æ€§èƒ½ï¼Œä½†LLMåœ¨é‡‘èæ•°å€¼æ¨ç†æ•°æ®é›†ï¼ˆå¦‚FinQAå’ŒConvFinQAï¼‰ä¸Šçš„è¡¨ç°ä»ç„¶è½åäºæœ€æ–°æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†FINDERï¼Œä¸€ä¸ªæ–°å‹çš„ä¸¤æ­¥æ¡†æ¶ï¼Œä»¥å¢å¼ºLLMåœ¨é‡‘èæ•°å€¼æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ç¬¬ä¸€æ­¥æ˜¯åˆ©ç”¨ç”Ÿæˆå¼æ£€ç´¢å™¨ä»éç»“æ„åŒ–æ•°æ®ä¸­æå–ç›¸å…³äº‹å®ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œè¡¨æ ¼ã€‚ç„¶åæ˜¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç¨‹åºåŒ–æ€ç»´æç¤ºï¼Œå¹¶åŠ¨æ€é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„FINDERæ¨¡å‹åœ¨FinQAå’ŒConvFinQAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œåˆ†åˆ«è¶…è¶Šäº†ä¹‹å‰çš„åŸºå‡†æµ‹è¯•ï¼Œæ‰§è¡Œå‡†ç¡®ç‡æé«˜äº†5.98%å’Œ4.05%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13157v1">PDF</a> This work has been accepted for publication in the Main Conference of   the Empirical Methods in Natural Language Processing (EMNLP) 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æŒç»­è¿›æ­¥ï¼Œæ•°å€¼æ¨ç†ä»æ˜¯å…¶é¢ä¸´çš„ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥FINDERè¿™ä¸€æ–°å‹ä¸¤æ­¥æ¡†æ¶ï¼Œæé«˜äº†LLMåœ¨è´¢åŠ¡æ•°å€¼æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é¦–å…ˆï¼Œåˆ©ç”¨ç”Ÿæˆå¼æ£€ç´¢å™¨ä»éç»“æ„åŒ–æ•°æ®ä¸­æå–ç›¸å…³äº‹å®ï¼Œç„¶åè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„â€œç¨‹åºæ€ç»´â€æç¤ºå¹¶åŠ¨æ€é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚æ¨¡å‹åœ¨FinQAå’ŒConvFinQAæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°æ–°çš„é¢†å…ˆæ°´å¹³ï¼Œæ‰§è¡Œå‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†5.98%å’Œ4.05%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å€¼æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>FINDERæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œåˆ†ä¸ºä¸¤æ­¥ï¼Œæ—¨åœ¨æé«˜LLMåœ¨è´¢åŠ¡æ•°å€¼æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç”Ÿæˆå¼æ£€ç´¢å™¨å¯ä»éç»“æ„åŒ–æ•°æ®ä¸­æå–ç›¸å…³äº‹å®ã€‚</li>
<li>ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„â€œç¨‹åºæ€ç»´â€æç¤ºèƒ½æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŠ¨æ€é€‰æ‹©ä¸Šä¸‹æ–‡ç¤ºä¾‹æ˜¯FINDERæ¡†æ¶çš„ä¸€ä¸ªé‡è¦ç‰¹ç‚¹ã€‚</li>
<li>FINDERåœ¨FinQAå’ŒConvFinQAæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°æ–°çš„é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3e9288067c813b39cce6839910b913b2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733218&auth_key=1760733218-0-0-1f70ebc98539fc03569475f65f0b0a7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-205050cf20c72746d8de5a7c8322a231~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733225&auth_key=1760733225-0-0-376a10e7503a69234321010e222379b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c67c53005378ff2b3f483a9cbdf336d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733232&auth_key=1760733232-0-0-a745e9c03eeb7748fa1c2837b82a2692&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-271b73c9179c68c8bb80f6015a2d9eec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733239&auth_key=1760733239-0-0-578376c108633b12d409761c972b2bea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66dc1a81e9b953e5c7084fef69c688af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733245&auth_key=1760733245-0-0-68b9d3b900b33352dac0f8bdef69ea07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Semi-Supervised-Learning-for-Abnormal-Stop-Detection-from-Sparse-GPS-Trajectories"><a href="#Few-Shot-Semi-Supervised-Learning-for-Abnormal-Stop-Detection-from-Sparse-GPS-Trajectories" class="headerlink" title="Few Shot Semi-Supervised Learning for Abnormal Stop Detection from   Sparse GPS Trajectories"></a>Few Shot Semi-Supervised Learning for Abnormal Stop Detection from   Sparse GPS Trajectories</h2><p><strong>Authors:Muhammad Ayub Sabir, Junbiao Pang, Jiaqi Wu, Fatima Ashraf</strong></p>
<p>Abnormal stop detection (ASD) in intercity coach transportation is critical for ensuring passenger safety, operational reliability, and regulatory compliance. However, two key challenges hinder ASD effectiveness: sparse GPS trajectories, which obscure short or unauthorized stops, and limited labeled data, which restricts supervised learning. Existing methods often assume dense sampling or regular movement patterns, limiting their applicability. To address data sparsity, we propose a Sparsity-Aware Segmentation (SAS) method that adaptively defines segment boundaries based on local spatial-temporal density. Building upon these segments, we introduce three domain-specific indicators to capture abnormal stop behaviors. To further mitigate the impact of sparsity, we develop Locally Temporal-Indicator Guided Adjustment (LTIGA), which smooths these indicators via local similarity graphs. To overcome label scarcity, we construct a spatial-temporal graph where each segment is a node with LTIGA-refined features. We apply label propagation to expand weak supervision across the graph, followed by a GCN to learn relational patterns. A final self-training module incorporates high-confidence pseudo-labels to iteratively improve predictions. Experiments on real-world coach data show an AUC of 0.854 and AP of 0.866 using only 10 labeled instances, outperforming prior methods. The code and dataset are publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git%7D">https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git}</a> </p>
<blockquote>
<p>åŸé™…å®¢è½¦è¿è¾“ä¸­çš„å¼‚å¸¸åœè½¦æ£€æµ‹ï¼ˆASDï¼‰å¯¹äºç¡®ä¿ä¹˜å®¢å®‰å…¨ã€è¿è¥å¯é æ€§å’Œæ³•è§„åˆè§„æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜é˜»ç¢äº†ASDçš„æœ‰æ•ˆæ€§ï¼šç¨€ç–çš„GPSè½¨è¿¹ï¼Œè¿™æ©ç›–äº†çŸ­æš‚æˆ–æœªç»æˆæƒçš„åœè½¦ï¼›ä»¥åŠæœ‰é™çš„æœ‰æ ‡ç­¾æ•°æ®ï¼Œè¿™é™åˆ¶äº†ç›‘ç£å­¦ä¹ ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾å¯†é›†é‡‡æ ·æˆ–è§„å¾‹çš„è¿åŠ¨æ¨¡å¼ï¼Œä»è€Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç–é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨€ç–æ„ŸçŸ¥åˆ†å‰²ï¼ˆSASï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŸºäºå±€éƒ¨æ—¶ç©ºå¯†åº¦è‡ªé€‚åº”åœ°å®šä¹‰æ®µè¾¹ç•Œã€‚åœ¨è¿™äº›åˆ†æ®µçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªé¢†åŸŸç‰¹å®šçš„æŒ‡æ ‡æ¥æ•æ‰å¼‚å¸¸åœè½¦è¡Œä¸ºã€‚ä¸ºäº†å‡è½»ç¨€ç–æ€§é€ æˆçš„å½±å“ï¼Œæˆ‘ä»¬å¼€å‘äº†å±€éƒ¨æ—¶é—´æŒ‡æ ‡å¼•å¯¼è°ƒæ•´ï¼ˆLTIGAï¼‰ï¼Œé€šè¿‡å±€éƒ¨ç›¸ä¼¼å›¾å¹³æ»‘è¿™äº›æŒ‡æ ‡ã€‚ä¸ºäº†å…‹æœæ ‡ç­¾ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ—¶ç©ºå›¾ï¼Œå…¶ä¸­æ¯ä¸ªåˆ†æ®µéƒ½æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå…·æœ‰LTIGAç²¾ç‚¼çš„ç‰¹å¾ã€‚æˆ‘ä»¬åº”ç”¨æ ‡ç­¾ä¼ æ’­æ¥åœ¨å›¾ä¸­æ‰©å±•å¼±ç›‘ç£ï¼Œéšåä½¿ç”¨GCNæ¥å­¦ä¹ å…³ç³»æ¨¡å¼ã€‚æœ€ç»ˆçš„è‡ªæˆ‘è®­ç»ƒæ¨¡å—ç»“åˆäº†é«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾ï¼Œä»¥è¿­ä»£åœ°æ”¹è¿›é¢„æµ‹ã€‚åœ¨ç°å®ä¸–ç•Œçš„å®¢è½¦æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨10ä¸ªæœ‰æ ‡ç­¾çš„å®ä¾‹ï¼ŒAUCä¸º0.854ï¼ŒAPä¸º0.866ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å·²åœ¨[\url{<a target="_blank" rel="noopener" href="https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git%7D">https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git}</a> ]ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12686v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸé™…å®¢è½¦è¿è¾“ä¸­çš„å¼‚å¸¸åœè½¦æ£€æµ‹ï¼ˆASDï¼‰å¯¹äºç¡®ä¿ä¹˜å®¢å®‰å…¨ã€è¿è¥å¯é æ€§å’Œæ³•è§„åˆè§„æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¨€ç–çš„GPSè½¨è¿¹ï¼Œæ©ç›–äº†çŸ­æš‚æˆ–æœªç»æˆæƒçš„åœè½¦ï¼›ä»¥åŠæœ‰é™çš„æœ‰æ ‡ç­¾æ•°æ®ï¼Œé™åˆ¶äº†ç›‘ç£å­¦ä¹ ã€‚ä¸ºåº”å¯¹æ•°æ®ç¨€ç–é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨€ç–æ„ŸçŸ¥åˆ†æ®µï¼ˆSASï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯åŸºäºå±€éƒ¨æ—¶ç©ºå¯†åº¦è‡ªé€‚åº”åœ°å®šä¹‰åˆ†æ®µè¾¹ç•Œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥ä¸‰ä¸ªé¢†åŸŸç‰¹å®šçš„æŒ‡æ ‡æ¥æ•æ‰å¼‚å¸¸åœè½¦è¡Œä¸ºã€‚ä¸ºè¿›ä¸€æ­¥å‡è½»ç¨€ç–æ€§çš„å½±å“ï¼Œæˆ‘ä»¬å¼€å‘äº†å±€éƒ¨æ—¶é—´æŒ‡æ ‡å¼•å¯¼è°ƒæ•´ï¼ˆLTIGAï¼‰ï¼Œé€šè¿‡å±€éƒ¨ç›¸ä¼¼å›¾å¹³æ»‘è¿™äº›æŒ‡æ ‡ã€‚ä¸ºè§£å†³æ ‡ç­¾ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ—¶ç©ºå›¾ï¼Œæ¯ä¸ªåˆ†æ®µéƒ½æ˜¯ä¸€ä¸ªå¸¦æœ‰LTIGAä¼˜åŒ–ç‰¹å¾çš„èŠ‚ç‚¹ã€‚æˆ‘ä»¬åº”ç”¨æ ‡ç­¾ä¼ æ’­åœ¨å›¾ä¸­æ‰©å±•å¼±ç›‘ç£ï¼Œéšåä½¿ç”¨GCNå­¦ä¹ å…³ç³»æ¨¡å¼ã€‚æœ€åçš„è‡ªè®­ç»ƒæ¨¡å—ç»“åˆé«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾ï¼Œä»¥è¿­ä»£æ–¹å¼æ”¹è¿›é¢„æµ‹ã€‚åœ¨çœŸå®å®¢è½¦æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨10ä¸ªæœ‰æ ‡ç­¾å®ä¾‹çš„AUCä¸º0.854ï¼ŒAPä¸º0.866ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼‚å¸¸åœè½¦æ£€æµ‹åœ¨åŸé™…å®¢è½¦è¿è¾“ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æ•°æ®ç¨€ç–å’Œæ ‡ç­¾ç¨€ç¼ºæ˜¯å¼‚å¸¸åœè½¦æ£€æµ‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¨€ç–æ„ŸçŸ¥åˆ†æ®µï¼ˆSASï¼‰æ–¹æ³•ï¼Œè‡ªé€‚åº”åœ°å¤„ç†æ•°æ®ç¨€ç–é—®é¢˜ã€‚</li>
<li>å¼•å…¥ä¸‰ä¸ªé¢†åŸŸç‰¹å®šæŒ‡æ ‡æ¥æ•æ‰å¼‚å¸¸åœè½¦è¡Œä¸ºã€‚</li>
<li>å¼€å‘å±€éƒ¨æ—¶é—´æŒ‡æ ‡å¼•å¯¼è°ƒæ•´ï¼ˆLTIGAï¼‰ä»¥å¹³æ»‘æŒ‡æ ‡ã€‚</li>
<li>æ„å»ºæ—¶ç©ºå›¾å¹¶ä½¿ç”¨æ ‡ç­¾ä¼ æ’­å’ŒGCNæ¥æ‰©å±•å¼±ç›‘ç£å¹¶å­¦ä¹ å…³ç³»æ¨¡å¼ã€‚</li>
<li>é€šè¿‡è‡ªè®­ç»ƒæ¨¡å—ç»“åˆé«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾æé«˜é¢„æµ‹æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-712e81e7ad7c5589975e5e6b373079e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733253&auth_key=1760733253-0-0-c982f4ee0b06b0e7b11bd37c4b9a92a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-64e943545903c3af72f67e3de49b4e04~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733260&auth_key=1760733260-0-0-70154aca43a746f0ce95a199df796912&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de372039edeea3ee054fab3fc4f5d5b2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733267&auth_key=1760733267-0-0-fee762d081a332ebf27bd390f2bd9b5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f092627e73477fcd17a8254a50d03c71~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733273&auth_key=1760733273-0-0-41f6ab13c48628d5c6c6622cb26d9fb4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb4353d335beaef38da3f865eaa362f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733281&auth_key=1760733281-0-0-98f658ec5a235ef0f9cb6686431075b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ee38ef12df0feaefa7821aca0eab88a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733288&auth_key=1760733288-0-0-79ec9ccb9ce76f0edf154789a840704b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43849d3ede57b1627c4889bc537361ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733294&auth_key=1760733294-0-0-c51dfa92314c5dcd8ab7f1d8cd2d6155&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef320d97760894c00cd22617286f23e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733301&auth_key=1760733301-0-0-a503cf3f413624963395fea52e082664&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CoRA-Covariate-Aware-Adaptation-of-Time-Series-Foundation-Models"><a href="#CoRA-Covariate-Aware-Adaptation-of-Time-Series-Foundation-Models" class="headerlink" title="CoRA: Covariate-Aware Adaptation of Time Series Foundation Models"></a>CoRA: Covariate-Aware Adaptation of Time Series Foundation Models</h2><p><strong>Authors:Guo Qin, Zhi Chen, Yong Liu, Zhiyuan Shi, Haixuan Liu, Xiangdong Huang, Jianmin Wang, Mingsheng Long</strong></p>
<p>Time Series Foundation Models (TSFMs) have shown significant impact through their model capacity, scalability, and zero-shot generalization. However, due to the heterogeneity of inter-variate dependencies and the backbone scalability on large-scale multivariate datasets, most TSFMs are typically pre-trained on univariate time series. This limitation renders them oblivious to crucial information from diverse covariates in real-world forecasting tasks. To further enhance the performance of TSFMs, we propose a general covariate-aware adaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones of foundation models while effectively incorporating exogenous covariates from various modalities, including time series, language, and images, to improve the quality of predictions. Technically, CoRA maintains the equivalence of initialization and parameter consistency during adaptation. With preserved backbones of foundation models as frozen feature extractors, the outcome embeddings from foundation models are empirically demonstrated more informative than raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE) to automatically evaluate covariates regarding their causal predictability with respect to the target variate. We incorporate these weighted embeddings with a zero-initialized condition-injection mechanism, avoiding catastrophic forgetting of pre-trained foundation models and gradually integrates exogenous information. Extensive experiments show that CoRA of TSFMs surpasses state-of-the-art covariate-aware deep forecasters with full or few-shot training samples, achieving 31.1% MSE reduction on covariate-aware forecasting. Compared to other adaptation methods, CoRA exhibits strong compatibility with various advanced TSFMs and extends the scope of covariates to other modalities, presenting a practical paradigm for the application of TSFMs. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰é€šè¿‡å…¶æ¨¡å‹å®¹é‡ã€å¯æ‰©å±•æ€§å’Œé›¶å°„æ³›åŒ–èƒ½åŠ›äº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚ç„¶è€Œï¼Œç”±äºå˜é‡é—´ä¾èµ–æ€§çš„å¼‚è´¨æ€§å’Œå¤§è§„æ¨¡å¤šå…ƒæ•°æ®é›†åœ¨ä¸»å¹²ä¸Šçš„å¯æ‰©å±•æ€§ï¼Œå¤§å¤šæ•°TSFMé€šå¸¸æ˜¯åœ¨å•å˜é‡æ—¶é—´åºåˆ—ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ã€‚è¿™ä¸€é™åˆ¶ä½¿å¾—å®ƒä»¬æ— æ³•æ³¨æ„åˆ°çœŸå®ä¸–ç•Œé¢„æµ‹ä»»åŠ¡ä¸­æ¥è‡ªä¸åŒåå˜é‡çš„å…³é”®ä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜TSFMçš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä¸ºTSFMæå‡ºäº†ä¸€ä¸ªé€šç”¨çš„åå˜é‡æ„ŸçŸ¥é€‚åº”ï¼ˆCoRAï¼‰æ¡†æ¶ã€‚å®ƒåˆ©ç”¨åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒä¸»å¹²ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ç»“åˆäº†æ¥è‡ªå„ç§æ¨¡æ€çš„å¤–æºåå˜é‡ï¼ŒåŒ…æ‹¬æ—¶é—´åºåˆ—ã€è¯­è¨€å’Œå›¾åƒï¼Œä»¥æé«˜é¢„æµ‹çš„è´¨é‡ã€‚æŠ€æœ¯ä¸Šï¼ŒCoRAåœ¨é€‚åº”è¿‡ç¨‹ä¸­ä¿æŒåˆå§‹åŒ–å’Œå‚æ•°çš„ä¸€è‡´æ€§ã€‚åŸºç¡€æ¨¡å‹çš„ä¿ç•™ä¸»å¹²ä½œä¸ºå†»ç»“çš„ç‰¹å¾æå–å™¨ï¼ŒåŸºç¡€æ¨¡å‹çš„è¾“å‡ºåµŒå…¥è¢«è¯æ˜æ¯”åŸå§‹æ•°æ®æ›´å…·ä¿¡æ¯æ€§ã€‚æ­¤å¤–ï¼ŒCoRAé‡‡ç”¨æ–°é¢–çš„æ ¼å…°æ°å› æœåµŒå…¥ï¼ˆGCEï¼‰è‡ªåŠ¨è¯„ä¼°åå˜é‡ä¸ç›®æ ‡å˜é‡ä¹‹é—´çš„å› æœå¯é¢„æµ‹æ€§ã€‚æˆ‘ä»¬å°†è¿™äº›åŠ æƒåµŒå…¥ä¸é›¶åˆå§‹åŒ–æ¡ä»¶æ³¨å…¥æœºåˆ¶ç›¸ç»“åˆï¼Œé¿å…äº†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶é€æ­¥æ•´åˆäº†å¤–æ¥ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTSFMçš„CoRAè¶…è¶Šäº†å…·æœ‰å…¨æ•°æ®æˆ–å°‘é‡è®­ç»ƒæ ·æœ¬çš„å…ˆè¿›åå˜é‡æ„ŸçŸ¥æ·±åº¦é¢„æµ‹å™¨ï¼Œåœ¨åå˜é‡æ„ŸçŸ¥é¢„æµ‹ä¸Šå®ç°äº†31.1%çš„MSEé™ä½ã€‚ä¸å…¶ä»–é€‚åº”æ–¹æ³•ç›¸æ¯”ï¼ŒCoRAä¸å„ç§å…ˆè¿›TSFMå…·æœ‰å¾ˆå¼ºçš„å…¼å®¹æ€§ï¼Œå¹¶å°†åå˜é‡çš„èŒƒå›´æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œä¸ºTSFMçš„åº”ç”¨æä¾›äº†ä¸€ä¸ªå®ç”¨çš„èŒƒä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12681v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ—¶é—´åºåˆ—è¡¨å¾æ¨¡å‹ï¼ˆTSFMsï¼‰åœ¨å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„åå˜é‡æ„ŸçŸ¥é€‚åº”ï¼ˆCoRAï¼‰æ¡†æ¶æ¥å¢å¼ºTSFMsçš„æ€§èƒ½ã€‚CoRAæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹éª¨æ¶ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ç»“åˆæ¥è‡ªå„ç§æ¨¡æ€çš„å¤–æºæ€§åå˜é‡ï¼ŒåŒ…æ‹¬æ—¶é—´åºåˆ—ã€è¯­è¨€å’Œå›¾åƒï¼Œä»¥æé«˜é¢„æµ‹è´¨é‡ã€‚é€šè¿‡ä¿æŒåˆå§‹åŒ–å’Œå‚æ•°ä¸€è‡´æ€§ï¼ŒCoRAåœ¨é€‚åº”è¿‡ç¨‹ä¸­é¿å…äº†ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶é€šè¿‡åµŒå…¥Grangerå› æœåµŒå…¥ï¼ˆGCEï¼‰è‡ªåŠ¨è¯„ä¼°åå˜é‡ä¸ç›®æ ‡å˜é‡ä¹‹é—´çš„å› æœé¢„æµ‹å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼ŒCoRAåœ¨åå˜é‡æ„ŸçŸ¥é¢„æµ‹æ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ·±åº¦é¢„æµ‹å™¨ï¼Œå¹¶ä¸”åœ¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸‹ä¹Ÿèƒ½å®ç°å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—è¡¨å¾æ¨¡å‹ï¼ˆTSFMsï¼‰åœ¨å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦é¢„è®­ç»ƒäºå•å˜é‡æ—¶é—´åºåˆ—ï¼Œå¿½ç•¥å¤šæ ·åå˜é‡çš„ä¿¡æ¯ã€‚</li>
<li>æå‡ºCoRAæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹éª¨æ¶å¹¶èå…¥å¤–æºæ€§åå˜é‡ï¼Œæé«˜é¢„æµ‹è´¨é‡ã€‚</li>
<li>CoRAé€šè¿‡ç»´æŠ¤åˆå§‹åŒ–å’Œå‚æ•°ä¸€è‡´æ€§ï¼Œåœ¨é€‚åº”è¿‡ç¨‹ä¸­é¿å…ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>CoRAç»“åˆGrangerå› æœåµŒå…¥ï¼ˆGCEï¼‰è¯„ä¼°åå˜é‡çš„é¢„æµ‹è´¡çŒ®ã€‚</li>
<li>CoRAæ¡†æ¶ä¸å…¶ä»–å…ˆè¿›çš„TSFMså…¼å®¹æ€§å¼ºï¼Œå¹¶èƒ½å°†åå˜é‡çš„åº”ç”¨èŒƒå›´æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºCoRAåœ¨åå˜é‡æ„ŸçŸ¥é¢„æµ‹æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-81d709292afc53524a12153ccae374b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733308&auth_key=1760733308-0-0-aab445afc1cdc0842f626fcbfd80a0ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8932777f384bce2a1f56edd621c0b90b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733315&auth_key=1760733315-0-0-c7d916e2e2fd78865abf5c37166de747&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2fae9b09036b210312c9dd2748f5607~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733322&auth_key=1760733322-0-0-6d40377d1034aa9b526f4e55bab14dad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a103094a47500d33c029ef9b44166f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733328&auth_key=1760733328-0-0-b906c4fce8cf53b689a70c5a82b9da08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Graph-Few-Shot-Learning-via-Adaptive-Spectrum-Experts-and-Cross-Set-Distribution-Calibration"><a href="#Graph-Few-Shot-Learning-via-Adaptive-Spectrum-Experts-and-Cross-Set-Distribution-Calibration" class="headerlink" title="Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set   Distribution Calibration"></a>Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set   Distribution Calibration</h2><p><strong>Authors:Yonghao Liu, Yajun Wang, Chunli Guo, Wei Pang, Ximing Li, Fausto Giunchiglia, Xiaoyue Feng, Renchu Guan</strong></p>
<p>Graph few-shot learning has attracted increasing attention due to its ability to rapidly adapt models to new tasks with only limited labeled nodes. Despite the remarkable progress made by existing graph few-shot learning methods, several key limitations remain. First, most current approaches rely on predefined and unified graph filters (e.g., low-pass or high-pass filters) to globally enhance or suppress node frequency signals. Such fixed spectral operations fail to account for the heterogeneity of local topological structures inherent in real-world graphs. Moreover, these methods often assume that the support and query sets are drawn from the same distribution. However, under few-shot conditions, the limited labeled data in the support set may not sufficiently capture the complex distribution of the query set, leading to suboptimal generalization. To address these challenges, we propose GRACE, a novel Graph few-shot leaRning framework that integrates Adaptive spectrum experts with Cross-sEt distribution calibration techniques. Theoretically, the proposed approach enhances model generalization by adapting to both local structural variations and cross-set distribution calibration. Empirically, GRACE consistently outperforms state-of-the-art baselines across a wide range of experimental settings. Our code can be found here. </p>
<blockquote>
<p>å›¾å°‘æ•°æ ·æœ¬å­¦ä¹ å› å…¶ä»…åˆ©ç”¨æœ‰é™æ ‡è®°èŠ‚ç‚¹å°±èƒ½è¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚å°½ç®¡ç°æœ‰çš„å›¾å°‘æ•°æ ·æœ¬å­¦ä¹ æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å…³é”®å±€é™æ€§ã€‚é¦–å…ˆï¼Œå¤§å¤šæ•°å½“å‰æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰å’Œç»Ÿä¸€çš„å›¾æ»¤æ³¢å™¨ï¼ˆä¾‹å¦‚ä½é€šæˆ–é«˜é€šæ»¤æ³¢å™¨ï¼‰æ¥å…¨å±€å¢å¼ºæˆ–æŠ‘åˆ¶èŠ‚ç‚¹é¢‘ç‡ä¿¡å·ã€‚è¿™ç§å›ºå®šçš„è°±æ“ä½œæœªèƒ½è€ƒè™‘åˆ°çœŸå®ä¸–ç•Œå›¾ä¸­å›ºæœ‰çš„å±€éƒ¨æ‹“æ‰‘ç»“æ„çš„å¼‚è´¨æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å‡è®¾æ”¯æ’‘é›†å’ŒæŸ¥è¯¢é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨å°‘æ•°æ ·æœ¬æ¡ä»¶ä¸‹ï¼Œæ”¯æ’‘é›†ä¸­æœ‰é™çš„æ ‡è®°æ•°æ®å¯èƒ½æ— æ³•å……åˆ†æ•æ‰æŸ¥è¯¢é›†çš„å¤æ‚åˆ†å¸ƒï¼Œå¯¼è‡´æ¬¡ä¼˜æ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GRACEï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å›¾å°‘æ•°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œèåˆäº†è‡ªé€‚åº”è°±ä¸“å®¶ä¸è·¨é›†åˆ†å¸ƒæ ¡å‡†æŠ€æœ¯ã€‚ç†è®ºä¸Šï¼Œè¯¥æ–¹æ³•é€šè¿‡é€‚åº”å±€éƒ¨ç»“æ„å˜åŒ–å’Œè·¨é›†åˆ†å¸ƒæ ¡å‡†ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»éªŒä¸Šï¼ŒGRACEåœ¨å¹¿æ³›çš„å®éªŒè®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨æ­¤å¤„æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12140v1">PDF</a> NeurIPS25</p>
<p><strong>Summary</strong><br>     å›¾å°‘æ ·æœ¬å­¦ä¹ å› èƒ½å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ä¸”åªéœ€æœ‰é™æ ‡è®°èŠ‚ç‚¹è€Œå¤‡å—å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•è™½æœ‰æ‰€è¿›å±•ï¼Œä½†ä»å­˜åœ¨å…³é”®å±€é™ã€‚å¤šæ•°æ–¹æ³•ä¾èµ–é¢„è®¾çš„ç»Ÿä¸€å›¾æ»¤æ³¢å™¨è¿›è¡Œå…¨å±€å¢å¼ºæˆ–æŠ‘åˆ¶èŠ‚ç‚¹é¢‘ç‡ä¿¡å·ï¼Œå¿½ç•¥äº†çœŸå®å›¾ä¸­å±€éƒ¨æ‹“æ‰‘ç»“æ„çš„å¼‚è´¨æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¸¸å‡è®¾æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œä½†åœ¨å°‘æ ·æœ¬æ¡ä»¶ä¸‹ï¼Œæ”¯æŒé›†ä¸­æœ‰é™çš„æ ‡è®°æ•°æ®å¯èƒ½ä¸è¶³ä»¥æ•æ‰æŸ¥è¯¢é›†çš„å¤æ‚åˆ†å¸ƒï¼Œå¯¼è‡´æ¬¡ä¼˜æ³›åŒ–ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºGRACEï¼Œä¸€ä¸ªæ•´åˆè‡ªé€‚åº”é¢‘è°±ä¸“å®¶ä¸è·¨é›†åˆ†å¸ƒæ ¡å‡†æŠ€æœ¯çš„å›¾å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ã€‚ç†è®ºä¸Šï¼Œè¯¥æ–¹æ³•é€šè¿‡é€‚åº”å±€éƒ¨ç»“æ„å˜åŒ–å’Œè·¨é›†åˆ†å¸ƒæ ¡å‡†æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ç»éªŒä¸Šï¼ŒGRACEåœ¨å¹¿æ³›å®éªŒè®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾å°‘æ ·æœ¬å­¦ä¹ èƒ½è¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œåªéœ€æœ‰é™æ ‡è®°èŠ‚ç‚¹ã€‚</li>
<li>ç°æœ‰å›¾å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•å­˜åœ¨ä¾èµ–é¢„è®¾å›¾æ»¤æ³¢å™¨çš„å±€é™ï¼Œå¿½è§†çœŸå®å›¾ä¸­å±€éƒ¨æ‹“æ‰‘ç»“æ„çš„å¼‚è´¨æ€§ã€‚</li>
<li>å¤šæ•°æ–¹æ³•å‡è®¾æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œä½†åœ¨å°‘æ ·æœ¬æ¡ä»¶ä¸‹è¿™ä¸€å‡è®¾å¯èƒ½ä¸æˆç«‹ã€‚</li>
<li>GRACEæ˜¯ä¸€ä¸ªæ–°çš„å›¾å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œæ•´åˆè‡ªé€‚åº”é¢‘è°±ä¸“å®¶ä¸è·¨é›†åˆ†å¸ƒæ ¡å‡†æŠ€æœ¯ã€‚</li>
<li>GRACEé€šè¿‡é€‚åº”å±€éƒ¨ç»“æ„å˜åŒ–å’Œè·¨é›†åˆ†å¸ƒæ ¡å‡†æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>GRACEåœ¨å¹¿æ³›å®éªŒè®¾ç½®ä¸‹æ€§èƒ½ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</li>
<li>GRACEçš„ä»£ç å¯å…¬å¼€è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7d48f06b6c71ebe97f82c88d9c61314d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733336&auth_key=1760733336-0-0-8e2fe2618cd8bd03e55a989b9d5e400d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-448f826c3caab6d6b234767a0f166501~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733344&auth_key=1760733344-0-0-df7952d5eb725b3c1c1ebfd9d2139868&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be8d2a9e6e39492b2873df1796e47c5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733350&auth_key=1760733350-0-0-6f0e2feb08e4e1a684b9e43d984f717f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-Representation-Gaps-Be-the-Key-to-Enhancing-Robustness-in-Graph-Text-Alignment"><a href="#Can-Representation-Gaps-Be-the-Key-to-Enhancing-Robustness-in-Graph-Text-Alignment" class="headerlink" title="Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text   Alignment?"></a>Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text   Alignment?</h2><p><strong>Authors:Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Zijian Zhang, Yilei Yuan, Hao Zhang, Jin Huang</strong></p>
<p>Representation learning on text-attributed graphs (TAGs) integrates structural connectivity with rich textual semantics, enabling applications in diverse domains. Current methods largely rely on contrastive learning to maximize cross-modal similarity, assuming tighter coupling between graph and text representations improves transfer performance. However, our empirical analysis reveals that both natural gap expansion and forced gap reduction result in performance degradation by disrupting pre-trained knowledge structures and impairing generalization. This arises from the geometric incompatibility between encoders, where graph encoders capture topological patterns, while text encoders capture semantic structures. Over-alignment compresses these distinct spaces into shared subspaces, causing structure collapse that diminishes both topological reasoning and semantic understanding. We propose \textbf{LLM4GTA}, a gap-aware alignment framework that preserves representation gaps as geometric necessities for maintaining modality-specific knowledge and improving transfer performance. LLM4GTA includes an adaptive gap preservation module to prevent over-alignment by monitoring similarity evolution and an intra-modal compensation mechanism that boosts discriminative power using auxiliary classifiers in graph space. Extensive experiments show significant improvements over existing methods in zero-shot and few-shot scenarios. </p>
<blockquote>
<p>æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰ä¸Šçš„è¡¨ç¤ºå­¦ä¹ ç»“åˆäº†ç»“æ„è¿é€šæ€§å’Œä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰ï¼Œä¸ºå„ä¸ªé¢†åŸŸçš„åº”ç”¨æä¾›äº†å¯èƒ½ã€‚å½“å‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºå¯¹æ¯”å­¦ä¹ ï¼Œä»¥æœ€å¤§åŒ–è·¨æ¨¡æ€ç›¸ä¼¼æ€§ä¸ºå‡è®¾ï¼Œè®¤ä¸ºå›¾è¡¨ç¤ºå’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„ç´§å¯†è€¦åˆå¯ä»¥æé«˜è¿ç§»æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç»éªŒåˆ†æè¡¨æ˜ï¼Œæ— è®ºæ˜¯è‡ªç„¶çš„é—´éš™æ‰©å¼ è¿˜æ˜¯å¼ºåˆ¶çš„é—´éš™å‡å°‘éƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºå®ƒä»¬ç ´åäº†é¢„è®­ç»ƒçš„çŸ¥è¯†ç»“æ„å¹¶æŸå®³äº†æ³›åŒ–èƒ½åŠ›ã€‚è¿™æ˜¯å› ä¸ºç¼–ç å™¨ä¹‹é—´çš„å‡ ä½•ä¸å…¼å®¹ï¼Œå…¶ä¸­å›¾ç¼–ç å™¨æ•è·æ‹“æ‰‘æ¨¡å¼ï¼Œè€Œæ–‡æœ¬ç¼–ç å™¨æ•è·è¯­ä¹‰ç»“æ„ã€‚è¿‡åº¦å¯¹é½å°†è¿™äº›ä¸åŒçš„ç©ºé—´å‹ç¼©åˆ°å…±äº«çš„å­ç©ºé—´ä¸­ï¼Œå¯¼è‡´ç»“æ„å´©æºƒï¼Œæ—¢æŸå®³äº†æ‹“æ‰‘æ¨ç†åˆæŸå®³äº†è¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†LLM4GTAï¼Œä¸€ä¸ªé—´éš™æ„ŸçŸ¥å¯¹é½æ¡†æ¶ï¼Œå®ƒä¿ç•™äº†è¡¨ç¤ºé—´éš™ä½œä¸ºä¿æŒç‰¹å®šæ¨¡æ€çŸ¥è¯†å’Œæé«˜è¿ç§»æ€§èƒ½çš„å‡ ä½•å¿…è¦ã€‚LLM4GTAåŒ…æ‹¬è‡ªé€‚åº”é—´éš™ä¿ç•™æ¨¡å—ï¼Œé€šè¿‡ç›‘æ§ç›¸ä¼¼æ€§æ¼”å˜æ¥é˜²æ­¢è¿‡åº¦å¯¹é½ï¼Œä»¥åŠä¸€ç§å¢å¼ºé‰´åˆ«åŠ›çš„å†…éƒ¨æ¨¡æ€è¡¥å¿æœºåˆ¶ï¼Œä½¿ç”¨å›¾ç©ºé—´ä¸­çš„è¾…åŠ©åˆ†ç±»å™¨æ¥æé«˜é‰´åˆ«åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12087v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰çš„è¡¨ç¤ºå­¦ä¹ ç»“åˆäº†ç»“æ„è¿é€šæ€§å’Œä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰ï¼Œå¯åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¯¹æ¯”å­¦ä¹ æ¥æœ€å¤§åŒ–è·¨æ¨¡æ€ç›¸ä¼¼æ€§ï¼Œå‡è®¾å›¾ä¸æ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„ç´§å¯†è€¦åˆèƒ½æé«˜è¿ç§»æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„å®è¯åˆ†æå‘ç°ï¼Œè‡ªç„¶é—´éš™æ‰©å¼ å’Œå¼ºåˆ¶é—´éš™ç¼©å‡éƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºå®ƒä»¬ç ´åäº†é¢„è®­ç»ƒçš„çŸ¥è¯†ç»“æ„å¹¶å½±å“äº†æ³›åŒ–ã€‚è¿™æ˜¯å› ä¸ºç¼–ç å™¨ä¹‹é—´çš„å‡ ä½•ä¸å…¼å®¹ï¼Œå…¶ä¸­å›¾ç¼–ç å™¨æ•è·æ‹“æ‰‘æ¨¡å¼ï¼Œè€Œæ–‡æœ¬ç¼–ç å™¨æ•è·è¯­ä¹‰ç»“æ„ã€‚è¿‡åº¦å¯¹é½å°†è¿™äº›ä¸åŒçš„ç©ºé—´å‹ç¼©æˆå…±äº«å­ç©ºé—´ï¼Œå¯¼è‡´ç»“æ„å´©æºƒï¼Œæ—¢é™ä½äº†æ‹“æ‰‘æ¨ç†ä¹Ÿé™ä½äº†è¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†LLM4GTAï¼Œä¸€ä¸ªé—´éš™æ„ŸçŸ¥å¯¹é½æ¡†æ¶ï¼Œä¿æŒè¡¨ç¤ºé—´éš™ä½œä¸ºä¿æŒæ¨¡æ€ç‰¹å®šçŸ¥è¯†å’Œæé«˜è¿ç§»æ€§èƒ½çš„å‡ ä½•å¿…è¦ã€‚LLM4GTAåŒ…æ‹¬è‡ªé€‚åº”é—´éš™ä¿ç•™æ¨¡å—ï¼Œé€šè¿‡ç›‘æ§ç›¸ä¼¼æ€§æ¼”å˜æ¥é˜²æ­¢è¿‡åº¦å¯¹é½ï¼Œä»¥åŠå†…éƒ¨æ¨¡æ€è¡¥å¿æœºåˆ¶ï¼Œé€šè¿‡åˆ©ç”¨å›¾ç©ºé—´ä¸­çš„è¾…åŠ©åˆ†ç±»å™¨æ¥æé«˜é‰´åˆ«åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGï¼‰çš„è¡¨ç¤ºå­¦ä¹ ç»“åˆäº†ç»“æ„è¿é€šæ€§å’Œä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¯¹æ¯”å­¦ä¹ å¹¶å‡è®¾ç´§å¯†è€¦åˆèƒ½æé«˜è¿ç§»æ€§èƒ½ï¼Œä½†è‡ªç„¶é—´éš™æ‰©å¼ å’Œå¼ºåˆ¶é—´éš™ç¼©å‡å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>é—´éš™æ„ŸçŸ¥å¯¹é½æ¡†æ¶ï¼ˆLLM4GTAï¼‰æ—¨åœ¨ä¿æŒè¡¨ç¤ºé—´éš™ä»¥æé«˜è¿ç§»æ€§èƒ½ã€‚</li>
<li>LLM4GTAåŒ…æ‹¬è‡ªé€‚åº”é—´éš™ä¿ç•™æ¨¡å—å’Œå†…éƒ¨æ¨¡æ€è¡¥å¿æœºåˆ¶ã€‚</li>
<li>è‡ªé€‚åº”é—´éš™ä¿ç•™æ¨¡å—é€šè¿‡ç›‘æ§ç›¸ä¼¼æ€§æ¼”å˜é˜²æ­¢è¿‡åº¦å¯¹é½ã€‚</li>
<li>å†…éƒ¨æ¨¡æ€è¡¥å¿æœºåˆ¶åˆ©ç”¨å›¾ç©ºé—´ä¸­çš„è¾…åŠ©åˆ†ç±»å™¨æé«˜é‰´åˆ«åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-857e184ac1a87a1dfa8199ab2dd1a211~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733357&auth_key=1760733357-0-0-1e2808cd8eb007a726af889ad3745957&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-abbad288137422873b750c7e3119c208~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733364&auth_key=1760733364-0-0-af8be3fca2c21dae40a3935ade9583ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0da7937bba8473a9e71c1e11b61de77~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733371&auth_key=1760733371-0-0-bce488f8917d7dafa1e43c21881dfb1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d680ba5b28efcb449f59a541b1d393e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733378&auth_key=1760733378-0-0-c899958c2c6e279447388039324c8609&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Connecting-Giants-Synergistic-Knowledge-Transfer-of-Large-Multimodal-Models-for-Few-Shot-Learning"><a href="#Connecting-Giants-Synergistic-Knowledge-Transfer-of-Large-Multimodal-Models-for-Few-Shot-Learning" class="headerlink" title="Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal   Models for Few-Shot Learning"></a>Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal   Models for Few-Shot Learning</h2><p><strong>Authors:Hao Tang, Shengfeng He, Jing Qin</strong></p>
<p>Few-shot learning (FSL) addresses the challenge of classifying novel classes with limited training samples. While some methods leverage semantic knowledge from smaller-scale models to mitigate data scarcity, these approaches often introduce noise and bias due to the dataâ€™s inherent simplicity. In this paper, we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which effectively transfers diverse and complementary knowledge from large multimodal models to empower the off-the-shelf few-shot learner. Specifically, SynTrans employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak student, distilling semantic-aligned visual knowledge via an unsupervised proxy task. Subsequently, a training-free synergistic knowledge mining module facilitates collaboration among large multimodal models to extract high-quality semantic knowledge. Building upon this, a visual-semantic bridging module enables bi-directional knowledge transfer between visual and semantic spaces, transforming explicit visual and implicit semantic knowledge into category-specific classifier weights. Finally, SynTrans introduces a visual weight generator and a semantic weight reconstructor to adaptively construct optimal multimodal FSL classifiers. Experimental results on four FSL datasets demonstrate that SynTrans, even when paired with a simple few-shot vision encoder, significantly outperforms current state-of-the-art methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨è§£å†³ç”¨æœ‰é™è®­ç»ƒæ ·æœ¬å¯¹æ–°å‹ç±»åˆ«è¿›è¡Œåˆ†ç±»çš„æŒ‘æˆ˜ã€‚è™½ç„¶ä¸€äº›æ–¹æ³•åˆ©ç”¨å°è§„æ¨¡æ¨¡å‹çš„è¯­ä¹‰çŸ¥è¯†æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•ç”±äºæ•°æ®çš„å›ºæœ‰ç®€å•æ€§è€Œç»å¸¸å¼•å…¥å™ªå£°å’Œåè§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³ååŒçŸ¥è¯†è½¬ç§»ï¼ˆSynTransï¼‰ï¼Œå®ƒå¯ä»¥ä»å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­æœ‰æ•ˆåœ°è½¬ç§»å¤šæ ·ä¸”äº’è¡¥çš„çŸ¥è¯†ï¼Œä»¥å¢å¼ºç°æœ‰çš„å°‘é‡æ ·æœ¬å­¦ä¹ è€…ã€‚å…·ä½“æ¥è¯´ï¼ŒSynTransé‡‡ç”¨CLIPä½œä¸ºç¨³å¥çš„æ•™å¸ˆï¼Œå¹¶ä½¿ç”¨å°‘é‡æ ·æœ¬è§†è§‰ç¼–ç å™¨ä½œä¸ºå¼±åŠ¿å­¦ç”Ÿï¼Œé€šè¿‡æ— ç›‘ç£çš„ä»£ç†ä»»åŠ¡è’¸é¦è¯­ä¹‰å¯¹é½çš„è§†è§‰çŸ¥è¯†ã€‚éšåï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒå³å¯ä½¿ç”¨çš„ååŒçŸ¥è¯†æŒ–æ˜æ¨¡å—æœ‰åŠ©äºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¹‹é—´çš„åˆä½œï¼Œä»¥æå–é«˜è´¨é‡è¯­ä¹‰çŸ¥è¯†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè§†è§‰è¯­ä¹‰æ¡¥æ¢æ¨¡å—å®ç°äº†è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¹‹é—´çš„åŒå‘çŸ¥è¯†è½¬ç§»ï¼Œå°†æ˜ç¡®çš„è§†è§‰çŸ¥è¯†å’Œéšå«çš„è¯­ä¹‰çŸ¥è¯†è½¬åŒ–ä¸ºç‰¹å®šçš„ç±»åˆ«åˆ†ç±»å™¨æƒé‡ã€‚æœ€åï¼ŒSynTranså¼•å…¥äº†è§†è§‰æƒé‡ç”Ÿæˆå™¨å’Œè¯­ä¹‰æƒé‡é‡å»ºå™¨ï¼Œä»¥è‡ªé€‚åº”åœ°æ„å»ºæœ€ä¼˜çš„å¤šæ¨¡æ€FSLåˆ†ç±»å™¨ã€‚åœ¨å››ä¸ªFSLæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿ä¸ç®€å•çš„å°‘é‡æ ·æœ¬è§†è§‰ç¼–ç å™¨é…å¯¹ï¼ŒSynTransä¹Ÿæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11115v1">PDF</a> Accepted by IJCAI 2025</p>
<p><strong>Summary</strong><br>å°‘é‡æ ·æœ¬å­¦ä¹ é¢ä¸´æ ·æœ¬ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæœ¬è®ºæ–‡æå‡ºä¸€ç§åä¸ºSynTransçš„æ–°æ¡†æ¶ï¼Œå®ƒæœ‰æ•ˆåœ°ä»å¤§æ¨¡å‹ä¸­è½¬ç§»å¤šæ ·ä¸”äº’è¡¥çš„çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚SynTransåˆ©ç”¨CLIPä½œä¸ºå¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œä½¿ç”¨å°‘é‡æ ·æœ¬çš„è§†è§‰ç¼–ç å™¨ä½œä¸ºå¼±å­¦ç”Ÿæ¨¡å‹ï¼Œé€šè¿‡æ— ç›‘ç£ä»£ç†ä»»åŠ¡è’¸é¦è¯­ä¹‰å¯¹é½çš„è§†è§‰çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†ä¸€ä¸ªè®­ç»ƒååŒçŸ¥è¯†æŒ–æ˜æ¨¡å—æ¥æå–é«˜è´¨é‡è¯­ä¹‰çŸ¥è¯†ï¼Œå¹¶åœ¨è§†è§‰è¯­ä¹‰æ¡¥æ¢æ¨¡å—çš„å¸®åŠ©ä¸‹å®ç°åŒå‘çŸ¥è¯†è½¬ç§»ã€‚æœ€åï¼Œå®éªŒè¯æ˜SynTranså³ä½¿åœ¨æ­é…ç®€å•çš„å°‘é‡æ ·æœ¬è§†è§‰ç¼–ç å™¨æ—¶ä¹Ÿèƒ½æ˜¾è‘—è¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot learning (FSL)é¢ä¸´æ ·æœ¬ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSynTransçš„æ–°æ¡†æ¶ï¼Œç”¨äºè§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>SynTransåˆ©ç”¨CLIPæ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨å¼±å­¦ç”Ÿæ¨¡å‹æ¥å¤„ç†æ ·æœ¬æ•°æ®ï¼Œé¿å…å™ªå£°å’Œåè§é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ— ç›‘ç£ä»£ç†ä»»åŠ¡å®ç°çŸ¥è¯†çš„è½¬ç§»å’Œè’¸é¦ã€‚</li>
<li>è®­ç»ƒååŒçŸ¥è¯†æŒ–æ˜æ¨¡å—å¸®åŠ©æå–é«˜è´¨é‡è¯­ä¹‰çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b83ab97c76f71a44f607a222d9d9cf5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733386&auth_key=1760733386-0-0-635ba3e88fbc0f9272f8359371804c6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea86e8dbd4d11448e0c90a466bce6f30~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733393&auth_key=1760733393-0-0-8a98f66d49aa1c6d69d010e92704c39b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89c9127a945c12265dd15ce9240678c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733400&auth_key=1760733400-0-0-5c93110cede06c88e4d8e53d84df8d98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ea2b7948d507d2a3f8a9514f3be86a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733406&auth_key=1760733406-0-0-c1df8e09920c2d8a6f5769cb5b41f9c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c2756ffadf19593a391ab63bd562eec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733414&auth_key=1760733414-0-0-d35ffd21de21c8bdcb3cd6351df047d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FusionGen-Feature-Fusion-Based-Few-Shot-EEG-Data-Generation"><a href="#FusionGen-Feature-Fusion-Based-Few-Shot-EEG-Data-Generation" class="headerlink" title="FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation"></a>FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation</h2><p><strong>Authors:Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu</strong></p>
<p>Brain-computer interfaces (BCIs) provide potential for applications ranging from medical rehabilitation to cognitive state assessment by establishing direct communication pathways between the brain and external devices via electroencephalography (EEG). However, EEG-based BCIs are severely constrained by data scarcity and significant inter-subject variability, which hinder the generalization and applicability of EEG decoding models in practical settings. To address these challenges, we propose FusionGen, a novel EEG data generation framework based on disentangled representation learning and feature fusion. By integrating features across trials through a feature matching fusion module and combining them with a lightweight feature extraction and reconstruction pipeline, FusionGen ensures both data diversity and trainability under limited data constraints. Extensive experiments on multiple publicly available EEG datasets demonstrate that FusionGen significantly outperforms existing augmentation techniques, yielding notable improvements in classification accuracy. </p>
<blockquote>
<p>è„‘æœºæ¥å£ï¼ˆBCIsï¼‰é€šè¿‡è„‘ç”µå›¾ï¼ˆEEGï¼‰å»ºç«‹å¤§è„‘ä¸å¤–éƒ¨è®¾å¤‡ä¹‹é—´çš„ç›´æ¥é€šä¿¡è·¯å¾„ï¼Œåœ¨åŒ»ç–—åº·å¤åˆ°è®¤çŸ¥çŠ¶æ€è¯„ä¼°ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŸºäºEEGçš„BCIså—åˆ°æ•°æ®ç¨€ç¼ºå’Œæ˜¾è‘—ä¸ªä½“å·®å¼‚çš„ä¸¥é‡åˆ¶çº¦ï¼Œè¿™é˜»ç¢äº†EEGè§£ç æ¨¡å‹åœ¨å®é™…ç¯å¢ƒä¸­çš„é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FusionGenï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§£çº ç¼ è¡¨ç¤ºå­¦ä¹ å’Œç‰¹å¾èåˆçš„æ–°å‹EEGæ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚å®ƒé€šè¿‡ç‰¹å¾åŒ¹é…èåˆæ¨¡å—æ•´åˆè¯•éªŒé—´çš„ç‰¹å¾ï¼Œå¹¶ç»“åˆè½»é‡çº§ç‰¹å¾æå–å’Œé‡å»ºç®¡é“ï¼Œç¡®ä¿åœ¨æœ‰é™çš„æ•°æ®çº¦æŸä¸‹å®ç°æ•°æ®å¤šæ ·æ€§å’Œå¯è®­ç»ƒæ€§ã€‚åœ¨å¤šä¸ªå…¬å¼€å¯ç”¨çš„EEGæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFusionGenæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¢å¼ºæŠ€æœ¯ï¼Œåœ¨åˆ†ç±»ç²¾åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10604v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è„‘æœºæ¥å£ï¼ˆBCIsï¼‰é€šè¿‡è„‘ç”µå›¾ï¼ˆEEGï¼‰å»ºç«‹å¤§è„‘å’Œå¤–éƒ¨è®¾å¤‡ä¹‹é—´çš„ç›´æ¥é€šä¿¡è·¯å¾„ï¼Œåœ¨åŒ»ç–—åº·å¤å’Œè®¤çŸ¥çŠ¶æ€è¯„ä¼°ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼ŒEEG-based BCIsé¢ä¸´æ•°æ®ç¨€ç¼ºå’Œä¸ªä½“é—´æ˜¾è‘—å·®å¼‚çš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†EEGè§£ç æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºè§£çº ç¼ è¡¨ç¤ºå­¦ä¹ å’Œç‰¹å¾èåˆçš„EEGæ•°æ®ç”Ÿæˆæ¡†æ¶FusionGenï¼Œé€šè¿‡è·¨è¯•éªŒç‰¹å¾èåˆæ¨¡å—å’Œè½»é‡çº§ç‰¹å¾æå–ä¸é‡å»ºç®¡é“ï¼Œç¡®ä¿åœ¨æœ‰é™æ•°æ®çº¦æŸä¸‹çš„æ•°æ®å¤šæ ·æ€§å’Œå¯è®­ç»ƒæ€§ã€‚åœ¨å¤šä¸ªå…¬å¼€EEGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFusionGenæ˜¾è‘—ä¼˜äºç°æœ‰å¢å¼ºæŠ€æœ¯ï¼Œåˆ†ç±»ç²¾åº¦å¾—åˆ°æ˜æ˜¾æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘æœºæ¥å£ï¼ˆBCIsï¼‰é€šè¿‡è„‘ç”µå›¾ï¼ˆEEGï¼‰å»ºç«‹å¤§è„‘ä¸å¤–éƒ¨è®¾å¤‡çš„ç›´æ¥é€šä¿¡ã€‚</li>
<li>EEG-based BCIsé¢ä¸´æ•°æ®ç¨€ç¼ºå’Œä¸ªä½“é—´å·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>FusionGenæ˜¯ä¸€ç§åŸºäºè§£çº ç¼ è¡¨ç¤ºå­¦ä¹ å’Œç‰¹å¾èåˆçš„EEGæ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>FusionGené€šè¿‡è·¨è¯•éªŒç‰¹å¾èåˆæ¨¡å—ç¡®ä¿æ•°æ®å¤šæ ·æ€§å’Œå¯è®­ç»ƒæ€§ã€‚</li>
<li>FusionGenåœ¨æœ‰é™æ•°æ®çº¦æŸä¸‹è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>FusionGenåœ¨å¤šä¸ªå…¬å¼€EEGæ•°æ®é›†ä¸Šçš„å®éªŒåˆ†ç±»ç²¾åº¦æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-11b8c43f63633d62dd0a7a042247a469~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733421&auth_key=1760733421-0-0-70a89c356721b3055959427a68391440&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d88b0122c9fb9d1dcc9776fb13dd5048~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733428&auth_key=1760733428-0-0-f2265a4b4a7f3f8524dd64730e389f81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3f79c6985139efbd70385d87475f9ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733435&auth_key=1760733435-0-0-a7a92cc6da3a9d8c93e26d366061e4ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18287ebc2dfb4ec1ba514e7c4581db6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733442&auth_key=1760733442-0-0-a7bdd27185e6f3b4dce8c107b589c235&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa6f97fdf6c6a175d71e9256a1c29f0f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733452&auth_key=1760733452-0-0-4ed8b4967b3446ea5f29ce1fcc48a42d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6e2ca348618b71dd0a97e7d73a91759~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733458&auth_key=1760733458-0-0-3d416212944b7b98cc2f9c95b0946223&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Opacity-Gradient-Driven-Density-Control-for-Compact-and-Efficient-Few-Shot-3D-Gaussian-Splatting"><a href="#Opacity-Gradient-Driven-Density-Control-for-Compact-and-Efficient-Few-Shot-3D-Gaussian-Splatting" class="headerlink" title="Opacity-Gradient Driven Density Control for Compact and Efficient   Few-Shot 3D Gaussian Splatting"></a>Opacity-Gradient Driven Density Control for Compact and Efficient   Few-Shot 3D Gaussian Splatting</h2><p><strong>Authors:Abdelrhman Elrawy, Emad A. Mohammed</strong></p>
<p>3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its standard adaptive density control (ADC) can lead to overfitting and bloated reconstructions. While state-of-the-art methods like FSGS improve quality, they often do so by significantly increasing the primitive count. This paper presents a framework that revises the core 3DGS optimization to prioritize efficiency. We replace the standard positional gradient heuristic with a novel densification trigger that uses the opacity gradient as a lightweight proxy for rendering error. We find this aggressive densification is only effective when paired with a more conservative pruning schedule, which prevents destructive optimization cycles. Combined with a standard depth-correlation loss for geometric guidance, our framework demonstrates a fundamental improvement in efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a reduction of approximately 70%. This dramatic gain in compactness is achieved with a modest trade-off in reconstruction metrics, establishing a new state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view synthesis. </p>
<blockquote>
<p>3Dé«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶æ ‡å‡†çš„è‡ªé€‚åº”å¯†åº¦æ§åˆ¶ï¼ˆADCï¼‰å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œå†—ä½™çš„é‡å»ºã€‚è™½ç„¶æœ€æ–°çš„æ–¹æ³•å¦‚FSGSæé«˜äº†è´¨é‡ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯é€šè¿‡å¤§å¹…å¢åŠ åŸºæœ¬å•ä½æ•°é‡æ¥å®ç°çš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¿®è®¢çš„3DGSä¼˜åŒ–æ¡†æ¶ï¼Œä»¥æ•ˆç‡ä¸ºä¼˜å…ˆã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªæ–°é¢–çš„è‡´å¯†åŒ–è§¦å‘å™¨æ›¿æ¢æ ‡å‡†çš„ä½ç½®æ¢¯åº¦å¯å‘å¼æ–¹æ³•ï¼Œä½¿ç”¨é€æ˜åº¦æ¢¯åº¦ä½œä¸ºæ¸²æŸ“é”™è¯¯çš„è½»é‡çº§ä»£ç†ã€‚æˆ‘ä»¬å‘ç°è¿™ç§ç§¯æçš„è‡´å¯†åŒ–åªæœ‰åœ¨ä¸æ›´ä¿å®ˆçš„ä¿®å‰ªè®¡åˆ’ç›¸ç»“åˆæ—¶æ‰æœ‰æ•ˆï¼Œå¯ä»¥é˜²æ­¢ç ´åæ€§çš„ä¼˜åŒ–å¾ªç¯ã€‚ç»“åˆç”¨äºå‡ ä½•å¼•å¯¼çš„æ·±åº¦ç›¸å…³æ€§æŸå¤±æ ‡å‡†ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ•ˆç‡ä¸Šå®ç°äº†æ ¹æœ¬æ€§çš„æ”¹è¿›ã€‚åœ¨LLFFä¸‰è§†å›¾æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ¯”FSGSæ›´ç´§å‡‘ï¼ˆä½¿ç”¨32kä¸ªåŸºæœ¬å•ä½å¯¹æ¯”FSGSçš„57kä¸ªåŸºæœ¬å•ä½ï¼‰ï¼Œè¾¾åˆ°äº†è¶…è¿‡40%çš„ç´§å‡‘åº¦æå‡ï¼›åœ¨Mip-NeRF 360æ•°æ®é›†ä¸Šï¼Œå®ç°äº†å¤§çº¦70%çš„å‡å°‘ã€‚è¿™ç§æ˜¾è‘—çš„ç´§å‡‘æ€§æå‡æ˜¯åœ¨é€‚åº¦ç‰ºç‰²é‡å»ºæŒ‡æ ‡çš„æƒ…å†µä¸‹å®ç°çš„ï¼Œåœ¨å°‘æ ·æœ¬è§†å›¾åˆæˆçš„è´¨é‡ä¸æ•ˆç‡å¸•ç´¯æ‰˜å‰æ²¿ä¸Šå»ºç«‹äº†æ–°çš„æŠ€æœ¯é¢†å…ˆåœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10257v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´é«˜æ–¯ç‚¹äº‘æ¨¡å‹ï¼ˆ3DGSï¼‰çš„ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºè§£å†³å…¶åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹çš„ä¸è¶³ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„å¯†åº¦æ§åˆ¶ç­–ç•¥ï¼Œåˆ©ç”¨é€æ˜åº¦æ¢¯åº¦ä½œä¸ºæ¸²æŸ“è¯¯å·®çš„è½»é‡çº§ä»£ç†ï¼Œä¼˜åŒ–äº†æ¨¡å‹åœ¨æ¸²æŸ“æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡å®ç°æ›´ä¸ºé«˜æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ä¸å…ˆè¿›çš„ä¿çœŸåº¦æ–¹æ³•ç»“åˆï¼Œæœ€ç»ˆä½¿æ¨¡å‹èƒ½åœ¨ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡ä¸Šå–å¾—äº†ä¼˜ç§€çš„æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹ä¸‰ç»´é«˜æ–¯ç‚¹äº‘æ¨¡å‹ï¼ˆ3DGSï¼‰åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ¡†æ¶ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨é€æ˜åº¦æ¢¯åº¦ä½œä¸ºè½»é‡çº§ä»£ç†æ¥è¡¡é‡æ¸²æŸ“è¯¯å·®ï¼Œä¼˜åŒ–æ¨¡å‹å¯†åº¦æ§åˆ¶ã€‚</li>
<li>æå‡ºä¸€ç§æ›´ä¿å®ˆçš„ä¿®å‰ªç­–ç•¥ï¼Œé¿å…ç ´åæ€§çš„ä¼˜åŒ–å¾ªç¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0e366d94e5781744933f3a35d166f777~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733466&auth_key=1760733466-0-0-dff6a0286573d5a827cd12b16fc68ccb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3de979f72741bec03be9742cc907899~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733473&auth_key=1760733473-0-0-e396a28487ae4b741138c9036580b4a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-778be6695db3f6b4e784a4076b6704aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733501&auth_key=1760733501-0-0-be274cc4750c31e117c4fad34ea8b157&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3fe79b503a97c128b08d3689bb9dff04~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733507&auth_key=1760733507-0-0-081b30e1709ccc2faabdd5c84dbac39f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ef883227a450bd7fcd84c3e862bc10c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733514&auth_key=1760733514-0-0-916741bc9f5c45905ce4808ae758ed18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Preference-driven-Knowledge-Distillation-for-Few-shot-Node-Classification"><a href="#Preference-driven-Knowledge-Distillation-for-Few-shot-Node-Classification" class="headerlink" title="Preference-driven Knowledge Distillation for Few-shot Node   Classification"></a>Preference-driven Knowledge Distillation for Few-shot Node   Classification</h2><p><strong>Authors:Xing Wei, Chunchun Chen, Rui Fan, Xiaofeng Cao, Sourav Medya, Wei Ye</strong></p>
<p>Graph neural networks (GNNs) can efficiently process text-attributed graphs (TAGs) due to their message-passing mechanisms, but their training heavily relies on the human-annotated labels. Moreover, the complex and diverse local topologies of nodes of real-world TAGs make it challenging for a single mechanism to handle. Large language models (LLMs) perform well in zero-&#x2F;few-shot learning on TAGs but suffer from a scalability challenge. Therefore, we propose a preference-driven knowledge distillation (PKD) framework to synergize the complementary strengths of LLMs and various GNNs for few-shot node classification. Specifically, we develop a GNN-preference-driven node selector that effectively promotes prediction distillation from LLMs to teacher GNNs. To further tackle nodesâ€™ intricate local topologies, we develop a node-preference-driven GNN selector that identifies the most suitable teacher GNN for each node, thereby facilitating tailored knowledge distillation from teacher GNNs to the student GNN. Extensive experiments validate the efficacy of our proposed framework in few-shot node classification on real-world TAGs. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ç”±äºå…¶æ¶ˆæ¯ä¼ é€’æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬çš„è®­ç»ƒä¸¥é‡ä¾èµ–äºäººå·¥æ ‡æ³¨çš„æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œç°å®ä¸–ç•Œä¸­çš„TAGèŠ‚ç‚¹çš„å¤æ‚å’Œå¤šæ ·åŒ–çš„å±€éƒ¨æ‹“æ‰‘ç»“æ„ä½¿å¾—å•ä¸€æœºåˆ¶å¤„ç†èµ·æ¥å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨TAGçš„é›¶&#x2F;å°‘é•œå¤´å­¦ä¹ ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåå¥½é©±åŠ¨çš„çŸ¥è¯†è’¸é¦ï¼ˆPKDï¼‰æ¡†æ¶ï¼Œä»¥ååŒLLMså’Œå„ç§GNNsçš„äº’è¡¥ä¼˜åŠ¿ï¼Œç”¨äºå°‘é•œå¤´èŠ‚ç‚¹åˆ†ç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªGNNåå¥½é©±åŠ¨èŠ‚ç‚¹é€‰æ‹©å™¨ï¼Œæœ‰æ•ˆåœ°ä¿ƒè¿›äº†ä»LLMsåˆ°æ•™å¸ˆGNNsçš„é¢„æµ‹è’¸é¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³èŠ‚ç‚¹çš„å¤æ‚å±€éƒ¨æ‹“æ‰‘é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªèŠ‚ç‚¹åå¥½é©±åŠ¨çš„GNNé€‰æ‹©å™¨ï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹ç¡®å®šæœ€åˆé€‚çš„æ•™å¸ˆGNNï¼Œä»è€Œä¿ƒè¿›ä»æ•™å¸ˆGNNsåˆ°å­¦ç”ŸGNNçš„çŸ¥è¯†è’¸é¦ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨ç°å®ä¸–ç•Œçš„TAGå°‘é•œå¤´èŠ‚ç‚¹åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10116v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åå¥½é©±åŠ¨çš„çŸ¥è¯†è’¸é¦ï¼ˆPKDï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„ä¼˜åŠ¿ï¼Œç”¨äºå°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ã€‚é€šè¿‡GNNåå¥½é©±åŠ¨èŠ‚ç‚¹é€‰æ‹©å™¨å’ŒèŠ‚ç‚¹åå¥½é©±åŠ¨GNNé€‰æ‹©å™¨ï¼Œä¿ƒè¿›é¢„æµ‹ä»LLMsåˆ°æ•™å¸ˆGNNsçš„è’¸é¦ï¼Œå¹¶è§£å†³äº†èŠ‚ç‚¹å¤æ‚å±€éƒ¨æ‹“æ‰‘çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNsèƒ½é«˜æ•ˆå¤„ç†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ï¼Œä½†å…¶è®­ç»ƒä¾èµ–äºäººå·¥æ ‡æ³¨çš„æ ‡ç­¾ã€‚</li>
<li>çœŸå®ä¸–ç•Œçš„TAGsèŠ‚ç‚¹çš„å¤æ‚å’Œå¤šæ ·å±€éƒ¨æ‹“æ‰‘ç»™å•ä¸€æœºåˆ¶å¤„ç†å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ å¯¹TAGsè¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåå¥½é©±åŠ¨çš„çŸ¥è¯†è’¸é¦ï¼ˆPKDï¼‰æ¡†æ¶ï¼Œç»“åˆLLMså’ŒGNNsçš„ä¼˜åŠ¿è¿›è¡Œå°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ã€‚</li>
<li>å¼€å‘GNNåå¥½é©±åŠ¨èŠ‚ç‚¹é€‰æ‹©å™¨ï¼Œæœ‰æ•ˆä¿ƒè¿›ä»LLMsåˆ°æ•™å¸ˆGNNsçš„é¢„æµ‹è’¸é¦ã€‚</li>
<li>ä¸ºåº”å¯¹èŠ‚ç‚¹çš„å¤æ‚å±€éƒ¨æ‹“æ‰‘ï¼Œå¼€å‘èŠ‚ç‚¹åå¥½é©±åŠ¨GNNé€‰æ‹©å™¨ï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹è¯†åˆ«æœ€åˆé€‚çš„æ•™å¸ˆGNNã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-30bfdd2bb8fe6aab1dc2cc3e97fbf484~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733522&auth_key=1760733522-0-0-95c41edebbc4a5d843081a841d647026&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7363b98a1b663253003ca807bab351b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733529&auth_key=1760733529-0-0-be917b7a5cc6c673aa60b84ee1d03121&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a793becd23a08c3e68f148f2dd9d7062~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733536&auth_key=1760733536-0-0-eb516b3e6eb1193f9e6902d1ebad19be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-873be2c212951f2b7c4989eca781bbb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733576&auth_key=1760733576-0-0-1bc74db1e747b3f9ca93e95b7a4326b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="FSP-DETR-Few-Shot-Prototypical-Parasitic-Ova-Detection"><a href="#FSP-DETR-Few-Shot-Prototypical-Parasitic-Ova-Detection" class="headerlink" title="FSP-DETR: Few-Shot Prototypical Parasitic Ova Detection"></a>FSP-DETR: Few-Shot Prototypical Parasitic Ova Detection</h2><p><strong>Authors:Shubham Trehan, Udhav Ramachandran, Akash Rao, Ruth Scimeca, Sathyanarayanan N. Aakur</strong></p>
<p>Object detection in biomedical settings is fundamentally constrained by the scarcity of labeled data and the frequent emergence of novel or rare categories. We present FSP-DETR, a unified detection framework that enables robust few-shot detection, open-set recognition, and generalization to unseen biomedical tasks within a single model. Built upon a class-agnostic DETR backbone, our approach constructs class prototypes from original support images and learns an embedding space using augmented views and a lightweight transformer decoder. Training jointly optimizes a prototype matching loss, an alignment-based separation loss, and a KL divergence regularization to improve discriminative feature learning and calibration under scarce supervision. Unlike prior work that tackles these tasks in isolation, FSP-DETR enables inference-time flexibility to support unseen class recognition, background rejection, and cross-task adaptation without retraining. We also introduce a new ova species detection benchmark with 20 parasite classes and establish standardized evaluation protocols. Extensive experiments across ova, blood cell, and malaria detection tasks demonstrate that FSP-DETR significantly outperforms prior few-shot and prototype-based detectors, especially in low-shot and open-set scenarios. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒä¸­çš„ç›®æ ‡æ£€æµ‹å—åˆ°æ ‡ç­¾æ•°æ®ç¨€ç¼ºå’Œæ–°å‹æˆ–ç½•è§ç±»åˆ«é¢‘ç¹å‡ºç°ç­‰æ ¹æœ¬æ€§çº¦æŸã€‚æˆ‘ä»¬æå‡ºäº†FSP-DETRï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ£€æµ‹æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªæ¨¡å‹å†…å®ç°ç¨³å¥çš„å°‘é‡æ ·æœ¬æ£€æµ‹ã€å¼€æ”¾é›†è¯†åˆ«å’Œæœªè§è¿‡ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡çš„æ³›åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨ç±»æ— å…³çš„DETRä¸»å¹²ç½‘ç»œä¸Šï¼Œé€šè¿‡æ„å»ºæ¥è‡ªåŸå§‹æ”¯æŒå›¾åƒçš„ç±»åŸå‹å¹¶ä½¿ç”¨å¢å¼ºè§†å›¾å’Œè½»é‡çº§è½¬æ¢å™¨è§£ç å™¨æ¥å­¦ä¹ åµŒå…¥ç©ºé—´ã€‚è®­ç»ƒé€šè¿‡è”åˆä¼˜åŒ–åŸå‹åŒ¹é…æŸå¤±ã€åŸºäºå¯¹é½çš„åˆ†ç¦»æŸå¤±å’ŒKLæ•£åº¦æ­£åˆ™åŒ–ï¼Œä»¥æé«˜åˆ¤åˆ«ç‰¹å¾çš„å­¦ä¹ å’Œç¨€ç¼ºç›‘ç£ä¸‹çš„æ ¡å‡†ã€‚ä¸ä»¥å‰åˆ†åˆ«å¤„ç†è¿™äº›ä»»åŠ¡çš„å·¥ä½œä¸åŒï¼ŒFSP-DETRæ”¯æŒæœªè§ç±»åˆ«è¯†åˆ«ã€èƒŒæ™¯æ’æ–¥å’Œè·¨ä»»åŠ¡é€‚åº”çš„æ¨ç†æ—¶é—´çµæ´»æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åµç‰©ç§æ£€æµ‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬20ä¸ªå¯„ç”Ÿè™«ç±»åˆ«ï¼Œå¹¶å»ºç«‹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€‚åœ¨åµã€è¡€ç»†èƒå’Œç–Ÿç–¾æ£€æµ‹ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFSP-DETRåœ¨å°‘é‡æ ·æœ¬å’Œå¼€æ”¾é›†åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºå…ˆå‰çš„å°‘é‡æ ·æœ¬å’ŒåŸºäºåŸå‹çš„æ£€æµ‹å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09583v1">PDF</a> 10 pages, 3 Figures, 5 Tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>FSP-DETRæ˜¯ä¸€ä¸ªç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹çš„æ¡†æ¶ï¼Œæ”¯æŒå°‘æ ·æœ¬æ£€æµ‹ã€å¼€æ”¾é›†è¯†åˆ«å’Œæœªè§è¿‡ä»»åŠ¡çš„ä¸€èˆ¬åŒ–ã€‚è¯¥æ¡†æ¶åŸºäºDETRæ„å»ºï¼Œé€šè¿‡æ”¯æŒå›¾åƒæ„å»ºç±»åŸå‹ï¼Œå¹¶ä½¿ç”¨å¢å¼ºè§†å›¾å’Œè½»é‡çº§è½¬æ¢å™¨è§£ç å™¨å­¦ä¹ åµŒå…¥ç©ºé—´ã€‚é€šè¿‡è”åˆä¼˜åŒ–åŸå‹åŒ¹é…æŸå¤±ã€åŸºäºå¯¹é½çš„åˆ†ç¦»æŸå¤±å’ŒKLæ•£åº¦æ­£åˆ™åŒ–ï¼Œæé«˜åˆ¤åˆ«ç‰¹å¾å­¦ä¹ å’Œç¨€ç¼ºç›‘ç£ä¸‹çš„æ ¡å‡†ã€‚FSP-DETRæ”¯æŒæœªè§ç±»è¯†åˆ«ã€èƒŒæ™¯æ‹’ç»å’Œè·¨ä»»åŠ¡é€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨åµã€è¡€ç»†èƒåŠç–Ÿç–¾æ£€æµ‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFSP-DETRæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å°‘æ ·æœ¬å’ŒåŸºäºåŸå‹çš„æ£€æµ‹å™¨ï¼Œå°¤å…¶åœ¨ä½æ ·æœ¬å’Œå¼€æ”¾é›†åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSP-DETRæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ£€æµ‹æ¡†æ¶ï¼Œæ”¯æŒå°‘æ ·æœ¬æ£€æµ‹ã€å¼€æ”¾é›†è¯†åˆ«å’Œæœªè§è¿‡ä»»åŠ¡çš„ä¸€èˆ¬åŒ–ã€‚</li>
<li>åŸºäºDETRæ„å»ºï¼Œåˆ©ç”¨æ”¯æŒå›¾åƒæ„å»ºç±»åŸå‹å¹¶å­¦ä¹ åµŒå…¥ç©ºé—´ã€‚</li>
<li>é€šè¿‡åŸå‹åŒ¹é…æŸå¤±ã€åŸºäºå¯¹é½çš„åˆ†ç¦»æŸå¤±å’ŒKLæ•£åº¦æ­£åˆ™åŒ–çš„è”åˆä¼˜åŒ–ï¼Œæé«˜åˆ¤åˆ«ç‰¹å¾å­¦ä¹ å’Œæ ¡å‡†ã€‚</li>
<li>ä¸ä¹‹å‰çš„ç ”ç©¶ä¸åŒï¼ŒFSP-DETRå¯ä»¥åœ¨æ¨ç†æ—¶æ”¯æŒæœªè§ç±»è¯†åˆ«ã€èƒŒæ™¯æ‹’ç»å’Œè·¨ä»»åŠ¡é€‚åº”ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åµç‰©ç§æ£€æµ‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬20ä¸ªå¯„ç”Ÿè™«ç±»åˆ«ï¼Œå¹¶å»ºç«‹æ ‡å‡†åŒ–è¯„ä¼°åè®®ã€‚</li>
<li>åœ¨å¤šä¸ªæ£€æµ‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFSP-DETRæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ ·æœ¬å’Œå¼€æ”¾é›†åœºæ™¯ä¸­ã€‚</li>
<li>FSP-DETRæ¡†æ¶å¯¹äºè§£å†³ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹é—®é¢˜å…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8b510e0ad28b466e6d3fda5ad072219c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733584&auth_key=1760733584-0-0-324aad4c2f17f5a07aee9bbd57180a44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-724dd97036674a72c112b7b9949f1ba4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733591&auth_key=1760733591-0-0-ab70f33f2f2d15bcfa6166a4529e5f25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17d94c80752b408325c6e2ac23a3a3c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733598&auth_key=1760733598-0-0-1d5bab5a0fd616ad4de67f307d8bbffa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0235eb4e4a43d8322851dc47e29ca73d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733606&auth_key=1760733606-0-0-592054bb2585dbf7dbe55190c8b63ce6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Higher-order-interactions-of-multi-layer-prompt"><a href="#Higher-order-interactions-of-multi-layer-prompt" class="headerlink" title="Higher-order interactions of multi-layer prompt"></a>Higher-order interactions of multi-layer prompt</h2><p><strong>Authors:Ziyu Zheng, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyan Huang, Weigang Lu</strong></p>
<p>The â€œpre-train, promptâ€ paradigm has successfully evolved in representation learning. While current prompt-tuning methods often introduce learnable prompts, they predominantly treat prompts as isolated, independent components across different network layers. This overlooks the complex and synergistic higher-order interactions that exist between prompts at various hierarchical depths, consequently limiting the expressive power and semantic richness of the prompted model. To address this fundamental gap, we propose a novel framework that explicitly models the Higher-order Interactions of Multi-layer Prompt. Our approach conceptualizes prompts from different layers not as separate entities, but as a cohesive system where their inter-relationships are critical. We design an innovative interaction module that captures these sophisticated, non-linear correlations among multi-layer prompts, effectively modeling their cooperative effects. This allows the model to dynamically aggregate and refine prompt information across the networkâ€™s depth, leading to a more integrated and powerful prompting strategy. Extensive experiments on eight benchmark datasets demonstrate that our method, by leveraging these higher-order interactions, consistently surpasses state-of-the-art prompt-tuning baselines. The performance advantage is particularly pronounced in few-shot scenarios, validating that capturing the intricate interplay between multi-layer prompts is key to unlocking more robust and generalizable representation learning. </p>
<blockquote>
<p>â€œé¢„è®­ç»ƒæç¤ºâ€èŒƒå¼åœ¨è¡¨å¾å­¦ä¹ ä¸­å·²ç»æˆåŠŸå‘å±•ã€‚è™½ç„¶å½“å‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ç»å¸¸å¼•å…¥å¯å­¦ä¹ çš„æç¤ºï¼Œä½†å®ƒä»¬ä¸»è¦å°†æç¤ºè§†ä¸ºä¸åŒç½‘ç»œå±‚ä¸­å­¤ç«‹çš„ç‹¬ç«‹ç»„ä»¶ã€‚è¿™å¿½è§†äº†æç¤ºä¹‹é—´åœ¨å„ç§å±‚æ¬¡æ·±åº¦ä¸Šå­˜åœ¨çš„å¤æ‚ä¸”ååŒçš„æ›´é«˜é˜¶äº¤äº’ï¼Œä»è€Œé™åˆ¶äº†æç¤ºæ¨¡å‹çš„è¡¨ç°åŠ›å’Œè¯­ä¹‰ä¸°å¯Œæ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€åŸºæœ¬å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ˜ç¡®å»ºæ¨¡å¤šå±‚æç¤ºé«˜é˜¶äº¤äº’çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»ä¸åŒå±‚é¢æ¦‚å¿µåŒ–æç¤ºï¼Œå¹¶ä¸å°†å…¶è§†ä¸ºç‹¬ç«‹å®ä½“ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªç´§å¯†è”ç³»çš„ç³»ç»Ÿï¼Œå…¶ç›¸äº’å…³ç³»è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåˆ›æ–°æ€§çš„äº¤äº’æ¨¡å—ï¼Œå¯ä»¥æ•æ‰å¤šå±‚æç¤ºä¹‹é—´å¤æ‚ã€éçº¿æ€§çš„å…³è”ï¼Œæœ‰æ•ˆåœ°å»ºæ¨¡å®ƒä»¬çš„ååŒä½œç”¨ã€‚è¿™å…è®¸æ¨¡å‹åŠ¨æ€åœ°èšåˆå’Œç²¾ç‚¼ç½‘ç»œæ·±åº¦ä¸­çš„æç¤ºä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´é›†æˆã€æ›´å¼ºå¤§çš„æç¤ºç­–ç•¥ã€‚åœ¨å…«ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨è¿™äº›é«˜é˜¶äº¤äº’ï¼Œå§‹ç»ˆè¶…è¶Šæœ€æ–°çš„æç¤ºè°ƒæ•´åŸºçº¿ã€‚æ€§èƒ½ä¼˜åŠ¿åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸­å°¤ä¸ºçªå‡ºï¼ŒéªŒè¯äº†æ•æ‰å¤šå±‚æç¤ºä¹‹é—´çš„å¾®å¦™ç›¸äº’ä½œç”¨æ˜¯è§£é”æ›´ç¨³å¥å’Œå¯æ³›åŒ–çš„è¡¨å¾å­¦ä¹ çš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09394v2">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†â€œé¢„è®­ç»ƒï¼Œæç¤ºâ€èŒƒå¼åœ¨è¡¨ç¤ºå­¦ä¹ ä¸­çš„æˆåŠŸåº”ç”¨ã€‚é’ˆå¯¹å½“å‰æç¤ºè°ƒæ•´æ–¹æ³•å¿½ç•¥å¤šå±‚æç¤ºä¹‹é—´å¤æ‚ååŒçš„é«˜é˜¶äº¤äº’é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œæ˜¾å¼åœ°å»ºæ¨¡å¤šå±‚æç¤ºçš„é«˜é˜¶äº¤äº’ã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªåˆ›æ–°æ€§çš„äº¤äº’æ¨¡å—ï¼Œæ•æ‰å¤šå±‚æç¤ºä¹‹é—´çš„å¤æ‚éçº¿æ€§å…³è”ï¼Œæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå®ƒä»¬çš„åˆä½œæ•ˆåº”ã€‚åœ¨å…«ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¿™äº›é«˜é˜¶äº¤äº’ï¼Œå§‹ç»ˆè¶…è¶Šæœ€å…ˆè¿›çš„æç¤ºè°ƒæ•´åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹æ€§èƒ½ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ï¼ŒéªŒè¯äº†æ•æ‰å¤šå±‚æç¤ºä¹‹é—´çš„å¾®å¦™äº’åŠ¨æ˜¯è§£é”æ›´ç¨³å¥å’Œå¯æ³›åŒ–çš„è¡¨ç¤ºå­¦ä¹ çš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æç¤ºè°ƒæ•´æ–¹æ³•ä¸»è¦å°†æç¤ºè§†ä¸ºç‹¬ç«‹ç»„ä»¶ï¼Œå¿½ç•¥äº†ä¸åŒç½‘ç»œå±‚ä¹‹é—´çš„å¤æ‚é«˜é˜¶äº¤äº’ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶æ˜¾å¼åœ°å»ºæ¨¡å¤šå±‚æç¤ºçš„é«˜é˜¶äº¤äº’ï¼Œå°†ä¸åŒå±‚çš„æç¤ºè§†ä¸ºä¸€ä¸ªç´§å¯†ç³»ç»Ÿã€‚</li>
<li>åˆ›æ–°æ€§çš„äº¤äº’æ¨¡å—æœ‰æ•ˆåœ°æ•æ‰å¤šå±‚æç¤ºä¹‹é—´çš„å¤æ‚éçº¿æ€§å…³è”ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨é«˜é˜¶äº¤äº’ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°çš„æç¤ºè°ƒæ•´æŠ€æœ¯ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚</li>
<li>æ•æ‰å¤šå±‚æç¤ºä¹‹é—´çš„å¾®å¦™äº’åŠ¨æ˜¯è§£é”æ›´ç¨³å¥å’Œå¯æ³›åŒ–çš„è¡¨ç¤ºå­¦ä¹ çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d6626e2636353bf686f37812a8ab52ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733613&auth_key=1760733613-0-0-e2d26852d8c0cb61cdd7873befd83652&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5a6b080d873aba9863bf146ad2ae156f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733620&auth_key=1760733620-0-0-525189d0505174adc1f75cd2ed86eb34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c0ff76dbf60cb311ad3870ff77a9084~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733627&auth_key=1760733627-0-0-d2cfb4343c54e3c941af32cf83021cf0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6141ff9c1250de2783af2e1e959cb4b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733633&auth_key=1760733633-0-0-68eff8e588b43ff862ab555b0cbf857b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eae5d6d373450494b6a22ae6f0daec33~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733640&auth_key=1760733640-0-0-e6ddc6b13e8c83267cf239c5d80d3ddc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Diffusion-Classifier-Synergy-Reward-Aligned-Learning-via-Mutual-Boosting-Loop-for-FSCIL"><a href="#Diffusion-Classifier-Synergy-Reward-Aligned-Learning-via-Mutual-Boosting-Loop-for-FSCIL" class="headerlink" title="Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual   Boosting Loop for FSCIL"></a>Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual   Boosting Loop for FSCIL</h2><p><strong>Authors:Ruitao Wu, Yifan Zhao, Guangyao Chen, Jia Li</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially learn new classes from minimal examples without forgetting prior knowledge, a task complicated by the stability-plasticity dilemma and data scarcity. Current FSCIL methods often struggle with generalization due to their reliance on limited datasets. While diffusion models offer a path for data augmentation, their direct application can lead to semantic misalignment or ineffective guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel framework that establishes a mutual boosting loop between diffusion model and FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a dynamic, multi-faceted reward function derived from the classifierâ€™s state directs the diffusion model. This reward system operates at two levels: the feature level ensures semantic coherence and diversity using prototype-anchored maximum mean discrepancy and dimension-wise variance matching, while the logits level promotes exploratory image generation and enhances inter-class discriminability through confidence recalibration and cross-session confusion-aware mechanisms. This co-evolutionary process, where generated images refine the classifier and an improved classifier state yields better reward signals, demonstrably achieves state-of-the-art performance on FSCIL benchmarks, significantly enhancing both knowledge retention and new class learning. </p>
<blockquote>
<p>å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æŒ‘æˆ˜æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬ä¸­æŒ‰é¡ºåºå­¦ä¹ æ–°ç±»åˆ«è€Œä¸é—å¿˜å…ˆå‰çŸ¥è¯†çš„èƒ½åŠ›ï¼Œè¿™ä¸€ä»»åŠ¡å› ç¨³å®šæ€§ä¸å¯å¡‘æ€§ä¹‹é—´çš„å†²çªå’Œæ•°æ®ç¨€ç¼ºè€Œå˜å¾—å¤æ‚ã€‚å½“å‰çš„FSCILæ–¹æ³•ç”±äºä¾èµ–æœ‰é™æ•°æ®é›†è€Œå¾€å¾€é¢ä¸´æ³›åŒ–å›°éš¾çš„é—®é¢˜ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹ä¸ºæ•°æ®å¢å¼ºæä¾›äº†é€”å¾„ï¼Œä½†å…¶ç›´æ¥åº”ç”¨å¯èƒ½å¯¼è‡´è¯­ä¹‰ä¸å¯¹é½æˆ–æŒ‡å¯¼æ— æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£åˆ†ç±»ååŒï¼ˆDCSï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ‰©æ•£æ¨¡å‹å’ŒFSCILåˆ†ç±»å™¨ä¹‹é—´å»ºç«‹äº†ç›¸äº’å¢å¼ºçš„å¾ªç¯ã€‚DCSåˆ©ç”¨å¥–åŠ±å¯¹é½å­¦ä¹ ç­–ç•¥ï¼Œå…¶ä¸­ç”±åˆ†ç±»å™¨çš„çŠ¶æ€æ´¾ç”Ÿå‡ºçš„åŠ¨æ€ã€å¤šæ–¹é¢çš„å¥–åŠ±å‡½æ•°æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚è¿™ç§å¥–åŠ±ç³»ç»Ÿåœ¨ä¸¤ä¸ªå±‚é¢ä¸Šè¿è¡Œï¼šç‰¹å¾å±‚é¢åˆ©ç”¨åŸå‹é”šå®šçš„æœ€å¤§å‡å€¼å·®å¼‚å’Œç»´åº¦æ–¹å·®åŒ¹é…ç¡®ä¿è¯­ä¹‰è¿è´¯æ€§å’Œå¤šæ ·æ€§ï¼›é€»è¾‘å±‚é¢é€šè¿‡ç½®ä¿¡åº¦å†æ ¡å‡†å’Œè·¨ä¼šè¯æ··æ·†æ„ŸçŸ¥æœºåˆ¶ä¿ƒè¿›æ¢ç´¢æ€§å›¾åƒç”Ÿæˆå¹¶å¢å¼ºç±»é—´å¯åŒºåˆ†æ€§ã€‚è¿™ç§ååŒè¿›åŒ–è¿‡ç¨‹ï¼Œå…¶ä¸­ç”Ÿæˆçš„å›¾åƒä¼˜åŒ–åˆ†ç±»å™¨ï¼Œæ”¹è¿›çš„åˆ†ç±»å™¨çŠ¶æ€äº§ç”Ÿæ›´å¥½çš„å¥–åŠ±ä¿¡å·ï¼Œåœ¨FSCILåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†çŸ¥è¯†ä¿ç•™å’Œæ–°ç±»åˆ«å­¦ä¹ çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03608v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Diffusion-Classifier Synergyï¼ˆDCSï¼‰æ¡†æ¶ï¼Œè§£å†³äº†Few-Shot Class-Incremental Learningï¼ˆFSCILï¼‰ä¸­çš„æŒ‘æˆ˜ã€‚DCSå»ºç«‹äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹å’ŒFSCILåˆ†ç±»å™¨ä¹‹é—´çš„äº’åŠ©å¾ªç¯ï¼Œåˆ©ç”¨åŸºäºåˆ†ç±»å™¨çŠ¶æ€çš„åŠ¨æ€å¤šå…ƒå¥–åŠ±å‡½æ•°å¼•å¯¼æ‰©æ•£æ¨¡å‹ã€‚DCSåœ¨ç‰¹å¾çº§åˆ«å’Œlogitsçº§åˆ«é‡‡ç”¨å¥–åŠ±ç³»ç»Ÿï¼Œç¡®ä¿è¯­ä¹‰çš„è¿è´¯æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶ä¿ƒè¿›æ¢ç´¢æ€§å›¾åƒç”Ÿæˆï¼Œå¢å¼ºç±»é—´åˆ¤åˆ«åŠ›ã€‚è¯¥ååŒè¿›åŒ–è¿‡ç¨‹å®ç°äº†çŸ¥è¯†ä¿ç•™å’Œæ–°ç±»å­¦ä¹ çš„æ˜¾è‘—å¢å¼ºï¼Œåœ¨FSCILåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion-Classifier Synergy (DCS)æ¡†æ¶è§£å†³äº†Few-Shot Class-Incremental Learning (FSCIL)çš„æŒ‘æˆ˜ã€‚</li>
<li>DCSå»ºç«‹äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹å’ŒFSCILåˆ†ç±»å™¨ä¹‹é—´çš„äº’åŠ©å¾ªç¯ã€‚</li>
<li>DCSåˆ©ç”¨åŸºäºåˆ†ç±»å™¨çŠ¶æ€çš„åŠ¨æ€å¤šå…ƒå¥–åŠ±å‡½æ•°æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>DCSåœ¨ç‰¹å¾çº§åˆ«é‡‡ç”¨å¥–åŠ±ç³»ç»Ÿï¼Œç¡®ä¿è¯­ä¹‰çš„è¿è´¯æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>DCSåœ¨logitsçº§åˆ«ä¿ƒè¿›æ¢ç´¢æ€§å›¾åƒç”Ÿæˆï¼Œå¢å¼ºç±»é—´åˆ¤åˆ«åŠ›ã€‚</li>
<li>DCSååŒè¿›åŒ–è¿‡ç¨‹å®ç°äº†çŸ¥è¯†ä¿ç•™å’Œæ–°ç±»å­¦ä¹ çš„æ˜¾è‘—å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bba60ceb00cce73819b669432a25f854~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733648&auth_key=1760733648-0-0-704f9e94453d2a6987984f01e71f93fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb2cac8e6df7743596a864d743e82a7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733655&auth_key=1760733655-0-0-a8d2069f9c67a196aa8a8e8ecae4884d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5daf4391ac828dd97053f5fd4064c56b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733662&auth_key=1760733662-0-0-2af6f2b76f0e8d54048c0c969c3042d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Geo-R1-Improving-Few-Shot-Geospatial-Referring-Expression-Understanding-with-Reinforcement-Fine-Tuning"><a href="#Geo-R1-Improving-Few-Shot-Geospatial-Referring-Expression-Understanding-with-Reinforcement-Fine-Tuning" class="headerlink" title="Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding   with Reinforcement Fine-Tuning"></a>Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding   with Reinforcement Fine-Tuning</h2><p><strong>Authors:Zilun Zhang, Zian Guan, Tiancheng Zhao, Haozhan Shen, Tianyu Li, Yuxiang Cai, Zhonggen Su, Zhaojun Liu, Jianwei Yin, Xiang Li</strong></p>
<p>Referring expression understanding in remote sensing poses unique challenges, as it requires reasoning over complex object-context relationships. While supervised fine-tuning (SFT) on multimodal large language models achieves strong performance with massive labeled datasets, they struggle in data-scarce scenarios, leading to poor generalization. To address this limitation, we propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm for few-shot geospatial referring. Geo-R1 enforces the model to first generate explicit, interpretable reasoning chains that decompose referring expressions, and then leverage these rationales to localize target objects. This â€œreason first, then actâ€ process enables the model to make more effective use of limited annotations, enhances generalization, and provides interpretability. We validate Geo-R1 on three carefully designed few-shot geospatial referring benchmarks, where our model consistently and substantially outperforms SFT baselines. It also demonstrates strong cross-dataset generalization, highlighting its robustness. Code and data will be released at: <a target="_blank" rel="noopener" href="https://github.com/Geo-R1/geo-r1">https://github.com/Geo-R1/geo-r1</a>. </p>
<blockquote>
<p>é¥æ„Ÿä¸­çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦æ¨ç†å¤æ‚çš„å¯¹è±¡ä¸Šä¸‹æ–‡å…³ç³»ã€‚è™½ç„¶åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤§é‡æ ‡è®°æ•°æ®é›†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹å´è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ³›åŒ–æ€§èƒ½å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Geo-R1ï¼Œè¿™æ˜¯ä¸€ç§ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èŒƒå¼ï¼Œç”¨äºå°‘æ•°åœ°ç†ç©ºé—´æŒ‡ä»£ã€‚Geo-R1å¼ºåˆ¶æ¨¡å‹é¦–å…ˆç”Ÿæˆæ˜ç¡®ã€å¯è§£é‡Šçš„æ¨ç†é“¾ï¼Œå¯¹æŒ‡ä»£è¡¨è¾¾å¼è¿›è¡Œåˆ†è§£ï¼Œç„¶ååˆ©ç”¨è¿™äº›ç†æ€§æ¥å®šä½ç›®æ ‡å¯¹è±¡ã€‚è¿™ç§â€œå…ˆæ¨ç†ï¼Œåè¡ŒåŠ¨â€çš„è¿‡ç¨‹ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ³¨é‡Šï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æä¾›å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç²¾å¿ƒè®¾è®¡çš„å°‘æ•°åœ°ç†ç©ºé—´æŒ‡ä»£åŸºå‡†ä¸Šå¯¹Geo-R1è¿›è¡Œäº†éªŒè¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆä¸”å¤§å¹…åº¦åœ°è¶…è¶Šäº†SFTåŸºå‡†æµ‹è¯•ã€‚å®ƒè¿˜å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥æ€§ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Geo-R1/geo-r1%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Geo-R1/geo-r1ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21976v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹æ„ŸçŸ¥ä¸­çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œéœ€è¦æ¨ç†å¤æ‚å¯¹è±¡ä¸Šä¸‹æ–‡å…³ç³»ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šåº”ç”¨æ—¶ï¼Œåœ¨å¤§é‡æ ‡è®°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸­å´è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGeo-R1ï¼Œä¸€ç§é’ˆå¯¹å°‘æ•°åœ°ç†ç©ºé—´æŒ‡ä»£çš„æ¨ç†ä¸­å¿ƒå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èŒƒå¼ã€‚Geo-R1å¼ºåˆ¶æ¨¡å‹é¦–å…ˆç”Ÿæˆæ˜ç¡®ã€å¯è§£é‡Šçš„æ¨ç†é“¾ï¼Œåˆ†è§£æŒ‡ä»£è¡¨è¾¾å¼ï¼Œç„¶ååˆ©ç”¨è¿™äº›ç†æ€§æ¥å®šä½ç›®æ ‡å¯¹è±¡ã€‚è¿™ç§â€œå…ˆæ¨ç†ï¼Œåè¡ŒåŠ¨â€çš„è¿‡ç¨‹ä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™æ³¨é‡Šï¼Œå¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç²¾å¿ƒè®¾è®¡çš„å°‘æ•°åœ°ç†ç©ºé—´æŒ‡ä»£åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†Geo-R1ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆä¸”å¤§å¹…åº¦ä¼˜äºSFTåŸºå‡†æµ‹è¯•ã€‚å®ƒè¿˜å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹æ„ŸçŸ¥ä¸­çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£éœ€è¦å¤„ç†å¤æ‚çš„å¯¹è±¡ä¸Šä¸‹æ–‡å…³ç³»ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºæ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>Geo-R1æ˜¯ä¸€ç§é’ˆå¯¹å°‘æ•°åœ°ç†ç©ºé—´æŒ‡ä»£çš„æ¨ç†ä¸­å¿ƒå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•ã€‚</li>
<li>Geo-R1é€šè¿‡ç”Ÿæˆæ˜ç¡®ã€å¯è§£é‡Šçš„æ¨ç†é“¾æ¥åˆ†è§£æŒ‡ä»£è¡¨è¾¾å¼ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>â€œå…ˆæ¨ç†ï¼Œåè¡ŒåŠ¨â€çš„è¿‡ç¨‹ä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™æ³¨é‡Šã€‚</li>
<li>Geo-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ˜¾è‘—ä¼˜äºSFTæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b469fb31430db349d67f62c7861e49fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733669&auth_key=1760733669-0-0-5d2016d4f70fdce5a272d4d7a160a1e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-994db49426bf0366cabd4d19111fa23b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733676&auth_key=1760733676-0-0-fbbbdf5093b00beef57d7b2829284681&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7821f1f8223ce5a782bbe37b2b9682c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733683&auth_key=1760733683-0-0-f6c6e18275aaff8422a14a9559b33eb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c712fe35903eb4dcf028fa8f5c1c9c4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733690&auth_key=1760733690-0-0-7ec87104bfd00e1a44784341d044f851&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c73613c190cee85accb10e9ea1075930~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733697&auth_key=1760733697-0-0-2745bf56f88ac5c60f5c02a15aecd1fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SCENEFORGE-Enhancing-3D-text-alignment-with-Structured-Scene-Compositions"><a href="#SCENEFORGE-Enhancing-3D-text-alignment-with-Structured-Scene-Compositions" class="headerlink" title="SCENEFORGE: Enhancing 3D-text alignment with Structured Scene   Compositions"></a>SCENEFORGE: Enhancing 3D-text alignment with Structured Scene   Compositions</h2><p><strong>Authors:Cristian Sbrolli, Matteo Matteucci</strong></p>
<p>The whole is greater than the sum of its parts-even in 3D-text contrastive learning. We introduce SceneForge, a novel framework that enhances contrastive alignment between 3D point clouds and text through structured multi-object scene compositions. SceneForge leverages individual 3D shapes to construct multi-object scenes with explicit spatial relations, pairing them with coherent multi-object descriptions refined by a large language model. By augmenting contrastive training with these structured, compositional samples, SceneForge effectively addresses the scarcity of large-scale 3D-text datasets, significantly enriching data complexity and diversity. We systematically investigate critical design elements, such as the optimal number of objects per scene, the proportion of compositional samples in training batches, and scene construction strategies. Extensive experiments demonstrate that SceneForge delivers substantial performance gains across multiple tasks, including zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet, as well as few-shot part segmentation on ShapeNetPart. SceneForgeâ€™s compositional augmentations are model-agnostic, consistently improving performance across multiple encoder architectures. Moreover, SceneForge improves 3D visual question answering on ScanQA, generalizes robustly to retrieval scenarios with increasing scene complexity, and showcases spatial reasoning capabilities by adapting spatial configurations to align precisely with textual instructions. </p>
<blockquote>
<p>æ•´ä½“å¤§äºéƒ¨åˆ†ä¹‹å’Œï¼Œç”šè‡³åœ¨3Dæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬æ¨å‡ºäº†SceneForgeï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çš„å¤šç›®æ ‡åœºæ™¯ç»„åˆï¼Œå¢å¼ºäº†3Dç‚¹äº‘å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹æ¯”å¯¹é½ã€‚SceneForgeåˆ©ç”¨å•ä¸ª3Då½¢çŠ¶æ„å»ºå…·æœ‰æ˜ç¡®ç©ºé—´å…³ç³»çš„å¤šç›®æ ‡åœºæ™¯ï¼Œå°†å®ƒä»¬ä¸ç”±å¤§å‹è¯­è¨€æ¨¡å‹å®Œå–„çš„ä¸€è‡´å¤šç›®æ ‡æè¿°é…å¯¹ã€‚é€šè¿‡å°†è¿™äº›ç»“æ„åŒ–çš„ç»„åˆæ ·æœ¬å¢å¼ºå¯¹æ¯”è®­ç»ƒï¼ŒSceneForgeæœ‰æ•ˆåœ°è§£å†³äº†å¤§è§„æ¨¡3Dæ–‡æœ¬æ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ï¼Œæå¤§åœ°ä¸°å¯Œäº†æ•°æ®çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å…³é”®çš„è®¾è®¡å…ƒç´ ï¼Œå¦‚æ¯ä¸ªåœºæ™¯ä¸­çš„æœ€ä½³ç›®æ ‡æ•°ã€è®­ç»ƒæ‰¹æ¬¡ä¸­ç»„åˆæ ·æœ¬çš„æ¯”ä¾‹ä»¥åŠåœºæ™¯æ„å»ºç­–ç•¥ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒSceneForgeåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬ModelNetã€ScanObjNNã€Objaverse-LVISå’ŒScanNetä¸Šçš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œä»¥åŠShapeNetPartä¸Šçš„å°‘æ ·æœ¬éƒ¨ä»¶åˆ†å‰²ã€‚SceneForgeçš„ç»„åˆå¢å¼ºæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œåœ¨å¤šç§ç¼–ç å™¨æ¶æ„ä¸Šéƒ½èƒ½æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSceneForgeæ”¹è¿›äº†ScanQAçš„3Dè§†è§‰é—®ç­”ï¼Œèƒ½ç¨³å¥åœ°é€‚åº”æ—¥ç›Šå¤æ‚çš„åœºæ™¯æ£€ç´¢ï¼Œå¹¶é€šè¿‡é€‚åº”ç©ºé—´é…ç½®æ¥ç²¾ç¡®ç¬¦åˆæ–‡æœ¬æŒ‡ä»¤ï¼Œå±•ç¤ºäº†ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15693v2">PDF</a> to appear in NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>SceneForgeæ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„å¤šå¯¹è±¡åœºæ™¯ç»„åˆï¼Œå¢å¼ºäº†3Dç‚¹äº‘å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹æ¯”å¯¹é½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸ªä½“3Då½¢çŠ¶æ„å»ºå…·æœ‰æ˜ç¡®ç©ºé—´å…³ç³»çš„å¤šå¯¹è±¡åœºæ™¯ï¼Œå¹¶ä¸ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç²¾ç»†æè¿°çš„å¤šå¯¹è±¡ç›¸ç»“åˆã€‚é€šè¿‡å¯¹æ¯”è®­ç»ƒï¼ŒSceneForgeæœ‰æ•ˆè§£å†³äº†å¤§è§„æ¨¡3D-æ–‡æœ¬æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå¤§åœ°ä¸°å¯Œäº†æ•°æ®çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SceneForgeæ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç”¨äºå¢å¼º3Dç‚¹äº‘å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹æ¯”å¯¹é½ï¼Œé€šè¿‡ç»“æ„åŒ–çš„å¤šå¯¹è±¡åœºæ™¯ç»„åˆå®ç°ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨ä¸ªä½“3Då½¢çŠ¶æ„å»ºå¤šå¯¹è±¡åœºæ™¯ï¼Œå¹¶æ˜ç¡®å…¶ç©ºé—´å…³ç³»ï¼Œé…åˆå¤§å‹è¯­è¨€æ¨¡å‹æè¿°å¤šå¯¹è±¡ã€‚</li>
<li>SceneForgeé€šè¿‡å¯¹æ¯”è®­ç»ƒï¼Œæœ‰æ•ˆè§£å†³å¤§è§„æ¨¡3D-æ–‡æœ¬æ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¦‚ModelNetã€ScanObjNNã€Objaverse-LVISå’ŒScanNetçš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œä»¥åŠShapeNetPartçš„å°‘æ•°æ ·æœ¬éƒ¨åˆ†åˆ†å‰²ã€‚</li>
<li>SceneForgeçš„ç»„åˆå¢å¼ºæ–¹æ³•æ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå¯åœ¨å¤šç§ç¼–ç å™¨æ¶æ„ä¸Šå®ç°æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>SceneForgeåœ¨3Dè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½é€‚åº”åœºæ™¯å¤æ‚æ€§çš„å¢åŠ ï¼Œå¹¶å±•ç¤ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7e7284038282396f06d222491b00da08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733704&auth_key=1760733704-0-0-53aa0582af34d3bc884ffe25bd0998b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-88ba3f49cbebe88aa79e0a7efaf748a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733711&auth_key=1760733711-0-0-ba5a513ff19bd3dfd1e3a73e3e248ac1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4673e24d4a7e20ca8ac3b2f863d923b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733717&auth_key=1760733717-0-0-720d180d19a494a5aaf8db17afbd10aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-06cf11e090f5bf37cc4276e9432d5ad3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733724&auth_key=1760733724-0-0-fa92939cfff6253b4fef250384f276cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67d2cb36f4a32a59858875a8a33584cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733731&auth_key=1760733731-0-0-10ca01d1149d1136809798c3a9e618d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SPADE-Spatial-Transcriptomics-and-Pathology-Alignment-Using-a-Mixture-of-Data-Experts-for-an-Expressive-Latent-Space"><a href="#SPADE-Spatial-Transcriptomics-and-Pathology-Alignment-Using-a-Mixture-of-Data-Experts-for-an-Expressive-Latent-Space" class="headerlink" title="SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture   of Data Experts for an Expressive Latent Space"></a>SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture   of Data Experts for an Expressive Latent Space</h2><p><strong>Authors:Ekaterina Redekop, Mara Pleasure, Zichen Wang, Kimberly Flores, Anthony Sisk, William Speier, Corey W. Arnold</strong></p>
<p>The rapid growth of digital pathology and advances in self-supervised deep learning have enabled the development of foundational models for various pathology tasks across diverse diseases. While multimodal approaches integrating diverse data sources have emerged, a critical gap remains in the comprehensive integration of whole-slide images (WSIs) with spatial transcriptomics (ST), which is crucial for capturing critical molecular heterogeneity beyond standard hematoxylin &amp; eosin (H&amp;E) staining. We introduce SPADE, a foundation model that integrates histopathology with ST data to guide image representation learning within a unified framework, in effect creating an ST-informed latent space. SPADE leverages a mixture-of-data experts technique, where experts are created via two-stage imaging feature-space clustering using contrastive learning to learn representations of co-registered WSI patches and gene expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is evaluated on 20 downstream tasks, demonstrating significantly superior few-shot performance compared to baseline models, highlighting the benefits of integrating morphological and molecular information into one latent space. Code and pretrained weights are available at <a target="_blank" rel="noopener" href="https://github.com/uclabair/SPADE">https://github.com/uclabair/SPADE</a>. </p>
<blockquote>
<p>æ•°å­—ç—…ç†å­¦çš„å¿«é€Ÿå‘å±•å’Œè‡ªç›‘ç£æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ä¸ºå„ç§ç–¾ç—…çš„ä¸åŒç—…ç†ä»»åŠ¡çš„æ¨¡å‹å¼€å‘æä¾›äº†å¯èƒ½ã€‚è™½ç„¶å‡ºç°äº†èåˆå¤šç§æ•°æ®æºçš„å¤šæ¨¡å¼æ–¹æ³•ï¼Œä½†åœ¨æ•´åˆå…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸ç©ºé—´è½¬å½•å­¦ï¼ˆSTï¼‰æ–¹é¢ä»å­˜åœ¨å…³é”®å·®è·ï¼Œè¿™å¯¹äºæ•è·æ ‡å‡†è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²ä¹‹å¤–çš„åˆ†å­å¼‚è´¨æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†SPADEï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ç»„ç»‡ç—…ç†å­¦æ•°æ®ä¸STæ•°æ®ç›¸ç»“åˆçš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…æŒ‡å¯¼å›¾åƒè¡¨ç¤ºå­¦ä¹ ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªå—STå¯å‘çš„æ½œåœ¨ç©ºé—´ã€‚SPADEåˆ©ç”¨æ•°æ®æ··åˆä¸“å®¶æŠ€æœ¯ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæˆåƒç‰¹å¾ç©ºé—´èšç±»å¯¹æ¯”å­¦ä¹ æ¥åˆ›å»ºä¸“å®¶ï¼Œå­¦ä¹ å·²æ³¨å†ŒWSIè¡¥ä¸å’ŒåŸºå› è¡¨è¾¾è°±çš„è¡¨ç¤ºã€‚åœ¨å…¨é¢çš„HEST-1kæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼ŒSPADEåœ¨2wä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå…¶å°‘æ•°æ ·æœ¬æ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜è¶Šï¼Œçªæ˜¾äº†å°†å½¢æ€å­¦å’Œåˆ†å­ä¿¡æ¯æ•´åˆåˆ°ä¸€ä¸ªæ½œåœ¨ç©ºé—´ä¸­çš„ä¼˜åŠ¿ã€‚ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/uclabair/SPADE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/uclabair/SPADEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21857v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°å­—ç—…ç†å­¦çš„å¿«é€Ÿå‘å±•å’Œè‡ªç›‘ç£æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œä¸ºå„ç§ç—…ç†ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹å¼€å‘æä¾›äº†å¯èƒ½ã€‚è¯¥ç ”ç©¶å¼•å…¥SPADEæ¨¡å‹ï¼Œæ•´åˆç»„ç»‡ç—…ç†å­¦ä¸ç©ºé—´è½¬å½•ç»„æ•°æ®ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹æŒ‡å¯¼å›¾åƒè¡¨ç¤ºå­¦ä¹ ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå—ç©ºé—´è½¬å½•ç»„ä¿¡æ¯å½±å“çš„æ•°æ®ç©ºé—´ã€‚SPADEä½¿ç”¨æ··åˆæ•°æ®ä¸“å®¶æŠ€æœ¯ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œä¸¤é˜¶æ®µæˆåƒç‰¹å¾ç©ºé—´èšç±»æ¥åˆ›å»ºä¸“å®¶æ¨¡å‹ï¼Œå­¦ä¹ å…±æ³¨å†Œæ˜¾å¾®é•œå›¾åƒåˆ‡ç‰‡è¡¥ä¸çš„åŸºå› è¡¨è¾¾è°±çš„è¡¨ç¤ºã€‚åœ¨å¤§é‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°è¡¨æ˜ï¼ŒSPADEæ¨¡å‹å±•ç°å‡ºå“è¶Šçš„å°æ ·æœ¬æ€§èƒ½ï¼Œè¯æ˜å°†å½¢æ€å­¦å’Œåˆ†å­ä¿¡æ¯é›†æˆåˆ°å•ä¸€æ½œåœ¨ç©ºé—´çš„ä¼˜åŠ¿ã€‚ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—ç—…ç†å­¦å’Œè‡ªç›‘ç£æ·±åº¦å­¦ä¹ è¿›æ­¥ä¿ƒè¿›äº†ç—…ç†å­¦åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚</li>
<li>SPADEæ¨¡å‹æ•´åˆç»„ç»‡ç—…ç†å­¦å’Œç©ºé—´è½¬å½•ç»„æ•°æ®ã€‚</li>
<li>SPADEåœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹æŒ‡å¯¼å›¾åƒè¡¨ç¤ºå­¦ä¹ ï¼Œåˆ›å»ºST-informedæ½œåœ¨ç©ºé—´ã€‚</li>
<li>SPADEä½¿ç”¨æ··åˆæ•°æ®ä¸“å®¶æŠ€æœ¯ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œç‰¹å¾ç©ºé—´èšç±»ã€‚</li>
<li>æ¨¡å‹èƒ½å­¦ä¹ å…±æ³¨å†Œæ˜¾å¾®é•œå›¾åƒåˆ‡ç‰‡è¡¥ä¸çš„åŸºå› è¡¨è¾¾è°±è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒSPADEå±•ç°å‡ºå“è¶Šçš„å°æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>æ•´åˆå½¢æ€å­¦å’Œåˆ†å­ä¿¡æ¯åˆ°å•ä¸€æ½œåœ¨ç©ºé—´æ˜¯SPADEæ¨¡å‹çš„ä¸»è¦ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-55ebeb4b28ee73656a2c57d6433bc0bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733739&auth_key=1760733739-0-0-3f1ef9b07b94c2de7607da133edc8d7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-932305a0df02ee2a1cf61bbacea44684~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733747&auth_key=1760733747-0-0-a881d9c03afe2d55eb8ec6e001ca7084&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-94d2f501aa22e6a9e80e834bef2a1c6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735322&auth_key=1760735322-0-0-ad58a50ab5e277c334599a4dff60e46d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-0891dd7b028f247a1bd8a7257aed0b40~resize:0:q75.jpg?source=1f5c5e47&expiration=1760730653&auth_key=1760730653-0-0-fe26461d7f44becd1dc3a4381b9e1399&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Agentic Design of Compositional Machines
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
