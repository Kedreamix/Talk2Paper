<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Ponimator Unfolding Interactive Pose for Versatile Human-human   Interaction Animation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-ac6c5c0957151008bca8310ad194fe14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909001&auth_key=1760909001-0-0-48f9c342cbcde7ab0627438807ebf7de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation"><a href="#Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation" class="headerlink" title="Ponimator: Unfolding Interactive Pose for Versatile Human-human   Interaction Animation"></a>Ponimator: Unfolding Interactive Pose for Versatile Human-human   Interaction Animation</h2><p><strong>Authors:Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang</strong></p>
<p>Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework. </p>
<blockquote>
<p>è¿‘è·ç¦»äººæœºäº¤äº’å§¿åŠ¿ä¼ è¾¾äº†ä¸°å¯Œçš„äº¤äº’åŠ¨æ€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åŸºäºè¿™äº›å§¿åŠ¿ï¼Œäººç±»å¯ä»¥ç›´è§‚åœ°æ¨æ–­ä¸Šä¸‹æ–‡å¹¶é¢„æµ‹å¯èƒ½çš„è¿‡å»å’Œæœªæ¥åŠ¨æ€ï¼Œè¿™ä¾èµ–äºäººç±»è¡Œä¸ºçš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ã€‚å—æ­¤è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Ponimatorï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è¿‘è·ç¦»äº¤äº’å§¿åŠ¿ä¸ºåŸºç¡€çš„é€šç”¨äº¤äº’åŠ¨ç”»ç®€å•æ¡†æ¶ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®æ¥è‡ªåŠ¨ä½œæ•æ‰äº¤äº’æ•°æ®é›†ä¸­çš„è¿‘è·ç¦»åŒäººå§¿åŠ¿åŠå…¶å‘¨å›´çš„æ—¶ç©ºä¸Šä¸‹æ–‡ã€‚Ponimatoråˆ©ç”¨äº¤äº’å§¿åŠ¿å…ˆéªŒï¼Œé‡‡ç”¨ä¸¤ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼šï¼ˆ1ï¼‰å§¿æ€åŠ¨ç”»å¸ˆä½¿ç”¨æ—¶é—´å…ˆéªŒæ ¹æ®äº¤äº’å§¿åŠ¿ç”ŸæˆåŠ¨æ€è¿åŠ¨åºåˆ—ï¼›ï¼ˆ2ï¼‰å§¿æ€ç”Ÿæˆå™¨åœ¨äº¤äº’å§¿åŠ¿ä¸å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨ç©ºé—´å…ˆéªŒä»å•ä¸ªå§¿åŠ¿ã€æ–‡æœ¬æˆ–ä¸¤è€…åˆæˆäº¤äº’å§¿åŠ¿ã€‚æ€»ä½“ä¸Šï¼ŒPonimatoræ”¯æŒå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬åŸºäºå›¾åƒçš„äº¤æµåŠ¨ç”»ã€ååº”åŠ¨ç”»å’Œæ–‡æœ¬åˆ°äº¤æµçš„åˆæˆï¼Œä¿ƒè¿›äº†ä»é«˜è´¨é‡mocapæ•°æ®åˆ°å¼€æ”¾ä¸–ç•Œåœºæ™¯çš„äº¤äº’çŸ¥è¯†çš„è½¬ç§»ã€‚åœ¨å¤šä¸ªæ•°æ®é›†å’Œåº”ç”¨ä¸Šçš„å®éªŒè¯æ˜äº†å§¿æ€å…ˆéªŒçš„æ™®éæ€§ä»¥åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14976v1">PDF</a> Accepted to ICCV 2025. Project page:   <a target="_blank" rel="noopener" href="https://stevenlsw.github.io/ponimator/">https://stevenlsw.github.io/ponimator/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPonimatorçš„ç®€å•æ¡†æ¶ï¼Œç”¨äºåŸºäºè¿‘è·ç¦»äº¤äº’å¼å§¿åŠ¿è¿›è¡Œå¤šç§äº¤äº’åŠ¨ç”»ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äººä½“è¡Œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡ä¸¤ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ç”ŸæˆåŠ¨æ€è¿åŠ¨åºåˆ—å’Œäº¤äº’å¼å§¿åŠ¿ã€‚å®ƒèƒ½å¤„ç†å›¾åƒäº¤äº’åŠ¨ç”»ã€ååº”åŠ¨ç”»å’Œæ–‡å­—äº¤äº’åˆæˆç­‰ä»»åŠ¡ï¼Œå¹¶èƒ½å°†é«˜è´¨é‡çš„è¿åŠ¨æ•æ‰æ•°æ®ä¸­çš„äº¤äº’çŸ¥è¯†è½¬ç§»åˆ°å¼€æ”¾åœºæ™¯ä¸­ã€‚å®éªŒè¯æ˜è¯¥æ¡†æ¶å…·æœ‰é€šç”¨æ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ponimatoræ¡†æ¶åˆ©ç”¨è¿‘è·ç¦»äº¤äº’å¼å§¿åŠ¿è¿›è¡Œå¤šç§äº¤äº’åŠ¨ç”»ã€‚</li>
<li>PonimatoråŒ…å«ä¸¤ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼šå§¿åŠ¿åŠ¨ç”»å™¨å’Œå§¿åŠ¿ç”Ÿæˆå™¨ã€‚</li>
<li>å§¿åŠ¿åŠ¨ç”»å™¨ä½¿ç”¨æ—¶é—´å…ˆéªŒçŸ¥è¯†ä»äº¤äº’å¼å§¿åŠ¿ç”ŸæˆåŠ¨æ€è¿åŠ¨åºåˆ—ã€‚</li>
<li>å§¿åŠ¿ç”Ÿæˆå™¨åœ¨ç©ºé—´å…ˆéªŒçŸ¥è¯†çš„åŸºç¡€ä¸Šåˆæˆäº¤äº’å¼å§¿åŠ¿ï¼Œå¯ä»å•å§¿æ€ã€æ–‡æœ¬æˆ–ä¸¤è€…çš„ç»„åˆä¸­ç”Ÿæˆã€‚</li>
<li>Ponimatoræ”¯æŒå›¾åƒäº¤äº’åŠ¨ç”»ã€ååº”åŠ¨ç”»å’Œæ–‡å­—äº¤äº’åˆæˆç­‰ä»»åŠ¡ã€‚</li>
<li>Ponimatorèƒ½å°†é«˜è´¨é‡çš„è¿åŠ¨æ•æ‰æ•°æ®ä¸­çš„äº¤äº’çŸ¥è¯†è½¬ç§»åˆ°å¼€æ”¾åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-32478407633160efcad0845ccac2ca80~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909008&auth_key=1760909008-0-0-50bf276138d43cbe69c9e7701f6f647a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cbe959c2ee18155b2f0c472ffcc092df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909015&auth_key=1760909015-0-0-4efcc8f8d1225ed46d9baae079cc4918&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9eb4857dc7e99ef124af7548e3e5b88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909022&auth_key=1760909022-0-0-71b0d9566b642b98f6cd4884b013b385&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b37e0ca908229a64867d84db9eef804f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909042&auth_key=1760909042-0-0-8ca0f16c9a21e1ce2faed59cbab3537c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72035e67ae63da68888edd0a4435a458~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909049&auth_key=1760909049-0-0-13b5e2ddc7bc643cd83927e29ff1ead4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ab2b5fadb0bf58488848e87a4c5dae0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909057&auth_key=1760909057-0-0-12c78a8e13cb722ad0eb510915ff08ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FraQAT-Quantization-Aware-Training-with-Fractional-bits"><a href="#FraQAT-Quantization-Aware-Training-with-Fractional-bits" class="headerlink" title="FraQAT: Quantization Aware Training with Fractional bits"></a>FraQAT: Quantization Aware Training with Fractional bits</h2><p><strong>Authors:Luca Morreale, Alberto Gil C. P. Ramos, Malcolm Chadwick, Mehid Noroozi, Ruchika Chavhan, Abhinav Mehrotra, Sourav Bhattacharya</strong></p>
<p>State-of-the-art (SOTA) generative models have demonstrated impressive capabilities in image synthesis or text generation, often with a large capacity model. However, these large models cannot be deployed on smartphones due to the limited availability of on-board memory and computations. Quantization methods lower the precision of the model parameters, allowing for efficient computations, \eg, in \INT{8}. Although aggressive quantization addresses efficiency and memory constraints, preserving the quality of the model remains a challenge. To retain quality in previous aggressive quantization, we propose a new fractional bits quantization (\short) approach. The novelty is a simple yet effective idea: we progressively reduce the modelâ€™s precision from 32 to 4 bits per parameter, and exploit the fractional bits during optimization to maintain high generation quality. We show that the \short{} yields improved quality on a variety of diffusion models, including SD3.5-Medium, Sana, \pixart, and FLUX.1-schnell, while achieving $4-7%$ lower FiD than standard QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP). </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒåˆæˆæˆ–æ–‡æœ¬ç”Ÿæˆæ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œé€šå¸¸ä¾èµ–äºå¤§å‹æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºæ‰‹æœºå†…å­˜å’Œè®¡ç®—èµ„æºçš„æœ‰é™ï¼Œè¿™äº›å¤§å‹æ¨¡å‹æ— æ³•éƒ¨ç½²åœ¨æ‰‹æœºä¸Šã€‚é‡åŒ–æ–¹æ³•å¯ä»¥é™ä½æ¨¡å‹å‚æ•°çš„ç²¾åº¦ï¼Œä»è€Œå®ç°é«˜æ•ˆè®¡ç®—ï¼Œä¾‹å¦‚åœ¨INT8ä¸­ã€‚è™½ç„¶æ¿€è¿›é‡åŒ–è§£å†³äº†æ•ˆç‡å’Œå†…å­˜çº¦æŸé—®é¢˜ï¼Œä½†ä¿æŒæ¨¡å‹çš„è´¨é‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†ä¿æŒå…ˆå‰çš„æ¿€è¿›é‡åŒ–çš„è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éƒ¨åˆ†ä½é‡åŒ–ï¼ˆshortï¼‰æ–¹æ³•ã€‚å…¶æ–°é¢–ä¹‹å¤„åœ¨äºä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æƒ³æ³•ï¼šæˆ‘ä»¬é€æ­¥å°†æ¨¡å‹çš„ç²¾åº¦ä»æ¯ä¸ªå‚æ•°çš„32ä½é™ä½åˆ°4ä½ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ©ç”¨éƒ¨åˆ†ä½æ¥ä¿æŒé«˜æ°´å¹³çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å¤šç§æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒ…æ‹¬SD3.5-Mediumã€Sanaã€pixartå’ŒFLUX.1-schnellç­‰æ¨¡å‹ä½¿ç”¨shortåï¼Œåœ¨è¾¾åˆ°æ¯”æ ‡å‡†QATä½4-7%çš„FiDçš„åŒæ—¶ï¼Œç”Ÿæˆè´¨é‡æœ‰æ‰€æé«˜ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æ­è½½é«˜é€šSM8750-ABéªé¾™8ç²¾è‹±Hexagon Tensorå¤„ç†å™¨ï¼ˆHTPï¼‰çš„ä¸‰æ˜ŸS25Uä¸Šéƒ¨ç½²å¹¶è¿è¡Œäº†Sanaã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14823v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒåˆæˆå’Œæ–‡æœ¬ç”Ÿæˆç­‰ç”Ÿæˆå¼ä»»åŠ¡çš„åŒæ—¶ï¼Œç”±äºå…¶å†…å­˜å ç”¨å’Œè®¡ç®—éœ€æ±‚è¿‡å¤§ï¼Œæ— æ³•éƒ¨ç½²äºæ™ºèƒ½æ‰‹æœºä¸Šã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æå‡ºä¸€ç§åä¸ºFBQï¼ˆåˆ†æ•°ä½é‡åŒ–ï¼‰çš„æ–°å‹é‡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨é™ä½æ¨¡å‹å‚æ•°ç²¾åº¦ä»¥æå‡è®¡ç®—æ•ˆç‡å’ŒèŠ‚çœå†…å­˜å ç”¨ã€‚FBQé€šè¿‡é€æ­¥é™ä½æ¨¡å‹å‚æ•°ç²¾åº¦è‡³æ¯å‚æ•°4ä½ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ©ç”¨åˆ†æ•°ä½æ¥ä¿æŒç”Ÿæˆè´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒFBQèƒ½åœ¨å¤šç§æ‰©æ•£æ¨¡å‹ä¸Šå®ç°æ›´å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”åœ¨ç‰¹å®šçš„FIDè¯„ä»·æŒ‡æ ‡ä¸Šç›¸å¯¹äºæ ‡å‡†é‡åŒ–æ–¹æ³•æœ‰æ›´å¥½çš„æ•ˆæœã€‚æœ€åæˆåŠŸåœ¨é…å¤‡éªé¾™é«˜æ€§èƒ½è®¡ç®—å¹³å°éªé¾™ç¥ç»å¤„ç†å•å…ƒçš„ä¸‰æ˜ŸGalaxy S25Uæ‰‹æœºä¸Šéƒ¨ç½²å¹¶è¿è¡Œäº†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒåˆæˆå’Œæ–‡æœ¬ç”Ÿæˆç­‰é¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½†ç”±äºå†…å­˜å’Œè®¡ç®—é™åˆ¶æ— æ³•ç›´æ¥éƒ¨ç½²äºæ™ºèƒ½æ‰‹æœºä¸Šã€‚</li>
<li>é‡åŒ–æ–¹æ³•èƒ½æœ‰æ•ˆé™ä½æ¨¡å‹å‚æ•°ç²¾åº¦ï¼Œæå‡è®¡ç®—æ•ˆç‡å’ŒèŠ‚çœå†…å­˜å ç”¨ã€‚ä½†å¦‚ä½•åœ¨ä¿è¯æ•ˆç‡çš„åŒæ—¶ä¿æŒæ¨¡å‹è´¨é‡æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0c01e23207f2bc4481e584f0b569383b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909064&auth_key=1760909064-0-0-3a63ff03ba20fdf014a73248fa8fed54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94917ab592fcb0d8fa9ca6c490c1ead8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909072&auth_key=1760909072-0-0-c16f2e59df1d4600b15594c855ece256&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-754b5faf30fbfcccda68bc0d5c0143e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909078&auth_key=1760909078-0-0-97cce98fbaa91a7f41656f58737170ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models"><a href="#DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models" class="headerlink" title="DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models"></a>DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models</h2><p><strong>Authors:Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato</strong></p>
<p>Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifierâ€™s decision process without access to training data or ground-truth labels. We demonstrate DEXTERâ€™s flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter">https://github.com/perceivelab/dexter</a>. </p>
<blockquote>
<p>ç†è§£å’Œè§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡Œä¸ºå¯¹äºæ„å»ºé€æ˜å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†DEXTERï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€æ•°æ®çš„æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆè§†è§‰åˆ†ç±»å™¨çš„å…¨å±€æ–‡æœ¬è§£é‡Šã€‚DEXTERé€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥åˆæˆèƒ½å¼ºçƒˆæ¿€æ´»ç›®æ ‡åˆ†ç±»å™¨çš„ç±»åˆ«æ¡ä»¶å›¾åƒã€‚è¿™äº›åˆæˆæ ·æœ¬ç„¶åç”¨äºç”Ÿæˆè¯¦ç»†çš„è‡ªç„¶è¯­è¨€æŠ¥å‘Šï¼Œæè¿°ç‰¹å®šç±»åˆ«çš„å†³ç­–æ¨¡å¼å’Œåè§ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼ŒDEXTERèƒ½å¤Ÿåœ¨æ— éœ€è®¿é—®è®­ç»ƒæ•°æ®æˆ–çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œæä¾›å…³äºåˆ†ç±»å™¨å†³ç­–è¿‡ç¨‹çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šå±•ç¤ºäº†DEXTERçš„çµæ´»æ€§ï¼šæ¿€æ´»æœ€å¤§åŒ–ã€åˆ‡ç‰‡å‘ç°å’Œå»åä»¥åŠåè§è§£é‡Šï¼Œæ¯ä¸ªä»»åŠ¡éƒ½å±•ç¤ºäº†å…¶æ­ç¤ºè§†è§‰åˆ†ç±»å™¨å†…éƒ¨æœºåˆ¶çš„èƒ½åŠ›ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œè¡¨æ˜DEXTERäº§ç”Ÿçš„è¾“å‡ºå‡†ç¡®ä¸”æ˜“äºè§£é‡Šã€‚åœ¨ImageNetã€Waterbirdsã€CelebAå’ŒFairFacesä¸Šçš„å®éªŒè¯å®ï¼ŒDEXTERåœ¨å…¨å±€æ¨¡å‹è§£é‡Šå’Œç±»åˆ«çº§åˆ«åè§æŠ¥å‘Šæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/perceivelab/dexterè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14741v1">PDF</a> Accepted to NeurIPS 2025 (spotlight)</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ä¸ªæ•°æ®è‡ªç”±æ¡†æ¶DEXTERï¼Œå®ƒé€šè¿‡æ‰©æ•£æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè§†è§‰åˆ†ç±»å™¨çš„å…¨å±€æ–‡æœ¬è§£é‡Šã€‚DEXTERé€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥åˆæˆèƒ½å¤Ÿå¼ºçƒˆæ¿€æ´»ç›®æ ‡åˆ†ç±»å™¨çš„ç±»åˆ«æ¡ä»¶å›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›åˆæˆæ ·æœ¬å¼•å‡ºè¯¦ç»†çš„è‡ªç„¶è¯­è¨€æŠ¥å‘Šæ¥æè¿°ç‰¹å®šç±»åˆ«çš„å†³ç­–æ¨¡å¼å’Œåè§ã€‚DEXTERæ— éœ€è®¿é—®è®­ç»ƒæ•°æ®æˆ–çœŸå®æ ‡ç­¾å³å¯æä¾›è‡ªç„¶è¯­è¨€è§£é‡Šåˆ†ç±»å™¨çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨æ¿€æ´»æœ€å¤§åŒ–ã€åˆ‡ç‰‡å‘ç°ä¸å»åä»¥åŠåè§è§£é‡Šä¸‰ä¸ªä»»åŠ¡ä¸Šçš„æ¼”ç¤ºæ˜¾ç¤ºäº†DEXTERæ­ç¤ºè§†è§‰åˆ†ç±»å™¨å†…éƒ¨æœºåˆ¶çš„èƒ½åŠ›ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œå‡è¡¨æ˜DEXTERäº§ç”Ÿçš„è¾“å‡ºå‡†ç¡®ä¸”æ˜“äºè§£è¯»ã€‚åœ¨ImageNetã€Waterbirdsã€CelebAå’ŒFairFacesä¸Šçš„å®éªŒè¯å®ï¼ŒDEXTERåœ¨å…¨çƒæ¨¡å‹è§£é‡Šå’Œç±»åˆ«çº§åˆ«åè§æŠ¥å‘Šæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DEXTERæ˜¯ä¸€ä¸ªæ•°æ®è‡ªç”±æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè§†è§‰åˆ†ç±»å™¨çš„å…¨å±€æ–‡æœ¬è§£é‡Šã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥åˆæˆç±»åˆ«æ¡ä»¶å›¾åƒï¼Œå¼ºçƒˆæ¿€æ´»ç›®æ ‡åˆ†ç±»å™¨ã€‚</li>
<li>åˆ©ç”¨åˆæˆæ ·æœ¬å¼•å‡ºè‡ªç„¶è¯­è¨€æŠ¥å‘Šï¼Œæè¿°ç‰¹å®šç±»åˆ«çš„å†³ç­–æ¨¡å¼å’Œåè§ã€‚</li>
<li>DEXTERæ— éœ€è®¿é—®è®­ç»ƒæ•°æ®æˆ–çœŸå®æ ‡ç­¾å³å¯è§£é‡Šåˆ†ç±»å™¨çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ¼”ç¤ºäº†DEXTERçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¿€æ´»æœ€å¤§åŒ–ã€åˆ‡ç‰‡å‘ç°ä¸å»åä»¥åŠåè§è§£é‡Šã€‚</li>
<li>å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ç”¨æˆ·ç ”ç©¶ï¼Œè¡¨æ˜DEXTERäº§ç”Ÿçš„è¾“å‡ºå‡†ç¡®ä¸”æ˜“äºè§£è¯»ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯å®ï¼ŒDEXTERåœ¨å…¨çƒæ¨¡å‹è§£é‡Šå’Œç±»åˆ«çº§åˆ«åè§æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7da9bc4c36b2a1e029838525c732fe31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909086&auth_key=1760909086-0-0-884add92440fb09c99561f1f87270949&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5487f02bf6fbb21fa801270a588322ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909093&auth_key=1760909093-0-0-97920b36f6ab23b44ba2f9a381deb548&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection"><a href="#A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection" class="headerlink" title="A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection"></a>A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection</h2><p><strong>Authors:Shivangi Yadav, Arun Ross</strong></p>
<p>An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method. </p>
<blockquote>
<p>è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿå¯èƒ½ä¼šå—åˆ°å‘ˆç°æ”»å‡»ï¼ˆPAsï¼‰çš„å¨èƒï¼Œå…¶ä¸­äººå·¥çœ¼ã€æ‰“å°çš„çœ¼å›¾åƒæˆ–ç¾å®¹éšå½¢çœ¼é•œç­‰ä¼ªåˆ¶å“ä¼šè¢«å‘ˆç°ç»™ç³»ç»Ÿã€‚ä¸ºäº†å¯¹æŠ—è¿™ä¸€ç‚¹ï¼Œå·²ç»å¼€å‘äº†å‡ ç§ç”¨äºæ£€æµ‹å‘ˆç°çš„å‘ˆç°æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è™¹è†œPADæŠ€æœ¯çš„æ•°æ®é›†ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ„å»ºå’ŒæˆåƒPAså­˜åœ¨éšæ€§çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šåŸŸå›¾åƒç¿»è¯‘æ‰©æ•£é£æ ¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆMID-StyleGANï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç”Ÿæˆåˆæˆçœ¼å›¾åƒçš„æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿæ•æ‰å¤šä¸ªåŸŸä¸­çš„å‘ˆç°æ”»å‡»å’ŒçœŸå®çœ¼ç‰¹å¾ï¼Œå¦‚çœŸå®çœ¼ã€æ‰“å°çš„çœ¼ç›å’Œç¾å®¹éšå½¢çœ¼é•œã€‚MID-StyleGANç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ï¼Œç”Ÿæˆé€¼çœŸçš„å¤šæ ·åŒ–çš„åˆæˆæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šåŸŸæ¶æ„ï¼Œå®ç°çœŸå®çœ¼å›¾åƒå’Œä¸åŒå‘ˆç°æ”»å‡»åŸŸä¹‹é—´çš„ç¿»è¯‘ã€‚æ¨¡å‹é‡‡ç”¨é€‚åˆçœ¼æ•°æ®çš„è‡ªé€‚åº”æŸå¤±å‡½æ•°ï¼Œä»¥ä¿æŒåŸŸä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆçœ¼å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç”Ÿæˆçš„æ•°æ®è¢«ç”¨æ¥æ˜¾è‘—æé«˜PADç³»ç»Ÿçš„æ€§èƒ½ï¼Œä¸ºè§£å†³è™¹è†œå’Œçœ¼éƒ¨ç”Ÿç‰©è¯†åˆ«ä¸­çš„æ•°çŸ­ç¼ºé—®é¢˜æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œåœ¨LivDet2020æ•°æ®é›†ä¸Šï¼Œåœ¨åƒåˆ†ä¹‹ä¸€çš„è¯¯æ£€ç‡ä¸‹ï¼ŒçœŸå®æ£€æµ‹ç‡ä»93.41%æé«˜åˆ°98.72%ï¼Œå±•ç¤ºäº†æ‰€æå‡ºæ–¹æ³•çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14314v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šä¸ºè§£å†³è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­å› å±•ç¤ºæ”»å‡»ï¼ˆPAsï¼‰å¸¦æ¥çš„å®‰å…¨é—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ç§æ–°å‹çš„å¤šåŸŸå›¾åƒè½¬æ¢æ‰©æ•£é£æ ¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆMID-StyleGANï¼‰ã€‚æ­¤ç½‘ç»œå¯ç”Ÿæˆåˆæˆçœ¼éƒ¨å›¾åƒï¼Œè¦†ç›–å¤šä¸ªé¢†åŸŸå¦‚çœŸå®å›¾åƒå’Œæ‰“å°å›¾åƒã€éšå½¢çœ¼é•œç­‰è™šå‡å›¾åƒçš„ç‰¹ç‚¹ã€‚è¯¥ç½‘ç»œç»“åˆæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ç”Ÿæˆé€¼çœŸçš„å¤šæ ·æ€§åˆæˆæ•°æ®ã€‚åœ¨å¤šä¸ªå®éªŒä¸­ï¼ŒMID-StyleGANè¡¨ç°ä¼˜è¶Šï¼Œèƒ½æœ‰æ•ˆæé«˜è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿçš„æ€§èƒ½ã€‚å¯¹äºLivDet2020æ•°æ®é›†ï¼Œåœ¨è¯¯æŠ¥ç‡ä¸º1%çš„æƒ…å†µä¸‹ï¼ŒçœŸå®æ£€æµ‹ç‡ä»93.41%æå‡è‡³98.72%ã€‚è¿™ä¸€æ–¹æ³•ä¸ºè™¹è†œå’Œçœ¼éƒ¨ç”Ÿç‰©è¯†åˆ«ä¸­æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿæ˜“å—å±•ç¤ºæ”»å‡»ï¼ˆPAsï¼‰å½±å“ï¼Œéœ€è¦å¼€å‘åº”å¯¹æ–¹æ³•ã€‚</li>
<li>MID-StyleGANæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆåˆæˆçœ¼éƒ¨å›¾åƒï¼Œè¦†ç›–å¤šä¸ªé¢†åŸŸçš„ç‰¹ç‚¹ã€‚</li>
<li>MID-StyleGANç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ã€‚</li>
<li>MID-StyleGANé‡‡ç”¨å¤šåŸŸæ¶æ„ï¼Œèƒ½åœ¨çœŸå®çœ¼éƒ¨å›¾åƒå’Œä¸åŒè™šå‡å›¾åƒé¢†åŸŸä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨é’ˆå¯¹çœ¼éƒ¨æ•°æ®çš„è‡ªé€‚åº”æŸå¤±å‡½æ•°ä»¥ä¿æŒé¢†åŸŸä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜MID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆçœ¼éƒ¨å›¾åƒæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-80a4fdafcc32495b50c09a446a994bfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909101&auth_key=1760909101-0-0-8ab465ecb0e39be689d38aa5540d230d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6056cd86ebec8885b5424fb61832431d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909129&auth_key=1760909129-0-0-0aa25b7c4d828318109cd0fb4f39aa2c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43c738f3cd0db04b00bbb1720790275e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909135&auth_key=1760909135-0-0-2901b93b04029cb6ec016bb5157c6f51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-122efb3575dd31fe74dde03e03b33246~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909142&auth_key=1760909142-0-0-faa1a457f7c1cd0d49da5b5518ee5e3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb6859920f9fd33d336e66f021b91ed8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909148&auth_key=1760909148-0-0-27289408d58a664e3c44c0ad19ecebd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20972dcc99576616073b9315b35c5966~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909155&auth_key=1760909155-0-0-94405eb0216669eabfe6ccc93d8855c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9aa34d8e9eb70a66a0a99f36cdcd4fd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909161&auth_key=1760909161-0-0-95f81877c76d4a57c637f93394a94213&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Nonparametric-Data-Attribution-for-Diffusion-Models"><a href="#Nonparametric-Data-Attribution-for-Diffusion-Models" class="headerlink" title="Nonparametric Data Attribution for Diffusion Models"></a>Nonparametric Data Attribution for Diffusion Models</h2><p><strong>Authors:Yutian Zhao, Chao Du, Xiaosen Zheng, Tianyu Pang, Min Lin</strong></p>
<p>Data attribution for generative models seeks to quantify the influence of individual training examples on model outputs. Existing methods for diffusion models typically require access to model gradients or retraining, limiting their applicability in proprietary or large-scale settings. We propose a nonparametric attribution method that operates entirely on data, measuring influence via patch-level similarity between generated and training images. Our approach is grounded in the analytical form of the optimal score function and naturally extends to multiscale representations, while remaining computationally efficient through convolution-based acceleration. In addition to producing spatially interpretable attributions, our framework uncovers patterns that reflect intrinsic relationships between training data and outputs, independent of any specific model. Experiments demonstrate that our method achieves strong attribution performance, closely matching gradient-based approaches and substantially outperforming existing nonparametric baselines. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/NDA">https://github.com/sail-sg/NDA</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„æ•°æ®å½’å› æ—¨åœ¨é‡åŒ–å•ä¸ªè®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•é€šå¸¸éœ€è¦è®¿é—®æ¨¡å‹æ¢¯åº¦æˆ–è¿›è¡Œå†è®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸“æœ‰æˆ–å¤§è§„æ¨¡ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§éå‚æ•°å½’å› æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®Œå…¨åŸºäºæ•°æ®è¿è¡Œï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒå’Œè®­ç»ƒå›¾åƒä¹‹é—´çš„è¡¥ä¸çº§åˆ«ç›¸ä¼¼æ€§æ¥è¡¡é‡å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥æœ€ä¼˜åˆ†æ•°å‡½æ•°çš„è§£æå½¢å¼ä¸ºåŸºç¡€ï¼Œè‡ªç„¶åœ°æ‰©å±•åˆ°å¤šå°ºåº¦è¡¨ç¤ºï¼ŒåŒæ—¶é€šè¿‡åŸºäºå·ç§¯çš„åŠ é€Ÿä¿æŒè®¡ç®—æ•ˆç‡ã€‚é™¤äº†äº§ç”Ÿç©ºé—´å¯è§£é‡Šçš„å½’å› å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¿˜æ­ç¤ºäº†åæ˜ è®­ç»ƒæ•°æ®å’Œè¾“å‡ºä¹‹é—´å†…åœ¨å…³ç³»çš„æ¨¡å¼ï¼Œç‹¬ç«‹äºä»»ä½•ç‰¹å®šæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¼ºå¤§çš„å½’å› æ€§èƒ½ï¼Œä¸åŸºäºæ¢¯åº¦çš„æ–¹æ³•éå¸¸åŒ¹é…ï¼Œå¹¶ä¸”å¤§å¤§ä¼˜äºç°æœ‰çš„éå‚æ•°åŸºçº¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sail-sg/NDA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sail-sg/NDAä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14269v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€æ¨¡å‹æ¢¯åº¦æˆ–é‡æ–°è®­ç»ƒçš„å…¨æ•°æ®éå‚æ•°å½’å› æ–¹æ³•ï¼Œç”¨äºé‡åŒ–ç”Ÿæˆæ¨¡å‹ä¸­å•ä¸ªè®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚è¯¥æ–¹æ³•åŸºäºæœ€ä¼˜è¯„åˆ†å‡½æ•°çš„è§£æå½¢å¼ï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒä¹‹é—´çš„æ–‘å—çº§ç›¸ä¼¼æ€§æ¥æµ‹é‡å½±å“ï¼Œå¹¶è‡ªç„¶åœ°æ‰©å±•åˆ°å¤šå°ºåº¦è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é™¤äº†äº§ç”Ÿç©ºé—´å¯è§£é‡Šçš„å½’å› å¤–ï¼Œè¿˜æ­ç¤ºäº†è®­ç»ƒæ•°æ®ä¸è¾“å‡ºä¹‹é—´çš„å†…åœ¨å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å¼ºå¤§çš„å½’å› æ€§èƒ½ï¼Œä¸åŸºäºæ¢¯åº¦çš„æ–¹æ³•ç´§å¯†åŒ¹é…ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†ç°æœ‰çš„éå‚æ•°åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„å…¨æ•°æ®éå‚æ•°å½’å› æ–¹æ³•ï¼Œæ— éœ€æ¨¡å‹æ¢¯åº¦æˆ–é‡æ–°è®­ç»ƒã€‚</li>
<li>æ–¹æ³•åŸºäºæœ€ä¼˜è¯„åˆ†å‡½æ•°çš„è§£æå½¢å¼ï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒçš„æ–‘å—çº§ç›¸ä¼¼æ€§æ¥æµ‹é‡å½±å“ã€‚</li>
<li>æ–¹æ³•å¯è‡ªç„¶åœ°æ‰©å±•åˆ°å¤šå°ºåº¦è¡¨ç¤ºï¼Œå¹¶å¯é€šè¿‡å·ç§¯åŠ é€Ÿè®¡ç®—æ•ˆç‡ã€‚</li>
<li>äº§ç”Ÿçš„ç©ºé—´å¯è§£é‡Šå½’å› æœ‰åŠ©äºç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ­ç¤ºäº†è®­ç»ƒæ•°æ®ä¸æ¨¡å‹è¾“å‡ºä¹‹é—´çš„å†…åœ¨å…³ç³»ï¼Œè¿™ç§å…³ç³»ç‹¬ç«‹äºä»»ä½•ç‰¹å®šæ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å¼ºå¤§çš„å½’å› æ€§èƒ½ï¼Œä¸åŸºäºæ¢¯åº¦çš„æ–¹æ³•ç›¸åŒ¹é…ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„éå‚æ•°åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2598f6c3da2a7af6b1f6f97409361fbf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909169&auth_key=1760909169-0-0-934b4e83b07691ba82c72ab80486642e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8241257430125418097e0f500b9b5a85~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909176&auth_key=1760909176-0-0-e8eab6fdd6e2079ab512ed2e9046097d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Identity-Preserving-Image-to-Video-Generation-via-Reward-Guided-Optimization"><a href="#Identity-Preserving-Image-to-Video-Generation-via-Reward-Guided-Optimization" class="headerlink" title="Identity-Preserving Image-to-Video Generation via Reward-Guided   Optimization"></a>Identity-Preserving Image-to-Video Generation via Reward-Guided   Optimization</h2><p><strong>Authors:Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo Cao, Bo Zheng</strong></p>
<p>Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at \href{<a target="_blank" rel="noopener" href="https://ipro-alimama.github.io/%7D%7Bhttps://ipro-alimama.github.io/%7D">https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}</a>. </p>
<blockquote>
<p>è¿‘æœŸå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆçš„è¿›å±•åœ¨ä»æœªé™æ€å›¾åƒåˆæˆé«˜è´¨é‡ã€æ—¶é—´è¿è´¯çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚åœ¨æ‰€æœ‰çš„I2Våº”ç”¨ä¸­ï¼Œä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆå æ®å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„I2Væ¨¡å‹åœ¨ä¿æŒè¾“å…¥äººåƒä¸ç”Ÿæˆè§†é¢‘ä¹‹é—´èº«ä»½ä¸€è‡´æ€§æ–¹é¢é‡åˆ°å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ä¸­äººç‰©è¡¨æƒ…å˜åŒ–å’ŒåŠ¨ä½œæ˜¾è‘—æ—¶ã€‚å½“äººè„¸åªå å›¾åƒçš„ä¸€å°éƒ¨åˆ†æ—¶ï¼Œè¿™ä¸ªé—®é¢˜å˜å¾—æ›´ä¸ºå…³é”®ã€‚ç”±äºäººç±»å¯¹èº«ä»½å˜åŒ–é«˜åº¦æ•æ„Ÿï¼Œè¿™ä¸ºI2Vç”Ÿæˆæå‡ºäº†ä¸€ä¸ªè‡³å…³é‡è¦ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„èº«ä»½ä¿ç•™å¥–åŠ±å¼•å¯¼ä¼˜åŒ–ï¼ˆIPROï¼‰æ–°å‹è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œä»¥æå‡èº«ä»½ä¿ç•™èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯å¼•å…¥è¾…åŠ©æ¨¡å—æˆ–æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§ç›´æ¥æœ‰æ•ˆçš„è°ƒæ•´ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨é¢éƒ¨èº«ä»½è¯„åˆ†è€…å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚ä¸ºäº†æå‡æ€§èƒ½å’ŒåŠ é€Ÿæ”¶æ•›ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡‡æ ·é“¾çš„æœ€åå‡ æ­¥åå‘ä¼ æ’­å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°æ›´ä¸°å¯Œçš„æ¢¯åº¦åé¦ˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„é¢éƒ¨è¯„åˆ†æœºåˆ¶ï¼Œå°†çœŸå®è§†é¢‘ä¸­çš„é¢éƒ¨è§†ä¸ºé¢éƒ¨ç‰¹å¾æ± ï¼Œæä¾›å¤šè§’åº¦çš„é¢éƒ¨ä¿¡æ¯ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚è¿›ä¸€æ­¥å¼•å…¥äº†KLæ•£åº¦æ­£åˆ™åŒ–ä»¥ç¨³å®šè®­ç»ƒå¹¶é˜²æ­¢å¯¹å¥–åŠ±ä¿¡å·çš„è¿‡åº¦æ‹Ÿåˆã€‚åœ¨Wan 2.2 I2Væ¨¡å‹å’Œæˆ‘ä»¬çš„å†…éƒ¨I2Væ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://ipro-alimama.github.io/">https://ipro-alimama.github.io/</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14255v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ çš„äººè„¸èº«ä»½ä¿ç•™å¥–åŠ±å¯¼å‘ä¼˜åŒ–ï¼ˆIPROï¼‰è§†é¢‘æ‰©æ•£æ¡†æ¶è¢«æå‡ºï¼Œç”¨äºè§£å†³å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­èº«ä»½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚æ­¤æ¡†æ¶ä¸é€šè¿‡å¼•å…¥è¾…åŠ©æ¨¡å—æˆ–æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯é‡‡ç”¨ç›´æ¥æœ‰æ•ˆçš„è°ƒæ•´ç®—æ³•ï¼Œä½¿ç”¨äººè„¸èº«ä»½è¯„åˆ†è€…å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡æ”¹è¿›æ€§èƒ½å¹¶åŠ é€Ÿæ”¶æ•›ï¼Œè¯¥æ–¹æ³•é€šè¿‡é‡‡æ ·é“¾çš„æœ€åå‡ æ­¥åå‘ä¼ æ’­å¥–åŠ±ä¿¡å·ï¼Œæä¾›ä¸°å¯Œçš„æ¢¯åº¦åé¦ˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„é¢éƒ¨è¯„åˆ†æœºåˆ¶ï¼Œå°†çœŸå®è§†é¢‘ä¸­çš„é¢éƒ¨ä½œä¸ºé¢éƒ¨ç‰¹å¾æ± ï¼Œæä¾›å¤šè§’åº¦çš„é¢éƒ¨ä¿¡æ¯ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚KLæ•£åº¦æ­£åˆ™åŒ–è¢«è¿›ä¸€æ­¥å¼•å…¥ä»¥ç¨³å®šè®­ç»ƒå¹¶é˜²æ­¢å¯¹å¥–åŠ±ä¿¡å·çš„è¿‡åº¦æ‹Ÿåˆã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨Wang 2.2çš„I2Væ¨¡å‹åŠè‡ªæœ‰I2Væ¨¡å‹ä¸Šå‡æœ‰æ•ˆã€‚é¡¹ç›®å’Œä»£ç å¯åœ¨ç›¸åº”ç½‘ç«™ä¸ŠæŸ¥çœ‹ã€‚è¯¥ç ”ç©¶çš„åˆ›æ–°ç‚¹åœ¨äºä¼˜åŒ–äº†æ‰©æ•£æ¨¡å‹åœ¨äººè„¸å›¾åƒç”Ÿæˆè§†é¢‘æ—¶çš„èº«ä»½ä¸€è‡´æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”ŸæˆæŠ€æœ¯åœ¨ä¿æŒäººç‰©èº«ä»½ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ˜¾è‘—çš„è¡¨æƒ…å˜åŒ–å’ŒåŠ¨ä½œæ—¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„èº«ä»½ä¿ç•™å¥–åŠ±å¯¼å‘ä¼˜åŒ–ï¼ˆIPROï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>IPROæ¡†æ¶ä¸ä½¿ç”¨é¢å¤–çš„è¾…åŠ©æ¨¡å—æˆ–å¤æ‚çš„æ¨¡å‹æ¶æ„æ”¹å˜ï¼Œè€Œæ˜¯é‡‡ç”¨ç›´æ¥æœ‰æ•ˆçš„è°ƒæ•´ç®—æ³•å’Œäººè„¸èº«ä»½è¯„åˆ†è€…æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é€šè¿‡åœ¨é‡‡æ ·é“¾çš„æœ€åå‡ æ­¥åå‘ä¼ æ’­å¥–åŠ±ä¿¡å·ï¼Œæé«˜äº†æ€§èƒ½å¹¶åŠ é€Ÿäº†æ”¶æ•›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢éƒ¨è¯„åˆ†æœºåˆ¶ï¼Œåˆ©ç”¨çœŸå®è§†é¢‘ä¸­çš„é¢éƒ¨ç‰¹å¾æ± æä¾›å¤šè§’åº¦ä¿¡æ¯ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†KLæ•£åº¦æ­£åˆ™åŒ–ä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶é˜²æ­¢æ¨¡å‹å¯¹å¥–åŠ±ä¿¡å·çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>åœ¨å¤šä¸ªå®éªŒæ¨¡å‹ä¸Šçš„ç»“æœè¡¨æ˜IPROæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a0a2bf0e2f3c06e55890864b5a099ecf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909184&auth_key=1760909184-0-0-bdd94e8ef4c4c8feb850b7718653792b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7cf879188ae17216f9459224677b10b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909191&auth_key=1760909191-0-0-1885a01d3595da9670c76427fa906a78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01c98ea8db09ccdc59a53bf97b3f8b54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909198&auth_key=1760909198-0-0-29e13d50ea7e1e3ed81ed127f9fc2139&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdaa33f520f7d4971cdfb6c6b67192c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909206&auth_key=1760909206-0-0-2c333ab1636af2ae19b0e216bfeca54a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0741636324814ce13d38f5d7343259e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909213&auth_key=1760909213-0-0-427fd189960c4a37ff090bac0fef003f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LOTA-Bit-Planes-Guided-AI-Generated-Image-Detection"><a href="#LOTA-Bit-Planes-Guided-AI-Generated-Image-Detection" class="headerlink" title="LOTA: Bit-Planes Guided AI-Generated Image Detection"></a>LOTA: Bit-Planes Guided AI-Generated Image Detection</h2><p><strong>Authors:Hongsong Wang, Renxi Cheng, Yang Zhang, Chaolei Han, Jie Gui</strong></p>
<p>The rapid advancement of GAN and Diffusion models makes it more difficult to distinguish AI-generated images from real ones. Recent studies often use image-based reconstruction errors as an important feature for determining whether an image is AI-generated. However, these approaches typically incur high computational costs and also fail to capture intrinsic noisy features present in the raw images. To solve these problems, we innovatively refine error extraction by using bit-plane-based image processing, as lower bit planes indeed represent noise patterns in images. We introduce an effective bit-planes guided noisy image generation and exploit various image normalization strategies, including scaling and thresholding. Then, to amplify the noise signal for easier AI-generated image detection, we design a maximum gradient patch selection that applies multi-directional gradients to compute the noise score and selects the region with the highest score. Finally, we propose a lightweight and effective classification head and explore two different structures: noise-based classifier and noise-guided classifier. Extensive experiments on the GenImage benchmark demonstrate the outstanding performance of our method, which achieves an average accuracy of \textbf{98.9%} (\textbf{11.9}%~$\uparrow$) and shows excellent cross-generator generalization capability. Particularly, our method achieves an accuracy of over 98.2% from GAN to Diffusion and over 99.2% from Diffusion to GAN. Moreover, it performs error extraction at the millisecond level, nearly a hundred times faster than existing methods. The code is at <a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA">https://github.com/hongsong-wang/LOTA</a>. </p>
<blockquote>
<p>GANå’ŒDiffusionæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—åŒºåˆ†AIç”Ÿæˆçš„å›¾åƒå’ŒçœŸå®å›¾åƒå˜å¾—æ›´åŠ å›°éš¾ã€‚è¿‘æœŸçš„ç ”ç©¶ç»å¸¸å°†åŸºäºå›¾åƒçš„é‡å»ºè¯¯å·®ä½œä¸ºåˆ¤æ–­å›¾åƒæ˜¯å¦ç”±AIç”Ÿæˆçš„é‡è¦ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”æ— æ³•æ•æ‰åˆ°åŸå§‹å›¾åƒä¸­å­˜åœ¨çš„å†…åœ¨å™ªå£°ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›æ–°æ€§åœ°é€šè¿‡åŸºäºä½å¹³é¢çš„å›¾åƒå¤„ç†æ¥æ”¹è¿›è¯¯å·®æå–ï¼Œå› ä¸ºä½ä½å¹³é¢ç¡®å®ä»£è¡¨äº†å›¾åƒä¸­çš„å™ªå£°æ¨¡å¼ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„ä½å¹³é¢å¼•å¯¼å™ªå£°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå¹¶åˆ©ç”¨äº†å„ç§å›¾åƒå½’ä¸€åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬ç¼©æ”¾å’Œé˜ˆå€¼åŒ–ã€‚æ¥ç€ï¼Œä¸ºäº†æ”¾å¤§å™ªå£°ä¿¡å·ï¼Œä¾¿äºæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœ€å¤§æ¢¯åº¦è¡¥ä¸é€‰æ‹©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åº”ç”¨å¤šæ–¹å‘æ¢¯åº¦æ¥è®¡ç®—å™ªå£°åˆ†æ•°ï¼Œå¹¶é€‰æ‹©å¾—åˆ†æœ€é«˜çš„åŒºåŸŸã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»ä¾¿æœ‰æ•ˆçš„åˆ†ç±»å¤´ï¼Œå¹¶æ¢ç´¢äº†ä¸¤ç§ä¸åŒç»“æ„ï¼šåŸºäºå™ªå£°çš„åˆ†ç±»å™¨å’Œå™ªå£°å¼•å¯¼çš„åˆ†ç±»å™¨ã€‚åœ¨GenImageåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„å‡ºè‰²æ€§èƒ½ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†å¹³å‡å‡†ç¡®ç‡<strong>98.9%ï¼ˆä¸Šå‡11.9%ï¼‰</strong>ï¼Œå¹¶è¡¨ç°å‡ºå‡ºè‰²çš„è·¨ç”Ÿæˆå™¨æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»GANåˆ°Diffusionçš„å‡†ç¡®ç‡è¶…è¿‡98.2%ï¼Œè€Œä»Diffusionåˆ°GANçš„å‡†ç¡®ç‡è¶…è¿‡99.2%ã€‚æ­¤å¤–ï¼Œå®ƒä»¥æ¯«ç§’çº§çš„é€Ÿåº¦è¿›è¡Œè¯¯å·®æå–ï¼Œå‡ ä¹æ˜¯ç°æœ‰æ–¹æ³•çš„100å€ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA%E3%80%82">https://github.com/hongsong-wang/LOTAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14230v1">PDF</a> Published in the ICCV2025, COde is   <a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA">https://github.com/hongsong-wang/LOTA</a></p>
<p><strong>Summary</strong><br>     è¿‘æœŸGANå’ŒDiffusionæ¨¡å‹çš„å‘å±•ä½¿å¾—åŒºåˆ†AIç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒå˜å¾—å›°éš¾ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–å›¾åƒé‡å»ºè¯¯å·®æ¥åˆ¤æ–­å›¾åƒæ˜¯å¦ç”±AIç”Ÿæˆï¼Œä½†è®¡ç®—æˆæœ¬é«˜ä¸”å¿½ç•¥å›¾åƒæœ¬èº«çš„å™ªå£°ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºä½å¹³é¢çš„å›¾åƒå¤„ç†æ–¹æ³•è¿›è¡Œè¯¯å·®æå–ï¼Œå¹¶åˆ©ç”¨å„ç§å›¾åƒå½’ä¸€åŒ–ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®¾è®¡æœ€å¤§æ¢¯åº¦è¡¥ä¸é€‰æ‹©æ¥æ”¾å¤§å™ªå£°ä¿¡å·ï¼Œè¿›è€Œæå‡ºè½»é‡çº§æœ‰æ•ˆçš„åˆ†ç±»å¤´ã€‚åœ¨GenImageåŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡é«˜è¾¾98.9%ï¼Œä¸”è·¨ç”Ÿæˆå™¨æ³›åŒ–èƒ½åŠ›å¼ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰§è¡Œè¯¯å·®æå–çš„é€Ÿåº¦æ¥è¿‘æ¯«ç§’çº§åˆ«ï¼Œæ˜¯ç°æœ‰æ–¹æ³•çš„è¿‘ç™¾å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANå’ŒDiffusionæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´åŒºåˆ†AIç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒæ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å›¾åƒé‡å»ºè¯¯å·®åˆ¤æ–­å›¾åƒæ˜¯å¦ç”±AIç”Ÿæˆï¼Œä½†è®¡ç®—æˆæœ¬é«˜ä¸”å¿½ç•¥å™ªå£°ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨åŸºäºä½å¹³é¢çš„å›¾åƒå¤„ç†æ–¹æ³•è¿›è¡Œè¯¯å·®æå–ï¼Œä»¥æ•æ‰å›¾åƒä¸­çš„å™ªå£°æ¨¡å¼ã€‚</li>
<li>åˆ©ç”¨å„ç§å›¾åƒå½’ä¸€åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬ç¼©æ”¾å’Œé˜ˆå€¼åŒ–ã€‚</li>
<li>è®¾è®¡æœ€å¤§æ¢¯åº¦è¡¥ä¸é€‰æ‹©æ¥æ”¾å¤§å™ªå£°ä¿¡å·ï¼Œä¾¿äºæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>æå‡ºè½»é‡çº§æœ‰æ•ˆçš„åˆ†ç±»å¤´ï¼ŒåŒ…æ‹¬åŸºäºå™ªå£°çš„åˆ†ç±»å™¨å’Œå¼•å¯¼å™ªå£°çš„åˆ†ç±»å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c4b90f46d84dc0793bab0a7629f5d391~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909220&auth_key=1760909220-0-0-4ee2da5dca74a01cbf9963545e38eadf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-88e4463b350a13dfd876c873c2229e66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909227&auth_key=1760909227-0-0-cdbd9eba25a949dc4490fb97cbcc2e8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e36e96c097e6fedff5bc7e1ec57dfc00~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909234&auth_key=1760909234-0-0-9bd2b6085fd90be50e7371f095254be4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7f5f134905652cfc5d37d57c217ee61~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909240&auth_key=1760909240-0-0-5d3fe10b66e414cec1c3af6f3084cb26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7239b3a08036f28d666d0f89f51ecac0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909247&auth_key=1760909247-0-0-6fa6e05d5df14f77df02e3c3994291b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NoisePrints-Distortion-Free-Watermarks-for-Authorship-in-Private-Diffusion-Models"><a href="#NoisePrints-Distortion-Free-Watermarks-for-Authorship-in-Private-Diffusion-Models" class="headerlink" title="NoisePrints: Distortion-Free Watermarks for Authorship in Private   Diffusion Models"></a>NoisePrints: Distortion-Free Watermarks for Authorship in Private   Diffusion Models</h2><p><strong>Authors:Nir Goren, Oren Katzir, Abhinav Nakarmi, Eyal Ronen, Mahmood Sharif, Or Patashnik</strong></p>
<p>With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆé¢†åŸŸçš„å¿«é€Ÿé‡‡çº³ï¼Œè¯æ˜ä½œè€…èº«ä»½å’Œä¿æŠ¤ç‰ˆæƒå˜å¾—è‡³å…³é‡è¦ã€‚å½“æ¨¡å‹æ‰€æœ‰è€…å°†å…¶æ¨¡å‹ä¿æŒä¸ºç§æœ‰å¹¶ä¸”å¯èƒ½ä¸æ„¿æ„æˆ–æ— æ³•å¤„ç†ä½œè€…é—®é¢˜æ—¶ï¼Œè¿™ä¸€æŒ‘æˆ˜å°¤ä¸ºé‡è¦ï¼Œè¿™ä½¿å¾—ç¬¬ä¸‰æ–¹éªŒè¯å˜å¾—å¿…ä¸å¯å°‘ã€‚ä¸€ç§è‡ªç„¶çš„è§£å†³æ–¹æ¡ˆæ˜¯åµŒå…¥æ°´å°ä»¥ä¾›æ—¥åéªŒè¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éœ€è¦è®¿é—®æ¨¡å‹æƒé‡å¹¶ä¾èµ–äºè®¡ç®—å¯†é›†å‹çš„ç¨‹åºï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸åˆ‡å®é™…ä¸”æ— æ³•æ‰©å±•ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ°´å°æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨ç”¨äºåˆå§‹åŒ–æ‰©æ•£è¿‡ç¨‹çš„éšæœºç§å­ä½œä¸ºä½œè€…èº«ä»½çš„è¯æ˜ï¼Œè€Œä¸ä¿®æ”¹ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿç»“æœæ˜¯ï¼Œä»ç§å­æ´¾ç”Ÿçš„åˆå§‹å™ªå£°ä¸ç”Ÿæˆçš„è§†è§‰å†…å®¹é«˜åº¦ç›¸å…³ã€‚é€šè¿‡å°†å“ˆå¸Œå‡½æ•°èå…¥å™ªå£°é‡‡æ ·è¿‡ç¨‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç¡®ä¿äº†ä»å†…å®¹ä¸­æ¢å¤æœ‰æ•ˆçš„ç§å­æ˜¯ä¸å¯èƒ½çš„ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†é‡‡æ ·é€šè¿‡éªŒè¯çš„æ›¿ä»£ç§å­æ˜¯ä¸å¯èƒ½çš„ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ“ä½œä¸‹çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é›¶çŸ¥è¯†è¯æ˜æ¥åœ¨ä¸é€éœ²ç§å­çš„æƒ…å†µä¸‹è¯æ˜æ‰€æœ‰æƒã€‚é€šè¿‡ä¿æŒç§å­ç§˜å¯†ï¼Œæˆ‘ä»¬å¢åŠ äº†å»é™¤æ°´å°çš„éš¾åº¦ã€‚æˆ‘ä»¬åœ¨å®éªŒä¸­åœ¨å¤šä¸ªæœ€å…ˆè¿›çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ä¸ŠéªŒè¯äº†NoisePrintsï¼Œè¯æ˜äº†ä»…ä½¿ç”¨ç§å­å’Œè¾“å‡ºè¿›è¡Œé«˜æ•ˆéªŒè¯çš„å¯èƒ½æ€§ï¼Œè€Œæ— éœ€è®¿é—®æ¨¡å‹æƒé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13793v1">PDF</a> code available at: <a target="_blank" rel="noopener" href="https://github.com/nirgoren/NoisePrints">https://github.com/nirgoren/NoisePrints</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰å†…å®¹å¸¦æ¥çš„ç‰ˆæƒå’Œä½œè€…è¯æ˜é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹æ‰€æœ‰è€…é€‰æ‹©ä¿æŒæ¨¡å‹ç§å¯†æ€§è€Œå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ°´å°æ–¹æ¡ˆNoisePrintsã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨æ‰©æ•£è¿‡ç¨‹åˆå§‹åŒ–æ—¶ä½¿ç”¨çš„éšæœºç§å­ä½œä¸ºä½œè€…è¯æ˜ï¼Œæ— éœ€ä¿®æ”¹ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡ç»“åˆå™ªå£°é‡‡æ ·è¿‡ç¨‹ä¸­çš„å“ˆå¸Œå‡½æ•°ï¼Œç¡®ä¿ä»å†…å®¹ä¸­æ¢å¤æœ‰æ•ˆç§å­æ˜¯ä¸å¯èƒ½çš„ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆå±•ç¤ºäº†ä½¿ç”¨é›¶çŸ¥è¯†è¯æ˜è¯æ˜æ‰€æœ‰æƒçš„æ–¹æ³•ï¼Œä¿æŒç§å­çš„ç§˜å¯†æ€§ï¼Œæé«˜äº†æ°´å°ç§»é™¤çš„éš¾åº¦ã€‚å®éªŒè¯æ˜ï¼ŒNoisePrintsåœ¨å¤šä¸ªå…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œä»…ä½¿ç”¨ç§å­å’Œè¾“å‡ºå³å¯å®ç°é«˜æ•ˆéªŒè¯ï¼Œæ— éœ€è®¿é—®æ¨¡å‹æƒé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆä¸­çš„å¿«é€Ÿé‡‡çº³ä½¿å¾—ç‰ˆæƒå’Œä½œè€…è¯æ˜å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>å½“æ¨¡å‹æ‰€æœ‰è€…ä¿æŒæ¨¡å‹ç§å¯†æ—¶ï¼Œç¬¬ä¸‰æ–¹éªŒè¯å˜å¾—å°¤ä¸ºé‡è¦ã€‚</li>
<li>ç°æœ‰æ°´å°æ–¹æ³•éœ€è¦è®¿é—®æ¨¡å‹æƒé‡å¹¶ä¾èµ–è®¡ç®—å¯†é›†å‹ç¨‹åºï¼Œä¸å®ç”¨ä¸”ä¸å¯æ‰©å±•ã€‚</li>
<li>NoisePrintsåˆ©ç”¨æ‰©æ•£è¿‡ç¨‹åˆå§‹åŒ–æ—¶çš„éšæœºç§å­ä½œä¸ºä½œè€…è¯æ˜ï¼Œæ— éœ€ä¿®æ”¹ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>ç»“åˆå™ªå£°é‡‡æ ·ä¸­çš„å“ˆå¸Œå‡½æ•°ï¼Œç¡®ä¿ä»å†…å®¹ä¸­æœ‰æ•ˆæ¢å¤ç§å­æ˜¯ä¸å¯èƒ½çš„ã€‚</li>
<li>ä½¿ç”¨é›¶çŸ¥è¯†è¯æ˜å±•ç¤ºæ‰€æœ‰æƒçš„æ–¹æ³•ï¼Œæé«˜æ°´å°ç§»é™¤çš„éš¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-557027163dd9f69f5ba7af92342c4267~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909255&auth_key=1760909255-0-0-4713e42fe3e6eb9f9749e6755d8e8815&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-688dcae599ed79912fc15af9b502de75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909261&auth_key=1760909261-0-0-7f5fd0e5346c7584cf884b9f9c4b8170&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generating-healthy-counterfactuals-with-denoising-diffusion-bridge-models"><a href="#Generating-healthy-counterfactuals-with-denoising-diffusion-bridge-models" class="headerlink" title="Generating healthy counterfactuals with denoising diffusion bridge   models"></a>Generating healthy counterfactuals with denoising diffusion bridge   models</h2><p><strong>Authors:Ana Lawry Aguila, Peirong Liu, Marina Crespo Aguirre, Juan Eugenio Iglesias</strong></p>
<p>Generating healthy counterfactuals from pathological images holds significant promise in medical imaging, e.g., in anomaly detection or for application of analysis tools that are designed for healthy scans. These counterfactuals should represent what a patientâ€™s scan would plausibly look like in the absence of pathology, preserving individual anatomical characteristics while modifying only the pathological regions. Denoising diffusion probabilistic models (DDPMs) have become popular methods for generating healthy counterfactuals of pathology data. Typically, this involves training on solely healthy data with the assumption that a partial denoising process will be unable to model disease regions and will instead reconstruct a closely matched healthy counterpart. More recent methods have incorporated synthetic pathological images to better guide the diffusion process. However, it remains challenging to guide the generative process in a way that effectively balances the removal of anomalies with the retention of subject-specific features. To solve this problem, we propose a novel application of denoising diffusion bridge models (DDBMs) - which, unlike DDPMs, condition the diffusion process not only on the initial point (i.e., the healthy image), but also on the final point (i.e., a corresponding synthetically generated pathological image). Treating the pathological image as a structurally informative prior enables us to generate counterfactuals that closely match the patientâ€™s anatomy while selectively removing pathology. The results show that our DDBM outperforms previously proposed diffusion models and fully supervised approaches at segmentation and anomaly detection tasks. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œä»ç—…ç†å›¾åƒç”Ÿæˆå¥åº·çš„åäº‹å®ï¼ˆcounterfactualsï¼‰å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¾‹å¦‚åœ¨å¼‚å¸¸æ£€æµ‹æˆ–ä¸ºå¥åº·æ‰«æè®¾è®¡çš„åˆ†æå·¥å…·çš„åº”ç”¨ä¸­ã€‚è¿™äº›åäº‹å®åº”ä»£è¡¨æ‚£è€…åœ¨æ²¡æœ‰ç—…ç†çš„æƒ…å†µä¸‹æ‰«æå¯èƒ½çš„æ ·å­ï¼Œä¿ç•™ä¸ªäººè§£å‰–ç‰¹å¾ï¼ŒåŒæ—¶åªä¿®æ”¹ç—…ç†åŒºåŸŸã€‚å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å·²æˆä¸ºç”Ÿæˆç—…ç†æ•°æ®çš„å¥åº·åäº‹å®æµè¡Œæ–¹æ³•ã€‚é€šå¸¸ï¼Œè¿™æ¶‰åŠä»…åœ¨å¥åº·æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå‡è®¾éƒ¨åˆ†å»å™ªè¿‡ç¨‹æ— æ³•å¯¹ç–¾ç—…åŒºåŸŸè¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ˜¯é‡å»ºä¸€ä¸ªåŒ¹é…ç´§å¯†çš„å¥åº·å¯¹åº”ç‰©ã€‚æœ€è¿‘çš„æ–¹æ³•å·²ç»ç»“åˆäº†åˆæˆç—…ç†å›¾åƒæ¥æ›´å¥½åœ°å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå¦‚ä½•å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å»é™¤å¼‚å¸¸å’Œä¿ç•™ä¸»ä½“ç‰¹å®šç‰¹å¾ä¹‹é—´å–å¾—æœ‰æ•ˆå¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆDDBMsï¼‰çš„æ–°åº”ç”¨â€”â€”ä¸DDPMä¸åŒï¼ŒDDBMçš„æ‰©æ•£è¿‡ç¨‹ä¸ä»…ä»¥åˆå§‹ç‚¹ï¼ˆå³å¥åº·å›¾åƒï¼‰ä¸ºæ¡ä»¶ï¼Œä¹Ÿä»¥æœ€ç»ˆç‚¹ï¼ˆå³ç›¸åº”çš„åˆæˆç—…ç†å›¾åƒï¼‰ä¸ºæ¡ä»¶ã€‚å°†ç—…ç†å›¾åƒè§†ä¸ºç»“æ„ä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆä¸å¥åº·å›¾åƒç´§å¯†åŒ¹é…çš„åäº‹å®ï¼ŒåŒæ—¶æœ‰é€‰æ‹©åœ°å»é™¤ç—…ç†ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DDBMåœ¨åˆ†å‰²å’Œå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºå…ˆå‰æå‡ºçš„æ‰©æ•£æ¨¡å‹å’Œå®Œå…¨ç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13684v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç”Ÿæˆå¥åº·åäº‹å®å›¾åƒï¼ˆcounterfactualsï¼‰åœ¨åŒ»å­¦æˆåƒä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¾‹å¦‚åœ¨å¼‚å¸¸æ£€æµ‹æˆ–ä¸ºå¥åº·æ‰«æè®¾è®¡çš„åˆ†æå·¥å…·çš„åº”ç”¨ä¸­ã€‚è¿™äº›åäº‹å®å›¾åƒåº”ä»£è¡¨åœ¨ä¸å­˜åœ¨ç—…ç†çš„æƒ…å†µä¸‹ï¼Œæ‚£è€…çš„æ‰«æå¯èƒ½å‘ˆç°çš„æ ·å­ï¼Œä¿ç•™ä¸ªä½“è§£å‰–ç‰¹å¾ï¼ŒåŒæ—¶åªä¿®æ”¹ç—…ç†åŒºåŸŸã€‚å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰å·²æˆä¸ºç”Ÿæˆç—…ç†æ•°æ®å¥åº·åäº‹å®çš„å¸¸è§æ–¹æ³•ã€‚é€šå¸¸ï¼Œè¿™æ˜¯é€šè¿‡åœ¨å¥åº·æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå‡è®¾éƒ¨åˆ†å»å™ªè¿‡ç¨‹æ— æ³•æ¨¡æ‹Ÿç–¾ç—…åŒºåŸŸï¼Œè€Œæ˜¯é‡å»ºç´§å¯†åŒ¹é…çš„å¥åº·å¯¹åº”ç‰©ã€‚æœ€è¿‘çš„æ–¹æ³•å·²èå…¥åˆæˆç—…ç†å›¾åƒä»¥æ›´å¥½åœ°å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå¦‚ä½•å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä»¥åœ¨æ¶ˆé™¤å¼‚å¸¸å’Œä¿ç•™ä¸»ä½“ç‰¹å®šç‰¹å¾ä¹‹é—´å–å¾—æœ‰æ•ˆå¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆDDBMï¼‰çš„æ–°åº”ç”¨ï¼Œä¸DDPMä¸åŒï¼ŒDDBMçš„æ‰©æ•£è¿‡ç¨‹ä¸ä»…å—åˆå§‹ç‚¹ï¼ˆå³å¥åº·å›¾åƒï¼‰çš„å½±å“ï¼Œè¿˜å—æœ€ç»ˆç‚¹ï¼ˆå³åˆæˆçš„ç—…ç†å›¾åƒï¼‰çš„å½±å“ã€‚å°†ç—…ç†å›¾åƒè§†ä¸ºç»“æ„ä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆç´§å¯†åŒ¹é…æ‚£è€…è§£å‰–ç»“æ„çš„åäº‹å®å›¾åƒï¼ŒåŒæ—¶æœ‰é€‰æ‹©åœ°æ¶ˆé™¤ç—…ç†ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DDBMåœ¨åˆ†å‰²å’Œå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºå…ˆå‰æå‡ºçš„æ‰©æ•£æ¨¡å‹å’Œå®Œå…¨ç›‘ç£çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”Ÿæˆå¥åº·åäº‹å®å›¾åƒåœ¨åŒ»å­¦æˆåƒä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶åœ¨å¼‚å¸¸æ£€æµ‹å’Œåˆ†æå·¥å…·çš„åº”ç”¨ä¸­ã€‚</li>
<li>å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰å·²ç”¨äºç”Ÿæˆç—…ç†æ•°æ®çš„å¥åº·åäº‹å®å›¾åƒã€‚</li>
<li>è¿‘æœŸæ–¹æ³•ç»“åˆåˆæˆç—…ç†å›¾åƒä»¥æ”¹è¿›æ‰©æ•£è¿‡ç¨‹çš„æŒ‡å¯¼ã€‚</li>
<li>å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä»¥å¹³è¡¡æ¶ˆé™¤å¼‚å¸¸å’Œä¿ç•™ä¸»ä½“ç‰¹å®šç‰¹å¾ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆDDBMï¼‰åº”ç”¨ï¼Œè¯¥æ¨¡å‹å—å¥åº·å›¾åƒå’Œåˆæˆç—…ç†å›¾åƒä¸¤è€…å½±å“ã€‚</li>
<li>DDBMèƒ½å¤Ÿç”Ÿæˆç´§å¯†åŒ¹é…æ‚£è€…è§£å‰–ç»“æ„çš„åäº‹å®å›¾åƒï¼ŒåŒæ—¶æœ‰é€‰æ‹©åœ°æ¶ˆé™¤ç—…ç†ã€‚</li>
<li>DDBMåœ¨åˆ†å‰²å’Œå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ‰©æ•£æ¨¡å‹å’Œå®Œå…¨ç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6e464f33706878e48b3aaf261f32eec6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909269&auth_key=1760909269-0-0-854ab04e6c126644faa9c6a039363082&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c10b14acd76d2738f1d747a422c4a02~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909276&auth_key=1760909276-0-0-331b8765a163ef50456b1f67a0595564&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ceeeaed71bf0795096f0f0c6fb32e0fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909283&auth_key=1760909283-0-0-d2c8b40eb51b715cb2fa92dc978c8aa2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FlashWorld-High-quality-3D-Scene-Generation-within-Seconds"><a href="#FlashWorld-High-quality-3D-Scene-Generation-within-Seconds" class="headerlink" title="FlashWorld: High-quality 3D Scene Generation within Seconds"></a>FlashWorld: High-quality 3D Scene Generation within Seconds</h2><p><strong>Authors:Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, Liujuan Cao</strong></p>
<p>We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100$\times$ faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the modelâ€™s generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FlashWorldï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…ä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆ3Dåœºæ™¯ï¼Œä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†10åˆ°100å€ï¼ŒåŒæ—¶æ‹¥æœ‰å“è¶Šçš„æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¾å¼ƒäº†ä¼ ç»Ÿçš„é¢å‘å¤šè§†è§’ï¼ˆMV-orientedï¼‰çš„èŒƒå¼ï¼Œè¯¥èŒƒå¼ç”Ÿæˆå¤šè§†è§’å›¾åƒç”¨äºéšåçš„3Dé‡å»ºï¼Œè€Œè½¬å‘ä¸€ç§é¢å‘3Dçš„æ–¹æ³•ï¼Œå…¶ä¸­æ¨¡å‹åœ¨ç”Ÿæˆå¤šè§†è§’æ—¶ç›´æ¥äº§ç”Ÿ3Dé«˜æ–¯è¡¨ç¤ºã€‚åœ¨ä¿éšœ3Dä¸€è‡´æ€§çš„åŒæ—¶ï¼Œé¢å‘3Dçš„æ–¹æ³•é€šå¸¸ä¼šå‡ºç°è§†è§‰è´¨é‡å·®çš„é—®é¢˜ã€‚FlashWorldåŒ…æ‹¬åŒæ¨¡å¼é¢„è®­ç»ƒé˜¶æ®µå’Œè·¨æ¨¡å¼åè®­ç»ƒé˜¶æ®µï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†ä¸¤ç§èŒƒå¼çš„ä¼˜ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œé¦–å…ˆé¢„è®­ç»ƒä¸€ä¸ªåŒæ¨¡å¼å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒMV-orientedå’Œ3D-orientedç”Ÿæˆæ¨¡å¼ã€‚ä¸ºäº†ç¼©å°é¢å‘3Dç”Ÿæˆçš„å“è´¨å·®è·ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è·¨æ¨¡å¼åè®­ç»ƒè’¸é¦ï¼Œé€šè¿‡åŒ¹é…ä¸€è‡´çš„é¢å‘3Dæ¨¡å¼çš„åˆ†å¸ƒä¸é«˜è´¨é‡é¢å‘MVæ¨¡å¼çš„åˆ†å¸ƒæ¥å®ç°ã€‚è¿™ä¸ä»…æé«˜äº†è§†è§‰è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†3Dä¸€è‡´æ€§ï¼Œè¿˜å‡å°‘äº†æ¨ç†æ‰€éœ€çš„é™å™ªæ­¥éª¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç­–ç•¥ï¼Œåˆ©ç”¨å¤§é‡çš„å•è§†è§’å›¾åƒå’Œæ–‡æœ¬æç¤ºåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ç¦»ç¾¤è¾“å…¥çš„ä¸€èˆ¬åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13678v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://imlixinyang.github.io/FlashWorld-Project-Page/">https://imlixinyang.github.io/FlashWorld-Project-Page/</a></p>
<p><strong>Summary</strong><br>    FlashWorldæ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œèƒ½ä»å•å¼ å›¾ç‰‡æˆ–æ–‡æœ¬æç¤ºåœ¨æ•°ç§’å†…ç”Ÿæˆ3Dåœºæ™¯ï¼Œè¾ƒä¹‹å‰çš„å·¥ä½œå¿«10~100å€ï¼ŒåŒæ—¶æ‹¥æœ‰å“è¶Šçš„æ¸²æŸ“è´¨é‡ã€‚å®ƒé‡‡ç”¨ç›´æ¥ç”Ÿæˆ3Dé«˜æ–¯è¡¨ç¤ºçš„3Då¯¼å‘æ–¹æ³•ï¼Œä¿è¯3Dä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå…‹æœäº†ä¼ ç»Ÿå¤šè§†è§’å¯¼å‘æ–¹æ³•è§†è§‰è´¨é‡å·®çš„ç¼ºé™·ã€‚é€šè¿‡åŒæ¨¡å¼é¢„è®­ç»ƒä¸è·¨æ¨¡å¼åè®­ç»ƒï¼Œç»“åˆäº†ä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿ã€‚å€ŸåŠ©è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ï¼Œè¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºè¿›ä¸€æ­¥ç¼©å°3Då¯¼å‘ç”Ÿæˆçš„è´¨é‡å·®è·ï¼Œæå‡ºè·¨æ¨¡å¼åè®­ç»ƒè’¸é¦æ–¹æ³•ã€‚è¿™ä¸ä»…æé«˜äº†è§†è§‰è´¨é‡ï¼Œä¿æŒäº†3Dä¸€è‡´æ€§ï¼Œè¿˜å‡å°‘äº†æ¨ç†æ‰€éœ€çš„å»å™ªæ­¥éª¤ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨å¤§é‡å•è§†è§’å›¾åƒå’Œæ–‡æœ¬æç¤ºå¢å¼ºæ¨¡å‹å¯¹ç¦»ç¾¤è¾“å…¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlashWorldæ˜¯ä¸€ä¸ªå¿«é€Ÿç”Ÿæˆ3Dåœºæ™¯çš„ç”Ÿæˆæ¨¡å‹ï¼Œè¾ƒä¼ ç»Ÿæ–¹æ³•å¤§å¤§æå‡äº†ç”Ÿæˆé€Ÿåº¦ã€‚</li>
<li>å®ƒé‡‡ç”¨äº†ç›´æ¥çš„3Dé«˜æ–¯è¡¨ç¤ºç”Ÿæˆæ–¹æ³•ï¼Œä¿è¯äº†3Dåœºæ™¯çš„ä¸€è‡´æ€§ã€‚</li>
<li>FlashWorldé€šè¿‡åŒæ¨¡å¼é¢„è®­ç»ƒå’Œè·¨æ¨¡å¼åè®­ç»ƒï¼Œèåˆäº†å¤šè§†è§’å¯¼å‘å’Œ3Då¯¼å‘æ–¹æ³•çš„ä¼˜ç‚¹ã€‚</li>
<li>æ¨¡å‹å€ŸåŠ©è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯è¿›è¡Œé¢„è®­ç»ƒï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>è·¨æ¨¡å¼åè®­ç»ƒè’¸é¦æ–¹æ³•ç”¨äºæé«˜3Då¯¼å‘ç”Ÿæˆçš„è§†è§‰è´¨é‡ï¼Œå¹¶å‡å°‘äº†å»å™ªæ­¥éª¤ã€‚</li>
<li>FlashWorldèƒ½å¤Ÿåˆ©ç”¨å¤§é‡å•è§†è§’å›¾åƒå’Œæ–‡æœ¬æç¤ºè¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b8648f29ff4a7bcb56061e6ac0ed6ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909290&auth_key=1760909290-0-0-f990bf1345f754961267fb319fdca1f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa80452a267f2ee0d95772aec293bfec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909298&auth_key=1760909298-0-0-ba68afeabb2ca2d7782cf396345ec634&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d53d318021376249b9b3f6dc1fe49db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909304&auth_key=1760909304-0-0-ddd93bb965fd2c4cadcc956d39c64aab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c01b4e27a21b244bf374ce629303cba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909311&auth_key=1760909311-0-0-1d32378a8bb4ec667edcda33ab2f656b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Steerable-Conditional-Diffusion-for-Domain-Adaptation-in-PET-Image-Reconstruction"><a href="#Steerable-Conditional-Diffusion-for-Domain-Adaptation-in-PET-Image-Reconstruction" class="headerlink" title="Steerable Conditional Diffusion for Domain Adaptation in PET Image   Reconstruction"></a>Steerable Conditional Diffusion for Domain Adaptation in PET Image   Reconstruction</h2><p><strong>Authors:George Webber, Alexander Hammers, Andrew P. King, Andrew J. Reader</strong></p>
<p>Diffusion models have recently enabled state-of-the-art reconstruction of positron emission tomography (PET) images while requiring only image training data. However, domain shift remains a key concern for clinical adoption: priors trained on images from one anatomy, acquisition protocol or pathology may produce artefacts on out-of-distribution data. We propose integrating steerable conditional diffusion (SCD) with our previously-introduced likelihood-scheduled diffusion (PET-LiSch) framework to improve the alignment of the diffusion modelâ€™s prior to the target subject. At reconstruction time, for each diffusion step, we use low-rank adaptation (LoRA) to align the diffusion model prior with the target domain on the fly. Experiments on realistic synthetic 2D brain phantoms demonstrate that our approach suppresses hallucinated artefacts under domain shift, i.e. when our diffusion model is trained on perturbed images and tested on normal anatomy, our approach suppresses the hallucinated structure, outperforming both OSEM and diffusion model baselines qualitatively and quantitatively. These results provide a proof-of-concept that steerable priors can mitigate domain shift in diffusion-based PET reconstruction and motivate future evaluation on real data. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æœ€è¿‘ä»…ä½¿ç”¨å›¾åƒè®­ç»ƒæ•°æ®å°±å®ç°äº†æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒçš„å…ˆè¿›é‡å»ºã€‚ç„¶è€Œï¼Œé¢†åŸŸåç§»ä»ç„¶æ˜¯ä¸´åºŠé‡‡ç”¨çš„å…³é”®é—®é¢˜ï¼šåœ¨ä¸€ç§è§£å‰–å­¦ã€é‡‡é›†åè®®æˆ–ç—…ç†å­¦å›¾åƒä¸Šè®­ç»ƒçš„å…ˆéªŒå¯èƒ½ä¼šåœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ•°æ®ä¸Šäº§ç”Ÿä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºå°†å¯æ§åˆ¶æ¡ä»¶æ‰©æ•£ï¼ˆSCDï¼‰ä¸æˆ‘ä»¬å…ˆå‰å¼•å…¥çš„ä¼¼ç„¶è°ƒåº¦æ‰©æ•£ï¼ˆPET-LiSchï¼‰æ¡†æ¶ç›¸ç»“åˆï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡ä¸»é¢˜çš„å¯¹é½ç¨‹åº¦ã€‚åœ¨é‡å»ºè¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ªæ‰©æ•£æ­¥éª¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¥å®æ—¶è°ƒæ•´æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡åŸŸçš„å¯¹é½ã€‚åœ¨çœŸå®åˆæˆäºŒç»´è„‘å¹»å½±ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢†åŸŸåç§»æ—¶æŠ‘åˆ¶äº†å¹»è§‰ä¼ªå½±ï¼Œå³å½“æˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹åœ¨å—å¹²æ‰°çš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒå¹¶åœ¨æ­£å¸¸è§£å‰–å­¦ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŠ‘åˆ¶äº†å¹»è§‰ç»“æ„ï¼Œåœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½ä¼˜äºOSEMå’Œæ‰©æ•£æ¨¡å‹åŸºçº¿ã€‚è¿™äº›ç»“æœè¯æ˜äº†å¯æ§åˆ¶å…ˆéªŒå¯ä»¥å‡è½»æ‰©æ•£å¼PETé‡å»ºä¸­çš„é¢†åŸŸåç§»ï¼Œå¹¶æ¿€åŠ±æˆ‘ä»¬åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œæœªæ¥è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13441v1">PDF</a> Accepted for oral presentation at IEEE NSS MIC RTSD 2025 (submitted   May 2025; accepted July 2025; to be presented Nov 2025)</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹é€šè¿‡å›¾åƒè®­ç»ƒæ•°æ®å®ç°äº†æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒçš„é‡æ„æŠ€æœ¯å‰æ²¿ã€‚é’ˆå¯¹ä¸´åºŠåº”ç”¨ä¸­å­˜åœ¨çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†å¯æ§åˆ¶çš„æ¡ä»¶æ‰©æ•£ä¸å…ˆå‰å¼•å…¥çš„ä¼¼ç„¶è°ƒåº¦æ‰©æ•£æ¡†æ¶ç›¸ç»“åˆï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡ä¸»ä½“ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚åœ¨é‡æ„è¿‡ç¨‹ä¸­ï¼Œé’ˆå¯¹æ¯ä¸ªæ‰©æ•£æ­¥éª¤ï¼Œæˆ‘ä»¬é‡‡ç”¨ä½é˜¶é€‚é…æŠ€æœ¯ï¼Œä»¥å³æ—¶å¯¹é½ç›®æ ‡åŸŸçš„æ‰©æ•£æ¨¡å‹å…ˆéªŒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢†åŸŸåç§»æƒ…å†µä¸‹æŠ‘åˆ¶äº†å¹»è§‰ä¼ªå½±çš„äº§ç”Ÿï¼Œå³å½“æ‰©æ•£æ¨¡å‹åœ¨æ‰°åŠ¨å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒå¹¶åœ¨æ­£å¸¸è§£å‰–ç»“æ„ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤ŸæŠ‘åˆ¶å¹»è§‰ç»“æ„ï¼Œåœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºOSEMå’Œæ‰©æ•£æ¨¡å‹åŸºçº¿ã€‚è¿™ä¸ºå¯æ§åˆ¶å…ˆéªŒåœ¨æ‰©æ•£å¼PETé‡å»ºä¸­ç¼“è§£é¢†åŸŸåç§»é—®é¢˜æä¾›äº†æ¦‚å¿µéªŒè¯ï¼Œå¹¶æ¿€åŠ±æˆ‘ä»¬åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œæœªæ¥è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²ç”¨äºPETå›¾åƒé‡æ„ï¼Œä»…ä¾èµ–å›¾åƒè®­ç»ƒæ•°æ®ã€‚</li>
<li>é¢†åŸŸåç§»æ˜¯ä¸´åºŠé‡‡çº³çš„å…³é”®é—®é¢˜ï¼Œç°æœ‰æ¨¡å‹çš„å…ˆéªŒå¯èƒ½åœ¨æ–°æ•°æ®ä¸Šäº§ç”Ÿä¼ªå½±ã€‚</li>
<li>æå‡ºç»“åˆå¯æ§åˆ¶çš„æ¡ä»¶æ‰©æ•£å’Œä¼¼ç„¶è°ƒåº¦æ‰©æ•£æ¡†æ¶ï¼Œæé«˜æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡ä¸»ä½“é—´çš„å¯¹é½ã€‚</li>
<li>åœ¨é‡æ„è¿‡ç¨‹ä¸­é‡‡ç”¨ä½é˜¶é€‚é…æŠ€æœ¯ï¼Œå³æ—¶å¯¹é½ç›®æ ‡åŸŸã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œåœ¨é¢†åŸŸåç§»æƒ…å†µä¸‹ï¼Œæ–°æ–¹æ³•èƒ½æŠ‘åˆ¶å¹»è§‰ä¼ªå½±ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸Šå‡è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-daae8d2807475670baa395404ff48ad5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909318&auth_key=1760909318-0-0-23e37175ed87908a22a161f648458314&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f06a91f46e3357e1a8bc83128d90f1f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909326&auth_key=1760909326-0-0-0a401634d3d3d49dcbfcf10e93271f6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Ultra-High-Resolution-Image-Inpainting-with-Patch-Based-Content-Consistency-Adapter"><a href="#Ultra-High-Resolution-Image-Inpainting-with-Patch-Based-Content-Consistency-Adapter" class="headerlink" title="Ultra High-Resolution Image Inpainting with Patch-Based Content   Consistency Adapter"></a>Ultra High-Resolution Image Inpainting with Patch-Based Content   Consistency Adapter</h2><p><strong>Authors:Jianhui Zhang, Sheng Cheng, Qirui Sun, Jia Liu, Wang Luyang, Chaoyu Feng, Chen Fang, Lei Lei, Jue Wang, Shuaicheng Liu</strong></p>
<p>In this work, we present Patch-Adapter, an effective framework for high-resolution text-guided image inpainting. Unlike existing methods limited to lower resolutions, our approach achieves 4K+ resolution while maintaining precise content consistency and prompt alignment, two critical challenges in image inpainting that intensify with increasing resolution and texture complexity. Patch-Adapter leverages a two-stage adapter architecture to scale the diffusion modelâ€™s resolution from 1K to 4K+ without requiring structural overhauls: (1) Dual Context Adapter learns coherence between masked and unmasked regions at reduced resolutions to establish global structural consistency; and (2) Reference Patch Adapter implements a patch-level attention mechanism for full-resolution inpainting, preserving local detail fidelity through adaptive feature fusion. This dual-stage architecture uniquely addresses the scalability gap in high-resolution inpainting by decoupling global semantics from localized refinement. Experiments demonstrate that Patch-Adapter not only resolves artifacts common in large-scale inpainting but also achieves state-of-the-art performance on the OpenImages and Photo-Concept-Bucket datasets, outperforming existing methods in both perceptual quality and text-prompt adherence. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Patch-Adapterï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜åˆ†è¾¨ç‡æ–‡æœ¬å¼•å¯¼çš„å›¾åƒè¡¥å…¨çš„æœ‰æ•ˆçš„æ¡†æ¶ã€‚ä¸åŒäºç°æœ‰ä»…é™äºè¾ƒä½åˆ†è¾¨ç‡çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†4K+åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç²¾ç¡®çš„å†…å®¹ä¸€è‡´æ€§å’Œæç¤ºå¯¹é½ï¼Œè¿™æ˜¯å›¾åƒè¡¥å…¨ä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œéšç€åˆ†è¾¨ç‡å’Œçº¹ç†å¤æ‚æ€§çš„å¢åŠ ï¼Œè¿™ä¸¤ä¸ªæŒ‘æˆ˜ä¼šåŠ å‰§ã€‚Patch-Adapteråˆ©ç”¨ä¸¤é˜¶æ®µé€‚é…å™¨æ¶æ„ï¼Œæ— éœ€è¿›è¡Œç»“æ„æ€§çš„å¤§è§„æ¨¡æ”¹é€ ï¼Œå³å¯å°†æ‰©æ•£æ¨¡å‹çš„åˆ†è¾¨ç‡ä»1Kæ‰©å±•åˆ°4K+ï¼šï¼ˆ1ï¼‰åŒä¸Šä¸‹æ–‡é€‚é…å™¨åœ¨é™ä½çš„åˆ†è¾¨ç‡ä¸Šå­¦ä¹ æ©ç åŒºåŸŸå’Œéæ©ç åŒºåŸŸä¹‹é—´çš„è¿è´¯æ€§ï¼Œä»¥å»ºç«‹å…¨å±€ç»“æ„ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰å‚è€ƒè¡¥ä¸é€‚é…å™¨å®ç°äº†è¡¥ä¸çº§åˆ«çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå…¨åˆ†è¾¨ç‡è¡¥å…¨ï¼Œé€šè¿‡è‡ªé€‚åº”ç‰¹å¾èåˆä¿ç•™å±€éƒ¨ç»†èŠ‚ä¿çœŸåº¦ã€‚è¿™ç§åŒé˜¶æ®µæ¶æ„é€šè¿‡è§£è€¦å…¨å±€è¯­ä¹‰å’Œå±€éƒ¨ç»†åŒ–ï¼Œç‹¬ç‰¹åœ°è§£å†³äº†é«˜åˆ†è¾¨ç‡è¡¥å…¨ä¸­çš„å¯æ‰©å±•æ€§å·®è·ã€‚å®éªŒè¡¨æ˜ï¼ŒPatch-Adapterä¸ä»…è§£å†³äº†å¤§è§„æ¨¡è¡¥å…¨ä¸­å¸¸è§çš„ä¼ªå½±é—®é¢˜ï¼Œè¿˜åœ¨OpenImageså’ŒPhoto-Concept-Bucketæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡å’Œæ–‡æœ¬æç¤ºéµå¾ªæ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Patch-Adapteræ¡†æ¶ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡æ–‡æœ¬å¼•å¯¼çš„å›¾åƒè¡¥å…¨ã€‚è¯¥æ¡†æ¶å®ç°äº†4K+åˆ†è¾¨ç‡çš„å›¾åƒè¡¥å…¨ï¼Œç»´æŒäº†å†…å®¹çš„ä¸€è‡´æ€§å’Œæ–‡æœ¬æç¤ºçš„å¯¹é½ï¼Œè§£å†³äº†éšç€åˆ†è¾¨ç‡å’Œçº¹ç†å¤æ‚åº¦å¢åŠ è€ŒåŠ å‰§çš„å›¾åƒè¡¥å…¨ä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚Patch-Adapteråˆ©ç”¨ä¸¤é˜¶æ®µé€‚é…å™¨æ¶æ„ï¼Œæ— éœ€ç»“æ„å¤§æ”¹ï¼Œå³å¯å°†æ‰©æ•£æ¨¡å‹çš„åˆ†è¾¨ç‡ä»1Kæ‰©å±•åˆ°4K+ã€‚å®éªŒè¡¨æ˜ï¼ŒPatch-Adapterä¸ä»…è§£å†³äº†å¤§è§„æ¨¡è¡¥å…¨ä¸­çš„å¸¸è§ä¼ªå½±é—®é¢˜ï¼Œè¿˜åœ¨OpenImageså’ŒPhoto-Concept-Bucketæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡å’Œæ–‡æœ¬æç¤ºéµå¾ªæ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Patch-Adapteræ˜¯ä¸€ä¸ªç”¨äºé«˜åˆ†è¾¨ç‡æ–‡æœ¬å¼•å¯¼å›¾åƒè¡¥å…¨çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†4K+åˆ†è¾¨ç‡çš„å›¾åƒè¡¥å…¨ã€‚</li>
<li>Patch-Adapterè§£å†³äº†å†…å®¹ä¸€è‡´æ€§å’Œæ–‡æœ¬æç¤ºå¯¹é½è¿™ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µé€‚é…å™¨æ¶æ„ï¼Œä»1Kæ‰©å±•åˆ°4K+åˆ†è¾¨ç‡ï¼Œæ— éœ€å¤§è§„æ¨¡çš„ç»“æ„è°ƒæ•´ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µæ˜¯åŒè¯­å¢ƒé€‚é…å™¨ï¼Œæ—¨åœ¨å»ºç«‹å…¨å±€ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µæ˜¯å‚è€ƒè¡¥ä¸é€‚é…å™¨ï¼Œé‡‡ç”¨è¡¥ä¸çº§åˆ«çš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå…¨åˆ†è¾¨ç‡è¡¥å…¨ï¼Œä¿ç•™å±€éƒ¨ç»†èŠ‚ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-81fec26a580d6579e0b0a24ca05cb549~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909333&auth_key=1760909333-0-0-bc193def1a06eab32ab6bc150ab85b7f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35202a78b34246c73ab79fae19f4520e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909340&auth_key=1760909340-0-0-d7822c31719ca0271bfd092ebc22c18a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a346831efba42b70ad55291fec58a85~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909348&auth_key=1760909348-0-0-577b2a563159fbbdface31dde0f088e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-feeb29cbf5e38c2262ae24a6b532ceee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909355&auth_key=1760909355-0-0-628c649eb728560282e4aa8f1d128d89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SceneTextStylizer-A-Training-Free-Scene-Text-Style-Transfer-Framework-with-Diffusion-Model"><a href="#SceneTextStylizer-A-Training-Free-Scene-Text-Style-Transfer-Framework-with-Diffusion-Model" class="headerlink" title="SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework   with Diffusion Model"></a>SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework   with Diffusion Model</h2><p><strong>Authors:Honghui Yuan, Keiji Yanai</strong></p>
<p>With the rapid development of diffusion models, style transfer has made remarkable progress. However, flexible and localized style editing for scene text remains an unsolved challenge. Although existing scene text editing methods have achieved text region editing, they are typically limited to content replacement and simple styles, which lack the ability of free-style transfer. In this paper, we introduce SceneTextStylizer, a novel training-free diffusion-based framework for flexible and high-fidelity style transfer of text in scene images. Unlike prior approaches that either perform global style transfer or focus solely on textual content modification, our method enables prompt-guided style transformation specifically for text regions, while preserving both text readability and stylistic consistency. To achieve this, we design a feature injection module that leverages diffusion model inversion and self-attention to transfer style features effectively. Additionally, a region control mechanism is introduced by applying a distance-based changing mask at each denoising step, enabling precise spatial control. To further enhance visual quality, we incorporate a style enhancement module based on the Fourier transform to reinforce stylistic richness. Extensive experiments demonstrate that our method achieves superior performance in scene text style transformation, outperforming existing state-of-the-art methods in both visual fidelity and text preservation. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œé£æ ¼è½¬æ¢å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œåœºæ™¯æ–‡æœ¬çš„å¯çµæ´»æ€§å’Œæœ¬åœ°åŒ–çš„é£æ ¼ç¼–è¾‘ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„éš¾é¢˜ã€‚è™½ç„¶ç°æœ‰çš„åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ–¹æ³•å·²ç»å®ç°äº†æ–‡æœ¬åŒºåŸŸçš„ç¼–è¾‘ï¼Œä½†å®ƒä»¬é€šå¸¸ä»…é™äºå†…å®¹æ›¿æ¢å’Œç®€å•é£æ ¼ï¼Œç¼ºä¹è‡ªç”±é£æ ¼è½¬æ¢çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SceneTextStylizerï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºåœºæ™¯å›¾åƒä¸­æ–‡æœ¬çš„çµæ´»å’Œé«˜ä¿çœŸé£æ ¼è½¬æ¢ã€‚ä¸åŒäºä¹‹å‰çš„å…¨å±€é£æ ¼è½¬æ¢æ–¹æ³•æˆ–ä»…ä¸“æ³¨äºæ–‡æœ¬å†…å®¹ä¿®æ”¹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æç¤ºå¼•å¯¼è¿›è¡Œæ–‡æœ¬åŒºåŸŸçš„é£æ ¼è½¬æ¢ï¼ŒåŒæ—¶ä¿ç•™æ–‡æœ¬çš„å¯è¯»æ€§å’Œé£æ ¼çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹å¾æ³¨å…¥æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨æ‰©æ•£æ¨¡å‹åæ¼”å’Œè‡ªæˆ‘æ³¨æ„æ¥æœ‰æ•ˆåœ°è½¬ç§»é£æ ¼ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­åº”ç”¨åŸºäºè·ç¦»çš„å˜åŒ–æ©è†œï¼Œå¼•å…¥äº†åŒºåŸŸæ§åˆ¶æœºåˆ¶ï¼Œå®ç°äº†ç²¾ç¡®çš„ç©ºé—´æ§åˆ¶ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è§†è§‰è´¨é‡ï¼Œæˆ‘ä»¬ç»“åˆåŸºäºå‚…é‡Œå¶å˜æ¢çš„é£æ ¼å¢å¼ºæ¨¡å—ï¼Œä»¥å¢å¼ºé£æ ¼çš„ä¸°å¯Œæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åœºæ™¯æ–‡æœ¬é£æ ¼è½¬æ¢æ–¹é¢è¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œæ–‡æœ¬ä¿ç•™æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10910v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œåœºæ™¯æ–‡æœ¬çš„é£æ ¼è½¬æ¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå®ç°çµæ´»ä¸”å±€éƒ¨åŒ–çš„æ–‡æœ¬é£æ ¼ç¼–è¾‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åŸºäºæ‰©æ•£çš„åœºæ™¯æ–‡æœ¬é£æ ¼è½¬æ¢å™¨ï¼ˆSceneTextStylizerï¼‰ï¼Œèƒ½å¤Ÿåœ¨åœºæ™¯å›¾åƒä¸­å®ç°çµæ´»ä¸”é«˜ä¿çœŸåº¦çš„æ–‡æœ¬é£æ ¼è½¬æ¢ã€‚ä¸åŒäºç°æœ‰çš„å…¨å±€é£æ ¼è½¬æ¢æˆ–ä»…å…³æ³¨æ–‡æœ¬å†…å®¹ä¿®æ”¹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æç¤ºå¼•å¯¼è¿›è¡Œæ–‡æœ¬åŒºåŸŸçš„ç‰¹å®šé£æ ¼è½¬æ¢ï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬å¯è¯»æ€§å’Œé£æ ¼ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†åœºæ™¯æ–‡æœ¬é£æ ¼è½¬æ¢çš„è¿›æ­¥ã€‚</li>
<li>å½“å‰åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ–¹æ³•ä¸»è¦å±€é™äºå†…å®¹æ›¿æ¢å’Œç®€å•é£æ ¼ï¼Œç¼ºä¹è‡ªç”±é£æ ¼è½¬æ¢èƒ½åŠ›ã€‚</li>
<li>SceneTextStylizeræ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œå¯å®ç°åœºæ™¯æ–‡æœ¬çš„é«˜ä¿çœŸé£æ ¼è½¬æ¢ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾æ³¨å…¥æ¨¡å—å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°é£æ ¼ç‰¹å¾çš„è½¬ç§»ã€‚</li>
<li>é€šè¿‡åº”ç”¨åŸºäºè·ç¦»å˜åŒ–çš„é®æŒ¡æ©è†œï¼Œå®ç°äº†å¯¹æ–‡æœ¬åŒºåŸŸçš„ç²¾ç¡®ç©ºé—´æ§åˆ¶ã€‚</li>
<li>å¼•å…¥åŸºäºå‚…é‡Œå¶å˜æ¢çš„é£æ ¼å¢å¼ºæ¨¡å—ï¼Œæé«˜äº†é£æ ¼ä¸°å¯Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8d8d611f0692fff0a4e1cee75b4e7b6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909362&auth_key=1760909362-0-0-2e68f4b74fff962adc5c6bcb01cb3481&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4acc1fca65bb8a844cd958d0cac3eb9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909369&auth_key=1760909369-0-0-141c202ecbb92c40fca5942b03959614&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-569a04139613b84db2c8126f2ea13290~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909376&auth_key=1760909376-0-0-09b28f7291f645d86b258c483087fa85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f4fce91b288fe5b0a738602ff172836~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909383&auth_key=1760909383-0-0-c55792f1dbfbeb1290c2db0563157097&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="One-Stone-with-Two-Birds-A-Null-Text-Null-Frequency-Aware-Diffusion-Models-for-Text-Guided-Image-Inpainting"><a href="#One-Stone-with-Two-Birds-A-Null-Text-Null-Frequency-Aware-Diffusion-Models-for-Text-Guided-Image-Inpainting" class="headerlink" title="One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion   Models for Text-Guided Image Inpainting"></a>One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion   Models for Text-Guided Image Inpainting</h2><p><strong>Authors:Haipeng Liu, Yang Wang, Meng Wang</strong></p>
<p>Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from <a target="_blank" rel="noopener" href="https://github.com/htyjers/NTN-Diff">https://github.com/htyjers/NTN-Diff</a>. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤æ—¨åœ¨æ ¹æ®æ–‡æœ¬æç¤ºé‡å»ºæ©ç åŒºåŸŸï¼Œé•¿æœŸä»¥æ¥çš„æŒ‘æˆ˜åœ¨äºä¿æŒæœªæ©ç åŒºåŸŸçš„å®Œæ•´æ€§ï¼ŒåŒæ—¶å®ç°æœªæ©ç å’Œä¿®å¤æ©ç åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä»¥å‰çš„æ–¹æ³•æœªèƒ½åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œé€šå¸¸åªèƒ½è§£å†³å…¶ä¸­ä¹‹ä¸€ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™ç§æƒ…å†µæºäºæ··åˆï¼ˆä¾‹å¦‚ä¸­ä½é¢‘ï¼‰é¢‘å¸¦çº ç¼ ï¼Œè¿™äº›é¢‘å¸¦ç¼–ç äº†ä¸åŒçš„å›¾åƒå±æ€§ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­å¯¹æ–‡æœ¬æç¤ºçš„æ•æ„Ÿæ€§ä¸åŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤çš„é›¶æ–‡æœ¬é›¶é¢‘ç‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸ºNTN-Diffã€‚å®ƒé€šè¿‡åˆ†è§£æ©ç å’Œæœªæ©ç åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œé’ˆå¯¹æ¯ä¸ªé¢‘å¸¦ä¿æŒæœªæ©ç åŒºåŸŸçš„å®Œæ•´æ€§ï¼Œä»¥å…‹æœè¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚åŸºäºæ‰©æ•£è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†å»å™ªè¿‡ç¨‹è¿›ä¸€æ­¥åˆ†ä¸ºæ—©æœŸï¼ˆé«˜çº§å™ªå£°ï¼‰å’Œæ™šæœŸï¼ˆä½çº§å™ªå£°ï¼‰é˜¶æ®µï¼Œå…¶ä¸­åœ¨å»å™ªè¿‡ç¨‹ä¸­è§£å¼€ä¸­ä½é¢‘å¸¦ã€‚è§‚å¯Ÿåˆ°ç¨³å®šçš„ä¸­é¢‘å¸¦åœ¨æ–‡æœ¬å¼•å¯¼çš„å»å™ªè¿‡ç¨‹ä¸­é€æ­¥é™å™ªä»¥å®ç°è¯­ä¹‰å¯¹é½ï¼ŒåŒæ—¶ä½œä¸ºé›¶æ–‡æœ¬å»å™ªè¿‡ç¨‹çš„æŒ‡å¯¼æ¥é™å™ªä½é¢‘å¸¦åœ¨æ©ç åŒºåŸŸã€‚éšååœ¨æ™šæœŸé˜¶æ®µè¿›è¡Œåç»­æ–‡æœ¬å¼•å¯¼çš„å»å™ªè¿‡ç¨‹ï¼Œä»¥å®ç°æ©ç å’Œæœªæ©ç åŒºåŸŸä¹‹é—´çš„ä¸­ä½é¢‘å¸¦çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒæœªæ©ç åŒºåŸŸçš„å®Œæ•´æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†NTN-Diffåœ¨æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹æ–¹é¢çš„ä¼˜è¶Šæ€§è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/htyjers/NTN-Diff%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/htyjers/NTN-Diffè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08273v4">PDF</a> 27 pages, 11 figures, to appear at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºNTN-Diffçš„é¢‘ç‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ†è§£æ©ç åŒºåŸŸå’Œéæ©ç åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œé’ˆå¯¹æ¯ä¸ªé¢‘ç‡èŒƒå›´è¿›è¡Œä¸€è‡´æ€§çš„è®­ç»ƒï¼Œä»¥æ­¤è§£å†³æ©ç åŒºåŸŸçš„é‡å»ºå’Œéæ©ç åŒºåŸŸçš„ä¿ç•™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚æ¨¡å‹åœ¨é™å™ªè¿‡ç¨‹ä¸­å°†å›¾åƒåˆ†ä¸ºæ—©æœŸå’Œæ™šæœŸä¸¤ä¸ªé˜¶æ®µï¼Œé€šè¿‡æ–‡æœ¬å¼•å¯¼é™å™ªå¤„ç†ä¸­é¢‘åŒºåŸŸï¼Œä»¥å®ç°å¯¹éæ©ç åŒºåŸŸçš„æŒ‡å¯¼å¹¶ä¿æŒä¸€è‡´æ€§ã€‚æœ€ç»ˆå®ç°äº†ä¸æœ€æ–°æ‰©æ•£æ¨¡å‹ç›¸æ¯”æ›´ä¸ºä¼˜è¶Šçš„æ–‡æœ¬å¼•å¯¼å›¾åƒä¿®å¤æ€§èƒ½ã€‚æœ‰å…³ä»£ç å¯è®¿é—®é“¾æ¥ï¼š[è®¿é—®é“¾æ¥]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢‘ç‡æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹NTN-Diffç”¨äºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¤„ç†æ¯ä¸ªé¢‘ç‡å¸¦ä»¥å®ç°æ©ç å’Œéæ©ç åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>æ¨¡å‹è§£å†³äº†åœ¨å›¾åƒä¿®å¤è¿‡ç¨‹ä¸­ä¿æŒéæ©ç åŒºåŸŸçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å°†é™å™ªè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼ˆæ—©æœŸå’Œæ™šæœŸï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†æ–‡æœ¬å¼•å¯¼ä¸‹çš„é™å™ªè¿‡ç¨‹ã€‚</li>
<li>ä¸­é¢‘åŒºåŸŸçš„ç¨³å®šå¤„ç†ä½œä¸ºéæ©ç åŒºåŸŸçš„æŒ‡å¯¼ï¼Œç¡®ä¿äº†æ©ç åŒºåŸŸçš„ä¸€è‡´æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-77ab6a50b8ebae77df5e1360d5a332e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909390&auth_key=1760909390-0-0-95b4a3acf1d2ae61e887f42444bb80ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac6c5c0957151008bca8310ad194fe14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909397&auth_key=1760909397-0-0-2dafe3bc71402fd0b23df7fcdfa01ecd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a95c749b1d6b09894cf42c9781b335d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909403&auth_key=1760909403-0-0-f07e32411b8bc421e1b2a2e4136b1e85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c847f25fd14599d3c9e8563c8b452bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909410&auth_key=1760909410-0-0-0459a98d5cbb57cbf17f11ad01d3713e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8c43ee5ef7742e2384e977ff0101c0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909417&auth_key=1760909417-0-0-28d999f8fcd059c32d283b729468041b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Real-Time-Motion-Controllable-Autoregressive-Video-Diffusion"><a href="#Real-Time-Motion-Controllable-Autoregressive-Video-Diffusion" class="headerlink" title="Real-Time Motion-Controllable Autoregressive Video Diffusion"></a>Real-Time Motion-Controllable Autoregressive Video Diffusion</h2><p><strong>Authors:Kesen Zhao, Jiaxin Shi, Beier Zhu, Junbao Zhou, Xiaolong Shen, Yuan Zhou, Qianru Sun, Hanwang Zhang</strong></p>
<p>Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: <a target="_blank" rel="noopener" href="https://kesenzhao.github.io/AR-Drag.github.io/">https://kesenzhao.github.io/AR-Drag.github.io/</a>. </p>
<blockquote>
<p>å®æ—¶è¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºåŒå‘æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰å»¶è¿Ÿå’Œç¼ºä¹æœ‰æ•ˆçš„è‡ªå›å½’ï¼ˆARï¼‰æ–¹æ³•ã€‚ç°æœ‰çš„ARè§†é¢‘æ‰©æ•£æ¨¡å‹ä»…é™äºç®€å•çš„æ§åˆ¶ä¿¡å·æˆ–æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆï¼Œå¹¶ä¸”åœ¨å°‘æ­¥éª¤ç”Ÿæˆä¸­ç»å¸¸é­å—è´¨é‡ä¸‹é™å’Œè¿åŠ¨ä¼ªå½±çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AR-Dragï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ çš„å°‘æ•°æ­¥éª¤ARè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå®æ—¶å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆå…·æœ‰å¤šç§è¿åŠ¨æ§åˆ¶ã€‚æˆ‘ä»¬é¦–å…ˆå¾®è°ƒåŸºç¡€I2Væ¨¡å‹ä»¥æ”¯æŒåŸºæœ¬è¿åŠ¨æ§åˆ¶ï¼Œç„¶åé€šè¿‡åŸºäºè½¨è¿¹çš„å¥–åŠ±æ¨¡å‹è¿›ä¸€æ­¥æ”¹è¿›ã€‚æˆ‘ä»¬çš„è®¾è®¡é€šè¿‡è‡ªæˆ‘æ»šåŠ¨æœºåˆ¶ä¿ç•™äº†é©¬å°”å¯å¤«å±æ€§ï¼Œå¹¶é€šè¿‡åœ¨é™å™ªæ­¥éª¤ä¸­é€‰æ‹©æ€§å¼•å…¥éšæœºæ€§æ¥åŠ é€Ÿè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAR-Dragå®ç°äº†é«˜è§†è§‰ä¿çœŸåº¦å’Œç²¾ç¡®çš„è¿åŠ¨å¯¹é½ï¼Œä¸æœ€å…ˆè¿›çš„è¿åŠ¨æ§åˆ¶VDMç›¸æ¯”ï¼Œå»¶è¿Ÿæ—¶é—´å¤§å¤§é™ä½ï¼Œè€Œä¸”ä»…ä½¿ç”¨1.3Bå‚æ•°ã€‚æ›´å¤šå¯è§†åŒ–å†…å®¹å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://kesenzhao.github.io/AR-Drag.github.io/">https://kesenzhao.github.io/AR-Drag.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08131v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å®æ—¶è¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åŒå‘æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰å»¶è¿Ÿå’Œç¼ºä¹æœ‰æ•ˆçš„è‡ªå›å½’ï¼ˆARï¼‰æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†AR-Dragæ¨¡å‹ã€‚å®ƒæ˜¯é¦–ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å°‘æ•°æ­¥éª¤è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒå®æ—¶å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆï¼Œå¹¶å…·æœ‰å¤šæ ·çš„è¿åŠ¨æ§åˆ¶ã€‚AR-Dragé€šè¿‡å¾®è°ƒåŸºç¡€I2Væ¨¡å‹ä»¥æ”¯æŒåŸºæœ¬è¿åŠ¨æ§åˆ¶ï¼Œå¹¶é€šè¿‡åŸºäºè½¨è¿¹çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æ”¹è¿›ã€‚è®¾è®¡ä¿ç•™äº†é©¬å°”å¯å¤«å±æ€§ï¼Œé€šè¿‡è‡ªæˆ‘æ»šåŠ¨æœºåˆ¶åŠ é€Ÿè®­ç»ƒï¼Œå¹¶åœ¨å»å™ªæ­¥éª¤ä¸­é€‰æ‹©æ€§åœ°å¼•å…¥éšæœºæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒAR-Dragåœ¨è§†è§‰ä¿çœŸåº¦å’Œç²¾ç¡®è¿åŠ¨å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºå…ˆè¿›çš„è¿åŠ¨æ§åˆ¶VDMæ¨¡å‹å¤§å¹…é™ä½å»¶è¿Ÿï¼Œä¸”ä»…ä½¿ç”¨1.3Bå‚æ•°ã€‚æ›´å¤šå¯è§†åŒ–å†…å®¹è¯·è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AR-Dragæ˜¯é¦–ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ çš„å°‘æ•°æ­¥éª¤è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å®æ—¶è¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹æ”¯æŒå›¾åƒåˆ°è§†é¢‘çš„å®æ—¶ç”Ÿæˆå¹¶å…·æœ‰å¤šæ ·çš„è¿åŠ¨æ§åˆ¶åŠŸèƒ½ã€‚</li>
<li>AR-Dragé€šè¿‡å¾®è°ƒåŸºç¡€I2Væ¨¡å‹ä»¥æ”¯æŒåŸºæœ¬è¿åŠ¨æ§åˆ¶ï¼Œå¹¶é‡‡ç”¨åŸºäºè½¨è¿¹çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ”¹è¿›ã€‚</li>
<li>è®¾è®¡ä¿ç•™äº†é©¬å°”å¯å¤«å±æ€§ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘æ»šåŠ¨æœºåˆ¶åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹åœ¨é€‰æ‹©æ€§æ­¥éª¤ä¸­å¼•å…¥éšæœºæ€§ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAR-Dragåœ¨è§†è§‰ä¿çœŸåº¦å’Œç²¾ç¡®è¿åŠ¨å¯¹é½æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
<li>AR-Dragç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„è¿åŠ¨æ§åˆ¶VDMæ¨¡å‹å¤§å¹…é™ä½å»¶è¿Ÿï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„å‚æ•°ï¼ˆä»…1.3Bï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1f254fac4a76d68b1b38026118218862~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909424&auth_key=1760909424-0-0-a05a1a042857da41436199aebd8d65c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f0614d596d8261fec724b5df11baabf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909431&auth_key=1760909431-0-0-58c240c95a699f57520f1aa244f4fb75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fbba12f5ddd7564cccd995c2be6aa5eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909437&auth_key=1760909437-0-0-a00ca816254dc39b85102c2a17f33ac4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On"><a href="#ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On" class="headerlink" title="ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On"></a>ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On</h2><p><strong>Authors:Junseo Park, Hyeryung Jang</strong></p>
<p>Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines. </p>
<blockquote>
<p>è™šæ‹Ÿè¯•è¡£ï¼ˆVITONï¼‰æ—¨åœ¨ç”Ÿæˆç”¨æˆ·ç©¿æˆ´ç›®æ ‡æœé¥°çš„ç°å®å›¾åƒï¼Œéœ€è¦åœ¨è¯•è¡£åŒºåŸŸè¿›è¡Œç²¾ç¡®æœé¥°å¯¹é½ï¼Œå¹¶åœ¨éè¯•è¡£åŒºåŸŸå¿ å®ä¿ç•™èº«ä»½å’ŒèƒŒæ™¯ã€‚è™½ç„¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å·²ç»æé«˜äº†å¯¹é½å’Œç»†èŠ‚åˆæˆçš„èƒ½åŠ›ï¼Œä½†ä¿ç•™éè¯•è¡£åŒºåŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸€ç§å¸¸è§çš„åå¤„ç†æ–¹æ³•æ˜¯ç›´æ¥æ›¿æ¢è¿™äº›åŒºåŸŸä»¥ä½¿ç”¨åŸå§‹å†…å®¹ï¼Œä½†çªå…€çš„è¿‡æ¸¡é€šå¸¸ä¼šäº§ç”Ÿè¾¹ç•Œä¼ªå½±ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†VITONé‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªçº¿æ€§é€†é—®é¢˜ï¼Œå¹¶é‡‡ç”¨è½¨è¿¹å¯¹é½æ±‚è§£å™¨ï¼Œé€æ­¥å¼ºåˆ¶æ‰§è¡Œæµ‹é‡ä¸€è‡´æ€§ï¼Œå‡å°‘éè¯•è¡£åŒºåŸŸçš„çªå…€å˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ±‚è§£å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä»å­˜åœ¨è¯­ä¹‰æ¼‚ç§»é—®é¢˜ï¼Œå¯¼è‡´å‡ºç°ä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºäº†ART-VITONï¼Œä¸€ç§æµ‹é‡æŒ‡å¯¼çš„æ‰©æ•£æ¡†æ¶ï¼Œç¡®ä¿æµ‹é‡ä¾ä»æ€§åŒæ—¶ä¿æŒæ— ä¼ªå½±åˆæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†åŸºäºæ®‹å·®å…ˆéªŒçš„åˆå§‹åŒ–ï¼Œä»¥å‡è½»è®­ç»ƒæ¨ç†ä¸åŒ¹é…é—®é¢˜ï¼Œä»¥åŠæ— ä¼ªå½±æµ‹é‡æŒ‡å¯¼é‡‡æ ·ï¼Œç»“åˆäº†æ•°æ®ä¸€è‡´æ€§ã€é¢‘ç‡çº§æ ¡æ­£å’Œå‘¨æœŸæ€§æ ‡å‡†å»å™ªã€‚åœ¨VITON-HDã€DressCodeå’ŒSHHQ-1.0ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒART-VITONæœ‰æ•ˆåœ°ä¿ç•™äº†èº«ä»½å’ŒèƒŒæ™¯ï¼Œæ¶ˆé™¤äº†è¾¹ç•Œä¼ªå½±ï¼Œå¹¶ä¸”ç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œç¨³å¥æ€§æ–¹é¢éƒ½æœ‰äº†ä¸€è‡´çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25749v2">PDF</a> 21 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è™šæ‹Ÿè¯•ç©¿ï¼ˆVITONï¼‰æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®å¯¹å‡†è¡£ç‰©ã€åœ¨éè¯•ç©¿åŒºåŸŸä¿æŒèº«ä»½å’ŒèƒŒæ™¯çš„çœŸå®æ€§ã€‚è™½ç„¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æé«˜äº†å¯¹å‡†å’Œç»†èŠ‚åˆæˆçš„èƒ½åŠ›ï¼Œä½†ä¿ç•™éè¯•ç©¿åŒºåŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æµ‹é‡æ–¹æ³•å¼•å¯¼æ‰©æ•£æ¡†æ¶ART-VITONï¼Œç¡®ä¿æµ‹é‡ä¸€è‡´æ€§å¹¶ç»´æŒæ— ç‘•ç–µçš„åˆæˆæ•ˆæœã€‚é€šè¿‡ç»“åˆæ®‹å·®å…ˆéªŒåˆå§‹åŒ–ä¸æ•°æ®ä¸€è‡´æ€§ã€é¢‘ç‡çº§æ ¡æ­£å’Œå‘¨æœŸæ€§æ ‡å‡†å»å™ªç­‰æ–¹æ³•ï¼ŒART-VITONå¯æœ‰æ•ˆä¿ç•™èº«ä»½å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œæ¶ˆé™¤è¾¹ç•Œä¼ªå½±ï¼Œå¹¶åœ¨è§†è§‰ä¿çœŸåº¦å’Œç¨³å¥æ€§æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VITONæŠ€æœ¯çš„ç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä¸ªäººç©¿ç€ç›®æ ‡è¡£ç‰©çš„çœŸå®å›¾åƒï¼Œè¦æ±‚ç²¾ç¡®å¯¹å‡†è¡£ç‰©ï¼ŒåŒæ—¶ä¿æŒéè¯•ç©¿åŒºåŸŸçš„èº«ä»½å’ŒèƒŒæ™¯çœŸå®æ€§ã€‚</li>
<li>LDMåœ¨è™šæ‹Ÿè¯•è¡£æŠ€æœ¯ä¸­æœ‰åŠ©äºæé«˜å¯¹å‡†å’Œç»†èŠ‚åˆæˆèƒ½åŠ›ã€‚</li>
<li>éè¯•ç©¿åŒºåŸŸçš„ä¿ç•™æ˜¯è™šæ‹Ÿè¯•è¡£æŠ€æœ¯ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºç›´æ¥æ›¿æ¢è¿™äº›åŒºåŸŸå¸¸å¸¸ä¼šå¯¼è‡´è¾¹ç•Œä¼ªå½±ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æµ‹é‡æ–¹æ³•å¼•å¯¼æ‰©æ•£æ¡†æ¶ART-VITONæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ¸è¿›åœ°å¼ºåˆ¶æ‰§è¡Œæµ‹é‡ä¸€è‡´æ€§æ¥å‡å°‘éè¯•ç©¿åŒºåŸŸçš„çªå…€å˜åŒ–ã€‚</li>
<li>ART-VITONç»“åˆæ®‹å·®å…ˆéªŒåˆå§‹åŒ–æ¥ç¼“è§£è®­ç»ƒæ¨ç†ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•°æ®ä¸€è‡´æ€§ã€é¢‘ç‡çº§æ ¡æ­£å’Œå‘¨æœŸæ€§æ ‡å‡†å»å™ªç­‰æŠ€æœ¯ç»“åˆçš„é‡‡æ ·æ–¹æ³•ï¼ŒART-VITONå¯å®ç°æ— ç‘•ç–µçš„åˆæˆæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3c3493d79f26d00f839f75e11e8934ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909445&auth_key=1760909445-0-0-3855c63eea9152a7ec39081b220164d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f032b546f34c662175629a51480f6dce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909452&auth_key=1760909452-0-0-eab5fd77e4ad15bab0dddf64789e5d2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37dae7edb1af311709ec2f93d77fc5f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909460&auth_key=1760909460-0-0-8b29da6b8b568f75b78d60737fedc4eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab0e4bb4c9e9116f96c662c60ff1e90c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909466&auth_key=1760909466-0-0-1d17f314d8a60542c9b5df96755995d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fbb7fa6ccc9f27fd65d8375ddc4b7de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909472&auth_key=1760909472-0-0-4f9c286a024ec37b82774732a902e93c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition"><a href="#Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition" class="headerlink" title="Does FLUX Already Know How to Perform Physically Plausible Image   Composition?"></a>Does FLUX Already Know How to Perform Physically Plausible Image   Composition?</h2><p><strong>Authors:Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</strong></p>
<p>Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication. </p>
<blockquote>
<p>å›¾åƒç»„åˆæ—¨åœ¨æ— ç¼æ’å…¥ç”¨æˆ·æŒ‡å®šçš„å¯¹è±¡åˆ°æ–°åœºæ™¯ä¸­ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å…‰ç…§ï¼ˆä¾‹å¦‚å‡†ç¡®é˜´å½±ã€æ°´é¢åå°„ï¼‰å’Œå¤šæ ·åŒ–ã€é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶é‡åˆ°å›°éš¾ã€‚ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚SD3.5ã€FLUXï¼‰å·²ç»ç¼–ç äº†é‡è¦çš„ç‰©ç†å’Œåˆ†è¾¨ç‡å…ˆéªŒçŸ¥è¯†ï¼Œä½†ç¼ºä¹ä¸€ä¸ªæ¡†æ¶æ¥é‡Šæ”¾å®ƒä»¬ï¼Œè€Œä¸æ±‚åŠ©äºæ½œåœ¨çš„åè½¬ï¼Œè¿™é€šå¸¸ä¼šå°†ç‰©ä½“çš„å§¿åŠ¿é”å®šåœ¨ä¸Šä¸‹æ–‡ä¸é€‚å½“çš„æ–¹å‘ä¸Šï¼Œæˆ–è„†å¼±çš„æ³¨æ„åŠ›æ‰‹æœ¯ã€‚æˆ‘ä»¬æå‡ºäº†SHINEï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ— ç¼ã€é«˜ä¿çœŸæ’å…¥ä¸­æ€§åŒ–é”™è¯¯æ¡†æ¶ã€‚SHINEå¼•å…¥äº†æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å®šåˆ¶é€‚é…å™¨ï¼ˆä¾‹å¦‚IP-Adapterï¼‰æ¥å¼•å¯¼æ½œåœ¨è¡¨ç¤ºä»¥å¿ å®äºä¸»é¢˜è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒèƒŒæ™¯å®Œæ•´æ€§ã€‚æå‡ºé€€åŒ–æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆï¼Œä»¥è¿›ä¸€æ­¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚ä¸ºäº†è§£å†³ç¼ºä¹ä¸¥æ ¼åŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ComplexCompoï¼Œå…·æœ‰å¤šç§åˆ†è¾¨ç‡å’ŒæŒ‘æˆ˜æ€§æ¡ä»¶ï¼Œå¦‚ä½å…‰ç…§ã€å¼ºçƒˆç…§æ˜ã€å¤æ‚é˜´å½±å’Œåå°„è¡¨é¢ã€‚åœ¨ComplexCompoå’ŒDreamEditBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨æ ‡å‡†æŒ‡æ ‡ï¼ˆä¾‹å¦‚DINOv2ï¼‰å’Œäººç±»å¯¹é½å¾—åˆ†ï¼ˆä¾‹å¦‚DreamSimã€ImageRewardã€VisionRewardï¼‰ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ä»£ç å’ŒåŸºå‡†æµ‹è¯•å°†åœ¨å‘è¡¨æ—¶å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21278v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SHINEæ¡†æ¶ï¼Œå®ƒæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å®ç°æ— ç¼ã€é«˜ä¿çœŸæ’å…¥çš„æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±å’Œé¢„è®­ç»ƒå®šåˆ¶é€‚é…å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸ç ´åèƒŒæ™¯å®Œæ•´æ€§çš„æƒ…å†µä¸‹ï¼Œå¿ å®å‘ˆç°ä¸»é¢˜è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é€€åŒ–æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆç­‰æ–¹æ³•ï¼Œä»¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚ä¸ºè§£å†³ç¼ºä¹ä¸¥æ ¼åŸºå‡†çš„é—®é¢˜ï¼Œå¼•å…¥äº†ComplexCompoåŸºå‡†æµ‹è¯•ï¼Œä»¥å±•ç¤ºåœ¨å„ç§åˆ†è¾¨ç‡å’ŒæŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHINEæ¡†æ¶å®ç°äº†æ— ç¼ã€é«˜ä¿çœŸæ’å…¥æŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å…‰ç…§å’Œå¤šæ ·åŒ–é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶çš„å›°éš¾ã€‚</li>
<li>SHINEå¼•å…¥äº†æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å®šåˆ¶é€‚é…å™¨æ¥æŒ‡å¯¼æ½œåœ¨è¡¨ç¤ºï¼Œå¿ å®å‘ˆç°ä¸»é¢˜è¡¨ç¤ºåŒæ—¶ä¿æŒèƒŒæ™¯å®Œæ•´æ€§ã€‚</li>
<li>é€šè¿‡æå‡ºé€€åŒ–æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆç­‰æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æ¶ˆé™¤äº†ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚</li>
<li>ComplexCompoåŸºå‡†æµ‹è¯•åŒ…å«äº†å„ç§åˆ†è¾¨ç‡å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ï¼Œå¦‚ä½å…‰ç…§ã€å¼ºçƒˆç…§æ˜ã€å¤æ‚é˜´å½±å’Œåå°„è¡¨é¢ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSHINEåœ¨ComplexCompoå’ŒDreamEditBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¾—åˆ°äº†æ ‡å‡†æŒ‡æ ‡å’Œäººç±»å¯¹é½æŒ‡æ ‡çš„éªŒè¯ã€‚</li>
<li>SHINEçš„å®ç°ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆã€å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6d8f46a73a5547f50c2d01bc9db0a0b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909480&auth_key=1760909480-0-0-bfd9a6178089a712aab153fb146405a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ba6bb2bd5ce0d999d970cc157e4588a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909487&auth_key=1760909487-0-0-17a544d5de3b3b8e47f426fa16feeb34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a494a2309aa5d06db54970e9294e54f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909494&auth_key=1760909494-0-0-a688b43130a81798803721551ab4bc3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8eef3c127dde9c8902d46ba17ce9cb03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909500&auth_key=1760909500-0-0-be02dd274c3ef5215069227bc375be4b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d9536c9d39e20d9203b47665018ffc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909507&auth_key=1760909507-0-0-14786a53dff1b60205513589e5bdb851&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ComposeMe-Attribute-Specific-Image-Prompts-for-Controllable-Human-Image-Generation"><a href="#ComposeMe-Attribute-Specific-Image-Prompts-for-Controllable-Human-Image-Generation" class="headerlink" title="ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image   Generation"></a>ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image   Generation</h2><p><strong>Authors:Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, Kfir Aberman</strong></p>
<p>Generating high-fidelity images of humans with fine-grained control over attributes such as hairstyle and clothing remains a core challenge in personalized text-to-image synthesis. While prior methods emphasize identity preservation from a reference image, they lack modularity and fail to provide disentangled control over specific visual attributes. We introduce a new paradigm for attribute-specific image prompting, in which distinct sets of reference images are used to guide the generation of individual aspects of human appearance, such as hair, clothing, and identity. Our method encodes these inputs into attribute-specific tokens, which are injected into a pre-trained text-to-image diffusion model. This enables compositional and disentangled control over multiple visual factors, even across multiple people within a single image. To promote natural composition and robust disentanglement, we curate a cross-reference training dataset featuring subjects in diverse poses and expressions, and propose a multi-attribute cross-reference training strategy that encourages the model to generate faithful outputs from misaligned attribute inputs while adhering to both identity and textual conditioning. Extensive experiments show that our method achieves state-of-the-art performance in accurately following both visual and textual prompts. Our framework paves the way for more configurable human image synthesis by combining visual prompting with text-driven generation. Webpage is available at: <a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/">https://snap-research.github.io/composeme/</a>. </p>
<blockquote>
<p>ç”Ÿæˆå…·æœ‰ç²¾ç»†å±æ€§æ§åˆ¶çš„é«˜ä¿çœŸäººç±»å›¾åƒï¼Œå¦‚å‘å‹å’Œæœè£…ï¼Œä»ç„¶æ˜¯ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è™½ç„¶ä¹‹å‰çš„æ–¹æ³•å¼ºè°ƒä»å‚è€ƒå›¾åƒä¸­ä¿ç•™èº«ä»½ï¼Œä½†å®ƒä»¬ç¼ºä¹æ¨¡å—åŒ–ï¼Œæ— æ³•æä¾›å¯¹ç‰¹å®šè§†è§‰å±æ€§çš„åˆ†ç¦»æ§åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å±æ€§ç‰¹å®šå›¾åƒæç¤ºèŒƒå¼ï¼Œå…¶ä¸­ä½¿ç”¨ä¸åŒçš„å‚è€ƒå›¾åƒæ¥æŒ‡å¯¼äººç±»å¤–è§‚çš„å„ä¸ªæ–¹é¢ï¼Œå¦‚å¤´å‘ã€æœè£…å’Œèº«ä»½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¿™äº›è¾“å…¥ç¼–ç ä¸ºå±æ€§ç‰¹å®šä»¤ç‰Œï¼Œå¹¶æ³¨å…¥é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ã€‚è¿™å®ç°äº†å¯¹å¤šä¸ªè§†è§‰å› ç´ çš„ç»„åˆå’Œåˆ†ç¦»æ§åˆ¶ï¼Œç”šè‡³åœ¨å•ä¸ªå›¾åƒä¸­çš„å¤šä¸ªäººä¹‹é—´ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†ä¿ƒè¿›è‡ªç„¶çš„æ„å›¾å’Œç¨³å¥çš„åˆ†ç¦»ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªè·¨å‚è€ƒè®­ç»ƒæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸åŒå§¿åŠ¿å’Œè¡¨æƒ…çš„ä¸»ä½“ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤šå±æ€§è·¨å‚è€ƒè®­ç»ƒç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹åœ¨èº«ä»½å’Œæ–‡æœ¬æ¡ä»¶çš„åŒæ—¶ï¼Œä»é”™ä½å±æ€§è¾“å…¥ç”Ÿæˆå¿ å®è¾“å‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®éµå¾ªè§†è§‰å’Œæ–‡æœ¬æç¤ºæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ç»“åˆè§†è§‰æç¤ºå’Œæ–‡æœ¬é©±åŠ¨ç”Ÿæˆï¼Œä¸ºäººåƒåˆæˆé“ºå¹³äº†æ›´åŠ å¯é…ç½®çš„é“è·¯ã€‚ç½‘é¡µåœ°å€ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/]">https://snap-research.github.io/composeme/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18092v2">PDF</a> Accepted to SIGGRAPH Asia 2025, webpage:   <a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/">https://snap-research.github.io/composeme/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§æ–°å‹å±æ€§ç‰¹å®šå›¾åƒæç¤ºæ–¹æ³•ï¼Œé‡‡ç”¨å¤šç§å‚è€ƒå›¾åƒæ¥æŒ‡å¯¼äººç‰©å¤–è§‚çš„å„ä¸ªæ–¹é¢ç”Ÿæˆï¼Œå¦‚å¤´å‘ã€æœè£…å’Œèº«ä»½ç­‰ã€‚é€šè¿‡ç¼–ç è¿™äº›è¾“å…¥æˆç‰¹å®šå±æ€§æ ‡è®°ï¼Œæ³¨å…¥é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°å¯¹å¤šä¸ªè§†è§‰å› ç´ çš„åˆ†ç¦»æ§åˆ¶ï¼Œç”šè‡³åœ¨å•å¼ å›¾åƒä¸­å¤šäººä¹‹é—´ä¹Ÿèƒ½å®ç°ã€‚è¯¥ç ”ç©¶é‡‡ç”¨è·¨å‚è€ƒè®­ç»ƒæ•°æ®é›†å’Œå¤šå±æ€§è·¨å‚è€ƒè®­ç»ƒç­–ç•¥ï¼Œä¿ƒè¿›äº†è‡ªç„¶æ„å›¾å’Œç¨³å¥çš„åˆ†ç¦»ã€‚è¯¥æ–¹æ³•åœ¨å‡†ç¡®éµå¾ªè§†è§‰å’Œæ–‡æœ¬æç¤ºæ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œä¸ºé€šè¿‡è§†è§‰æç¤ºå’Œæ–‡æœ¬é©±åŠ¨ç”Ÿæˆæ›´å¯é…ç½®çš„äººç‰©å›¾åƒåˆæˆé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°å‹å±æ€§ç‰¹å®šå›¾åƒæç¤ºæ–¹æ³•ï¼Œä½¿ç”¨å¤šç§å‚è€ƒå›¾åƒæŒ‡å¯¼äººç‰©å¤–è§‚ç”Ÿæˆã€‚</li>
<li>é€šè¿‡å±æ€§ç‰¹å®šæ ‡è®°ç¼–ç è¾“å…¥ï¼Œæ³¨å…¥é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°å¤šè§†è§‰å› ç´ çš„åˆ†ç¦»æ§åˆ¶ã€‚</li>
<li>é‡‡ç”¨è·¨å‚è€ƒè®­ç»ƒæ•°æ®é›†ï¼Œä¿ƒè¿›è‡ªç„¶æ„å›¾å’Œç¨³å¥çš„åˆ†ç¦»ã€‚</li>
<li>æå‡ºå¤šå±æ€§è·¨å‚è€ƒè®­ç»ƒç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å±æ€§è¾“å…¥é”™ä½æ—¶ç”Ÿæˆå¿ å®è¾“å‡ºã€‚</li>
<li>æ–¹æ³•åœ¨éµå¾ªè§†è§‰å’Œæ–‡æœ¬æç¤ºæ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ›´å¯é…ç½®çš„äººç‰©å›¾åƒåˆæˆé“ºå¹³äº†é“è·¯ï¼Œç»“åˆè§†è§‰æç¤ºå’Œæ–‡æœ¬é©±åŠ¨ç”Ÿæˆã€‚</li>
<li>ç ”ç©¶æˆæœå¯é€šè¿‡ç½‘é¡µï¼ˆ<a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/%EF%BC%89%E8%AE%BF%E9%97%AE%E3%80%82">https://snap-research.github.io/composeme/ï¼‰è®¿é—®ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4a56d87b7e635aab92cff3b02193c39f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909514&auth_key=1760909514-0-0-51f1ed85af9f09b97501a8001eb3c182&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a9fa0e74d3ec1e24ab9037f856ed56a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909522&auth_key=1760909522-0-0-5c81651e2a3ecf421b194741b2f0bb0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34a8fadf906e602ae869281379071917~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909528&auth_key=1760909528-0-0-49190727e3127b74b56d545cc11dcc09&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c0fd78375d566aadccb76b03543b546~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909535&auth_key=1760909535-0-0-8bcc9575118e6aeaf331cf4b9eed17b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e6d7181865feb607c8d14d8605f239d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909542&auth_key=1760909542-0-0-fc4efa571630a00d7e62d7cd45e05f34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8260110b2f847b5d63349dea3856474~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909550&auth_key=1760909550-0-0-9a43fa826131000880ef85a034302887&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective"><a href="#AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective" class="headerlink" title="AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective"></a>AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective</h2><p><strong>Authors:Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</strong></p>
<p>Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. In addition, to ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text&#x2F;audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized Phoneme-Frame Causal Attention Mask to support multi-step parallel acceleration. Extensive experiments conducted on both Chinese (CMLR) and English (HDTF) datasets demonstrate that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution. </p>
<blockquote>
<p>å¤´éƒ¨åŠ¨ç”»ç”Ÿæˆä¸»è¦å…³æ³¨ä»éŸ³é¢‘è¾“å…¥ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨è§†é¢‘ã€‚ç»§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¹‹åï¼Œæ‰©æ•£æ¨¡å‹ç”±äºå…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºä¸»æµã€‚ç„¶è€Œï¼Œæ‰©æ•£è¿‡ç¨‹çš„å›ºæœ‰å±€é™æ€§é€šå¸¸ä¼šå¯¼è‡´å¸§é—´é—ªçƒå’Œæ¨ç†ç¼“æ…¢ï¼Œä»è€Œé™åˆ¶äº†å…¶å®é™…éƒ¨ç½²ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AvatarSyncï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºéŸ³ç´ è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•ä¸ªå‚è€ƒå›¾åƒç”Ÿæˆé€¼çœŸä¸”å¯æ§çš„å¤´éƒ¨åŠ¨ç”»ï¼Œç›´æ¥ç”±æ–‡æœ¬æˆ–éŸ³é¢‘è¾“å…¥é©±åŠ¨ã€‚ä¸ºäº†å‡è½»é—ªçƒå¹¶ç¡®ä¿è¿ç»­æ€§ï¼ŒAvatarSyncåˆ©ç”¨äº†ä¸€ä¸ªè‡ªå›å½’ç®¡é“ï¼Œå¢å¼ºäº†æ—¶é—´å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿å¯æ§æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†éŸ³ç´ ï¼ˆå³è¯­éŸ³çš„åŸºæœ¬å•å…ƒï¼‰ï¼Œæ„å»ºäº†ä»æ–‡æœ¬&#x2F;éŸ³é¢‘åˆ°éŸ³ç´ çš„å¤šå¯¹ä¸€æ˜ å°„ï¼Œå®ç°äº†ç²¾ç¡®çš„éŸ³ç´ åˆ°è§†è§‰çš„å¯¹é½ã€‚å¦å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œå°†è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨æ€è§£è€¦ï¼Œå¹¶å¼•å…¥å®šåˆ¶çš„éŸ³ç´ å¸§å› æœæ³¨æ„åŠ›æ©ç ï¼Œæ”¯æŒå¤šæ­¥å¹¶è¡ŒåŠ é€Ÿã€‚åœ¨ä¸­æ–‡ï¼ˆCMLRï¼‰å’Œè‹±æ–‡ï¼ˆHDTFï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„å¤´éƒ¨åŠ¨ç”»æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12052v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŠ¨ç”»ä»éŸ³é¢‘è¾“å…¥ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨è§†é¢‘çš„æŠ€æœ¯è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåŸºäºè¯­éŸ³è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶AvatarSyncï¼Œç”¨äºè§£å†³ç°æœ‰æŠ€æœ¯çš„å¸§é—´é—ªçƒå’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¯ä»å•ä¸ªå‚è€ƒå›¾åƒç”Ÿæˆå¯æ§çš„å¤´éƒ¨åŠ¨ç”»ï¼Œå¹¶å…·æœ‰é«˜åº¦çœŸå®æ„Ÿã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¯­éŸ³ç‰¹å¾æ„å»ºäº†ä¸€ç§æ–‡æœ¬åˆ°è¯­éŸ³çš„å¤šå¯¹ä¸€æ˜ å°„ï¼Œå®ç°ç²¾ç¡®çš„è¯­éŸ³ç‰¹å¾å¯¹é½ï¼Œå¹¶åˆ©ç”¨ä¸¤çº§ç”Ÿæˆç­–ç•¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯äººå¤´åƒåŠ¨ç”»æŠ€æœ¯æ­£å—åˆ°å…³æ³¨ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„GANså·²æˆä¸ºä¸»æµã€‚</li>
<li>ç°æœ‰æŠ€æœ¯å­˜åœ¨å¸§é—´é—ªçƒå’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>AvatarSyncæ¡†æ¶ä½¿ç”¨è‡ªå›å½’æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ï¼Œç”ŸæˆçœŸå®å¯æ§çš„å¤´éƒ¨åŠ¨ç”»ã€‚</li>
<li>åˆ©ç”¨è¯­éŸ³ç‰¹å¾æ„å»ºæ–‡æœ¬åˆ°è¯­éŸ³çš„å¤šå¯¹ä¸€æ˜ å°„ï¼Œæé«˜ç²¾åº¦å’Œè¿ç»­æ€§ã€‚</li>
<li>é‡‡ç”¨ä¸¤çº§ç”Ÿæˆç­–ç•¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜AvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-048d641b03e67540af17e882e55f70c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909557&auth_key=1760909557-0-0-aa3d197678ecdfacbde6252bfcbc092b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8077d61ade754d02cc81cb658becfc0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909564&auth_key=1760909564-0-0-2f8164f56f66238d12e7b4d7f67cc7bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b6c5da1fb33bc9098fcecb7a7597568~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909571&auth_key=1760909571-0-0-7359ada84eaff22d98db4499bedb09e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07e6ea6fa8caed71571d12ebb34a9ea0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909578&auth_key=1760909578-0-0-ae70072115c2d7a1a9af55db7ff927ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UniEgoMotion-A-Unified-Model-for-Egocentric-Motion-Reconstruction-Forecasting-and-Generation"><a href="#UniEgoMotion-A-Unified-Model-for-Egocentric-Motion-Reconstruction-Forecasting-and-Generation" class="headerlink" title="UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,   Forecasting, and Generation"></a>UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,   Forecasting, and Generation</h2><p><strong>Authors:Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli</strong></p>
<p>Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR&#x2F;VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotionâ€™s simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications. </p>
<blockquote>
<p>ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åŠ¨æ€ç”Ÿæˆä¸é¢„æµ‹ï¼Œå¹¶ç»“åˆåœºæ™¯ä¸Šä¸‹æ–‡ï¼Œå¯¹äºå¢å¼ºAR&#x2F;VRä½“éªŒã€æ”¹å–„äººæœºäº¤äº’ã€æ¨åŠ¨è¾…åŠ©æŠ€æœ¯å’Œå®ç°è‡ªé€‚åº”åŒ»ç–—è§£å†³æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚é€šè¿‡ä»ç¬¬ä¸€äººç§°è§†è§’å‡†ç¡®é¢„æµ‹å’Œæ¨¡æ‹ŸåŠ¨ä½œï¼Œæˆ‘ä»¬å¯ä»¥å®ç°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç¬¬ä¸‰äººç§°åŠ¨ä½œåˆæˆä¸ç»“æ„åŒ–ä¸‰ç»´åœºæ™¯ä¸Šä¸‹æ–‡çš„ç»“åˆä¸Šï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç¯å¢ƒä¸­æ•ˆæœæœ‰é™ã€‚åœ¨è¿™ç§ç¯å¢ƒä¸­ï¼Œæœ‰é™çš„è§†é‡ã€é¢‘ç¹çš„é®æŒ¡å’ŒåŠ¨æ€ç›¸æœºé˜»ç¢äº†åœºæ™¯æ„ŸçŸ¥ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åŠ¨æ€ç”Ÿæˆå’Œä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åŠ¨æ€é¢„æµ‹ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡åˆ©ç”¨ç¬¬ä¸€äººç§°å›¾åƒè¿›è¡Œåœºæ™¯æ„ŸçŸ¥åŠ¨ä½œåˆæˆï¼Œæ— éœ€ä¾èµ–æ˜ç¡®çš„ä¸‰ç»´åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†UniEgoMotionï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„æ¡ä»¶è¿åŠ¨æ‰©æ•£æ¨¡å‹ï¼Œå…·æœ‰æ–°é¢–çš„å¤´éƒ¨ä½ç½®åŠ¨æ€è¡¨ç¤ºå½¢å¼ï¼Œé€‚ç”¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è®¾å¤‡ã€‚UniEgoMotionç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡æ”¯æŒä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åŠ¨ä½œé‡å»ºã€é¢„æµ‹å’Œç”Ÿæˆï¼Œä»ç¬¬ä¸€äººç§°è§†è§‰è¾“å…¥åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­è¿›è¡Œã€‚ä¸å¿½è§†åœºæ™¯è¯­ä¹‰çš„å…ˆå‰å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æå–åŸºäºå›¾åƒçš„åœºæ™¯ä¸Šä¸‹æ–‡æ¥æ¨æ–­åˆç†çš„ä¸‰ç»´åŠ¨ä½œã€‚ä¸ºäº†ä¿ƒè¿›è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†EE4D-Motionæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯åŸºäºEgoExo4Dæ´¾ç”Ÿçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶å¢åŠ äº†ä¼ªåœ°é¢çœŸå®ä¸‰ç»´è¿åŠ¨æ³¨é‡Šã€‚UniEgoMotionåœ¨è‡ªæˆ‘ä¸­å¿ƒåŠ¨æ€é‡å»ºæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªèƒ½å¤Ÿä»å•ä¸€è‡ªæˆ‘ä¸­å¿ƒå›¾åƒç”ŸæˆåŠ¨ä½œçš„æ–¹æ³•ã€‚å¹¿æ³›çš„è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬ç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè‡ªæˆ‘ä¸­å¿ƒåŠ¨æ€å»ºæ¨¡è®¾å®šäº†æ–°çš„åŸºå‡†å¹¶è§£é”äº†è‡ªæˆ‘ä¸­å¿ƒåº”ç”¨çš„æ–°å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01126v2">PDF</a> ICCV 2025. Project Page:   <a target="_blank" rel="noopener" href="https://chaitanya100100.github.io/UniEgoMotion/">https://chaitanya100100.github.io/UniEgoMotion/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åŠ¨æ€è¿åŠ¨ç”Ÿæˆä¸é¢„æµ‹çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨å¢å¼ºAR&#x2F;VRä½“éªŒã€æ”¹å–„äººæœºäº’åŠ¨ã€æ¨è¿›è¾…åŠ©æŠ€æœ¯å’Œè‡ªé€‚åº”åŒ»ç–—è§£å†³æ–¹æ¡ˆç­‰é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°è¿åŠ¨åˆæˆä¸ç»“æ„åŒ–3Dåœºæ™¯ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä»¥ç¬¬ä¸€äººç§°å›¾åƒä¸ºåŸºç¡€çš„æ— ä¾èµ–æ˜¾å¼3Dåœºæ™¯çš„Egocentric Motion Generationå’ŒEgocentric Motion Forecastingä¸¤ä¸ªæ–°ä»»åŠ¡ã€‚åŒæ—¶ï¼Œä»‹ç»äº†ä¸€ç§ç»Ÿä¸€æ¡ä»¶è¿åŠ¨æ‰©æ•£æ¨¡å‹UniEgoMotionï¼Œè¯¥æ¨¡å‹å…·æœ‰é’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è®¾å¤‡çš„æ–°å‹å¤´ä¸­å¿ƒè¿åŠ¨è¡¨ç¤ºã€‚UniEgoMotionçš„ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡æ”¯æŒä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¿åŠ¨é‡å»ºã€é¢„æµ‹å’Œç”Ÿæˆï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…ä»ç¬¬ä¸€äººç§°è§†è§‰è¾“å…¥ä¸­è¿›è¡Œå¤„ç†ã€‚ä¸å¿½è§†åœºæ™¯è¯­ä¹‰çš„å…ˆå‰å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æå–åŸºäºå›¾åƒçš„åœºæ™¯ä¸Šä¸‹æ–‡æ¥æ¨æ–­åˆç†çš„3Dè¿åŠ¨ã€‚ä¸ºäº†ä¿ƒè¿›è®­ç»ƒï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä»EgoExo4Dæ´¾ç”Ÿçš„EE4D-Motionå¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¢å¼ºäº†ä¼ªåœ°é¢çœŸå®3Dè¿åŠ¨æ³¨é‡Šã€‚UniEgoMotionåœ¨è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¿åŠ¨é‡å»ºæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªèƒ½å¤Ÿä»å•ä¸€è‡ªæˆ‘ä¸ºä¸­å¿ƒå›¾åƒç”Ÿæˆè¿åŠ¨çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äººç±»è¿åŠ¨ç”Ÿæˆå’Œé¢„æµ‹å¯¹äºå¢å¼ºAR&#x2F;VRä½“éªŒã€æ”¹å–„äººæœºäº’åŠ¨ç­‰å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°è¿åŠ¨åˆæˆï¼Œåœ¨çœŸå®ä¸–ç•Œçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­æ•ˆæœæœ‰é™ã€‚</li>
<li>å¼•å…¥Egocentric Motion Generationå’ŒEgocentric Motion Forecastingä¸¤ä¸ªæ–°ä»»åŠ¡ï¼Œåˆ©ç”¨ç¬¬ä¸€äººç§°å›¾åƒè¿›è¡Œåœºæ™¯æ„ŸçŸ¥è¿åŠ¨åˆæˆã€‚</li>
<li>æå‡ºç»Ÿä¸€æ¡ä»¶è¿åŠ¨æ‰©æ•£æ¨¡å‹UniEgoMotionï¼Œæ”¯æŒä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¿åŠ¨é‡å»ºã€é¢„æµ‹å’Œç”Ÿæˆã€‚</li>
<li>UniEgoMotionèƒ½å¤Ÿæœ‰æ•ˆæå–åŸºäºå›¾åƒçš„åœºæ™¯ä¸Šä¸‹æ–‡ï¼Œæ¨æ–­åˆç†çš„3Dè¿åŠ¨ã€‚</li>
<li>å¼•å…¥EE4D-Motionæ•°æ®é›†ï¼Œå¢å¼ºä¼ªåœ°é¢çœŸå®3Dè¿åŠ¨æ³¨é‡Šï¼Œç”¨äºä¿ƒè¿›æ¨¡å‹è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-243f85cbfadc3fc2bb3f123895476aee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909585&auth_key=1760909585-0-0-46cbf0807a1489a33e5f8ccc84de584d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28fb6055d233bba28109600a1aa056d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909592&auth_key=1760909592-0-0-e7f01aab5fd45202869352250555f91b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7cbc92e29798a4565fbde5e43ac8676~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909599&auth_key=1760909599-0-0-415deaf41f25d6b8fe3735f1ab00e230&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-081c465c5cf95574b7e1c3d44818df90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760749458&auth_key=1760749458-0-0-d0462cc130b6b4965a75ae1843eba879&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Towards Generalist Intelligence in Dentistry Vision Foundation Models   for Oral and Maxillofacial Radiology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-be184f978b42f0ba861eeded4fd503ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760908435&auth_key=1760908435-0-0-b655b2f0c07d14ed37b54503db7119fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  GauSSmart Enhanced 3D Reconstruction through 2D Foundation Models and   Geometric Filtering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
