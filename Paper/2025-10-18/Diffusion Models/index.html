<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-18  Ponimator Unfolding Interactive Pose for Versatile Human-human   Interaction Animation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08273v4/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation"><a href="#Ponimator-Unfolding-Interactive-Pose-for-Versatile-Human-human-Interaction-Animation" class="headerlink" title="Ponimator: Unfolding Interactive Pose for Versatile Human-human   Interaction Animation"></a>Ponimator: Unfolding Interactive Pose for Versatile Human-human   Interaction Animation</h2><p><strong>Authors:Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang</strong></p>
<p>Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework. </p>
<blockquote>
<p>近距离人机交互姿势传达了丰富的交互动态上下文信息。基于这些姿势，人类可以直观地推断上下文并预测可能的过去和未来动态，这依赖于人类行为的强大先验知识。受此观察启发，我们提出了Ponimator，这是一个以近距离交互姿势为基础的通用交互动画简单框架。我们的训练数据来自动作捕捉交互数据集中的近距离双人姿势及其周围的时空上下文。Ponimator利用交互姿势先验，采用两个条件扩散模型：（1）姿态动画师使用时间先验根据交互姿势生成动态运动序列；（2）姿态生成器在交互姿势不可用的情况下，利用空间先验从单个姿势、文本或两者合成交互姿势。总体上，Ponimator支持多样化的任务，包括基于图像的交流动画、反应动画和文本到交流的合成，促进了从高质量mocap数据到开放世界场景的交互知识的转移。在多个数据集和应用上的实验证明了姿态先验的普遍性以及我们框架的有效性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14976v1">PDF</a> Accepted to ICCV 2025. Project page:   <a target="_blank" rel="noopener" href="https://stevenlsw.github.io/ponimator/">https://stevenlsw.github.io/ponimator/</a></p>
<p><strong>Summary</strong></p>
<p>该文提出了一个名为Ponimator的简单框架，用于基于近距离交互式姿势进行多种交互动画。该框架利用人体行为先验知识，通过两个条件扩散模型生成动态运动序列和交互式姿势。它能处理图像交互动画、反应动画和文字交互合成等任务，并能将高质量的运动捕捉数据中的交互知识转移到开放场景中。实验证明该框架具有通用性、有效性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ponimator框架利用近距离交互式姿势进行多种交互动画。</li>
<li>Ponimator包含两个条件扩散模型：姿势动画器和姿势生成器。</li>
<li>姿势动画器使用时间先验知识从交互式姿势生成动态运动序列。</li>
<li>姿势生成器在空间先验知识的基础上合成交互式姿势，可从单姿态、文本或两者的组合中生成。</li>
<li>Ponimator支持图像交互动画、反应动画和文字交互合成等任务。</li>
<li>Ponimator能将高质量的运动捕捉数据中的交互知识转移到开放场景中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14976v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14976v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14976v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14976v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14976v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14976v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FraQAT-Quantization-Aware-Training-with-Fractional-bits"><a href="#FraQAT-Quantization-Aware-Training-with-Fractional-bits" class="headerlink" title="FraQAT: Quantization Aware Training with Fractional bits"></a>FraQAT: Quantization Aware Training with Fractional bits</h2><p><strong>Authors:Luca Morreale, Alberto Gil C. P. Ramos, Malcolm Chadwick, Mehid Noroozi, Ruchika Chavhan, Abhinav Mehrotra, Sourav Bhattacharya</strong></p>
<p>State-of-the-art (SOTA) generative models have demonstrated impressive capabilities in image synthesis or text generation, often with a large capacity model. However, these large models cannot be deployed on smartphones due to the limited availability of on-board memory and computations. Quantization methods lower the precision of the model parameters, allowing for efficient computations, \eg, in \INT{8}. Although aggressive quantization addresses efficiency and memory constraints, preserving the quality of the model remains a challenge. To retain quality in previous aggressive quantization, we propose a new fractional bits quantization (\short) approach. The novelty is a simple yet effective idea: we progressively reduce the model’s precision from 32 to 4 bits per parameter, and exploit the fractional bits during optimization to maintain high generation quality. We show that the \short{} yields improved quality on a variety of diffusion models, including SD3.5-Medium, Sana, \pixart, and FLUX.1-schnell, while achieving $4-7%$ lower FiD than standard QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP). </p>
<blockquote>
<p>当前最先进的生成模型在图像合成或文本生成方面展现出了令人印象深刻的能力，通常依赖于大型模型。然而，由于手机内存和计算资源的有限，这些大型模型无法部署在手机上。量化方法可以降低模型参数的精度，从而实现高效计算，例如在INT8中。虽然激进量化解决了效率和内存约束问题，但保持模型的质量仍然是一个挑战。为了保持先前的激进量化的质量，我们提出了一种新的部分位量化（short）方法。其新颖之处在于一个简单而有效的想法：我们逐步将模型的精度从每个参数的32位降低到4位，并在优化过程中利用部分位来保持高水平的生成质量。我们证明，在多种扩散模型中，包括SD3.5-Medium、Sana、pixart和FLUX.1-schnell等模型使用short后，在达到比标准QAT低4-7%的FiD的同时，生成质量有所提高。最后，我们在搭载高通SM8750-AB骁龙8精英Hexagon Tensor处理器（HTP）的三星S25U上部署并运行了Sana。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14823v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大型模型广泛应用于图像合成和文本生成等生成式任务的同时，由于其内存占用和计算需求过大，无法部署于智能手机上。为此，研究人员提出一种名为FBQ（分数位量化）的新型量化方法，旨在降低模型参数精度以提升计算效率和节省内存占用。FBQ通过逐步降低模型参数精度至每参数4位，并在优化过程中利用分数位来保持生成质量。实验证明，FBQ能在多种扩散模型上实现更好的性能表现，且在特定的FID评价指标上相对于标准量化方法有更好的效果。最后成功在配备骁龙高性能计算平台骁龙神经处理单元的三星Galaxy S25U手机上部署并运行了模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型生成模型在图像合成和文本生成等领域表现卓越，但由于内存和计算限制无法直接部署于智能手机上。</li>
<li>量化方法能有效降低模型参数精度，提升计算效率和节省内存占用。但如何在保证效率的同时保持模型质量是一大挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14823">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14823v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14823v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14823v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models"><a href="#DEXTER-Diffusion-Guided-EXplanations-with-TExtual-Reasoning-for-Vision-Models" class="headerlink" title="DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models"></a>DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision   Models</h2><p><strong>Authors:Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato</strong></p>
<p>Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier’s decision process without access to training data or ground-truth labels. We demonstrate DEXTER’s flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter">https://github.com/perceivelab/dexter</a>. </p>
<blockquote>
<p>理解和解释机器学习模型的行为对于构建透明和可信赖的AI系统至关重要。我们引入了DEXTER，这是一个无需数据的框架，它采用扩散模型和大语言模型来生成视觉分类器的全局文本解释。DEXTER通过优化文本提示来合成能强烈激活目标分类器的类别条件图像。这些合成样本然后用于生成详细的自然语言报告，描述特定类别的决策模式和偏见。与以前的工作不同，DEXTER能够在无需访问训练数据或真实标签的情况下，提供关于分类器决策过程的自然语言解释。我们在三个任务上展示了DEXTER的灵活性：激活最大化、切片发现和去偏以及偏见解释，每个任务都展示了其揭示视觉分类器内部机制的能力。定量和定性评估，包括一项用户研究，表明DEXTER产生的输出准确且易于解释。在ImageNet、Waterbirds、CelebA和FairFaces上的实验证实，DEXTER在全局模型解释和类别级别偏见报告方面优于现有方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/perceivelab/dexter%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/perceivelab/dexter获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14741v1">PDF</a> Accepted to NeurIPS 2025 (spotlight)</p>
<p><strong>摘要</strong><br>    本摘要介绍了一个数据自由框架DEXTER，它通过扩散模型与大型语言模型生成视觉分类器的全局文本解释。DEXTER通过优化文本提示来合成能够强烈激活目标分类器的类别条件图像，并使用这些合成样本引出详细的自然语言报告来描述特定类别的决策模式和偏见。DEXTER无需访问训练数据或真实标签即可提供自然语言解释分类器的决策过程。在激活最大化、切片发现与去偏以及偏见解释三个任务上的演示显示了DEXTER揭示视觉分类器内部机制的能力。定量和定性评估，包括一项用户研究，均表明DEXTER产生的输出准确且易于解读。在ImageNet、Waterbirds、CelebA和FairFaces上的实验证实，DEXTER在全球模型解释和类别级别偏见报告方面优于现有方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>DEXTER是一个数据自由框架，用于生成视觉分类器的全局文本解释。</li>
<li>通过优化文本提示来合成类别条件图像，强烈激活目标分类器。</li>
<li>利用合成样本引出自然语言报告，描述特定类别的决策模式和偏见。</li>
<li>DEXTER无需访问训练数据或真实标签即可解释分类器的决策过程。</li>
<li>在多个任务上演示了DEXTER的能力，包括激活最大化、切片发现与去偏以及偏见解释。</li>
<li>定量和定性评估，包括用户研究，表明DEXTER产生的输出准确且易于解读。</li>
<li>在多个数据集上的实验证实，DEXTER在全球模型解释和类别级别偏见报告方面的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14741">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14741v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14741v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection"><a href="#A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection" class="headerlink" title="A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection"></a>A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection</h2><p><strong>Authors:Shivangi Yadav, Arun Ross</strong></p>
<p>An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method. </p>
<blockquote>
<p>虹膜生物识别系统可能会受到呈现攻击（PAs）的威胁，其中人工眼、打印的眼图像或美容隐形眼镜等伪制品会被呈现给系统。为了对抗这一点，已经开发了几种用于检测呈现的呈现攻击检测（PAD）方法。然而，由于缺乏用于训练和评估虹膜PAD技术的数据集，这主要是因为构建和成像PAs存在隐性的困难。为了解决这个问题，我们引入了多域图像翻译扩散风格生成对抗网络（MID-StyleGAN），这是一个新的生成合成眼图像的框架，它能够捕捉多个域中的呈现攻击和真实眼特征，如真实眼、打印的眼睛和美容隐形眼镜。MID-StyleGAN结合了扩散模型和生成对抗网络（GANs）的优点，生成逼真的多样化的合成数据。我们的方法利用多域架构，实现真实眼图像和不同呈现攻击域之间的翻译。模型采用适合眼数据的自适应损失函数，以保持域一致性。大量实验表明，MID-StyleGAN在生成高质量合成眼图像方面优于现有方法。生成的数据被用来显著提高PAD系统的性能，为解决虹膜和眼部生物识别中的数短缺问题提供了可扩展的解决方案。例如，在LivDet2020数据集上，在千分之一的误检率下，真实检测率从93.41%提高到98.72%，展示了所提出方法的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14314v1">PDF</a> </p>
<p><strong>Summary</strong>：为解决虹膜生物识别系统中因展示攻击（PAs）带来的安全问题，研究者开发了一种新型的多域图像转换扩散风格生成对抗网络（MID-StyleGAN）。此网络可生成合成眼部图像，覆盖多个领域如真实图像和打印图像、隐形眼镜等虚假图像的特点。该网络结合扩散模型和生成对抗网络（GANs）的优点生成逼真的多样性合成数据。在多个实验中，MID-StyleGAN表现优越，能有效提高虹膜生物识别系统的性能。对于LivDet2020数据集，在误报率为1%的情况下，真实检测率从93.41%提升至98.72%。这一方法为虹膜和眼部生物识别中数据稀缺问题提供了可扩展的解决方案。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>虹膜生物识别系统易受展示攻击（PAs）影响，需要开发应对方法。</li>
<li>MID-StyleGAN是一种新型框架，用于生成合成眼部图像，覆盖多个领域的特点。</li>
<li>MID-StyleGAN结合了扩散模型和生成对抗网络（GANs）的优点。</li>
<li>MID-StyleGAN采用多域架构，能在真实眼部图像和不同虚假图像领域之间进行转换。</li>
<li>模型使用针对眼部数据的自适应损失函数以保持领域一致性。</li>
<li>实验表明MID-StyleGAN在生成高质量合成眼部图像方面表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14314">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14314v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14314v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14314v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14314v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14314v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14314v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14314v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Nonparametric-Data-Attribution-for-Diffusion-Models"><a href="#Nonparametric-Data-Attribution-for-Diffusion-Models" class="headerlink" title="Nonparametric Data Attribution for Diffusion Models"></a>Nonparametric Data Attribution for Diffusion Models</h2><p><strong>Authors:Yutian Zhao, Chao Du, Xiaosen Zheng, Tianyu Pang, Min Lin</strong></p>
<p>Data attribution for generative models seeks to quantify the influence of individual training examples on model outputs. Existing methods for diffusion models typically require access to model gradients or retraining, limiting their applicability in proprietary or large-scale settings. We propose a nonparametric attribution method that operates entirely on data, measuring influence via patch-level similarity between generated and training images. Our approach is grounded in the analytical form of the optimal score function and naturally extends to multiscale representations, while remaining computationally efficient through convolution-based acceleration. In addition to producing spatially interpretable attributions, our framework uncovers patterns that reflect intrinsic relationships between training data and outputs, independent of any specific model. Experiments demonstrate that our method achieves strong attribution performance, closely matching gradient-based approaches and substantially outperforming existing nonparametric baselines. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/NDA">https://github.com/sail-sg/NDA</a>. </p>
<blockquote>
<p>生成模型的数据归因旨在量化单个训练样本对模型输出的影响。现有的扩散模型方法通常需要访问模型梯度或进行再训练，这限制了它们在专有或大规模环境中的适用性。我们提出了一种非参数归因方法，该方法完全基于数据运行，通过生成图像和训练图像之间的补丁级别相似性来衡量影响。我们的方法以最优分数函数的解析形式为基础，自然地扩展到多尺度表示，同时通过基于卷积的加速保持计算效率。除了产生空间可解释的归因外，我们的框架还揭示了反映训练数据和输出之间内在关系的模式，独立于任何特定模型。实验表明，我们的方法实现了强大的归因性能，与基于梯度的方法非常匹配，并且大大优于现有的非参数基线。代码可在<a target="_blank" rel="noopener" href="https://github.com/sail-sg/NDA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sail-sg/NDA中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14269v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种无需模型梯度或重新训练的全数据非参数归因方法，用于量化生成模型中单个训练样本对模型输出的影响。该方法基于最优评分函数的解析形式，通过生成图像与训练图像之间的斑块级相似性来测量影响，并自然地扩展到多尺度表示。该方法除了产生空间可解释的归因外，还揭示了训练数据与输出之间的内在关系。实验表明，该方法实现了强大的归因性能，与基于梯度的方法紧密匹配，并大幅超越了现有的非参数基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了针对扩散模型的全数据非参数归因方法，无需模型梯度或重新训练。</li>
<li>方法基于最优评分函数的解析形式，通过生成图像与训练图像的斑块级相似性来测量影响。</li>
<li>方法可自然地扩展到多尺度表示，并可通过卷积加速计算效率。</li>
<li>产生的空间可解释归因有助于理解模型决策过程。</li>
<li>该方法揭示了训练数据与模型输出之间的内在关系，这种关系独立于任何特定模型。</li>
<li>实验结果表明，该方法实现了强大的归因性能，与基于梯度的方法相匹配，并超越了现有的非参数基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14269v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14269v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Identity-Preserving-Image-to-Video-Generation-via-Reward-Guided-Optimization"><a href="#Identity-Preserving-Image-to-Video-Generation-via-Reward-Guided-Optimization" class="headerlink" title="Identity-Preserving Image-to-Video Generation via Reward-Guided   Optimization"></a>Identity-Preserving Image-to-Video Generation via Reward-Guided   Optimization</h2><p><strong>Authors:Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo Cao, Bo Zheng</strong></p>
<p>Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at \href{<a target="_blank" rel="noopener" href="https://ipro-alimama.github.io/%7D%7Bhttps://ipro-alimama.github.io/%7D">https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}</a>. </p>
<blockquote>
<p>近期图像到视频（I2V）生成的进展在从未静态图像合成高质量、时间连贯的视频方面取得了显著成效。在所有的I2V应用中，以人类为中心的视频生成占据很大一部分。然而，现有的I2V模型在保持输入人像与生成视频之间身份一致性方面遇到困难，特别是在视频中人物表情变化和动作显著时。当人脸只占图像的一小部分时，这个问题变得更为关键。由于人类对身份变化高度敏感，这为I2V生成提出了一个至关重要但尚未被充分研究的挑战。在本文中，我们提出了基于强化学习的身份保留奖励引导优化（IPRO）新型视频扩散框架，以提升身份保留能力。我们的方法不是引入辅助模块或改变模型架构，而是引入了一种直接有效的调整算法，该算法使用面部身份评分者对扩散模型进行优化。为了提升性能和加速收敛，我们的方法通过采样链的最后几步反向传播奖励信号，从而实现更丰富的梯度反馈。我们还提出了一种新型的面部评分机制，将真实视频中的面部视为面部特征池，提供多角度的面部信息以增强泛化能力。进一步引入了KL散度正则化以稳定训练并防止对奖励信号的过度拟合。在Wan 2.2 I2V模型和我们的内部I2V模型上的大量实验证明了我们的方法的有效性。我们的项目和代码可在<a target="_blank" rel="noopener" href="https://ipro-alimama.github.io/">https://ipro-alimama.github.io/</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14255v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于强化学习的人脸身份保留奖励导向优化（IPRO）视频扩散框架被提出，用于解决图像到视频生成中身份不一致的问题。此框架不通过引入辅助模块或改变模型架构，而是采用直接有效的调整算法，使用人脸身份评分者对扩散模型进行优化。通过改进性能并加速收敛，该方法通过采样链的最后几步反向传播奖励信号，提供丰富的梯度反馈。此外，还提出了一种新的面部评分机制，将真实视频中的面部作为面部特征池，提供多角度的面部信息以增强泛化能力。KL散度正则化被进一步引入以稳定训练并防止对奖励信号的过度拟合。实验证明该方法在Wang 2.2的I2V模型及自有I2V模型上均有效。项目和代码可在相应网站上查看。该研究的创新点在于优化了扩散模型在人脸图像生成视频时的身份一致性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>当前图像到视频（I2V）生成技术在保持人物身份一致性方面存在挑战，特别是在显著的表情变化和动作时。</li>
<li>提出了一种基于强化学习的身份保留奖励导向优化（IPRO）框架，旨在解决这一问题。</li>
<li>IPRO框架不使用额外的辅助模块或复杂的模型架构改变，而是采用直接有效的调整算法和人脸身份评分者来优化扩散模型。</li>
<li>通过在采样链的最后几步反向传播奖励信号，提高了性能并加速了收敛。</li>
<li>提出了一种新的面部评分机制，利用真实视频中的面部特征池提供多角度信息以增强泛化能力。</li>
<li>引入了KL散度正则化以稳定训练过程并防止模型对奖励信号的过度拟合。</li>
<li>在多个实验模型上的结果表明IPRO方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14255v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14255v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14255v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14255v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14255v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LOTA-Bit-Planes-Guided-AI-Generated-Image-Detection"><a href="#LOTA-Bit-Planes-Guided-AI-Generated-Image-Detection" class="headerlink" title="LOTA: Bit-Planes Guided AI-Generated Image Detection"></a>LOTA: Bit-Planes Guided AI-Generated Image Detection</h2><p><strong>Authors:Hongsong Wang, Renxi Cheng, Yang Zhang, Chaolei Han, Jie Gui</strong></p>
<p>The rapid advancement of GAN and Diffusion models makes it more difficult to distinguish AI-generated images from real ones. Recent studies often use image-based reconstruction errors as an important feature for determining whether an image is AI-generated. However, these approaches typically incur high computational costs and also fail to capture intrinsic noisy features present in the raw images. To solve these problems, we innovatively refine error extraction by using bit-plane-based image processing, as lower bit planes indeed represent noise patterns in images. We introduce an effective bit-planes guided noisy image generation and exploit various image normalization strategies, including scaling and thresholding. Then, to amplify the noise signal for easier AI-generated image detection, we design a maximum gradient patch selection that applies multi-directional gradients to compute the noise score and selects the region with the highest score. Finally, we propose a lightweight and effective classification head and explore two different structures: noise-based classifier and noise-guided classifier. Extensive experiments on the GenImage benchmark demonstrate the outstanding performance of our method, which achieves an average accuracy of \textbf{98.9%} (\textbf{11.9}%~$\uparrow$) and shows excellent cross-generator generalization capability. Particularly, our method achieves an accuracy of over 98.2% from GAN to Diffusion and over 99.2% from Diffusion to GAN. Moreover, it performs error extraction at the millisecond level, nearly a hundred times faster than existing methods. The code is at <a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA">https://github.com/hongsong-wang/LOTA</a>. </p>
<blockquote>
<p>GAN和Diffusion模型的快速发展使得区分AI生成的图像和真实图像变得更加困难。近期的研究经常将基于图像的重建误差作为判断图像是否由AI生成的重要特征。然而，这些方法通常会导致较高的计算成本，并且无法捕捉到原始图像中存在的内在噪声特征。为了解决这些问题，我们创新性地通过基于位平面的图像处理来改进误差提取，因为低位平面确实代表了图像中的噪声模式。我们引入了一种有效的位平面引导噪声图像生成方法，并利用了各种图像归一化策略，包括缩放和阈值化。接着，为了放大噪声信号，便于检测AI生成的图像，我们设计了一种最大梯度补丁选择方法，该方法应用多方向梯度来计算噪声分数，并选择得分最高的区域。最后，我们提出了一种轻便有效的分类头，并探索了两种不同结构：基于噪声的分类器和噪声引导的分类器。在GenImage基准测试上的广泛实验证明了我们方法的出色性能，该方法达到了平均准确率<strong>98.9%（上升11.9%）</strong>，并表现出出色的跨生成器泛化能力。特别是，我们的方法从GAN到Diffusion的准确率超过98.2%，而从Diffusion到GAN的准确率超过99.2%。此外，它以毫秒级的速度进行误差提取，几乎是现有方法的100倍。代码地址为：<a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA%E3%80%82">https://github.com/hongsong-wang/LOTA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14230v1">PDF</a> Published in the ICCV2025, COde is   <a target="_blank" rel="noopener" href="https://github.com/hongsong-wang/LOTA">https://github.com/hongsong-wang/LOTA</a></p>
<p><strong>Summary</strong><br>     近期GAN和Diffusion模型的发展使得区分AI生成图像和真实图像变得困难。传统方法主要依赖图像重建误差来判断图像是否由AI生成，但计算成本高且忽略图像本身的噪声特征。为此，我们采用基于位平面的图像处理方法进行误差提取，并利用各种图像归一化策略。此外，设计最大梯度补丁选择来放大噪声信号，进而提出轻量级有效的分类头。在GenImage基准测试上，我们的方法表现出卓越性能，平均准确率高达98.9%，且跨生成器泛化能力强。此外，我们的方法执行误差提取的速度接近毫秒级别，是现有方法的近百倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GAN和Diffusion模型的快速发展导致区分AI生成图像和真实图像更具挑战性。</li>
<li>传统方法依赖图像重建误差判断图像是否由AI生成，但计算成本高且忽略噪声特征。</li>
<li>采用基于位平面的图像处理方法进行误差提取，以捕捉图像中的噪声模式。</li>
<li>利用各种图像归一化策略，包括缩放和阈值化。</li>
<li>设计最大梯度补丁选择来放大噪声信号，便于检测AI生成的图像。</li>
<li>提出轻量级有效的分类头，包括基于噪声的分类器和引导噪声的分类器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14230">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14230v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14230v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14230v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14230v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.14230v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NoisePrints-Distortion-Free-Watermarks-for-Authorship-in-Private-Diffusion-Models"><a href="#NoisePrints-Distortion-Free-Watermarks-for-Authorship-in-Private-Diffusion-Models" class="headerlink" title="NoisePrints: Distortion-Free Watermarks for Authorship in Private   Diffusion Models"></a>NoisePrints: Distortion-Free Watermarks for Authorship in Private   Diffusion Models</h2><p><strong>Authors:Nir Goren, Oren Katzir, Abhinav Nakarmi, Eyal Ronen, Mahmood Sharif, Or Patashnik</strong></p>
<p>With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights. </p>
<blockquote>
<p>随着扩散模型在视觉内容生成领域的快速采纳，证明作者身份和保护版权变得至关重要。当模型所有者将其模型保持为私有并且可能不愿意或无法处理作者问题时，这一挑战尤为重要，这使得第三方验证变得必不可少。一种自然的解决方案是嵌入水印以供日后验证。然而，现有方法需要访问模型权重并依赖于计算密集型的程序，这使得它们不切实际且无法扩展。为了应对这些挑战，我们提出了一种轻量级的水印方案，该方案利用用于初始化扩散过程的随机种子作为作者身份的证明，而不修改生成过程。我们的关键观察结果是，从种子派生的初始噪声与生成的视觉内容高度相关。通过将哈希函数融入噪声采样过程，我们进一步确保了从内容中恢复有效的种子是不可能的。我们还展示了采样通过验证的替代种子是不可能的，并证明了我们的方法在多种操作下的稳健性。最后，我们展示了如何使用零知识证明来在不透露种子的情况下证明所有权。通过保持种子秘密，我们增加了去除水印的难度。我们在实验中在多个最先进的图像和视频扩散模型上验证了NoisePrints，证明了仅使用种子和输出进行高效验证的可能性，而无需访问模型权重。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13793v1">PDF</a> code available at: <a target="_blank" rel="noopener" href="https://github.com/nirgoren/NoisePrints">https://github.com/nirgoren/NoisePrints</a></p>
<p><strong>Summary</strong><br>     针对扩散模型生成视觉内容带来的版权和作者证明问题，特别是模型所有者选择保持模型私密性而带来的挑战，提出了一种轻量级的水印方案NoisePrints。该方案利用扩散过程初始化时使用的随机种子作为作者证明，无需修改生成过程。通过结合噪声采样过程中的哈希函数，确保从内容中恢复有效种子是不可能的。此外，该方案展示了使用零知识证明证明所有权的方法，保持种子的秘密性，提高了水印移除的难度。实验证明，NoisePrints在多个先进的扩散模型上有效，仅使用种子和输出即可实现高效验证，无需访问模型权重。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在视觉内容生成中的快速采纳使得版权和作者证明变得至关重要。</li>
<li>当模型所有者保持模型私密时，第三方验证变得尤为重要。</li>
<li>现有水印方法需要访问模型权重并依赖计算密集型程序，不实用且不可扩展。</li>
<li>NoisePrints利用扩散过程初始化时的随机种子作为作者证明，无需修改生成过程。</li>
<li>结合噪声采样中的哈希函数，确保从内容中有效恢复种子是不可能的。</li>
<li>使用零知识证明展示所有权的方法，提高水印移除的难度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13793">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13793v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13793v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generating-healthy-counterfactuals-with-denoising-diffusion-bridge-models"><a href="#Generating-healthy-counterfactuals-with-denoising-diffusion-bridge-models" class="headerlink" title="Generating healthy counterfactuals with denoising diffusion bridge   models"></a>Generating healthy counterfactuals with denoising diffusion bridge   models</h2><p><strong>Authors:Ana Lawry Aguila, Peirong Liu, Marina Crespo Aguirre, Juan Eugenio Iglesias</strong></p>
<p>Generating healthy counterfactuals from pathological images holds significant promise in medical imaging, e.g., in anomaly detection or for application of analysis tools that are designed for healthy scans. These counterfactuals should represent what a patient’s scan would plausibly look like in the absence of pathology, preserving individual anatomical characteristics while modifying only the pathological regions. Denoising diffusion probabilistic models (DDPMs) have become popular methods for generating healthy counterfactuals of pathology data. Typically, this involves training on solely healthy data with the assumption that a partial denoising process will be unable to model disease regions and will instead reconstruct a closely matched healthy counterpart. More recent methods have incorporated synthetic pathological images to better guide the diffusion process. However, it remains challenging to guide the generative process in a way that effectively balances the removal of anomalies with the retention of subject-specific features. To solve this problem, we propose a novel application of denoising diffusion bridge models (DDBMs) - which, unlike DDPMs, condition the diffusion process not only on the initial point (i.e., the healthy image), but also on the final point (i.e., a corresponding synthetically generated pathological image). Treating the pathological image as a structurally informative prior enables us to generate counterfactuals that closely match the patient’s anatomy while selectively removing pathology. The results show that our DDBM outperforms previously proposed diffusion models and fully supervised approaches at segmentation and anomaly detection tasks. </p>
<blockquote>
<p>在医学成像中，从病理图像生成健康的反事实（counterfactuals）具有巨大潜力，例如在异常检测或为健康扫描设计的分析工具的应用中。这些反事实应代表患者在没有病理的情况下扫描可能的样子，保留个人解剖特征，同时只修改病理区域。去噪扩散概率模型（DDPMs）已成为生成病理数据的健康反事实流行方法。通常，这涉及仅在健康数据上进行训练，假设部分去噪过程无法对疾病区域进行建模，而是重建一个匹配紧密的健康对应物。最近的方法已经结合了合成病理图像来更好地引导扩散过程。然而，如何引导生成过程仍然是一个挑战，需要在去除异常和保留主体特定特征之间取得有效平衡。为了解决这一问题，我们提出了一种去噪扩散桥梁模型（DDBMs）的新应用——与DDPM不同，DDBM的扩散过程不仅以初始点（即健康图像）为条件，也以最终点（即相应的合成病理图像）为条件。将病理图像视为结构信息丰富的先验，使我们能够生成与健康图像紧密匹配的反事实，同时有选择地去除病理。结果表明，我们的DDBM在分割和异常检测任务上优于先前提出的扩散模型和完全监督方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13684v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>生成健康反事实图像（counterfactuals）在医学成像中具有巨大潜力，例如在异常检测或为健康扫描设计的分析工具的应用中。这些反事实图像应代表在不存在病理的情况下，患者的扫描可能呈现的样子，保留个体解剖特征，同时只修改病理区域。去噪扩散概率模型（DDPM）已成为生成病理数据健康反事实的常见方法。通常，这是通过在健康数据上进行训练，假设部分去噪过程无法模拟疾病区域，而是重建紧密匹配的健康对应物。最近的方法已融入合成病理图像以更好地引导扩散过程。然而，如何引导生成过程以在消除异常和保留主体特定特征之间取得有效平衡仍然是一个挑战。为解决此问题，我们提出了一种去噪扩散桥梁模型（DDBM）的新应用，与DDPM不同，DDBM的扩散过程不仅受初始点（即健康图像）的影响，还受最终点（即合成的病理图像）的影响。将病理图像视为结构信息丰富的先验，使我们能够生成紧密匹配患者解剖结构的反事实图像，同时有选择地消除病理。结果表明，我们的DDBM在分割和异常检测任务上优于先前提出的扩散模型和完全监督的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>生成健康反事实图像在医学成像中具有重要意义，尤其在异常检测和分析工具的应用中。</li>
<li>去噪扩散概率模型（DDPM）已用于生成病理数据的健康反事实图像。</li>
<li>近期方法结合合成病理图像以改进扩散过程的指导。</li>
<li>引导生成过程以平衡消除异常和保留主体特定特征仍是挑战。</li>
<li>提出了一种新的去噪扩散桥梁模型（DDBM）应用，该模型受健康图像和合成病理图像两者影响。</li>
<li>DDBM能够生成紧密匹配患者解剖结构的反事实图像，同时有选择地消除病理。</li>
<li>DDBM在分割和异常检测任务上的性能优于其他扩散模型和完全监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13684v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13684v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13684v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FlashWorld-High-quality-3D-Scene-Generation-within-Seconds"><a href="#FlashWorld-High-quality-3D-Scene-Generation-within-Seconds" class="headerlink" title="FlashWorld: High-quality 3D Scene Generation within Seconds"></a>FlashWorld: High-quality 3D Scene Generation within Seconds</h2><p><strong>Authors:Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, Liujuan Cao</strong></p>
<p>We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100$\times$ faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model’s generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method. </p>
<blockquote>
<p>我们提出了FlashWorld，这是一种生成模型，能够在几秒钟内从单张图像或文本提示生成3D场景，与以前的工作相比，速度提高了10到100倍，同时拥有卓越的渲染质量。我们的方法放弃了传统的面向多视角（MV-oriented）的范式，该范式生成多视角图像用于随后的3D重建，而转向一种面向3D的方法，其中模型在生成多视角时直接产生3D高斯表示。在保障3D一致性的同时，面向3D的方法通常会出现视觉质量差的问题。FlashWorld包括双模式预训练阶段和跨模式后训练阶段，有效地结合了两种范式的优点。具体来说，我们利用视频扩散模型的先验知识，首先预训练一个双模式多视角扩散模型，同时支持MV-oriented和3D-oriented生成模式。为了缩小面向3D生成的品质差距，我们进一步提出了跨模式后训练蒸馏，通过匹配一致的面向3D模式的分布与高质量面向MV模式的分布来实现。这不仅提高了视觉质量，同时保持了3D一致性，还减少了推理所需的降噪步骤。此外，我们还提出了一种策略，利用大量的单视角图像和文本提示在此过程中，以增强模型对离群输入的一般化能力。大量实验证明了我们方法的优越性和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13678v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://imlixinyang.github.io/FlashWorld-Project-Page/">https://imlixinyang.github.io/FlashWorld-Project-Page/</a></p>
<p><strong>Summary</strong><br>    FlashWorld是一个生成模型，能从单张图片或文本提示在数秒内生成3D场景，较之前的工作快10~100倍，同时拥有卓越的渲染质量。它采用直接生成3D高斯表示的3D导向方法，保证3D一致性的同时，克服了传统多视角导向方法视觉质量差的缺陷。通过双模式预训练与跨模式后训练，结合了两种方法的优势。借助视频扩散模型的先验信息，进行预训练。为进一步缩小3D导向生成的质量差距，提出跨模式后训练蒸馏方法。这不仅提高了视觉质量，保持了3D一致性，还减少了推理所需的去噪步骤。此外，利用大量单视角图像和文本提示增强模型对离群输入的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlashWorld是一个快速生成3D场景的生成模型，较传统方法大大提升了生成速度。</li>
<li>它采用了直接的3D高斯表示生成方法，保证了3D场景的一致性。</li>
<li>FlashWorld通过双模式预训练和跨模式后训练，融合了多视角导向和3D导向方法的优点。</li>
<li>模型借助视频扩散模型的先验信息进行预训练，提高了性能。</li>
<li>跨模式后训练蒸馏方法用于提高3D导向生成的视觉质量，并减少了去噪步骤。</li>
<li>FlashWorld能够利用大量单视角图像和文本提示进行训练，增强了模型的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13678">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13678v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13678v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13678v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13678v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Steerable-Conditional-Diffusion-for-Domain-Adaptation-in-PET-Image-Reconstruction"><a href="#Steerable-Conditional-Diffusion-for-Domain-Adaptation-in-PET-Image-Reconstruction" class="headerlink" title="Steerable Conditional Diffusion for Domain Adaptation in PET Image   Reconstruction"></a>Steerable Conditional Diffusion for Domain Adaptation in PET Image   Reconstruction</h2><p><strong>Authors:George Webber, Alexander Hammers, Andrew P. King, Andrew J. Reader</strong></p>
<p>Diffusion models have recently enabled state-of-the-art reconstruction of positron emission tomography (PET) images while requiring only image training data. However, domain shift remains a key concern for clinical adoption: priors trained on images from one anatomy, acquisition protocol or pathology may produce artefacts on out-of-distribution data. We propose integrating steerable conditional diffusion (SCD) with our previously-introduced likelihood-scheduled diffusion (PET-LiSch) framework to improve the alignment of the diffusion model’s prior to the target subject. At reconstruction time, for each diffusion step, we use low-rank adaptation (LoRA) to align the diffusion model prior with the target domain on the fly. Experiments on realistic synthetic 2D brain phantoms demonstrate that our approach suppresses hallucinated artefacts under domain shift, i.e. when our diffusion model is trained on perturbed images and tested on normal anatomy, our approach suppresses the hallucinated structure, outperforming both OSEM and diffusion model baselines qualitatively and quantitatively. These results provide a proof-of-concept that steerable priors can mitigate domain shift in diffusion-based PET reconstruction and motivate future evaluation on real data. </p>
<blockquote>
<p>扩散模型最近仅使用图像训练数据就实现了正电子发射断层扫描（PET）图像的先进重建。然而，领域偏移仍然是临床采用的关键问题：在一种解剖学、采集协议或病理学图像上训练的先验可能会在超出分布范围的数据上产生伪影。我们提出将可控制条件扩散（SCD）与我们先前引入的似然调度扩散（PET-LiSch）框架相结合，以提高扩散模型先验与目标主题的对齐程度。在重建过程中，对于每个扩散步骤，我们使用低秩适应（LoRA）来实时调整扩散模型先验与目标域的对齐。在真实合成二维脑幻影上的实验表明，我们的方法在领域偏移时抑制了幻觉伪影，即当我们的扩散模型在受干扰的图像上进行训练并在正常解剖学上进行测试时，我们的方法抑制了幻觉结构，在定性和定量方面都优于OSEM和扩散模型基线。这些结果证明了可控制先验可以减轻扩散式PET重建中的领域偏移，并激励我们在真实数据上进行未来评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13441v1">PDF</a> Accepted for oral presentation at IEEE NSS MIC RTSD 2025 (submitted   May 2025; accepted July 2025; to be presented Nov 2025)</p>
<p><strong>Summary</strong><br>     扩散模型通过图像训练数据实现了正电子发射断层扫描（PET）图像的重构技术前沿。针对临床应用中存在的领域偏移问题，我们提出将可控制的条件扩散与先前引入的似然调度扩散框架相结合，以提高扩散模型先验与目标主体之间的对齐程度。在重构过程中，针对每个扩散步骤，我们采用低阶适配技术，以即时对齐目标域的扩散模型先验。实验表明，我们的方法在领域偏移情况下抑制了幻觉伪影的产生，即当扩散模型在扰动图像上进行训练并在正常解剖结构上进行测试时，我们的方法能够抑制幻觉结构，在定性和定量上均优于OSEM和扩散模型基线。这为可控制先验在扩散式PET重建中缓解领域偏移问题提供了概念验证，并激励我们在真实数据上进行未来评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已用于PET图像重构，仅依赖图像训练数据。</li>
<li>领域偏移是临床采纳的关键问题，现有模型的先验可能在新数据上产生伪影。</li>
<li>提出结合可控制的条件扩散和似然调度扩散框架，提高扩散模型先验与目标主体间的对齐。</li>
<li>在重构过程中采用低阶适配技术，即时对齐目标域。</li>
<li>实验证明，在领域偏移情况下，新方法能抑制幻觉伪影。</li>
<li>与现有方法相比，新方法在定性和定量评估上均表现更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13441v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13441v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Ultra-High-Resolution-Image-Inpainting-with-Patch-Based-Content-Consistency-Adapter"><a href="#Ultra-High-Resolution-Image-Inpainting-with-Patch-Based-Content-Consistency-Adapter" class="headerlink" title="Ultra High-Resolution Image Inpainting with Patch-Based Content   Consistency Adapter"></a>Ultra High-Resolution Image Inpainting with Patch-Based Content   Consistency Adapter</h2><p><strong>Authors:Jianhui Zhang, Sheng Cheng, Qirui Sun, Jia Liu, Wang Luyang, Chaoyu Feng, Chen Fang, Lei Lei, Jue Wang, Shuaicheng Liu</strong></p>
<p>In this work, we present Patch-Adapter, an effective framework for high-resolution text-guided image inpainting. Unlike existing methods limited to lower resolutions, our approach achieves 4K+ resolution while maintaining precise content consistency and prompt alignment, two critical challenges in image inpainting that intensify with increasing resolution and texture complexity. Patch-Adapter leverages a two-stage adapter architecture to scale the diffusion model’s resolution from 1K to 4K+ without requiring structural overhauls: (1) Dual Context Adapter learns coherence between masked and unmasked regions at reduced resolutions to establish global structural consistency; and (2) Reference Patch Adapter implements a patch-level attention mechanism for full-resolution inpainting, preserving local detail fidelity through adaptive feature fusion. This dual-stage architecture uniquely addresses the scalability gap in high-resolution inpainting by decoupling global semantics from localized refinement. Experiments demonstrate that Patch-Adapter not only resolves artifacts common in large-scale inpainting but also achieves state-of-the-art performance on the OpenImages and Photo-Concept-Bucket datasets, outperforming existing methods in both perceptual quality and text-prompt adherence. </p>
<blockquote>
<p>在这项工作中，我们提出了Patch-Adapter，这是一个用于高分辨率文本引导的图像补全的有效的框架。不同于现有仅限于较低分辨率的方法，我们的方法实现了4K+分辨率，同时保持了精确的内容一致性和提示对齐，这是图像补全中的两个关键挑战，随着分辨率和纹理复杂性的增加，这两个挑战会加剧。Patch-Adapter利用两阶段适配器架构，无需进行结构性的大规模改造，即可将扩散模型的分辨率从1K扩展到4K+：（1）双上下文适配器在降低的分辨率上学习掩码区域和非掩码区域之间的连贯性，以建立全局结构一致性；（2）参考补丁适配器实现了补丁级别的注意力机制，用于全分辨率补全，通过自适应特征融合保留局部细节保真度。这种双阶段架构通过解耦全局语义和局部细化，独特地解决了高分辨率补全中的可扩展性差距。实验表明，Patch-Adapter不仅解决了大规模补全中常见的伪影问题，还在OpenImages和Photo-Concept-Bucket数据集上实现了最先进的性能，在感知质量和文本提示遵循方面都优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了Patch-Adapter框架，用于高分辨率文本引导的图像补全。该框架实现了4K+分辨率的图像补全，维持了内容的一致性和文本提示的对齐，解决了随着分辨率和纹理复杂度增加而加剧的图像补全中的两个关键挑战。Patch-Adapter利用两阶段适配器架构，无需结构大改，即可将扩散模型的分辨率从1K扩展到4K+。实验表明，Patch-Adapter不仅解决了大规模补全中的常见伪影问题，还在OpenImages和Photo-Concept-Bucket数据集上实现了最先进的性能，在感知质量和文本提示遵循方面都超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Patch-Adapter是一个用于高分辨率文本引导图像补全的框架。</li>
<li>该框架实现了4K+分辨率的图像补全。</li>
<li>Patch-Adapter解决了内容一致性和文本提示对齐这两个关键挑战。</li>
<li>该框架采用两阶段适配器架构，从1K扩展到4K+分辨率，无需大规模的结构调整。</li>
<li>第一阶段是双语境适配器，旨在建立全局结构一致性。</li>
<li>第二阶段是参考补丁适配器，采用补丁级别的注意力机制进行全分辨率补全，保留局部细节保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13419v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13419v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13419v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.13419v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SceneTextStylizer-A-Training-Free-Scene-Text-Style-Transfer-Framework-with-Diffusion-Model"><a href="#SceneTextStylizer-A-Training-Free-Scene-Text-Style-Transfer-Framework-with-Diffusion-Model" class="headerlink" title="SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework   with Diffusion Model"></a>SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework   with Diffusion Model</h2><p><strong>Authors:Honghui Yuan, Keiji Yanai</strong></p>
<p>With the rapid development of diffusion models, style transfer has made remarkable progress. However, flexible and localized style editing for scene text remains an unsolved challenge. Although existing scene text editing methods have achieved text region editing, they are typically limited to content replacement and simple styles, which lack the ability of free-style transfer. In this paper, we introduce SceneTextStylizer, a novel training-free diffusion-based framework for flexible and high-fidelity style transfer of text in scene images. Unlike prior approaches that either perform global style transfer or focus solely on textual content modification, our method enables prompt-guided style transformation specifically for text regions, while preserving both text readability and stylistic consistency. To achieve this, we design a feature injection module that leverages diffusion model inversion and self-attention to transfer style features effectively. Additionally, a region control mechanism is introduced by applying a distance-based changing mask at each denoising step, enabling precise spatial control. To further enhance visual quality, we incorporate a style enhancement module based on the Fourier transform to reinforce stylistic richness. Extensive experiments demonstrate that our method achieves superior performance in scene text style transformation, outperforming existing state-of-the-art methods in both visual fidelity and text preservation. </p>
<blockquote>
<p>随着扩散模型的快速发展，风格转换已经取得了显著的进步。然而，场景文本的可灵活性和本地化的风格编辑仍然是一个未解决的难题。虽然现有的场景文本编辑方法已经实现了文本区域的编辑，但它们通常仅限于内容替换和简单风格，缺乏自由风格转换的能力。在本文中，我们介绍了SceneTextStylizer，这是一种无需训练的新型基于扩散的框架，用于场景图像中文本的灵活和高保真风格转换。不同于之前的全局风格转换方法或仅专注于文本内容修改的方法，我们的方法通过提示引导进行文本区域的风格转换，同时保留文本的可读性和风格的一致性。为了实现这一点，我们设计了一个特征注入模块，该模块利用扩散模型反演和自我注意来有效地转移风格特征。此外，通过每个去噪步骤中应用基于距离的变化掩膜，引入了区域控制机制，实现了精确的空间控制。为了进一步提高视觉质量，我们结合基于傅里叶变换的风格增强模块，以增强风格的丰富性。大量实验表明，我们的方法在场景文本风格转换方面达到了卓越的性能，在视觉保真度和文本保留方面都优于现有的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10910v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着扩散模型技术的快速发展，场景文本的风格转换取得了显著的进步。然而，实现灵活且局部化的文本风格编辑仍然是一个挑战。本文提出了一种无需训练的基于扩散的场景文本风格转换器（SceneTextStylizer），能够在场景图像中实现灵活且高保真度的文本风格转换。不同于现有的全局风格转换或仅关注文本内容修改的方法，我们的方法通过提示引导进行文本区域的特定风格转换，同时保持文本可读性和风格一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的快速发展推动了场景文本风格转换的进步。</li>
<li>当前场景文本编辑方法主要局限于内容替换和简单风格，缺乏自由风格转换能力。</li>
<li>SceneTextStylizer是一个无需训练的基于扩散的框架，可实现场景文本的高保真风格转换。</li>
<li>该方法通过特征注入模块和自注意力机制实现风格特征的转移。</li>
<li>通过应用基于距离变化的遮挡掩膜，实现了对文本区域的精确空间控制。</li>
<li>引入基于傅里叶变换的风格增强模块，提高了风格丰富性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10910">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.10910v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.10910v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.10910v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.10910v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="One-Stone-with-Two-Birds-A-Null-Text-Null-Frequency-Aware-Diffusion-Models-for-Text-Guided-Image-Inpainting"><a href="#One-Stone-with-Two-Birds-A-Null-Text-Null-Frequency-Aware-Diffusion-Models-for-Text-Guided-Image-Inpainting" class="headerlink" title="One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion   Models for Text-Guided Image Inpainting"></a>One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion   Models for Text-Guided Image Inpainting</h2><p><strong>Authors:Haipeng Liu, Yang Wang, Meng Wang</strong></p>
<p>Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from <a target="_blank" rel="noopener" href="https://github.com/htyjers/NTN-Diff">https://github.com/htyjers/NTN-Diff</a>. </p>
<blockquote>
<p>文本引导的图像修复旨在根据文本提示重建掩码区域，长期以来的挑战在于保持未掩码区域的完整性，同时实现未掩码和修复掩码区域之间的语义一致性。以前的方法未能同时解决这两个问题，通常只能解决其中之一。我们观察到，这种情况源于混合（例如中低频）频带纠缠，这些频带编码了不同的图像属性，在去噪过程中对文本提示的敏感性不同。在本文中，我们提出了一种用于文本引导的图像修复的零文本零频率感知扩散模型，称为NTN-Diff。它通过分解掩码和未掩码区域之间的语义一致性，针对每个频带保持未掩码区域的完整性，以克服这两个挑战。基于扩散过程，我们将去噪过程进一步分为早期（高级噪声）和晚期（低级噪声）阶段，其中在去噪过程中解开中低频带。观察到稳定的中频带在文本引导的去噪过程中逐步降噪以实现语义对齐，同时作为零文本去噪过程的指导来降噪低频带在掩码区域。随后在晚期阶段进行后续文本引导的去噪过程，以实现掩码和未掩码区域之间的中低频带的语义一致性，同时保持未掩码区域的完整性。大量实验验证了NTN-Diff在文本引导扩散模型方面的优越性超过了当前最先进的扩散模型。我们的代码可从<a target="_blank" rel="noopener" href="https://github.com/htyjers/NTN-Diff%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/htyjers/NTN-Diff访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08273v4">PDF</a> 27 pages, 11 figures, to appear at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为NTN-Diff的频率感知扩散模型，用于文本引导的图像修复。该模型通过分解掩码区域和非掩码区域之间的语义一致性，针对每个频率范围进行一致性的训练，以此解决掩码区域的重建和非掩码区域的保留两个挑战。模型在降噪过程中将图像分为早期和晚期两个阶段，通过文本引导降噪处理中频区域，以实现对非掩码区域的指导并保持一致性。最终实现了与最新扩散模型相比更为优越的文本引导图像修复性能。有关代码可访问链接：[访问链接]。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种新的频率感知扩散模型NTN-Diff用于文本引导的图像修复任务。</li>
<li>该模型通过处理每个频率带以实现掩码和非掩码区域之间的语义一致性。</li>
<li>模型解决了在图像修复过程中保持非掩码区域的问题。</li>
<li>通过将降噪过程分为两个阶段（早期和晚期），模型能够更有效地处理文本引导下的降噪过程。</li>
<li>中频区域的稳定处理作为非掩码区域的指导，确保了掩码区域的一致性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08273v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08273v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08273v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08273v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08273v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Real-Time-Motion-Controllable-Autoregressive-Video-Diffusion"><a href="#Real-Time-Motion-Controllable-Autoregressive-Video-Diffusion" class="headerlink" title="Real-Time Motion-Controllable Autoregressive Video Diffusion"></a>Real-Time Motion-Controllable Autoregressive Video Diffusion</h2><p><strong>Authors:Kesen Zhao, Jiaxin Shi, Beier Zhu, Junbao Zhou, Xiaolong Shen, Yuan Zhou, Qianru Sun, Hanwang Zhang</strong></p>
<p>Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: <a target="_blank" rel="noopener" href="https://kesenzhao.github.io/AR-Drag.github.io/">https://kesenzhao.github.io/AR-Drag.github.io/</a>. </p>
<blockquote>
<p>实时运动控制视频生成仍然具有挑战性，这主要是由于双向扩散模型的固有延迟和缺乏有效的自回归（AR）方法。现有的AR视频扩散模型仅限于简单的控制信号或文本到视频生成，并且在少步骤生成中经常遭受质量下降和运动伪影的问题。为了应对这些挑战，我们提出了AR-Drag，这是第一个结合强化学习的少数步骤AR视频扩散模型，用于实时图像到视频生成具有多种运动控制。我们首先微调基础I2V模型以支持基本运动控制，然后通过基于轨迹的奖励模型进一步改进。我们的设计通过自我滚动机制保留了马尔可夫属性，并通过在降噪步骤中选择性引入随机性来加速训练。大量实验表明，AR-Drag实现了高视觉保真度和精确的运动对齐，与最先进的运动控制VDM相比，延迟时间大大降低，而且仅使用1.3B参数。更多可视化内容可在我们的项目页面找到：<a target="_blank" rel="noopener" href="https://kesenzhao.github.io/AR-Drag.github.io/">https://kesenzhao.github.io/AR-Drag.github.io/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08131v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对实时运动控制视频生成面临的挑战，如双向扩散模型的固有延迟和缺乏有效的自回归（AR）方法，我们提出了AR-Drag模型。它是首个结合强化学习（RL）的少数步骤自回归视频扩散模型，支持实时图像到视频的生成，并具有多样的运动控制。AR-Drag通过微调基础I2V模型以支持基本运动控制，并通过基于轨迹的奖励模型进行强化学习进一步改进。设计保留了马尔可夫属性，通过自我滚动机制加速训练，并在去噪步骤中选择性地引入随机性。实验表明，AR-Drag在视觉保真度和精确运动对齐方面表现出色，相较于先进的运动控制VDM模型大幅降低延迟，且仅使用1.3B参数。更多可视化内容请见我们的项目页面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AR-Drag是首个结合强化学习的少数步骤自回归视频扩散模型，旨在解决实时运动控制视频生成中的挑战。</li>
<li>模型支持图像到视频的实时生成并具有多样的运动控制功能。</li>
<li>AR-Drag通过微调基础I2V模型以支持基本运动控制，并采用基于轨迹的奖励模型进行强化学习改进。</li>
<li>设计保留了马尔可夫属性，并通过自我滚动机制加速训练过程。</li>
<li>模型在选择性步骤中引入随机性以优化性能。</li>
<li>实验结果表明，AR-Drag在视觉保真度和精确运动对齐方面超越现有技术。</li>
<li>AR-Drag相较于其他先进的运动控制VDM模型大幅降低延迟，且使用较少的参数（仅1.3B）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08131v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08131v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2510.08131v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On"><a href="#ART-VITON-Measurement-Guided-Latent-Diffusion-for-Artifact-Free-Virtual-Try-On" class="headerlink" title="ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On"></a>ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual   Try-On</h2><p><strong>Authors:Junseo Park, Hyeryung Jang</strong></p>
<p>Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines. </p>
<blockquote>
<p>虚拟试衣（VITON）旨在生成用户穿戴目标服饰的现实图像，需要在试衣区域进行精确服饰对齐，并在非试衣区域忠实保留身份和背景。虽然潜在扩散模型（LDM）已经提高了对齐和细节合成的能力，但保留非试衣区域仍然具有挑战性。一种常见的后处理方法是直接替换这些区域以使用原始内容，但突兀的过渡通常会产生边界伪影。为了克服这一问题，我们将VITON重新表述为一个线性逆问题，并采用轨迹对齐求解器，逐步强制执行测量一致性，减少非试衣区域的突兀变化。然而，现有求解器在生成过程中仍存在语义漂移问题，导致出现伪影。我们提出了ART-VITON，一种测量指导的扩散框架，确保测量依从性同时保持无伪影合成。我们的方法集成了基于残差先验的初始化，以减轻训练推理不匹配问题，以及无伪影测量指导采样，结合了数据一致性、频率级校正和周期性标准去噪。在VITON-HD、DressCode和SHHQ-1.0上的实验表明，ART-VITON有效地保留了身份和背景，消除了边界伪影，并且相较于最先进的基线方法，在视觉保真度和稳健性方面都有了一致的提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25749v2">PDF</a> 21 pages</p>
<p><strong>摘要</strong></p>
<p>本文介绍了虚拟试穿（VITON）技术面临的挑战，包括精确对准衣物、在非试穿区域保持身份和背景的真实性。虽然潜在扩散模型（LDM）提高了对准和细节合成的能力，但保留非试穿区域仍然具有挑战性。为了解决这个问题，本文提出了一种新的测量方法引导扩散框架ART-VITON，确保测量一致性并维持无瑕疵的合成效果。通过结合残差先验初始化与数据一致性、频率级校正和周期性标准去噪等方法，ART-VITON可有效保留身份和背景信息，消除边界伪影，并在视觉保真度和稳健性方面超越现有技术基线。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>VITON技术的目标是生成一个人穿着目标衣物的真实图像，要求精确对准衣物，同时保持非试穿区域的身份和背景真实性。</li>
<li>LDM在虚拟试衣技术中有助于提高对准和细节合成能力。</li>
<li>非试穿区域的保留是虚拟试衣技术中的一大挑战，因为直接替换这些区域常常会导致边界伪影。</li>
<li>本文提出了一种新的测量方法引导扩散框架ART-VITON来解决这一问题，通过渐进地强制执行测量一致性来减少非试穿区域的突兀变化。</li>
<li>ART-VITON结合残差先验初始化来缓解训练推理不匹配的问题。</li>
<li>通过数据一致性、频率级校正和周期性标准去噪等技术结合的采样方法，ART-VITON可实现无瑕疵的合成效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25749">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.25749v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.25749v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.25749v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.25749v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.25749v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition"><a href="#Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition" class="headerlink" title="Does FLUX Already Know How to Perform Physically Plausible Image   Composition?"></a>Does FLUX Already Know How to Perform Physically Plausible Image   Composition?</h2><p><strong>Authors:Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</strong></p>
<p>Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication. </p>
<blockquote>
<p>图像组合旨在无缝插入用户指定的对象到新场景中，但现有模型在处理复杂光照（例如准确阴影、水面反射）和多样化、高分辨率输入时遇到困难。现代文本到图像扩散模型（例如SD3.5、FLUX）已经编码了重要的物理和分辨率先验知识，但缺乏一个框架来释放它们，而不求助于潜在的反转，这通常会将物体的姿势锁定在上下文不适当的方向上，或脆弱的注意力手术。我们提出了SHINE，这是一个无需训练的无缝、高保真插入中性化错误框架。SHINE引入了流形引导锚点损失，利用预训练的定制适配器（例如IP-Adapter）来引导潜在表示以忠实于主题表示，同时保持背景完整性。提出退化抑制指导和自适应背景融合，以进一步消除低质量输出和可见接缝。为了解决缺乏严格基准测试的问题，我们推出了ComplexCompo，具有多种分辨率和挑战性条件，如低光照、强烈照明、复杂阴影和反射表面。在ComplexCompo和DreamEditBench上的实验表明，我们在标准指标（例如DINOv2）和人类对齐得分（例如DreamSim、ImageReward、VisionReward）上达到了最新性能。代码和基准测试将在发表时公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21278v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SHINE框架，它是一种无需训练即可实现无缝、高保真插入的技术，通过引入流形引导锚点损失和预训练定制适配器，能够在不破坏背景完整性的情况下，忠实呈现主题表示。此外，还提出了退化抑制指导和自适应背景融合等方法，以消除低质量输出和可见接缝。为解决缺乏严格基准的问题，引入了ComplexCompo基准测试，以展示在各种分辨率和挑战性条件下的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SHINE框架实现了无缝、高保真插入技术，解决了现有模型在处理复杂光照和多样化高分辨率输入时的困难。</li>
<li>SHINE引入了流形引导锚点损失，利用预训练的定制适配器来指导潜在表示，忠实呈现主题表示同时保持背景完整性。</li>
<li>通过提出退化抑制指导和自适应背景融合等方法，进一步消除了低质量输出和可见接缝。</li>
<li>ComplexCompo基准测试包含了各种分辨率和具有挑战性的条件，如低光照、强烈照明、复杂阴影和反射表面，用于评估模型性能。</li>
<li>实验结果表明，SHINE在ComplexCompo和DreamEditBench等基准测试上达到了最先进的性能，得到了标准指标和人类对齐指标的验证。</li>
<li>SHINE的实现不需要额外的训练，提供了一种高效、实用的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.21278v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.21278v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.21278v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.21278v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.21278v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ComposeMe-Attribute-Specific-Image-Prompts-for-Controllable-Human-Image-Generation"><a href="#ComposeMe-Attribute-Specific-Image-Prompts-for-Controllable-Human-Image-Generation" class="headerlink" title="ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image   Generation"></a>ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image   Generation</h2><p><strong>Authors:Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, Kfir Aberman</strong></p>
<p>Generating high-fidelity images of humans with fine-grained control over attributes such as hairstyle and clothing remains a core challenge in personalized text-to-image synthesis. While prior methods emphasize identity preservation from a reference image, they lack modularity and fail to provide disentangled control over specific visual attributes. We introduce a new paradigm for attribute-specific image prompting, in which distinct sets of reference images are used to guide the generation of individual aspects of human appearance, such as hair, clothing, and identity. Our method encodes these inputs into attribute-specific tokens, which are injected into a pre-trained text-to-image diffusion model. This enables compositional and disentangled control over multiple visual factors, even across multiple people within a single image. To promote natural composition and robust disentanglement, we curate a cross-reference training dataset featuring subjects in diverse poses and expressions, and propose a multi-attribute cross-reference training strategy that encourages the model to generate faithful outputs from misaligned attribute inputs while adhering to both identity and textual conditioning. Extensive experiments show that our method achieves state-of-the-art performance in accurately following both visual and textual prompts. Our framework paves the way for more configurable human image synthesis by combining visual prompting with text-driven generation. Webpage is available at: <a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/">https://snap-research.github.io/composeme/</a>. </p>
<blockquote>
<p>生成具有精细属性控制的高保真人类图像，如发型和服装，仍然是个性化文本到图像合成中的核心挑战。虽然之前的方法强调从参考图像中保留身份，但它们缺乏模块化，无法提供对特定视觉属性的分离控制。我们引入了一种新的属性特定图像提示范式，其中使用不同的参考图像来指导人类外观的各个方面，如头发、服装和身份。我们的方法将这些输入编码为属性特定令牌，并注入预训练的文本到图像扩散模型中。这实现了对多个视觉因素的组合和分离控制，甚至在单个图像中的多个人之间也是如此。为了促进自然的构图和稳健的分离，我们整理了一个跨参考训练数据集，其中包括不同姿势和表情的主体，并提出了一种多属性跨参考训练策略，鼓励模型在身份和文本条件的同时，从错位属性输入生成忠实输出。大量实验表明，我们的方法在准确遵循视觉和文本提示方面达到了最新性能。我们的框架通过结合视觉提示和文本驱动生成，为人像合成铺平了更加可配置的道路。网页地址为：[<a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/]">https://snap-research.github.io/composeme/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18092v2">PDF</a> Accepted to SIGGRAPH Asia 2025, webpage:   <a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/">https://snap-research.github.io/composeme/</a></p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了一种新型属性特定图像提示方法，采用多种参考图像来指导人物外观的各个方面生成，如头发、服装和身份等。通过编码这些输入成特定属性标记，注入预训练文本到图像扩散模型，实现对多个视觉因素的分离控制，甚至在单张图像中多人之间也能实现。该研究采用跨参考训练数据集和多属性跨参考训练策略，促进了自然构图和稳健的分离。该方法在准确遵循视觉和文本提示方面达到领先水平，为通过视觉提示和文本驱动生成更可配置的人物图像合成铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入新型属性特定图像提示方法，使用多种参考图像指导人物外观生成。</li>
<li>通过属性特定标记编码输入，注入预训练文本到图像扩散模型，实现多视觉因素的分离控制。</li>
<li>采用跨参考训练数据集，促进自然构图和稳健的分离。</li>
<li>提出多属性跨参考训练策略，鼓励模型在属性输入错位时生成忠实输出。</li>
<li>方法在遵循视觉和文本提示方面达到领先水平。</li>
<li>该研究为更可配置的人物图像合成铺平了道路，结合视觉提示和文本驱动生成。</li>
<li>研究成果可通过网页（<a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/%EF%BC%89%E8%AE%BF%E9%97%AE%E3%80%82">https://snap-research.github.io/composeme/）访问。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18092">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.18092v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.18092v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.18092v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.18092v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.18092v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.18092v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective"><a href="#AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective" class="headerlink" title="AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective"></a>AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective</h2><p><strong>Authors:Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</strong></p>
<p>Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. In addition, to ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text&#x2F;audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized Phoneme-Frame Causal Attention Mask to support multi-step parallel acceleration. Extensive experiments conducted on both Chinese (CMLR) and English (HDTF) datasets demonstrate that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution. </p>
<blockquote>
<p>头部动画生成主要关注从音频输入生成逼真的面部视频。继生成对抗网络（GANs）之后，扩散模型由于其强大的生成能力已成为主流。然而，扩散过程的固有局限性通常会导致帧间闪烁和推理缓慢，从而限制了其实际部署。为了解决这一问题，我们引入了AvatarSync，这是一个基于音素表示的自回归框架，能够从单个参考图像生成逼真且可控的头部动画，直接由文本或音频输入驱动。为了减轻闪烁并确保连续性，AvatarSync利用了一个自回归管道，增强了时间建模。此外，为了确保可控性，我们引入了音素（即语音的基本单元），构建了从文本&#x2F;音频到音素的多对一映射，实现了精确的音素到视觉的对齐。另外，为了进一步加速推理，我们采用了两阶段生成策略，将语义建模与视觉动态解耦，并引入定制的音素帧因果注意力掩码，支持多步并行加速。在中文（CMLR）和英文（HDTF）数据集上进行的广泛实验表明，AvatarSync在视觉保真度、时间一致性和计算效率方面优于现有的头部动画方法，提供了一种可扩展且可控的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12052v2">PDF</a> </p>
<p><strong>Summary</strong><br>动画从音频输入生成逼真的面部视频的技术越来越受欢迎。本文引入了一个基于语音表示的自回归框架AvatarSync，用于解决现有技术的帧间闪烁和推理速度慢的问题。该框架可从单个参考图像生成可控的头部动画，并具有高度真实感。此外，采用语音特征构建了一种文本到语音的多对一映射，实现精确的语音特征对齐，并利用两级生成策略加速推理过程。实验证明，AvatarSync在视觉保真度、时间一致性和计算效率方面优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>说话人头像动画技术正受到关注，基于扩散模型的GANs已成为主流。</li>
<li>现有技术存在帧间闪烁和推理速度慢的问题。</li>
<li>AvatarSync框架使用自回归方法解决这些问题，生成真实可控的头部动画。</li>
<li>利用语音特征构建文本到语音的多对一映射，提高精度和连续性。</li>
<li>采用两级生成策略加速推理过程，提高计算效率。</li>
<li>实验证明AvatarSync在视觉保真度方面优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.12052v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.12052v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.12052v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2509.12052v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UniEgoMotion-A-Unified-Model-for-Egocentric-Motion-Reconstruction-Forecasting-and-Generation"><a href="#UniEgoMotion-A-Unified-Model-for-Egocentric-Motion-Reconstruction-Forecasting-and-Generation" class="headerlink" title="UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,   Forecasting, and Generation"></a>UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,   Forecasting, and Generation</h2><p><strong>Authors:Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli</strong></p>
<p>Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR&#x2F;VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion’s simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications. </p>
<blockquote>
<p>以自我为中心的动态生成与预测，并结合场景上下文，对于增强AR&#x2F;VR体验、改善人机交互、推动辅助技术和实现自适应医疗解决方案至关重要。通过从第一人称视角准确预测和模拟动作，我们可以实现这一点。然而，现有的方法主要集中在第三人称动作合成与结构化三维场景上下文的结合上，这在现实世界的以自我为中心的环境中效果有限。在这种环境中，有限的视野、频繁的遮挡和动态相机阻碍了场景感知。为了弥补这一差距，我们引入了以自我为中心的动态生成和以自我为中心的动态预测两个新任务，这两个任务利用第一人称图像进行场景感知动作合成，无需依赖明确的三维场景。我们提出了UniEgoMotion，这是一种统一的条件运动扩散模型，具有新颖的头部位置动态表示形式，适用于以自我为中心的设备。UniEgoMotion简单而有效的设计支持以自我为中心的动作重建、预测和生成，从第一人称视觉输入在统一框架中进行。与忽视场景语义的先前工作不同，我们的模型能够有效地提取基于图像的场景上下文来推断合理的三维动作。为了促进训练，我们引入了EE4D-Motion数据集，该数据集是基于EgoExo4D派生的大规模数据集，并增加了伪地面真实三维运动注释。UniEgoMotion在自我中心动态重建方面取得了最先进的性能表现，并且是首个能够从单一自我中心图像生成动作的方法。广泛的评估证明了我们统一框架的有效性，为自我中心动态建模设定了新的基准并解锁了自我中心应用的新可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01126v2">PDF</a> ICCV 2025. Project Page:   <a target="_blank" rel="noopener" href="https://chaitanya100100.github.io/UniEgoMotion/">https://chaitanya100100.github.io/UniEgoMotion/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了以自我为中心的动态运动生成与预测的重要性，并指出其在增强AR&#x2F;VR体验、改善人机互动、推进辅助技术和自适应医疗解决方案等领域的应用。针对现有方法主要关注第三人称运动合成与结构化3D场景上下文的问题，本文提出了以第一人称图像为基础的无依赖显式3D场景的Egocentric Motion Generation和Egocentric Motion Forecasting两个新任务。同时，介绍了一种统一条件运动扩散模型UniEgoMotion，该模型具有针对以自我为中心的设备的新型头中心运动表示。UniEgoMotion的简单而有效的设计支持以自我为中心的运动重建、预测和生成，在一个统一框架内从第一人称视觉输入中进行处理。与忽视场景语义的先前工作不同，我们的模型能够有效地提取基于图像的场景上下文来推断合理的3D运动。为了促进训练，我们还引入了从EgoExo4D派生的EE4D-Motion大规模数据集，该数据集增强了伪地面真实3D运动注释。UniEgoMotion在自我为中心的运动重建方面取得了最先进的性能表现，并且是首个能够从单一自我为中心图像生成运动的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>以自我为中心的人类运动生成和预测对于增强AR&#x2F;VR体验、改善人机互动等具有关键作用。</li>
<li>现有方法主要关注第三人称运动合成，在真实世界的以自我为中心的场景中效果有限。</li>
<li>引入Egocentric Motion Generation和Egocentric Motion Forecasting两个新任务，利用第一人称图像进行场景感知运动合成。</li>
<li>提出统一条件运动扩散模型UniEgoMotion，支持以自我为中心的运动重建、预测和生成。</li>
<li>UniEgoMotion能够有效提取基于图像的场景上下文，推断合理的3D运动。</li>
<li>引入EE4D-Motion数据集，增强伪地面真实3D运动注释，用于促进模型训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2508.01126v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2508.01126v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_Diffusion Models/2508.01126v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-081c465c5cf95574b7e1c3d44818df90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760749458&auth_key=1760749458-0-0-d0462cc130b6b4965a75ae1843eba879&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-10-18  Towards Generalist Intelligence in Dentistry Vision Foundation Models   for Oral and Maxillofacial Radiology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_NeRF/2510.09586v1/page_2_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-10-18  GauSSmart Enhanced 3D Reconstruction through 2D Foundation Models and   Geometric Filtering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
