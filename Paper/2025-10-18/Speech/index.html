<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  OmniMotion Multimodal Motion Generation with Continuous Masked   Autoregression">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-d6bfaa649b47b04d3bacd47d20079f98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827986&auth_key=1760827986-0-0-834b1b8626b951b1eb6d720e17669e79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="OmniMotion-Multimodal-Motion-Generation-with-Continuous-Masked-Autoregression"><a href="#OmniMotion-Multimodal-Motion-Generation-with-Continuous-Masked-Autoregression" class="headerlink" title="OmniMotion: Multimodal Motion Generation with Continuous Masked   Autoregression"></a>OmniMotion: Multimodal Motion Generation with Continuous Masked   Autoregression</h2><p><strong>Authors:Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu</strong></p>
<p>Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public. </p>
<blockquote>
<p>å…¨èº«å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆä¸»è¦é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåˆ›å»ºæœ‰æ•ˆçš„è¿åŠ¨ç”Ÿæˆæœºåˆ¶ï¼Œä»¥åŠå°†æ–‡æœ¬ã€è¯­éŸ³ã€éŸ³ä¹ç­‰ä¸åŒæ¨¡æ€é›†æˆåˆ°ä¸€ä¸ªåè°ƒçš„æ¡†æ¶ä¸­ã€‚ä¸åŒäºé€šå¸¸é‡‡ç”¨ç¦»æ•£æ©è”½å»ºæ¨¡æˆ–è‡ªå›å½’å»ºæ¨¡çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è¿ç»­æ©è”½è‡ªå›å½’è¿åŠ¨è½¬æ¢å™¨ï¼Œå…¶ä¸­è€ƒè™‘åˆ°äººç±»è¿åŠ¨çš„åºåˆ—æ€§è¿›è¡Œäº†å› æœæ³¨æ„åŠ›å¤„ç†ã€‚åœ¨è¯¥è½¬æ¢å™¨ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥é—¨æ§çº¿æ€§æ³¨æ„åŠ›å’ŒRMSNormæ¨¡å—ï¼Œæ¨åŠ¨è½¬æ¢å™¨å…³æ³¨å…³é”®åŠ¨ä½œï¼Œå¹¶æŠ‘åˆ¶ç”±äºå¤šæ¨¡æ€ä¸­çš„å¼‚å¸¸åŠ¨ä½œæˆ–å¼‚è´¨åˆ†å¸ƒæ‰€å¯¼è‡´çš„ç¨³å®šæ€§é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¿åŠ¨ç”Ÿæˆå’Œå¤šæ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨DiTç»“æ„å°†è½¬æ¢å™¨çš„æ¡ä»¶å‘ç›®æ ‡æ‰©æ•£ã€‚ä¸ºäº†èåˆä¸åŒçš„æ¨¡æ€ï¼Œåˆ©ç”¨AdaLNå’Œäº¤å‰æ³¨æ„åŠ›æ¥æ³¨å…¥æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨åŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€è¯­éŸ³åˆ°åŠ¨ä½œå’ŒéŸ³ä¹åˆ°èˆè¹ˆçš„æ‰€æœ‰æ¨¡æ€ä¸Šéƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ–¹æ³•çš„ä»£ç å°†å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14954v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§å…¨æ–°çš„å…¨èº«å¤šæ¨¡æ€äººç±»åŠ¨ä½œç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬è¿ç»­é®è”½è‡ªå›å½’åŠ¨ä½œè½¬æ¢å™¨ï¼ˆContinuous Masked Autoregressive Motion Transformerï¼‰ï¼Œèƒ½å¤Ÿè§£å†³åŠ¨ä½œç”Ÿæˆæœºåˆ¶å’Œå¤šæ¨¡æ€é›†æˆé—®é¢˜ã€‚é€šè¿‡å¼•å…¥å› æœæ³¨æ„åŠ›æœºåˆ¶ã€é—¨æ§çº¿æ€§æ³¨æ„åŠ›æ¨¡å—å’ŒRMSNormæ¨¡å—ï¼Œè§£å†³äº†å…³é”®åŠ¨ä½œå…³æ³¨ä¸å¤šæ¨¡æ€å¼‚å¸¸æŠ‘åˆ¶é—®é¢˜ã€‚ä½¿ç”¨DiTç»“æ„ä¿ƒè¿›æ¡ä»¶æ‰©æ•£å’Œæ¨¡æ€èåˆï¼Œé€šè¿‡AdaLNå’Œè·¨æ³¨æ„åŠ›æ³¨å…¥æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ä¿¡å·ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€åŠ¨ä½œç”Ÿæˆä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºè¿ç»­é®è”½è‡ªå›å½’åŠ¨ä½œè½¬æ¢å™¨ï¼Œé’ˆå¯¹å…¨èº«å¤šæ¨¡æ€äººç±»åŠ¨ä½œç”Ÿæˆè¿›è¡Œé«˜æ•ˆå»ºæ¨¡ã€‚</li>
<li>åˆ©ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶å¤„ç†åŠ¨ä½œåºåˆ—çš„è¿ç»­æ€§ã€‚</li>
<li>é—¨æ§çº¿æ€§æ³¨æ„åŠ›æ¨¡å—å’ŒRMSNormæ¨¡å—å¢å¼ºäº†å¯¹å…³é”®åŠ¨ä½œçš„å…³æ³¨å¹¶æŠ‘åˆ¶äº†å¤šæ¨¡æ€å¼‚å¸¸ã€‚</li>
<li>ä½¿ç”¨DiTç»“æ„ä¿ƒè¿›æ¡ä»¶æ‰©æ•£ï¼Œæå‡åŠ¨ä½œç”Ÿæˆå’Œè·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>AdaLNå’Œè·¨æ³¨æ„åŠ›æŠ€æœ¯ç”¨äºèåˆä¸åŒæ¨¡æ€ï¼ˆæ–‡æœ¬ã€è¯­éŸ³ã€éŸ³ä¹ï¼‰ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§æ¨¡æ€ä¸Šçš„ä¼˜è¶Šæ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬è½¬åŠ¨ä½œã€è¯­éŸ³è½¬æ‰‹åŠ¿å’ŒéŸ³ä¹è½¬èˆè¹ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-189b07e6b9f3216468de0245f6de2aea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827994&auth_key=1760827994-0-0-37681b2163d7c74f33b77716612604a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-26c92e782bbdc517d8079ac97344daf0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828001&auth_key=1760828001-0-0-cae25ce58f4bd3fbb664a21fe578a042&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-87e5f3813f5dff73ff3d1ebe19fb7726~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828007&auth_key=1760828007-0-0-5a90c4fc2868a1aa725f519eed0a64c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ffb66aad3cc5db70ab7ab6f661397f08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828014&auth_key=1760828014-0-0-bd259b9c1c8914262b89f6111ac363f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TRI-DEP-A-Trimodal-Comparative-Study-for-Depression-Detection-Using-Speech-Text-and-EEG"><a href="#TRI-DEP-A-Trimodal-Comparative-Study-for-Depression-Detection-Using-Speech-Text-and-EEG" class="headerlink" title="TRI-DEP: A Trimodal Comparative Study for Depression Detection Using   Speech, Text, and EEG"></a>TRI-DEP: A Trimodal Comparative Study for Depression Detection Using   Speech, Text, and EEG</h2><p><strong>Authors:Annisaa Fitri Nurfidausi, Eleonora Mancini, Paolo Torroni</strong></p>
<p>Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection. </p>
<blockquote>
<p>æŠ‘éƒç—‡æ˜¯ä¸€ç§æ™®éçš„å¿ƒç†å¥åº·éšœç¢ï¼Œä½†å…¶è‡ªåŠ¨æ£€æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ—©æœŸçš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å•æ¨¡æ€å’Œå¤šæ¨¡æ€çš„æ–¹æ³•ï¼Œå¤šæ¨¡æ€ç³»ç»Ÿé€šè¿‡åˆ©ç”¨äº’è¡¥ä¿¡å·æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶åœ¨èŒƒå›´ä¸Šæœ‰é™ï¼Œç¼ºä¹ç‰¹å¾çš„ç³»ç»Ÿæ¯”è¾ƒï¼Œå¹¶ä¸”å—åˆ°è¯„ä¼°åè®®ä¸ä¸€è‡´çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢è„‘ç”µå›¾ã€è¯­éŸ³å’Œæ–‡æœ¬çš„ç‰¹å¾è¡¨ç¤ºå’Œå»ºæ¨¡ç­–ç•¥æ¥è§£å†³è¿™äº›ç©ºç™½ã€‚æˆ‘ä»¬è¯„ä¼°äº†æ‰‹å·¥ç‰¹å¾ä¸é¢„è®­ç»ƒåµŒå…¥çš„æ•ˆæœï¼Œè¯„ä¼°äº†ä¸åŒç¥ç»ç½‘ç»œç¼–ç å™¨çš„æœ‰æ•ˆæ€§ï¼Œæ¯”è¾ƒäº†å•æ¨¡æ€ã€åŒæ¨¡æ€å’Œä¸‰æ¨¡æ€é…ç½®ï¼Œå¹¶åˆ†æäº†èåˆç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨è„‘ç”µå›¾çš„ä½œç”¨ã€‚åº”ç”¨ä¸€è‡´çš„å—è¯•è€…ç‹¬ç«‹åˆ†å‰²ï¼Œä»¥ç¡®ä¿ç¨³å¥ã€å¯å¤åˆ¶çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆiï¼‰è„‘ç”µå›¾ã€è¯­éŸ³å’Œæ–‡æœ¬æ¨¡æ€çš„ç»„åˆå¢å¼ºäº†å¤šæ¨¡æ€æ£€æµ‹æ•ˆæœï¼›ï¼ˆiiï¼‰é¢„è®­ç»ƒçš„åµŒå…¥ç‰¹å¾è¡¨ç°ä¼˜äºæ‰‹å·¥ç‰¹å¾ï¼›ï¼ˆiiiï¼‰ç²¾å¿ƒè®¾è®¡çš„ä¸‰æ¨¡æ€æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¤šæ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹çš„æœªæ¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14922v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…³äºæŠ‘éƒç—‡çš„è‡ªåŠ¨æ£€æµ‹æ˜¯ä¸€é¡¹é‡è¦çš„æŒ‘æˆ˜ï¼Œè™½ç„¶å·²ç»æœ‰è®¸å¤šå…³äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€æ–¹æ³•çš„ç ”ç©¶ï¼Œä½†ç°æœ‰çš„ç ”ç©¶ä»å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹ç³»ç»Ÿçš„ç‰¹å¾æ¯”è¾ƒå’Œä¸€è‡´çš„è¯„ä»·åè®®ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†EEGã€è¯­éŸ³å’Œæ–‡æœ¬çš„ç‰¹å¾è¡¨ç¤ºå’Œå»ºæ¨¡ç­–ç•¥ï¼Œè¯„ä¼°äº†æ‰‹å·¥ç‰¹å¾å’Œé¢„è®­ç»ƒåµŒå…¥çš„æ•ˆæœï¼Œæ¯”è¾ƒäº†å•æ¨¡æ€ã€åŒæ¨¡æ€å’Œä¸‰æ¨¡æ€é…ç½®ï¼Œå¹¶åˆ†æäº†èåˆç­–ç•¥ä¸­EEGçš„ä½œç”¨ã€‚ç ”ç©¶é‡‡ç”¨ä¸€è‡´çš„å—è¯•è€…ç‹¬ç«‹åˆ†å‰²æ–¹æ³•ï¼Œä»¥ç¡®ä¿ç¨³å¥ã€å¯é‡å¤æ€§çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ£€æµ‹ä¸­ç»“åˆEEGã€è¯­éŸ³å’Œæ–‡æœ¬æ¨¡æ€çš„æ•ˆæœæ›´ä½³ï¼Œé¢„è®­ç»ƒåµŒå…¥ä¼˜äºæ‰‹å·¥ç‰¹å¾ï¼Œç²¾å¿ƒè®¾è®¡çš„ä¸‰æ¨¡æ€æ¨¡å‹è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<p>ä¸€ã€å¤šæ¨¡æ€æ£€æµ‹èåˆEEGã€è¯­éŸ³å’Œæ–‡æœ¬æ¨¡æ€å¯ä»¥æé«˜æŠ‘éƒç—‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚<br>äºŒã€é¢„è®­ç»ƒåµŒå…¥ç‰¹å¾åœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸­è¡¨ç°ä¼˜äºæ‰‹å·¥ç‰¹å¾ã€‚<br>ä¸‰ã€ç²¾å¿ƒè®¾è®¡çš„ä¸‰æ¨¡æ€æ¨¡å‹åœ¨å¤šæ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚<br>å››ã€æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†ä¸åŒç¥ç»ç¼–ç å™¨çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ¯”è¾ƒäº†å•æ¨¡æ€ã€åŒæ¨¡æ€å’Œä¸‰æ¨¡æ€é…ç½®ã€‚<br>äº”ã€èåˆç­–ç•¥åœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸­èµ·ç€é‡è¦ä½œç”¨ï¼Œå…¶ä¸­EEGçš„ä½œç”¨ä¸å¯å¿½è§†ã€‚<br>å…­ã€é‡‡ç”¨ä¸€è‡´çš„å—è¯•è€…ç‹¬ç«‹åˆ†å‰²æ–¹æ³•ï¼Œç¡®ä¿è¯„ä¼°çš„ç¨³å¥æ€§å’Œå¯é‡å¤æ€§ã€‚<br>ä¸ƒã€æœ¬ç ”ç©¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒå’ŒåŸºç¡€ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ab15b3de4151bb2872a7425fd588e87a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828022&auth_key=1760828022-0-0-46b16bef5266903e767c1a62f0adf477&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d1835824817d1e78f30ac774dd9d123~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828029&auth_key=1760828029-0-0-519f34631e0032bcddda4f6b2e125c25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8594e8c2ced03adebe61158efb931507~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828036&auth_key=1760828036-0-0-bd8b2fa06078baec825b027196762e40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d820b3f62ada946fd024717bc426150~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828042&auth_key=1760828042-0-0-34b0b6f831df9f2ed62a4a7b46a3cb34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14ca6840905f4edaa7462729f3d389ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828049&auth_key=1760828049-0-0-e690450725588555edc48d7748243e75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b465d6f71459bf78993c84e3be3452cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828056&auth_key=1760828056-0-0-c4cfef6fd84631a41b3410164d93d060&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RLAIF-SPA-Optimizing-LLM-based-Emotional-Speech-Synthesis-via-RLAIF"><a href="#RLAIF-SPA-Optimizing-LLM-based-Emotional-Speech-Synthesis-via-RLAIF" class="headerlink" title="RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF"></a>RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF</h2><p><strong>Authors:Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao</strong></p>
<p>Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³åˆæˆåœ¨ä¸­æ€§è¯­éŸ³æ–¹é¢å·²ç»è¾¾åˆ°äº†æ¥è¿‘äººç±»çš„è´¨é‡ï¼Œä½†åœ¨æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„æƒ…æ„Ÿæ³¨é‡Šï¼Œæˆ–ä¼˜åŒ–æœªèƒ½æ•æ‰åˆ°è¯­éŸ³çš„æƒ…æ„Ÿè¡¨è¾¾åŠ›å’Œæ„ŸçŸ¥è‡ªç„¶åº¦çš„é—´æ¥ç›®æ ‡ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¯­éŸ³è™½ç„¶å‡†ç¡®ä½†æƒ…æ„Ÿå¹³æ·¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RLAIF-SPAæ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æœºåˆ¶ï¼Œé‡‡ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯æ¥åˆ†åˆ«åˆ¤æ–­è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿæ ‡ç­¾å¯¹é½ï¼Œä½œä¸ºæƒ…æ„Ÿè¡¨è¾¾åŠ›å’Œå¯ç†è§£æ€§ä¼˜åŒ–çš„ç›´æ¥å¥–åŠ±ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ©ç”¨éŸµå¾‹æ ‡ç­¾å¯¹é½æ¥æé«˜è¡¨è¾¾è´¨é‡ï¼ŒåŒæ—¶è€ƒè™‘è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿå¯¹é½å››ä¸ªç²¾ç»†ç»´åº¦ï¼šç»“æ„ã€æƒ…æ„Ÿã€é€Ÿåº¦å’Œè¯­è°ƒã€‚æ­¤å¤–ï¼Œå®ƒç»“åˆäº†è¯­ä¹‰å‡†ç¡®æ€§åé¦ˆï¼Œä»¥ç¡®ä¿æ¸…æ™°å‡†ç¡®çš„è¯­éŸ³ç”Ÿæˆã€‚åœ¨Libri Speechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRLAIF-SPAä¼˜äºChat-TTSï¼Œç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†26.1%ï¼ŒSIM-Oå¢åŠ äº†9.1%ï¼Œäººç±»è¯„ä¼°æ”¹è¿›è¶…è¿‡10%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14628v1">PDF</a> </p>
<p><strong>Summary</strong><br>æƒ…æ„ŸåŒ–è¯­éŸ³åˆæˆä»é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µçš„æƒ…æ„Ÿæ ‡æ³¨æˆ–ä¼˜åŒ–é—´æ¥ç›®æ ‡ï¼Œæ— æ³•æ•æ‰æƒ…æ„Ÿçš„è¡¨è¾¾åŠ›å’Œè¯­éŸ³çš„è‡ªç„¶æ„ŸçŸ¥ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†RLAIF-SPAæ¡†æ¶ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆæœºåˆ¶ï¼Œç»“åˆè¯­éŸ³è¯†åˆ«å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œåˆ†åˆ«åˆ¤æ–­è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿæ ‡ç­¾å¯¹é½ï¼Œä½œä¸ºæƒ…æ„Ÿè¡¨è¾¾å’Œä¼˜åŒ–å¯ç†è§£æ€§çš„ç›´æ¥å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒRLAIF-SPAåœ¨Libri Speechæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºChat-TTSï¼Œå­—è¯é”™è¯¯ç‡é™ä½26.1%ï¼ŒSIM-Oå¾—åˆ†æé«˜9.1%ï¼Œäººç±»è¯„ä¼°å¾—åˆ†æé«˜è¶…è¿‡10%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„ŸåŒ–è¯­éŸ³åˆæˆæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰æƒ…æ„Ÿçš„è¡¨è¾¾åŠ›å’Œè¯­éŸ³çš„è‡ªç„¶æ„ŸçŸ¥ã€‚</li>
<li>RLAIF-SPAæ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆæœºåˆ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>æ¡†æ¶ç»“åˆè¯­éŸ³è¯†åˆ«å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œåˆ¤æ–­è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿæ ‡ç­¾å¯¹é½ã€‚</li>
<li>RLAIF-SPAæ¡†æ¶é€šè¿‡è€ƒè™‘è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿå¯¹é½ï¼Œåœ¨å››ä¸ªç²¾ç»†ç²’åº¦ï¼ˆç»“æ„ã€æƒ…æ„Ÿã€é€Ÿåº¦å’ŒéŸ³è°ƒï¼‰ä¸Šæé«˜è¡¨è¾¾è´¨é‡ã€‚</li>
<li>æ¡†æ¶è¿˜ç»“åˆäº†è¯­ä¹‰å‡†ç¡®æ€§åé¦ˆï¼Œç¡®ä¿ç”Ÿæˆçš„è¯­éŸ³æ¸…æ™°å‡†ç¡®ã€‚</li>
<li>åœ¨Libri Speechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRLAIF-SPAä¼˜äºChat-TTSï¼Œå­—è¯é”™è¯¯ç‡é™ä½26.1%ï¼ŒSIM-Oå¾—åˆ†æé«˜9.1%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cd3d94e775c86d09973da7ed7182d47a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828064&auth_key=1760828064-0-0-161845be8aeec64bed27de4031ef7e6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de5622c0e7e1fed81861644404a38f47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828071&auth_key=1760828071-0-0-47a1b247f200b7c24ec99011b9b0479d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c1799d390763cdc6326b6d3a34c26859~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828079&auth_key=1760828079-0-0-bc4a34026d2f8440c5fb8874b03ed30b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36108540ac8cd7c87bb282886eea4e19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828086&auth_key=1760828086-0-0-421fa8d9163aeb987d0d14d8cb004ddb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Do-Slides-Help-Multi-modal-Context-for-Automatic-Transcription-of-Conference-Talks"><a href="#Do-Slides-Help-Multi-modal-Context-for-Automatic-Transcription-of-Conference-Talks" class="headerlink" title="Do Slides Help? Multi-modal Context for Automatic Transcription of   Conference Talks"></a>Do Slides Help? Multi-modal Context for Automatic Transcription of   Conference Talks</h2><p><strong>Authors:Supriti Sinhamahapatra, Jan Niehues</strong></p>
<p>State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily rely on acoustic information while disregarding additional multi-modal context. However, visual information are essential in disambiguation and adaptation. While most work focus on speaker images to handle noise conditions, this work also focuses on integrating presentation slides for the use cases of scientific presentation.   In a first step, we create a benchmark for multi-modal presentation including an automatic analysis of transcribing domain-specific terminology. Next, we explore methods for augmenting speech models with multi-modal information. We mitigate the lack of datasets with accompanying slides by a suitable approach of data augmentation. Finally, we train a model using the augmented dataset, resulting in a relative reduction in word error rate of approximately 34%, across all words and 35%, for domain-specific terms compared to the baseline model. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸»è¦ä¾èµ–äºå£°å­¦ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†é¢å¤–çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œè§†è§‰ä¿¡æ¯åœ¨æ¾„æ¸…å’Œé€‚åº”æ–¹é¢éå¸¸é‡è¦ã€‚è™½ç„¶å¤§å¤šæ•°å·¥ä½œéƒ½ä¸“æ³¨äºä½¿ç”¨å‘è¨€äººå›¾åƒæ¥å¤„ç†å™ªå£°æ¡ä»¶ï¼Œä½†è¿™é¡¹å·¥ä½œè¿˜ä¸“æ³¨äºæ•´åˆæ¼”ç¤ºå¹»ç¯ç‰‡ä»¥ç”¨äºç§‘å­¦æ¼”ç¤ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºåŒ…æ‹¬ç‰¹å®šé¢†åŸŸæœ¯è¯­çš„è‡ªåŠ¨è½¬å½•çš„å¤šæ¨¡æ€æ¼”ç¤ºåˆ›å»ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç”¨å¤šæ¨¡æ€ä¿¡æ¯å¢å¼ºè¯­éŸ³æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡é€‚å½“çš„æ•°æ®å¢å¼ºæ–¹æ³•ç¼“è§£äº†ç¼ºä¹é™„å¸¦å¹»ç¯ç‰‡çš„æ•°æ®é›†çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨æ‰€æœ‰å•è¯ä¸Šçš„è¯é”™è¯¯ç‡å¤§çº¦é™ä½äº†3 4%ï¼Œåœ¨ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­ä¸Šé™ä½äº†3 5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13979v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€ä¿¡æ¯åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯è§†è§‰ä¿¡æ¯åœ¨è§£å†³æ­§ä¹‰å’Œé€‚åº”ä¸åŒåœºæ™¯ä¸­çš„ä½œç”¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§é›†æˆæ¼”è®²å¹»ç¯ç‰‡çš„å¤šæ¨¡æ€æ¼”ç¤ºæ–¹æ³•ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ•°æ®é›†ç”¨äºè¯„ä¼°é¢†åŸŸç‰¹å®šæœ¯è¯­çš„è‡ªåŠ¨åˆ†æã€‚é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯å¼¥è¡¥äº†ç¼ºä¹é™„å¸¦å¹»ç¯ç‰‡çš„æ•°æ®é›†é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨è¯é”™è¯¯ç‡æ–¹é¢ç›¸æ¯”åŸºçº¿æ¨¡å‹é™ä½äº†çº¦34%ï¼ˆé’ˆå¯¹æ‰€æœ‰å•è¯ï¼‰å’Œ35%ï¼ˆé’ˆå¯¹é¢†åŸŸç‰¹å®šæœ¯è¯­ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä¿¡æ¯åœ¨ASRç³»ç»Ÿä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œå°¤å…¶æ˜¯è§†è§‰ä¿¡æ¯ç”¨äºè§£å†³æ­§ä¹‰å’Œé€‚åº”ä¸åŒåœºæ™¯ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¼”ç¤ºæ–¹æ³•ï¼Œé›†æˆäº†æ¼”è®²å¹»ç¯ç‰‡ï¼Œä¸ºç§‘å­¦æ¼”è®²ç­‰åœºæ™¯æä¾›è¾…åŠ©ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªæ•°æ®é›†ç”¨äºè¯„ä¼°é¢†åŸŸç‰¹å®šæœ¯è¯­çš„è‡ªåŠ¨åˆ†æã€‚</li>
<li>é‡‡ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æ¥å¼¥è¡¥ç¼ºä¹é™„å¸¦å¹»ç¯ç‰‡çš„æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”åŸºçº¿æ¨¡å‹åœ¨è¯é”™è¯¯ç‡æ–¹é¢æœ‰äº†æ˜¾è‘—é™ä½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…å…³æ³¨å™ªå£°æ¡ä»¶ä¸‹çš„å¤„ç†ï¼Œä¹Ÿæ³¨é‡ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fd9852fd1a17e25239bd60b64e3e7ed8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828094&auth_key=1760828094-0-0-b4744a9bfc34e99c14ee51169e727f35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7716e0cbf090c2eba335267e51b5e8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828101&auth_key=1760828101-0-0-aa238302267a8dabad52ba7a6a647c05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b7a5875ac064a853425e8897bf1dcdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828108&auth_key=1760828108-0-0-d457f9a7ad32c783837ad26d40f92e91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Switchboard-Affect-Emotion-Perception-Labels-from-Conversational-Speech"><a href="#Switchboard-Affect-Emotion-Perception-Labels-from-Conversational-Speech" class="headerlink" title="Switchboard-Affect: Emotion Perception Labels from Conversational Speech"></a>Switchboard-Affect: Emotion Perception Labels from Conversational Speech</h2><p><strong>Authors:Amrit Romana, Jaya Narain, Tien Dung Tran, Andrea Davis, Jason Fong, Ramya Rasipuram, Vikramjit Mitra</strong></p>
<p>Understanding the nuances of speech emotion dataset curation and labeling is essential for assessing speech emotion recognition (SER) model potential in real-world applications. Most training and evaluation datasets contain acted or pseudo-acted speech (e.g., podcast speech) in which emotion expressions may be exaggerated or otherwise intentionally modified. Furthermore, datasets labeled based on crowd perception often lack transparency regarding the guidelines given to annotators. These factors make it difficult to understand model performance and pinpoint necessary areas for improvement. To address this gap, we identified the Switchboard corpus as a promising source of naturalistic conversational speech, and we trained a crowd to label the dataset for categorical emotions (anger, contempt, disgust, fear, sadness, surprise, happiness, tenderness, calmness, and neutral) and dimensional attributes (activation, valence, and dominance). We refer to this label set as Switchboard-Affect (SWB-Affect). In this work, we present our approach in detail, including the definitions provided to annotators and an analysis of the lexical and paralinguistic cues that may have played a role in their perception. In addition, we evaluate state-of-the-art SER models, and we find variable performance across the emotion categories with especially poor generalization for anger. These findings underscore the importance of evaluation with datasets that capture natural affective variations in speech. We release the labels for SWB-Affect to enable further analysis in this domain. </p>
<blockquote>
<p>ç†è§£å’ŒæŠŠæ¡è¯­éŸ³æƒ…æ„Ÿæ•°æ®é›†æ•´ç†å’Œæ ‡æ³¨çš„ç»†å¾®å·®åˆ«ï¼Œå¯¹äºè¯„ä¼°è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†åŒ…å«çš„è¡Œä¸ºæˆ–æ¨¡æ‹Ÿè¡Œä¸ºçš„è¯­éŸ³ï¼ˆä¾‹å¦‚ï¼Œæ’­å®¢è¯­éŸ³ï¼‰ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾å¯èƒ½è¿‡äºå¤¸å¼ æˆ–æ•…æ„ä¿®æ”¹ã€‚æ­¤å¤–ï¼ŒåŸºäºç¾¤ä½“æ„ŸçŸ¥æ ‡æ³¨çš„æ•°æ®é›†å¾€å¾€ç¼ºä¹å¯¹æ ‡æ³¨è€…æ‰€ç»™æŒ‡å—çš„é€æ˜åº¦ã€‚è¿™äº›å› ç´ ä½¿å¾—ç†è§£æ¨¡å‹æ€§èƒ½å¹¶æŒ‡å‡ºå¿…è¦çš„æ”¹è¿›é¢†åŸŸå˜å¾—å›°éš¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ç¡®å®šäº†Switchboardè¯­æ–™åº“æ˜¯è‡ªç„¶ä¼šè¯è¯­éŸ³çš„æœ‰å‰é€”çš„æ¥æºï¼Œæˆ‘ä»¬åŸ¹è®­äº†ä¸€ä¸ªç¾¤ä½“æ¥æ ‡æ³¨æ•°æ®é›†ä¸­çš„åˆ†ç±»æƒ…æ„Ÿï¼ˆæ„¤æ€’ã€è”‘è§†ã€åŒæ¶ã€ææƒ§ã€æ‚²ä¼¤ã€æƒŠè®¶ã€å¿«ä¹ã€æ¸©æŸ”ã€å†·é™å’Œä¸­æ€§ï¼‰å’Œç»´åº¦å±æ€§ï¼ˆæ¿€æ´»ã€æ•ˆä»·å’Œæ”¯é…æ€§ï¼‰ã€‚æˆ‘ä»¬å°†è¿™ä¸ªæ ‡ç­¾é›†ç§°ä¸ºSwitchboard-Affectï¼ˆSWB-Affectï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸ºæ ‡æ³¨è€…æä¾›çš„å®šä¹‰ï¼Œä»¥åŠåˆ†æå¯èƒ½åœ¨å…¶æ„ŸçŸ¥ä¸­èµ·ä½œç”¨çš„è¯æ±‡å’Œå‰¯è¯­è¨€çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„SERæ¨¡å‹ï¼Œå‘ç°æƒ…ç»ªç±»åˆ«ä¹‹é—´çš„æ€§èƒ½å„ä¸ç›¸åŒï¼Œå…¶ä¸­å¯¹æ„¤æ€’çš„æ³›åŒ–å°¤å…¶å·®ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ä½¿ç”¨æ•æ‰è‡ªç„¶æƒ…æ„Ÿå˜åŒ–çš„è¯­éŸ³æ•°æ®é›†è¿›è¡Œè¯„ä¼°çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘å¸ƒSWB-Affectçš„æ ‡ç­¾ï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13906v1">PDF</a> 2025 13th International Conference on Affective Computing and   Intelligent Interaction (ACII) <a target="_blank" rel="noopener" href="https://github.com/apple/ml-switchboard-affect">https://github.com/apple/ml-switchboard-affect</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­è¯„ä¼°çš„é‡è¦æ€§ï¼Œä»¥åŠæƒ…æ„Ÿæ•°æ®é›†æ”¶é›†å’Œæ ‡æ³¨çš„å¤æ‚æ€§ã€‚æ–‡ç« ä»‹ç»äº†ä½¿ç”¨Switchboardè¯­æ–™åº“æ„å»ºæ–°æ•°æ®é›†Switchboard-Affectï¼ˆSWB-Affectï¼‰çš„è¿‡ç¨‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«è‡ªç„¶å¯¹è¯è¯­éŸ³ï¼Œå¹¶è¦æ±‚æ ‡æ³¨äººå‘˜å¯¹å…¶è¿›è¡Œæƒ…æ„Ÿç±»åˆ«å’Œç»´åº¦å±æ€§çš„æ ‡æ³¨ã€‚é€šè¿‡å¯¹å½“å‰å…ˆè¿›çš„SERæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°æ¨¡å‹åœ¨ä¸åŒæƒ…æ„Ÿç±»åˆ«ä¸Šçš„è¡¨ç°å­˜åœ¨å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«æ„¤æ€’æƒ…ç»ªæ–¹é¢çš„æ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚æœ¬æ–‡å¼ºè°ƒäº†ä½¿ç”¨æ•æ‰è‡ªç„¶æƒ…æ„Ÿå˜åŒ–çš„è¯­éŸ³æ•°æ®é›†è¿›è¡Œè¯„ä¼°çš„é‡è¦æ€§ï¼Œå¹¶å‘å¸ƒäº†SWB-Affectçš„æ ‡ç­¾ä¾›è¿›ä¸€æ­¥ç ”ç©¶åˆ†æä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è®­ç»ƒä¸è¯„ä¼°æ•°æ®é›†å¾€å¾€åŒ…å«å¤¸å¼ æˆ–æ•…æ„ä¿®æ”¹çš„æƒ…æ„Ÿè¡¨è¾¾å½¢å¼ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½éš¾ä»¥å‡†ç¡®è¯„ä¼°ã€‚</li>
<li>Switchboardè¯­æ–™åº“è¢«ç”¨ä½œæ„å»ºæ–°çš„æ•°æ®é›†Switchboard-Affectï¼ˆSWB-Affectï¼‰ï¼Œä»¥åŒ…å«è‡ªç„¶å¯¹è¯è¯­éŸ³ä¸ºç‰¹è‰²ã€‚</li>
<li>SWB-Affectè¦æ±‚æ ‡æ³¨äººå‘˜æŒ‰æƒ…æ„Ÿç±»åˆ«ï¼ˆå¦‚æ„¤æ€’ã€æ‚²ä¼¤ç­‰ï¼‰å’Œç»´åº¦å±æ€§ï¼ˆå¦‚æ´»è·ƒåº¦ã€ä»·å€¼å€¾å‘æ€§ç­‰ï¼‰è¿›è¡Œæ ‡æ³¨ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„SERæ¨¡å‹åœ¨æƒ…æ„Ÿç±»åˆ«ä¸Šè¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯æ„¤æ€’æƒ…ç»ªçš„è¯†åˆ«æ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚</li>
<li>ä½¿ç”¨æ•æ‰è‡ªç„¶æƒ…æ„Ÿå˜åŒ–çš„è¯­éŸ³æ•°æ®é›†è¿›è¡Œè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2a45b4c53b4d81a5253cf4754f9eb4b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760828115&auth_key=1760828115-0-0-cca7a6fd358d332305506b89255a41f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b7c222b349304d6faf7d533a4acc1ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906549&auth_key=1760906549-0-0-f7308dbc1acf75a7c58aa31371e4db1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57c801841817f86187535a9ffef08040~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906556&auth_key=1760906556-0-0-77c5d735d267b9de05972c76e113f496&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bf6c656a3764ac7142c14a8d9e8bcd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906562&auth_key=1760906562-0-0-41876fe33f33bfaceff36cbcf6cab7a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5006ea6bd7eaed2a5ca432c21794969~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906569&auth_key=1760906569-0-0-f4e932eefc5ac35531b4afe409540870&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue"><a href="#InteractiveOmni-A-Unified-Omni-modal-Model-for-Audio-Visual-Multi-turn-Dialogue" class="headerlink" title="InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn   Dialogue"></a>InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn   Dialogue</h2><p><strong>Authors:Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu</strong></p>
<p>We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the modelâ€™s ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»InteractiveOmniï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€å’Œå¼€æºçš„è·¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè§†å¬å¤šè½®äº¤äº’ï¼Œå‚æ•°èŒƒå›´ä»4Båˆ°8Bã€‚æ—¨åœ¨é€šè¿‡æä¾›å…¨é¢çš„è·¨æ¨¡æ€ç†è§£å’Œè¯­éŸ³ç”Ÿæˆèƒ½åŠ›ï¼Œå¼•é¢†è½»é‡çº§æ¨¡å‹é¢†åŸŸã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å°†è§†è§‰ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³è§£ç å™¨é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ä¸­ï¼Œç”¨äºç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿å¼ºå¤§çš„è·¨æ¨¡æ€èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç”¨äºè·¨æ¨¡æ€ç†è§£çš„é¢„è®­ç»ƒï¼Œå…¶æ¬¡æ˜¯ä½¿ç”¨è¯­éŸ³å¯¹è¯å’Œè§†å¬äº¤äº’çš„åè®­ç»ƒã€‚ä¸ºäº†å®ç°ç±»ä¼¼äººç±»çš„é•¿æœŸå¯¹è¯èƒ½åŠ›ï¼Œæˆ‘ä»¬ç²¾å¿ƒç­–åˆ’äº†ä¸€ä¸ªå¤šè½®è®­ç»ƒæ•°æ®é›†ï¼Œä»¥å¢å¼ºæ¨¡å‹å¤„ç†å¤æ‚å’Œå¤šè½®äº¤äº’çš„èƒ½åŠ›ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è¯„ä¼°å¤šè½®è®°å¿†å’Œè¯­éŸ³äº¤äº’èƒ½åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤šæ¨¡æ€å¤šè½®è®°å¿†åŸºå‡†å’Œå¤šè½®è¯­éŸ³äº¤äº’åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒInteractiveOmniæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼Œå¹¶æä¾›æ›´æ™ºèƒ½çš„å¤šè½®è§†å¬ä½“éªŒï¼Œå°¤å…¶åœ¨é•¿æœŸè®°å¿†èƒ½åŠ›æ–¹é¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒInteractiveOmni-4Båœ¨é€šç”¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æ›´å¤§çš„æ¨¡å‹å¦‚Qwen2.5-Omni-7Bç›¸å½“ï¼Œå¹¶ä¸”åœ¨ä¿ç•™InteractiveOmni-8Bæ€§èƒ½çš„97%çš„åŒæ—¶ï¼Œä»…ä½¿ç”¨ä¸€åŠçš„æ¨¡å‹å¤§å°ã€‚InteractiveOmniåœ¨å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç†è§£å’Œè¯­éŸ³ç”Ÿæˆä»»åŠ¡æ–¹é¢è¾¾åˆ°äº†ä¸ç±»ä¼¼è§„æ¨¡æ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¯ä¸‹ä¸€ä»£æ™ºèƒ½äº¤äº’ç³»ç»Ÿçš„å¼€æ”¾ã€å¼€æºåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13747v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†InteractiveOmniï¼Œä¸€ä¸ªç»Ÿä¸€ã€å¼€æºçš„è·¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€‚ç”¨äºéŸ³é¢‘è§†è§‰å¤šè½®äº¤äº’ã€‚è¯¥æ¨¡å‹ä»4Båˆ°8Bå‚æ•°ï¼Œè®¾è®¡ç”¨äºè½»å‹æ¨¡å‹é¢†åŸŸï¼Œæä¾›å…¨é¢çš„è·¨æ¨¡æ€ç†è§£å’Œè¯­éŸ³ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡æ•´åˆè§†è§‰ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³è§£ç å™¨ï¼Œå®ç°ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„ç»Ÿä¸€æ¨¡å‹ã€‚é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿è·¨æ¨¡æ€èƒ½åŠ›ç¨³å¥ï¼ŒåŒ…æ‹¬å…ˆå¯¹è·¨æ¨¡æ€ç†è§£çš„é¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œè¯­éŸ³å¯¹è¯å’Œè§†å¬äº¤äº’çš„åè®­ç»ƒã€‚ä¸ºäº†å…·å¤‡ç±»ä¼¼äººç±»çš„é•¿æ•ˆå¯¹è¯èƒ½åŠ›ï¼Œç²¾å¿ƒç­–åˆ’äº†å¤šè½®è®­ç»ƒæ•°æ®é›†ï¼Œå¢å¼ºæ¨¡å‹å¤„ç†å¤æ‚å’Œå¤šè½®äº¤äº’çš„èƒ½åŠ›ã€‚ä¸ºæœ‰æ•ˆè¯„ä¼°å¤šè½®è®°å¿†å’Œè¯­éŸ³äº¤äº’èƒ½åŠ›ï¼Œæ„å»ºäº†å¤šæ¨¡æ€å¤šè½®è®°å¿†åŸºå‡†å’Œå¤šè½®è¯­éŸ³äº¤äº’åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒInteractiveOmniæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼Œæä¾›äº†æ›´æ™ºèƒ½çš„å¤šè½®è§†å¬ä½“éªŒï¼Œå°¤å…¶åœ¨é•¿æœŸè®°å¿†èƒ½åŠ›æ–¹é¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒInteractiveOmni-4Båœ¨ä¸€èˆ¬åŸºå‡†æµ‹è¯•ä¸Šä¸æ›´å¤§çš„æ¨¡å‹å¦‚Qwen2.5-Omni-7Bç›¸å½“ï¼Œå¹¶ä¸”èƒ½åœ¨ä»…ä½¿ç”¨50%æ¨¡å‹å¤§å°çš„æƒ…å†µä¸‹ä¿ç•™InteractiveOmni-8Bçš„97%æ€§èƒ½ã€‚InteractiveOmniåœ¨å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç†è§£å’Œè¯­éŸ³ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†åŒç±»æ¨¡å‹çš„æœ€ä¼˜ç»“æœï¼Œæ˜¯ä¸‹ä¸€ä»£æ™ºèƒ½äº¤äº’ç³»ç»Ÿçš„å¼€æ”¾ã€å¯è®¿é—®åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>InteractiveOmniæ˜¯ä¸€ä¸ªè·¨æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°éŸ³é¢‘è§†è§‰å¤šè½®äº¤äº’ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç»Ÿä¸€æ¡†æ¶ï¼Œæ•´åˆäº†è§†è§‰ã€éŸ³é¢‘ã€è¯­è¨€å’Œè¯­éŸ³è§£ç å™¨ã€‚</li>
<li>é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ç¡®ä¿è·¨æ¨¡æ€èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚</li>
<li>æ¨¡å‹å…·å¤‡äººç±»é•¿æœŸå¯¹è¯èƒ½åŠ›ï¼Œé€šè¿‡ç²¾å¿ƒç­–åˆ’çš„å¤šè½®è®­ç»ƒæ•°æ®é›†å®ç°ã€‚</li>
<li>å»ºç«‹äº†å¤šæ¨¡æ€å¤šè½®è®°å¿†åŸºå‡†å’Œå¤šè½®è¯­éŸ³äº¤äº’åŸºå‡†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>InteractiveOmniåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æœŸè®°å¿†èƒ½åŠ›æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d6a85d0c6be8ba8e8742260a5fe74040~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906577&auth_key=1760906577-0-0-b785d7e17c553e63994bd25a0a3f34d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-001a822e9b184ac31bf31765602769b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906584&auth_key=1760906584-0-0-ce4d78f1aa1701d04d0bb430a723c6f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6779f5978e449712f5d381540fea6ea5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906590&auth_key=1760906590-0-0-eb709a86d5dae62e83a0711087ef9216&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8f92a472d01db5c062216bcc6854fbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906598&auth_key=1760906598-0-0-31e42fa7907b88b60c92ce1d748a39a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a87400cdc8cbfff718761ad680f3358e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906604&auth_key=1760906604-0-0-7f7ed6ce0f1e71e722f5a7267a37887e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebb08387efa7cd2a9e6b1e1b7818fad6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906611&auth_key=1760906611-0-0-57a806d41242a62fd17a280092bb0d43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f061ec47a9ba4cc43ac75869adde1d47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906617&auth_key=1760906617-0-0-7dcfec86ce899c500a54d8f002bd6a95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE"><a href="#UniMoE-Audio-Unified-Speech-and-Music-Generation-with-Dynamic-Capacity-MoE" class="headerlink" title="UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity   MoE"></a>UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity   MoE</h2><p><strong>Authors:Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang</strong></p>
<p>Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each â€œproto-expertâ€ without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: <a target="_blank" rel="noopener" href="https://mukioxun.github.io/Uni-MoE-site/home.html">https://mukioxun.github.io/Uni-MoE-site/home.html</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€æ¨¡å‹è¿›å±•æ˜¾ç¤ºäº†ä¸€ç§æœç€å…¨é¢å†…å®¹ç”Ÿæˆå‘å±•çš„æ˜ç¡®è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œå¬è§‰é¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼ŒéŸ³ä¹ä¸è¯­éŸ³å¸¸å¸¸å­¤ç«‹å‘å±•ï¼Œé˜»ç¢äº†é€šç”¨éŸ³é¢‘åˆæˆçš„è¿›æ­¥ã€‚è¿™ç§åˆ†ç¦»æºäºå›ºæœ‰çš„ä»»åŠ¡å†²çªå’Œä¸¥é‡çš„æ•°æ®ä¸å¹³è¡¡ï¼Œé˜»ç¢äº†çœŸæ­£ç»Ÿä¸€çš„éŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniMoE-Audioï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨æ–°å‹åŠ¨æ€å®¹é‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¡†æ¶å†…çš„ç»Ÿä¸€è¯­éŸ³å’ŒéŸ³ä¹ç”Ÿæˆæ¨¡å‹ã€‚åœ¨ç»“æ„ä¸Šï¼ŒUniMoE-Audioå¼•å…¥äº†åŠ¨æ€ä¸“å®¶æ•°é‡åˆ†é…çš„Top-Pè·¯ç”±ç­–ç•¥ï¼Œä»¥åŠåŒ…å«é¢å‘ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„è·¯ç”±ä¸“å®¶ã€é¢å‘é€šç”¨ç‰¹å¾çš„å…±äº«ä¸“å®¶å’Œç”¨äºè‡ªé€‚åº”è®¡ç®—è·³è¿‡çš„ç©ºä¸“å®¶çš„æ··åˆä¸“å®¶è®¾è®¡ã€‚ä¸ºäº†è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰é˜¶æ®µè®­ç»ƒè¯¾ç¨‹ï¼š1ï¼‰ç‹¬ç«‹ä¸“å®¶è®­ç»ƒåˆ©ç”¨åŸå§‹æ•°æ®é›†æ¥å‘æ¯ä¸ªâ€œåŸå‹ä¸“å®¶â€çŒè¾“ç‰¹å®šé¢†åŸŸçŸ¥è¯†ï¼Œè€Œä¸ä¼šç›¸äº’å¹²æ‰°ï¼›2ï¼‰MoEé›†æˆå’Œé¢„çƒ­å°†è¿™äº›ä¸“å®¶çº³å…¥UniMoE-Audioæ¶æ„ï¼Œä½¿ç”¨å¹³è¡¡æ•°æ®é›†çš„å­é›†æ¥é¢„çƒ­é—¨æ¨¡å—å’Œå…±äº«ä¸“å®¶ï¼›3ï¼‰ååŒè”åˆè®­ç»ƒåœ¨å®Œå…¨å¹³è¡¡çš„æ•°æ®é›†ä¸Šå¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä¿ƒè¿›å¢å¼ºè·¨åŸŸååŒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniMoE-Audioä¸ä»…åœ¨ä¸»è¦è¯­éŸ³å’ŒéŸ³ä¹ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸”è¿˜å±•ç¤ºäº†å“è¶ŠååŒå­¦ä¹ èƒ½åŠ›ï¼Œå‡è½»äº†è”åˆè®­ç»ƒä¸­é€šå¸¸å‡ºç°çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†ä¸“ä¸šåŒ–MoEæ¶æ„å’Œå®šåˆ¶è®­ç»ƒç­–ç•¥åœ¨æ¨è¿›é€šç”¨éŸ³é¢‘ç”Ÿæˆé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚ä¸»é¡µé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://mukioxun.github.io/Uni-MoE-site/home.html">https://mukioxun.github.io/Uni-MoE-site/home.html</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13344v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€è¿‘æœŸç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„è¿›å±•ï¼Œå†…å®¹ç”Ÿæˆè¶‹å‘å…¨é¢ï¼Œä½†éŸ³é¢‘é¢†åŸŸçš„æŒ‘æˆ˜ä¾æ—§æ˜¾è‘—ã€‚éŸ³ä¹å’Œè¯­éŸ³çš„å­¤ç«‹å¼€å‘é˜»ç¢äº†é€šç”¨éŸ³é¢‘åˆæˆçš„è¿›å±•ã€‚UniMoE-Audioæ¨¡å‹åœ¨åŠ¨æ€å®¹é‡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¡†æ¶ä¸‹ç»Ÿä¸€äº†è¯­éŸ³å’ŒéŸ³ä¹ç”Ÿæˆï¼Œé€šè¿‡é¡¶çº§Pè·¯ç”±ç­–ç•¥å®ç°ä¸“å®¶æ•°é‡åŠ¨æ€åˆ†é…ï¼Œå¹¶è®¾è®¡æ··åˆä¸“å®¶æ¶æ„å¤„ç†é¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œé€šç”¨ç‰¹å¾ã€‚ä¸ºåº”å¯¹æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒè¯¾ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒUniMoE-Audioä¸ä»…åœ¨è¯­éŸ³å’ŒéŸ³ä¹ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œè¿˜å±•ç°å‡ºä¼˜è¶Šçš„ç»“åˆå­¦ä¹ èƒ½åŠ›ï¼Œç¼“è§£è”åˆè®­ç»ƒä¸­çš„æ€§èƒ½ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€è¿‘çš„å¤šæ¨¡æ€æ¨¡å‹è¿›å±•æ¨åŠ¨äº†å†…å®¹ç”Ÿæˆçš„å…¨é¢æ€§ï¼Œä½†éŸ³é¢‘ç”Ÿæˆä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>éŸ³ä¹å’Œè¯­éŸ³çš„å­¤ç«‹å¼€å‘é˜»ç¢äº†é€šç”¨éŸ³é¢‘åˆæˆçš„è¿›å±•ã€‚</li>
<li>UniMoE-Audioæ¨¡å‹åœ¨åŠ¨æ€å®¹é‡MoEæ¡†æ¶ä¸‹å®ç°äº†è¯­éŸ³å’ŒéŸ³ä¹çš„ç»Ÿä¸€ç”Ÿæˆã€‚</li>
<li>UniMoE-Audioé‡‡ç”¨é¡¶çº§Pè·¯ç”±ç­–ç•¥å’Œæ··åˆä¸“å®¶æ¶æ„è®¾è®¡ã€‚</li>
<li>ä¸‰é˜¶æ®µè®­ç»ƒè¯¾ç¨‹è§£å†³äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>UniMoE-Audioåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>UniMoE-Audioå±•ç°å‡ºä¼˜è¶Šçš„ç»“åˆå­¦ä¹ èƒ½åŠ›ï¼Œç¼“è§£è”åˆè®­ç»ƒä¸­çš„æ€§èƒ½ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8acb614012ff1f88d095ee9662de4bbe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906625&auth_key=1760906625-0-0-d5c52ac341276e09bf6f01e997b33bfb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82cc4e6cb50c323b45534fdafbec8d78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906632&auth_key=1760906632-0-0-0bf10713c3a4e52a492242440a45ae40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42f871764de92dfc386eb0b20cf8a20f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906639&auth_key=1760906639-0-0-4cbf9b378bca91668f30def9416652a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f3c6062676052f7efe128ee3f8ed86c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906645&auth_key=1760906645-0-0-6d5c4a8834f380409b222f9c8c85a8a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-820bc5bdefb41e596dde6409ea18e964~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906652&auth_key=1760906652-0-0-0a55ba9a3b2a36acb83bc412d4fb3e1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Mismatch-Aware-Guidance-for-Robust-Emotion-Control-in-Auto-Regressive-TTS-Models"><a href="#Mismatch-Aware-Guidance-for-Robust-Emotion-Control-in-Auto-Regressive-TTS-Models" class="headerlink" title="Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models"></a>Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models</h2><p><strong>Authors:Yizhou Peng, Yukun Ma, Chong Zhang, Yi-Wen Chao, Chongjia Ni, Bin Ma</strong></p>
<p>While Text-to-Speech (TTS) systems can achieve fine-grained control over emotional expression via natural language prompts, a significant challenge emerges when the desired emotion (style prompt) conflicts with the semantic content of the text. This mismatch often results in unnatural-sounding speech, undermining the goal of achieving fine-grained emotional control. Classifier-Free Guidance (CFG) is a key technique for enhancing prompt alignment; however, its application to auto-regressive (AR) TTS models remains underexplored, which can lead to degraded audio quality. This paper directly addresses the challenge of style-content mismatch in AR TTS models by proposing an adaptive CFG scheme that adjusts to different levels of the detected mismatch, as measured using large language models or natural language inference models. This solution is based on a comprehensive analysis of CFGâ€™s impact on emotional expressiveness in state-of-the-art AR TTS models. Our results demonstrate that the proposed adaptive CFG scheme improves the emotional expressiveness of the AR TTS model while maintaining audio quality and intelligibility. </p>
<blockquote>
<p>è™½ç„¶æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç»†ç²’åº¦çš„æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶ï¼Œä½†å½“æ‰€æœŸæœ›çš„æƒ…æ„Ÿï¼ˆé£æ ¼æç¤ºï¼‰ä¸æ–‡æœ¬è¯­ä¹‰å†…å®¹ç›¸å†²çªæ—¶ï¼Œå°±ä¼šå‡ºç°ä¸€ä¸ªé‡å¤§çš„æŒ‘æˆ˜ã€‚è¿™ç§ä¸åŒ¹é…é€šå¸¸ä¼šå¯¼è‡´è¯­éŸ³å¬èµ·æ¥ä¸è‡ªç„¶ï¼Œä»è€Œç ´åäº†å®ç°ç²¾ç»†æƒ…æ„Ÿæ§åˆ¶çš„ç›®æ ‡ã€‚æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯å¢å¼ºæç¤ºå¯¹é½çš„å…³é”®æŠ€æœ¯ï¼Œä½†å…¶åº”ç”¨äºè‡ªå›å½’ï¼ˆARï¼‰TTSæ¨¡å‹çš„æƒ…å†µä»è¢«è¾ƒå°‘æ¢ç´¢ï¼Œè¿™å¯èƒ½å¯¼è‡´éŸ³é¢‘è´¨é‡ä¸‹é™ã€‚æœ¬æ–‡ç›´æ¥è§£å†³äº†AR TTSæ¨¡å‹ä¸­é£æ ¼ä¸å†…å®¹ä¸åŒ¹é…è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”CFGæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æ ¹æ®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æˆ–è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹æ£€æµ‹åˆ°çš„ä¸åŒçº§åˆ«çš„ä¸åŒ¹é…è¿›è¡Œè°ƒæ•´ã€‚è¯¥è§£å†³æ–¹æ¡ˆåŸºäºå¯¹CFGåœ¨æœ€æ–°AR TTSæ¨¡å‹ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾å½±å“è¿›è¡Œå…¨é¢åˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è‡ªé€‚åº”CFGæ–¹æ¡ˆåœ¨ä¿æŒéŸ³é¢‘è´¨é‡å’Œæ¸…æ™°åº¦çš„åŒæ—¶ï¼Œæé«˜äº†AR TTSæ¨¡å‹çš„æƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13293v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ¢è®¨äº†è‡ªåŠ¨å›å½’ï¼ˆARï¼‰æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ä¸­çš„é£æ ¼ä¸å†…å®¹ä¸åŒ¹é…é—®é¢˜ã€‚å½“æœŸæœ›çš„æƒ…æ„Ÿï¼ˆé£æ ¼æç¤ºï¼‰ä¸æ–‡æœ¬è¯­ä¹‰å†…å®¹å†²çªæ—¶ï¼Œä¼šå¯¼è‡´è¯­éŸ³ä¸è‡ªç„¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„åˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ï¼ˆCFGï¼‰æ–¹æ¡ˆï¼Œæ ¹æ®æ£€æµ‹åˆ°çš„ä¸åŒ¹é…ç¨‹åº¦è¿›è¡Œè°ƒæ•´ï¼Œä»¥æé«˜AR TTSæ¨¡å‹çš„æƒ…æ„Ÿè¡¨ç°åŠ›ï¼ŒåŒæ—¶ä¿æŒéŸ³é¢‘è´¨é‡å’Œå¯ç†è§£æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­çš„æœŸæœ›æƒ…æ„Ÿä¸æ–‡æœ¬è¯­ä¹‰å†…å®¹å†²çªæ—¶ï¼Œä¼šäº§ç”Ÿä¸è‡ªç„¶çš„å£°éŸ³ã€‚</li>
<li>åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ï¼ˆCFGï¼‰æ˜¯æé«˜æç¤ºå¯¹é½çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>AR TTSæ¨¡å‹ä¸­CFGçš„åº”ç”¨ä»å­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½å¯¼è‡´éŸ³é¢‘è´¨é‡ä¸‹é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”CFGæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æ ¹æ®æ£€æµ‹åˆ°çš„ä¸åŒ¹é…ç¨‹åº¦è¿›è¡Œè°ƒæ•´ã€‚</li>
<li>è¯¥æ–¹æ¡ˆé€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æˆ–è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹æ¥è¡¡é‡ä¸åŒ¹é…ç¨‹åº¦ã€‚</li>
<li>æå‡ºçš„è‡ªé€‚åº”CFGæ–¹æ¡ˆåœ¨æé«˜AR TTSæ¨¡å‹çš„æƒ…æ„Ÿè¡¨ç°åŠ›çš„åŒæ—¶ï¼Œç»´æŒäº†éŸ³é¢‘è´¨é‡å’Œå¯ç†è§£æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9802c040a729a6eee08b03622cad723d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906659&auth_key=1760906659-0-0-472d5605f0f0d30bdafef9a590703fff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-069665a81899348a05fa0b7f33335002~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906666&auth_key=1760906666-0-0-755028a6b1c538751c441f3eb7937e60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33b1ca97204eced5728bc010c6025614~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906673&auth_key=1760906673-0-0-a35b898a8d159cadb3aa7630f334f0e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6fa97faff93f495d68a0c05d3afca89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906680&auth_key=1760906680-0-0-93c2bf6eda259a79056cdc742cc48c3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6f7a00647c25ef291476a522fbe8d9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906687&auth_key=1760906687-0-0-4cf658a475821aca9c2ec377cd1afb13&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Two-Heads-Are-Better-Than-One-Audio-Visual-Speech-Error-Correction-with-Dual-Hypotheses"><a href="#Two-Heads-Are-Better-Than-One-Audio-Visual-Speech-Error-Correction-with-Dual-Hypotheses" class="headerlink" title="Two Heads Are Better Than One: Audio-Visual Speech Error Correction with   Dual Hypotheses"></a>Two Heads Are Better Than One: Audio-Visual Speech Error Correction with   Dual Hypotheses</h2><p><strong>Authors:Sungnyun Kim, Kangwook Jang, Sungwoo Cho, Joon Son Chung, Hoirin Kim, Se-Young Yun</strong></p>
<p>This paper introduces a new paradigm for generative error correction (GER) framework in audio-visual speech recognition (AVSR) that reasons over modality-specific evidences directly in the language space. Our framework, DualHyp, empowers a large language model (LLM) to compose independent N-best hypotheses from separate automatic speech recognition (ASR) and visual speech recognition (VSR) models. To maximize the effectiveness of DualHyp, we further introduce RelPrompt, a noise-aware guidance mechanism that provides modality-grounded prompts to the LLM. RelPrompt offers the temporal reliability of each modality stream, guiding the model to dynamically switch its focus between ASR and VSR hypotheses for an accurate correction. Under various corruption scenarios, our framework attains up to 57.7% error rate gain on the LRS2 benchmark over standard ASR baseline, contrary to single-stream GER approaches that achieve only 10% gain. To facilitate research within our DualHyp framework, we release the code and the dataset comprising ASR and VSR hypotheses at <a target="_blank" rel="noopener" href="https://github.com/sungnyun/dualhyp">https://github.com/sungnyun/dualhyp</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ä¸­ç”Ÿæˆå¼è¯¯å·®æ ¡æ­£ï¼ˆGERï¼‰æ¡†æ¶çš„æ–°èŒƒå¼ã€‚è¯¥æ¡†æ¶ï¼ˆDualHypï¼‰èµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»å•ç‹¬çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰æ¨¡å‹ä¸­ç»„åˆå‡ºç‹¬ç«‹çš„N-bestå‡è®¾ã€‚ä¸ºäº†æœ€å¤§åŒ–DualHypçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†RelPromptï¼Œè¿™æ˜¯ä¸€ç§å™ªå£°æ„ŸçŸ¥æŒ‡å¯¼æœºåˆ¶ï¼Œä¸ºLLMæä¾›æ¨¡æ€åŸºç¡€æç¤ºã€‚RelPromptæä¾›å„æ¨¡æ€æµçš„æ—¶åºå¯é æ€§ï¼ŒæŒ‡å¯¼æ¨¡å‹åœ¨ASRå’ŒVSRå‡è®¾ä¹‹é—´åŠ¨æ€åˆ‡æ¢ï¼Œä»¥å®ç°å‡†ç¡®æ ¡æ­£ã€‚åœ¨å„ç§è…è´¥åœºæ™¯ä¸‹ï¼Œä¸æ ‡å‡†çš„ASRåŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨LRS2åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜è¾¾57.7%çš„é”™è¯¯ç‡å¢ç›Šï¼Œè€Œå•æµGERæ–¹æ³•ä»…å®ç°äº†10%çš„å¢ç›Šã€‚ä¸ºäº†åœ¨æˆ‘ä»¬çš„DualHypæ¡†æ¶å†…è¿›è¡Œç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/sungnyun/dualhyp%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%8C%85%E5%90%ABASR%E5%92%8CVSR%E5%81%87%E8%AE%BE%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/sungnyun/dualhypä¸Šå‘å¸ƒäº†åŒ…å«ASRå’ŒVSRå‡è®¾çš„ä»£ç å’Œæ•°æ®é›†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13281v1">PDF</a> Preprint work</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç”Ÿæˆå¼é”™è¯¯æ ¡æ­£ï¼ˆGERï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”¨äºè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ã€‚æå‡ºçš„DualHypæ¡†æ¶ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿç›´æ¥ä»è¯­è¨€ç©ºé—´ä¸­æ¨ç†æ¨¡æ€ç‰¹å®šè¯æ®ï¼Œå¹¶ç»„åˆæ¥è‡ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰æ¨¡å‹çš„ç‹¬ç«‹N-bestå‡è®¾ã€‚ä¸ºäº†æœ€å¤§åŒ–DualHypçš„æœ‰æ•ˆæ€§ï¼Œè¿›ä¸€æ­¥å¼•å…¥äº†RelPromptï¼Œè¿™æ˜¯ä¸€ç§å™ªå£°æ„ŸçŸ¥æŒ‡å¯¼æœºåˆ¶ï¼Œä¸ºLLMæä¾›æ¨¡æ€åŸºç¡€çš„æç¤ºã€‚RelPromptæä¾›äº†å„æ¨¡æ€æµçš„æ—¶åºå¯é æ€§ï¼ŒæŒ‡å¯¼æ¨¡å‹åœ¨ASRå’ŒVSRå‡è®¾ä¹‹é—´åŠ¨æ€åˆ‡æ¢ï¼Œä»¥å®ç°å‡†ç¡®æ ¡æ­£ã€‚åœ¨å„ç§è…è´¥åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨LRS2åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜è¾¾57.7%çš„é”™è¯¯ç‡å¢ç›Šï¼Œè€Œå•æµGERæ–¹æ³•ä»…å®ç°10%çš„å¢ç›Šã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æå‡ºäº†æ–°çš„ç”Ÿæˆå¼é”™è¯¯æ ¡æ­£ï¼ˆGERï¼‰æ¡†æ¶ï¼Œç”¨äºè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ã€‚</li>
<li>DualHypæ¡†æ¶ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿç›´æ¥å¤„ç†æ¨¡æ€ç‰¹å®šè¯æ®å¹¶ç»„åˆN-bestå‡è®¾ã€‚</li>
<li>RelPromptæœºåˆ¶æä¾›æ¨¡æ€åŸºç¡€æç¤ºï¼ŒæŒ‡å¯¼æ¨¡å‹åœ¨ASRå’ŒVSRå‡è®¾ä¹‹é—´åŠ¨æ€åˆ‡æ¢ã€‚</li>
<li>åœ¨å„ç§åœºæ™¯ä¸‹ï¼ŒDualHypæ¡†æ¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œé”™è¯¯ç‡é™ä½é«˜è¾¾57.7%ã€‚</li>
<li>ç›¸è¾ƒäºå•æµGERæ–¹æ³•ï¼ŒDualHypæ¡†æ¶æ€§èƒ½æ›´ä¼˜ã€‚</li>
<li>å…¬å¼€äº†åŒ…å«ASRå’ŒVSRå‡è®¾çš„ä»£ç å’Œæ•°æ®é›†ï¼Œä»¥æ¨åŠ¨ç ”ç©¶ã€‚</li>
<li>DualHypæ¡†æ¶å¯¹äºå¤æ‚ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«é”™è¯¯æ ¡æ­£å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-67beb0bdd11865242bc2b07b441dd455~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906695&auth_key=1760906695-0-0-f8329a302d46bd608b9b38806427bd80&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27e65dc7612b6d9c0a86fab8079c3eea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906702&auth_key=1760906702-0-0-ad7355a8ce7f2bc0508a2533b93214d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ede14b0f1b86c098efd64178e5f1398~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906709&auth_key=1760906709-0-0-90e0e9d0e9dc506e4fd1cf2cb60d32b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-680653cd9ebb1ada4f5bbc15228ab506~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906716&auth_key=1760906716-0-0-01f3a08c11cfe754366b565c240bb6a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-685cc15a0919340fb85adc8f4d0a4523~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906723&auth_key=1760906723-0-0-1de13326a90e227eca36110d3fc7d632&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2164224d6dd4967762cfbdf8bcba9ed5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906729&auth_key=1760906729-0-0-41a4939823a6cf3411fcea506a0d22cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50af7f761f5b4eaaef00e384a965c82f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906736&auth_key=1760906736-0-0-1d425a037ac07ab69425fb90cf85700d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Adaptive-vector-steering-A-training-free-layer-wise-intervention-for-hallucination-mitigation-in-large-audio-and-multimodal-models"><a href="#Adaptive-vector-steering-A-training-free-layer-wise-intervention-for-hallucination-mitigation-in-large-audio-and-multimodal-models" class="headerlink" title="Adaptive vector steering: A training-free, layer-wise intervention for   hallucination mitigation in large audio and multimodal models"></a>Adaptive vector steering: A training-free, layer-wise intervention for   hallucination mitigation in large audio and multimodal models</h2><p><strong>Authors:Tsung-En Lin, Kuan-Yi Lee, Hung-Yi Lee</strong></p>
<p>Large Audio-Language Models and Multi-Modal Large Language Models have demonstrated strong capabilities in tasks such as Audio Question Answering (AQA), Audio Captioning, and Automatic Speech Recognition (ASR). However, there is growing evidence that these models can hallucinate about the content of the audio. To address this issue, we probe the modelsâ€™ internal states and propose Adaptive Vector Steering (AVS), a method that better grounds generation in audio content. We also identify a strong correlation between output correctness and internal representations. Experiments show consistent performance gains across two models and two benchmarks. On the Audio Hallucination QA dataset, our method boosts the F1-score of Gemma from 0.550 to 0.619 and Qwen from 0.626 to 0.632. Furthermore, our method increases the accuracy of Qwen on MMAU from 0.548 to 0.592, marking an 8% relative increase. To the best of our knowledge, this is the first work to apply vector steering to mitigate hallucination in audio. </p>
<blockquote>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘é—®ç­”ï¼ˆAQAï¼‰ã€éŸ³é¢‘æè¿°å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ‰è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹ä¼šå¯¹éŸ³é¢‘å†…å®¹äº§ç”Ÿå¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€ï¼Œå¹¶æå‡ºäº†è‡ªé€‚åº”å‘é‡è½¬å‘ï¼ˆAVSï¼‰æ–¹æ³•ï¼Œæ›´å¥½åœ°å°†ç”Ÿæˆä¸éŸ³é¢‘å†…å®¹ç›¸ç»“åˆã€‚æˆ‘ä»¬è¿˜å‘ç°è¾“å‡ºæ­£ç¡®æ€§ä¸å†…éƒ¨è¡¨ç¤ºä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ¨¡å‹å’Œä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡å–å¾—äº†æ€§èƒ½æå‡ã€‚åœ¨éŸ³é¢‘å¹»è§‰é—®ç­”æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†Gemmaçš„F1åˆ†æ•°ä»0.550æé«˜åˆ°0.619ï¼Œå°†Qwençš„åˆ†æ•°ä»0.626æé«˜åˆ°0.632ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å°†Qwenåœ¨MMAUä¸Šçš„å‡†ç¡®ç‡ä»0.548æé«˜åˆ°0.592ï¼Œç›¸å¯¹æé«˜äº†8%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†å‘é‡è½¬å‘åº”ç”¨äºå‡è½»éŸ³é¢‘å¹»è§‰çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12851v1">PDF</a> Note: This preprint is a version of the paper submitted to ICASSP   2026. The author list here includes contributors who provided additional   supervision and guidance. The official ICASSP submission may differ slightly   in author composition</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘é—®ç­”ã€éŸ³é¢‘æ ‡æ³¨å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ‰è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜è¿™äº›æ¨¡å‹ä¼šå¯¹éŸ³é¢‘å†…å®¹äº§ç”Ÿå¹»è§‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ¢è®¨äº†æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å‘é‡è½¬å‘ï¼ˆAVSï¼‰æ–¹æ³•ï¼Œæ›´å¥½åœ°å°†ç”Ÿæˆå†…å®¹ä¸éŸ³é¢‘å†…å®¹ç›¸ç»“åˆã€‚åŒæ—¶ï¼Œæœ¬æ–‡å‘ç°è¾“å‡ºæ­£ç¡®æ€§ä¸å†…éƒ¨è¡¨å¾ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ç§æ¨¡å‹å’Œä¸¤ç§åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½å‡æœ‰æ‰€æé«˜ã€‚åœ¨éŸ³é¢‘å¹»è§‰é—®ç­”æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†Gemmaçš„F1åˆ†æ•°ä»0.550æé«˜åˆ°0.619ï¼Œå°†Qwençš„åˆ†æ•°ä»0.626æé«˜åˆ°0.632ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†Qwenåœ¨MMAUä¸Šçš„å‡†ç¡®ç‡ä»0.548æé«˜åˆ°0.592ï¼Œç›¸å¯¹æé«˜äº†8%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†å‘é‡è½¬å‘åº”ç”¨äºç¼“è§£éŸ³é¢‘å¹»è§‰é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å¯¹éŸ³é¢‘å†…å®¹äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†è‡ªé€‚åº”å‘é‡è½¬å‘ï¼ˆAVSï¼‰æ–¹æ³•ï¼Œä»¥æ”¹å–„æ¨¡å‹åœ¨éŸ³é¢‘å†…å®¹ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>å‘ç°è¾“å‡ºæ­£ç¡®æ€§ä¸æ¨¡å‹å†…éƒ¨è¡¨å¾ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚</li>
<li>AVSæ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†æ€§èƒ½ã€‚</li>
<li>åœ¨éŸ³é¢‘å¹»è§‰é—®ç­”æ•°æ®é›†ä¸Šï¼ŒAVSæ–¹æ³•æé«˜äº†Gemmaå’ŒQwençš„F1åˆ†æ•°ã€‚</li>
<li>AVSæ–¹æ³•æé«˜äº†Qwenåœ¨MMAUä¸Šçš„å‡†ç¡®ç‡ï¼Œç›¸å¯¹æé«˜äº†8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d6bfaa649b47b04d3bacd47d20079f98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906744&auth_key=1760906744-0-0-832397d675516792297b709ce9313aee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08b85101632a8938499b5a44622e3b05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906751&auth_key=1760906751-0-0-3db01695a5b2a00d8ce3a34c2ed13d7c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0690bdced066f3c591cfb05d143b1ca7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906758&auth_key=1760906758-0-0-3796cb8685dc16103d5ee1905f81455b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Structured-Sparsity-and-Weight-adaptive-Pruning-for-Memory-and-Compute-efficient-Whisper-models"><a href="#Structured-Sparsity-and-Weight-adaptive-Pruning-for-Memory-and-Compute-efficient-Whisper-models" class="headerlink" title="Structured Sparsity and Weight-adaptive Pruning for Memory and Compute   efficient Whisper models"></a>Structured Sparsity and Weight-adaptive Pruning for Memory and Compute   efficient Whisper models</h2><p><strong>Authors:Prasenjit K Mudi, Anshi Sachan, Dahlia Devapriya, Sheetal Kalyani</strong></p>
<p>Whisper models have achieved remarkable progress in speech recognition; yet their large size remains a bottleneck for deployment on resource-constrained edge devices. This paper proposes a framework to design fine-tuned variants of Whisper which address the above problem. Structured sparsity is enforced via the Sparse Group LASSO penalty as a loss regularizer, to reduce the number of FLOating Point operations (FLOPs). Further, a weight statistics aware pruning algorithm is proposed. We also design our custom text normalizer for WER evaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading WER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption and 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model parameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on Whisper-medium; and, (c) substantially outperform the state-of-the-art Iterative Magnitude Pruning based method by pruning 18.7% more parameters along with a 12.31 reduction in WER. </p>
<blockquote>
<p>whisperæ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡ä»æ˜¯éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè®¾è®¡whisperå¾®è°ƒå˜ç§ï¼ˆwhisper fine-tuned variantsï¼‰çš„æ¡†æ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨ç¨€ç–ç»„LASSOæƒ©ç½šä½œä¸ºæŸå¤±æ­£åˆ™åŒ–å™¨æ¥å¼ºåˆ¶å®æ–½ç»“æ„åŒ–ç¨€ç–æ€§ï¼Œä»¥å‡å°‘æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFLOPsï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æƒé‡ç»Ÿè®¡æ„ŸçŸ¥å‰ªæç®—æ³•ã€‚æˆ‘ä»¬è¿˜ä¸ºWERè¯„ä¼°è®¾è®¡äº†è‡ªå®šä¹‰æ–‡æœ¬è§„èŒƒåŒ–å™¨ã€‚åœ¨Common Voice 11.0å°åº¦æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬åœ¨ä¸é™ä½WERçš„æƒ…å†µä¸‹è·å¾—äº†ï¼ˆaï¼‰whisperå°å‹æ¨¡å‹çš„æ¨¡å‹å‚æ•°å‡å°‘35.4%ï¼Œå†…å­˜æ¶ˆè€—é™ä½14.25%ï¼ŒFLOPså‡å°‘18.5%ï¼›ï¼ˆbï¼‰whisperä¸­å‹æ¨¡å‹çš„æ¨¡å‹å‚æ•°å‡å°‘31%ï¼Œå†…å­˜æ¶ˆè€—é™ä½15.29%ï¼ŒFLOPså‡å°‘16.95%ï¼›ï¼ˆcï¼‰é€šè¿‡è¿­ä»£å¹…åº¦å‰ªææ–¹æ³•å¤§å¹…æé«˜äº†æ€§èƒ½ï¼Œå‰ªæå‚æ•°å¢åŠ äº†é«˜è¾¾è¾¾åˆ°æˆ–è¶…è¿‡ä¸šç•Œé¢†å…ˆæ°´å¹³è¾¾åˆ°äº†æœ€é«˜å¯è¾¾çš„æ°´å¹³æœ€å¤šå‡å°‘æ›´å¤šç™¾åˆ†æ¯”ä¸ºæŠ€æœ¯æ”¹è¿›æ–¹æ³•çš„å¯å‰”é™¤æ›´å¤šçš„å‚æ•°ç™¾åˆ†ä¹‹ æ›´é«˜çš„é™ä½åŒæ—¶ä¹Ÿè·å¾—äº†æ›´å¥½çš„è¯é”™è¯¯ç‡æ€§èƒ½é™ä½ç‡ï¼Œè¾¾åˆ°12.3%ã€‚å…¶ä¸­ã€‚é€šè¿‡å¯¹æ›´å¤šçš„å‚æ•°è¿›è¡Œå‰ªæå¹¶é™ä½è¯é”™è¯¯ç‡ï¼Œå®è´¨ä¸Šè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„è¿­ä»£å¹…åº¦å‰ªææ–¹æ³•ï¼Œå³æœ€å¤šå¯å‰ªææ›´å¤šçš„å‚æ•°å‡å°‘åŒæ—¶è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½æ”¹è¿›ç‡ï¼Œå¹¶ä¸”å®è´¨ä¸Šè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„è¿­ä»£å¹…åº¦å‰ªææ–¹æ³•æœ€å¤šå‡å°‘é«˜è¾¾æ›´å¤šç™¾åˆ†æ¯”çš„å‚æ•°ã€‚åŒæ—¶å¤§å¹…æå‡äº†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12666v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹Whisperæ¨¡å‹çš„ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–ç¨€ç–æ€§å’Œæƒé‡ç»Ÿè®¡æ„ŸçŸ¥çš„å‰ªæç®—æ³•ï¼Œå‡å°‘äº†æ¨¡å‹çš„å‚æ•°ã€å†…å­˜æ¶ˆè€—å’Œæµ®ç‚¹è¿ç®—æ•°é‡ã€‚åœ¨Common Voice 11.0 Hindiæ•°æ®é›†ä¸Šï¼Œå¯¹Whisper-smallå’ŒWhisper-mediumæ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶åœ¨ä¿æŒå­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚ç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¿›ä¸€æ­¥å‰ªææ›´å¤šçš„å‚æ•°å¹¶é™ä½WERã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Whisperæ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ¨¡å‹å¤§å°ä»æ˜¯é™åˆ¶å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²çš„ç“¶é¢ˆã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹Whisperæ¨¡å‹çš„ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–ç¨€ç–æ€§å’Œæƒé‡ç»Ÿè®¡æ„ŸçŸ¥çš„å‰ªæç®—æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨Sparse Group LASSOæƒ©ç½šä½œä¸ºæŸå¤±æ­£åˆ™åŒ–å™¨æ¥å®ç°ç»“æ„åŒ–ç¨€ç–æ€§ã€‚</li>
<li>å®šåˆ¶äº†æ–‡æœ¬è§„èŒƒåŒ–å™¨è¿›è¡ŒWERè¯„ä¼°ã€‚</li>
<li>åœ¨Common Voice 11.0 Hindiæ•°æ®é›†ä¸Šï¼Œå¯¹Whisper-smallå’ŒWhisper-mediumæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨¡å‹å‚æ•°ã€å†…å­˜æ¶ˆè€—å’Œæµ®ç‚¹è¿ç®—æ•°é‡ã€‚</li>
<li>ä¼˜åŒ–åçš„æ¨¡å‹åœ¨ä¿æŒå­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ•ˆç‡æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6085a9438dc5aa582a646644af1d9eba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906766&auth_key=1760906766-0-0-f3f15351a7cd9148b9ea7e9df6709c79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-acf412745bbc298649f438ccff4f4975~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906773&auth_key=1760906773-0-0-328b7f985d658476cc735fe1e570af6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07664d582e2b474c08891d679c4527a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906779&auth_key=1760906779-0-0-c85b2673f1a545fa86d1de33618febf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-631adf31758e085383f580893149f21f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906786&auth_key=1760906786-0-0-5abb38f4afd774c9a2dbd8eebf6870f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d28b41436d391521f80039d8763164d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906794&auth_key=1760906794-0-0-3999b6aa22d6082d1a6e121f9e654c84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-657f8a5313e2bfc3e254347e89de4b60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906800&auth_key=1760906800-0-0-ed7d24708c9296a0204ebd822f3218e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="I-DCCRN-VAE-An-Improved-Deep-Representation-Learning-Framework-for-Complex-VAE-based-Single-channel-Speech-Enhancement"><a href="#I-DCCRN-VAE-An-Improved-Deep-Representation-Learning-Framework-for-Complex-VAE-based-Single-channel-Speech-Enhancement" class="headerlink" title="I-DCCRN-VAE: An Improved Deep Representation Learning Framework for   Complex VAE-based Single-channel Speech Enhancement"></a>I-DCCRN-VAE: An Improved Deep Representation Learning Framework for   Complex VAE-based Single-channel Speech Enhancement</h2><p><strong>Authors:Jiatong Li, Simon Doclo</strong></p>
<p>Recently, a complex variational autoencoder (VAE)-based single-channel speech enhancement system based on the DCCRN architecture has been proposed. In this system, a noise suppression VAE (NSVAE) learns to extract clean speech representations from noisy speech using pretrained clean speech and noise VAEs with skip connections. In this paper, we improve DCCRN-VAE by incorporating three key modifications: 1) removing the skip connections in the pretrained VAEs to encourage more informative speech and noise latent representations; 2) using $\beta$-VAE in pretraining to better balance reconstruction and latent space regularization; and 3) a NSVAE generating both speech and noise latent representations. Experiments show that the proposed system achieves comparable performance as the DCCRN and DCCRN-VAE baselines on the matched DNS3 dataset but outperforms the baselines on mismatched datasets (WSJ0-QUT, Voicebank-DEMEND), demonstrating improved generalization ability. In addition, an ablation study shows that a similar performance can be achieved with classical fine-tuning instead of adversarial training, resulting in a simpler training pipeline. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºå¤æ‚å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’ŒDCCRNæ¶æ„çš„å•é€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿå·²ç»è¢«æå‡ºã€‚åœ¨è¿™ä¸ªç³»ç»Ÿä¸­ï¼Œå™ªå£°æŠ‘åˆ¶VAEï¼ˆNSVAEï¼‰å­¦ä¹ ä½¿ç”¨å¸¦æœ‰è·³è·ƒè¿æ¥çš„é¢„è®­ç»ƒå¹²å‡€è¯­éŸ³å’Œå™ªå£°VAEä»å¸¦å™ªå£°çš„è¯­éŸ³ä¸­æå–å¹²å‡€çš„è¯­éŸ³è¡¨ç¤ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡èå…¥ä¸‰ä¸ªå…³é”®æ”¹è¿›æ¥æå‡DCCRN-VAEçš„æ€§èƒ½ï¼š1ï¼‰ç§»é™¤é¢„è®­ç»ƒVAEä¸­çš„è·³è·ƒè¿æ¥ï¼Œä»¥é¼“åŠ±æ›´æœ‰ä¿¡æ¯é‡çš„è¯­éŸ³å’Œå™ªå£°æ½œåœ¨è¡¨ç¤ºï¼›2ï¼‰åœ¨é¢„è®­ç»ƒä¸­ä½¿ç”¨Î²-VAEï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡é‡å»ºå’Œæ½œåœ¨ç©ºé—´æ­£åˆ™åŒ–ï¼›3ï¼‰NSVAEç”Ÿæˆè¯­éŸ³å’Œå™ªå£°çš„æ½œåœ¨è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åŒ¹é…çš„DNS3æ•°æ®é›†ä¸Šï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿè¾¾åˆ°DCCRNå’ŒDCCRN-VAEåŸºå‡†çº¿çš„æ€§èƒ½æ°´å¹³ï¼Œä½†åœ¨ä¸åŒ¹é…çš„æ•°æ®é›†ï¼ˆWSJ0-QUTï¼ŒVoicebank-DEMENDï¼‰ä¸Šè¡¨ç°ä¼˜äºåŸºå‡†çº¿ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸€é¡¹æ¶ˆèç ”ç©¶ï¼ˆablation studyï¼‰è¡¨æ˜ï¼Œä½¿ç”¨ç»å…¸å¾®è°ƒè€Œéå¯¹æŠ—è®­ç»ƒä¹Ÿèƒ½è¾¾åˆ°ç±»ä¼¼æ€§èƒ½ï¼Œä»è€Œå¾—åˆ°æ›´ç®€å•çš„è®­ç»ƒæµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12485v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºå¤æ‚å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„å•ä¸€é€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿè¿‘æ—¥å·²æœ‰æå‡ºï¼Œå…¶ä¸­å™ªå£°æŠ‘åˆ¶VAEï¼ˆNSVAEï¼‰åˆ©ç”¨å¸¦æœ‰è·³è·ƒè¿æ¥çš„é¢„è®­ç»ƒå¹²å‡€è¯­éŸ³å’Œå™ªå£°VAEå­¦ä¹ ä»å¸¦å™ªè¯­éŸ³ä¸­æå–å¹²å‡€è¯­éŸ³è¡¨ç¤ºã€‚æœ¬æ–‡æ”¹è¿›äº†DCCRN-VAEï¼Œé€šè¿‡èå…¥ä¸‰é¡¹å…³é”®ä¿®æ”¹ï¼š1ï¼‰ç§»é™¤é¢„è®­ç»ƒVAEä¸­çš„è·³è·ƒè¿æ¥ä»¥é¼“åŠ±æ›´æœ‰ä¿¡æ¯é‡çš„è¯­éŸ³å’Œå™ªå£°æ½œåœ¨è¡¨å¾ï¼›2ï¼‰åœ¨é¢„è®­ç»ƒä¸­ä½¿ç”¨Î²-VAEä»¥æ›´å¥½åœ°å¹³è¡¡é‡å»ºå’Œæ½œåœ¨ç©ºé—´æ­£åˆ™åŒ–ï¼›3ï¼‰NSVAEç”Ÿæˆè¯­éŸ³å’Œå™ªå£°çš„æ½œåœ¨è¡¨å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨åŒ¹é…çš„DNS3æ•°æ®é›†ä¸Šè¡¨ç°ä¸DCCRNå’ŒDCCRN-VAEåŸºçº¿ç›¸å½“ï¼Œä½†åœ¨ä¸åŒ¹é…çš„WSJ0-QUTå’ŒVoicebank-DEMENDæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ç»å…¸å¾®è°ƒè€Œéå¯¹æŠ—æ€§è®­ç»ƒä¹Ÿå¯è¾¾åˆ°ç±»ä¼¼æ€§èƒ½ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºå¤æ‚å˜åˆ†è‡ªç¼–ç å™¨çš„å•ä¸€é€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿã€‚</li>
<li>é€šè¿‡å™ªå£°æŠ‘åˆ¶VAEå­¦ä¹ ä»å¸¦å™ªè¯­éŸ³ä¸­æå–å¹²å‡€è¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>å¯¹DCCRN-VAEè¿›è¡Œäº†ä¸‰é¡¹å…³é”®æ”¹è¿›ï¼ŒåŒ…æ‹¬å»é™¤é¢„è®­ç»ƒVAEçš„è·³è·ƒè¿æ¥ã€ä½¿ç”¨Î²-VAEè¿›è¡Œé¢„è®­ç»ƒä»¥åŠNSVAEç”Ÿæˆè¯­éŸ³å’Œå™ªå£°çš„æ½œåœ¨è¡¨å¾ã€‚</li>
<li>ç³»ç»Ÿåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ä¸åŒ¹é…çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†ä½¿ç”¨ç»å…¸å¾®è°ƒå¯ä»¥è¾¾åˆ°ä¸å¯¹æŠ—æ€§è®­ç»ƒç›¸ä¼¼çš„æ€§èƒ½ï¼Œç®€åŒ–äº†è®­ç»ƒæµç¨‹ã€‚</li>
<li>è¯¥ç³»ç»Ÿæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œæœ‰åŠ©äºæ”¹å–„è¯­éŸ³è´¨é‡å’Œå¯æ‡‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-01d96450f40122b1da9709fad485655f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906807&auth_key=1760906807-0-0-8bbbfcd8e46cbd1a4d9095c1f64c7449&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-05ac230e3591c6f181b00b4bcbee2e06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906815&auth_key=1760906815-0-0-547b5748a55f75a277ac1c881b4d76d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-030b95f3f1722a47b5b610703ad7456e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906823&auth_key=1760906823-0-0-486ba8aa516db08e7bc6cdf86bda29eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TFGA-Net-Temporal-Frequency-Graph-Attention-Network-for-Brain-Controlled-Speaker-Extraction"><a href="#TFGA-Net-Temporal-Frequency-Graph-Attention-Network-for-Brain-Controlled-Speaker-Extraction" class="headerlink" title="TFGA-Net: Temporal-Frequency Graph Attention Network for   Brain-Controlled Speaker Extraction"></a>TFGA-Net: Temporal-Frequency Graph Attention Network for   Brain-Controlled Speaker Extraction</h2><p><strong>Authors:Youhao Si, Yuan Liao, Qiushi Han, Yuhang Yang, Rui Dai, Liya Huang</strong></p>
<p>The rapid development of auditory attention decoding (AAD) based on electroencephalography (EEG) signals offers the possibility EEG-driven target speaker extraction. However, how to effectively utilize the target-speaker common information between EEG and speech remains an unresolved problem. In this paper, we propose a model for brain-controlled speaker extraction, which utilizes the EEG recorded from the listener to extract the target speech. In order to effectively extract information from EEG signals, we derive multi-scale timeâ€“frequency features and further incorporate cortical topological structures that are selectively engaged during the task. Moreover, to effectively exploit the non-Euclidean structure of EEG signals and capture their global features, the graph convolutional networks and self-attention mechanism are used in the EEG encoder. In addition, to make full use of the fused EEG and speech feature and preserve global context and capture speech rhythm and prosody, we introduce MossFormer2 which combines MossFormer and RNN-Free Recurrent as separator. Experimental results on both the public Cocktail Party and KUL dataset in this paper show that our TFGA-Net model significantly outper-forms the state-of-the-art method in certain objective evaluation metrics. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/LaoDa-X/TFGA-NET">https://github.com/LaoDa-X/TFGA-NET</a>. </p>
<blockquote>
<p>åŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·çš„å¬è§‰æ³¨æ„åŠ›è§£ç ï¼ˆAADï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºEEGé©±åŠ¨çš„ç›®æ ‡è¯´è¯äººæå–æä¾›äº†å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨EEGå’Œè¯­éŸ³ä¹‹é—´çš„ç›®æ ‡è¯´è¯äººçš„å…±åŒä¿¡æ¯ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è„‘æ§è¯´è¯äººæå–æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨è®°å½•ä¸‹æ¥çš„å¬ä¼—çš„è„‘ç”µå›¾æ¥æå–ç›®æ ‡è¯­éŸ³ã€‚ä¸ºäº†æœ‰æ•ˆåœ°ä»è„‘ç”µå›¾ä¿¡å·ä¸­æå–ä¿¡æ¯ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†å¤šå°ºåº¦æ—¶é—´-é¢‘ç‡ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥ç»“åˆäº†æ‰§è¡Œä»»åŠ¡æ—¶é€‰æ‹©æ€§æ¶‰åŠçš„çš®å±‚æ‹“æ‰‘ç»“æ„ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆåˆ©ç”¨è„‘ç”µå›¾ä¿¡å·çš„éæ¬§å‡ é‡Œå¾—ç»“æ„å¹¶æ•æ‰å…¶å…¨å±€ç‰¹å¾ï¼Œæˆ‘ä»¬åœ¨EEGç¼–ç å™¨ä¸­ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨èåˆçš„EEGå’Œè¯­éŸ³ç‰¹å¾ï¼Œå¹¶ä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶æ•æ‰è¯­éŸ³çš„èŠ‚å¥å’ŒéŸµå¾‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†MossFormer2ï¼Œå®ƒç»“åˆäº†MossFormerå’ŒRNN-Free Recurrentä½œä¸ºåˆ†ç¦»å™¨ã€‚æœ¬æ–‡åœ¨å…¬å…±é¸¡å°¾é…’ä¼šå’ŒKULæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„TFGA-Netæ¨¡å‹åœ¨æŸäº›å®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/LaoDa-X/TFGA-NET%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LaoDa-X/TFGA-NETè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12275v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè„‘ç”µå›¾ä¿¡å·çš„å¬è§‰æ³¨æ„åŠ›è§£ç ï¼ˆAADï¼‰å¿«é€Ÿå‘å±•ï¼Œä¸ºEEGé©±åŠ¨çš„ç›®æ ‡è¯­éŸ³æå–æä¾›äº†å¯èƒ½ã€‚æœ¬æ–‡æå‡ºä¸€ç§è„‘æ§è¯­éŸ³æå–æ¨¡å‹ï¼Œåˆ©ç”¨å¬ä¼—çš„è„‘ç”µå›¾è¿›è¡Œç›®æ ‡è¯­éŸ³æå–ã€‚é€šè¿‡æå–EEGä¿¡å·çš„å¤šå°ºåº¦æ—¶é—´-é¢‘ç‡ç‰¹å¾å¹¶èå…¥çš®è´¨æ‹“æ‰‘ç»“æ„ï¼Œç»“åˆå›¾å·ç§¯ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„EEGç¼–ç å™¨ï¼Œæœ‰æ•ˆæŒ–æ˜EEGä¿¡å·çš„éæ¬§å‡ é‡Œå¾—ç»“æ„å¹¶æ•æ‰å…¨å±€ç‰¹å¾ã€‚åŒæ—¶ï¼Œé€šè¿‡MossFormer2åˆ†ç¦»å™¨ç»“åˆMossFormerå’ŒRNN-Free RecurrentæŠ€æœ¯ï¼Œå……åˆ†åˆ©ç”¨èåˆåçš„EEGå’Œè¯­éŸ³ç‰¹å¾ï¼Œä¿ç•™å…¨å±€è¯­å¢ƒå¹¶æ•æ‰è¯­éŸ³èŠ‚å¥å’Œè¯­è°ƒã€‚åœ¨å…¬å…±é¸¡å°¾é…’ä¼šå’ŒKULæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„TFGA-Netæ¨¡å‹åœ¨æŸäº›å®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AADåŸºäºEEGä¿¡å·å®ç°ç›®æ ‡è¯­éŸ³æå–ã€‚</li>
<li>åˆ©ç”¨å¬ä¼—çš„EEGæ•°æ®è¿›è¡Œç›®æ ‡è¯­éŸ³æå–ã€‚</li>
<li>é€šè¿‡å¤šå°ºåº¦æ—¶é—´-é¢‘ç‡ç‰¹å¾å’Œçš®è´¨æ‹“æ‰‘ç»“æ„è¿›è¡ŒEEGä¿¡å·å¤„ç†ã€‚</li>
<li>ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„EEGç¼–ç å™¨ã€‚</li>
<li>MossFormer2åˆ†ç¦»å™¨ç»“åˆMossFormerå’ŒRNN-Free RecurrentæŠ€æœ¯ç”¨äºå¤„ç†èåˆåçš„EEGå’Œè¯­éŸ³ç‰¹å¾ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜TFGA-Netæ¨¡å‹åœ¨å®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-505be52f29c101a292491a91ac9e6f13~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906831&auth_key=1760906831-0-0-bee682174c8256436523bc243568055a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b67ffdba5ab4d88899d436bbf52d8970~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906838&auth_key=1760906838-0-0-78eb8afb5f0ffe36d2c968196a0c268f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-592d7a9502357da237673c20b7566af7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906845&auth_key=1760906845-0-0-4774487f22233567ea0d7d634c34084d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e8cdad09017bb37fc34176cd8338cd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906852&auth_key=1760906852-0-0-703f6c342bcdb0029c3267ff8ece9e15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7eea00489ebb9eee024869c99924983~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906859&auth_key=1760906859-0-0-27f6521ee4370d14e3dada35a6ba4f56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="BridgeCode-A-Dual-Speech-Representation-Paradigm-for-Autoregressive-Zero-Shot-Text-to-Speech-Synthesis"><a href="#BridgeCode-A-Dual-Speech-Representation-Paradigm-for-Autoregressive-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis"></a>BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis</h2><p><strong>Authors:Jingyuan Xing, Mingru Yang, Zhipeng Li, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Autoregressive (AR) frameworks have recently achieved remarkable progress in zero-shot text-to-speech (TTS) by leveraging discrete speech tokens and large language model techniques. Despite their success, existing AR-based zero-shot TTS systems face two critical limitations: (i) an inherent speed-quality trade-off, as sequential token generation either reduces frame rates at the cost of expressiveness or enriches tokens at the cost of efficiency, and (ii) a text-oriented supervision mismatch, as cross-entropy loss penalizes token errors uniformly without considering the fine-grained acoustic similarity among adjacent tokens. To address these challenges, we propose BridgeTTS, a novel AR-TTS framework built upon the dual speech representation paradigm BridgeCode. BridgeTTS reduces AR iterations by predicting sparse tokens while reconstructing rich continuous features for high-quality synthesis. Joint optimization of token-level and feature-level objectives further enhances naturalness and intelligibility. Experiments demonstrate that BridgeTTS achieves competitive quality and speaker similarity while significantly accelerating synthesis. Speech demos are available at <a target="_blank" rel="noopener" href="https://test1562.github.io/demo/">https://test1562.github.io/demo/</a>. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶æœ€è¿‘å€ŸåŠ©ç¦»æ•£è¯­éŸ³æ ‡è®°å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œåœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†ç°æœ‰çš„åŸºäºARçš„é›¶æ ·æœ¬TTSç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆiï¼‰å›ºæœ‰çš„é€Ÿåº¦ä¸è´¨é‡æƒè¡¡ï¼Œå› ä¸ºé¡ºåºæ ‡è®°ç”Ÿæˆè¦ä¹ˆä»¥é™ä½å¸§ç‡ä¸ºä»£ä»·æ¢å–è¡¨ç°åŠ›ï¼Œè¦ä¹ˆä»¥ç‰ºç‰²æ•ˆç‡ä¸ºä»£ä»·ä¸°å¯Œæ ‡è®°ï¼›ï¼ˆiiï¼‰æ–‡æœ¬å¯¼å‘çš„ç›‘ç£ä¸åŒ¹é…ï¼Œå› ä¸ºäº¤å‰ç†µæŸå¤±ä¼šç»Ÿä¸€æƒ©ç½šæ ‡è®°é”™è¯¯ï¼Œè€Œä¸ä¼šè€ƒè™‘ç›¸é‚»æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å£°å­¦ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŒè¯­éŸ³è¡¨ç¤ºèŒƒå¼çš„BridgeCodeæ„å»ºçš„å…¨æ–°AR-TTSæ¡†æ¶BridgeTTSã€‚BridgeTTSé€šè¿‡é¢„æµ‹ç¨€ç–æ ‡è®°å’Œé‡å»ºä¸°å¯Œçš„è¿ç»­ç‰¹å¾æ¥å‡å°‘ARè¿­ä»£ï¼Œä»¥å®ç°é«˜è´¨é‡åˆæˆã€‚å¯¹æ ‡è®°çº§å’Œç›®æ ‡ç‰¹å¾çº§çš„è”åˆä¼˜åŒ–è¿›ä¸€æ­¥æé«˜äº†è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒBridgeTTSåœ¨ä¿è¯ç«äº‰è´¨é‡çš„åŒæ—¶æ˜¾è‘—æé«˜äº†è¯­éŸ³åˆæˆçš„é€Ÿåº¦ï¼Œå¹¶å…·æœ‰é«˜åº¦çš„è¯´è¯äººç›¸ä¼¼æ€§ã€‚æ›´å¤šè¯­éŸ³æ¼”ç¤ºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://test1562.github.io/demo/%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://test1562.github.io/demo/è¿›è¡Œè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11646v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¦»æ•£è¯­éŸ³æ ‡è®°æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶åœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AR-TTSç³»ç»Ÿé¢ä¸´é€Ÿåº¦ä¸è´¨é‡çš„æƒè¡¡é—®é¢˜ä»¥åŠæ–‡æœ¬å¯¼å‘çš„ç›‘ç£ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŒé‡è¯­éŸ³è¡¨ç¤ºèŒƒå¼BridgeCodeçš„BridgeTTSæ¡†æ¶ã€‚BridgeTTSé€šè¿‡é¢„æµ‹ç¨€ç–æ ‡è®°å¹¶ç»“åˆé‡å»ºä¸°å¯Œçš„è¿ç»­ç‰¹å¾æ¥å‡å°‘ARè¿­ä»£æ¬¡æ•°ï¼Œä»¥å®ç°é«˜è´¨é‡åˆæˆã€‚å¯¹æ ‡è®°çº§å’Œç‰¹å¾çº§çš„è”åˆä¼˜åŒ–è¿›ä¸€æ­¥æé«˜äº†è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦ã€‚å®éªŒè¯æ˜ï¼ŒBridgeTTSåœ¨ä¿è¯è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†åˆæˆé€Ÿåº¦ã€‚æ›´å¤šè¯­éŸ³æ¼”ç¤ºè¯·è®¿é—®ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARæ¡†æ¶ç»“åˆç¦»æ•£è¯­éŸ³æ ‡è®°å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯åœ¨é›¶æ ·æœ¬TTSä¸­è¡¨ç°çªå‡ºã€‚</li>
<li>AR-TTSç³»ç»Ÿé¢ä¸´é€Ÿåº¦ä¸è´¨é‡çš„æƒè¡¡æŒ‘æˆ˜ã€‚</li>
<li>BridgeTTSæ¡†æ¶åŸºäºåŒé‡è¯­éŸ³è¡¨ç¤ºèŒƒå¼BridgeCodeï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>BridgeTTSé€šè¿‡é¢„æµ‹ç¨€ç–æ ‡è®°å¹¶é‡å»ºè¿ç»­ç‰¹å¾æ¥å‡å°‘ARè¿­ä»£æ¬¡æ•°ã€‚</li>
<li>è”åˆä¼˜åŒ–æ ‡è®°çº§å’Œç‰¹å¾çº§ç›®æ ‡æé«˜äº†è¯­éŸ³çš„è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦ã€‚</li>
<li>å®éªŒè¯æ˜BridgeTTSåœ¨ä¿è¯è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†åˆæˆé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3a310a8e7ca4822aeffacc3273ea3de7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906866&auth_key=1760906866-0-0-2e675cadc1db89830d7435cc006592d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebf0c6fd96d2b87f549ae943780e1ae6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906873&auth_key=1760906873-0-0-a0e472544d0a50d06acc3f0df8fe0ca9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e411ae7c3c3606dd378eddb02a79b88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906880&auth_key=1760906880-0-0-ffeb6e6e81a1c936738b3ddfe6cce2b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ac66923924d9f580cb0adced9e8327e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906887&auth_key=1760906887-0-0-903cb736603c540b7993fafb74ad9266&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed0be693d5f89f9a078a8a7e79bd5ce2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906893&auth_key=1760906893-0-0-151a1f3899b2beed84c46a3f52cd04c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="An-Encoder-Integrated-PhoBERT-with-Graph-Attention-for-Vietnamese-Token-Level-Classification"><a href="#An-Encoder-Integrated-PhoBERT-with-Graph-Attention-for-Vietnamese-Token-Level-Classification" class="headerlink" title="An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese   Token-Level Classification"></a>An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese   Token-Level Classification</h2><p><strong>Authors:Ba-Quang Nguyen</strong></p>
<p>We propose a novel neural architecture named TextGraphFuseGAT, which integrates a pretrained transformer encoder (PhoBERT) with Graph Attention Networks for token-level classification tasks. The proposed model constructs a fully connected graph over the token embeddings produced by PhoBERT, enabling the GAT layer to capture rich inter-token dependencies beyond those modeled by sequential context alone. To further enhance contextualization, a Transformer-style self-attention layer is applied on top of the graph-enhanced embeddings. The final token representations are passed through a classification head to perform sequence labeling. We evaluate our approach on three Vietnamese benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19 domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER dataset, featuring 18 entity types collected from real-world medical speech transcripts and annotated with the BIO tagging scheme. Its specialized vocabulary and domain-specific expressions make it a challenging benchmark for token-level classification models. Experimental results show that our method consistently outperforms strong baselines, including transformer-only and hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness of combining pretrained semantic features with graph-based relational modeling for improved token classification across multiple domains. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåä¸ºTextGraphFuseGATï¼Œå®ƒå°†é¢„è®­ç»ƒçš„è½¬æ¢å™¨ç¼–ç å™¨ï¼ˆPhoBERTï¼‰ä¸å›¾æ³¨æ„åŠ›ç½‘ç»œé›†æˆï¼Œç”¨äºè¿›è¡Œæ ‡è®°çº§åˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨PhoBERTäº§ç”Ÿçš„æ ‡è®°åµŒå…¥ä¸Šæ„å»ºäº†ä¸€ä¸ªå…¨è¿æ¥å›¾ï¼Œä½¿GATå±‚èƒ½å¤Ÿæ•è·ä¸°å¯Œçš„æ ‡è®°é—´ä¾èµ–å…³ç³»ï¼Œè€Œä¸ä»…ä»…æ˜¯é€šè¿‡é¡ºåºä¸Šä¸‹æ–‡è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºä¸Šä¸‹æ–‡åŒ–ï¼Œåœ¨å›¾å¢å¼ºåµŒå…¥ä¹‹ä¸Šåº”ç”¨äº†Transformeré£æ ¼çš„è‡ªæ³¨æ„åŠ›å±‚ã€‚æœ€ç»ˆçš„æ ‡è®°è¡¨ç¤ºé€šè¿‡åˆ†ç±»å¤´è¿›è¡Œä¼ é€’ï¼Œä»¥æ‰§è¡Œåºåˆ—æ ‡æ³¨ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªè¶Šå—åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šç”¨äºCOVID-19é¢†åŸŸå‘½åå®ä½“è¯†åˆ«çš„PhoNER-COVID19ã€ç”¨äºè¯­éŸ³ä¸æµç•…æ£€æµ‹çš„PhoDisfluencyï¼Œä»¥åŠç”¨äºåŒ»ç–—é¢†åŸŸå‘½åå®ä½“è¯†åˆ«çš„VietMed-NERã€‚VietMed-NERæ˜¯ç¬¬ä¸€ä¸ªè¶Šå—åŒ»ç–—å£è¯­å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†ï¼Œå®ƒåŒ…å«ä»ç°å®ä¸–ç•ŒåŒ»ç–—è¯­éŸ³è½¬å½•ä¸­æ”¶é›†çš„18ç§å®ä½“ç±»å‹ï¼Œå¹¶æŒ‰BIOæ ‡æ³¨æ–¹æ¡ˆè¿›è¡Œæ ‡æ³¨ã€‚å…¶ä¸“ä¸šè¯æ±‡å’Œé¢†åŸŸç‰¹å®šè¡¨è¾¾å¼ä½¿å…¶æˆä¸ºæ ‡è®°çº§åˆ†ç±»æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒåŒ…æ‹¬ä»…ä½¿ç”¨è½¬æ¢å™¨å’Œæ··åˆç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚BiLSTM+CNN+CRFï¼‰ï¼Œè¿™è¯å®äº†å°†é¢„è®­ç»ƒè¯­ä¹‰ç‰¹å¾ä¸åŸºäºå›¾çš„å…³ç³»å»ºæ¨¡ç›¸ç»“åˆï¼Œåœ¨å¤šä¸ªé¢†åŸŸè¿›è¡Œæ”¹è¿›çš„ä»¤ç‰Œåˆ†ç±»çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11537v1">PDF</a> 11 pages, 1 figure. Submitted to VLSP 2025 and reviewed</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºä¸€ç§åä¸ºTextGraphFuseGATçš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„PhoBERTç¼–ç å™¨å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œç”¨äºä»¤ç‰Œçº§åˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªå®Œå…¨è¿æ¥çš„å›¾ï¼ŒåŸºäºPhoBERTäº§ç”Ÿçš„ä»¤ç‰ŒåµŒå…¥ï¼Œä½¿GATå±‚èƒ½å¤Ÿæ•è·ä¸°å¯Œçš„ä»¤ç‰Œé—´ä¾èµ–å…³ç³»ï¼Œè€Œä¸ä»…ä»…æ˜¯åŸºäºé¡ºåºä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚é€šè¿‡å›¾å¢å¼ºåµŒå…¥ä¹‹ä¸Šåº”ç”¨Transformeré£æ ¼çš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ä¸Šä¸‹æ–‡ã€‚æœ€ç»ˆçš„ä»¤ç‰Œè¡¨ç¤ºé€šè¿‡åˆ†ç±»å¤´è¿›è¡Œä¼ é€’ï¼Œä»¥æ‰§è¡Œåºåˆ—æ ‡æ³¨ã€‚åœ¨è¶Šå—è¯­åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬PhoNER-COVID19ã€PhoDisfluencyå’ŒVietMed-NERã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŒ…æ‹¬ä»…ä½¿ç”¨å˜æ¢å™¨å’Œæ··åˆç¥ç»ç½‘ç»œæ¨¡å‹åœ¨å†…çš„å¼ºåŸºçº¿ï¼Œè¯æ˜äº†å°†é¢„è®­ç»ƒè¯­ä¹‰ç‰¹å¾ä¸å›¾å…³ç³»å»ºæ¨¡ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œå¯æ”¹å–„è·¨å¤šä¸ªé¢†åŸŸçš„ä»¤ç‰Œåˆ†ç±»ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†åä¸ºTextGraphFuseGATçš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ã€‚</li>
<li>è¯¥æ¶æ„ç»“åˆäº†é¢„è®­ç»ƒçš„PhoBERTç¼–ç å™¨å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œã€‚</li>
<li>é€šè¿‡æ„å»ºå®Œå…¨è¿æ¥çš„å›¾æ¥æ•è·ä»¤ç‰Œé—´çš„ä¸°å¯Œä¾èµ–å…³ç³»ã€‚</li>
<li>åœ¨å›¾å¢å¼ºåµŒå…¥ä¹‹ä¸Šåº”ç”¨äº†Transformeré£æ ¼çš„è‡ªæ³¨æ„åŠ›å±‚ä»¥å¢å¼ºä¸Šä¸‹æ–‡ã€‚</li>
<li>åœ¨å¤šä¸ªè¶Šå—è¯­åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬PhoNER-COVID19ã€PhoDisfluencyå’ŒVietMed-NERã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œç›¸æ¯”å¼ºåŸºçº¿æœ‰æ›´ä½³è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-640050b974aa9a2f472fadd56a201804~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906901&auth_key=1760906901-0-0-917405b68a0395e65579da4a7d12d194&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d351b22f97c859b440c0508fa2b6cf43~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906908&auth_key=1760906908-0-0-b746397a3fe04d2725675da350cc930e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Dynamically-Slimmable-Speech-Enhancement-Network-with-Metric-Guided-Training"><a href="#Dynamically-Slimmable-Speech-Enhancement-Network-with-Metric-Guided-Training" class="headerlink" title="Dynamically Slimmable Speech Enhancement Network with Metric-Guided   Training"></a>Dynamically Slimmable Speech Enhancement Network with Metric-Guided   Training</h2><p><strong>Authors:Haixin Zhao, Kaixuan Yang, Nilesh Madhu</strong></p>
<p>To further reduce the complexity of lightweight speech enhancement models, we introduce a gating-based Dynamically Slimmable Network (DSN). The DSN comprises static and dynamic components. For architecture-independent applicability, we introduce distinct dynamic structures targeting the commonly used components, namely, grouped recurrent neural network units, multi-head attention, convolutional, and fully connected layers. A policy module adaptively governs the use of dynamic parts at a frame-wise resolution according to the input signal quality, controlling computational load. We further propose Metric-Guided Training (MGT) to explicitly guide the policy module in assessing input speech quality. Experimental results demonstrate that the DSN achieves comparable enhancement performance in instrumental metrics to the state-of-the-art lightweight baseline, while using only 73% of its computational load on average. Evaluations of dynamic component usage ratios indicate that the MGT-DSN can appropriately allocate network resources according to the severity of input signal distortion. </p>
<blockquote>
<p>ä¸ºäº†è¿›ä¸€æ­¥ç®€åŒ–è½»é‡çº§è¯­éŸ³å¢å¼ºæ¨¡å‹çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºé—¨æ§çš„åŠ¨æ€å¯ç¼©æ”¾ç½‘ç»œï¼ˆDSNï¼‰ã€‚DSNåŒ…å«é™æ€å’ŒåŠ¨æ€ç»„ä»¶ã€‚ä¸ºäº†å®ç°ç‹¬ç«‹äºæ¶æ„çš„é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬é’ˆå¯¹å¸¸ç”¨ç»„ä»¶å¼•å…¥äº†ä¸åŒçš„åŠ¨æ€ç»“æ„ï¼Œå³åˆ†ç»„å¾ªç¯ç¥ç»ç½‘ç»œå•å…ƒã€å¤šå¤´æ³¨æ„åŠ›ã€å·ç§¯å’Œå…¨è¿æ¥å±‚ã€‚ç­–ç•¥æ¨¡å—æ ¹æ®è¾“å…¥ä¿¡å·è´¨é‡ä»¥å¸§çº§åˆ†è¾¨ç‡è‡ªé€‚åº”åœ°æ§åˆ¶åŠ¨æ€éƒ¨åˆ†çš„ä½¿ç”¨ï¼Œæ§åˆ¶è®¡ç®—è´Ÿè½½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Metric-Guided Trainingï¼ˆMGTï¼‰æ¥æ˜ç¡®æŒ‡å¯¼ç­–ç•¥æ¨¡å—è¯„ä¼°è¾“å…¥è¯­éŸ³è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSNåœ¨ä»ªå™¨æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸æœ€æ–°è½»é‡çº§åŸºçº¿ç›¸å½“çš„å¢å¼ºæ€§èƒ½ï¼Œè€Œå¹³å‡è®¡ç®—è´Ÿè½½ä»…ä¸ºå…¶73%ã€‚å¯¹åŠ¨æ€ç»„ä»¶ä½¿ç”¨ç‡çš„è¯„ä¼°è¡¨æ˜ï¼ŒMGT-DSNå¯ä»¥æ ¹æ®è¾“å…¥ä¿¡å·å¤±çœŸçš„ä¸¥é‡ç¨‹åº¦é€‚å½“åœ°åˆ†é…ç½‘ç»œèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11395v1">PDF</a> Preprint version of a paper under review at ICASSP2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé—¨æ§çš„åŠ¨æ€å¯ç¼©æ”¾ç½‘ç»œï¼ˆDSNï¼‰ï¼Œä»¥è¿›ä¸€æ­¥ç®€åŒ–è½»é‡çº§è¯­éŸ³å¢å¼ºæ¨¡å‹çš„å¤æ‚æ€§ã€‚DSNåŒ…æ‹¬é™æ€å’ŒåŠ¨æ€ç»„ä»¶ï¼Œä¸ºäº†å…·æœ‰ç‹¬ç«‹äºæ¶æ„çš„é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬é’ˆå¯¹å¸¸ç”¨çš„ç»„ä»¶ï¼ˆå¦‚åˆ†ç»„å¾ªç¯ç¥ç»ç½‘ç»œå•å…ƒã€å¤šå¤´æ³¨æ„åŠ›ã€å·ç§¯å’Œå…¨è¿æ¥å±‚ï¼‰å¼•å…¥äº†ä¸åŒçš„åŠ¨æ€ç»“æ„ã€‚ç­–ç•¥æ¨¡å—æ ¹æ®è¾“å…¥ä¿¡å·è´¨é‡ä»¥å¸§çº§åˆ†è¾¨ç‡è‡ªé€‚åº”åœ°æ§åˆ¶åŠ¨æ€éƒ¨åˆ†çš„ä½¿ç”¨ï¼Œä»è€Œæ§åˆ¶è®¡ç®—è´Ÿè½½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åº¦é‡æŒ‡å¯¼è®­ç»ƒï¼ˆMGTï¼‰æ¥æ˜ç¡®æŒ‡å¯¼ç­–ç•¥æ¨¡å—è¯„ä¼°è¾“å…¥è¯­éŸ³è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSNåœ¨ä»ªå™¨åº¦é‡ä¸Šå®ç°äº†ä¸æœ€æ–°è½»é‡çº§åŸºçº¿ç›¸å½“çš„å¢å¼ºæ€§èƒ½ï¼Œå¹³å‡è®¡ç®—è´Ÿè½½ä»…ä¸ºå…¶73%ã€‚å¯¹åŠ¨æ€ç»„ä»¶ä½¿ç”¨ç‡çš„è¯„ä¼°è¡¨æ˜ï¼ŒMGT-DSNå¯ä»¥æ ¹æ®è¾“å…¥ä¿¡å·å¤±çœŸçš„ä¸¥é‡ç¨‹åº¦é€‚å½“åœ°åˆ†é…ç½‘ç»œèµ„æºã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¼•å…¥äº†åŸºäºé—¨æ§çš„åŠ¨æ€å¯ç¼©æ”¾ç½‘ç»œï¼ˆDSNï¼‰æ¥ç®€åŒ–è½»é‡çº§è¯­éŸ³å¢å¼ºæ¨¡å‹çš„å¤æ‚æ€§ã€‚</li>
<li>DSNåŒ…å«é™æ€å’ŒåŠ¨æ€ç»„ä»¶ï¼Œé€‚ç”¨äºå¤šç§æ¶æ„ã€‚</li>
<li>é’ˆå¯¹å¸¸è§çš„ç»„ä»¶å¦‚åˆ†ç»„å¾ªç¯ç¥ç»ç½‘ç»œå•å…ƒã€å¤šå¤´æ³¨æ„åŠ›ã€å·ç§¯å’Œå…¨è¿æ¥å±‚ï¼Œå¼•å…¥äº†åŠ¨æ€ç»“æ„ã€‚</li>
<li>ç­–ç•¥æ¨¡å—èƒ½è‡ªé€‚åº”åœ°æ ¹æ®è¾“å…¥ä¿¡å·è´¨é‡æ§åˆ¶åŠ¨æ€éƒ¨åˆ†çš„ä½¿ç”¨ï¼Œå¹¶ç®¡ç†è®¡ç®—è´Ÿè½½ã€‚</li>
<li>æå‡ºäº†åº¦é‡æŒ‡å¯¼è®­ç»ƒï¼ˆMGTï¼‰æ¥æŒ‡å¯¼ç­–ç•¥æ¨¡å—è¯„ä¼°è¾“å…¥è¯­éŸ³è´¨é‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒDSNåœ¨ä»ªå™¨åº¦é‡ä¸Šçš„å¢å¼ºæ€§èƒ½ä¸æœ€æ–°è½»é‡çº§åŸºçº¿ç›¸å½“ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-71fb77c79d4fdf9f0d96a3231a27e147~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906916&auth_key=1760906916-0-0-fb2f5c5257c31206623d7713998d3c29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c158ea8bb981d701b4409e261e69b16e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906924&auth_key=1760906924-0-0-fe77c8e21e620cd96702085219e07b63&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f2fccd5bef00ca0e32b07b72678c5185~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906931&auth_key=1760906931-0-0-91fe800e4cde01801e461b0bc3acfbbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6934a04244238674cf9ef96ac72aba5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906938&auth_key=1760906938-0-0-7410231454c2e5e194bb61c1ecfc660b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8119ce679511b0e626706e90412f3ac8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906944&auth_key=1760906944-0-0-8b8e23e7b3cf2c79373b0a3ad7bd15a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5edca961d139b48a48e22ef14e1fb16e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906951&auth_key=1760906951-0-0-5ecf947b71a5b3b80e98f85933bccb76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Nepali-Sign-Language-Characters-Recognition-Dataset-Development-and-Deep-Learning-Approaches"><a href="#Nepali-Sign-Language-Characters-Recognition-Dataset-Development-and-Deep-Learning-Approaches" class="headerlink" title="Nepali Sign Language Characters Recognition: Dataset Development and   Deep Learning Approaches"></a>Nepali Sign Language Characters Recognition: Dataset Development and   Deep Learning Approaches</h2><p><strong>Authors:Birat Poudel, Satyam Ghimire, Sijan Bhattarai, Saurav Bhandari, Suramya Sharma Dahal</strong></p>
<p>Sign languages serve as essential communication systems for individuals with hearing and speech impairments. However, digital linguistic dataset resources for underrepresented sign languages, such as Nepali Sign Language (NSL), remain scarce. This study introduces the first benchmark dataset for NSL, consisting of 36 gesture classes with 1,500 samples per class, designed to capture the structural and visual features of the language. To evaluate recognition performance, we fine-tuned MobileNetV2 and ResNet50 architectures on the dataset, achieving classification accuracies of 90.45% and 88.78%, respectively. These findings demonstrate the effectiveness of convolutional neural networks in sign recognition tasks, particularly within low-resource settings. To the best of our knowledge, this work represents the first systematic effort to construct a benchmark dataset and assess deep learning approaches for NSL recognition, highlighting the potential of transfer learning and fine-tuning for advancing research in underexplored sign languages. </p>
<blockquote>
<p>æ‰‹è¯­å¯¹äºå¬åŠ›å’Œè¯­è¨€éšœç¢äººå£«æ¥è¯´æ˜¯ä¸€ç§é‡è¦çš„æ²Ÿé€šç³»ç»Ÿã€‚ç„¶è€Œï¼Œå¯¹äºä»£è¡¨æ€§ä¸è¶³çš„å°¼æ³Šå°”æ‰‹è¯­ï¼ˆNSLï¼‰ç­‰æ‰‹è¯­çš„æ•°å­—è¯­è¨€æ•°æ®é›†èµ„æºä»ç„¶éå¸¸ç¨€ç¼ºã€‚æœ¬ç ”ç©¶ä»‹ç»äº†é¦–ä¸ªå°¼æ³Šå°”æ‰‹è¯­åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«36ä¸ªæ‰‹åŠ¿ç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«æœ‰1500ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨æ•æ‰è¯¥è¯­è¨€çš„ç»“æ„å’Œè§†è§‰ç‰¹å¾ã€‚ä¸ºäº†è¯„ä¼°è¯†åˆ«æ€§èƒ½ï¼Œæˆ‘ä»¬å¯¹MobileNetV2å’ŒResNet50æ¶æ„è¿›è¡Œäº†å¾®è°ƒï¼Œåˆ†åˆ«å–å¾—äº†90.45%å’Œ88.78%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™äº›å‘ç°è¯æ˜äº†å·ç§¯ç¥ç»ç½‘ç»œåœ¨æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºç¨€ç¼ºçš„ç¯å¢ƒä¸­ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œä»£è¡¨äº†æ„å»ºåŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æ·±åº¦å­¦ä¹ åœ¨æ‰‹åŠ¿è¯†åˆ«æ–¹é¢çš„é¦–æ¬¡ç³»ç»Ÿæ€§åŠªåŠ›ï¼Œçªæ˜¾äº†è¿ç§»å­¦ä¹ å’Œå¾®è°ƒåœ¨æ¨è¿›å¯¹æœªè¢«å……åˆ†ç ”ç©¶çš„æ‰‹è¯­çš„æ¢ç´¢æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11243v1">PDF</a> 6 pages, 9 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†å°¼æ³Šå°”æ‰‹è¯­ï¼ˆNSLï¼‰çš„é¦–ä¸ªåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«36ä¸ªæ‰‹åŠ¿ç±»åˆ«ï¼Œæ¯ç±»1500ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨æ•æ‰è¯¥è¯­è¨€çš„ç»“æ„å’Œè§†è§‰ç‰¹å¾ã€‚ç ”ç©¶é€šè¿‡MobileNetV2å’ŒResNet50æ¶æ„å¯¹è¯¥æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œåˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«ä¸º90.45%å’Œ88.78%ï¼Œè¡¨æ˜å·ç§¯ç¥ç»ç½‘ç»œåœ¨æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„ç¯å¢ƒä¸‹ã€‚æ­¤ç ”ç©¶æ˜¯æ„å»ºNSLè¯†åˆ«åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æ·±åº¦å­¦ä¹ æ–¹æ³•çš„é¦–ä¸ªç³»ç»Ÿæ€§å°è¯•ï¼Œçªæ˜¾äº†è¿ç§»å­¦ä¹ å’Œå¾®è°ƒåœ¨æ¨è¿›å¯¹æœªè¢«å……åˆ†ç ”ç©¶çš„æ‰‹è¯­ç ”ç©¶æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶å»ºç«‹äº†å°¼æ³Šå°”æ‰‹è¯­ï¼ˆNSLï¼‰çš„é¦–ä¸ªåŸºå‡†æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«36ä¸ªæ‰‹åŠ¿ç±»åˆ«ï¼Œæ¯ç±»æœ‰1500ä¸ªæ ·æœ¬ã€‚</li>
<li>é€šè¿‡MobileNetV2å’ŒResNet50æ¶æ„å¯¹æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè·å¾—äº†è¾ƒé«˜çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜å·ç§¯ç¥ç»ç½‘ç»œåœ¨æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ­¤ç ”ç©¶å°¤å…¶åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸‹å…·æœ‰æ˜¾è‘—æ„ä¹‰ã€‚</li>
<li>è¯¥ç ”ç©¶æ˜¯ç³»ç»Ÿæ€§åœ°æ„å»ºæ‰‹è¯­è¯†åˆ«åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æ·±åº¦å­¦ä¹ æ–¹æ³•çš„é¦–ä¸ªå°è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-12620c8c2ffaebceb9a5130a00c9b435~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906959&auth_key=1760906959-0-0-c3f594b3fb4166b670f5867b8892756a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ac5f1e4890a57ab12624ea30c405ff9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906966&auth_key=1760906966-0-0-97b8c2c94b3477f9b1524d7ad053d402&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ca65dad2b35afdc828c351457129c31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906973&auth_key=1760906973-0-0-bde2696cf305771ae0e99a59e4d52963&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c75c6ef05e68efaa2ee70c21727caa1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906979&auth_key=1760906979-0-0-ad38efc8e010f4088b68e04ee3b1adba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d36b26d94846e9190d07eff1571f9721~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906986&auth_key=1760906986-0-0-152875a58612b4780aca951b3e47858f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45d89827d19247d2e8748a0b661706de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760906993&auth_key=1760906993-0-0-a94360cbae7151114b8c1924b6b93355&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4d1fba0483cb1c9764112bd322a37e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907000&auth_key=1760907000-0-0-1e71a928e811d1b14fff8e612863b1e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Efficient-Edge-Test-Time-Adaptation-via-Latent-Feature-Coordinate-Correction"><a href="#Efficient-Edge-Test-Time-Adaptation-via-Latent-Feature-Coordinate-Correction" class="headerlink" title="Efficient Edge Test-Time Adaptation via Latent Feature Coordinate   Correction"></a>Efficient Edge Test-Time Adaptation via Latent Feature Coordinate   Correction</h2><p><strong>Authors:Xinyu Luo, Jie Liu, Kecheng Chen, Junyi Yang, Bo Ding, Arindam Basu, Haoliang Li</strong></p>
<p>Edge devices face significant challenges due to limited computational resources and distribution shifts, making efficient and adaptable machine learning essential. Existing test-time adaptation (TTA) methods often rely on gradient-based optimization or batch processing, which are inherently unsuitable for resource-constrained edge scenarios due to their reliance on backpropagation and high computational demands. Gradient-free alternatives address these issues but often suffer from limited learning capacity, lack flexibility, or impose architectural constraints. To overcome these limitations, we propose a novel single-instance TTA method tailored for edge devices (TED), which employs forward-only coordinate optimization in the principal subspace of latent using the covariance matrix adaptation evolution strategy (CMA-ES). By updating a compact low-dimensional vector, TED not only enhances output confidence but also aligns the latent representation closer to the source latent distribution within the latent principal subspace. This is achieved without backpropagation, keeping the model parameters frozen, and enabling efficient, forgetting-free adaptation with minimal memory and computational overhead. Experiments on image classification and keyword spotting tasks across the ImageNet and Google Speech Commands series datasets demonstrate that TED achieves state-of-the-art performance while $\textit{reducing computational complexity by up to 63 times}$, offering a practical and scalable solution for real-world edge applications. Furthermore, we successfully $\textit{deployed TED on the ZYNQ-7020 platform}$, demonstrating its feasibility and effectiveness for resource-constrained edge devices in real-world deployments. </p>
<blockquote>
<p>è¾¹ç¼˜è®¾å¤‡ç”±äºè®¡ç®—èµ„æºæœ‰é™å’Œåˆ†å¸ƒè½¬ç§»è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä½¿å¾—é«˜æ•ˆå’Œå¯é€‚åº”çš„æœºå™¨å­¦ä¹ æ–¹æ³•å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æˆ–æ‰¹é‡å¤„ç†ï¼Œç”±äºå…¶ä¾èµ–åå‘ä¼ æ’­å’Œè®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œå› æ­¤åœ¨èµ„æºå—é™çš„è¾¹ç¼˜åœºæ™¯ä¸­å¹¶ä¸é€‚åˆã€‚æ— æ¢¯åº¦æ›¿ä»£æ–¹æ¡ˆè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½†å¾€å¾€å­˜åœ¨å­¦ä¹ å®¹é‡æœ‰é™ã€ç¼ºä¹çµæ´»æ€§æˆ–æ–½åŠ æ¶æ„çº¦æŸç­‰ç¼ºç‚¹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡çš„æ–°å‹å•å®ä¾‹TTAæ–¹æ³•ï¼ˆTEDï¼‰ï¼Œå®ƒé‡‡ç”¨ä»…å‰å‘ä¼ æ’­çš„åæ ‡ä¼˜åŒ–ï¼Œåœ¨æ½œåœ¨çš„ä¸»å­ç©ºé—´ä¸­ä½¿ç”¨åæ–¹å·®çŸ©é˜µé€‚åº”è¿›åŒ–ç­–ç•¥ï¼ˆCMA-ESï¼‰ã€‚é€šè¿‡æ›´æ–°ç´§å‡‘çš„ä½ç»´å‘é‡ï¼ŒTEDä¸ä»…æé«˜äº†è¾“å‡ºä¿¡å¿ƒï¼Œè€Œä¸”ä½¿æ½œåœ¨è¡¨ç¤ºæ›´æ¥è¿‘æºæ½œåœ¨åˆ†å¸ƒåœ¨æ½œåœ¨çš„ä¸»å­ç©ºé—´å†…ã€‚è¿™æ˜¯åœ¨ä¸è¿›è¡Œåå‘ä¼ æ’­çš„æƒ…å†µä¸‹å®ç°çš„ï¼Œä¿æŒæ¨¡å‹å‚æ•°å†»ç»“ï¼Œå¹¶å®ç°äº†é«˜æ•ˆã€æ— é—å¿˜çš„é€‚åº”ï¼Œå…·æœ‰æä½çš„å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚åœ¨ImageNetå’ŒGoogleè¯­éŸ³å‘½ä»¤ç³»åˆ—æ•°æ®é›†ä¸Šè¿›è¡Œå›¾åƒåˆ†ç±»å’Œå…³é”®è¯è¯†åˆ«ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼ŒTEDåœ¨å‡å°‘é«˜è¾¾63å€çš„è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸ºç°å®ä¸–ç•Œçš„è¾¹ç¼˜åº”ç”¨æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ZYNQ-7020å¹³å°ä¸ŠæˆåŠŸéƒ¨ç½²äº†TEDï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡çš„å®é™…éƒ¨ç½²ä¸­çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11068v1">PDF</a> Under review</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡èµ„æºå—é™å’Œåˆ†å¸ƒå˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„å•å®ä¾‹æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ï¼ˆTEDï¼‰ï¼Œé‡‡ç”¨å‰å‘åæ ‡ä¼˜åŒ–ï¼Œåœ¨æ½œåœ¨ä¸»æˆåˆ†å­ç©ºé—´ä¸­ä½¿ç”¨åæ–¹å·®çŸ©é˜µè‡ªé€‚åº”è¿›åŒ–ç­–ç•¥ï¼ˆCMA-ESï¼‰ã€‚é€šè¿‡æ›´æ–°ç´§å‡‘çš„ä½ç»´å‘é‡ï¼ŒTEDåœ¨ä¸è¿›è¡Œåå‘ä¼ æ’­ã€å†»ç»“æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†è¾“å‡ºä¿¡å¿ƒï¼Œå¹¶ä½¿æ½œåœ¨è¡¨ç¤ºæ›´æ¥è¿‘æºæ½œåœ¨åˆ†å¸ƒã€‚åœ¨ImageNetå’ŒGoogleè¯­éŸ³å‘½ä»¤ç³»åˆ—æ•°æ®é›†ä¸Šçš„å›¾åƒåˆ†ç±»å’Œå…³é”®è¯è¯†åˆ«ä»»åŠ¡å®éªŒè¡¨æ˜ï¼ŒTEDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾63å€çš„è®¡ç®—å¤æ‚åº¦ï¼Œä¸ºç°å®ä¸–ç•Œä¸­çš„è¾¹ç¼˜åº”ç”¨æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼ŒæˆåŠŸåœ¨ZYNQ-7020å¹³å°ä¸Šéƒ¨ç½²TEDï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¾¹ç¼˜è®¾å¤‡é¢ä¸´è®¡ç®—èµ„æºæœ‰é™å’Œåˆ†å¸ƒå˜åŒ–çš„æŒ‘æˆ˜ï¼Œéœ€è¦é«˜æ•ˆä¸”å¯é€‚åº”çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ä¾èµ–äºæ¢¯åº¦ä¼˜åŒ–æˆ–æ‰¹é‡å¤„ç†ï¼Œä¸é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜åœºæ™¯ã€‚</li>
<li>æ¢¯åº¦æ›¿ä»£æ–¹æ¡ˆè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½†å­˜åœ¨å­¦ä¹ å®¹é‡æœ‰é™ã€ç¼ºä¹çµæ´»æ€§æˆ–æ–½åŠ æ¶æ„çº¦æŸçš„ç¼ºç‚¹ã€‚</li>
<li>æå‡ºçš„TEDæ–¹æ³•æ˜¯ä¸€ç§æ–°å‹å•å®ä¾‹TTAæ–¹æ³•ï¼Œé‡‡ç”¨å‰å‘åæ ‡ä¼˜åŒ–ï¼Œæ›´æ–°ä½ç»´å‘é‡ä»¥æé«˜è¾“å‡ºä¿¡å¿ƒå’Œæ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>TEDä¸ä½¿ç”¨åå‘ä¼ æ’­ï¼Œå†»ç»“æ¨¡å‹å‚æ•°ï¼Œå®ç°é«˜æ•ˆã€æ— é—å¿˜çš„é€‚åº”ï¼Œå…·æœ‰æœ€å°çš„å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTEDåœ¨å›¾åƒåˆ†ç±»å’Œå…³é”®è¯è¯†åˆ«ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¤æ‚åº¦ã€‚</li>
<li>æˆåŠŸåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²TEDï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e8adff4516367a2600de98a8102de0e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907008&auth_key=1760907008-0-0-e5e98374bb592bc38d4c25f72554e6cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-435113998017453044da5cc73111bd88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907015&auth_key=1760907015-0-0-8bea1ccc29fc6d29567b4d36546f1e9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da5c41e152cbd52d8768df8337247e8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907022&auth_key=1760907022-0-0-7d5eb3579b585b7bc4214bc55729ab2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d005bd2aeb4a7400961e91b8f80e1f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907072&auth_key=1760907072-0-0-777668720db77f36607f92fdf2351241&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-A-Reproducibility-Study"><a href="#End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-A-Reproducibility-Study" class="headerlink" title="End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A   Reproducibility Study"></a>End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A   Reproducibility Study</h2><p><strong>Authors:Anirudh Ganesh, Jayavardhan Reddy</strong></p>
<p>We present a reproducibility study of the state-of-the-art neural architecture for sequence labeling proposed by Ma and Hovy (2016)\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines character-level representations via Convolutional Neural Networks (CNNs), word-level context modeling through Bi-directional Long Short-Term Memory networks (BiLSTMs), and structured prediction using Conditional Random Fields (CRFs). This end-to-end approach eliminates the need for hand-crafted features while achieving excellent performance on named entity recognition (NER) and part-of-speech (POS) tagging tasks. Our implementation successfully reproduces the key results, achieving 91.18% F1-score on CoNLL-2003 NER and demonstrating the modelâ€™s effectiveness across sequence labeling tasks. We provide a detailed analysis of the architecture components and release an open-source PyTorch implementation to facilitate further research. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹Maå’ŒHovyï¼ˆ2016ï¼‰æå‡ºçš„æœ€æ–°ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œäº†å¯é‡å¤æ€§ç ”ç©¶\cite{ma2016end}ã€‚åŸå§‹çš„BiLSTM-CNN-CRFæ¨¡å‹é€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç»“åˆå­—ç¬¦çº§è¡¨ç¤ºï¼Œé€šè¿‡åŒå‘é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆBiLSTMï¼‰è¿›è¡Œå•è¯çº§ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨æ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰è¿›è¡Œç»“æ„åŒ–é¢„æµ‹ã€‚è¿™ç§ç«¯åˆ°ç«¯çš„æ–¹æ³•ä¸éœ€è¦æ‰‹å·¥ç‰¹å¾ï¼ŒåŒæ—¶åœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œè¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°æˆåŠŸåœ°å†ç°äº†å…³é”®ç»“æœï¼Œåœ¨CoNLL-2003 NERä¸Šè¾¾åˆ°äº†91.18%çš„F1åˆ†æ•°ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨åºåˆ—æ ‡æ³¨ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æä¾›äº†æ¶æ„ç»„ä»¶çš„è¯¦ç»†åˆ†æï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªå¼€æºçš„PyTorchå®ç°ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10936v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹Maå’ŒHovyï¼ˆ2016ï¼‰æå‡ºçš„æœ€æ–°ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œå¯é‡å¤æ€§ç ”ç©¶ã€‚åŸBiLSTM-CNN-CRFæ¨¡å‹é€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå­—ç¬¦çº§è¡¨ç¤ºï¼Œé€šè¿‡åŒå‘é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆBiLSTMï¼‰è¿›è¡Œå•è¯çº§ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨æ¡ä»¶éšæœºå­—æ®µï¼ˆCRFï¼‰è¿›è¡Œç»“æ„åŒ–é¢„æµ‹ã€‚è¿™ç§ç«¯åˆ°ç«¯çš„æ–¹æ³•æ— éœ€æ‰‹å·¥ç‰¹å¾ï¼Œåœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œè¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬çš„å®ç°æˆåŠŸå¤ç°äº†å…³é”®ç»“æœï¼Œåœ¨CoNLL-2003 NERä»»åŠ¡ä¸Šå®ç°äº†91.18%çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨åºåˆ—æ ‡æ³¨ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æä¾›äº†å¯¹è¯¥æ¶æ„ç»„ä»¶çš„è¯¦ç»†åˆ†æï¼Œå¹¶å‘å¸ƒäº†å¼€æºPyTorchå®ç°ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å¯¹Maå’ŒHovyï¼ˆ2016ï¼‰æå‡ºçš„å…ˆè¿›ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œäº†å¯é‡å¤æ€§éªŒè¯ã€‚</li>
<li>BiLSTM-CNN-CRFæ¨¡å‹ç»“åˆäº†å­—ç¬¦çº§è¡¨ç¤ºã€å•è¯çº§ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œç»“æ„åŒ–é¢„æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œæ— éœ€æ‰‹å·¥ç‰¹å¾ã€‚</li>
<li>åœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œè¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æˆåŠŸå¤ç°äº†å…³é”®ç»“æœï¼Œå¹¶åœ¨CoNLL-2003 NERä»»åŠ¡ä¸Šè¾¾åˆ°äº†91.18%çš„F1åˆ†æ•°ã€‚</li>
<li>æä¾›äº†å¯¹è¯¥ç¥ç»ç½‘ç»œæ¶æ„çš„è¯¦ç»†åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0413cbdba2d990a08dbdc092d99d2ed7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907079&auth_key=1760907079-0-0-9d471849788186dbcea410930e566d19&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfa9c4a4d9e5acb78f11b70dd2044b50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907086&auth_key=1760907086-0-0-fc58e835f71c44b581dd384228904214&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LSZone-A-Lightweight-Spatial-Information-Modeling-Architecture-for-Real-time-In-car-Multi-zone-Speech-Separation"><a href="#LSZone-A-Lightweight-Spatial-Information-Modeling-Architecture-for-Real-time-In-car-Multi-zone-Speech-Separation" class="headerlink" title="LSZone: A Lightweight Spatial Information Modeling Architecture for   Real-time In-car Multi-zone Speech Separation"></a>LSZone: A Lightweight Spatial Information Modeling Architecture for   Real-time In-car Multi-zone Speech Separation</h2><p><strong>Authors:Jun Chen, Shichao Hu, Jiuxin Lin, Wenjie Li, Zihan Zhang, Xingchen Li, JinJiang Liu, Longshuai Xiao, Chao Weng, Lei Xie, Zhiyong Wu</strong></p>
<p>In-car multi-zone speech separation, which captures voices from different speech zones, plays a crucial role in human-vehicle interaction. Although previous SpatialNet has achieved notable results, its high computational cost still hinders real-time applications in vehicles. To this end, this paper proposes LSZone, a lightweight spatial information modeling architecture for real-time in-car multi-zone speech separation. We design a spatial information extraction-compression (SpaIEC) module that combines Mel spectrogram and Interaural Phase Difference (IPD) to reduce computational burden while maintaining performance. Additionally, to efficiently model spatial information, we introduce an extremely lightweight Conv-GRU crossband-narrowband processing (CNP) module. Experimental results demonstrate that LSZone, with a complexity of 0.56G MACs and a real-time factor (RTF) of 0.37, delivers impressive performance in complex noise and multi-speaker scenarios. </p>
<blockquote>
<p>è½¦å†…å¤šåŒºåŸŸè¯­éŸ³åˆ†ç¦»æŠ€æœ¯æ•æ‰æ¥è‡ªä¸åŒè¯­éŸ³åŒºåŸŸçš„å£°éŸ³ï¼Œåœ¨äººæœºäº¤äº’ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚å°½ç®¡ä¹‹å‰çš„SpatialNetå·²ç»å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å…¶è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ä»ç„¶é˜»ç¢äº†å…¶åœ¨è½¦è¾†ä¸­çš„å®æ—¶åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†LSZoneï¼Œä¸€ç§ç”¨äºå®æ—¶è½¦å†…å¤šåŒºåŸŸè¯­éŸ³åˆ†ç¦»è½»é‡çº§ç©ºé—´ä¿¡æ¯å»ºæ¨¡æ¶æ„ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç©ºé—´ä¿¡æ¯æå–-å‹ç¼©ï¼ˆSpaIECï¼‰æ¨¡å—ï¼Œç»“åˆæ¢…å°”é¢‘è°±å›¾å’Œè€³é—´ç›¸ä½å·®ï¼ˆIPDï¼‰ï¼Œä»¥å‡å°‘è®¡ç®—è´Ÿæ‹…çš„åŒæ—¶ä¿æŒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆåœ°å¯¹ç©ºé—´ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæè½»é‡çº§çš„Conv-GRUè·¨é¢‘å¸¦çª„é¢‘å¸¦å¤„ç†ï¼ˆCNPï¼‰æ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSZoneçš„è®¡ç®—å¤æ‚åº¦ä¸º0.56GMACsï¼Œå®æ—¶å› å­ï¼ˆRTFï¼‰ä¸º0.37ï¼Œåœ¨å¤æ‚å™ªå£°å’Œå¤šè¯´è¯äººåœºæ™¯ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10687v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLSZoneçš„è½»é‡çº§ç©ºé—´ä¿¡æ¯å»ºæ¨¡æ¶æ„ï¼Œç”¨äºå®æ—¶è½¦å†…å¤šåŒºåŸŸè¯­éŸ³åˆ†ç¦»ã€‚è¯¥æ¶æ„é€šè¿‡ç»“åˆæ¢…å°”é¢‘è°±å›¾å’Œè·¨è€³ç›¸ä½å·®è®¾è®¡äº†ä¸€ä¸ªç©ºé—´ä¿¡æ¯æå–å‹ç¼©æ¨¡å—ï¼Œä»¥å‡å°‘è®¡ç®—è´Ÿæ‹…åŒæ—¶ä¿æŒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆåœ°å»ºæ¨¡ç©ºé—´ä¿¡æ¯ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªè¶…è½»é‡çº§çš„Conv-GRUè·¨é¢‘å¸¦å¤„ç†æ¨¡å—ã€‚LSZoneåœ¨å¤æ‚å™ªå£°å’Œå¤šè¯´è¯äººåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®¡ç®—å¤æ‚åº¦ä¸º0.56G MACsï¼Œå®æ—¶å› å­ä¸º0.37ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LSZoneæ˜¯ä¸€ä¸ªè½»é‡çº§çš„ç©ºé—´ä¿¡æ¯å»ºæ¨¡æ¶æ„ï¼Œç”¨äºå®æ—¶è½¦å†…å¤šåŒºåŸŸè¯­éŸ³åˆ†ç¦»ã€‚</li>
<li>é€šè¿‡ç»“åˆæ¢…å°”é¢‘è°±å›¾å’Œè·¨è€³ç›¸ä½å·®è®¾è®¡äº†ä¸€ä¸ªç©ºé—´ä¿¡æ¯æå–å‹ç¼©æ¨¡å—ï¼Œå‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªè¶…è½»é‡çº§çš„Conv-GRUè·¨é¢‘å¸¦å¤„ç†æ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°å»ºæ¨¡ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>LSZoneåœ¨å¤æ‚å™ªå£°å’Œå¤šè¯´è¯äººåœºæ™¯ä¸­å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
<li>LSZoneçš„è®¡ç®—å¤æ‚åº¦ä¸º0.56G MACsï¼Œå®æ—¶å› å­ä¸º0.37ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚</li>
<li>SpatialNetè™½ç„¶æœ‰ä¸€å®šçš„æˆæœï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬é˜»ç¢äº†å®æ—¶åº”ç”¨ï¼Œè€ŒLSZoneè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-be8ca1d99b5fa385606aebbc5e2b21c7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907094&auth_key=1760907094-0-0-9df04f5b50eb9d532753dd162f2aaf5c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dec59932c1cc50b1fe4946404a7cd58c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907101&auth_key=1760907101-0-0-bcf0b4a7358dc15bdb1a43cc98946606&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e226f77d9742acb0aecec336adc70b98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907108&auth_key=1760907108-0-0-b428de55249598e2728a407fda283323&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2cd76b8c87ddbc1c6a7736ebee494719~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907115&auth_key=1760907115-0-0-b91ae4e43beef132517f9743e543a4d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-221339e4fe9daa95e0bac391bcfb220f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760907122&auth_key=1760907122-0-0-9bb8e6efe3c59854ae785ac5a32aedd0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  PIA Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic   Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-7af5e3021c1a455a2f91f444bd570b6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827947&auth_key=1760827947-0-0-314d7864236d0929dbcafad5e0dfddbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Evaluating the Explainability of Vision Transformers in Medical Imaging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
