<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  RLAIF-SPA Optimizing LLM-based Emotional Speech Synthesis via RLAIF">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-df7301cfa07ebc24146ffb38a5a4a919')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="RLAIF-SPA-Optimizing-LLM-based-Emotional-Speech-Synthesis-via-RLAIF"><a href="#RLAIF-SPA-Optimizing-LLM-based-Emotional-Speech-Synthesis-via-RLAIF" class="headerlink" title="RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF"></a>RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF</h2><p><strong>Authors:Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao</strong></p>
<p>Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³åˆæˆåœ¨ä¸­æ€§è¯­éŸ³æ–¹é¢å·²æ¥è¿‘äººç±»æ°´å¹³ï¼Œä½†åœ¨æƒ…æ„Ÿè¡¨è¾¾ä¸Šä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„æƒ…æ„Ÿæ ‡æ³¨ï¼Œæˆ–ä¼˜åŒ–æœªèƒ½æ•æ‰åˆ°è¯­éŸ³æƒ…æ„Ÿè¡¨è¾¾å’Œæƒ…æ„Ÿæ„ŸçŸ¥è‡ªç„¶æ€§çš„é—´æ¥ç›®æ ‡ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¯­éŸ³å‡†ç¡®ä½†æƒ…æ„Ÿå¹³æ·¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RLAIF-SPAæ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æœºåˆ¶ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯åˆ†åˆ«åˆ¤æ–­è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿæ ‡ç­¾å¯¹é½ï¼Œä½œä¸ºæƒ…æ„Ÿè¡¨è¾¾å’Œå¯ç†è§£æ€§ä¼˜åŒ–çš„ç›´æ¥å¥–åŠ±ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ©ç”¨éŸµå¾‹æ ‡ç­¾å¯¹é½ï¼Œé€šè¿‡åŒæ—¶è€ƒè™‘è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿå¯¹é½ï¼Œæ²¿ç€ç»“æ„ã€æƒ…æ„Ÿã€é€Ÿåº¦å’ŒéŸ³è°ƒå››ä¸ªç²¾ç»†ç²’åº¦æ–¹å‘ï¼Œæé«˜è¡¨è¾¾è´¨é‡ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ç»“åˆäº†è¯­ä¹‰å‡†ç¡®æ€§åé¦ˆï¼Œä»¥ç¡®ä¿æ¸…æ™°å‡†ç¡®çš„è¯­éŸ³ç”Ÿæˆã€‚åœ¨Libri Speechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRLAIF-SPAä¼˜äºChat-TTSï¼Œç›¸å¯¹å‡å°‘äº†26.1%çš„WERï¼ˆè¯é”™è¯¯ç‡ï¼‰ï¼ŒSIM-Oå¢åŠ äº†9.1%ï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ä¸­è·å¾—äº†è¶…è¿‡10%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14628v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æƒ…æ„Ÿè¡¨è¾¾æ˜¯æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å®ç°è¿‘äººç±»è´¨é‡çš„ä¸­æ€§è¯­éŸ³æ—¶ã€‚ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–æ˜‚è´µçš„æƒ…æ„Ÿæ ‡æ³¨æˆ–ä¼˜åŒ–é—´æ¥ç›®æ ‡ï¼Œéš¾ä»¥æ•æ‰è¯­éŸ³çš„æƒ…æ„Ÿè¡¨è¾¾åŠ›å’Œæ„ŸçŸ¥è‡ªç„¶æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„è¯­éŸ³å‡†ç¡®ä½†æƒ…æ„Ÿå¹³æ·¡ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RLAIF-SPAæ¡†æ¶ï¼Œé‡‡ç”¨äººå·¥æ™ºèƒ½åé¦ˆå¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼Œåˆ©ç”¨è¯­éŸ³è¯†åˆ«å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯åˆ†åˆ«åˆ¤æ–­è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿæ ‡ç­¾å¯¹é½ï¼Œä½œä¸ºæƒ…æ„Ÿè¡¨è¾¾åŠ›å’Œå¯ç†è§£æ€§ä¼˜åŒ–çš„ç›´æ¥å¥–åŠ±ã€‚è¯¥æ¡†æ¶é€šè¿‡éŸµå¾‹æ ‡ç­¾å¯¹é½ï¼Œè”åˆè€ƒè™‘è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿå¯¹é½å››ä¸ªç²¾ç»†ç»´åº¦ï¼šç»“æ„ã€æƒ…æ„Ÿã€é€Ÿåº¦å’Œè¯­è°ƒï¼Œæé«˜è¡¨è¾¾è´¨é‡ã€‚åŒæ—¶ï¼Œç»“åˆè¯­ä¹‰å‡†ç¡®æ€§åé¦ˆï¼Œç¡®ä¿ç”Ÿæˆæ¸…æ™°å‡†ç¡®çš„è¯­éŸ³ã€‚åœ¨Libri Speechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRLAIF-SPAåœ¨è¯­éŸ³è½¬æ–‡æœ¬æ–¹é¢ä¼˜äºChat-TTSç³»ç»Ÿï¼Œé™ä½äº†å­—é”™è¯¯ç‡ï¼ˆWERï¼‰é«˜è¾¾26.1%ï¼ŒSIM-Oå¢åŠ äº†9.1%ï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ä¸­å–å¾—äº†è¶…è¿‡10%çš„æ”¹è¿›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æƒ…æ„Ÿè¡¨è¾¾åœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°½ç®¡ä¸­æ€§è¯­éŸ³å·²æ¥è¿‘äººç±»è´¨é‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç»å¸¸å› ä¸ºä¾èµ–æ˜‚è´µçš„æƒ…æ„Ÿæ ‡æ³¨æˆ–ä¼˜åŒ–é—´æ¥ç›®æ ‡ï¼Œæ— æ³•å……åˆ†æ•æ‰è¯­éŸ³çš„æƒ…æ„Ÿè¡¨è¾¾åŠ›å’Œæ„ŸçŸ¥è‡ªç„¶æ€§ã€‚</li>
<li>RLAIF-SPAæ¡†æ¶é€šè¿‡ç»“åˆäººå·¥æ™ºèƒ½åé¦ˆå¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼Œæé«˜äº†æƒ…æ„Ÿè¡¨è¾¾åŠ›å’Œè¯­éŸ³çš„è‡ªç„¶æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨è¯­éŸ³è¯†åˆ«å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œåˆ†åˆ«åˆ¤æ–­è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿæ ‡ç­¾å¯¹é½ã€‚</li>
<li>RLAIF-SPAæ¡†æ¶é€šè¿‡è”åˆè€ƒè™‘è¯­ä¹‰å‡†ç¡®æ€§å’ŒéŸµå¾‹æƒ…æ„Ÿå¯¹é½çš„å››ä¸ªç»´åº¦ï¼Œæé«˜äº†è¯­éŸ³è¡¨è¾¾çš„ç²¾ç»†åº¦ã€‚</li>
<li>è¯­ä¹‰å‡†ç¡®æ€§åé¦ˆçš„å¼•å…¥ç¡®ä¿äº†ç”Ÿæˆçš„è¯­éŸ³æ¸…æ™°å‡†ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd3d94e775c86d09973da7ed7182d47a" align="middle">
<img src="https://picx.zhimg.com/v2-de5622c0e7e1fed81861644404a38f47" align="middle">
<img src="https://picx.zhimg.com/v2-c1799d390763cdc6326b6d3a34c26859" align="middle">
<img src="https://picx.zhimg.com/v2-36108540ac8cd7c87bb282886eea4e19" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Optimal-Aggregation-of-LLM-and-PRM-Signals-for-Efficient-Test-Time-Scaling"><a href="#Optimal-Aggregation-of-LLM-and-PRM-Signals-for-Efficient-Test-Time-Scaling" class="headerlink" title="Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time   Scaling"></a>Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time   Scaling</h2><p><strong>Authors:Peng Kuang, Yanli Wang, Xiaoyu Han, Yaowenqi Liu, Kaidi Xu, Haohan Wang</strong></p>
<p>Process reward models (PRMs) are a cornerstone of test-time scaling (TTS), designed to verify and select the best responses from large language models (LLMs). However, this promise is challenged by recent benchmarks where simple majority voting, which ignores PRM signals, occasionally outperforms standard PRM-based selection. This raises a critical question: How can we effectively utilize verification signals from PRMs for TTS? To address this, we start by developing a theoretical framework for optimally combining signals from both the LLM and the PRM. Our framework reveals that the optimal strategy is a weighted aggregation of responses, a strategy whose effectiveness hinges on estimating weights that capture the complex interplay between the models. Based on our theoretical results, we empirically show that these optimal weighting functions differ significantly across LLM-PRM pairs and, notably, often assign substantial negative weights. Motivated by these insights, we propose efficient pre-computation methods to calibrate these weighting functions. Extensive experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method significantly boosts the TTS efficiency, surpassing the performance of vanilla weighted majority voting while using only $21.3%$ of the computation. Ultimately, our work demonstrates that investing in a more intelligent aggregation strategy can be a more convincing path to performance gains than simply scaling test-time computation. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ˜¯æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰çš„æ ¸å¿ƒï¼Œæ—¨åœ¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­éªŒè¯å’Œé€‰æ‹©æœ€ä½³å“åº”ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå¿½ç•¥PRMä¿¡å·çš„ç®€å•å¤šæ•°æŠ•ç¥¨æ³•å¶å°”ä¼šä¼˜äºåŸºäºPRMçš„æ ‡å‡†é€‰æ‹©æ–¹æ³•ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨PRMä¸­çš„éªŒè¯ä¿¡å·æ¥è¿›è¡ŒTTSï¼Ÿä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç”¨äºæœ€ä¼˜åœ°ç»“åˆLLMå’ŒPRMçš„ä¿¡å·ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¡¨æ˜ï¼Œæœ€ä½³ç­–ç•¥æ˜¯å“åº”çš„åŠ æƒèšåˆï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºä¼°è®¡èƒ½å¤Ÿæ•æ‰æ¨¡å‹ä¹‹é—´å¤æ‚ç›¸äº’ä½œç”¨çš„æƒé‡ã€‚åŸºäºæˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œæˆ‘ä»¬å®è¯è¡¨æ˜ï¼Œè¿™äº›æœ€ä½³åŠ æƒå‡½æ•°åœ¨ä¸åŒLLM-PRMå¯¹ä¹‹é—´å·®å¼‚å¾ˆå¤§ï¼Œå¹¶ä¸”é€šå¸¸ä¼šåˆ†é…ç›¸å½“å¤§çš„è´Ÿæƒé‡ã€‚å—è¿™äº›è§è§£çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æœ‰æ•ˆçš„é¢„è®¡ç®—æ–¹æ³•ä»¥æ ¡å‡†è¿™äº›åŠ æƒå‡½æ•°ã€‚åœ¨5ä¸ªLLMå’Œ7ä¸ªPRMä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ ¡å‡†æ–¹æ³•å¤§å¤§æé«˜äº†TTSçš„æ•ˆç‡ï¼Œåœ¨ä»…ä½¿ç”¨21.3%çš„è®¡ç®—é‡çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†ç®€å•åŠ æƒå¤šæ•°æŠ•ç¥¨çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼ŒæŠ•èµ„äºæ›´æ™ºèƒ½çš„èšåˆç­–ç•¥å¯èƒ½æ˜¯å®ç°æ€§èƒ½æå‡çš„æ›´ä»¤äººä¿¡æœçš„é€”å¾„ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ‰©å¤§æµ‹è¯•æ—¶é—´çš„è®¡ç®—è§„æ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13918v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„åº”ç”¨ã€‚è™½ç„¶PRMæ˜¯TTSçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œç”¨äºéªŒè¯å’Œé€‰æ‹©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€ä½³å“åº”ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¿½ç•¥PRMä¿¡å·çš„ç®€å•å¤šæ•°æŠ•ç¥¨å¯èƒ½ä¼˜äºåŸºäºPRMçš„é€‰æ‹©æ–¹æ³•ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å»ºç«‹äº†ç†è®ºæ¡†æ¶ï¼Œä»¥æœ€ä¼˜æ–¹å¼ç»“åˆLLMå’ŒPRMçš„ä¿¡å·ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€ä¼˜ç­–ç•¥æ˜¯å¯¹å“åº”çš„åŠ æƒèšåˆï¼Œè¿™å–å†³äºå¦‚ä½•ä¼°ç®—æ•æ‰æ¨¡å‹é—´å¤æ‚äº¤äº’çš„æƒé‡ã€‚åŸºäºè¿™äº›ç†è®ºç»“æœï¼Œé€šè¿‡å®è¯ç ”ç©¶éªŒè¯äº†æœ€ä½³åŠ æƒå‡½æ•°åœ¨ä¸åŒLLM-PRMç»„åˆä¸­çš„å·®å¼‚æ˜¾è‘—ï¼Œå¹¶ä¸”é€šå¸¸ä¼šåˆ†é…è¾ƒå¤§çš„è´Ÿæƒé‡ã€‚é€šè¿‡é¢„å…ˆè®¡ç®—æ ¡å‡†è¿™äº›åŠ æƒå‡½æ•°çš„æ–¹æ³•ï¼Œå®è¯ç»“æœè¡¨æ˜è¯¥æ–¹æ³•å¤§å¤§æé«˜äº†TTSçš„æ•ˆç‡ï¼Œè¶…è¶Šäº†ç®€å•çš„åŠ æƒå¤šæ•°æŠ•ç¥¨æ–¹æ³•ï¼Œä¸”è®¡ç®—æˆæœ¬ä»…ä¸ºåŸæ¥çš„21.3%ã€‚è¿™è¡¨æ˜é‡‡ç”¨æ›´æ™ºèƒ½çš„èšåˆç­–ç•¥æ˜¯æé«˜æ€§èƒ½çš„æ›´å¯é é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œç”¨äºéªŒè¯å’Œé€‰æ‹©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€ä½³å“åº”ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶å‘ç°ç®€å•å¤šæ•°æŠ•ç¥¨åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºåŸºäºPRMçš„é€‰æ‹©æ–¹æ³•ã€‚</li>
<li>å»ºç«‹ç†è®ºæ¡†æ¶ä»¥æœ€ä¼˜æ–¹å¼ç»“åˆLLMå’ŒPRMçš„ä¿¡å·æ˜¯å…³é”®ã€‚</li>
<li>æœ€ä¼˜ç­–ç•¥æ˜¯å¯¹å“åº”è¿›è¡ŒåŠ æƒèšåˆï¼Œè€Œè¿™å–å†³äºå¦‚ä½•ä¼°ç®—æƒé‡ä»¥æ•æ‰æ¨¡å‹é—´çš„å¤æ‚äº¤äº’ã€‚</li>
<li>æœ€ä½³åŠ æƒå‡½æ•°åœ¨ä¸åŒLLM-PRMç»„åˆä¸­å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶å¯èƒ½åˆ†é…è¾ƒå¤§çš„è´Ÿæƒé‡ã€‚</li>
<li>é€šè¿‡é¢„å…ˆè®¡ç®—æ ¡å‡†åŠ æƒå‡½æ•°çš„æ–¹æ³•å¯ä»¥æé«˜TTSçš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a2c42264e9595806b58144bc1f99099" align="middle">
<img src="https://picx.zhimg.com/v2-172ddc22040d11f5da2ca708fd364d54" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner"><a href="#Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner" class="headerlink" title="Generative Universal Verifier as Multimodal Meta-Reasoner"></a>Generative Universal Verifier as Multimodal Meta-Reasoner</h2><p><strong>Authors:Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang</strong></p>
<p>We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ç”Ÿæˆå¼é€šç”¨éªŒè¯å™¨ï¼ˆGenerative Universal Verifierï¼‰è¿™ä¸€æ–°æ¦‚å¿µåŠæ’ä»¶ï¼Œå®ƒæ˜¯ä¸“ä¸ºä¸‹ä¸€ä»£è§†è§‰è¯­è¨€æ¨¡å‹å’Œç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„è·¨æ¨¡æ€æ¨ç†è€Œè®¾è®¡çš„ï¼Œä¸ºæ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„è§†è§‰ç»“æœæä¾›äº†åæ€å’Œæ”¹è¿›çš„åŸºæœ¬èƒ½åŠ›ã€‚è¿™ç¯‡è®ºæ–‡ä¸»è¦æœ‰ä¸‰ä¸ªè´¡çŒ®ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰æˆ‘ä»¬æ„å»ºäº†ViVerBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–16ç±»å…³é”®ä»»åŠ¡çš„å…¨é¢åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨ç†ä¸­çš„è§†è§‰ç»“æœã€‚ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸€ç›´ä¸ä½³ï¼Œè¿™çªæ˜¾å‡ºåœ¨å¯é è§†è§‰éªŒè¯æ–¹é¢ä¸äººç±»èƒ½åŠ›ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ã€‚</p>
<p>ï¼ˆ2ï¼‰æˆ‘ä»¬è®¾è®¡äº†ä¸¤æ¡è‡ªåŠ¨åŒ–ç®¡é“æ¥æ„å»ºå¤§è§„æ¨¡è§†è§‰éªŒè¯æ•°æ®ï¼Œå¹¶è®­ç»ƒäº†OmniVerifier-7Bï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºé€šç”¨è§†è§‰éªŒè¯çš„å…¨èƒ½ç”Ÿæˆå¼éªŒè¯å™¨ï¼Œåœ¨ViVerBenchä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼ˆ+8.3ï¼‰ã€‚é€šè¿‡è®­ç»ƒï¼Œæˆ‘ä»¬ç¡®å®šäº†è§†è§‰éªŒè¯ä¸­çš„ä¸‰ç§åŸºæœ¬èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬å¦‚ä½•ååŒæ¨å¹¿å’Œäº¤äº’ã€‚</p>
<p>ï¼ˆ3ï¼‰æˆ‘ä»¬æå‡ºäº†OmniVerifier-TTSï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é€šç”¨éªŒè¯å™¨åœ¨ç»Ÿä¸€æ¨¡å‹å†…å®ç°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„åºåˆ—æµ‹è¯•æ—¶é—´ç¼©æ”¾èŒƒå¼ï¼Œé€šè¿‡è¿­ä»£ç²¾ç»†ä¼˜åŒ–æé«˜ç”Ÿæˆèƒ½åŠ›ä¸Šé™ã€‚é™¤äº†ç”Ÿæˆï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å°†é€šç”¨éªŒè¯å™¨æ‰©å±•åˆ°æ›´å¹¿æ³›çš„ä¸–ç•Œå»ºæ¨¡äº¤å‰æ¨ç†åœºæ™¯ã€‚ç»éªŒä¸Šï¼ŒOmniVerifier-TTSåœ¨T2I-ReasonBenchï¼ˆæé«˜+3.7ï¼‰å’ŒGenEval++ï¼ˆæé«˜+4.3ï¼‰ä¸Šå–å¾—äº†æ”¹è¿›ï¼Œä¼˜äºç°æœ‰çš„å¹¶è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œå¦‚Best-of-Nã€‚é€šè¿‡ä¸ºè·¨æ¨¡æ€æ¨ç†èµ‹äºˆå¯é çš„è§†è§‰éªŒè¯ï¼ŒOmniVerifierä¸ä»…æ¨åŠ¨äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¯é åæ€ï¼Œè¿˜æ¨åŠ¨äº†å¯æ‰©å±•çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–ï¼Œæœç€æ›´å¯é ã€å¯æ§çš„ä¸‹ä¸€ä»£æ¨ç†ç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13804v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘ä¸‹ä¸€ä»£å¤šæ¨¡æ€æ¨ç†çš„ç”Ÿæˆå¼é€šç”¨éªŒè¯å™¨ï¼ˆGenerative Universal Verifierï¼‰è¿™ä¸€æ¦‚å¿µåŠæ’ä»¶è®¾è®¡ã€‚è¯¥è®¾è®¡ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹å’Œç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­ï¼Œä¸ºæ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„è§†è§‰ç»“æœæä¾›åæ€å’Œä¼˜åŒ–çš„åŸºæœ¬èƒ½åŠ›ã€‚æœ¬æ–‡æœ‰ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼š</p>
<p>ä¸€ã€æ„å»ºäº†ViVerBenchåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«ç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨ç†ä¸­è§†è§‰ç»“æœçš„16ç±»å…³é”®ä»»åŠ¡ã€‚ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå‡¸æ˜¾äº†ä¸äººç±»å¯é è§†è§‰éªŒè¯èƒ½åŠ›ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚</p>
<p>äºŒã€è®¾è®¡äº†è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºå¤§è§„æ¨¡è§†è§‰éªŒè¯æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†OmniVerifier-7Bé€šç”¨è§†è§‰éªŒè¯å™¨ï¼Œåœ¨ViVerBenchä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ˆ+8.3ï¼‰ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†è§†è§‰éªŒè¯ä¸­çš„ä¸‰ä¸ªåŸºæœ¬èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬å¦‚ä½•ååŒå·¥ä½œå¹¶ç›¸äº’åä½œã€‚</p>
<p>ä¸‰ã€æå‡ºäº†OmniVerifier-TTSåºåˆ—æµ‹è¯•æ—¶é—´ç¼©æ”¾èŒƒå¼ï¼Œåˆ©ç”¨é€šç”¨éªŒè¯å™¨åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å»ºç«‹å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä¹‹é—´çš„æ¡¥æ¢ï¼Œé€šè¿‡è¿­ä»£å¾®è°ƒä¼˜åŒ–å¢å¼ºç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å°†é€šç”¨éªŒè¯å™¨æ‰©å±•åˆ°æ›´å¹¿æ³›çš„ä¸–ç•Œå»ºæ¨¡æ¨ç†åœºæ™¯ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒOmniVerifier-TTSåœ¨T2I-ReasonBenchå’ŒGenEval++ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¹³è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚Best-of-Nï¼‰ã€‚é€šè¿‡èµ‹äºˆå¤šæ¨¡æ€æ¨ç†å¯é çš„è§†è§‰éªŒè¯èƒ½åŠ›ï¼ŒOmniVerifierå®ç°äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¯é åæ€å’Œå¯ä¼¸ç¼©çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–ï¼Œæ ‡å¿—ç€æ›´å€¼å¾—ä¿¡èµ–å’Œå¯æ§çš„ä¸‹ä¸€ä»£æ¨ç†ç³»ç»Ÿçš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€å¼•å…¥äº†ä¸€ç§æ–°çš„æ¦‚å¿µâ€”â€”â€œç”Ÿæˆå¼é€šç”¨éªŒè¯å™¨â€ï¼Œä¸“ä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿè®¾è®¡ï¼Œå¼ºåŒ–äº†ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹è§†è§‰ç»“æœçš„åæ€å’Œä¼˜åŒ–èƒ½åŠ›ã€‚å¯¹äºæ¨åŠ¨AIæŠ€æœ¯çš„å‘å±•æœ‰é‡è¦çš„è´¡çŒ®ã€‚<br>äºŒã€æ„å»ºäº†ä¸€ç§å…¨é¢çš„åŸºå‡†æµ‹è¯•é›†â€”â€”ViVerBenchï¼Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºå‡ºç°æœ‰æ¨¡å‹çš„ä¸è¶³ä¹‹å¤„ä»¥åŠæå‡ç©ºé—´ã€‚<br>ä¸‰ã€è®¾è®¡äº†è‡ªåŠ¨åŒ–ç®¡é“ç”¨äºå¤§è§„æ¨¡è§†è§‰éªŒè¯æ•°æ®çš„æ„å»ºå’Œè®­ç»ƒå‡ºOmniVerifier-7Bæ¨¡å‹ç”¨äºé€šç”¨è§†è§‰éªŒè¯ã€‚åœ¨æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½æå‡ã€‚<br>å››ã€æå‡ºäº†OmniVerifier-TTSåºåˆ—æµ‹è¯•æ—¶é—´ç¼©æ”¾èŒƒå¼ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å¹¶ä¼˜åŒ–äº†æ¨¡å‹çš„è¡¨ç°æ€§èƒ½ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•æœ‰æ›´ä¼˜è¶Šçš„è¡¨ç°ã€‚<br>äº”ã€æˆåŠŸå°†é€šç”¨éªŒè¯å™¨åº”ç”¨äºæ›´å¹¿æ³›çš„ä¸–ç•Œå»ºæ¨¡æ¨ç†åœºæ™¯ï¼ŒéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼å’Œæ½œåŠ›ã€‚<br>å…­ã€å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniVerifierç³»åˆ—æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58d4d2cc280728ecad0a9f8446897444" align="middle">
<img src="https://picx.zhimg.com/v2-e844630321f891a459a4e7f3094b5c34" align="middle">
<img src="https://picx.zhimg.com/v2-c33d2868eb904f1fd25c350d3956c0fd" align="middle">
<img src="https://picx.zhimg.com/v2-b6179c86ec697f23999d8f81d2a8cc99" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mismatch-Aware-Guidance-for-Robust-Emotion-Control-in-Auto-Regressive-TTS-Models"><a href="#Mismatch-Aware-Guidance-for-Robust-Emotion-Control-in-Auto-Regressive-TTS-Models" class="headerlink" title="Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models"></a>Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models</h2><p><strong>Authors:Yizhou Peng, Yukun Ma, Chong Zhang, Yi-Wen Chao, Chongjia Ni, Bin Ma</strong></p>
<p>While Text-to-Speech (TTS) systems can achieve fine-grained control over emotional expression via natural language prompts, a significant challenge emerges when the desired emotion (style prompt) conflicts with the semantic content of the text. This mismatch often results in unnatural-sounding speech, undermining the goal of achieving fine-grained emotional control. Classifier-Free Guidance (CFG) is a key technique for enhancing prompt alignment; however, its application to auto-regressive (AR) TTS models remains underexplored, which can lead to degraded audio quality. This paper directly addresses the challenge of style-content mismatch in AR TTS models by proposing an adaptive CFG scheme that adjusts to different levels of the detected mismatch, as measured using large language models or natural language inference models. This solution is based on a comprehensive analysis of CFGâ€™s impact on emotional expressiveness in state-of-the-art AR TTS models. Our results demonstrate that the proposed adaptive CFG scheme improves the emotional expressiveness of the AR TTS model while maintaining audio quality and intelligibility. </p>
<blockquote>
<p>è™½ç„¶æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç²¾ç»†çš„æƒ…ç»ªè¡¨è¾¾æ§åˆ¶ï¼Œä½†å½“æœŸæœ›çš„æƒ…ç»ªï¼ˆé£æ ¼æç¤ºï¼‰ä¸æ–‡æœ¬è¯­ä¹‰å†…å®¹å‘ç”Ÿå†²çªæ—¶ï¼Œå°±ä¼šå‡ºç°ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è¿™ç§ä¸åŒ¹é…é€šå¸¸ä¼šå¯¼è‡´è¯­éŸ³å¬èµ·æ¥ä¸è‡ªç„¶ï¼Œä»è€Œæ— æ³•å®ç°ç²¾ç»†çš„æƒ…ç»ªæ§åˆ¶ç›®æ ‡ã€‚æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯æé«˜æç¤ºå¯¹é½çš„å…³é”®æŠ€æœ¯ï¼Œç„¶è€Œå®ƒåœ¨è‡ªåŠ¨å›å½’ï¼ˆARï¼‰TTSæ¨¡å‹ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œè¿™å¯èƒ½å¯¼è‡´éŸ³é¢‘è´¨é‡ä¸‹é™ã€‚æœ¬æ–‡é’ˆå¯¹AR TTSæ¨¡å‹ä¸­é£æ ¼ä¸å†…å®¹ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”CFGæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æ ¹æ®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æˆ–è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹æ£€æµ‹åˆ°çš„ä¸åŒä¸åŒ¹é…ç¨‹åº¦è¿›è¡Œè°ƒæ•´ã€‚è¿™ä¸ªè§£å†³æ–¹æ¡ˆåŸºäºCFGå¯¹æœ€æ–°AR TTSæ¨¡å‹çš„æƒ…æ„Ÿè¡¨ç°åŠ›å½±å“çš„ç»¼åˆåˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è‡ªé€‚åº”CFGæ–¹æ¡ˆæé«˜äº†AR TTSæ¨¡å‹çš„æƒ…æ„Ÿè¡¨ç°åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†éŸ³é¢‘è´¨é‡å’Œå¯ç†è§£æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13293v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬è‡³è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿåœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç²¾ç»†æƒ…ç»ªè¡¨è¾¾æ§åˆ¶æ—¶ï¼Œå½“æœŸæœ›çš„æƒ…ç»ªï¼ˆé£æ ¼æç¤ºï¼‰ä¸æ–‡æœ¬è¯­ä¹‰å†…å®¹å‘ç”Ÿå†²çªæ—¶ï¼Œä¼šå‡ºç°æ˜¾è‘—æŒ‘æˆ˜ã€‚è¿™ç§ä¸åŒ¹é…ä¼šå¯¼è‡´è¯­éŸ³å¬èµ·æ¥ä¸è‡ªç„¶ï¼Œæ— æ³•å®ç°ç²¾ç»†æƒ…ç»ªæ§åˆ¶çš„ç›®æ ‡ã€‚è™½ç„¶æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯æé«˜æç¤ºå¯¹é½çš„å…³é”®æŠ€æœ¯ï¼Œä½†å…¶åº”ç”¨äºè‡ªå›å½’ï¼ˆARï¼‰TTSæ¨¡å‹æ—¶ï¼ŒéŸ³é¢‘è´¨é‡ä¼šä¸‹é™ã€‚æœ¬æ–‡ç›´æ¥è§£å†³äº†AR TTSæ¨¡å‹ä¸­çš„é£æ ¼å†…å®¹ä¸åŒ¹é…é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”CFGæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æ ¹æ®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æˆ–è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹æ£€æµ‹åˆ°çš„ä¸åŒ¹é…ç¨‹åº¦çš„ä¸åŒè¿›è¡Œè°ƒæ•´ã€‚è¯¥æ–¹æ¡ˆåŸºäºCFGå¯¹æœ€å…ˆè¿›AR TTSæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å½±å“çš„å…¨é¢åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªé€‚åº”CFGæ–¹æ¡ˆæé«˜äº†AR TTSæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†éŸ³é¢‘è´¨é‡å’Œæ¸…æ™°åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TTSç³»ç»Ÿåœ¨å®ç°æƒ…ç»ªè¡¨è¾¾çš„ç²¾ç»†æ§åˆ¶æ—¶é¢ä¸´é£æ ¼æç¤ºä¸æ–‡æœ¬è¯­ä¹‰å†…å®¹çš„å†²çªæŒ‘æˆ˜ã€‚</li>
<li>é£æ ¼ä¸å†…å®¹çš„ä¸åŒ¹é…ä¼šå¯¼è‡´è¯­éŸ³ä¸è‡ªç„¶ï¼Œå½±å“ç²¾ç»†æƒ…ç»ªæ§åˆ¶ç›®æ ‡çš„å®ç°ã€‚</li>
<li>æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æŠ€æœ¯å¯¹äºæé«˜æç¤ºå¯¹é½åœ¨TTSç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚</li>
<li>åœ¨è‡ªå›å½’ï¼ˆARï¼‰TTSæ¨¡å‹ä¸­åº”ç”¨CFGæ—¶ï¼ŒéŸ³é¢‘è´¨é‡å¯èƒ½ä¼šä¸‹é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”CFGæ–¹æ¡ˆï¼Œèƒ½æ ¹æ®æ£€æµ‹åˆ°çš„é£æ ¼ä¸å†…å®¹ä¸åŒ¹é…ç¨‹åº¦è¿›è¡Œè°ƒæ•´ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåŸºäºCFGå¯¹æœ€æ–°AR TTSæ¨¡å‹æƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›çš„å…¨é¢åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9802c040a729a6eee08b03622cad723d" align="middle">
<img src="https://picx.zhimg.com/v2-069665a81899348a05fa0b7f33335002" align="middle">
<img src="https://picx.zhimg.com/v2-33b1ca97204eced5728bc010c6025614" align="middle">
<img src="https://picx.zhimg.com/v2-e6fa97faff93f495d68a0c05d3afca89" align="middle">
<img src="https://picx.zhimg.com/v2-b6f7a00647c25ef291476a522fbe8d9f" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StressTransfer-Stress-Aware-Speech-to-Speech-Translation-with-Emphasis-Preservation"><a href="#StressTransfer-Stress-Aware-Speech-to-Speech-Translation-with-Emphasis-Preservation" class="headerlink" title="StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis   Preservation"></a>StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis   Preservation</h2><p><strong>Authors:Xi Chen, Yuchen Song, Satoshi Nakamura</strong></p>
<p>We propose a stress-aware speech-to-speech translation (S2ST) system that preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis conversion. Our method translates source-language stress into target-language tags that guide a controllable TTS model. To overcome data scarcity, we developed a pipeline to automatically generate aligned training data and introduce the â€œLLM-as-Judgeâ€ for evaluation. Experiments show our approach substantially outperforms baselines in preserving emphasis while maintaining comparable translation quality, speaker intent, and naturalness. Our work highlights the importance of prosody in translation and provides an effective, data-efficient solution for preserving paralinguistic cues in S2ST. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ„ŸçŸ¥å‹åŠ›çš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€å¼ºè°ƒè½¬æ¢ï¼Œä»¥ä¿ç•™å•è¯çº§åˆ«çš„å¼ºè°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æºè¯­è¨€çš„å‹åŠ›è½¬åŒ–ä¸ºç›®æ ‡è¯­è¨€çš„æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å¯ä»¥å¼•å¯¼å¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ¨¡å‹ã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç®¡é“æ¥è‡ªåŠ¨äº§ç”Ÿå¯¹é½çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶å¼•å…¥äº†â€œå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä»·è€…â€æ¥è¿›è¡Œè¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒé‡ç‚¹çš„åŒæ—¶ï¼Œåœ¨ç¿»è¯‘è´¨é‡ã€è¯´è¯è€…çš„æ„å›¾å’Œè‡ªç„¶åº¦æ–¹é¢ä¸åŸºçº¿ç›¸æ¯”æœ‰æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†éŸµå¾‹åœ¨ç¿»è¯‘ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”æ•°æ®é«˜æ•ˆçš„æ–¹æ³•æ¥ä¿ç•™è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ä¸­çš„è¯­è¨€å¤–çº¿ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‹åŠ›æ„ŸçŸ¥çš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè·¨è¯­è¨€å¼ºè°ƒè½¬æ¢ï¼Œä»¥ä¿ç•™å•è¯çº§åˆ«çš„å¼ºè°ƒä¿¡æ¯ã€‚é€šè¿‡å°†æºè¯­è¨€çš„åº”åŠ›ç¿»è¯‘ä¸ºå¼•å¯¼å¯æ§æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„ç›®æ ‡è¯­è¨€æ ‡ç­¾ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä»èƒ½æœ‰æ•ˆå·¥ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¼ºè°ƒã€ç¿»è¯‘è´¨é‡ã€è¯´è¯è€…æ„å›¾å’Œè‡ªç„¶åº¦æ–¹é¢å‡ä¼˜äºåŸºçº¿ã€‚æœ¬æ–‡å¼ºè°ƒäº†è¯­è°ƒåœ¨ç¿»è¯‘ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§æœ‰æ•ˆã€æ•°æ®é«˜æ•ˆçš„æ–¹æ³•æ¥ä¿ç•™è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ä¸­çš„å‰¯è¯­è¨€çº¿ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºå‹åŠ›æ„ŸçŸ¥çš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆS2STï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè·¨è¯­è¨€å¼ºè°ƒè½¬æ¢ã€‚</li>
<li>é€šè¿‡å°†æºè¯­è¨€çš„åº”åŠ›è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€çš„æ ‡ç­¾ï¼Œå¼•å¯¼å¯æ§æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå®ç°å•è¯çº§åˆ«çš„å¼ºè°ƒä¿¡æ¯ä¿ç•™ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆå¯¹é½è®­ç»ƒæ•°æ®çš„ç®¡é“ï¼Œä»¥å…‹æœæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†â€œLLM-as-Judgeâ€è¿›è¡Œè¯„ä¼°ï¼Œå¼ºè°ƒäº†è¯­è°ƒåœ¨ç¿»è¯‘ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¼ºè°ƒã€ç¿»è¯‘è´¨é‡ã€è¯´è¯è€…æ„å›¾å’Œè‡ªç„¶åº¦æ–¹é¢å‡ä¼˜äºåŸºçº¿ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æœ‰æ•ˆã€æ•°æ®é«˜æ•ˆçš„æ–¹æ³•æ¥ä¿ç•™è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ä¸­çš„å‰¯è¯­è¨€çº¿ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d30165f56e56e88b6717e789ef82c027" align="middle">
<img src="https://picx.zhimg.com/v2-961ab85c7c9939be62e06a7ab5185272" align="middle">
<img src="https://picx.zhimg.com/v2-2ea99ac0829b3d0bf9ee1175b8ce9880" align="middle">
<img src="https://picx.zhimg.com/v2-cd7ff22f14ece7fc3f6c566b95740416" align="middle">
<img src="https://picx.zhimg.com/v2-955ae31a38fa630f712183d35258a45b" align="middle">
<img src="https://picx.zhimg.com/v2-6fe7389fd852fbecf1c6c936a1246f86" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Continuous-Token-Diffusion-for-Speaker-Referenced-TTS-in-Multimodal-LLMs"><a href="#Continuous-Token-Diffusion-for-Speaker-Referenced-TTS-in-Multimodal-LLMs" class="headerlink" title="Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs"></a>Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs</h2><p><strong>Authors:Xinlu He, Swayambhu Nath Ray, Harish Mallidi, Jia-Hong Huang, Ashwin Bellur, Chander Chandak, M. Maruf, Venkatesh Ravichandran</strong></p>
<p>Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information.In this work, we investigate the TTS within the MLLM paradigm using continuous speech representations. We design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis. (3) Masked training is employed to address exposure bias in autoregressive decoding. (4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ï¼Œç»Ÿä¸€æ¶æ„åœ¨å•ä¸€æ¡†æ¶å†…å¤„ç†å„ç§ä»»åŠ¡æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ä»»åŠ¡ä¸­ï¼Œå½“å‰çš„åŸºäºMLLMçš„æ–¹æ³•ä¾èµ–äºç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºï¼Œè¿™å¿½ç•¥äº†è¯­éŸ³çš„å›ºæœ‰è¿ç»­æ€§ï¼Œå¹¶å¯èƒ½å¯¼è‡´ç²¾ç»†çš„å£°å­¦ä¿¡æ¯ä¸¢å¤±ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿ç»­è¯­éŸ³è¡¨ç¤ºæ¥ç ”ç©¶MLLMèŒƒå¼ä¸­çš„TTSã€‚æˆ‘ä»¬è®¾è®¡äº†åŒå¤´æ¶æ„ï¼Œå¹¶å®æ–½äº†ä¸¤ç§äº’è¡¥çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥æ„å»ºç¨³å¥çš„æ¨¡å‹ã€‚ï¼ˆ1ï¼‰åœ¨MLLMä¸Šå¢åŠ äº†ä¸€ä¸ªç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºçš„åˆ†å½¢å¤´ï¼Œè¯¥å¤´æ˜¯å¸§çº§çš„ï¼Œå¹¶ä¸”ä¸¥æ ¼è‡ªå›å½’ã€‚ï¼ˆ2ï¼‰ä¿ç•™åŸå§‹è¯­è¨€æ¨¡å‹å¤´ä»¥ä¿ç•™å¤šä»»åŠ¡èƒ½åŠ›å¹¶æ§åˆ¶è¯­éŸ³åˆæˆçš„å¼€å§‹å’Œç»“æŸã€‚ï¼ˆ3ï¼‰é‡‡ç”¨æ©ç è®­ç»ƒæ¥è§£å†³è‡ªå›å½’è§£ç ä¸­çš„æš´éœ²åè§ã€‚ï¼ˆ4ï¼‰ä¸ºäº†ä¼˜åŒ–ç¨³å®šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ¡ˆï¼Œåœ¨ç¬¬äºŒé˜¶æ®µå†»ç»“LMï¼Œç¡®ä¿åˆ†å½¢å¤´ä»å›ºå®šçš„è¾“å…¥åˆ†å¸ƒä¸­å­¦ä¹ ã€‚åœ¨LibriSpeechï¼ˆPCï¼‰æµ‹è¯•æ¸…æ´åº¦è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è‡ªå›å½’æ€§èƒ½ï¼Œè¯é”™è¯¯ç‡ä¸º1.95%ï¼Œè¯´è¯äººç›¸ä¼¼åº¦ä¸º0.54ï¼ŒUTMOSä¸º4.00ã€‚ä¸å•é˜¶æ®µè®­ç»ƒåŸºçº¿ç›¸æ¯”ï¼Œä¸¤é˜¶æ®µè®­ç»ƒä½¿è¯é”™è¯¯ç‡ç›¸å¯¹é™ä½äº†46%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç»“åˆè‡ªå›å½’å»ºæ¨¡ä¸è¿ç»­ä»¤ç‰Œæ‰©æ•£çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¾—åˆ°ä¸¤é˜¶æ®µè®­ç»ƒç¨‹åºçš„æ”¯æ’‘ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ä¸‹å®ç°æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä»»åŠ¡çš„æ–°æ–¹æ³•ã€‚ä¸åŒäºç°æœ‰ä¾èµ–äºç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨è¿ç»­è¯­éŸ³è¡¨ç¤ºï¼Œè®¾è®¡äº†ä¸€ä¸ªåŒå¤´æ¶æ„ï¼Œå¹¶å®æ–½äº†ä¸¤ç§äº’è¡¥çš„è®­ç»ƒç­–ç•¥ã€‚é€šè¿‡æ‰©æ•£å¤´ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™åŸå§‹è¯­è¨€æ¨¡å‹å¤´ä»¥ä¿ç•™å¤šä»»åŠ¡èƒ½åŠ›å’Œæ§åˆ¶è¯­éŸ³åˆæˆçš„å¼€å§‹å’Œç»“æŸã€‚ä½¿ç”¨æ©ç è®­ç»ƒè§£å†³è‡ªå›å½’è§£ç ä¸­çš„æ›å…‰åå·®é—®é¢˜ï¼Œå¹¶æå‡ºä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ¡ˆï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LibriSpeechæµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„è‡ªå›å½’æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‡ç”¨äº†åŸºäºè¿ç»­è¯­éŸ³è¡¨ç¤ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¶æ„è¿›è¡Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä»»åŠ¡ã€‚</li>
<li>è®¾è®¡äº†åŒå¤´æ¶æ„ï¼ŒåŒ…æ‹¬ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºçš„æ‰©æ•£å¤´ï¼Œå¹¶ä¿ç•™äº†åŸå§‹è¯­è¨€æ¨¡å‹å¤´ä»¥å®ç°å¤šä»»åŠ¡èƒ½åŠ›å’Œæ§åˆ¶è¯­éŸ³åˆæˆçš„å¼€å§‹å’Œç»“æŸã€‚</li>
<li>å®æ–½äº†ä¸¤ç§äº’è¡¥çš„è®­ç»ƒç­–ç•¥ï¼šä½¿ç”¨æ©ç è®­ç»ƒè§£å†³è‡ªå›å½’è§£ç ä¸­çš„æ›å…‰åå·®é—®é¢˜ï¼Œå¹¶æå‡ºä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ¡ˆä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>åœ¨LibriSpeechæµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†å…ˆè¿›çš„è‡ªå›å½’æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€è¯´è¯äººç›¸ä¼¼åº¦å’ŒUTMOSè¯„åˆ†ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ç›¸è¾ƒäºå•é˜¶æ®µè®­ç»ƒï¼Œå®ç°äº†46%çš„ç›¸å¯¹è¯é”™è¯¯ç‡é™ä½ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†è‡ªå›å½’å»ºæ¨¡å’Œè¿ç»­ä»¤ç‰Œæ‰©æ•£ï¼Œæ˜¾ç¤ºå‡ºäº†è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5077e519ab104b726b010a4c7d7012e3" align="middle">
<img src="https://picx.zhimg.com/v2-5c70f1847607fef6a922e9950524f98d" align="middle">
<img src="https://picx.zhimg.com/v2-698bf94b3c69268a3697bd2d462f6e26" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Content-Anonymization-for-Privacy-in-Long-form-Audio"><a href="#Content-Anonymization-for-Privacy-in-Long-form-Audio" class="headerlink" title="Content Anonymization for Privacy in Long-form Audio"></a>Content Anonymization for Privacy in Long-form Audio</h2><p><strong>Authors:Cristina Aggazzotti, Ashi Garg, Zexin Cai, Nicholas Andrews</strong></p>
<p>Voice anonymization techniques have been found to successfully obscure a speakerâ€™s acoustic identity in short, isolated utterances in benchmarks such as the VoicePrivacy Challenge. In practice, however, utterances seldom occur in isolation: long-form audio is commonplace in domains such as interviews, phone calls, and meetings. In these cases, many utterances from the same speaker are available, which pose a significantly greater privacy risk: given multiple utterances from the same speaker, an attacker could exploit an individualâ€™s vocabulary, syntax, and turns of phrase to re-identify them, even when their voice is completely disguised. To address this risk, we propose new content anonymization approaches. Our approach performs a contextual rewriting of the transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while preserving meaning. We present results in a long-form telephone conversation setting demonstrating the effectiveness of a content-based attack on voice-anonymized speech. Then we show how the proposed content-based anonymization methods can mitigate this risk while preserving speech utility. Overall, we find that paraphrasing is an effective defense against content-based attacks and recommend that stakeholders adopt this step to ensure anonymity in long-form audio. </p>
<blockquote>
<p>è¯­éŸ³åŒ¿ååŒ–æŠ€æœ¯åœ¨åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VoicePrivacy Challengeï¼‰ä¸­æˆåŠŸåœ°å°†å‘è¨€è€…çš„å£°éŸ³ç‰¹å¾éšè—åœ¨çŸ­è€Œå­¤ç«‹çš„è¯­å¥é‡Œã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¯­å¥å¾ˆå°‘æ˜¯å­¤ç«‹çš„ï¼šé•¿éŸ³é¢‘åœ¨è®¿è°ˆã€ç”µè¯å’Œä¼šè®®ç­‰é¢†åŸŸä¸­å¾ˆå¸¸è§ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ¥è‡ªåŒä¸€å‘è¨€è€…çš„è®¸å¤šè¯­å¥æ„æˆäº†æ›´å¤§çš„éšç§é£é™©ï¼šç»™å®šåŒä¸€å‘è¨€è€…çš„å¤šä¸ªè¯­å¥ï¼Œæ”»å‡»è€…å¯èƒ½ä¼šåˆ©ç”¨ä¸ªäººçš„è¯æ±‡ã€è¯­æ³•å’Œæªè¾æ¥é‡æ–°è¯†åˆ«ä»–ä»¬ï¼Œå³ä½¿ä»–ä»¬çš„å£°éŸ³å®Œå…¨è¢«ä¼ªè£…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„å†…å®¹åŒ¿ååŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€ä¸ªASR-TTSç®¡é“ä¸­å¯¹è½¬å½•è¿›è¡Œä¸Šä¸‹æ–‡é‡å†™ï¼Œä»¥æ¶ˆé™¤è¯´è¯äººçš„ç‰¹å®šé£æ ¼ï¼ŒåŒæ—¶ä¿ç•™åŸæ„ã€‚æˆ‘ä»¬åœ¨é•¿ç”µè¯å¯¹è¯è®¾ç½®ä¸­å±•ç¤ºäº†åŸºäºå†…å®¹çš„æ”»å‡»å¯¹è¯­éŸ³åŒ¿ååŒ–è¯­éŸ³çš„æœ‰æ•ˆæ€§ç»“æœã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„å†…å®¹åŒ¿ååŒ–æ–¹æ³•å¦‚ä½•å‡è½»è¿™ç§é£é™©ï¼ŒåŒæ—¶ä¿ç•™è¯­éŸ³çš„å®ç”¨æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æ”¹è¿°æ˜¯æŠµå¾¡åŸºäºå†…å®¹çš„æ”»å‡»çš„æœ‰æ•ˆé˜²å¾¡æ‰‹æ®µï¼Œå¹¶å»ºè®®åˆ©ç›Šç›¸å…³è€…é‡‡å–è¿™ä¸€æ­¥éª¤ï¼Œä»¥ç¡®ä¿é•¿éŸ³é¢‘çš„åŒ¿åæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12780v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åœ¨VoicePrivacy Challengeç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯­éŸ³åŒ¿ååŒ–æŠ€æœ¯å·²æˆåŠŸç”¨äºæ©ç›–è¯´è¯è€…çš„å£°éŸ³èº«ä»½åœ¨çŸ­æš‚çš„å­¤ç«‹å‘è¨€ä¸­ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå‘è¨€å¾ˆå°‘æ˜¯å­¤ç«‹çš„ï¼šé•¿éŸ³é¢‘åœ¨è®¿è°ˆã€ç”µè¯å’Œä¼šè®®ç­‰é¢†åŸŸä¸­å¾ˆå¸¸è§ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼ŒåŒä¸€è¯´è¯è€…çš„è®¸å¤šå‘è¨€éƒ½æ˜¯å¯ç”¨çš„ï¼Œè¿™å¸¦æ¥äº†æ›´å¤§çš„éšç§é£é™©ï¼šå³ä½¿å£°éŸ³å®Œå…¨ä¼ªè£…ï¼Œæ”»å‡»è€…ä»å¯ä»¥åˆ©ç”¨åŒä¸€äººçš„è¯æ±‡ã€è¯­æ³•å’Œæªè¾æ¥é‡æ–°è¯†åˆ«èº«ä»½ã€‚ä¸ºåº”å¯¹è¿™ä¸€é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹å†…å®¹åŒ¿ååŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ASR-TTSç®¡é“ä¸­çš„ä¸Šä¸‹æ–‡é‡å†™æ¥æ¶ˆé™¤è¯´è¯è€…ç‰¹æœ‰çš„é£æ ¼ï¼ŒåŒæ—¶ä¿ç•™æ„ä¹‰ã€‚æˆ‘ä»¬åœ¨ç”µè¯å¯¹è¯çš„é•¿éŸ³é¢‘è®¾ç½®å±•ç¤ºäº†å†…å®¹æ”»å‡»å¯¹è¯­éŸ³åŒ¿ååŒ–çš„è¯­éŸ³çš„æœ‰æ•ˆæ€§ï¼Œç„¶åå±•ç¤ºäº†æ‰€æå‡ºçš„å†…å®¹åŒ¿ååŒ–æ–¹æ³•å¦‚ä½•é™ä½è¿™ç§é£é™©åŒæ—¶ä¿ç•™è¯­éŸ³å®ç”¨æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æ”¹è¿°æ˜¯æŠµå¾¡å†…å®¹æ”»å‡»çš„æœ‰æ•ˆé˜²å¾¡æ‰‹æ®µï¼Œå»ºè®®ç›¸å…³æ–¹é‡‡å–è¿™ä¸€æ­¥éª¤ä»¥ç¡®ä¿é•¿éŸ³é¢‘çš„åŒ¿åæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³åŒ¿ååŒ–æŠ€æœ¯åœ¨çŸ­è€Œå­¤ç«‹çš„å‘è¨€ä¸­å¾ˆæœ‰æ•ˆï¼Œä½†åœ¨é•¿éŸ³é¢‘ä¸­å­˜åœ¨é‡å¤§éšç§é£é™©ã€‚</li>
<li>åŒä¸€è¯´è¯è€…çš„å¤šä¸ªå‘è¨€å¯ä½¿æ”»å‡»è€…é€šè¿‡è¯æ±‡ã€è¯­æ³•å’Œæªè¾é‡æ–°è¯†åˆ«èº«ä»½ã€‚</li>
<li>æå‡ºæ–°å‹å†…å®¹åŒ¿ååŒ–æ–¹æ³•ï¼Œé€šè¿‡ASR-TTSç®¡é“ä¸­çš„ä¸Šä¸‹æ–‡é‡å†™æ¶ˆé™¤è¯´è¯è€…ç‰¹æœ‰é£æ ¼ã€‚</li>
<li>åœ¨ç”µè¯å¯¹è¯çš„é•¿éŸ³é¢‘è®¾ç½®ä¸­ï¼Œå±•ç¤ºäº†å†…å®¹æ”»å‡»çš„æœ‰æ•ˆæ€§ä»¥åŠæ‰€ææ–¹æ³•çš„æ•ˆç”¨ã€‚</li>
<li>æ”¹è¿°æ˜¯æœ‰æ•ˆæŠµå¾¡å†…å®¹æ”»å‡»çš„æ‰‹æ®µã€‚</li>
<li>å†…å®¹åŒ¿ååŒ–æ–¹æ³•åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ï¼Œèƒ½ä¿ç•™è¯­éŸ³çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0cdeaa27367e40e1fc28476c61d472c" align="middle">
<img src="https://picx.zhimg.com/v2-538a1b519677a7f64ff5b078dc594859" align="middle">
<img src="https://picx.zhimg.com/v2-e8063155d0b8cee5c9ca0d8cf28a6261" align="middle">
<img src="https://picx.zhimg.com/v2-c2a199bc798b181bbbc0a22dec39a867" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TerraCodec-Compressing-Earth-Observations"><a href="#TerraCodec-Compressing-Earth-Observations" class="headerlink" title="TerraCodec: Compressing Earth Observations"></a>TerraCodec: Compressing Earth Observations</h2><p><strong>Authors:Julen Costa-Watanabe, Isabelle Wittmann, Benedikt Blumenstiel, Konrad Schindler</strong></p>
<p>Earth observation (EO) satellites produce massive streams of multispectral image time series, posing pressing challenges for storage and transmission. Yet, learned EO compression remains fragmented, lacking publicly available pretrained models and misaligned with advances in compression for natural imagery. Image codecs overlook temporal redundancy, while video codecs rely on motion priors that fail to capture the radiometric evolution of largely static scenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to EO. TEC includes efficient image-based variants adapted to multispectral inputs, as well as a Temporal Transformer model (TEC-TT) that leverages dependencies across time. To overcome the fixed-rate setting of todayâ€™s neural codecs, we present Latent Repacking, a novel method for training flexible-rate transformer models that operate on varying rate-distortion settings. Trained on Sentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x stronger compression at equivalent image quality. Beyond compression, TEC-TT enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the AllClear benchmark. Our results establish bespoke, learned compression algorithms as a promising direction for Earth observation. Code and model weights will be released under a permissive license. </p>
<blockquote>
<p>åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰å«æ˜Ÿäº§ç”Ÿå¤§é‡çš„å¤šå…‰è°±å›¾åƒæ—¶é—´åºåˆ—ï¼Œå¯¹å­˜å‚¨å’Œä¼ è¾“æå‡ºäº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„EOå‹ç¼©æŠ€æœ¯ä»ç„¶åˆ†æ•£ï¼Œç¼ºä¹å…¬å¼€çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”ä¸è‡ªç„¶å›¾åƒå‹ç¼©æŠ€æœ¯çš„è¿›å±•ä¸åŒ¹é…ã€‚å›¾åƒç¼–è§£ç å™¨å¿½ç•¥äº†æ—¶é—´å†—ä½™ï¼Œè€Œè§†é¢‘ç¼–è§£ç å™¨åˆ™ä¾èµ–äºè¿åŠ¨ä¼˜å…ˆæƒï¼Œæ— æ³•æ•æ‰é™æ€åœºæ™¯çš„å…‰åº¦å˜åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†TerraCodecï¼ˆTECï¼‰ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸“ä¸ºEOå®šåˆ¶çš„å­¦ä¹ ç¼–è§£ç å™¨ã€‚TECåŒ…æ‹¬é€‚åº”å¤šå…‰è°±è¾“å…¥çš„åŸºäºå›¾åƒçš„é«˜æ•ˆå˜ä½“ï¼Œä»¥åŠåˆ©ç”¨æ—¶é—´ä¾èµ–æ€§çš„Temporal Transformeræ¨¡å‹ï¼ˆTEC-TTï¼‰ã€‚ä¸ºäº†å…‹æœå½“å‰ç¥ç»ç¼–è§£ç å™¨çš„å›ºå®šé€Ÿç‡è®¾ç½®ï¼Œæˆ‘ä»¬æå‡ºäº†Latent Repackingï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è®­ç»ƒçµæ´»é€Ÿç‡å˜æ¢æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯åœ¨ä¸åŒçš„é€Ÿç‡å¤±çœŸè®¾ç½®ä¸‹è¿è¡Œã€‚åœ¨Sentinel-2æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒTerraCodecåœ¨å›¾åƒè´¨é‡ç›¸å½“çš„æƒ…å†µä¸‹å®ç°äº†3-10å€çš„å¼ºå‹ç¼©ã€‚é™¤äº†å‹ç¼©åŠŸèƒ½å¤–ï¼ŒTEC-TTè¿˜å®ç°äº†é›¶æ ·æœ¬äº‘è¡¥å…¨ï¼Œåœ¨AllClearåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜ï¼Œä¸“ç”¨çš„å­¦ä¹ å‹ç¼©ç®—æ³•å¯¹äºåœ°çƒè§‚æµ‹æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸Œæœ›çš„ç ”ç©¶æ–¹å‘ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å°†åœ¨è®¸å¯ä¸‹å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12670v1">PDF</a> </p>
<p><strong>Summary</strong><br>    TerraCodecï¼ˆTECï¼‰æ˜¯ä¸€ç§ä¸“ä¸ºåœ°çƒè§‚æµ‹è®¾è®¡çš„ç¥ç»ç½‘ç»œç¼–ç è§£ç å™¨ï¼Œå®ƒé’ˆå¯¹å¤šå…‰è°±å›¾åƒæ—¶é—´åºåˆ—è¿›è¡Œå‹ç¼©ã€‚é€šè¿‡é‡‡ç”¨å›¾åƒå’Œå¤šå…‰è°±è¾“å…¥çš„å˜ç§ä»¥åŠåˆ©ç”¨æ—¶é—´ä¾èµ–æ€§çš„Temporal Transformeræ¨¡å‹ï¼ˆTEC-TTï¼‰ï¼ŒTerraCodecå®ç°äº†é«˜æ•ˆå‹ç¼©ã€‚ä¸ä¼ ç»Ÿç¼–ç è§£ç å™¨ç›¸æ¯”ï¼ŒTerraCodecåœ¨ç›¸åŒå›¾åƒè´¨é‡ä¸‹å®ç°äº†3è‡³10å€çš„å‹ç¼©æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTEC-TTè¿˜æ”¯æŒé›¶æ ·æœ¬äº‘å¡«å……åŠŸèƒ½ï¼Œåœ¨AllClearåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚è¯¥è®ºæ–‡çš„ç ”ç©¶ç»“æœè¯æ˜äº†ä¸ºåœ°çƒè§‚æµ‹é‡èº«å®šåˆ¶çš„ç¥ç»ç½‘ç»œç¼–ç è§£ç å™¨ç®—æ³•å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å°†åœ¨è®¸å¯ä¸‹å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TerraCodecï¼ˆTECï¼‰æ˜¯ä¸“ä¸ºåœ°çƒè§‚æµ‹è®¾è®¡çš„ç¥ç»ç½‘ç»œç¼–ç è§£ç å™¨å®¶æ—ã€‚</li>
<li>TECè§£å†³äº†å¤šå…‰è°±å›¾åƒæ—¶é—´åºåˆ—å­˜å‚¨å’Œä¼ è¾“çš„æŒ‘æˆ˜ã€‚</li>
<li>TECåŒ…æ‹¬é’ˆå¯¹å¤šå…‰è°±è¾“å…¥çš„å›¾åƒå˜ç§å’ŒTemporal Transformeræ¨¡å‹ï¼ˆTEC-TTï¼‰ã€‚</li>
<li>Latent Repackingæ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒçµæ´»é€Ÿç‡çš„è½¬æ¢å™¨æ¨¡å‹ï¼Œé€‚ç”¨äºä¸åŒçš„é€Ÿç‡å¤±çœŸè®¾ç½®ã€‚</li>
<li>TerraCodecå®ç°äº†å¯¹ä¼ ç»Ÿç¼–ç è§£ç å™¨çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œåœ¨ç›¸åŒå›¾åƒè´¨é‡ä¸‹å®ç°äº†æ›´é«˜çš„å‹ç¼©ç‡ã€‚</li>
<li>é™¤äº†å‹ç¼©åŠŸèƒ½å¤–ï¼ŒTEC-TTè¿˜æ”¯æŒé›¶æ ·æœ¬äº‘å¡«å……åŠŸèƒ½ï¼Œå¹¶åœ¨AllClearåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c76e6ab4d9d65b5f4c3dacee033423b" align="middle">
<img src="https://picx.zhimg.com/v2-431aee6faebf436df37ca4297962d099" align="middle">
<img src="https://picx.zhimg.com/v2-7841112502f7e1b5f108bceb86efc86e" align="middle">
<img src="https://picx.zhimg.com/v2-4da0c9e1f054adbb1e0f29ae39b5079f" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiSTAR-Diffusion-over-a-Scalable-Token-Autoregressive-Representation-for-Speech-Generation"><a href="#DiSTAR-Diffusion-over-a-Scalable-Token-Autoregressive-Representation-for-Speech-Generation" class="headerlink" title="DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation"></a>DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation</h2><p><strong>Authors:Yakun Song, Xiaobin Zhuang, Jiawei Chen, Zhikang Niu, Guanrou Yang, Chenpeng Du, Dongya Jia, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen</strong></p>
<p>Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce DISTAR, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, DISTAR drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: DISTAR produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that DISTAR surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker&#x2F;style consistency, while maintaining rich output diversity. Audio samples are provided on <a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/DiSTAR_demo">https://anonymous.4open.science/w/DiSTAR_demo</a>. </p>
<blockquote>
<p>æœ€è¿‘å°è¯•å°†è‡ªå›å½’ï¼ˆARï¼‰ç´ æå¸ˆä¸åŸºäºæ‰©æ•£çš„ç»†åŒ–å™¨åœ¨è¿ç»­è¯­éŸ³è¡¨ç¤ºä¸­è¿›è¡Œäº¤ç»‡ï¼Œæ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†åœ¨åˆ†å¸ƒè½¬ç§»çš„æƒ…å†µä¸‹ä»ç„¶æ˜¾å¾—è„†å¼±ï¼Œå¯¹å¯æ§æ€§çš„æ§åˆ¶æ æ†æœ‰é™ã€‚æˆ‘ä»¬å¼•å…¥äº†DISTARï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„æ¡†æ¶ï¼Œå®ƒå®Œå…¨åœ¨ç¦»æ•£æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰çš„ç¼–ç ç©ºé—´ä¸­è¿è¡Œï¼Œç´§å¯†åœ°å°†ARè¯­è¨€æ¨¡å‹ä¸æ©ç æ‰©æ•£æ¨¡å‹è€¦åˆï¼Œæ— éœ€å¼ºåˆ¶å¯¹é½æˆ–æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚å…·ä½“æ¥è¯´ï¼ŒDISTARä½¿ç”¨ARè¯­è¨€æ¨¡å‹ç»˜åˆ¶åŸºäºå—çš„RVQä»¤ç‰Œï¼Œç„¶ååœ¨ç»™å®šè‰å›¾çš„æƒ…å†µä¸‹æ‰§è¡Œå¹¶è¡Œæ©ç æ‰©æ•£å¡«å……ï¼Œä»¥å®Œæˆä¸‹ä¸€ä¸ªå—ï¼Œä»è€Œåœ¨å—å¹¶è¡Œçš„æƒ…å†µä¸‹å®ç°é•¿å½¢å¼åˆæˆï¼ŒåŒæ—¶å‡è½»ç»å…¸çš„ARæ›å…‰åå·®ã€‚ç¦»æ•£ç¼–ç ç©ºé—´ä¸ºæ¨ç†æä¾›äº†æ˜ç¡®çš„æ§åˆ¶ï¼šDISTARåœ¨è´ªå©ªå’ŒåŸºäºæ ·æœ¬çš„è§£ç ä¸‹äº§ç”Ÿé«˜è´¨é‡çš„éŸ³é¢‘ï¼Œä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œæ”¯æŒåœ¨é²æ£’æ€§å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶ä¸”é€šè¿‡RVQå±‚åœ¨æµ‹è¯•æ—¶çš„ä¿®å‰ªå®ç°å¯å˜æ¯”ç‰¹ç‡å’Œå¯æ§çš„è®¡ç®—ã€‚å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒDISTARåœ¨é²æ£’æ€§ã€è‡ªç„¶æ€§å’Œè¯´è¯äºº&#x2F;é£æ ¼ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒäº†ä¸°å¯Œçš„è¾“å‡ºå¤šæ ·æ€§ã€‚éŸ³é¢‘æ ·æœ¬è¯·è®¿é—®ï¼š[åŒ¿åé“¾æ¥åœ°å€]ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12210v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DISTARæ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³æ¡†æ¶ï¼Œé‡‡ç”¨ç¦»æ•£æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰ç ç©ºé—´æ“ä½œï¼Œç´§å¯†è€¦åˆè‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œæ©ç æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¼ºåˆ¶å¯¹é½æˆ–æŒç»­æ—¶é—´é¢„æµ‹ã€‚å®ƒä½¿ç”¨ARè¯­è¨€æ¨¡å‹èµ·è‰å—çº§RVQä»¤ç‰Œï¼Œç„¶åé€šè¿‡å¹¶è¡Œæ©ç æ‰©æ•£å¡«å……æ¡ä»¶å®Œæˆä¸‹ä¸€ä¸ªå—ï¼Œå®ç°é•¿å½¢å¼åˆæˆä¸å—å¹¶è¡Œå¤„ç†ï¼Œå¹¶å‡è½»ä¼ ç»ŸARæš´éœ²åå·®ã€‚ç¦»æ•£ç ç©ºé—´åœ¨æ¨ç†æ—¶æä¾›æ˜¾å¼æ§åˆ¶ï¼šDISTARå¯åœ¨è´ªå©ªå’Œæ ·æœ¬åŸºç¡€è§£ç ä¸‹äº§ç”Ÿé«˜è´¨é‡éŸ³é¢‘ï¼Œæ”¯æŒé²æ£’æ€§å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶é€šè¿‡RVQå±‚ä¿®å‰ªåœ¨æµ‹è¯•æ—¶å®ç°å¯å˜æ¯”ç‰¹ç‡å’Œå¯æ§è®¡ç®—ã€‚å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒDISTARåœ¨é²æ£’æ€§ã€è‡ªç„¶æ€§å’Œè¯­éŸ³&#x2F;é£æ ¼ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒäº†ä¸°å¯Œçš„è¾“å‡ºå¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DISTARæ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ¡†æ¶ï¼Œå®Œå…¨åœ¨ç¦»æ•£æ®‹å·®å‘é‡é‡åŒ–ç ç©ºé—´ä¸­æ“ä½œã€‚</li>
<li>DISTARç»“åˆäº†è‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œæ©ç æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¼ºåˆ¶å¯¹é½æˆ–æŒç»­æ—¶é—´é¢„æµ‹ã€‚</li>
<li>é€šè¿‡å—çº§RVQä»¤ç‰Œèµ·è‰å’Œå¹¶è¡Œæ©ç æ‰©æ•£å¡«å……ï¼Œå®ç°äº†é•¿å½¢å¼åˆæˆä¸å—å¹¶è¡Œå¤„ç†ã€‚</li>
<li>ç¦»æ•£ç ç©ºé—´æä¾›äº†åœ¨æ¨ç†æ—¶çš„æ˜¾å¼æ§åˆ¶ï¼ŒåŒ…æ‹¬é«˜è´¨é‡éŸ³é¢‘çš„äº§ç”Ÿã€é²æ£’æ€§å’Œå¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡ä»¥åŠå¯å˜æ¯”ç‰¹ç‡å’Œå¯æ§è®¡ç®—ã€‚</li>
<li>DISTARé€šè¿‡RVQå±‚ä¿®å‰ªåœ¨æµ‹è¯•æ—¶å®ç°äº†æ€§èƒ½ä¼˜åŒ–ã€‚</li>
<li>å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒDISTARåœ¨å¤šä¸ªæ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c619729f0ca848c30c01b49b2e5447b3" align="middle">
<img src="https://picx.zhimg.com/v2-23f74beb6a9844a5efdef72f6555f77f" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BridgeCode-A-Dual-Speech-Representation-Paradigm-for-Autoregressive-Zero-Shot-Text-to-Speech-Synthesis"><a href="#BridgeCode-A-Dual-Speech-Representation-Paradigm-for-Autoregressive-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis"></a>BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis</h2><p><strong>Authors:Jingyuan Xing, Mingru Yang, Zhipeng Li, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Autoregressive (AR) frameworks have recently achieved remarkable progress in zero-shot text-to-speech (TTS) by leveraging discrete speech tokens and large language model techniques. Despite their success, existing AR-based zero-shot TTS systems face two critical limitations: (i) an inherent speed-quality trade-off, as sequential token generation either reduces frame rates at the cost of expressiveness or enriches tokens at the cost of efficiency, and (ii) a text-oriented supervision mismatch, as cross-entropy loss penalizes token errors uniformly without considering the fine-grained acoustic similarity among adjacent tokens. To address these challenges, we propose BridgeTTS, a novel AR-TTS framework built upon the dual speech representation paradigm BridgeCode. BridgeTTS reduces AR iterations by predicting sparse tokens while reconstructing rich continuous features for high-quality synthesis. Joint optimization of token-level and feature-level objectives further enhances naturalness and intelligibility. Experiments demonstrate that BridgeTTS achieves competitive quality and speaker similarity while significantly accelerating synthesis. Speech demos are available at <a target="_blank" rel="noopener" href="https://test1562.github.io/demo/">https://test1562.github.io/demo/</a>. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶æœ€è¿‘å€ŸåŠ©ç¦»æ•£è¯­éŸ³æ ‡è®°å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œåœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†ç°æœ‰çš„åŸºäºARçš„é›¶æ ·æœ¬TTSç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆiï¼‰å›ºæœ‰çš„é€Ÿåº¦ä¸è´¨é‡æƒè¡¡ï¼Œå› ä¸ºé¡ºåºæ ‡è®°ç”Ÿæˆè¦ä¹ˆä»¥é™ä½å¸§ç‡ä¸ºä»£ä»·æ¢å–è¡¨ç°åŠ›ï¼Œè¦ä¹ˆä»¥ç‰ºç‰²æ•ˆç‡ä¸ºä»£ä»·ä¸°å¯Œæ ‡è®°ï¼›ï¼ˆiiï¼‰æ–‡æœ¬å¯¼å‘çš„ç›‘ç£ä¸åŒ¹é…ï¼Œå› ä¸ºäº¤å‰ç†µæŸå¤±ä¼šç»Ÿä¸€æƒ©ç½šæ ‡è®°é”™è¯¯ï¼Œè€Œä¸ä¼šè€ƒè™‘ç›¸é‚»æ ‡è®°ä¹‹é—´çš„ç²¾ç»†å£°å­¦ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŒè¯­éŸ³è¡¨ç¤ºèŒƒå¼BridgeCodeçš„BridgeTTSæ–°å‹AR-TTSæ¡†æ¶ã€‚BridgeTTSé€šè¿‡é¢„æµ‹ç¨€ç–æ ‡è®°å¹¶é‡å»ºä¸°å¯Œçš„è¿ç»­ç‰¹å¾æ¥å‡å°‘ARè¿­ä»£ï¼Œä»¥å®ç°é«˜è´¨é‡åˆæˆã€‚æ ‡è®°çº§å’Œç‰¹å¾çº§çš„è”åˆä¼˜åŒ–ç›®æ ‡è¿›ä¸€æ­¥æé«˜äº†è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒBridgeTTSåœ¨ä¿æŒç«äº‰åŠ›çš„è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—åŠ å¿«äº†åˆæˆé€Ÿåº¦ã€‚è¯­éŸ³æ¼”ç¤ºå¯åœ¨[<a target="_blank" rel="noopener" href="https://test1562.github.io/demo/]%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://test1562.github.io/demo/]ä¸ŠæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11646v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¦»æ•£è¯­éŸ³ä»¤ç‰Œå’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶åœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AR-basedé›¶æ ·æœ¬TTSç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ï¼šä¸€æ˜¯é€Ÿåº¦ä¸è´¨é‡çš„æƒè¡¡ï¼ŒäºŒæ˜¯æ–‡æœ¬å¯¼å‘çš„ç›‘ç£ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BridgeTTSï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåŒè¯­éŸ³è¡¨ç¤ºèŒƒå¼BridgeCodeçš„æ–°å‹AR-TTSæ¡†æ¶ã€‚BridgeTTSé€šè¿‡é¢„æµ‹ç¨€ç–ä»¤ç‰Œå¹¶é‡å»ºä¸°å¯Œçš„è¿ç»­ç‰¹å¾æ¥å‡å°‘ARè¿­ä»£ï¼Œå®ç°é«˜è´¨é‡åˆæˆã€‚å¯¹ä»¤ç‰Œçº§åˆ«å’ŒåŠŸèƒ½çº§åˆ«çš„ç›®æ ‡è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒBridgeTTSåœ¨è´¨é‡ã€è¯´è¯äººç›¸ä¼¼åº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æ˜¾è‘—åŠ é€Ÿäº†åˆæˆè¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ARæ¡†æ¶åœ¨é›¶æ ·æœ¬TTSä¸­åˆ©ç”¨ç¦»æ•£è¯­éŸ³ä»¤ç‰Œå’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç°æœ‰AR-basedé›¶æ ·æœ¬TTSç³»ç»Ÿé¢ä¸´é€Ÿåº¦ä¸è´¨é‡çš„æƒè¡¡åŠæ–‡æœ¬å¯¼å‘çš„ç›‘ç£ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>BridgeTTSæ˜¯ä¸€ä¸ªåŸºäºBridgeCodeçš„æ–°å‹AR-TTSæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>BridgeTTSé€šè¿‡é¢„æµ‹ç¨€ç–ä»¤ç‰Œå¹¶é‡å»ºè¿ç»­ç‰¹å¾æ¥å‡å°‘ARè¿­ä»£ï¼Œå®ç°é«˜è´¨é‡åˆæˆã€‚</li>
<li>è”åˆä¼˜åŒ–ä»¤ç‰Œçº§åˆ«å’ŒåŠŸèƒ½çº§åˆ«çš„ç›®æ ‡ï¼Œæé«˜è¯­éŸ³è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦ã€‚</li>
<li>BridgeTTSåœ¨è´¨é‡å’Œåˆæˆé€Ÿåº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a310a8e7ca4822aeffacc3273ea3de7" align="middle">
<img src="https://picx.zhimg.com/v2-ebf0c6fd96d2b87f549ae943780e1ae6" align="middle">
<img src="https://picx.zhimg.com/v2-1e411ae7c3c3606dd378eddb02a79b88" align="middle">
<img src="https://picx.zhimg.com/v2-4ac66923924d9f580cb0adced9e8327e" align="middle">
<img src="https://picx.zhimg.com/v2-ed0be693d5f89f9a078a8a7e79bd5ce2" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ParsVoice-A-Large-Scale-Multi-Speaker-Persian-Speech-Corpus-for-Text-to-Speech-Synthesis"><a href="#ParsVoice-A-Large-Scale-Multi-Speaker-Persian-Speech-Corpus-for-Text-to-Speech-Synthesis" class="headerlink" title="ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for   Text-to-Speech Synthesis"></a>ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for   Text-to-Speech Synthesis</h2><p><strong>Authors:Mohammad Javad Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery</strong></p>
<p>Existing Persian speech datasets are typically smaller than their English counterparts, which creates a key limitation for developing Persian speech technologies. We address this gap by introducing ParsVoice, the largest Persian speech corpus designed specifically for text-to-speech(TTS) applications. We created an automated pipeline that transforms raw audiobook content into TTS-ready data, incorporating components such as a BERT-based sentence completion detector, a binary search boundary optimization method for precise audio-text alignment, and audio-text quality assessment frameworks tailored to Persian. The pipeline processes 2,000 audiobooks, yielding 3,526 hours of clean speech, which was further filtered into a 1,804-hour high-quality subset suitable for TTS, featuring more than 470 speakers. To validate the dataset, we fine-tuned XTTS for Persian, achieving a naturalness Mean Opinion Score (MOS) of 3.6&#x2F;5 and a Speaker Similarity Mean Opinion Score (SMOS) of 4.0&#x2F;5 demonstrating ParsVoiceâ€™s effectiveness for training multi-speaker TTS systems. ParsVoice is the largest high-quality Persian speech dataset, offering speaker diversity and audio quality comparable to major English corpora. The complete dataset has been made publicly available to accelerate the development of Persian speech technologies. The ParsVoice dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice">https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice</a>. </p>
<blockquote>
<p>ç°æœ‰çš„æ³¢æ–¯è¯­è¯­éŸ³æ•°æ®é›†é€šå¸¸æ¯”è‹±è¯­æ•°æ®é›†å°ï¼Œè¿™ä¸ºå¼€å‘æ³¢æ–¯è¯­è¯­éŸ³æŠ€æœ¯é€ æˆäº†å…³é”®é™åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ParsVoiceæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒParsVoiceæ˜¯ä¸“é—¨ä¸ºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åº”ç”¨è®¾è®¡çš„å¤§å‹æ³¢æ–¯è¯­è¯­éŸ³è¯­æ–™åº“ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œå°†åŸå§‹æœ‰å£°ä¹¦å†…å®¹è½¬æ¢ä¸ºé€‚ç”¨äºTTSçš„æ•°æ®ï¼Œå…¶ä¸­åŒ…æ‹¬åŸºäºBERTçš„å¥å­å®Œæˆæ£€æµ‹å™¨ã€ç”¨äºç²¾ç¡®éŸ³è§†é¢‘å¯¹é½çš„äºŒè¿›åˆ¶æœç´¢è¾¹ç•Œä¼˜åŒ–æ–¹æ³•ï¼Œä»¥åŠé’ˆå¯¹æ³¢æ–¯è¯­çš„éŸ³è§†é¢‘è´¨é‡è¯„ä¼°æ¡†æ¶ã€‚è¯¥ç®¡é“å¤„ç†äº†2000æœ¬æœ‰å£°ä¹¦ï¼Œäº§ç”Ÿäº†3526å°æ—¶çš„æ¸…æ™°è¯­éŸ³ï¼Œè¿›ä¸€æ­¥ç­›é€‰ä¸ºé€‚åˆTTSçš„1804å°æ—¶çš„é«˜è´¨é‡å­é›†ï¼Œæ¶µç›–äº†470å¤šåå‘è¨€äººçš„å£°éŸ³ã€‚ä¸ºäº†éªŒè¯æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹XTTSè¿›è¡Œäº†æ³¢æ–¯è¯­å¾®è°ƒï¼Œè¾¾åˆ°äº†è‡ªç„¶åº¦å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ä¸º3.6&#x2F;5å’Œæ¼”è®²è€…ç›¸ä¼¼æ€§å¹³å‡æ„è§å¾—åˆ†ï¼ˆSMOSï¼‰ä¸º4.0&#x2F;5ï¼Œè¯æ˜äº†ParsVoiceåœ¨è®­ç»ƒå¤šå‘è¨€äººTTSç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ParsVoiceæ˜¯æœ€å¤§ä¸”è´¨é‡æœ€é«˜çš„æ³¢æ–¯è¯­è¯­éŸ³æ•°æ®é›†ï¼Œæä¾›å¯ä¸ä¸»è¦è‹±è¯­è¯­æ–™åº“ç›¸æ¯”çš„å‘è¨€äººå¤šæ ·æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚ä¸ºäº†åŠ é€Ÿæ³¢æ–¯è¯­è¯­éŸ³æŠ€æœ¯çš„å‘å±•ï¼Œæˆ‘ä»¬å·²å…¬å¼€æä¾›å®Œæ•´çš„ParsVoiceæ•°æ®é›†ã€‚ParsVoiceæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice">https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10774v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªç ”ç©¶å›¢é˜Ÿä¸ºäº†è§£å†³æ³¢æ–¯è¯­è¯­éŸ³æ•°æ®é›†è§„æ¨¡è¾ƒå°çš„é—®é¢˜ï¼Œå¼€å‘äº†ParsVoiceï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åº”ç”¨è®¾è®¡çš„æœ€å¤§æ³¢æ–¯è¯­è¯­éŸ³è¯­æ–™åº“ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œè¯¥ç®¡é“å°†æœ‰å£°ä¹¦å†…å®¹è½¬åŒ–ä¸ºTTSå°±ç»ªæ•°æ®ï¼Œè¯¥ç®¡é“åŒ…æ‹¬åŸºäºBERTçš„å¥å­å®Œæˆæ£€æµ‹å™¨ã€ç”¨äºç²¾ç¡®éŸ³è§†é¢‘å¯¹é½çš„äºŒè¿›åˆ¶æœç´¢è¾¹ç•Œä¼˜åŒ–æ–¹æ³•ç­‰ã€‚ç»è¿‡å¤„ç†2000æœ¬æœ‰å£°ä¹¦åï¼Œè·å¾—äº†3526å°æ—¶æ¸…æ´è¯­éŸ³ï¼Œè¿›ä¸€æ­¥ç­›é€‰ä¸ºé€‚åˆTTSçš„1804å°æ—¶é«˜è´¨é‡å­é›†ï¼Œæ¶µç›–äº†è¶…è¿‡470åå‘è¨€äººã€‚ä¸ºäº†éªŒè¯æ•°æ®é›†ï¼Œä»–ä»¬å¯¹XTTSè¿›è¡Œäº†æ³¢æ–¯è¯­å¾®è°ƒï¼Œå®ç°äº†è‡ªç„¶åº¦å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ä¸º3.6&#x2F;5å’Œå‘éŸ³äººç›¸ä¼¼åº¦å¹³å‡æ„è§å¾—åˆ†ï¼ˆSMOSï¼‰ä¸º4.0&#x2F;5ï¼Œè¯æ˜äº†ParsVoiceåœ¨è®­ç»ƒå¤šå‘è¨€äººTTSç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ParsVoiceæ˜¯é«˜è´¨é‡æ³¢æ–¯è¯­è¯­éŸ³æ•°æ®é›†ä¸­æœ€å¤§çš„ï¼Œå¯æä¾›ä¸ä¸»è¦è‹±è¯­è¯­æ–™åº“ç›¸å½“çš„å‘éŸ³äººå¤šæ ·æ€§å’ŒéŸ³é¢‘è´¨é‡ï¼Œå¹¶å·²å…¬å¼€ä¾›åŠ é€Ÿæ³¢æ–¯è¯­è¯­éŸ³æŠ€æœ¯å¼€å‘ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ³¢æ–¯è¯­è¯­éŸ³æ•°æ®é›†ç›¸è¾ƒäºè‹±è¯­æ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼Œå¯¹å¼€å‘æ³¢æ–¯è¯­è¯­éŸ³æŠ€æœ¯é€ æˆé™åˆ¶ã€‚</li>
<li>æ¨å‡ºParsVoiceï¼Œæœ€å¤§æ³¢æ–¯è¯­è¯­éŸ³è¯­æ–™åº“ï¼Œä¸“ä¸ºTTSåº”ç”¨è®¾è®¡ã€‚</li>
<li>åˆ›å»ºè‡ªåŠ¨åŒ–ç®¡é“å¤„ç†éŸ³é¢‘ä¹¦ç±ï¼Œè½¬åŒ–ä¸ºTTSå°±ç»ªæ•°æ®ã€‚</li>
<li>ç®¡é“åŒ…å«å¤šé¡¹æŠ€æœ¯ç»„ä»¶ï¼Œå¦‚åŸºäºBERTçš„å¥å­å®Œæˆæ£€æµ‹å™¨ã€äºŒè¿›åˆ¶æœç´¢è¾¹ç•Œä¼˜åŒ–æ–¹æ³•ç­‰ã€‚</li>
<li>ParsVoiceåŒ…å«3526å°æ—¶æ¸…æ´è¯­éŸ³æ•°æ®ï¼Œè¿›ä¸€æ­¥ç­›é€‰ä¸º1804å°æ—¶é«˜è´¨é‡å­é›†ï¼Œæ¶µç›–470+å‘è¨€äººã€‚</li>
<li>XTTSæ¨¡å‹åœ¨ParsVoiceæ•°æ®é›†ä¸Šçš„è‡ªç„¶åº¦MOSä¸º3.6&#x2F;5ï¼Œå‘éŸ³äººç›¸ä¼¼æ€§SMOSä¸º4.0&#x2F;5ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f1f6fcd3d5e4b6b55910f691aee6126" align="middle">
<img src="https://picx.zhimg.com/v2-a5a3efa7071c2309dd8f8e0ba6f932f6" align="middle">
<img src="https://picx.zhimg.com/v2-da4c0d0f8bfe0513dc86129a650f3e55" align="middle">
<img src="https://picx.zhimg.com/v2-34b47d1451e52b7b15f10832d0321737" align="middle">
<img src="https://picx.zhimg.com/v2-c14863e970185f506434a36091802e07" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MRSAudio-A-Large-Scale-Multimodal-Recorded-Spatial-Audio-Dataset-with-Refined-Annotations"><a href="#MRSAudio-A-Large-Scale-Multimodal-Recorded-Spatial-Audio-Dataset-with-Refined-Annotations" class="headerlink" title="MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations"></a>MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations</h2><p><strong>Authors:Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Xintong Hu, Yu Zhang, Li Tang, Rui Yang, Han Wang, Zongbao Zhang, Yuhan Wang, Yixuan Chen, Hankun Xu, Ke Xu, Pengfei Fan, Zhetao Chen, Yanhao Yu, Qiange Huang, Fei Wu, Zhou Zhao</strong></p>
<p>Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space. Despite the critical role of spatial audio in immersive technologies such as VR&#x2F;AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding. To address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation. MRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios. The dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts. To demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection. Results show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research. Demos and dataset access are available at <a target="_blank" rel="noopener" href="https://mrsaudio.github.io/">https://mrsaudio.github.io</a>. </p>
<blockquote>
<p>äººç±»ä¾èµ–å¤šæ„Ÿå®˜èåˆæ¥æ„ŸçŸ¥ç©ºé—´ç¯å¢ƒï¼Œå…¶ä¸­å¬è§‰çº¿ç´¢ä½¿å£°éŸ³æºèƒ½åœ¨ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œå®šä½ã€‚å°½ç®¡ç©ºé—´éŸ³é¢‘åœ¨VR&#x2F;ARç­‰æ²‰æµ¸å¼æŠ€æœ¯ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„å¤šæ¨¡å¼æ•°æ®é›†ä»…æä¾›å•å£°é“éŸ³é¢‘ï¼Œè¿™é™åˆ¶äº†ç©ºé—´éŸ³é¢‘ç”Ÿæˆå’Œç†è§£çš„å‘å±•ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MRSAudioï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼ç©ºé—´éŸ³é¢‘æ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨ç©ºé—´éŸ³é¢‘ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„ç ”ç©¶ã€‚MRSAudioè·¨è¶Šå››ä¸ªç‹¬ç‰¹ç»„æˆéƒ¨åˆ†ï¼šMRSLifeã€MRSSpeechã€MRSMusicå’ŒMRSSingï¼Œæ¶µç›–å„ç§çœŸå®ä¸–ç•Œåœºæ™¯ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬åŒæ­¥çš„åŒè€³å’Œç¯ç»•å£°éŸ³é¢‘ã€å¤–å‘ä¸­å¿ƒå’Œå†…å‘ä¸­å¿ƒè§†é¢‘ã€è¿åŠ¨è½¨è¿¹ï¼Œä»¥åŠç²¾ç»†æ³¨é‡Šï¼Œå¦‚æ–‡æœ¬ã€éŸ³ç´ è¾¹ç•Œã€æ­Œè¯ã€ä¹è°±å’Œæç¤ºã€‚ä¸ºäº†å±•ç¤ºMRSAudioçš„å®ç”¨æ€§å’Œå¤šåŠŸèƒ½æ€§ï¼Œæˆ‘ä»¬å»ºç«‹äº†äº”ä¸ªåŸºæœ¬ä»»åŠ¡ï¼šéŸ³é¢‘å®šä½ã€ç©ºé—´æ–‡æœ¬è½¬è¯­éŸ³ã€ç©ºé—´æ­Œå£°åˆæˆã€ç©ºé—´éŸ³ä¹ç”Ÿæˆå’Œå£°éŸ³äº‹ä»¶å®šä½å’Œæ£€æµ‹ã€‚ç»“æœè¡¨æ˜ï¼ŒMRSAudioèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ç©ºé—´å»ºæ¨¡ï¼Œå¹¶æ”¯æŒå¹¿æ³›çš„ç©ºé—´éŸ³é¢‘ç ”ç©¶ã€‚æ¼”ç¤ºå’Œæ•°æ®é›†è®¿é—®è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://mrsaudio.github.io./">https://mrsaudio.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10396v2">PDF</a> 24 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººç±»ä¾èµ–å¤šæ„Ÿå®˜æ•´åˆæ¥æ„ŸçŸ¥ç©ºé—´ç¯å¢ƒï¼Œå¬è§‰çº¿ç´¢èƒ½ä½¿å£°éŸ³æºåœ¨ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œå®šä½ã€‚å°½ç®¡ç©ºé—´éŸ³é¢‘åœ¨è™šæ‹Ÿç°å®&#x2F;å¢å¼ºç°å®ç­‰æ²‰æµ¸å¼æŠ€æœ¯ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰çš„å¤šæ¨¡æ€æ•°æ®é›†å¤§å¤šä¸ºå•å£°é“éŸ³é¢‘ï¼Œè¿™é™åˆ¶äº†ç©ºé—´éŸ³é¢‘ç”Ÿæˆå’Œç†è§£çš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MRSAudioå¤§è§„æ¨¡å¤šæ¨¡æ€ç©ºé—´éŸ³é¢‘æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›ç©ºé—´éŸ³é¢‘ç†è§£å’Œç”Ÿæˆçš„ç ”ç©¶ã€‚MRSAudioåŒ…æ‹¬å››ä¸ªç‹¬ç‰¹ç»„æˆéƒ¨åˆ†ï¼šMRSLifeã€MRSSpeechã€MRSMusicå’ŒMRSSingï¼Œæ¶µç›–å„ç§çœŸå®åœºæ™¯ã€‚æ•°æ®é›†åŒ…æ‹¬åŒæ­¥çš„åŒè€³å’Œç¯ç»•å£°éŸ³é¢‘ã€å¤–å‘ä¸­å¿ƒå’Œå†…å‘ä¸­å¿ƒè§†é¢‘ã€è¿åŠ¨è½¨è¿¹ä»¥åŠç²¾ç»†æ³¨é‡Šï¼Œå¦‚æ–‡æœ¬ã€éŸ³ç´ è¾¹ç•Œã€æ­Œè¯ã€åˆ†æ•°å’Œæç¤ºã€‚ä¸ºäº†å±•ç¤ºMRSAudioçš„å®ç”¨æ€§å’Œé€šç”¨æ€§ï¼Œæˆ‘ä»¬å»ºç«‹äº†äº”ä¸ªåŸºæœ¬ä»»åŠ¡ï¼šéŸ³é¢‘å®šä½ã€ç©ºé—´æ–‡æœ¬è½¬è¯­éŸ³ã€ç©ºé—´æ­Œå£°åˆæˆã€ç©ºé—´éŸ³ä¹ç”Ÿæˆä»¥åŠå£°éŸ³äº‹ä»¶å®šä½å’Œæ£€æµ‹ã€‚ç»“æœè¡¨æ˜ï¼ŒMRSAudioèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ç©ºé—´å»ºæ¨¡ï¼Œå¹¶æ”¯æŒå¹¿æ³›çš„ç©ºé—´éŸ³é¢‘ç ”ç©¶ã€‚ç›¸å…³æ¼”ç¤ºå’Œæ•°æ®é›†è®¿é—®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://mrsaudio.github.io./">https://mrsaudio.github.ioã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººç±»ä¾èµ–å¤šæ„Ÿå®˜æ•´åˆæ„ŸçŸ¥ç©ºé—´ç¯å¢ƒï¼Œå¬è§‰åœ¨å®šä½å£°æºæ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å¤šä¸ºå•å£°é“éŸ³é¢‘ï¼Œé™åˆ¶äº†ç©ºé—´éŸ³é¢‘ç”Ÿæˆå’Œç†è§£çš„ç ”ç©¶è¿›å±•ã€‚</li>
<li>æ¨å‡ºMRSAudioæ•°æ®é›†ï¼ŒåŒ…å«åŒæ­¥å¤šæ¨¡æ€æ•°æ®ï¼ˆåŒè€³éŸ³é¢‘ã€ç¯ç»•å£°éŸ³é¢‘ç­‰ï¼‰ï¼Œé€‚ç”¨äºå¤šç§çœŸå®åœºæ™¯ã€‚</li>
<li>MRSAudioæ¶µç›–å¤šæ ·åŒ–ç²¾ç»†æ³¨é‡Šï¼Œä¸ºå¤šç§ä»»åŠ¡æä¾›ä¸°å¯Œæ•°æ®ã€‚</li>
<li>å»ºç«‹äº”ä¸ªåŸºæœ¬ä»»åŠ¡ä»¥å±•ç¤ºMRSAudioçš„å®ç”¨æ€§å’Œé€šç”¨æ€§ï¼ŒåŒ…æ‹¬éŸ³é¢‘å®šä½ã€ç©ºé—´æ–‡æœ¬è½¬è¯­éŸ³ç­‰ã€‚</li>
<li>MRSAudioèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ç©ºé—´å»ºæ¨¡ï¼Œå¯¹ç©ºé—´éŸ³é¢‘ç ”ç©¶æœ‰å¹¿æ³›æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9894cf9eb14cf8a28f40f4f68e9f90dd" align="middle">
<img src="https://picx.zhimg.com/v2-a482ecd4f7832d0ace316af507c83a70" align="middle">
<img src="https://picx.zhimg.com/v2-98c408eb971480b0f405a8a52bf40f2a" align="middle">
<img src="https://picx.zhimg.com/v2-b3bf3278b24d675e8c64d7860212474b" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Unifying-Tree-Search-Algorithm-and-Reward-Design-for-LLM-Reasoning-A-Survey"><a href="#Unifying-Tree-Search-Algorithm-and-Reward-Design-for-LLM-Reasoning-A-Survey" class="headerlink" title="Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A   Survey"></a>Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A   Survey</h2><p><strong>Authors:Jiaqi Wei, Xiang Zhang, Yuejin Yang, Wenxuan Huang, Juntai Cao, Sheng Xu, Xiang Zhuang, Zhangyang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Chenyu You, Wanli Ouyang, Siqi Sun</strong></p>
<p>Deliberative tree search is a cornerstone of modern Large Language Model (LLM) research, driving the pivot from brute-force scaling toward algorithmic efficiency. This single paradigm unifies two critical frontiers: \textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve hard problems, and \textbf{Self-Improvement}, which uses search-generated data to durably enhance model parameters. However, this burgeoning field is fragmented and lacks a common formalism, particularly concerning the ambiguous role of the reward signal â€“ is it a transient heuristic or a durable learning target? This paper resolves this ambiguity by introducing a unified framework that deconstructs search algorithms into three core components: the \emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition Function}. We establish a formal distinction between transient \textbf{Search Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for Self-Improvement. Building on this formalism, we introduce a component-centric taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward more systematic progress in creating autonomous, self-improving agents. </p>
<blockquote>
<p>å†³ç­–æ ‘æœç´¢æ˜¯ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶çš„åŸºçŸ³ï¼Œæ¨åŠ¨äº†ä»ç²—æš´æ‰©å±•è½¬å‘ç®—æ³•æ•ˆç‡çš„è½¬å˜ã€‚è¿™ä¸€å•ä¸€èŒƒå¼ç»Ÿä¸€äº†ä¸¤ä¸ªå…³é”®å‰æ²¿é¢†åŸŸï¼š<strong>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰</strong>ï¼Œå®ƒæŒ‰éœ€éƒ¨ç½²è®¡ç®—æ¥è§£å†³éš¾é¢˜ï¼Œä»¥åŠ<strong>è‡ªæˆ‘æ”¹è¿›</strong>ï¼Œå®ƒä½¿ç”¨æœç´¢ç”Ÿæˆçš„æ•°æ®æ¥æŒä¹…åœ°å¢å¼ºæ¨¡å‹å‚æ•°ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ–°å…´é¢†åŸŸæ˜¯é›¶ç¢çš„ï¼Œç¼ºä¹é€šç”¨çš„å½¢å¼åŒ–ç†è®ºï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±ä¿¡å·çš„ä¸æ˜ç¡®è§’è‰²æ–¹é¢â€”â€”å®ƒæ˜¯çŸ­æš‚çš„å¯å‘å¼æ–¹æ³•è¿˜æ˜¯æŒä¹…çš„å­¦ä¹ ç›®æ ‡ï¼Ÿæœ¬æ–‡é€šè¿‡å¼•å…¥ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶æ¥è§£å†³è¿™ä¸€æ¨¡ç³Šæ€§ï¼Œè¯¥æ¡†æ¶å°†æœç´¢ç®—æ³•åˆ†è§£ä¸ºä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š<strong>æœç´¢æœºåˆ¶ã€å¥–åŠ±å…¬å¼å’Œè½¬æ¢å‡½æ•°</strong>ã€‚æˆ‘ä»¬æ­£å¼åŒºåˆ†äº†ç”¨äºTTSçš„çŸ­æš‚<strong>æœç´¢æŒ‡å¯¼</strong>å’Œç”¨äºè‡ªæˆ‘æ”¹è¿›çš„æŒä¹…<strong>å‚æ•°å¥–åŠ±å»ºæ¨¡</strong>ã€‚åŸºäºè¿™ä¸€å½¢å¼åŒ–ç†è®ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥ç»„ä»¶ä¸ºä¸­å¿ƒçš„åˆ†ç±»å­¦ï¼Œç»¼åˆäº†å½“å‰çš„ç ”ç©¶çŠ¶å†µï¼Œå¹¶ä¸ºåœ¨åˆ›å»ºè‡ªä¸»ã€è‡ªæˆ‘æ”¹è¿›çš„æ™ºèƒ½ä½“æ–¹é¢å–å¾—æ›´æœ‰ç³»ç»Ÿçš„è¿›æ­¥ç»˜åˆ¶äº†ç ”ç©¶è·¯çº¿å›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09988v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬è®¨è®ºäº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶ä¸­çš„å®¡æ…æ ‘æœç´¢çš„é‡è¦æ€§ï¼Œè¯¥èŒƒå¼èåˆäº†æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰å’Œè‡ªæˆ‘æ”¹è¿›ä¸¤ä¸ªå…³é”®å‰æ²¿é¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯å…³äºå¥–åŠ±ä¿¡å·çš„æ¨¡ç³Šè§’è‰²ï¼ˆæ˜¯çŸ­æš‚çš„å¯å‘å¼è¿˜æ˜¯æŒä¹…çš„å­¦ä¹ ç›®æ ‡ï¼Ÿï¼‰çš„é—®é¢˜ï¼Œè¯¥é¢†åŸŸçš„å‘å±•å­˜åœ¨ç¢ç‰‡åŒ–ç°è±¡ã€‚æœ¬æ–‡è§£å†³è¿™ä¸€æ¨¡ç³Šæ€§ï¼Œå¼•å…¥ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†æœç´¢ç®—æ³•åˆ†è§£ä¸ºä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæœç´¢æœºåˆ¶ã€å¥–åŠ±å…¬å¼å’Œè½¬æ¢å‡½æ•°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡å»ºç«‹å¯¹TTSçš„çŸ­æš‚æ€§æœç´¢æŒ‡å¯¼ä¸è‡ªæˆ‘æ”¹è¿›çš„æŒä¹…å‚æ•°å¥–åŠ±å»ºæ¨¡ä¹‹é—´çš„æ­£å¼åŒºåˆ«ã€‚ä¸ºæœªæ¥çš„ç ”ç©¶å¼€è¾Ÿäº†ä¸€æ¡é“è·¯ï¼Œæœç€åˆ›å»ºè‡ªä¸»ã€è‡ªæˆ‘æ”¹è¿›çš„æ™ºèƒ½ä½“æ–¹é¢å–å¾—æ›´æœ‰ç³»ç»Ÿçš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®¡æ…æ ‘æœç´¢æ˜¯ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ç ”ç©¶çš„åŸºçŸ³ï¼Œèåˆäº†æµ‹è¯•æ—¶é—´ç¼©æ”¾å’Œè‡ªæˆ‘æ”¹è¿›ä¸¤ä¸ªå…³é”®é¢†åŸŸã€‚</li>
<li>ç¼ºä¹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ˜¯è¯¥é¢†åŸŸå‘å±•çš„ä¸»è¦é—®é¢˜ä¹‹ä¸€ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±ä¿¡å·çš„æ¨¡ç³Šè§’è‰²æ–¹é¢ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå°†æœç´¢ç®—æ³•åˆ†è§£ä¸ºä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæœç´¢æœºåˆ¶ã€å¥–åŠ±å…¬å¼å’Œè½¬æ¢å‡½æ•°ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹äº†å¯¹æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„çŸ­æš‚æ€§æœç´¢æŒ‡å¯¼å’Œè‡ªæˆ‘æ”¹è¿›çš„æŒä¹…å‚æ•°å¥–åŠ±å»ºæ¨¡ä¹‹é—´çš„æ­£å¼åŒºåˆ«ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºç†è§£æœç´¢ç®—æ³•ä¸­å„ä¸ªç»„ä»¶çš„ä½œç”¨å’Œç›¸äº’å…³ç³»ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</li>
<li>åŸºäºè¯¥æ¡†æ¶ï¼Œæœ¬æ–‡è¿›è¡Œäº†æœ€æ–°çš„å…ˆè¿›æŠ€æœ¯çš„æ¢³ç†å’Œç ”ç©¶è·¯çº¿å›¾è§„åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1851c40c4f0f43168d87335b5c952cdf" align="middle">
<img src="https://picx.zhimg.com/v2-534e32e16297156ea74ab5bb77911b20" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="O-O-VC-Synthetic-Data-Driven-One-to-One-Alignment-for-Any-to-Any-Voice-Conversion"><a href="#O-O-VC-Synthetic-Data-Driven-One-to-One-Alignment-for-Any-to-Any-Voice-Conversion" class="headerlink" title="O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice   Conversion"></a>O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice   Conversion</h2><p><strong>Authors:Huu Tuong Tu, Huan Vu, cuong tien nguyen, Dien Hy Ngo, Nguyen Thi Thu Trang</strong></p>
<p>Traditional voice conversion (VC) methods typically attempt to separate speaker identity and linguistic information into distinct representations, which are then combined to reconstruct the audio. However, effectively disentangling these factors remains challenging, often leading to information loss during training. In this paper, we propose a new approach that leverages synthetic speech data generated by a high-quality, pretrained multispeaker text-to-speech (TTS) model. Specifically, synthetic data pairs that share the same linguistic content but differ in speaker identity are used as input-output pairs to train the voice conversion model. This enables the model to learn a direct mapping between source and target voices, effectively capturing speaker-specific characteristics while preserving linguistic content. Additionally, we introduce a flexible training strategy for any-to-any voice conversion that generalizes well to unseen speakers and new languages, enhancing adaptability and performance in zero-shot scenarios. Our experiments show that our proposed method achieves a 16.35% relative reduction in word error rate and a 5.91% improvement in speaker cosine similarity, outperforming several state-of-the-art methods. Voice conversion samples can be accessed at: <a target="_blank" rel="noopener" href="https://oovc-emnlp-2025.github.io/">https://oovc-emnlp-2025.github.io/</a> </p>
<blockquote>
<p>ä¼ ç»Ÿè¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ–¹æ³•é€šå¸¸å°è¯•å°†è¯´è¯äººèº«ä»½å’Œè¯­éŸ³ä¿¡æ¯åˆ†ç¦»æˆä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼Œç„¶åå°†å®ƒä»¬ç»“åˆèµ·æ¥é‡å»ºéŸ³é¢‘ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°è§£å¼€è¿™äº›å› ç´ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸ä¼šå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç”±é«˜è´¨é‡é¢„è®­ç»ƒå¤šè¯´è¯äººæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç”Ÿæˆçš„äººé€ è¯­éŸ³æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œäººé€ æ•°æ®å¯¹å…±äº«ç›¸åŒçš„è¯­è¨€å†…å®¹ä½†è¯´è¯äººèº«ä»½ä¸åŒï¼Œè¢«ç”¨ä½œè¾“å…¥å’Œè¾“å‡ºæ¥è®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³ä¹‹é—´çš„ç›´æ¥æ˜ å°„ï¼Œæœ‰æ•ˆåœ°æ•æ‰è¯´è¯äººçš„ç‰¹å®šç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çµæ´»çš„ä»»ä½•åˆ°ä»»ä½•è¯­éŸ³è½¬æ¢è®­ç»ƒç­–ç•¥ï¼Œå®ƒå¯¹äºæœªè§è¿‡çš„è¯´è¯äººå’Œæ–°è¯­è¨€å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¢å¼ºäº†é›¶æ ·æœ¬åœºæ™¯ä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å®ç°äº†å•è¯é”™è¯¯ç‡ç›¸å¯¹é™ä½16.35%ï¼Œè¯´è¯äººä½™å¼¦ç›¸ä¼¼æ€§æé«˜5.91%ï¼Œä¼˜äºå‡ ç§æœ€æ–°æ–¹æ³•ã€‚è¯­éŸ³è½¬æ¢æ ·æœ¬å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://oovc-emnlp-2025.github.io/">https://oovc-emnlp-2025.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09061v1">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong><br>è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰çš„ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å°è¯•å°†è¯´è¯äººèº«ä»½å’Œè¯­è¨€ä¿¡æ¯åˆ†ç¦»æˆä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼Œç„¶åå†ç»“åˆè¿™äº›è¡¨ç¤ºå½¢å¼æ¥é‡å»ºéŸ³é¢‘ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°åˆ†ç¦»è¿™äº›å› ç´ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¾€å¾€ä¼šå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨é«˜è´¨é‡é¢„è®­ç»ƒå¤šè¯´è¯äººæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç”Ÿæˆåˆæˆè¯­éŸ³æ•°æ®ã€‚åˆæˆæ•°æ®å¯¹å…·æœ‰ç›¸åŒçš„è¯­è¨€å†…å®¹ä½†è¯´è¯äººèº«ä»½ä¸åŒï¼Œç”¨ä½œè¯­éŸ³è½¬æ¢æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºå¯¹ï¼Œè®­ç»ƒæ¨¡å‹å­¦ä¹ æºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³ä¹‹é—´çš„ç›´æ¥æ˜ å°„å…³ç³»ï¼Œæœ‰æ•ˆæ•æ‰è¯´è¯äººçš„ç‰¹å¾åŒæ—¶ä¿ç•™è¯­è¨€å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çµæ´»çš„ä»»ä½•åˆ°ä»»ä½•è¯­éŸ³è½¬æ¢è®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°é€‚åº”æœªè§è¿‡çš„è¯´è¯äººå’Œæ–°è¯­è¨€ï¼Œæé«˜äº†é›¶æ ·æœ¬åœºæ™¯ä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ç›¸å¯¹é™ä½è¯é”™è¯¯ç‡16.35%ï¼Œè¯´è¯äººä½™å¼¦ç›¸ä¼¼æ€§æé«˜5.91%ï¼Œä¼˜äºå‡ ç§æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿè¯­éŸ³è½¬æ¢æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜æ˜¯æœ‰æ•ˆåœ°åˆ†ç¦»è¯´è¯äººèº«ä»½å’Œè¯­è¨€ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè¯´è¯äººæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç”Ÿæˆçš„åˆæˆè¯­éŸ³æ•°æ®æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>åˆæˆæ•°æ®å¯¹å…·æœ‰ç›¸åŒçš„è¯­è¨€å†…å®¹ä½†ä¸åŒçš„è¯´è¯äººèº«ä»½ï¼Œç”¨äºè®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å­¦ä¹ æºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³ä¹‹é—´çš„ç›´æ¥æ˜ å°„å…³ç³»ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€å†…å®¹å¹¶æ•æ‰è¯´è¯äººçš„ç‰¹å¾ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§çµæ´»çš„ä»»ä½•åˆ°ä»»ä½•è¯­éŸ³è½¬æ¢è®­ç»ƒç­–ç•¥ï¼Œé€‚åº”æœªè§è¿‡çš„è¯´è¯äººå’Œæ–°è¯­è¨€ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯é”™è¯¯ç‡å’Œè¯´è¯äººä½™å¼¦ç›¸ä¼¼æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0bc629c568b5fbfe0fbf155a09bd4f0" align="middle">
<img src="https://picx.zhimg.com/v2-df7811b536b3398043b4121e16997782" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models"><a href="#Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models" class="headerlink" title="Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models"></a>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models</h2><p><strong>Authors:Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</strong></p>
<p>Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: <a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</a> </p>
<blockquote>
<p>è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€å…·æŒ‘æˆ˜æ€§çš„å‰æ²¿æ–¹å‘ï¼Œå®ƒè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€é•¿æœŸä¾èµ–å…³ç³»å’Œå¤šæ¨¡æ€è¯æ®ã€‚æœ€è¿‘å‡ºç°çš„è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVideo-LMMsï¼‰ï¼Œé›†æˆäº†è§†è§‰ç¼–ç å™¨å’ŒåŸºäºè§£ç å™¨çš„å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ä»åŸºæœ¬æ„ŸçŸ¥ç³»ç»Ÿè½¬å˜ä¸ºå…ˆè¿›æ¨ç†å¼•æ“çš„å…³é”®é˜¶æ®µâ€”â€”åè®­ç»ƒï¼Œåœ¨æ–‡çŒ®ä¸­ä»ç„¶åˆ†æ•£ã€‚è¿™ç¯‡ç»¼è¿°æä¾›äº†å¯¹è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åè®­ç»ƒæ–¹æ³•çš„é¦–æ¬¡å…¨é¢ç ”ç©¶ï¼Œæ¶µç›–äº†ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼šé€šè¿‡æ€ç»´é“¾è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€åŸºäºå¯éªŒè¯ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠé€šè¿‡å¢å¼ºæ¨ç†è®¡ç®—è¿›è¡Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–åˆ†ç±»æ³•ï¼Œé˜æ˜äº†è¿™äº›æ–¹æ³•çš„ä½œç”¨ã€ç›¸äº’å…³è”ä»¥åŠé’ˆå¯¹è§†é¢‘çš„ç‰¹å®šé€‚åº”ï¼Œåº”å¯¹è¯¸å¦‚æ—¶é—´å®šä½ã€æ—¶ç©ºå®šä½ã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®èåˆç­‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ç»¼åˆäº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ï¼ŒåŒæ—¶ç¡®å®šäº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æ•´ç†äº†é‡è¦çš„åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¾¿å¯¹åè®­ç»ƒçš„æœ‰æ•ˆæ€§è¿›è¡Œä¸¥æ ¼è¯„ä¼°ã€‚æœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œä»¥æ¨è¿›è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ã€‚æ›´å¤šèµ„æºå’Œæ›´æ–°ä¿¡æ¯è¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training]">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05034v4">PDF</a> The 1st version</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€å…·æŒ‘æˆ˜æ€§çš„å‰æ²¿è¯¾é¢˜ï¼Œéœ€è¦æ¨¡å‹æ¨ç†å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€é•¿æœŸä¾èµ–æ€§å’Œå¤šæ¨¡æ€è¯æ®ã€‚è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVideo-LMMsï¼‰çš„å‡ºç°ï¼Œé€šè¿‡æ•´åˆè§†è§‰ç¼–ç å™¨å’ŒåŸºäºè¯­è¨€æ¨¡å‹çš„å¼ºå¤§è§£ç å™¨ï¼Œå±•ç°äº†è§†é¢‘ç†è§£ä»»åŠ¡çš„å“è¶Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ä»åŸºæœ¬æ„ŸçŸ¥ç³»ç»Ÿè½¬å˜ä¸ºå…ˆè¿›æ¨ç†å¼•æ“çš„åè®­ç»ƒé˜¶æ®µï¼Œåœ¨æ–‡çŒ®ä¸­ä»ç„¶åˆ†æ•£ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢è°ƒæŸ¥äº†Video-LMMsçš„åè®­ç»ƒæ–¹æ³•è®ºï¼ŒåŒ…æ‹¬ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼šä»¥æ€ç»´é“¾è¿›è¡Œçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ä»¥å¯éªŒè¯ç›®æ ‡è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠé€šè¿‡å¢å¼ºæ¨ç†è®¡ç®—è¿›è¡Œçš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ã€‚æœ¬æ–‡å‘ˆç°äº†ä¸€ä¸ªç»“æ„åŒ–åˆ†ç±»æ³•ï¼Œé˜æ˜äº†è¿™äº›æ–¹æ³•çš„æŠ€æœ¯è§’è‰²ã€ç›¸äº’å…³è”ä»¥åŠé’ˆå¯¹è§†é¢‘çš„ç‰¹å®šé€‚åº”ï¼Œåº”å¯¹ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´å®šä½ã€æ—¶ç©ºå®šä½ã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®æ•´åˆç­‰ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ç»¼åˆäº†å…³é”®è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ï¼ŒåŒæ—¶ç¡®å®šäº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ•´ç†äº†é‡è¦çš„åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¿ƒè¿›å¯¹åè®­ç»ƒæ•ˆæœçš„ä¸¥æ ¼è¯„ä¼°ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›ä¸€ä¸ªæ¨è¿›Video-LMMèƒ½åŠ›çš„ç»Ÿä¸€æ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€é•¿æœŸä¾èµ–æ€§å’Œå¤šæ¨¡æ€è¯æ®ã€‚</li>
<li>Video-Large Multimodal Modelsï¼ˆVideo-LMMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œé€šè¿‡æ•´åˆè§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åè®­ç»ƒé˜¶æ®µå¯¹äºå°†æ¨¡å‹ä»åŸºæœ¬æ„ŸçŸ¥ç³»ç»Ÿè½¬å˜ä¸ºå…ˆè¿›æ¨ç†å¼•æ“è‡³å…³é‡è¦ã€‚</li>
<li>åè®­ç»ƒæ–¹æ³•è®ºåŒ…æ‹¬ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼šæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ã€‚</li>
<li>è¿™äº›æ–¹æ³•é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´å®šä½ã€æ—¶ç©ºå®šä½ã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®æ•´åˆç­‰ã€‚</li>
<li>ç³»ç»Ÿåˆ†æäº†åè®­ç»ƒæ–¹æ³•çš„ä»£è¡¨æ¡ˆä¾‹ï¼Œå¹¶æ€»ç»“äº†å…³é”®è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ã€‚</li>
<li>å­˜åœ¨å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ€§èƒ½ä¼˜åŒ–ç­‰å…³é”®å¼€æ”¾æŒ‘æˆ˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b50a53356bec6f9e47e9cb0c07d61095" align="middle">
<img src="https://picx.zhimg.com/v2-85a61878c24e7a8ad77236b533d85f14" align="middle">
<img src="https://picx.zhimg.com/v2-df7301cfa07ebc24146ffb38a5a4a919" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MSR-Codec-A-Low-Bitrate-Multi-Stream-Residual-Codec-for-High-Fidelity-Speech-Generation-with-Information-Disentanglement"><a href="#MSR-Codec-A-Low-Bitrate-Multi-Stream-Residual-Codec-for-High-Fidelity-Speech-Generation-with-Information-Disentanglement" class="headerlink" title="MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement"></a>MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement</h2><p><strong>Authors:Jingyu Li, Guangyan Zhang, Zhen Ye, Yiwen Guo</strong></p>
<p>Audio codecs are a critical component of modern speech generation systems. This paper introduces a low-bitrate, multi-scale residual codec that encodes speech into four distinct streams: semantic, timbre, prosody, and residual. This architecture achieves high-fidelity speech reconstruction at competitive low bitrates while demonstrating an inherent ability for information disentanglement. We construct a two-stage language model for text-to-speech (TTS) synthesis using this codec, which, despite its lightweight design and minimal data requirements, achieves a state-of-the-art Word Error Rate (WER) and superior speaker similarity compared to several larger models. Furthermore, the codecâ€™s design proves highly effective for voice conversion, enabling independent manipulation of speaker timbre and prosody. Our inference code, pre-trained models, and audio samples are available at <a target="_blank" rel="noopener" href="https://github.com/herbertLJY/MSRCodec">https://github.com/herbertLJY/MSRCodec</a>. </p>
<blockquote>
<p>éŸ³é¢‘ç¼–ç æ˜¯ç°ä»£è¯­éŸ³ç”Ÿæˆç³»ç»Ÿçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä½æ¯”ç‰¹ç‡çš„å¤šå°ºåº¦æ®‹å·®ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å°†è¯­éŸ³ç¼–ç ä¸ºå››ä¸ªä¸åŒçš„æµï¼šè¯­ä¹‰ã€éŸ³è‰²ã€è¯­è°ƒå’Œæ®‹å·®ã€‚è¯¥æ¶æ„åœ¨å…·æœ‰ç«äº‰åŠ›çš„ä½æ¯”ç‰¹ç‡ä¸‹å®ç°äº†é«˜ä¿çœŸè¯­éŸ³é‡å»ºï¼Œå¹¶å±•ç¤ºäº†å›ºæœ‰çš„ä¿¡æ¯è§£çº ç¼ èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤ç¼–ç å™¨æ„å»ºäº†ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆçš„ä¸¤é˜¶æ®µè¯­è¨€æ¨¡å‹ï¼Œå°½ç®¡å…¶è®¾è®¡è½»å·§ä¸”å¯¹æ•°æ®è¦æ±‚è¾ƒä½ï¼Œä½†ä»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå‡ºè‰²çš„è¯´è¯äººç›¸ä¼¼æ€§ï¼Œä¼˜äºè®¸å¤šæ›´å¤§çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç¼–ç å™¨çš„è®¾è®¡å¯¹äºè¯­éŸ³è½¬æ¢éå¸¸æœ‰æ•ˆï¼Œèƒ½å¤Ÿå®ç°è¯´è¯äººçš„éŸ³è‰²å’Œè¯­è°ƒç‹¬ç«‹æ“çºµã€‚æˆ‘ä»¬çš„æ¨ç†ä»£ç ã€é¢„è®­ç»ƒæ¨¡å‹å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/herbertLJY/MSRCodec%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/herbertLJY/MSRCodecä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13068v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä½æ¯”ç‰¹ç‡çš„å¤šå°ºåº¦æ®‹å·®éŸ³é¢‘ç¼–ç æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†è¯­éŸ³åˆ†ä¸ºè¯­ä¹‰ã€éŸ³è‰²ã€è¯­è°ƒå’Œæ®‹å·®å››ä¸ªç‹¬ç«‹æµè¿›è¡Œç¼–ç ã€‚è¯¥æŠ€æœ¯å®ç°äº†åœ¨ä½æ¯”ç‰¹ç‡ä¸‹çš„é«˜ä¿çœŸè¯­éŸ³é‡å»ºï¼Œå¹¶å±•ç¤ºäº†å†…åœ¨çš„ä¿¡æ¯åˆ†ç¦»èƒ½åŠ›ã€‚åˆ©ç”¨è¿™ç§ç¼–ç æŠ€æœ¯æ„å»ºçš„ä¸¤é˜¶æ®µè¯­éŸ³åˆæˆè¯­è¨€æ¨¡å‹ï¼Œåœ¨è½»é‡çº§è®¾è®¡å’Œè¾ƒå°‘æ•°æ®éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯é”™è¯¯ç‡ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒé«˜çš„å‘éŸ³äººç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç¼–ç æŠ€æœ¯å¯¹äºè¯­éŸ³è½¬æ¢éå¸¸æœ‰æ•ˆï¼Œèƒ½å¤Ÿç‹¬ç«‹åœ°æ“ä½œå‘éŸ³äººçš„éŸ³è‰²å’Œè¯­è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ä½æ¯”ç‰¹ç‡çš„å¤šå°ºåº¦æ®‹å·®éŸ³é¢‘ç¼–ç æŠ€æœ¯ã€‚</li>
<li>ç¼–ç æŠ€æœ¯å°†è¯­éŸ³åˆ†ä¸ºè¯­ä¹‰ã€éŸ³è‰²ã€è¯­è°ƒå’Œæ®‹å·®å››ä¸ªç‹¬ç«‹æµã€‚</li>
<li>æŠ€æœ¯å®ç°äº†é«˜ä¿çœŸè¯­éŸ³é‡å»ºï¼Œå³ä½¿åœ¨ä½æ¯”ç‰¹ç‡ä¸‹ã€‚</li>
<li>å±•ç¤ºå†…åœ¨çš„ä¿¡æ¯åˆ†ç¦»èƒ½åŠ›ã€‚</li>
<li>ä¸¤é˜¶æ®µè¯­éŸ³åˆæˆè¯­è¨€æ¨¡å‹è¾¾åˆ°äº†å…ˆè¿›çš„è¯é”™è¯¯ç‡ã€‚</li>
<li>æ¨¡å‹åœ¨è½»é‡çº§è®¾è®¡å’Œå°‘é‡æ•°æ®éœ€æ±‚ä¸‹è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-319e2a2eea5dca5c2f5247d9707a8fda" align="middle">
<img src="https://picx.zhimg.com/v2-dc1df9ea0bc02d342ab02553890f3c27" align="middle">
<img src="https://picx.zhimg.com/v2-abe2209b6ef5a380449c8e746686e613" align="middle">
<img src="https://picx.zhimg.com/v2-0a14c31c569c97f5704ecf765bc56901" align="middle">
<img src="https://picx.zhimg.com/v2-f8bac2eeababfb358b9e0c21e4f6e74f" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="NEP-Autoregressive-Image-Editing-via-Next-Editing-Token-Prediction"><a href="#NEP-Autoregressive-Image-Editing-via-Next-Editing-Token-Prediction" class="headerlink" title="NEP: Autoregressive Image Editing via Next Editing Token Prediction"></a>NEP: Autoregressive Image Editing via Next Editing Token Prediction</h2><p><strong>Authors:Huimin Wu, Xiaojian Ma, Haozhe Zhao, Yanpeng Zhao, Qing Li</strong></p>
<p>Text-guided image editing involves modifying a source image based on a language instruction and, typically, requires changes to only small local regions. However, existing approaches generate the entire target image rather than selectively regenerate only the intended editing areas. This results in (1) unnecessary computational costs and (2) a bias toward reconstructing non-editing regions, which compromises the quality of the intended edits. To resolve these limitations, we propose to formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation, where only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas. To enable any-region editing, we propose to pre-train an any-order autoregressive text-to-image (T2I) model. Once trained, it is capable of zero-shot image editing and can be easily adapted to NEP for image editing, which achieves a new state-of-the-art on widely used image editing benchmarks. Moreover, our model naturally supports test-time scaling (TTS) through iteratively refining its generation in a zero-shot manner. The project page is: <a target="_blank" rel="noopener" href="https://nep-bigai.github.io/">https://nep-bigai.github.io/</a> </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ˜¯æ ¹æ®è¯­è¨€æŒ‡ä»¤ä¿®æ”¹æºå›¾åƒï¼Œé€šå¸¸åªéœ€è¦å¯¹è¾ƒå°çš„å±€éƒ¨åŒºåŸŸè¿›è¡Œä¿®æ”¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¼šç”Ÿæˆæ•´ä¸ªç›®æ ‡å›¾åƒï¼Œè€Œä¸æ˜¯æœ‰é€‰æ‹©åœ°é‡æ–°ç”Ÿæˆé¢„æœŸçš„ç¼–è¾‘åŒºåŸŸã€‚è¿™å¯¼è‡´äº†ï¼ˆ1ï¼‰ä¸å¿…è¦çš„è®¡ç®—æˆæœ¬ï¼›ï¼ˆ2ï¼‰å¯¹éç¼–è¾‘åŒºåŸŸçš„é‡å»ºå­˜åœ¨åè§ï¼Œä»è€ŒæŸå®³äº†é¢„æœŸçš„ç¼–è¾‘è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºå°†å›¾åƒç¼–è¾‘åˆ¶å®šä¸ºåŸºäºè‡ªå›å½’å›¾åƒç”Ÿæˆçš„ä¸‹ä¸€ä¸ªç¼–è¾‘ä»¤ç‰Œé¢„æµ‹ï¼ˆNEPï¼‰ï¼Œå…¶ä¸­ä»…é‡æ–°ç”Ÿæˆéœ€è¦ç¼–è¾‘çš„åŒºåŸŸï¼Œä»è€Œé¿å…å¯¹éç¼–è¾‘åŒºåŸŸçš„æ„å¤–ä¿®æ”¹ã€‚ä¸ºäº†å®ç°ä»»ä½•åŒºåŸŸçš„ç¼–è¾‘ï¼Œæˆ‘ä»¬æå‡ºé¢„è®­ç»ƒä¸€ä¸ªä»»æ„é¡ºåºçš„è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œå®ƒå°±èƒ½å¤Ÿè¿›è¡Œé›¶æ ·æœ¬å›¾åƒç¼–è¾‘ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°é€‚åº”NEPç”¨äºå›¾åƒç¼–è¾‘ï¼Œåœ¨å¹¿æ³›ä½¿ç”¨çš„å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è‡ªç„¶åœ°æ”¯æŒæµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰ï¼Œé€šè¿‡ä»¥é›¶æ ·æœ¬çš„æ–¹å¼è¿­ä»£ä¼˜åŒ–å…¶ç”Ÿæˆã€‚é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://nep-bigai.github.io/">https://nep-bigai.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06044v2">PDF</a> The project page is: <a target="_blank" rel="noopener" href="https://nep-bigai.github.io/">https://nep-bigai.github.io/</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ˜¯æ ¹æ®è¯­è¨€æŒ‡ä»¤ä¿®æ”¹æºå›¾åƒï¼Œé€šå¸¸åªéœ€è¦æ”¹å˜å°éƒ¨åˆ†å±€éƒ¨åŒºåŸŸã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¼šç”Ÿæˆæ•´ä¸ªç›®æ ‡å›¾åƒï¼Œè€Œéé€‰æ‹©æ€§é‡æ–°ç”Ÿæˆä»…æ„å›¾ç¼–è¾‘çš„åŒºåŸŸï¼Œå¯¼è‡´ï¼ˆ1ï¼‰ä¸å¿…è¦çš„è®¡ç®—æˆæœ¬ï¼›ï¼ˆ2ï¼‰é‡å»ºéç¼–è¾‘åŒºåŸŸçš„åå‘ï¼Œå½±å“ç¼–è¾‘è´¨é‡ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºå°†å›¾åƒç¼–è¾‘åˆ¶å®šä¸ºåŸºäºè‡ªå›å½’å›¾åƒç”Ÿæˆçš„ä¸‹ä¸€ä¸ªç¼–è¾‘ä»¤ç‰Œé¢„æµ‹ï¼ˆNEPï¼‰ï¼Œä»…é‡æ–°ç”Ÿæˆéœ€è¦ç¼–è¾‘çš„åŒºåŸŸï¼Œé¿å…å¯¹éç¼–è¾‘åŒºåŸŸçš„æ„å¤–ä¿®æ”¹ã€‚ä¸ºå®ç°ä»»æ„åŒºåŸŸç¼–è¾‘ï¼Œæˆ‘ä»¬æå‡ºé¢„è®­ç»ƒä¸€ä¸ªä»»æ„é¡ºåºè‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ã€‚è®­ç»ƒåï¼Œå®ƒèƒ½å¤Ÿå®ç°é›¶æ ·æœ¬å›¾åƒç¼–è¾‘ï¼Œå¹¶æ˜“äºé€‚åº”NEPè¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œåœ¨å¹¿æ³›ä½¿ç”¨çš„å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è‡ªç„¶æ”¯æŒæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ï¼Œé€šè¿‡é›¶æ ·æœ¬æ–¹å¼è¿­ä»£ä¼˜åŒ–å…¶ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ä¸»è¦é›†ä¸­äºæ ¹æ®è¯­è¨€æŒ‡ä»¤ä¿®æ”¹å±€éƒ¨åŒºåŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç”Ÿæˆæ•´ä¸ªç›®æ ‡å›¾åƒï¼Œå¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—æˆæœ¬å’Œå¯¹éç¼–è¾‘åŒºåŸŸçš„é‡å»ºåå‘ã€‚</li>
<li>æå‡ºå°†å›¾åƒç¼–è¾‘åˆ¶å®šä¸ºä¸‹ä¸€ä¸ªç¼–è¾‘ä»¤ç‰Œé¢„æµ‹ï¼ˆNEPï¼‰ä»¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>NEPä»…é‡æ–°ç”Ÿæˆéœ€è¦ç¼–è¾‘çš„åŒºåŸŸï¼Œé¿å…å¯¹éç¼–è¾‘åŒºåŸŸçš„æ„å¤–ä¿®æ”¹ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒä»»æ„é¡ºåºè‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹æ¥å®ç°ä»»æ„åŒºåŸŸç¼–è¾‘ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬å›¾åƒç¼–è¾‘ï¼Œå¹¶åœ¨å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹æ”¯æŒæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e09b6a1162fce99039a5f8853f20fee6" align="middle">
<img src="https://picx.zhimg.com/v2-591508eedfc6efc55e79ed54ddee2e8b" align="middle">
<img src="https://picx.zhimg.com/v2-51bc2da91852bed1919856721e03bd14" align="middle">
<img src="https://picx.zhimg.com/v2-66ed77644e6d0a9b0d3e0b937f447344" align="middle">
<img src="https://picx.zhimg.com/v2-42f43aaad1411200ac62a64db2b6f1c3" align="middle">
<img src="https://picx.zhimg.com/v2-86cff562872671cecef234b406060434" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Phonikud-Hebrew-Grapheme-to-Phoneme-Conversion-for-Real-Time-Text-to-Speech"><a href="#Phonikud-Hebrew-Grapheme-to-Phoneme-Conversion-for-Real-Time-Text-to-Speech" class="headerlink" title="Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time   Text-to-Speech"></a>Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time   Text-to-Speech</h2><p><strong>Authors:Yakov Kolani, Maxim Melichov, Cobi Calev, Morris Alper</strong></p>
<p>Real-time text-to-speech (TTS) for Modern Hebrew is challenging due to the languageâ€™s orthographic complexity. Existing solutions ignore crucial phonetic features such as stress that remain underspecified even when vowel marks are added. To address these limitations, we introduce Phonikud, a lightweight, open-source Hebrew grapheme-to-phoneme (G2P) system that outputs fully-specified IPA transcriptions. Our approach adapts an existing diacritization model with lightweight adaptors, incurring negligible additional latency. We also contribute the ILSpeech dataset of transcribed Hebrew speech with IPA annotations, serving as a benchmark for Hebrew G2P, as training data for TTS systems, and enabling audio-to-IPA for evaluating TTS performance while capturing important phonetic details. Our results demonstrate that Phonikud G2P conversion more accurately predicts phonemes from Hebrew text compared to prior methods, and that this enables training of effective real-time Hebrew TTS models with superior speed-accuracy trade-offs. We release our code, data, and models at https: &#x2F;&#x2F;phonikud.github.io. </p>
<blockquote>
<p>ç°ä»£å¸Œä¼¯æ¥è¯­çš„å®æ—¶æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰é¢ä¸´ç€ç”±äºè¯¥è¯­è¨€æ­£å­—æ³•å¤æ‚æ€§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆå¿½ç•¥äº†é‡è¦çš„è¯­éŸ³ç‰¹å¾ï¼Œå¦‚å³ä½¿åœ¨æ·»åŠ å…ƒéŸ³æ ‡è®°åä»ç„¶æœªæ˜ç¡®æŒ‡å®šçš„é‡éŸ³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Phonikudï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¼€æºå¸Œä¼¯æ¥å­—æ¯åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰ç³»ç»Ÿï¼Œå¯ä»¥è¾“å‡ºå®Œå…¨æŒ‡å®šçš„å›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰è½¬å½•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è½»é‡çº§é€‚é…å™¨é€‚åº”ç°æœ‰çš„æ³¨éŸ³æ¨¡å‹ï¼Œå¢åŠ äº†å‡ ä¹å¯ä»¥å¿½ç•¥çš„é¢å¤–å»¶è¿Ÿã€‚æˆ‘ä»¬è¿˜è´¡çŒ®äº†å¸¦æœ‰å›½é™…éŸ³æ ‡æ³¨é‡Šçš„ILSpeechå¸Œä¼¯æ¥è¯­è¯­éŸ³è½¬å½•æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä½œä¸ºå¸Œä¼¯æ¥å­—æ¯åˆ°éŸ³ç´ çš„æ¯”å‡†æµ‹è¯•ï¼ŒTTSç³»ç»Ÿçš„è®­ç»ƒæ•°æ®ä»¥åŠèƒ½å¤Ÿè¯„ä¼°TTSæ€§èƒ½åŒæ—¶æ•æ‰é‡è¦è¯­éŸ³ç»†èŠ‚çš„éŸ³é¢‘åˆ°å›½é™…éŸ³æ ‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•ï¼ŒPhonikudçš„G2Pè½¬æ¢æ›´èƒ½å‡†ç¡®åœ°ä»å¸Œä¼¯æ¥è¯­æ–‡æœ¬é¢„æµ‹éŸ³ç´ ï¼Œå¹¶ä¸”è¿™èƒ½å¤Ÿè®­ç»ƒå‡ºæœ‰æ•ˆçš„å®æ—¶å¸Œä¼¯æ¥è¯­TTSæ¨¡å‹ï¼Œå®ç°å‡ºè‰²çš„é€Ÿåº¦å‡†ç¡®æ€§æƒè¡¡ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://phonikud.github.ioå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹./">https://phonikud.github.ioå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12311v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://phonikud.github.io/">https://phonikud.github.io</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹ç°ä»£å¸Œä¼¯æ¥è¯­çš„å®æ—¶æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰å­˜åœ¨æŒ‘æˆ˜ï¼Œå› å¸Œä¼¯æ¥è¯­çš„å­—å½¢å¤æ‚ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆå¿½ç•¥äº†é‡è¦çš„è¯­éŸ³ç‰¹å¾ï¼Œå¦‚å³ä½¿æ·»åŠ å…ƒéŸ³æ ‡è®°ä»æœªæ˜ç¡®æŒ‡å®šçš„é‡éŸ³ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Phonikudï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¼€æºå¸Œä¼¯æ¥å­—æ¯åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰ç³»ç»Ÿï¼Œå¯è¾“å‡ºå®Œå…¨æŒ‡å®šçš„å›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰è½¬å½•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è½»é‡çº§é€‚é…å™¨é€‚åº”ç°æœ‰çš„æ³¨éŸ³æ¨¡å‹ï¼Œå‡ ä¹ä¸ä¼šå¢åŠ é¢å¤–çš„å»¶è¿Ÿã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†ILSpeechæ•°æ®é›†ï¼ŒåŒ…å«å¸Œä¼¯æ¥è¯­è¯­éŸ³çš„IPAæ³¨é‡Šè½¬å½•ï¼Œä½œä¸ºå¸Œä¼¯æ¥è¯­G2Pçš„åŸºå‡†æµ‹è¯•ã€TTSç³»ç»Ÿçš„è®­ç»ƒæ•°æ®ï¼Œä»¥åŠç”¨äºè¯„ä¼°TTSæ€§èƒ½çš„éŸ³é¢‘åˆ°IPAï¼Œèƒ½å¤Ÿæ•æ‰é‡è¦çš„è¯­éŸ³ç»†èŠ‚ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•ï¼ŒPhonikudçš„G2Pè½¬æ¢å¯ä»¥æ›´å‡†ç¡®åœ°ä»å¸Œä¼¯æ¥è¯­æ–‡æœ¬é¢„æµ‹éŸ³ç´ ï¼Œå¹¶ä¸”å¯ä»¥è®­ç»ƒå‡ºé«˜æ•ˆå®æ—¶çš„å¸Œä¼¯æ¥è¯­TTSæ¨¡å‹ï¼Œå®ç°é€Ÿåº¦å’Œç²¾åº¦çš„è‰¯å¥½å¹³è¡¡ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨httpsï¼š&#x2F;&#x2F;phonikud.github.ioä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¸Œä¼¯æ¥è¯­å®æ—¶æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œå› å­—å½¢å¤æ‚å¯¼è‡´ç°æœ‰è§£å†³æ–¹æ¡ˆéš¾ä»¥å¤„ç†ã€‚</li>
<li>Phonikudæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¸Œä¼¯æ¥å­—æ¯åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰ç³»ç»Ÿï¼Œèƒ½å¤Ÿè¾“å‡ºå®Œå…¨æŒ‡å®šçš„å›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰è½¬å½•ã€‚</li>
<li>Phonikudé€šè¿‡è½»é‡çº§é€‚é…å™¨æ”¹è¿›äº†ç°æœ‰æ³¨éŸ³æ¨¡å‹ï¼Œå‡ ä¹ä¸å¢åŠ é¢å¤–å»¶è¿Ÿã€‚</li>
<li>æ¨å‡ºäº†ILSpeechæ•°æ®é›†ï¼ŒåŒ…å«å¸Œä¼¯æ¥è¯­è¯­éŸ³çš„IPAæ³¨é‡Šè½¬å½•ï¼Œä¸ºå¸Œä¼¯æ¥è¯­G2Pæä¾›äº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>Phonikudçš„G2Pè½¬æ¢èƒ½æ›´å‡†ç¡®åœ°é¢„æµ‹å¸Œä¼¯æ¥è¯­æ–‡æœ¬ä¸­çš„éŸ³ç´ ã€‚</li>
<li>Phonikudæœ‰åŠ©äºè®­ç»ƒå‡ºé«˜æ•ˆå®æ—¶çš„å¸Œä¼¯æ¥è¯­TTSæ¨¡å‹ã€‚</li>
<li>ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºè®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eaef27ee53d0ebee62684d08be313d47" align="middle">
<img src="https://picx.zhimg.com/v2-3875757975e05467da5c1c7f7983af4f" align="middle">
<img src="https://picx.zhimg.com/v2-90fce1372e73bafb3cd32f74f3e74acd" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Saten-Sparse-Augmented-Tensor-Networks-for-Post-Training-Compression-of-Large-Language-Models"><a href="#Saten-Sparse-Augmented-Tensor-Networks-for-Post-Training-Compression-of-Large-Language-Models" class="headerlink" title="Saten: Sparse Augmented Tensor Networks for Post-Training Compression of   Large Language Models"></a>Saten: Sparse Augmented Tensor Networks for Post-Training Compression of   Large Language Models</h2><p><strong>Authors:Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang</strong></p>
<p>The efficient implementation of large language models (LLMs) is crucial for deployment on resource-constrained devices. Low-rank tensor compression techniques, such as tensor-train (TT) networks, have been widely studied for over-parameterized neural networks. However, their applications to compress pre-trained large language models (LLMs) for downstream tasks (post-training) remains challenging due to the high-rank nature of pre-trained LLMs and the lack of access to pretraining data. In this study, we investigate low-rank tensorized LLMs during fine-tuning and propose sparse augmented tensor networks (Saten) to enhance their performance. The proposed Saten framework enables full model compression. Experimental results demonstrate that Saten enhances both accuracy and compression efficiency in tensorized language models, achieving state-of-the-art performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆå®ç°å¯¹äºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šè¿›è¡Œéƒ¨ç½²è‡³å…³é‡è¦ã€‚å¼ é‡åˆ†è§£æŠ€æœ¯ï¼Œå¦‚å¼ é‡åˆ—è½¦ï¼ˆTTï¼‰ç½‘ç»œï¼Œå·²è¢«å¹¿æ³›åº”ç”¨äºå‚æ•°è¿‡å¤šçš„ç¥ç»ç½‘ç»œã€‚ç„¶è€Œï¼Œç”±äºå…¶é«˜ç§©ç‰¹æ€§å’Œæ— æ³•è®¿é—®é¢„è®­ç»ƒæ•°æ®ï¼Œå°†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆåè®­ç»ƒï¼‰è¿›è¡Œå‹ç¼©ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åœ¨å¾®è°ƒæœŸé—´è¿›è¡Œå¼ é‡åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ç¨€ç–å¢å¼ºå¼ é‡ç½‘ç»œï¼ˆSatenï¼‰æ¥æé«˜å…¶æ€§èƒ½ã€‚æ‰€æå‡ºçš„Satenæ¡†æ¶èƒ½å¤Ÿå®ç°å…¨æ¨¡å‹å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSatenåœ¨æé«˜äº†å¼ é‡åŒ–è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå‹ç¼©æ•ˆç‡çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14871v2">PDF</a> Accepted to EMNLP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆå®ç°å¯¹äºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²è‡³å…³é‡è¦ã€‚ä½ç§©å¼ é‡å‹ç¼©æŠ€æœ¯ï¼Œå¦‚å¼ é‡è®­ç»ƒï¼ˆTTï¼‰ç½‘ç»œï¼Œå·²è¢«å¹¿æ³›åº”ç”¨äºè¿‡å‚æ•°åŒ–çš„ç¥ç»ç½‘ç»œã€‚ç„¶è€Œï¼Œç”±äºå…¶é«˜ç§©ç‰¹æ€§å’Œç¼ºä¹é¢„è®­ç»ƒæ•°æ®çš„è®¿é—®æƒé™ï¼Œå°†è¿™äº›æŠ€æœ¯åº”ç”¨äºå¯¹é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„å‹ç¼©ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ç»†è°ƒè¿‡ç¨‹ä¸­çš„ä½ç§©å¼ é‡åŒ–LLMï¼Œå¹¶æå‡ºäº†ç¨€ç–å¢å¼ºå¼ é‡ç½‘ç»œï¼ˆSatenï¼‰ä»¥æé«˜å…¶æ€§èƒ½ã€‚æ‰€æå‡ºçš„Satenæ¡†æ¶èƒ½å¤Ÿå®ç°å…¨æ¨¡å‹å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSatenåœ¨æé«˜å¼ é‡åŒ–è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå‹ç¼©æ•ˆç‡æ–¹é¢å‡è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„æœ‰æ•ˆå®ç°éå¸¸é‡è¦ã€‚</li>
<li>ä½ç§©å¼ é‡å‹ç¼©æŠ€æœ¯å¹¿æ³›åº”ç”¨äºè¿‡å‚æ•°åŒ–ç¥ç»ç½‘ç»œã€‚</li>
<li>å°†ä½ç§©å¼ é‡å‹ç¼©æŠ€æœ¯åº”ç”¨äºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºé«˜ç§©ç‰¹æ€§å’Œç¼ºä¹é¢„è®­ç»ƒæ•°æ®çš„è®¿é—®ã€‚</li>
<li>ç ”ç©¶è°ƒæŸ¥äº†ç»†è°ƒè¿‡ç¨‹ä¸­çš„ä½ç§©å¼ é‡åŒ–LLMã€‚</li>
<li>æå‡ºäº†ç¨€ç–å¢å¼ºå¼ é‡ç½‘ç»œï¼ˆSatenï¼‰æ¡†æ¶ä»¥æé«˜ä½ç§©å¼ é‡åŒ–LLMçš„æ€§èƒ½ã€‚</li>
<li>Satenæ¡†æ¶å¯å®ç°å…¨æ¨¡å‹å‹ç¼©ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSatenåœ¨æå‡å¼ é‡åŒ–è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå‹ç¼©æ•ˆç‡æ–¹é¢è¡¨ç°å“è¶Šï¼Œè¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12b98894067edcc048d8059361b3ba85" align="middle">
<img src="https://picx.zhimg.com/v2-3862ab512a1a7d47697956085cfdeaf5" align="middle">
<img src="https://picx.zhimg.com/v2-eb2a2608d3a7a2af45c020fd6fa58f08" align="middle">
<img src="https://picx.zhimg.com/v2-9b19d3126656cd9c1252fc11b5e57586" align="middle">
<img src="https://picx.zhimg.com/v2-6e29e934e9664c13549f8e399a5588ad" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2c4e9633cb4c6fd8e7aa998727f89d4e" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Evaluating & Reducing Deceptive Dialogue From Language Models with   Multi-turn RL
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-83794a71db03833ebd774b4f189266b9" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Sampling Density Compensation using Fast Fourier Deconvolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
