<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-10-18  RLAIF-SPA Optimizing LLM-based Emotional Speech Synthesis via RLAIF">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-df7301cfa07ebc24146ffb38a5a4a919~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753396&auth_key=1760753396-0-0-c80ffd4ac8e468208506d88d129aa37e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="RLAIF-SPA-Optimizing-LLM-based-Emotional-Speech-Synthesis-via-RLAIF"><a href="#RLAIF-SPA-Optimizing-LLM-based-Emotional-Speech-Synthesis-via-RLAIF" class="headerlink" title="RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF"></a>RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF</h2><p><strong>Authors:Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao</strong></p>
<p>Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation. </p>
<blockquote>
<p>文本转语音合成在中性语音方面已接近人类水平，但在情感表达上仍存在挑战。现有方法通常依赖于昂贵的情感标注，或优化未能捕捉到语音情感表达和情感感知自然性的间接目标，导致生成的语音准确但情感平淡。为了解决这些挑战，我们提出了RLAIF-SPA框架，它结合了强化学习从人工智能反馈（RLAIF）机制，利用自动语音识别（ASR）和大型语言模型（LLM）技术分别判断语义准确性和韵律情感标签对齐，作为情感表达和可理解性优化的直接奖励。具体来说，它利用韵律标签对齐，通过同时考虑语义准确性和韵律情感对齐，沿着结构、情感、速度和音调四个精细粒度方向，提高表达质量。此外，它还结合了语义准确性反馈，以确保清晰准确的语音生成。在Libri Speech数据集上的实验表明，RLAIF-SPA优于Chat-TTS，相对减少了26.1%的WER（词错误率），SIM-O增加了9.1%，并在人类评估中获得了超过10%的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14628v1">PDF</a> </p>
<p><strong>摘要</strong><br>情感表达是文本到语音合成（TTS）中的一个挑战，尤其是实现近人类质量的中性语音时。现有方法常依赖昂贵的情感标注或优化间接目标，难以捕捉语音的情感表达力和感知自然性，导致生成的语音准确但情感平淡。为解决这些挑战，我们提出了RLAIF-SPA框架，采用人工智能反馈强化学习机制，利用语音识别和大型语言模型技术分别判断语义准确性和韵律情感标签对齐，作为情感表达力和可理解性优化的直接奖励。该框架通过韵律标签对齐，联合考虑语义准确性和韵律情感对齐四个精细维度：结构、情感、速度和语调，提高表达质量。同时，结合语义准确性反馈，确保生成清晰准确的语音。在Libri Speech数据集上的实验表明，RLAIF-SPA在语音转文本方面优于Chat-TTS系统，降低了字错误率（WER）高达26.1%，SIM-O增加了9.1%，并在人类评估中取得了超过10%的改进。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>情感表达在文本到语音合成中仍然是一个挑战，尽管中性语音已接近人类质量。</li>
<li>现有方法经常因为依赖昂贵的情感标注或优化间接目标，无法充分捕捉语音的情感表达力和感知自然性。</li>
<li>RLAIF-SPA框架通过结合人工智能反馈强化学习机制，提高了情感表达力和语音的自然性。</li>
<li>该框架利用语音识别和大型语言模型技术，分别判断语义准确性和韵律情感标签对齐。</li>
<li>RLAIF-SPA框架通过联合考虑语义准确性和韵律情感对齐的四个维度，提高了语音表达的精细度。</li>
<li>语义准确性反馈的引入确保了生成的语音清晰准确。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cd3d94e775c86d09973da7ed7182d47a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760752977&auth_key=1760752977-0-0-7bc28b9b68dadb26829b20686b9febe1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de5622c0e7e1fed81861644404a38f47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760752984&auth_key=1760752984-0-0-43201d4c2b18d82d7e35f27bdcbb81bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c1799d390763cdc6326b6d3a34c26859~resize:0:q75.jpg?source=1f5c5e47&expiration=1760752992&auth_key=1760752992-0-0-03dc9b138a5205080f702565bb6ea28c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36108540ac8cd7c87bb282886eea4e19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760752998&auth_key=1760752998-0-0-99dd06f78f1b08812134895bf77182d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Optimal-Aggregation-of-LLM-and-PRM-Signals-for-Efficient-Test-Time-Scaling"><a href="#Optimal-Aggregation-of-LLM-and-PRM-Signals-for-Efficient-Test-Time-Scaling" class="headerlink" title="Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time   Scaling"></a>Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time   Scaling</h2><p><strong>Authors:Peng Kuang, Yanli Wang, Xiaoyu Han, Yaowenqi Liu, Kaidi Xu, Haohan Wang</strong></p>
<p>Process reward models (PRMs) are a cornerstone of test-time scaling (TTS), designed to verify and select the best responses from large language models (LLMs). However, this promise is challenged by recent benchmarks where simple majority voting, which ignores PRM signals, occasionally outperforms standard PRM-based selection. This raises a critical question: How can we effectively utilize verification signals from PRMs for TTS? To address this, we start by developing a theoretical framework for optimally combining signals from both the LLM and the PRM. Our framework reveals that the optimal strategy is a weighted aggregation of responses, a strategy whose effectiveness hinges on estimating weights that capture the complex interplay between the models. Based on our theoretical results, we empirically show that these optimal weighting functions differ significantly across LLM-PRM pairs and, notably, often assign substantial negative weights. Motivated by these insights, we propose efficient pre-computation methods to calibrate these weighting functions. Extensive experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method significantly boosts the TTS efficiency, surpassing the performance of vanilla weighted majority voting while using only $21.3%$ of the computation. Ultimately, our work demonstrates that investing in a more intelligent aggregation strategy can be a more convincing path to performance gains than simply scaling test-time computation. </p>
<blockquote>
<p>流程奖励模型（PRM）是测试时间缩放（TTS）的核心，旨在从大型语言模型（LLM）中验证和选择最佳响应。然而，最近的基准测试显示，忽略PRM信号的简单多数投票法偶尔会优于基于PRM的标准选择方法。这引发了一个关键问题：我们如何有效地利用PRM中的验证信号来进行TTS？为解决这一问题，我们首先开发了一个理论框架，用于最优地结合LLM和PRM的信号。我们的框架表明，最佳策略是响应的加权聚合，其有效性取决于估计能够捕捉模型之间复杂相互作用的权重。基于我们的理论结果，我们实证表明，这些最佳加权函数在不同LLM-PRM对之间差异很大，并且通常会分配相当大的负权重。受这些见解的启发，我们提出了有效的预计算方法以校准这些加权函数。在5个LLM和7个PRM上进行的广泛实验表明，我们的校准方法大大提高了TTS的效率，在仅使用21.3%的计算量的情况下，超越了简单加权多数投票的性能。最终，我们的工作证明，投资于更智能的聚合策略可能是实现性能提升的更令人信服的途径，而不是简单地扩大测试时间的计算规模。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13918v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了测试时间缩放（TTS）中的过程奖励模型（PRM）的应用。虽然PRM是TTS的核心组成部分，用于验证和选择大型语言模型（LLM）的最佳响应，但最近的研究表明，在某些情况下，忽略PRM信号的简单多数投票可能优于基于PRM的选择方法。为解决此问题，本文建立了理论框架，以最优方式结合LLM和PRM的信号。结果显示，最优策略是对响应的加权聚合，这取决于如何估算捕捉模型间复杂交互的权重。基于这些理论结果，通过实证研究验证了最佳加权函数在不同LLM-PRM组合中的差异显著，并且通常会分配较大的负权重。通过预先计算校准这些加权函数的方法，实证结果表明该方法大大提高了TTS的效率，超越了简单的加权多数投票方法，且计算成本仅为原来的21.3%。这表明采用更智能的聚合策略是提高性能的更可靠途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>过程奖励模型（PRM）在测试时间缩放（TTS）中占据核心地位，用于验证和选择大型语言模型（LLM）的最佳响应。</li>
<li>最近的研究发现简单多数投票在某些情况下优于基于PRM的选择方法。</li>
<li>建立理论框架以最优方式结合LLM和PRM的信号是关键。</li>
<li>最优策略是对响应进行加权聚合，而这取决于如何估算权重以捕捉模型间的复杂交互。</li>
<li>最佳加权函数在不同LLM-PRM组合中存在显著差异，并可能分配较大的负权重。</li>
<li>通过预先计算校准加权函数的方法可以提高TTS的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7a2c42264e9595806b58144bc1f99099~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753005&auth_key=1760753005-0-0-ccb160dd8a364a7abb05113ed5161c6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-172ddc22040d11f5da2ca708fd364d54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753012&auth_key=1760753012-0-0-01d8c46623c6e60e906a1b31c96b26bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner"><a href="#Generative-Universal-Verifier-as-Multimodal-Meta-Reasoner" class="headerlink" title="Generative Universal Verifier as Multimodal Meta-Reasoner"></a>Generative Universal Verifier as Multimodal Meta-Reasoner</h2><p><strong>Authors:Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang</strong></p>
<p>We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems. </p>
<blockquote>
<p>我们引入了生成式通用验证器（Generative Universal Verifier）这一新概念及插件，它是专为下一代视觉语言模型和统一多模态模型的跨模态推理而设计的，为推理和生成过程中的视觉结果提供了反思和改进的基本能力。这篇论文主要有三个贡献：</p>
</blockquote>
<p>（1）我们构建了ViVerBench，这是一个涵盖16类关键任务的全面基准测试平台，用于评估多模态推理中的视觉结果。结果表明，现有的视觉语言模型在这些任务上表现一直不佳，这突显出在可靠视觉验证方面与人类能力之间存在巨大差距。</p>
<p>（2）我们设计了两条自动化管道来构建大规模视觉验证数据，并训练了OmniVerifier-7B，这是首个用于通用视觉验证的全能生成式验证器，在ViVerBench上取得了显著的进步（+8.3）。通过训练，我们确定了视觉验证中的三种基本能力，并展示了它们如何协同推广和交互。</p>
<p>（3）我们提出了OmniVerifier-TTS，这是一种利用通用验证器在统一模型内实现图像生成和编辑的序列测试时间缩放范式，通过迭代精细优化提高生成能力上限。除了生成，我们还进一步将通用验证器扩展到更广泛的世界建模交叉推理场景。经验上，OmniVerifier-TTS在T2I-ReasonBench（提高+3.7）和GenEval++（提高+4.3）上取得了改进，优于现有的并行测试时间缩放方法，如Best-of-N。通过为跨模态推理赋予可靠的视觉验证，OmniVerifier不仅推动了生成过程中的可靠反思，还推动了可扩展的测试时间优化，朝着更可靠、可控的下一代推理系统迈出了重要一步。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13804v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了面向下一代多模态推理的生成式通用验证器（Generative Universal Verifier）这一概念及插件设计。该设计用于视觉语言模型和统一多模态模型中，为推理和生成过程中的视觉结果提供反思和优化的基本能力。本文有三个主要贡献：</p>
<p>一、构建了ViVerBench基准测试集，包含用于评估多模态推理中视觉结果的16类关键任务。结果显示，现有视觉语言模型在这些任务上表现不佳，凸显了与人类可靠视觉验证能力之间的巨大差距。</p>
<p>二、设计了自动化管道构建大规模视觉验证数据集，并训练了OmniVerifier-7B通用视觉验证器，在ViVerBench上取得了显著的提升（+8.3）。训练过程中，我们确定了视觉验证中的三个基本能力，并展示了它们如何协同工作并相互协作。</p>
<p>三、提出了OmniVerifier-TTS序列测试时间缩放范式，利用通用验证器在统一模型中建立图像生成和编辑之间的桥梁，通过迭代微调优化增强生成能力。此外，还将通用验证器扩展到更广泛的世界建模推理场景中。实验表明，OmniVerifier-TTS在T2I-ReasonBench和GenEval++上的表现优于现有平行测试时间缩放方法（如Best-of-N）。通过赋予多模态推理可靠的视觉验证能力，OmniVerifier实现了生成过程中的可靠反思和可伸缩的测试时间优化，标志着更值得信赖和可控的下一代推理系统的发展。</p>
<p><strong>关键见解</strong></p>
<p>一、引入了一种新的概念——“生成式通用验证器”，专为下一代多模态推理系统设计，强化了生成过程中对视觉结果的反思和优化能力。对于推动AI技术的发展有重要的贡献。<br>二、构建了一种全面的基准测试集——ViVerBench，评估视觉语言模型在多模态推理中的表现，揭示出现有模型的不足之处以及提升空间。<br>三、设计了自动化管道用于大规模视觉验证数据的构建和训练出OmniVerifier-7B模型用于通用视觉验证。在测试中表现出卓越的性能提升。<br>四、提出了OmniVerifier-TTS序列测试时间缩放范式，增强了模型的生成能力并优化了模型的表现性能。相较于传统的测试时间缩放方法有更优越的表现。<br>五、成功将通用验证器应用于更广泛的世界建模推理场景，验证了其在实际应用中的价值和潜力。<br>六、实验结果显示，OmniVerifier系列模型在多模态推理任务中取得了显著的性能提升，尤其是在图像生成和编辑等任务上表现尤为突出。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-58d4d2cc280728ecad0a9f8446897444~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753020&auth_key=1760753020-0-0-533ad996331db2d97881c8a02f3cd6f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e844630321f891a459a4e7f3094b5c34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753028&auth_key=1760753028-0-0-3b5dc5fa0c230ef4e86946a10ceaa343&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c33d2868eb904f1fd25c350d3956c0fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753035&auth_key=1760753035-0-0-83ba5994a16a69182501c707543f8bd1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6179c86ec697f23999d8f81d2a8cc99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753041&auth_key=1760753041-0-0-d38e5b447bb40a8c01439b80e6e2fbf6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mismatch-Aware-Guidance-for-Robust-Emotion-Control-in-Auto-Regressive-TTS-Models"><a href="#Mismatch-Aware-Guidance-for-Robust-Emotion-Control-in-Auto-Regressive-TTS-Models" class="headerlink" title="Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models"></a>Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive   TTS Models</h2><p><strong>Authors:Yizhou Peng, Yukun Ma, Chong Zhang, Yi-Wen Chao, Chongjia Ni, Bin Ma</strong></p>
<p>While Text-to-Speech (TTS) systems can achieve fine-grained control over emotional expression via natural language prompts, a significant challenge emerges when the desired emotion (style prompt) conflicts with the semantic content of the text. This mismatch often results in unnatural-sounding speech, undermining the goal of achieving fine-grained emotional control. Classifier-Free Guidance (CFG) is a key technique for enhancing prompt alignment; however, its application to auto-regressive (AR) TTS models remains underexplored, which can lead to degraded audio quality. This paper directly addresses the challenge of style-content mismatch in AR TTS models by proposing an adaptive CFG scheme that adjusts to different levels of the detected mismatch, as measured using large language models or natural language inference models. This solution is based on a comprehensive analysis of CFG’s impact on emotional expressiveness in state-of-the-art AR TTS models. Our results demonstrate that the proposed adaptive CFG scheme improves the emotional expressiveness of the AR TTS model while maintaining audio quality and intelligibility. </p>
<blockquote>
<p>虽然文本转语音（TTS）系统可以通过自然语言提示实现精细的情绪表达控制，但当期望的情绪（风格提示）与文本语义内容发生冲突时，就会出现一个重大挑战。这种不匹配通常会导致语音听起来不自然，从而无法实现精细的情绪控制目标。无分类器引导（CFG）是提高提示对齐的关键技术，然而它在自动回归（AR）TTS模型中的应用仍然被探索得不够深入，这可能导致音频质量下降。本文针对AR TTS模型中风格与内容不匹配的问题，提出了一种自适应CFG方案，该方案可以根据使用大型语言模型或自然语言推理模型检测到的不同不匹配程度进行调整。这个解决方案基于CFG对最新AR TTS模型的情感表现力影响的综合分析。我们的结果表明，所提出的自适应CFG方案提高了AR TTS模型的情感表现力，同时保持了音频质量和可理解性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13293v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>摘要</strong></p>
<p>文本至语音（TTS）系统在通过自然语言提示实现精细情绪表达控制时，当期望的情绪（风格提示）与文本语义内容发生冲突时，会出现显著挑战。这种不匹配会导致语音听起来不自然，无法实现精细情绪控制的目标。虽然无分类器引导（CFG）是提高提示对齐的关键技术，但其应用于自回归（AR）TTS模型时，音频质量会下降。本文直接解决了AR TTS模型中的风格内容不匹配问题，提出了一种自适应CFG方案，该方案可以根据使用大型语言模型或自然语言推理模型检测到的不匹配程度的不同进行调整。该方案基于CFG对最先进AR TTS模型的表达能力影响的全面分析。结果表明，自适应CFG方案提高了AR TTS模型的表达能力，同时保持了音频质量和清晰度。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>TTS系统在实现情绪表达的精细控制时面临风格提示与文本语义内容的冲突挑战。</li>
<li>风格与内容的不匹配会导致语音不自然，影响精细情绪控制目标的实现。</li>
<li>无分类器引导（CFG）技术对于提高提示对齐在TTS系统中至关重要。</li>
<li>在自回归（AR）TTS模型中应用CFG时，音频质量可能会下降。</li>
<li>本文提出了一种自适应CFG方案，能根据检测到的风格与内容不匹配程度进行调整。</li>
<li>该方案基于CFG对最新AR TTS模型情感表达能力的全面分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13293">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9802c040a729a6eee08b03622cad723d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753049&auth_key=1760753049-0-0-246eaedcc640f607932e5fc6e1519bc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-069665a81899348a05fa0b7f33335002~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753057&auth_key=1760753057-0-0-aa8e93e4fd7065eb3e9cf5c8e6d9a0f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33b1ca97204eced5728bc010c6025614~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753063&auth_key=1760753063-0-0-cb475bff371e5462c9311ff4f2e87805&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6fa97faff93f495d68a0c05d3afca89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753075&auth_key=1760753075-0-0-ce6180a840ba3f9fe9a5f55f11937d38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6f7a00647c25ef291476a522fbe8d9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753083&auth_key=1760753083-0-0-c3255cc3bf88836250a800a852621af6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StressTransfer-Stress-Aware-Speech-to-Speech-Translation-with-Emphasis-Preservation"><a href="#StressTransfer-Stress-Aware-Speech-to-Speech-Translation-with-Emphasis-Preservation" class="headerlink" title="StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis   Preservation"></a>StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis   Preservation</h2><p><strong>Authors:Xi Chen, Yuchen Song, Satoshi Nakamura</strong></p>
<p>We propose a stress-aware speech-to-speech translation (S2ST) system that preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis conversion. Our method translates source-language stress into target-language tags that guide a controllable TTS model. To overcome data scarcity, we developed a pipeline to automatically generate aligned training data and introduce the “LLM-as-Judge” for evaluation. Experiments show our approach substantially outperforms baselines in preserving emphasis while maintaining comparable translation quality, speaker intent, and naturalness. Our work highlights the importance of prosody in translation and provides an effective, data-efficient solution for preserving paralinguistic cues in S2ST. </p>
<blockquote>
<p>我们提出了一种感知压力的语音到语音翻译（S2ST）系统，该系统利用大型语言模型进行跨语言强调转换，以保留单词级别的强调。我们的方法将源语言的压力转化为目标语言的标签，这些标签可以引导可控的文本到语音转换模型。为了克服数据稀缺的问题，我们开发了一个管道来自动产生对齐的训练数据，并引入了“大型语言模型作为评价者”来进行评估。实验表明，我们的方法在保持重点的同时，在翻译质量、说话者的意图和自然度方面与基线相比有显著提高。我们的工作强调了韵律在翻译中的重要性，并提供了一种有效且数据高效的方法来保留语音到语音翻译中的语言外线索。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于压力感知的语音到语音翻译（S2ST）系统，该系统利用大型语言模型（LLMs）进行跨语言强调转换，以保留单词级别的强调信息。通过将源语言的应力翻译为引导可控文本到语音（TTS）模型的目标语言标签，该方法在数据稀缺的情况下仍能有效工作。实验表明，该方法在保持强调、翻译质量、说话者意图和自然度方面均优于基线。本文强调了语调在翻译中的重要性，并提供了一种有效、数据高效的方法来保留语音到语音翻译中的副语言线索。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一个基于压力感知的语音到语音翻译（S2ST）系统，该系统能够利用大型语言模型（LLMs）进行跨语言强调转换。</li>
<li>通过将源语言的应力转换为目标语言的标签，引导可控文本到语音（TTS）模型，实现单词级别的强调信息保留。</li>
<li>开发了一个自动生成对齐训练数据的管道，以克服数据稀缺的问题。</li>
<li>引入了“LLM-as-Judge”进行评估，强调了语调在翻译中的重要性。</li>
<li>实验结果表明，该方法在保持强调、翻译质量、说话者意图和自然度方面均优于基线。</li>
<li>该方法提供了一种有效、数据高效的方法来保留语音到语音翻译中的副语言线索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13194">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d30165f56e56e88b6717e789ef82c027~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753091&auth_key=1760753091-0-0-8a1c1220be5f060c6ecd7bf7d5e043c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-961ab85c7c9939be62e06a7ab5185272~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753098&auth_key=1760753098-0-0-b3cfc487fd5d84ad955fb5efdae99356&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ea99ac0829b3d0bf9ee1175b8ce9880~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753105&auth_key=1760753105-0-0-e8bade38b32e68e147ed2cd09080cde6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd7ff22f14ece7fc3f6c566b95740416~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753111&auth_key=1760753111-0-0-b05faad9e579554496e398158d3d92c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-955ae31a38fa630f712183d35258a45b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753121&auth_key=1760753121-0-0-ec086bf879f8e41869217582bcb50e44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6fe7389fd852fbecf1c6c936a1246f86~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753127&auth_key=1760753127-0-0-baf6dea63b57d809572a042e1cdefe58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Continuous-Token-Diffusion-for-Speaker-Referenced-TTS-in-Multimodal-LLMs"><a href="#Continuous-Token-Diffusion-for-Speaker-Referenced-TTS-in-Multimodal-LLMs" class="headerlink" title="Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs"></a>Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs</h2><p><strong>Authors:Xinlu He, Swayambhu Nath Ray, Harish Mallidi, Jia-Hong Huang, Ashwin Bellur, Chander Chandak, M. Maruf, Venkatesh Ravichandran</strong></p>
<p>Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information.In this work, we investigate the TTS within the MLLM paradigm using continuous speech representations. We design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis. (3) Masked training is employed to address exposure bias in autoregressive decoding. (4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure. </p>
<blockquote>
<p>在多模态大型语言模型（MLLM）中，统一架构在单一框架内处理各种任务方面展现出潜力。在文本到语音（TTS）任务中，当前的基于MLLM的方法依赖于离散令牌表示，这忽略了语音的固有连续性，并可能导致精细的声学信息丢失。</p>
</blockquote>
<p>在这项工作中，我们使用连续语音表示来研究MLLM范式中的TTS。我们设计了双头架构，并实施了两种互补的训练策略，以构建稳健的模型。（1）在MLLM上增加了一个生成连续语音表示的分形头，该头是帧级的，并且严格自回归。（2）保留原始语言模型头以保留多任务能力并控制语音合成的开始和结束。（3）采用掩码训练来解决自回归解码中的暴露偏见。（4）为了优化稳定，我们提出了一种两阶段方案，在第二阶段冻结LM，确保分形头从固定的输入分布中学习。在LibriSpeech（PC）测试清洁度评估中，我们的方法达到了最先进的自回归性能，词错误率为1.95%，说话人相似度为0.54，UTMOS为4.00。与单阶段训练基线相比，两阶段训练使词错误率相对降低了46%。这些结果突显了结合自回归建模与连续令牌扩散的有效性，并得到两阶段训练程序的支撑。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了在多模态大型语言模型（MLLM）框架下实现文本转语音（TTS）任务的新方法。不同于现有依赖于离散令牌表示的方法，该方法采用连续语音表示，设计了一个双头架构，并实施了两种互补的训练策略。通过扩散头生成连续语音表示，同时保留原始语言模型头以保留多任务能力和控制语音合成的开始和结束。使用掩码训练解决自回归解码中的曝光偏差问题，并提出两阶段优化方案，以稳定训练过程。实验结果表明，该方法在LibriSpeech测试集上取得了最先进的自回归性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>采用了基于连续语音表示的多模态大型语言模型（MLLM）架构进行文本转语音（TTS）任务。</li>
<li>设计了双头架构，包括生成连续语音表示的扩散头，并保留了原始语言模型头以实现多任务能力和控制语音合成的开始和结束。</li>
<li>实施了两种互补的训练策略：使用掩码训练解决自回归解码中的曝光偏差问题，并提出两阶段优化方案以稳定训练过程。</li>
<li>在LibriSpeech测试集上的实验结果表明，该方法取得了先进的自回归性能，包括词错误率（WER）、说话人相似度和UTMOS评分。</li>
<li>两阶段训练方法相较于单阶段训练，实现了46%的相对词错误率降低。</li>
<li>该方法结合了自回归建模和连续令牌扩散，显示出了良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-5077e519ab104b726b010a4c7d7012e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753135&auth_key=1760753135-0-0-def8ce778abad06ed566b6e684c94d23&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c70f1847607fef6a922e9950524f98d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753146&auth_key=1760753146-0-0-c4568fdb95e9b5849c366da310073f1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-698bf94b3c69268a3697bd2d462f6e26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753152&auth_key=1760753152-0-0-f4096f103a3ffbe3e43f689254ade58a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Content-Anonymization-for-Privacy-in-Long-form-Audio"><a href="#Content-Anonymization-for-Privacy-in-Long-form-Audio" class="headerlink" title="Content Anonymization for Privacy in Long-form Audio"></a>Content Anonymization for Privacy in Long-form Audio</h2><p><strong>Authors:Cristina Aggazzotti, Ashi Garg, Zexin Cai, Nicholas Andrews</strong></p>
<p>Voice anonymization techniques have been found to successfully obscure a speaker’s acoustic identity in short, isolated utterances in benchmarks such as the VoicePrivacy Challenge. In practice, however, utterances seldom occur in isolation: long-form audio is commonplace in domains such as interviews, phone calls, and meetings. In these cases, many utterances from the same speaker are available, which pose a significantly greater privacy risk: given multiple utterances from the same speaker, an attacker could exploit an individual’s vocabulary, syntax, and turns of phrase to re-identify them, even when their voice is completely disguised. To address this risk, we propose new content anonymization approaches. Our approach performs a contextual rewriting of the transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while preserving meaning. We present results in a long-form telephone conversation setting demonstrating the effectiveness of a content-based attack on voice-anonymized speech. Then we show how the proposed content-based anonymization methods can mitigate this risk while preserving speech utility. Overall, we find that paraphrasing is an effective defense against content-based attacks and recommend that stakeholders adopt this step to ensure anonymity in long-form audio. </p>
<blockquote>
<p>语音匿名化技术在基准测试（如VoicePrivacy Challenge）中成功地将发言者的声音特征隐藏在短而孤立的语句里。然而，在实际应用中，语句很少是孤立的：长音频在访谈、电话和会议等领域中很常见。在这些情况下，来自同一发言者的许多语句构成了更大的隐私风险：给定同一发言者的多个语句，攻击者可能会利用个人的词汇、语法和措辞来重新识别他们，即使他们的声音完全被伪装。为了应对这一风险，我们提出了全新的内容匿名化方法。我们的方法在一个ASR-TTS管道中对转录进行上下文重写，以消除说话人的特定风格，同时保留原意。我们在长电话对话设置中展示了基于内容的攻击对语音匿名化语音的有效性结果。然后，我们展示了所提出的内容匿名化方法如何减轻这种风险，同时保留语音的实用性。总的来说，我们发现改述是抵御基于内容的攻击的有效防御手段，并建议利益相关者采取这一步骤，以确保长音频的匿名性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12780v1">PDF</a> </p>
<p><strong>摘要</strong><br>    在VoicePrivacy Challenge等基准测试中，语音匿名化技术已成功用于掩盖说话者的声音身份在短暂的孤立发言中。然而，在实际应用中，发言很少是孤立的：长音频在访谈、电话和会议等领域中很常见。在这些情况下，同一说话者的许多发言都是可用的，这带来了更大的隐私风险：即使声音完全伪装，攻击者仍可以利用同一人的词汇、语法和措辞来重新识别身份。为应对这一风险，我们提出了新型内容匿名化方法。我们的方法通过ASR-TTS管道中的上下文重写来消除说话者特有的风格，同时保留意义。我们在电话对话的长音频设置展示了内容攻击对语音匿名化的语音的有效性，然后展示了所提出的内容匿名化方法如何降低这种风险同时保留语音实用性。总的来说，我们发现改述是抵御内容攻击的有效防御手段，建议相关方采取这一步骤以确保长音频的匿名性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音匿名化技术在短而孤立的发言中很有效，但在长音频中存在重大隐私风险。</li>
<li>同一说话者的多个发言可使攻击者通过词汇、语法和措辞重新识别身份。</li>
<li>提出新型内容匿名化方法，通过ASR-TTS管道中的上下文重写消除说话者特有风格。</li>
<li>在电话对话的长音频设置中，展示了内容攻击的有效性以及所提方法的效用。</li>
<li>改述是有效抵御内容攻击的手段。</li>
<li>内容匿名化方法在保护隐私的同时，能保留语音的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12780">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c0cdeaa27367e40e1fc28476c61d472c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753160&auth_key=1760753160-0-0-fd89f2dc6d7c1e6692fc2c7e4e92bf55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-538a1b519677a7f64ff5b078dc594859~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753167&auth_key=1760753167-0-0-333e3c1eb3baaaad7f366acfe70afd81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8063155d0b8cee5c9ca0d8cf28a6261~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753188&auth_key=1760753188-0-0-fca8f94f994fad9b1c95dc667601445a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a199bc798b181bbbc0a22dec39a867~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753197&auth_key=1760753197-0-0-1a71702a60f294e3d2f54eb9d16887ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TerraCodec-Compressing-Earth-Observations"><a href="#TerraCodec-Compressing-Earth-Observations" class="headerlink" title="TerraCodec: Compressing Earth Observations"></a>TerraCodec: Compressing Earth Observations</h2><p><strong>Authors:Julen Costa-Watanabe, Isabelle Wittmann, Benedikt Blumenstiel, Konrad Schindler</strong></p>
<p>Earth observation (EO) satellites produce massive streams of multispectral image time series, posing pressing challenges for storage and transmission. Yet, learned EO compression remains fragmented, lacking publicly available pretrained models and misaligned with advances in compression for natural imagery. Image codecs overlook temporal redundancy, while video codecs rely on motion priors that fail to capture the radiometric evolution of largely static scenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to EO. TEC includes efficient image-based variants adapted to multispectral inputs, as well as a Temporal Transformer model (TEC-TT) that leverages dependencies across time. To overcome the fixed-rate setting of today’s neural codecs, we present Latent Repacking, a novel method for training flexible-rate transformer models that operate on varying rate-distortion settings. Trained on Sentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x stronger compression at equivalent image quality. Beyond compression, TEC-TT enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the AllClear benchmark. Our results establish bespoke, learned compression algorithms as a promising direction for Earth observation. Code and model weights will be released under a permissive license. </p>
<blockquote>
<p>地球观测（EO）卫星产生大量的多光谱图像时间序列，对存储和传输提出了巨大的挑战。然而，现有的EO压缩技术仍然分散，缺乏公开的预训练模型，并且与自然图像压缩技术的进展不匹配。图像编解码器忽略了时间冗余，而视频编解码器则依赖于运动优先权，无法捕捉静态场景的光度变化。我们引入了TerraCodec（TEC），这是一系列专为EO定制的学习编解码器。TEC包括适应多光谱输入的基于图像的高效变体，以及利用时间依赖性的Temporal Transformer模型（TEC-TT）。为了克服当前神经编解码器的固定速率设置，我们提出了Latent Repacking，这是一种新的训练灵活速率变换模型的方法，可在不同的速率失真设置下运行。在Sentinel-2数据上进行训练后，TerraCodec在图像质量相当的情况下实现了3-10倍的强压缩。除了压缩功能外，TEC-TT还实现了零样本云补全，在AllClear基准测试中超过了最先进的方法。我们的结果证明，专用的学习压缩算法对于地球观测是一个很有希望的研究方向。代码和模型权重将在许可下发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12670v1">PDF</a> </p>
<p><strong>Summary</strong><br>    TerraCodec（TEC）是一种专为地球观测设计的神经网络编码解码器，它针对多光谱图像时间序列进行压缩。通过采用图像和多光谱输入的变种以及利用时间依赖性的Temporal Transformer模型（TEC-TT），TerraCodec实现了高效压缩。与传统编码解码器相比，TerraCodec在相同图像质量下实现了3至10倍的压缩性能。此外，TEC-TT还支持零样本云填充功能，在AllClear基准测试中超越了现有方法。该论文的研究结果证明了为地球观测量身定制的神经网络编码解码器算法具有广阔的发展前景。代码和模型权重将在许可下发布。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TerraCodec（TEC）是专为地球观测设计的神经网络编码解码器家族。</li>
<li>TEC解决了多光谱图像时间序列存储和传输的挑战。</li>
<li>TEC包括针对多光谱输入的图像变种和Temporal Transformer模型（TEC-TT）。</li>
<li>Latent Repacking是一种新型方法，用于训练灵活速率的转换器模型，适用于不同的速率失真设置。</li>
<li>TerraCodec实现了对传统编码解码器的显著性能提升，在相同图像质量下实现了更高的压缩率。</li>
<li>除了压缩功能外，TEC-TT还支持零样本云填充功能，并在AllClear基准测试中表现出色。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12670">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9c76e6ab4d9d65b5f4c3dacee033423b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753211&auth_key=1760753211-0-0-04bef9c3f1063c9a9be45d9a3e9fb9fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-431aee6faebf436df37ca4297962d099~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753218&auth_key=1760753218-0-0-44568495a5b4b2e25462f84e04286d61&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7841112502f7e1b5f108bceb86efc86e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753225&auth_key=1760753225-0-0-e26bb3f54ce8191ca394447dfe40eaa7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4da0c9e1f054adbb1e0f29ae39b5079f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753231&auth_key=1760753231-0-0-adbc7763fef6d58857a7482c91f355b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiSTAR-Diffusion-over-a-Scalable-Token-Autoregressive-Representation-for-Speech-Generation"><a href="#DiSTAR-Diffusion-over-a-Scalable-Token-Autoregressive-Representation-for-Speech-Generation" class="headerlink" title="DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation"></a>DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation</h2><p><strong>Authors:Yakun Song, Xiaobin Zhuang, Jiawei Chen, Zhikang Niu, Guanrou Yang, Chenpeng Du, Dongya Jia, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen</strong></p>
<p>Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce DISTAR, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, DISTAR drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: DISTAR produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that DISTAR surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker&#x2F;style consistency, while maintaining rich output diversity. Audio samples are provided on <a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/DiSTAR_demo">https://anonymous.4open.science/w/DiSTAR_demo</a>. </p>
<blockquote>
<p>最近尝试将自回归（AR）素描师与基于扩散的细化器在连续语音表示中进行交织，显示出了一定的前景，但在分布转移的情况下仍然显得脆弱，对可控性的控制杠杆有限。我们引入了DISTAR，这是一个零样本文本到语音的框架，它完全在离散残差向量量化（RVQ）的编码空间中运行，紧密地将AR语言模型与掩码扩散模型耦合，无需强制对齐或持续时间预测器。具体来说，DISTAR使用AR语言模型绘制基于块的RVQ令牌，然后在给定草图的情况下执行并行掩码扩散填充，以完成下一个块，从而在块并行的情况下实现长形式合成，同时减轻经典的AR曝光偏差。离散编码空间为推理提供了明确的控制：DISTAR在贪婪和基于样本的解码下产生高质量的音频，使用无分类器指导，支持在鲁棒性和多样性之间的权衡，并且通过RVQ层在测试时的修剪实现可变比特率和可控的计算。大量实验和消融研究表明，DISTAR在鲁棒性、自然性和说话人&#x2F;风格一致性方面超越了最先进的零样本TTS系统，同时保持了丰富的输出多样性。音频样本请访问：[匿名链接地址]。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12210v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DISTAR是一个零样本文本到语音框架，采用离散残差向量量化（RVQ）码空间操作，紧密耦合自回归语言模型和掩码扩散模型，无需强制对齐或持续时间预测。它使用AR语言模型起草块级RVQ令牌，然后通过并行掩码扩散填充条件完成下一个块，实现长形式合成与块并行处理，并减轻传统AR暴露偏差。离散码空间在推理时提供显式控制：DISTAR可在贪婪和样本基础解码下产生高质量音频，支持鲁棒性和多样性之间的权衡，并通过RVQ层修剪在测试时实现可变比特率和可控计算。实验和消融研究表明，DISTAR在鲁棒性、自然性和语音&#x2F;风格一致性方面超越了最先进的零样本TTS系统，同时保持了丰富的输出多样性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DISTAR是一个零样本文本到语音转换框架，完全在离散残差向量量化码空间中操作。</li>
<li>DISTAR结合了自回归语言模型和掩码扩散模型，无需强制对齐或持续时间预测。</li>
<li>通过块级RVQ令牌起草和并行掩码扩散填充，实现了长形式合成与块并行处理。</li>
<li>离散码空间提供了在推理时的显式控制，包括高质量音频的产生、鲁棒性和多样性之间的权衡以及可变比特率和可控计算。</li>
<li>DISTAR通过RVQ层修剪在测试时实现了性能优化。</li>
<li>实验和消融研究表明，DISTAR在多个方面超越了最先进的零样本TTS系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12210">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c619729f0ca848c30c01b49b2e5447b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753239&auth_key=1760753239-0-0-329d2c5a0a9c9ecb06b28b183d7c5623&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-23f74beb6a9844a5efdef72f6555f77f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753246&auth_key=1760753246-0-0-26677fbb99c594c72025bdf2dd703218&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BridgeCode-A-Dual-Speech-Representation-Paradigm-for-Autoregressive-Zero-Shot-Text-to-Speech-Synthesis"><a href="#BridgeCode-A-Dual-Speech-Representation-Paradigm-for-Autoregressive-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis"></a>BridgeCode: A Dual Speech Representation Paradigm for Autoregressive   Zero-Shot Text-to-Speech Synthesis</h2><p><strong>Authors:Jingyuan Xing, Mingru Yang, Zhipeng Li, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Autoregressive (AR) frameworks have recently achieved remarkable progress in zero-shot text-to-speech (TTS) by leveraging discrete speech tokens and large language model techniques. Despite their success, existing AR-based zero-shot TTS systems face two critical limitations: (i) an inherent speed-quality trade-off, as sequential token generation either reduces frame rates at the cost of expressiveness or enriches tokens at the cost of efficiency, and (ii) a text-oriented supervision mismatch, as cross-entropy loss penalizes token errors uniformly without considering the fine-grained acoustic similarity among adjacent tokens. To address these challenges, we propose BridgeTTS, a novel AR-TTS framework built upon the dual speech representation paradigm BridgeCode. BridgeTTS reduces AR iterations by predicting sparse tokens while reconstructing rich continuous features for high-quality synthesis. Joint optimization of token-level and feature-level objectives further enhances naturalness and intelligibility. Experiments demonstrate that BridgeTTS achieves competitive quality and speaker similarity while significantly accelerating synthesis. Speech demos are available at <a target="_blank" rel="noopener" href="https://test1562.github.io/demo/">https://test1562.github.io/demo/</a>. </p>
<blockquote>
<p>自回归（AR）框架最近借助离散语音标记和大型语言模型技术，在零样本文本到语音（TTS）方面取得了显著进展。尽管取得了成功，但现有的基于AR的零样本TTS系统面临两个关键局限性：（i）固有的速度与质量权衡，因为顺序标记生成要么以降低帧率为代价换取表现力，要么以牺牲效率为代价丰富标记；（ii）文本导向的监督不匹配，因为交叉熵损失会统一惩罚标记错误，而不会考虑相邻标记之间的精细声学相似性。为了解决这些挑战，我们提出了基于双语音表示范式BridgeCode的BridgeTTS新型AR-TTS框架。BridgeTTS通过预测稀疏标记并重建丰富的连续特征来减少AR迭代，以实现高质量合成。标记级和特征级的联合优化目标进一步提高了自然度和清晰度。实验表明，BridgeTTS在保持竞争力的质量和说话人相似性的同时，显著加快了合成速度。语音演示可在[<a target="_blank" rel="noopener" href="https://test1562.github.io/demo/]%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://test1562.github.io/demo/]上查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11646v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于离散语音令牌和大型语言模型技术的自回归（AR）框架在零样本文本到语音（TTS）领域取得了显著进展。然而，现有的AR-based零样本TTS系统面临两个关键局限：一是速度与质量的权衡，二是文本导向的监督不匹配问题。为解决这些问题，我们提出了BridgeTTS，这是一个基于双语音表示范式BridgeCode的新型AR-TTS框架。BridgeTTS通过预测稀疏令牌并重建丰富的连续特征来减少AR迭代，实现高质量合成。对令牌级别和功能级别的目标进行联合优化，进一步提高自然度和清晰度。实验表明，BridgeTTS在质量、说话人相似度方面表现优异，同时显著加速了合成过程。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AR框架在零样本TTS中利用离散语音令牌和大型语言模型技术取得显著进步。</li>
<li>现有AR-based零样本TTS系统面临速度与质量的权衡及文本导向的监督不匹配问题。</li>
<li>BridgeTTS是一个基于BridgeCode的新型AR-TTS框架，旨在解决上述问题。</li>
<li>BridgeTTS通过预测稀疏令牌并重建连续特征来减少AR迭代，实现高质量合成。</li>
<li>联合优化令牌级别和功能级别的目标，提高语音自然度和清晰度。</li>
<li>BridgeTTS在质量和合成速度方面表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3a310a8e7ca4822aeffacc3273ea3de7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753253&auth_key=1760753253-0-0-e1436b9f1d023a918daa3d40587e3577&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebf0c6fd96d2b87f549ae943780e1ae6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753261&auth_key=1760753261-0-0-41a0d873cf6896717c57fa7a9ca77221&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e411ae7c3c3606dd378eddb02a79b88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753268&auth_key=1760753268-0-0-eb14bd472f155b7739c3fe8613f3b3f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ac66923924d9f580cb0adced9e8327e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753274&auth_key=1760753274-0-0-624c9ffd54e4fb67a49546e9150211bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed0be693d5f89f9a078a8a7e79bd5ce2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753281&auth_key=1760753281-0-0-5bb2799c572373357cebe0a1656d0ea2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ParsVoice-A-Large-Scale-Multi-Speaker-Persian-Speech-Corpus-for-Text-to-Speech-Synthesis"><a href="#ParsVoice-A-Large-Scale-Multi-Speaker-Persian-Speech-Corpus-for-Text-to-Speech-Synthesis" class="headerlink" title="ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for   Text-to-Speech Synthesis"></a>ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for   Text-to-Speech Synthesis</h2><p><strong>Authors:Mohammad Javad Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery</strong></p>
<p>Existing Persian speech datasets are typically smaller than their English counterparts, which creates a key limitation for developing Persian speech technologies. We address this gap by introducing ParsVoice, the largest Persian speech corpus designed specifically for text-to-speech(TTS) applications. We created an automated pipeline that transforms raw audiobook content into TTS-ready data, incorporating components such as a BERT-based sentence completion detector, a binary search boundary optimization method for precise audio-text alignment, and audio-text quality assessment frameworks tailored to Persian. The pipeline processes 2,000 audiobooks, yielding 3,526 hours of clean speech, which was further filtered into a 1,804-hour high-quality subset suitable for TTS, featuring more than 470 speakers. To validate the dataset, we fine-tuned XTTS for Persian, achieving a naturalness Mean Opinion Score (MOS) of 3.6&#x2F;5 and a Speaker Similarity Mean Opinion Score (SMOS) of 4.0&#x2F;5 demonstrating ParsVoice’s effectiveness for training multi-speaker TTS systems. ParsVoice is the largest high-quality Persian speech dataset, offering speaker diversity and audio quality comparable to major English corpora. The complete dataset has been made publicly available to accelerate the development of Persian speech technologies. The ParsVoice dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice">https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice</a>. </p>
<blockquote>
<p>现有的波斯语语音数据集通常比英语数据集小，这为开发波斯语语音技术造成了关键限制。我们通过引入ParsVoice来解决这一问题，ParsVoice是专门为文本到语音（TTS）应用设计的大型波斯语语音语料库。我们创建了一个自动化管道，将原始有声书内容转换为适用于TTS的数据，其中包括基于BERT的句子完成检测器、用于精确音视频对齐的二进制搜索边界优化方法，以及针对波斯语的音视频质量评估框架。该管道处理了2000本有声书，产生了3526小时的清晰语音，进一步筛选为适合TTS的1804小时的高质量子集，涵盖了470多名发言人的声音。为了验证数据集，我们对XTTS进行了波斯语微调，达到了自然度平均意见得分（MOS）为3.6&#x2F;5和演讲者相似性平均意见得分（SMOS）为4.0&#x2F;5，证明了ParsVoice在训练多发言人TTS系统方面的有效性。ParsVoice是最大且质量最高的波斯语语音数据集，提供可与主要英语语料库相比的发言人多样性和音频质量。为了加速波斯语语音技术的发展，我们已公开提供完整的ParsVoice数据集。ParsVoice数据集可在以下网址公开获取：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice">https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10774v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>一个研究团队为了解决波斯语语音数据集规模较小的问题，开发了ParsVoice，这是专门为文本到语音（TTS）应用设计的最大波斯语语音语料库。他们创建了一个自动化管道，该管道将有声书内容转化为TTS就绪数据，该管道包括基于BERT的句子完成检测器、用于精确音视频对齐的二进制搜索边界优化方法等。经过处理2000本有声书后，获得了3526小时清洁语音，进一步筛选为适合TTS的1804小时高质量子集，涵盖了超过470名发言人。为了验证数据集，他们对XTTS进行了波斯语微调，实现了自然度平均意见得分（MOS）为3.6&#x2F;5和发音人相似度平均意见得分（SMOS）为4.0&#x2F;5，证明了ParsVoice在训练多发言人TTS系统方面的有效性。ParsVoice是高质量波斯语语音数据集中最大的，可提供与主要英语语料库相当的发音人多样性和音频质量，并已公开供加速波斯语语音技术开发使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>波斯语语音数据集相较于英语数据集规模较小，对开发波斯语语音技术造成限制。</li>
<li>推出ParsVoice，最大波斯语语音语料库，专为TTS应用设计。</li>
<li>创建自动化管道处理音频书籍，转化为TTS就绪数据。</li>
<li>管道包含多项技术组件，如基于BERT的句子完成检测器、二进制搜索边界优化方法等。</li>
<li>ParsVoice包含3526小时清洁语音数据，进一步筛选为1804小时高质量子集，涵盖470+发言人。</li>
<li>XTTS模型在ParsVoice数据集上的自然度MOS为3.6&#x2F;5，发音人相似性SMOS为4.0&#x2F;5。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6f1f6fcd3d5e4b6b55910f691aee6126~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753289&auth_key=1760753289-0-0-f5ea69d959601ce0af0b95db33636fc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5a3efa7071c2309dd8f8e0ba6f932f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753295&auth_key=1760753295-0-0-c474566fe3f045c8908ebf2a3d0d70d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da4c0d0f8bfe0513dc86129a650f3e55~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753302&auth_key=1760753302-0-0-a2e4bb621633accb9cd2aa130aa15206&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34b47d1451e52b7b15f10832d0321737~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753308&auth_key=1760753308-0-0-845c4c64045090737f16423c363b1e3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c14863e970185f506434a36091802e07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753315&auth_key=1760753315-0-0-fcd60615dba38f28472d62904ae7a558&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MRSAudio-A-Large-Scale-Multimodal-Recorded-Spatial-Audio-Dataset-with-Refined-Annotations"><a href="#MRSAudio-A-Large-Scale-Multimodal-Recorded-Spatial-Audio-Dataset-with-Refined-Annotations" class="headerlink" title="MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations"></a>MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations</h2><p><strong>Authors:Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Xintong Hu, Yu Zhang, Li Tang, Rui Yang, Han Wang, Zongbao Zhang, Yuhan Wang, Yixuan Chen, Hankun Xu, Ke Xu, Pengfei Fan, Zhetao Chen, Yanhao Yu, Qiange Huang, Fei Wu, Zhou Zhao</strong></p>
<p>Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space. Despite the critical role of spatial audio in immersive technologies such as VR&#x2F;AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding. To address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation. MRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios. The dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts. To demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection. Results show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research. Demos and dataset access are available at <a target="_blank" rel="noopener" href="https://mrsaudio.github.io/">https://mrsaudio.github.io</a>. </p>
<blockquote>
<p>人类依赖多感官融合来感知空间环境，其中听觉线索使声音源能在三维空间中进行定位。尽管空间音频在VR&#x2F;AR等沉浸式技术中发挥着关键作用，但大多数现有的多模式数据集仅提供单声道音频，这限制了空间音频生成和理解的发展。为了应对这些挑战，我们引入了MRSAudio，这是一个大规模的多模式空间音频数据集，旨在推动空间音频理解和生成方面的研究。MRSAudio跨越四个独特组成部分：MRSLife、MRSSpeech、MRSMusic和MRSSing，涵盖各种真实世界场景。该数据集包括同步的双耳和环绕声音频、外向中心和内向中心视频、运动轨迹，以及精细注释，如文本、音素边界、歌词、乐谱和提示。为了展示MRSAudio的实用性和多功能性，我们建立了五个基本任务：音频定位、空间文本转语音、空间歌声合成、空间音乐生成和声音事件定位和检测。结果表明，MRSAudio能够实现高质量的空间建模，并支持广泛的空间音频研究。演示和数据集访问请访问：<a target="_blank" rel="noopener" href="https://mrsaudio.github.io./">https://mrsaudio.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10396v2">PDF</a> 24 pages</p>
<p><strong>摘要</strong></p>
<p>人类依赖多感官整合来感知空间环境，听觉线索能使声音源在三维空间中进行定位。尽管空间音频在虚拟现实&#x2F;增强现实等沉浸式技术中发挥着关键作用，但现有的多模态数据集大多为单声道音频，这限制了空间音频生成和理解的发展。为了解决这些挑战，我们推出了MRSAudio大规模多模态空间音频数据集，以促进空间音频理解和生成的研究。MRSAudio包括四个独特组成部分：MRSLife、MRSSpeech、MRSMusic和MRSSing，涵盖各种真实场景。数据集包括同步的双耳和环绕声音频、外向中心和内向中心视频、运动轨迹以及精细注释，如文本、音素边界、歌词、分数和提示。为了展示MRSAudio的实用性和通用性，我们建立了五个基本任务：音频定位、空间文本转语音、空间歌声合成、空间音乐生成以及声音事件定位和检测。结果表明，MRSAudio能够实现高质量的空间建模，并支持广泛的空间音频研究。相关演示和数据集访问地址：<a target="_blank" rel="noopener" href="https://mrsaudio.github.io./">https://mrsaudio.github.io。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>人类依赖多感官整合感知空间环境，听觉在定位声源方面发挥关键作用。</li>
<li>现有数据集多为单声道音频，限制了空间音频生成和理解的研究进展。</li>
<li>推出MRSAudio数据集，包含同步多模态数据（双耳音频、环绕声音频等），适用于多种真实场景。</li>
<li>MRSAudio涵盖多样化精细注释，为多种任务提供丰富数据。</li>
<li>建立五个基本任务以展示MRSAudio的实用性和通用性，包括音频定位、空间文本转语音等。</li>
<li>MRSAudio能够实现高质量的空间建模，对空间音频研究有广泛支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9894cf9eb14cf8a28f40f4f68e9f90dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753322&auth_key=1760753322-0-0-f654c39251f453f824eb5b4d20ea0775&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a482ecd4f7832d0ace316af507c83a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753330&auth_key=1760753330-0-0-7f7b7332358ee6262f30f9b7cc0c1928&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98c408eb971480b0f405a8a52bf40f2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753338&auth_key=1760753338-0-0-89a02384f941fc07112451a0cd4dbb42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3bf3278b24d675e8c64d7860212474b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753344&auth_key=1760753344-0-0-5d605508d2ff055fa118c3285b2a7f47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Unifying-Tree-Search-Algorithm-and-Reward-Design-for-LLM-Reasoning-A-Survey"><a href="#Unifying-Tree-Search-Algorithm-and-Reward-Design-for-LLM-Reasoning-A-Survey" class="headerlink" title="Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A   Survey"></a>Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A   Survey</h2><p><strong>Authors:Jiaqi Wei, Xiang Zhang, Yuejin Yang, Wenxuan Huang, Juntai Cao, Sheng Xu, Xiang Zhuang, Zhangyang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Chenyu You, Wanli Ouyang, Siqi Sun</strong></p>
<p>Deliberative tree search is a cornerstone of modern Large Language Model (LLM) research, driving the pivot from brute-force scaling toward algorithmic efficiency. This single paradigm unifies two critical frontiers: \textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve hard problems, and \textbf{Self-Improvement}, which uses search-generated data to durably enhance model parameters. However, this burgeoning field is fragmented and lacks a common formalism, particularly concerning the ambiguous role of the reward signal – is it a transient heuristic or a durable learning target? This paper resolves this ambiguity by introducing a unified framework that deconstructs search algorithms into three core components: the \emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition Function}. We establish a formal distinction between transient \textbf{Search Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for Self-Improvement. Building on this formalism, we introduce a component-centric taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward more systematic progress in creating autonomous, self-improving agents. </p>
<blockquote>
<p>决策树搜索是现代大型语言模型（LLM）研究的基石，推动了从粗暴扩展转向算法效率的转变。这一单一范式统一了两个关键前沿领域：<strong>测试时间缩放（TTS）</strong>，它按需部署计算来解决难题，以及<strong>自我改进</strong>，它使用搜索生成的数据来持久地增强模型参数。然而，这个新兴领域是零碎的，缺乏通用的形式化理论，特别是在奖励信号的不明确角色方面——它是短暂的启发式方法还是持久的学习目标？本文通过引入一个统一框架来解决这一模糊性，该框架将搜索算法分解为三个核心组件：<strong>搜索机制、奖励公式和转换函数</strong>。我们正式区分了用于TTS的短暂<strong>搜索指导</strong>和用于自我改进的持久<strong>参数奖励建模</strong>。基于这一形式化理论，我们引入了以组件为中心的分类学，综合了当前的研究状况，并为在创建自主、自我改进的智能体方面取得更有系统的进步绘制了研究路线图。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09988v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本讨论了现代大型语言模型（LLM）研究中的审慎树搜索的重要性，该范式融合了测试时间缩放（TTS）和自我改进两个关键前沿领域。然而，由于缺乏统一的理论框架，特别是关于奖励信号的模糊角色（是短暂的启发式还是持久的学习目标？）的问题，该领域的发展存在碎片化现象。本文解决这一模糊性，引入一个统一框架，将搜索算法分解为三个核心组件：搜索机制、奖励公式和转换函数。在此基础上，本文建立对TTS的短暂性搜索指导与自我改进的持久参数奖励建模之间的正式区别。为未来的研究开辟了一条道路，朝着创建自主、自我改进的智能体方面取得更有系统的进步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>审慎树搜索是现代大型语言模型研究的基石，融合了测试时间缩放和自我改进两个关键领域。</li>
<li>缺乏统一的理论框架是该领域发展的主要问题之一，特别是在奖励信号的模糊角色方面。</li>
<li>本文引入一个统一框架来解决这一问题，将搜索算法分解为三个核心组件：搜索机制、奖励公式和转换函数。</li>
<li>本文建立了对测试时间缩放的短暂性搜索指导和自我改进的持久参数奖励建模之间的正式区别。</li>
<li>该框架有助于理解搜索算法中各个组件的作用和相互关系，为未来的研究提供了基础。</li>
<li>基于该框架，本文进行了最新的先进技术的梳理和研究路线图规划。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1851c40c4f0f43168d87335b5c952cdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753352&auth_key=1760753352-0-0-e53b57a4e408e3f71b6ff92987d5efcb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-534e32e16297156ea74ab5bb77911b20~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753359&auth_key=1760753359-0-0-dd868a67876c3a5a81c0c147a85e5fe1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="O-O-VC-Synthetic-Data-Driven-One-to-One-Alignment-for-Any-to-Any-Voice-Conversion"><a href="#O-O-VC-Synthetic-Data-Driven-One-to-One-Alignment-for-Any-to-Any-Voice-Conversion" class="headerlink" title="O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice   Conversion"></a>O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice   Conversion</h2><p><strong>Authors:Huu Tuong Tu, Huan Vu, cuong tien nguyen, Dien Hy Ngo, Nguyen Thi Thu Trang</strong></p>
<p>Traditional voice conversion (VC) methods typically attempt to separate speaker identity and linguistic information into distinct representations, which are then combined to reconstruct the audio. However, effectively disentangling these factors remains challenging, often leading to information loss during training. In this paper, we propose a new approach that leverages synthetic speech data generated by a high-quality, pretrained multispeaker text-to-speech (TTS) model. Specifically, synthetic data pairs that share the same linguistic content but differ in speaker identity are used as input-output pairs to train the voice conversion model. This enables the model to learn a direct mapping between source and target voices, effectively capturing speaker-specific characteristics while preserving linguistic content. Additionally, we introduce a flexible training strategy for any-to-any voice conversion that generalizes well to unseen speakers and new languages, enhancing adaptability and performance in zero-shot scenarios. Our experiments show that our proposed method achieves a 16.35% relative reduction in word error rate and a 5.91% improvement in speaker cosine similarity, outperforming several state-of-the-art methods. Voice conversion samples can be accessed at: <a target="_blank" rel="noopener" href="https://oovc-emnlp-2025.github.io/">https://oovc-emnlp-2025.github.io/</a> </p>
<blockquote>
<p>传统语音转换（VC）方法通常尝试将说话人身份和语音信息分离成不同的表示形式，然后将它们结合起来重建音频。然而，有效地解开这些因素仍然具有挑战性，通常会导致训练过程中的信息丢失。在本文中，我们提出了一种新方法，该方法利用由高质量预训练多说话人文本到语音（TTS）模型生成的人造语音数据。具体来说，人造数据对共享相同的语言内容但说话人身份不同，被用作输入和输出来训练语音转换模型。这使得模型能够学习源语音和目标语音之间的直接映射，有效地捕捉说话人的特定特征，同时保留语言内容。此外，我们引入了一种灵活的任何到任何语音转换训练策略，它对于未见过的说话人和新语言具有良好的通用性，增强了零样本场景中的适应性和性能。我们的实验表明，我们提出的方法实现了单词错误率相对降低16.35%，说话人余弦相似性提高5.91%，优于几种最新方法。语音转换样本可访问：<a target="_blank" rel="noopener" href="https://oovc-emnlp-2025.github.io/">https://oovc-emnlp-2025.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09061v1">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong><br>语音转换（VC）的传统方法通常尝试将说话人身份和语言信息分离成不同的表示形式，然后再结合这些表示形式来重建音频。然而，有效地分离这些因素仍然具有挑战性，往往会导致训练过程中的信息丢失。本文提出了一种新方法，利用高质量预训练多说话人文本到语音（TTS）模型生成合成语音数据。合成数据对具有相同的语言内容但说话人身份不同，用作语音转换模型的输入和输出对，训练模型学习源语音和目标语音之间的直接映射关系，有效捕捉说话人的特征同时保留语言内容。此外，我们引入了一种灵活的任何到任何语音转换训练策略，能够很好地适应未见过的说话人和新语言，提高了零样本场景中的适应性和性能。实验表明，我们的方法实现了相对降低词错误率16.35%，说话人余弦相似性提高5.91%，优于几种最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统语音转换方法面临的挑战是有效地分离说话人身份和语言信息。</li>
<li>本文利用预训练的多说话人文本到语音（TTS）模型生成的合成语音数据来解决这一挑战。</li>
<li>合成数据对具有相同的语言内容但不同的说话人身份，用于训练语音转换模型。</li>
<li>模型学习源语音和目标语音之间的直接映射关系，同时保留语言内容并捕捉说话人的特征。</li>
<li>引入了一种灵活的任何到任何语音转换训练策略，适应未见过的说话人和新语言。</li>
<li>实验结果表明，该方法在词错误率和说话人余弦相似性方面取得了显著的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09061">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a0bc629c568b5fbfe0fbf155a09bd4f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753367&auth_key=1760753367-0-0-dfa14298b2fdcc56c2d0263e36991002&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df7811b536b3398043b4121e16997782~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753374&auth_key=1760753374-0-0-385891424584b9b544387678bc4e17e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models"><a href="#Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models" class="headerlink" title="Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models"></a>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models</h2><p><strong>Authors:Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</strong></p>
<p>Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: <a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</a> </p>
<blockquote>
<p>视频理解是计算机视觉领域最具挑战性的前沿方向，它要求模型能够推理复杂的时空关系、长期依赖关系和多模态证据。最近出现的视频大型多模态模型（Video-LMMs），集成了视觉编码器和基于解码器的强大语言模型，在视频理解任务中表现出了卓越的能力。然而，将这些模型从基本感知系统转变为先进推理引擎的关键阶段——后训练，在文献中仍然分散。这篇综述提供了对视频大型多模态模型后训练方法的首次全面研究，涵盖了三个基本支柱：通过思维链进行有监督微调（SFT）、基于可验证目标的强化学习（RL）以及通过增强推理计算进行测试时缩放（TTS）。我们提出了一个结构化分类法，阐明了这些方法的作用、相互关联以及针对视频的特定适应，应对诸如时间定位、时空定位、长视频效率和多模态证据融合等独特挑战。通过对代表性方法的系统分析，我们综合了关键的设计原则、见解和评估协议，同时确定了奖励设计、可扩展性和成本性能优化方面的关键开放挑战。我们还整理了重要的基准测试、数据集和指标，以便对后训练的有效性进行严格评估。本综述旨在为研究人员和从业者提供一个统一框架，以推进视频大型多模态模型的能力。更多资源和更新信息请访问：[<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training]">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05034v4">PDF</a> The 1st version</p>
<p><strong>摘要</strong></p>
<p>视频理解是计算机视觉领域最具挑战性的前沿课题，需要模型推理复杂的时空关系、长期依赖性和多模态证据。视频大型多模态模型（Video-LMMs）的出现，通过整合视觉编码器和基于语言模型的强大解码器，展现了视频理解任务的卓越能力。然而，将这些模型从基本感知系统转变为先进推理引擎的后训练阶段，在文献中仍然分散。本文首次全面调查了Video-LMMs的后训练方法论，包括三个基本支柱：以思维链进行的有监督微调（SFT）、以可验证目标进行强化学习（RL）以及通过增强推理计算进行的测试时缩放（TTS）。本文呈现了一个结构化分类法，阐明了这些方法的技术角色、相互关联以及针对视频的特定适应，应对独特挑战，如时间定位、时空定位、长视频效率和多模态证据整合等。通过对代表性方法的系统分析，我们综合了关键设计原则、见解和评估协议，同时确定了奖励设计、可扩展性和成本性能优化方面的关键开放挑战。此外，我们还整理了重要的基准测试、数据集和指标，以促进对后训练效果的严格评估。本文旨在为研究人员和从业者提供一个推进Video-LMM能力的统一框架。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频理解是计算机视觉中最具挑战性的任务，需要处理复杂的时空关系、长期依赖性和多模态证据。</li>
<li>Video-Large Multimodal Models（Video-LMMs）在视频理解任务上表现出卓越的能力，通过整合视觉编码器与语言模型。</li>
<li>后训练阶段对于将模型从基本感知系统转变为先进推理引擎至关重要。</li>
<li>后训练方法论包括三个基本支柱：有监督微调（SFT）、强化学习（RL）和测试时缩放（TTS）。</li>
<li>这些方法面临独特挑战，如时间定位、时空定位、长视频效率和多模态证据整合等。</li>
<li>系统分析了后训练方法的代表案例，并总结了关键设计原则、见解和评估协议。</li>
<li>存在奖励设计、可扩展性和成本性能优化等关键开放挑战，需要进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b50a53356bec6f9e47e9cb0c07d61095~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753382&auth_key=1760753382-0-0-fb96789df2ddec409641293b04a51e41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85a61878c24e7a8ad77236b533d85f14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753389&auth_key=1760753389-0-0-83f8e7dba420cecb7e33496cdca33f8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df7301cfa07ebc24146ffb38a5a4a919~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753396&auth_key=1760753396-0-0-c80ffd4ac8e468208506d88d129aa37e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MSR-Codec-A-Low-Bitrate-Multi-Stream-Residual-Codec-for-High-Fidelity-Speech-Generation-with-Information-Disentanglement"><a href="#MSR-Codec-A-Low-Bitrate-Multi-Stream-Residual-Codec-for-High-Fidelity-Speech-Generation-with-Information-Disentanglement" class="headerlink" title="MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement"></a>MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity   Speech Generation with Information Disentanglement</h2><p><strong>Authors:Jingyu Li, Guangyan Zhang, Zhen Ye, Yiwen Guo</strong></p>
<p>Audio codecs are a critical component of modern speech generation systems. This paper introduces a low-bitrate, multi-scale residual codec that encodes speech into four distinct streams: semantic, timbre, prosody, and residual. This architecture achieves high-fidelity speech reconstruction at competitive low bitrates while demonstrating an inherent ability for information disentanglement. We construct a two-stage language model for text-to-speech (TTS) synthesis using this codec, which, despite its lightweight design and minimal data requirements, achieves a state-of-the-art Word Error Rate (WER) and superior speaker similarity compared to several larger models. Furthermore, the codec’s design proves highly effective for voice conversion, enabling independent manipulation of speaker timbre and prosody. Our inference code, pre-trained models, and audio samples are available at <a target="_blank" rel="noopener" href="https://github.com/herbertLJY/MSRCodec">https://github.com/herbertLJY/MSRCodec</a>. </p>
<blockquote>
<p>音频编码是现代语音生成系统的关键组成部分。本文介绍了一种低比特率的多尺度残差编码器，该编码器将语音编码为四个不同的流：语义、音色、语调和残差。该架构在具有竞争力的低比特率下实现了高保真语音重建，并展示了固有的信息解纠缠能力。我们使用此编码器构建了一个用于文本到语音（TTS）合成的两阶段语言模型，尽管其设计轻巧且对数据要求较低，但仍达到了最先进的单词错误率（WER）和出色的说话人相似性，优于许多更大的模型。此外，编码器的设计对于语音转换非常有效，能够实现说话人的音色和语调独立操纵。我们的推理代码、预训练模型和音频样本可在<a target="_blank" rel="noopener" href="https://github.com/herbertLJY/MSRCodec%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/herbertLJY/MSRCodec中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13068v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种低比特率的多尺度残差音频编码技术，该技术将语音分为语义、音色、语调和残差四个独立流进行编码。该技术实现了在低比特率下的高保真语音重建，并展示了内在的信息分离能力。利用这种编码技术构建的两阶段语音合成语言模型，在轻量级设计和较少数据需求的情况下，达到了最先进的词错误率，并且具有较高的发音人相似性。此外，该编码技术对于语音转换非常有效，能够独立地操作发音人的音色和语调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新的低比特率的多尺度残差音频编码技术。</li>
<li>编码技术将语音分为语义、音色、语调和残差四个独立流。</li>
<li>技术实现了高保真语音重建，即使在低比特率下。</li>
<li>展示内在的信息分离能力。</li>
<li>两阶段语音合成语言模型达到了先进的词错误率。</li>
<li>模型在轻量级设计和少量数据需求下表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-319e2a2eea5dca5c2f5247d9707a8fda~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753404&auth_key=1760753404-0-0-668dfbda5307d8592772c35bbc461d60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc1df9ea0bc02d342ab02553890f3c27~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753411&auth_key=1760753411-0-0-ec598e246ed8eab8ff8f1dea703ac040&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-abe2209b6ef5a380449c8e746686e613~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753418&auth_key=1760753418-0-0-c512704b04fa783bd989676017b61c93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a14c31c569c97f5704ecf765bc56901~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753424&auth_key=1760753424-0-0-b344bf84d520dcad5854d17a5295d40a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f8bac2eeababfb358b9e0c21e4f6e74f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753431&auth_key=1760753431-0-0-e4aa97d8f9df868056be877e4844ce1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="NEP-Autoregressive-Image-Editing-via-Next-Editing-Token-Prediction"><a href="#NEP-Autoregressive-Image-Editing-via-Next-Editing-Token-Prediction" class="headerlink" title="NEP: Autoregressive Image Editing via Next Editing Token Prediction"></a>NEP: Autoregressive Image Editing via Next Editing Token Prediction</h2><p><strong>Authors:Huimin Wu, Xiaojian Ma, Haozhe Zhao, Yanpeng Zhao, Qing Li</strong></p>
<p>Text-guided image editing involves modifying a source image based on a language instruction and, typically, requires changes to only small local regions. However, existing approaches generate the entire target image rather than selectively regenerate only the intended editing areas. This results in (1) unnecessary computational costs and (2) a bias toward reconstructing non-editing regions, which compromises the quality of the intended edits. To resolve these limitations, we propose to formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation, where only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas. To enable any-region editing, we propose to pre-train an any-order autoregressive text-to-image (T2I) model. Once trained, it is capable of zero-shot image editing and can be easily adapted to NEP for image editing, which achieves a new state-of-the-art on widely used image editing benchmarks. Moreover, our model naturally supports test-time scaling (TTS) through iteratively refining its generation in a zero-shot manner. The project page is: <a target="_blank" rel="noopener" href="https://nep-bigai.github.io/">https://nep-bigai.github.io/</a> </p>
<blockquote>
<p>文本引导的图像编辑是根据语言指令修改源图像，通常只需要对较小的局部区域进行修改。然而，现有的方法会生成整个目标图像，而不是有选择地重新生成预期的编辑区域。这导致了（1）不必要的计算成本；（2）对非编辑区域的重建存在偏见，从而损害了预期的编辑质量。为了解决这些局限性，我们提出将图像编辑制定为基于自回归图像生成的下一个编辑令牌预测（NEP），其中仅重新生成需要编辑的区域，从而避免对非编辑区域的意外修改。为了实现任何区域的编辑，我们提出预训练一个任意顺序的自回归文本到图像（T2I）模型。一旦训练完成，它就能够进行零样本图像编辑，并且可以轻松地适应NEP用于图像编辑，在广泛使用的图像编辑基准测试中达到了新的先进水平。此外，我们的模型自然地支持测试时间缩放（TTS），通过以零样本的方式迭代优化其生成。项目页面是：<a target="_blank" rel="noopener" href="https://nep-bigai.github.io/">https://nep-bigai.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06044v2">PDF</a> The project page is: <a target="_blank" rel="noopener" href="https://nep-bigai.github.io/">https://nep-bigai.github.io/</a></p>
<p><strong>Summary</strong><br>文本引导的图像编辑是根据语言指令修改源图像，通常只需要改变小部分局部区域。然而，现有方法会生成整个目标图像，而非选择性重新生成仅意图编辑的区域，导致（1）不必要的计算成本；（2）重建非编辑区域的偏向，影响编辑质量。为解决这些限制，我们提出将图像编辑制定为基于自回归图像生成的下一个编辑令牌预测（NEP），仅重新生成需要编辑的区域，避免对非编辑区域的意外修改。为实现任意区域编辑，我们提出预训练一个任意顺序自回归文本到图像（T2I）模型。训练后，它能够实现零样本图像编辑，并易于适应NEP进行图像编辑，在广泛使用的图像编辑基准测试中达到最新水平。此外，我们的模型自然支持测试时缩放（TTS），通过零样本方式迭代优化其生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本引导的图像编辑主要集中于根据语言指令修改局部区域。</li>
<li>现有方法生成整个目标图像，导致不必要的计算成本和对非编辑区域的重建偏向。</li>
<li>提出将图像编辑制定为下一个编辑令牌预测（NEP）以解决此问题。</li>
<li>NEP仅重新生成需要编辑的区域，避免对非编辑区域的意外修改。</li>
<li>通过预训练任意顺序自回归文本到图像（T2I）模型来实现任意区域编辑。</li>
<li>模型能够实现零样本图像编辑，并在图像编辑基准测试中表现优异。</li>
<li>模型支持测试时缩放（TTS），通过迭代优化生成结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06044">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e09b6a1162fce99039a5f8853f20fee6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753438&auth_key=1760753438-0-0-b40b45cb03865725ab4fd762c5051960&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-591508eedfc6efc55e79ed54ddee2e8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753445&auth_key=1760753445-0-0-cf5cf7dbeb00fe9b851def18662d8f1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51bc2da91852bed1919856721e03bd14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753452&auth_key=1760753452-0-0-d7b2645489e5eaeb13e8a94123f82b8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66ed77644e6d0a9b0d3e0b937f447344~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753459&auth_key=1760753459-0-0-f9b117f95690c153addbd84841391183&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42f43aaad1411200ac62a64db2b6f1c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753465&auth_key=1760753465-0-0-33602d1806daf7fcecc699f756f461d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86cff562872671cecef234b406060434~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753471&auth_key=1760753471-0-0-e5107d381635b23202a4c5c94d5ee548&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Phonikud-Hebrew-Grapheme-to-Phoneme-Conversion-for-Real-Time-Text-to-Speech"><a href="#Phonikud-Hebrew-Grapheme-to-Phoneme-Conversion-for-Real-Time-Text-to-Speech" class="headerlink" title="Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time   Text-to-Speech"></a>Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time   Text-to-Speech</h2><p><strong>Authors:Yakov Kolani, Maxim Melichov, Cobi Calev, Morris Alper</strong></p>
<p>Real-time text-to-speech (TTS) for Modern Hebrew is challenging due to the language’s orthographic complexity. Existing solutions ignore crucial phonetic features such as stress that remain underspecified even when vowel marks are added. To address these limitations, we introduce Phonikud, a lightweight, open-source Hebrew grapheme-to-phoneme (G2P) system that outputs fully-specified IPA transcriptions. Our approach adapts an existing diacritization model with lightweight adaptors, incurring negligible additional latency. We also contribute the ILSpeech dataset of transcribed Hebrew speech with IPA annotations, serving as a benchmark for Hebrew G2P, as training data for TTS systems, and enabling audio-to-IPA for evaluating TTS performance while capturing important phonetic details. Our results demonstrate that Phonikud G2P conversion more accurately predicts phonemes from Hebrew text compared to prior methods, and that this enables training of effective real-time Hebrew TTS models with superior speed-accuracy trade-offs. We release our code, data, and models at https: &#x2F;&#x2F;phonikud.github.io. </p>
<blockquote>
<p>现代希伯来语的实时文本转语音（TTS）面临着由于该语言正字法复杂性所带来的挑战。现有解决方案忽略了重要的语音特征，如即使在添加元音标记后仍然未明确指定的重音。为了解决这些局限性，我们引入了Phonikud，这是一个轻量级的开源希伯来字母到音素（G2P）系统，可以输出完全指定的国际音标（IPA）转录。我们的方法通过轻量级适配器适应现有的注音模型，增加了几乎可以忽略的额外延迟。我们还贡献了带有国际音标注释的ILSpeech希伯来语语音转录数据集，该数据集作为希伯来字母到音素的比准测试，TTS系统的训练数据以及能够评估TTS性能同时捕捉重要语音细节的音频到国际音标。我们的结果表明，相较于之前的方法，Phonikud的G2P转换更能准确地从希伯来语文本预测音素，并且这能够训练出有效的实时希伯来语TTS模型，实现出色的速度准确性权衡。我们在<a target="_blank" rel="noopener" href="https://phonikud.github.io发布我们的代码、数据和模型./">https://phonikud.github.io发布我们的代码、数据和模型。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12311v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://phonikud.github.io/">https://phonikud.github.io</a></p>
<p><strong>摘要</strong></p>
<p>针对现代希伯来语的实时文本转语音（TTS）存在挑战，因希伯来语的字形复杂，现有解决方案忽略了重要的语音特征，如即使添加元音标记仍未明确指定的重音。为解决这些局限性，我们推出了Phonikud，这是一个轻量级的开源希伯来字母到音素（G2P）系统，可输出完全指定的国际音标（IPA）转录。我们的方法通过轻量级适配器适应现有的注音模型，几乎不会增加额外的延迟。我们还推出了ILSpeech数据集，包含希伯来语语音的IPA注释转录，作为希伯来语G2P的基准测试、TTS系统的训练数据，以及用于评估TTS性能的音频到IPA，能够捕捉重要的语音细节。我们的结果表明，相较于之前的方法，Phonikud的G2P转换可以更准确地从希伯来语文本预测音素，并且可以训练出高效实时的希伯来语TTS模型，实现速度和精度的良好平衡。我们的代码、数据和模型已发布在https：&#x2F;&#x2F;phonikud.github.io上。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>希伯来语实时文本转语音（TTS）面临挑战，因字形复杂导致现有解决方案难以处理。</li>
<li>Phonikud是一个轻量级的希伯来字母到音素（G2P）系统，能够输出完全指定的国际音标（IPA）转录。</li>
<li>Phonikud通过轻量级适配器改进了现有注音模型，几乎不增加额外延迟。</li>
<li>推出了ILSpeech数据集，包含希伯来语语音的IPA注释转录，为希伯来语G2P提供了基准测试。</li>
<li>Phonikud的G2P转换能更准确地预测希伯来语文本中的音素。</li>
<li>Phonikud有助于训练出高效实时的希伯来语TTS模型。</li>
<li>代码、数据和模型已公开发布，便于访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12311">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-eaef27ee53d0ebee62684d08be313d47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753480&auth_key=1760753480-0-0-c61e712c3fb8c4c10119d31832d5c5ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3875757975e05467da5c1c7f7983af4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753487&auth_key=1760753487-0-0-dd4eca11d56d0668d4787380c9defd25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-90fce1372e73bafb3cd32f74f3e74acd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753494&auth_key=1760753494-0-0-aa3d0965a65cf433646ddaaa481bd579&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Saten-Sparse-Augmented-Tensor-Networks-for-Post-Training-Compression-of-Large-Language-Models"><a href="#Saten-Sparse-Augmented-Tensor-Networks-for-Post-Training-Compression-of-Large-Language-Models" class="headerlink" title="Saten: Sparse Augmented Tensor Networks for Post-Training Compression of   Large Language Models"></a>Saten: Sparse Augmented Tensor Networks for Post-Training Compression of   Large Language Models</h2><p><strong>Authors:Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang</strong></p>
<p>The efficient implementation of large language models (LLMs) is crucial for deployment on resource-constrained devices. Low-rank tensor compression techniques, such as tensor-train (TT) networks, have been widely studied for over-parameterized neural networks. However, their applications to compress pre-trained large language models (LLMs) for downstream tasks (post-training) remains challenging due to the high-rank nature of pre-trained LLMs and the lack of access to pretraining data. In this study, we investigate low-rank tensorized LLMs during fine-tuning and propose sparse augmented tensor networks (Saten) to enhance their performance. The proposed Saten framework enables full model compression. Experimental results demonstrate that Saten enhances both accuracy and compression efficiency in tensorized language models, achieving state-of-the-art performance. </p>
<blockquote>
<p>大型语言模型（LLM）的有效实现对于在资源受限设备上进行部署至关重要。张量分解技术，如张量列车（TT）网络，已被广泛应用于参数过多的神经网络。然而，由于其高秩特性和无法访问预训练数据，将预训练的大型语言模型（LLM）应用于下游任务（后训练）进行压缩仍然具有挑战性。在这项研究中，我们调查了在微调期间进行张量化的大型语言模型，并提出了稀疏增强张量网络（Saten）来提高其性能。所提出的Saten框架能够实现全模型压缩。实验结果表明，Saten在提高了张量化语言模型的准确性和压缩效率的同时，达到了最先进的性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14871v2">PDF</a> Accepted to EMNLP 2025</p>
<p><strong>总结</strong></p>
<p>大型语言模型（LLM）的有效实现对于在资源受限设备上的部署至关重要。低秩张量压缩技术，如张量训练（TT）网络，已被广泛应用于过参数化的神经网络。然而，由于其高秩特性和缺乏预训练数据的访问权限，将这些技术应用于对预训练的大型语言模型（LLM）进行下游任务的压缩仍然具有挑战性。本研究调查了细调过程中的低秩张量化LLM，并提出了稀疏增强张量网络（Saten）以提高其性能。所提出的Saten框架能够实现全模型压缩。实验结果表明，Saten在提高张量化语言模型的准确性和压缩效率方面均达到先进水平。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）在资源受限设备上的有效实现非常重要。</li>
<li>低秩张量压缩技术广泛应用于过参数化神经网络。</li>
<li>将低秩张量压缩技术应用于预训练的大型语言模型（LLM）在下游任务上仍存在挑战，主要由于高秩特性和缺乏预训练数据的访问。</li>
<li>研究调查了细调过程中的低秩张量化LLM。</li>
<li>提出了稀疏增强张量网络（Saten）框架以提高低秩张量化LLM的性能。</li>
<li>Saten框架可实现全模型压缩。</li>
<li>实验结果表明，Saten在提升张量化语言模型的准确性和压缩效率方面表现卓越，达到先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14871">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-12b98894067edcc048d8059361b3ba85~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753501&auth_key=1760753501-0-0-c9ee413e3623f6e5bba9f3cb1d618d04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3862ab512a1a7d47697956085cfdeaf5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753508&auth_key=1760753508-0-0-9ce8a9a4929b0804581c61fd4c9f9852&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb2a2608d3a7a2af45c020fd6fa58f08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753515&auth_key=1760753515-0-0-c380b247914673382e0d1a7a5582cb27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b19d3126656cd9c1252fc11b5e57586~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753521&auth_key=1760753521-0-0-67cc7f84a8c65a80ad2a101e263fa74e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e29e934e9664c13549f8e399a5588ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753528&auth_key=1760753528-0-0-7130cb3e9322d869f4bf933349c06313&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2c4e9633cb4c6fd8e7aa998727f89d4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760754643&auth_key=1760754643-0-0-bb1c02fc9c629e95160f8f5fd3f44ec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-10-18  Evaluating & Reducing Deceptive Dialogue From Language Models with   Multi-turn RL
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-83794a71db03833ebd774b4f189266b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751315&auth_key=1760751315-0-0-2b58ecfae5c81d3df51fff56f8fd442e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-10-18  Sampling Density Compensation using Fast Fourier Deconvolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
