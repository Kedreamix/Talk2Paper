<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  CoT-PL Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-9131e57d65e898131cf44f4154f5d4ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827360&auth_key=1760827360-0-0-43a30a48882eaab0d7821289e670234e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection"><a href="#CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection" class="headerlink" title="CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection"></a>CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection</h2><p><strong>Authors:Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim</strong></p>
<p>Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰æ—¨åœ¨è¯†åˆ«å’Œå®šä½è¶…å‡ºè®­ç»ƒæœŸé—´æ‰€è§çš„å¯¹è±¡ç±»åˆ«ã€‚æœ€è¿‘çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡å›¾åƒæ–‡æœ¬å¯¹é½ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä½¿å¾—æ£€æµ‹å™¨èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§ç±»åˆ«è€Œæ— éœ€æ˜ç¡®ç›‘ç£ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸¥é‡ä¾èµ–äºç›´æ¥çš„å›¾åƒæ–‡æœ¬åŒ¹é…ï¼Œå¿½ç•¥äº†è§£é‡Šè¯­ä¹‰å¤æ‚åœºæ™¯æ‰€å¿…éœ€çš„ä¸­ä»‹æ¨ç†æ­¥éª¤ã€‚è¿™å¯¼è‡´åœ¨é¢å¯¹æ‹¥æŒ¤æˆ–é®æŒ¡çš„è§†è§‰ç¯å¢ƒæ—¶ç¨³å¥æ€§æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CoT-PLï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒå°†ç»“æ„åŒ–çš„è§†è§‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èå…¥ä¼ªæ ‡ç­¾ç”Ÿæˆè¿‡ç¨‹ã€‚CoT-PLå°†å¯¹è±¡ç†è§£åˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„æ­¥éª¤ï¼šï¼ˆ1ï¼‰å¯¹æœªçŸ¥å¯¹è±¡çš„åŒºåŸŸæ„ŸçŸ¥ï¼Œï¼ˆ2ï¼‰é€šè¿‡é›¶æ ·æœ¬æ¨ç†è¿›è¡Œç±»åˆ«è¯†åˆ«ï¼Œä»¥åŠï¼ˆ3ï¼‰èƒŒæ™¯å®šä½ä»¥åŒºåˆ†è¯­ä¹‰å¤æ‚çš„å¯¹è±¡ã€‚å…³é”®çš„æ˜¯ï¼Œç¬¬ä¸‰æ­¥è‡ªç„¶åœ°æ¿€åŠ±äº†æˆ‘ä»¬çš„å¯¹æ¯”èƒŒæ™¯å­¦ä¹ ï¼ˆCBLï¼‰ï¼Œå®ƒä½¿ç”¨é¢„å…ˆè®¡ç®—çš„èƒŒæ™¯çº¿ç´¢ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œä»¥ä¿ƒè¿›å¯¹è±¡å’ŒèƒŒæ™¯ç‰¹å¾ä¹‹é—´çš„ç‰¹å¾è§£è€¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCoTæ¨ç†å’ŒCBLå½¢æˆäº†ä¸€æ¡é’ˆå¯¹æ‹¥æŒ¤æˆ–é®æŒ¡åœºæ™¯ä¸­ç¨³å¥ä¼ªæ ‡ç­¾ç”Ÿæˆçš„é›†æˆç®¡é“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ä¸¤ç§åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–°å‹ä¼ªæ ‡ç­¾è´¨é‡åˆ†åˆ«å®ç°äº†ç›¸å¯¹äºæœ€ä½³å…ˆå‰æŠ€æœ¯çš„103.4%å’Œ168.4%çš„ç›¸å¯¹æ”¹è¿›ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoT-PLåœ¨å¼€æ”¾è¯æ±‡COCOä¸Šå®ç°äº†+7.7çš„AP50ï¼Œåœ¨LVISä¸Šå®ç°äº†+2.9çš„æ©è†œAPï¼ˆé’ˆå¯¹æ–°å‹ç±»åˆ«ï¼‰ï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯æ°´å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14792v1">PDF</a> 28 pages, 13 Figures, 12 Tables</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æ¡†æ¶CoT-PLï¼Œå®ƒç»“åˆäº†ç»“æ„åŒ–è§†è§‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œå¯¹æ¯”èƒŒæ™¯å­¦ä¹ ï¼ˆCBLï¼‰ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å¤æ‚åœºæ™¯æ—¶é‡åˆ°çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‹¥æŒ¤æˆ–é®æŒ¡çš„è§†è§‰ç¯å¢ƒä¸­ã€‚é€šè¿‡å¼•å…¥CoTæ¨ç†å’ŒCBLï¼ŒCoT-PLèƒ½å¤Ÿåˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„æ­¥éª¤ï¼ŒåŒ…æ‹¬åŒºåŸŸæ„ŸçŸ¥ã€ç±»åˆ«è¯†åˆ«å’ŒèƒŒæ™¯å®šä½ï¼Œä»è€Œæé«˜æœªè§è¿‡ç±»åˆ«å¯¹è±¡çš„æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰èƒ½è¯†åˆ«è®­ç»ƒæœŸé—´æœªè§è¿‡çš„å¯¹è±¡ç±»åˆ«ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å›¾åƒæ–‡æœ¬åŒ¹é…ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¿½è§†äº†å¤æ‚çš„åœºæ™¯ç†è§£æ­¥éª¤ã€‚</li>
<li>CoT-PLæ¡†æ¶å¼•å…¥ç»“æ„åŒ–è§†è§‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œåˆ†è§£å¯¹è±¡ç†è§£ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šåŒºåŸŸæ„ŸçŸ¥ã€ç±»åˆ«è¯†åˆ«å’ŒèƒŒæ™¯å®šä½ã€‚</li>
<li>å¯¹æ¯”èƒŒæ™¯å­¦ä¹ ï¼ˆCBLï¼‰æ–¹æ³•è¢«æå‡ºï¼Œåˆ©ç”¨é¢„è®¡ç®—çš„èƒŒæ™¯çº¿ç´¢ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œä¿ƒè¿›å¯¹è±¡å’ŒèƒŒæ™¯ç‰¹å¾çš„åˆ†ç¦»ã€‚</li>
<li>CoT-PLæ¡†æ¶æé«˜äº†åœ¨æ‹¥æŒ¤æˆ–é®æŒ¡åœºæ™¯ä¸­çš„ä¼ªæ ‡ç­¾è´¨é‡ï¼Œç›¸å¯¹ç°æœ‰æœ€ä½³æ–¹æ³•åˆ†åˆ«æé«˜äº†103.4%å’Œ168.4%ã€‚</li>
<li>åœ¨å¼€æ”¾è¯æ±‡COCOå’ŒLVISæ•°æ®é›†ä¸Šï¼ŒCoT-PLå®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†7.7 AP50å’Œ2.9 mask APã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a51571a7753a8b8f1b0df0f2f6e96076~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739900&auth_key=1760739900-0-0-47f793f96af29683b051607bd21553d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11ae2304570f3115aa020a2f8ca8e459~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739907&auth_key=1760739907-0-0-64d9393619bdbe4298562866d49ed5c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a7c94335e1212f32458fa6635cc6a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739914&auth_key=1760739914-0-0-fff3dfef56afaa8b90888db74af2bc60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a163431ddc0d286b1581671654c24eac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739920&auth_key=1760739920-0-0-55d66cd9bfaf2928f132930b38e6aa8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multiplicative-Loss-for-Enhancing-Semantic-Segmentation-in-Medical-and-Cellular-Images"><a href="#Multiplicative-Loss-for-Enhancing-Semantic-Segmentation-in-Medical-and-Cellular-Images" class="headerlink" title="Multiplicative Loss for Enhancing Semantic Segmentation in Medical and   Cellular Images"></a>Multiplicative Loss for Enhancing Semantic Segmentation in Medical and   Cellular Images</h2><p><strong>Authors:Yuto Yokoi, Kazuhiro Hotta</strong></p>
<p>We propose two novel loss functions, Multiplicative Loss and Confidence-Adaptive Multiplicative Loss, for semantic segmentation in medical and cellular images. Although Cross Entropy and Dice Loss are widely used, their additive combination is sensitive to hyperparameters and often performs suboptimally, especially with limited data. Medical images suffer from data scarcity due to privacy, ethics, and costly annotations, requiring robust and efficient training objectives. Our Multiplicative Loss combines Cross Entropy and Dice losses multiplicatively, dynamically modulating gradients based on prediction confidence. This reduces penalties for confident correct predictions and amplifies gradients for incorrect overconfident ones, stabilizing optimization. Building on this, Confidence-Adaptive Multiplicative Loss applies a confidence-driven exponential scaling inspired by Focal Loss, integrating predicted probabilities and Dice coefficients to emphasize difficult samples. This enhances learning under extreme data scarcity by strengthening gradients when confidence is low. Experiments on cellular and medical segmentation benchmarks show our framework consistently outperforms tuned additive and existing loss functions, offering a simple, effective, and hyperparameter-free mechanism for robust segmentation under challenging data limitations. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹åŒ»å­¦å’Œç»†èƒå›¾åƒè¯­ä¹‰åˆ†å‰²æå‡ºäº†ä¸¤ç§æ–°å‹æŸå¤±å‡½æ•°ï¼Œå³ä¹˜æ³•æŸå¤±å’Œç½®ä¿¡è‡ªé€‚åº”ä¹˜æ³•æŸå¤±ã€‚å°½ç®¡äº¤å‰ç†µå’ŒDice Losså·²å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶åŠ æ³•ç»„åˆå¯¹è¶…å‚æ•°æ•æ„Ÿï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ç”±äºéšç§ã€ä¼¦ç†å’Œæ˜‚è´µçš„æ ‡æ³¨é—®é¢˜ï¼ŒåŒ»ç–—å›¾åƒé¢ä¸´æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå› æ­¤éœ€è¦é²æ£’ä¸”é«˜æ•ˆçš„è®­ç»ƒç›®æ ‡ã€‚æˆ‘ä»¬çš„ä¹˜æ³•æŸå¤±å°†äº¤å‰ç†µå’ŒDiceæŸå¤±ç›¸ä¹˜ï¼ŒåŸºäºé¢„æµ‹ç½®ä¿¡åº¦åŠ¨æ€è°ƒèŠ‚æ¢¯åº¦ã€‚è¿™å‡å°‘äº†è‡ªä¿¡æ­£ç¡®é¢„æµ‹çš„æƒ©ç½šï¼Œå¹¶æ”¾å¤§äº†é”™è¯¯è‡ªä¿¡é¢„æµ‹çš„æ¢¯åº¦ï¼Œä»è€Œç¨³å®šäº†ä¼˜åŒ–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç½®ä¿¡è‡ªé€‚åº”ä¹˜æ³•æŸå¤±åº”ç”¨äº†ä¸€ç§å—Focal Losså¯å‘çš„ç½®ä¿¡é©±åŠ¨æŒ‡æ•°ç¼©æ”¾æ–¹æ³•ï¼Œå°†é¢„æµ‹æ¦‚ç‡å’ŒDiceç³»æ•°ç›¸ç»“åˆï¼Œä»¥å¼ºè°ƒå›°éš¾æ ·æœ¬ã€‚åœ¨ç»†èƒåŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é™åˆ¶æ¡ä»¶ä¸‹å§‹ç»ˆä¼˜äºç»è¿‡è°ƒæ•´çš„åŠ æ€§æŸå¤±å‡½æ•°å’Œç°æœ‰æŸå¤±å‡½æ•°ï¼Œæä¾›äº†ä¸€ç§ç®€å•ã€æœ‰æ•ˆä¸”æ— éœ€è¶…å‚æ•°æœºåˆ¶çš„ç¨³å¥åˆ†å‰²æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12258v1">PDF</a> Accepted by ICCV2025 Workshop â€œThird Workshop on Computer Vision for   Automated Medical Diagnosisâ€</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸¤ç§æ–°é¢–çš„é€‚ç”¨äºåŒ»å­¦å’Œç»†èƒå›¾åƒè¯­ä¹‰åˆ†å‰²çš„æŸå¤±å‡½æ•°ï¼šä¹˜æ³•æŸå¤±å’Œç½®ä¿¡è‡ªé€‚åº”ä¹˜æ³•æŸå¤±ã€‚è™½ç„¶äº¤å‰ç†µå’ŒDiceæŸå¤±å¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œä½†åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬çš„ç»„åˆå¯¹è¶…å‚æ•°æ•æ„Ÿä¸”æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºçš„ä¹˜æ³•æŸå¤±é€šè¿‡ä¹˜æ³•æ–¹å¼ç»“åˆäº¤å‰ç†µå’ŒDiceæŸå¤±ï¼Œæ ¹æ®é¢„æµ‹ç½®ä¿¡åº¦åŠ¨æ€è°ƒèŠ‚æ¢¯åº¦ï¼Œå‡å°‘å¯¹è‡ªä¿¡æ­£ç¡®é¢„æµ‹çš„æƒ©ç½šï¼Œå¹¶æ”¾å¤§é”™è¯¯è¿‡åº¦è‡ªä¿¡é¢„æµ‹çš„æ¢¯åº¦ï¼Œä»è€Œç¨³å®šä¼˜åŒ–è¿‡ç¨‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç½®ä¿¡è‡ªé€‚åº”ä¹˜æ³•æŸå¤±åº”ç”¨äº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„æŒ‡æ•°ç¼©æ”¾ç­–ç•¥ï¼Œé›†æˆé¢„æµ‹æ¦‚ç‡å’ŒDiceç³»æ•°ä»¥çªå‡ºå›°éš¾æ ·æœ¬ã€‚åœ¨ç»†èƒå’ŒåŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é™åˆ¶æ¡ä»¶ä¸‹ï¼Œå§‹ç»ˆä¼˜äºè°ƒä¼˜çš„æ·»åŠ æŸå¤±å’Œç°æœ‰æŸå¤±å‡½æ•°ï¼Œä¸ºç¨³å¥åˆ†å‰²æä¾›äº†ä¸€ä¸ªç®€å•ã€æœ‰æ•ˆä¸”æ— éœ€è¶…å‚æ•°è°ƒæ•´çš„æ–°æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸¤ç§æ–°çš„æŸå¤±å‡½æ•°ï¼šä¹˜æ³•æŸå¤±å’Œç½®ä¿¡è‡ªé€‚åº”ä¹˜æ³•æŸå¤±ï¼Œç”¨äºåŒ»å­¦å’Œç»†èƒå›¾åƒçš„è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>é’ˆå¯¹æ•°æ®ç¨€ç¼ºçš„åŒ»å­¦å›¾åƒé—®é¢˜ï¼Œéœ€è¦ç¨³å¥å’Œé«˜æ•ˆçš„è®­ç»ƒç›®æ ‡ã€‚</li>
<li>ä¹˜æ³•æŸå¤±ç»“åˆäº¤å‰ç†µå’ŒDiceæŸå¤±ï¼Œé€šè¿‡ä¹˜æ³•æ–¹å¼ç»„åˆï¼Œæ ¹æ®é¢„æµ‹ç½®ä¿¡åº¦åŠ¨æ€è°ƒèŠ‚æ¢¯åº¦ã€‚</li>
<li>ç½®ä¿¡è‡ªé€‚åº”ä¹˜æ³•æŸå¤±åº”ç”¨äº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„æŒ‡æ•°ç¼©æ”¾ç­–ç•¥ï¼Œé›†æˆé¢„æµ‹æ¦‚ç‡å’ŒDiceç³»æ•°ï¼Œå¼ºè°ƒå›°éš¾æ ·æœ¬çš„å­¦ä¹ ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶åœ¨ç»†èƒå’ŒåŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºå…¶ä»–æŸå¤±å‡½æ•°å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ–°æ¡†æ¶é€‚ç”¨äºæ•°æ®æœ‰é™çš„æƒ…å†µï¼Œå¯¹äºæŒ‘æˆ˜æ€§çš„æ•°æ®é™åˆ¶æ¡ä»¶å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-deef09f4ddbe938c97d15562d06f9c2f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739927&auth_key=1760739927-0-0-82dedc1f18a250a82d53ca17b0b5c916&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c9a75d51e1087d2a05507b64615420c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739935&auth_key=1760739935-0-0-723d701667838fcd3c6e57234c9857ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c03a602a834d29440fc681276a925b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739942&auth_key=1760739942-0-0-059f56e84600853f565e7f8cec586005&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04ad1e888458d1b131c3f447eea83dba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739948&auth_key=1760739948-0-0-cd553d78a99f6a8724b5d5135075d775&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0830b18ac6dcd1a7a0afc3b63a266f6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739955&auth_key=1760739955-0-0-e988dcbe913926d159d685d726778482&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fc3840c74d43ba79267a1ab8ba72d9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739962&auth_key=1760739962-0-0-bcf4687c20aad25c79f894a92f71df97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="APGNet-Adaptive-Prior-Guided-for-Underwater-Camouflaged-Object-Detection"><a href="#APGNet-Adaptive-Prior-Guided-for-Underwater-Camouflaged-Object-Detection" class="headerlink" title="APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object   Detection"></a>APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object   Detection</h2><p><strong>Authors:Xinxin Huang, Han Sun, Junmin Cai, Ningzhong Liu, Huiyu Zhou</strong></p>
<p>Detecting camouflaged objects in underwater environments is crucial for marine ecological research and resource exploration. However, existing methods face two key challenges: underwater image degradation, including low contrast and color distortion, and the natural camouflage of marine organisms. Traditional image enhancement techniques struggle to restore critical features in degraded images, while camouflaged object detection (COD) methods developed for terrestrial scenes often fail to adapt to underwater environments due to the lack of consideration for underwater optical characteristics.   To address these issues, we propose APGNet, an Adaptive Prior-Guided Network, which integrates a Siamese architecture with a novel prior-guided mechanism to enhance robustness and detection accuracy. First, we employ the Multi-Scale Retinex with Color Restoration (MSRCR) algorithm for data augmentation, generating illumination-invariant images to mitigate degradation effects. Second, we design an Extended Receptive Field (ERF) module combined with a Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual information and refine feature representations. Furthermore, we propose an adaptive prior-guided mechanism that hierarchically fuses position and boundary priors by embedding spatial attention in high-level features for coarse localization and using deformable convolution to refine contours in low-level features.   Extensive experimental results on two public MAS datasets demonstrate that our proposed method APGNet outperforms 15 state-of-art methods under widely used evaluation metrics. </p>
<blockquote>
<p>åœ¨æ°´ä¸‹ç¯å¢ƒä¸­æ£€æµ‹éšè”½ç‰©ä½“å¯¹æµ·æ´‹ç”Ÿæ€ç ”ç©¶å’Œèµ„æºå‹˜æ¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ°´ä¸‹å›¾åƒé€€åŒ–ï¼ŒåŒ…æ‹¬å¯¹æ¯”åº¦ä½å’Œé¢œè‰²å¤±çœŸï¼Œä»¥åŠæµ·æ´‹ç”Ÿç‰©çš„è‡ªç„¶ä¼ªè£…ã€‚ä¼ ç»Ÿå›¾åƒå¢å¼ºæŠ€æœ¯åœ¨æ¢å¤é€€åŒ–å›¾åƒçš„å…³é”®ç‰¹å¾æ—¶è¡¨ç°æŒ£æ‰ï¼Œè€Œé’ˆå¯¹é™†åœ°åœºæ™¯å¼€å‘çš„éšè”½ç‰©ä½“æ£€æµ‹ï¼ˆCODï¼‰æ–¹æ³•å¾€å¾€æ— æ³•é€‚åº”æ°´ä¸‹ç¯å¢ƒï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰è€ƒè™‘åˆ°æ°´ä¸‹å…‰å­¦ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12056v1">PDF</a> 6 pages. accepted by ACM MM Asia 2025</p>
<p><strong>Summary</strong><br>æ°´ä¸‹ä¼ªè£…ç‰©ä½“æ£€æµ‹å¯¹äºæµ·æ´‹ç”Ÿæ€ç ”ç©¶å’Œèµ„æºæ¢ç´¢è‡³å…³é‡è¦ã€‚å½“å‰æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ°´ä¸‹å›¾åƒé€€åŒ–å’Œæµ·æ´‹ç”Ÿç‰©çš„å¤©ç„¶ä¼ªè£…ã€‚æå‡ºçš„APGNetç½‘ç»œç»“åˆSiameseæ¶æ„å’Œæ–°é¢–å…ˆéªŒå¼•å¯¼æœºåˆ¶ï¼Œæé«˜ç¨³å¥æ€§å’Œæ£€æµ‹ç²¾åº¦ã€‚é‡‡ç”¨MSRCRç®—æ³•è¿›è¡Œæ•°æ®å¢å¼ºï¼Œç”Ÿæˆå…‰ç…§ä¸å˜å›¾åƒä»¥å‡è½»é€€åŒ–å½±å“ã€‚è®¾è®¡ERFæ¨¡å—å’ŒMPDè§£ç å™¨ï¼Œæ•æ‰å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚æå‡ºè‡ªé€‚åº”å…ˆéªŒå¼•å¯¼æœºåˆ¶ï¼Œåˆ†å±‚èåˆä½ç½®å’Œè¾¹ç•Œå…ˆéªŒï¼ŒåµŒå…¥é«˜çº§ç‰¹å¾ä¸­çš„ç©ºé—´æ³¨æ„åŠ›å®ç°ç²—ç•¥å®šä½ï¼Œå¹¶ä½¿ç”¨å¯å˜å½¢å·ç§¯ä¼˜åŒ–ä½çº§ç‰¹å¾ä¸­çš„è½®å»“ã€‚åœ¨å…¬å…±MASæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAPGNetä¼˜äºå…¶ä»–15ç§å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹ä¼ªè£…ç‰©ä½“æ£€æµ‹åœ¨æµ·æ´‹ç”Ÿæ€ç ”ç©¶å’Œèµ„æºæ¢ç´¢ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´æ°´ä¸‹å›¾åƒé€€åŒ–å’Œæµ·æ´‹ç”Ÿç‰©è‡ªç„¶ä¼ªè£…ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>APGNetç½‘ç»œé€šè¿‡ç»“åˆSiameseæ¶æ„å’Œæ–°é¢–å…ˆéªŒå¼•å¯¼æœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨MSRCRç®—æ³•è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä»¥å‡è½»å›¾åƒé€€åŒ–çš„å½±å“ã€‚</li>
<li>ERFæ¨¡å—å’ŒMPDè§£ç å™¨çš„è®¾è®¡ç”¨äºæ•æ‰å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>è‡ªé€‚åº”å…ˆéªŒå¼•å¯¼æœºåˆ¶ç»“åˆäº†ä½ç½®å’Œè¾¹ç•Œå…ˆéªŒï¼Œä»¥å®ç°ç²—ç•¥å®šä½å’Œè½®å»“ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-126af9de147c0132c976f9d58850227f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739969&auth_key=1760739969-0-0-c8b37c80545a5d8bf18a43944d342b98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b5e9b160a7c3cd6c3226a526ba0416c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739976&auth_key=1760739976-0-0-f9624d1baad4b4ab9de53da8758fb6d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8e864548451e851c228a37bc896d6ca0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739983&auth_key=1760739983-0-0-2635a4541bad7947cdddf634bbd8e265&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e345e15cd098e7561a40a3ad0c26510~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739989&auth_key=1760739989-0-0-29b506ab18a2b5879f587fb1a4f4eeb4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d31fe4376287141395d207066808a470~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739996&auth_key=1760739996-0-0-d4edf91450a2ead3d0447f53934464d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b29177254a542d434d7680d942ef7bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740003&auth_key=1760740003-0-0-46a31d9c66be5dce0eb9a3377e5377a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation"><a href="#A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation" class="headerlink" title="A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation"></a>A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation</h2><p><strong>Authors:Denis Zavadski, Damjan KalÅ¡an, Tim KÃ¼chler, Haebom Lee, Stefan Roth, Carsten Rother</strong></p>
<p>Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding. </p>
<blockquote>
<p>åˆæˆæ•°æ®é›†å¹¿æ³›ç”¨äºè®­ç»ƒåŸå¸‚åœºæ™¯è¯†åˆ«æ¨¡å‹ï¼Œä½†å³ä½¿æ˜¯éå¸¸é€¼çœŸçš„æ¸²æŸ“ä¹Ÿä¸çœŸå®å›¾åƒä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚åœ¨é€‚åº”ç‰¹å®šç›®æ ‡åŸŸï¼ˆä¾‹å¦‚Cityscapesï¼‰æ—¶ï¼Œè¿™ç§å·®è·å°¤ä¸ºçªå‡ºï¼Œç›®æ ‡åŸŸä¸­çš„å»ºç­‘ã€æ¤è¢«ã€ç‰©ä½“å¤–è§‚å’Œç›¸æœºç‰¹æ€§çš„å·®å¼‚é™åˆ¶äº†ä¸‹æ¸¸æ€§èƒ½ã€‚ä½¿ç”¨æ›´è¯¦ç»†çš„3Då»ºæ¨¡æ¥å¼¥è¡¥è¿™ä¸€å·®è·å°†éœ€è¦æ˜‚è´µçš„èµ„äº§å’Œåœºæ™¯è®¾è®¡ï¼Œè¿™è¿èƒŒäº†ä½æˆæœ¬æ ‡æ³¨æ•°æ®çš„åˆè¡·ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ä»…ä¸å®Œç¾çš„ä¼ªæ ‡ç­¾æ¥é€‚åº”ç°æˆçš„æ‰©æ•£æ¨¡å‹ä»¥åŒ¹é…ç›®æ ‡åŸŸã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œå®ƒå°±å¯ä»¥ä»ä»»ä½•åˆæˆæ•°æ®é›†ï¼ˆåŒ…æ‹¬å‡ å°æ—¶è€Œéæ•°æœˆåˆ›å»ºçš„ä½æŠ•å…¥æ¥æºï¼‰çš„è¯­ä¹‰å›¾ä¸­ç”Ÿæˆé«˜ä¿çœŸã€ä¸ç›®æ ‡ç›¸åŒ¹é…çš„å›¾ç‰‡ã€‚è¯¥æ–¹æ³•ä¼šè¿‡æ»¤æ‰æ¬¡ä¼˜ç”Ÿæˆï¼Œçº æ­£å›¾åƒæ ‡ç­¾çš„é”™ä½ï¼Œå¹¶æ ‡å‡†åŒ–è·¨æ•°æ®é›†è¯­ä¹‰ï¼Œå°†è–„å¼±çš„åˆæˆæ•°æ®è½¬åŒ–ä¸ºå…·æœ‰ç«äº‰åŠ›çš„çœŸå®åŸŸè®­ç»ƒé›†ã€‚åœ¨äº”ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®ç›®æ ‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†å‰²æŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†é«˜è¾¾+8.0%çš„mIoUã€‚è¿™ä½¿å¾—å¿«é€Ÿæ„å»ºçš„åˆæˆæ•°æ®é›†ä¸éœ€è¦å¤§é‡æ‰‹åŠ¨è®¾è®¡çš„è€—æ—¶ã€é«˜æŠ•å…¥çš„åˆæˆæ•°æ®é›†ä¸€æ ·æœ‰æ•ˆã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åä½œæ¨¡å¼ï¼Œå³å¿«é€Ÿè¯­ä¹‰åŸå‹è®¾è®¡ä¸ç”Ÿæˆæ¨¡å‹çš„ç»“åˆï¼Œå¯ä¸ºåŸå¸‚åœºæ™¯ç†è§£å®ç°å¯æ‰©å±•çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®åˆ›å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11567v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸å®Œç¾çš„ä¼ªæ ‡ç­¾å°†ä¸€ä¸ªç°æˆçš„æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç›®æ ‡åŸŸã€‚æ­¤æ¡†æ¶é€šè¿‡è¯­ä¹‰åœ°å›¾ä»ä»»ä½•åˆæˆæ•°æ®é›†ä¸­ç”Ÿæˆé«˜ä¿çœŸã€ä¸ç›®æ ‡å¯¹é½çš„å›¾åƒï¼Œå¯ä»¥å¤„ç†ä½æˆæœ¬çš„åˆæˆæ•°æ®ã€‚ç»è¿‡è®­ç»ƒåï¼Œè¯¥æ–¹æ³•èƒ½è¿‡æ»¤å‡ºä¸ä½³çš„ç”Ÿæˆå›¾åƒï¼Œä¿®æ­£å›¾åƒæ ‡ç­¾ä¸å¯¹é½çš„é—®é¢˜ï¼Œæ ‡å‡†åŒ–è·¨æ•°æ®é›†çš„è¯­ä¹‰ï¼Œä»è€Œå°†å¼±åˆæˆæ•°æ®è½¬åŒ–ä¸ºæœ‰ç«äº‰åŠ›çš„çœŸå®åŸŸè®­ç»ƒé›†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®ç›®æ ‡æ•°æ®é›†ä¸Šçš„åˆ†å‰²å¢ç›Šè¾¾åˆ°æœ€é«˜8%ï¼Œä½¿å¿«é€Ÿæ„å»ºçš„åˆæˆæ•°æ®é›†ä¸é«˜æˆæœ¬è€—æ—¶çš„å¤§é‡æ‰‹åŠ¨è®¾è®¡çš„åˆæˆæ•°æ®é›†å…·æœ‰åŒç­‰æ•ˆæœã€‚è¿™é¡¹ç ”ç©¶çªæ˜¾äº†ä¸€ç§åˆä½œèŒƒä¾‹ï¼Œé€šè¿‡å¿«é€Ÿè¯­ä¹‰åŸå‹ä¸ç”Ÿæˆæ¨¡å‹çš„ç»“åˆï¼Œå®ç°äº†å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„å¿«é€Ÿåˆ›å»ºï¼Œä¸ºåŸå¸‚åœºæ™¯ç†è§£æä¾›æœ‰åŠ›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆæˆæ•°æ®é›†å¹¿æ³›ç”¨äºè®­ç»ƒåŸå¸‚åœºæ™¯è¯†åˆ«æ¨¡å‹ï¼Œä½†ä¸ç°å®å›¾åƒå­˜åœ¨å·®è·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡æ›´è¯¦ç»†çš„3Då»ºæ¨¡æ¥ç¼©å°å·®è·ä¼šå¢åŠ æˆæœ¬å’Œè€—æ—¶ï¼Œä¸ç¬¦åˆä½æˆæœ¬æ ‡ç­¾æ•°æ®çš„åˆè¡·ã€‚</li>
<li>æ–°æ¡†æ¶é€‚åº”ç›®æ ‡åŸŸä»…ä½¿ç”¨ä¸å®Œç¾çš„ä¼ªæ ‡ç­¾ï¼Œèƒ½å¿«é€Ÿä»è¯­ä¹‰åœ°å›¾ç”Ÿæˆé«˜ä¿çœŸã€ç›®æ ‡å¯¹é½çš„å›¾åƒã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿè¿‡æ»¤ä¸ä½³ç”Ÿæˆå›¾åƒï¼Œä¿®æ­£å›¾åƒæ ‡ç­¾ä¸å¯¹é½é—®é¢˜ï¼Œæ ‡å‡†åŒ–è·¨æ•°æ®é›†çš„è¯­ä¹‰ã€‚</li>
<li>ä¸ç°æœ‰ç¿»è¯‘æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åˆ†å‰²å¢ç›Šæ˜¾è‘—ã€‚</li>
<li>å¿«é€Ÿè¯­ä¹‰åŸå‹ä¸ç”Ÿæˆæ¨¡å‹çš„ç»“åˆæé«˜äº†åˆæˆæ•°æ®çš„è´¨é‡ï¼Œä½¿å…¶æ¥è¿‘çœŸå®æ•°æ®çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-44f56406048b2d3b9f0254bf7305b839~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740011&auth_key=1760740011-0-0-5f3bd4557707da33d996331d2afbe452&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7c7959ed7552ebe8041cbf8ff70bfeb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740018&auth_key=1760740018-0-0-a9729201021dad328c9c73e5af8354e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf10e433c7c366ef0effd80719584c35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740025&auth_key=1760740025-0-0-8baceb2b72d39603eb78454c0b31894a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-Perspectives-Foundation-Model-Guided-BEV-Maps-for-3D-Object-Detection-and-Tracking"><a href="#Bridging-Perspectives-Foundation-Model-Guided-BEV-Maps-for-3D-Object-Detection-and-Tracking" class="headerlink" title="Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object   Detection and Tracking"></a>Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object   Detection and Tracking</h2><p><strong>Authors:Markus KÃ¤ppeler, Ã–zgÃ¼n Ã‡iÃ§ek, Daniele Cattaneo, Claudius GlÃ¤ser, Yakov Miron, Abhinav Valada</strong></p>
<p>Camera-based 3D object detection and tracking are essential for perception in autonomous driving. Current state-of-the-art approaches often rely exclusively on either perspective-view (PV) or birdâ€™s-eye-view (BEV) features, limiting their ability to leverage both fine-grained object details and spatially structured scene representations. In this work, we propose DualViewDistill, a hybrid detection and tracking framework that incorporates both PV and BEV camera image features to leverage their complementary strengths. Our approach introduces BEV maps guided by foundation models, leveraging descriptive DINOv2 features that are distilled into BEV representations through a novel distillation process. By integrating PV features with BEV maps enriched with semantic and geometric features from DINOv2, our model leverages this hybrid representation via deformable aggregation to enhance 3D object detection and tracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarks demonstrate that DualViewDistill achieves state-of-the-art performance. The results showcase the potential of foundation model BEV maps to enable more reliable perception for autonomous driving. We make the code and pre-trained models available at <a target="_blank" rel="noopener" href="https://dualviewdistill.cs.uni-freiburg.de/">https://dualviewdistill.cs.uni-freiburg.de</a> . </p>
<blockquote>
<p>åŸºäºæ‘„åƒå¤´çš„3Dç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªæ˜¯è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä¸­çš„å…³é”®æŠ€æœ¯ã€‚ç›®å‰æœ€å…ˆè¿›çš„æ–¹æ¡ˆé€šå¸¸åªä¾èµ–äºé€è§†è§†å›¾ï¼ˆPVï¼‰æˆ–é¸Ÿç°è§†å›¾ï¼ˆBEVï¼‰ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åˆ©ç”¨ç²¾ç»†ç›®æ ‡ç»†èŠ‚å’Œç©ºé—´ç»“æ„åŒ–åœºæ™¯è¡¨ç¤ºçš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DualViewDistillï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ£€æµ‹å’Œè·Ÿè¸ªæ¡†æ¶ï¼Œå®ƒç»“åˆäº†PVå’ŒBEVæ‘„åƒå¤´å›¾åƒç‰¹å¾ï¼Œä»¥åˆ©ç”¨å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç”±åŸºç¡€æ¨¡å‹å¼•å¯¼çš„BEVåœ°å›¾ï¼Œåˆ©ç”¨æè¿°æ€§çš„DINOv2ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾é€šè¿‡ä¸€ç§æ–°å‹è’¸é¦è¿‡ç¨‹è’¸é¦æˆBEVè¡¨ç¤ºã€‚é€šè¿‡å°†PVç‰¹å¾ä¸é€šè¿‡åŸºç¡€æ¨¡å‹DINOv2æä¾›çš„ä¸°å¯Œè¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾çš„BEVåœ°å›¾ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¯å˜å½¢èšåˆåˆ©ç”¨è¿™ç§æ··åˆè¡¨ç¤ºï¼Œä»¥æé«˜3Dç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªèƒ½åŠ›ã€‚åœ¨nuSceneså’ŒArgoverse 2åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDualViewDistillè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç»“æœå±•ç¤ºäº†åŸºç¡€æ¨¡å‹BEVåœ°å›¾åœ¨å®ç°æ›´å¯é çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://dualviewdistill.cs.uni-freiburg.deä¸Šæä¾›äº†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹./">https://dualviewdistill.cs.uni-freiburg.deä¸Šæä¾›äº†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10287v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºDualViewDistillçš„æ··åˆæ£€æµ‹ä¸è·Ÿè¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é€è§†è§†å›¾ï¼ˆPVï¼‰å’Œé¸Ÿç°è§†å›¾ï¼ˆBEVï¼‰ç›¸æœºå›¾åƒç‰¹å¾ï¼Œä»¥æé«˜è‡ªä¸»é©¾é©¶ä¸­çš„ä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸è·Ÿè¸ªæ€§èƒ½ã€‚é€šè¿‡å¼•å…¥ç”±åŸºç¡€æ¨¡å‹å¼•å¯¼çš„BEVåœ°å›¾ï¼Œå¹¶ç»“åˆé€è§†è§†å›¾ç‰¹å¾ï¼Œå®ç°äº†å¯¹ä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸è·Ÿè¸ªçš„å¢å¼ºã€‚åœ¨nuSceneså’ŒArgoverse 2åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªä¸»é©¾é©¶ä¸­ï¼Œä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸è·Ÿè¸ªè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–é€è§†è§†å›¾ï¼ˆPVï¼‰æˆ–é¸Ÿç°è§†å›¾ï¼ˆBEVï¼‰ç‰¹å¾ï¼Œå„æœ‰å±€é™æ€§ã€‚</li>
<li>DualViewDistillæ¡†æ¶ç»“åˆäº†PVå’ŒBEVç‰¹å¾ï¼Œå‘æŒ¥ä¸¤è€…ä¼˜åŠ¿ã€‚</li>
<li>å¼•å…¥åŸºç¡€æ¨¡å‹å¼•å¯¼çš„BEVåœ°å›¾ï¼Œé€šè¿‡æ–°é¢–è’¸é¦è¿‡ç¨‹è·å–DINOv2ç‰¹å¾ã€‚</li>
<li>ç»“åˆPVç‰¹å¾ä¸ä¸°å¯Œçš„BEVåœ°å›¾ï¼Œé€šè¿‡å¯å˜å½¢èšåˆå¢å¼ºä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸è·Ÿè¸ªã€‚</li>
<li>åœ¨nuSceneså’ŒArgoverse 2åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDualViewDistillè¾¾åˆ°å…ˆè¿›æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10287">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d50d8b502c1056ec8fa578db9b98b103~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740032&auth_key=1760740032-0-0-3173a6ef9959132defe70ff95f4884d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7b3ea18d4f41c3a68d23ed7c02f396a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740039&auth_key=1760740039-0-0-b144555effcc76395265579779149d2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d907ab171867bc3ec5b3668b132c1be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740046&auth_key=1760740046-0-0-487e1be28ccb2160c1a6fe3c2f5384e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f0b26a09dad3aa0643825865d46539e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740053&auth_key=1760740053-0-0-b30c4fcafb3dc9e080f24f0b4a8d4b59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-Single-Domain-Generalization-of-LiDAR-based-Semantic-Segmentation-under-Imperfect-Labels"><a href="#Exploring-Single-Domain-Generalization-of-LiDAR-based-Semantic-Segmentation-under-Imperfect-Labels" class="headerlink" title="Exploring Single Domain Generalization of LiDAR-based Semantic   Segmentation under Imperfect Labels"></a>Exploring Single Domain Generalization of LiDAR-based Semantic   Segmentation under Imperfect Labels</h2><p><strong>Authors:Weitong Kong, Zichao Zeng, Di Wen, Jiale Wei, Kunyu Peng, June Moh Goo, Jan Boehm, Rainer Stiefelhagen</strong></p>
<p>Accurate perception is critical for vehicle safety, with LiDAR as a key enabler in autonomous driving. To ensure robust performance across environments, sensor types, and weather conditions without costly re-annotation, domain generalization in LiDAR-based 3D semantic segmentation is essential. However, LiDAR annotations are often noisy due to sensor imperfections, occlusions, and human errors. Such noise degrades segmentation accuracy and is further amplified under domain shifts, threatening system reliability. While noisy-label learning is well-studied in images, its extension to 3D LiDAR segmentation under domain generalization remains largely unexplored, as the sparse and irregular structure of point clouds limits direct use of 2D methods. To address this gap, we introduce the novel task Domain Generalization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL) and establish the first benchmark by adapting three representative noisy-label learning strategies from image classification to 3D segmentation. However, we find that existing noisy-label learning approaches adapt poorly to LiDAR data. We therefore propose DuNe, a dual-view framework with strong and weak branches that enforce feature-level consistency and apply cross-entropy loss based on confidence-aware filtering of predictions. Our approach shows state-of-the-art performance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and 52.58% on SemanticPOSS under 10% symmetric label noise, with an overall Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby demonstrating robust domain generalization in DGLSS-NL tasks. The code is available on our project page. </p>
<blockquote>
<p>ç²¾ç¡®æ„ŸçŸ¥å¯¹äºè½¦è¾†å®‰å…¨è‡³å…³é‡è¦ï¼Œæ¿€å…‰é›·è¾¾ä½œä¸ºè‡ªåŠ¨é©¾é©¶çš„å…³é”®æŠ€æœ¯èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚ä¸ºäº†ç¡®ä¿åœ¨å„ç§ç¯å¢ƒã€ä¼ æ„Ÿå™¨ç±»å‹å’Œå¤©æ°”æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§èƒ½ï¼Œä¸”æ— éœ€æ˜‚è´µçš„é‡æ–°æ ‡æ³¨ï¼Œæ¿€å…‰é›·è¾¾åŸºäºåŸŸçš„3Dè¯­ä¹‰åˆ†å‰²æ³›åŒ–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºä¼ æ„Ÿå™¨ç¼ºé™·ã€é®æŒ¡å’Œäººä¸ºé”™è¯¯ï¼Œæ¿€å…‰é›·è¾¾æ ‡æ³¨å¾€å¾€å­˜åœ¨å™ªå£°ã€‚è¿™ç§å™ªå£°ä¼šé™ä½åˆ†å‰²ç²¾åº¦ï¼Œåœ¨åŸŸè½¬ç§»çš„æƒ…å†µä¸‹ä¼šè¿›ä¸€æ­¥æ”¾å¤§ï¼Œå¨èƒç³»ç»Ÿå¯é æ€§ã€‚è™½ç„¶å™ªå£°æ ‡ç­¾å­¦ä¹ åœ¨å›¾åƒä¸­å¾—åˆ°äº†å¾ˆå¥½çš„ç ”ç©¶ï¼Œä½†å…¶æ‰©å±•åˆ°åŸŸæ³›åŒ–ä¸‹çš„3Dæ¿€å…‰é›·è¾¾åˆ†å‰²ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ï¼Œç‚¹äº‘çš„ç¨€ç–å’Œä¸è§„åˆ™ç»“æ„é™åˆ¶äº†ç›´æ¥ä½¿ç”¨2Dæ–¹æ³•ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å™ªå£°æ ‡ç­¾ä¸‹çš„æ¿€å…‰é›·è¾¾è¯­ä¹‰åˆ†å‰²åŸŸæ³›åŒ–ï¼ˆDGLSS-NLï¼‰çš„æ–°ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å°†ä¸‰ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å™ªå£°æ ‡ç­¾å­¦ä¹ ç­–ç•¥ä»å›¾åƒåˆ†ç±»é€‚åº”åˆ°3Dåˆ†å‰²ï¼Œå»ºç«‹äº†ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰çš„å™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•å¯¹æ¿€å…‰é›·è¾¾æ•°æ®é€‚åº”æ€§è¾ƒå·®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DuNeï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å¼ºåˆ†æ”¯å’Œå¼±åˆ†æ”¯çš„åŒè§†å›¾æ¡†æ¶ï¼Œå®ƒå¼ºåˆ¶å®æ–½ç‰¹å¾çº§åˆ«çš„ä¸€è‡´æ€§ï¼Œå¹¶åŸºäºé¢„æµ‹çš„ä¿¡å¿ƒæ„ŸçŸ¥è¿‡æ»¤åº”ç”¨äº¤å‰ç†µæŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨SemanticKITTIä¸Šå®ç°äº†56.86%çš„mIoUï¼ŒnuScenesä¸Šå®ç°äº†42.28%ï¼ŒSemanticPOSSä¸Šå®ç°äº†52.58%ï¼Œåœ¨10%å¯¹ç§°æ ‡ç­¾å™ªå£°çš„æƒ…å†µä¸‹ï¼Œæ•´ä½“ç®—æœ¯å¹³å‡å€¼ï¼ˆAMï¼‰ä¸º49.57%ï¼Œè°ƒå’Œå¹³å‡å€¼ï¼ˆHMï¼‰ä¸º48.50%ï¼Œä»è€Œåœ¨DGLSS-NLä»»åŠ¡ä¸­å®ç°äº†ç¨³å¥çš„åŸŸæ³›åŒ–ã€‚ä»£ç å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09035v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†LiDARåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶é’ˆå¯¹LiDARæ ‡æ³¨å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†åŸŸæ³›åŒ–ä¸‹çš„LiDARè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼ˆDGLSS-NLï¼‰ã€‚ç”±äºç°æœ‰å™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•å¯¹LiDARæ•°æ®é€‚åº”æ€§è¾ƒå·®ï¼Œå› æ­¤æå‡ºä¸€ç§åŒè§†å›¾æ¡†æ¶DuNeï¼Œé€šè¿‡å¼ºå¼±åˆ†æ”¯å®ç°ç‰¹å¾çº§åˆ«çš„ä¸€è‡´æ€§ï¼Œå¹¶åº”ç”¨åŸºäºç½®ä¿¡åº¦è¿‡æ»¤çš„é¢„æµ‹äº¤å‰ç†µæŸå¤±ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LiDARåœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†å®‰å…¨ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä½†ç¯å¢ƒã€ä¼ æ„Ÿå™¨ç±»å‹å’Œå¤©æ°”æ¡ä»¶çš„å·®å¼‚è¦æ±‚ä¼ æ„Ÿå™¨å…·æœ‰åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LiDARæ ‡æ³¨å­˜åœ¨å™ªå£°é—®é¢˜ï¼Œå½±å“åˆ†å‰²ç²¾åº¦ï¼Œä¸”åœ¨åŸŸå˜åŒ–ä¸‹é—®é¢˜åŠ å‰§ã€‚</li>
<li>ç°æœ‰å™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•åœ¨LiDARæ•°æ®ä¸Šçš„é€‚åº”æ€§è¾ƒå·®ã€‚</li>
<li>æå‡ºäº†DGLSS-NLä»»åŠ¡ï¼Œå¹¶å»ºç«‹äº†ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ã€‚</li>
<li>æå‡ºäº†DuNeæ¡†æ¶ï¼Œé€šè¿‡å¼ºå¼±åˆ†æ”¯å®ç°ç‰¹å¾ä¸€è‡´æ€§ï¼Œå¹¶åº”ç”¨ç½®ä¿¡åº¦è¿‡æ»¤çš„é¢„æµ‹äº¤å‰ç†µæŸå¤±ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-833d7787dae54d0da2787dc3dfed2900~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740061&auth_key=1760740061-0-0-06e1eb266d88c034b70b8461027ae7a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d68607e4f383b3aa86cd594760b3bb62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740068&auth_key=1760740068-0-0-889401486203d57a66084ff2a0dc1285&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1eed66d7473cd0dbac240948e5fe2868~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740074&auth_key=1760740074-0-0-d0e329bf346fede74166ecdbb3512e98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4158c4bcd63acab11be7add9978a7783~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740081&auth_key=1760740081-0-0-548d3d67bae5855f12d92db9cedca556&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8fc3203d6d5c30fb19f9b20cd28fa81~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740088&auth_key=1760740088-0-0-a1548ec65538ba91e9835c1991392a3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FOLK-Fast-Open-Vocabulary-3D-Instance-Segmentation-via-Label-guided-Knowledge-Distillation"><a href="#FOLK-Fast-Open-Vocabulary-3D-Instance-Segmentation-via-Label-guided-Knowledge-Distillation" class="headerlink" title="FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided   Knowledge Distillation"></a>FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided   Knowledge Distillation</h2><p><strong>Authors:Hongrui Wu, Zhicheng Gao, Jin Cao, Kelu Yao, Wen Shen, Zhihua Wei</strong></p>
<p>Open-vocabulary 3D instance segmentation seeks to segment and classify instances beyond the annotated label space. Existing methods typically map 3D instances to 2D RGB-D images, and then employ vision-language models (VLMs) for classification. However, such a mapping strategy usually introduces noise from 2D occlusions and incurs substantial computational and memory costs during inference, slowing down the inference speed. To address the above problems, we propose a Fast Open-vocabulary 3D instance segmentation method via Label-guided Knowledge distillation (FOLK). Our core idea is to design a teacher model that extracts high-quality instance embeddings and distills its open-vocabulary knowledge into a 3D student model. In this way, during inference, the distilled 3D model can directly classify instances from the 3D point cloud, avoiding noise caused by occlusions and significantly accelerating the inference process. Specifically, we first design a teacher model to generate a 2D CLIP embedding for each 3D instance, incorporating both visibility and viewpoint diversity, which serves as the learning target for distillation. We then develop a 3D student model that directly produces a 3D embedding for each 3D instance. During training, we propose a label-guided distillation algorithm to distill open-vocabulary knowledge from label-consistent 2D embeddings into the student model. FOLK conducted experiments on the ScanNet200 and Replica datasets, achieving state-of-the-art performance on the ScanNet200 dataset with an AP50 score of 35.7, while running approximately 6.0x to 152.2x faster than previous methods. All codes will be released after the paper is accepted. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¡¨ä¸‹çš„ä¸‰ç»´å®ä¾‹åˆ†å‰²æ—¨åœ¨åˆ†å‰²å¹¶åˆ†ç±»æ ‡æ³¨æ ‡ç­¾ç©ºé—´ä¹‹å¤–çš„å®ä¾‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä¸‰ç»´å®ä¾‹æ˜ å°„åˆ°äºŒç»´RGB-Då›¾åƒä¸Šï¼Œç„¶åé‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œè¿™ç§æ˜ å°„ç­–ç•¥é€šå¸¸ä¼šå¼•å…¥æ¥è‡ªäºŒç»´é®æŒ¡çš„å™ªå£°ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå·¨å¤§çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œä»è€Œé™ä½äº†æ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ ‡ç­¾å¼•å¯¼çŸ¥è¯†è’¸é¦çš„å¿«é€Ÿå¼€æ”¾è¯æ±‡è¡¨ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼ˆFOLKï¼‰ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®¾è®¡ä¸€ç§æ•™å¸ˆæ¨¡å‹ï¼Œæå–é«˜è´¨é‡å®ä¾‹åµŒå…¥ï¼Œå¹¶å°†å…¶å¼€æ”¾è¯æ±‡è¡¨çŸ¥è¯†è’¸é¦åˆ°ä¸‰ç»´å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè’¸é¦åçš„ä¸‰ç»´æ¨¡å‹å¯ä»¥ç›´æ¥å¯¹ä¸‰ç»´ç‚¹äº‘ä¸­çš„å®ä¾‹è¿›è¡Œåˆ†ç±»ï¼Œé¿å…äº†é®æŒ¡å¼•èµ·çš„å™ªå£°ï¼Œå¹¶æ˜¾è‘—åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œä¸ºæ¯ä¸ªä¸‰ç»´å®ä¾‹ç”Ÿæˆä¸€ä¸ªäºŒç»´CLIPåµŒå…¥ï¼ŒåŒæ—¶è€ƒè™‘å¯è§æ€§å’Œè§†ç‚¹å¤šæ ·æ€§ï¼Œä½œä¸ºè’¸é¦çš„å­¦ä¹ ç›®æ ‡ã€‚ç„¶åæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰ç»´å­¦ç”Ÿæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç›´æ¥ä¸ºæ¯ä¸ªä¸‰ç»´å®ä¾‹ç”Ÿæˆä¸€ä¸ªä¸‰ç»´åµŒå…¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ ‡ç­¾å¼•å¯¼çš„è’¸é¦ç®—æ³•ï¼Œå°†æ ‡ç­¾ä¸€è‡´æ€§çš„äºŒç»´åµŒå…¥ä¸­çš„å¼€æ”¾è¯æ±‡è¡¨çŸ¥è¯†è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚FOLKåœ¨ScanNet200å’ŒReplicaæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨ScanNet200æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒAP50å¾—åˆ†ä¸º35.7ï¼ŒåŒæ—¶è¿è¡Œé€Ÿåº¦æ¯”ä»¥å‰çš„æ–¹æ³•å¿«å¤§çº¦6.0å€åˆ°152.2å€ã€‚è®ºæ–‡è¢«æ¥å—åå°†å‘å¸ƒæ‰€æœ‰ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08849v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ ‡ç­¾å¼•å¯¼çŸ¥è¯†è’¸é¦çš„å¿«é€Ÿå¼€æ”¾è¯æ±‡è¡¨ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼ˆFOLKï¼‰ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ç°æœ‰å¼€æ”¾è¯æ±‡è¡¨ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œè¯†åˆ«ç²¾åº¦æ–¹é¢çš„é—®é¢˜ã€‚é€šè¿‡å»ºç«‹æ•™å¸ˆæ¨¡å‹ï¼Œæå–é«˜è´¨é‡å®ä¾‹åµŒå…¥ï¼Œå¹¶è’¸é¦å…¶å¼€æ”¾è¯æ±‡è¡¨çŸ¥è¯†åˆ°ä¸‰ç»´å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œä½¿å¾—åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè’¸é¦åçš„ä¸‰ç»´æ¨¡å‹å¯ä»¥ç›´æ¥å¯¹ä¸‰ç»´ç‚¹äº‘ä¸­çš„å®ä¾‹è¿›è¡Œåˆ†ç±»ï¼Œé¿å…äº†äºŒç»´é®æŒ¡å¼•èµ·çš„å™ªå£°ï¼Œå¹¶æ˜¾è‘—åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFOLKæ–¹æ³•åœ¨ScanNet200æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ ‡ç­¾å¼•å¯¼çŸ¥è¯†è’¸é¦çš„å¼€æ”¾è¯æ±‡è¡¨ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼ˆFOLKï¼‰ã€‚</li>
<li>é€šè¿‡å»ºç«‹æ•™å¸ˆæ¨¡å‹æå–é«˜è´¨é‡å®ä¾‹åµŒå…¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œè¯†åˆ«ç²¾åº¦æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>è’¸é¦åçš„ä¸‰ç»´æ¨¡å‹å¯ä»¥ç›´æ¥å¯¹ä¸‰ç»´ç‚¹äº‘ä¸­çš„å®ä¾‹è¿›è¡Œåˆ†ç±»ï¼Œé¿å…äº†äºŒç»´é®æŒ¡å¼•èµ·çš„å™ªå£°ã€‚</li>
<li>FOLKæ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>åœ¨ScanNet200æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼ŒAP50åˆ†æ•°ä¸º35.7ã€‚</li>
<li>ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFOLKæ–¹æ³•çš„è®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œè¿è¡Œé€Ÿåº¦æé«˜äº†å¤§çº¦6.0è‡³152.2å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a09b90587882239cdbb878137b1aed8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740096&auth_key=1760740096-0-0-05d00193d0a92500adabc6c447c3ee18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-64a9e19faf869e5e0496eaf48044fc5f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740104&auth_key=1760740104-0-0-c9a89e68af1dedb78b8c49bfa9866943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95b2f94d321f32e7eabcf9e585e4cdd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740111&auth_key=1760740111-0-0-33bb1f95b34d151b31ad9cd5641a19a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-070a6b4bb736e17adc2d00640598bcc2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827367&auth_key=1760827367-0-0-6ac425cc871672ac157b0aa98dc20c28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-841cfb907c8a890cef644b1f1e00b732~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827375&auth_key=1760827375-0-0-22a3853512b44cd3c0a26ad13e2c3a7b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ALISE-Annotation-Free-LiDAR-Instance-Segmentation-for-Autonomous-Driving"><a href="#ALISE-Annotation-Free-LiDAR-Instance-Segmentation-for-Autonomous-Driving" class="headerlink" title="ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous   Driving"></a>ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous   Driving</h2><p><strong>Authors:Yongxuan Lyu, Guangfeng Jiang, Hongsi Liu, Jun Liu</strong></p>
<p>The manual annotation of outdoor LiDAR point clouds for instance segmentation is extremely costly and time-consuming. Current methods attempt to reduce this burden but still rely on some form of human labeling. To completely eliminate this dependency, we introduce ALISE, a novel framework that performs LiDAR instance segmentation without any annotations. The central challenge is to generate high-quality pseudo-labels in a fully unsupervised manner. Our approach starts by employing Vision Foundation Models (VFMs), guided by text and images, to produce initial pseudo-labels. We then refine these labels through a dedicated spatio-temporal voting module, which combines 2D and 3D semantics for both offline and online optimization. To achieve superior feature learning, we further introduce two forms of semantic supervision: a set of 2D prior-based losses that inject visual knowledge into the 3D network, and a novel prototype-based contrastive loss that builds a discriminative feature space by exploiting 3D semantic consistency. This comprehensive design results in significant performance gains, establishing a new state-of-the-art for unsupervised 3D instance segmentation. Remarkably, our approach even outperforms MWSIS, a method that operates with supervision from ground-truth (GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%). </p>
<blockquote>
<p>å®¤å¤–æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„å®ä¾‹åˆ†å‰²æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬æé«˜ä¸”è€—æ—¶ã€‚å½“å‰çš„æ–¹æ³•è¯•å›¾å‡å°‘è¿™ä¸€è´Ÿæ‹…ï¼Œä½†ä»ä¾èµ–äºæŸç§å½¢å¼çš„äººå·¥æ ‡æ³¨ã€‚ä¸ºäº†å®Œå…¨æ¶ˆé™¤è¿™ç§ä¾èµ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ALISEè¿™ä¸€å…¨æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åœ¨æ— éœ€ä»»ä½•æ ‡æ³¨çš„æƒ…å†µä¸‹æ‰§è¡Œæ¿€å…‰é›·è¾¾å®ä¾‹åˆ†å‰²ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºä»¥å®Œå…¨æ— ç›‘ç£çš„æ–¹å¼ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒæŒ‡å¯¼çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ç”Ÿæˆåˆå§‹ä¼ªæ ‡ç­¾ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä¸“ç”¨çš„æ—¶ç©ºæŠ•ç¥¨æ¨¡å—å¯¹è¿™äº›æ ‡ç­¾è¿›è¡Œç²¾ç‚¼ï¼Œè¯¥æ¨¡å—ç»“åˆäº†äºŒç»´å’Œä¸‰ç»´è¯­ä¹‰ï¼Œç”¨äºåœ¨çº¿å’Œç¦»çº¿ä¼˜åŒ–ã€‚ä¸ºäº†å®ç°å“è¶Šçš„ç‰¹å¾å­¦ä¹ ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§å½¢å¼çš„è¯­ä¹‰ç›‘ç£ï¼šä¸€ç»„åŸºäºäºŒç»´å…ˆéªŒçš„æŸå¤±ï¼Œå°†è§†è§‰çŸ¥è¯†æ³¨å…¥ä¸‰ç»´ç½‘ç»œï¼›ä¸€ç§æ–°å‹åŸºäºåŸå‹å¯¹æ¯”æŸå¤±ï¼Œé€šè¿‡åˆ©ç”¨ä¸‰ç»´è¯­ä¹‰ä¸€è‡´æ€§æ„å»ºåˆ¤åˆ«ç‰¹å¾ç©ºé—´ã€‚è¿™ä¸€å…¨é¢çš„è®¾è®¡å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºæ— ç›‘ç£ä¸‰ç»´å®ä¾‹åˆ†å‰²å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”šè‡³è¶…è¶Šäº†ä½¿ç”¨çœŸå®äºŒç»´è¾¹ç•Œæ¡†è¿›è¡Œç›‘ç£çš„æ–¹æ³•MWSISï¼Œåœ¨mAPä¸Šé«˜å‡º2.53%ï¼ˆ50.95%å¯¹æ¯”48.42%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05752v2">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨æ ‡æ³¨æ¿€å…‰é›·è¾¾ç‚¹äº‘å®ä¾‹æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„ä»»åŠ¡ã€‚ç ”ç©¶è€…ä»¬ä¸€ç›´åœ¨å°è¯•é™ä½è¿™ç§è´Ÿæ‹…ï¼Œä½†å¤§éƒ¨åˆ†æ–¹æ³•ä»ç„¶ä¾èµ–äººå·¥æ ‡æ³¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºALISEçš„æ–°æ¡†æ¶ï¼Œå®ƒæ— éœ€ä»»ä½•æ ‡æ³¨å³å¯è¿›è¡Œæ¿€å…‰é›·è¾¾å®ä¾‹åˆ†å‰²ã€‚è¯¥æ¡†æ¶é€šè¿‡è§†è§‰åŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œå¹¶é€šè¿‡æ—¶ç©ºæŠ•ç¥¨æ¨¡å—è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œç»“åˆäºŒç»´å’Œä¸‰ç»´è¯­ä¹‰è¿›è¡Œåœ¨çº¿å’Œç¦»çº¿ä¼˜åŒ–ã€‚é€šè¿‡å¼•å…¥ä¸¤ç§è¯­ä¹‰ç›‘ç£æ–¹å¼ï¼Œè¯¥æ¡†æ¶å®ç°äº†å“è¶Šçš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶è¾¾åˆ°äº†ç›‘ç£å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¿€å…‰é›·è¾¾ç‚¹äº‘å®ä¾‹æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>å½“å‰æ–¹æ³•è™½å°è¯•å‡å°‘äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä½†ä»éœ€æŸç§å½¢å¼çš„æ ‡æ³¨ã€‚</li>
<li>ALISEæ¡†æ¶æ— éœ€ä»»ä½•æ ‡æ³¨å³å¯è¿›è¡Œæ¿€å…‰é›·è¾¾å®ä¾‹åˆ†å‰²ã€‚</li>
<li>ALISEæ¡†æ¶é€šè¿‡è§†è§‰åŸºç¡€æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶é‡‡ç”¨æ—¶ç©ºæŠ•ç¥¨æ¨¡å—è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäºŒç»´å’Œä¸‰ç»´è¯­ä¹‰è¿›è¡Œåœ¨çº¿å’Œç¦»çº¿ä¼˜åŒ–ï¼Œå®ç°å“è¶Šçš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥ä¸¤ç§è¯­ä¹‰ç›‘ç£æ–¹å¼ï¼Œæå‡ç‰¹å¾å­¦ä¹ æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-30b825f4ed039b4e460ba3f4f1abe7e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827383&auth_key=1760827383-0-0-97660bcac763ee67eaf3a62a6708d33e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a1aed8741ec2a3f5bcc79ddbf161d5d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827390&auth_key=1760827390-0-0-19fc12306887332893a44a7e4fcb501a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-84dec6df53954f21f251f7bf73a74120~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827397&auth_key=1760827397-0-0-3e119bdf4ab0c8603126f4302cbf0061&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Impact-of-Radiographic-Noise-on-Chest-X-ray-Semantic-Segmentation-and-Disease-Classification-Using-a-Scalable-Noise-Injection-Framework"><a href="#Evaluating-the-Impact-of-Radiographic-Noise-on-Chest-X-ray-Semantic-Segmentation-and-Disease-Classification-Using-a-Scalable-Noise-Injection-Framework" class="headerlink" title="Evaluating the Impact of Radiographic Noise on Chest X-ray Semantic   Segmentation and Disease Classification Using a Scalable Noise Injection   Framework"></a>Evaluating the Impact of Radiographic Noise on Chest X-ray Semantic   Segmentation and Disease Classification Using a Scalable Noise Injection   Framework</h2><p><strong>Authors:Derek Jiu, Kiran Nijjer, Nishant Chinta, Ryan Bui, Kevin Zhu</strong></p>
<p>Deep learning models are increasingly used for radiographic analysis, but their reliability is challenged by the stochastic noise inherent in clinical imaging. A systematic, cross-task understanding of how different noise types impact these models is lacking. Here, we evaluate the robustness of state-of-the-art convolutional neural networks (CNNs) to simulated quantum (Poisson) and electronic (Gaussian) noise in two key chest X-ray tasks: semantic segmentation and pulmonary disease classification. Using a novel, scalable noise injection framework, we applied controlled, clinically-motivated noise severities to common architectures (UNet, DeepLabV3, FPN; ResNet, DenseNet, EfficientNet) on public datasets (Landmark, ChestX-ray14). Our results reveal a stark dichotomy in task robustness. Semantic segmentation models proved highly vulnerable, with lung segmentation performance collapsing under severe electronic noise (Dice Similarity Coefficient drop of 0.843), signifying a near-total model failure. In contrast, classification tasks demonstrated greater overall resilience, but this robustness was not uniform. We discovered a differential vulnerability: certain tasks, such as distinguishing Pneumothorax from Atelectasis, failed catastrophically under quantum noise (AUROC drop of 0.355), while others were more susceptible to electronic noise. These findings demonstrate that while classification models possess a degree of inherent robustness, pixel-level segmentation tasks are far more brittle. The task- and noise-specific nature of model failure underscores the critical need for targeted validation and mitigation strategies before the safe clinical deployment of diagnostic AI. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ”¾å°„å­¦åˆ†æä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†ä¸´åºŠå›¾åƒä¸­å›ºæœ‰çš„éšæœºå™ªå£°å¯¹å…¶å¯é æ€§æå‡ºäº†æŒ‘æˆ˜ã€‚ç›®å‰è¿˜ç¼ºä¹å…³äºä¸åŒå™ªå£°ç±»å‹å¦‚ä½•å½±å“è¿™äº›æ¨¡å‹çš„ç³»ç»Ÿã€è·¨ä»»åŠ¡çš„äº†è§£ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹ä¸¤ç§å…³é”®èƒ¸éƒ¨Xå°„çº¿ä»»åŠ¡ä¸­æ¨¡æ‹Ÿçš„é‡å­ï¼ˆæ³Šæ¾ï¼‰å™ªå£°å’Œç”µå­ï¼ˆé«˜æ–¯ï¼‰å™ªå£°çš„ç¨³å¥æ€§ï¼šè¯­ä¹‰åˆ†å‰²å’Œè‚ºéƒ¨ç–¾ç—…åˆ†ç±»ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„å¯æ‰©å±•å™ªå£°æ³¨å…¥æ¡†æ¶ï¼Œå¯¹å…¬å…±æ•°æ®é›†ï¼ˆLandmarkã€ChestX-ray14ï¼‰ä¸Šçš„å¸¸è§æ¶æ„ï¼ˆUNetã€DeepLabV3ã€FPNï¼›ResNetã€DenseNetã€EfficientNetï¼‰åº”ç”¨äº†å—æ§çš„ã€ä¸´åºŠæ¿€åŠ±çš„å™ªå£°ä¸¥é‡ç¨‹åº¦ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†ä»»åŠ¡ç¨³å¥æ€§æ–¹é¢çš„é²œæ˜å·®å¼‚ã€‚è¯­ä¹‰åˆ†å‰²æ¨¡å‹è¯æ˜é«˜åº¦è„†å¼±ï¼Œåœ¨ä¸¥é‡çš„ç”µå­å™ªå£°ä¸‹ï¼Œè‚ºéƒ¨åˆ†å‰²æ€§èƒ½å¤§å¹…ä¸‹é™ï¼ˆDiceç›¸ä¼¼ç³»æ•°ä¸‹é™0.843ï¼‰ï¼Œè¿™æ ‡å¿—ç€æ¨¡å‹è¿‘ä¹å®Œå…¨å¤±è´¥ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ†ç±»ä»»åŠ¡è¡¨ç°å‡ºæ›´å¤§çš„æ•´ä½“éŸ§æ€§ï¼Œä½†è¿™ç§ç¨³å¥æ€§å¹¶ä¸ç»Ÿä¸€ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ç§å·®å¼‚æ€§è„†å¼±æ€§ï¼šæŸäº›ä»»åŠ¡ï¼Œå¦‚åŒºåˆ†æ°”èƒ¸å’Œè‚ºä¸å¼ ï¼Œåœ¨é‡å­å™ªå£°ä¸‹é­é‡äº†ç¾éš¾æ€§å¤±è´¥ï¼ˆAUROCä¸‹é™0.355ï¼‰ï¼Œè€Œå…¶ä»–ä»»åŠ¡åˆ™æ›´å®¹æ˜“å—åˆ°ç”µå­å™ªå£°çš„å½±å“ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè™½ç„¶åˆ†ç±»æ¨¡å‹å…·æœ‰ä¸€å®šçš„å›ºæœ‰ç¨³å¥æ€§ï¼Œä½†åƒç´ çº§åˆ†å‰²ä»»åŠ¡åˆ™æ›´ä¸ºè„†å¼±ã€‚æ¨¡å‹å¤±è´¥çš„ç‰¹å®šä»»åŠ¡å’Œå™ªå£°æ€§è´¨çš„ç‰¹ç‚¹å¼ºè°ƒäº†æœ‰é’ˆå¯¹æ€§çš„éªŒè¯å’Œåœ¨è¯Šæ–­äººå·¥æ™ºèƒ½ä¸´åºŠéƒ¨ç½²å‰é‡‡å–ç¼“è§£ç­–ç•¥çš„å…³é”®éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25265v2">PDF</a> Accepted to ARRS 2026 Annual Meeting</p>
<p><strong>Summary</strong>ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ”¾å°„å­¦åˆ†æä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†åœ¨ä¸´åºŠå›¾åƒä¸­å­˜åœ¨çš„éšæœºå™ªå£°å¯¹å…¶å¯é æ€§æå‡ºäº†æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹æ¨¡æ‹Ÿé‡å­ï¼ˆæ³Šæ¾ï¼‰å’Œç”µå­ï¼ˆé«˜æ–¯ï¼‰å™ªå£°çš„é²æ£’æ€§ï¼Œæ¶‰åŠä¸¤ä¸ªå…³é”®èƒ¸éƒ¨Xå°„çº¿ä»»åŠ¡ï¼šè¯­ä¹‰åˆ†å‰²å’Œè‚ºéƒ¨ç–¾ç—…åˆ†ç±»ã€‚é€šè¿‡ä½¿ç”¨æ–°å‹çš„å¯æ‰©å±•å™ªå£°æ³¨å…¥æ¡†æ¶ï¼Œåœ¨å…¬å…±æ•°æ®é›†ä¸Šåº”ç”¨äº†å—æ§çš„ä¸´åºŠå™ªå£°ä¸¥é‡ç¨‹åº¦ã€‚ç»“æœæ˜¾ç¤ºä»»åŠ¡é²æ£’æ€§å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚è¯­ä¹‰åˆ†å‰²æ¨¡å‹é«˜åº¦è„†å¼±ï¼Œåœ¨ä¸¥é‡ç”µå­å™ªå£°ä¸‹è‚ºéƒ¨åˆ†å‰²æ€§èƒ½å¤§å¹…ä¸‹é™ï¼ˆDiceç›¸ä¼¼ç³»æ•°ä¸‹é™0.843ï¼‰ï¼Œå‡ ä¹å¯¼è‡´æ¨¡å‹å®Œå…¨å¤±æ•ˆã€‚åˆ†ç±»ä»»åŠ¡åˆ™è¡¨ç°å‡ºæ›´å¤§çš„æ•´ä½“ç¨³å¥æ€§ï¼Œä½†è¿™ç§ç¨³å¥æ€§å¹¶ä¸ç»Ÿä¸€ã€‚ç ”ç©¶å‘ç°ä¸åŒä»»åŠ¡çš„è„†å¼±æ€§å­˜åœ¨å·®å¼‚ï¼ŒæŸäº›ä»»åŠ¡å¦‚åŒºåˆ†æ°”èƒ¸ä¸è‚ºä¸å¼ åœ¨é‡å­å™ªå£°ä¸‹å¤±æ•ˆï¼ˆAUROCä¸‹é™0.355ï¼‰ï¼Œè€Œå…¶ä»–ä»»åŠ¡åˆ™æ›´æ˜“å—ç”µå­å™ªå£°å½±å“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶åˆ†ç±»æ¨¡å‹å…·æœ‰ä¸€å®šçš„å›ºæœ‰ç¨³å¥æ€§ï¼Œä½†åƒç´ çº§åˆ†å‰²ä»»åŠ¡æ›´ä¸ºè„†å¼±ã€‚æ¨¡å‹å¤±è´¥çš„ä»»åŠ¡å’Œå™ªå£°ç‰¹å®šæ€§è´¨å¼ºè°ƒäº†åœ¨éƒ¨ç½²è¯Šæ–­äººå·¥æ™ºèƒ½ä¹‹å‰éœ€è¦æœ‰é’ˆå¯¹æ€§çš„éªŒè¯å’Œç¼“è§£ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¸´åºŠå›¾åƒåˆ†æä¸­çš„åº”ç”¨å—åˆ°éšæœºå™ªå£°çš„å½±å“ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å·ç§¯ç¥ç»ç½‘ç»œå¯¹é‡å­å™ªå£°å’Œç”µå­å™ªå£°çš„é²æ£’æ€§ã€‚</li>
<li>è¯­ä¹‰åˆ†å‰²æ¨¡å‹å¯¹ç”µå­å™ªå£°é«˜åº¦è„†å¼±ï¼Œè‚ºéƒ¨åˆ†å‰²æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚</li>
<li>åˆ†ç±»ä»»åŠ¡è¡¨ç°å‡ºæ›´å¤§çš„æ•´ä½“ç¨³å¥æ€§ï¼Œä½†ä¸åŒä»»åŠ¡é—´çš„è„†å¼±æ€§å­˜åœ¨å·®å¼‚ã€‚</li>
<li>æŸäº›åˆ†ç±»ä»»åŠ¡åœ¨é‡å­å™ªå£°ä¸‹ä¼šå¤±æ•ˆï¼Œè€Œå…¶ä»–ä»»åŠ¡æ›´æ˜“å—ç”µå­å™ªå£°å½±å“ã€‚</li>
<li>åˆ†ç±»æ¨¡å‹å…·æœ‰ä¸€å®šçš„å›ºæœ‰ç¨³å¥æ€§ï¼Œä½†åƒç´ çº§åˆ†å‰²ä»»åŠ¡æ›´ä¸ºè„†å¼±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8259b917a9b739412add3452488d003f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827404&auth_key=1760827404-0-0-c48495ac19d86f6306f2d7d8d6bd9659&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6979ef414884c30455190fa002270ac5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827412&auth_key=1760827412-0-0-f669495e3a505dc59279b37470764bec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GLSim-Detecting-Object-Hallucinations-in-LVLMs-via-Global-Local-Similarity"><a href="#GLSim-Detecting-Object-Hallucinations-in-LVLMs-via-Global-Local-Similarity" class="headerlink" title="GLSim: Detecting Object Hallucinations in LVLMs via Global-Local   Similarity"></a>GLSim: Detecting Object Hallucinations in LVLMs via Global-Local   Similarity</h2><p><strong>Authors:Seongheon Park, Sharon Li</strong></p>
<p>Object hallucination in large vision-language models presents a significant challenge to their safe deployment in real-world applications. Recent works have proposed object-level hallucination scores to estimate the likelihood of object hallucination; however, these methods typically adopt either a global or local perspective in isolation, which may limit detection reliability. In this paper, we introduce GLSim, a novel training-free object hallucination detection framework that leverages complementary global and local embedding similarity signals between image and text modalities, enabling more accurate and reliable hallucination detection in diverse scenarios. We comprehensively benchmark existing object hallucination detection methods and demonstrate that GLSim achieves superior detection performance, outperforming competitive baselines by a significant margin. </p>
<blockquote>
<p>å¯¹è±¡å¹»è§‰åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå¯¹äºå…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„å®‰å…¨éƒ¨ç½²æ„æˆäº†å¨èƒã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»æå‡ºäº†å¯¹è±¡çº§åˆ«çš„å¹»è§‰è¯„åˆ†æ¥ä¼°è®¡å‡ºç°å¯¹è±¡å¹»è§‰çš„å¯èƒ½æ€§ï¼›ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­¤ç«‹åœ°é‡‡ç”¨å…¨å±€æˆ–å±€éƒ¨è§†è§’ï¼Œå¯èƒ½ä¼šé™åˆ¶æ£€æµ‹çš„å¯é æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GLSimï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒå¯¹è±¡å¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å…¨å±€å’Œå±€éƒ¨åµŒå…¥ç›¸ä¼¼æ€§ä¿¡å·çš„äº’è¡¥æ€§ï¼Œèƒ½å¤Ÿåœ¨å„ç§åœºæ™¯ä¸­å®ç°æ›´å‡†ç¡®ã€æ›´å¯é åœ°å¹»è§‰æ£€æµ‹ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„å¯¹è±¡å¹»è§‰æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜GLSimçš„æ£€æµ‹æ€§èƒ½ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19972v3">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å¯¹è±¡è‡†æµ‹ç°è±¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è™½ç„¶å·²æœ‰æ–¹æ³•é€šè¿‡å¯¹è±¡çº§è‡†æµ‹åˆ†æ•°ä¼°è®¡è‡†æµ‹å‘ç”Ÿçš„å¯èƒ½æ€§ï¼Œä½†å®ƒä»¬å¾€å¾€é‡‡ç”¨å…¨å±€æˆ–å±€éƒ¨å•ä¸€è§†è§’ï¼Œå¯èƒ½å½±å“æ£€æµ‹å¯é æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§å…¨æ–°çš„è®­ç»ƒå¤–å¯¹è±¡è‡†æµ‹æ£€æµ‹æ¡†æ¶GLSimï¼Œåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å…¨å±€å’Œå±€éƒ¨åµŒå…¥ç›¸ä¼¼æ€§ä¿¡å·ï¼Œæé«˜ä¸åŒåœºæ™¯ä¸‹çš„è‡†æµ‹æ£€æµ‹å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ç»è¿‡å¯¹ç°æœ‰å¯¹è±¡è‡†æµ‹æ£€æµ‹æ–¹æ³•çš„å…¨é¢è¯„ä¼°ï¼ŒGLSimè¡¨ç°å‡ºä¼˜è¶Šçš„æ£€æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¯¹è±¡è‡†æµ‹æ˜¯å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å…¨å±€æˆ–å±€éƒ¨å•ä¸€è§†è§’æ£€æµ‹å¯¹è±¡è‡†æµ‹ï¼Œå¯èƒ½é™åˆ¶æ£€æµ‹å¯é æ€§ã€‚</li>
<li>GLSimæ˜¯ä¸€ç§å…¨æ–°çš„è®­ç»ƒå¤–å¯¹è±¡è‡†æµ‹æ£€æµ‹æ¡†æ¶ï¼Œç»“åˆå…¨å±€å’Œå±€éƒ¨åµŒå…¥ç›¸ä¼¼æ€§ä¿¡å·ï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>GLSimå®ç°äº†å¯¹ç°æœ‰å¯¹è±¡è‡†æµ‹æ£€æµ‹æ–¹æ³•çš„å…¨é¢è¯„ä¼°ï¼Œå¹¶å±•ç°å‡ºä¼˜è¶Šçš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>GLSimåœ¨å¤šç§åœºæ™¯ä¸‹éƒ½èƒ½æœ‰æ•ˆæ£€æµ‹å¯¹è±¡è‡†æµ‹ï¼Œæ€§èƒ½è¿œè¶…å…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹äºç¡®ä¿è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒä¸­çš„å®‰å…¨éƒ¨ç½²å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-93577923371109752f6bbb2c13de7321~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827419&auth_key=1760827419-0-0-5a4a0428801ca2769bf24fd5ef3aa2b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07c1c72849709cc7ee66072bba0b7a6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827428&auth_key=1760827428-0-0-651227f7e52849d2888b2bf9eff0c81b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62a4d14cbf026c7ef643db9a8cfe3605~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827434&auth_key=1760827434-0-0-a09eb32a4c1d6c7bbfeac689b8615c2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Contour-Errors-An-Ego-Centric-Metric-for-Reliable-3D-Multi-Object-Tracking"><a href="#Contour-Errors-An-Ego-Centric-Metric-for-Reliable-3D-Multi-Object-Tracking" class="headerlink" title="Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object   Tracking"></a>Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object   Tracking</h2><p><strong>Authors:Sharang Kaul, Mario Berk, Thiemo Gerbich, Abhinav Valada</strong></p>
<p>Finding reliable matches is essential in multi-object tracking to ensure the accuracy and reliability of perception systems in safety-critical applications such as autonomous vehicles. Effective matching mitigates perception errors, enhancing object identification and tracking for improved performance and safety. However, traditional metrics such as Intersection over Union (IoU) and Center Point Distances (CPDs), which are effective in 2D image planes, often fail to find critical matches in complex 3D scenes. To address this limitation, we introduce Contour Errors (CEs), an ego or object-centric metric for identifying matches of interest in tracking scenarios from a functional perspective. By comparing bounding boxes in the ego vehicleâ€™s frame, contour errors provide a more functionally relevant assessment of object matches. Extensive experiments on the nuScenes dataset demonstrate that contour errors improve the reliability of matches over the state-of-the-art 2D IoU and CPD metrics in tracking-by-detection methods. In 3D car tracking, our results show that Contour Errors reduce functional failures (FPs&#x2F;FNs) by 80% at close ranges and 60% at far ranges compared to IoU in the evaluation stage. </p>
<blockquote>
<p>åœ¨å¤šç›®æ ‡è·Ÿè¸ªä¸­ï¼Œæ‰¾åˆ°å¯é çš„åŒ¹é…æ˜¯ç¡®ä¿å®‰å…¨å…³é”®åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼‰ä¸­çš„æ„ŸçŸ¥ç³»ç»Ÿå‡†ç¡®æ€§å’Œå¯é æ€§çš„å…³é”®ã€‚æœ‰æ•ˆçš„åŒ¹é…å¯ä»¥å‡è½»æ„ŸçŸ¥é”™è¯¯ï¼Œå¢å¼ºå¯¹è±¡è¯†åˆ«å’Œè·Ÿè¸ªï¼Œä»è€Œæé«˜æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åº¦é‡æ ‡å‡†ï¼Œå¦‚äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å’Œä¸­å¿ƒç‚¹è·ç¦»ï¼ˆCPDï¼‰ï¼Œåœ¨äºŒç»´å›¾åƒå¹³é¢ä¸Šæ˜¯æœ‰æ•ˆçš„ï¼Œä½†åœ¨å¤æ‚çš„ä¸‰ç»´åœºæ™¯ä¸­å¾€å¾€éš¾ä»¥æ‰¾åˆ°å…³é”®çš„åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è½®å»“è¯¯å·®ï¼ˆCEsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä»¥è‡ªæˆ‘æˆ–å¯¹è±¡ä¸ºä¸­å¿ƒçš„åº¦é‡æ ‡å‡†ï¼Œç”¨äºä»åŠŸèƒ½è§’åº¦è¯†åˆ«è·Ÿè¸ªåœºæ™¯ä¸­çš„åŒ¹é…é¡¹ã€‚é€šè¿‡æ¯”è¾ƒè‡ªæˆ‘è½¦è¾†æ¡†æ¶ä¸­çš„è¾¹ç•Œæ¡†ï¼Œè½®å»“è¯¯å·®ä¸ºå¯¹è±¡åŒ¹é…æä¾›äº†æ›´ä¸ºåŠŸèƒ½ç›¸å…³çš„è¯„ä¼°ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„åŸºäºæ£€æµ‹çš„è·Ÿè¸ªæ–¹æ³•çš„IoUå’ŒCPDåº¦é‡ç›¸æ¯”ï¼Œè½®å»“è¯¯å·®æé«˜äº†åŒ¹é…çš„å¯é æ€§ã€‚åœ¨ä¸‰ç»´æ±½è½¦è·Ÿè¸ªä¸­ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¯„ä¼°é˜¶æ®µï¼Œä¸IoUç›¸æ¯”ï¼Œè½®å»“è¯¯å·®åœ¨è¿‘è·ç¦»å‡å°‘äº†80%ã€è¿œè·ç¦»å‡å°‘äº†60%çš„åŠŸèƒ½æ€§æ•…éšœï¼ˆFPs&#x2F;FNsï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04122v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å¤šç›®æ ‡è·Ÿè¸ªä¸­å¯é åŒ¹é…çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿåº¦é‡æ ‡å‡†å¦‚IoUå’ŒCPDåœ¨å¤æ‚3Dåœºæ™¯ä¸­éš¾ä»¥æ‰¾åˆ°å…³é”®åŒ¹é…çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥äº†è½®å»“è¯¯å·®ï¼ˆCEsï¼‰è¿™ä¸€ä»¥è‡ªæˆ‘æˆ–å¯¹è±¡ä¸ºä¸­å¿ƒçš„åº¦é‡æ ‡å‡†ï¼Œä»åŠŸèƒ½è§’åº¦è¯„ä¼°è·Ÿè¸ªåœºæ™¯ä¸­çš„æ„Ÿå…´è¶£åŒ¹é…ã€‚è½®å»“è¯¯å·®é€šè¿‡æ¯”è¾ƒä»¥è‡ªæˆ‘è½¦è¾†ä¸ºæ¡†æ¶çš„è¾¹ç•Œæ¡†ï¼Œæä¾›äº†å¯¹ç›®æ ‡åŒ¹é…çš„æ›´åŠŸèƒ½ç›¸å…³çš„è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼Œè½®å»“è¯¯å·®åœ¨è·Ÿè¸ªæ£€æµ‹æ–¹æ³•å’Œä¸‰ç»´è½¦è¾†è·Ÿè¸ªä¸­æé«˜äº†åŒ¹é…çš„å¯é æ€§ï¼Œå‡å°‘äº†åŠŸèƒ½æ•…éšœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç›®æ ‡è·Ÿè¸ªä¸­çš„å¯é åŒ¹é…å¯¹äºç¡®ä¿å®‰å…¨å…³é”®åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼‰çš„æ„ŸçŸ¥ç³»ç»Ÿå‡†ç¡®æ€§å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿåº¦é‡æ ‡å‡†å¦‚IoUå’ŒCPDåœ¨å¤æ‚3Dåœºæ™¯ä¸­éš¾ä»¥æ‰¾åˆ°å…³é”®åŒ¹é…ã€‚</li>
<li>è½®å»“è¯¯å·®ï¼ˆCEsï¼‰æ˜¯ä¸€ç§æ–°å‹çš„åŒ¹é…åº¦é‡æ ‡å‡†ï¼Œé€šè¿‡æ¯”è¾ƒä»¥è‡ªæˆ‘è½¦è¾†ä¸ºæ¡†æ¶çš„è¾¹ç•Œæ¡†æ¥è¯„ä¼°ç›®æ ‡åŒ¹é…ã€‚</li>
<li>è½®å»“è¯¯å·®ä»åŠŸèƒ½è§’åº¦è¯„ä¼°åŒ¹é…ï¼Œæä¾›æ›´åŠŸèƒ½ç›¸å…³çš„è¯„ä¼°ç»“æœã€‚</li>
<li>åœ¨è·Ÿè¸ªæ£€æµ‹æ–¹æ³•ä¸­ï¼Œè½®å»“è¯¯å·®æé«˜äº†åŒ¹é…çš„å¯é æ€§ï¼Œä¼˜äºç°æœ‰çš„IoUå’ŒCPDåº¦é‡æ ‡å‡†ã€‚</li>
<li>åœ¨ä¸‰ç»´è½¦è¾†è·Ÿè¸ªä¸­ï¼Œè½®å»“è¯¯å·®æ˜¾è‘—å‡å°‘äº†åŠŸèƒ½æ•…éšœï¼ˆFPs&#x2F;FNsï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-86a028dd49c2c890042716968fe97597~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827442&auth_key=1760827442-0-0-001c08c858549ec096ae1ed2b7601342&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9131e57d65e898131cf44f4154f5d4ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827449&auth_key=1760827449-0-0-a064b2edc1b06c620afd0a252f027709&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85fe54addb797daa1f4468e71a514fd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827457&auth_key=1760827457-0-0-05bcdc642d1e88cea579b05ee187851e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f1b93083f4f45f06f6bf202a339a9575~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827464&auth_key=1760827464-0-0-e942c003eba5c842ca7497c38408a3fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2393925bc7215a16cb767a2b46d63774~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827471&auth_key=1760827471-0-0-764515173d6f336c9d516ec4c0cecad1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control"><a href="#Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control" class="headerlink" title="Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise   Shape and Semantic Control"></a>Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise   Shape and Semantic Control</h2><p><strong>Authors:Danfeng li, Hui Zhang, Sheng Wang, Jiacheng Li, Zuxuan Wu</strong></p>
<p>Despite recent advances in diffusion models, top-tier text-to-image (T2I) models still struggle to achieve precise spatial layout control, i.e. accurately generating entities with specified attributes and locations. Segmentation-mask-to-image (S2I) generation has emerged as a promising solution by incorporating pixel-level spatial guidance and regional text prompts. However, existing S2I methods fail to simultaneously ensure semantic consistency and shape consistency. To address these challenges, we propose Seg2Any, a novel S2I framework built upon advanced multimodal diffusion transformers (e.g. FLUX). First, to achieve both semantic and shape consistency, we decouple segmentation mask conditions into regional semantic and high-frequency shape components. The regional semantic condition is introduced by a Semantic Alignment Attention Mask, ensuring that generated entities adhere to their assigned text prompts. The high-frequency shape condition, representing entity boundaries, is encoded as an Entity Contour Map and then introduced as an additional modality via multi-modal attention to guide image spatial structure. Second, to prevent attribute leakage across entities in multi-entity scenarios, we introduce an Attribute Isolation Attention Mask mechanism, which constrains each entityâ€™s image tokens to attend exclusively to themselves during image self-attention. To support open-set S2I generation, we construct SACap-1M, a large-scale dataset containing 1 million images with 5.9 million segmented entities and detailed regional captions, along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive experiments demonstrate that Seg2Any achieves state-of-the-art performance on both open-set and closed-set S2I benchmarks, particularly in fine-grained spatial and attribute control of entities. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œé¡¶çº§æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä»ç„¶éš¾ä»¥å®ç°ç²¾ç¡®çš„ç©ºé—´å¸ƒå±€æ§åˆ¶ï¼Œå³å‡†ç¡®ç”Ÿæˆå…·æœ‰æŒ‡å®šå±æ€§å’Œä½ç½®çš„å®ä½“ã€‚å›¾åƒåˆ†å‰²æ©è†œåˆ°å›¾åƒï¼ˆS2Iï¼‰ç”ŸæˆæŠ€æœ¯ä½œä¸ºä¸€ç§ç»“åˆåƒç´ çº§ç©ºé—´æŒ‡å¯¼å’ŒåŒºåŸŸæ–‡æœ¬æç¤ºçš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„S2Iæ–¹æ³•æ— æ³•åŒæ—¶ç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§å’Œå½¢çŠ¶ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Seg2Anyï¼Œä¸€ä¸ªåŸºäºå…ˆè¿›çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆä¾‹å¦‚FLUXï¼‰çš„æ–°å‹S2Iæ¡†æ¶ã€‚é¦–å…ˆï¼Œä¸ºäº†å®ç°è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å°†åˆ†å‰²æ©è†œæ¡ä»¶åˆ†è§£ä¸ºåŒºåŸŸè¯­ä¹‰å’Œé«˜é¢‘å½¢çŠ¶ç»„ä»¶ã€‚é€šè¿‡è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©è†œå¼•å…¥åŒºåŸŸè¯­ä¹‰æ¡ä»¶ï¼Œç¡®ä¿ç”Ÿæˆçš„å®ä½“ç¬¦åˆå…¶åˆ†é…åˆ°çš„æ–‡æœ¬æç¤ºã€‚ä»£è¡¨å®ä½“è¾¹ç•Œçš„é«˜é¢‘å½¢çŠ¶æ¡ä»¶è¢«ç¼–ç ä¸ºå®ä½“è½®å»“å›¾ï¼Œç„¶åé€šè¿‡å¤šæ¨¡æ€æ³¨æ„åŠ›ä½œä¸ºé™„åŠ æ¨¡æ€å¼•å…¥ï¼Œä»¥æŒ‡å¯¼å›¾åƒçš„ç©ºé—´ç»“æ„ã€‚å…¶æ¬¡ï¼Œä¸ºäº†é˜²æ­¢å¤šå®ä½“åœºæ™¯ä¸­å±æ€§åœ¨å®ä½“ä¹‹é—´çš„æ³„éœ²ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©è†œæœºåˆ¶ï¼Œè¯¥æœºåˆ¶çº¦æŸæ¯ä¸ªå®ä½“çš„å›¾åƒä»¤ç‰Œåœ¨å›¾åƒè‡ªæ³¨æ„åŠ›æœŸé—´åªä¸“æ³¨äºè‡ªèº«ã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾é›†S2Iç”Ÿæˆï¼Œæˆ‘ä»¬æ„å»ºäº†SACap-1Mï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸‡å¼ å›¾åƒã€590ä¸‡ä¸ªåˆ†å‰²å®ä½“å’Œè¯¦ç»†åŒºåŸŸæè¿°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠç”¨äºå…¨é¢S2Iè¯„ä¼°çš„SACap-EvalåŸºå‡†æµ‹è¯•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSeg2Anyåœ¨å¼€æ”¾é›†å’Œå°é—­é›†S2IåŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å®ä½“çš„ç²¾ç»†ç©ºé—´æ§åˆ¶ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç‰¹åˆ«æ˜¯åœ¨å…·æœ‰ç²¾ç»†å±æ€§å’Œä½ç½®çš„åœºæ™¯ï¼ˆä¾‹å¦‚åŸå¸‚å’Œè‡ªç„¶é£æ™¯çš„è¯¦ç»†é‡å»ºï¼‰ä¸­è¡¨ç°å¾—æ›´åŠ å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00596v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²æ©è†œåˆ°å›¾åƒï¼ˆS2Iï¼‰æ¡†æ¶Seg2Anyï¼Œç”¨äºè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ç²¾ç¡®ç©ºé—´å¸ƒå±€æ§åˆ¶çš„é—®é¢˜ã€‚Seg2Anyåˆ©ç”¨å…ˆè¿›çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆå¦‚FLUXï¼‰ï¼Œé€šè¿‡è§£è€¦åˆ†å‰²æ©è†œæ¡ä»¶å®ç°è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œå¼•å…¥è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©è†œå’Œå®ä½“è½®å»“å›¾ï¼Œåˆ†åˆ«è´Ÿè´£åŒºåŸŸè¯­ä¹‰å’Œé«˜é¢‘å½¢çŠ¶æ¡ä»¶ã€‚ä¸ºé¢„é˜²å¤šå®ä½“åœºæ™¯ä¸­çš„å±æ€§æ³„æ¼ï¼Œé‡‡ç”¨äº†å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©è†œæœºåˆ¶ã€‚Seg2Anyåœ¨å¼€æ”¾é›†å’Œå°é—­é›†çš„S2IåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶æ“…é•¿å®ä½“ç»†ç²’åº¦ç©ºé—´å’Œå±æ€§çš„æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seg2Anyæ˜¯ä¸€ä¸ªæ–°å…´çš„S2Iæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³T2Iæ¨¡å‹åœ¨ç²¾ç¡®ç©ºé—´å¸ƒå±€æ§åˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>Seg2Anyåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆå¦‚FLUXï¼‰å®ç°è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©è†œå’Œå®ä½“è½®å»“å›¾è¢«å¼•å…¥ä»¥å¤„ç†åŒºåŸŸè¯­ä¹‰å’Œé«˜é¢‘å½¢çŠ¶æ¡ä»¶ã€‚</li>
<li>å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©è†œæœºåˆ¶é˜²æ­¢äº†å¤šå®ä½“åœºæ™¯ä¸­çš„å±æ€§æ³„æ¼ã€‚</li>
<li>Seg2Anyæ„å»ºäº†SACap-1Mæ•°æ®é›†ï¼Œç”¨äºæ”¯æŒå¼€æ”¾é›†S2Iç”Ÿæˆï¼Œå¹¶æä¾›äº†SACap-EvalåŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSeg2Anyåœ¨S2IåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä½“ç»†ç²’åº¦ç©ºé—´å’Œå±æ€§çš„æ§åˆ¶æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c617d74ef7e1e2f66178f9780656d08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827479&auth_key=1760827479-0-0-559e01bdbdaaac432d4ef13228114c76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-425cd00ada271bf2518d97c330d8b2c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827486&auth_key=1760827486-0-0-0e9c0756dde161c19a312babb0d37554&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7480047e8dd0565825522e8ffc48ec0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827493&auth_key=1760827493-0-0-5687e4ca694bcca10b1f5df2652fcf74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1f41217c3e38a43c1d413405394be04f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827499&auth_key=1760827499-0-0-1d61da3b65b3ad3fe8f8ef2781bf4b14&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TMT-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation"><a href="#TMT-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation" class="headerlink" title="TMT: Cross-domain Semantic Segmentation with Region-adaptive   Transferability Estimation"></a>TMT: Cross-domain Semantic Segmentation with Region-adaptive   Transferability Estimation</h2><p><strong>Authors:Enming Zhang, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Guan Wang, Yang Li, Xiaoping Zhang</strong></p>
<p>Recent advances in Vision Transformers (ViTs) have significantly advanced semantic segmentation performance. However, their adaptation to new target domains remains challenged by distribution shifts, which often disrupt global attention mechanisms. While existing global and patch-level adaptation methods offer some improvements, they overlook the spatially varying transferability inherent in different image regions. To address this, we propose the Transferable Mask Transformer (TMT), a region-adaptive framework designed to enhance cross-domain representation learning through transferability guidance. First, we dynamically partition the image into coherent regions, grouped by structural and semantic similarity, and estimates their domain transferability at a localized level. Then, we incorporate region-level transferability maps directly into the self-attention mechanism of ViTs, allowing the model to adaptively focus attention on areas with lower transferability and higher semantic uncertainty. Extensive experiments across 20 diverse cross-domain settings demonstrate that TMT not only mitigates the performance degradation typically associated with domain shift but also consistently outperforms existing approaches. </p>
<blockquote>
<p>è¿‘æœŸåœ¨è§†è§‰Transformerï¼ˆViTsï¼‰æ–¹é¢çš„è¿›å±•å·²ç»æ˜¾è‘—æé«˜äº†è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é€‚åº”æ–°ç›®æ ‡åŸŸæ—¶ä»ç„¶é¢ä¸´åˆ†å¸ƒè½¬ç§»çš„éš¾é¢˜ï¼Œè¿™é€šå¸¸ä¼šç ´åå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ã€‚è™½ç„¶ç°æœ‰çš„å…¨å±€å’Œè¡¥ä¸çº§è‡ªé€‚åº”æ–¹æ³•æä¾›äº†ä¸€äº›æ”¹è¿›ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†ä¸åŒå›¾åƒåŒºåŸŸå›ºæœ‰çš„ç©ºé—´å¯è½¬ç§»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯è½¬ç§»æ©è†œTransformerï¼ˆTMTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŒºåŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯è½¬ç§»æ€§æŒ‡å¯¼å¢å¼ºè·¨åŸŸè¡¨ç¤ºå­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®ç»“æ„æ€§å’Œè¯­ä¹‰ç›¸ä¼¼æ€§å°†å›¾åƒåŠ¨æ€åˆ’åˆ†ä¸ºè¿è´¯çš„åŒºåŸŸï¼Œå¹¶å¯¹è¿™äº›åŒºåŸŸè¿›è¡Œåˆ†ç»„ï¼Œç„¶ååœ¨å±€éƒ¨çº§åˆ«ä¼°è®¡å®ƒä»¬çš„åŸŸå¯è½¬ç§»æ€§ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†åŒºåŸŸçº§å¯è½¬ç§»æ€§å›¾ç›´æ¥èå…¥ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨å¯è½¬ç§»æ€§è¾ƒä½ã€è¯­ä¹‰ä¸ç¡®å®šæ€§è¾ƒé«˜çš„åŒºåŸŸã€‚åœ¨20ä¸ªä¸åŒçš„è·¨åŸŸè®¾ç½®ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTMTä¸ä»…å‡è½»äº†ä¸åŸŸåç§»ç›¸å…³çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œè€Œä¸”å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05774v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ViTçš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œä½†åœ¨ç›®æ ‡åŸŸé€‚åº”æ–¹é¢ä»é¢ä¸´åˆ†å¸ƒè½¬ç§»çš„æŒ‘æˆ˜ï¼Œè¿™å¯èƒ½ä¼šç ´åå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ã€‚ç°æœ‰å…¨å±€å’Œè¡¥ä¸çº§é€‚åº”æ–¹æ³•è™½æœ‰æ‰€æå‡ï¼Œä½†å¿½ç•¥äº†ä¸åŒå›¾åƒåŒºåŸŸå›ºæœ‰çš„ç©ºé—´å¯è½¬ç§»æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯è½¬ç§»æ©è†œè½¬æ¢å™¨ï¼ˆTMTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŒºåŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯è½¬ç§»æ€§æŒ‡å¯¼å¢å¼ºè·¨åŸŸè¡¨ç¤ºå­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®ç»“æ„å’Œè¯­ä¹‰ç›¸ä¼¼æ€§å°†å›¾åƒåŠ¨æ€åˆ’åˆ†ä¸ºè¿è´¯åŒºåŸŸï¼Œå¹¶ä¼°ç®—å…¶å±€éƒ¨åŸŸå¯è½¬ç§»æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†åŒºåŸŸçº§å¯è½¬ç§»æ€§å›¾ç›´æ¥èå…¥ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨ä½å¯è½¬ç§»æ€§å’Œé«˜è¯­ä¹‰ä¸ç¡®å®šæ€§çš„åŒºåŸŸã€‚åœ¨è·¨è¶ŠäºŒåå¤šç§è·¨åŸŸè®¾ç½®çš„å¤§é‡å®éªŒä¸­è¡¨æ˜ï¼ŒTMTä¸ä»…å‡è½»äº†ä¸åŸŸåç§»ç›¸å…³çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œè€Œä¸”å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨è¯­ä¹‰åˆ†å‰²æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>åˆ†å¸ƒè½¬ç§»åœ¨ViTçš„ç›®æ ‡åŸŸé€‚åº”ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå¯èƒ½å½±å“å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>ç°æœ‰é€‚åº”æ–¹æ³•è™½æœ‰æ‰€æå‡ï¼Œä½†å¿½ç•¥äº†ä¸åŒå›¾åƒåŒºåŸŸçš„ç©ºé—´å¯è½¬ç§»æ€§ã€‚</li>
<li>æå‡ºäº†å¯è½¬ç§»æ©è†œè½¬æ¢å™¨ï¼ˆTMTï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŒºåŸŸè‡ªé€‚åº”å¢å¼ºè·¨åŸŸè¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>TMTé€šè¿‡åŠ¨æ€åˆ’åˆ†å›¾åƒåŒºåŸŸå¹¶ä¼°ç®—å…¶å±€éƒ¨åŸŸå¯è½¬ç§»æ€§æ¥è§£å†³ç°æœ‰é—®é¢˜ã€‚</li>
<li>TMTå°†åŒºåŸŸçº§å¯è½¬ç§»æ€§å›¾èå…¥ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½è‡ªé€‚åº”å…³æ³¨å…³é”®åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c9de1c8b06b9ec12b130c98b84de6c9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827507&auth_key=1760827507-0-0-dd2d40771c28eb4d4161069b95097649&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cb197da16f3082de8029136851de4d71~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827515&auth_key=1760827515-0-0-7fdc049dd92bdd53fcd9cbbd6a1e4be8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7d6bae20760de2efc99d4edb3073440~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827522&auth_key=1760827522-0-0-9e7fc27453f24019baef9ce33529bffd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0897eaf21eecd3ae0f112947a2440207~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827529&auth_key=1760827529-0-0-620273c5e53aeee05bd74a417e138a79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a13548c173150200e5f36c5bcb9f53c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827535&auth_key=1760827535-0-0-5bd5b3e64472d11351b4e3bed9c1a189&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CQ-DINO-Mitigating-Gradient-Dilution-via-Category-Queries-for-Vast-Vocabulary-Object-Detection"><a href="#CQ-DINO-Mitigating-Gradient-Dilution-via-Category-Queries-for-Vast-Vocabulary-Object-Detection" class="headerlink" title="CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast   Vocabulary Object Detection"></a>CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast   Vocabulary Object Detection</h2><p><strong>Authors:Zhichao Sun, Huazhang Hu, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</strong></p>
<p>With the exponential growth of data, traditional object detection methods are increasingly struggling to handle vast vocabulary object detection tasks effectively. We analyze two key limitations of classification-based detectors: positive gradient dilution, where rare positive categories receive insufficient learning signals, and hard negative gradient dilution, where discriminative gradients are overwhelmed by numerous easy negatives. To address these challenges, we propose CQ-DINO, a category query-based object detection framework that reformulates classification as a contrastive task between object queries and learnable category queries. Our method introduces image-guided query selection, which reduces the negative space by adaptively retrieving top-K relevant categories per image via cross-attention, thereby rebalancing gradient distributions and facilitating implicit hard example mining. Furthermore, CQ-DINO flexibly integrates explicit hierarchical category relationships in structured datasets (e.g., V3Det) or learns implicit category correlations via self-attention in generic datasets (e.g., COCO). Experiments demonstrate that CQ-DINO achieves superior performance on the challenging V3Det benchmark (surpassing previous methods by 2.1% AP) while maintaining competitiveness in COCO. Our work provides a scalable solution for real-world detection systems requiring wide category coverage. The code is publicly at <a target="_blank" rel="noopener" href="https://github.com/FireRedTeam/CQ-DINO">https://github.com/FireRedTeam/CQ-DINO</a>. </p>
<blockquote>
<p>éšç€æ•°æ®çš„æŒ‡æ•°çº§å¢é•¿ï¼Œä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ–¹æ³•è¶Šæ¥è¶Šéš¾ä»¥æœ‰æ•ˆåœ°å¤„ç†å¤§è§„æ¨¡è¯æ±‡è¡¨çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ†æäº†åŸºäºåˆ†ç±»çš„æ£€æµ‹å™¨çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šæ­£å‘æ¢¯åº¦ç¨€é‡Šï¼Œå³ç½•è§çš„æ­£å‘ç±»åˆ«æ¥æ”¶åˆ°çš„å­¦ä¹ ä¿¡å·ä¸è¶³ï¼›ä»¥åŠéš¾ä»¥åº”å¯¹çš„è´Ÿæ¢¯åº¦ç¨€é‡Šï¼Œå³åŒºåˆ†æ¢¯åº¦è¢«å¤§é‡ç®€å•çš„è´Ÿæ ·æœ¬æ‰€æ·¹æ²¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç±»åˆ«æŸ¥è¯¢çš„ç›®æ ‡æ£€æµ‹æ¡†æ¶CQ-DINOï¼Œå®ƒå°†åˆ†ç±»é‡æ–°åˆ¶å®šä¸ºç›®æ ‡æŸ¥è¯¢å’Œå¯å­¦ä¹ ç±»åˆ«æŸ¥è¯¢ä¹‹é—´çš„å¯¹æ¯”ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†å›¾åƒå¼•å¯¼æŸ¥è¯¢é€‰æ‹©ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›è‡ªé€‚åº”åœ°æ£€ç´¢æ¯å¼ å›¾åƒçš„å‰Kä¸ªç›¸å…³ç±»åˆ«ï¼Œä»è€Œå‡å°‘è´Ÿç©ºé—´ï¼Œä»è€Œé‡æ–°å¹³è¡¡æ¢¯åº¦åˆ†å¸ƒå¹¶ä¿ƒè¿›éšå¼ç¡¬æ ·æœ¬æŒ–æ˜ã€‚æ­¤å¤–ï¼ŒCQ-DINOå¯ä»¥çµæ´»åœ°é›†æˆç»“æ„åŒ–æ•°æ®é›†ï¼ˆä¾‹å¦‚V3Detï¼‰ä¸­çš„æ˜¾å¼å±‚æ¬¡ç±»åˆ«å…³ç³»ï¼Œæˆ–é€šè¿‡é€šç”¨æ•°æ®é›†ï¼ˆä¾‹å¦‚COCOï¼‰ä¸­çš„è‡ªæ³¨æ„åŠ›å­¦ä¹ éšå¼ç±»åˆ«å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒCQ-DINOåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„V3DetåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ˆè¶…å‡º2.1%çš„APï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨COCOä¸­ä¹Ÿä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºéœ€è¦å¹¿æ³›ç±»åˆ«è¦†ç›–çš„çœŸå®ä¸–ç•Œæ£€æµ‹ç³»ç»Ÿã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/FireRedTeam/CQ-DINO%E3%80%82">https://github.com/FireRedTeam/CQ-DINOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18430v4">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æ•°æ®é‡çš„æŒ‡æ•°çº§å¢é•¿ï¼Œä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•åœ¨åº”å¯¹å¤§è§„æ¨¡è¯æ±‡ç›®æ ‡æ£€æµ‹ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡åˆ†æäº†åŸºäºåˆ†ç±»çš„æ£€æµ‹å™¨çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šæ­£æ¢¯åº¦ç¨€é‡Šå’Œç¡¬è´Ÿæ¢¯åº¦ç¨€é‡Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CQ-DINOï¼Œä¸€ç§åŸºäºç±»åˆ«æŸ¥è¯¢çš„ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œå°†åˆ†ç±»é‡æ–°æ„å»ºä¸ºå¯¹è±¡æŸ¥è¯¢å’Œå¯å­¦ä¹ ç±»åˆ«æŸ¥è¯¢ä¹‹é—´çš„å¯¹æ¯”ä»»åŠ¡ã€‚é€šè¿‡å›¾åƒå¼•å¯¼æŸ¥è¯¢é€‰æ‹©ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡å°‘äº†è´Ÿç©ºé—´ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›è‡ªé€‚åº”åœ°æ£€ç´¢æ¯å¼ å›¾åƒçš„å‰Kä¸ªç›¸å…³ç±»åˆ«ï¼Œä»è€Œå¹³è¡¡æ¢¯åº¦åˆ†å¸ƒå¹¶ä¿ƒè¿›éšå¼ç¡¬æ ·æœ¬æŒ–æ˜ã€‚æ­¤å¤–ï¼ŒCQ-DINOçµæ´»æ•´åˆäº†ç»“æ„åŒ–æ•°æ®é›†ï¼ˆå¦‚V3Detï¼‰ä¸­çš„æ˜¾å¼å±‚æ¬¡ç±»åˆ«å…³ç³»ï¼Œæˆ–åœ¨é€šç”¨æ•°æ®é›†ï¼ˆå¦‚COCOï¼‰ä¸­å­¦ä¹ é€šè¿‡è‡ªæ³¨æ„åŠ›å®ç°çš„éšå¼ç±»åˆ«å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒCQ-DINOåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„V3DetåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ˆæé«˜2.1%çš„APï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨COCOä¸­ä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºéœ€è¦å¹¿æ³›ç±»åˆ«è¦†ç›–çš„çœŸå®ä¸–ç•Œæ£€æµ‹ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•åœ¨åº”å¯¹å¤§è§„æ¨¡æ•°æ®æŒ‘æˆ˜æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>åˆ†æäº†åŸºäºåˆ†ç±»çš„æ£€æµ‹å™¨çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šæ­£æ¢¯åº¦ç¨€é‡Šå’Œç¡¬è´Ÿæ¢¯åº¦ç¨€é‡Šã€‚</li>
<li>æå‡ºäº†CQ-DINOæ¡†æ¶ï¼Œå°†åˆ†ç±»è½¬åŒ–ä¸ºå¯¹æ¯”ä»»åŠ¡ï¼Œæé«˜ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å›¾åƒå¼•å¯¼æŸ¥è¯¢é€‰æ‹©å‡å°‘è´Ÿç©ºé—´ï¼Œå¹³è¡¡æ¢¯åº¦åˆ†å¸ƒï¼Œä¿ƒè¿›éšå¼ç¡¬æ ·æœ¬æŒ–æ˜ã€‚</li>
<li>CQ-DINOèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„æ•°æ®é›†ï¼Œæ•´åˆå±‚æ¬¡ç±»åˆ«å…³ç³»æˆ–å­¦ä¹ éšå¼ç±»åˆ«å…³è”ã€‚</li>
<li>åœ¨V3DetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCQ-DINOæ€§èƒ½ä¼˜è¶Šï¼Œè¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜äº†2.1%çš„APã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-713f5978a5ab429204c1d1e12a21ce1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827542&auth_key=1760827542-0-0-c96773c82eeef9b43840de690a123cd5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-206d1356164a04c3bcc0524f84b1511e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827550&auth_key=1760827550-0-0-bbecdfbc66f898605568e41e64e00a76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-632ffd2a9fba18f64b75585b3318d62a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827557&auth_key=1760827557-0-0-b811e6c364211fc0f9928cca092014dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da67566be75c68cb132813aabf3264b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827564&auth_key=1760827564-0-0-d4302b8452e318c0cde3077e407ee5fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2017b352abcf72f9e0d228878becab59~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827570&auth_key=1760827570-0-0-7273e9476d8b565e9a600a48746c1991&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation"><a href="#OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation" class="headerlink" title="OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation"></a>OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation</h2><p><strong>Authors:Ding Zhong, Xu Zheng, Chenfei Liao, Yuanhuiyi Lyu, Jialei Chen, Shengyang Wu, Linfeng Zhang, Xuming Hu</strong></p>
<p>Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to $360^\circ$ domain, the significant field-of-view (FoV) gap between pinhole ($70^\circ \times 70^\circ$) and panoramic images ($180^\circ \times 360^\circ$) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2â€™s memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that OmniSAM outperforms the state-of-the-art methods by large margins, e.g., 79.06% (+10.22%) on SPin8-to-SPan8, 62.46% (+6.58%) on CS13-to-DP13. </p>
<blockquote>
<p>Segment Anything Model 2ï¼ˆSAM2ï¼‰åœ¨å„ç§é’ˆå­”æˆåƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„åŸºç¡€æ¨¡å‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å°†å…¶åº”ç”¨äº$360^\circ$é¢†åŸŸæ—¶ï¼Œé’ˆå­”ï¼ˆ$70^\circ \times 70^\circ$ï¼‰ä¸å…¨æ™¯å›¾åƒï¼ˆ$180^\circ \times 360^\circ$ï¼‰ä¹‹é—´çš„è§†é‡ï¼ˆFoVï¼‰å·®è·å·¨å¤§ï¼Œå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚è¯¥åº”ç”¨çš„ä¸»è¦æœ‰ä¸¤ä¸ªæ‹…å¿§ç‚¹ï¼š1ï¼‰ç”±é¢†åŸŸé—´å¤§è§†é‡å·®å¼‚å¸¦æ¥çš„ä¸å¯é¿å…çš„å½¢çŠ¶æ‰­æ›²å’Œç‰©ä½“å˜å½¢ï¼›2ï¼‰åŸå§‹SAM2æ— æ³•æä¾›åƒç´ çº§åˆ«çš„è¯­ä¹‰ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„OmniSAMæ¡†æ¶ï¼Œå®ƒé¦–æ¬¡å°è¯•å°†SAM2åº”ç”¨äºå…¨æ™¯è¯­ä¹‰åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¼¥è¡¥ç¬¬ä¸€ä¸ªå·®è·ï¼ŒOmniSAMé¦–å…ˆæŠŠå…¨æ™¯å›¾åƒåˆ†æˆä¸€ç³»åˆ—çš„è¡¥ä¸ï¼ˆpatchesï¼‰ï¼Œç„¶åå°†è¿™äº›è¡¥ä¸è§†ä¸ºç±»ä¼¼è§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸­çš„å›¾åƒåºåˆ—ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æ¥æå–è·¨è¡¥ä¸å¯¹åº”å…³ç³»ï¼Œè¿™äº›å¯¹åº”å…³ç³»åµŒå…¥è·¨è§†é‡çš„ä¾èµ–å…³ç³»ï¼Œæé«˜äº†ç‰¹å¾è¿ç»­æ€§å’Œæ©è†œè¾¹ç•Œçš„é¢„æµ‹ä¸€è‡´æ€§ã€‚å¯¹äºç¬¬äºŒä¸ªå·®è·ï¼ŒOmniSAMå¾®è°ƒäº†é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å¹¶é‡æ–°ä½¿ç”¨æ©è†œè§£ç å™¨è¿›è¡Œè¯­ä¹‰é¢„æµ‹ã€‚è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºè§†é‡çš„åŸå‹é€‚åº”æ¨¡å—ï¼Œå¹¶å¸¦æœ‰åŠ¨æ€ä¼ªæ ‡ç­¾æ›´æ–°æœºåˆ¶ï¼Œä»¥ä¿ƒè¿›è®°å¿†å’Œä¸»å¹²ç‰¹å¾çš„å¯¹é½ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ä¸åŒå°ºå¯¸æºæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼ŒOmniSAMå¤§å¹…è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨SPin8-to-SPan8ä¸Šè¾¾åˆ°79.06%ï¼ˆ+10.22%ï¼‰ï¼Œåœ¨CS13-to-DP13ä¸Šè¾¾åˆ°62.46%ï¼ˆ+6.58%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07098v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å…¨æ™¯è¯­ä¹‰åˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒOmniSAMæ¡†æ¶é€šè¿‡åº”ç”¨SAM2æ¨¡å‹æå‡ºäº†ä¸€ç³»åˆ—åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚OmniSAMé€šè¿‡åˆ’åˆ†å…¨æ™¯å›¾åƒä¸ºå¤šä¸ªè¡¥ä¸åºåˆ—ï¼Œå¹¶åˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æå–è·¨è¡¥ä¸å¯¹åº”å…³ç³»ï¼Œä»¥è§£å†³ä¸åŒè§†åœºè§’ä¹‹é—´çš„å·®å¼‚æ€§é—®é¢˜å’Œè¯­ä¹‰ç†è§£çš„ç¼ºå¤±ã€‚ç»è¿‡å¾®è°ƒå›¾åƒç¼–ç å™¨å’Œé‡æ–°åˆ©ç”¨é®ç½©è§£ç å™¨è¿›è¡Œè¯­ä¹‰é¢„æµ‹ï¼ŒOmniSAMæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒæºæ¨¡å‹å¤§å°ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniSAMåœ¨SPin8-to-SPan8å’ŒCS13-to-DP13ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿œè¶…ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>OmniSAMæ¡†æ¶é¦–æ¬¡å°†SAM2æ¨¡å‹åº”ç”¨äºå…¨æ™¯è¯­ä¹‰åˆ†å‰²ï¼Œè§£å†³åœ¨$360^\circ$é¢†åŸŸä¸­åº”ç”¨æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>OmniSAMé€šè¿‡åˆ’åˆ†å…¨æ™¯å›¾åƒä¸ºè¡¥ä¸åºåˆ—ï¼Œåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æå–è·¨è¡¥ä¸å¯¹åº”å…³ç³»ï¼Œä»¥å¼¥è¡¥è§†åœºè§’å·®å¼‚å’Œè¯­ä¹‰ç†è§£çš„ä¸è¶³ã€‚</li>
<li>OmniSAMå¾®è°ƒäº†é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨å¹¶é‡æ–°åˆ©ç”¨é®ç½©è§£ç å™¨è¿›è¡Œè¯­ä¹‰é¢„æµ‹ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŸºäºè§†åœºçš„åŸå‹é€‚åº”æ¨¡å—ï¼Œå¸¦æœ‰åŠ¨æ€ä¼ªæ ‡ç­¾æ›´æ–°æœºåˆ¶ï¼Œä¿ƒè¿›è®°å¿†å’Œä¸»å¹²ç‰¹å¾çš„å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniSAMåœ¨SPin8-to-SPan8å’ŒCS13-to-DP13ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿œè¶…ç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚</li>
<li>OmniSAMæ¡†æ¶åœ¨æ¡¥æ¥ä¸åŒè§†åœºè§’å·®å¼‚å’Œå¢å¼ºè¯­ä¹‰ç†è§£æ–¹é¢çš„åˆ›æ–°ç­–ç•¥ä¸ºå…¨æ™¯æˆåƒåˆ†å‰²ä»»åŠ¡æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b4d1adcd930bc5b107de2ab946377c78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827578&auth_key=1760827578-0-0-57c5f8454fa29199c1439432899c535c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94c8f3311bdc992adb79f8a5f04f62cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827585&auth_key=1760827585-0-0-61f917d8fc66b517818872caf4e9d54d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1df461a5cb4e9d75988e9a24f595d681~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827591&auth_key=1760827591-0-0-c0469a23a7b7494ad627ee1d7e6ac0cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94de54ef925e2a27fece0ce0bb6ec5d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827598&auth_key=1760827598-0-0-bd118326c18ef43560e29660ce38533b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5dc30991e726ba52f85c7ca42508b6c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827604&auth_key=1760827604-0-0-76bb3754edd178b9622e30561b25020a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-385b875c0bdecefbbc6f3ce8e7c900be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827610&auth_key=1760827610-0-0-742b257068566b1d14323f9ebeacdcd2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models"><a href="#SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models" class="headerlink" title="SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models"></a>SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models</h2><p><strong>Authors:Harsh Goel, Sai Shankar Narasimhan, Oguzhan Akcin, Sandeep Chinchali</strong></p>
<p>In recent years, significant progress has been made in collecting large-scale datasets to improve segmentation and autonomous driving models. These large-scale datasets are often dominated by common environmental conditions such as â€œClear and Dayâ€ weather, leading to decreased performance in under-represented conditions like â€œRainy and Nightâ€. To address this issue, we introduce SynDiff-AD, a novel data augmentation pipeline that leverages diffusion models (DMs) to generate realistic images for such subgroups. SynDiff-AD uses ControlNet-a DM that guides data generation conditioned on semantic maps-along with a novel prompting scheme that generates subgroup-specific, semantically dense prompts. By augmenting datasets with SynDiff-AD, we improve the performance of segmentation models like Mask2Former and SegFormer by up to 1.2% and 2.3% on the Waymo dataset, and up to 1.4% and 0.7% on the DeepDrive dataset, respectively. Additionally, we demonstrate that our SynDiff-AD pipeline enhances the driving performance of end-to-end autonomous driving models, like AIM-2D and AIM-BEV, by up to 20% across diverse environmental conditions in the CARLA autonomous driving simulator, providing a more robust model. We release our code and pipeline at <a target="_blank" rel="noopener" href="https://github.com/UTAustin-SwarmLab/SynDiff-AD">https://github.com/UTAustin-SwarmLab/SynDiff-AD</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåœ¨æ”¶é›†å¤§è§„æ¨¡æ•°æ®é›†ä»¥æ”¹è¿›åˆ†å‰²å’Œè‡ªåŠ¨é©¾é©¶æ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è¿™äº›å¤§è§„æ¨¡æ•°æ®é›†é€šå¸¸ä»¥â€œæ™´æœ—å’Œç™½å¤©â€ç­‰å¸¸è§ç¯å¢ƒæ¡ä»¶ä¸ºä¸»ï¼Œå¯¼è‡´åœ¨ä»£è¡¨æ€§ä¸è¶³çš„æ¡ä»¶ï¼ˆå¦‚â€œé›¨å¤©å¤œé—´â€ï¼‰ä¸‹æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SynDiff-ADï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºç®¡é“ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç”Ÿæˆæ­¤ç±»å­ç»„çš„ç°å®å›¾åƒã€‚SynDiff-ADä½¿ç”¨ControlNetï¼ˆä¸€ç§åŸºäºè¯­ä¹‰åœ°å›¾å¼•å¯¼æ•°æ®ç”Ÿæˆçš„DMï¼‰ä»¥åŠä¸€ç§æ–°å‹æç¤ºæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆç”Ÿæˆå­ç»„ç‰¹å®šçš„è¯­ä¹‰å¯†é›†æç¤ºã€‚é€šè¿‡SynDiff-ADå¢å¼ºæ•°æ®é›†ï¼Œæˆ‘ä»¬åœ¨Waymoæ•°æ®é›†ä¸Šæé«˜äº†Mask2Formerå’ŒSegFormerç­‰åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ†åˆ«é«˜è¾¾1.2%å’Œ2.3%ï¼›åœ¨DeepDriveæ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜äº†1.4%å’Œ0.7%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜æˆ‘ä»¬çš„SynDiff-ADç®¡é“åœ¨CARLAè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸­çš„å¤šç§ç¯å¢ƒæ¡ä»¶ä¸‹ï¼Œèƒ½å¤Ÿæé«˜ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹ï¼ˆå¦‚AIM-2Då’ŒAIM-BEVï¼‰çš„é©¾é©¶æ€§èƒ½ï¼Œæœ€é«˜æé«˜20%ï¼Œä¸ºæ›´ç¨³å¥çš„æ¨¡å‹æä¾›æ”¯æ’‘ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/UTAustin-SwarmLab/SynDiff-AD">https://github.com/UTAustin-SwarmLab/SynDiff-AD</a>å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œç®¡é“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16776v2">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SynDiff-ADæ•°æ®å¢å¼ºç®¡é“çš„ç ”ç©¶ï¼Œè¯¥ç®¡é“åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé’ˆå¯¹ç‰¹å®šå­ç»„çš„çœŸå®å›¾åƒï¼Œè§£å†³äº†å¤§è§„æ¨¡æ•°æ®é›†åœ¨å¤æ‚ç¯å¢ƒæ¡ä»¶ä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚é€šè¿‡SynDiff-ADå¢å¼ºæ•°æ®é›†ï¼Œæ”¹å–„äº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œæé«˜äº†ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨CARLAè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸­çš„é©¾é©¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynDiff-ADæ˜¯ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºç®¡é“ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ•°æ®é›†ä¸­ç‰¹å®šç¯å¢ƒæ¡ä»¶ä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç”ŸæˆçœŸå®å›¾åƒï¼Œç‰¹åˆ«é’ˆå¯¹é‚£äº›è¢«å¿½è§†çš„ã€ç‰¹å®šçš„ç¯å¢ƒæ¡ä»¶å¦‚â€œé›¨å¤©å¤œæ™šâ€ã€‚</li>
<li>é€šè¿‡SynDiff-ADå¢å¼ºæ•°æ®é›†ï¼Œæé«˜äº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œå¦‚Mask2Formerå’ŒSegFormeråœ¨Waymoå’ŒDeepDriveæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>SynDiff-ADç®¡é“ä¸ä»…æé«˜äº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„é©¾é©¶æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨CARLAè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸Šæµ‹è¯•äº†SynDiff-ADçš„æ•ˆèƒ½ï¼Œç»“æœæ˜¾ç¤ºæ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>ç ”ç©¶äººå‘˜å‘å¸ƒäº†SynDiff-ADçš„ä»£ç å’Œç®¡é“åœ¨<a target="_blank" rel="noopener" href="https://github.com/UTAustin-SwarmLab/SynDiff-AD%E3%80%82">https://github.com/UTAustin-SwarmLab/SynDiff-ADã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e15166a4fff049811b35cbcd268e6406~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827618&auth_key=1760827618-0-0-6b33da3cc49dcb0cc6f3318aa54e5986&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1234c89c5b2203061bc76f37ec95b32a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827624&auth_key=1760827624-0-0-eff46cdf01bcec0afe386075b6daf646&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a9f8b2ffa57ed394bd0f74eb9c7c3dc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827631&auth_key=1760827631-0-0-0ede4009e0e5df210dfd3144e4ed1966&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7697aa86d9bac5c5e167cb74012ebce0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827637&auth_key=1760827637-0-0-da3db6301dc0db088b4037a585982471&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14b0b32f7286938c199dd20aa11b0956~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827644&auth_key=1760827644-0-0-9f5d9181ca6af5ff4b253923203fc39f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-RGB-D-Semantic-Segmentation-with-Deformable-Attention-Transformer"><a href="#Diffusion-based-RGB-D-Semantic-Segmentation-with-Deformable-Attention-Transformer" class="headerlink" title="Diffusion-based RGB-D Semantic Segmentation with Deformable Attention   Transformer"></a>Diffusion-based RGB-D Semantic Segmentation with Deformable Attention   Transformer</h2><p><strong>Authors:Minh Bui, Kostas Alexis</strong></p>
<p>Vision-based perception and reasoning is essential for scene understanding in any autonomous system. RGB and depth images are commonly used to capture both the semantic and geometric features of the environment. Developing methods to reliably interpret this data is critical for real-world applications, where noisy measurements are often unavoidable. In this work, we introduce a diffusion-based framework to address the RGB-D semantic segmentation problem. Additionally, we demonstrate that utilizing a Deformable Attention Transformer as the encoder to extract features from depth images effectively captures the characteristics of invalid regions in depth measurements. Our generative framework shows a greater capacity to model the underlying distribution of RGB-D images, achieving robust performance in challenging scenarios with significantly less training time compared to discriminative methods. Experimental results indicate that our approach achieves State-of-the-Art performance on both the NYUv2 and SUN-RGBD datasets in general and especially in the most challenging of their image data. Our project page will be available at <a target="_blank" rel="noopener" href="https://diffusionmms.github.io/">https://diffusionmms.github.io/</a> </p>
<blockquote>
<p>åŸºäºè§†è§‰çš„æ„ŸçŸ¥å’Œæ¨ç†å¯¹äºä»»ä½•è‡ªä¸»ç³»ç»Ÿçš„åœºæ™¯ç†è§£éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚RGBå’Œæ·±åº¦å›¾åƒé€šå¸¸ç”¨äºæ•æ‰ç¯å¢ƒçš„è¯­ä¹‰å’Œå‡ ä½•ç‰¹å¾ã€‚å¼€å‘èƒ½å¤Ÿå¯é è§£é‡Šè¿™äº›æ•°æ®çš„æ–¹æ³•å¯¹äºå®é™…åº”ç”¨è‡³å…³é‡è¦ï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œå­˜åœ¨å™ªå£°çš„æµ‹é‡å¾€å¾€ä¸å¯é¿å…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶æ¥è§£å†³RGB-Dè¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåˆ©ç”¨å¯å˜å½¢æ³¨æ„åŠ›å˜å‹å™¨ä½œä¸ºç¼–ç å™¨ä»æ·±åº¦å›¾åƒä¸­æå–ç‰¹å¾ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•æ‰æ·±åº¦æµ‹é‡ä¸­æ— æ•ˆåŒºåŸŸçš„ç‰¹æ€§ã€‚æˆ‘ä»¬çš„ç”Ÿæˆæ¡†æ¶è¡¨ç°å‡ºæ›´å¤§çš„èƒ½åŠ›æ¥æ¨¡æ‹ŸRGB-Då›¾åƒçš„åŸºç¡€åˆ†å¸ƒï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å®ç°äº†ç¨³å¥çš„æ€§èƒ½ï¼Œä¸åˆ¤åˆ«æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´å¤§å¤§ç¼©çŸ­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨NYUv2å’ŒSUN-RGBDæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå°¤å…¶æ˜¯åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„å›¾åƒæ•°æ®ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://diffusionmms.github.io/">https://diffusionmms.github.io/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15117v3">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡å¼•å…¥åŸºäºæ‰©æ•£çš„æ¡†æ¶è§£å†³RGB-Dè¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¯å˜å½¢æ³¨æ„åŠ›è½¬æ¢å™¨ä½œä¸ºç¼–ç å™¨æå–æ·±åº¦å›¾åƒç‰¹å¾ï¼Œæœ‰æ•ˆæ•æ‰æ·±åº¦æµ‹é‡ä¸­æ— æ•ˆåŒºåŸŸçš„ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨NYUv2å’ŒSUN-RGBDæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒæ•°æ®ä¸Šè¡¨ç°æ›´å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºæ‰©æ•£çš„æ¡†æ¶æ¥è§£å†³RGB-Dè¯­ä¹‰åˆ†å‰²é—®é¢˜çš„é‡è¦æ€§ã€‚</li>
<li>å¯å˜å½¢æ³¨æ„åŠ›è½¬æ¢å™¨èƒ½æœ‰æ•ˆæå–æ·±åº¦å›¾åƒç‰¹å¾ï¼Œå¹¶æ•æ‰æ·±åº¦æµ‹é‡ä¸­æ— æ•ˆåŒºåŸŸçš„ç‰¹æ€§ã€‚</li>
<li>ä¸åˆ¤åˆ«å¼æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æç”Ÿæˆå¼æ¡†æ¶å…·æœ‰æ›´å¼ºçš„å»ºæ¨¡RGB-Då›¾åƒåŸºç¡€åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ—¶é—´ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨ä¸»æµæ•°æ®é›†NYUv2å’ŒSUN-RGBDä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹è¯¥æ–¹æ³•çš„ç¨³å¥æ€§èƒ½è¡¨ç°ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºè‡ªä¸»ç³»ç»Ÿä¸­åœºæ™¯ç†è§£çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¸¦æœ‰å™ªå£°çš„æµ‹é‡æ•°æ®æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b95d073186551e1bf408589959b9d70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827651&auth_key=1760827651-0-0-7676f669b25de2349c3651a753861558&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bd24e24090711bdec410241ce866bc44~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827658&auth_key=1760827658-0-0-65a3d1ba995a18b7fb16e45ff3ae6392&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2b6e31e37a2b8be95b023bfa61ab013~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827665&auth_key=1760827665-0-0-69736f462778a552d9fb1cee69ab27c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28cf8cf339a25b32219af9a7fa810120~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827671&auth_key=1760827671-0-0-f8ec6e17f1329d14e6c05c4e54aa6ef4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d29557a119099802166e6f1684a792ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827678&auth_key=1760827678-0-0-924a0584e880081b727140b62cf80dcd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-bf2a2ca4252bda68102150e46b9dee33~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827686&auth_key=1760827686-0-0-87bb501e59999ed0f18e58f5f1a0bb51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Scalable Face Security Vision Foundation Model for Deepfake, Diffusion,   and Spoofing Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-85b3126f3e4d8c14cccbeb1047147760~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738137&auth_key=1760738137-0-0-73b410b64a50a459b0e2ccbcfac14c92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Towards Generalist Intelligence in Dentistry Vision Foundation Models   for Oral and Maxillofacial Radiology
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
