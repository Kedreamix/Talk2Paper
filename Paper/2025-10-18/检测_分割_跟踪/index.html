<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-10-18  CoT-PL Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.04122v2/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    70 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection"><a href="#CoT-PL-Visual-Chain-of-Thought-Reasoning-Meets-Pseudo-Labeling-for-Open-Vocabulary-Object-Detection" class="headerlink" title="CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection"></a>CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection</h2><p><strong>Authors:Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim</strong></p>
<p>Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art. </p>
<blockquote>
<p>开放词汇对象检测（OVD）旨在识别和定位超出训练期间所见的对象类别。最近的方法通常利用视觉语言模型（VLM）通过图像文本对齐生成伪标签，使得检测器能够推广到未见类别而无需明确监督。然而，这些方法严重依赖于直接的图像文本匹配，忽略了解释语义复杂场景所必需的中介推理步骤。这导致在面对拥挤或遮挡的视觉环境时稳健性有限。在本文中，我们介绍了CoT-PL，这是一个新的框架，它将结构化的视觉思维链（CoT）推理融入伪标签生成过程。CoT-PL将对象理解分解为三个可解释的步骤：（1）对未知对象的区域感知，（2）通过零样本推理进行类别识别，以及（3）背景定位以区分语义复杂的对象。关键的是，第三步自然地激励了我们的对比背景学习（CBL），它使用预先计算的背景线索作为负样本，以促进对象和背景特征之间的特征解耦。通过这种方式，CoT推理和CBL形成了一条针对拥挤或遮挡场景中稳健伪标签生成的集成管道。值得注意的是，在这两种场景中，我们的新型伪标签质量分别实现了相对于最佳先前技术的103.4%和168.4%的相对改进。我们的广泛实验表明，CoT-PL在开放词汇COCO上实现了+7.7的AP50，在LVIS上实现了+2.9的掩膜AP（针对新型类别），创造了新的技术水准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14792v1">PDF</a> 28 pages, 13 Figures, 12 Tables</p>
<p><strong>Summary</strong>：</p>
<p>该文提出了一种新的开放词汇对象检测框架CoT-PL，它结合了结构化视觉思维链（CoT）推理和对比背景学习（CBL）。该框架解决了现有方法在面对复杂场景时遇到的局限性，特别是在拥挤或遮挡的视觉环境中。通过引入CoT推理和CBL，CoT-PL能够分解为三个可解释的步骤，包括区域感知、类别识别和背景定位，从而提高未见过类别对象的检测性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>开放词汇对象检测（OVD）能识别训练期间未见过的对象类别。</li>
<li>现有方法主要依赖图像文本匹配生成伪标签，忽视了复杂的场景理解步骤。</li>
<li>CoT-PL框架引入结构化视觉思维链（CoT）推理，分解对象理解为三个步骤：区域感知、类别识别和背景定位。</li>
<li>对比背景学习（CBL）方法被提出，利用预计算的背景线索作为负样本，促进对象和背景特征的分离。</li>
<li>CoT-PL框架提高了在拥挤或遮挡场景中的伪标签质量，相对现有最佳方法分别提高了103.4%和168.4%。</li>
<li>在开放词汇COCO和LVIS数据集上，CoT-PL实现了先进性能，分别提高了7.7 AP50和2.9 mask AP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14792">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a51571a7753a8b8f1b0df0f2f6e96076~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739900&auth_key=1760739900-0-0-47f793f96af29683b051607bd21553d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11ae2304570f3115aa020a2f8ca8e459~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739907&auth_key=1760739907-0-0-64d9393619bdbe4298562866d49ed5c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a7c94335e1212f32458fa6635cc6a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739914&auth_key=1760739914-0-0-fff3dfef56afaa8b90888db74af2bc60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a163431ddc0d286b1581671654c24eac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739920&auth_key=1760739920-0-0-55d66cd9bfaf2928f132930b38e6aa8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multiplicative-Loss-for-Enhancing-Semantic-Segmentation-in-Medical-and-Cellular-Images"><a href="#Multiplicative-Loss-for-Enhancing-Semantic-Segmentation-in-Medical-and-Cellular-Images" class="headerlink" title="Multiplicative Loss for Enhancing Semantic Segmentation in Medical and   Cellular Images"></a>Multiplicative Loss for Enhancing Semantic Segmentation in Medical and   Cellular Images</h2><p><strong>Authors:Yuto Yokoi, Kazuhiro Hotta</strong></p>
<p>We propose two novel loss functions, Multiplicative Loss and Confidence-Adaptive Multiplicative Loss, for semantic segmentation in medical and cellular images. Although Cross Entropy and Dice Loss are widely used, their additive combination is sensitive to hyperparameters and often performs suboptimally, especially with limited data. Medical images suffer from data scarcity due to privacy, ethics, and costly annotations, requiring robust and efficient training objectives. Our Multiplicative Loss combines Cross Entropy and Dice losses multiplicatively, dynamically modulating gradients based on prediction confidence. This reduces penalties for confident correct predictions and amplifies gradients for incorrect overconfident ones, stabilizing optimization. Building on this, Confidence-Adaptive Multiplicative Loss applies a confidence-driven exponential scaling inspired by Focal Loss, integrating predicted probabilities and Dice coefficients to emphasize difficult samples. This enhances learning under extreme data scarcity by strengthening gradients when confidence is low. Experiments on cellular and medical segmentation benchmarks show our framework consistently outperforms tuned additive and existing loss functions, offering a simple, effective, and hyperparameter-free mechanism for robust segmentation under challenging data limitations. </p>
<blockquote>
<p>我们针对医学和细胞图像语义分割提出了两种新型损失函数，即乘法损失和置信自适应乘法损失。尽管交叉熵和Dice Loss已得到广泛应用，但其加法组合对超参数敏感，在数据有限的情况下往往表现不佳。由于隐私、伦理和昂贵的标注问题，医疗图像面临数据稀缺的问题，因此需要鲁棒且高效的训练目标。我们的乘法损失将交叉熵和Dice损失相乘，基于预测置信度动态调节梯度。这减少了自信正确预测的惩罚，并放大了错误自信预测的梯度，从而稳定了优化。在此基础上，置信自适应乘法损失应用了一种受Focal Loss启发的置信驱动指数缩放方法，将预测概率和Dice系数相结合，以强调困难样本。在细胞医学分割基准测试上的实验表明，我们的框架在具有挑战性的数据限制条件下始终优于经过调整的加性损失函数和现有损失函数，提供了一种简单、有效且无需超参数机制的稳健分割方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12258v1">PDF</a> Accepted by ICCV2025 Workshop “Third Workshop on Computer Vision for   Automated Medical Diagnosis”</p>
<p><strong>Summary</strong></p>
<p>本文提出两种新颖的适用于医学和细胞图像语义分割的损失函数：乘法损失和置信自适应乘法损失。虽然交叉熵和Dice损失广泛应用于图像分割任务，但在数据有限的情况下，它们的组合对超参数敏感且性能不佳。本文提出的乘法损失通过乘法方式结合交叉熵和Dice损失，根据预测置信度动态调节梯度，减少对自信正确预测的惩罚，并放大错误过度自信预测的梯度，从而稳定优化过程。在此基础上，置信自适应乘法损失应用了一种基于信心的指数缩放策略，集成预测概率和Dice系数以突出困难样本。在细胞和医学图像分割基准测试上的实验表明，该框架在具有挑战性的数据限制条件下，始终优于调优的添加损失和现有损失函数，为稳健分割提供了一个简单、有效且无需超参数调整的新机制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出两种新的损失函数：乘法损失和置信自适应乘法损失，用于医学和细胞图像的语义分割。</li>
<li>针对数据稀缺的医学图像问题，需要稳健和高效的训练目标。</li>
<li>乘法损失结合交叉熵和Dice损失，通过乘法方式组合，根据预测置信度动态调节梯度。</li>
<li>置信自适应乘法损失应用了一种基于信心的指数缩放策略，集成预测概率和Dice系数，强调困难样本的学习。</li>
<li>提出的框架在细胞和医学图像分割基准测试上表现优越，相较于其他损失函数具有优势。</li>
<li>新框架适用于数据有限的情况，对于挑战性的数据限制条件具有稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-deef09f4ddbe938c97d15562d06f9c2f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739927&auth_key=1760739927-0-0-82dedc1f18a250a82d53ca17b0b5c916&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c9a75d51e1087d2a05507b64615420c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739935&auth_key=1760739935-0-0-723d701667838fcd3c6e57234c9857ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c03a602a834d29440fc681276a925b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739942&auth_key=1760739942-0-0-059f56e84600853f565e7f8cec586005&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04ad1e888458d1b131c3f447eea83dba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739948&auth_key=1760739948-0-0-cd553d78a99f6a8724b5d5135075d775&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0830b18ac6dcd1a7a0afc3b63a266f6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739955&auth_key=1760739955-0-0-e988dcbe913926d159d685d726778482&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fc3840c74d43ba79267a1ab8ba72d9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739962&auth_key=1760739962-0-0-bcf4687c20aad25c79f894a92f71df97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="APGNet-Adaptive-Prior-Guided-for-Underwater-Camouflaged-Object-Detection"><a href="#APGNet-Adaptive-Prior-Guided-for-Underwater-Camouflaged-Object-Detection" class="headerlink" title="APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object   Detection"></a>APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object   Detection</h2><p><strong>Authors:Xinxin Huang, Han Sun, Junmin Cai, Ningzhong Liu, Huiyu Zhou</strong></p>
<p>Detecting camouflaged objects in underwater environments is crucial for marine ecological research and resource exploration. However, existing methods face two key challenges: underwater image degradation, including low contrast and color distortion, and the natural camouflage of marine organisms. Traditional image enhancement techniques struggle to restore critical features in degraded images, while camouflaged object detection (COD) methods developed for terrestrial scenes often fail to adapt to underwater environments due to the lack of consideration for underwater optical characteristics.   To address these issues, we propose APGNet, an Adaptive Prior-Guided Network, which integrates a Siamese architecture with a novel prior-guided mechanism to enhance robustness and detection accuracy. First, we employ the Multi-Scale Retinex with Color Restoration (MSRCR) algorithm for data augmentation, generating illumination-invariant images to mitigate degradation effects. Second, we design an Extended Receptive Field (ERF) module combined with a Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual information and refine feature representations. Furthermore, we propose an adaptive prior-guided mechanism that hierarchically fuses position and boundary priors by embedding spatial attention in high-level features for coarse localization and using deformable convolution to refine contours in low-level features.   Extensive experimental results on two public MAS datasets demonstrate that our proposed method APGNet outperforms 15 state-of-art methods under widely used evaluation metrics. </p>
<blockquote>
<p>在水下环境中检测隐蔽物体对海洋生态研究和资源勘探至关重要。然而，现有方法面临两大挑战：水下图像退化，包括对比度低和颜色失真，以及海洋生物的自然伪装。传统图像增强技术在恢复退化图像的关键特征时表现挣扎，而针对陆地场景开发的隐蔽物体检测（COD）方法往往无法适应水下环境，因为它们没有考虑到水下光学特性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12056v1">PDF</a> 6 pages. accepted by ACM MM Asia 2025</p>
<p><strong>Summary</strong><br>水下伪装物体检测对于海洋生态研究和资源探索至关重要。当前方法面临两大挑战：水下图像退化和海洋生物的天然伪装。提出的APGNet网络结合Siamese架构和新颖先验引导机制，提高稳健性和检测精度。采用MSRCR算法进行数据增强，生成光照不变图像以减轻退化影响。设计ERF模块和MPD解码器，捕捉多尺度上下文信息并优化特征表示。提出自适应先验引导机制，分层融合位置和边界先验，嵌入高级特征中的空间注意力实现粗略定位，并使用可变形卷积优化低级特征中的轮廓。在公共MAS数据集上的实验表明，APGNet优于其他15种先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下伪装物体检测在海洋生态研究和资源探索中具有重要作用。</li>
<li>当前方法面临水下图像退化和海洋生物自然伪装两大挑战。</li>
<li>APGNet网络通过结合Siamese架构和新颖先验引导机制来解决这些问题。</li>
<li>使用MSRCR算法进行数据增强，以减轻图像退化的影响。</li>
<li>ERF模块和MPD解码器的设计用于捕捉多尺度上下文信息并优化特征表示。</li>
<li>自适应先验引导机制结合了位置和边界先验，以实现粗略定位和轮廓优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12056">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-126af9de147c0132c976f9d58850227f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739969&auth_key=1760739969-0-0-c8b37c80545a5d8bf18a43944d342b98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b5e9b160a7c3cd6c3226a526ba0416c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739976&auth_key=1760739976-0-0-f9624d1baad4b4ab9de53da8758fb6d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8e864548451e851c228a37bc896d6ca0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739983&auth_key=1760739983-0-0-2635a4541bad7947cdddf634bbd8e265&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e345e15cd098e7561a40a3ad0c26510~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739989&auth_key=1760739989-0-0-29b506ab18a2b5879f587fb1a4f4eeb4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d31fe4376287141395d207066808a470~resize:0:q75.jpg?source=1f5c5e47&expiration=1760739996&auth_key=1760739996-0-0-d4edf91450a2ead3d0447f53934464d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b29177254a542d434d7680d942ef7bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740003&auth_key=1760740003-0-0-46a31d9c66be5dce0eb9a3377e5377a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation"><a href="#A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation" class="headerlink" title="A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation"></a>A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation</h2><p><strong>Authors:Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother</strong></p>
<p>Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding. </p>
<blockquote>
<p>合成数据集广泛用于训练城市场景识别模型，但即使是非常逼真的渲染也与真实图像之间存在明显的差距。在适应特定目标域（例如Cityscapes）时，这种差距尤为突出，目标域中的建筑、植被、物体外观和相机特性的差异限制了下游性能。使用更详细的3D建模来弥补这一差距将需要昂贵的资产和场景设计，这违背了低成本标注数据的初衷。针对这一问题，我们提出了一种新的框架，该框架使用仅不完美的伪标签来适应现成的扩散模型以匹配目标域。一旦训练完成，它就可以从任何合成数据集（包括几小时而非数月创建的低投入来源）的语义图中生成高保真、与目标相匹配的图片。该方法会过滤掉次优生成，纠正图像标签的错位，并标准化跨数据集语义，将薄弱的合成数据转化为具有竞争力的真实域训练集。在五个合成数据集和两个真实目标数据集上的实验表明，与最先进的方法相比，使用我们的方法在分割指标上平均提高了高达+8.0%的mIoU。这使得快速构建的合成数据集与需要大量手动设计的耗时、高投入的合成数据集一样有效。这项工作突显了一个有价值的协作模式，即快速语义原型设计与生成模型的结合，可为城市场景理解实现可扩展的高质量训练数据创建。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11567v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文提出了一种新的框架，该框架利用不完美的伪标签将一个现成的扩散模型适应到目标域。此框架通过语义地图从任何合成数据集中生成高保真、与目标对齐的图像，可以处理低成本的合成数据。经过训练后，该方法能过滤出不佳的生成图像，修正图像标签不对齐的问题，标准化跨数据集的语义，从而将弱合成数据转化为有竞争力的真实域训练集。实验表明，该方法在五个合成数据集和两个真实目标数据集上的分割增益达到最高8%，使快速构建的合成数据集与高成本耗时的大量手动设计的合成数据集具有同等效果。这项研究突显了一种合作范例，通过快速语义原型与生成模型的结合，实现了大规模高质量训练数据的快速创建，为城市场景理解提供有力支持。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>合成数据集广泛用于训练城市场景识别模型，但与现实图像存在差距。</li>
<li>现有方法通过更详细的3D建模来缩小差距会增加成本和耗时，不符合低成本标签数据的初衷。</li>
<li>新框架适应目标域仅使用不完美的伪标签，能快速从语义地图生成高保真、目标对齐的图像。</li>
<li>方法能够过滤不佳生成图像，修正图像标签不对齐问题，标准化跨数据集的语义。</li>
<li>与现有翻译方法相比，新框架在多个数据集上的分割增益显著。</li>
<li>快速语义原型与生成模型的结合提高了合成数据的质量，使其接近真实数据的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11567">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-44f56406048b2d3b9f0254bf7305b839~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740011&auth_key=1760740011-0-0-5f3bd4557707da33d996331d2afbe452&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7c7959ed7552ebe8041cbf8ff70bfeb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740018&auth_key=1760740018-0-0-a9729201021dad328c9c73e5af8354e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf10e433c7c366ef0effd80719584c35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740025&auth_key=1760740025-0-0-8baceb2b72d39603eb78454c0b31894a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-Perspectives-Foundation-Model-Guided-BEV-Maps-for-3D-Object-Detection-and-Tracking"><a href="#Bridging-Perspectives-Foundation-Model-Guided-BEV-Maps-for-3D-Object-Detection-and-Tracking" class="headerlink" title="Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object   Detection and Tracking"></a>Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object   Detection and Tracking</h2><p><strong>Authors:Markus Käppeler, Özgün Çiçek, Daniele Cattaneo, Claudius Gläser, Yakov Miron, Abhinav Valada</strong></p>
<p>Camera-based 3D object detection and tracking are essential for perception in autonomous driving. Current state-of-the-art approaches often rely exclusively on either perspective-view (PV) or bird’s-eye-view (BEV) features, limiting their ability to leverage both fine-grained object details and spatially structured scene representations. In this work, we propose DualViewDistill, a hybrid detection and tracking framework that incorporates both PV and BEV camera image features to leverage their complementary strengths. Our approach introduces BEV maps guided by foundation models, leveraging descriptive DINOv2 features that are distilled into BEV representations through a novel distillation process. By integrating PV features with BEV maps enriched with semantic and geometric features from DINOv2, our model leverages this hybrid representation via deformable aggregation to enhance 3D object detection and tracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarks demonstrate that DualViewDistill achieves state-of-the-art performance. The results showcase the potential of foundation model BEV maps to enable more reliable perception for autonomous driving. We make the code and pre-trained models available at <a target="_blank" rel="noopener" href="https://dualviewdistill.cs.uni-freiburg.de/">https://dualviewdistill.cs.uni-freiburg.de</a> . </p>
<blockquote>
<p>基于摄像头的3D目标检测和跟踪是自动驾驶感知中的关键技术。目前最先进的方案通常只依赖于透视视图（PV）或鸟瞰视图（BEV）特征，这限制了它们利用精细目标细节和空间结构化场景表示的能力。在这项工作中，我们提出了DualViewDistill，这是一种混合检测和跟踪框架，它结合了PV和BEV摄像头图像特征，以利用它们的互补优势。我们的方法引入了由基础模型引导的BEV地图，利用描述性的DINOv2特征，这些特征通过一种新型蒸馏过程蒸馏成BEV表示。通过将PV特征与通过基础模型DINOv2提供的丰富语义和几何特征的BEV地图相结合，我们的模型通过可变形聚合利用这种混合表示，以提高3D目标检测和跟踪能力。在nuScenes和Argoverse 2基准测试上的广泛实验表明，DualViewDistill达到了最先进的性能。结果展示了基础模型BEV地图在实现更可靠的自动驾驶感知方面的潜力。我们在<a target="_blank" rel="noopener" href="https://dualviewdistill.cs.uni-freiburg.de上提供了代码和预训练模型./">https://dualviewdistill.cs.uni-freiburg.de上提供了代码和预训练模型。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10287v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于DualViewDistill的混合检测与跟踪框架，该框架结合了透视视图（PV）和鸟瞰视图（BEV）相机图像特征，以提高自主驾驶中的三维物体检测与跟踪性能。通过引入由基础模型引导的BEV地图，并结合透视视图特征，实现了对三维物体检测与跟踪的增强。在nuScenes和Argoverse 2基准测试中，该框架达到了先进性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自主驾驶中，三维物体检测与跟踪至关重要。</li>
<li>当前方法主要依赖透视视图（PV）或鸟瞰视图（BEV）特征，各有局限性。</li>
<li>DualViewDistill框架结合了PV和BEV特征，发挥两者优势。</li>
<li>引入基础模型引导的BEV地图，通过新颖蒸馏过程获取DINOv2特征。</li>
<li>结合PV特征与丰富的BEV地图，通过可变形聚合增强三维物体检测与跟踪。</li>
<li>在nuScenes和Argoverse 2基准测试中，DualViewDistill达到先进性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10287">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d50d8b502c1056ec8fa578db9b98b103~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740032&auth_key=1760740032-0-0-3173a6ef9959132defe70ff95f4884d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7b3ea18d4f41c3a68d23ed7c02f396a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740039&auth_key=1760740039-0-0-b144555effcc76395265579779149d2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d907ab171867bc3ec5b3668b132c1be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740046&auth_key=1760740046-0-0-487e1be28ccb2160c1a6fe3c2f5384e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f0b26a09dad3aa0643825865d46539e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740053&auth_key=1760740053-0-0-b30c4fcafb3dc9e080f24f0b4a8d4b59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-Single-Domain-Generalization-of-LiDAR-based-Semantic-Segmentation-under-Imperfect-Labels"><a href="#Exploring-Single-Domain-Generalization-of-LiDAR-based-Semantic-Segmentation-under-Imperfect-Labels" class="headerlink" title="Exploring Single Domain Generalization of LiDAR-based Semantic   Segmentation under Imperfect Labels"></a>Exploring Single Domain Generalization of LiDAR-based Semantic   Segmentation under Imperfect Labels</h2><p><strong>Authors:Weitong Kong, Zichao Zeng, Di Wen, Jiale Wei, Kunyu Peng, June Moh Goo, Jan Boehm, Rainer Stiefelhagen</strong></p>
<p>Accurate perception is critical for vehicle safety, with LiDAR as a key enabler in autonomous driving. To ensure robust performance across environments, sensor types, and weather conditions without costly re-annotation, domain generalization in LiDAR-based 3D semantic segmentation is essential. However, LiDAR annotations are often noisy due to sensor imperfections, occlusions, and human errors. Such noise degrades segmentation accuracy and is further amplified under domain shifts, threatening system reliability. While noisy-label learning is well-studied in images, its extension to 3D LiDAR segmentation under domain generalization remains largely unexplored, as the sparse and irregular structure of point clouds limits direct use of 2D methods. To address this gap, we introduce the novel task Domain Generalization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL) and establish the first benchmark by adapting three representative noisy-label learning strategies from image classification to 3D segmentation. However, we find that existing noisy-label learning approaches adapt poorly to LiDAR data. We therefore propose DuNe, a dual-view framework with strong and weak branches that enforce feature-level consistency and apply cross-entropy loss based on confidence-aware filtering of predictions. Our approach shows state-of-the-art performance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and 52.58% on SemanticPOSS under 10% symmetric label noise, with an overall Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby demonstrating robust domain generalization in DGLSS-NL tasks. The code is available on our project page. </p>
<blockquote>
<p>精确感知对于车辆安全至关重要，激光雷达作为自动驾驶的关键技术起到了关键作用。为了确保在各种环境、传感器类型和天气条件下的稳健性能，且无需昂贵的重新标注，激光雷达基于域的3D语义分割泛化至关重要。然而，由于传感器缺陷、遮挡和人为错误，激光雷达标注往往存在噪声。这种噪声会降低分割精度，在域转移的情况下会进一步放大，威胁系统可靠性。虽然噪声标签学习在图像中得到了很好的研究，但其扩展到域泛化下的3D激光雷达分割仍然很大程度上未被探索，点云的稀疏和不规则结构限制了直接使用2D方法。为了填补这一空白，我们引入了噪声标签下的激光雷达语义分割域泛化（DGLSS-NL）的新任务，并通过将三个具有代表性的噪声标签学习策略从图像分类适应到3D分割，建立了第一个基准测试。然而，我们发现现有的噪声标签学习方法对激光雷达数据适应性较差。因此，我们提出了DuNe，这是一个具有强分支和弱分支的双视图框架，它强制实施特征级别的一致性，并基于预测的信心感知过滤应用交叉熵损失。我们的方法达到了最先进的性能，在SemanticKITTI上实现了56.86%的mIoU，nuScenes上实现了42.28%，SemanticPOSS上实现了52.58%，在10%对称标签噪声的情况下，整体算术平均值（AM）为49.57%，调和平均值（HM）为48.50%，从而在DGLSS-NL任务中实现了稳健的域泛化。代码可在我们的项目页面获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09035v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了LiDAR在自动驾驶中的重要作用，并针对LiDAR标注噪声问题，提出了域泛化下的LiDAR语义分割任务（DGLSS-NL）。由于现有噪声标签学习方法对LiDAR数据适应性较差，因此提出一种双视图框架DuNe，通过强弱分支实现特征级别的一致性，并应用基于置信度过滤的预测交叉熵损失。该方法在多个数据集上实现了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LiDAR在自动驾驶车辆安全中起到关键作用，但环境、传感器类型和天气条件的差异要求传感器具有域泛化能力。</li>
<li>LiDAR标注存在噪声问题，影响分割精度，且在域变化下问题加剧。</li>
<li>现有噪声标签学习方法在LiDAR数据上的适应性较差。</li>
<li>提出了DGLSS-NL任务，并建立了第一个基准测试。</li>
<li>提出了DuNe框架，通过强弱分支实现特征一致性，并应用置信度过滤的预测交叉熵损失。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09035">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-833d7787dae54d0da2787dc3dfed2900~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740061&auth_key=1760740061-0-0-06e1eb266d88c034b70b8461027ae7a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d68607e4f383b3aa86cd594760b3bb62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740068&auth_key=1760740068-0-0-889401486203d57a66084ff2a0dc1285&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1eed66d7473cd0dbac240948e5fe2868~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740074&auth_key=1760740074-0-0-d0e329bf346fede74166ecdbb3512e98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4158c4bcd63acab11be7add9978a7783~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740081&auth_key=1760740081-0-0-548d3d67bae5855f12d92db9cedca556&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8fc3203d6d5c30fb19f9b20cd28fa81~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740088&auth_key=1760740088-0-0-a1548ec65538ba91e9835c1991392a3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FOLK-Fast-Open-Vocabulary-3D-Instance-Segmentation-via-Label-guided-Knowledge-Distillation"><a href="#FOLK-Fast-Open-Vocabulary-3D-Instance-Segmentation-via-Label-guided-Knowledge-Distillation" class="headerlink" title="FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided   Knowledge Distillation"></a>FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided   Knowledge Distillation</h2><p><strong>Authors:Hongrui Wu, Zhicheng Gao, Jin Cao, Kelu Yao, Wen Shen, Zhihua Wei</strong></p>
<p>Open-vocabulary 3D instance segmentation seeks to segment and classify instances beyond the annotated label space. Existing methods typically map 3D instances to 2D RGB-D images, and then employ vision-language models (VLMs) for classification. However, such a mapping strategy usually introduces noise from 2D occlusions and incurs substantial computational and memory costs during inference, slowing down the inference speed. To address the above problems, we propose a Fast Open-vocabulary 3D instance segmentation method via Label-guided Knowledge distillation (FOLK). Our core idea is to design a teacher model that extracts high-quality instance embeddings and distills its open-vocabulary knowledge into a 3D student model. In this way, during inference, the distilled 3D model can directly classify instances from the 3D point cloud, avoiding noise caused by occlusions and significantly accelerating the inference process. Specifically, we first design a teacher model to generate a 2D CLIP embedding for each 3D instance, incorporating both visibility and viewpoint diversity, which serves as the learning target for distillation. We then develop a 3D student model that directly produces a 3D embedding for each 3D instance. During training, we propose a label-guided distillation algorithm to distill open-vocabulary knowledge from label-consistent 2D embeddings into the student model. FOLK conducted experiments on the ScanNet200 and Replica datasets, achieving state-of-the-art performance on the ScanNet200 dataset with an AP50 score of 35.7, while running approximately 6.0x to 152.2x faster than previous methods. All codes will be released after the paper is accepted. </p>
<blockquote>
<p>开放词汇表下的三维实例分割旨在分割并分类标注标签空间之外的实例。现有方法通常将三维实例映射到二维RGB-D图像上，然后采用视觉语言模型（VLMs）进行分类。然而，这种映射策略通常会引入来自二维遮挡的噪声，并在推理过程中产生巨大的计算和内存成本，从而降低了推理速度。为了解决上述问题，我们提出了一种基于标签引导知识蒸馏的快速开放词汇表三维实例分割方法（FOLK）。我们的核心思想是设计一种教师模型，提取高质量实例嵌入，并将其开放词汇表知识蒸馏到三维学生模型中。通过这种方式，在推理过程中，蒸馏后的三维模型可以直接对三维点云中的实例进行分类，避免了遮挡引起的噪声，并显著加速了推理过程。具体来说，我们首先设计了一个教师模型，为每个三维实例生成一个二维CLIP嵌入，同时考虑可见性和视点多样性，作为蒸馏的学习目标。然后我们开发了一个三维学生模型，该模型直接为每个三维实例生成一个三维嵌入。在训练过程中，我们提出了一种标签引导的蒸馏算法，将标签一致性的二维嵌入中的开放词汇表知识蒸馏到学生模型中。FOLK在ScanNet200和Replica数据集上进行了实验，在ScanNet200数据集上取得了最先进的性能，AP50得分为35.7，同时运行速度比以前的方法快大约6.0倍到152.2倍。论文被接受后将发布所有代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08849v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出了一种基于标签引导知识蒸馏的快速开放词汇表三维实例分割方法（FOLK）。该方法旨在解决现有开放词汇表三维实例分割方法在计算效率和识别精度方面的问题。通过建立教师模型，提取高质量实例嵌入，并蒸馏其开放词汇表知识到三维学生模型中，使得在推理过程中，蒸馏后的三维模型可以直接对三维点云中的实例进行分类，避免了二维遮挡引起的噪声，并显著加速了推理过程。实验结果表明，FOLK方法在ScanNet200数据集上达到了先进性能，同时显著提高了计算效率。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了基于标签引导知识蒸馏的开放词汇表三维实例分割方法（FOLK）。</li>
<li>通过建立教师模型提取高质量实例嵌入，解决了现有方法在计算效率和识别精度方面的问题。</li>
<li>蒸馏后的三维模型可以直接对三维点云中的实例进行分类，避免了二维遮挡引起的噪声。</li>
<li>FOLK方法显著加速了推理过程。</li>
<li>在ScanNet200数据集上达到了先进性能，AP50分数为35.7。</li>
<li>与之前的方法相比，FOLK方法的计算效率更高，运行速度提高了大约6.0至152.2倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08849">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a09b90587882239cdbb878137b1aed8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740096&auth_key=1760740096-0-0-05d00193d0a92500adabc6c447c3ee18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-64a9e19faf869e5e0496eaf48044fc5f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740104&auth_key=1760740104-0-0-c9a89e68af1dedb78b8c49bfa9866943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95b2f94d321f32e7eabcf9e585e4cdd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760740111&auth_key=1760740111-0-0-33bb1f95b34d151b31ad9cd5641a19a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2510.08849v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2510.08849v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ALISE-Annotation-Free-LiDAR-Instance-Segmentation-for-Autonomous-Driving"><a href="#ALISE-Annotation-Free-LiDAR-Instance-Segmentation-for-Autonomous-Driving" class="headerlink" title="ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous   Driving"></a>ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous   Driving</h2><p><strong>Authors:Yongxuan Lyu, Guangfeng Jiang, Hongsi Liu, Jun Liu</strong></p>
<p>The manual annotation of outdoor LiDAR point clouds for instance segmentation is extremely costly and time-consuming. Current methods attempt to reduce this burden but still rely on some form of human labeling. To completely eliminate this dependency, we introduce ALISE, a novel framework that performs LiDAR instance segmentation without any annotations. The central challenge is to generate high-quality pseudo-labels in a fully unsupervised manner. Our approach starts by employing Vision Foundation Models (VFMs), guided by text and images, to produce initial pseudo-labels. We then refine these labels through a dedicated spatio-temporal voting module, which combines 2D and 3D semantics for both offline and online optimization. To achieve superior feature learning, we further introduce two forms of semantic supervision: a set of 2D prior-based losses that inject visual knowledge into the 3D network, and a novel prototype-based contrastive loss that builds a discriminative feature space by exploiting 3D semantic consistency. This comprehensive design results in significant performance gains, establishing a new state-of-the-art for unsupervised 3D instance segmentation. Remarkably, our approach even outperforms MWSIS, a method that operates with supervision from ground-truth (GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%). </p>
<blockquote>
<p>室外激光雷达点云的实例分割手动标注成本极高且耗时。当前的方法试图减少这一负担，但仍依赖于某种形式的人工标注。为了完全消除这种依赖，我们引入了ALISE这一全新框架，该框架可在无需任何标注的情况下执行激光雷达实例分割。核心挑战在于以完全无监督的方式生成高质量伪标签。我们的方法首先使用文本和图像指导的视觉基础模型（VFMs）生成初始伪标签。然后，我们通过专用的时空投票模块对这些标签进行精炼，该模块结合了二维和三维语义，用于在线和离线优化。为了实现卓越的特征学习，我们还引入了两种形式的语义监督：一组基于二维先验的损失，将视觉知识注入三维网络；一种新型基于原型对比损失，通过利用三维语义一致性构建判别特征空间。这一全面的设计带来了显著的性能提升，为无监督三维实例分割建立了新的最先进的水平。值得注意的是，我们的方法甚至超越了使用真实二维边界框进行监督的方法MWSIS，在mAP上高出2.53%（50.95%对比48.42%）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05752v2">PDF</a> </p>
<p><strong>Summary</strong><br>自动标注激光雷达点云实例是一个耗时且成本高昂的任务。研究者们一直在尝试降低这种负担，但大部分方法仍然依赖人工标注。本文提出一种名为ALISE的新框架，它无需任何标注即可进行激光雷达实例分割。该框架通过视觉基础模型生成高质量伪标签，并通过时空投票模块进行精细化处理，结合二维和三维语义进行在线和离线优化。通过引入两种语义监督方式，该框架实现了卓越的特征学习能力，并达到了监督学习方法的性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>激光雷达点云实例手动标注成本高昂且耗时。</li>
<li>当前方法虽尝试减少人工标注的依赖，但仍需某种形式的标注。</li>
<li>ALISE框架无需任何标注即可进行激光雷达实例分割。</li>
<li>ALISE框架通过视觉基础模型生成伪标签，并采用时空投票模块进行精细化处理。</li>
<li>该框架结合二维和三维语义进行在线和离线优化，实现卓越的特征学习能力。</li>
<li>引入两种语义监督方式，提升特征学习效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05752">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2510.05752v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2510.05752v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2510.05752v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Impact-of-Radiographic-Noise-on-Chest-X-ray-Semantic-Segmentation-and-Disease-Classification-Using-a-Scalable-Noise-Injection-Framework"><a href="#Evaluating-the-Impact-of-Radiographic-Noise-on-Chest-X-ray-Semantic-Segmentation-and-Disease-Classification-Using-a-Scalable-Noise-Injection-Framework" class="headerlink" title="Evaluating the Impact of Radiographic Noise on Chest X-ray Semantic   Segmentation and Disease Classification Using a Scalable Noise Injection   Framework"></a>Evaluating the Impact of Radiographic Noise on Chest X-ray Semantic   Segmentation and Disease Classification Using a Scalable Noise Injection   Framework</h2><p><strong>Authors:Derek Jiu, Kiran Nijjer, Nishant Chinta, Ryan Bui, Kevin Zhu</strong></p>
<p>Deep learning models are increasingly used for radiographic analysis, but their reliability is challenged by the stochastic noise inherent in clinical imaging. A systematic, cross-task understanding of how different noise types impact these models is lacking. Here, we evaluate the robustness of state-of-the-art convolutional neural networks (CNNs) to simulated quantum (Poisson) and electronic (Gaussian) noise in two key chest X-ray tasks: semantic segmentation and pulmonary disease classification. Using a novel, scalable noise injection framework, we applied controlled, clinically-motivated noise severities to common architectures (UNet, DeepLabV3, FPN; ResNet, DenseNet, EfficientNet) on public datasets (Landmark, ChestX-ray14). Our results reveal a stark dichotomy in task robustness. Semantic segmentation models proved highly vulnerable, with lung segmentation performance collapsing under severe electronic noise (Dice Similarity Coefficient drop of 0.843), signifying a near-total model failure. In contrast, classification tasks demonstrated greater overall resilience, but this robustness was not uniform. We discovered a differential vulnerability: certain tasks, such as distinguishing Pneumothorax from Atelectasis, failed catastrophically under quantum noise (AUROC drop of 0.355), while others were more susceptible to electronic noise. These findings demonstrate that while classification models possess a degree of inherent robustness, pixel-level segmentation tasks are far more brittle. The task- and noise-specific nature of model failure underscores the critical need for targeted validation and mitigation strategies before the safe clinical deployment of diagnostic AI. </p>
<blockquote>
<p>深度学习模型在放射学分析中的应用越来越广泛，但临床图像中固有的随机噪声对其可靠性提出了挑战。目前还缺乏关于不同噪声类型如何影响这些模型的系统、跨任务的了解。在这里，我们评估了最先进的卷积神经网络（CNN）对两种关键胸部X射线任务中模拟的量子（泊松）噪声和电子（高斯）噪声的稳健性：语义分割和肺部疾病分类。我们采用了一种新型的可扩展噪声注入框架，对公共数据集（Landmark、ChestX-ray14）上的常见架构（UNet、DeepLabV3、FPN；ResNet、DenseNet、EfficientNet）应用了受控的、临床激励的噪声严重程度。我们的结果揭示了任务稳健性方面的鲜明差异。语义分割模型证明高度脆弱，在严重的电子噪声下，肺部分割性能大幅下降（Dice相似系数下降0.843），这标志着模型近乎完全失败。相比之下，分类任务表现出更大的整体韧性，但这种稳健性并不统一。我们发现了一种差异性脆弱性：某些任务，如区分气胸和肺不张，在量子噪声下遭遇了灾难性失败（AUROC下降0.355），而其他任务则更容易受到电子噪声的影响。这些结果表明，虽然分类模型具有一定的固有稳健性，但像素级分割任务则更为脆弱。模型失败的特定任务和噪声性质的特点强调了有针对性的验证和在诊断人工智能临床部署前采取缓解策略的关键需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25265v2">PDF</a> Accepted to ARRS 2026 Annual Meeting</p>
<p><strong>Summary</strong>：深度学习模型在放射学分析中的应用日益广泛，但在临床图像中存在的随机噪声对其可靠性提出了挑战。本研究评估了最先进的卷积神经网络（CNN）对模拟量子（泊松）和电子（高斯）噪声的鲁棒性，涉及两个关键胸部X射线任务：语义分割和肺部疾病分类。通过使用新型的可扩展噪声注入框架，在公共数据集上应用了受控的临床噪声严重程度。结果显示任务鲁棒性存在明显差异。语义分割模型高度脆弱，在严重电子噪声下肺部分割性能大幅下降（Dice相似系数下降0.843），几乎导致模型完全失效。分类任务则表现出更大的整体稳健性，但这种稳健性并不统一。研究发现不同任务的脆弱性存在差异，某些任务如区分气胸与肺不张在量子噪声下失效（AUROC下降0.355），而其他任务则更易受电子噪声影响。这些发现表明，虽然分类模型具有一定的固有稳健性，但像素级分割任务更为脆弱。模型失败的任务和噪声特定性质强调了在部署诊断人工智能之前需要有针对性的验证和缓解策略。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>深度学习模型在临床图像分析中的应用受到随机噪声的影响。</li>
<li>本研究评估了卷积神经网络对量子噪声和电子噪声的鲁棒性。</li>
<li>语义分割模型对电子噪声高度脆弱，肺部分割性能大幅下降。</li>
<li>分类任务表现出更大的整体稳健性，但不同任务间的脆弱性存在差异。</li>
<li>某些分类任务在量子噪声下会失效，而其他任务更易受电子噪声影响。</li>
<li>分类模型具有一定的固有稳健性，但像素级分割任务更为脆弱。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2509.25265v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2509.25265v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GLSim-Detecting-Object-Hallucinations-in-LVLMs-via-Global-Local-Similarity"><a href="#GLSim-Detecting-Object-Hallucinations-in-LVLMs-via-Global-Local-Similarity" class="headerlink" title="GLSim: Detecting Object Hallucinations in LVLMs via Global-Local   Similarity"></a>GLSim: Detecting Object Hallucinations in LVLMs via Global-Local   Similarity</h2><p><strong>Authors:Seongheon Park, Sharon Li</strong></p>
<p>Object hallucination in large vision-language models presents a significant challenge to their safe deployment in real-world applications. Recent works have proposed object-level hallucination scores to estimate the likelihood of object hallucination; however, these methods typically adopt either a global or local perspective in isolation, which may limit detection reliability. In this paper, we introduce GLSim, a novel training-free object hallucination detection framework that leverages complementary global and local embedding similarity signals between image and text modalities, enabling more accurate and reliable hallucination detection in diverse scenarios. We comprehensively benchmark existing object hallucination detection methods and demonstrate that GLSim achieves superior detection performance, outperforming competitive baselines by a significant margin. </p>
<blockquote>
<p>对象幻觉在大型视觉语言模型中是一个重大挑战，对于其在真实世界应用中的安全部署构成了威胁。近期的研究工作已经提出了对象级别的幻觉评分来估计出现对象幻觉的可能性；然而，这些方法通常孤立地采用全局或局部视角，可能会限制检测的可靠性。在本文中，我们介绍了GLSim，这是一种新型的无训练对象幻觉检测框架，它利用图像和文本模态之间的全局和局部嵌入相似性信号的互补性，能够在各种场景中实现更准确、更可靠地幻觉检测。我们对现有的对象幻觉检测方法进行了全面的基准测试，并证明GLSim的检测性能优于其他基线方法，具有显著的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19972v3">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型视觉语言模型中对象臆测现象带来的挑战。虽然已有方法通过对象级臆测分数估计臆测发生的可能性，但它们往往采用全局或局部单一视角，可能影响检测可靠性。本文提出一种全新的训练外对象臆测检测框架GLSim，利用图像和文本模态之间的全局和局部嵌入相似性信号，提高不同场景下的臆测检测准确性和可靠性。经过对现有对象臆测检测方法的全面评估，GLSim表现出优越的检测性能，显著超越了其他基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>对象臆测是大型视觉语言模型在现实世界应用中的一大挑战。</li>
<li>现有方法主要通过全局或局部单一视角检测对象臆测，可能限制检测可靠性。</li>
<li>GLSim是一种全新的训练外对象臆测检测框架，结合全局和局部嵌入相似性信号，提高检测准确性。</li>
<li>GLSim实现了对现有对象臆测检测方法的全面评估，并展现出优越的检测性能。</li>
<li>GLSim在多种场景下都能有效检测对象臆测，性能远超其他基线方法。</li>
<li>该框架对于确保视觉语言模型在真实环境中的安全部署具有重要意义。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19972">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2508.19972v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2508.19972v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2508.19972v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Contour-Errors-An-Ego-Centric-Metric-for-Reliable-3D-Multi-Object-Tracking"><a href="#Contour-Errors-An-Ego-Centric-Metric-for-Reliable-3D-Multi-Object-Tracking" class="headerlink" title="Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object   Tracking"></a>Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object   Tracking</h2><p><strong>Authors:Sharang Kaul, Mario Berk, Thiemo Gerbich, Abhinav Valada</strong></p>
<p>Finding reliable matches is essential in multi-object tracking to ensure the accuracy and reliability of perception systems in safety-critical applications such as autonomous vehicles. Effective matching mitigates perception errors, enhancing object identification and tracking for improved performance and safety. However, traditional metrics such as Intersection over Union (IoU) and Center Point Distances (CPDs), which are effective in 2D image planes, often fail to find critical matches in complex 3D scenes. To address this limitation, we introduce Contour Errors (CEs), an ego or object-centric metric for identifying matches of interest in tracking scenarios from a functional perspective. By comparing bounding boxes in the ego vehicle’s frame, contour errors provide a more functionally relevant assessment of object matches. Extensive experiments on the nuScenes dataset demonstrate that contour errors improve the reliability of matches over the state-of-the-art 2D IoU and CPD metrics in tracking-by-detection methods. In 3D car tracking, our results show that Contour Errors reduce functional failures (FPs&#x2F;FNs) by 80% at close ranges and 60% at far ranges compared to IoU in the evaluation stage. </p>
<blockquote>
<p>在多目标跟踪中，找到可靠的匹配是确保安全关键应用（如自动驾驶汽车）中的感知系统准确性和可靠性的关键。有效的匹配可以减轻感知错误，增强对象识别和跟踪，从而提高性能和安全性。然而，传统的度量标准，如交并比（IoU）和中心点距离（CPD），在二维图像平面上是有效的，但在复杂的三维场景中往往难以找到关键的匹配。为了解决这一局限性，我们引入了轮廓误差（CEs），这是一种以自我或对象为中心的度量标准，用于从功能角度识别跟踪场景中的匹配项。通过比较自我车辆框架中的边界框，轮廓误差为对象匹配提供了更为功能相关的评估。在nuScenes数据集上的大量实验表明，与当前最先进的基于检测的跟踪方法的IoU和CPD度量相比，轮廓误差提高了匹配的可靠性。在三维汽车跟踪中，我们的结果表明，在评估阶段，与IoU相比，轮廓误差在近距离减少了80%、远距离减少了60%的功能性故障（FPs&#x2F;FNs）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04122v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了多目标跟踪中可靠匹配的重要性，并指出传统度量标准如IoU和CPD在复杂3D场景中难以找到关键匹配的问题。为解决此问题，引入了轮廓误差（CEs）这一以自我或对象为中心的度量标准，从功能角度评估跟踪场景中的感兴趣匹配。轮廓误差通过比较以自我车辆为框架的边界框，提供了对目标匹配的更功能相关的评估。实验证明，轮廓误差在跟踪检测方法和三维车辆跟踪中提高了匹配的可靠性，减少了功能故障。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多目标跟踪中的可靠匹配对于确保安全关键应用（如自动驾驶车辆）的感知系统准确性和可靠性至关重要。</li>
<li>传统度量标准如IoU和CPD在复杂3D场景中难以找到关键匹配。</li>
<li>轮廓误差（CEs）是一种新型的匹配度量标准，通过比较以自我车辆为框架的边界框来评估目标匹配。</li>
<li>轮廓误差从功能角度评估匹配，提供更功能相关的评估结果。</li>
<li>在跟踪检测方法中，轮廓误差提高了匹配的可靠性，优于现有的IoU和CPD度量标准。</li>
<li>在三维车辆跟踪中，轮廓误差显著减少了功能故障（FPs&#x2F;FNs）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.04122v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.04122v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.04122v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.04122v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.04122v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control"><a href="#Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control" class="headerlink" title="Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise   Shape and Semantic Control"></a>Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise   Shape and Semantic Control</h2><p><strong>Authors:Danfeng li, Hui Zhang, Sheng Wang, Jiacheng Li, Zuxuan Wu</strong></p>
<p>Despite recent advances in diffusion models, top-tier text-to-image (T2I) models still struggle to achieve precise spatial layout control, i.e. accurately generating entities with specified attributes and locations. Segmentation-mask-to-image (S2I) generation has emerged as a promising solution by incorporating pixel-level spatial guidance and regional text prompts. However, existing S2I methods fail to simultaneously ensure semantic consistency and shape consistency. To address these challenges, we propose Seg2Any, a novel S2I framework built upon advanced multimodal diffusion transformers (e.g. FLUX). First, to achieve both semantic and shape consistency, we decouple segmentation mask conditions into regional semantic and high-frequency shape components. The regional semantic condition is introduced by a Semantic Alignment Attention Mask, ensuring that generated entities adhere to their assigned text prompts. The high-frequency shape condition, representing entity boundaries, is encoded as an Entity Contour Map and then introduced as an additional modality via multi-modal attention to guide image spatial structure. Second, to prevent attribute leakage across entities in multi-entity scenarios, we introduce an Attribute Isolation Attention Mask mechanism, which constrains each entity’s image tokens to attend exclusively to themselves during image self-attention. To support open-set S2I generation, we construct SACap-1M, a large-scale dataset containing 1 million images with 5.9 million segmented entities and detailed regional captions, along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive experiments demonstrate that Seg2Any achieves state-of-the-art performance on both open-set and closed-set S2I benchmarks, particularly in fine-grained spatial and attribute control of entities. </p>
<blockquote>
<p>尽管扩散模型最近有所进展，顶级文本到图像（T2I）模型仍然难以实现精确的空间布局控制，即准确生成具有指定属性和位置的实体。图像分割掩膜到图像（S2I）生成技术作为一种结合像素级空间指导和区域文本提示的有前途的解决方案而出现。然而，现有的S2I方法无法同时确保语义一致性和形状一致性。为了解决这些挑战，我们提出了Seg2Any，一个基于先进的多模态扩散变压器（例如FLUX）的新型S2I框架。首先，为了实现语义和形状的一致性，我们将分割掩膜条件分解为区域语义和高频形状组件。通过语义对齐注意力掩膜引入区域语义条件，确保生成的实体符合其分配到的文本提示。代表实体边界的高频形状条件被编码为实体轮廓图，然后通过多模态注意力作为附加模态引入，以指导图像的空间结构。其次，为了防止多实体场景中属性在实体之间的泄露，我们引入了属性隔离注意力掩膜机制，该机制约束每个实体的图像令牌在图像自注意力期间只专注于自身。为了支持开放集S2I生成，我们构建了SACap-1M，这是一个包含100万张图像、590万个分割实体和详细区域描述的大规模数据集，以及用于全面S2I评估的SACap-Eval基准测试。大量实验表明，Seg2Any在开放集和封闭集S2I基准测试上均达到了最先进的性能，尤其是在实体的精细空间控制上表现出色。特别是在具有精细属性和位置的场景（例如城市和自然风景的详细重建）中表现得更加出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00596v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的分割掩膜到图像（S2I）框架Seg2Any，用于解决文本到图像生成中精确空间布局控制的问题。Seg2Any利用先进的多模态扩散变压器（如FLUX），通过解耦分割掩膜条件实现语义和形状的一致性。同时，引入语义对齐注意力掩膜和实体轮廓图，分别负责区域语义和高频形状条件。为预防多实体场景中的属性泄漏，采用了属性隔离注意力掩膜机制。Seg2Any在开放集和封闭集的S2I基准测试中均取得最佳性能，尤其擅长实体细粒度空间和属性的控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seg2Any是一个新兴的S2I框架，旨在解决T2I模型在精确空间布局控制方面的挑战。</li>
<li>Seg2Any利用多模态扩散变压器（如FLUX）实现语义和形状的一致性。</li>
<li>语义对齐注意力掩膜和实体轮廓图被引入以处理区域语义和高频形状条件。</li>
<li>属性隔离注意力掩膜机制防止了多实体场景中的属性泄漏。</li>
<li>Seg2Any构建了SACap-1M数据集，用于支持开放集S2I生成，并提供了SACap-Eval基准测试。</li>
<li>实验表明，Seg2Any在S2I基准测试中表现最佳，特别是在实体细粒度空间和属性的控制方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.00596v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.00596v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.00596v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2506.00596v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TMT-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation"><a href="#TMT-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation" class="headerlink" title="TMT: Cross-domain Semantic Segmentation with Region-adaptive   Transferability Estimation"></a>TMT: Cross-domain Semantic Segmentation with Region-adaptive   Transferability Estimation</h2><p><strong>Authors:Enming Zhang, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Guan Wang, Yang Li, Xiaoping Zhang</strong></p>
<p>Recent advances in Vision Transformers (ViTs) have significantly advanced semantic segmentation performance. However, their adaptation to new target domains remains challenged by distribution shifts, which often disrupt global attention mechanisms. While existing global and patch-level adaptation methods offer some improvements, they overlook the spatially varying transferability inherent in different image regions. To address this, we propose the Transferable Mask Transformer (TMT), a region-adaptive framework designed to enhance cross-domain representation learning through transferability guidance. First, we dynamically partition the image into coherent regions, grouped by structural and semantic similarity, and estimates their domain transferability at a localized level. Then, we incorporate region-level transferability maps directly into the self-attention mechanism of ViTs, allowing the model to adaptively focus attention on areas with lower transferability and higher semantic uncertainty. Extensive experiments across 20 diverse cross-domain settings demonstrate that TMT not only mitigates the performance degradation typically associated with domain shift but also consistently outperforms existing approaches. </p>
<blockquote>
<p>近期在视觉Transformer（ViTs）方面的进展已经显著提高了语义分割的性能。然而，它们在适应新目标域时仍然面临分布转移的难题，这通常会破坏全局注意力机制。虽然现有的全局和补丁级自适应方法提供了一些改进，但它们忽略了不同图像区域固有的空间可转移性。为了解决这一问题，我们提出了可转移掩膜Transformer（TMT），这是一种区域自适应框架，旨在通过可转移性指导增强跨域表示学习。首先，我们根据结构性和语义相似性将图像动态划分为连贯的区域，并对这些区域进行分组，然后在局部级别估计它们的域可转移性。接着，我们将区域级可转移性图直接融入ViT的自注意力机制中，使模型能够自适应地关注可转移性较低、语义不确定性较高的区域。在20个不同的跨域设置上进行的广泛实验表明，TMT不仅减轻了与域偏移相关的性能下降问题，而且始终优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05774v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ViT的最新进展极大地提高了语义分割性能，但在目标域适应方面仍面临分布转移的挑战，这可能会破坏全局注意力机制。现有全局和补丁级适应方法虽有所提升，但忽略了不同图像区域固有的空间可转移性。为解决此问题，我们提出了可转移掩膜转换器（TMT），这是一种区域自适应框架，旨在通过可转移性指导增强跨域表示学习。首先，我们根据结构和语义相似性将图像动态划分为连贯区域，并估算其局部域可转移性。然后，我们将区域级可转移性图直接融入ViT的自注意力机制中，使模型能够自适应地关注低可转移性和高语义不确定性的区域。在跨越二十多种跨域设置的大量实验中表明，TMT不仅减轻了与域偏移相关的性能下降问题，而且始终优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) 在语义分割性能上取得了显著进展。</li>
<li>分布转移在ViT的目标域适应中仍然是一个挑战，可能影响全局注意力机制。</li>
<li>现有适应方法虽有所提升，但忽略了不同图像区域的空间可转移性。</li>
<li>提出了可转移掩膜转换器（TMT）框架，通过区域自适应增强跨域表示学习。</li>
<li>TMT通过动态划分图像区域并估算其局部域可转移性来解决现有问题。</li>
<li>TMT将区域级可转移性图融入ViT的自注意力机制中，使模型能自适应关注关键区域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2504.05774v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2504.05774v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2504.05774v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2504.05774v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2504.05774v3/page_3_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CQ-DINO-Mitigating-Gradient-Dilution-via-Category-Queries-for-Vast-Vocabulary-Object-Detection"><a href="#CQ-DINO-Mitigating-Gradient-Dilution-via-Category-Queries-for-Vast-Vocabulary-Object-Detection" class="headerlink" title="CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast   Vocabulary Object Detection"></a>CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast   Vocabulary Object Detection</h2><p><strong>Authors:Zhichao Sun, Huazhang Hu, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</strong></p>
<p>With the exponential growth of data, traditional object detection methods are increasingly struggling to handle vast vocabulary object detection tasks effectively. We analyze two key limitations of classification-based detectors: positive gradient dilution, where rare positive categories receive insufficient learning signals, and hard negative gradient dilution, where discriminative gradients are overwhelmed by numerous easy negatives. To address these challenges, we propose CQ-DINO, a category query-based object detection framework that reformulates classification as a contrastive task between object queries and learnable category queries. Our method introduces image-guided query selection, which reduces the negative space by adaptively retrieving top-K relevant categories per image via cross-attention, thereby rebalancing gradient distributions and facilitating implicit hard example mining. Furthermore, CQ-DINO flexibly integrates explicit hierarchical category relationships in structured datasets (e.g., V3Det) or learns implicit category correlations via self-attention in generic datasets (e.g., COCO). Experiments demonstrate that CQ-DINO achieves superior performance on the challenging V3Det benchmark (surpassing previous methods by 2.1% AP) while maintaining competitiveness in COCO. Our work provides a scalable solution for real-world detection systems requiring wide category coverage. The code is publicly at <a target="_blank" rel="noopener" href="https://github.com/FireRedTeam/CQ-DINO">https://github.com/FireRedTeam/CQ-DINO</a>. </p>
<blockquote>
<p>随着数据的指数级增长，传统的目标检测方法越来越难以有效地处理大规模词汇表的目标检测任务。我们分析了基于分类的检测器的两个关键局限性：正向梯度稀释，即罕见的正向类别接收到的学习信号不足；以及难以应对的负梯度稀释，即区分梯度被大量简单的负样本所淹没。为了应对这些挑战，我们提出了基于类别查询的目标检测框架CQ-DINO，它将分类重新制定为目标查询和可学习类别查询之间的对比任务。我们的方法引入了图像引导查询选择，通过跨注意力自适应地检索每张图像的前K个相关类别，从而减少负空间，从而重新平衡梯度分布并促进隐式硬样本挖掘。此外，CQ-DINO可以灵活地集成结构化数据集（例如V3Det）中的显式层次类别关系，或通过通用数据集（例如COCO）中的自注意力学习隐式类别关联。实验表明，CQ-DINO在具有挑战性的V3Det基准测试上取得了优于其他方法（超出2.1%的AP）的性能，同时在COCO中也保持竞争力。我们的工作提供了一种可扩展的解决方案，适用于需要广泛类别覆盖的真实世界检测系统。代码公开在<a target="_blank" rel="noopener" href="https://github.com/FireRedTeam/CQ-DINO%E3%80%82">https://github.com/FireRedTeam/CQ-DINO。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18430v4">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>随着数据量的指数级增长，传统目标检测方法在应对大规模词汇目标检测任务时面临挑战。本文分析了基于分类的检测器的两个主要局限性：正梯度稀释和硬负梯度稀释。为此，我们提出了CQ-DINO，一种基于类别查询的目标检测框架，将分类重新构建为对象查询和可学习类别查询之间的对比任务。通过图像引导查询选择，我们的方法减少了负空间，通过跨注意力自适应地检索每张图像的前K个相关类别，从而平衡梯度分布并促进隐式硬样本挖掘。此外，CQ-DINO灵活整合了结构化数据集（如V3Det）中的显式层次类别关系，或在通用数据集（如COCO）中学习通过自注意力实现的隐式类别关联。实验表明，CQ-DINO在具有挑战性的V3Det基准测试中取得了优于其他方法（提高2.1%的AP）的性能，同时在COCO中保持竞争力。我们的研究为需要广泛类别覆盖的真实世界检测系统提供了可扩展的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统目标检测方法在应对大规模数据挑战时存在困难。</li>
<li>分析了基于分类的检测器的两个主要局限性：正梯度稀释和硬负梯度稀释。</li>
<li>提出了CQ-DINO框架，将分类转化为对比任务，提高目标检测的准确性。</li>
<li>通过图像引导查询选择减少负空间，平衡梯度分布，促进隐式硬样本挖掘。</li>
<li>CQ-DINO能够灵活适应不同的数据集，整合层次类别关系或学习隐式类别关联。</li>
<li>在V3Det基准测试中，CQ-DINO性能优越，较之前的方法提高了2.1%的AP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.18430v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.18430v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.18430v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.18430v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.18430v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation"><a href="#OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation" class="headerlink" title="OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation"></a>OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation</h2><p><strong>Authors:Ding Zhong, Xu Zheng, Chenfei Liao, Yuanhuiyi Lyu, Jialei Chen, Shengyang Wu, Linfeng Zhang, Xuming Hu</strong></p>
<p>Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to $360^\circ$ domain, the significant field-of-view (FoV) gap between pinhole ($70^\circ \times 70^\circ$) and panoramic images ($180^\circ \times 360^\circ$) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2’s memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that OmniSAM outperforms the state-of-the-art methods by large margins, e.g., 79.06% (+10.22%) on SPin8-to-SPan8, 62.46% (+6.58%) on CS13-to-DP13. </p>
<blockquote>
<p>Segment Anything Model 2（SAM2）在各种针孔成像分割任务中表现出强大的基础模型能力。然而，当将其应用于$360^\circ$领域时，针孔（$70^\circ \times 70^\circ$）与全景图像（$180^\circ \times 360^\circ$）之间的视野（FoV）差距巨大，带来了独特的挑战。该应用的主要有两个担忧点：1）由领域间大视野差异带来的不可避免的形状扭曲和物体变形；2）原始SAM2无法提供像素级别的语义理解。为了解决这些问题，我们提出了全新的OmniSAM框架，它首次尝试将SAM2应用于全景语义分割。具体来说，为了弥补第一个差距，OmniSAM首先把全景图像分成一系列的补丁（patches），然后将这些补丁视为类似视频分割任务中的图像序列。然后，我们利用SAM2的记忆机制来提取跨补丁对应关系，这些对应关系嵌入跨视野的依赖关系，提高了特征连续性和掩膜边界的预测一致性。对于第二个差距，OmniSAM微调了预训练图像编码器并重新使用掩膜解码器进行语义预测。还引入了一个基于视野的原型适应模块，并带有动态伪标签更新机制，以促进记忆和主干特征的对齐，从而提高模型在不同尺寸源模型上的泛化能力。大量的实验结果证明，OmniSAM大幅超越了现有方法，例如在SPin8-to-SPan8上达到79.06%（+10.22%），在CS13-to-DP13上达到62.46%（+6.58%）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07098v3">PDF</a> </p>
<p><strong>Summary</strong>：针对全景语义分割面临的挑战，OmniSAM框架通过应用SAM2模型提出了一系列创新解决方案。OmniSAM通过划分全景图像为多个补丁序列，并利用SAM2的记忆机制提取跨补丁对应关系，以解决不同视场角之间的差异性问题和语义理解的缺失。经过微调图像编码器和重新利用遮罩解码器进行语义预测，OmniSAM显著提高了模型在不同源模型大小上的泛化能力。实验结果表明，OmniSAM在SPin8-to-SPan8和CS13-to-DP13任务上的性能远超现有方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>OmniSAM框架首次将SAM2模型应用于全景语义分割，解决在$360^\circ$领域中应用时面临的挑战。</li>
<li>OmniSAM通过划分全景图像为补丁序列，利用SAM2的记忆机制提取跨补丁对应关系，以弥补视场角差异和语义理解的不足。</li>
<li>OmniSAM微调了预训练的图像编码器并重新利用遮罩解码器进行语义预测，提高了模型的泛化能力。</li>
<li>引入了一个基于视场的原型适应模块，带有动态伪标签更新机制，促进记忆和主干特征的对齐。</li>
<li>实验结果表明，OmniSAM在SPin8-to-SPan8和CS13-to-DP13任务上的性能远超现有方法，显示出其强大的实际应用潜力。</li>
<li>OmniSAM框架在桥接不同视场角差异和增强语义理解方面的创新策略为全景成像分割任务提供了新的研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07098">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.07098v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.07098v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.07098v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.07098v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.07098v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2503.07098v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models"><a href="#SynDiff-AD-Improving-Semantic-Segmentation-and-End-to-End-Autonomous-Driving-with-Synthetic-Data-from-Latent-Diffusion-Models" class="headerlink" title="SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models"></a>SynDiff-AD: Improving Semantic Segmentation and End-to-End Autonomous   Driving with Synthetic Data from Latent Diffusion Models</h2><p><strong>Authors:Harsh Goel, Sai Shankar Narasimhan, Oguzhan Akcin, Sandeep Chinchali</strong></p>
<p>In recent years, significant progress has been made in collecting large-scale datasets to improve segmentation and autonomous driving models. These large-scale datasets are often dominated by common environmental conditions such as “Clear and Day” weather, leading to decreased performance in under-represented conditions like “Rainy and Night”. To address this issue, we introduce SynDiff-AD, a novel data augmentation pipeline that leverages diffusion models (DMs) to generate realistic images for such subgroups. SynDiff-AD uses ControlNet-a DM that guides data generation conditioned on semantic maps-along with a novel prompting scheme that generates subgroup-specific, semantically dense prompts. By augmenting datasets with SynDiff-AD, we improve the performance of segmentation models like Mask2Former and SegFormer by up to 1.2% and 2.3% on the Waymo dataset, and up to 1.4% and 0.7% on the DeepDrive dataset, respectively. Additionally, we demonstrate that our SynDiff-AD pipeline enhances the driving performance of end-to-end autonomous driving models, like AIM-2D and AIM-BEV, by up to 20% across diverse environmental conditions in the CARLA autonomous driving simulator, providing a more robust model. We release our code and pipeline at <a target="_blank" rel="noopener" href="https://github.com/UTAustin-SwarmLab/SynDiff-AD">https://github.com/UTAustin-SwarmLab/SynDiff-AD</a>. </p>
<blockquote>
<p>近年来，在收集大规模数据集以改进分割和自动驾驶模型方面取得了显著进展。这些大规模数据集通常以“晴朗和白天”等常见环境条件为主，导致在代表性不足的条件（如“雨天夜间”）下性能下降。为了解决这一问题，我们引入了SynDiff-AD，这是一种新的数据增强管道，它利用扩散模型（DM）生成此类子组的现实图像。SynDiff-AD使用ControlNet（一种基于语义地图引导数据生成的DM）以及一种新型提示方案，该方案生成子组特定的语义密集提示。通过SynDiff-AD增强数据集，我们在Waymo数据集上提高了Mask2Former和SegFormer等分割模型的性能，分别高达1.2%和2.3%；在DeepDrive数据集上分别提高了1.4%和0.7%。此外，我们还证明我们的SynDiff-AD管道在CARLA自动驾驶模拟器中的多种环境条件下，能够提高端到端自动驾驶模型（如AIM-2D和AIM-BEV）的驾驶性能，最高提高20%，为更稳健的模型提供支撑。我们在<a target="_blank" rel="noopener" href="https://github.com/UTAustin-SwarmLab/SynDiff-AD">https://github.com/UTAustin-SwarmLab/SynDiff-AD</a>发布我们的代码和管道。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16776v2">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SynDiff-AD数据增强管道的研究，该管道利用扩散模型生成针对特定子组的真实图像，解决了大规模数据集在复杂环境条件下的性能下降问题。通过SynDiff-AD增强数据集，改善了分割模型的性能，提高了端到端自动驾驶模型在CARLA自动驾驶模拟器中的驾驶性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynDiff-AD是一种新的数据增强管道，旨在解决大规模数据集中特定环境条件下的性能下降问题。</li>
<li>该方法利用扩散模型（DMs）生成真实图像，特别针对那些被忽视的、特定的环境条件如“雨天夜晚”。</li>
<li>通过SynDiff-AD增强数据集，提高了分割模型的性能，如Mask2Former和SegFormer在Waymo和DeepDrive数据集上的表现。</li>
<li>SynDiff-AD管道不仅提高了分割模型的性能，还增强了端到端自动驾驶模型在模拟环境中的驾驶性能。</li>
<li>该研究在CARLA自动驾驶模拟器上测试了SynDiff-AD的效能，结果显示性能提升显著。</li>
<li>研究人员发布了SynDiff-AD的代码和管道在<a target="_blank" rel="noopener" href="https://github.com/UTAustin-SwarmLab/SynDiff-AD%E3%80%82">https://github.com/UTAustin-SwarmLab/SynDiff-AD。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16776">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2411.16776v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2411.16776v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2411.16776v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2411.16776v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2411.16776v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-RGB-D-Semantic-Segmentation-with-Deformable-Attention-Transformer"><a href="#Diffusion-based-RGB-D-Semantic-Segmentation-with-Deformable-Attention-Transformer" class="headerlink" title="Diffusion-based RGB-D Semantic Segmentation with Deformable Attention   Transformer"></a>Diffusion-based RGB-D Semantic Segmentation with Deformable Attention   Transformer</h2><p><strong>Authors:Minh Bui, Kostas Alexis</strong></p>
<p>Vision-based perception and reasoning is essential for scene understanding in any autonomous system. RGB and depth images are commonly used to capture both the semantic and geometric features of the environment. Developing methods to reliably interpret this data is critical for real-world applications, where noisy measurements are often unavoidable. In this work, we introduce a diffusion-based framework to address the RGB-D semantic segmentation problem. Additionally, we demonstrate that utilizing a Deformable Attention Transformer as the encoder to extract features from depth images effectively captures the characteristics of invalid regions in depth measurements. Our generative framework shows a greater capacity to model the underlying distribution of RGB-D images, achieving robust performance in challenging scenarios with significantly less training time compared to discriminative methods. Experimental results indicate that our approach achieves State-of-the-Art performance on both the NYUv2 and SUN-RGBD datasets in general and especially in the most challenging of their image data. Our project page will be available at <a target="_blank" rel="noopener" href="https://diffusionmms.github.io/">https://diffusionmms.github.io/</a> </p>
<blockquote>
<p>基于视觉的感知和推理对于任何自主系统的场景理解都是至关重要的。RGB和深度图像通常用于捕捉环境的语义和几何特征。开发能够可靠解释这些数据的方法对于实际应用至关重要，在真实世界中，存在噪声的测量往往不可避免。在这项工作中，我们引入了一个基于扩散的框架来解决RGB-D语义分割问题。此外，我们证明，利用可变形注意力变压器作为编码器从深度图像中提取特征，可以有效地捕捉深度测量中无效区域的特性。我们的生成框架表现出更大的能力来模拟RGB-D图像的基础分布，在具有挑战性的场景中实现了稳健的性能，与判别方法相比，训练时间大大缩短。实验结果表明，我们的方法在NYUv2和SUN-RGBD数据集上均达到了最新技术水平，尤其是在最具挑战性的图像数据上表现尤为出色。我们的项目页面可在<a target="_blank" rel="noopener" href="https://diffusionmms.github.io/">https://diffusionmms.github.io/上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15117v3">PDF</a> </p>
<p><strong>Summary</strong><br>本文引入基于扩散的框架解决RGB-D语义分割问题，并利用可变形注意力转换器作为编码器提取深度图像特征，有效捕捉深度测量中无效区域的特性。实验结果表明，该方法在NYUv2和SUN-RGBD数据集上实现了最新性能，尤其在具有挑战性的图像数据上表现更出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了基于扩散的框架来解决RGB-D语义分割问题的重要性。</li>
<li>可变形注意力转换器能有效提取深度图像特征，并捕捉深度测量中无效区域的特性。</li>
<li>与判别式方法相比，所提生成式框架具有更强的建模RGB-D图像基础分布的能力，并且在训练时间上更具优势。</li>
<li>实验结果证明该方法在主流数据集NYUv2和SUN-RGBD上达到了最先进的性能。</li>
<li>挑战性场景下该方法的稳健性能表现。</li>
<li>该方法对于自主系统中场景理解的重要性，特别是在处理带有噪声的测量数据时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15117">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2409.15117v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2409.15117v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2409.15117v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2409.15117v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_检测_分割_跟踪/2409.15117v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-10-18\./crop_人脸相关/2510.10663v1/page_0_0.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-10-18  Scalable Face Security Vision Foundation Model for Deepfake, Diffusion,   and Spoofing Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-85b3126f3e4d8c14cccbeb1047147760~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738137&auth_key=1760738137-0-0-73b410b64a50a459b0e2ccbcfac14c92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-10-18  Towards Generalist Intelligence in Dentistry Vision Foundation Models   for Oral and Maxillofacial Radiology
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
