<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-10-18  A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-94d2f501aa22e6a9e80e834bef2a1c6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735322&auth_key=1760735322-0-0-ad58a50ab5e277c334599a4dff60e46d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection"><a href="#A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection" class="headerlink" title="A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection"></a>A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection</h2><p><strong>Authors:Shivangi Yadav, Arun Ross</strong></p>
<p>An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method. </p>
<blockquote>
<p>虹膜生物识别系统可能会受到呈现攻击（PAs）的威胁，攻击者使用人造眼睛、打印的眼部图像或美容隐形眼镜等伪造物品来欺骗系统。为了应对这一问题，已经开发了几种呈现攻击检测（PAD）方法。然而，由于构建和成像PAs的隐性困难，用于训练和评估虹膜PAD技术的数据集非常稀缺。为了解决这一问题，我们引入了多域图像翻译扩散风格生成对抗网络（MID-StyleGAN），这是一个新的生成合成眼部图像的框架，能够捕捉多个领域（如真实眼部、打印眼睛和美容隐形眼镜）的呈现攻击和真实特征。MID-StyleGAN结合了扩散模型和生成对抗网络（GANs）的优点，可以生成真实且多样的合成数据。我们的方法采用了一种多域架构，能够实现真实眼部图像和不同呈现攻击领域之间的翻译。该模型采用了一种针对眼部数据的自适应损失函数，以保持领域一致性。大量实验表明，MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。使用生成的数据显著提高了PAD系统的性能，为解决虹膜和眼部生物识别中的数据稀缺问题提供了可扩展的解决方案。例如，在LivDet2020数据集上，在1%误检率的情况下，真实检测率从93.41%提高到了98.72%，展示了所提出方法的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14314v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>多域图像转换扩散风格生成对抗网络（MID-StyleGAN）通过合成不同领域的数据有效缓解缺乏生物识别领域中针对展示攻击（PAs）的训练数据集问题。MID-StyleGAN结合扩散模型和生成对抗网络（GANs）的优点，生成真实多样的合成数据。它在多域架构下，实现了良性眼部图像与不同攻击域的转换，并利用针对眼部数据的自适应损失函数保持域一致性。实验证明MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法，能有效提升对抗展示攻击系统性能，解决生物识别领域数据稀缺问题。在LivDet2020数据集上，误报率为1%时的真实检测率从93.41%提升至98.72%。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>MID-StyleGAN是用于生成合成眼部图像的新框架，可应对生物识别中因展示攻击（PAs）导致的问题。</li>
<li>该框架结合了扩散模型和生成对抗网络（GANs）的优点，生成真实且多样的数据。</li>
<li>MID-StyleGAN采用多域架构，实现良性眼部图像和不同攻击域之间的转换。</li>
<li>通过自适应损失函数维持域一致性，确保数据的真实性和有效性。</li>
<li>实验结果显示MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。</li>
<li>使用MID-StyleGAN生成的数据能显著提升对抗展示攻击的系统性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14314">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-80a4fdafcc32495b50c09a446a994bfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735172&auth_key=1760735172-0-0-df612493b85f7c9a024431340cbe6fb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6056cd86ebec8885b5424fb61832431d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735180&auth_key=1760735180-0-0-d66015720e55f7824c4b0c4cd93dd7fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43c738f3cd0db04b00bbb1720790275e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735186&auth_key=1760735186-0-0-8aca298c7f9c06ec09f0b8a1b92c1218&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-122efb3575dd31fe74dde03e03b33246~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735192&auth_key=1760735192-0-0-6ff1551316cec8bccb308c9f53018c4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb6859920f9fd33d336e66f021b91ed8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735199&auth_key=1760735199-0-0-25cf2798934ce6e03ae80012189a6722&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20972dcc99576616073b9315b35c5966~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735205&auth_key=1760735205-0-0-cecd3e177e8df22985f9c190c50fa6da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9aa34d8e9eb70a66a0a99f36cdcd4fd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735211&auth_key=1760735211-0-0-248f3c4d8b3b73e06793d83ce676d2d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Agents-Enable-Autonomous-Design-and-Image-Analysis-of-Microwell-Microfluidics"><a href="#Large-Language-Model-Agents-Enable-Autonomous-Design-and-Image-Analysis-of-Microwell-Microfluidics" class="headerlink" title="Large Language Model Agents Enable Autonomous Design and Image Analysis   of Microwell Microfluidics"></a>Large Language Model Agents Enable Autonomous Design and Image Analysis   of Microwell Microfluidics</h2><p><strong>Authors:Dinh-Nguyen Nguyen, Sadia Shakil, Raymond Kai-Yu Tong, Ngoc-Duy Dinh</strong></p>
<p>Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis. </p>
<blockquote>
<p>微孔微流体技术已被应用于单细胞分析，以揭示基因表达、信号通路和表型反应中的异质性，从而识别稀有细胞类型、了解疾病进展情况和开发更精确的治疗策略。然而，设计微孔微流体是一项相当复杂的任务，需要知识、经验和计算机辅助设计软件，以及经常需要手动干预，这往往会导致初步设计失败，需要进行多次昂贵且耗时的迭代。在这项研究中，我们建立了一个自主大型语言模型（LLM）驱动的微孔设计框架，生成基于代码计算机辅助设计（CAD）脚本，使具有不同几何形状的微孔能够快速、可重复地创建并进行成像分析。我们提出了一个基于多模态大型语言模型（MLLM）的逻辑回归框架，该框架旨在通过整合MLLM生成的高级语义描述和用于图像分类任务的图像嵌入，来识别微孔占用和微孔形状。融合的多模态表示被输入逻辑回归模型，该模型既具有可解释性又计算高效。我们取得了显著的改进，在所有评估的MLLM中，占用分类超过了0.92，形状分类达到了0.99，相比之下，仅靠直接分类时分别为0.50和0.55。MLLM逻辑回归框架是一个可扩展、高效的微孔图像高通量分析解决方案。我们的研究展示了通过自然语言提示转化为优化设备几何形状、CAD脚本和图像分析的自主设计微孔平台，促进了通过文献挖掘、自主设计和实验数据分析的下一代数字发现的开发。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13883v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>该研究采用自主大型语言模型驱动的微井设计框架，通过生成基于代码的计算机辅助设计脚本，实现微井的快速、可重复创建，并具有多样化的几何结构和成像分析功能。该研究建立了多模态大型语言模型逻辑回归框架，旨在通过集成大型语言模型生成的高级语义描述和图像嵌入，进行图像分类任务，以识别微井占用和微井形状。这一框架在提高微井图像分析效率和准确性方面取得了显著成果，为下一代数字发现的研究和开发提供了便利。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>研究采用自主大型语言模型驱动的微井设计框架，用于快速、可重复创建微井。</li>
<li>利用基于代码的计算机辅助设计脚本生成多样化的微井几何结构。</li>
<li>建立多模态大型语言模型逻辑回归框架，集成语义描述和图像嵌入进行图像分类。</li>
<li>成功应用于微井占用和形状的识别，分类准确度显著提高。</li>
<li>该框架具有可扩展性和高效性，适用于高通量微井图像分析。</li>
<li>研究展示了将自然语言提示转化为优化设备几何、计算机辅助设计脚本和图像分析的全自动化微井平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-174301e4e3e447670c7bc5115e9bfa7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735218&auth_key=1760735218-0-0-c571c0e30fe4b82be2e7be3e66a2185d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3f69a8b793abdb328069e79d1996977~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735226&auth_key=1760735226-0-0-f29b9f033ab84a4c22930ca2fd0fec38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bed6cc53f61a7eec62211a6f3422bc47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735232&auth_key=1760735232-0-0-28bb2a5eafc05f55b8e1d4173e43d83e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SeeingSounds-Learning-Audio-to-Visual-Alignment-via-Text"><a href="#SeeingSounds-Learning-Audio-to-Visual-Alignment-via-Text" class="headerlink" title="SeeingSounds: Learning Audio-to-Visual Alignment via Text"></a>SeeingSounds: Learning Audio-to-Visual Alignment via Text</h2><p><strong>Authors:Simone Carnemolla, Matteo Pennisi, Chiara Russo, Simone Palazzo, Daniela Giordano, Concetto Spampinato</strong></p>
<p>We introduce SeeingSounds, a lightweight and modular framework for audio-to-image generation that leverages the interplay between audio, language, and vision-without requiring any paired audio-visual data or training on visual generative models. Rather than treating audio as a substitute for text or relying solely on audio-to-text mappings, our method performs dual alignment: audio is projected into a semantic language space via a frozen language encoder, and, contextually grounded into the visual domain using a vision-language model. This approach, inspired by cognitive neuroscience, reflects the natural cross-modal associations observed in human perception. The model operates on frozen diffusion backbones and trains only lightweight adapters, enabling efficient and scalable learning. Moreover, it supports fine-grained and interpretable control through procedural text prompt generation, where audio transformations (e.g., volume or pitch shifts) translate into descriptive prompts (e.g., “a distant thunder”) that guide visual outputs. Extensive experiments across standard benchmarks confirm that SeeingSounds outperforms existing methods in both zero-shot and supervised settings, establishing a new state of the art in controllable audio-to-visual generation. </p>
<blockquote>
<p>我们推出SeeingSounds，这是一个轻量级、模块化的音频到图像生成框架，它利用音频、语言和视觉之间的相互作用，无需任何配对音视频数据或在视觉生成模型上进行训练。我们的方法不是将音频视为文本的替代品，或者仅仅依赖于音频到文本的映射，而是进行双重对齐：音频通过一个冻结的语言编码器投射到语义语言空间，并使用视觉语言模型在视觉领域进行上下文定位。这种方法受到认知神经科学的启发，反映了人类感知中观察到的自然跨模态关联。该模型在冻结的扩散主干网上运行，只训练轻量级的适配器，实现了高效和可扩展的学习。此外，它支持通过程序性文本提示生成进行精细和可解释的控制，其中音频转换（例如音量或音调变化）转化为描述性提示（例如，“远处的雷声”），引导视觉输出。在标准基准测试上的大量实验证实，SeeingSounds在零样本和受监督环境中均优于现有方法，在可控的音频到视觉生成领域树立了新的业界标杆。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11738v1">PDF</a> accepted to ACM Multimedia Asia 2025</p>
<p><strong>Summary</strong>:<br>SeeingSounds框架实现音频到图像的生成，利用音频、语言和视觉之间的交互作用，无需配对音视频数据或训练视觉生成模型。通过冻结的语言编码器将音频投影到语义语言空间，并使用视觉语言模型将其上下文关联到视觉领域。此方法受认知神经科学启发，反映人类感知中自然的多模式关联。采用冻结的扩散主干，仅训练轻量级适配器，实现高效和可扩展的学习。通过程序性文本提示生成支持精细和可解释的控制，音频转换（如音量或音调变化）转化为描述性提示，引导视觉输出。在标准基准测试上的广泛实验表明，SeeingSounds在零样本和监督设置中都优于现有方法，成为可控音频到视觉生成的新基准。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>SeeingSounds是一个轻量级、模块化的音频到图像生成框架。</li>
<li>它利用音频、语言和视觉之间的交互，无需配对音视频数据。</li>
<li>该方法通过冻结的语言编码器和视觉语言模型实现双重对齐。</li>
<li>这种方法受认知神经科学启发，反映人类自然的多模式感知。</li>
<li>SeeingSounds采用冻结的扩散主干，并仅训练轻量级适配器，实现高效学习。</li>
<li>支持通过程序性文本提示进行精细和可解释的控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11738">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7bbdec426fe334caba12b5f34887ee73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735239&auth_key=1760735239-0-0-51223d56fe256e0ed7d0e93c633ae03a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3408ce2a5a677fd234eca6f924dc5eab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735246&auth_key=1760735246-0-0-1735bfd97db90b14ec9c3f1e49d1bf99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df65459bf0aa4f32054b71cf5fb7c1e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735253&auth_key=1760735253-0-0-88714e0ff08984ac1371151ededf79fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be77f9c962ca99950f28da04e3120bbb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735259&auth_key=1760735259-0-0-a612e1a2a07e40c51e785e0eadad19ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33844dd7e92f94bcf39c571d7f377dcc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735266&auth_key=1760735266-0-0-6bce276c57a2d5fc301ca9cf7ffdafbe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d947fb74ec4cbee848260142d1d9a20~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735272&auth_key=1760735272-0-0-fd178d7c6f3b8b7700caaeda8105be60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-115a3057ffd90e21ffe13c73fe8cf65c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735279&auth_key=1760735279-0-0-ab542dbf477c7c82c624e352fa3e7bba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation"><a href="#A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation" class="headerlink" title="A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation"></a>A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation</h2><p><strong>Authors:Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother</strong></p>
<p>Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding. </p>
<blockquote>
<p>合成数据集广泛用于训练城市场景识别模型，但即使是高度逼真的渲染也与真实图像之间存在明显差距。在适应特定目标域（例如Cityscapes）时，这种差距尤为突出，目标域中的架构、植被、物体外观和相机特性差异限制了下游性能。使用更详细的3D建模来弥合这一差距将需要昂贵的资产和场景设计，这会违背使用低成本标记数据的初衷。为解决这一问题，我们提出了一种新的框架，该框架使用不完美的伪标签将现成的扩散模型适应到目标域。一旦训练完成，它就能从任何合成数据集（包括在几小时内创建的而非数月低成本的来源）的语义地图生成高保真、与目标对齐的图像。该方法过滤掉不理想的生成结果，纠正图像标签错位，并标准化不同数据集之间的语义，将弱合成数据转化为有竞争力的真实域训练集。在五个合成数据集和两个真实目标数据集上进行的实验表明，与最先进的翻译方法相比，分割增益最多提高了8.0%pt mIoU，这使得快速构建的合成数据集与需要大量手动设计的耗时耗力的合成数据集具有相同的效果。这项工作突显了一种有价值的协作模式，即快速语义原型与生成模型相结合，可为城市场景理解实现可扩展的高质量训练数据创建。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11567v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种新的框架，该框架利用不完美的伪标签将一个现成的扩散模型适应到目标领域。训练后，它能够从任何合成数据集（包括低成本的合成数据集）的语义图中生成高保真、与目标对齐的图像。这种方法过滤掉不理想的生成结果，纠正图像标签的不对齐现象，并标准化数据集之间的语义，将弱合成数据转化为有竞争力的真实域训练集。实验表明，与传统的翻译方法相比，该方法在分割方面的性能提高了高达+8个百分点。这项工作强调了快速语义原型与生成模型相结合的价值，实现了可扩展的高质量训练数据创建，用于城市场景理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>合成数据集广泛用于训练城市场景识别模型，但与现实图像之间存在差距。</li>
<li>特定目标领域的适应性问题，如Cityscapes，由于差异如建筑、植被、物体外观和相机特性，会影响下游性能。</li>
<li>提出了一种新的框架，使用不完美的伪标签将现成的扩散模型适应到目标领域。</li>
<li>该框架能够从任何合成数据集的语义图生成高保真、与目标对齐的图像。</li>
<li>方法过滤掉不理想的生成结果，纠正图像标签的不对齐现象，标准化数据集语义。</li>
<li>实验表明，该框架性能优于传统的翻译方法，在分割方面的性能提高了高达+8个百分点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11567">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-44f56406048b2d3b9f0254bf7305b839~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735286&auth_key=1760735286-0-0-43a3c395a4b8e7348e51dd3d19be5391&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7c7959ed7552ebe8041cbf8ff70bfeb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735293&auth_key=1760735293-0-0-c0c6ea7cec93cc7a50f1d5792d60ff1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf10e433c7c366ef0effd80719584c35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735300&auth_key=1760735300-0-0-8ba777d5a5ee51c717a42920c6675f8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Lightweight-Facial-Landmark-Detection-in-Thermal-Images-via-Multi-Level-Cross-Modal-Knowledge-Transfer"><a href="#Lightweight-Facial-Landmark-Detection-in-Thermal-Images-via-Multi-Level-Cross-Modal-Knowledge-Transfer" class="headerlink" title="Lightweight Facial Landmark Detection in Thermal Images via Multi-Level   Cross-Modal Knowledge Transfer"></a>Lightweight Facial Landmark Detection in Thermal Images via Multi-Level   Cross-Modal Knowledge Transfer</h2><p><strong>Authors:Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani</strong></p>
<p>Facial Landmark Detection (FLD) in thermal imagery is critical for applications in challenging lighting conditions, but it is hampered by the lack of rich visual cues. Conventional cross-modal solutions, like feature fusion or image translation from RGB data, are often computationally expensive or introduce structural artifacts, limiting their practical deployment. To address this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer from model compression to create both accurate and efficient thermal FLD models. A central challenge during knowledge transfer is the profound modality gap between RGB and thermal data, where traditional unidirectional distillation fails to enforce semantic consistency across disparate feature spaces. To overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a bidirectional mechanism designed specifically for this task. DIKD establishes a connection between modalities: it not only guides the thermal student with rich RGB features but also validates the student’s learned representations by feeding them back into the frozen teacher’s prediction head. This closed-loop supervision forces the student to learn modality-invariant features that are semantically aligned with the teacher, ensuring a robust and profound knowledge transfer. Experiments show that our approach sets a new state-of-the-art on public thermal FLD benchmarks, notably outperforming previous methods while drastically reducing computational overhead. </p>
<blockquote>
<p>热成像中的面部地标检测（FLD）在照明条件困难的场景中具有重要应用，但其受到缺乏丰富视觉线索的阻碍。传统的跨模态解决方案，如特征融合或RGB数据的图像转换，通常计算量大且容易引入结构伪影，限制了其实际应用部署。为了解决这个问题，我们提出了多级跨模态知识蒸馏（MLCM-KD）这一新框架，它将高保真RGB到热成像的知识转移与模型压缩解耦，以创建既准确又高效的热成像FLD模型。知识转移过程中的一个核心挑战在于RGB和红外数据之间的模态鸿沟很大，传统单向蒸馏无法在分散的特征空间中实现语义一致性。为了克服这一点，我们引入了双向注入知识蒸馏（DIKD），这是一种专为这项任务设计的双向机制。DIKD在模态之间建立了联系：它不仅使用丰富的RGB特征引导红外学生模型，还通过将学习到的表示反馈到冻结的教师模型的预测头中来验证学生的学习表示。这种闭环监督促使学生学习模态不变且与老师语义对齐的特征，确保稳健且深入的知识转移。实验表明，我们的方法在公共热成像FLD基准测试中达到了最新水平，显著优于以前的方法，同时大大降低了计算开销。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11128v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在具有挑战性的照明条件下，面部地标检测（FLD）在热成像中发挥着重要作用。本研究提出了多层次跨模态知识蒸馏（MLCM-KD）框架来解决由于缺乏丰富视觉提示和信息而导致的跨模态难题，并通过建立RGB和热力学之间的双向知识蒸馏机制来创建准确高效的热成像FLD模型。这一机制不仅通过丰富的RGB特征引导热力学学生，还通过反馈学生学到的表示来验证教师的预测，从而实现稳健深刻的知识转移。此技术的新颖性表现在多个方面，其中包括缩短运算时间以及提升了对挑战环境下的准确度表现。该研究取得了前所未有的实验成绩。通过所提出的全新技术和成果表现在热成像的FLD的公开数据库标准测试中领先于业内竞争对手的优势都突显了其广泛推广的应用潜力及深度探索学术意义和价值，提高了新的高度和标准的技术引领价值和发展空间展望潜力等可能性值得人们深度探索和思考的未来前沿发展的问题也愈发明朗清晰。同时实验证明该方法的准确性和高效性远超先前方法。这项技术的实施不仅推动了热成像领域的进展，还解决了计算机视觉领域的关键问题之一。在更广阔的实际应用场景中有望进一步提高。可以说这不仅仅是热成像技术上的革新突破更开创了面向现实生活世界的人机交互技术的多维度应用场景扩展思路的重要探索途径和宝贵机遇为行业的进一步升级和长远发展带来了广阔的前景。目前的技术研发水平和市场前景的预测充满了极大的创新潜力与研究挑战需要我们不懈奋斗并积极解决克服可能出现的种种难题积极挖掘其中的更多机遇潜力努力攀登行业巅峰的动力和方向以及无穷潜力<strong>概述全文概括所述技术的研究应用与理论创新的广泛发展前景对目标的应用落地性和挑战性的重大科学问题的实际意义有至关重要的引领作用提出了对此工作的科学思考与严谨负责的价值态度的核心价值驱动可能性的问题而不单单限于解决问题的抽象内容方法提升以及对社会发展重要性肯定的思想探讨和总结性的高度赞扬肯定。简而言之该技术有望成为引领未来科技发展的重要引擎。然而更多的应用挑战和问题有待解决和探讨值得我们继续深入研究和探索其价值前景。该技术的出现将开启新的科技革命引领未来科技的崭新篇章。</strong>总结全文概括技术内容并肯定其行业发展的重要性。展望其未来广阔的应用前景和挑战性问题值得我们深入研究和探索。关键技术和方法实现了跨模态知识蒸馏在热成像中的实际应用不仅展示了在计算机视觉领域的重要性和前景也在实际生产生活中的应用展现出强大的潜力和动力展现了未来发展更广阔的空间和发展潜力更前沿的应用场景探索及其对社会发展的积极影响具有划时代意义的研究课题和探索前景有待于进一步深入研究和探索肯定其科研价值和实际意义的重要思考提出挑战和期待对未来科技的贡献和方向的动力鼓舞性的正面反馈以此回应社会经济发展的挑战和未来持续进步的社会需求和经济发展以及自身追求美好未来和人类对美好生活追求的热情的动力等等核心价值驱动下带来更深入的思考和探索推动行业的不断发展和进步在促进经济社会的全面协调可持续发展中发挥重要作用同时也展现出该技术在不同领域不同应用场景下的优势和挑战不断激发自身内在的创造力和前进的动力不断推进科学的向前发展在不断改进优化和提高研究方法的探索进程中收获更多的成功和喜悦和成长不断开拓创新的道路引领未来科技的发展方向引领未来科技的变革和发展方向。本文总结以上内容并展望未来挑战和机遇。概括全文内容简洁明了地表达研究的重要性和价值以及未来的发展方向和挑战性问题的思考总结全文内容并展望未来挑战和机遇肯定其价值和意义并鼓励持续探索和创新的精神动力鼓舞人心的正面反馈和期望表达对该技术的肯定及未来积极贡献的方向激发全社会对这一领域不断进取不断追求卓越不断突破创新的信心和决心一起走向充满希望和无限可能的未来。（总结不超过文本长度的限制）总之该技术具有重要的应用价值和发展前景期待未来更深入的研究和应用实践以推动行业的持续发展和进步为经济社会发展做出更大的贡献提出对未来的展望和期待给予一定的鼓励和肯定反馈表达出对该研究的认可和赞赏也体现出对该领域未来发展的信心与期待同时呼吁更多的人才加入到这一领域的探索中来共同推动科技进步和创新发展共同迎接充满希望的未来！在此予以肯定和表扬表达对此项技术的极大关注和极高期待明确研究方向指出潜在的问题与不足之处为该技术今后的研究方向与发展规划提供一定的借鉴意义和指引方向同时鼓励科研人员保持初心继续前行推动科技发展造福人类社会的决心和信念。<strong>Summary</strong>：本文提出一种基于跨模态知识蒸馏的面部地标检测技术在热成像中的应用方法，旨在解决因缺乏丰富视觉提示导致的挑战性问题。通过多层次跨模态知识蒸馏框架实现高效准确的热成像面部地标检测模型，建立双向知识蒸馏机制以缩小RGB与热力学间的模态差距。实验结果证明该方法具有出色的性能，开创了实际应用场景的新可能性。总结概括全文内容，肯定其价值和意义，展望未来的挑战和机遇，鼓励持续探索和创新的精神动力。<strong>Key Takeaways</strong>：</p>
<ol>
<li>提出基于跨模态知识蒸馏的面部地标检测技术在热成像中的应用方法。</li>
<li>采用多层次跨模态知识蒸馏框架实现高效准确的检测模型。</li>
<li>建立双向知识蒸馏机制以缩小RGB与热力学间的模态差距，确保稳健深刻的知识转移。</li>
<li>实验结果证明该方法在公开数据库测试中表现优异，显著超越先前方法。</li>
<li>技术在实际应用场景中具有广阔的应用前景和挑战性问题值得深入研究。</li>
<li>该技术为行业发展提供了广阔的前景，促进经济社会的全面协调可持续发展具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e8e2571aa4d6179d16b4c142589b8a74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735307&auth_key=1760735307-0-0-c21784fe7691993f838cc8f79a2cce92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-429cf1c19d4e1cf5439632e6fafbfe99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735315&auth_key=1760735315-0-0-3981636f2ad25336877546667fe6e1df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Cross-Sensor-Touch-Generation"><a href="#Cross-Sensor-Touch-Generation" class="headerlink" title="Cross-Sensor Touch Generation"></a>Cross-Sensor Touch Generation</h2><p><strong>Authors:Samanta Rodriguez, Yiming Dou, Miquel Oller, Andrew Owens, Nima Fazeli</strong></p>
<p>Today’s visuo-tactile sensors come in many shapes and sizes, making it challenging to develop general-purpose tactile representations. This is because most models are tied to a specific sensor design. To address this challenge, we propose two approaches to cross-sensor image generation. The first is an end-to-end method that leverages paired data (Touch2Touch). The second method builds an intermediate depth representation and does not require paired data (T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific models across multiple sensors via the cross-sensor touch generation process. Together, these models offer flexible solutions for sensor translation, depending on data availability and application needs. We demonstrate their effectiveness on downstream tasks such as in-hand pose estimation and behavior cloning, successfully transferring models trained on one sensor to another. Project page: <a target="_blank" rel="noopener" href="https://samantabelen.github.io/cross_sensor_touch_generation">https://samantabelen.github.io/cross_sensor_touch_generation</a>. </p>
<blockquote>
<p>当前的视觉触觉传感器有各种形状和大小，这为开发通用触觉表征带来了挑战。这是因为大多数模型都与特定的传感器设计相关联。为了应对这一挑战，我们提出了两种跨传感器图像生成方法。第一种是端到端的方法，它利用配对数据（Touch2Touch）。第二种方法建立中间深度表征，不需要配对数据（T2D2：Touch-to-Depth-to-Touch）。两种方法都通过跨传感器触摸生成过程，使得特定传感器的模型能够在多个传感器上使用。根据数据可用性和应用需求，这些模型为传感器转换提供了灵活的解决方案。我们在下游任务（例如手部姿态估计和行为克隆）上证明了它们的有效性，成功地将一个传感器上训练的模型转移到另一个传感器上。项目页面：<a target="_blank" rel="noopener" href="https://samantabelen.github.io/cross_sensor_touch_generation">https://samantabelen.github.io/cross_sensor_touch_generation</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09817v1">PDF</a> CoRL 2025</p>
<p><strong>Summary</strong>：<br>现代视觉触觉传感器种类繁多，给通用触觉表示的开发带来挑战。本文提出两种跨传感器图像生成方法应对此挑战。第一种为端到端的配对数据方法（Touch2Touch）；第二种构建中间深度表示，无需配对数据（T2D2：Touch-to-Depth-to-Touch）。两种方法都通过跨传感器触摸生成过程，使传感器特定模型可在多个传感器上应用。根据数据可用性和应用需求，这些模型提供灵活的传感器翻译解决方案。在下游任务（如手部姿态估计和行为克隆）中，成功将在一种传感器上训练的模型转移到另一种传感器上。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>现代视觉触觉传感器种类繁多，开发通用触觉表示面临挑战。</li>
<li>提出两种跨传感器图像生成方法：端到端的配对数据方法（Touch2Touch）和构建中间深度表示的方法（T2D2）。</li>
<li>两种方法都使传感器特定模型可在多个传感器上应用。</li>
<li>根据数据可用性和应用需求，这些模型提供灵活的传感器翻译解决方案。</li>
<li>在下游任务中，成功将一种传感器上训练的模型转移到另一种传感器上。</li>
<li>有效解决由于传感器种类多样性带来的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09817">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-94d2f501aa22e6a9e80e834bef2a1c6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735322&auth_key=1760735322-0-0-ad58a50ab5e277c334599a4dff60e46d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-688a94e6a056861a8ee10fe23f8180dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735329&auth_key=1760735329-0-0-11fcdc94ffd414df3b39324d96ebec94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75349c2f873693f74c6a4920ed0778cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735336&auth_key=1760735336-0-0-9002fbb50ddef9dfad737bbde6c75d04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lesion-Aware-Post-Training-of-Latent-Diffusion-Models-for-Synthesizing-Diffusion-MRI-from-CT-Perfusion"><a href="#Lesion-Aware-Post-Training-of-Latent-Diffusion-Models-for-Synthesizing-Diffusion-MRI-from-CT-Perfusion" class="headerlink" title="Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing   Diffusion MRI from CT Perfusion"></a>Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing   Diffusion MRI from CT Perfusion</h2><p><strong>Authors:Junhyeok Lee, Hyunwoong Kim, Hyungjin Chung, Heeseong Eom, Joon Jang, Chul-Ho Sohn, Kyu Sung Choi</strong></p>
<p>Image-to-Image translation models can help mitigate various challenges inherent to medical image acquisition. Latent diffusion models (LDMs) leverage efficient learning in compressed latent space and constitute the core of state-of-the-art generative image models. However, this efficiency comes with a trade-off, potentially compromising crucial pixel-level detail essential for high-fidelity medical images. This limitation becomes particularly critical when generating clinically significant structures, such as lesions, which often occupy only a small portion of the image. Failure to accurately reconstruct these regions can severely impact diagnostic reliability and clinical decision-making. To overcome this limitation, we propose a novel post-training framework for LDMs in medical image-to-image translation by incorporating lesion-aware medical pixel space objectives. This approach is essential, as it not only enhances overall image quality but also improves the precision of lesion delineation. We evaluate our framework on brain CT-to-MRI translation in acute ischemic stroke patients, where early and accurate diagnosis is critical for optimal treatment selection and improved patient outcomes. While diffusion MRI is the gold standard for stroke diagnosis, its clinical utility is often constrained by high costs and low accessibility. Using a dataset of 817 patients, we demonstrate that our framework improves overall image quality and enhances lesion delineation when synthesizing DWI and ADC images from CT perfusion scans, outperforming existing image-to-image translation models. Furthermore, our post-training strategy is easily adaptable to pre-trained LDMs and exhibits substantial potential for broader applications across diverse medical image translation tasks. </p>
<blockquote>
<p>图像到图像的翻译模型可以帮助缓解医疗图像采集过程中存在的各种挑战。潜在扩散模型（LDMs）在压缩的潜在空间中实现了高效学习，构成了最先进的生成图像模型的核心。然而，这种效率是有代价的，可能会损害对高保真医疗图像至关重要的像素级细节。当生成临床上重要的结构（如病变）时，这种限制变得尤为重要，因为这些结构通常只占图像的一小部分。无法准确重建这些区域可能会严重影响诊断的可靠性和临床决策。为了克服这一限制，我们提出了一种新型的后训练框架，用于医学图像到图像翻译中的LDM，通过引入病变感知医学像素空间目标。这种方法至关重要，因为它不仅提高了整体图像质量，而且提高了病变勾勒的精确度。我们在急性缺血性卒中患者的脑部CT到MRI翻译中评估了我们的框架，早期和准确的诊断对于选择最佳治疗方案和改善患者预后至关重要。虽然扩散MRI是卒中诊断的金标准，但其临床实用性往往受到成本高和可及性低的限制。我们使用包含817名患者的数据集证明了我们的框架在合成DWI和ADC图像时的效果，这些图像是从CT灌注扫描中得出的，我们的框架提高了整体图像质量并增强了病变的勾勒能力，超越了现有的图像到图像翻译模型。此外，我们的后训练策略可以轻松适应预训练的LDM，并在各种医疗图像翻译任务中具有巨大的应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09056v1">PDF</a> MICCAI 2025, Lecture Notes in Computer Science Vol. 15961</p>
<p><strong>Summary</strong></p>
<pre><code>图像到图像的翻译模型有助于缓解医学图像采集过程中存在的各种挑战。潜在扩散模型（LDMs）在压缩的潜在空间中实现了高效学习，构成了当前最先进的生成图像模型的核心。然而，这种效率是以牺牲像素级的细节为代价的，这对于高保真医学图像至关重要。为了克服这一局限性，特别是在生成临床重要结构如病灶时，我们提出了一种用于医学图像到图像翻译的后训练框架，通过引入病灶感知医学像素空间目标来解决这个问题。我们评估了该框架在急性缺血性卒中患者的脑部CT到MRI翻译中的应用，早期和准确的诊断对于选择最佳治疗方案和改善患者预后至关重要。我们的框架不仅提高了整体图像质量，还提高了病灶勾勒的精确度，并且在合成DWI和ADC图像从CT灌注扫描中表现出优于现有图像到图像翻译模型的效果。此外，我们的后训练策略可以轻松适应预训练的LDMs，并在各种医学图像翻译任务中具有巨大的应用潜力。

**Key Takeaways**

1. 图像到图像的翻译模型有助于解决医学图像采集中的挑战。
2. 潜在扩散模型（LDMs）在压缩的潜在空间中实现了高效学习，但可能牺牲像素级细节。
3. 病灶等临床重要结构的生成对高保真医学图像至关重要。
4. 提出了一种用于医学图像到图像翻译的后训练框架，通过引入病灶感知医学像素空间目标来提升图像质量。
5. 该框架在急性缺血性卒中患者的脑部CT到MRI翻译中表现优异。
6. 相较于现有模型，提出的框架提高了图像整体质量和病灶勾勒的准确性。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09056">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6e4b48995cf1d566c55ee7d3cb15dcfe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735343&auth_key=1760735343-0-0-9e8db06eec2d5436f66383da8b51a0f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f7c0df3242f3e231e2bd72f34984f2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735350&auth_key=1760735350-0-0-8fe2ba370b50192c4392f1459457039e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da3d52280bd20f5577fe1c3631721f2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735358&auth_key=1760735358-0-0-92ee8674a223a586edd5f08671582b65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1faf5bc906f8d57d051c2b4480913130~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735365&auth_key=1760735365-0-0-7e6ecef6e962b938b27080ddf3b10317&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FS-RWKV-Leveraging-Frequency-Spatial-Aware-RWKV-for-3T-to-7T-MRI-Translation"><a href="#FS-RWKV-Leveraging-Frequency-Spatial-Aware-RWKV-for-3T-to-7T-MRI-Translation" class="headerlink" title="FS-RWKV: Leveraging Frequency Spatial-Aware RWKV for 3T-to-7T MRI   Translation"></a>FS-RWKV: Leveraging Frequency Spatial-Aware RWKV for 3T-to-7T MRI   Translation</h2><p><strong>Authors:Yingtie Lei, Zimeng Li, Chi-Man Pun, Yupeng Liu, Xuhang Chen</strong></p>
<p>Ultra-high-field 7T MRI offers enhanced spatial resolution and tissue contrast that enables the detection of subtle pathological changes in neurological disorders. However, the limited availability of 7T scanners restricts widespread clinical adoption due to substantial infrastructure costs and technical demands. Computational approaches for synthesizing 7T-quality images from accessible 3T acquisitions present a viable solution to this accessibility challenge. Existing CNN approaches suffer from limited spatial coverage, while Transformer models demand excessive computational overhead. RWKV architectures offer an efficient alternative for global feature modeling in medical image synthesis, combining linear computational complexity with strong long-range dependency capture. Building on this foundation, we propose Frequency Spatial-RWKV (FS-RWKV), an RWKV-based framework for 3T-to-7T MRI translation. To better address the challenges of anatomical detail preservation and global tissue contrast recovery, FS-RWKV incorporates two key modules: (1) Frequency-Spatial Omnidirectional Shift (FSO-Shift), which performs discrete wavelet decomposition followed by omnidirectional spatial shifting on the low-frequency branch to enhance global contextual representation while preserving high-frequency anatomical details; and (2) Structural Fidelity Enhancement Block (SFEB), a module that adaptively reinforces anatomical structure through frequency-aware feature fusion. Comprehensive experiments on UNC and BNU datasets demonstrate that FS-RWKV consistently outperforms existing CNN-, Transformer-, GAN-, and RWKV-based baselines across both T1w and T2w modalities, achieving superior anatomical fidelity and perceptual quality. </p>
<blockquote>
<p>超高场7T MRI提供了更高的空间分辨率和组织对比度，能够检测到神经疾病中的细微病理变化。然而，由于基础设施成本和技术需求较高，7T扫描仪的有限可用性限制了其在临床上的广泛应用。利用计算合成技术从可获得的3T扫描图像生成7T质量的图像，是解决这一可访问性挑战的一种可行方法。现有的CNN方法存在空间覆盖范围有限的问题，而Transformer模型则要求过高的计算开销。RWKV架构为医学图像合成中的全局特征建模提供了一种有效的替代方案，结合了线性计算复杂度和强大的长距离依赖捕获能力。在此基础上，我们提出了基于RWKV架构的频率空间FS-RWKV（Frequency Spatial-RWKV），用于实现3T到7TMR图像翻译。为了更好地解决保留解剖细节和恢复全局组织对比度的问题，FS-RWKV引入了两个关键模块：（1）频率空间全方向移位（Frequency-Spatial Omnidirectional Shift，FSO-Shift），它执行离散小波分解，然后对低频分支进行全空间移位，以增强全局上下文表示的同时保留高频解剖细节；（2）结构保真度增强块（Structural Fidelity Enhancement Block，SFEB），该模块通过频率感知特征融合自适应地强化解剖结构。在UNC和BNU数据集上的综合实验表明，FS-RWKV在T1w和T2w模态上均持续优于现有的CNN、Transformer、GAN和RWKV基线方法，达到优越的解剖保真度和感知质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08951v1">PDF</a> Accepted by BIBM 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了超高场7T MRI的技术优势及普及难点，提出了基于RWKV架构的Frequency Spatial-RWKV（FS-RWKV）框架，用于从可获取的3T MRI图像合成7T质量的图像。该框架包含两个关键模块：Frequency-Spatial Omnidirectional Shift（FSO-Shift）和Structural Fidelity Enhancement Block（SFEB），旨在增强全局上下文表征并保留高频解剖细节，同时自适应地强化解剖结构。实验证明，FS-RWKV在UNC和BNU数据集上均优于现有CNN、Transformer、GAN和RWKV基线方法，在T1w和T2w模态下实现优越的解剖保真度和感知质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>超高场7T MRI具有优秀的空间分辨率和组织对比度，能检测神经疾病的细微病理变化。</li>
<li>7T扫描仪的有限可用性和高昂成本限制了其临床普及。</li>
<li>计算合成7T质量图像的方法从可获取的3T图像出发，是解决这一挑战的有效途径。</li>
<li>现有CNN方法空间覆盖有限，而Transformer模型计算开销大。</li>
<li>RWKV架构具有线性计算复杂性和强大的长程依赖性捕捉，适用于医学图像合成。</li>
<li>提出的FS-RWKV框架结合FSO-Shift和SFEB模块，旨在增强全局特征建模和解剖细节保留。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a23413fcbe2217687c4b5d45e1ba2cdd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735372&auth_key=1760735372-0-0-7871eeff509556dbc1e01bfff5543a9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0583d67c0b286f956a9e8c9c98892c03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735379&auth_key=1760735379-0-0-65033497b189942f8fa00aefb4854ba7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ee69d0d91f9b3e60daf38b080584a7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735386&auth_key=1760735386-0-0-e3d9ebac30cfa497c2a14bdc5c9d69a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-800ef8b51421ab76d5f96e4f14d71fe4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735396&auth_key=1760735396-0-0-5b86cdbac431e77694a31bcc69658f1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34c0807621d4105f1a310f6c90502f87~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735405&auth_key=1760735405-0-0-6175161d668eb267b4bb81cffae3ef95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d7c3af629069c9e8cd71f0f39ac3116~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735412&auth_key=1760735412-0-0-ad9eb607db3c62d2336ee7b3d9c58f6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Benchmarking-AI-evolved-cosmological-structure-formation"><a href="#Benchmarking-AI-evolved-cosmological-structure-formation" class="headerlink" title="Benchmarking AI-evolved cosmological structure formation"></a>Benchmarking AI-evolved cosmological structure formation</h2><p><strong>Authors:Xiaofeng Dong, Nesar Ramachandra, Salman Habib, Katrin Heitmann</strong></p>
<p>The potential of deep learning-based image-to-image translations has recently attracted significant attention. One possible application of such a framework is as a fast, approximate alternative to cosmological simulations, which would be particularly useful in various contexts, including covariance studies, investigations of systematics, and cosmological parameter inference. To investigate different aspects of learning-based cosmological mappings, we choose two approaches for generating suitable cosmological matter fields as datasets: a simple analytical prescription provided by the Zel’dovich approximation, and a numerical N-body method using the Particle-Mesh approach. The evolution of structure formation is modeled using U-Net, a widely employed convolutional image translation framework. Because of the lack of a controlled methodology, validation of these learned mappings requires multiple benchmarks beyond simple visual comparisons and summary statistics. A comprehensive list of metrics is considered, including higher-order correlation functions, conservation laws, topological indicators, and statistical independence of density fields. We find that the U-Net approach performs well only for some of these physical metrics, and accuracy is worse at increasingly smaller scales, where the dynamic range in density is large. By introducing a custom density-weighted loss function during training, we demonstrate a significant improvement in the U-Net results at smaller scales. This study provides an example of how a family of physically motivated benchmarks can, in turn, be used to fine-tune optimization schemes – such as the density-weighted loss used here – to significantly enhance the accuracy of scientific machine learning approaches by focusing attention on relevant features. </p>
<blockquote>
<p>深度学习图像到图像的转换潜力最近引起了广泛关注。该框架的一个可能应用是作为宇宙学模拟的快速近似替代方案，在各种背景下都将特别有用，包括协方差研究、系统调查以及宇宙学参数推断。为了研究基于学习的宇宙学映射的不同方面，我们选择了两种生成合适的宇宙学物质场数据集的方法：Zel’dovich近似提供的简单分析公式和采用粒子网格方法的数值N体方法。我们使用广泛采用的卷积图像翻译框架U-Net对结构形成的演化进行建模。由于缺乏受控的方法论，这些学习到的映射的验证需要超越简单视觉比较和总结统计的多个基准测试。所考虑的指标包括高阶相关函数、守恒定律、拓扑指标以及密度场的统计独立性。我们发现U-Net方法仅对这些物理指标中的部分表现良好，而且在规模越来越小的地方精度更差，那里的密度动态范围很大。通过在训练过程中引入自定义的密度加权损失函数，我们证明了U-Net在小规模上的结果有了显着改善。这项研究提供了一个例子，说明一系列基于物理的基准测试如何反过来用于微调优化方案——如此处使用的密度加权损失——通过关注相关特征来显著提高科学机器学习方法的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06731v2">PDF</a> Expanded and thoroughly revised version of our prior NeurIPS   submission (arXiv:2112.05681; which has no DOI), with new sections,   experiments, and analyses</p>
<p><strong>Summary</strong>：基于深度学习图像到图像转换的潜力已引起广泛关注。本研究选择使用U-Net卷积图像翻译框架对基于学习的宇宙学映射进行建模，并通过多种物理指标对其性能进行评估。为提高其在较小尺度上的准确性，引入自定义密度加权损失函数进行训练。此研究展示了如何通过关注相关特征对科学机器学习方法进行微调以提高其准确性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>深度学习在图像到图像转换方面的潜力巨大，可作为一种快速近似替代宇宙学模拟。</li>
<li>U-Net卷积图像翻译框架用于建模基于学习的宇宙学映射。</li>
<li>对学习映射的性能进行了全面的评估，包括高阶相关函数、守恒定律、拓扑指标和密度场的统计独立性等物理指标。</li>
<li>U-Net在某些物理指标上表现良好，但在较小尺度上的准确性有待提高。</li>
<li>通过引入自定义密度加权损失函数进行训练，在较小尺度上显著提高U-Net的结果。</li>
<li>物理指标的基准测试可用于微调优化方案，如通过密度加权损失来提高科学机器学习的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06731">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a9106792a67f9fb8790ca9572c92649f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735419&auth_key=1760735419-0-0-8c0fadfdbac0faaa21abea3919f4c090&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd501b03ff2ba0cea5403fd25544bb8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735426&auth_key=1760735426-0-0-4643f728fb5e50eff57fe3f72f6feb83&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46e19434ee3f8a8220b28f30ca8dcdc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735433&auth_key=1760735433-0-0-2f735749aa1b46901533665ce998450f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-1st-Solution-for-CARE-Liver-Task-Challenge-2025-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation"><a href="#The-1st-Solution-for-CARE-Liver-Task-Challenge-2025-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation" class="headerlink" title="The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware   Semi-Supervised Segmentation with Domain Generalization and Test-Time   Adaptation"></a>The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware   Semi-Supervised Segmentation with Domain Generalization and Test-Time   Adaptation</h2><p><strong>Authors:Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang</strong></p>
<p>Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions. </p>
<blockquote>
<p>从对比增强MRI准确分割肝脏对于诊断、治疗计划和疾病监测至关重要。然而，由于标注数据有限、增强协议存在异质性以及扫描仪和机构间领域转移显著，这仍然是一个挑战。传统的图像到图像翻译框架在领域通用化方面取得了很大进展，但其应用并不直接。例如，Pix2Pix需要进行图像注册，而循环GAN无法无缝集成到分割管道中。同时，这些方法最初是用于处理跨模态场景的，往往会引入结构失真并面临训练不稳定的问题，这在我们单模态场景中可能会带来弊端。为了解决这些挑战，我们提出了CoSSeg-TTA，这是一个紧凑的分割框架，适用于GED4（Gd-EOB-DTPA增强肝胆相MRI）模态，建立在nnU-Netv2之上，并用半监督均值教师方案增强，以利用大量未标记体积数据。一个领域适应模块，结合基于随机直方图的风格外观转换函数和可训练的对比感知网络，丰富了领域多样性并减轻了跨中心差异性。此外，采用连续测试时间适应策略，以提高推理过程中的稳健性。大量实验表明，我们的框架始终优于nnU-Netv2基线，在狄克分数和海森距离方面表现优越，同时在低注释条件下对未见领域具有很强的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04243v2">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对GED4模态的紧凑分割框架CoSSeg-TTA，用于准确地进行对比增强MRI的肝脏分割。该框架基于nnU-Netv2，采用半监督均值教师方案，利用大量未标注体积数据。通过领域适应模块和持续测试时适应策略，提高了框架在有限标注数据下的领域适应性和鲁棒性。实验表明，该框架优于nnU-Netv2基准模型，具有良好的泛化能力和性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoSSeg-TTA框架用于对比增强MRI的肝脏分割，适用于GED4模态。</li>
<li>框架基于nnU-Netv2，结合半监督均值教师方案，利用大量未标注数据。</li>
<li>领域适应模块通过随机直方图风格转换和可训练对比网络丰富领域多样性，减少跨中心差异。</li>
<li>采用持续测试时适应策略，提高模型在推理阶段的鲁棒性。</li>
<li>实验显示CoSSeg-TTA框架优于nnU-Netv2基准模型，具有更高的Dice分数和更好的Hausdorff距离表现。</li>
<li>该框架在有限标注数据下表现出良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04243">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d3d7bfd5e60b514a48bb97b3ef7c2171~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735441&auth_key=1760735441-0-0-d58c2a58a05f090384a4ac349e098a22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fd93b6d3ead1a4ebf9ee116bf4377edf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735448&auth_key=1760735448-0-0-39aa23320cb37b9cc339de36a0236418&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4759b31fe7fc13d067ccf4222b0acdc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735455&auth_key=1760735455-0-0-179706403015f9a596cdea1d16cf02f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="IWR-Bench-Can-LVLMs-reconstruct-interactive-webpage-from-a-user-interaction-video"><a href="#IWR-Bench-Can-LVLMs-reconstruct-interactive-webpage-from-a-user-interaction-video" class="headerlink" title="IWR-Bench: Can LVLMs reconstruct interactive webpage from a user   interaction video?"></a>IWR-Bench: Can LVLMs reconstruct interactive webpage from a user   interaction video?</h2><p><strong>Authors:Yang Chen, Minghao Liu, Yufan Shen, Yunwen Li, Tianyuan Huang, Xinyu Fang, Tianyu Zheng, Wenxuan Huang, Cheng Yang, Daocheng Fu, Jianbiao Mei, Rong Wu, Yunfei Zhao, Licheng Wen, Xuemeng Yang, Song Mao, Qunshu Lin, Zhi Yu, Yongliang Shen, Yu Qiao, Botian Shi</strong></p>
<p>The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models’ ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/L-O-I/IWR-Bench">https://github.com/L-O-I/IWR-Bench</a>. </p>
<blockquote>
<p>网页到代码的任务要求模型理解网页的视觉表示并生成相应的代码。然而，现有的基准测试主要关注静态截图到代码的任务，从而忽略了现实世界网页应用中至关重要的动态交互。为了解决这一局限性，本文介绍了IWR-Bench，一个用于评估大型视觉语言模型（LVLMs）从视频中进行交互式网页重建能力的新型基准测试。IWR-Bench包含来自100个真实网站的113个精心策划的任务，包含1001个动作，并呈现多样化的交互复杂性（如网页游戏）、视觉风格和领域。与标准网页开发实践相一致，每个任务不仅包含用户交互视频，还包含所有爬取的静态资产（如图片、视频）。此基准测试评估模型在两个基本挑战上的表现：通过视频和资产推断交互逻辑的综合多模式推理，以及将这一逻辑转化为功能代码的先进代码生成。一个以法官为角色的框架，配备有全面的度量系统，可以自动评估生成网页的功能正确性和视觉保真度。在28个LVLMs上的大量实验显示了一个重大挑战：最佳模型的总体得分仅为36.35%，其中功能正确性（24.39% IFS）远远落后于视觉保真度（64.25% VFS）。这些结果突显了当前模型在推理时间动态和合成事件驱动逻辑方面的关键局限性，使IWR-Bench成为视觉语言研究的前沿挑战。基准测试和评估代码将在<a target="_blank" rel="noopener" href="https://github.com/L-O-I/IWR-Bench%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/L-O-I/IWR-Bench上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24709v2">PDF</a> </p>
<p><strong>Summary</strong><br>网页到代码的转换任务需要模型理解网页的视觉表示并生成相应的代码。然而，现有的基准测试主要关注静态截图到代码的转换，忽略了网页动态交互在现实应用中的重要性。为解决这一问题，本文引入了IWR-Bench，一个评估大型视觉语言模型（LVLMs）从视频中进行交互式网页重建能力的新型基准测试。IWR-Bench包含来自100个真实网站的113个精心挑选的任务，涵盖1001个动作，具有多样的交互复杂性（如网页游戏）、视觉风格和领域。该基准测试评估模型在两个基本挑战上的表现：从视频和资产中推断交互逻辑的综合多模式推理，以及将这一逻辑转化为功能代码的先进代码生成。采用代理评委框架和全面的度量系统，自动评估生成网页的功能正确性和视觉保真度。对28个LVLMs的广泛实验显示了一个重大的挑战：最佳模型的整体得分仅为36.35%，功能正确性（24.39% IFS）远远落后于视觉保真度（64.25% VFS）。这表明当前模型在推理时间动态和合成事件驱动逻辑方面存在关键局限性，使IWR-Bench成为视觉语言研究的前沿挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>网页到代码的转换任务需要模型理解网页的视觉表示并生成代码。</li>
<li>现有基准测试主要关注静态截图到代码的转换，忽略了动态交互的重要性。</li>
<li>IWR-Bench是一个新的基准测试，用于评估模型从视频中进行交互式网页重建的能力。</li>
<li>IWR-Bench包含来自真实网站的多样化任务，涵盖不同的交互复杂性、视觉风格和领域。</li>
<li>该基准测试评估模型在综合多模式推理和先进代码生成方面的表现。</li>
<li>最佳模型在功能正确性方面存在显著挑战，表明当前模型在推理时间动态和合成事件驱动逻辑方面的局限性。</li>
<li>IWR-Bench为视觉语言研究提供了一个具有挑战性的前沿。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24709">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-28be6e9bd56a26bd99618c34c78292ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735461&auth_key=1760735461-0-0-70fb8f0cda272707d7d0e9e89fc6f58c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b1516eb9baee34ba4a11622042a14b73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735468&auth_key=1760735468-0-0-ad465eb44b20f5b8b49eac4dfa7f92df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30e69b2dcb1dbda02380105734227290~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735498&auth_key=1760735498-0-0-997f874d7a3989c0440ae202fb1a6c42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3c345c8ea14fee54c2909c055fbc983~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735504&auth_key=1760735504-0-0-4f918f4351fc8729b0317f70d9b12978&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c2a6461336a9f48dac88ec8d27ffa72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735532&auth_key=1760735532-0-0-613f118b05e22be0d1ef5917fd11b13b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fc74284a4c241dd9a946837c261af5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735538&auth_key=1760735538-0-0-65f5f971a60d72b1adaaaf774c7a3b1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Goal-Based-Vision-Language-Driving"><a href="#Goal-Based-Vision-Language-Driving" class="headerlink" title="Goal-Based Vision-Language Driving"></a>Goal-Based Vision-Language Driving</h2><p><strong>Authors:Santosh Patapati, Trisanth Srinivasan</strong></p>
<p>Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes &#x2F; Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive’s shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well. </p>
<blockquote>
<p>我们引入了NovaDrive，这是一种单分支视觉语言架构，能够在单个分支中处理前置摄像头图像、高清地图瓦片、激光雷达深度和文本航点。通过一个轻便的两阶段交叉注意力块，首先对齐航点标记与高清地图，然后精细调整图像和深度补丁的注意力。结合一种新型平滑损失，该设计可避免急转弯和速度突变，消除了对循环内存的需求。我们微调了11B LLaMA-3.2视觉语言主干的顶层15层，实现了实时推理。在MD-NEX户外基准的nuScenes&#x2F;Waymo子集上，NovaDrive将成功率提高到84%（+4%），路径效率（SPL）提高到0.66（+0.11），与之前的最新技术相比，碰撞频率从2.6%降低到1.2%（-1.4%）。我们的消融实验证实，航点标记、部分VLM微调以及交叉注意力融合都对这些增益贡献最大。除了提高安全性外，NovaDrive的更短路线（由新型平滑损失产生）转化为更低的燃油或电池使用量，指向更精简、更容易更新的驾驶堆栈。NovaDrive还可以扩展到其他实体人工智能领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23042v2">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>文章介绍了NovaDrive系统，这是一种单一分支的视觉语言架构，用于处理自动驾驶中的复杂情况。该系统能够实时处理前视摄像头图像、高清地图瓦片、激光雷达深度和文本航点信息。通过一种新的跨注意力机制，NovaDrive能够精细地分析路况并做出决策，同时结合新型平滑损失函数，减少了急转弯和速度变化，提高了驾驶的安全性和效率。在MD-NEX Outdoor基准测试中，NovaDrive相较于其他技术取得了更高的成功率、路径效率和更低的碰撞频率。此外，NovaDrive还可以应用于其他嵌入式人工智能领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NovaDrive是一个单一分支的视觉语言架构，用于处理自动驾驶中的复杂情况。它实时处理多种信息如摄像头图像、高清地图瓦片等。</li>
<li>NovaDrive采用了创新的跨注意力机制，能够精细地分析路况并做出决策。通过这种方式，系统可以快速响应并处理紧急情况。</li>
<li>NovaDrive结合了新型平滑损失函数，减少了急转弯和速度变化，提高了驾驶的安全性和效率。</li>
<li>在基准测试中，NovaDrive相较于其他技术取得了更高的成功率、路径效率和更低的碰撞频率。</li>
<li>NovaDrive通过缩短路线降低了燃料或电池的使用量，使驾驶更加环保和高效。此外，其设计还使其易于更新和维护。</li>
<li>NovaDrive的应用不仅限于自动驾驶领域，还可以扩展到其他嵌入式人工智能领域。这表明其具有很强的通用性和潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b89d9e8e6d07ffbb2d4eef856153608e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735546&auth_key=1760735546-0-0-9c2efae19ef4bbdc1a1f27e8253c7d60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73a16cc0d5562a56bdcda872a1a506ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735553&auth_key=1760735553-0-0-e716a4279619287301aba5e3cb60ee50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1113797b854b9521854db2a6f596277a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735559&auth_key=1760735559-0-0-456674184d15b63d2410352d484d1474&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Endoscopic-Depth-Estimation-Based-on-Deep-Learning-A-Survey"><a href="#Endoscopic-Depth-Estimation-Based-on-Deep-Learning-A-Survey" class="headerlink" title="Endoscopic Depth Estimation Based on Deep Learning: A Survey"></a>Endoscopic Depth Estimation Based on Deep Learning: A Survey</h2><p><strong>Authors:Ke Niu, Zeyun Liu, Xue Feng, Heng Li, Qika Lin, Kaize Shi</strong></p>
<p>Endoscopic depth estimation is a critical technology for improving the safety and precision of minimally invasive surgery. It has attracted considerable attention from researchers in medical imaging, computer vision, and robotics. Over the past decade, a large number of methods have been developed. Despite the existence of several related surveys, a comprehensive overview focusing on recent deep learning-based techniques is still limited. This paper endeavors to bridge this gap by systematically reviewing the state-of-the-art literature. Specifically, we provide a thorough survey of the field from three key perspectives: data, methods, and applications. Firstly, at the data level, we describe the acquisition process of publicly available datasets. Secondly, at the methodological level, we introduce both monocular and stereo deep learning-based approaches for endoscopic depth estimation. Thirdly, at the application level, we identify the specific challenges and corresponding solutions for the clinical implementation of depth estimation technology, situated within concrete clinical scenarios. Finally, we outline potential directions for future research, such as domain adaptation, real-time implementation, and the synergistic fusion of depth information with sensor technologies, thereby providing a valuable starting point for researchers to engage with and advance the field toward clinical translation. </p>
<blockquote>
<p>内窥镜深度估计是提高微创手术安全性和精度的一项关键技术。它引起了医学影像、计算机视觉和机器人等领域研究人员的广泛关注。在过去的十年里，已经开发了大量的方法。尽管存在几个相关调查，但重点关注最近基于深度学习的技术的全面概述仍然有限。本文旨在通过系统回顾最新文献来填补这一空白。具体来说，我们从数据、方法和应用三个关键角度对该领域进行了全面调查。首先，在数据层面，我们描述了公开数据集的采集过程。其次，在方法层面，我们介绍了基于单目和立体视觉的深度学习内窥镜深度估计方法。第三，在应用层面，我们确定了在临床场景中实施深度估计技术所面临的挑战以及相应的解决方案。最后，我们概述了未来研究的方向，如域适应、实时实施以及深度信息与传感器技术的协同融合，从而为研究人员提供了一个宝贵的起点，推动该领域向临床转化。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20881v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文系统综述了基于深度学习的内镜深度估计技术的最新进展，从数据、方法和应用三个关键角度进行了全面概述。介绍了公开数据集的采集过程，深度学习方法中的单目和立体视觉方法，以及深度估计技术在具体临床场景中的应用挑战和解决方案。最后，本文还指出了未来研究方向，如域适应、实时实现以及深度信息与传感器技术的融合。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>内镜深度估计是提高微创外科手术安全性和精度的重要技术。</li>
<li>该领域已吸引了医疗成像、计算机视觉和机器人技术等方面研究人员的关注。</li>
<li>综述文章系统介绍了基于深度学习的内镜深度估计技术的最新进展。</li>
<li>从数据、方法和应用三个角度全面概述了该领域的现状。</li>
<li>公开数据集的采集过程、深度学习方法中的单目和立体视觉方法得到了详细介绍。</li>
<li>文章指出了深度估计技术在具体临床场景中的应用挑战和解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3f4bb4609e800341f1517a285c0d6755~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735566&auth_key=1760735566-0-0-bc6305965096c233fd66f68ad6148de2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bea19531445d83d4ecd1a604b2ef164b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735573&auth_key=1760735573-0-0-8880caa2485ca11c6bb13c27c35e983e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6db195581bcc94d50438af72e93c30bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735579&auth_key=1760735579-0-0-68136160e9599fbf63cd75860820ad2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72e35ac3aadf30a2832988ada6f1bb29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735586&auth_key=1760735586-0-0-a02c43d284f1ef8e565f74550e357ba8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be2e6e5378b80d55fb30570fd9861838~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735592&auth_key=1760735592-0-0-97158b7fe856519c4c2fa9352e4c9c2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d49db0d17856618039e96b04edb40849~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735599&auth_key=1760735599-0-0-746919876d8d3145b2527c71272b6ddc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling"><a href="#GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling" class="headerlink" title="GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling"></a>GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling</h2><p><strong>Authors:Siran Li, Ruiyang Liu, Chen Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang</strong></p>
<p>Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments. We introduce GarmageNet, a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. Central to our approach is Garmage, a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment geometries. Followed by GarmageNet, a latent diffusion transformer to synthesize panel-wise geometry images and GarmageJigsaw, a neural module for predicting point-to-point sewing connections along panel contours. To support training and evaluation, we build GarmageSet, a large-scale dataset comprising 14,801 professionally designed garments with detailed structural and style annotations. Our method demonstrates versatility and efficacy across multiple application scenarios, including scalable garment generation from multi-modal design concepts (text prompts, sketches, photographs), automatic modeling from raw flat sewing patterns, pattern recovery from unstructured point clouds, and progressive garment editing using conventional instructions, laying the foundation for fully automated, production-ready pipelines in digital fashion. Project page: <a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet/">https://style3d.github.io/garmagenet/</a>. </p>
<blockquote>
<p>现实主义的数字服装建模仍然是一个劳动密集型的任务，这主要是由于将二维缝纫图案转化为高保真、可进行模拟的3D服装的复杂过程。我们引入了GarmageNet，这是一个统一的生成框架，可以自动创建2D缝纫图案、构建缝纫关系，并合成与基于物理的模拟兼容的3D服装初始化。我们的方法的核心是Garmage，这是一种新的服装表示方法，它将每个面板编码为结构化的几何图像，有效地弥合了二维结构图案和三维服装几何之间的语义和几何差距。其次是GarmageNet，一个用于合成面板级几何图像的潜在扩散变压器，以及GarmageJigsaw，一个用于预测面板轮廓沿线点对点缝纫连接的神经网络模块。为了支持和评估我们的方法，我们构建了GarmageSet，这是一个包含14801个专业设计的服装的大规模数据集，具有详细的结构和风格注释。我们的方法展示了在多个应用场景下的通用性和有效性，包括从多模式设计概念（文本提示、草图、照片）进行可扩展的服装生成、从原始平面缝纫图案的自动建模、从非结构化的点云中的模式恢复，以及使用常规指令进行渐进式的服装编辑，为数字时尚的完全自动化、生产准备管道奠定了基础。项目页面：<a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet/%E3%80%82">https://style3d.github.io/garmagenet/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01483v4">PDF</a> 23 pages,20 figures</p>
<p><strong>Summary</strong><br>     本文介绍了GarmageNet框架，该框架实现了自动化创建二维缝纫图案、构建缝纫关系以及合成可用于物理模拟的三维服装初始化。其核心在于Garmage技术，该技术将每个服装面板编码为结构化几何图像，有效桥接了二维结构图案和三维服装几何之间的语义和几何差距。此外，还有GarmageNet用于合成面板几何图像，以及GarmageJigsaw预测面板轮廓的点到点缝纫连接。为支持和评估该方法，建立了包含专业设计服装的大型数据集GarmageSet。此技术具有跨多种应用场景的通用性和有效性，包括从多模态设计概念（文本提示、草图、照片）生成服装、从原始平面缝纫图案自动建模、从无序点云中恢复图案以及使用常规指令进行渐进式服装编辑。这为全自动生产准备管道奠定了数字时尚的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GarmageNet是一个统一的生成框架，能够自动化创建二维缝纫图案、构建缝纫关系，并合成可用于物理模拟的三维服装初始化。</li>
<li>Garmage是一种新型的服装表示方法，将每个服装面板编码为结构化几何图像，以桥接二维和三维之间的语义和几何差距。</li>
<li>GarmageNet用于合成面板几何图像，而GarmageJigsaw则预测面板轮廓的点到点缝纫连接。</li>
<li>为支持训练和评估，建立了一个大型数据集GarmageSet，包含专业设计的服装和详细的结构和风格注释。</li>
<li>该技术具有多种应用场景，包括从多模态设计概念生成服装、从原始平面缝纫图案建模、从无序点云中恢复图案以及使用常规指令进行渐进式服装编辑。</li>
<li>此技术有助于建立全自动、可用于生产的数字时尚管道。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-412579f9702b54d2218d6cd838a2cdbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735606&auth_key=1760735606-0-0-19f9712344346e782193ffba6fcb36c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f22961a8a33f4707616da41edcc123c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735613&auth_key=1760735613-0-0-50b3ad6b387c608110c63c6120d16728&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aee5334c1385747fb6e9ddea7a21897e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735620&auth_key=1760735620-0-0-233513eea089e70697d5c341329517e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fcd444a9028a1d788913ab89c3c426ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735627&auth_key=1760735627-0-0-4721a646cc07cadd39a031cd2c861ae2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input"><a href="#Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input" class="headerlink" title="Real-Time Position-Aware View Synthesis from Single-View Input"></a>Real-Time Position-Aware View Synthesis from Single-View Input</h2><p><strong>Authors:Manu Gond, Emin Zerman, Sebastian Knorr, Mårten Sjöström</strong></p>
<p>Recent advancements in view synthesis have significantly enhanced immersive experiences across various computer graphics and multimedia applications, including telepresence and entertainment. By enabling the generation of new perspectives from a single input view, view synthesis allows users to better perceive and interact with their environment. However, many state-of-the-art methods, while achieving high visual quality, face limitations in real-time performance, which makes them less suitable for live applications where low latency is critical. In this paper, we present a lightweight, position-aware network designed for real-time view synthesis from a single input image and a target camera pose. The proposed framework consists of a Position Aware Embedding, which efficiently maps positional information from the target pose to generate high dimensional feature maps. These feature maps, along with the input image, are fed into a Rendering Network that merges features from dual encoder branches to resolve both high and low level details, producing a realistic new view of the scene. Experimental results demonstrate that our method achieves superior efficiency and visual quality compared to existing approaches, particularly in handling complex translational movements without explicit geometric operations like warping. This work marks a step toward enabling real-time live and interactive telepresence applications. </p>
<blockquote>
<p>近期视图合成技术的进展在多种计算机图形学和多媒体应用中极大地增强了沉浸式体验，包括远程存在和娱乐。视图合成能够通过单个输入视图生成新的视角，使用户更好地感知和与环境互动。然而，许多最先进的方法虽然视觉质量很高，但在实时性能方面存在局限性，这使得它们在低延迟至关重要的实时应用中不太适用。在本文中，我们提出了一种轻量级、位置感知的网络，用于从单个输入图像和目标相机姿态进行实时视图合成。所提出的框架包含一个位置感知嵌入，它有效地将目标姿态的位置信息映射到生成的高维特征图上。这些特征图与输入图像一起输入到渲染网络中，该网络合并来自双编码器分支的特征以解决高级和低级细节问题，生成场景的真实新视图。实验结果表明，与现有方法相比，我们的方法在效率和视觉质量方面实现了优越性，特别是在处理复杂的平移运动而无需进行如扭曲等明确的几何操作方面。这项工作标志着实时直播和交互式远程存在应用走向可能的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14005v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>新一代视图合成技术增强了计算机图形和多媒体应用的沉浸式体验，包括远程存在和娱乐。通过单一视角生成新视角的能力，视图合成让用户更好地感知和交互他们的环境。但许多最新方法虽然视觉质量高，但实时性能有限，不适用于低延迟的实时应用。本文提出了一种轻量级、位置感知的网络，专为从单一输入图像和目标相机姿态进行实时视图合成而设计。该框架包括位置感知嵌入，有效地将目标姿态的位置信息映射到生成的高维特征图上。这些特征图与输入图像一起输入到渲染网络中，合并来自双编码器分支的特征以解决高级和低级细节问题，生成逼真的新场景视图。实验结果表明，该方法与现有方法相比具有更高的效率和视觉质量，特别是在处理复杂的平移运动方面，无需明确的几何操作如扭曲。这项工作朝着实现实时直播和交互式远程存在应用迈出了一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视图合成技术增强了计算机图形和多媒体应用的沉浸式体验。</li>
<li>现有视图合成方法在实时性能方面存在局限性，不适用于低延迟的实时应用。</li>
<li>本文提出了一种轻量级、位置感知的网络，用于从单一输入图像和目标相机姿态进行实时视图合成。</li>
<li>位置感知嵌入有效地映射目标姿态的位置信息到高维特征图。</li>
<li>渲染网络合并来自双编码器分支的特征以生成逼真的新场景视图。</li>
<li>该方法具有更高的效率和视觉质量，尤其是在处理复杂的平移运动方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-164f05d12290fd506323ede81f11d057~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735634&auth_key=1760735634-0-0-8b1e83413de3ee332a709e592830584f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-391699baa8e0a7214936ac4da2e7026a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735643&auth_key=1760735643-0-0-6a687c102cfea99a3718b6f4b9bb285e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models"><a href="#Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models" class="headerlink" title="Go With the Flow: Fast Diffusion for Gaussian Mixture Models"></a>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</h2><p><strong>Authors:George Rapakoulias, Ali Reza Pedram, Fengjiao Liu, Lingjiong Zhu, Panagiotis Tsiotras</strong></p>
<p>Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SBs between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SBs, and various other examples. The implementation is publicly available at <a target="_blank" rel="noopener" href="https://github.com/georgeRapa/GMMflow">https://github.com/georgeRapa/GMMflow</a>. </p>
<blockquote>
<p>薛定谔桥（SBs）是一种扩散过程，在有限时间内，它将给定的初始分布引导到另一个最终分布，同时最小化适当的成本函数。尽管最近在文献中提出了各种计算SBs的方法，但即使对于解决低维问题，大多数这些方法也需要计算成本高昂的训练方案。在本文中，我们提出了一组可行策略的解析参数化方法，以引导动力系统从一个高斯混合模型（GMM）到另一个GMM的分布。我们不必依赖于标准的非凸优化技术，而是可以通过求解一个低维线性规划问题来近似集合中的最优策略，该问题的维度随着混合中每个组件的数量而线性扩展。所提出的方法自然地推广到更一般的动力系统类别，如可控线性时变系统，为解决多边缘动量SBs之间的GMM问题提供了有效解决方案，这是一个具有挑战性的分布插值问题。我们通过图像到图像翻译、在自动编码器的潜在空间中学习细胞动力学以及使用多边缘动量SBs的各种其他示例来展示此方法的潜力。实现公开可用在：<a target="_blank" rel="noopener" href="https://github.com/georgeRapa/GMMflow%E3%80%82">https://github.com/georgeRapa/GMMflow。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09059v6">PDF</a> NIPS 2025 (spotlight)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于高斯混合模型（GMM）的Schrodinger桥（SB）的解析参数化方法，用于在有限时间内引导系统的分布从一个状态转移到另一个状态。该方法通过低维线性规划来近似最优策略，可自然推广到更一般的动力系统，如可控线性时变系统。此方法解决了多边缘动量SB在GMM之间的分布插值问题，并在图像翻译、细胞动力学学习等领域展示了潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Schrodinger Bridges (SBs) 是引导初始分布到最终分布的同时最小化成本功能的扩散过程。</li>
<li>现有计算SB的方法大多需要计算昂贵的训练方案，即使对于低维问题也是如此。</li>
<li>本文提出一种解析参数化方法，为从高斯混合模型（GMM）引导系统分布提供一组可行的策略。</li>
<li>该方法通过低维线性规划来近似最优策略，该规划的维度与混合中组件的数量成线性关系。</li>
<li>该方法可自然推广到更一般的动力系统，如可控线性时变系统。</li>
<li>此方法解决了多边缘动量SB在GMM之间的分布插值问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09059">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-98ed64e337a389029100911058523aeb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735650&auth_key=1760735650-0-0-a0dae611298fa3331e25e7bcb2bcea3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-06e62343c630cc627d1133022c4bb17a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760736831&auth_key=1760736831-0-0-0a05db017bad519d502f9835cbd35a03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-10-18  Shot2Tactic-Caption Multi-Scale Captioning of Badminton Videos for   Tactical Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-9ea2b7948d507d2a3f8a9514f3be86a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733406&auth_key=1760733406-0-0-c1df8e09920c2d8a6f5769cb5b41f9c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-10-18  Unifying Environment Perception and Route Choice Modeling for Trajectory   Representation Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
