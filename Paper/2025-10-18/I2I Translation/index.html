<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-94d2f501aa22e6a9e80e834bef2a1c6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735322&auth_key=1760735322-0-0-ad58a50ab5e277c334599a4dff60e46d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection"><a href="#A-Multi-domain-Image-Translative-Diffusion-StyleGAN-for-Iris-Presentation-Attack-Detection" class="headerlink" title="A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection"></a>A Multi-domain Image Translative Diffusion StyleGAN for Iris   Presentation Attack Detection</h2><p><strong>Authors:Shivangi Yadav, Arun Ross</strong></p>
<p>An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method. </p>
<blockquote>
<p>è™¹è†œç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿå¯èƒ½ä¼šå—åˆ°å‘ˆç°æ”»å‡»ï¼ˆPAsï¼‰çš„å¨èƒï¼Œæ”»å‡»è€…ä½¿ç”¨äººé€ çœ¼ç›ã€æ‰“å°çš„çœ¼éƒ¨å›¾åƒæˆ–ç¾å®¹éšå½¢çœ¼é•œç­‰ä¼ªé€ ç‰©å“æ¥æ¬ºéª—ç³»ç»Ÿã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå·²ç»å¼€å‘äº†å‡ ç§å‘ˆç°æ”»å‡»æ£€æµ‹ï¼ˆPADï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºæ„å»ºå’ŒæˆåƒPAsçš„éšæ€§å›°éš¾ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°è™¹è†œPADæŠ€æœ¯çš„æ•°æ®é›†éå¸¸ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šåŸŸå›¾åƒç¿»è¯‘æ‰©æ•£é£æ ¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆMID-StyleGANï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç”Ÿæˆåˆæˆçœ¼éƒ¨å›¾åƒçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•æ‰å¤šä¸ªé¢†åŸŸï¼ˆå¦‚çœŸå®çœ¼éƒ¨ã€æ‰“å°çœ¼ç›å’Œç¾å®¹éšå½¢çœ¼é•œï¼‰çš„å‘ˆç°æ”»å‡»å’ŒçœŸå®ç‰¹å¾ã€‚MID-StyleGANç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ï¼Œå¯ä»¥ç”ŸæˆçœŸå®ä¸”å¤šæ ·çš„åˆæˆæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§å¤šåŸŸæ¶æ„ï¼Œèƒ½å¤Ÿå®ç°çœŸå®çœ¼éƒ¨å›¾åƒå’Œä¸åŒå‘ˆç°æ”»å‡»é¢†åŸŸä¹‹é—´çš„ç¿»è¯‘ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§é’ˆå¯¹çœ¼éƒ¨æ•°æ®çš„è‡ªé€‚åº”æŸå¤±å‡½æ•°ï¼Œä»¥ä¿æŒé¢†åŸŸä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆçœ¼éƒ¨å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä½¿ç”¨ç”Ÿæˆçš„æ•°æ®æ˜¾è‘—æé«˜äº†PADç³»ç»Ÿçš„æ€§èƒ½ï¼Œä¸ºè§£å†³è™¹è†œå’Œçœ¼éƒ¨ç”Ÿç‰©è¯†åˆ«ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œåœ¨LivDet2020æ•°æ®é›†ä¸Šï¼Œåœ¨1%è¯¯æ£€ç‡çš„æƒ…å†µä¸‹ï¼ŒçœŸå®æ£€æµ‹ç‡ä»93.41%æé«˜åˆ°äº†98.72%ï¼Œå±•ç¤ºäº†æ‰€æå‡ºæ–¹æ³•çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14314v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>å¤šåŸŸå›¾åƒè½¬æ¢æ‰©æ•£é£æ ¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆMID-StyleGANï¼‰é€šè¿‡åˆæˆä¸åŒé¢†åŸŸçš„æ•°æ®æœ‰æ•ˆç¼“è§£ç¼ºä¹ç”Ÿç‰©è¯†åˆ«é¢†åŸŸä¸­é’ˆå¯¹å±•ç¤ºæ”»å‡»ï¼ˆPAsï¼‰çš„è®­ç»ƒæ•°æ®é›†é—®é¢˜ã€‚MID-StyleGANç»“åˆæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ï¼Œç”ŸæˆçœŸå®å¤šæ ·çš„åˆæˆæ•°æ®ã€‚å®ƒåœ¨å¤šåŸŸæ¶æ„ä¸‹ï¼Œå®ç°äº†è‰¯æ€§çœ¼éƒ¨å›¾åƒä¸ä¸åŒæ”»å‡»åŸŸçš„è½¬æ¢ï¼Œå¹¶åˆ©ç”¨é’ˆå¯¹çœ¼éƒ¨æ•°æ®çš„è‡ªé€‚åº”æŸå¤±å‡½æ•°ä¿æŒåŸŸä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜MID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆçœ¼éƒ¨å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆæå‡å¯¹æŠ—å±•ç¤ºæ”»å‡»ç³»ç»Ÿæ€§èƒ½ï¼Œè§£å†³ç”Ÿç‰©è¯†åˆ«é¢†åŸŸæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚åœ¨LivDet2020æ•°æ®é›†ä¸Šï¼Œè¯¯æŠ¥ç‡ä¸º1%æ—¶çš„çœŸå®æ£€æµ‹ç‡ä»93.41%æå‡è‡³98.72%ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>MID-StyleGANæ˜¯ç”¨äºç”Ÿæˆåˆæˆçœ¼éƒ¨å›¾åƒçš„æ–°æ¡†æ¶ï¼Œå¯åº”å¯¹ç”Ÿç‰©è¯†åˆ«ä¸­å› å±•ç¤ºæ”»å‡»ï¼ˆPAsï¼‰å¯¼è‡´çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜ç‚¹ï¼Œç”ŸæˆçœŸå®ä¸”å¤šæ ·çš„æ•°æ®ã€‚</li>
<li>MID-StyleGANé‡‡ç”¨å¤šåŸŸæ¶æ„ï¼Œå®ç°è‰¯æ€§çœ¼éƒ¨å›¾åƒå’Œä¸åŒæ”»å‡»åŸŸä¹‹é—´çš„è½¬æ¢ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”æŸå¤±å‡½æ•°ç»´æŒåŸŸä¸€è‡´æ€§ï¼Œç¡®ä¿æ•°æ®çš„çœŸå®æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºMID-StyleGANåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆçœ¼éƒ¨å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨MID-StyleGANç”Ÿæˆçš„æ•°æ®èƒ½æ˜¾è‘—æå‡å¯¹æŠ—å±•ç¤ºæ”»å‡»çš„ç³»ç»Ÿæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-80a4fdafcc32495b50c09a446a994bfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735172&auth_key=1760735172-0-0-df612493b85f7c9a024431340cbe6fb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6056cd86ebec8885b5424fb61832431d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735180&auth_key=1760735180-0-0-d66015720e55f7824c4b0c4cd93dd7fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43c738f3cd0db04b00bbb1720790275e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735186&auth_key=1760735186-0-0-8aca298c7f9c06ec09f0b8a1b92c1218&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-122efb3575dd31fe74dde03e03b33246~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735192&auth_key=1760735192-0-0-6ff1551316cec8bccb308c9f53018c4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb6859920f9fd33d336e66f021b91ed8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735199&auth_key=1760735199-0-0-25cf2798934ce6e03ae80012189a6722&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20972dcc99576616073b9315b35c5966~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735205&auth_key=1760735205-0-0-cecd3e177e8df22985f9c190c50fa6da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9aa34d8e9eb70a66a0a99f36cdcd4fd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735211&auth_key=1760735211-0-0-248f3c4d8b3b73e06793d83ce676d2d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Agents-Enable-Autonomous-Design-and-Image-Analysis-of-Microwell-Microfluidics"><a href="#Large-Language-Model-Agents-Enable-Autonomous-Design-and-Image-Analysis-of-Microwell-Microfluidics" class="headerlink" title="Large Language Model Agents Enable Autonomous Design and Image Analysis   of Microwell Microfluidics"></a>Large Language Model Agents Enable Autonomous Design and Image Analysis   of Microwell Microfluidics</h2><p><strong>Authors:Dinh-Nguyen Nguyen, Sadia Shakil, Raymond Kai-Yu Tong, Ngoc-Duy Dinh</strong></p>
<p>Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis. </p>
<blockquote>
<p>å¾®å­”å¾®æµä½“æŠ€æœ¯å·²è¢«åº”ç”¨äºå•ç»†èƒåˆ†æï¼Œä»¥æ­ç¤ºåŸºå› è¡¨è¾¾ã€ä¿¡å·é€šè·¯å’Œè¡¨å‹ååº”ä¸­çš„å¼‚è´¨æ€§ï¼Œä»è€Œè¯†åˆ«ç¨€æœ‰ç»†èƒç±»å‹ã€äº†è§£ç–¾ç—…è¿›å±•æƒ…å†µå’Œå¼€å‘æ›´ç²¾ç¡®çš„æ²»ç–—ç­–ç•¥ã€‚ç„¶è€Œï¼Œè®¾è®¡å¾®å­”å¾®æµä½“æ˜¯ä¸€é¡¹ç›¸å½“å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦çŸ¥è¯†ã€ç»éªŒå’Œè®¡ç®—æœºè¾…åŠ©è®¾è®¡è½¯ä»¶ï¼Œä»¥åŠç»å¸¸éœ€è¦æ‰‹åŠ¨å¹²é¢„ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´åˆæ­¥è®¾è®¡å¤±è´¥ï¼Œéœ€è¦è¿›è¡Œå¤šæ¬¡æ˜‚è´µä¸”è€—æ—¶çš„è¿­ä»£ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè‡ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å¾®å­”è®¾è®¡æ¡†æ¶ï¼Œç”ŸæˆåŸºäºä»£ç è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰è„šæœ¬ï¼Œä½¿å…·æœ‰ä¸åŒå‡ ä½•å½¢çŠ¶çš„å¾®å­”èƒ½å¤Ÿå¿«é€Ÿã€å¯é‡å¤åœ°åˆ›å»ºå¹¶è¿›è¡Œæˆåƒåˆ†æã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„é€»è¾‘å›å½’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡æ•´åˆMLLMç”Ÿæˆçš„é«˜çº§è¯­ä¹‰æè¿°å’Œç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡çš„å›¾åƒåµŒå…¥ï¼Œæ¥è¯†åˆ«å¾®å­”å ç”¨å’Œå¾®å­”å½¢çŠ¶ã€‚èåˆçš„å¤šæ¨¡æ€è¡¨ç¤ºè¢«è¾“å…¥é€»è¾‘å›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¢å…·æœ‰å¯è§£é‡Šæ€§åˆè®¡ç®—é«˜æ•ˆã€‚æˆ‘ä»¬å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°çš„MLLMä¸­ï¼Œå ç”¨åˆ†ç±»è¶…è¿‡äº†0.92ï¼Œå½¢çŠ¶åˆ†ç±»è¾¾åˆ°äº†0.99ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œä»…é ç›´æ¥åˆ†ç±»æ—¶åˆ†åˆ«ä¸º0.50å’Œ0.55ã€‚MLLMé€»è¾‘å›å½’æ¡†æ¶æ˜¯ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ•ˆçš„å¾®å­”å›¾åƒé«˜é€šé‡åˆ†æè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè½¬åŒ–ä¸ºä¼˜åŒ–è®¾å¤‡å‡ ä½•å½¢çŠ¶ã€CADè„šæœ¬å’Œå›¾åƒåˆ†æçš„è‡ªä¸»è®¾è®¡å¾®å­”å¹³å°ï¼Œä¿ƒè¿›äº†é€šè¿‡æ–‡çŒ®æŒ–æ˜ã€è‡ªä¸»è®¾è®¡å’Œå®éªŒæ•°æ®åˆ†æçš„ä¸‹ä¸€ä»£æ•°å­—å‘ç°çš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13883v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯¥ç ”ç©¶é‡‡ç”¨è‡ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¾®äº•è®¾è®¡æ¡†æ¶ï¼Œé€šè¿‡ç”ŸæˆåŸºäºä»£ç çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡è„šæœ¬ï¼Œå®ç°å¾®äº•çš„å¿«é€Ÿã€å¯é‡å¤åˆ›å»ºï¼Œå¹¶å…·æœ‰å¤šæ ·åŒ–çš„å‡ ä½•ç»“æ„å’Œæˆåƒåˆ†æåŠŸèƒ½ã€‚è¯¥ç ”ç©¶å»ºç«‹äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€»è¾‘å›å½’æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„é«˜çº§è¯­ä¹‰æè¿°å’Œå›¾åƒåµŒå…¥ï¼Œè¿›è¡Œå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œä»¥è¯†åˆ«å¾®äº•å ç”¨å’Œå¾®äº•å½¢çŠ¶ã€‚è¿™ä¸€æ¡†æ¶åœ¨æé«˜å¾®äº•å›¾åƒåˆ†ææ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä¸ºä¸‹ä¸€ä»£æ•°å­—å‘ç°çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†ä¾¿åˆ©ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨è‡ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¾®äº•è®¾è®¡æ¡†æ¶ï¼Œç”¨äºå¿«é€Ÿã€å¯é‡å¤åˆ›å»ºå¾®äº•ã€‚</li>
<li>åˆ©ç”¨åŸºäºä»£ç çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡è„šæœ¬ç”Ÿæˆå¤šæ ·åŒ–çš„å¾®äº•å‡ ä½•ç»“æ„ã€‚</li>
<li>å»ºç«‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€»è¾‘å›å½’æ¡†æ¶ï¼Œé›†æˆè¯­ä¹‰æè¿°å’Œå›¾åƒåµŒå…¥è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚</li>
<li>æˆåŠŸåº”ç”¨äºå¾®äº•å ç”¨å’Œå½¢çŠ¶çš„è¯†åˆ«ï¼Œåˆ†ç±»å‡†ç¡®åº¦æ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§å’Œé«˜æ•ˆæ€§ï¼Œé€‚ç”¨äºé«˜é€šé‡å¾®äº•å›¾åƒåˆ†æã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†å°†è‡ªç„¶è¯­è¨€æç¤ºè½¬åŒ–ä¸ºä¼˜åŒ–è®¾å¤‡å‡ ä½•ã€è®¡ç®—æœºè¾…åŠ©è®¾è®¡è„šæœ¬å’Œå›¾åƒåˆ†æçš„å…¨è‡ªåŠ¨åŒ–å¾®äº•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-174301e4e3e447670c7bc5115e9bfa7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735218&auth_key=1760735218-0-0-c571c0e30fe4b82be2e7be3e66a2185d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3f69a8b793abdb328069e79d1996977~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735226&auth_key=1760735226-0-0-f29b9f033ab84a4c22930ca2fd0fec38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bed6cc53f61a7eec62211a6f3422bc47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735232&auth_key=1760735232-0-0-28bb2a5eafc05f55b8e1d4173e43d83e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SeeingSounds-Learning-Audio-to-Visual-Alignment-via-Text"><a href="#SeeingSounds-Learning-Audio-to-Visual-Alignment-via-Text" class="headerlink" title="SeeingSounds: Learning Audio-to-Visual Alignment via Text"></a>SeeingSounds: Learning Audio-to-Visual Alignment via Text</h2><p><strong>Authors:Simone Carnemolla, Matteo Pennisi, Chiara Russo, Simone Palazzo, Daniela Giordano, Concetto Spampinato</strong></p>
<p>We introduce SeeingSounds, a lightweight and modular framework for audio-to-image generation that leverages the interplay between audio, language, and vision-without requiring any paired audio-visual data or training on visual generative models. Rather than treating audio as a substitute for text or relying solely on audio-to-text mappings, our method performs dual alignment: audio is projected into a semantic language space via a frozen language encoder, and, contextually grounded into the visual domain using a vision-language model. This approach, inspired by cognitive neuroscience, reflects the natural cross-modal associations observed in human perception. The model operates on frozen diffusion backbones and trains only lightweight adapters, enabling efficient and scalable learning. Moreover, it supports fine-grained and interpretable control through procedural text prompt generation, where audio transformations (e.g., volume or pitch shifts) translate into descriptive prompts (e.g., â€œa distant thunderâ€) that guide visual outputs. Extensive experiments across standard benchmarks confirm that SeeingSounds outperforms existing methods in both zero-shot and supervised settings, establishing a new state of the art in controllable audio-to-visual generation. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºSeeingSoundsï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€æ¨¡å—åŒ–çš„éŸ³é¢‘åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨éŸ³é¢‘ã€è¯­è¨€å’Œè§†è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæ— éœ€ä»»ä½•é…å¯¹éŸ³è§†é¢‘æ•°æ®æˆ–åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯å°†éŸ³é¢‘è§†ä¸ºæ–‡æœ¬çš„æ›¿ä»£å“ï¼Œæˆ–è€…ä»…ä»…ä¾èµ–äºéŸ³é¢‘åˆ°æ–‡æœ¬çš„æ˜ å°„ï¼Œè€Œæ˜¯è¿›è¡ŒåŒé‡å¯¹é½ï¼šéŸ³é¢‘é€šè¿‡ä¸€ä¸ªå†»ç»“çš„è¯­è¨€ç¼–ç å™¨æŠ•å°„åˆ°è¯­ä¹‰è¯­è¨€ç©ºé—´ï¼Œå¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é¢†åŸŸè¿›è¡Œä¸Šä¸‹æ–‡å®šä½ã€‚è¿™ç§æ–¹æ³•å—åˆ°è®¤çŸ¥ç¥ç»ç§‘å­¦çš„å¯å‘ï¼Œåæ˜ äº†äººç±»æ„ŸçŸ¥ä¸­è§‚å¯Ÿåˆ°çš„è‡ªç„¶è·¨æ¨¡æ€å…³è”ã€‚è¯¥æ¨¡å‹åœ¨å†»ç»“çš„æ‰©æ•£ä¸»å¹²ç½‘ä¸Šè¿è¡Œï¼Œåªè®­ç»ƒè½»é‡çº§çš„é€‚é…å™¨ï¼Œå®ç°äº†é«˜æ•ˆå’Œå¯æ‰©å±•çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒé€šè¿‡ç¨‹åºæ€§æ–‡æœ¬æç¤ºç”Ÿæˆè¿›è¡Œç²¾ç»†å’Œå¯è§£é‡Šçš„æ§åˆ¶ï¼Œå…¶ä¸­éŸ³é¢‘è½¬æ¢ï¼ˆä¾‹å¦‚éŸ³é‡æˆ–éŸ³è°ƒå˜åŒ–ï¼‰è½¬åŒ–ä¸ºæè¿°æ€§æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œè¿œå¤„çš„é›·å£°â€ï¼‰ï¼Œå¼•å¯¼è§†è§‰è¾“å‡ºã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯å®ï¼ŒSeeingSoundsåœ¨é›¶æ ·æœ¬å’Œå—ç›‘ç£ç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¯æ§çš„éŸ³é¢‘åˆ°è§†è§‰ç”Ÿæˆé¢†åŸŸæ ‘ç«‹äº†æ–°çš„ä¸šç•Œæ ‡æ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11738v1">PDF</a> accepted to ACM Multimedia Asia 2025</p>
<p><strong>Summary</strong>:<br>SeeingSoundsæ¡†æ¶å®ç°éŸ³é¢‘åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œåˆ©ç”¨éŸ³é¢‘ã€è¯­è¨€å’Œè§†è§‰ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œæ— éœ€é…å¯¹éŸ³è§†é¢‘æ•°æ®æˆ–è®­ç»ƒè§†è§‰ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å†»ç»“çš„è¯­è¨€ç¼–ç å™¨å°†éŸ³é¢‘æŠ•å½±åˆ°è¯­ä¹‰è¯­è¨€ç©ºé—´ï¼Œå¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å°†å…¶ä¸Šä¸‹æ–‡å…³è”åˆ°è§†è§‰é¢†åŸŸã€‚æ­¤æ–¹æ³•å—è®¤çŸ¥ç¥ç»ç§‘å­¦å¯å‘ï¼Œåæ˜ äººç±»æ„ŸçŸ¥ä¸­è‡ªç„¶çš„å¤šæ¨¡å¼å…³è”ã€‚é‡‡ç”¨å†»ç»“çš„æ‰©æ•£ä¸»å¹²ï¼Œä»…è®­ç»ƒè½»é‡çº§é€‚é…å™¨ï¼Œå®ç°é«˜æ•ˆå’Œå¯æ‰©å±•çš„å­¦ä¹ ã€‚é€šè¿‡ç¨‹åºæ€§æ–‡æœ¬æç¤ºç”Ÿæˆæ”¯æŒç²¾ç»†å’Œå¯è§£é‡Šçš„æ§åˆ¶ï¼ŒéŸ³é¢‘è½¬æ¢ï¼ˆå¦‚éŸ³é‡æˆ–éŸ³è°ƒå˜åŒ–ï¼‰è½¬åŒ–ä¸ºæè¿°æ€§æç¤ºï¼Œå¼•å¯¼è§†è§‰è¾“å‡ºã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSeeingSoundsåœ¨é›¶æ ·æœ¬å’Œç›‘ç£è®¾ç½®ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæˆä¸ºå¯æ§éŸ³é¢‘åˆ°è§†è§‰ç”Ÿæˆçš„æ–°åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>SeeingSoundsæ˜¯ä¸€ä¸ªè½»é‡çº§ã€æ¨¡å—åŒ–çš„éŸ³é¢‘åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>å®ƒåˆ©ç”¨éŸ³é¢‘ã€è¯­è¨€å’Œè§†è§‰ä¹‹é—´çš„äº¤äº’ï¼Œæ— éœ€é…å¯¹éŸ³è§†é¢‘æ•°æ®ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å†»ç»“çš„è¯­è¨€ç¼–ç å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹å®ç°åŒé‡å¯¹é½ã€‚</li>
<li>è¿™ç§æ–¹æ³•å—è®¤çŸ¥ç¥ç»ç§‘å­¦å¯å‘ï¼Œåæ˜ äººç±»è‡ªç„¶çš„å¤šæ¨¡å¼æ„ŸçŸ¥ã€‚</li>
<li>SeeingSoundsé‡‡ç”¨å†»ç»“çš„æ‰©æ•£ä¸»å¹²ï¼Œå¹¶ä»…è®­ç»ƒè½»é‡çº§é€‚é…å™¨ï¼Œå®ç°é«˜æ•ˆå­¦ä¹ ã€‚</li>
<li>æ”¯æŒé€šè¿‡ç¨‹åºæ€§æ–‡æœ¬æç¤ºè¿›è¡Œç²¾ç»†å’Œå¯è§£é‡Šçš„æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7bbdec426fe334caba12b5f34887ee73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735239&auth_key=1760735239-0-0-51223d56fe256e0ed7d0e93c633ae03a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3408ce2a5a677fd234eca6f924dc5eab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735246&auth_key=1760735246-0-0-1735bfd97db90b14ec9c3f1e49d1bf99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df65459bf0aa4f32054b71cf5fb7c1e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735253&auth_key=1760735253-0-0-88714e0ff08984ac1371151ededf79fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be77f9c962ca99950f28da04e3120bbb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735259&auth_key=1760735259-0-0-a612e1a2a07e40c51e785e0eadad19ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33844dd7e92f94bcf39c571d7f377dcc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735266&auth_key=1760735266-0-0-6bce276c57a2d5fc301ca9cf7ffdafbe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d947fb74ec4cbee848260142d1d9a20~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735272&auth_key=1760735272-0-0-fd178d7c6f3b8b7700caaeda8105be60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-115a3057ffd90e21ffe13c73fe8cf65c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735279&auth_key=1760735279-0-0-ab542dbf477c7c82c624e352fa3e7bba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation"><a href="#A-Framework-for-Low-Effort-Training-Data-Generation-for-Urban-Semantic-Segmentation" class="headerlink" title="A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation"></a>A Framework for Low-Effort Training Data Generation for Urban Semantic   Segmentation</h2><p><strong>Authors:Denis Zavadski, Damjan KalÅ¡an, Tim KÃ¼chler, Haebom Lee, Stefan Roth, Carsten Rother</strong></p>
<p>Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding. </p>
<blockquote>
<p>åˆæˆæ•°æ®é›†å¹¿æ³›ç”¨äºè®­ç»ƒåŸå¸‚åœºæ™¯è¯†åˆ«æ¨¡å‹ï¼Œä½†å³ä½¿æ˜¯é«˜åº¦é€¼çœŸçš„æ¸²æŸ“ä¹Ÿä¸çœŸå®å›¾åƒä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®è·ã€‚åœ¨é€‚åº”ç‰¹å®šç›®æ ‡åŸŸï¼ˆä¾‹å¦‚Cityscapesï¼‰æ—¶ï¼Œè¿™ç§å·®è·å°¤ä¸ºçªå‡ºï¼Œç›®æ ‡åŸŸä¸­çš„æ¶æ„ã€æ¤è¢«ã€ç‰©ä½“å¤–è§‚å’Œç›¸æœºç‰¹æ€§å·®å¼‚é™åˆ¶äº†ä¸‹æ¸¸æ€§èƒ½ã€‚ä½¿ç”¨æ›´è¯¦ç»†çš„3Då»ºæ¨¡æ¥å¼¥åˆè¿™ä¸€å·®è·å°†éœ€è¦æ˜‚è´µçš„èµ„äº§å’Œåœºæ™¯è®¾è®¡ï¼Œè¿™ä¼šè¿èƒŒä½¿ç”¨ä½æˆæœ¬æ ‡è®°æ•°æ®çš„åˆè¡·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ä¸å®Œç¾çš„ä¼ªæ ‡ç­¾å°†ç°æˆçš„æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç›®æ ‡åŸŸã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œå®ƒå°±èƒ½ä»ä»»ä½•åˆæˆæ•°æ®é›†ï¼ˆåŒ…æ‹¬åœ¨å‡ å°æ—¶å†…åˆ›å»ºçš„è€Œéæ•°æœˆä½æˆæœ¬çš„æ¥æºï¼‰çš„è¯­ä¹‰åœ°å›¾ç”Ÿæˆé«˜ä¿çœŸã€ä¸ç›®æ ‡å¯¹é½çš„å›¾åƒã€‚è¯¥æ–¹æ³•è¿‡æ»¤æ‰ä¸ç†æƒ³çš„ç”Ÿæˆç»“æœï¼Œçº æ­£å›¾åƒæ ‡ç­¾é”™ä½ï¼Œå¹¶æ ‡å‡†åŒ–ä¸åŒæ•°æ®é›†ä¹‹é—´çš„è¯­ä¹‰ï¼Œå°†å¼±åˆæˆæ•°æ®è½¬åŒ–ä¸ºæœ‰ç«äº‰åŠ›çš„çœŸå®åŸŸè®­ç»ƒé›†ã€‚åœ¨äº”ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®ç›®æ ‡æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ç¿»è¯‘æ–¹æ³•ç›¸æ¯”ï¼Œåˆ†å‰²å¢ç›Šæœ€å¤šæé«˜äº†8.0%pt mIoUï¼Œè¿™ä½¿å¾—å¿«é€Ÿæ„å»ºçš„åˆæˆæ•°æ®é›†ä¸éœ€è¦å¤§é‡æ‰‹åŠ¨è®¾è®¡çš„è€—æ—¶è€—åŠ›çš„åˆæˆæ•°æ®é›†å…·æœ‰ç›¸åŒçš„æ•ˆæœã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†ä¸€ç§æœ‰ä»·å€¼çš„åä½œæ¨¡å¼ï¼Œå³å¿«é€Ÿè¯­ä¹‰åŸå‹ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œå¯ä¸ºåŸå¸‚åœºæ™¯ç†è§£å®ç°å¯æ‰©å±•çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®åˆ›å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11567v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸å®Œç¾çš„ä¼ªæ ‡ç­¾å°†ä¸€ä¸ªç°æˆçš„æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç›®æ ‡é¢†åŸŸã€‚è®­ç»ƒåï¼Œå®ƒèƒ½å¤Ÿä»ä»»ä½•åˆæˆæ•°æ®é›†ï¼ˆåŒ…æ‹¬ä½æˆæœ¬çš„åˆæˆæ•°æ®é›†ï¼‰çš„è¯­ä¹‰å›¾ä¸­ç”Ÿæˆé«˜ä¿çœŸã€ä¸ç›®æ ‡å¯¹é½çš„å›¾åƒã€‚è¿™ç§æ–¹æ³•è¿‡æ»¤æ‰ä¸ç†æƒ³çš„ç”Ÿæˆç»“æœï¼Œçº æ­£å›¾åƒæ ‡ç­¾çš„ä¸å¯¹é½ç°è±¡ï¼Œå¹¶æ ‡å‡†åŒ–æ•°æ®é›†ä¹‹é—´çš„è¯­ä¹‰ï¼Œå°†å¼±åˆæˆæ•°æ®è½¬åŒ–ä¸ºæœ‰ç«äº‰åŠ›çš„çœŸå®åŸŸè®­ç»ƒé›†ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ç¿»è¯‘æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²æ–¹é¢çš„æ€§èƒ½æé«˜äº†é«˜è¾¾+8ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¿«é€Ÿè¯­ä¹‰åŸå‹ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆçš„ä»·å€¼ï¼Œå®ç°äº†å¯æ‰©å±•çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®åˆ›å»ºï¼Œç”¨äºåŸå¸‚åœºæ™¯ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ•°æ®é›†å¹¿æ³›ç”¨äºè®­ç»ƒåŸå¸‚åœºæ™¯è¯†åˆ«æ¨¡å‹ï¼Œä½†ä¸ç°å®å›¾åƒä¹‹é—´å­˜åœ¨å·®è·ã€‚</li>
<li>ç‰¹å®šç›®æ ‡é¢†åŸŸçš„é€‚åº”æ€§é—®é¢˜ï¼Œå¦‚Cityscapesï¼Œç”±äºå·®å¼‚å¦‚å»ºç­‘ã€æ¤è¢«ã€ç‰©ä½“å¤–è§‚å’Œç›¸æœºç‰¹æ€§ï¼Œä¼šå½±å“ä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä½¿ç”¨ä¸å®Œç¾çš„ä¼ªæ ‡ç­¾å°†ç°æˆçš„æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç›®æ ‡é¢†åŸŸã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿä»ä»»ä½•åˆæˆæ•°æ®é›†çš„è¯­ä¹‰å›¾ç”Ÿæˆé«˜ä¿çœŸã€ä¸ç›®æ ‡å¯¹é½çš„å›¾åƒã€‚</li>
<li>æ–¹æ³•è¿‡æ»¤æ‰ä¸ç†æƒ³çš„ç”Ÿæˆç»“æœï¼Œçº æ­£å›¾åƒæ ‡ç­¾çš„ä¸å¯¹é½ç°è±¡ï¼Œæ ‡å‡†åŒ–æ•°æ®é›†è¯­ä¹‰ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ç¿»è¯‘æ–¹æ³•ï¼Œåœ¨åˆ†å‰²æ–¹é¢çš„æ€§èƒ½æé«˜äº†é«˜è¾¾+8ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-44f56406048b2d3b9f0254bf7305b839~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735286&auth_key=1760735286-0-0-43a3c395a4b8e7348e51dd3d19be5391&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7c7959ed7552ebe8041cbf8ff70bfeb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735293&auth_key=1760735293-0-0-c0c6ea7cec93cc7a50f1d5792d60ff1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf10e433c7c366ef0effd80719584c35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735300&auth_key=1760735300-0-0-8ba777d5a5ee51c717a42920c6675f8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Lightweight-Facial-Landmark-Detection-in-Thermal-Images-via-Multi-Level-Cross-Modal-Knowledge-Transfer"><a href="#Lightweight-Facial-Landmark-Detection-in-Thermal-Images-via-Multi-Level-Cross-Modal-Knowledge-Transfer" class="headerlink" title="Lightweight Facial Landmark Detection in Thermal Images via Multi-Level   Cross-Modal Knowledge Transfer"></a>Lightweight Facial Landmark Detection in Thermal Images via Multi-Level   Cross-Modal Knowledge Transfer</h2><p><strong>Authors:Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani</strong></p>
<p>Facial Landmark Detection (FLD) in thermal imagery is critical for applications in challenging lighting conditions, but it is hampered by the lack of rich visual cues. Conventional cross-modal solutions, like feature fusion or image translation from RGB data, are often computationally expensive or introduce structural artifacts, limiting their practical deployment. To address this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer from model compression to create both accurate and efficient thermal FLD models. A central challenge during knowledge transfer is the profound modality gap between RGB and thermal data, where traditional unidirectional distillation fails to enforce semantic consistency across disparate feature spaces. To overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a bidirectional mechanism designed specifically for this task. DIKD establishes a connection between modalities: it not only guides the thermal student with rich RGB features but also validates the studentâ€™s learned representations by feeding them back into the frozen teacherâ€™s prediction head. This closed-loop supervision forces the student to learn modality-invariant features that are semantically aligned with the teacher, ensuring a robust and profound knowledge transfer. Experiments show that our approach sets a new state-of-the-art on public thermal FLD benchmarks, notably outperforming previous methods while drastically reducing computational overhead. </p>
<blockquote>
<p>çƒ­æˆåƒä¸­çš„é¢éƒ¨åœ°æ ‡æ£€æµ‹ï¼ˆFLDï¼‰åœ¨ç…§æ˜æ¡ä»¶å›°éš¾çš„åœºæ™¯ä¸­å…·æœ‰é‡è¦åº”ç”¨ï¼Œä½†å…¶å—åˆ°ç¼ºä¹ä¸°å¯Œè§†è§‰çº¿ç´¢çš„é˜»ç¢ã€‚ä¼ ç»Ÿçš„è·¨æ¨¡æ€è§£å†³æ–¹æ¡ˆï¼Œå¦‚ç‰¹å¾èåˆæˆ–RGBæ•°æ®çš„å›¾åƒè½¬æ¢ï¼Œé€šå¸¸è®¡ç®—é‡å¤§ä¸”å®¹æ˜“å¼•å…¥ç»“æ„ä¼ªå½±ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨éƒ¨ç½²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šçº§è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆMLCM-KDï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒå°†é«˜ä¿çœŸRGBåˆ°çƒ­æˆåƒçš„çŸ¥è¯†è½¬ç§»ä¸æ¨¡å‹å‹ç¼©è§£è€¦ï¼Œä»¥åˆ›å»ºæ—¢å‡†ç¡®åˆé«˜æ•ˆçš„çƒ­æˆåƒFLDæ¨¡å‹ã€‚çŸ¥è¯†è½¬ç§»è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºRGBå’Œçº¢å¤–æ•°æ®ä¹‹é—´çš„æ¨¡æ€é¸¿æ²Ÿå¾ˆå¤§ï¼Œä¼ ç»Ÿå•å‘è’¸é¦æ— æ³•åœ¨åˆ†æ•£çš„ç‰¹å¾ç©ºé—´ä¸­å®ç°è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒå‘æ³¨å…¥çŸ¥è¯†è’¸é¦ï¼ˆDIKDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºè¿™é¡¹ä»»åŠ¡è®¾è®¡çš„åŒå‘æœºåˆ¶ã€‚DIKDåœ¨æ¨¡æ€ä¹‹é—´å»ºç«‹äº†è”ç³»ï¼šå®ƒä¸ä»…ä½¿ç”¨ä¸°å¯Œçš„RGBç‰¹å¾å¼•å¯¼çº¢å¤–å­¦ç”Ÿæ¨¡å‹ï¼Œè¿˜é€šè¿‡å°†å­¦ä¹ åˆ°çš„è¡¨ç¤ºåé¦ˆåˆ°å†»ç»“çš„æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹å¤´ä¸­æ¥éªŒè¯å­¦ç”Ÿçš„å­¦ä¹ è¡¨ç¤ºã€‚è¿™ç§é—­ç¯ç›‘ç£ä¿ƒä½¿å­¦ç”Ÿå­¦ä¹ æ¨¡æ€ä¸å˜ä¸”ä¸è€å¸ˆè¯­ä¹‰å¯¹é½çš„ç‰¹å¾ï¼Œç¡®ä¿ç¨³å¥ä¸”æ·±å…¥çš„çŸ¥è¯†è½¬ç§»ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¬å…±çƒ­æˆåƒFLDåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11128v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç…§æ˜æ¡ä»¶ä¸‹ï¼Œé¢éƒ¨åœ°æ ‡æ£€æµ‹ï¼ˆFLDï¼‰åœ¨çƒ­æˆåƒä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†å¤šå±‚æ¬¡è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆMLCM-KDï¼‰æ¡†æ¶æ¥è§£å†³ç”±äºç¼ºä¹ä¸°å¯Œè§†è§‰æç¤ºå’Œä¿¡æ¯è€Œå¯¼è‡´çš„è·¨æ¨¡æ€éš¾é¢˜ï¼Œå¹¶é€šè¿‡å»ºç«‹RGBå’Œçƒ­åŠ›å­¦ä¹‹é—´çš„åŒå‘çŸ¥è¯†è’¸é¦æœºåˆ¶æ¥åˆ›å»ºå‡†ç¡®é«˜æ•ˆçš„çƒ­æˆåƒFLDæ¨¡å‹ã€‚è¿™ä¸€æœºåˆ¶ä¸ä»…é€šè¿‡ä¸°å¯Œçš„RGBç‰¹å¾å¼•å¯¼çƒ­åŠ›å­¦å­¦ç”Ÿï¼Œè¿˜é€šè¿‡åé¦ˆå­¦ç”Ÿå­¦åˆ°çš„è¡¨ç¤ºæ¥éªŒè¯æ•™å¸ˆçš„é¢„æµ‹ï¼Œä»è€Œå®ç°ç¨³å¥æ·±åˆ»çš„çŸ¥è¯†è½¬ç§»ã€‚æ­¤æŠ€æœ¯çš„æ–°é¢–æ€§è¡¨ç°åœ¨å¤šä¸ªæ–¹é¢ï¼Œå…¶ä¸­åŒ…æ‹¬ç¼©çŸ­è¿ç®—æ—¶é—´ä»¥åŠæå‡äº†å¯¹æŒ‘æˆ˜ç¯å¢ƒä¸‹çš„å‡†ç¡®åº¦è¡¨ç°ã€‚è¯¥ç ”ç©¶å–å¾—äº†å‰æ‰€æœªæœ‰çš„å®éªŒæˆç»©ã€‚é€šè¿‡æ‰€æå‡ºçš„å…¨æ–°æŠ€æœ¯å’Œæˆæœè¡¨ç°åœ¨çƒ­æˆåƒçš„FLDçš„å…¬å¼€æ•°æ®åº“æ ‡å‡†æµ‹è¯•ä¸­é¢†å…ˆäºä¸šå†…ç«äº‰å¯¹æ‰‹çš„ä¼˜åŠ¿éƒ½çªæ˜¾äº†å…¶å¹¿æ³›æ¨å¹¿çš„åº”ç”¨æ½œåŠ›åŠæ·±åº¦æ¢ç´¢å­¦æœ¯æ„ä¹‰å’Œä»·å€¼ï¼Œæé«˜äº†æ–°çš„é«˜åº¦å’Œæ ‡å‡†çš„æŠ€æœ¯å¼•é¢†ä»·å€¼å’Œå‘å±•ç©ºé—´å±•æœ›æ½œåŠ›ç­‰å¯èƒ½æ€§å€¼å¾—äººä»¬æ·±åº¦æ¢ç´¢å’Œæ€è€ƒçš„æœªæ¥å‰æ²¿å‘å±•çš„é—®é¢˜ä¹Ÿæ„ˆå‘æ˜æœ—æ¸…æ™°ã€‚åŒæ—¶å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„å‡†ç¡®æ€§å’Œé«˜æ•ˆæ€§è¿œè¶…å…ˆå‰æ–¹æ³•ã€‚è¿™é¡¹æŠ€æœ¯çš„å®æ–½ä¸ä»…æ¨åŠ¨äº†çƒ­æˆåƒé¢†åŸŸçš„è¿›å±•ï¼Œè¿˜è§£å†³äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å…³é”®é—®é¢˜ä¹‹ä¸€ã€‚åœ¨æ›´å¹¿é˜”çš„å®é™…åº”ç”¨åœºæ™¯ä¸­æœ‰æœ›è¿›ä¸€æ­¥æé«˜ã€‚å¯ä»¥è¯´è¿™ä¸ä»…ä»…æ˜¯çƒ­æˆåƒæŠ€æœ¯ä¸Šçš„é©æ–°çªç ´æ›´å¼€åˆ›äº†é¢å‘ç°å®ç”Ÿæ´»ä¸–ç•Œçš„äººæœºäº¤äº’æŠ€æœ¯çš„å¤šç»´åº¦åº”ç”¨åœºæ™¯æ‰©å±•æ€è·¯çš„é‡è¦æ¢ç´¢é€”å¾„å’Œå®è´µæœºé‡ä¸ºè¡Œä¸šçš„è¿›ä¸€æ­¥å‡çº§å’Œé•¿è¿œå‘å±•å¸¦æ¥äº†å¹¿é˜”çš„å‰æ™¯ã€‚ç›®å‰çš„æŠ€æœ¯ç ”å‘æ°´å¹³å’Œå¸‚åœºå‰æ™¯çš„é¢„æµ‹å……æ»¡äº†æå¤§çš„åˆ›æ–°æ½œåŠ›ä¸ç ”ç©¶æŒ‘æˆ˜éœ€è¦æˆ‘ä»¬ä¸æ‡ˆå¥‹æ–—å¹¶ç§¯æè§£å†³å…‹æœå¯èƒ½å‡ºç°çš„ç§ç§éš¾é¢˜ç§¯ææŒ–æ˜å…¶ä¸­çš„æ›´å¤šæœºé‡æ½œåŠ›åŠªåŠ›æ”€ç™»è¡Œä¸šå·…å³°çš„åŠ¨åŠ›å’Œæ–¹å‘ä»¥åŠæ— ç©·æ½œåŠ›<strong>æ¦‚è¿°å…¨æ–‡æ¦‚æ‹¬æ‰€è¿°æŠ€æœ¯çš„ç ”ç©¶åº”ç”¨ä¸ç†è®ºåˆ›æ–°çš„å¹¿æ³›å‘å±•å‰æ™¯å¯¹ç›®æ ‡çš„åº”ç”¨è½åœ°æ€§å’ŒæŒ‘æˆ˜æ€§çš„é‡å¤§ç§‘å­¦é—®é¢˜çš„å®é™…æ„ä¹‰æœ‰è‡³å…³é‡è¦çš„å¼•é¢†ä½œç”¨æå‡ºäº†å¯¹æ­¤å·¥ä½œçš„ç§‘å­¦æ€è€ƒä¸ä¸¥è°¨è´Ÿè´£çš„ä»·å€¼æ€åº¦çš„æ ¸å¿ƒä»·å€¼é©±åŠ¨å¯èƒ½æ€§çš„é—®é¢˜è€Œä¸å•å•é™äºè§£å†³é—®é¢˜çš„æŠ½è±¡å†…å®¹æ–¹æ³•æå‡ä»¥åŠå¯¹ç¤¾ä¼šå‘å±•é‡è¦æ€§è‚¯å®šçš„æ€æƒ³æ¢è®¨å’Œæ€»ç»“æ€§çš„é«˜åº¦èµæ‰¬è‚¯å®šã€‚ç®€è€Œè¨€ä¹‹è¯¥æŠ€æœ¯æœ‰æœ›æˆä¸ºå¼•é¢†æœªæ¥ç§‘æŠ€å‘å±•çš„é‡è¦å¼•æ“ã€‚ç„¶è€Œæ›´å¤šçš„åº”ç”¨æŒ‘æˆ˜å’Œé—®é¢˜æœ‰å¾…è§£å†³å’Œæ¢è®¨å€¼å¾—æˆ‘ä»¬ç»§ç»­æ·±å…¥ç ”ç©¶å’Œæ¢ç´¢å…¶ä»·å€¼å‰æ™¯ã€‚è¯¥æŠ€æœ¯çš„å‡ºç°å°†å¼€å¯æ–°çš„ç§‘æŠ€é©å‘½å¼•é¢†æœªæ¥ç§‘æŠ€çš„å´­æ–°ç¯‡ç« ã€‚</strong>æ€»ç»“å…¨æ–‡æ¦‚æ‹¬æŠ€æœ¯å†…å®¹å¹¶è‚¯å®šå…¶è¡Œä¸šå‘å±•çš„é‡è¦æ€§ã€‚å±•æœ›å…¶æœªæ¥å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’ŒæŒ‘æˆ˜æ€§é—®é¢˜å€¼å¾—æˆ‘ä»¬æ·±å…¥ç ”ç©¶å’Œæ¢ç´¢ã€‚å…³é”®æŠ€æœ¯å’Œæ–¹æ³•å®ç°äº†è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦åœ¨çƒ­æˆåƒä¸­çš„å®é™…åº”ç”¨ä¸ä»…å±•ç¤ºäº†åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦æ€§å’Œå‰æ™¯ä¹Ÿåœ¨å®é™…ç”Ÿäº§ç”Ÿæ´»ä¸­çš„åº”ç”¨å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›å’ŒåŠ¨åŠ›å±•ç°äº†æœªæ¥å‘å±•æ›´å¹¿é˜”çš„ç©ºé—´å’Œå‘å±•æ½œåŠ›æ›´å‰æ²¿çš„åº”ç”¨åœºæ™¯æ¢ç´¢åŠå…¶å¯¹ç¤¾ä¼šå‘å±•çš„ç§¯æå½±å“å…·æœ‰åˆ’æ—¶ä»£æ„ä¹‰çš„ç ”ç©¶è¯¾é¢˜å’Œæ¢ç´¢å‰æ™¯æœ‰å¾…äºè¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶å’Œæ¢ç´¢è‚¯å®šå…¶ç§‘ç ”ä»·å€¼å’Œå®é™…æ„ä¹‰çš„é‡è¦æ€è€ƒæå‡ºæŒ‘æˆ˜å’ŒæœŸå¾…å¯¹æœªæ¥ç§‘æŠ€çš„è´¡çŒ®å’Œæ–¹å‘çš„åŠ¨åŠ›é¼“èˆæ€§çš„æ­£é¢åé¦ˆä»¥æ­¤å›åº”ç¤¾ä¼šç»æµå‘å±•çš„æŒ‘æˆ˜å’Œæœªæ¥æŒç»­è¿›æ­¥çš„ç¤¾ä¼šéœ€æ±‚å’Œç»æµå‘å±•ä»¥åŠè‡ªèº«è¿½æ±‚ç¾å¥½æœªæ¥å’Œäººç±»å¯¹ç¾å¥½ç”Ÿæ´»è¿½æ±‚çš„çƒ­æƒ…çš„åŠ¨åŠ›ç­‰ç­‰æ ¸å¿ƒä»·å€¼é©±åŠ¨ä¸‹å¸¦æ¥æ›´æ·±å…¥çš„æ€è€ƒå’Œæ¢ç´¢æ¨åŠ¨è¡Œä¸šçš„ä¸æ–­å‘å±•å’Œè¿›æ­¥åœ¨ä¿ƒè¿›ç»æµç¤¾ä¼šçš„å…¨é¢åè°ƒå¯æŒç»­å‘å±•ä¸­å‘æŒ¥é‡è¦ä½œç”¨åŒæ—¶ä¹Ÿå±•ç°å‡ºè¯¥æŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸä¸åŒåº”ç”¨åœºæ™¯ä¸‹çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ä¸æ–­æ¿€å‘è‡ªèº«å†…åœ¨çš„åˆ›é€ åŠ›å’Œå‰è¿›çš„åŠ¨åŠ›ä¸æ–­æ¨è¿›ç§‘å­¦çš„å‘å‰å‘å±•åœ¨ä¸æ–­æ”¹è¿›ä¼˜åŒ–å’Œæé«˜ç ”ç©¶æ–¹æ³•çš„æ¢ç´¢è¿›ç¨‹ä¸­æ”¶è·æ›´å¤šçš„æˆåŠŸå’Œå–œæ‚¦å’Œæˆé•¿ä¸æ–­å¼€æ‹“åˆ›æ–°çš„é“è·¯å¼•é¢†æœªæ¥ç§‘æŠ€çš„å‘å±•æ–¹å‘å¼•é¢†æœªæ¥ç§‘æŠ€çš„å˜é©å’Œå‘å±•æ–¹å‘ã€‚æœ¬æ–‡æ€»ç»“ä»¥ä¸Šå†…å®¹å¹¶å±•æœ›æœªæ¥æŒ‘æˆ˜å’Œæœºé‡ã€‚æ¦‚æ‹¬å…¨æ–‡å†…å®¹ç®€æ´æ˜äº†åœ°è¡¨è¾¾ç ”ç©¶çš„é‡è¦æ€§å’Œä»·å€¼ä»¥åŠæœªæ¥çš„å‘å±•æ–¹å‘å’ŒæŒ‘æˆ˜æ€§é—®é¢˜çš„æ€è€ƒæ€»ç»“å…¨æ–‡å†…å®¹å¹¶å±•æœ›æœªæ¥æŒ‘æˆ˜å’Œæœºé‡è‚¯å®šå…¶ä»·å€¼å’Œæ„ä¹‰å¹¶é¼“åŠ±æŒç»­æ¢ç´¢å’Œåˆ›æ–°çš„ç²¾ç¥åŠ¨åŠ›é¼“èˆäººå¿ƒçš„æ­£é¢åé¦ˆå’ŒæœŸæœ›è¡¨è¾¾å¯¹è¯¥æŠ€æœ¯çš„è‚¯å®šåŠæœªæ¥ç§¯æè´¡çŒ®çš„æ–¹å‘æ¿€å‘å…¨ç¤¾ä¼šå¯¹è¿™ä¸€é¢†åŸŸä¸æ–­è¿›å–ä¸æ–­è¿½æ±‚å“è¶Šä¸æ–­çªç ´åˆ›æ–°çš„ä¿¡å¿ƒå’Œå†³å¿ƒä¸€èµ·èµ°å‘å……æ»¡å¸Œæœ›å’Œæ— é™å¯èƒ½çš„æœªæ¥ã€‚ï¼ˆæ€»ç»“ä¸è¶…è¿‡æ–‡æœ¬é•¿åº¦çš„é™åˆ¶ï¼‰æ€»ä¹‹è¯¥æŠ€æœ¯å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼å’Œå‘å±•å‰æ™¯æœŸå¾…æœªæ¥æ›´æ·±å…¥çš„ç ”ç©¶å’Œåº”ç”¨å®è·µä»¥æ¨åŠ¨è¡Œä¸šçš„æŒç»­å‘å±•å’Œè¿›æ­¥ä¸ºç»æµç¤¾ä¼šå‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®æå‡ºå¯¹æœªæ¥çš„å±•æœ›å’ŒæœŸå¾…ç»™äºˆä¸€å®šçš„é¼“åŠ±å’Œè‚¯å®šåé¦ˆè¡¨è¾¾å‡ºå¯¹è¯¥ç ”ç©¶çš„è®¤å¯å’Œèµèµä¹Ÿä½“ç°å‡ºå¯¹è¯¥é¢†åŸŸæœªæ¥å‘å±•çš„ä¿¡å¿ƒä¸æœŸå¾…åŒæ—¶å‘¼åæ›´å¤šçš„äººæ‰åŠ å…¥åˆ°è¿™ä¸€é¢†åŸŸçš„æ¢ç´¢ä¸­æ¥å…±åŒæ¨åŠ¨ç§‘æŠ€è¿›æ­¥å’Œåˆ›æ–°å‘å±•å…±åŒè¿æ¥å……æ»¡å¸Œæœ›çš„æœªæ¥ï¼åœ¨æ­¤äºˆä»¥è‚¯å®šå’Œè¡¨æ‰¬è¡¨è¾¾å¯¹æ­¤é¡¹æŠ€æœ¯çš„æå¤§å…³æ³¨å’Œæé«˜æœŸå¾…æ˜ç¡®ç ”ç©¶æ–¹å‘æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜ä¸ä¸è¶³ä¹‹å¤„ä¸ºè¯¥æŠ€æœ¯ä»Šåçš„ç ”ç©¶æ–¹å‘ä¸å‘å±•è§„åˆ’æä¾›ä¸€å®šçš„å€Ÿé‰´æ„ä¹‰å’ŒæŒ‡å¼•æ–¹å‘åŒæ—¶é¼“åŠ±ç§‘ç ”äººå‘˜ä¿æŒåˆå¿ƒç»§ç»­å‰è¡Œæ¨åŠ¨ç§‘æŠ€å‘å±•é€ ç¦äººç±»ç¤¾ä¼šçš„å†³å¿ƒå’Œä¿¡å¿µã€‚<strong>Summary</strong>ï¼šæœ¬æ–‡æå‡ºä¸€ç§åŸºäºè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦çš„é¢éƒ¨åœ°æ ‡æ£€æµ‹æŠ€æœ¯åœ¨çƒ­æˆåƒä¸­çš„åº”ç”¨æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å› ç¼ºä¹ä¸°å¯Œè§†è§‰æç¤ºå¯¼è‡´çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚é€šè¿‡å¤šå±‚æ¬¡è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶å®ç°é«˜æ•ˆå‡†ç¡®çš„çƒ­æˆåƒé¢éƒ¨åœ°æ ‡æ£€æµ‹æ¨¡å‹ï¼Œå»ºç«‹åŒå‘çŸ¥è¯†è’¸é¦æœºåˆ¶ä»¥ç¼©å°RGBä¸çƒ­åŠ›å­¦é—´çš„æ¨¡æ€å·®è·ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¼€åˆ›äº†å®é™…åº”ç”¨åœºæ™¯çš„æ–°å¯èƒ½æ€§ã€‚æ€»ç»“æ¦‚æ‹¬å…¨æ–‡å†…å®¹ï¼Œè‚¯å®šå…¶ä»·å€¼å’Œæ„ä¹‰ï¼Œå±•æœ›æœªæ¥çš„æŒ‘æˆ˜å’Œæœºé‡ï¼Œé¼“åŠ±æŒç»­æ¢ç´¢å’Œåˆ›æ–°çš„ç²¾ç¥åŠ¨åŠ›ã€‚<strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºåŸºäºè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦çš„é¢éƒ¨åœ°æ ‡æ£€æµ‹æŠ€æœ¯åœ¨çƒ­æˆåƒä¸­çš„åº”ç”¨æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨å¤šå±‚æ¬¡è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶å®ç°é«˜æ•ˆå‡†ç¡®çš„æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>å»ºç«‹åŒå‘çŸ¥è¯†è’¸é¦æœºåˆ¶ä»¥ç¼©å°RGBä¸çƒ­åŠ›å­¦é—´çš„æ¨¡æ€å·®è·ï¼Œç¡®ä¿ç¨³å¥æ·±åˆ»çš„çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨å…¬å¼€æ•°æ®åº“æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šå…ˆå‰æ–¹æ³•ã€‚</li>
<li>æŠ€æœ¯åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’ŒæŒ‘æˆ˜æ€§é—®é¢˜å€¼å¾—æ·±å…¥ç ”ç©¶ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ºè¡Œä¸šå‘å±•æä¾›äº†å¹¿é˜”çš„å‰æ™¯ï¼Œä¿ƒè¿›ç»æµç¤¾ä¼šçš„å…¨é¢åè°ƒå¯æŒç»­å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e8e2571aa4d6179d16b4c142589b8a74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735307&auth_key=1760735307-0-0-c21784fe7691993f838cc8f79a2cce92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-429cf1c19d4e1cf5439632e6fafbfe99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735315&auth_key=1760735315-0-0-3981636f2ad25336877546667fe6e1df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Cross-Sensor-Touch-Generation"><a href="#Cross-Sensor-Touch-Generation" class="headerlink" title="Cross-Sensor Touch Generation"></a>Cross-Sensor Touch Generation</h2><p><strong>Authors:Samanta Rodriguez, Yiming Dou, Miquel Oller, Andrew Owens, Nima Fazeli</strong></p>
<p>Todayâ€™s visuo-tactile sensors come in many shapes and sizes, making it challenging to develop general-purpose tactile representations. This is because most models are tied to a specific sensor design. To address this challenge, we propose two approaches to cross-sensor image generation. The first is an end-to-end method that leverages paired data (Touch2Touch). The second method builds an intermediate depth representation and does not require paired data (T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific models across multiple sensors via the cross-sensor touch generation process. Together, these models offer flexible solutions for sensor translation, depending on data availability and application needs. We demonstrate their effectiveness on downstream tasks such as in-hand pose estimation and behavior cloning, successfully transferring models trained on one sensor to another. Project page: <a target="_blank" rel="noopener" href="https://samantabelen.github.io/cross_sensor_touch_generation">https://samantabelen.github.io/cross_sensor_touch_generation</a>. </p>
<blockquote>
<p>å½“å‰çš„è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨æœ‰å„ç§å½¢çŠ¶å’Œå¤§å°ï¼Œè¿™ä¸ºå¼€å‘é€šç”¨è§¦è§‰è¡¨å¾å¸¦æ¥äº†æŒ‘æˆ˜ã€‚è¿™æ˜¯å› ä¸ºå¤§å¤šæ•°æ¨¡å‹éƒ½ä¸ç‰¹å®šçš„ä¼ æ„Ÿå™¨è®¾è®¡ç›¸å…³è”ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§è·¨ä¼ æ„Ÿå™¨å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯ç«¯åˆ°ç«¯çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é…å¯¹æ•°æ®ï¼ˆTouch2Touchï¼‰ã€‚ç¬¬äºŒç§æ–¹æ³•å»ºç«‹ä¸­é—´æ·±åº¦è¡¨å¾ï¼Œä¸éœ€è¦é…å¯¹æ•°æ®ï¼ˆT2D2ï¼šTouch-to-Depth-to-Touchï¼‰ã€‚ä¸¤ç§æ–¹æ³•éƒ½é€šè¿‡è·¨ä¼ æ„Ÿå™¨è§¦æ‘¸ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å¾—ç‰¹å®šä¼ æ„Ÿå™¨çš„æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªä¼ æ„Ÿå™¨ä¸Šä½¿ç”¨ã€‚æ ¹æ®æ•°æ®å¯ç”¨æ€§å’Œåº”ç”¨éœ€æ±‚ï¼Œè¿™äº›æ¨¡å‹ä¸ºä¼ æ„Ÿå™¨è½¬æ¢æä¾›äº†çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚æ‰‹éƒ¨å§¿æ€ä¼°è®¡å’Œè¡Œä¸ºå…‹éš†ï¼‰ä¸Šè¯æ˜äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸåœ°å°†ä¸€ä¸ªä¼ æ„Ÿå™¨ä¸Šè®­ç»ƒçš„æ¨¡å‹è½¬ç§»åˆ°å¦ä¸€ä¸ªä¼ æ„Ÿå™¨ä¸Šã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://samantabelen.github.io/cross_sensor_touch_generation">https://samantabelen.github.io/cross_sensor_touch_generation</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09817v1">PDF</a> CoRL 2025</p>
<p><strong>Summary</strong>ï¼š<br>ç°ä»£è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨ç§ç±»ç¹å¤šï¼Œç»™é€šç”¨è§¦è§‰è¡¨ç¤ºçš„å¼€å‘å¸¦æ¥æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸¤ç§è·¨ä¼ æ„Ÿå™¨å›¾åƒç”Ÿæˆæ–¹æ³•åº”å¯¹æ­¤æŒ‘æˆ˜ã€‚ç¬¬ä¸€ç§ä¸ºç«¯åˆ°ç«¯çš„é…å¯¹æ•°æ®æ–¹æ³•ï¼ˆTouch2Touchï¼‰ï¼›ç¬¬äºŒç§æ„å»ºä¸­é—´æ·±åº¦è¡¨ç¤ºï¼Œæ— éœ€é…å¯¹æ•°æ®ï¼ˆT2D2ï¼šTouch-to-Depth-to-Touchï¼‰ã€‚ä¸¤ç§æ–¹æ³•éƒ½é€šè¿‡è·¨ä¼ æ„Ÿå™¨è§¦æ‘¸ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿ä¼ æ„Ÿå™¨ç‰¹å®šæ¨¡å‹å¯åœ¨å¤šä¸ªä¼ æ„Ÿå™¨ä¸Šåº”ç”¨ã€‚æ ¹æ®æ•°æ®å¯ç”¨æ€§å’Œåº”ç”¨éœ€æ±‚ï¼Œè¿™äº›æ¨¡å‹æä¾›çµæ´»çš„ä¼ æ„Ÿå™¨ç¿»è¯‘è§£å†³æ–¹æ¡ˆã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚æ‰‹éƒ¨å§¿æ€ä¼°è®¡å’Œè¡Œä¸ºå…‹éš†ï¼‰ä¸­ï¼ŒæˆåŠŸå°†åœ¨ä¸€ç§ä¼ æ„Ÿå™¨ä¸Šè®­ç»ƒçš„æ¨¡å‹è½¬ç§»åˆ°å¦ä¸€ç§ä¼ æ„Ÿå™¨ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°ä»£è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨ç§ç±»ç¹å¤šï¼Œå¼€å‘é€šç”¨è§¦è§‰è¡¨ç¤ºé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸¤ç§è·¨ä¼ æ„Ÿå™¨å›¾åƒç”Ÿæˆæ–¹æ³•ï¼šç«¯åˆ°ç«¯çš„é…å¯¹æ•°æ®æ–¹æ³•ï¼ˆTouch2Touchï¼‰å’Œæ„å»ºä¸­é—´æ·±åº¦è¡¨ç¤ºçš„æ–¹æ³•ï¼ˆT2D2ï¼‰ã€‚</li>
<li>ä¸¤ç§æ–¹æ³•éƒ½ä½¿ä¼ æ„Ÿå™¨ç‰¹å®šæ¨¡å‹å¯åœ¨å¤šä¸ªä¼ æ„Ÿå™¨ä¸Šåº”ç”¨ã€‚</li>
<li>æ ¹æ®æ•°æ®å¯ç”¨æ€§å’Œåº”ç”¨éœ€æ±‚ï¼Œè¿™äº›æ¨¡å‹æä¾›çµæ´»çš„ä¼ æ„Ÿå™¨ç¿»è¯‘è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸå°†ä¸€ç§ä¼ æ„Ÿå™¨ä¸Šè®­ç»ƒçš„æ¨¡å‹è½¬ç§»åˆ°å¦ä¸€ç§ä¼ æ„Ÿå™¨ä¸Šã€‚</li>
<li>æœ‰æ•ˆè§£å†³ç”±äºä¼ æ„Ÿå™¨ç§ç±»å¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-94d2f501aa22e6a9e80e834bef2a1c6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735322&auth_key=1760735322-0-0-ad58a50ab5e277c334599a4dff60e46d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-688a94e6a056861a8ee10fe23f8180dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735329&auth_key=1760735329-0-0-11fcdc94ffd414df3b39324d96ebec94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75349c2f873693f74c6a4920ed0778cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735336&auth_key=1760735336-0-0-9002fbb50ddef9dfad737bbde6c75d04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lesion-Aware-Post-Training-of-Latent-Diffusion-Models-for-Synthesizing-Diffusion-MRI-from-CT-Perfusion"><a href="#Lesion-Aware-Post-Training-of-Latent-Diffusion-Models-for-Synthesizing-Diffusion-MRI-from-CT-Perfusion" class="headerlink" title="Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing   Diffusion MRI from CT Perfusion"></a>Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing   Diffusion MRI from CT Perfusion</h2><p><strong>Authors:Junhyeok Lee, Hyunwoong Kim, Hyungjin Chung, Heeseong Eom, Joon Jang, Chul-Ho Sohn, Kyu Sung Choi</strong></p>
<p>Image-to-Image translation models can help mitigate various challenges inherent to medical image acquisition. Latent diffusion models (LDMs) leverage efficient learning in compressed latent space and constitute the core of state-of-the-art generative image models. However, this efficiency comes with a trade-off, potentially compromising crucial pixel-level detail essential for high-fidelity medical images. This limitation becomes particularly critical when generating clinically significant structures, such as lesions, which often occupy only a small portion of the image. Failure to accurately reconstruct these regions can severely impact diagnostic reliability and clinical decision-making. To overcome this limitation, we propose a novel post-training framework for LDMs in medical image-to-image translation by incorporating lesion-aware medical pixel space objectives. This approach is essential, as it not only enhances overall image quality but also improves the precision of lesion delineation. We evaluate our framework on brain CT-to-MRI translation in acute ischemic stroke patients, where early and accurate diagnosis is critical for optimal treatment selection and improved patient outcomes. While diffusion MRI is the gold standard for stroke diagnosis, its clinical utility is often constrained by high costs and low accessibility. Using a dataset of 817 patients, we demonstrate that our framework improves overall image quality and enhances lesion delineation when synthesizing DWI and ADC images from CT perfusion scans, outperforming existing image-to-image translation models. Furthermore, our post-training strategy is easily adaptable to pre-trained LDMs and exhibits substantial potential for broader applications across diverse medical image translation tasks. </p>
<blockquote>
<p>å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¨¡å‹å¯ä»¥å¸®åŠ©ç¼“è§£åŒ»ç–—å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­å­˜åœ¨çš„å„ç§æŒ‘æˆ˜ã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­å®ç°äº†é«˜æ•ˆå­¦ä¹ ï¼Œæ„æˆäº†æœ€å…ˆè¿›çš„ç”Ÿæˆå›¾åƒæ¨¡å‹çš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œè¿™ç§æ•ˆç‡æ˜¯æœ‰ä»£ä»·çš„ï¼Œå¯èƒ½ä¼šæŸå®³å¯¹é«˜ä¿çœŸåŒ»ç–—å›¾åƒè‡³å…³é‡è¦çš„åƒç´ çº§ç»†èŠ‚ã€‚å½“ç”Ÿæˆä¸´åºŠä¸Šé‡è¦çš„ç»“æ„ï¼ˆå¦‚ç—…å˜ï¼‰æ—¶ï¼Œè¿™ç§é™åˆ¶å˜å¾—å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºè¿™äº›ç»“æ„é€šå¸¸åªå å›¾åƒçš„ä¸€å°éƒ¨åˆ†ã€‚æ— æ³•å‡†ç¡®é‡å»ºè¿™äº›åŒºåŸŸå¯èƒ½ä¼šä¸¥é‡å½±å“è¯Šæ–­çš„å¯é æ€§å’Œä¸´åºŠå†³ç­–ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„LDMï¼Œé€šè¿‡å¼•å…¥ç—…å˜æ„ŸçŸ¥åŒ»å­¦åƒç´ ç©ºé—´ç›®æ ‡ã€‚è¿™ç§æ–¹æ³•è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä¸ä»…æé«˜äº†æ•´ä½“å›¾åƒè´¨é‡ï¼Œè€Œä¸”æé«˜äº†ç—…å˜å‹¾å‹’çš„ç²¾ç¡®åº¦ã€‚æˆ‘ä»¬åœ¨æ€¥æ€§ç¼ºè¡€æ€§å’ä¸­æ‚£è€…çš„è„‘éƒ¨CTåˆ°MRIç¿»è¯‘ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­å¯¹äºé€‰æ‹©æœ€ä½³æ²»ç–—æ–¹æ¡ˆå’Œæ”¹å–„æ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚è™½ç„¶æ‰©æ•£MRIæ˜¯å’ä¸­è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œä½†å…¶ä¸´åºŠå®ç”¨æ€§å¾€å¾€å—åˆ°æˆæœ¬é«˜å’Œå¯åŠæ€§ä½çš„é™åˆ¶ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«817åæ‚£è€…çš„æ•°æ®é›†è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨åˆæˆDWIå’ŒADCå›¾åƒæ—¶çš„æ•ˆæœï¼Œè¿™äº›å›¾åƒæ˜¯ä»CTçŒæ³¨æ‰«æä¸­å¾—å‡ºçš„ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æé«˜äº†æ•´ä½“å›¾åƒè´¨é‡å¹¶å¢å¼ºäº†ç—…å˜çš„å‹¾å‹’èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åè®­ç»ƒç­–ç•¥å¯ä»¥è½»æ¾é€‚åº”é¢„è®­ç»ƒçš„LDMï¼Œå¹¶åœ¨å„ç§åŒ»ç–—å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09056v1">PDF</a> MICCAI 2025, Lecture Notes in Computer Science Vol. 15961</p>
<p><strong>Summary</strong></p>
<pre><code>å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¨¡å‹æœ‰åŠ©äºç¼“è§£åŒ»å­¦å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­å­˜åœ¨çš„å„ç§æŒ‘æˆ˜ã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­å®ç°äº†é«˜æ•ˆå­¦ä¹ ï¼Œæ„æˆäº†å½“å‰æœ€å…ˆè¿›çš„ç”Ÿæˆå›¾åƒæ¨¡å‹çš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œè¿™ç§æ•ˆç‡æ˜¯ä»¥ç‰ºç‰²åƒç´ çº§çš„ç»†èŠ‚ä¸ºä»£ä»·çš„ï¼Œè¿™å¯¹äºé«˜ä¿çœŸåŒ»å­¦å›¾åƒè‡³å…³é‡è¦ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆä¸´åºŠé‡è¦ç»“æ„å¦‚ç—…ç¶æ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç—…ç¶æ„ŸçŸ¥åŒ»å­¦åƒç´ ç©ºé—´ç›®æ ‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¯¥æ¡†æ¶åœ¨æ€¥æ€§ç¼ºè¡€æ€§å’ä¸­æ‚£è€…çš„è„‘éƒ¨CTåˆ°MRIç¿»è¯‘ä¸­çš„åº”ç”¨ï¼Œæ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­å¯¹äºé€‰æ‹©æœ€ä½³æ²»ç–—æ–¹æ¡ˆå’Œæ”¹å–„æ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ä»…æé«˜äº†æ•´ä½“å›¾åƒè´¨é‡ï¼Œè¿˜æé«˜äº†ç—…ç¶å‹¾å‹’çš„ç²¾ç¡®åº¦ï¼Œå¹¶ä¸”åœ¨åˆæˆDWIå’ŒADCå›¾åƒä»CTçŒæ³¨æ‰«æä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åè®­ç»ƒç­–ç•¥å¯ä»¥è½»æ¾é€‚åº”é¢„è®­ç»ƒçš„LDMsï¼Œå¹¶åœ¨å„ç§åŒ»å­¦å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚

**Key Takeaways**

1. å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¨¡å‹æœ‰åŠ©äºè§£å†³åŒ»å­¦å›¾åƒé‡‡é›†ä¸­çš„æŒ‘æˆ˜ã€‚
2. æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­å®ç°äº†é«˜æ•ˆå­¦ä¹ ï¼Œä½†å¯èƒ½ç‰ºç‰²åƒç´ çº§ç»†èŠ‚ã€‚
3. ç—…ç¶ç­‰ä¸´åºŠé‡è¦ç»“æ„çš„ç”Ÿæˆå¯¹é«˜ä¿çœŸåŒ»å­¦å›¾åƒè‡³å…³é‡è¦ã€‚
4. æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç—…ç¶æ„ŸçŸ¥åŒ»å­¦åƒç´ ç©ºé—´ç›®æ ‡æ¥æå‡å›¾åƒè´¨é‡ã€‚
5. è¯¥æ¡†æ¶åœ¨æ€¥æ€§ç¼ºè¡€æ€§å’ä¸­æ‚£è€…çš„è„‘éƒ¨CTåˆ°MRIç¿»è¯‘ä¸­è¡¨ç°ä¼˜å¼‚ã€‚
6. ç›¸è¾ƒäºç°æœ‰æ¨¡å‹ï¼Œæå‡ºçš„æ¡†æ¶æé«˜äº†å›¾åƒæ•´ä½“è´¨é‡å’Œç—…ç¶å‹¾å‹’çš„å‡†ç¡®æ€§ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6e4b48995cf1d566c55ee7d3cb15dcfe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735343&auth_key=1760735343-0-0-9e8db06eec2d5436f66383da8b51a0f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f7c0df3242f3e231e2bd72f34984f2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735350&auth_key=1760735350-0-0-8fe2ba370b50192c4392f1459457039e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da3d52280bd20f5577fe1c3631721f2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735358&auth_key=1760735358-0-0-92ee8674a223a586edd5f08671582b65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1faf5bc906f8d57d051c2b4480913130~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735365&auth_key=1760735365-0-0-7e6ecef6e962b938b27080ddf3b10317&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FS-RWKV-Leveraging-Frequency-Spatial-Aware-RWKV-for-3T-to-7T-MRI-Translation"><a href="#FS-RWKV-Leveraging-Frequency-Spatial-Aware-RWKV-for-3T-to-7T-MRI-Translation" class="headerlink" title="FS-RWKV: Leveraging Frequency Spatial-Aware RWKV for 3T-to-7T MRI   Translation"></a>FS-RWKV: Leveraging Frequency Spatial-Aware RWKV for 3T-to-7T MRI   Translation</h2><p><strong>Authors:Yingtie Lei, Zimeng Li, Chi-Man Pun, Yupeng Liu, Xuhang Chen</strong></p>
<p>Ultra-high-field 7T MRI offers enhanced spatial resolution and tissue contrast that enables the detection of subtle pathological changes in neurological disorders. However, the limited availability of 7T scanners restricts widespread clinical adoption due to substantial infrastructure costs and technical demands. Computational approaches for synthesizing 7T-quality images from accessible 3T acquisitions present a viable solution to this accessibility challenge. Existing CNN approaches suffer from limited spatial coverage, while Transformer models demand excessive computational overhead. RWKV architectures offer an efficient alternative for global feature modeling in medical image synthesis, combining linear computational complexity with strong long-range dependency capture. Building on this foundation, we propose Frequency Spatial-RWKV (FS-RWKV), an RWKV-based framework for 3T-to-7T MRI translation. To better address the challenges of anatomical detail preservation and global tissue contrast recovery, FS-RWKV incorporates two key modules: (1) Frequency-Spatial Omnidirectional Shift (FSO-Shift), which performs discrete wavelet decomposition followed by omnidirectional spatial shifting on the low-frequency branch to enhance global contextual representation while preserving high-frequency anatomical details; and (2) Structural Fidelity Enhancement Block (SFEB), a module that adaptively reinforces anatomical structure through frequency-aware feature fusion. Comprehensive experiments on UNC and BNU datasets demonstrate that FS-RWKV consistently outperforms existing CNN-, Transformer-, GAN-, and RWKV-based baselines across both T1w and T2w modalities, achieving superior anatomical fidelity and perceptual quality. </p>
<blockquote>
<p>è¶…é«˜åœº7T MRIæä¾›äº†æ›´é«˜çš„ç©ºé—´åˆ†è¾¨ç‡å’Œç»„ç»‡å¯¹æ¯”åº¦ï¼Œèƒ½å¤Ÿæ£€æµ‹åˆ°ç¥ç»ç–¾ç—…ä¸­çš„ç»†å¾®ç—…ç†å˜åŒ–ã€‚ç„¶è€Œï¼Œç”±äºåŸºç¡€è®¾æ–½æˆæœ¬å’ŒæŠ€æœ¯éœ€æ±‚è¾ƒé«˜ï¼Œ7Tæ‰«æä»ªçš„æœ‰é™å¯ç”¨æ€§é™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„å¹¿æ³›åº”ç”¨ã€‚åˆ©ç”¨è®¡ç®—åˆæˆæŠ€æœ¯ä»å¯è·å¾—çš„3Tæ‰«æå›¾åƒç”Ÿæˆ7Tè´¨é‡çš„å›¾åƒï¼Œæ˜¯è§£å†³è¿™ä¸€å¯è®¿é—®æ€§æŒ‘æˆ˜çš„ä¸€ç§å¯è¡Œæ–¹æ³•ã€‚ç°æœ‰çš„CNNæ–¹æ³•å­˜åœ¨ç©ºé—´è¦†ç›–èŒƒå›´æœ‰é™çš„é—®é¢˜ï¼Œè€ŒTransformeræ¨¡å‹åˆ™è¦æ±‚è¿‡é«˜çš„è®¡ç®—å¼€é”€ã€‚RWKVæ¶æ„ä¸ºåŒ»å­¦å›¾åƒåˆæˆä¸­çš„å…¨å±€ç‰¹å¾å»ºæ¨¡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œç»“åˆäº†çº¿æ€§è®¡ç®—å¤æ‚åº¦å’Œå¼ºå¤§çš„é•¿è·ç¦»ä¾èµ–æ•è·èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºRWKVæ¶æ„çš„é¢‘ç‡ç©ºé—´FS-RWKVï¼ˆFrequency Spatial-RWKVï¼‰ï¼Œç”¨äºå®ç°3Tåˆ°7TMRå›¾åƒç¿»è¯‘ã€‚ä¸ºäº†æ›´å¥½åœ°è§£å†³ä¿ç•™è§£å‰–ç»†èŠ‚å’Œæ¢å¤å…¨å±€ç»„ç»‡å¯¹æ¯”åº¦çš„é—®é¢˜ï¼ŒFS-RWKVå¼•å…¥äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šï¼ˆ1ï¼‰é¢‘ç‡ç©ºé—´å…¨æ–¹å‘ç§»ä½ï¼ˆFrequency-Spatial Omnidirectional Shiftï¼ŒFSO-Shiftï¼‰ï¼Œå®ƒæ‰§è¡Œç¦»æ•£å°æ³¢åˆ†è§£ï¼Œç„¶åå¯¹ä½é¢‘åˆ†æ”¯è¿›è¡Œå…¨ç©ºé—´ç§»ä½ï¼Œä»¥å¢å¼ºå…¨å±€ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„åŒæ—¶ä¿ç•™é«˜é¢‘è§£å‰–ç»†èŠ‚ï¼›ï¼ˆ2ï¼‰ç»“æ„ä¿çœŸåº¦å¢å¼ºå—ï¼ˆStructural Fidelity Enhancement Blockï¼ŒSFEBï¼‰ï¼Œè¯¥æ¨¡å—é€šè¿‡é¢‘ç‡æ„ŸçŸ¥ç‰¹å¾èåˆè‡ªé€‚åº”åœ°å¼ºåŒ–è§£å‰–ç»“æ„ã€‚åœ¨UNCå’ŒBNUæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFS-RWKVåœ¨T1wå’ŒT2wæ¨¡æ€ä¸Šå‡æŒç»­ä¼˜äºç°æœ‰çš„CNNã€Transformerã€GANå’ŒRWKVåŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°ä¼˜è¶Šçš„è§£å‰–ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08951v1">PDF</a> Accepted by BIBM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…é«˜åœº7T MRIçš„æŠ€æœ¯ä¼˜åŠ¿åŠæ™®åŠéš¾ç‚¹ï¼Œæå‡ºäº†åŸºäºRWKVæ¶æ„çš„Frequency Spatial-RWKVï¼ˆFS-RWKVï¼‰æ¡†æ¶ï¼Œç”¨äºä»å¯è·å–çš„3T MRIå›¾åƒåˆæˆ7Tè´¨é‡çš„å›¾åƒã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šFrequency-Spatial Omnidirectional Shiftï¼ˆFSO-Shiftï¼‰å’ŒStructural Fidelity Enhancement Blockï¼ˆSFEBï¼‰ï¼Œæ—¨åœ¨å¢å¼ºå…¨å±€ä¸Šä¸‹æ–‡è¡¨å¾å¹¶ä¿ç•™é«˜é¢‘è§£å‰–ç»†èŠ‚ï¼ŒåŒæ—¶è‡ªé€‚åº”åœ°å¼ºåŒ–è§£å‰–ç»“æ„ã€‚å®éªŒè¯æ˜ï¼ŒFS-RWKVåœ¨UNCå’ŒBNUæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰CNNã€Transformerã€GANå’ŒRWKVåŸºçº¿æ–¹æ³•ï¼Œåœ¨T1wå’ŒT2wæ¨¡æ€ä¸‹å®ç°ä¼˜è¶Šçš„è§£å‰–ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¶…é«˜åœº7T MRIå…·æœ‰ä¼˜ç§€çš„ç©ºé—´åˆ†è¾¨ç‡å’Œç»„ç»‡å¯¹æ¯”åº¦ï¼Œèƒ½æ£€æµ‹ç¥ç»ç–¾ç—…çš„ç»†å¾®ç—…ç†å˜åŒ–ã€‚</li>
<li>7Tæ‰«æä»ªçš„æœ‰é™å¯ç”¨æ€§å’Œé«˜æ˜‚æˆæœ¬é™åˆ¶äº†å…¶ä¸´åºŠæ™®åŠã€‚</li>
<li>è®¡ç®—åˆæˆ7Tè´¨é‡å›¾åƒçš„æ–¹æ³•ä»å¯è·å–çš„3Tå›¾åƒå‡ºå‘ï¼Œæ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æœ‰æ•ˆé€”å¾„ã€‚</li>
<li>ç°æœ‰CNNæ–¹æ³•ç©ºé—´è¦†ç›–æœ‰é™ï¼Œè€ŒTransformeræ¨¡å‹è®¡ç®—å¼€é”€å¤§ã€‚</li>
<li>RWKVæ¶æ„å…·æœ‰çº¿æ€§è®¡ç®—å¤æ‚æ€§å’Œå¼ºå¤§çš„é•¿ç¨‹ä¾èµ–æ€§æ•æ‰ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒåˆæˆã€‚</li>
<li>æå‡ºçš„FS-RWKVæ¡†æ¶ç»“åˆFSO-Shiftå’ŒSFEBæ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºå…¨å±€ç‰¹å¾å»ºæ¨¡å’Œè§£å‰–ç»†èŠ‚ä¿ç•™ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a23413fcbe2217687c4b5d45e1ba2cdd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735372&auth_key=1760735372-0-0-7871eeff509556dbc1e01bfff5543a9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0583d67c0b286f956a9e8c9c98892c03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735379&auth_key=1760735379-0-0-65033497b189942f8fa00aefb4854ba7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ee69d0d91f9b3e60daf38b080584a7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735386&auth_key=1760735386-0-0-e3d9ebac30cfa497c2a14bdc5c9d69a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-800ef8b51421ab76d5f96e4f14d71fe4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735396&auth_key=1760735396-0-0-5b86cdbac431e77694a31bcc69658f1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34c0807621d4105f1a310f6c90502f87~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735405&auth_key=1760735405-0-0-6175161d668eb267b4bb81cffae3ef95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d7c3af629069c9e8cd71f0f39ac3116~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735412&auth_key=1760735412-0-0-ad9eb607db3c62d2336ee7b3d9c58f6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Benchmarking-AI-evolved-cosmological-structure-formation"><a href="#Benchmarking-AI-evolved-cosmological-structure-formation" class="headerlink" title="Benchmarking AI-evolved cosmological structure formation"></a>Benchmarking AI-evolved cosmological structure formation</h2><p><strong>Authors:Xiaofeng Dong, Nesar Ramachandra, Salman Habib, Katrin Heitmann</strong></p>
<p>The potential of deep learning-based image-to-image translations has recently attracted significant attention. One possible application of such a framework is as a fast, approximate alternative to cosmological simulations, which would be particularly useful in various contexts, including covariance studies, investigations of systematics, and cosmological parameter inference. To investigate different aspects of learning-based cosmological mappings, we choose two approaches for generating suitable cosmological matter fields as datasets: a simple analytical prescription provided by the Zelâ€™dovich approximation, and a numerical N-body method using the Particle-Mesh approach. The evolution of structure formation is modeled using U-Net, a widely employed convolutional image translation framework. Because of the lack of a controlled methodology, validation of these learned mappings requires multiple benchmarks beyond simple visual comparisons and summary statistics. A comprehensive list of metrics is considered, including higher-order correlation functions, conservation laws, topological indicators, and statistical independence of density fields. We find that the U-Net approach performs well only for some of these physical metrics, and accuracy is worse at increasingly smaller scales, where the dynamic range in density is large. By introducing a custom density-weighted loss function during training, we demonstrate a significant improvement in the U-Net results at smaller scales. This study provides an example of how a family of physically motivated benchmarks can, in turn, be used to fine-tune optimization schemes â€“ such as the density-weighted loss used here â€“ to significantly enhance the accuracy of scientific machine learning approaches by focusing attention on relevant features. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢æ½œåŠ›æœ€è¿‘å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è¯¥æ¡†æ¶çš„ä¸€ä¸ªå¯èƒ½åº”ç”¨æ˜¯ä½œä¸ºå®‡å®™å­¦æ¨¡æ‹Ÿçš„å¿«é€Ÿè¿‘ä¼¼æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨å„ç§èƒŒæ™¯ä¸‹éƒ½å°†ç‰¹åˆ«æœ‰ç”¨ï¼ŒåŒ…æ‹¬åæ–¹å·®ç ”ç©¶ã€ç³»ç»Ÿè°ƒæŸ¥ä»¥åŠå®‡å®™å­¦å‚æ•°æ¨æ–­ã€‚ä¸ºäº†ç ”ç©¶åŸºäºå­¦ä¹ çš„å®‡å®™å­¦æ˜ å°„çš„ä¸åŒæ–¹é¢ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸¤ç§ç”Ÿæˆåˆé€‚çš„å®‡å®™å­¦ç‰©è´¨åœºæ•°æ®é›†çš„æ–¹æ³•ï¼šZelâ€™dovichè¿‘ä¼¼æä¾›çš„ç®€å•åˆ†æå…¬å¼å’Œé‡‡ç”¨ç²’å­ç½‘æ ¼æ–¹æ³•çš„æ•°å€¼Nä½“æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨å¹¿æ³›é‡‡ç”¨çš„å·ç§¯å›¾åƒç¿»è¯‘æ¡†æ¶U-Netå¯¹ç»“æ„å½¢æˆçš„æ¼”åŒ–è¿›è¡Œå»ºæ¨¡ã€‚ç”±äºç¼ºä¹å—æ§çš„æ–¹æ³•è®ºï¼Œè¿™äº›å­¦ä¹ åˆ°çš„æ˜ å°„çš„éªŒè¯éœ€è¦è¶…è¶Šç®€å•è§†è§‰æ¯”è¾ƒå’Œæ€»ç»“ç»Ÿè®¡çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚æ‰€è€ƒè™‘çš„æŒ‡æ ‡åŒ…æ‹¬é«˜é˜¶ç›¸å…³å‡½æ•°ã€å®ˆæ’å®šå¾‹ã€æ‹“æ‰‘æŒ‡æ ‡ä»¥åŠå¯†åº¦åœºçš„ç»Ÿè®¡ç‹¬ç«‹æ€§ã€‚æˆ‘ä»¬å‘ç°U-Netæ–¹æ³•ä»…å¯¹è¿™äº›ç‰©ç†æŒ‡æ ‡ä¸­çš„éƒ¨åˆ†è¡¨ç°è‰¯å¥½ï¼Œè€Œä¸”åœ¨è§„æ¨¡è¶Šæ¥è¶Šå°çš„åœ°æ–¹ç²¾åº¦æ›´å·®ï¼Œé‚£é‡Œçš„å¯†åº¦åŠ¨æ€èŒƒå›´å¾ˆå¤§ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥è‡ªå®šä¹‰çš„å¯†åº¦åŠ æƒæŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬è¯æ˜äº†U-Netåœ¨å°è§„æ¨¡ä¸Šçš„ç»“æœæœ‰äº†æ˜¾ç€æ”¹å–„ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜ä¸€ç³»åˆ—åŸºäºç‰©ç†çš„åŸºå‡†æµ‹è¯•å¦‚ä½•åè¿‡æ¥ç”¨äºå¾®è°ƒä¼˜åŒ–æ–¹æ¡ˆâ€”â€”å¦‚æ­¤å¤„ä½¿ç”¨çš„å¯†åº¦åŠ æƒæŸå¤±â€”â€”é€šè¿‡å…³æ³¨ç›¸å…³ç‰¹å¾æ¥æ˜¾è‘—æé«˜ç§‘å­¦æœºå™¨å­¦ä¹ æ–¹æ³•çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.06731v2">PDF</a> Expanded and thoroughly revised version of our prior NeurIPS   submission (arXiv:2112.05681; which has no DOI), with new sections,   experiments, and analyses</p>
<p><strong>Summary</strong>ï¼šåŸºäºæ·±åº¦å­¦ä¹ å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„æ½œåŠ›å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚æœ¬ç ”ç©¶é€‰æ‹©ä½¿ç”¨U-Netå·ç§¯å›¾åƒç¿»è¯‘æ¡†æ¶å¯¹åŸºäºå­¦ä¹ çš„å®‡å®™å­¦æ˜ å°„è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡å¤šç§ç‰©ç†æŒ‡æ ‡å¯¹å…¶æ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚ä¸ºæé«˜å…¶åœ¨è¾ƒå°å°ºåº¦ä¸Šçš„å‡†ç¡®æ€§ï¼Œå¼•å…¥è‡ªå®šä¹‰å¯†åº¦åŠ æƒæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å…³æ³¨ç›¸å…³ç‰¹å¾å¯¹ç§‘å­¦æœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œå¾®è°ƒä»¥æé«˜å…¶å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æ–¹é¢çš„æ½œåŠ›å·¨å¤§ï¼Œå¯ä½œä¸ºä¸€ç§å¿«é€Ÿè¿‘ä¼¼æ›¿ä»£å®‡å®™å­¦æ¨¡æ‹Ÿã€‚</li>
<li>U-Netå·ç§¯å›¾åƒç¿»è¯‘æ¡†æ¶ç”¨äºå»ºæ¨¡åŸºäºå­¦ä¹ çš„å®‡å®™å­¦æ˜ å°„ã€‚</li>
<li>å¯¹å­¦ä¹ æ˜ å°„çš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬é«˜é˜¶ç›¸å…³å‡½æ•°ã€å®ˆæ’å®šå¾‹ã€æ‹“æ‰‘æŒ‡æ ‡å’Œå¯†åº¦åœºçš„ç»Ÿè®¡ç‹¬ç«‹æ€§ç­‰ç‰©ç†æŒ‡æ ‡ã€‚</li>
<li>U-Netåœ¨æŸäº›ç‰©ç†æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¾ƒå°å°ºåº¦ä¸Šçš„å‡†ç¡®æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥è‡ªå®šä¹‰å¯†åº¦åŠ æƒæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œåœ¨è¾ƒå°å°ºåº¦ä¸Šæ˜¾è‘—æé«˜U-Netçš„ç»“æœã€‚</li>
<li>ç‰©ç†æŒ‡æ ‡çš„åŸºå‡†æµ‹è¯•å¯ç”¨äºå¾®è°ƒä¼˜åŒ–æ–¹æ¡ˆï¼Œå¦‚é€šè¿‡å¯†åº¦åŠ æƒæŸå¤±æ¥æé«˜ç§‘å­¦æœºå™¨å­¦ä¹ çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.06731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a9106792a67f9fb8790ca9572c92649f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735419&auth_key=1760735419-0-0-8c0fadfdbac0faaa21abea3919f4c090&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd501b03ff2ba0cea5403fd25544bb8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735426&auth_key=1760735426-0-0-4643f728fb5e50eff57fe3f72f6feb83&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46e19434ee3f8a8220b28f30ca8dcdc8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735433&auth_key=1760735433-0-0-2f735749aa1b46901533665ce998450f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-1st-Solution-for-CARE-Liver-Task-Challenge-2025-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation"><a href="#The-1st-Solution-for-CARE-Liver-Task-Challenge-2025-Contrast-Aware-Semi-Supervised-Segmentation-with-Domain-Generalization-and-Test-Time-Adaptation" class="headerlink" title="The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware   Semi-Supervised Segmentation with Domain Generalization and Test-Time   Adaptation"></a>The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware   Semi-Supervised Segmentation with Domain Generalization and Test-Time   Adaptation</h2><p><strong>Authors:Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang</strong></p>
<p>Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions. </p>
<blockquote>
<p>ä»å¯¹æ¯”å¢å¼ºMRIå‡†ç¡®åˆ†å‰²è‚è„å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ ‡æ³¨æ•°æ®æœ‰é™ã€å¢å¼ºåè®®å­˜åœ¨å¼‚è´¨æ€§ä»¥åŠæ‰«æä»ªå’Œæœºæ„é—´é¢†åŸŸè½¬ç§»æ˜¾è‘—ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¡†æ¶åœ¨é¢†åŸŸé€šç”¨åŒ–æ–¹é¢å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†å…¶åº”ç”¨å¹¶ä¸ç›´æ¥ã€‚ä¾‹å¦‚ï¼ŒPix2Pixéœ€è¦è¿›è¡Œå›¾åƒæ³¨å†Œï¼Œè€Œå¾ªç¯GANæ— æ³•æ— ç¼é›†æˆåˆ°åˆ†å‰²ç®¡é“ä¸­ã€‚åŒæ—¶ï¼Œè¿™äº›æ–¹æ³•æœ€åˆæ˜¯ç”¨äºå¤„ç†è·¨æ¨¡æ€åœºæ™¯çš„ï¼Œå¾€å¾€ä¼šå¼•å…¥ç»“æ„å¤±çœŸå¹¶é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¿™åœ¨æˆ‘ä»¬å•æ¨¡æ€åœºæ™¯ä¸­å¯èƒ½ä¼šå¸¦æ¥å¼Šç«¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoSSeg-TTAï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘çš„åˆ†å‰²æ¡†æ¶ï¼Œé€‚ç”¨äºGED4ï¼ˆGd-EOB-DTPAå¢å¼ºè‚èƒ†ç›¸MRIï¼‰æ¨¡æ€ï¼Œå»ºç«‹åœ¨nnU-Netv2ä¹‹ä¸Šï¼Œå¹¶ç”¨åŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆå¢å¼ºï¼Œä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°ä½“ç§¯æ•°æ®ã€‚ä¸€ä¸ªé¢†åŸŸé€‚åº”æ¨¡å—ï¼Œç»“åˆåŸºäºéšæœºç›´æ–¹å›¾çš„é£æ ¼å¤–è§‚è½¬æ¢å‡½æ•°å’Œå¯è®­ç»ƒçš„å¯¹æ¯”æ„ŸçŸ¥ç½‘ç»œï¼Œä¸°å¯Œäº†é¢†åŸŸå¤šæ ·æ€§å¹¶å‡è½»äº†è·¨ä¸­å¿ƒå·®å¼‚æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¿ç»­æµ‹è¯•æ—¶é—´é€‚åº”ç­–ç•¥ï¼Œä»¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºnnU-Netv2åŸºçº¿ï¼Œåœ¨ç‹„å…‹åˆ†æ•°å’Œæµ·æ£®è·ç¦»æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼ŒåŒæ—¶åœ¨ä½æ³¨é‡Šæ¡ä»¶ä¸‹å¯¹æœªè§é¢†åŸŸå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04243v2">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹GED4æ¨¡æ€çš„ç´§å‡‘åˆ†å‰²æ¡†æ¶CoSSeg-TTAï¼Œç”¨äºå‡†ç¡®åœ°è¿›è¡Œå¯¹æ¯”å¢å¼ºMRIçš„è‚è„åˆ†å‰²ã€‚è¯¥æ¡†æ¶åŸºäºnnU-Netv2ï¼Œé‡‡ç”¨åŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡æ³¨ä½“ç§¯æ•°æ®ã€‚é€šè¿‡é¢†åŸŸé€‚åº”æ¨¡å—å’ŒæŒç»­æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥ï¼Œæé«˜äº†æ¡†æ¶åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„é¢†åŸŸé€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¼˜äºnnU-Netv2åŸºå‡†æ¨¡å‹ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoSSeg-TTAæ¡†æ¶ç”¨äºå¯¹æ¯”å¢å¼ºMRIçš„è‚è„åˆ†å‰²ï¼Œé€‚ç”¨äºGED4æ¨¡æ€ã€‚</li>
<li>æ¡†æ¶åŸºäºnnU-Netv2ï¼Œç»“åˆåŠç›‘ç£å‡å€¼æ•™å¸ˆæ–¹æ¡ˆï¼Œåˆ©ç”¨å¤§é‡æœªæ ‡æ³¨æ•°æ®ã€‚</li>
<li>é¢†åŸŸé€‚åº”æ¨¡å—é€šè¿‡éšæœºç›´æ–¹å›¾é£æ ¼è½¬æ¢å’Œå¯è®­ç»ƒå¯¹æ¯”ç½‘ç»œä¸°å¯Œé¢†åŸŸå¤šæ ·æ€§ï¼Œå‡å°‘è·¨ä¸­å¿ƒå·®å¼‚ã€‚</li>
<li>é‡‡ç”¨æŒç»­æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥ï¼Œæé«˜æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µçš„é²æ£’æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºCoSSeg-TTAæ¡†æ¶ä¼˜äºnnU-Netv2åŸºå‡†æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„Diceåˆ†æ•°å’Œæ›´å¥½çš„Hausdorffè·ç¦»è¡¨ç°ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d3d7bfd5e60b514a48bb97b3ef7c2171~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735441&auth_key=1760735441-0-0-d58c2a58a05f090384a4ac349e098a22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fd93b6d3ead1a4ebf9ee116bf4377edf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735448&auth_key=1760735448-0-0-39aa23320cb37b9cc339de36a0236418&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4759b31fe7fc13d067ccf4222b0acdc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735455&auth_key=1760735455-0-0-179706403015f9a596cdea1d16cf02f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="IWR-Bench-Can-LVLMs-reconstruct-interactive-webpage-from-a-user-interaction-video"><a href="#IWR-Bench-Can-LVLMs-reconstruct-interactive-webpage-from-a-user-interaction-video" class="headerlink" title="IWR-Bench: Can LVLMs reconstruct interactive webpage from a user   interaction video?"></a>IWR-Bench: Can LVLMs reconstruct interactive webpage from a user   interaction video?</h2><p><strong>Authors:Yang Chen, Minghao Liu, Yufan Shen, Yunwen Li, Tianyuan Huang, Xinyu Fang, Tianyu Zheng, Wenxuan Huang, Cheng Yang, Daocheng Fu, Jianbiao Mei, Rong Wu, Yunfei Zhao, Licheng Wen, Xuemeng Yang, Song Mao, Qunshu Lin, Zhi Yu, Yongliang Shen, Yu Qiao, Botian Shi</strong></p>
<p>The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current modelsâ€™ ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/L-O-I/IWR-Bench">https://github.com/L-O-I/IWR-Bench</a>. </p>
<blockquote>
<p>ç½‘é¡µåˆ°ä»£ç çš„ä»»åŠ¡è¦æ±‚æ¨¡å‹ç†è§£ç½‘é¡µçš„è§†è§‰è¡¨ç¤ºå¹¶ç”Ÿæˆç›¸åº”çš„ä»£ç ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é™æ€æˆªå›¾åˆ°ä»£ç çš„ä»»åŠ¡ï¼Œä»è€Œå¿½ç•¥äº†ç°å®ä¸–ç•Œç½‘é¡µåº”ç”¨ä¸­è‡³å…³é‡è¦çš„åŠ¨æ€äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡ä»‹ç»äº†IWR-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä»è§†é¢‘ä¸­è¿›è¡Œäº¤äº’å¼ç½‘é¡µé‡å»ºèƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚IWR-BenchåŒ…å«æ¥è‡ª100ä¸ªçœŸå®ç½‘ç«™çš„113ä¸ªç²¾å¿ƒç­–åˆ’çš„ä»»åŠ¡ï¼ŒåŒ…å«1001ä¸ªåŠ¨ä½œï¼Œå¹¶å‘ˆç°å¤šæ ·åŒ–çš„äº¤äº’å¤æ‚æ€§ï¼ˆå¦‚ç½‘é¡µæ¸¸æˆï¼‰ã€è§†è§‰é£æ ¼å’Œé¢†åŸŸã€‚ä¸æ ‡å‡†ç½‘é¡µå¼€å‘å®è·µç›¸ä¸€è‡´ï¼Œæ¯ä¸ªä»»åŠ¡ä¸ä»…åŒ…å«ç”¨æˆ·äº¤äº’è§†é¢‘ï¼Œè¿˜åŒ…å«æ‰€æœ‰çˆ¬å–çš„é™æ€èµ„äº§ï¼ˆå¦‚å›¾ç‰‡ã€è§†é¢‘ï¼‰ã€‚æ­¤åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹åœ¨ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ä¸Šçš„è¡¨ç°ï¼šé€šè¿‡è§†é¢‘å’Œèµ„äº§æ¨æ–­äº¤äº’é€»è¾‘çš„ç»¼åˆå¤šæ¨¡å¼æ¨ç†ï¼Œä»¥åŠå°†è¿™ä¸€é€»è¾‘è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç çš„å…ˆè¿›ä»£ç ç”Ÿæˆã€‚ä¸€ä¸ªä»¥æ³•å®˜ä¸ºè§’è‰²çš„æ¡†æ¶ï¼Œé…å¤‡æœ‰å…¨é¢çš„åº¦é‡ç³»ç»Ÿï¼Œå¯ä»¥è‡ªåŠ¨è¯„ä¼°ç”Ÿæˆç½‘é¡µçš„åŠŸèƒ½æ­£ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚åœ¨28ä¸ªLVLMsä¸Šçš„å¤§é‡å®éªŒæ˜¾ç¤ºäº†ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼šæœ€ä½³æ¨¡å‹çš„æ€»ä½“å¾—åˆ†ä»…ä¸º36.35%ï¼Œå…¶ä¸­åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆ24.39% IFSï¼‰è¿œè¿œè½åäºè§†è§‰ä¿çœŸåº¦ï¼ˆ64.25% VFSï¼‰ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨æ¨ç†æ—¶é—´åŠ¨æ€å’Œåˆæˆäº‹ä»¶é©±åŠ¨é€»è¾‘æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œä½¿IWR-Benchæˆä¸ºè§†è§‰è¯­è¨€ç ”ç©¶çš„å‰æ²¿æŒ‘æˆ˜ã€‚åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/L-O-I/IWR-Bench%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/L-O-I/IWR-Benchä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24709v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç½‘é¡µåˆ°ä»£ç çš„è½¬æ¢ä»»åŠ¡éœ€è¦æ¨¡å‹ç†è§£ç½‘é¡µçš„è§†è§‰è¡¨ç¤ºå¹¶ç”Ÿæˆç›¸åº”çš„ä»£ç ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é™æ€æˆªå›¾åˆ°ä»£ç çš„è½¬æ¢ï¼Œå¿½ç•¥äº†ç½‘é¡µåŠ¨æ€äº¤äº’åœ¨ç°å®åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†IWR-Benchï¼Œä¸€ä¸ªè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä»è§†é¢‘ä¸­è¿›è¡Œäº¤äº’å¼ç½‘é¡µé‡å»ºèƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚IWR-BenchåŒ…å«æ¥è‡ª100ä¸ªçœŸå®ç½‘ç«™çš„113ä¸ªç²¾å¿ƒæŒ‘é€‰çš„ä»»åŠ¡ï¼Œæ¶µç›–1001ä¸ªåŠ¨ä½œï¼Œå…·æœ‰å¤šæ ·çš„äº¤äº’å¤æ‚æ€§ï¼ˆå¦‚ç½‘é¡µæ¸¸æˆï¼‰ã€è§†è§‰é£æ ¼å’Œé¢†åŸŸã€‚è¯¥åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹åœ¨ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ä¸Šçš„è¡¨ç°ï¼šä»è§†é¢‘å’Œèµ„äº§ä¸­æ¨æ–­äº¤äº’é€»è¾‘çš„ç»¼åˆå¤šæ¨¡å¼æ¨ç†ï¼Œä»¥åŠå°†è¿™ä¸€é€»è¾‘è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç çš„å…ˆè¿›ä»£ç ç”Ÿæˆã€‚é‡‡ç”¨ä»£ç†è¯„å§”æ¡†æ¶å’Œå…¨é¢çš„åº¦é‡ç³»ç»Ÿï¼Œè‡ªåŠ¨è¯„ä¼°ç”Ÿæˆç½‘é¡µçš„åŠŸèƒ½æ­£ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚å¯¹28ä¸ªLVLMsçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºäº†ä¸€ä¸ªé‡å¤§çš„æŒ‘æˆ˜ï¼šæœ€ä½³æ¨¡å‹çš„æ•´ä½“å¾—åˆ†ä»…ä¸º36.35%ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§ï¼ˆ24.39% IFSï¼‰è¿œè¿œè½åäºè§†è§‰ä¿çœŸåº¦ï¼ˆ64.25% VFSï¼‰ã€‚è¿™è¡¨æ˜å½“å‰æ¨¡å‹åœ¨æ¨ç†æ—¶é—´åŠ¨æ€å’Œåˆæˆäº‹ä»¶é©±åŠ¨é€»è¾‘æ–¹é¢å­˜åœ¨å…³é”®å±€é™æ€§ï¼Œä½¿IWR-Benchæˆä¸ºè§†è§‰è¯­è¨€ç ”ç©¶çš„å‰æ²¿æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç½‘é¡µåˆ°ä»£ç çš„è½¬æ¢ä»»åŠ¡éœ€è¦æ¨¡å‹ç†è§£ç½‘é¡µçš„è§†è§‰è¡¨ç¤ºå¹¶ç”Ÿæˆä»£ç ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é™æ€æˆªå›¾åˆ°ä»£ç çš„è½¬æ¢ï¼Œå¿½ç•¥äº†åŠ¨æ€äº¤äº’çš„é‡è¦æ€§ã€‚</li>
<li>IWR-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ä»è§†é¢‘ä¸­è¿›è¡Œäº¤äº’å¼ç½‘é¡µé‡å»ºçš„èƒ½åŠ›ã€‚</li>
<li>IWR-BenchåŒ…å«æ¥è‡ªçœŸå®ç½‘ç«™çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼Œæ¶µç›–ä¸åŒçš„äº¤äº’å¤æ‚æ€§ã€è§†è§‰é£æ ¼å’Œé¢†åŸŸã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹åœ¨ç»¼åˆå¤šæ¨¡å¼æ¨ç†å’Œå…ˆè¿›ä»£ç ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨åŠŸèƒ½æ­£ç¡®æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œè¡¨æ˜å½“å‰æ¨¡å‹åœ¨æ¨ç†æ—¶é—´åŠ¨æ€å’Œåˆæˆäº‹ä»¶é©±åŠ¨é€»è¾‘æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>IWR-Benchä¸ºè§†è§‰è¯­è¨€ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å‰æ²¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24709">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-28be6e9bd56a26bd99618c34c78292ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735461&auth_key=1760735461-0-0-70fb8f0cda272707d7d0e9e89fc6f58c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b1516eb9baee34ba4a11622042a14b73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735468&auth_key=1760735468-0-0-ad465eb44b20f5b8b49eac4dfa7f92df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30e69b2dcb1dbda02380105734227290~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735498&auth_key=1760735498-0-0-997f874d7a3989c0440ae202fb1a6c42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3c345c8ea14fee54c2909c055fbc983~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735504&auth_key=1760735504-0-0-4f918f4351fc8729b0317f70d9b12978&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c2a6461336a9f48dac88ec8d27ffa72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735532&auth_key=1760735532-0-0-613f118b05e22be0d1ef5917fd11b13b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fc74284a4c241dd9a946837c261af5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735538&auth_key=1760735538-0-0-65f5f971a60d72b1adaaaf774c7a3b1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Goal-Based-Vision-Language-Driving"><a href="#Goal-Based-Vision-Language-Driving" class="headerlink" title="Goal-Based Vision-Language Driving"></a>Goal-Based Vision-Language Driving</h2><p><strong>Authors:Santosh Patapati, Trisanth Srinivasan</strong></p>
<p>Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes &#x2F; Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDriveâ€™s shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†NovaDriveï¼Œè¿™æ˜¯ä¸€ç§å•åˆ†æ”¯è§†è§‰è¯­è¨€æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªåˆ†æ”¯ä¸­å¤„ç†å‰ç½®æ‘„åƒå¤´å›¾åƒã€é«˜æ¸…åœ°å›¾ç“¦ç‰‡ã€æ¿€å…‰é›·è¾¾æ·±åº¦å’Œæ–‡æœ¬èˆªç‚¹ã€‚é€šè¿‡ä¸€ä¸ªè½»ä¾¿çš„ä¸¤é˜¶æ®µäº¤å‰æ³¨æ„åŠ›å—ï¼Œé¦–å…ˆå¯¹é½èˆªç‚¹æ ‡è®°ä¸é«˜æ¸…åœ°å›¾ï¼Œç„¶åç²¾ç»†è°ƒæ•´å›¾åƒå’Œæ·±åº¦è¡¥ä¸çš„æ³¨æ„åŠ›ã€‚ç»“åˆä¸€ç§æ–°å‹å¹³æ»‘æŸå¤±ï¼Œè¯¥è®¾è®¡å¯é¿å…æ€¥è½¬å¼¯å’Œé€Ÿåº¦çªå˜ï¼Œæ¶ˆé™¤äº†å¯¹å¾ªç¯å†…å­˜çš„éœ€æ±‚ã€‚æˆ‘ä»¬å¾®è°ƒäº†11B LLaMA-3.2è§†è§‰è¯­è¨€ä¸»å¹²çš„é¡¶å±‚15å±‚ï¼Œå®ç°äº†å®æ—¶æ¨ç†ã€‚åœ¨MD-NEXæˆ·å¤–åŸºå‡†çš„nuScenes&#x2F;Waymoå­é›†ä¸Šï¼ŒNovaDriveå°†æˆåŠŸç‡æé«˜åˆ°84%ï¼ˆ+4%ï¼‰ï¼Œè·¯å¾„æ•ˆç‡ï¼ˆSPLï¼‰æé«˜åˆ°0.66ï¼ˆ+0.11ï¼‰ï¼Œä¸ä¹‹å‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œç¢°æ’é¢‘ç‡ä»2.6%é™ä½åˆ°1.2%ï¼ˆ-1.4%ï¼‰ã€‚æˆ‘ä»¬çš„æ¶ˆèå®éªŒè¯å®ï¼Œèˆªç‚¹æ ‡è®°ã€éƒ¨åˆ†VLMå¾®è°ƒä»¥åŠäº¤å‰æ³¨æ„åŠ›èåˆéƒ½å¯¹è¿™äº›å¢ç›Šè´¡çŒ®æœ€å¤§ã€‚é™¤äº†æé«˜å®‰å…¨æ€§å¤–ï¼ŒNovaDriveçš„æ›´çŸ­è·¯çº¿ï¼ˆç”±æ–°å‹å¹³æ»‘æŸå¤±äº§ç”Ÿï¼‰è½¬åŒ–ä¸ºæ›´ä½çš„ç‡ƒæ²¹æˆ–ç”µæ± ä½¿ç”¨é‡ï¼ŒæŒ‡å‘æ›´ç²¾ç®€ã€æ›´å®¹æ˜“æ›´æ–°çš„é©¾é©¶å †æ ˆã€‚NovaDriveè¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å®ä½“äººå·¥æ™ºèƒ½é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23042v2">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†NovaDriveç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ç§å•ä¸€åˆ†æ”¯çš„è§†è§‰è¯­è¨€æ¶æ„ï¼Œç”¨äºå¤„ç†è‡ªåŠ¨é©¾é©¶ä¸­çš„å¤æ‚æƒ…å†µã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿå®æ—¶å¤„ç†å‰è§†æ‘„åƒå¤´å›¾åƒã€é«˜æ¸…åœ°å›¾ç“¦ç‰‡ã€æ¿€å…‰é›·è¾¾æ·±åº¦å’Œæ–‡æœ¬èˆªç‚¹ä¿¡æ¯ã€‚é€šè¿‡ä¸€ç§æ–°çš„è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼ŒNovaDriveèƒ½å¤Ÿç²¾ç»†åœ°åˆ†æè·¯å†µå¹¶åšå‡ºå†³ç­–ï¼ŒåŒæ—¶ç»“åˆæ–°å‹å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œå‡å°‘äº†æ€¥è½¬å¼¯å’Œé€Ÿåº¦å˜åŒ–ï¼Œæé«˜äº†é©¾é©¶çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚åœ¨MD-NEX OutdooråŸºå‡†æµ‹è¯•ä¸­ï¼ŒNovaDriveç›¸è¾ƒäºå…¶ä»–æŠ€æœ¯å–å¾—äº†æ›´é«˜çš„æˆåŠŸç‡ã€è·¯å¾„æ•ˆç‡å’Œæ›´ä½çš„ç¢°æ’é¢‘ç‡ã€‚æ­¤å¤–ï¼ŒNovaDriveè¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–åµŒå…¥å¼äººå·¥æ™ºèƒ½é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NovaDriveæ˜¯ä¸€ä¸ªå•ä¸€åˆ†æ”¯çš„è§†è§‰è¯­è¨€æ¶æ„ï¼Œç”¨äºå¤„ç†è‡ªåŠ¨é©¾é©¶ä¸­çš„å¤æ‚æƒ…å†µã€‚å®ƒå®æ—¶å¤„ç†å¤šç§ä¿¡æ¯å¦‚æ‘„åƒå¤´å›¾åƒã€é«˜æ¸…åœ°å›¾ç“¦ç‰‡ç­‰ã€‚</li>
<li>NovaDriveé‡‡ç”¨äº†åˆ›æ–°çš„è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿç²¾ç»†åœ°åˆ†æè·¯å†µå¹¶åšå‡ºå†³ç­–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç³»ç»Ÿå¯ä»¥å¿«é€Ÿå“åº”å¹¶å¤„ç†ç´§æ€¥æƒ…å†µã€‚</li>
<li>NovaDriveç»“åˆäº†æ–°å‹å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œå‡å°‘äº†æ€¥è½¬å¼¯å’Œé€Ÿåº¦å˜åŒ–ï¼Œæé«˜äº†é©¾é©¶çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒNovaDriveç›¸è¾ƒäºå…¶ä»–æŠ€æœ¯å–å¾—äº†æ›´é«˜çš„æˆåŠŸç‡ã€è·¯å¾„æ•ˆç‡å’Œæ›´ä½çš„ç¢°æ’é¢‘ç‡ã€‚</li>
<li>NovaDriveé€šè¿‡ç¼©çŸ­è·¯çº¿é™ä½äº†ç‡ƒæ–™æˆ–ç”µæ± çš„ä½¿ç”¨é‡ï¼Œä½¿é©¾é©¶æ›´åŠ ç¯ä¿å’Œé«˜æ•ˆã€‚æ­¤å¤–ï¼Œå…¶è®¾è®¡è¿˜ä½¿å…¶æ˜“äºæ›´æ–°å’Œç»´æŠ¤ã€‚</li>
<li>NovaDriveçš„åº”ç”¨ä¸ä»…é™äºè‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œè¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–åµŒå…¥å¼äººå·¥æ™ºèƒ½é¢†åŸŸã€‚è¿™è¡¨æ˜å…¶å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§å’Œæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b89d9e8e6d07ffbb2d4eef856153608e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735546&auth_key=1760735546-0-0-9c2efae19ef4bbdc1a1f27e8253c7d60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73a16cc0d5562a56bdcda872a1a506ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735553&auth_key=1760735553-0-0-e716a4279619287301aba5e3cb60ee50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1113797b854b9521854db2a6f596277a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735559&auth_key=1760735559-0-0-456674184d15b63d2410352d484d1474&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Endoscopic-Depth-Estimation-Based-on-Deep-Learning-A-Survey"><a href="#Endoscopic-Depth-Estimation-Based-on-Deep-Learning-A-Survey" class="headerlink" title="Endoscopic Depth Estimation Based on Deep Learning: A Survey"></a>Endoscopic Depth Estimation Based on Deep Learning: A Survey</h2><p><strong>Authors:Ke Niu, Zeyun Liu, Xue Feng, Heng Li, Qika Lin, Kaize Shi</strong></p>
<p>Endoscopic depth estimation is a critical technology for improving the safety and precision of minimally invasive surgery. It has attracted considerable attention from researchers in medical imaging, computer vision, and robotics. Over the past decade, a large number of methods have been developed. Despite the existence of several related surveys, a comprehensive overview focusing on recent deep learning-based techniques is still limited. This paper endeavors to bridge this gap by systematically reviewing the state-of-the-art literature. Specifically, we provide a thorough survey of the field from three key perspectives: data, methods, and applications. Firstly, at the data level, we describe the acquisition process of publicly available datasets. Secondly, at the methodological level, we introduce both monocular and stereo deep learning-based approaches for endoscopic depth estimation. Thirdly, at the application level, we identify the specific challenges and corresponding solutions for the clinical implementation of depth estimation technology, situated within concrete clinical scenarios. Finally, we outline potential directions for future research, such as domain adaptation, real-time implementation, and the synergistic fusion of depth information with sensor technologies, thereby providing a valuable starting point for researchers to engage with and advance the field toward clinical translation. </p>
<blockquote>
<p>å†…çª¥é•œæ·±åº¦ä¼°è®¡æ˜¯æé«˜å¾®åˆ›æ‰‹æœ¯å®‰å…¨æ€§å’Œç²¾åº¦çš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚å®ƒå¼•èµ·äº†åŒ»å­¦å½±åƒã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººç­‰é¢†åŸŸç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚åœ¨è¿‡å»çš„åå¹´é‡Œï¼Œå·²ç»å¼€å‘äº†å¤§é‡çš„æ–¹æ³•ã€‚å°½ç®¡å­˜åœ¨å‡ ä¸ªç›¸å…³è°ƒæŸ¥ï¼Œä½†é‡ç‚¹å…³æ³¨æœ€è¿‘åŸºäºæ·±åº¦å­¦ä¹ çš„æŠ€æœ¯çš„å…¨é¢æ¦‚è¿°ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç³»ç»Ÿå›é¡¾æœ€æ–°æ–‡çŒ®æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»æ•°æ®ã€æ–¹æ³•å’Œåº”ç”¨ä¸‰ä¸ªå…³é”®è§’åº¦å¯¹è¯¥é¢†åŸŸè¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ã€‚é¦–å…ˆï¼Œåœ¨æ•°æ®å±‚é¢ï¼Œæˆ‘ä»¬æè¿°äº†å…¬å¼€æ•°æ®é›†çš„é‡‡é›†è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œåœ¨æ–¹æ³•å±‚é¢ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºå•ç›®å’Œç«‹ä½“è§†è§‰çš„æ·±åº¦å­¦ä¹ å†…çª¥é•œæ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚ç¬¬ä¸‰ï¼Œåœ¨åº”ç”¨å±‚é¢ï¼Œæˆ‘ä»¬ç¡®å®šäº†åœ¨ä¸´åºŠåœºæ™¯ä¸­å®æ–½æ·±åº¦ä¼°è®¡æŠ€æœ¯æ‰€é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ï¼Œå¦‚åŸŸé€‚åº”ã€å®æ—¶å®æ–½ä»¥åŠæ·±åº¦ä¿¡æ¯ä¸ä¼ æ„Ÿå™¨æŠ€æœ¯çš„ååŒèåˆï¼Œä»è€Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå®è´µçš„èµ·ç‚¹ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸå‘ä¸´åºŠè½¬åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20881v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å†…é•œæ·±åº¦ä¼°è®¡æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œä»æ•°æ®ã€æ–¹æ³•å’Œåº”ç”¨ä¸‰ä¸ªå…³é”®è§’åº¦è¿›è¡Œäº†å…¨é¢æ¦‚è¿°ã€‚ä»‹ç»äº†å…¬å¼€æ•°æ®é›†çš„é‡‡é›†è¿‡ç¨‹ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•ä¸­çš„å•ç›®å’Œç«‹ä½“è§†è§‰æ–¹æ³•ï¼Œä»¥åŠæ·±åº¦ä¼°è®¡æŠ€æœ¯åœ¨å…·ä½“ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚æœ€åï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¦‚åŸŸé€‚åº”ã€å®æ—¶å®ç°ä»¥åŠæ·±åº¦ä¿¡æ¯ä¸ä¼ æ„Ÿå™¨æŠ€æœ¯çš„èåˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†…é•œæ·±åº¦ä¼°è®¡æ˜¯æé«˜å¾®åˆ›å¤–ç§‘æ‰‹æœ¯å®‰å…¨æ€§å’Œç²¾åº¦çš„é‡è¦æŠ€æœ¯ã€‚</li>
<li>è¯¥é¢†åŸŸå·²å¸å¼•äº†åŒ»ç–—æˆåƒã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ç­‰æ–¹é¢ç ”ç©¶äººå‘˜çš„å…³æ³¨ã€‚</li>
<li>ç»¼è¿°æ–‡ç« ç³»ç»Ÿä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å†…é•œæ·±åº¦ä¼°è®¡æŠ€æœ¯çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>ä»æ•°æ®ã€æ–¹æ³•å’Œåº”ç”¨ä¸‰ä¸ªè§’åº¦å…¨é¢æ¦‚è¿°äº†è¯¥é¢†åŸŸçš„ç°çŠ¶ã€‚</li>
<li>å…¬å¼€æ•°æ®é›†çš„é‡‡é›†è¿‡ç¨‹ã€æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸­çš„å•ç›®å’Œç«‹ä½“è§†è§‰æ–¹æ³•å¾—åˆ°äº†è¯¦ç»†ä»‹ç»ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†æ·±åº¦ä¼°è®¡æŠ€æœ¯åœ¨å…·ä½“ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3f4bb4609e800341f1517a285c0d6755~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735566&auth_key=1760735566-0-0-bc6305965096c233fd66f68ad6148de2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bea19531445d83d4ecd1a604b2ef164b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735573&auth_key=1760735573-0-0-8880caa2485ca11c6bb13c27c35e983e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6db195581bcc94d50438af72e93c30bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735579&auth_key=1760735579-0-0-68136160e9599fbf63cd75860820ad2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-72e35ac3aadf30a2832988ada6f1bb29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735586&auth_key=1760735586-0-0-a02c43d284f1ef8e565f74550e357ba8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be2e6e5378b80d55fb30570fd9861838~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735592&auth_key=1760735592-0-0-97158b7fe856519c4c2fa9352e4c9c2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d49db0d17856618039e96b04edb40849~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735599&auth_key=1760735599-0-0-746919876d8d3145b2527c71272b6ddc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling"><a href="#GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling" class="headerlink" title="GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling"></a>GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling</h2><p><strong>Authors:Siran Li, Ruiyang Liu, Chen Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang</strong></p>
<p>Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments. We introduce GarmageNet, a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. Central to our approach is Garmage, a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment geometries. Followed by GarmageNet, a latent diffusion transformer to synthesize panel-wise geometry images and GarmageJigsaw, a neural module for predicting point-to-point sewing connections along panel contours. To support training and evaluation, we build GarmageSet, a large-scale dataset comprising 14,801 professionally designed garments with detailed structural and style annotations. Our method demonstrates versatility and efficacy across multiple application scenarios, including scalable garment generation from multi-modal design concepts (text prompts, sketches, photographs), automatic modeling from raw flat sewing patterns, pattern recovery from unstructured point clouds, and progressive garment editing using conventional instructions, laying the foundation for fully automated, production-ready pipelines in digital fashion. Project page: <a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet/">https://style3d.github.io/garmagenet/</a>. </p>
<blockquote>
<p>ç°å®ä¸»ä¹‰çš„æ•°å­—æœè£…å»ºæ¨¡ä»ç„¶æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå°†äºŒç»´ç¼çº«å›¾æ¡ˆè½¬åŒ–ä¸ºé«˜ä¿çœŸã€å¯è¿›è¡Œæ¨¡æ‹Ÿçš„3Dæœè£…çš„å¤æ‚è¿‡ç¨‹ã€‚æˆ‘ä»¬å¼•å…¥äº†GarmageNetï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥è‡ªåŠ¨åˆ›å»º2Dç¼çº«å›¾æ¡ˆã€æ„å»ºç¼çº«å…³ç³»ï¼Œå¹¶åˆæˆä¸åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå…¼å®¹çš„3Dæœè£…åˆå§‹åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯Garmageï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æœè£…è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒå°†æ¯ä¸ªé¢æ¿ç¼–ç ä¸ºç»“æ„åŒ–çš„å‡ ä½•å›¾åƒï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†äºŒç»´ç»“æ„å›¾æ¡ˆå’Œä¸‰ç»´æœè£…å‡ ä½•ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚å…¶æ¬¡æ˜¯GarmageNetï¼Œä¸€ä¸ªç”¨äºåˆæˆé¢æ¿çº§å‡ ä½•å›¾åƒçš„æ½œåœ¨æ‰©æ•£å˜å‹å™¨ï¼Œä»¥åŠGarmageJigsawï¼Œä¸€ä¸ªç”¨äºé¢„æµ‹é¢æ¿è½®å»“æ²¿çº¿ç‚¹å¯¹ç‚¹ç¼çº«è¿æ¥çš„ç¥ç»ç½‘ç»œæ¨¡å—ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†GarmageSetï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«14801ä¸ªä¸“ä¸šè®¾è®¡çš„æœè£…çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…·æœ‰è¯¦ç»†çš„ç»“æ„å’Œé£æ ¼æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†åœ¨å¤šä¸ªåº”ç”¨åœºæ™¯ä¸‹çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä»å¤šæ¨¡å¼è®¾è®¡æ¦‚å¿µï¼ˆæ–‡æœ¬æç¤ºã€è‰å›¾ã€ç…§ç‰‡ï¼‰è¿›è¡Œå¯æ‰©å±•çš„æœè£…ç”Ÿæˆã€ä»åŸå§‹å¹³é¢ç¼çº«å›¾æ¡ˆçš„è‡ªåŠ¨å»ºæ¨¡ã€ä»éç»“æ„åŒ–çš„ç‚¹äº‘ä¸­çš„æ¨¡å¼æ¢å¤ï¼Œä»¥åŠä½¿ç”¨å¸¸è§„æŒ‡ä»¤è¿›è¡Œæ¸è¿›å¼çš„æœè£…ç¼–è¾‘ï¼Œä¸ºæ•°å­—æ—¶å°šçš„å®Œå…¨è‡ªåŠ¨åŒ–ã€ç”Ÿäº§å‡†å¤‡ç®¡é“å¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet/%E3%80%82">https://style3d.github.io/garmagenet/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01483v4">PDF</a> 23 pages,20 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†GarmageNetæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®ç°äº†è‡ªåŠ¨åŒ–åˆ›å»ºäºŒç»´ç¼çº«å›¾æ¡ˆã€æ„å»ºç¼çº«å…³ç³»ä»¥åŠåˆæˆå¯ç”¨äºç‰©ç†æ¨¡æ‹Ÿçš„ä¸‰ç»´æœè£…åˆå§‹åŒ–ã€‚å…¶æ ¸å¿ƒåœ¨äºGarmageæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†æ¯ä¸ªæœè£…é¢æ¿ç¼–ç ä¸ºç»“æ„åŒ–å‡ ä½•å›¾åƒï¼Œæœ‰æ•ˆæ¡¥æ¥äº†äºŒç»´ç»“æ„å›¾æ¡ˆå’Œä¸‰ç»´æœè£…å‡ ä½•ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰GarmageNetç”¨äºåˆæˆé¢æ¿å‡ ä½•å›¾åƒï¼Œä»¥åŠGarmageJigsawé¢„æµ‹é¢æ¿è½®å»“çš„ç‚¹åˆ°ç‚¹ç¼çº«è¿æ¥ã€‚ä¸ºæ”¯æŒå’Œè¯„ä¼°è¯¥æ–¹æ³•ï¼Œå»ºç«‹äº†åŒ…å«ä¸“ä¸šè®¾è®¡æœè£…çš„å¤§å‹æ•°æ®é›†GarmageSetã€‚æ­¤æŠ€æœ¯å…·æœ‰è·¨å¤šç§åº”ç”¨åœºæ™¯çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä»å¤šæ¨¡æ€è®¾è®¡æ¦‚å¿µï¼ˆæ–‡æœ¬æç¤ºã€è‰å›¾ã€ç…§ç‰‡ï¼‰ç”Ÿæˆæœè£…ã€ä»åŸå§‹å¹³é¢ç¼çº«å›¾æ¡ˆè‡ªåŠ¨å»ºæ¨¡ã€ä»æ— åºç‚¹äº‘ä¸­æ¢å¤å›¾æ¡ˆä»¥åŠä½¿ç”¨å¸¸è§„æŒ‡ä»¤è¿›è¡Œæ¸è¿›å¼æœè£…ç¼–è¾‘ã€‚è¿™ä¸ºå…¨è‡ªåŠ¨ç”Ÿäº§å‡†å¤‡ç®¡é“å¥ å®šäº†æ•°å­—æ—¶å°šçš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GarmageNetæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–åˆ›å»ºäºŒç»´ç¼çº«å›¾æ¡ˆã€æ„å»ºç¼çº«å…³ç³»ï¼Œå¹¶åˆæˆå¯ç”¨äºç‰©ç†æ¨¡æ‹Ÿçš„ä¸‰ç»´æœè£…åˆå§‹åŒ–ã€‚</li>
<li>Garmageæ˜¯ä¸€ç§æ–°å‹çš„æœè£…è¡¨ç¤ºæ–¹æ³•ï¼Œå°†æ¯ä¸ªæœè£…é¢æ¿ç¼–ç ä¸ºç»“æ„åŒ–å‡ ä½•å›¾åƒï¼Œä»¥æ¡¥æ¥äºŒç»´å’Œä¸‰ç»´ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚</li>
<li>GarmageNetç”¨äºåˆæˆé¢æ¿å‡ ä½•å›¾åƒï¼Œè€ŒGarmageJigsawåˆ™é¢„æµ‹é¢æ¿è½®å»“çš„ç‚¹åˆ°ç‚¹ç¼çº«è¿æ¥ã€‚</li>
<li>ä¸ºæ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†GarmageSetï¼ŒåŒ…å«ä¸“ä¸šè®¾è®¡çš„æœè£…å’Œè¯¦ç»†çš„ç»“æ„å’Œé£æ ¼æ³¨é‡Šã€‚</li>
<li>è¯¥æŠ€æœ¯å…·æœ‰å¤šç§åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬ä»å¤šæ¨¡æ€è®¾è®¡æ¦‚å¿µç”Ÿæˆæœè£…ã€ä»åŸå§‹å¹³é¢ç¼çº«å›¾æ¡ˆå»ºæ¨¡ã€ä»æ— åºç‚¹äº‘ä¸­æ¢å¤å›¾æ¡ˆä»¥åŠä½¿ç”¨å¸¸è§„æŒ‡ä»¤è¿›è¡Œæ¸è¿›å¼æœè£…ç¼–è¾‘ã€‚</li>
<li>æ­¤æŠ€æœ¯æœ‰åŠ©äºå»ºç«‹å…¨è‡ªåŠ¨ã€å¯ç”¨äºç”Ÿäº§çš„æ•°å­—æ—¶å°šç®¡é“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-412579f9702b54d2218d6cd838a2cdbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735606&auth_key=1760735606-0-0-19f9712344346e782193ffba6fcb36c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f22961a8a33f4707616da41edcc123c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735613&auth_key=1760735613-0-0-50b3ad6b387c608110c63c6120d16728&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aee5334c1385747fb6e9ddea7a21897e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735620&auth_key=1760735620-0-0-233513eea089e70697d5c341329517e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fcd444a9028a1d788913ab89c3c426ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735627&auth_key=1760735627-0-0-4721a646cc07cadd39a031cd2c861ae2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input"><a href="#Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input" class="headerlink" title="Real-Time Position-Aware View Synthesis from Single-View Input"></a>Real-Time Position-Aware View Synthesis from Single-View Input</h2><p><strong>Authors:Manu Gond, Emin Zerman, Sebastian Knorr, MÃ¥rten SjÃ¶strÃ¶m</strong></p>
<p>Recent advancements in view synthesis have significantly enhanced immersive experiences across various computer graphics and multimedia applications, including telepresence and entertainment. By enabling the generation of new perspectives from a single input view, view synthesis allows users to better perceive and interact with their environment. However, many state-of-the-art methods, while achieving high visual quality, face limitations in real-time performance, which makes them less suitable for live applications where low latency is critical. In this paper, we present a lightweight, position-aware network designed for real-time view synthesis from a single input image and a target camera pose. The proposed framework consists of a Position Aware Embedding, which efficiently maps positional information from the target pose to generate high dimensional feature maps. These feature maps, along with the input image, are fed into a Rendering Network that merges features from dual encoder branches to resolve both high and low level details, producing a realistic new view of the scene. Experimental results demonstrate that our method achieves superior efficiency and visual quality compared to existing approaches, particularly in handling complex translational movements without explicit geometric operations like warping. This work marks a step toward enabling real-time live and interactive telepresence applications. </p>
<blockquote>
<p>è¿‘æœŸè§†å›¾åˆæˆæŠ€æœ¯çš„è¿›å±•åœ¨å¤šç§è®¡ç®—æœºå›¾å½¢å­¦å’Œå¤šåª’ä½“åº”ç”¨ä¸­æå¤§åœ°å¢å¼ºäº†æ²‰æµ¸å¼ä½“éªŒï¼ŒåŒ…æ‹¬è¿œç¨‹å­˜åœ¨å’Œå¨±ä¹ã€‚è§†å›¾åˆæˆèƒ½å¤Ÿé€šè¿‡å•ä¸ªè¾“å…¥è§†å›¾ç”Ÿæˆæ–°çš„è§†è§’ï¼Œä½¿ç”¨æˆ·æ›´å¥½åœ°æ„ŸçŸ¥å’Œä¸ç¯å¢ƒäº’åŠ¨ã€‚ç„¶è€Œï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ–¹æ³•è™½ç„¶è§†è§‰è´¨é‡å¾ˆé«˜ï¼Œä½†åœ¨å®æ—¶æ€§èƒ½æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨ä½å»¶è¿Ÿè‡³å…³é‡è¦çš„å®æ—¶åº”ç”¨ä¸­ä¸å¤ªé€‚ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§ã€ä½ç½®æ„ŸçŸ¥çš„ç½‘ç»œï¼Œç”¨äºä»å•ä¸ªè¾“å…¥å›¾åƒå’Œç›®æ ‡ç›¸æœºå§¿æ€è¿›è¡Œå®æ—¶è§†å›¾åˆæˆã€‚æ‰€æå‡ºçš„æ¡†æ¶åŒ…å«ä¸€ä¸ªä½ç½®æ„ŸçŸ¥åµŒå…¥ï¼Œå®ƒæœ‰æ•ˆåœ°å°†ç›®æ ‡å§¿æ€çš„ä½ç½®ä¿¡æ¯æ˜ å°„åˆ°ç”Ÿæˆçš„é«˜ç»´ç‰¹å¾å›¾ä¸Šã€‚è¿™äº›ç‰¹å¾å›¾ä¸è¾“å…¥å›¾åƒä¸€èµ·è¾“å…¥åˆ°æ¸²æŸ“ç½‘ç»œä¸­ï¼Œè¯¥ç½‘ç»œåˆå¹¶æ¥è‡ªåŒç¼–ç å™¨åˆ†æ”¯çš„ç‰¹å¾ä»¥è§£å†³é«˜çº§å’Œä½çº§ç»†èŠ‚é—®é¢˜ï¼Œç”Ÿæˆåœºæ™¯çš„çœŸå®æ–°è§†å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œè§†è§‰è´¨é‡æ–¹é¢å®ç°äº†ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¹³ç§»è¿åŠ¨è€Œæ— éœ€è¿›è¡Œå¦‚æ‰­æ›²ç­‰æ˜ç¡®çš„å‡ ä½•æ“ä½œæ–¹é¢ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€å®æ—¶ç›´æ’­å’Œäº¤äº’å¼è¿œç¨‹å­˜åœ¨åº”ç”¨èµ°å‘å¯èƒ½çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14005v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£è§†å›¾åˆæˆæŠ€æœ¯å¢å¼ºäº†è®¡ç®—æœºå›¾å½¢å’Œå¤šåª’ä½“åº”ç”¨çš„æ²‰æµ¸å¼ä½“éªŒï¼ŒåŒ…æ‹¬è¿œç¨‹å­˜åœ¨å’Œå¨±ä¹ã€‚é€šè¿‡å•ä¸€è§†è§’ç”Ÿæˆæ–°è§†è§’çš„èƒ½åŠ›ï¼Œè§†å›¾åˆæˆè®©ç”¨æˆ·æ›´å¥½åœ°æ„ŸçŸ¥å’Œäº¤äº’ä»–ä»¬çš„ç¯å¢ƒã€‚ä½†è®¸å¤šæœ€æ–°æ–¹æ³•è™½ç„¶è§†è§‰è´¨é‡é«˜ï¼Œä½†å®æ—¶æ€§èƒ½æœ‰é™ï¼Œä¸é€‚ç”¨äºä½å»¶è¿Ÿçš„å®æ—¶åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€ä½ç½®æ„ŸçŸ¥çš„ç½‘ç»œï¼Œä¸“ä¸ºä»å•ä¸€è¾“å…¥å›¾åƒå’Œç›®æ ‡ç›¸æœºå§¿æ€è¿›è¡Œå®æ—¶è§†å›¾åˆæˆè€Œè®¾è®¡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä½ç½®æ„ŸçŸ¥åµŒå…¥ï¼Œæœ‰æ•ˆåœ°å°†ç›®æ ‡å§¿æ€çš„ä½ç½®ä¿¡æ¯æ˜ å°„åˆ°ç”Ÿæˆçš„é«˜ç»´ç‰¹å¾å›¾ä¸Šã€‚è¿™äº›ç‰¹å¾å›¾ä¸è¾“å…¥å›¾åƒä¸€èµ·è¾“å…¥åˆ°æ¸²æŸ“ç½‘ç»œä¸­ï¼Œåˆå¹¶æ¥è‡ªåŒç¼–ç å™¨åˆ†æ”¯çš„ç‰¹å¾ä»¥è§£å†³é«˜çº§å’Œä½çº§ç»†èŠ‚é—®é¢˜ï¼Œç”Ÿæˆé€¼çœŸçš„æ–°åœºæ™¯è§†å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œè§†è§‰è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¹³ç§»è¿åŠ¨æ–¹é¢ï¼Œæ— éœ€æ˜ç¡®çš„å‡ ä½•æ“ä½œå¦‚æ‰­æ›²ã€‚è¿™é¡¹å·¥ä½œæœç€å®ç°å®æ—¶ç›´æ’­å’Œäº¤äº’å¼è¿œç¨‹å­˜åœ¨åº”ç”¨è¿ˆå‡ºäº†ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†å›¾åˆæˆæŠ€æœ¯å¢å¼ºäº†è®¡ç®—æœºå›¾å½¢å’Œå¤šåª’ä½“åº”ç”¨çš„æ²‰æµ¸å¼ä½“éªŒã€‚</li>
<li>ç°æœ‰è§†å›¾åˆæˆæ–¹æ³•åœ¨å®æ—¶æ€§èƒ½æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸é€‚ç”¨äºä½å»¶è¿Ÿçš„å®æ—¶åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€ä½ç½®æ„ŸçŸ¥çš„ç½‘ç»œï¼Œç”¨äºä»å•ä¸€è¾“å…¥å›¾åƒå’Œç›®æ ‡ç›¸æœºå§¿æ€è¿›è¡Œå®æ—¶è§†å›¾åˆæˆã€‚</li>
<li>ä½ç½®æ„ŸçŸ¥åµŒå…¥æœ‰æ•ˆåœ°æ˜ å°„ç›®æ ‡å§¿æ€çš„ä½ç½®ä¿¡æ¯åˆ°é«˜ç»´ç‰¹å¾å›¾ã€‚</li>
<li>æ¸²æŸ“ç½‘ç»œåˆå¹¶æ¥è‡ªåŒç¼–ç å™¨åˆ†æ”¯çš„ç‰¹å¾ä»¥ç”Ÿæˆé€¼çœŸçš„æ–°åœºæ™¯è§†å›¾ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œè§†è§‰è´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¹³ç§»è¿åŠ¨æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-164f05d12290fd506323ede81f11d057~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735634&auth_key=1760735634-0-0-8b1e83413de3ee332a709e592830584f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-391699baa8e0a7214936ac4da2e7026a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735643&auth_key=1760735643-0-0-6a687c102cfea99a3718b6f4b9bb285e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models"><a href="#Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models" class="headerlink" title="Go With the Flow: Fast Diffusion for Gaussian Mixture Models"></a>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</h2><p><strong>Authors:George Rapakoulias, Ali Reza Pedram, Fengjiao Liu, Lingjiong Zhu, Panagiotis Tsiotras</strong></p>
<p>Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SBs between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SBs, and various other examples. The implementation is publicly available at <a target="_blank" rel="noopener" href="https://github.com/georgeRapa/GMMflow">https://github.com/georgeRapa/GMMflow</a>. </p>
<blockquote>
<p>è–›å®šè°”æ¡¥ï¼ˆSBsï¼‰æ˜¯ä¸€ç§æ‰©æ•£è¿‡ç¨‹ï¼Œåœ¨æœ‰é™æ—¶é—´å†…ï¼Œå®ƒå°†ç»™å®šçš„åˆå§‹åˆ†å¸ƒå¼•å¯¼åˆ°å¦ä¸€ä¸ªæœ€ç»ˆåˆ†å¸ƒï¼ŒåŒæ—¶æœ€å°åŒ–é€‚å½“çš„æˆæœ¬å‡½æ•°ã€‚å°½ç®¡æœ€è¿‘åœ¨æ–‡çŒ®ä¸­æå‡ºäº†å„ç§è®¡ç®—SBsçš„æ–¹æ³•ï¼Œä½†å³ä½¿å¯¹äºè§£å†³ä½ç»´é—®é¢˜ï¼Œå¤§å¤šæ•°è¿™äº›æ–¹æ³•ä¹Ÿéœ€è¦è®¡ç®—æˆæœ¬é«˜æ˜‚çš„è®­ç»ƒæ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç»„å¯è¡Œç­–ç•¥çš„è§£æå‚æ•°åŒ–æ–¹æ³•ï¼Œä»¥å¼•å¯¼åŠ¨åŠ›ç³»ç»Ÿä»ä¸€ä¸ªé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰åˆ°å¦ä¸€ä¸ªGMMçš„åˆ†å¸ƒã€‚æˆ‘ä»¬ä¸å¿…ä¾èµ–äºæ ‡å‡†çš„éå‡¸ä¼˜åŒ–æŠ€æœ¯ï¼Œè€Œæ˜¯å¯ä»¥é€šè¿‡æ±‚è§£ä¸€ä¸ªä½ç»´çº¿æ€§è§„åˆ’é—®é¢˜æ¥è¿‘ä¼¼é›†åˆä¸­çš„æœ€ä¼˜ç­–ç•¥ï¼Œè¯¥é—®é¢˜çš„ç»´åº¦éšç€æ··åˆä¸­æ¯ä¸ªç»„ä»¶çš„æ•°é‡è€Œçº¿æ€§æ‰©å±•ã€‚æ‰€æå‡ºçš„æ–¹æ³•è‡ªç„¶åœ°æ¨å¹¿åˆ°æ›´ä¸€èˆ¬çš„åŠ¨åŠ›ç³»ç»Ÿç±»åˆ«ï¼Œå¦‚å¯æ§çº¿æ€§æ—¶å˜ç³»ç»Ÿï¼Œä¸ºè§£å†³å¤šè¾¹ç¼˜åŠ¨é‡SBsä¹‹é—´çš„GMMé—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åˆ†å¸ƒæ’å€¼é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å›¾åƒåˆ°å›¾åƒç¿»è¯‘ã€åœ¨è‡ªåŠ¨ç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ ç»†èƒåŠ¨åŠ›å­¦ä»¥åŠä½¿ç”¨å¤šè¾¹ç¼˜åŠ¨é‡SBsçš„å„ç§å…¶ä»–ç¤ºä¾‹æ¥å±•ç¤ºæ­¤æ–¹æ³•çš„æ½œåŠ›ã€‚å®ç°å…¬å¼€å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/georgeRapa/GMMflow%E3%80%82">https://github.com/georgeRapa/GMMflowã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09059v6">PDF</a> NIPS 2025 (spotlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰çš„Schrodingeræ¡¥ï¼ˆSBï¼‰çš„è§£æå‚æ•°åŒ–æ–¹æ³•ï¼Œç”¨äºåœ¨æœ‰é™æ—¶é—´å†…å¼•å¯¼ç³»ç»Ÿçš„åˆ†å¸ƒä»ä¸€ä¸ªçŠ¶æ€è½¬ç§»åˆ°å¦ä¸€ä¸ªçŠ¶æ€ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½ç»´çº¿æ€§è§„åˆ’æ¥è¿‘ä¼¼æœ€ä¼˜ç­–ç•¥ï¼Œå¯è‡ªç„¶æ¨å¹¿åˆ°æ›´ä¸€èˆ¬çš„åŠ¨åŠ›ç³»ç»Ÿï¼Œå¦‚å¯æ§çº¿æ€§æ—¶å˜ç³»ç»Ÿã€‚æ­¤æ–¹æ³•è§£å†³äº†å¤šè¾¹ç¼˜åŠ¨é‡SBåœ¨GMMä¹‹é—´çš„åˆ†å¸ƒæ’å€¼é—®é¢˜ï¼Œå¹¶åœ¨å›¾åƒç¿»è¯‘ã€ç»†èƒåŠ¨åŠ›å­¦å­¦ä¹ ç­‰é¢†åŸŸå±•ç¤ºäº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Schrodinger Bridges (SBs) æ˜¯å¼•å¯¼åˆå§‹åˆ†å¸ƒåˆ°æœ€ç»ˆåˆ†å¸ƒçš„åŒæ—¶æœ€å°åŒ–æˆæœ¬åŠŸèƒ½çš„æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>ç°æœ‰è®¡ç®—SBçš„æ–¹æ³•å¤§å¤šéœ€è¦è®¡ç®—æ˜‚è´µçš„è®­ç»ƒæ–¹æ¡ˆï¼Œå³ä½¿å¯¹äºä½ç»´é—®é¢˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§è§£æå‚æ•°åŒ–æ–¹æ³•ï¼Œä¸ºä»é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰å¼•å¯¼ç³»ç»Ÿåˆ†å¸ƒæä¾›ä¸€ç»„å¯è¡Œçš„ç­–ç•¥ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä½ç»´çº¿æ€§è§„åˆ’æ¥è¿‘ä¼¼æœ€ä¼˜ç­–ç•¥ï¼Œè¯¥è§„åˆ’çš„ç»´åº¦ä¸æ··åˆä¸­ç»„ä»¶çš„æ•°é‡æˆçº¿æ€§å…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ³•å¯è‡ªç„¶æ¨å¹¿åˆ°æ›´ä¸€èˆ¬çš„åŠ¨åŠ›ç³»ç»Ÿï¼Œå¦‚å¯æ§çº¿æ€§æ—¶å˜ç³»ç»Ÿã€‚</li>
<li>æ­¤æ–¹æ³•è§£å†³äº†å¤šè¾¹ç¼˜åŠ¨é‡SBåœ¨GMMä¹‹é—´çš„åˆ†å¸ƒæ’å€¼é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-98ed64e337a389029100911058523aeb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760735650&auth_key=1760735650-0-0-a0dae611298fa3331e25e7bcb2bcea3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-06e62343c630cc627d1133022c4bb17a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760736831&auth_key=1760736831-0-0-0a05db017bad519d502f9835cbd35a03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Shot2Tactic-Caption Multi-Scale Captioning of Badminton Videos for   Tactical Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-9ea2b7948d507d2a3f8a9514f3be86a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760733406&auth_key=1760733406-0-0-c1df8e09920c2d8a6f5769cb5b41f9c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Unifying Environment Perception and Route Choice Modeling for Trajectory   Representation Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
