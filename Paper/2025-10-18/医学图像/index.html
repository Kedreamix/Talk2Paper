<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Sampling Density Compensation using Fast Fourier Deconvolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-83794a71db03833ebd774b4f189266b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751315&auth_key=1760751315-0-0-2b58ecfae5c81d3df51fff56f8fd442e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Sampling-Density-Compensation-using-Fast-Fourier-Deconvolution"><a href="#Sampling-Density-Compensation-using-Fast-Fourier-Deconvolution" class="headerlink" title="Sampling Density Compensation using Fast Fourier Deconvolution"></a>Sampling Density Compensation using Fast Fourier Deconvolution</h2><p><strong>Authors:Rui Luo, Peng Hu, Haikun Qi</strong></p>
<p>Density Compensation Function (DCF) is widely used in non-Cartesian MRI reconstruction, either for direct Non-Uniform Fast Fourier Transform (NUFFT) reconstruction or for iterative undersampled reconstruction. Current state-of-the-art methods involve time-consuming tens of iterations, which is one of the main hurdles for widespread application of the highly efficient non-Cartesian MRI. In this paper, we propose an efficient, non-iterative method to calculate DCF for arbitrary non-Cartesian $k$-space trajectories using Fast Fourier Deconvolution. Simulation experiments demonstrate that the proposed method is able to yield DCF for 3D non-Cartesian reconstruction in around 20 seconds, achieving orders of magnitude speed improvement compared to the state-of-the-art method while achieving similar reconstruction quality. </p>
<blockquote>
<p>å¯†åº¦è¡¥å¿å‡½æ•°ï¼ˆDCFï¼‰åœ¨éç¬›å¡å°”MRIé‡å»ºä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œæ— è®ºæ˜¯ç”¨äºç›´æ¥éå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆNUFFTï¼‰é‡å»ºï¼Œè¿˜æ˜¯ç”¨äºè¿­ä»£æ¬ é‡‡æ ·é‡å»ºã€‚å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ¶‰åŠè€—æ—¶çš„æ•°åæ¬¡è¿­ä»£ï¼Œè¿™æ˜¯é«˜æ•ˆéç¬›å¡å°”MRIå¹¿æ³›åº”ç”¨çš„ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆã€éè¿­ä»£çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶åå·ç§¯ä¸ºä»»æ„éç¬›å¡å°”kç©ºé—´è½¨è¿¹è®¡ç®—DCFã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨çº¦20ç§’å†…ä¸º3Déç¬›å¡å°”é‡å»ºç”ŸæˆDCFï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ•°é‡çº§çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶è¾¾åˆ°äº†ç±»ä¼¼çš„é‡å»ºè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14873v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆéè¿­ä»£æ–¹æ³•ï¼Œåˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶åå·ç§¯è®¡ç®—ä»»æ„éç¬›å¡å°”kç©ºé—´è½¨è¿¹çš„å¯†åº¦è¡¥å¿å‡½æ•°ï¼ˆDCFï¼‰ï¼Œç”¨äºéç¬›å¡å°”MRIé‡å»ºã€‚è¯¥æ–¹æ³•å¯åœ¨çº¦20ç§’å†…ç”ŸæˆDCFï¼Œå®ç°ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”çš„æ˜¾è‘—é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„éè¿­ä»£æ–¹æ³•æ¥è®¡ç®—å¯†åº¦è¡¥å¿å‡½æ•°ï¼ˆDCFï¼‰ï¼Œé€‚ç”¨äºä»»æ„éç¬›å¡å°”kç©ºé—´è½¨è¿¹ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¿«é€Ÿå‚…é‡Œå¶åå·ç§¯æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—DCFçš„æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨çº¦20ç§’å†…ç”ŸæˆDCFï¼Œå®ç°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”çš„æ˜¾è‘—é€Ÿåº¦æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿æŒç›¸ä¼¼é‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œæé«˜äº†éç¬›å¡å°”MRIé‡å»ºçš„æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ç›´æ¥åº”ç”¨äºéå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆNUFFTï¼‰é‡å»ºæˆ–è¿­ä»£æ¬ é‡‡æ ·é‡å»ºã€‚</li>
<li>å½“å‰çš„æ–¹æ³•ä¸»è¦ç“¶é¢ˆæ˜¯è€—æ—¶çš„è¿­ä»£è¿‡ç¨‹ï¼Œè€Œè¯¥è®ºæ–‡æå‡ºçš„éè¿­ä»£æ–¹æ³•æœ‰æœ›è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6af9bab29d8d4016a892bbfda5eeeee5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751159&auth_key=1760751159-0-0-0c8a944527c5dd2f845a56377a30131e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3798614091f63a8e4d0dfc5b9db3ebd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751166&auth_key=1760751166-0-0-7d731435c172b464002d81339ce2bd3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rethinking-Hebbian-Principle-Low-Dimensional-Structural-Projection-for-Unsupervised-Learning"><a href="#Rethinking-Hebbian-Principle-Low-Dimensional-Structural-Projection-for-Unsupervised-Learning" class="headerlink" title="Rethinking Hebbian Principle: Low-Dimensional Structural Projection for   Unsupervised Learning"></a>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for   Unsupervised Learning</h2><p><strong>Authors:Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu</strong></p>
<p>Hebbian learning is a biological principle that intuitively describes how neurons adapt their connections through repeated stimuli. However, when applied to machine learning, it suffers serious issues due to the unconstrained updates of the connections and the lack of accounting for feedback mediation. Such shortcomings limit its effective scaling to complex network architectures and tasks. To this end, here we introduce the Structural Projection Hebbian Representation (SPHeRe), a novel unsupervised learning method that integrates orthogonality and structural information preservation through a local auxiliary nonlinear block. The loss for structural information preservation backpropagates to the input through an auxiliary lightweight projection that conceptually serves as feedback mediation while the orthogonality constraints account for the boundedness of updating magnitude. Extensive experimental results show that SPHeRe achieves SOTA performance among unsupervised synaptic plasticity approaches on standard image classification benchmarks, including CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong effectiveness in continual learning and transfer learning scenarios, and image reconstruction tasks show the robustness and generalizability of the extracted features. This work demonstrates the competitiveness and potential of Hebbian unsupervised learning rules within modern deep learning frameworks, demonstrating the possibility of efficient and biologically inspired learning algorithms without the strong dependence on strict backpropagation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/SPHeRe">https://github.com/brain-intelligence-lab/SPHeRe</a>. </p>
<blockquote>
<p>èµ«å¸ƒå­¦ä¹ æ˜¯ä¸€ç§ç”Ÿç‰©åŸç†ï¼Œç›´è§‚åœ°æè¿°äº†ç¥ç»å…ƒå¦‚ä½•é€šè¿‡é‡å¤åˆºæ¿€è°ƒæ•´å…¶è¿æ¥ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºæœºå™¨å­¦ä¹ æ—¶ï¼Œå®ƒç”±äºè¿æ¥çš„æ— é™æ›´æ–°å’Œç¼ºä¹åé¦ˆè°ƒèŠ‚è€Œé¢ä¸´ä¸¥é‡é—®é¢˜ã€‚è¿™äº›ç¼ºç‚¹é™åˆ¶äº†å…¶åœ¨å¤æ‚ç½‘ç»œç»“æ„å’Œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ‰©å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¼•å…¥äº†ç»“æ„æŠ•å½±èµ«å¸ƒè¡¨ç¤ºï¼ˆSPHeReï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å±€éƒ¨è¾…åŠ©éçº¿æ€§å—æ•´åˆæ­£äº¤æ€§å’Œç»“æ„ä¿¡æ¯ä¿ç•™ã€‚ç»“æ„ä¿¡æ¯ä¿ç•™çš„æŸå¤±é€šè¿‡è¾…åŠ©è½»é‡çº§æŠ•å½±åå‘ä¼ æ’­åˆ°è¾“å…¥ç«¯ï¼Œè¿™åœ¨æ¦‚å¿µä¸Šå……å½“äº†åé¦ˆè°ƒèŠ‚çš„ä½œç”¨ï¼Œè€Œæ­£äº¤æ€§çº¦æŸåˆ™è´Ÿè´£æ›´æ–°å¹…åº¦çš„æœ‰ç•Œæ€§ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSPHeReåœ¨CIFAR-10ã€CIFAR-100å’ŒTiny-ImageNetç­‰æ ‡å‡†å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ— ç›‘ç£çªè§¦å¯å¡‘æ€§æ–¹æ³•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸­å…·æœ‰å¾ˆå¼ºçš„æœ‰æ•ˆæ€§ï¼Œå›¾åƒé‡å»ºä»»åŠ¡æ˜¾ç¤ºäº†æ‰€æå–ç‰¹å¾çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†èµ«å¸ƒæ— ç›‘ç£å­¦ä¹ è§„åˆ™åœ¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„ç«äº‰åŠ›å’Œæ½œåŠ›ï¼Œå±•ç¤ºäº†åœ¨æ²¡æœ‰ä¸¥æ ¼ä¾èµ–åå‘ä¼ æ’­çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆä¸”å—ç”Ÿç‰©å¯å‘çš„å­¦ä¹ ç®—æ³•çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/SPHeRe%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/brain-intelligence-lab/SPHeReæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14810v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç»“æ„æ€§æŠ•å½±æµ·å¸ƒå­¦ä¹ æ³•ï¼ˆSPHeReï¼‰æ˜¯ä¸€ç§æ–°å‹æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡å±€éƒ¨è¾…åŠ©éçº¿æ€§å—æ•´åˆæ­£äº¤æ€§å’Œç»“æ„æ€§ä¿¡æ¯ä¿ç•™ï¼Œè§£å†³äº†ä¼ ç»Ÿæµ·å¸ƒå­¦ä¹ åœ¨åº”ç”¨äºæœºå™¨å­¦ä¹ æ—¶çš„ç¼ºé™·ã€‚SPHeReå…·æœ‰åé¦ˆè°ƒè§£æœºåˆ¶å¹¶é€šè¿‡æ­£äº¤æ€§çº¦æŸå®ç°æ›´æ–°å¹…åº¦æœ‰ç•Œæ€§ã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»æ ‡å‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œå¦‚CIFAR-10ã€CIFAR-100å’ŒTiny-ImageNetç­‰ï¼ŒåŒæ—¶åœ¨æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ åœºæ™¯åŠå›¾åƒé‡å»ºä»»åŠ¡ä¸­å±•ç°å¼ºå¤§çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­æµ·å¸ƒæ— ç›‘ç£å­¦ä¹ è§„åˆ™çš„ç«äº‰åŠ›å’Œæ½œåŠ›ï¼Œé™ä½äº†å¯¹ä¸¥æ ¼åå‘ä¼ æ’­çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPHeReæ˜¯ä¸€ç§æ–°å‹æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ•´åˆäº†æ­£äº¤æ€§å’Œç»“æ„æ€§ä¿¡æ¯ä¿ç•™ï¼Œè§£å†³äº†æµ·å¸ƒå­¦ä¹ åº”ç”¨äºæœºå™¨å­¦ä¹ æ—¶çš„ç¼ºé™·ã€‚</li>
<li>SPHeReé€šè¿‡åé¦ˆè°ƒè§£æœºåˆ¶å’Œæ­£äº¤æ€§çº¦æŸå®ç°æ›´æ–°å¹…åº¦æœ‰ç•Œæ€§ã€‚</li>
<li>SPHeReåœ¨å¤šç§å›¾åƒåˆ†ç±»æ ‡å‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼ŒåŒ…æ‹¬CIFAR-10ã€CIFAR-100å’ŒTiny-ImageNetç­‰ã€‚</li>
<li>SPHeReåœ¨æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸­å…·æœ‰å¼ºå¤§çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å›¾åƒé‡å»ºä»»åŠ¡å±•ç¤ºäº†SPHeReçš„é²æ£’æ€§å’Œç‰¹å¾æå–çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥å·¥ä½œå±•ç¤ºäº†æµ·å¸ƒæ— ç›‘ç£å­¦ä¹ è§„åˆ™åœ¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-71edc2a47ee9f07c7b8d32b5c85f9c37~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751182&auth_key=1760751182-0-0-3d310f0305eb72a4a82eadf4847ad407&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Scaling-Artificial-Intelligence-for-Multi-Tumor-Early-Detection-with-More-Reports-Fewer-Masks"><a href="#Scaling-Artificial-Intelligence-for-Multi-Tumor-Early-Detection-with-More-Reports-Fewer-Masks" class="headerlink" title="Scaling Artificial Intelligence for Multi-Tumor Early Detection with   More Reports, Fewer Masks"></a>Scaling Artificial Intelligence for Multi-Tumor Early Detection with   More Reports, Fewer Masks</h2><p><strong>Authors:Pedro R. A. S. Bassi, Xinze Zhou, Wenxuan Li, Szymon PÅ‚otka, Jieneng Chen, Qi Chen, Zheren Zhu, Jakub PrzÄ…do, Ibrahim E. HamacÄ±, Sezgin Er, Yuhan Wang, Ashwin Kumar, Bjoern Menze, JarosÅ‚aw B. Ä†wikÅ‚a, Yuyin Zhou, Akshay S. Chaudhari, Curtis P. Langlotz, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou</strong></p>
<p>Early tumor detection save lives. Each year, more than 300 million computed tomography (CT) scans are performed worldwide, offering a vast opportunity for effective cancer screening. However, detecting small or early-stage tumors on these CT scans remains challenging, even for experts. Artificial intelligence (AI) models can assist by highlighting suspicious regions, but training such models typically requires extensive tumor masksâ€“detailed, voxel-wise outlines of tumors manually drawn by radiologists. Drawing these masks is costly, requiring years of effort and millions of dollars. In contrast, nearly every CT scan in clinical practice is already accompanied by medical reports describing the tumorâ€™s size, number, appearance, and sometimes, pathology resultsâ€“information that is rich, abundant, and often underutilized for AI training. We introduce R-Super, which trains AI to segment tumors that match their descriptions in medical reports. This approach scales AI training with large collections of readily available medical reports, substantially reducing the need for manually drawn tumor masks. When trained on 101,654 reports, AI models achieved performance comparable to those trained on 723 masks. Combining reports and masks further improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting five of the seven tumor types. Notably, R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate, bladder, uterus, and esophagus, for which no public masks or AI models previously existed. This study challenges the long-held belief that large-scale, labor-intensive tumor mask creation is indispensable, establishing a scalable and accessible path toward early detection across diverse tumor types.   We plan to release our trained models, code, and dataset at <a target="_blank" rel="noopener" href="https://github.com/MrGiovanni/R-Super">https://github.com/MrGiovanni/R-Super</a> </p>
<blockquote>
<p>æ—©æœŸè‚¿ç˜¤æ£€æµ‹å¯ä»¥æŒ½æ•‘ç”Ÿå‘½ã€‚æ¯å¹´å…¨çƒæœ‰è¶…è¿‡3äº¿æ¬¡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„å®æ–½ï¼Œä¸ºæœ‰æ•ˆçš„ç™Œç—‡ç­›æŸ¥æä¾›äº†å·¨å¤§çš„æœºä¼šã€‚ç„¶è€Œï¼Œåœ¨CTæ‰«æä¸Šæ£€æµ‹è¿™äº›å¾®å°æˆ–æ—©æœŸé˜¶æ®µçš„è‚¿ç˜¤ä»ç„¶æ˜¯å……æ»¡æŒ‘æˆ˜çš„ï¼Œå³ä½¿æ˜¯ä¸“å®¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹å¯ä»¥é€šè¿‡çªå‡ºå¯ç–‘åŒºåŸŸæ¥ååŠ©æ£€æµ‹ï¼Œä½†è®­ç»ƒè¿™æ ·çš„æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è‚¿ç˜¤æ©è†œâ€”â€”ç”±æ”¾å°„ç§‘åŒ»ç”Ÿæ‰‹åŠ¨ç»˜åˆ¶çš„è‚¿ç˜¤çš„è¯¦ç»†ã€é€åƒç´ çš„è½®å»“ã€‚ç»˜åˆ¶è¿™äº›æ©è†œæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¤šå¹´çš„åŠªåŠ›å’Œæ•°ç™¾ä¸‡ç¾å…ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸´åºŠå®è·µä¸­å‡ ä¹æ¯å¼ CTæ‰«æéƒ½é™„æœ‰æè¿°è‚¿ç˜¤å¤§å°ã€æ•°é‡ã€å¤–è§‚ä»¥åŠæœ‰æ—¶è¿˜æœ‰ç—…ç†ç»“æœçš„åŒ»ç–—æŠ¥å‘Šâ€”â€”å¯¹äºAIè®­ç»ƒè€Œè¨€ï¼Œè¿™äº›ä¿¡æ¯ä¸°å¯Œä¸”å……è¶³ï¼Œä½†é€šå¸¸æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†R-Superï¼Œå®ƒè®­ç»ƒAIå¯¹åŒ»ç–—æŠ¥å‘Šä¸­çš„æè¿°ç›¸åŒ¹é…çš„è‚¿ç˜¤è¿›è¡Œåˆ†å‰²ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨å¤§é‡ç°æˆåŒ»ç–—æŠ¥å‘Šæ¥æ‰©å±•AIè®­ç»ƒï¼Œå¤§å¤§é™ä½äº†å¯¹æ‰‹åŠ¨ç»˜åˆ¶è‚¿ç˜¤æ©è†œçš„éœ€æ±‚ã€‚åœ¨101,654ä»½æŠ¥å‘Šä¸Šè¿›è¡Œè®­ç»ƒçš„AIæ¨¡å‹çš„è¡¨ç°ï¼Œä¸åœ¨723ä¸ªæ©è†œä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸å½“ã€‚ç»“åˆæŠ¥å‘Šå’Œæ©è†œï¼Œæ•æ„Ÿæ€§æé«˜äº†+13%ï¼Œç‰¹å¼‚æ€§æé«˜äº†+8%ï¼Œåœ¨æ£€æµ‹ä¸ƒç§è‚¿ç˜¤ä¸­çš„äº”ç§æ—¶è¶…è¿‡äº†æ”¾å°„ç§‘åŒ»ç”Ÿã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒR-Superèƒ½å¤Ÿåœ¨è„¾è„ã€èƒ†å›Šã€å‰åˆ—è…ºã€è†€èƒ±ã€å­å®«å’Œé£Ÿé“ç­‰éƒ¨ä½å®ç°è‚¿ç˜¤çš„åˆ†å‰²ï¼Œä¹‹å‰è¿™äº›éƒ¨ä½æ²¡æœ‰å…¬å…±æ©è†œæˆ–AIæ¨¡å‹å­˜åœ¨ã€‚è¿™é¡¹ç ”ç©¶æŒ‘æˆ˜äº†é•¿æœŸä»¥æ¥è®¤ä¸ºå¤§è§„æ¨¡ã€åŠ³åŠ¨å¯†é›†å‹çš„è‚¿ç˜¤æ©è†œåˆ¶ä½œæ˜¯ä¸å¯æˆ–ç¼ºçš„è¿™ä¸€è§‚ç‚¹ï¼Œä¸ºå¤šç§ç±»å‹çš„è‚¿ç˜¤æ—©æœŸæ£€æµ‹å»ºç«‹äº†ä¸€æ¡å¯æ‰©å±•ä¸”æ˜“äºå®æ–½çš„é“è·¯ã€‚æˆ‘ä»¬è®¡åˆ’åœ¨æˆ‘ä»¬çš„ç½‘ç«™å‘å¸ƒæˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹ã€ä»£ç å’Œæ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://github.com/MrGiovanni/R-Super">https://github.com/MrGiovanni/R-Super</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14803v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç»“åˆåŒ»å­¦æŠ¥å‘Šè¿›è¡Œè‚¿ç˜¤æ£€æµ‹çš„æ–°æ–¹æ³•R-Superã€‚ä¼ ç»Ÿä¸Šï¼Œè®­ç»ƒAIæ¨¡å‹è¿›è¡Œè‚¿ç˜¤æ£€æµ‹éœ€è¦å¤§é‡æ‰‹åŠ¨ç»˜åˆ¶çš„è‚¿ç˜¤æ©è†œï¼Œæˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚è€ŒR-Superåˆ©ç”¨ä¸°å¯Œçš„åŒ»å­¦æŠ¥å‘Šä¿¡æ¯æ¥è®­ç»ƒAIæ¨¡å‹è¿›è¡Œè‚¿ç˜¤åˆ†å‰²ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹æ‰‹åŠ¨ç»˜åˆ¶è‚¿ç˜¤æ©è†œçš„éœ€æ±‚ã€‚åœ¨å¤§é‡æŠ¥å‘Šæ•°æ®è®­ç»ƒä¸‹ï¼ŒAIæ¨¡å‹çš„æ€§èƒ½ä¸åœ¨å°‘é‡æ©è†œæ•°æ®è®­ç»ƒä¸‹çš„æ¨¡å‹ç›¸å½“ï¼Œå¹¶å¯é€šè¿‡ç»“åˆæŠ¥å‘Šå’Œæ©è†œè¿›ä¸€æ­¥æé«˜æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ã€‚R-Superè¿˜èƒ½æ£€æµ‹ä¹‹å‰æœªæœ‰å…¬å¼€æ©è†œæˆ–AIæ¨¡å‹çš„å¤šç§è‚¿ç˜¤ç±»å‹ï¼Œå¦‚è„¾è„ã€èƒ†å›Šã€å‰åˆ—è…ºç­‰ã€‚è¯¥ç ”ç©¶æŒ‘æˆ˜äº†é•¿æœŸä»¥æ¥è®¤ä¸ºå¤§è§„æ¨¡ã€åŠ³åŠ¨å¯†é›†å‹çš„è‚¿ç˜¤æ©è†œåˆ›å»ºä¸å¯æˆ–ç¼ºçš„è§‚å¿µï¼Œä¸ºå„ç§è‚¿ç˜¤ç±»å‹çš„æ—©æœŸæ£€æµ‹æä¾›äº†å¯è§„æ¨¡åŒ–ã€æ˜“äºè·å–çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—©æœŸè‚¿ç˜¤æ£€æµ‹çš„é‡è¦æ€§åŠå…¶åœ¨å…¨çƒèŒƒå›´å†…çš„æŒ‘æˆ˜ã€‚</li>
<li>AIåœ¨è‚¿ç˜¤æ£€æµ‹ä¸­çš„åº”ç”¨åŠå¯¹ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•çš„æŒ‘æˆ˜ã€‚</li>
<li>R-Superæ–¹æ³•åˆ©ç”¨åŒ»å­¦æŠ¥å‘Šä¿¡æ¯æ¥è®­ç»ƒAIæ¨¡å‹è¿›è¡Œè‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>R-Superæ˜¾è‘—å‡å°‘äº†å¯¹æ‰‹åŠ¨ç»˜åˆ¶è‚¿ç˜¤æ©è†œçš„éœ€æ±‚ã€‚</li>
<li>AIæ¨¡å‹åœ¨å¤§é‡æŠ¥å‘Šæ•°æ®è®­ç»ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ç»“åˆæŠ¥å‘Šå’Œæ©è†œèƒ½æé«˜æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e6c8ebe24e616b9f213c8adc8c0226c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751190&auth_key=1760751190-0-0-2824749b07b56b74e108b588da1d8785&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3cf6e40b1f08e27c8079412f5a73118f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751197&auth_key=1760751197-0-0-07aea1f0b833a100ac4c370c3378b2e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Eclipsing-Stellar-Flare-on-the-Demon-Star-Algol-Binary-System-Observed-during-the-MAXI-NICER-Follow-up-Campaign-in-2018"><a href="#Eclipsing-Stellar-Flare-on-the-Demon-Star-Algol-Binary-System-Observed-during-the-MAXI-NICER-Follow-up-Campaign-in-2018" class="headerlink" title="Eclipsing Stellar Flare on the Demon Star Algol Binary System Observed   during the MAXI-NICER Follow-up Campaign in 2018"></a>Eclipsing Stellar Flare on the Demon Star Algol Binary System Observed   during the MAXI-NICER Follow-up Campaign in 2018</h2><p><strong>Authors:Kazuya Nakayama, Wataru Buz Iwakiri, Teruaki Enoto, Shun Inoue, Yuta Notsu, Keith Gendreau, Zaven Arzoumanian, Kenji Hamaguchi, Tatehiro Mihara</strong></p>
<p>Algol is a well-known eclipsing binary hosting an active and variable star that exhibits frequent stellar flares. Here, we report our pre-planned and coordinated rapid X-ray follow-up observations of an eclipsing flare on Algol. The Monitor of All-sky X-ray Image (MAXI) detected a flare on Algol at 05:52 UT on 2018 July 4. Subsequently, we carried out a prompt X-ray monitoring with the Neutron star Interior Composition Explorer (NICER) starting at 19:45 UT on the same day, and the observation ended at 06:02 UT on 2018 July 6. During the decaying phase of the flare, we successfully detected a 5.8-hour-long eclipse, corresponding to the secondary eclipse in which Algol A blocks the line of sight to Algol B. During the eclipse, the 2â€“10 keV X-ray flux is decreased to 20% level from $1.9\times10^{-10}~ \mathrm{erg<del>cm^{-2}</del>s^{-1} }$ to $4.5\times10^{-11}~ \mathrm{erg<del>cm^{-2}</del>s^{-1} }$. We found a configuration of the flare size and location to explain the X-ray observations; e.g., the flare occurred at the latitude 45{\deg}S of the Algol B surface with a flare height of $1.9\times10^{11}~\mathrm{cm}$, corresponding to 0.8 times the stellar radius of Algol B, giving 80% obscuration of the flare loop by Algol A. The apparent absorption increase before the eclipse might originate from coronal mass ejection (CME) in the line of sight ejected during the flare. </p>
<blockquote>
<p>æœ¬æ–‡æŠ¥é“äº†æˆ‘ä»¬é’ˆå¯¹Algolä¸Šçš„æ—¥èš€è€€æ–‘è¿›è¡Œçš„æœ‰è®¡åˆ’å’Œåè°ƒçš„Xå°„çº¿å¿«é€Ÿè·Ÿè¸ªè§‚æµ‹ã€‚å…¨å¤©ç©ºXå°„çº¿å›¾åƒç›‘è§†å™¨ï¼ˆMAXIï¼‰äº2018å¹´7æœˆ4æ—¥åè°ƒä¸–ç•Œæ—¶ï¼ˆUTCï¼‰ä¸Šåˆ5ç‚¹52åˆ†æ£€æµ‹åˆ°Algolä¸Šçš„ä¸€ä¸ªè€€æ–‘ã€‚éšåï¼Œæˆ‘ä»¬å½“å¤©åè°ƒä¸–ç•Œæ—¶ä¸‹åˆ7ç‚¹45åˆ†å¼€å§‹è¿›è¡Œä¸­å­æ˜Ÿå†…éƒ¨ç»“æ„æ¢æµ‹ä»ªï¼ˆNICERï¼‰çš„å¿«é€ŸXå°„çº¿ç›‘æµ‹ï¼Œå¹¶äºåè°ƒä¸–ç•Œæ—¶æ—©ä¸Š6ç‚¹02åˆ†ç»“æŸè§‚æµ‹ã€‚åœ¨è€€æ–‘è¡°å‡é˜¶æ®µï¼Œæˆ‘ä»¬æˆåŠŸæ£€æµ‹åˆ°ä¸€æ¬¡é•¿è¾¾5.8å°æ—¶çš„æ—¥èš€ç°è±¡ï¼Œè¿™æ˜¯Algol AæŒ¡ä½äº†å¯¹Algol Bçš„è§†çº¿æ‰€å¼•å‘çš„æ¬¡çº§æ—¥èš€ã€‚åœ¨æ—¥èš€æœŸé—´ï¼Œ2è‡³10åƒç”µå­ä¼ç‰¹çš„Xå°„çº¿æµé‡ä»æ¯ç§’æ¯å¹³æ–¹å˜ç±³æ¥æ”¶åˆ°çš„èƒ½é‡å‡å°‘åˆ°åŸæ¥çš„ç™¾åˆ†ä¹‹äºŒåæ°´å¹³ï¼Œå³ä»åŸæ¥çš„$ 1.9 \times 10^{-10}$æ¯å°”æ ¼å‡è‡³æ–°å‘ç°å¹¶ä¿®æ­£ä¸º  è‡³$4.5 \times 10^{-11}$ æ¯å°”æ ¼å¹³æ–¹å˜ç±³æ¯ç§’ä¹‹é—´ä¸ç­‰ï¼‰ã€‚æˆ‘ä»¬å‘ç°ä¸€ä¸ªä¸è€€æ–‘å¤§å°å’Œä½ç½®ç›¸ç¬¦çš„é…ç½®æ¥è§£é‡Šè¿™äº›Xå°„çº¿è§‚æµ‹ç»“æœï¼›ä¾‹å¦‚ï¼Œè€€æ–‘å‘ç”Ÿåœ¨Algol Bè¡¨é¢çº¬åº¦ä¸ºå—çº¬å››åäº”åº¦å¤„ï¼Œè€€æ–‘é«˜åº¦ä¸º$ 1.9 \times 10^{11}$å˜ç±³ï¼Œç›¸å½“äºAlgol Bæ’æ˜ŸåŠå¾„çš„ç™¾åˆ†ä¹‹å…«åï¼Œä½¿å¾—è€€æ–‘ç¯è¢«Algol Aé®æŒ¡äº†ç™¾åˆ†ä¹‹å…«åã€‚åœ¨æ—¥èš€ä¹‹å‰è§‚å¯Ÿåˆ°çš„æ˜æ˜¾å¸æ”¶å¢åŠ å¯èƒ½æºäºåœ¨è€€æ–‘æœŸé—´å–·å°„çš„è§†çº¿ä¸Šçš„æ—¥å†•ç‰©è´¨æŠ›å°„ï¼ˆCMEï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14587v1">PDF</a> Accepted for publication in ApJ. 13 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>Algolæ˜¯ä¸€é¢—å·²çŸ¥å‘ç”Ÿé¢‘ç¹æ’æ˜Ÿè€€æ–‘çš„æ´»è·ƒå˜æ˜Ÿï¼Œåœ¨è¿‘æ—¥å‡ºç°ä¸€æ¬¡ä¸æ—¥èš€ç›¸å…³çš„è€€æ–‘æ´»åŠ¨ã€‚é€šè¿‡MAXIæ¢æµ‹å™¨è§‚æµ‹åˆ°æ­¤æ¬¡è€€æ–‘ï¼Œéšååˆ©ç”¨NICERè¿›è¡ŒåŠæ—¶çš„Xå°„çº¿ç›‘æµ‹ã€‚è§‚æµ‹å‘ç°ï¼Œåœ¨è€€æ–‘è¡°å‡é˜¶æ®µå­˜åœ¨é•¿è¾¾5.8å°æ—¶çš„æ—¥èš€ç°è±¡ï¼ŒæœŸé—´Xå°„çº¿æµé‡é™è‡³åŸæ¥çš„ç™¾åˆ†ä¹‹äºŒåã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡é…ç½®è€€æ–‘çš„å¤§å°å’Œä½ç½®æ¥è§£é‡Šæ­¤æ¬¡Xå°„çº¿è§‚æµ‹ç»“æœï¼Œæ¨æµ‹è€€æ–‘å‘ç”Ÿåœ¨Algol Bè¡¨é¢çº¬åº¦ä¸ºå—çº¬45åº¦å¤„ï¼Œè·ç¦»æ˜Ÿä½“è¡¨é¢é«˜åº¦çº¦ä¸ºAlgol BåŠå¾„çš„ç™¾åˆ†ä¹‹å…«åã€‚æ—¥èš€å‰è§‚æµ‹åˆ°çš„å¸æ”¶å¢åŠ å¯èƒ½æºäºè€€æ–‘æœŸé—´å–·å°„å‡ºçš„æ—¥å†•ç‰©è´¨æŠ›å°„ç‰©é®æŒ¡è§†çº¿æ‰€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Algolå‘ç”Ÿé¢‘ç¹æ’æ˜Ÿè€€æ–‘æ´»åŠ¨ï¼ŒMAXIæ¢æµ‹å™¨æˆåŠŸæ£€æµ‹åˆ°ä¸€æ¬¡ç‰¹å®šçš„è€€æ–‘ã€‚</li>
<li>NICERå¯¹è€€æ–‘è¿›è¡Œäº†åŠæ—¶çš„Xå°„çº¿ç›‘æµ‹ï¼Œè§‚å¯Ÿåˆ°è€€æ–‘è¡°å‡é˜¶æ®µçš„æ—¥èš€ç°è±¡ã€‚</li>
<li>æ—¥èš€æœŸé—´è§‚æµ‹åˆ°Xå°„çº¿æµé‡å¤§å¹…ä¸‹é™ï¼Œä¸‹é™å¹…åº¦è¾¾åˆ°åŸæµé‡çš„ç™¾åˆ†ä¹‹äºŒåã€‚</li>
<li>æ ¹æ®è§‚æµ‹ç»“æœæ¨æµ‹è€€æ–‘å‘ç”Ÿåœ¨Algol Bè¡¨é¢çš„ç‰¹å®šçº¬åº¦ä½ç½®ï¼Œé«˜åº¦çº¦ä¸ºæ˜Ÿä½“åŠå¾„çš„ç™¾åˆ†ä¹‹å…«åã€‚</li>
<li>æ—¥èš€å‰çš„å¸æ”¶å¢åŠ å¯èƒ½ä¸è€€æ–‘æœŸé—´å–·å°„çš„æ—¥å†•ç‰©è´¨æŠ›å°„ç‰©æœ‰å…³ã€‚</li>
<li>æœ¬æ¬¡ç ”ç©¶æä¾›äº†å¯¹Algolæ˜Ÿç³»ä¸­æ’æ˜Ÿè€€æ–‘ä¸æ—¥èš€ç›¸äº’ä½œç”¨çš„æ·±å…¥äº†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-60c55f28fc156d5d34ade31015b485f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751205&auth_key=1760751205-0-0-0a87aaab21152bd72510aa80487b2a03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c5b74c0b52da6a867ae4cf3f99851107~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751212&auth_key=1760751212-0-0-96d594ae0e59d6d1617c1fda5a8c93a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2db4fb5ad38b50fe5f502cb704f171ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751218&auth_key=1760751218-0-0-d387741dea4076466b3b720c39de1b79&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8ccde344b30941b015ea9954975c9a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751225&auth_key=1760751225-0-0-db1ea7b1930259718ae36dfcc6393380&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3aafa6fa74ccd6e4b8fc36a8d402a8f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751232&auth_key=1760751232-0-0-b5026f379ab1b149e93ee88ec199105e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcf8e703dc568142f822879cf61b1479~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751240&auth_key=1760751240-0-0-34da1f4b339fa83d76a1d06f1c3554be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Generalist-Intelligence-in-Dentistry-Vision-Foundation-Models-for-Oral-and-Maxillofacial-Radiology"><a href="#Towards-Generalist-Intelligence-in-Dentistry-Vision-Foundation-Models-for-Oral-and-Maxillofacial-Radiology" class="headerlink" title="Towards Generalist Intelligence in Dentistry: Vision Foundation Models   for Oral and Maxillofacial Radiology"></a>Towards Generalist Intelligence in Dentistry: Vision Foundation Models   for Oral and Maxillofacial Radiology</h2><p><strong>Authors:Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang</strong></p>
<p>Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare. </p>
<blockquote>
<p>å£è…”é¢Œé¢æ”¾å°„å­¦åœ¨ç‰™ç§‘å¥åº·æŠ¤ç†ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†æ˜¯ç”±äºä¸“ä¸šè®­ç»ƒäººå‘˜çŸ­ç¼ºï¼Œé™åˆ¶äº†æ”¾å°„å›¾åƒçš„è§£é‡Šèƒ½åŠ›ã€‚äººå·¥æ™ºèƒ½æ–¹æ³•å·²ç»å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç°æœ‰çš„ç‰™ç§‘äººå·¥æ™ºèƒ½ç³»ç»Ÿå—é™äºå…¶å•ä¸€æ¨¡æ€å…³æ³¨ç‚¹ã€ç‰¹å®šä»»åŠ¡è®¾è®¡å’Œä¾èµ–æˆæœ¬é«˜æ˜‚çš„æ ‡è®°æ•°æ®ï¼Œé˜»ç¢äº†å…¶åœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentVFMï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç‰™ç§‘è®¾è®¡çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å®¶æ—ã€‚DentVFMä¸ºå¹¿æ³›çš„ç‰™ç§‘åº”ç”¨ç¨‹åºç”Ÿæˆä»»åŠ¡é€šç”¨çš„è§†è§‰è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨DentVistaï¼ˆä¸€ä¸ªå¤§å‹ç²¾é€‰ç‰™ç§‘æˆåƒæ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«æ¥è‡ªä¸åŒåŒ»ç–—ä¸­å¿ƒçš„çº¦160ä¸‡å¤šç§æ¨¡æ€çš„æ”¾å°„å›¾åƒã€‚DentVFMåŒ…æ‹¬åŸºäºVision Transformerï¼ˆViTï¼‰æ¶æ„çš„2Då’Œ3Då˜ä½“ã€‚ä¸ºäº†è§£å†³ç‰™ç§‘æ™ºèƒ½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å…«ä¸ªç‰™ç§‘ä¸“ç§‘ã€æ›´å¤šç–¾ç—…ã€æˆåƒæ¨¡å¼ä»¥åŠå¹¿æ³›çš„åœ°ç†åˆ†å¸ƒã€‚DentVFMå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é€šç”¨æ™ºèƒ½ï¼Œè¯æ˜å…¶åœ¨å¤šç§ç‰™ç§‘ä»»åŠ¡ä¸Šçš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼Œå¦‚ç–¾ç—…è¯Šæ–­ã€æ²»ç–—åˆ†æã€ç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«ä»¥åŠè§£å‰–æ ‡å¿—ç‚¹çš„æ£€æµ‹å’Œåˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDentVFMæ˜¾è‘—ä¼˜äºç›‘ç£å­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ å’Œå¼±ç›‘ç£å­¦ä¹ çš„åŸºçº¿æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€æ ‡ç­¾æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒDentVFMå®ç°äº†è·¨æ¨¡æ€è¯Šæ–­ï¼Œåœ¨å¸¸è§„æˆåƒæ— æ³•ä½¿ç”¨çš„æƒ…å†µä¸‹æä¾›æ›´å¯é çš„ç»“æœï¼Œç»éªŒä¸°å¯Œçš„ç‰™åŒ»ä¹Ÿæ— æ³•åŒ¹æ•Œã€‚DentVFMä¸ºç‰™ç§‘äººå·¥æ™ºèƒ½è®¾å®šäº†æ–°çš„èŒƒå¼ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯é€‚åº”å’Œæ ‡ç­¾æ•ˆç‡é«˜çš„æ¨¡å‹ï¼Œä»¥æ”¹å–„æ™ºèƒ½ç‰™ç§‘æŠ¤ç†å¹¶è§£å†³å…¨çƒå£è…”æŠ¤ç†ä¸­çš„å…³é”®å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14532v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å£è…”é¢Œé¢æ”¾å°„å­¦åœ¨ç‰™ç§‘å¥åº·æŠ¤ç†ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†ç”±äºè®­ç»ƒæœ‰ç´ çš„ä¸“å®¶çŸ­ç¼ºï¼Œæ”¾å°„å›¾åƒè§£è¯»å—åˆ°é™åˆ¶ã€‚ç°æœ‰ç‰™ç§‘äººå·¥æ™ºèƒ½ç³»ç»Ÿå—é™äºå•ä¸€æ¨¡æ€ã€ç‰¹å®šä»»åŠ¡è®¾è®¡å’Œä¾èµ–æˆæœ¬é«˜æ˜‚çš„æ ‡ç­¾æ•°æ®ï¼Œéš¾ä»¥åœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸­è¿›è¡Œæ¨å¹¿ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentVFMâ€”â€”é¦–ä¸ªé’ˆå¯¹ç‰™ç§‘é¢†åŸŸçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ã€‚DentVFMä¸ºå„ç§ç‰™ç§‘åº”ç”¨ç”Ÿæˆä»»åŠ¡é€šç”¨çš„è§†è§‰è¡¨å¾ï¼Œå¹¶åˆ©ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨å¤§å‹ç‰™ç§‘æˆåƒæ•°æ®é›†DentVistaä¸Šè¿›è¡Œè®­ç»ƒã€‚DentVFMåŒ…å«åŸºäºVision Transformerï¼ˆViTï¼‰æ¶æ„çš„äºŒç»´å’Œä¸‰ç»´å˜ä½“ã€‚ä¸ºè§£å†³ç‰™ç§‘æ™ºèƒ½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„ç©ºç¼ºï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentBenchï¼Œæ¶µç›–å…«ç§ç‰™ç§‘ä¸“ä¸šé¢†åŸŸçš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚DentVFMå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ™ºèƒ½ï¼Œåœ¨ç–¾ç—…è¯Šæ–­ã€æ²»ç–—åˆ†æã€ç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«ä»¥åŠè§£å‰–æ ‡å¿—ç‚¹æ£€æµ‹å’Œåˆ†å‰²ç­‰å¤šç§ä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDentVFMæ˜¾è‘—ä¼˜äºç›‘ç£å­¦ä¹ ã€è‡ªæˆ‘ç›‘ç£å­¦ä¹ å’Œå¼±ç›‘ç£å­¦ä¹ çš„åŸºçº¿æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€æ ‡ç­¾æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒDentVFMèƒ½å¤Ÿå®ç°è·¨æ¨¡æ€è¯Šæ–­ï¼Œåœ¨æŸäº›ä¼ ç»ŸæˆåƒæŠ€æœ¯æ— æ³•è·å–çš„æƒ…å†µä¸‹æä¾›æ›´å¯é çš„ç»“æœã€‚å®ƒä¸ºç‰™ç§‘äººå·¥æ™ºèƒ½é¢†åŸŸæ ‘ç«‹äº†æ–°çš„èŒƒä¾‹ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯é€‚åº”å’Œæ ‡ç­¾æ•ˆç‡é«˜çš„æ¨¡å‹ï¼Œæœ‰åŠ©äºæ”¹å–„æ™ºèƒ½ç‰™ç§‘æŠ¤ç†å¹¶è§£å†³å…¨çƒå£è…”æŠ¤ç†ä¸­çš„å…³é”®å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£è…”é¢Œé¢æ”¾å°„å­¦åœ¨ç‰™ç§‘å¥åº·æŠ¤ç†ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†æ”¾å°„å›¾åƒè§£è¯»å—é™äºä¸“ä¸šäººå‘˜çŸ­ç¼ºã€‚</li>
<li>ç°æœ‰ç‰™ç§‘äººå·¥æ™ºèƒ½ç³»ç»Ÿå­˜åœ¨å±€é™æ€§ï¼Œå¦‚å•ä¸€æ¨¡æ€ã€ç‰¹å®šä»»åŠ¡è®¾è®¡å’Œä¾èµ–å¤§é‡æ ‡ç­¾æ•°æ®ã€‚</li>
<li>DentVFMæ˜¯é¦–ä¸ªé’ˆå¯¹ç‰™ç§‘çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ï¼Œæ”¯æŒå¤šç§ç‰™ç§‘åº”ç”¨å¹¶ç”Ÿæˆä»»åŠ¡é€šç”¨çš„è§†è§‰è¡¨å¾ã€‚</li>
<li>DentVFMåˆ©ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨å¤§å‹ç‰™ç§‘æˆåƒæ•°æ®é›†DentVistaä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>DentVFMåŒ…æ‹¬åŸºäºVision Transformerï¼ˆViTï¼‰æ¶æ„çš„äºŒç»´å’Œä¸‰ç»´æ¨¡å‹å˜ä½“ã€‚</li>
<li>DentBenchçš„æ¨å‡ºè§£å†³äº†ç‰™ç§‘æ™ºèƒ½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„ç©ºç¼ºï¼Œæ¶µç›–äº†å¤šç§ç‰™ç§‘ä¸“ä¸šé¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-081c465c5cf95574b7e1c3d44818df90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751247&auth_key=1760751247-0-0-8b6829336a165a1b6b09ee929546f64f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DCMIL-A-Progressive-Representation-Learning-Model-of-Whole-Slide-Images-for-Cancer-Prognosis-Analysis"><a href="#DCMIL-A-Progressive-Representation-Learning-Model-of-Whole-Slide-Images-for-Cancer-Prognosis-Analysis" class="headerlink" title="DCMIL: A Progressive Representation Learning Model of Whole Slide Images   for Cancer Prognosis Analysis"></a>DCMIL: A Progressive Representation Learning Model of Whole Slide Images   for Cancer Prognosis Analysis</h2><p><strong>Authors:Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning</strong></p>
<p>The burgeoning discipline of computational pathology shows promise in harnessing whole slide images (WSIs) to quantify morphological heterogeneity and develop objective prognostic modes for human cancers. However, progress is impeded by the computational bottleneck of gigapixel-size inputs and the scarcity of dense manual annotations. Current methods often overlook fine-grained information across multi-magnification WSIs and variations in tumor microenvironments. Here, we propose an easy-to-hard progressive representation learning model, termed dual-curriculum contrastive multi-instance learning (DCMIL), to efficiently process WSIs for cancer prognosis. The model does not rely on dense annotations and enables the direct transformation of gigapixel-size WSIs into outcome predictions. Extensive experiments on twelve cancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMIL outperforms standard WSI-based prognostic models. Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides robust instance uncertainty estimation, and captures morphological differences between normal and tumor tissues, with the potential to generate new biological insights. All codes have been made publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/tuuuc/DCMIL">https://github.com/tuuuc/DCMIL</a>. </p>
<blockquote>
<p>è®¡ç®—ç—…ç†å­¦è¿™ä¸€æ–°å…´å­¦ç§‘å±•ç°å‡ºåˆ©ç”¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWhole Slide Images, WSIï¼‰åœ¨é‡åŒ–å½¢æ€å­¦å¼‚è´¨æ€§å’Œå¼€å‘äººç±»ç™Œç—‡å®¢è§‚é¢„åæ¨¡å¼æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿›å±•å—åˆ°å·¨åƒç´ è§„æ¨¡è¾“å…¥çš„è®¡ç®—ç“¶é¢ˆå’Œå¯†é›†æ‰‹åŠ¨æ³¨é‡Šç¨€ç¼ºçš„é˜»ç¢ã€‚å½“å‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†å¤šå€ç‡WSIsä¸­çš„ç²¾ç»†é¢—ç²’ä¿¡æ¯å’Œè‚¿ç˜¤å¾®ç¯å¢ƒçš„å·®å¼‚ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»æ˜“åˆ°éš¾æ¸è¿›è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼Œç§°ä¸ºåŒè¯¾ç¨‹å¯¹æ¯”å¤šå®ä¾‹å­¦ä¹ ï¼ˆDCMILï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°å¤„ç†WSIsè¿›è¡Œç™Œç—‡é¢„åã€‚è¯¥æ¨¡å‹ä¸ä¾èµ–äºå¯†é›†æ³¨é‡Šï¼Œèƒ½å¤Ÿå®ç°å·¨åƒç´ å°ºå¯¸WSIsçš„ç›´æ¥è½¬æ¢ä»¥è¿›è¡Œç»“æœé¢„æµ‹ã€‚å¯¹åäºŒç§ç™Œç—‡ç±»å‹ï¼ˆæ¶‰åŠ5954åæ‚£è€…ï¼Œå…±æ¶‰åŠ1254ä¸‡ç“¦å—å›¾åƒï¼‰çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDCMILä¼˜äºåŸºäºæ ‡å‡†WSIçš„é¢„åæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒDCMILè¿˜å¯ä»¥è¯†åˆ«ä¸é¢„åç›¸å…³çš„ç»†å¾®åŒºåŸŸï¼Œæä¾›ç¨³å¥çš„å®ä¾‹ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶æ•æ‰æ­£å¸¸ç»„ç»‡ä¸è‚¿ç˜¤ç»„ç»‡ä¹‹é—´çš„å½¢æ€å·®å¼‚ï¼Œå…·æœ‰äº§ç”Ÿæ–°ç”Ÿç‰©å­¦è§è§£çš„æ½œåŠ›ã€‚æ‰€æœ‰ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/tuuuc/DCMIL%E3%80%82">https://github.com/tuuuc/DCMILã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†è®¡ç®—ç—…ç†å­¦åœ¨åˆ©ç”¨å…¨åˆ‡ç‰‡å›¾åƒè¿›è¡Œç™Œç—‡é¢„åé¢„æµ‹æ–¹é¢çš„æ½œåŠ›ã€‚é’ˆå¯¹å¤§è§„æ¨¡å›¾åƒçš„è®¡ç®—ç“¶é¢ˆå’Œæ‰‹åŠ¨æ ‡æ³¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºåŒè¯¾ç¨‹å¯¹æ¯”å¤šå®ä¾‹å­¦ä¹ ï¼ˆDCMILï¼‰çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ— éœ€å¯†é›†æ ‡æ³¨ï¼Œèƒ½å¤Ÿç›´æ¥ä»å¤§è§„æ¨¡å›¾åƒè¿›è¡Œç»“æœé¢„æµ‹ã€‚åœ¨å¤šç§ç™Œç—‡ç±»å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDCMILæ¨¡å‹ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶èƒ½è¯†åˆ«é¢„åç›¸å…³çš„ç²¾ç»†åŒºåŸŸï¼Œæä¾›ç¨³å¥çš„å®ä¾‹ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ•æ‰æ­£å¸¸ä¸è‚¿ç˜¤ç»„ç»‡çš„å½¢æ€å·®å¼‚ï¼Œä¸ºç”Ÿæˆæ–°çš„ç”Ÿç‰©å­¦è§è§£æä¾›äº†å¯èƒ½ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦åœ¨åˆ©ç”¨å…¨åˆ‡ç‰‡å›¾åƒè¿›è¡Œç™Œç—‡é¢„åé¢„æµ‹æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´è®¡ç®—ç“¶é¢ˆå’Œæ‰‹åŠ¨æ ‡æ³¨ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>DCMILæ¨¡å‹æ— éœ€å¯†é›†æ ‡æ³¨ï¼Œèƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡å›¾åƒã€‚</li>
<li>DCMILæ¨¡å‹åœ¨å¤šç§ç™Œç—‡ç±»å‹ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>DCMILæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«é¢„åç›¸å…³çš„ç²¾ç»†åŒºåŸŸã€‚</li>
<li>DCMILæ¨¡å‹æä¾›ç¨³å¥çš„å®ä¾‹ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-76b9bec574e52edf73e35308c6700485~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751294&auth_key=1760751294-0-0-594b0299185a50ebb2a520ff93c130c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d499986d486d64dbd5d8b637ee372ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751302&auth_key=1760751302-0-0-98dbadd52022f3233c6d0580eb194722&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82a6fae4bf7415879cbe2ede9598aadd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751308&auth_key=1760751308-0-0-639cfb9cd86b46c606fad52c9c9df4d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83794a71db03833ebd774b4f189266b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751315&auth_key=1760751315-0-0-2b58ecfae5c81d3df51fff56f8fd442e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DRBD-Mamba-for-Robust-and-Efficient-Brain-Tumor-Segmentation-with-Analytical-Insights"><a href="#DRBD-Mamba-for-Robust-and-Efficient-Brain-Tumor-Segmentation-with-Analytical-Insights" class="headerlink" title="DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with   Analytical Insights"></a>DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with   Analytical Insights</h2><p><strong>Authors:Danish Ali, Ajmal Mian, Naveed Akhtar, Ghulam Mubashar Hassan</strong></p>
<p>Accurate brain tumor segmentation is significant for clinical diagnosis and treatment. It is challenging due to the heterogeneity of tumor subregions. Mamba-based State Space Models have demonstrated promising performance. However, they incur significant computational overhead due to sequential feature computation across multiple spatial axes. Moreover, their robustness across diverse BraTS data partitions remains largely unexplored, leaving a critical gap in reliable evaluation. To address these limitations, we propose dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation model that captures multi-scale long-range dependencies with minimal computational overhead. We leverage a space-filling curve to preserve spatial locality during 3D-to-1D feature mapping, thereby reducing reliance on computationally expensive multi-axial feature scans. To enrich feature representation, we propose a gated fusion module that adaptively integrates forward and reverse contexts, along with a quantization block that discretizes features to improve robustness. In addition, we propose five systematic folds on BraTS2023 for rigorous evaluation of segmentation techniques under diverse conditions and present detailed analysis of common failure scenarios. On the 20% test set used by recent methods, our model achieves Dice improvements of 0.10% for whole tumor, 1.75% for tumor core, and 0.93% for enhancing tumor. Evaluations on the proposed systematic five folds demonstrate that our model maintains competitive whole tumor accuracy while achieving clear average Dice gains of 0.86% for tumor core and 1.45% for enhancing tumor over existing state-of-the-art. Furthermore, our model attains 15 times improvement in efficiency while maintaining high segmentation accuracy, highlighting its robustness and computational advantage over existing approaches. </p>
<blockquote>
<p>ç²¾ç¡®çš„å¤§è„‘è‚¿ç˜¤åˆ†å‰²å¯¹äºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç”±äºè‚¿ç˜¤äºšåŒºçš„å¼‚è´¨æ€§ï¼Œè¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šä¸ªç©ºé—´è½´ä¸Šè¿›è¡Œé¡ºåºç‰¹å¾è®¡ç®—ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¾ˆå¤§ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨å¤šç§BraTSæ•°æ®åˆ†åŒºä¸Šçš„ç¨³å¥æ€§å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ï¼Œä»è€Œç•™ä¸‹äº†å¯é è¯„ä¼°çš„ç©ºç™½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŒåˆ†è¾¨ç‡åŒå‘Mambaï¼ˆDRBD-Mambaï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„3Dåˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿä»¥æœ€å°çš„è®¡ç®—å¼€é”€æ•è·å¤šå°ºåº¦é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬åˆ©ç”¨å¡«å……æ›²çº¿åœ¨3D-to-1Dç‰¹å¾æ˜ å°„è¿‡ç¨‹ä¸­ä¿ç•™ç©ºé—´å±€éƒ¨æ€§ï¼Œä»è€Œå‡å°‘äº†å¯¹è®¡ç®—é‡å¤§çš„å¤šè½´ç‰¹å¾æ‰«æçš„ä¾èµ–ã€‚ä¸ºäº†ä¸°å¯Œç‰¹å¾è¡¨ç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé—¨æ§èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªé€‚åº”åœ°èåˆäº†æ­£å‘å’Œåå‘ä¸Šä¸‹æ–‡ï¼Œä»¥åŠä¸€ä¸ªé‡åŒ–å—ï¼Œè¯¥å—å°†ç‰¹å¾ç¦»æ•£åŒ–ä»¥æé«˜ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨BraTS2023ä¸Šæå‡ºäº†äº”ä¸ªç³»ç»Ÿçš„æŠ˜å æ–¹æ³•ï¼Œä»¥åœ¨å¤šç§æ¡ä»¶ä¸‹å¯¹åˆ†å‰²æŠ€æœ¯è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œå¹¶å¯¹å¸¸è§çš„å¤±è´¥åœºæ™¯è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚åœ¨æœ€è¿‘æ–¹æ³•ä½¿ç”¨çš„20%æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†æ•´ä½“è‚¿ç˜¤Diceç³»æ•°æé«˜0.10%ï¼Œè‚¿ç˜¤æ ¸å¿ƒæé«˜1.75%ï¼Œå¢å¼ºè‚¿ç˜¤æé«˜0.93%ã€‚åœ¨æå‡ºçš„äº”ä¸ªç³»ç»ŸæŠ˜å ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¿æŒæ•´ä½“è‚¿ç˜¤å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œè‚¿ç˜¤æ ¸å¿ƒçš„Diceç³»æ•°å¹³å‡æé«˜äº†0.86%ï¼Œå¢å¼ºè‚¿ç˜¤çš„Diceç³»æ•°æé«˜äº†1.45%ï¼Œè¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†é«˜åˆ†å‰²ç²¾åº¦ï¼Œçªæ˜¾äº†å…¶åœ¨ç¨³å¥æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14383v1">PDF</a> </p>
<p><strong>Summary</strong><br>    Mamba-based State Space Modelsåœ¨è„‘è‚¿ç˜¤åˆ†å‰²ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä½†å­˜åœ¨è®¡ç®—å¼€é”€å¤§åŠåœ¨BraTSæ•°æ®åˆ†åŒºä¸Šçš„ç¨³å¥æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŒåˆ†è¾¨ç‡åŒå‘Mambaï¼ˆDRBD-Mambaï¼‰æ¨¡å‹ï¼Œé€šè¿‡ç©ºé—´å¡«å……æ›²çº¿å’Œç‰¹å¾é‡åŒ–æŠ€æœ¯æé«˜è®¡ç®—æ•ˆç‡å’Œç¨³å¥æ€§ï¼Œå¹¶å¯¹BraTSæ•°æ®é›†è¿›è¡Œäº”é‡ç³»ç»ŸåŒ–æŠ˜å è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒDRBD-Mambaæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡ä¸Šæå‡æ˜¾è‘—ï¼ŒåŒæ—¶ä¿æŒäº†é«˜åˆ†å‰²ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mamba-based State Space Modelsåœ¨è„‘è‚¿ç˜¤åˆ†å‰²ä¸­å…·æœ‰æŒ‘æˆ˜æ€§å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚</li>
<li>DRBD-Mambaæ¨¡å‹è§£å†³äº†åŸæœ‰æ¨¡å‹çš„è®¡ç®—å¼€é”€å¤§åŠåœ¨ä¸åŒæ•°æ®é›†ç¨³å¥æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>DRBD-Mambaæ¨¡å‹é€šè¿‡ç©ºé—´å¡«å……æ›²çº¿å’Œç‰¹å¾é‡åŒ–æŠ€æœ¯æé«˜äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨BraTSæ•°æ®é›†ä¸Šçš„äº”æŠ˜è¯„ä¼°æ–¹æ³•å±•ç¤ºäº†å…¶ç¨³å¥æ€§ã€‚</li>
<li>DRBD-Mambaæ¨¡å‹åœ¨åˆ†å‰²ç²¾åº¦ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¿ç˜¤æ ¸å¿ƒå’Œå¢å¼ºè‚¿ç˜¤çš„åˆ†å‰²ä¸Šã€‚</li>
<li>æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†é«˜åˆ†å‰²å‡†ç¡®æ€§çš„åŒæ—¶ä¿è¯äº†è®¡ç®—æ•ˆç‡çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-63c7c56b202edb451773975cd63d9b19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751324&auth_key=1760751324-0-0-201994abb32ad5dd133d1beeca0d288a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-682e4e8f1c2a4ff8eed17a53cdd83d41~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751331&auth_key=1760751331-0-0-fecde867d19079b2ed3c661e32fbad5c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0fbb95d3591825ee9a8fec4b0440f49f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751338&auth_key=1760751338-0-0-72dfbd7a81f4733899165d7e80baee38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-for-Unsupervised-Domain-Adaptation-in-Spatio-Temporal-Echocardiography-Segmentation"><a href="#Reinforcement-Learning-for-Unsupervised-Domain-Adaptation-in-Spatio-Temporal-Echocardiography-Segmentation" class="headerlink" title="Reinforcement Learning for Unsupervised Domain Adaptation in   Spatio-Temporal Echocardiography Segmentation"></a>Reinforcement Learning for Unsupervised Domain Adaptation in   Spatio-Temporal Echocardiography Segmentation</h2><p><strong>Authors:Arnaud Judge, Nicolas Duchateau, Thierry Judge, Roman A. Sandler, Joseph Z. Sokol, Christian Desrosiers, Olivier Bernard, Pierre-Marc Jodoin</strong></p>
<p>Domain adaptation methods aim to bridge the gap between datasets by enabling knowledge transfer across domains, reducing the need for additional expert annotations. However, many approaches struggle with reliability in the target domain, an issue particularly critical in medical image segmentation, where accuracy and anatomical validity are essential. This challenge is further exacerbated in spatio-temporal data, where the lack of temporal consistency can significantly degrade segmentation quality, and particularly in echocardiography, where the presence of artifacts and noise can further hinder segmentation performance. To address these issues, we present RL4Seg3D, an unsupervised domain adaptation framework for 2D + time echocardiography segmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to enhance key landmark precision in its segmentations while processing full-sized input videos. By leveraging reinforcement learning for image segmentation, our approach improves accuracy, anatomical validity, and temporal consistency while also providing, as a beneficial side effect, a robust uncertainty estimator, which can be used at test time to further enhance segmentation performance. We demonstrate the effectiveness of our framework on over 30,000 echocardiographic videos, showing that it outperforms standard domain adaptation techniques without the need for any labels on the target domain. Code is available at <a target="_blank" rel="noopener" href="https://github.com/arnaudjudge/RL4Seg3D">https://github.com/arnaudjudge/RL4Seg3D</a>. </p>
<blockquote>
<p>é¢†åŸŸé€‚åº”æ–¹æ³•æ—¨åœ¨é€šè¿‡è·¨é¢†åŸŸçš„çŸ¥è¯†è½¬ç§»æ¥ç¼©å°æ•°æ®é›†ä¹‹é—´çš„å·®è·ï¼Œå‡å°‘å¯¹ç›®æ ‡é¢†åŸŸé¢å¤–ä¸“å®¶æ³¨é‡Šçš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œè®¸å¤šæ–¹æ³•åœ¨å¤„ç†ç›®æ ‡åŸŸçš„å¯é æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™ä¸€é—®é¢˜åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å°¤å…¶å…³é”®ï¼Œå‡†ç¡®æ€§å’Œè§£å‰–æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚åœ¨æ—¶ç©ºæ•°æ®ä¸­ï¼Œç”±äºç¼ºä¹æ—¶é—´ä¸€è‡´æ€§ï¼Œè¿™ä¸€æŒ‘æˆ˜è¿›ä¸€æ­¥åŠ å‰§ï¼Œå¯èƒ½ä¼šä¸¥é‡é™ä½åˆ†å‰²è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨ä¼ªå½±å’Œå™ªå£°çš„è¶…å£°å¿ƒåŠ¨å›¾ä¸­ï¼Œä¼šè¿›ä¸€æ­¥é˜»ç¢åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RL4Seg3Dï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºäºŒç»´åŠ æ—¶é—´è¶…å£°å¿ƒåŠ¨å›¾åˆ†å‰²çš„æ— ç›‘ç£åŸŸé€‚åº”æ¡†æ¶ã€‚RL4Seg3Dé›†æˆäº†æ–°å‹å¥–åŠ±å‡½æ•°å’Œèåˆæ–¹æ¡ˆï¼Œåœ¨å¤„ç†å…¨å°ºå¯¸è¾“å…¥è§†é¢‘æ—¶æé«˜äº†åˆ†å‰²ä¸­çš„å…³é”®åœ°æ ‡ç²¾åº¦ã€‚é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å‡†ç¡®æ€§ã€è§£å‰–æœ‰æ•ˆæ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä½œä¸ºæœ‰ç›Šçš„å‰¯ä½œç”¨ï¼Œæä¾›äº†ä¸€ä¸ªç¨³å¥çš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ï¼Œå¯åœ¨æµ‹è¯•æ—¶ç”¨äºè¿›ä¸€æ­¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨è¶…è¿‡3ä¸‡éƒ¨è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘ä¸Šå±•ç¤ºäº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å®ƒåœ¨ä¸éœ€è¦ç›®æ ‡åŸŸä»»ä½•æ ‡ç­¾çš„æƒ…å†µä¸‹è¶…è¶Šäº†æ ‡å‡†çš„åŸŸé€‚åº”æŠ€æœ¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/arnaudjudge/RL4Seg3D">https://github.com/arnaudjudge/RL4Seg3D</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14244v1">PDF</a> 10 pages, submitted to IEEE TMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºRL4Seg3Dçš„æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºå¤„ç†äºŒç»´åŠ æ—¶é—´çš„è¶…å£°å¿ƒåŠ¨å›¾åˆ†å‰²é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œæ”¹å–„äº†å…³é”®åœ°æ ‡çš„ç²¾ç¡®åº¦ï¼Œå¹¶æå‡äº†å›¾åƒåˆ†å‰²çš„å‡†ç¡®åº¦ã€è§£å‰–æœ‰æ•ˆæ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æä¾›äº†ç¨³å¥çš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ï¼Œå¯åœ¨æµ‹è¯•æ—¶è¿›ä¸€æ­¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚åœ¨è¶…è¿‡ä¸‰ä¸‡éƒ¨è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¼˜äºæ ‡å‡†é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œä¸”æ— éœ€ç›®æ ‡é¢†åŸŸçš„ä»»ä½•æ ‡ç­¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RL4Seg3Dæ˜¯ä¸€ä¸ªé’ˆå¯¹äºŒç»´åŠ æ—¶é—´è¶…å£°å¿ƒåŠ¨å›¾åˆ†å‰²çš„æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æé«˜äº†å…³é”®åœ°æ ‡çš„ç²¾ç¡®åº¦ã€‚</li>
<li>RL4Seg3Dæå‡äº†å›¾åƒåˆ†å‰²çš„å‡†ç¡®åº¦ã€è§£å‰–æœ‰æ•ˆæ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>RL4Seg3Dæä¾›äº†ç¨³å¥çš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ï¼Œå¯è¿›ä¸€æ­¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRL4Seg3Dåœ¨è¶…è¿‡ä¸‰ä¸‡éƒ¨è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘ä¸Šçš„è¡¨ç°ä¼˜äºæ ‡å‡†é¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯ã€‚</li>
<li>RL4Seg3Dæ— éœ€ç›®æ ‡é¢†åŸŸçš„ä»»ä½•æ ‡ç­¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9fe224dab819a71f08d5f33b0caa5b47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751346&auth_key=1760751346-0-0-f8dd1d9524a384fe2cae3707e2ae8844&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22a5c21db609d7f69e30e2037b3daf0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751353&auth_key=1760751353-0-0-ebbf83b1d878a1f3443734207c34f6de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-769f7e3529890d3aee0794511db872bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751361&auth_key=1760751361-0-0-65c959f7dd940f1129db75ccc3291871&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Finding-Holes-Pathologist-Level-Performance-Using-AI-for-Cribriform-Morphology-Detection-in-Prostate-Cancer"><a href="#Finding-Holes-Pathologist-Level-Performance-Using-AI-for-Cribriform-Morphology-Detection-in-Prostate-Cancer" class="headerlink" title="Finding Holes: Pathologist Level Performance Using AI for Cribriform   Morphology Detection in Prostate Cancer"></a>Finding Holes: Pathologist Level Performance Using AI for Cribriform   Morphology Detection in Prostate Cancer</h2><p><strong>Authors:Kelvin Szolnoky, Anders Blilie, Nita Mulliqi, Toyonori Tsuzuki, Hemamali Samaratunga, Matteo Titus, Xiaoyi Ji, Sol Erika Boman, Einar Gudlaugsson, Svein Reidar Kjosavik, JosÃ© Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, RadisÅ‚aw Kordek, Roman Åowicki, Brett Delahunt, Kenneth A. Iczkowski, Theo van der Kwast, Geert J. L. H. van Leenders, Katia R. M. Leite, Chin-Chen Pan, Emiel Adrianus Maria Janssen, Martin Eklund, Lars Egevad, Kimmo Kartasalo</strong></p>
<p>Background: Cribriform morphology in prostate cancer is a histological feature that indicates poor prognosis and contraindicates active surveillance. However, it remains underreported and subject to significant interobserver variability amongst pathologists. We aimed to develop and validate an AI-based system to improve cribriform pattern detection.   Methods: We created a deep learning model using an EfficientNetV2-S encoder with multiple instance learning for end-to-end whole-slide classification. The model was trained on 640 digitised prostate core needle biopsies from 430 patients, collected across three cohorts. It was validated internally (261 slides from 171 patients) and externally (266 slides, 104 patients from three independent cohorts). Internal validation cohorts included laboratories or scanners from the development set, while external cohorts used completely independent instruments and laboratories. Annotations were provided by three expert uropathologists with known high concordance. Additionally, we conducted an inter-rater analysis and compared the modelâ€™s performance against nine expert uropathologists on 88 slides from the internal validation cohort.   Results: The model showed strong internal validation performance (AUC: 0.97, 95% CI: 0.95-0.99; Cohenâ€™s kappa: 0.81, 95% CI: 0.72-0.89) and robust external validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohenâ€™s kappa: 0.55, 95% CI: 0.45-0.64). In our inter-rater analysis, the model achieved the highest average agreement (Cohenâ€™s kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine pathologists whose Cohenâ€™s kappas ranged from 0.35 to 0.62.   Conclusion: Our AI model demonstrates pathologist-level performance for cribriform morphology detection in prostate cancer. This approach could enhance diagnostic reliability, standardise reporting, and improve treatment decisions for prostate cancer patients. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šå‰åˆ—è…ºç™Œä¸­çš„ç­›çŠ¶å½¢æ€æ˜¯ä¸€ç§é¢„ç¤ºé¢„åä¸è‰¯å¹¶ç¦å¿Œç§¯æç›‘æµ‹çš„ç»„ç»‡å­¦ç‰¹å¾ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶è¢«è¾ƒå°‘æŠ¥é“ï¼Œä¸”åœ¨ç—…ç†å­¦å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è§‚å¯Ÿè€…é—´å˜å¼‚ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘å¹¶éªŒè¯ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„ç³»ç»Ÿï¼Œä»¥æé«˜ç­›çŠ¶æ¨¡å¼çš„æ£€æµ‹ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨EfficientNetV2-Sç¼–ç å™¨ä¸å¤šå®ä¾‹å­¦ä¹ åˆ›å»ºäº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºç«¯åˆ°ç«¯çš„æ•´å¼ åˆ‡ç‰‡åˆ†ç±»ã€‚è¯¥æ¨¡å‹åœ¨æ¥è‡ª430åæ‚£è€…çš„640å¼ æ•°å­—åŒ–å‰åˆ—è…ºé’ˆå¸æ´»æ£€ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè¿™äº›æ ·æœ¬æ¥è‡ªä¸‰ä¸ªé˜Ÿåˆ—ã€‚å®ƒåœ¨å†…éƒ¨ï¼ˆæ¥è‡ª171æ‚£è€…çš„261å¼ åˆ‡ç‰‡ï¼‰å’Œå¤–éƒ¨ï¼ˆæ¥è‡ªä¸‰ä¸ªç‹¬ç«‹é˜Ÿåˆ—çš„266å¼ åˆ‡ç‰‡ï¼Œå…¶ä¸­104åæ‚£è€…ï¼‰è¿›è¡Œäº†éªŒè¯ã€‚å†…éƒ¨éªŒè¯é˜Ÿåˆ—åŒ…æ‹¬å¼€å‘é›†åˆä¸­çš„å®éªŒå®¤æˆ–æ‰«æä»ªï¼Œè€Œå¤–éƒ¨é˜Ÿåˆ—åˆ™ä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„ä»ªå™¨å’Œå®éªŒå®¤ã€‚ç”±ä¸‰ä½å…·æœ‰å·²çŸ¥é«˜ä¸€è‡´æ€§çš„æ³Œå°¿ç—…ç†å­¦å®¶æä¾›æ³¨é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è¯„å§”é—´åˆ†æï¼Œå¹¶å°†è¯¥æ¨¡å‹ä¸ä¹ä½æ³Œå°¿ç—…ç†ä¸“å®¶åœ¨å†…éƒ¨éªŒè¯é˜Ÿåˆ—çš„88å¼ åˆ‡ç‰‡ä¸Šçš„è¡¨ç°è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
<p>ç»“æœï¼šè¯¥æ¨¡å‹åœ¨å†…éƒ¨éªŒè¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ˆAUCï¼š0.97ï¼Œ95% CIï¼š0.95-0.99ï¼›Cohençš„kappaå€¼ï¼š0.81ï¼Œ95% CIï¼š0.72-0.89ï¼‰ï¼Œå¹¶ä¸”åœ¨å¤–éƒ¨éªŒè¯ä¸­è¡¨ç°ç¨³å¥ï¼ˆAUCï¼š0.90ï¼Œ95% CIï¼š0.86-0.93ï¼›Cohençš„kappaå€¼ï¼š0.55ï¼Œ95% CIï¼š0.45-0.64ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„è¯„å§”é—´åˆ†æä¸­ï¼Œè¯¥æ¨¡å‹çš„å¹³å‡åè®®è¾¾æˆæœ€é«˜ï¼ˆCohençš„kappaå€¼ï¼š0.66ï¼Œ95% CIï¼š0.57-0.74ï¼‰ï¼Œä¼˜äºæ‰€æœ‰ä¹ä½ç—…ç†å­¦å®¶ï¼Œä»–ä»¬çš„Cohençš„kappaå€¼èŒƒå›´åœ¨0.35åˆ°0.62ä¹‹é—´ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13995v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘å¹¶éªŒè¯ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„ç³»ç»Ÿï¼Œä»¥æé«˜å‰åˆ—è…ºç™Œä¸­ç­›çŠ¶ç»“æ„æ¨¡å¼çš„æ£€æµ‹å‡†ç¡®æ€§ã€‚é‡‡ç”¨EfficientNetV2-Sç¼–ç å™¨ç»“åˆå¤šé‡å®ä¾‹å­¦ä¹ è¿›è¡Œç«¯åˆ°ç«¯çš„æ•´ç‰‡å¹»ç¯ç‰‡åˆ†ç±»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç»è¿‡è·¨ä¸‰ä¸ªé˜Ÿåˆ—çš„430ä¾‹æ‚£è€…çš„640ä»½æ•°å­—åŒ–å‰åˆ—è…ºèŠ¯é’ˆç©¿åˆºæ´»æ£€è®­ç»ƒã€‚æ¨¡å‹åœ¨å†…éƒ¨éªŒè¯é˜Ÿåˆ—ï¼ˆ171ä¾‹æ‚£è€…çš„261å¼ å¹»ç¯ç‰‡ï¼‰å’Œå¤–éƒ¨éªŒè¯é˜Ÿåˆ—ï¼ˆæ¥è‡ªä¸‰ä¸ªç‹¬ç«‹é˜Ÿåˆ—çš„104ä¾‹æ‚£è€…çš„266å¼ å¹»ç¯ç‰‡ï¼‰ä¸­å¾—åˆ°äº†éªŒè¯ã€‚ä¸ä¸‰ä½é«˜å…±è¯†çš„ä¸“å®¶æ³Œå°¿ç—…ç†å­¦å®¶ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨å†…éƒ¨éªŒè¯é˜Ÿåˆ—çš„88å¼ å¹»ç¯ç‰‡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„è¯„ä¼°ä¸€è‡´æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å†…éƒ¨å’Œå¤–éƒ¨éªŒè¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹³å‡åè®®é«˜äºæ‰€æœ‰ä¹ä½ç—…ç†å­¦å®¶ã€‚ç»“è®ºï¼šè¯¥äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æ£€æµ‹å‰åˆ—è…ºç™Œä¸­çš„ç­›çŠ¶å½¢æ€æ–¹é¢è¡¨ç°å‡ºç—…ç†å­¦å®¶çº§çš„æ€§èƒ½ï¼Œå¯å¢å¼ºè¯Šæ–­çš„å¯é æ€§ã€æ ‡å‡†åŒ–æŠ¥å‘Šï¼Œå¹¶æ”¹å–„å‰åˆ—è…ºç™Œæ‚£è€…çš„æ²»ç–—å†³ç­–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶èƒŒæ™¯å¼ºè°ƒç­›çŠ¶å½¢æ€åœ¨å‰åˆ—è…ºç™Œä¸­çš„é‡è¦æ€§ï¼Œä½†ç°æœ‰æŠ¥å‘Šå­˜åœ¨ä¸è¶³ï¼Œä¸”ç—…ç†å­¦å®¶ä¹‹é—´å­˜åœ¨è¾ƒå¤§çš„è§‚å¯Ÿè€…é—´å˜å¼‚ã€‚</li>
<li>æå‡ºå¼€å‘ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„ç³»ç»Ÿæ¥æ”¹å–„ç­›çŠ¶æ¨¡å¼æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨æ·±åº¦å­¦ä¹ å’ŒEfficientNetV2-Sç¼–ç å™¨æŠ€æœ¯ï¼Œç»“åˆå¤šé‡å®ä¾‹å­¦ä¹ è¿›è¡Œæ•´ç‰‡å¹»ç¯ç‰‡åˆ†ç±»ã€‚</li>
<li>æ¨¡å‹åœ¨å†…éƒ¨å’Œå¤–éƒ¨éªŒè¯ä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¸ç—…ç†å­¦å®¶ç›¸æ¯”å…·æœ‰æ›´é«˜çš„è¯„ä¼°ä¸€è‡´æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸ä¹ä½ä¸“å®¶æ³Œå°¿ç—…ç†å­¦å®¶çš„æ¯”è¾ƒä¸­å±•ç°å‡ºæœ€ä½³å¹³å‡åè®®ã€‚</li>
<li>æ¨¡å‹å¯ä»¥æé«˜è¯Šæ–­çš„å¯é æ€§ï¼Œæ ‡å‡†åŒ–æŠ¥å‘Šæµç¨‹ï¼Œå¹¶å¯¹å‰åˆ—è…ºç™Œæ‚£è€…çš„æ²»ç–—å†³ç­–äº§ç”Ÿç§¯æå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-de9e09d67464c67e4764577bd5144ae6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751368&auth_key=1760751368-0-0-07a18f5a2be4abb7e3d23fde85ab8086&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af4ff410b92090d5b50c242463b67617~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751375&auth_key=1760751375-0-0-b9ea1049b2422ed391448e8f4de15b2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2eb80868f6ba15d3abcd4933b9baae4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751382&auth_key=1760751382-0-0-a030ff86e28c3cc5a0393c186b52078d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Distributional-Consistency-Loss-Beyond-Pointwise-Data-Terms-in-Inverse-Problems"><a href="#Distributional-Consistency-Loss-Beyond-Pointwise-Data-Terms-in-Inverse-Problems" class="headerlink" title="Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse   Problems"></a>Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse   Problems</h2><p><strong>Authors:George Webber, Andrew J. Reader</strong></p>
<p>Recovering true signals from noisy measurements is a central challenge in inverse problems spanning medical imaging, geophysics, and signal processing. Current solutions balance prior assumptions regarding the true signal (regularization) with agreement to noisy measured data (data-fidelity). Conventional data-fidelity loss functions, such as mean-squared error (MSE) or negative log-likelihood, seek pointwise agreement with noisy measurements, often leading to overfitting to noise. In this work, we instead evaluate data-fidelity collectively by testing whether the observed measurements are statistically consistent with the noise distributions implied by the current estimate. We adopt this aggregated perspective and introduce distributional consistency (DC) loss, a data-fidelity objective that replaces pointwise matching with distribution-level calibration using model-based probability scores for each measurement. DC loss acts as a direct and practical plug-in replacement for standard data consistency terms: i) it is compatible with modern regularizers, ii) it is optimized in the same way as traditional losses, and iii) it avoids overfitting to measurement noise even without the use of priors. Its scope naturally fits many practical inverse problems where the measurement-noise distribution is known and where the measured dataset consists of many independent noisy values. We demonstrate efficacy in two key example application areas: i) in image denoising with deep image prior, using DC instead of MSE loss removes the need for early stopping and achieves higher PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss reduces artifacts in highly-iterated reconstructions and enhances the efficacy of hand-crafted regularization. These results position DC loss as a statistically grounded, performance-enhancing alternative to conventional fidelity losses for inverse problems. </p>
<blockquote>
<p>ä»å™ªå£°æµ‹é‡ä¸­æ¢å¤çœŸå®ä¿¡å·æ˜¯è·¨è¶ŠåŒ»å­¦æˆåƒã€åœ°çƒç‰©ç†å’Œä¿¡å·å¤„ç†ç­‰é¢†åŸŸçš„åé—®é¢˜çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å½“å‰çš„è§£å†³æ–¹æ¡ˆå¹³è¡¡äº†å¯¹çœŸå®ä¿¡å·ï¼ˆæ­£åˆ™åŒ–ï¼‰çš„å…ˆéªŒå‡è®¾ä¸å¯¹å™ªå£°æµ‹é‡æ•°æ®ï¼ˆæ•°æ®ä¿çœŸåº¦ï¼‰çš„å¥‘åˆåº¦ã€‚ä¼ ç»Ÿçš„æ•°æ®ä¿çœŸåº¦æŸå¤±å‡½æ•°ï¼Œå¦‚å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æˆ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œå¯»æ±‚ä¸å™ªå£°æµ‹é‡çš„ç‚¹å¯¹ç‚¹ä¸€è‡´ï¼Œè¿™å¾€å¾€å¯¼è‡´å¯¹å™ªå£°çš„è¿‡æ‹Ÿåˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è½¬è€Œé€šè¿‡è¯„ä¼°æ•°æ®é›†ä½“ä¿çœŸåº¦ï¼Œæ£€éªŒè§‚å¯Ÿåˆ°çš„æµ‹é‡å€¼æ˜¯å¦ä¸å½“å‰ä¼°è®¡æ‰€æš—ç¤ºçš„å™ªå£°åˆ†å¸ƒåœ¨ç»Ÿè®¡ä¸Šä¸€è‡´ã€‚æˆ‘ä»¬é‡‡ç”¨è¿™ç§èšåˆè§†è§’ï¼Œå¹¶å¼•å…¥åˆ†å¸ƒä¸€è‡´æ€§ï¼ˆDCï¼‰æŸå¤±ï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®ä¿çœŸåº¦ç›®æ ‡ï¼Œå®ƒç”¨åˆ†å¸ƒçº§åˆ«çš„æ ¡å‡†æ›¿æ¢ç‚¹å¯¹ç‚¹åŒ¹é…ï¼Œé’ˆå¯¹æ¯æ¬¡æµ‹é‡ä½¿ç”¨åŸºäºæ¨¡å‹çš„æ¦‚ç‡åˆ†æ•°ã€‚DCæŸå¤±ä½œä¸ºæ ‡å‡†æ•°æ®ä¸€è‡´æ€§æœ¯è¯­çš„ç›´æ¥ä¸”å®ç”¨çš„æ’ä»¶æ›¿ä»£å“ï¼šiï¼‰å®ƒä¸ç°ä»£æ­£åˆ™å™¨å…¼å®¹ï¼Œiiï¼‰å®ƒçš„ä¼˜åŒ–æ–¹å¼ä¸ä¼ ç»ŸæŸå¤±ç›¸åŒï¼Œiiiï¼‰å³ä½¿ä¸ä½¿ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå®ƒä¹Ÿèƒ½é¿å…å¯¹æµ‹é‡å™ªå£°çš„è¿‡æ‹Ÿåˆã€‚å…¶é€‚ç”¨èŒƒå›´è‡ªç„¶åœ°é€‚åº”äº†è®¸å¤šå®é™…çš„åé—®é¢˜ï¼Œå…¶ä¸­æµ‹é‡å™ªå£°åˆ†å¸ƒæ˜¯å·²çŸ¥çš„ï¼Œå¹¶ä¸”æ‰€æµ‹é‡çš„æ•°æ®é›†ç”±è®¸å¤šç‹¬ç«‹çš„å™ªå£°å€¼ç»„æˆã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…³é”®ç¤ºä¾‹åº”ç”¨é¢†åŸŸä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼šiï¼‰åœ¨åˆ©ç”¨æ·±åº¦å›¾åƒå…ˆéªŒè¿›è¡Œå›¾åƒå»å™ªçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨DCæŸå¤±ä»£æ›¿MSEæŸå¤±æ¶ˆé™¤äº†å¯¹æ—©æœŸåœæ­¢çš„éœ€è¦ï¼Œå¹¶å®ç°äº†æ›´é«˜çš„å³°å€¼ä¿¡å™ªæ¯”ï¼›iiï¼‰åœ¨ä»Poissonå™ªå£°æ•°æ®è¿›è¡ŒåŒ»å­¦å›¾åƒé‡å»ºçš„æƒ…å†µä¸‹ï¼ŒDCæŸå¤±å‡å°‘äº†é«˜åº¦è¿­ä»£é‡å»ºä¸­çš„ä¼ªå½±ï¼Œå¹¶å¢å¼ºäº†æ‰‹å·¥æ­£åˆ™åŒ–çš„æ•ˆæœã€‚è¿™äº›ç»“æœå°†DCæŸå¤±å®šä½ä¸ºåé—®é¢˜çš„ä¼ ç»Ÿå¿ å®åº¦æŸå¤±çš„ç»Ÿè®¡åŸºç¡€ã€æ€§èƒ½æå‡æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13972v1">PDF</a> Preprint; submitted to ICLR 2025 for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åé—®é¢˜ä¸­ä»å™ªå£°æµ‹é‡ä¸­æ¢å¤çœŸå®ä¿¡å·çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰è§£å†³æ–¹æ¡ˆçš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ä¿çœŸåº¦æŸå¤±å‡½æ•°â€”â€”åˆ†å¸ƒä¸€è‡´æ€§ï¼ˆDCï¼‰æŸå¤±ã€‚DCæŸå¤±é‡‡ç”¨é›†ä½“è¯„ä¼°æ•°æ®ä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹æ¦‚ç‡åˆ†æ•°å¯¹æ¯æ¬¡æµ‹é‡è¿›è¡Œåˆ†å¸ƒçº§åˆ«çš„æ ¡å‡†ï¼Œé¿å…äº†è¿‡åº¦æ‹Ÿåˆæµ‹é‡å™ªå£°ï¼Œå³ä½¿ä¸ä½¿ç”¨å…ˆéªŒä¿¡æ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ¬æ–‡åœ¨å›¾åƒå»å™ªå’ŒåŒ»å­¦å›¾åƒé‡å»ºç­‰ä¸¤ä¸ªå…³é”®åº”ç”¨é¢†åŸŸå±•ç¤ºäº†DCæŸå¤±çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åé—®é¢˜ä¸­ä»å™ªå£°æµ‹é‡æ¢å¤çœŸå®ä¿¡å·æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œæ¶‰åŠåŒ»å­¦æˆåƒã€åœ°çƒç‰©ç†å­¦å’Œä¿¡å·å¤„ç†ç­‰é¢†åŸŸã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆéœ€è¦åœ¨å…ˆéªŒå‡è®¾å’Œå™ªå£°æµ‹é‡æ•°æ®ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>ä¼ ç»Ÿçš„æ•°æ®ä¿çœŸåº¦æŸå¤±å‡½æ•°ï¼Œå¦‚å‡æ–¹è¯¯å·®æˆ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œå¯»æ±‚ä¸å™ªå£°æµ‹é‡çš„ç‚¹å¯¹ç‚¹åè®®ï¼Œå¾€å¾€å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå™ªå£°ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åˆ†å¸ƒä¸€è‡´æ€§ï¼ˆDCï¼‰æŸå¤±ï¼Œä½œä¸ºä¸€ç§æ–°çš„æ•°æ®ä¿çœŸåº¦ç›®æ ‡å‡½æ•°ã€‚</li>
<li>DCæŸå¤±é€šè¿‡æ¨¡å‹æ¦‚ç‡åˆ†æ•°å¯¹æ¯æ¬¡æµ‹é‡è¿›è¡Œåˆ†å¸ƒçº§åˆ«çš„æ ¡å‡†ï¼Œé¿å…äº†è¿‡åº¦æ‹Ÿåˆæµ‹é‡å™ªå£°ï¼Œå¹¶å¯ç›´æ¥æ›¿ä»£æ ‡å‡†æ•°æ®ä¸€è‡´æ€§æœ¯è¯­ã€‚</li>
<li>DCæŸå¤±é€‚ç”¨äºè®¸å¤šå®é™…åé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“æµ‹é‡å™ªå£°åˆ†å¸ƒå·²çŸ¥ä¸”æµ‹é‡æ•°æ®é›†åŒ…å«è®¸å¤šç‹¬ç«‹å™ªå£°å€¼æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-255930acb5a483395d5c2d659d26246a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751390&auth_key=1760751390-0-0-c3f2791cfaa5a2bf9f8da0db0474a760&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d81c58182b745b5c0b159adb2fb71ab4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751397&auth_key=1760751397-0-0-a43f2f65f6b00998c87dc7a6fe54e9a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd866a62481b05004f49212c478535fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751404&auth_key=1760751404-0-0-92e239f00000ea99df7da4c5b996277a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd3bf7c5123ddea8479602c47868261d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751410&auth_key=1760751410-0-0-57c1bba181e13647ebc104cf61955e34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-87dc0e5a94548a157622c5d4e4e91624~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751417&auth_key=1760751417-0-0-35c06d7432e74b47cf7768d6446688d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GenCellAgent-Generalizable-Training-Free-Cellular-Image-Segmentation-via-Large-Language-Model-Agents"><a href="#GenCellAgent-Generalizable-Training-Free-Cellular-Image-Segmentation-via-Large-Language-Model-Agents" class="headerlink" title="GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation   via Large Language Model Agents"></a>GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation   via Large Language Model Agents</h2><p><strong>Authors:Xi Yu, Yang Yang, Qun Liu, Yonghua Du, Sean McSweeney, Yuewei Lin</strong></p>
<p>Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\rightarrow$ run $\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences. </p>
<blockquote>
<p>ç»†èƒå›¾åƒåˆ†å‰²åœ¨å®šé‡ç”Ÿç‰©å­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†ç”±äºæ¨¡æ€çš„å¼‚è´¨æ€§ã€å½¢æ€çš„å¯å˜æ€§ä»¥åŠæ ‡æ³¨çš„æœ‰é™æ€§ï¼Œå®ƒä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†GenCellAgentï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒé€šè¿‡è§„åˆ’å™¨-æ‰§è¡Œå™¨-è¯„ä¼°å™¨å¾ªç¯ï¼ˆé€‰æ‹©å·¥å…·â†’è¿è¡Œâ†’è´¨é‡æ£€æŸ¥ï¼‰å’Œé•¿æœŸè®°å¿†ï¼Œåè°ƒä¸“ä¸šåˆ†å‰²å™¨å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿï¼ˆiï¼‰è‡ªåŠ¨å°†å›¾åƒè·¯ç”±åˆ°æœ€ä½³å·¥å…·ï¼Œï¼ˆiiï¼‰å½“æˆåƒæ¡ä»¶ä¸å·¥å…·é¢„æœŸä¸åŒæ—¶ï¼Œåˆ©ç”¨å°‘é‡å‚è€ƒå›¾åƒå³æ—¶é€‚åº”ï¼Œï¼ˆiiiï¼‰æ”¯æŒç°æœ‰æ¨¡å‹æœªæ¶µç›–çš„å™¨å®˜æ–‡æœ¬å¼•å¯¼åˆ†å‰²ï¼Œä»¥åŠï¼ˆivï¼‰å°†ä¸“å®¶ç¼–è¾‘æäº¤åˆ°å†…å­˜ï¼Œå®ç°è‡ªæˆ‘è¿›åŒ–ä¸ªæ€§åŒ–å·¥ä½œæµç¨‹ã€‚åœ¨å››ä¸ªç»†èƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ­¤è·¯ç”±æ–¹æ³•æ¯”æœ€æ–°åŸºçº¿æŠ€æœ¯å¹³å‡æé«˜äº†15.7%çš„å‡†ç¡®åº¦ã€‚å¯¹äºæ–°æ•°æ®é›†çš„å†…è´¨ç½‘å’Œçº¿ç²’ä½“ï¼ŒGenCellAgentè¾ƒä¸“ä¸šæ¨¡å‹å¹³å‡æé«˜äº†37.6%çš„IoUã€‚å®ƒè¿˜å¯ä»¥é€šè¿‡è¿­ä»£æ–‡æœ¬å¼•å¯¼ç»†åŒ–å¯¹æ–°çš„å¯¹è±¡å¦‚é«˜å°”åŸºä½“è¿›è¡Œåˆ†å‰²ï¼Œè½»å¾®çš„äººä¸ºä¿®æ­£è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›åŠŸèƒ½ä¸ºå®ç°ç¨³å¥ã€å¯é€‚åº”çš„ç»†èƒå›¾åƒåˆ†å‰²æä¾›äº†å®ç”¨é€”å¾„ï¼Œè€Œæ— éœ€è¿›è¡Œé‡æ–°è®­ç»ƒï¼ŒåŒæ—¶é™ä½äº†æ ‡æ³¨è´Ÿæ‹…å¹¶ç¬¦åˆç”¨æˆ·åå¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13896v1">PDF</a> 43 pages</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ç­–åˆ’å‘˜-æ‰§è¡Œè€…-è¯„ä¼°è€…çš„å¾ªç¯ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³å·¥å…·è¿›è¡Œç»†èƒå›¾åƒåˆ†å‰²ï¼Œå¯åœ¨å›¾åƒæ¡ä»¶ä¸å·¥å…·é¢„æœŸä¸ç¬¦æ—¶è¿›è¡Œå®æ—¶è°ƒæ•´ï¼Œå¹¶æ”¯æŒæ–‡æœ¬å¼•å¯¼çš„å™¨å®˜åˆ†å‰²ã€‚è¯¥æ¡†æ¶æé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ï¼Œé™ä½äº†æ ‡æ³¨è´Ÿæ‹…ï¼Œå¹¶åŒ¹é…ç”¨æˆ·åå¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GenCellAgentæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç»†èƒå›¾åƒåˆ†å‰²ã€‚</li>
<li>é€šè¿‡ç­–åˆ’å‘˜-æ‰§è¡Œè€…-è¯„ä¼°è€…çš„å¾ªç¯ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³å·¥å…·è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>æ¡†æ¶å¯åœ¨å›¾åƒæ¡ä»¶ä¸å·¥å…·é¢„æœŸä¸ç¬¦æ—¶è¿›è¡Œè‡ªæˆ‘è°ƒæ•´ã€‚</li>
<li>æ”¯æŒæ–‡æœ¬å¼•å¯¼çš„å™¨å®˜åˆ†å‰²ï¼Œå¯åˆ†å‰²æœªæ¶µç›–ç°æœ‰æ¨¡å‹çš„å™¨å®˜ã€‚</li>
<li>é€šè¿‡ä¸“å®¶ç¼–è¾‘çš„è®°å¿†åŠŸèƒ½ï¼Œå®ç°è‡ªæˆ‘è¿›åŒ–ä¸ªæ€§åŒ–å·¥ä½œæµç¨‹ã€‚</li>
<li>åœ¨å››ä¸ªç»†èƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGenCellAgentç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯åŸºçº¿æé«˜äº†å¹³å‡å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1a95875340000c58f4f397b3fc5023b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751424&auth_key=1760751424-0-0-4c89bf7d7fc4363b3aa8b87fe8773d1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ddcc7d7644c69ab2efe3dda854936285~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751432&auth_key=1760751432-0-0-1721c3d210f268fbeb348ccdf6f98cf2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e71333df437c5af01035863f538bc343~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751439&auth_key=1760751439-0-0-467a690277a36acbcd6c09ab524e84b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa6ebf1c217e234fde1e3c86fa370703~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751446&auth_key=1760751446-0-0-8c6704e9d3a711b54fe3b0f8a7040ba2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Unlocking-Public-Catalogues-Instruction-Tuning-LLMs-for-ICD-Coding-of-German-Tumor-Diagnoses"><a href="#Unlocking-Public-Catalogues-Instruction-Tuning-LLMs-for-ICD-Coding-of-German-Tumor-Diagnoses" class="headerlink" title="Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of   German Tumor Diagnoses"></a>Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of   German Tumor Diagnoses</h2><p><strong>Authors:Stefan Lenz, Lakisha Ortiz Rosario, Georg Vollmar, Arsenij Ustjanzew, Fatma Alickovic, Thomas Kindler, Torsten Panholzer</strong></p>
<p>Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential for structured cancer documentation in Germany. Smaller open-weight LLMs are appealing for privacy-preserving automation but often struggle with coding accuracy in German-language contexts. This study investigates whether instruction-based fine-tuning on public datasets improves the coding accuracy of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded diagnoses from the local tumor documentation system as test data. In a systematic data quality assessment, the upper limit for ICD-10 coding performance was estimated at 60-79% for exact and 81-94% for partial (three-character codes only) derivation. As training data, over 500,000 question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families (7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to 41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3 topography coding also improved but started and remained considerably lower with an exact accuracy of 22-40% and a partial accuracy of 56-67% after fine-tuning. Malformed code outputs dropped to 0% for all models. Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with model size, but gaps between small and large models narrowed after fine-tuning. The reasoning mode in Qwen3 generally yielded a lower performance than fine-tuning and was over 100 times slower. Our findings highlight the potential of leveraging public catalogues to build instruction datasets that improve LLMs in medical documentation tasks. The complete training dataset and the best-performing checkpoints of the fine-tuned models are available from <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024">https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024</a>. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œä½¿ç”¨ICD-10-GMå’ŒICD-O-3å¯¹è‚¿ç˜¤è¯Šæ–­è¿›è¡Œå‡†ç¡®çš„ç¼–ç å¯¹äºç»“æ„åŒ–ç™Œç—‡æ–‡æ¡£è®°å½•è‡³å…³é‡è¦ã€‚è¾ƒå°å‹çš„è½»é‡çº§LLMï¼ˆå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰åœ¨ä¿æŠ¤éšç§çš„è‡ªåŠ¨åŒ–æ–¹é¢å¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†åœ¨å¾·è¯­ç¯å¢ƒä¸­çš„ç¼–ç å‡†ç¡®æ€§æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥åŸºäºå…¬å…±æ•°æ®é›†çš„æŒ‡ä»¤å¾®è°ƒæ˜¯å¦æé«˜äº†å¾·å›½è‚¿ç˜¤è¯Šæ–­æ–‡æœ¬çš„ç¼–ç å‡†ç¡®æ€§ã€‚è¯„ä¼°ä½¿ç”¨æœ¬åœ°è‚¿ç˜¤æ–‡æ¡£ç³»ç»Ÿä¸­çš„ç¼–ç è¯Šæ–­ä½œä¸ºæµ‹è¯•æ•°æ®ã€‚åœ¨ç³»ç»Ÿçš„æ•°æ®è´¨é‡è¯„ä¼°ä¸­ï¼ŒICD-10ç¼–ç æ€§èƒ½çš„ä¸Šé™ä¼°è®¡ä¸ºç²¾ç¡®åº¦ä¸º60-79%ï¼Œéƒ¨åˆ†ï¼ˆä»…ä½¿ç”¨ä¸‰å­—ä»£ç ï¼‰æ¨å¯¼ä¸º81-94%ã€‚åŸºäºICD-10-GMã€ICD-O-3å’ŒOPSç›®å½•åˆ›å»ºäº†è¶…è¿‡50ä¸‡å¯¹é—®ç­”å¯¹ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚å¯¹æ¥è‡ªQwenã€Llamaå’ŒMistralå®¶æ—çš„8ä¸ªè½»é‡çº§æ¨¡å‹ï¼ˆ7-70Bå‚æ•°ï¼‰è¿›è¡Œäº†å¾®è°ƒã€‚ICD-10-GMå‡†ç¡®ç‡ä»1.4-24%æé«˜åˆ°41-58%ï¼Œéƒ¨åˆ†å‡†ç¡®ç‡ä»31-74%æé«˜åˆ°73-83%ã€‚ICD-O-3åœ°å½¢ç¼–ç çš„å‡†ç¡®æ€§ä¹Ÿæœ‰æ‰€æé«˜ï¼Œä½†å¼€å§‹å¹¶ä¿æŒåœ¨å¾®è°ƒåç²¾ç¡®åº¦ä¸º22-40%ï¼Œéƒ¨åˆ†å‡†ç¡®ç‡ä¸º56-67%ï¼Œä»ç„¶ç›¸å¯¹è¾ƒä½ã€‚æ‰€æœ‰æ¨¡å‹çš„æ— æ•ˆä»£ç è¾“å‡ºé™è‡³0%ã€‚è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ç‡è¾¾åˆ°99%ã€‚å‡†ç¡®æ€§ä¸æ¨¡å‹å¤§å°æ­£ç›¸å…³ï¼Œä½†åœ¨å¾®è°ƒåï¼Œå°å‹å’Œå¤§å‹æ¨¡å‹ä¹‹é—´çš„å·®è·ç¼©å°äº†ã€‚Qwen3çš„æ¨ç†æ¨¡å¼é€šå¸¸è¡¨ç°è¾ƒå·®ï¼Œè€Œä¸”è¿è¡Œé€Ÿåº¦æ˜¯å¾®è°ƒçš„100å€ä»¥ä¸Šã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†åˆ©ç”¨å…¬å…±ç›®å½•æ„å»ºæŒ‡ä»¤æ•°æ®é›†ä»¥æé«˜LLMåœ¨åŒ»å­¦æ–‡æ¡£è®°å½•ä»»åŠ¡ä¸­çš„æ½œåŠ›çš„æ½œåŠ›ã€‚å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†å’Œå¾®è°ƒæ¨¡å‹çš„æœ€ä½³æ£€æŸ¥ç‚¹å¯ä»<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024%E8%8E%B7%E5%BE%97%E3%80%82">https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13624v1">PDF</a> 19 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºå…¬å¼€æ•°æ®é›†çš„æŒ‡ä»¤å¾®è°ƒå¯¹å¾·å›½è‚¿ç˜¤è¯Šæ–­æ–‡æœ¬ç¼–ç å‡†ç¡®åº¦çš„æå‡æ•ˆæœã€‚ç ”ç©¶å‘ç°åœ¨ICD-10ç¼–ç æ–¹é¢ï¼Œå¾®è°ƒåå‡†ç¡®åº¦æ˜¾è‘—æå‡ï¼Œè€ŒICD-O-3ç¼–ç å‡†ç¡®åº¦è™½æœ‰æ‰€æå‡ä½†ä»è¾ƒä½ã€‚æ‰€æœ‰æ¨¡å‹çš„é”™è¯¯è¾“å‡ºé™ä½ä¸º0%ï¼Œå¹¶ä¸”è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ç‡é«˜è¾¾99%ã€‚å‡†ç¡®åº¦ä¸æ¨¡å‹å¤§å°æ­£ç›¸å…³ï¼Œä½†å¾®è°ƒåå¤§å°æ¨¡å‹ä¹‹é—´çš„å·®è·ç¼©å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICD-10ç¼–ç å‡†ç¡®åº¦çš„æå‡æ˜¾è‘—ï¼Œç»è¿‡å¾®è°ƒåï¼Œå‡†ç¡®åº¦ä»1.4-24%æå‡è‡³41-58%ã€‚</li>
<li>ICD-O-3ç¼–ç çš„å‡†ç¡®åº¦è™½ç„¶æœ‰æ‰€æå‡ï¼Œä½†èµ·å§‹å’Œæœ€ç»ˆå‡†ç¡®åº¦ä»ç„¶ç›¸å¯¹è¾ƒä½ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹çš„é”™è¯¯è¾“å‡ºåœ¨å¾®è°ƒåé™ä¸º0%ã€‚</li>
<li>è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ç‡é«˜è¾¾99%ã€‚</li>
<li>æ¨¡å‹çš„å‡†ç¡®åº¦ä¸æ¨¡å‹å¤§å°æ­£ç›¸å…³ï¼Œä½†å¾®è°ƒæœ‰åŠ©äºç¼©å°å¤§å°æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚</li>
<li>Qwen3çš„æ¨ç†æ¨¡å¼åœ¨å¾®è°ƒåçš„æ€§èƒ½æ™®éè¾ƒä½ï¼Œä¸”å¤„ç†é€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨å…¬å¼€ç›®å½•æ„å»ºæŒ‡ä»¤æ•°æ®é›†ï¼Œå¯ä»¥æå‡LLMsåœ¨åŒ»ç–—æ–‡æ¡£ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f1ff9449c54b8ec3f72a9db7571c5af0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751453&auth_key=1760751453-0-0-6aa17427de5e4d8c0eac38dee277312f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99f7c8d5e63d2477d785f0ef9ca0b69c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751461&auth_key=1760751461-0-0-034a0cea0429cd5c8482d75c3fb42296&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c39d1d1d4061f9f47764d27e4a22fd62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751467&auth_key=1760751467-0-0-1db65801b7980da1ead1e18fd41ab976&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Steerable-Conditional-Diffusion-for-Domain-Adaptation-in-PET-Image-Reconstruction"><a href="#Steerable-Conditional-Diffusion-for-Domain-Adaptation-in-PET-Image-Reconstruction" class="headerlink" title="Steerable Conditional Diffusion for Domain Adaptation in PET Image   Reconstruction"></a>Steerable Conditional Diffusion for Domain Adaptation in PET Image   Reconstruction</h2><p><strong>Authors:George Webber, Alexander Hammers, Andrew P. King, Andrew J. Reader</strong></p>
<p>Diffusion models have recently enabled state-of-the-art reconstruction of positron emission tomography (PET) images while requiring only image training data. However, domain shift remains a key concern for clinical adoption: priors trained on images from one anatomy, acquisition protocol or pathology may produce artefacts on out-of-distribution data. We propose integrating steerable conditional diffusion (SCD) with our previously-introduced likelihood-scheduled diffusion (PET-LiSch) framework to improve the alignment of the diffusion modelâ€™s prior to the target subject. At reconstruction time, for each diffusion step, we use low-rank adaptation (LoRA) to align the diffusion model prior with the target domain on the fly. Experiments on realistic synthetic 2D brain phantoms demonstrate that our approach suppresses hallucinated artefacts under domain shift, i.e. when our diffusion model is trained on perturbed images and tested on normal anatomy, our approach suppresses the hallucinated structure, outperforming both OSEM and diffusion model baselines qualitatively and quantitatively. These results provide a proof-of-concept that steerable priors can mitigate domain shift in diffusion-based PET reconstruction and motivate future evaluation on real data. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æœ€è¿‘å®ç°äº†æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒçš„æœ€å…ˆè¿›é‡å»ºï¼Œè€Œä»…éœ€å›¾åƒè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œé¢†åŸŸåç§»ä»ç„¶æ˜¯ä¸´åºŠé‡‡ç”¨çš„å…³é”®é—®é¢˜ï¼šé’ˆå¯¹æŸä¸€è§£å‰–å­¦ã€é‡‡é›†åè®®æˆ–ç—…ç†å­¦å›¾åƒè®­ç»ƒçš„å…ˆéªŒå¯èƒ½ä¼šåœ¨åˆ†å¸ƒå¤–çš„æ•°æ®ä¸Šäº§ç”Ÿä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºå°†å¯æ§åˆ¶æ¡ä»¶æ‰©æ•£ï¼ˆSCDï¼‰ä¸æˆ‘ä»¬ä¹‹å‰å¼•å…¥çš„ä¼¼ç„¶è°ƒåº¦æ‰©æ•£ï¼ˆPET-LiSchï¼‰æ¡†æ¶ç›¸ç»“åˆï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡ä¸»é¢˜çš„å¯¹é½æ€§ã€‚åœ¨é‡å»ºè¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸€æ­¥æ‰©æ•£ï¼Œæˆ‘ä»¬ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¥å®æ—¶è°ƒæ•´æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡åŸŸçš„å¯¹é½ã€‚åœ¨çœŸå®åˆæˆäºŒç»´è„‘å¹»å½±ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢†åŸŸåç§»æ—¶æŠ‘åˆ¶äº†å¹»è§‰ä¼ªå½±ï¼Œå³å½“æˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹åœ¨æ‰°åŠ¨å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒå¹¶åœ¨æ­£å¸¸è§£å‰–å­¦ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŠ‘åˆ¶äº†å¹»è§‰ç»“æ„ï¼Œåœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½ä¼˜äºOSEMå’Œæ‰©æ•£æ¨¡å‹åŸºçº¿ã€‚è¿™äº›ç»“æœä¸ºå¯æ§åˆ¶å…ˆéªŒèƒ½å¤Ÿå‡è½»åŸºäºæ‰©æ•£çš„PETé‡å»ºä¸­çš„é¢†åŸŸåç§»æä¾›äº†æ¦‚å¿µéªŒè¯ï¼Œå¹¶æ¿€åŠ±æˆ‘ä»¬åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œæœªæ¥è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13441v1">PDF</a> Accepted for oral presentation at IEEE NSS MIC RTSD 2025 (submitted   May 2025; accepted July 2025; to be presented Nov 2025)</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ä»…ä½¿ç”¨å›¾åƒè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒçš„å…ˆè¿›é‡å»ºã€‚ç„¶è€Œï¼Œé¢†åŸŸåç§»æ˜¯ä¸´åºŠé‡‡ç”¨çš„å…³é”®é—®é¢˜ï¼šé’ˆå¯¹æŸä¸€è§£å‰–å­¦ã€é‡‡é›†åè®®æˆ–ç—…ç†å­¦çš„å…ˆéªŒçŸ¥è¯†å¯èƒ½åœ¨éåˆ†å¸ƒæ•°æ®ä¸Šäº§ç”Ÿä¼ªå½±ã€‚æœ¬ç ”ç©¶å°†å¯æ§åˆ¶æ¡ä»¶æ‰©æ•£ï¼ˆSCDï¼‰ä¸å…ˆå‰å¼•å…¥çš„ä¼¼ç„¶è°ƒåº¦æ‰©æ•£ï¼ˆPET-LiSchï¼‰æ¡†æ¶ç›¸ç»“åˆï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡ä¸»ä½“ä¹‹é—´çš„å¯¹é½æ€§ã€‚åœ¨é‡å»ºè¿‡ç¨‹ä¸­ï¼Œé’ˆå¯¹æ¯ä¸ªæ‰©æ•£æ­¥éª¤ï¼Œæœ¬ç ”ç©¶ä½¿ç”¨ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰æ¥å®æ—¶è°ƒæ•´æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡é¢†åŸŸä¹‹é—´çš„å¯¹é½ã€‚åœ¨çœŸå®äºŒç»´è„‘å¹»å½±ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬ç ”ç©¶çš„æ–¹æ³•åœ¨é¢†åŸŸåç§»ä¸‹æŠ‘åˆ¶äº†å¹»è§‰ä¼ªå½±ï¼Œå³å½“æ‰©æ•£æ¨¡å‹ç»è¿‡æ‰°åŠ¨å›¾åƒè®­ç»ƒå¹¶åœ¨æ­£å¸¸è§£å‰–å­¦ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œè¯¥æ–¹æ³•æŠ‘åˆ¶äº†å¹»è§‰ç»“æ„ï¼Œåœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡ä¼˜äºOSEMå’Œæ‰©æ•£æ¨¡å‹åŸºçº¿ã€‚è¿™äº›ç»“æœè¯æ˜äº†å¯æ§åˆ¶å…ˆéªŒçŸ¥è¯†å¯ä»¥ç¼“è§£æ‰©æ•£å¼PETé‡å»ºä¸­çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œå¹¶æ¿€åŠ±æœªæ¥åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨ç‰¹å®šPETå›¾åƒä¿¡æ¯çš„æƒ…å†µä¸‹é‡å»ºå›¾åƒï¼Œä½†é¢ä¸´é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>é¢†åŸŸåç§»å¯èƒ½å¯¼è‡´åœ¨éåˆ†å¸ƒæ•°æ®ä¸Šå‡ºç°ä¼ªå½±ã€‚</li>
<li>ç»“åˆå¯æ§åˆ¶æ¡ä»¶æ‰©æ•£ï¼ˆSCDï¼‰å’Œä¼¼ç„¶è°ƒåº¦æ‰©æ•£ï¼ˆPET-LiSchï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸ç›®æ ‡ä¸»ä½“ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­ï¼Œä½¿ç”¨ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰æ¥å®æ—¶è°ƒæ•´æ¨¡å‹ä»¥é€‚åº”ç›®æ ‡é¢†åŸŸã€‚</li>
<li>åœ¨äºŒç»´è„‘å¹»å½±ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢†åŸŸåç§»æƒ…å†µä¸‹æ€§èƒ½ä¼˜è¶Šï¼ŒæŠ‘åˆ¶äº†å¹»è§‰ä¼ªå½±ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºOSEMå’Œå•çº¯çš„æ‰©æ•£æ¨¡å‹åŸºçº¿æœ‰æ›´å¥½çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-daae8d2807475670baa395404ff48ad5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751475&auth_key=1760751475-0-0-3bd9133d170da330d927097888d46d10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f06a91f46e3357e1a8bc83128d90f1f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751482&auth_key=1760751482-0-0-d71a4259f3b0795e984cfc421b9d4a16&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification"><a href="#Universal-Image-Restoration-Pre-training-via-Masked-Degradation-Classification" class="headerlink" title="Universal Image Restoration Pre-training via Masked Degradation   Classification"></a>Universal Image Restoration Pre-training via Masked Degradation   Classification</h2><p><strong>Authors:JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu</strong></p>
<p>This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at <a target="_blank" rel="noopener" href="https://github.com/MILab-PKU/MaskDCPT">https://github.com/MILab-PKU/MaskDCPT</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åä¸ºMasked Degradation Classification Pre-Trainingï¼ˆMaskDCPTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¾“å…¥å›¾åƒä¸­çš„é€€åŒ–ç±»å‹åˆ†ç±»ï¼Œä»è€Œå®ç°å…¨é¢çš„å›¾åƒæ¢å¤é¢„è®­ç»ƒã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒMaskDCPTä½¿ç”¨å›¾åƒçš„é€€åŒ–ç±»å‹ä½œä¸ºæå¼±çš„ç›‘ç£ä¿¡æ¯ï¼ŒåŒæ—¶åˆ©ç”¨å›¾åƒé‡å»ºæ¥æé«˜æ€§èƒ½å’Œç¨³å¥æ€§ã€‚MaskDCPTåŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨ï¼šç¼–ç å™¨ä»é®ç½©çš„ä½è´¨é‡è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ã€‚åˆ†ç±»è§£ç å™¨ä½¿ç”¨è¿™äº›ç‰¹å¾æ¥è¯†åˆ«é€€åŒ–ç±»å‹ï¼Œè€Œé‡å»ºè§£ç å™¨çš„ç›®æ ‡æ˜¯é‡å»ºç›¸åº”çš„é«˜è´¨é‡å›¾åƒã€‚è¿™ç§è®¾è®¡ä½¿é¢„è®­ç»ƒèƒ½å¤ŸåŒæ—¶å—ç›Šäºé®ç½©å›¾åƒå»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿé€‚åˆæ¢å¤ä»»åŠ¡çš„é€šç”¨è¡¨ç¤ºã€‚å¾—ç›Šäºç®€å•è€Œå¼ºå¤§çš„MaskDCPTï¼Œé¢„è®­ç»ƒçš„ç¼–ç å™¨å¯ç”¨äºé€šç”¨å›¾åƒæ¢å¤ï¼Œå¹¶å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚å®æ–½MaskDCPTå¯ä»¥æ˜¾è‘—æé«˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒTransformerçš„æ€§èƒ½ï¼Œåœ¨5Då…¨åŠŸèƒ½æ¢å¤ä»»åŠ¡ä¸­PSNRå€¼æé«˜è‡³å°‘3.77dBï¼Œåœ¨ç°å®é€€åŒ–åœºæ™¯ä¸­ç›¸æ¯”åŸºçº¿é™ä½PIQE 34.8%ã€‚å®ƒè¿˜è¡¨ç°å‡ºå¯¹ä»¥å‰æœªè§è¿‡çš„é€€åŒ–ç±»å‹å’Œç¨‹åº¦çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´ç†å¹¶å‘å¸ƒäº†UIR-2.5Mæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬è·¨è¶Š19ç§é€€åŒ–ç±»å‹å’Œè¶…è¿‡200ä¸ªé€€åŒ–çº§åˆ«çš„250ä¸‡å¯¹æ¢å¤æ ·æœ¬ï¼Œèåˆäº†åˆæˆæ•°æ®å’Œç°å®æ•°æ®ã€‚æ•°æ®é›†ã€æºä»£ç å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MILab-PKU/MaskDCPT">https://github.com/MILab-PKU/MaskDCPT</a>ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13282v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMaskDCPTçš„æ©è†œé€€åŒ–åˆ†ç±»é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¾“å…¥å›¾åƒçš„é€€åŒ–ç±»å‹åˆ†ç±»ï¼Œä»è€Œå®ç°å…¨é¢çš„å›¾åƒæ¢å¤é¢„è®­ç»ƒã€‚MaskDCPTä½¿ç”¨å›¾åƒçš„é€€åŒ–ç±»å‹ä½œä¸ºæå¼±çš„ç›‘ç£ä¿¡æ¯ï¼ŒåŒæ—¶åˆ©ç”¨å›¾åƒé‡å»ºæ¥æé«˜æ€§èƒ½å’Œç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨ï¼šç¼–ç å™¨ä»è¢«æ©è†œçš„ä½è´¨é‡è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ï¼›åˆ†ç±»è§£ç å™¨ä½¿ç”¨è¿™äº›ç‰¹å¾æ¥è¯†åˆ«é€€åŒ–ç±»å‹ï¼Œè€Œé‡å»ºè§£ç å™¨åˆ™æ—¨åœ¨é‡å»ºç›¸åº”çš„é«˜è´¨é‡å›¾åƒã€‚è¿™ç§è®¾è®¡ä½¿é¢„è®­ç»ƒå—ç›Šäºæ©è†œå›¾åƒå»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿé€‚åˆæ¢å¤ä»»åŠ¡çš„é€šç”¨è¡¨ç¤ºã€‚å¾—ç›ŠäºMaskDCPTçš„ç®€æ´è€Œå¼ºå¤§ï¼Œé¢„è®­ç»ƒçš„ç¼–ç å™¨å¯ç”¨äºé€šç”¨å›¾åƒæ¢å¤å¹¶å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚å®æ–½MaskDCPTæ˜¾è‘—æé«˜äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒTransformerçš„æ€§èƒ½ï¼Œåœ¨5Då…¨èƒ½æ¢å¤ä»»åŠ¡ä¸­å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†è‡³å°‘3.77 dBï¼Œåœ¨ç°å®é€€åŒ–åœºæ™¯ä¸­ä¸åŸºçº¿ç›¸æ¯”PIQEé™ä½äº†34.8%ã€‚æ­¤å¤–ï¼Œè¿˜æ•´ç†å’Œå‘å¸ƒäº†UIR-2.5Mæ•°æ®é›†ï¼ŒåŒ…å«250ä¸‡å¯¹æ¢å¤æ ·æœ¬ï¼Œæ¶µç›–19ç§é€€åŒ–ç±»å‹å’Œè¶…è¿‡200ä¸ªé€€åŒ–çº§åˆ«ï¼ŒåŒ…æ‹¬åˆæˆæ•°æ®å’Œç°å®æ•°æ®ã€‚æ•°æ®é›†ã€æºä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MILab-PKU/MaskDCPT">https://github.com/MILab-PKU/MaskDCPT</a>è·å¾—ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MaskDCPTæ˜¯ä¸€ç§æ–°çš„æ©è†œé€€åŒ–åˆ†ç±»é¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå›¾åƒæ¢å¤ã€‚</li>
<li>MaskDCPTåˆ©ç”¨å›¾åƒçš„é€€åŒ–ç±»å‹å’Œå›¾åƒé‡å»ºæ¥æé«˜æ€§èƒ½ã€‚</li>
<li>MaskDCPTåŒ…æ‹¬ç¼–ç å™¨ã€åˆ†ç±»è§£ç å™¨å’Œé‡å»ºè§£ç å™¨ã€‚</li>
<li>é¢„è®­ç»ƒå—ç›Šäºæ©è†œå›¾åƒå»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>MaskDCPTåœ¨å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>å‘å¸ƒäº†åŒ…å«åˆæˆæ•°æ®å’Œç°å®æ•°æ®çš„UIR-2.5Mæ•°æ®é›†ã€‚</li>
<li>MaskDCPTçš„æºä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å¯å…¬å¼€è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-701e639f8dafa7ee9afa1e8a657b786b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751490&auth_key=1760751490-0-0-22dc9e25c66dc571ca52319730a14a4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f6c1c2851fc2723ee013ed72d9878d0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751498&auth_key=1760751498-0-0-3f59d2946c32dddc83dce72b95d36099&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a3ed1d601cea99d0a0beb99595f5d65a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909606&auth_key=1760909606-0-0-9816cee3dc8225ec07ac294f231389b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a6ad3d355e1d4792e61ca73002defa3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751524&auth_key=1760751524-0-0-3dfcedbd29b7e4b595b968303db1e5a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VPREG-An-Optimal-Control-Formulation-for-Diffeomorphic-Image-Registration-Based-on-the-Variational-Principle-Grid-Generation-Method"><a href="#VPREG-An-Optimal-Control-Formulation-for-Diffeomorphic-Image-Registration-Based-on-the-Variational-Principle-Grid-Generation-Method" class="headerlink" title="VPREG: An Optimal Control Formulation for Diffeomorphic Image   Registration Based on the Variational Principle Grid Generation Method"></a>VPREG: An Optimal Control Formulation for Diffeomorphic Image   Registration Based on the Variational Principle Grid Generation Method</h2><p><strong>Authors:Zicong Zhou, Baihan Zhao, Andreas Mang, Guojun Liao</strong></p>
<p>This paper introduces VPreg, a novel diffeomorphic image registration method. This work provides several improvements to our past work on mesh generation and diffeomorphic image registration. VPreg aims to achieve excellent registration accuracy while controlling the quality of the registration transformations. It ensures a positive Jacobian determinant of the spatial transformation and provides an accurate approximation of the inverse of the registration, a crucial property for many neuroimaging workflows. Unlike conventional methods, VPreg generates this inverse transformation within the group of diffeomorphisms rather than operating on the image space. The core of VPreg is a grid generation approach, referred to as \emph{Variational Principle} (VP), which constructs non-folding grids with prescribed Jacobian determinant and curl. These VP-generated grids guarantee diffeomorphic spatial transformations essential for computational anatomy and morphometry, and provide a more accurate inverse than existing methods. To assess the potential of the proposed approach, we conduct a performance analysis for 150 registrations of brain scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for 35 regions of interest, along with an empirical analysis of the properties of the computed spatial transformations, demonstrates that VPreg outperforms state-of-the-art methods in terms of Dice scores, regularity properties of the computed transformation, and accuracy and consistency of the provided inverse map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ä¿å½¢å›¾åƒé…å‡†æ–¹æ³•VPregã€‚è¿™é¡¹å·¥ä½œå¯¹æˆ‘ä»¬ä¹‹å‰åœ¨ç½‘æ ¼ç”Ÿæˆå’Œä¿å½¢å›¾åƒé…å‡†æ–¹é¢çš„å·¥ä½œè¿›è¡Œäº†å‡ é¡¹æ”¹è¿›ã€‚VPregæ—¨åœ¨å®ç°å‡ºè‰²çš„é…å‡†ç²¾åº¦ï¼ŒåŒæ—¶æ§åˆ¶é…å‡†å˜æ¢çš„è´¨é‡ã€‚å®ƒç¡®ä¿ç©ºé—´å˜æ¢çš„æ­£é›…å¯æ¯”è¡Œåˆ—å¼ï¼Œå¹¶æä¾›é…å‡†é€†å˜æ¢çš„å‡†ç¡®è¿‘ä¼¼å€¼ï¼Œè¿™å¯¹äºè®¸å¤šç¥ç»æˆåƒå·¥ä½œæµç¨‹éƒ½æ˜¯è‡³å…³é‡è¦çš„å±æ€§ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒVPregåœ¨å¾®åˆ†åŒèƒšç»„å†…éƒ¨ç”Ÿæˆè¿™ç§é€†å˜æ¢ï¼Œè€Œä¸æ˜¯åœ¨å›¾åƒç©ºé—´ä¸Šæ“ä½œã€‚VPregçš„æ ¸å¿ƒæ˜¯ä¸€ç§ç½‘æ ¼ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºâ€œå˜åˆ†åŸç†â€ï¼ˆVPï¼‰ï¼Œè¯¥æ–¹æ³•æ„å»ºäº†å…·æœ‰è§„å®šé›…å¯æ¯”è¡Œåˆ—å¼å’Œæ›²ç‡çš„éæŠ˜å ç½‘æ ¼ã€‚è¿™äº›VPç”Ÿæˆçš„ç½‘æ ¼ä¿è¯äº†è®¡ç®—è§£å‰–å­¦å’Œå½¢æ€æµ‹é‡å­¦ä¸­å¿…éœ€çš„ä¿å½¢ç©ºé—´å˜æ¢ï¼Œå¹¶ä¸”æ¯”ç°æœ‰æ–¹æ³•æä¾›äº†æ›´å‡†ç¡®çš„é€†å˜æ¢ã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯¹æ¥è‡ªOASIS-1æ•°æ®é›†çš„150æ¬¡è„‘æ‰«æè¿›è¡Œäº†æ€§èƒ½åˆ†æã€‚åŸºäºDiceåˆ†æ•°çš„æ€§èƒ½è¯„ä¼°ä»¥åŠè®¡ç®—çš„ç©ºé—´å˜æ¢å±æ€§çš„ç»éªŒåˆ†æè¡¨æ˜ï¼ŒVPregåœ¨Diceåˆ†æ•°ã€è®¡ç®—å˜æ¢çš„æ­£åˆ™å±æ€§ä»¥åŠæä¾›çš„é€†æ˜ å°„çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å°†ç»“æœä¸ANTs-SyNã€Freesurfer-Easyregå’ŒFSL-Fnirtè¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13109v1">PDF</a> 30 pages, 9 figures</p>
<p><strong>Summary</strong><br>    VPregæ˜¯ä¸€ç§æ–°å‹ä¿å½¢å›¾åƒé…å‡†æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ä¼˜ç§€çš„é…å‡†ç²¾åº¦åŒæ—¶æ§åˆ¶é…å‡†è½¬æ¢çš„è´¨é‡ã€‚VPregé‡‡ç”¨ç½‘æ ¼ç”ŸæˆæŠ€æœ¯ä¸ºæ ¸å¿ƒï¼Œèƒ½ç”ŸæˆéæŠ˜å ç½‘æ ¼ï¼Œä¿è¯ç©ºé—´å˜æ¢çš„ä¿å½¢æ€§ï¼Œä¸ºç¥ç»å½±åƒå·¥ä½œæµæä¾›æ›´å‡†ç¡®çš„åå‘æ˜ å°„ã€‚VPregåœ¨OASIS-1æ•°æ®é›†ä¸Šçš„è„‘æ‰«ææ³¨å†Œæ€§èƒ½åˆ†ææ˜¾ç¤ºï¼Œå…¶åœ¨Diceå¾—åˆ†ã€è®¡ç®—è½¬æ¢çš„æ­£åˆ™æ€§ã€åå‘æ˜ å°„çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VPregæ˜¯ä¸€ç§æ–°å‹çš„ä¿å½¢å›¾åƒé…å‡†æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é…å‡†ç²¾åº¦å¹¶æ§åˆ¶é…å‡†è½¬æ¢çš„è´¨é‡ã€‚</li>
<li>VPregèƒ½å¤Ÿç¡®ä¿ç©ºé—´å˜æ¢çš„æ­£é›…å¯æ¯”è¡Œåˆ—å¼ï¼Œæä¾›æ³¨å†Œçš„åå‘å‡†ç¡®è¿‘ä¼¼ï¼Œè¿™å¯¹äºè®¸å¤šç¥ç»æˆåƒå·¥ä½œæµç¨‹è‡³å…³é‡è¦ã€‚</li>
<li>VPregçš„æ ¸å¿ƒæ˜¯ç½‘æ ¼ç”ŸæˆæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ„å»ºäº†éæŠ˜å ç½‘æ ¼ï¼Œå…·æœ‰è§„å®šçš„é›…å¯æ¯”è¡Œåˆ—å¼å’Œå·æ›²ï¼Œä¿è¯äº†ç©ºé—´å˜æ¢çš„ä¿å½¢æ€§ã€‚</li>
<li>VPregç”Ÿæˆçš„ç½‘æ ¼ä¸ºè®¡ç®—è§£å‰–å­¦å’Œå½¢æ€è®¡é‡å­¦æä¾›äº†æ›´å‡†ç¡®çš„åå‘æ˜ å°„ã€‚</li>
<li>VPregåœ¨OASIS-1æ•°æ®é›†ä¸Šçš„æ€§èƒ½åˆ†ææ˜¾ç¤ºï¼Œä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨Diceå¾—åˆ†æ–¹é¢è¡¨ç°æ›´å¥½ã€‚</li>
<li>VPregåœ¨è®¡ç®—çš„è½¬æ¢çš„æ­£åˆ™æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7ac4efad97028b506ce688ca8a49e4e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751532&auth_key=1760751532-0-0-6d35304e21075d55e4921afe04241b97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e3a78a4ca132af90fff5ff885c64e9c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751539&auth_key=1760751539-0-0-9d2b025c296930f5106575ef7e2f15d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Domain-Adaptation-via-Content-Alignment-for-Hippocampus-Segmentation"><a href="#Unsupervised-Domain-Adaptation-via-Content-Alignment-for-Hippocampus-Segmentation" class="headerlink" title="Unsupervised Domain Adaptation via Content Alignment for Hippocampus   Segmentation"></a>Unsupervised Domain Adaptation via Content Alignment for Hippocampus   Segmentation</h2><p><strong>Authors:Hoda Kalabizadeh, Ludovica Griffanti, Pak-Hei Yeung, Ana I. L. Namburete, Nicola K. Dinsdale, Konstantinos Kamnitsas</strong></p>
<p>Deep learning models for medical image segmentation often struggle when deployed across different datasets due to domain shifts - variations in both image appearance, known as style, and population-dependent anatomical characteristics, referred to as content. This paper presents a novel unsupervised domain adaptation framework that directly addresses domain shifts encountered in cross-domain hippocampus segmentation from MRI, with specific emphasis on content variations. Our approach combines efficient style harmonisation through z-normalisation with a bidirectional deformable image registration (DIR) strategy. The DIR network is jointly trained with segmentation and discriminator networks to guide the registration with respect to a region of interest and generate anatomically plausible transformations that align source images to the target domain. We validate our approach through comprehensive evaluations on both a synthetic dataset using Morpho-MNIST (for controlled validation of core principles) and three MRI hippocampus datasets representing populations with varying degrees of atrophy. Across all experiments, our method outperforms existing baselines. For hippocampus segmentation, when transferring from young, healthy populations to clinical dementia patients, our framework achieves up to 15% relative improvement in Dice score compared to standard augmentation methods, with the largest gains observed in scenarios with substantial content shift. These results highlight the efficacy of our approach for accurate hippocampus segmentation across diverse populations. </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦å­¦æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†éƒ¨ç½²æ—¶ï¼Œç”±äºé¢†åŸŸåç§»ï¼ˆå›¾åƒå¤–è§‚å˜åŒ–ï¼Œç§°ä¸ºé£æ ¼ï¼Œä»¥åŠå–å†³äºäººç¾¤çš„è§£å‰–ç‰¹å¾ï¼Œç§°ä¸ºå†…å®¹ï¼‰å¸¸å¸¸ä¼šé‡åˆ°å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œç›´æ¥è§£å†³åœ¨MRIè·¨åŸŸæµ·é©¬ä½“åˆ†å‰²ä¸­é‡åˆ°çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œç‰¹åˆ«ä¾§é‡äºå†…å®¹å˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†é€šè¿‡zå½’ä¸€åŒ–å®ç°çš„æœ‰æ•ˆé£æ ¼å’Œè°ä»¥åŠåŒå‘å¯å˜å½¢å›¾åƒæ³¨å†Œï¼ˆDIRï¼‰ç­–ç•¥ã€‚DIRç½‘ç»œé€šè¿‡ä¸åˆ†å‰²å’Œé‰´åˆ«ç½‘ç»œè”åˆè®­ç»ƒï¼Œä»¥æŒ‡å¯¼å…³äºæ„Ÿå…´è¶£åŒºåŸŸçš„æ³¨å†Œï¼Œå¹¶äº§ç”Ÿè§£å‰–ä¸Šåˆç†çš„è½¬æ¢ï¼Œä½¿æºå›¾åƒä¸ç›®æ ‡åŸŸå¯¹é½ã€‚æˆ‘ä»¬é€šè¿‡Morpho-MNISTåˆæˆæ•°æ®é›†ï¼ˆç”¨äºæ ¸å¿ƒåŸç†çš„å—æ§éªŒè¯ï¼‰å’Œä»£è¡¨ä¸åŒç¨‹åº¦èç¼©äººç¾¤çš„ä¸‰ä¸ªMRIæµ·é©¬æ•°æ®é›†è¿›è¡Œçš„å…¨é¢è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚åœ¨æ‰€æœ‰å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚å¯¹äºä»å¹´è½»å¥åº·äººç¾¤è½¬ç§»åˆ°ä¸´åºŠç—´å‘†æ‚£è€…çš„æµ·é©¬ä½“åˆ†å‰²ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸æ ‡å‡†å¢å¼ºæ–¹æ³•ç›¸æ¯”ï¼ŒDiceå¾—åˆ†ç›¸å¯¹æé«˜äº†é«˜è¾¾15%ï¼Œåœ¨å†…å®¹å˜åŒ–è¾ƒå¤§çš„åœºæ™¯ä¸­è§‚å¯Ÿåˆ°æœ€å¤§çš„æ”¶ç›Šã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ ·åŒ–äººç¾¤ä¸­å®ç°å‡†ç¡®æµ·é©¬ä½“åˆ†å‰²çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13075v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥è§£å†³äº†åœ¨è·¨åŸŸæµ·é©¬ä½“åˆ†å‰²ä¸­é‡åˆ°çš„åŸŸè½¬ç§»é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…å®¹å˜åŒ–æ–¹é¢ã€‚é€šè¿‡Zæ­£è§„åŒ–ä¸åŒå‘å¯å˜å½¢å›¾åƒæ³¨å†Œï¼ˆDIRï¼‰ç­–ç•¥çš„ç›¸ç»“åˆï¼Œè¯¥æ¡†æ¶å®ç°äº†æœ‰æ•ˆçš„é£æ ¼åè°ƒã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®é›†å’Œä»£è¡¨ä¸åŒèç¼©ç¨‹åº¦äººç¾¤çš„ä¸‰ä¸ªMRIæµ·é©¬æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ç›¸è¾ƒäºç°æœ‰åŸºå‡†çº¿ï¼Œæœ¬æ–¹æ³•åœ¨æ‰€æœ‰å®éªŒä¸­è¡¨ç°æ›´ä¼˜ã€‚å¯¹äºä»å¹´è½»å¥åº·äººç¾¤è½¬ç§»åˆ°ä¸´åºŠç—´å‘†æ‚£è€…çš„æµ·é©¬ä½“åˆ†å‰²ä»»åŠ¡ï¼Œä¸æ ‡å‡†å¢å¼ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ç‹„æ°ç³»æ•°ä¸Šå®ç°äº†é«˜è¾¾15%çš„ç›¸å¯¹æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å†…å®¹å˜åŒ–è¾ƒå¤§çš„åœºæ™¯ä¸­ï¼Œæå‡æœ€ä¸ºæ˜¾è‘—ã€‚è¿™è¯æ˜äº†è¯¥æ¡†æ¶åœ¨è·¨ä¸åŒäººç¾¤è¿›è¡Œå‡†ç¡®æµ·é©¬ä½“åˆ†å‰²æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä¸»è¦ç ”ç©¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­è·¨ä¸åŒæ•°æ®é›†éƒ¨ç½²æ—¶é‡åˆ°çš„åŸŸè½¬ç§»é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºç›´æ¥è§£å†³åœ¨è·¨åŸŸæµ·é©¬ä½“åˆ†å‰²ä¸­çš„åŸŸè½¬ç§»é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†Zæ­£è§„åŒ–çš„é£æ ¼åè°ƒå’ŒåŒå‘å¯å˜å½¢å›¾åƒæ³¨å†Œç­–ç•¥ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®é›†å’ŒçœŸå®MRIæµ·é©¬æ•°æ®é›†çš„å…¨é¢è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨æ‰€æœ‰å®éªŒä¸­è¡¨ç°æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ·é©¬ä½“åˆ†å‰²ä»»åŠ¡ä¸­ã€‚</li>
<li>å½“ä»å¹´è½»å¥åº·äººç¾¤è½¬ç§»åˆ°ä¸´åºŠç—´å‘†æ‚£è€…æ—¶ï¼Œè¯¥æ¡†æ¶åœ¨ç‹„æ°ç³»æ•°ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ac73306d1e6a7ba4f714ad21af477044~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751589&auth_key=1760751589-0-0-d32fd0f2edc8f49fc51d3f0fc3714309&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1b263190d768881aecab343f66fbb2d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751596&auth_key=1760751596-0-0-9593ffcfcbe51e78c38c2efb06f86316&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c2d72e496dbc8358e4c56e5e30c2ff5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751603&auth_key=1760751603-0-0-dd687864a31719d7f20dbc2d99b5f878&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a66c5294855e588feb208905383ae94f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751610&auth_key=1760751610-0-0-eab1e130137407a7387f66a80375fba5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62e9c50185dfba6f94c36b7bad5435b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760751616&auth_key=1760751616-0-0-25cd7e28a34ba84c17931e47788ee7e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Text-Image-Fusion-Method-with-Data-Augmentation-Capabilities-for-Referring-Medical-Image-Segmentation"><a href="#A-Text-Image-Fusion-Method-with-Data-Augmentation-Capabilities-for-Referring-Medical-Image-Segmentation" class="headerlink" title="A Text-Image Fusion Method with Data Augmentation Capabilities for   Referring Medical Image Segmentation"></a>A Text-Image Fusion Method with Data Augmentation Capabilities for   Referring Medical Image Segmentation</h2><p><strong>Authors:Shurong Chai, Rahul Kumar JAIN, Rui Xu, Shaocong Mo, Ruibo Hou, Shiyu Teng, Jiaqing Liu, Lanfen Lin, Yen-Wei Chen</strong></p>
<p>Deep learning relies heavily on data augmentation to mitigate limited data, especially in medical imaging. Recent multimodal learning integrates text and images for segmentation, known as referring or text-guided image segmentation. However, common augmentations like rotation and flipping disrupt spatial alignment between image and text, weakening performance. To address this, we propose an early fusion framework that combines text and visual features before augmentation, preserving spatial consistency. We also design a lightweight generator that projects text embeddings into visual space, bridging semantic gaps. Visualization of generated pseudo-images shows accurate region localization. Our method is evaluated on three medical imaging tasks and four segmentation frameworks, achieving state-of-the-art results. Code is publicly available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/11yxk/MedSeg_EarlyFusion">https://github.com/11yxk/MedSeg_EarlyFusion</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ•°æ®å¢å¼ºæ¥ç¼“è§£æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸã€‚æœ€è¿‘çš„å¤šæ¨¡å¼å­¦ä¹ å°†æ–‡æœ¬å’Œå›¾åƒç»“åˆèµ·æ¥è¿›è¡Œåˆ†å‰²ï¼Œè¿™è¢«ç§°ä¸ºå¼•ç”¨æˆ–æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ†å‰²ã€‚ç„¶è€Œï¼Œå¸¸è§çš„å¢å¼ºæ–¹æ³•å¦‚æ—‹è½¬å’Œç¿»è½¬ä¼šç ´åå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç©ºé—´å¯¹é½ï¼Œä»è€Œé™ä½æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—©æœŸèåˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å¢å¼ºä¹‹å‰å°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ç»“åˆèµ·æ¥ï¼Œä¿æŒç©ºé—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„ç”Ÿæˆå™¨ï¼Œå®ƒå°†æ–‡æœ¬åµŒå…¥æŠ•å½±åˆ°è§†è§‰ç©ºé—´ä¸­ï¼Œç¼©å°è¯­ä¹‰å·®è·ã€‚ç”Ÿæˆçš„ä¼ªå›¾åƒçš„å¯è§†åŒ–æ˜¾ç¤ºåŒºåŸŸå®šä½å‡†ç¡®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªåŒ»å­¦å½±åƒä»»åŠ¡å’Œå››ä¸ªåˆ†å‰²æ¡†æ¶ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/11yxk/MedSeg_EarlyFusion%E3%80%82">https://github.com/11yxk/MedSeg_EarlyFusionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12482v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å½±åƒé¢†åŸŸä¸­ï¼Œæ·±åº¦å­¦ä¹ ä¾é æ•°æ®å¢å¼ºæŠ€æœ¯æ¥åº”å¯¹æ•°æ®é‡æœ‰é™çš„é—®é¢˜ï¼Œå°¤å…¶åœ¨æ¶‰åŠå›¾åƒåˆ†å‰²æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚å¤šæ¨¡æ€å­¦ä¹ é€šè¿‡å°†æ–‡æœ¬å’Œå›¾åƒèåˆæ¥æå‡åˆ†å‰²æ€§èƒ½ï¼Œä½†ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•å¦‚æ—‹è½¬å’Œç¿»è½¬ä¼šç ´åå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç©ºé—´å¯¹é½å…³ç³»ï¼Œå½±å“æ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ—©æœŸèåˆæ¡†æ¶ï¼Œåœ¨æ•°æ®å¢å¼ºå‰ç»“åˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ï¼Œä¿æŒç©ºé—´ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§ç”Ÿæˆå™¨ï¼Œå°†æ–‡æœ¬åµŒå…¥æŠ•å½±åˆ°è§†è§‰ç©ºé—´ï¼Œç¼©å°è¯­ä¹‰é¸¿æ²Ÿã€‚å¯è§†åŒ–ç”Ÿæˆçš„ä¼ªå›¾åƒæ˜¾ç¤ºåŒºåŸŸå®šä½å‡†ç¡®ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŒ»å­¦å½±åƒä»»åŠ¡å’Œå››ä¸ªåˆ†å‰²æ¡†æ¶ä¸Šå–å¾—ä¸€æµæˆæœã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šåˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­ä¾èµ–æ•°æ®å¢å¼ºåº”å¯¹æœ‰é™æ•°æ®é—®é¢˜ã€‚</li>
<li>å¤šæ¨¡æ€å­¦ä¹ é€šè¿‡ç»“åˆæ–‡æœ¬å’Œå›¾åƒæå‡å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•å¯èƒ½ç ´åå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç©ºé—´å¯¹é½ï¼Œå½±å“æ€§èƒ½ã€‚</li>
<li>æå‡ºæ—©æœŸèåˆæ¡†æ¶ï¼Œåœ¨æ•°æ®å¢å¼ºå‰ç»“åˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ã€‚</li>
<li>è®¾è®¡è½»é‡çº§ç”Ÿæˆå™¨å°†æ–‡æœ¬åµŒå…¥æŠ•å½±åˆ°è§†è§‰ç©ºé—´ï¼Œç¼©å°è¯­ä¹‰é¸¿æ²Ÿã€‚</li>
<li>ç”Ÿæˆä¼ªå›¾åƒçš„å¯è§†åŒ–æ˜¾ç¤ºåŒºåŸŸå®šä½å‡†ç¡®ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªåŒ»å­¦å½±åƒä»»åŠ¡å’Œåˆ†å‰²æ¡†æ¶ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f974bb28bfb29550502ae9b8eaa113f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909614&auth_key=1760909614-0-0-6b5a00bf5c00caac1da07b9058ceb1c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b5db6f1b9deee19253b0f50d6f3eaa0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909622&auth_key=1760909622-0-0-6ff34c7b3cb4ee534c4a8a45a2136438&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8bed68aa36e82cafa3007e8cddf265a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909629&auth_key=1760909629-0-0-c832013110f0c591c6b269e01ce82348&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea14646214856478e039e1290864adc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909636&auth_key=1760909636-0-0-fdc9cc3f4e6d8aade43fd90c3241f40b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f83c55606728ea4c041a721795445bc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909642&auth_key=1760909642-0-0-8576baff06d256ffa8f2554a7d88740b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DPL-Spatial-Conditioned-Diffusion-Prototype-Enhancement-for-One-Shot-Medical-Segmentation"><a href="#DPL-Spatial-Conditioned-Diffusion-Prototype-Enhancement-for-One-Shot-Medical-Segmentation" class="headerlink" title="DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot   Medical Segmentation"></a>DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot   Medical Segmentation</h2><p><strong>Authors:Ziyuan Gao, Philippe Morel</strong></p>
<p>One-shot medical image segmentation faces fundamental challenges in prototype representation due to limited annotated data and significant anatomical variability across patients. Traditional prototype-based methods rely on deterministic averaging of support features, creating brittle representations that fail to capture intra-class diversity essential for robust generalization. This work introduces Diffusion Prototype Learning (DPL), a novel framework that reformulates prototype construction through diffusion-based feature space exploration. DPL models one-shot prototypes as learnable probability distributions, enabling controlled generation of diverse yet semantically coherent prototype variants from minimal labeled data. The framework operates through three core innovations: (1) a diffusion-based prototype enhancement module that transforms single support prototypes into diverse variant sets via forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism that leverages geometric properties derived from prototype feature statistics, and (3) a conservative fusion strategy that preserves prototype fidelity while maximizing representational diversity. DPL ensures training-inference consistency by using the same diffusion enhancement and fusion pipeline in both phases. This process generates enhanced prototypes that serve as the final representations for similarity calculations, while the diffusion process itself acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets demonstrate significant improvements respectively, establishing new state-of-the-art performance in one-shot medical image segmentation. </p>
<blockquote>
<p>ä¸€æ¬¡æ€§åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ç”±äºæ ‡æ³¨æ•°æ®æœ‰é™å’Œæ‚£è€…ä¹‹é—´è§£å‰–ç»“æ„å˜åŒ–æ˜¾è‘—è€Œå¯¼è‡´çš„åŸå‹è¡¨ç¤ºæ–¹é¢çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚ä¼ ç»ŸåŸºäºåŸå‹çš„æ–¹æ³•ä¾èµ–äºæ”¯æŒç‰¹å¾çš„ç¡®å®šæ€§å¹³å‡ï¼Œè¿™ä¼šäº§ç”Ÿè„†å¼±çš„è¡¨ç¤ºï¼Œæ— æ³•æ•è·å¯¹äºç¨³å¥æ³›åŒ–è‡³å…³é‡è¦çš„ç±»å†…å¤šæ ·æ€§ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†æ‰©æ•£åŸå‹å­¦ä¹ ï¼ˆDPLï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºæ‰©æ•£çš„ç‰¹å¾ç©ºé—´æ¢ç´¢æ¥é‡æ–°æ„å»ºåŸå‹ã€‚DPLå°†ä¸€æ¬¡æ€§åŸå‹å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿èƒ½å¤Ÿä»æå°‘çš„æ ‡è®°æ•°æ®ä¸­ç”Ÿæˆå¤šæ ·åŒ–ä½†è¯­ä¹‰è¿è´¯çš„åŸå‹å˜ä½“ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹æ¥è¿è¡Œï¼šï¼ˆ1ï¼‰åŸºäºæ‰©æ•£çš„åŸå‹å¢å¼ºæ¨¡å—ï¼Œå®ƒé€šè¿‡æ­£å‘-åå‘æ‰©æ•£è¿‡ç¨‹å°†å•ä¸€æ”¯æŒåŸå‹è½¬æ¢ä¸ºå¤šæ ·åŒ–çš„å˜ä½“é›†ï¼Œï¼ˆ2ï¼‰åˆ©ç”¨ä»åŸå‹ç‰¹å¾ç»Ÿè®¡æ´¾ç”Ÿçš„å‡ ä½•å±æ€§çš„ç©ºé—´æ„ŸçŸ¥æ¡ä»¶æœºåˆ¶ï¼Œä»¥åŠï¼ˆ3ï¼‰ä¸€ç§ä¿å®ˆçš„èåˆç­–ç•¥ï¼Œæ—¢ä¿æŒåŸå‹çš„ä¿çœŸåº¦ï¼Œåˆæœ€å¤§åŒ–è¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚DPLé€šè¿‡ä¸¤ä¸ªé˜¶æ®µéƒ½ä½¿ç”¨ç›¸åŒçš„æ‰©æ•£å¢å¼ºå’Œèåˆç®¡é“ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†çš„ä¸€è‡´æ€§ã€‚è¿™ä¸ªè¿‡ç¨‹ç”Ÿæˆå¢å¼ºçš„åŸå‹ï¼Œä½œä¸ºç›¸ä¼¼æ€§è®¡ç®—çš„æœ€ç»ˆè¡¨ç¤ºï¼Œè€Œæ‰©æ•£è¿‡ç¨‹æœ¬èº«åˆ™èµ·åˆ°æ­£åˆ™åŒ–çš„ä½œç”¨ã€‚åœ¨è…¹éƒ¨MRIå’ŒCTæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒåˆ†åˆ«è¯æ˜äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨ä¸€å‡»åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12159v1">PDF</a> Accepted at IVCNZ 2025. To be published in IEEE proceedings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºæ‰©æ•£åŸå‹å­¦ä¹ ï¼ˆDPLï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å•æ ·æœ¬é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŸºäºæ‰©æ•£çš„ç‰¹å¾ç©ºé—´æ¢ç´¢æ¥é‡æ–°æ„å»ºåŸå‹ï¼Œå°†å•æ ·æœ¬åŸå‹å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¯ä»å°‘é‡æ ‡è®°æ•°æ®ä¸­ç”Ÿæˆå¤šæ ·ä¸”è¯­ä¹‰è¿è´¯çš„åŸå‹å˜ä½“ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬æ‰©æ•£å¢å¼ºçš„åŸå‹æ¨¡å—ã€ç©ºé—´æ„ŸçŸ¥è°ƒèŠ‚æœºåˆ¶å’Œä¿å®ˆèåˆç­–ç•¥ã€‚DPLåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä½¿ç”¨ç›¸åŒçš„æ‰©æ•£å¢å¼ºå’Œèåˆç®¡é“ï¼Œç¡®ä¿ä¸€è‡´æ€§ã€‚æ­¤æ¡†æ¶åœ¨è…¹éƒ¨MRIå’ŒCTæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå»ºç«‹äº†ä¸€æµçš„å•æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å•æ ·æœ¬é—®é¢˜é¢ä¸´æœ‰é™æ ‡æ³¨æ•°æ®å’Œæ‚£è€…é—´è§£å‰–ç»“æ„å·®å¼‚çš„é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»ŸåŸºäºåŸå‹çš„æ–¹æ³•é€šè¿‡æ”¯æŒç‰¹å¾çš„ç¡®å®šæ€§å¹³å‡æ„å»ºåŸå‹ï¼Œè¿™å¿½ç•¥äº†ç±»å†…å¤šæ ·æ€§ï¼Œå½±å“æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£åŸå‹å­¦ä¹ ï¼ˆDPLï¼‰æ¡†æ¶é€šè¿‡åŸºäºæ‰©æ•£çš„ç‰¹å¾ç©ºé—´æ¢ç´¢é‡æ–°æ„å»ºåŸå‹ã€‚</li>
<li>DPLå°†å•æ ·æœ¬åŸå‹å»ºæ¨¡ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¯ä»å°‘é‡æ ‡æ³¨æ•°æ®ä¸­ç”Ÿæˆå¤šæ ·åŸå‹ã€‚</li>
<li>DPLæ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬æ‰©æ•£å¢å¼ºçš„åŸå‹æ¨¡å—ã€ç©ºé—´æ„ŸçŸ¥è°ƒèŠ‚å’Œä¿å®ˆèåˆç­–ç•¥ã€‚</li>
<li>DPLåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä½¿ç”¨ä¸€è‡´çš„æ‰©æ•£å¢å¼ºå’Œèåˆæµç¨‹ï¼Œç¡®ä¿æ¨¡å‹æ€§èƒ½çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-977e345d5198124baa41e45ed939054e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909650&auth_key=1760909650-0-0-8cd7dbbb8655e2463f7df5b13e483e07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5b0fc045cc903a5ed43f65895706ddc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909658&auth_key=1760909658-0-0-5185a7c2d8eebe08cb6a759f7c7d4ac1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8f7616d0ed303d759a2ea7f9dc557b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909664&auth_key=1760909664-0-0-a36d47e63f92b85f9c8d2fe888a53831&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e3dcbc9b82c472d42f7c1f5648768ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909671&auth_key=1760909671-0-0-df357adf7d5eab03a1d3a8ffbf9b9d1a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-243ed700b15091d5c04c5a8bbbe8094c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909678&auth_key=1760909678-0-0-e977fee06c081732f1c9a4d6ba92adf0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a039c7c946f15381149ecb62b20f5e58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909685&auth_key=1760909685-0-0-be3cffed0d8ac18e11ff312274c1c5d9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7bfa9e3f598506d4678b7e42df193268~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909692&auth_key=1760909692-0-0-32e018feebea61583c30f1b5b467f6c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Very-Long-Baseline-Interferometry-Imaging-with-Closure-Invariants-using-Conditional-Image-Diffusion"><a href="#Very-Long-Baseline-Interferometry-Imaging-with-Closure-Invariants-using-Conditional-Image-Diffusion" class="headerlink" title="Very-Long Baseline Interferometry Imaging with Closure Invariants using   Conditional Image Diffusion"></a>Very-Long Baseline Interferometry Imaging with Closure Invariants using   Conditional Image Diffusion</h2><p><strong>Authors:Samuel Lai, Nithyanandan Thyagarajan, O. Ivy Wong, Foivos Diakogiannis</strong></p>
<p>Image reconstruction in very-long baseline interferometry operates under severely sparse aperture coverage with calibration challenges from both the participating instruments and propagation medium, which introduce the risk of biases and artefacts. Interferometric closure invariants offers calibration-independent information on the true source morphology, but the inverse transformation from closure invariants to the source intensity distribution is an ill-posed problem. In this work, we present a generative deep learning approach to tackle the inverse problem of directly reconstructing images from their observed closure invariants. Trained in a supervised manner with simple shapes and the CIFAR-10 dataset, the resulting trained model achieves reduced chi-square data adherence scores of $\chi^2_{\rm CI} \lesssim 1$ and maximum normalised cross-correlation image fidelity scores of $\rho_{\rm NX} &gt; 0.9$ on tests of both trained and untrained morphologies, where $\rho_{\rm NX}&#x3D;1$ denotes a perfect reconstruction. We also adapt our model for the Next Generation Event Horizon Telescope total intensity analysis challenge. Our results on quantitative metrics are competitive to other state-of-the-art image reconstruction algorithms. As an algorithm that does not require finely hand-tuned hyperparameters, this method offers a relatively simple and reproducible calibration-independent imaging solution for very-long baseline interferometry, which ultimately enhances the reliability of sparse VLBI imaging results. </p>
<blockquote>
<p>åœ¨è¶…é•¿åŸºçº¿å¹²æ¶‰ä»ªçš„å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­ï¼Œå…¶å·¥ä½œå­”å¾„è¦†ç›–ç¨€ç–ï¼Œä¸”å­˜åœ¨æ¥è‡ªå‚ä¸ä»ªå™¨å’Œä¼ æ’­ä»‹è´¨çš„æ ¡å‡†æŒ‘æˆ˜ï¼Œè¿™å¼•å…¥äº†åå·®å’Œä¼ªå½±çš„é£é™©ã€‚å¹²æ¶‰é—­åˆä¸å˜é‡æä¾›äº†ç‹¬ç«‹äºæ ¡å‡†çš„çœŸå®æºå½¢æ€ä¿¡æ¯ï¼Œä½†ä»é—­åˆä¸å˜é‡åˆ°æºå¼ºåº¦åˆ†å¸ƒçš„é€†å‘è½¬æ¢æ˜¯ä¸€ä¸ªä¸é€‚å®šé—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ¥è§£å†³ä»è§‚å¯Ÿåˆ°çš„é—­åˆä¸å˜é‡ç›´æ¥é‡å»ºå›¾åƒçš„é€†å‘é—®é¢˜ã€‚ä½¿ç”¨ç®€å•å½¢çŠ¶å’ŒCIFAR-10æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒåï¼Œæ‰€å¾—æ¨¡å‹åœ¨è®­ç»ƒå’Œæœªè®­ç»ƒå½¢æ€æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†é™ä½çš„chiå¹³æ–¹æ•°æ®éµå¾ªåˆ†æ•°Ï‡^2CIâ‰¤1å’Œæœ€å¤§å½’ä¸€åŒ–äº¤å‰å…³è”å›¾åƒä¿çœŸåº¦åˆ†æ•°ÏNX&gt;0.9ï¼Œå…¶ä¸­ÏNX&#x3D;1è¡¨ç¤ºå®Œç¾é‡å»ºã€‚æˆ‘ä»¬è¿˜ä¸ºæˆ‘ä»¬çš„æ¨¡å‹é€‚åº”äº†ä¸‹ä¸€ä»£äº‹ä»¶è§†ç•Œæœ›è¿œé•œæ€»å¼ºåº¦åˆ†ææŒ‘æˆ˜ã€‚åœ¨å®šé‡æŒ‡æ ‡æ–¹é¢ï¼Œæˆ‘ä»¬çš„ç»“æœä¸å…¶ä»–æœ€å…ˆè¿›çš„å›¾åƒé‡å»ºç®—æ³•ç›¸å½“ã€‚ä½œä¸ºä¸€ç§ä¸éœ€è¦ç²¾ç»†æ‰‹åŠ¨è°ƒæ•´è¶…å‚æ•°çš„ç®—æ³•ï¼Œè¯¥æ–¹æ³•ä¸ºè¶…é•¿åŸºçº¿å¹²æ¶‰ä»ªæä¾›äº†ä¸€ç§ç›¸å¯¹ç®€å•ã€å¯é‡å¤çš„ç‹¬ç«‹äºæ ¡å‡†çš„æˆåƒè§£å†³æ–¹æ¡ˆï¼Œè¿™æœ€ç»ˆå¢å¼ºäº†ç¨€ç–VLBIæˆåƒç»“æœçš„å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12093v1">PDF</a> 20 pages, 8 figures, 2 tables, accepted in PASA</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹è¶…é•¿åŸºçº¿å¹²æ¶‰ä»ªçš„å›¾åƒé‡å»ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆã€‚ç”±äºç¨€ç–å­”å¾„è¦†ç›–å’Œä»ªå™¨åŠä¼ æ’­ä»‹è´¨çš„æ ¡å‡†æŒ‘æˆ˜ï¼Œå›¾åƒé‡å»ºå­˜åœ¨åå·®å’Œä¼ªå½±çš„é£é™©ã€‚å¹²æ¶‰é—­åˆä¸å˜æ€§æä¾›äº†ç‹¬ç«‹äºæ ¡å‡†çš„å…³äºçœŸå®æºå½¢æ€çš„ä¿¡æ¯ï¼Œä½†ä»é—­åˆä¸å˜æ€§åˆ°æºå¼ºåº¦åˆ†å¸ƒçš„é€†å‘è½¬æ¢æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡æ·±åº¦ç”Ÿæˆæ¨¡å‹ç›´æ¥å¯¹è§‚æµ‹åˆ°çš„é—­åˆä¸å˜æ€§è¿›è¡Œå›¾åƒé‡å»ºï¼Œè§£å†³é€†å‘é—®é¢˜ã€‚è¯¥æ¨¡å‹ç»è¿‡ç®€å•å½¢çŠ¶å’ŒCIFAR-10æ•°æ®é›†çš„ç›‘ç£è®­ç»ƒåï¼Œå¯¹è®­ç»ƒå’Œæœªè®­ç»ƒè¿‡çš„å½¢æ€å‡è¾¾åˆ°äº†é™ä½çš„chiå¹³æ–¹æ•°æ®è´´åˆåº¦å¾—åˆ†Ï‡CI2â‰¤1å’Œæœ€å¤§å½’ä¸€åŒ–äº¤å‰å…³è”å›¾åƒä¿çœŸåº¦å¾—åˆ†ÏNX&gt;0.9ï¼Œå…¶ä¸­ÏNX&#x3D;1è¡¨ç¤ºå®Œç¾é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é’ˆå¯¹ä¸‹ä¸€ä»£äº‹ä»¶è§†ç•Œæœ›è¿œé•œæ€»å¼ºåº¦åˆ†ææŒ‘æˆ˜è°ƒæ•´äº†æ¨¡å‹ã€‚åœ¨å®šé‡æŒ‡æ ‡ä¸Šï¼Œè¯¥æ–¹æ³•çš„ç»“æœä¸å…¶ä»–æœ€å…ˆè¿›çš„å›¾åƒé‡å»ºç®—æ³•å…·æœ‰ç«äº‰åŠ›ã€‚ä½œä¸ºä¸€ç§ä¸éœ€è¦ç²¾ç»†æ‰‹åŠ¨è°ƒæ•´è¶…å‚æ•°çš„ç®—æ³•ï¼Œè¯¥æ–¹æ³•ä¸ºè¶…é•¿åŸºçº¿å¹²æ¶‰ä»ªæä¾›äº†ä¸€ç§ç®€å•ä¸”å¯å¤åˆ¶çš„ç‹¬ç«‹äºæ ¡å‡†çš„æˆåƒè§£å†³æ–¹æ¡ˆï¼Œæœ€ç»ˆæé«˜äº†ç¨€ç–VLBIæˆåƒç»“æœçš„å¯é æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¶…é•¿åŸºçº¿å¹²æ¶‰ä»ªå›¾åƒé‡å»ºé¢ä¸´ç¨€ç–å­”å¾„è¦†ç›–å’Œæ ¡å‡†æŒ‘æˆ˜ã€‚</li>
<li>å¹²æ¶‰é—­åˆä¸å˜æ€§æä¾›ç‹¬ç«‹äºæ ¡å‡†çš„æºå½¢æ€ä¿¡æ¯ã€‚</li>
<li>ä»é—­åˆä¸å˜æ€§åˆ°æºå¼ºåº¦åˆ†å¸ƒçš„é€†å‘è½¬æ¢æ˜¯ä¸€ä¸ªä¸é€‚å®šé—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹ï¼Œç›´æ¥å¯¹è§‚å¯Ÿåˆ°çš„é—­åˆä¸å˜æ€§è¿›è¡Œå›¾åƒé‡å»ºã€‚</li>
<li>æ¨¡å‹ç»è¿‡ç®€å•å½¢çŠ¶å’ŒCIFAR-10æ•°æ®é›†çš„ç›‘ç£è®­ç»ƒï¼Œåœ¨æµ‹è¯•å’Œæœªè®­ç»ƒå½¢æ€ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å®šé‡æŒ‡æ ‡ä¸Šçš„ç»“æœä¸æœ€å…ˆè¿›çš„å›¾åƒé‡å»ºç®—æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4f7e300e26963b11194c25c4e2d29c83~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909699&auth_key=1760909699-0-0-f7fb6d946609635a05e926a9ea486efc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-accc67727bae704d0d447166e302bee5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909706&auth_key=1760909706-0-0-88954c7746df6bbb778cbdd075f2aa82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b903791d0c2e23d782f9f0c3323c5f5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909713&auth_key=1760909713-0-0-e851cda8a0d15d99ca38aa7b88dbb343&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU"><a href="#Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU" class="headerlink" title="Elevating Medical Image Security: A Cryptographic Framework Integrating   Hyperchaotic Map and GRU"></a>Elevating Medical Image Security: A Cryptographic Framework Integrating   Hyperchaotic Map and GRU</h2><p><strong>Authors:Weixuan Li, Guang Yu, Quanjun Li, Junhua Zhou, Jiajun Chen, Yihang Dong, Mengqian Wang, Zimeng Li, Changwei Gong, Lin Tang, Xuhang Chen</strong></p>
<p>Chaotic systems play a key role in modern image encryption due to their sensitivity to initial conditions, ergodicity, and complex dynamics. However, many existing chaos-based encryption methods suffer from vulnerabilities, such as inadequate permutation and diffusion, and suboptimal pseudorandom properties. This paper presents Kun-IE, a novel encryption framework designed to address these issues. The framework features two key contributions: the development of the 2D Sin-Cos Pi Hyperchaotic Map (2D-SCPHM), which offers a broader chaotic range and superior pseudorandom sequence generation, and the introduction of Kun-SCAN, a novel permutation strategy that significantly reduces pixel correlations, enhancing resistance to statistical attacks. Kun-IE is flexible and supports encryption for images of any size. Experimental results and security analyses demonstrate its robustness against various cryptanalytic attacks, making it a strong solution for secure image communication. The code is available at this \href{<a target="_blank" rel="noopener" href="https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU%7D%7Blink%7D">https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU}{link}</a>. </p>
<blockquote>
<p>æ··æ²Œç³»ç»Ÿåœ¨ç°ä»£å›¾åƒåŠ å¯†ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œå› ä¸ºå®ƒä»¬å¯¹åˆå§‹æ¡ä»¶æ•æ„Ÿã€å…·æœ‰éå†æ€§å’Œå¤æ‚çš„åŠ¨åŠ›å­¦ç‰¹æ€§ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„åŸºäºæ··æ²Œçš„åŠ å¯†æ–¹æ³•å­˜åœ¨æ¼æ´ï¼Œå¦‚ç½®æ¢å’Œæ‰©æ•£ä¸è¶³ä»¥åŠä¼ªéšæœºå±æ€§ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠ å¯†æ¡†æ¶Kun-IEï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ¡†æ¶æœ‰ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šä¸€æ˜¯å¼€å‘äº†äºŒç»´æ­£å¼¦ä½™å¼¦Ï€è¶…æ··æ²Œæ˜ å°„ï¼ˆ2D-SCPHMï¼‰ï¼Œå®ƒæä¾›äº†æ›´å¹¿æ³›çš„æ··æ²ŒèŒƒå›´å’Œæ›´ä¼˜è´¨çš„ä¼ªéšæœºåºåˆ—ç”Ÿæˆï¼›äºŒæ˜¯å¼•å…¥äº†Kun-SCANï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç½®æ¢ç­–ç•¥ï¼Œèƒ½æ˜¾è‘—é™ä½åƒç´ ç›¸å…³æ€§ï¼Œå¢å¼ºå¯¹ç»Ÿè®¡æ”»å‡»çš„æŠµæŠ—èƒ½åŠ›ã€‚Kun-IEçµæ´»æ”¯æŒä»»ä½•å¤§å°çš„å›¾åƒåŠ å¯†ã€‚å®éªŒç»“æœå’Œå®‰å…¨åˆ†æè¡¨æ˜ï¼Œå®ƒå¯¹å„ç§å¯†ç åˆ†ææ”»å‡»å…·æœ‰ç¨³å¥æ€§ï¼Œæ˜¯åŒ»ç–—å›¾åƒé€šä¿¡çš„å®‰å…¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU]">https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12084v1">PDF</a> Accepted By BIBM 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKun-IEçš„æ–°å‹å›¾åƒåŠ å¯†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äºŒç»´æ­£å¼¦ä½™å¼¦Piè¶…æ··æ²Œæ˜ å°„ï¼ˆ2D-SCPHMï¼‰å’ŒKun-SCANç½®æ¢ç­–ç•¥ï¼Œæé«˜äº†å›¾åƒåŠ å¯†çš„å®‰å…¨æ€§å’Œçµæ´»æ€§ã€‚è¯¥æ¡†æ¶å…·æœ‰å¹¿æ³›çš„æ··æ²ŒèŒƒå›´å’Œå‡ºè‰²çš„ä¼ªéšæœºåºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½æœ‰æ•ˆæŠµæŠ—ç»Ÿè®¡æ”»å‡»ï¼Œä¸”æ”¯æŒä»»æ„å¤§å°çš„å›¾åƒåŠ å¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kun-IEæ¡†æ¶åˆ©ç”¨æ··æ²Œç³»ç»Ÿçš„ç‰¹æ€§è¿›è¡Œå›¾åƒåŠ å¯†ï¼Œå¢å¼ºå®‰å…¨æ€§ã€‚</li>
<li>å¼•å…¥2D Sin-Cos Pi Hyperchaotic Mapï¼ˆ2D-SCPHMï¼‰ï¼Œæä¾›æ›´ä¸ºå¹¿æ³›çš„æ··æ²ŒèŒƒå›´å’Œä¼˜ç§€çš„ä¼ªéšæœºåºåˆ—ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>Kun-SCANç½®æ¢ç­–ç•¥æ˜¾è‘—å‡å°‘åƒç´ å…³è”ï¼Œå¢å¼ºæŠ—ç»Ÿè®¡æ”»å‡»çš„èƒ½åŠ›ã€‚</li>
<li>Kun-IEæ¡†æ¶å…·æœ‰çµæ´»æ€§ï¼Œæ”¯æŒä»»æ„å¤§å°çš„å›¾åƒåŠ å¯†ã€‚</li>
<li>å®éªŒç»“æœå’Œå®‰å…¨åˆ†æè¯æ˜ï¼ŒKun-IEèƒ½æŠµæŠ—å¤šç§å¯†ç åˆ†ææ”»å‡»ã€‚</li>
<li>è¯¥æ¡†æ¶ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
<li>è¯¥æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºæå‡åŒ»å­¦å›¾åƒçš„å®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3c282a94d7bad5a216cc5b63fc14d913~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909720&auth_key=1760909720-0-0-14b98406763ae71a6facdeb7ef2fae57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e74ba22e50b7c43931636d576035d292~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909728&auth_key=1760909728-0-0-f8597b6109ba1ccea9b8fd5bba1902aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb073a64c454173055190270d26fa6ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909734&auth_key=1760909734-0-0-7cdf6fea5289e07e50aba865fed04cee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a1a6012a39dcaccfa00de748013e841~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909741&auth_key=1760909741-0-0-2f701b231e72fb19f3910bd4e0389394&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95ed906c874d6d3e25f23993dd556459~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909748&auth_key=1760909748-0-0-5c64ee0fe56006e9c0d3918550dc94fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75d968ad408bd28310c18e5a752789f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909756&auth_key=1760909756-0-0-d4213c762d68a9f90b1ba57319cafd22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9361bc04f8a75c0960c09240cb3a17d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909762&auth_key=1760909762-0-0-88f4b2b3e1d8a63123135d08a301993f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b2297838c3c8d29947e5ce6ef2c0c66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909770&auth_key=1760909770-0-0-3bbefe87dff62a9363f9952a647e63ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-44a0058e75813d6d69993b137604f4a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760909777&auth_key=1760909777-0-0-051afe6e033d1820c601a48e24bf6954&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-df7301cfa07ebc24146ffb38a5a4a919~resize:0:q75.jpg?source=1f5c5e47&expiration=1760753396&auth_key=1760753396-0-0-c80ffd4ac8e468208506d88d129aa37e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  RLAIF-SPA Optimizing LLM-based Emotional Speech Synthesis via RLAIF
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-081c465c5cf95574b7e1c3d44818df90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760749458&auth_key=1760749458-0-0-d0462cc130b6b4965a75ae1843eba879&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Towards Generalist Intelligence in Dentistry Vision Foundation Models   for Oral and Maxillofacial Radiology
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
