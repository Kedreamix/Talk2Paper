<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-10-18  Talking Points Describing and Localizing Pixels">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-63ea9726baebdbc2f571e940f0ef5894~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756965&auth_key=1760756965-0-0-1a5e6d6ffd688eeee265c5a69bf2bd01&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    52 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-18-更新"><a href="#2025-10-18-更新" class="headerlink" title="2025-10-18 更新"></a>2025-10-18 更新</h1><h2 id="Talking-Points-Describing-and-Localizing-Pixels"><a href="#Talking-Points-Describing-and-Localizing-Pixels" class="headerlink" title="Talking Points: Describing and Localizing Pixels"></a>Talking Points: Describing and Localizing Pixels</h2><p><strong>Authors:Matan Rusanovsky, Shimon Malnick, Shai Avidan</strong></p>
<p>Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points">https://github.com/matanr/Talking_Points</a>. </p>
<blockquote>
<p>视觉语言模型在多模态理解方面取得了显著的成功。然而，这些模型仍然局限于对象级别或区域级别的定位，缺乏通过自然语言进行像素精确关键点理解的能力。我们引入了一个新颖的像素级别定位框架。该框架由两个互补的组件构成：一个点描述符，用于生成单个关键点的丰富上下文描述；一个点定位器，用于从这些描述中回归精确的像素坐标。不同于以前的工作依赖于模板提示或关键点名称，我们的方法生成自由形式的、从粗到细的描述，将关键点置于其视觉上下文中。由于没有可用的数据集来训练这样的系统，我们引入了LlamaPointInPart数据集，这是一个精心制作的由2万多个图像-关键点-描述三元组合成的数据集，从场景级别的上下文到关键点周围的视觉特征，捕捉多尺度信息。为了实现跨类别的泛化，我们使用GRPO优化点描述符在AP-10K上的性能，使用冻结的点定位器作为奖励模型来生成最大化定位精度的描述。为了评估我们的结果，我们建立了一个新的评估协议。不同于将我们的方法产生的文本描述与真实值进行比较，我们使用定位器来确定预测点与真实点之间的接近程度。在LlamaPointInPart数据集上的实验表明，与基线模型相比，我们的方法具有更优越的性能。我们框架的双向性质有望在未来实现关键点的图像理解和语言指导的精确定位的应用程序中得到应用。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/matanr/Talking_Points公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新颖的像素级定位框架，包含两个互补组件：点描述符和点定位器。点描述符生成个体关键点的丰富上下文描述，而点定位器则根据这些描述回归精确像素坐标。该方法能够产生自由形式的、从粗到细的描述，将关键点置于其视觉背景中，不同于依赖模板提示或关键点名称的先前方法。由于无法训练此类系统用的数据集，因此引入了LlamaPointInPart数据集，包含2万多个图像-关键点-描述三元组，由多个视觉语言模型合成，捕捉从场景级上下文到关键点周围视觉特征的多尺度信息。实验证明，该方法在LlamaPointInPart上的性能优于基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型在跨模态理解方面取得了显著成功，但仍局限于对象级或区域级的定位，缺乏通过自然语言进行像素精确的关键点理解。</li>
<li>提出了一种新颖的像素级定位框架，包含点描述符和点定位器两个互补组件。</li>
<li>点描述符能够生成关键点的丰富上下文描述，而点定位器可以根据这些描述回归精确像素坐标。</li>
<li>该方法能够产生自由形式的、置于其视觉背景中的描述，不同于依赖模板提示或关键点名称的先前方法。</li>
<li>引入了LlamaPointInPart数据集，用于训练此类系统，包含2万多个图像-关键点-描述三元组。</li>
<li>通过优化点描述符在AP-10K上的性能，实现了跨类别的泛化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-652edad153a670e8ef96c41230c0dec3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756899&auth_key=1760756899-0-0-286a6b73d4050d6d7ee5e6781aff53f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4cf2fa892aa95fc6f20cac393650257f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756906&auth_key=1760756906-0-0-69a904c6c903cf8007484572db87a949&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40a8a230e32586222cc7cab5d40aa88b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756913&auth_key=1760756913-0-0-8afe94040582db6b47ff11e4027d915d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a061fcf875e5f5c403e7da25446dd445~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756920&auth_key=1760756920-0-0-1056c52f312945e3831e6bcb83e52d6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Do-Slides-Help-Multi-modal-Context-for-Automatic-Transcription-of-Conference-Talks"><a href="#Do-Slides-Help-Multi-modal-Context-for-Automatic-Transcription-of-Conference-Talks" class="headerlink" title="Do Slides Help? Multi-modal Context for Automatic Transcription of   Conference Talks"></a>Do Slides Help? Multi-modal Context for Automatic Transcription of   Conference Talks</h2><p><strong>Authors:Supriti Sinhamahapatra, Jan Niehues</strong></p>
<p>State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily rely on acoustic information while disregarding additional multi-modal context. However, visual information are essential in disambiguation and adaptation. While most work focus on speaker images to handle noise conditions, this work also focuses on integrating presentation slides for the use cases of scientific presentation.   In a first step, we create a benchmark for multi-modal presentation including an automatic analysis of transcribing domain-specific terminology. Next, we explore methods for augmenting speech models with multi-modal information. We mitigate the lack of datasets with accompanying slides by a suitable approach of data augmentation. Finally, we train a model using the augmented dataset, resulting in a relative reduction in word error rate of approximately 34%, across all words and 35%, for domain-specific terms compared to the baseline model. </p>
<blockquote>
<p>当前最先进的自动语音识别（ASR）系统主要依赖于音频信息，而忽略了额外的多模态上下文。然而，视觉信息在解歧和适应方面至关重要。虽然大多数工作都专注于处理噪音条件下的演讲者图像，但这项工作还专注于将演示幻灯片集成到科学演示的应用场景中。首先，我们为多模态演示创建一个基准测试，包括自动分析转录专业术语的领域。接下来，我们探索用多模态信息增强语音模型的方法。我们通过适当的数据增强方法缓解了缺乏附带幻灯片的数据集问题。最后，我们使用增强数据集训练模型，与基准模型相比，在所有单词上的词错误率大约降低了34%，在专业术语上的词错误率降低了35%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13979v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文主要研究将多模态信息（包括演讲者的图像和演示文稿）融入自动语音识别（ASR）系统的方法。通过建立多模态演示的基准测试，探索了增强语音模型的方法，并通过数据增强技术解决了缺乏附带幻灯片的数据集问题。最终，使用增强数据集训练的模型相较于基准模型，在单词错误率上降低了约34%（针对所有单词）和针对特定领域的术语错误率降低了约35%。提高了系统的清晰度和准确性。简化后的内容便于实际应用与拓展研究。对于基于声学信息的现有技术是一个有益的补充与拓展。此为语音识别领域的进步提供了参考范例。有助于理解自动语音识别系统面临的挑战，以及如何利用多模态信息提高性能。在演讲和演示文稿识别方面展现出巨大潜力。该工作不仅解决了特定领域的术语识别问题，也为未来的语音识别技术提供了重要思路。为未来的ASR系统的发展提供了方向性的启示。有助于解决自动语音识别技术在复杂环境下的挑战。通过对数据的扩充技术为现有的语音识别数据集注入更多样化的内容提供了可能的解决方案。总结了工作提出的具体解决方案和对行业的贡献与意义。<strong>Key Takeaways</strong>： </p>
<ul>
<li>多模态信息融入自动语音识别系统提高性能研究对声学信息的局限提出新的思路方向 </li>
<li>创建了针对语音与幻灯片展示结合的展示系统处理研究的基准测试，通过该测试能够进一步扩展其应用与研究 </li>
<li>采用数据增强技术解决了缺乏幻灯片配套数据集的问题 </li>
<li>训练的新模型相比基准模型降低了大约百分之三十四的单词错误率和百分之三十五的专业术语错误率 </li>
<li>此研究在语音识别领域展现了极大的潜力，尤其是在解决特定术语识别问题方面表现出优势 </li>
<li>该研究对于未来解决自动语音识别系统在复杂环境下的挑战提供了重要启示</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fd9852fd1a17e25239bd60b64e3e7ed8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756928&auth_key=1760756928-0-0-839bb9088607a56a4ea52da89e7a4fc5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7716e0cbf090c2eba335267e51b5e8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756935&auth_key=1760756935-0-0-6c1328cb62b1bad8080d878e93eb667d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b7a5875ac064a853425e8897bf1dcdf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756942&auth_key=1760756942-0-0-372cf485e0512dea78eb5b41fc989054&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Playmate2-Training-Free-Multi-Character-Audio-Driven-Animation-via-Diffusion-Transformer-with-Reward-Feedback"><a href="#Playmate2-Training-Free-Multi-Character-Audio-Driven-Animation-via-Diffusion-Transformer-with-Reward-Feedback" class="headerlink" title="Playmate2: Training-Free Multi-Character Audio-Driven Animation via   Diffusion Transformer with Reward Feedback"></a>Playmate2: Training-Free Multi-Character Audio-Driven Animation via   Diffusion Transformer with Reward Feedback</h2><p><strong>Authors:Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang</strong></p>
<p>Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner. </p>
<blockquote>
<p>近期扩散模型（diffusion models）的进展极大地推动了音频驱动的人类视频生成技术，无论是在质量还是可控性方面都超越了传统方法。然而，现有方法仍然面临嘴唇同步精度、长视频生成的时序连贯性以及多角色动画方面的挑战。在这项工作中，我们提出了一个基于扩散变压器（DiT）的框架，用于生成任意长度的逼真谈话视频，并介绍了一种无训练的多角色音频驱动动画方法。首先，我们采用LoRA为基础的训练策略，结合位置偏移推理方法，既能够高效生成长视频，同时又保留了基础模型的能力。此外，我们将部分参数更新与奖励反馈相结合，以提高嘴唇同步和自然身体动作的质量。最后，我们提出了无训练的多角色动画方法——掩膜分类器免费指导（Mask Classifier-Free Guidance，Mask-CFG），它不需要特定的数据集或模型修改，并支持三个或更多角色的音频驱动动画。实验结果表明，我们的方法超越了现有的最先进方法，以简单、高效和成本效益高的方式实现了高质量、时序连贯和多角色的音频驱动视频生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12089v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于扩散模型的最新进展在音频驱动的人脸视频生成中的应用。提出了一种基于扩散变换器的框架，用于生成任意长度的逼真对话视频，并引入了一种无训练的多角色音频驱动动画方法。通过采用LoRA训练策略和位置偏移推断方法，实现了高效的长视频生成，同时保留了基础模型的能力。通过部分参数更新和奖励反馈，提高了唇同步和自然动作的表现。此外，还提出了一种无训练的多角色动画方法——Mask Classifier-Free Guidance（Mask-CFG），无需特定数据集或模型修改，支持三个或更多角色的音频驱动动画。实验结果证明，该方法优于现有先进技术，实现了高质量、时间连贯性和多角色音频驱动视频生成，简单、高效且经济实惠。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的最新进展显著改进了音频驱动的人脸视频生成技术，在质量和可控性方面超越了传统方法。</li>
<li>提出了基于扩散变换器的框架，用于生成任意长度的逼真对话视频。</li>
<li>采用LoRA训练策略和位置偏移推断方法，实现高效长视频生成，同时保留基础模型能力。</li>
<li>通过部分参数更新和奖励反馈增强唇同步和自然动作表现。</li>
<li>引入了无训练的多角色音频驱动动画方法——Mask Classifier-Free Guidance（Mask-CFG）。</li>
<li>该方法无需特定数据集或模型修改，支持三个或更多角色的音频驱动动画。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12089">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b1d8ff5b76d36acab85e20dff023f259~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756949&auth_key=1760756949-0-0-83768550c9ba0aadef6bf8957d9d3609&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bdd6273c9dd46541ff0cb7d3fa8677da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756957&auth_key=1760756957-0-0-b435656d97120bce0513ec26a45e6641&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63ea9726baebdbc2f571e940f0ef5894~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756965&auth_key=1760756965-0-0-1a5e6d6ffd688eeee265c5a69bf2bd01&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03abe6c86c76f8c21099d78b47d86f72~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756973&auth_key=1760756973-0-0-752aa5bbe7569821e21f6029875867aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DEMO-Disentangled-Motion-Latent-Flow-Matching-for-Fine-Grained-Controllable-Talking-Portrait-Synthesis"><a href="#DEMO-Disentangled-Motion-Latent-Flow-Matching-for-Fine-Grained-Controllable-Talking-Portrait-Synthesis" class="headerlink" title="DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained   Controllable Talking Portrait Synthesis"></a>DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained   Controllable Talking Portrait Synthesis</h2><p><strong>Authors:Peiyin Chen, Zhuowei Yang, Hui Feng, Sheng Jiang, Rui Yan</strong></p>
<p>Audio-driven talking-head generation has advanced rapidly with diffusion-based generative models, yet producing temporally coherent videos with fine-grained motion control remains challenging. We propose DEMO, a flow-matching generative framework for audio-driven talking-portrait video synthesis that delivers disentangled, high-fidelity control of lip motion, head pose, and eye gaze. The core contribution is a motion auto-encoder that builds a structured latent space in which motion factors are independently represented and approximately orthogonalized. On this disentangled motion space, we apply optimal-transport-based flow matching with a transformer predictor to generate temporally smooth motion trajectories conditioned on audio. Extensive experiments across multiple benchmarks show that DEMO outperforms prior methods in video realism, lip-audio synchronization, and motion fidelity. These results demonstrate that combining fine-grained motion disentanglement with flow-based generative modeling provides a powerful new paradigm for controllable talking-head video synthesis. </p>
<blockquote>
<p>基于扩散生成模型的音频驱动头部运动视频生成技术已经迅速发展，但生成时间连贯且具有精细动作控制的视频仍然具有挑战性。我们提出了DEMO，这是一个面向音频驱动的说话人视频合成的流匹配生成框架，实现了对唇部运动、头部姿势和眼睛注视的独立、高保真控制。核心贡献在于运动自编码器，它构建了一个结构化潜在空间，在该空间中独立表示运动因素并进行近似正交化处理。在这个解耦的运动空间上，我们应用基于最优传输的流匹配，结合转换器预测器，根据音频生成时间平滑的运动轨迹。在多个基准测试上的广泛实验表明，DEMO在视频逼真度、唇音同步和动作保真度方面优于以前的方法。这些结果表明，将精细动作解耦与基于流的生成模型相结合，为可控的说话人视频合成提供了强大的新范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10650v1">PDF</a> 5 pages</p>
<p><strong>摘要</strong></p>
<p>基于扩散模型的音频驱动谈话视频生成技术取得进展迅速，但产生时间连贯的视频并实现精细动作控制仍是挑战。我们提出DEMO，一个面向音频驱动谈话肖像视频合成的流匹配生成框架，提供唇部动作、头部姿态和眼睛注视的独立控制和高保真度。核心贡献在于运动自编码器，它构建了一个结构化潜在空间，其中运动因素独立表示并近似正交化。在此分离的运动空间中，我们使用基于最优传输的流匹配方法和变换预测器生成条件音频的时间平滑运动轨迹。在多个基准测试上的广泛实验表明，DEMO在视频逼真度、唇音同步和动作保真度方面优于先前的方法。这些结果证明，结合精细动作分离与流生成建模为可控谈话头视频合成提供了强大的新范式。</p>
<p><strong>要点</strong></p>
<ol>
<li>音频驱动谈话视频生成技术现状和挑战。</li>
<li>引入DEMO框架及其主要组件：运动自编码器。</li>
<li>运动自编码器构建结构化潜在空间，实现独立动作控制。</li>
<li>基于最优传输的流匹配方法和变换预测器生成条件音频的时间平滑运动轨迹。</li>
<li>在多个基准测试上的广泛实验表现优越。</li>
<li>结合精细动作分离与流生成建模为可控谈话头视频合成提供新范式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-eecaa9b787359d287a34297d2e3d7486~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756981&auth_key=1760756981-0-0-cfe62ad4ebe9c4b1f364d71e47d8a198&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-02ac1d21c28dd75d9ed679c7ad216510~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756988&auth_key=1760756988-0-0-c4683bf17c25b0e9314d7b5ce5600c8c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ead1df6bddcce9497d2b1b78be90dd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756996&auth_key=1760756996-0-0-59f889ec23687ec03aa6a511495a4998&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a574afc8ba6901aa1081167e6450aabf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757003&auth_key=1760757003-0-0-65069977ca10b5368303d8799c9848fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SyncLipMAE-Contrastive-Masked-Pretraining-for-Audio-Visual-Talking-Face-Representation"><a href="#SyncLipMAE-Contrastive-Masked-Pretraining-for-Audio-Visual-Talking-Face-Representation" class="headerlink" title="SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face   Representation"></a>SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face   Representation</h2><p><strong>Authors:Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou</strong></p>
<p>We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head&#x2F;face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining. </p>
<blockquote>
<p>我们介绍了SyncLipMAE，这是一种用于说话人脸视频的自我监督预训练框架，它可以从无标签的视听流中学习同步感知和可迁移的面动。我们的方法结合了掩模视觉建模和跨模态对比对齐，并采用了三个每帧提示令牌，显式编码说话人脸帧的关键因素：身份、语音运动（与语音同步的面动）和环境运动（与音频无关的动作，如眨眼和头部姿势）。对比目标使用时间对齐的语音运动和声学令牌作为正样本，错位的配对作为负样本，将两种模态驱动到共享嵌入空间，并产生令牌级的视听流同步。预训练后，对齐的声学令牌与视觉提示令牌（身份、语音运动、环境运动）形成了一个统一的接口，用于四种不同的下游场景：（i）视听流同步；（ii）面部情感识别和头部&#x2F;面部动作识别；（iii）视觉语音识别；（iv）视频配音，我们在单个模型中实现了不可区分的音频或视频驱动控制。在需要不同能力的四个任务家族中，SyncLipMAE取得了最先进的结果，强调了同步感知、因子化的自我监督预训练的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10069v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SyncLipMAE是一个用于说话人脸视频的自我监督预训练框架，它通过无标签的视听流学习同步感知和可迁移的面部动态。该方法结合了掩膜视觉建模和跨模态对比对齐，并采用三个每帧提示令牌显式编码说话人脸帧的关键因素：身份、语音动作（与语音同步的面部动态）和环境动作（与音频无关的动作，如眨眼和头部姿势）。对比目标使用时间对齐的语音动作和音频令牌作为正样本，错位的对作为负样本，推动两种模式进入共享嵌入空间，产生令牌级的视听流同步。预训练后，对齐的音频令牌与视觉提示令牌（身份、语音动作、环境动作）形成一个统一的接口，用于四种不同的下游设置，包括视听流同步、面部情感及头部&#x2F;面部动作识别、视觉语音识别和视觉配音。SyncLipMAE在四个需要不同能力的任务家族中取得了最先进的成果，突显了同步感知、分解自我监督预训练的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SyncLipMAE是一个自我监督预训练框架，专为说话人脸视频设计。</li>
<li>该框架通过无标签的视听流学习同步感知和可迁移的面部动态。</li>
<li>SyncLipMAE结合掩膜视觉建模和跨模态对比对齐。</li>
<li>使用三个每帧提示令牌来编码说话人脸帧的关键因素：身份、语音动作和环境动作。</li>
<li>对比目标使用时间对齐的语音动作和音频令牌，以推动两种模式进入共享嵌入空间。</li>
<li>预训练后，SyncLipMAE可在四种不同的下游设置中使用，包括视听流同步、面部情感及动作识别、视觉语音识别和视觉配音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10069">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9c6c231fed765423258fe234ae412690~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757011&auth_key=1760757011-0-0-4f345067e0367768897aba1a383c1fb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-391de2510e38295189764810bb249260~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757018&auth_key=1760757018-0-0-97dd0658fc7c42dc277704adb58a07e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-190877a8d3a02c18ee8f78ee7cbb4971~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757025&auth_key=1760757025-0-0-124166d6b2a33642e7fe8c508554614f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e99abe911990569078aa7a1f866286f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757031&auth_key=1760757031-0-0-59ddd2b80e38830948ebb47946bfc454&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a15551b727f9e713ce75cbbe3e045ac1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757038&auth_key=1760757038-0-0-78bc501bcfb001943153fdad924ca09c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d00000819b45008474ef976b1a274a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757044&auth_key=1760757044-0-0-5efa5e1d5192a27927765f0c32a442b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-with-Efficient-Gaussian-Deformation"><a href="#EGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-with-Efficient-Gaussian-Deformation" class="headerlink" title="EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient   Gaussian Deformation"></a>EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient   Gaussian Deformation</h2><p><strong>Authors:Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</strong></p>
<p>This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker’s potential for real-time multimedia applications. </p>
<blockquote>
<p>本文介绍了EGSTalker，一个基于3D高斯拼贴（3DGS）的实时音频驱动说话人头部生成框架。EGSTalker旨在提高速度和视觉保真度，仅需3-5分钟的训练视频即可合成高质量的面部动画。该框架包括两个关键阶段：静态高斯初始化和音频驱动变形。在第一阶段，使用多分辨率哈希三角平面和Kolmogorov-Arnold网络（KAN）提取空间特征并构建紧凑的3D高斯表示。在第二阶段，我们提出了有效的空间音频注意力（ESAA）模块，以融合音频和空间线索，而KAN则预测相应的高斯变形。大量实验表明，EGSTalker的渲染质量和嘴唇同步精度可与最先进的方法相媲美，同时在推理速度上显著优于它们。这些结果突出了EGSTalker在实时多媒体应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08587v1">PDF</a> Main paper (6 pages). Accepted for publication by IEEE International   Conference on Systems, Man, and Cybernetics 2025</p>
<p><strong>Summary</strong></p>
<p>基于3D高斯喷绘技术（3DGS），本文提出了实时音频驱动说话人头部生成框架EGSTalker。该框架旨在提高速度和视觉保真度，仅需要3-5分钟的训练视频即可合成高质量面部动画。它包含两个关键阶段：静态高斯初始化和音频驱动变形。首先，使用多分辨率哈希三平面和Kolmogorov-Arnold网络（KAN）提取空间特征并建立紧凑的3D高斯表示。然后，提出高效空间音频注意力（ESAA）模块来融合音频和空间线索，同时KAN预测相应的高斯变形。广泛实验表明，EGSTalker的渲染质量和唇同步准确性可与最新技术相媲美，且在推理速度上显著优于它们。这突显了EGSTalker在实时多媒体应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EGSTalker是一个基于3D高斯喷绘技术的实时音频驱动说话人头部生成框架。</li>
<li>框架包含静态高斯初始化和音频驱动变形两个关键阶段。</li>
<li>EGSTalker使用多分辨率哈希三平面和Kolmogorov-Arnold网络来提取空间特征并建立3D高斯表示。</li>
<li>提出了高效空间音频注意力模块来融合音频与空间线索。</li>
<li>EGSTalker仅需3-5分钟的训练视频即可合成高质量面部动画。</li>
<li>广泛实验表明，EGSTalker的渲染质量和唇同步准确性可与最新技术方法相媲美。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08587">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ddae32babae2278ef9e8ab7fadb34d82~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757051&auth_key=1760757051-0-0-d0da1eb55a2fb9602cbcaeaf1afda41a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9ac13bc6dcb5beee7bfbe6e5200c5dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757059&auth_key=1760757059-0-0-14e1b1ba9ad4904af87dfc583eda8b7d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0d5f6510dff4324afb9565ee6066dd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757065&auth_key=1760757065-0-0-2365b90bb63a83316f3f38dcfc20674b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2f3a8277e28e36aae03349ebdcf0c815~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757072&auth_key=1760757072-0-0-31432d2a75dea786c7ec041b424ccde3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e65d60aaf632f29d2df39174db6e96a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757080&auth_key=1760757080-0-0-b6b04fd3712197d5ecfb43c9c6527ae2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e45fabfd10490432cbc639b106fc5aef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757086&auth_key=1760757086-0-0-ce3f4d1b39a9b2fa6804ce8b6c4e10a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-726c52a499e0fd1215830be1e4113af0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757093&auth_key=1760757093-0-0-2840176b49a27afa7b52a7b0d5b157a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01c4fb226b5d3fd3d8c47516e2d4b1bb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757099&auth_key=1760757099-0-0-06d507cb06cfabd2de49ea4b8151f215&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective"><a href="#AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective" class="headerlink" title="AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective"></a>AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective</h2><p><strong>Authors:Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</strong></p>
<p>Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. In addition, to ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text&#x2F;audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized Phoneme-Frame Causal Attention Mask to support multi-step parallel acceleration. Extensive experiments conducted on both Chinese (CMLR) and English (HDTF) datasets demonstrate that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution. </p>
<blockquote>
<p>谈话头动画主要关注从音频输入生成逼真的面部视频。继生成对抗网络（GANs）之后，扩散模型由于其强大的生成能力已成为主流。然而，扩散过程本身的固有局限性常常导致帧间闪烁和推理缓慢，从而限制了其实际部署应用。为了解决这一问题，我们引入了AvatarSync，这是一个基于音素表示的自回归框架，能够从单个参考图像生成逼真且可控的谈话头动画，直接由文本或音频输入驱动。为了减轻闪烁并确保连续性，AvatarSync利用自回归管道增强了时间建模。此外，为了确保可控性，我们引入了音素（即语音声音的基本单位），构建了从文本&#x2F;音频到音素的多元到一元映射，实现了精确的音素到视觉的对应。另外，为了进一步加速推理，我们采用了两阶段生成策略，将语义建模与视觉动态解耦，并融入定制的音素帧因果注意力掩码，以支持多步并行加速。在中文（CMLR）和英文（HDTF）数据集上进行的广泛实验表明，AvatarSync在视觉保真度、时间一致性和计算效率方面超越了现有的谈话头动画方法，提供了一种可扩展且可控的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12052v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于语音输入的谈话头动画技术。采用生成对抗网络（GANs）的扩散模型因强大的生成能力而成为主流。然而，扩散过程固有的局限性导致帧间闪烁和推理速度慢，限制了实际应用。为解决这一问题，提出AvatarSync，一个基于语音表征的自回归框架，能从单一参考图像生成真实可控的谈话头动画，由文本或音频输入驱动。通过自回归管道减轻闪烁并确保连续性。引入音素（语音基本单位）构建文本&#x2F;音频到音素的多个一对一映射，实现精确的音素-视觉对齐。采用两阶段生成策略加速推理，将语义建模与视觉动态解耦，并融入定制的音素帧因果注意力掩膜，支持多步并行加速。在中文（CMLR）和英文（HDTF）数据集上的大量实验表明，AvatarSync在视觉保真度、时间一致性和计算效率方面优于现有谈话头动画方法，提供可伸缩和可控的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>谈话头动画技术重点从音频输入生成真实面部视频。</li>
<li>扩散模型因强大的生成能力成为主流，但存在帧间闪烁和推理速度慢的问题。</li>
<li>AvatarSync是一个自回归框架，能从单一参考图像生成真实可控的谈话头动画，由文本或音频驱动。</li>
<li>通过自回归管道和音素引入减轻闪烁，确保动画连续性并精确对齐音素和视觉。</li>
<li>采用两阶段生成策略加速推理，解耦语义建模和视觉动态。</li>
<li>定制的音素帧因果注意力掩膜支持多步并行加速。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-048d641b03e67540af17e882e55f70c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757107&auth_key=1760757107-0-0-13fcc6e85a17bc9c996b638b43664510&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8077d61ade754d02cc81cb658becfc0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757115&auth_key=1760757115-0-0-ba506fe59340673688140dd9afeeeaf9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b6c5da1fb33bc9098fcecb7a7597568~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757122&auth_key=1760757122-0-0-4a07d36271061e55281d82fb714b4ae7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07e6ea6fa8caed71571d12ebb34a9ea0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757131&auth_key=1760757131-0-0-d4b9843516128c3ada80d3fd3ef6619e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Talk-Isn’t-Always-Cheap-Understanding-Failure-Modes-in-Multi-Agent-Debate"><a href="#Talk-Isn’t-Always-Cheap-Understanding-Failure-Modes-in-Multi-Agent-Debate" class="headerlink" title="Talk Isn’t Always Cheap: Understanding Failure Modes in Multi-Agent   Debate"></a>Talk Isn’t Always Cheap: Understanding Failure Modes in Multi-Agent   Debate</h2><p><strong>Authors:Andrea Wynn, Harsh Satija, Gillian Hadfield</strong></p>
<p>While multi-agent debate has been proposed as a promising strategy for improving AI reasoning ability, we find that debate can sometimes be harmful rather than helpful. Prior work has primarily focused on debates within homogeneous groups of agents, whereas we explore how diversity in model capabilities influences the dynamics and outcomes of multi-agent interactions. Through a series of experiments, we demonstrate that debate can lead to a decrease in accuracy over time - even in settings where stronger (i.e., more capable) models outnumber their weaker counterparts. Our analysis reveals that models frequently shift from correct to incorrect answers in response to peer reasoning, favoring agreement over challenging flawed reasoning. We perform additional experiments investigating various potential contributing factors to these harmful shifts - including sycophancy, social conformity, and model and task type. These results highlight important failure modes in the exchange of reasons during multi-agent debate, suggesting that naive applications of debate may cause performance degradation when agents are neither incentivised nor adequately equipped to resist persuasive but incorrect reasoning. </p>
<blockquote>
<p>虽然多智能体辩论被认为是提高人工智能推理能力的一种有前途的策略，但我们发现辩论有时可能有害而非有益。先前的工作主要集中在同质智能体群体内的辩论上，而我们探索模型能力的多样性如何影响多智能体交互的动力和结果。通过一系列实验，我们证明了辩论会导致准确性随着时间的推移而下降——即使在更强的模型（即更强大的模型）数量超过较弱对手的情况下也是如此。我们的分析表明，模型经常从正确的答案转向错误的答案来回应同伴的推理，更喜欢接受有缺陷的推理而不愿意质疑挑战它。我们进行了更多的实验来调查导致这些有害转变的各种潜在因素，包括奉承、社会一致性和模型和任务类型。这些结果突出了多智能体辩论过程中交流推理的重要失败模式，表明当智能体既没有受到激励也没有得到充分准备来抵制具有说服力的错误推理时，简单应用辩论可能导致性能下降。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05396v2">PDF</a> ICML MAS Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了多智能体辩论对AI推理能力的影响，发现辩论有时可能产生负面影响。研究突破了以往对同构智能体辩论的局限，探讨了模型能力多样性对多智能体交互动态和结果的影响。实验表明，即使在强模型数量超过弱模型的情况下，辩论也可能导致准确性随时间降低。分析显示，智能体在回应同行推理时容易改变原先的正确答案而趋向错误的答案，且偏好于接受而非挑战错误的推理。实验还探讨了包括奉承、社会从众心理以及模型和任务类型等因素对有害影响的作用。这些结果凸显了多智能体辩论过程中交换理由的重要失败模式，提示在智能体未能受到激励或充分应对说服性但错误的推理时，盲目应用辩论可能导致性能下降。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体辩论虽被视为提升AI推理能力的有效策略，但有时可能产生负面影响。</li>
<li>研究考察了模型能力多样性对多智能体辩论的影响，突破了以往研究的主要局限。</li>
<li>实验显示，即使在强模型占多数的情况下，辩论也可能导致AI的准确性下降。</li>
<li>AI在回应同行推理时容易改变答案，更倾向于接受而非挑战错误的推理。</li>
<li>实验探讨了多种潜在因素，包括奉承、社会从众心理等对辩论过程中有害变化的影响。</li>
<li>这些发现揭示了多智能体辩论中的失败模式，指出了在某些情况下辩论可能导致AI性能下降。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-320b5d6dd380dc1c6e99f8907eef008e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757139&auth_key=1760757139-0-0-35a45d659e88bcee5b6bc46b30ed7dea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e8b9cee924f777dd267ad2782456b57~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757148&auth_key=1760757148-0-0-8e9b83df848eeed6e6ae528ee2787615&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09a1289c14f0375f2eba3bf188913bd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757155&auth_key=1760757155-0-0-85bec670ad2506296f5b8c4366cfc426&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Talk-Less-Call-Right-Enhancing-Role-Play-LLM-Agents-with-Automatic-Prompt-Optimization-and-Role-Prompting"><a href="#Talk-Less-Call-Right-Enhancing-Role-Play-LLM-Agents-with-Automatic-Prompt-Optimization-and-Role-Prompting" class="headerlink" title="Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic   Prompt Optimization and Role Prompting"></a>Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic   Prompt Optimization and Role Prompting</h2><p><strong>Authors:Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul</strong></p>
<p>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques-character-card&#x2F;scene-contract design and strict enforcement of function calling-which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool Source code is available at <a target="_blank" rel="noopener" href="https://github.com/scb-10x/apo">https://github.com/scb-10x/apo</a> </p>
<blockquote>
<p>本报告旨在探讨在Commonsense Persona-grounded Dialogue Challenge（CPDC 2025）的API赛道中，如何提示工具增强的大型语言模型（LLM）扮演角色扮演对话代理的方法。在此场景中，对话代理往往会生成过长的角色内响应（说话过多），同时未能根据角色有效地使用工具（表现不足），例如生成不存在的函数调用或在回答问题之前进行不必要的工具调用。我们探索了四种提示方法来解决这些问题：1）基本角色提示，2）改进的角色提示，3）自动提示优化（APO），以及4）基于规则的角色提示。基于规则的角色提示（RRP）方法表现最佳，它通过两种新技术——角色卡&#x2F;场景合约设计和严格的功能调用执行，获得了0.571的总体得分，相较于零基准线得分0.519有所提高。这些发现表明，与更精细的方法（如APO）相比，RRP设计可以显著提高角色扮演对话代理的有效性和可靠性。为了支持未来在开发个性化提示方面的努力，我们公开了所有表现最佳的提示和APO工具源代码，可在<a target="_blank" rel="noopener" href="https://github.com/scb-10x/apo%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/scb-10x/apo查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00482v2">PDF</a> EMNLP 2025 Wordplay Workshop (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>本报告研究了如何引导工具增强的大型语言模型（LLM）在Commonsense Persona-grounded Dialogue Challenge（CPDC）2025的API赛道中扮演角色对话代理的方法。针对对话代理在角色扮演过程中产生的对话过长、工具使用不当等问题，探索了四种提示方法。其中，基于规则的角色提示（RRP）方法表现最佳，通过角色卡&#x2F;场景合约设计和功能调用的严格实施，实现了整体得分0.571，相较于零基线提升了性能。研究表明，RRP设计能显著提升角色扮演对话代理的有效性和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>报告研究了在CPDC 2025的API赛道中引导LLM扮演角色对话代理的方法。</li>
<li>对话代理存在对话过长和工具使用不当的问题。</li>
<li>报告探索了四种解决上述问题的方法，包括基本角色提示、改进角色提示、自动提示优化和基于规则的角色提示。</li>
<li>基于规则的角色提示（RRP）方法表现最佳，通过角色卡&#x2F;场景合约设计和功能调用的严格实施，实现了整体得分为0.571。</li>
<li>RRP设计能显著提升角色扮演对话代理的有效性和可靠性。</li>
<li>报告公开了最佳性能的角色提示和自动提示优化工具源代码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3a31e7ef1228f3a99283e727761c7e39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757184&auth_key=1760757184-0-0-f0944a57604dc90980570808592f0326&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0daee9613e67a7f32c30092e7ac05f05~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757191&auth_key=1760757191-0-0-ec553a9c97e0bb3b09a988582213dd32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a21dcf6265072debfdd7cac965cbe424~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757198&auth_key=1760757198-0-0-0c9ad750ce89d26d4f822ccb65068d08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e7c8ee66492659cbc6434742a1c3fbd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757204&auth_key=1760757204-0-0-b55e44166cd19e5ef77ffe5e2b701f18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-146fcba28a3931abffa924c240c851c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757210&auth_key=1760757210-0-0-298af53f4da08431bdae08777f333870&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AsynFusion-Towards-Asynchronous-Latent-Consistency-Models-for-Decoupled-Whole-Body-Audio-Driven-Avatars"><a href="#AsynFusion-Towards-Asynchronous-Latent-Consistency-Models-for-Decoupled-Whole-Body-Audio-Driven-Avatars" class="headerlink" title="AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled   Whole-Body Audio-Driven Avatars"></a>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled   Whole-Body Audio-Driven Avatars</h2><p><strong>Authors:Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, Zhaoxin Fan, Wenjun Wu, Xuelong Li</strong></p>
<p>Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>全身音频驱动的角色姿态和表情生成对于创建逼真的数字人类并增强交互式虚拟代理的能力是一项至关重要的任务，在虚拟现实、数字娱乐和远程通信等领域具有广泛的应用。现有方法通常独立生成音频驱动的面部表情和动作，这引入了一个显著的局限性：面部表情和动作元素之间缺乏无缝协调，导致动画效果不那么自然和连贯。为了解决这一局限性，我们提出了AsynFusion，这是一个利用扩散变压器实现和谐表情和动作合成的新型框架。该方法建立在双分支DiT架构之上，实现了面部表情和动作的并行生成。在该模型中，我们引入了一个协作同步模块，以促进两种模式之间的双向特征交互，以及一种异步LCM采样策略，以减少计算开销的同时保持高质量输出。大量实验表明，AsynFusion在生成实时同步的全身动画方面达到了最新技术水平，在定量和定性评估中均超越了现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15058v2">PDF</a> 15pages, conference</p>
<p><strong>Summary</strong></p>
<p>本文介绍了全音频驱动的角色姿态与表情生成技术的重要性及其在虚拟现实、数字娱乐和远程通信等领域的应用。现有方法通常独立生成音频驱动的面部表情和动作，导致面部表情和动作之间缺乏无缝协调，动画效果不自然且连贯性不足。为解决这一问题，本文提出了AsynFusion框架，利用扩散变压器实现和谐的表情和动作合成。该方法基于双分支DiT架构构建，能够实现面部表情和动作的并行生成。此外，还引入了一种协同同步模块和一种异步LCM采样策略来促进面部和动作之间的双向特征交互并维持高质量输出同时降低计算开销。实验证明，AsynFusion在生成实时同步全身动画方面达到最佳性能。其在定量和定性评估中都超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>音频驱动的角色姿态与表情生成在虚拟现实、数字娱乐等领域具有广泛应用价值。</li>
<li>现有方法存在面部表情与动作不协调的问题，导致动画不自然且连贯性不足。</li>
<li>AsynFusion框架使用扩散变压器实现和谐的表情和动作合成，通过双分支DiT架构并行生成面部表情和动作。</li>
<li>引入协同同步模块促进面部和动作之间的双向特征交互。</li>
<li>提出一种异步LCM采样策略，以减少计算开销并维持高质量输出。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15058">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2e2254775b108fd18a8fa99ca94902de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757217&auth_key=1760757217-0-0-dac7e63563b34e2624f4090f2eda2c5d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2e0076f7d6e605ab591fe5ab1206a080~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757224&auth_key=1760757224-0-0-0fe404a1463e57c045678a027d2ba845&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0907e1cf507e65acde16d35da6c6fd52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757231&auth_key=1760757231-0-0-1be6041dad3294d35308a1098df8dd77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GestureCoach-Rehearsing-for-Engaging-Talks-with-LLM-Driven-Gesture-Recommendations"><a href="#GestureCoach-Rehearsing-for-Engaging-Talks-with-LLM-Driven-Gesture-Recommendations" class="headerlink" title="GestureCoach: Rehearsing for Engaging Talks with LLM-Driven Gesture   Recommendations"></a>GestureCoach: Rehearsing for Engaging Talks with LLM-Driven Gesture   Recommendations</h2><p><strong>Authors:Ashwin Ram, Varsha Suresh, Artin Saberpour Abadian, Vera Demberg, Jürgen Steimle</strong></p>
<p>This paper introduces GestureCoach, a system designed to help speakers deliver more engaging talks by guiding them to gesture effectively during rehearsal. GestureCoach combines an LLM-driven gesture recommendation model with a rehearsal interface that proactively cues speakers to gesture appropriately. Trained on experts’ gesturing patterns from TED talks, the model consists of two modules: an emphasis proposal module, which predicts when to gesture by identifying gesture-worthy text segments in the presenter notes, and a gesture identification module, which determines what gesture to use by retrieving semantically appropriate gestures from a curated gesture database. Results of a model performance evaluation and user study (N&#x3D;30) show that the emphasis proposal module outperforms off-the-shelf LLMs in identifying suitable gesture regions, and that participants rated the majority of these predicted regions and their corresponding gestures as highly appropriate. A subsequent user study (N&#x3D;10) showed that rehearsing with GestureCoach encouraged speakers to gesture and significantly increased gesture diversity, resulting in more engaging talks. We conclude with design implications for future AI-driven rehearsal systems. </p>
<blockquote>
<p>本文介绍了GestureCoach系统，该系统旨在通过指导演讲者在排练过程中进行有效的手势辅助，帮助演讲者进行更有吸引力的演讲。GestureCoach结合了一个由大型语言模型驱动的手势推荐模型和一个排练界面，该界面会主动提示演讲者进行适当的手势。该模型以TED演讲中专家手势模式为训练数据，包含两个模块：重点提案模块，通过识别演讲笔记中值得做手势的文本片段来预测何时进行手势；手势识别模块，通过从精选的手势数据库中检索语义适当的手势来确定应使用何种手势。模型性能评估和用户研究（N&#x3D;30）的结果显示，重点提案模块在识别合适的手势区域方面优于现有的大型语言模型，并且大多数参与者认为这些预测区域及其对应的手势高度合适。随后的用户研究（N&#x3D;10）表明，使用GestureCoach进行排练鼓励了演讲者进行手势，并显著增加了手势的多样性，从而产生了更具吸引力的演讲。最后，我们总结了对未来AI驱动排练系统的设计启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10706v2">PDF</a> Accepted at UIST 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了GestureCoach系统，该系统通过引导演讲者在排练过程中进行有效的手势，帮助演讲者进行更具吸引力的演讲。GestureCoach结合了LLM驱动的手势推荐模型与排练界面，提前提示演讲者进行适当的手势。该模型基于TED演讲专家的手势模式训练，包含两个模块：重点提示模块，通过识别值得手势的文本片段来预测何时进行手势；手势识别模块，通过从精选的手势数据库中检索语义适当的手势来确定应使用何种手势。模型性能评估和一项有30名参与者的用户研究表明，重点提示模块在识别合适的手势区域方面表现优于现成的LLM，并且大多数参与者认为这些预测区域及其对应的手势非常合适。另一项有10名参与者的研究表明，使用GestureCoach排练鼓励演讲者进行手势，并显著增加了手势的多样性，从而使演讲更加引人入胜。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GestureCoach系统旨在通过引导演讲者进行有效手势来提升演讲吸引力。</li>
<li>系统结合LLM驱动的手势推荐模型和排练界面，提前提示演讲者进行手势。</li>
<li>手势推荐模型包含重点提示模块和手势识别模块。</li>
<li>重点提示模块能识别值得手势的文本片段，预测最佳手势时机。</li>
<li>手势识别模块从精选数据库中检索语义适当的手势。</li>
<li>用户研究表明，该系统能有效提高演讲者的手势使用频率和多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10706">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d9aa628f6e95794b0c22ac87b6368e66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757238&auth_key=1760757238-0-0-4e3af604f71085cd05a08279cf39baaf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7c42a9ea9082ca194d6661c6ca46bca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757245&auth_key=1760757245-0-0-091dce4e1b251bce56364ae64bf1af47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95d0f4422d4e1d4f7d197aaa1d68e145~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757252&auth_key=1760757252-0-0-3302cd82d4653ecb04cd31e2c08a054a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d8871ef7e082caf7f2588333acdaa7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757258&auth_key=1760757258-0-0-eee8bde9f05de0b12ada45e4a5f46b0f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PASE-Phoneme-Aware-Speech-Encoder-to-Improve-Lip-Sync-Accuracy-for-Talking-Head-Synthesis"><a href="#PASE-Phoneme-Aware-Speech-Encoder-to-Improve-Lip-Sync-Accuracy-for-Talking-Head-Synthesis" class="headerlink" title="PASE: Phoneme-Aware Speech Encoder to Improve Lip Sync Accuracy for   Talking Head Synthesis"></a>PASE: Phoneme-Aware Speech Encoder to Improve Lip Sync Accuracy for   Talking Head Synthesis</h2><p><strong>Authors:Yihuan Huang, Jiajun Liu, Yanzhen Ren, Jun Xue, Wuyang Liu, Zongkun Sun</strong></p>
<p>Recent talking head synthesis works typically adopt speech features extracted from large-scale pre-trained acoustic models. However, the intrinsic many-to-many relationship between speech and lip motion causes phoneme-viseme alignment ambiguity, leading to inaccurate and unstable lips. To further improve lip sync accuracy, we propose PASE (Phoneme-Aware Speech Encoder), a novel speech representation model that bridges the gap between phonemes and visemes. PASE explicitly introduces phoneme embeddings as alignment anchors and employs a contrastive alignment module to enhance the discriminability between corresponding audio-visual pairs. In addition, a prediction and reconstruction task is designed to improve robustness under noise and partial modality absence. Experimental results show PASE significantly improves lip sync accuracy and achieves state-of-the-art performance across both NeRF- and 3DGS-based rendering frameworks, outperforming conventional methods based on acoustic features by 13.7 % and 14.2 %, respectively. Importantly, PASE can be seamlessly integrated into diverse talking head pipelines to improve the lip sync accuracy without architectural modifications. </p>
<blockquote>
<p>最新的说话人头部合成工作通常采用从大规模预训练声学模型中提取的语音特征。然而，语音和唇部运动之间固有的多对多关系导致了音素-动素对齐的模糊性，从而导致唇部不准确且不稳定。为了进一步提高唇部同步精度，我们提出了PASE（音素感知语音编码器），这是一种新型的语音表示模型，能够弥合音素和动素之间的鸿沟。PASE显式引入音素嵌入作为对齐锚点，并采用对比对齐模块增强相应视听对的辨别力。此外，还设计了预测和重建任务，以提高噪声和局部模态缺失情况下的稳健性。实验结果表明，PASE显著提高了唇部同步精度，在基于NeRF和3DGS的渲染框架下均达到了最先进的性能水平，基于声学特征的传统方法分别提高了13.7%和14.2%。重要的是，PASE可以无缝集成到多样化的说话人头部管道中，提高唇部同步精度而无需进行架构修改。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05803v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近期说话人头部合成研究通常采用从大规模预训练声音模型中提取的语音特征。然而，语音和唇部运动之间的内在多对多关系导致了音素-面部动素对齐的模糊性，从而导致唇部表现不准确且不稳定。为了进一步提高唇部同步精度，我们提出了PASE（音素感知语音编码器），这是一种新的语音表示模型，能够缩小音素和面部动素之间的差距。PASE通过引入音素嵌入作为对齐锚点，并采用了对比对齐模块来增强对应音频视觉对之间的区分度。此外，还设计了一个预测和重建任务，以提高在噪声和部分模态缺失下的稳健性。实验结果表明，PASE显著提高了唇部同步精度，并在基于NeRF和3DGS的渲染框架上实现了卓越性能，相较于基于声音特征的传统方法分别提高了13.7%和14.2%。重要的是，PASE可以无缝集成到各种说话人头部管道中，以提高唇部同步精度而无需进行架构修改。</p>
<p><strong>要点</strong></p>
<ol>
<li>说话人头部合成面临唇部同步的挑战，因为语音和唇部运动之间存在多对多的关系。</li>
<li>PASE模型通过引入音素感知机制来缩小音素和面部动素之间的差距。</li>
<li>PASE采用对比对齐模块增强音频视觉对的区分度。</li>
<li>PASE设计预测和重建任务以提高在噪声和部分模态缺失下的稳健性。</li>
<li>实验结果显示PASE显著提高唇部同步精度，并在不同渲染框架上表现优越。</li>
<li>PASE可在各种说话人头部管道中集成，提高唇部同步精度而无需改变架构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-c9e99c24acfcfd27d5a027db1761cbf7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757266&auth_key=1760757266-0-0-7fe61c23a972fc4821d5223cb1e31705&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4011cd7c5d0b0093cc3049e898cb7e52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757273&auth_key=1760757273-0-0-21bf4e4a24fd8998711404cf98f62fa2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee1750007411d3b30109bebebec20175~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757280&auth_key=1760757280-0-0-e1a98da983bacbe4f674daf71504c5e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-677514280c46ded2ea7b30c48dcbe8c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757288&auth_key=1760757288-0-0-1c5ca44058eebf5512566cee2f8de699&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b58549a1560c8461dde18a2a53c12f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757294&auth_key=1760757294-0-0-9d6d03be06e88e97e3bfa71b26864180&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca0f128bf6c3a05b2cf67fd80597d3f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757301&auth_key=1760757301-0-0-3dd0f87e5f61d8d61ef2170134090f33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0f246799c0aca76dc1bb2e15d22fcac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757308&auth_key=1760757308-0-0-f74e7462be80e31db04671eddae05462&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Tokenizing-Motion-A-Generative-Approach-for-Scene-Dynamics-Compression"><a href="#Tokenizing-Motion-A-Generative-Approach-for-Scene-Dynamics-Compression" class="headerlink" title="Tokenizing Motion: A Generative Approach for Scene Dynamics Compression"></a>Tokenizing Motion: A Generative Approach for Scene Dynamics Compression</h2><p><strong>Authors:Shanzhi Yin, Zihan Zhang, Bolin Chen, Shiqi Wang, Yan Ye</strong></p>
<p>This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-<a target="_blank" rel="noopener" href="https://github.com/xyzysz/GNVDC">https://github.com/xyzysz/GNVDC</a>. </p>
<blockquote>
<p>本文提出了一种新型生成式视频压缩框架，该框架利用来自常见场景中的细微动态（如摇曳的花朵或水上飘动的船只）所推导出的运动模式先验，而不是依赖于视频内容先验（如人脸或人体）。这些紧凑的运动先验知识为实现超低比特率通信提供了一种新方法，同时在各种场景内容中实现了高质量重建。在编码器端，通过稠密到稀疏的转换，可以将运动先验知识简化为紧凑的表示形式。在解码器端，这些先验知识有助于使用先进的流驱动扩散模型重建场景动态。实验结果表明，该方法在率失真性能方面表现优异，并且在场景动态序列上优于当前最先进的传统视频编码增强压缩模型（ECM）。该项目页面可在 <a target="_blank" rel="noopener" href="https://github.com/xyzysz/GNVDC">https://github.com/xyzysz/GNVDC</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09768v2">PDF</a> 5page, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的视频压缩框架，它利用从常见场景中的细微动态（如摇曳的花朵或水上漂流的船只）得出的运动模式先验知识，而不是依赖于视频内容先验知识（如人脸或人体）。这种紧凑的运动先验知识为实现超低比特率通信提供了新的途径，同时在各种场景内容中实现了高质量重建。编码器端可通过密集到稀疏的转换来简化运动先验知识表示，而解码器端则利用先进的流驱动扩散模型重建场景动态。实验结果表明，该方法在率失真性能上表现优越，并在场景动态序列上超越了现有的先进视频编码器增强压缩模型（ECM）。更多详情可访问项目页面：<a target="_blank" rel="noopener" href="https://github.com/xyzysz/GNVDC%E3%80%82">https://github.com/xyzysz/GNVDC。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新型视频压缩框架利用运动模式先验知识，这是从常见场景的细微动态中得出的。</li>
<li>框架实现了超低比特率通信，同时保持高质量重建。</li>
<li>编码器端通过密集到稀疏的转换简化运动先验知识表示。</li>
<li>解码器端利用先进的流驱动扩散模型重建场景动态。</li>
<li>该方法在实验中的率失真性能表现优越。</li>
<li>该方法超越了现有的先进视频编码器增强压缩模型（ECM）在场景动态序列上的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-97ddd84ea94a971d01a6f0755b643e9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757316&auth_key=1760757316-0-0-abfbca7077af37782531b2683e3e4d8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebecf704cfdbfacf434f66076a243ba8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757323&auth_key=1760757323-0-0-d17cc28982c68e43d3f130916e84a0e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-161ebb8b113c8259c1a3214ef0083975~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757329&auth_key=1760757329-0-0-d1cade64e428758faa0b9485293e44e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-956c009ac3781aa7d603c7e240f21d50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757336&auth_key=1760757336-0-0-99ede315490a085c4112df7ea5c5d0f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9273bb693d7dc9ca2338612918f3e2a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760757343&auth_key=1760757343-0-0-9ed2b3dc09b53b2961037ab487ba66c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-63ea9726baebdbc2f571e940f0ef5894~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756965&auth_key=1760756965-0-0-1a5e6d6ffd688eeee265c5a69bf2bd01&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-10-18  Talking Points Describing and Localizing Pixels
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-ffb66aad3cc5db70ab7ab6f661397f08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760755305&auth_key=1760755305-0-0-fa8ab0c1a2863fca365769c45f3ec8f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-10-18  OmniMotion Multimodal Motion Generation with Continuous Masked   Autoregression
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
