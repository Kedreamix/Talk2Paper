<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Talking Points Describing and Localizing Pixels">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-63ea9726baebdbc2f571e940f0ef5894')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Talking-Points-Describing-and-Localizing-Pixels"><a href="#Talking-Points-Describing-and-Localizing-Pixels" class="headerlink" title="Talking Points: Describing and Localizing Pixels"></a>Talking Points: Describing and Localizing Pixels</h2><p><strong>Authors:Matan Rusanovsky, Shimon Malnick, Shai Avidan</strong></p>
<p>Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points">https://github.com/matanr/Talking_Points</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶å±€é™äºå¯¹è±¡çº§åˆ«æˆ–åŒºåŸŸçº§åˆ«çš„å®šä½ï¼Œç¼ºä¹é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œåƒç´ ç²¾ç¡®å…³é”®ç‚¹ç†è§£çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„åƒç´ çº§åˆ«å®šä½æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªäº’è¡¥çš„ç»„ä»¶æ„æˆï¼šä¸€ä¸ªç‚¹æè¿°ç¬¦ï¼Œç”¨äºç”Ÿæˆå•ä¸ªå…³é”®ç‚¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡æè¿°ï¼›ä¸€ä¸ªç‚¹å®šä½å™¨ï¼Œç”¨äºä»è¿™äº›æè¿°ä¸­å›å½’ç²¾ç¡®çš„åƒç´ åæ ‡ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œä¾èµ–äºæ¨¡æ¿æç¤ºæˆ–å…³é”®ç‚¹åç§°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆè‡ªç”±å½¢å¼çš„ã€ä»ç²—åˆ°ç»†çš„æè¿°ï¼Œå°†å…³é”®ç‚¹ç½®äºå…¶è§†è§‰ä¸Šä¸‹æ–‡ä¸­ã€‚ç”±äºæ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†æ¥è®­ç»ƒè¿™æ ·çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†LlamaPointInPartæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒåˆ¶ä½œçš„ç”±2ä¸‡å¤šä¸ªå›¾åƒ-å…³é”®ç‚¹-æè¿°ä¸‰å…ƒç»„åˆæˆçš„æ•°æ®é›†ï¼Œä»åœºæ™¯çº§åˆ«çš„ä¸Šä¸‹æ–‡åˆ°å…³é”®ç‚¹å‘¨å›´çš„è§†è§‰ç‰¹å¾ï¼Œæ•æ‰å¤šå°ºåº¦ä¿¡æ¯ã€‚ä¸ºäº†å®ç°è·¨ç±»åˆ«çš„æ³›åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨GRPOä¼˜åŒ–ç‚¹æè¿°ç¬¦åœ¨AP-10Kä¸Šçš„æ€§èƒ½ï¼Œä½¿ç”¨å†»ç»“çš„ç‚¹å®šä½å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹æ¥ç”Ÿæˆæœ€å¤§åŒ–å®šä½ç²¾åº¦çš„æè¿°ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„ç»“æœï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åè®®ã€‚ä¸åŒäºå°†æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„æ–‡æœ¬æè¿°ä¸çœŸå®å€¼è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨å®šä½å™¨æ¥ç¡®å®šé¢„æµ‹ç‚¹ä¸çœŸå®ç‚¹ä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦ã€‚åœ¨LlamaPointInPartæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¡†æ¶çš„åŒå‘æ€§è´¨æœ‰æœ›åœ¨æœªæ¥å®ç°å…³é”®ç‚¹çš„å›¾åƒç†è§£å’Œè¯­è¨€æŒ‡å¯¼çš„ç²¾ç¡®å®šä½çš„åº”ç”¨ç¨‹åºä¸­å¾—åˆ°åº”ç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/matanr/Talking_Points%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/matanr/Talking_Pointså…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„åƒç´ çº§å®šä½æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªäº’è¡¥ç»„ä»¶ï¼šç‚¹æè¿°ç¬¦å’Œç‚¹å®šä½å™¨ã€‚ç‚¹æè¿°ç¬¦ç”Ÿæˆä¸ªä½“å…³é”®ç‚¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡æè¿°ï¼Œè€Œç‚¹å®šä½å™¨åˆ™æ ¹æ®è¿™äº›æè¿°å›å½’ç²¾ç¡®åƒç´ åæ ‡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿè‡ªç”±å½¢å¼çš„ã€ä»ç²—åˆ°ç»†çš„æè¿°ï¼Œå°†å…³é”®ç‚¹ç½®äºå…¶è§†è§‰èƒŒæ™¯ä¸­ï¼Œä¸åŒäºä¾èµ–æ¨¡æ¿æç¤ºæˆ–å…³é”®ç‚¹åç§°çš„å…ˆå‰æ–¹æ³•ã€‚ç”±äºæ— æ³•è®­ç»ƒæ­¤ç±»ç³»ç»Ÿç”¨çš„æ•°æ®é›†ï¼Œå› æ­¤å¼•å…¥äº†LlamaPointInPartæ•°æ®é›†ï¼ŒåŒ…å«2ä¸‡å¤šä¸ªå›¾åƒ-å…³é”®ç‚¹-æè¿°ä¸‰å…ƒç»„ï¼Œç”±å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹åˆæˆï¼Œæ•æ‰ä»åœºæ™¯çº§ä¸Šä¸‹æ–‡åˆ°å…³é”®ç‚¹å‘¨å›´è§†è§‰ç‰¹å¾çš„å¤šå°ºåº¦ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LlamaPointInPartä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨æ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä»å±€é™äºå¯¹è±¡çº§æˆ–åŒºåŸŸçº§çš„å®šä½ï¼Œç¼ºä¹é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œåƒç´ ç²¾ç¡®çš„å…³é”®ç‚¹ç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„åƒç´ çº§å®šä½æ¡†æ¶ï¼ŒåŒ…å«ç‚¹æè¿°ç¬¦å’Œç‚¹å®šä½å™¨ä¸¤ä¸ªäº’è¡¥ç»„ä»¶ã€‚</li>
<li>ç‚¹æè¿°ç¬¦èƒ½å¤Ÿç”Ÿæˆå…³é”®ç‚¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡æè¿°ï¼Œè€Œç‚¹å®šä½å™¨å¯ä»¥æ ¹æ®è¿™äº›æè¿°å›å½’ç²¾ç¡®åƒç´ åæ ‡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿè‡ªç”±å½¢å¼çš„ã€ç½®äºå…¶è§†è§‰èƒŒæ™¯ä¸­çš„æè¿°ï¼Œä¸åŒäºä¾èµ–æ¨¡æ¿æç¤ºæˆ–å…³é”®ç‚¹åç§°çš„å…ˆå‰æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†LlamaPointInPartæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ­¤ç±»ç³»ç»Ÿï¼ŒåŒ…å«2ä¸‡å¤šä¸ªå›¾åƒ-å…³é”®ç‚¹-æè¿°ä¸‰å…ƒç»„ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ç‚¹æè¿°ç¬¦åœ¨AP-10Kä¸Šçš„æ€§èƒ½ï¼Œå®ç°äº†è·¨ç±»åˆ«çš„æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-652edad153a670e8ef96c41230c0dec3" align="middle">
<img src="https://picx.zhimg.com/v2-4cf2fa892aa95fc6f20cac393650257f" align="middle">
<img src="https://picx.zhimg.com/v2-40a8a230e32586222cc7cab5d40aa88b" align="middle">
<img src="https://picx.zhimg.com/v2-a061fcf875e5f5c403e7da25446dd445" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Do-Slides-Help-Multi-modal-Context-for-Automatic-Transcription-of-Conference-Talks"><a href="#Do-Slides-Help-Multi-modal-Context-for-Automatic-Transcription-of-Conference-Talks" class="headerlink" title="Do Slides Help? Multi-modal Context for Automatic Transcription of   Conference Talks"></a>Do Slides Help? Multi-modal Context for Automatic Transcription of   Conference Talks</h2><p><strong>Authors:Supriti Sinhamahapatra, Jan Niehues</strong></p>
<p>State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily rely on acoustic information while disregarding additional multi-modal context. However, visual information are essential in disambiguation and adaptation. While most work focus on speaker images to handle noise conditions, this work also focuses on integrating presentation slides for the use cases of scientific presentation.   In a first step, we create a benchmark for multi-modal presentation including an automatic analysis of transcribing domain-specific terminology. Next, we explore methods for augmenting speech models with multi-modal information. We mitigate the lack of datasets with accompanying slides by a suitable approach of data augmentation. Finally, we train a model using the augmented dataset, resulting in a relative reduction in word error rate of approximately 34%, across all words and 35%, for domain-specific terms compared to the baseline model. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸»è¦ä¾èµ–äºéŸ³é¢‘ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†é¢å¤–çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œè§†è§‰ä¿¡æ¯åœ¨è§£æ­§å’Œé€‚åº”æ–¹é¢è‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å¤šæ•°å·¥ä½œéƒ½ä¸“æ³¨äºå¤„ç†å™ªéŸ³æ¡ä»¶ä¸‹çš„æ¼”è®²è€…å›¾åƒï¼Œä½†è¿™é¡¹å·¥ä½œè¿˜ä¸“æ³¨äºå°†æ¼”ç¤ºå¹»ç¯ç‰‡é›†æˆåˆ°ç§‘å­¦æ¼”ç¤ºçš„åº”ç”¨åœºæ™¯ä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºå¤šæ¨¡æ€æ¼”ç¤ºåˆ›å»ºä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åˆ†æè½¬å½•ä¸“ä¸šæœ¯è¯­çš„é¢†åŸŸã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢ç”¨å¤šæ¨¡æ€ä¿¡æ¯å¢å¼ºè¯­éŸ³æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡é€‚å½“çš„æ•°æ®å¢å¼ºæ–¹æ³•ç¼“è§£äº†ç¼ºä¹é™„å¸¦å¹»ç¯ç‰‡çš„æ•°æ®é›†é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨æ‰€æœ‰å•è¯ä¸Šçš„è¯é”™è¯¯ç‡å¤§çº¦é™ä½äº†34%ï¼Œåœ¨ä¸“ä¸šæœ¯è¯­ä¸Šçš„è¯é”™è¯¯ç‡é™ä½äº†35%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13979v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä¸»è¦ç ”ç©¶å°†å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ¼”è®²è€…çš„å›¾åƒå’Œæ¼”ç¤ºæ–‡ç¨¿ï¼‰èå…¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æ–¹æ³•ã€‚é€šè¿‡å»ºç«‹å¤šæ¨¡æ€æ¼”ç¤ºçš„åŸºå‡†æµ‹è¯•ï¼Œæ¢ç´¢äº†å¢å¼ºè¯­éŸ³æ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯è§£å†³äº†ç¼ºä¹é™„å¸¦å¹»ç¯ç‰‡çš„æ•°æ®é›†é—®é¢˜ã€‚æœ€ç»ˆï¼Œä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼Œåœ¨å•è¯é”™è¯¯ç‡ä¸Šé™ä½äº†çº¦34%ï¼ˆé’ˆå¯¹æ‰€æœ‰å•è¯ï¼‰å’Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­é”™è¯¯ç‡é™ä½äº†çº¦35%ã€‚æé«˜äº†ç³»ç»Ÿçš„æ¸…æ™°åº¦å’Œå‡†ç¡®æ€§ã€‚ç®€åŒ–åçš„å†…å®¹ä¾¿äºå®é™…åº”ç”¨ä¸æ‹“å±•ç ”ç©¶ã€‚å¯¹äºåŸºäºå£°å­¦ä¿¡æ¯çš„ç°æœ‰æŠ€æœ¯æ˜¯ä¸€ä¸ªæœ‰ç›Šçš„è¡¥å……ä¸æ‹“å±•ã€‚æ­¤ä¸ºè¯­éŸ³è¯†åˆ«é¢†åŸŸçš„è¿›æ­¥æä¾›äº†å‚è€ƒèŒƒä¾‹ã€‚æœ‰åŠ©äºç†è§£è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯æé«˜æ€§èƒ½ã€‚åœ¨æ¼”è®²å’Œæ¼”ç¤ºæ–‡ç¨¿è¯†åˆ«æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ä»…è§£å†³äº†ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­è¯†åˆ«é—®é¢˜ï¼Œä¹Ÿä¸ºæœªæ¥çš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯æä¾›äº†é‡è¦æ€è·¯ã€‚ä¸ºæœªæ¥çš„ASRç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–¹å‘æ€§çš„å¯ç¤ºã€‚æœ‰åŠ©äºè§£å†³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æ•°æ®çš„æ‰©å……æŠ€æœ¯ä¸ºç°æœ‰çš„è¯­éŸ³è¯†åˆ«æ•°æ®é›†æ³¨å…¥æ›´å¤šæ ·åŒ–çš„å†…å®¹æä¾›äº†å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚æ€»ç»“äº†å·¥ä½œæå‡ºçš„å…·ä½“è§£å†³æ–¹æ¡ˆå’Œå¯¹è¡Œä¸šçš„è´¡çŒ®ä¸æ„ä¹‰ã€‚<strong>Key Takeaways</strong>ï¼š </p>
<ul>
<li>å¤šæ¨¡æ€ä¿¡æ¯èå…¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæé«˜æ€§èƒ½ç ”ç©¶å¯¹å£°å­¦ä¿¡æ¯çš„å±€é™æå‡ºæ–°çš„æ€è·¯æ–¹å‘ </li>
<li>åˆ›å»ºäº†é’ˆå¯¹è¯­éŸ³ä¸å¹»ç¯ç‰‡å±•ç¤ºç»“åˆçš„å±•ç¤ºç³»ç»Ÿå¤„ç†ç ”ç©¶çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡è¯¥æµ‹è¯•èƒ½å¤Ÿè¿›ä¸€æ­¥æ‰©å±•å…¶åº”ç”¨ä¸ç ”ç©¶ </li>
<li>é‡‡ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯è§£å†³äº†ç¼ºä¹å¹»ç¯ç‰‡é…å¥—æ•°æ®é›†çš„é—®é¢˜ </li>
<li>è®­ç»ƒçš„æ–°æ¨¡å‹ç›¸æ¯”åŸºå‡†æ¨¡å‹é™ä½äº†å¤§çº¦ç™¾åˆ†ä¹‹ä¸‰åå››çš„å•è¯é”™è¯¯ç‡å’Œç™¾åˆ†ä¹‹ä¸‰åäº”çš„ä¸“ä¸šæœ¯è¯­é”™è¯¯ç‡ </li>
<li>æ­¤ç ”ç©¶åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå±•ç°äº†æå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³ç‰¹å®šæœ¯è¯­è¯†åˆ«é—®é¢˜æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ </li>
<li>è¯¥ç ”ç©¶å¯¹äºæœªæ¥è§£å†³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æŒ‘æˆ˜æä¾›äº†é‡è¦å¯ç¤º</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd9852fd1a17e25239bd60b64e3e7ed8" align="middle">
<img src="https://picx.zhimg.com/v2-f7716e0cbf090c2eba335267e51b5e8a" align="middle">
<img src="https://picx.zhimg.com/v2-9b7a5875ac064a853425e8897bf1dcdf" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Playmate2-Training-Free-Multi-Character-Audio-Driven-Animation-via-Diffusion-Transformer-with-Reward-Feedback"><a href="#Playmate2-Training-Free-Multi-Character-Audio-Driven-Animation-via-Diffusion-Transformer-with-Reward-Feedback" class="headerlink" title="Playmate2: Training-Free Multi-Character Audio-Driven Animation via   Diffusion Transformer with Reward Feedback"></a>Playmate2: Training-Free Multi-Character Audio-Driven Animation via   Diffusion Transformer with Reward Feedback</h2><p><strong>Authors:Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang</strong></p>
<p>Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†éŸ³é¢‘é©±åŠ¨çš„äººç±»è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œæ— è®ºæ˜¯åœ¨è´¨é‡è¿˜æ˜¯å¯æ§æ€§æ–¹é¢éƒ½è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´å˜´å”‡åŒæ­¥ç²¾åº¦ã€é•¿è§†é¢‘ç”Ÿæˆçš„æ—¶åºè¿è´¯æ€§ä»¥åŠå¤šè§’è‰²åŠ¨ç”»æ–¹é¢çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä»»æ„é•¿åº¦çš„é€¼çœŸè°ˆè¯è§†é¢‘ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ— è®­ç»ƒçš„å¤šè§’è‰²éŸ³é¢‘é©±åŠ¨åŠ¨ç”»æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨LoRAä¸ºåŸºç¡€çš„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆä½ç½®åç§»æ¨ç†æ–¹æ³•ï¼Œæ—¢èƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé•¿è§†é¢‘ï¼ŒåŒæ—¶åˆä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†éƒ¨åˆ†å‚æ•°æ›´æ–°ä¸å¥–åŠ±åé¦ˆç›¸ç»“åˆï¼Œä»¥æé«˜å˜´å”‡åŒæ­¥å’Œè‡ªç„¶èº«ä½“åŠ¨ä½œçš„è´¨é‡ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†æ— è®­ç»ƒçš„å¤šè§’è‰²åŠ¨ç”»æ–¹æ³•â€”â€”æ©è†œåˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ï¼ˆMask Classifier-Free Guidanceï¼ŒMask-CFGï¼‰ï¼Œå®ƒä¸éœ€è¦ç‰¹å®šçš„æ•°æ®é›†æˆ–æ¨¡å‹ä¿®æ”¹ï¼Œå¹¶æ”¯æŒä¸‰ä¸ªæˆ–æ›´å¤šè§’è‰²çš„éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä»¥ç®€å•ã€é«˜æ•ˆå’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ–¹å¼å®ç°äº†é«˜è´¨é‡ã€æ—¶åºè¿è´¯å’Œå¤šè§’è‰²çš„éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12089v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨éŸ³é¢‘é©±åŠ¨çš„äººè„¸è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä»»æ„é•¿åº¦çš„é€¼çœŸå¯¹è¯è§†é¢‘ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒçš„å¤šè§’è‰²éŸ³é¢‘é©±åŠ¨åŠ¨ç”»æ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨LoRAè®­ç»ƒç­–ç•¥å’Œä½ç½®åç§»æ¨æ–­æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆçš„é•¿è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡éƒ¨åˆ†å‚æ•°æ›´æ–°å’Œå¥–åŠ±åé¦ˆï¼Œæé«˜äº†å”‡åŒæ­¥å’Œè‡ªç„¶åŠ¨ä½œçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„å¤šè§’è‰²åŠ¨ç”»æ–¹æ³•â€”â€”Mask Classifier-Free Guidanceï¼ˆMask-CFGï¼‰ï¼Œæ— éœ€ç‰¹å®šæ•°æ®é›†æˆ–æ¨¡å‹ä¿®æ”¹ï¼Œæ”¯æŒä¸‰ä¸ªæˆ–æ›´å¤šè§’è‰²çš„éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå®ç°äº†é«˜è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œå¤šè§’è‰²éŸ³é¢‘é©±åŠ¨è§†é¢‘ç”Ÿæˆï¼Œç®€å•ã€é«˜æ•ˆä¸”ç»æµå®æƒ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ˜¾è‘—æ”¹è¿›äº†éŸ³é¢‘é©±åŠ¨çš„äººè„¸è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œåœ¨è´¨é‡å’Œå¯æ§æ€§æ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æå‡ºäº†åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä»»æ„é•¿åº¦çš„é€¼çœŸå¯¹è¯è§†é¢‘ã€‚</li>
<li>é‡‡ç”¨LoRAè®­ç»ƒç­–ç•¥å’Œä½ç½®åç§»æ¨æ–­æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆé•¿è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡éƒ¨åˆ†å‚æ•°æ›´æ–°å’Œå¥–åŠ±åé¦ˆå¢å¼ºå”‡åŒæ­¥å’Œè‡ªç„¶åŠ¨ä½œè¡¨ç°ã€‚</li>
<li>å¼•å…¥äº†æ— è®­ç»ƒçš„å¤šè§’è‰²éŸ³é¢‘é©±åŠ¨åŠ¨ç”»æ–¹æ³•â€”â€”Mask Classifier-Free Guidanceï¼ˆMask-CFGï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ç‰¹å®šæ•°æ®é›†æˆ–æ¨¡å‹ä¿®æ”¹ï¼Œæ”¯æŒä¸‰ä¸ªæˆ–æ›´å¤šè§’è‰²çš„éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1d8ff5b76d36acab85e20dff023f259" align="middle">
<img src="https://picx.zhimg.com/v2-bdd6273c9dd46541ff0cb7d3fa8677da" align="middle">
<img src="https://picx.zhimg.com/v2-63ea9726baebdbc2f571e940f0ef5894" align="middle">
<img src="https://picx.zhimg.com/v2-03abe6c86c76f8c21099d78b47d86f72" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DEMO-Disentangled-Motion-Latent-Flow-Matching-for-Fine-Grained-Controllable-Talking-Portrait-Synthesis"><a href="#DEMO-Disentangled-Motion-Latent-Flow-Matching-for-Fine-Grained-Controllable-Talking-Portrait-Synthesis" class="headerlink" title="DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained   Controllable Talking Portrait Synthesis"></a>DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained   Controllable Talking Portrait Synthesis</h2><p><strong>Authors:Peiyin Chen, Zhuowei Yang, Hui Feng, Sheng Jiang, Rui Yan</strong></p>
<p>Audio-driven talking-head generation has advanced rapidly with diffusion-based generative models, yet producing temporally coherent videos with fine-grained motion control remains challenging. We propose DEMO, a flow-matching generative framework for audio-driven talking-portrait video synthesis that delivers disentangled, high-fidelity control of lip motion, head pose, and eye gaze. The core contribution is a motion auto-encoder that builds a structured latent space in which motion factors are independently represented and approximately orthogonalized. On this disentangled motion space, we apply optimal-transport-based flow matching with a transformer predictor to generate temporally smooth motion trajectories conditioned on audio. Extensive experiments across multiple benchmarks show that DEMO outperforms prior methods in video realism, lip-audio synchronization, and motion fidelity. These results demonstrate that combining fine-grained motion disentanglement with flow-based generative modeling provides a powerful new paradigm for controllable talking-head video synthesis. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨å¤´éƒ¨è¿åŠ¨è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œä½†ç”Ÿæˆæ—¶é—´è¿è´¯ä¸”å…·æœ‰ç²¾ç»†åŠ¨ä½œæ§åˆ¶çš„è§†é¢‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†DEMOï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººè§†é¢‘åˆæˆçš„æµåŒ¹é…ç”Ÿæˆæ¡†æ¶ï¼Œå®ç°äº†å¯¹å”‡éƒ¨è¿åŠ¨ã€å¤´éƒ¨å§¿åŠ¿å’Œçœ¼ç›æ³¨è§†çš„ç‹¬ç«‹ã€é«˜ä¿çœŸæ§åˆ¶ã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºè¿åŠ¨è‡ªç¼–ç å™¨ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼Œåœ¨è¯¥ç©ºé—´ä¸­ç‹¬ç«‹è¡¨ç¤ºè¿åŠ¨å› ç´ å¹¶è¿›è¡Œè¿‘ä¼¼æ­£äº¤åŒ–å¤„ç†ã€‚åœ¨è¿™ä¸ªè§£è€¦çš„è¿åŠ¨ç©ºé—´ä¸Šï¼Œæˆ‘ä»¬åº”ç”¨åŸºäºæœ€ä¼˜ä¼ è¾“çš„æµåŒ¹é…ï¼Œç»“åˆè½¬æ¢å™¨é¢„æµ‹å™¨ï¼Œæ ¹æ®éŸ³é¢‘ç”Ÿæˆæ—¶é—´å¹³æ»‘çš„è¿åŠ¨è½¨è¿¹ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDEMOåœ¨è§†é¢‘é€¼çœŸåº¦ã€å”‡éŸ³åŒæ­¥å’ŒåŠ¨ä½œä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†ç²¾ç»†åŠ¨ä½œè§£è€¦ä¸åŸºäºæµçš„ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œä¸ºå¯æ§çš„è¯´è¯äººè§†é¢‘åˆæˆæä¾›äº†å¼ºå¤§çš„æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10650v1">PDF</a> 5 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è°ˆè¯è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—è¿›å±•è¿…é€Ÿï¼Œä½†äº§ç”Ÿæ—¶é—´è¿è´¯çš„è§†é¢‘å¹¶å®ç°ç²¾ç»†åŠ¨ä½œæ§åˆ¶ä»æ˜¯æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºDEMOï¼Œä¸€ä¸ªé¢å‘éŸ³é¢‘é©±åŠ¨è°ˆè¯è‚–åƒè§†é¢‘åˆæˆçš„æµåŒ¹é…ç”Ÿæˆæ¡†æ¶ï¼Œæä¾›å”‡éƒ¨åŠ¨ä½œã€å¤´éƒ¨å§¿æ€å’Œçœ¼ç›æ³¨è§†çš„ç‹¬ç«‹æ§åˆ¶å’Œé«˜ä¿çœŸåº¦ã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºè¿åŠ¨è‡ªç¼–ç å™¨ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼Œå…¶ä¸­è¿åŠ¨å› ç´ ç‹¬ç«‹è¡¨ç¤ºå¹¶è¿‘ä¼¼æ­£äº¤åŒ–ã€‚åœ¨æ­¤åˆ†ç¦»çš„è¿åŠ¨ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæœ€ä¼˜ä¼ è¾“çš„æµåŒ¹é…æ–¹æ³•å’Œå˜æ¢é¢„æµ‹å™¨ç”Ÿæˆæ¡ä»¶éŸ³é¢‘çš„æ—¶é—´å¹³æ»‘è¿åŠ¨è½¨è¿¹ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDEMOåœ¨è§†é¢‘é€¼çœŸåº¦ã€å”‡éŸ³åŒæ­¥å’ŒåŠ¨ä½œä¿çœŸåº¦æ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜ï¼Œç»“åˆç²¾ç»†åŠ¨ä½œåˆ†ç¦»ä¸æµç”Ÿæˆå»ºæ¨¡ä¸ºå¯æ§è°ˆè¯å¤´è§†é¢‘åˆæˆæä¾›äº†å¼ºå¤§çš„æ–°èŒƒå¼ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>éŸ³é¢‘é©±åŠ¨è°ˆè¯è§†é¢‘ç”ŸæˆæŠ€æœ¯ç°çŠ¶å’ŒæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥DEMOæ¡†æ¶åŠå…¶ä¸»è¦ç»„ä»¶ï¼šè¿åŠ¨è‡ªç¼–ç å™¨ã€‚</li>
<li>è¿åŠ¨è‡ªç¼–ç å™¨æ„å»ºç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼Œå®ç°ç‹¬ç«‹åŠ¨ä½œæ§åˆ¶ã€‚</li>
<li>åŸºäºæœ€ä¼˜ä¼ è¾“çš„æµåŒ¹é…æ–¹æ³•å’Œå˜æ¢é¢„æµ‹å™¨ç”Ÿæˆæ¡ä»¶éŸ³é¢‘çš„æ—¶é—´å¹³æ»‘è¿åŠ¨è½¨è¿¹ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç»“åˆç²¾ç»†åŠ¨ä½œåˆ†ç¦»ä¸æµç”Ÿæˆå»ºæ¨¡ä¸ºå¯æ§è°ˆè¯å¤´è§†é¢‘åˆæˆæä¾›æ–°èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eecaa9b787359d287a34297d2e3d7486" align="middle">
<img src="https://picx.zhimg.com/v2-02ac1d21c28dd75d9ed679c7ad216510" align="middle">
<img src="https://picx.zhimg.com/v2-8ead1df6bddcce9497d2b1b78be90dd0" align="middle">
<img src="https://picx.zhimg.com/v2-a574afc8ba6901aa1081167e6450aabf" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SyncLipMAE-Contrastive-Masked-Pretraining-for-Audio-Visual-Talking-Face-Representation"><a href="#SyncLipMAE-Contrastive-Masked-Pretraining-for-Audio-Visual-Talking-Face-Representation" class="headerlink" title="SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face   Representation"></a>SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face   Representation</h2><p><strong>Authors:Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou</strong></p>
<p>We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head&#x2F;face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SyncLipMAEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯´è¯äººè„¸è§†é¢‘çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»æ— æ ‡ç­¾çš„è§†å¬æµä¸­å­¦ä¹ åŒæ­¥æ„ŸçŸ¥å’Œå¯è¿ç§»çš„é¢åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ©æ¨¡è§†è§‰å»ºæ¨¡å’Œè·¨æ¨¡æ€å¯¹æ¯”å¯¹é½ï¼Œå¹¶é‡‡ç”¨äº†ä¸‰ä¸ªæ¯å¸§æç¤ºä»¤ç‰Œï¼Œæ˜¾å¼ç¼–ç è¯´è¯äººè„¸å¸§çš„å…³é”®å› ç´ ï¼šèº«ä»½ã€è¯­éŸ³è¿åŠ¨ï¼ˆä¸è¯­éŸ³åŒæ­¥çš„é¢åŠ¨ï¼‰å’Œç¯å¢ƒè¿åŠ¨ï¼ˆä¸éŸ³é¢‘æ— å…³çš„åŠ¨ä½œï¼Œå¦‚çœ¨çœ¼å’Œå¤´éƒ¨å§¿åŠ¿ï¼‰ã€‚å¯¹æ¯”ç›®æ ‡ä½¿ç”¨æ—¶é—´å¯¹é½çš„è¯­éŸ³è¿åŠ¨å’Œå£°å­¦ä»¤ç‰Œä½œä¸ºæ­£æ ·æœ¬ï¼Œé”™ä½çš„é…å¯¹ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œå°†ä¸¤ç§æ¨¡æ€é©±åŠ¨åˆ°å…±äº«åµŒå…¥ç©ºé—´ï¼Œå¹¶äº§ç”Ÿä»¤ç‰Œçº§çš„è§†å¬æµåŒæ­¥ã€‚é¢„è®­ç»ƒåï¼Œå¯¹é½çš„å£°å­¦ä»¤ç‰Œä¸è§†è§‰æç¤ºä»¤ç‰Œï¼ˆèº«ä»½ã€è¯­éŸ³è¿åŠ¨ã€ç¯å¢ƒè¿åŠ¨ï¼‰å½¢æˆäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œç”¨äºå››ç§ä¸åŒçš„ä¸‹æ¸¸åœºæ™¯ï¼šï¼ˆiï¼‰è§†å¬æµåŒæ­¥ï¼›ï¼ˆiiï¼‰é¢éƒ¨æƒ…æ„Ÿè¯†åˆ«å’Œå¤´éƒ¨&#x2F;é¢éƒ¨åŠ¨ä½œè¯†åˆ«ï¼›ï¼ˆiiiï¼‰è§†è§‰è¯­éŸ³è¯†åˆ«ï¼›ï¼ˆivï¼‰è§†é¢‘é…éŸ³ï¼Œæˆ‘ä»¬åœ¨å•ä¸ªæ¨¡å‹ä¸­å®ç°äº†ä¸å¯åŒºåˆ†çš„éŸ³é¢‘æˆ–è§†é¢‘é©±åŠ¨æ§åˆ¶ã€‚åœ¨éœ€è¦ä¸åŒèƒ½åŠ›çš„å››ä¸ªä»»åŠ¡å®¶æ—ä¸­ï¼ŒSyncLipMAEå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¼ºè°ƒäº†åŒæ­¥æ„ŸçŸ¥ã€å› å­åŒ–çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10069v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SyncLipMAEæ˜¯ä¸€ä¸ªç”¨äºè¯´è¯äººè„¸è§†é¢‘çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ— æ ‡ç­¾çš„è§†å¬æµå­¦ä¹ åŒæ­¥æ„ŸçŸ¥å’Œå¯è¿ç§»çš„é¢éƒ¨åŠ¨æ€ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ©è†œè§†è§‰å»ºæ¨¡å’Œè·¨æ¨¡æ€å¯¹æ¯”å¯¹é½ï¼Œå¹¶é‡‡ç”¨ä¸‰ä¸ªæ¯å¸§æç¤ºä»¤ç‰Œæ˜¾å¼ç¼–ç è¯´è¯äººè„¸å¸§çš„å…³é”®å› ç´ ï¼šèº«ä»½ã€è¯­éŸ³åŠ¨ä½œï¼ˆä¸è¯­éŸ³åŒæ­¥çš„é¢éƒ¨åŠ¨æ€ï¼‰å’Œç¯å¢ƒåŠ¨ä½œï¼ˆä¸éŸ³é¢‘æ— å…³çš„åŠ¨ä½œï¼Œå¦‚çœ¨çœ¼å’Œå¤´éƒ¨å§¿åŠ¿ï¼‰ã€‚å¯¹æ¯”ç›®æ ‡ä½¿ç”¨æ—¶é—´å¯¹é½çš„è¯­éŸ³åŠ¨ä½œå’ŒéŸ³é¢‘ä»¤ç‰Œä½œä¸ºæ­£æ ·æœ¬ï¼Œé”™ä½çš„å¯¹ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œæ¨åŠ¨ä¸¤ç§æ¨¡å¼è¿›å…¥å…±äº«åµŒå…¥ç©ºé—´ï¼Œäº§ç”Ÿä»¤ç‰Œçº§çš„è§†å¬æµåŒæ­¥ã€‚é¢„è®­ç»ƒåï¼Œå¯¹é½çš„éŸ³é¢‘ä»¤ç‰Œä¸è§†è§‰æç¤ºä»¤ç‰Œï¼ˆèº«ä»½ã€è¯­éŸ³åŠ¨ä½œã€ç¯å¢ƒåŠ¨ä½œï¼‰å½¢æˆä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œç”¨äºå››ç§ä¸åŒçš„ä¸‹æ¸¸è®¾ç½®ï¼ŒåŒ…æ‹¬è§†å¬æµåŒæ­¥ã€é¢éƒ¨æƒ…æ„ŸåŠå¤´éƒ¨&#x2F;é¢éƒ¨åŠ¨ä½œè¯†åˆ«ã€è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè§†è§‰é…éŸ³ã€‚SyncLipMAEåœ¨å››ä¸ªéœ€è¦ä¸åŒèƒ½åŠ›çš„ä»»åŠ¡å®¶æ—ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œçªæ˜¾äº†åŒæ­¥æ„ŸçŸ¥ã€åˆ†è§£è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SyncLipMAEæ˜¯ä¸€ä¸ªè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºè¯´è¯äººè„¸è§†é¢‘è®¾è®¡ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ— æ ‡ç­¾çš„è§†å¬æµå­¦ä¹ åŒæ­¥æ„ŸçŸ¥å’Œå¯è¿ç§»çš„é¢éƒ¨åŠ¨æ€ã€‚</li>
<li>SyncLipMAEç»“åˆæ©è†œè§†è§‰å»ºæ¨¡å’Œè·¨æ¨¡æ€å¯¹æ¯”å¯¹é½ã€‚</li>
<li>ä½¿ç”¨ä¸‰ä¸ªæ¯å¸§æç¤ºä»¤ç‰Œæ¥ç¼–ç è¯´è¯äººè„¸å¸§çš„å…³é”®å› ç´ ï¼šèº«ä»½ã€è¯­éŸ³åŠ¨ä½œå’Œç¯å¢ƒåŠ¨ä½œã€‚</li>
<li>å¯¹æ¯”ç›®æ ‡ä½¿ç”¨æ—¶é—´å¯¹é½çš„è¯­éŸ³åŠ¨ä½œå’ŒéŸ³é¢‘ä»¤ç‰Œï¼Œä»¥æ¨åŠ¨ä¸¤ç§æ¨¡å¼è¿›å…¥å…±äº«åµŒå…¥ç©ºé—´ã€‚</li>
<li>é¢„è®­ç»ƒåï¼ŒSyncLipMAEå¯åœ¨å››ç§ä¸åŒçš„ä¸‹æ¸¸è®¾ç½®ä¸­ä½¿ç”¨ï¼ŒåŒ…æ‹¬è§†å¬æµåŒæ­¥ã€é¢éƒ¨æƒ…æ„ŸåŠåŠ¨ä½œè¯†åˆ«ã€è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè§†è§‰é…éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c6c231fed765423258fe234ae412690" align="middle">
<img src="https://picx.zhimg.com/v2-391de2510e38295189764810bb249260" align="middle">
<img src="https://picx.zhimg.com/v2-190877a8d3a02c18ee8f78ee7cbb4971" align="middle">
<img src="https://picx.zhimg.com/v2-e99abe911990569078aa7a1f866286f6" align="middle">
<img src="https://picx.zhimg.com/v2-a15551b727f9e713ce75cbbe3e045ac1" align="middle">
<img src="https://picx.zhimg.com/v2-6d00000819b45008474ef976b1a274a9" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-with-Efficient-Gaussian-Deformation"><a href="#EGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-with-Efficient-Gaussian-Deformation" class="headerlink" title="EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient   Gaussian Deformation"></a>EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient   Gaussian Deformation</h2><p><strong>Authors:Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</strong></p>
<p>This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalkerâ€™s potential for real-time multimedia applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†EGSTalkerï¼Œä¸€ä¸ªåŸºäº3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„å®æ—¶éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶ã€‚EGSTalkeræ—¨åœ¨æé«˜é€Ÿåº¦å’Œè§†è§‰ä¿çœŸåº¦ï¼Œä»…éœ€3-5åˆ†é’Ÿçš„è®­ç»ƒè§†é¢‘å³å¯åˆæˆé«˜è´¨é‡çš„é¢éƒ¨åŠ¨ç”»ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šé™æ€é«˜æ–¯åˆå§‹åŒ–å’ŒéŸ³é¢‘é©±åŠ¨å˜å½¢ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œä¸‰è§’å¹³é¢å’ŒKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æå–ç©ºé—´ç‰¹å¾å¹¶æ„å»ºç´§å‡‘çš„3Dé«˜æ–¯è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†æœ‰æ•ˆçš„ç©ºé—´éŸ³é¢‘æ³¨æ„åŠ›ï¼ˆESAAï¼‰æ¨¡å—ï¼Œä»¥èåˆéŸ³é¢‘å’Œç©ºé—´çº¿ç´¢ï¼Œè€ŒKANåˆ™é¢„æµ‹ç›¸åº”çš„é«˜æ–¯å˜å½¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEGSTalkerçš„æ¸²æŸ“è´¨é‡å’Œå˜´å”‡åŒæ­¥ç²¾åº¦å¯ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åª²ç¾ï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºå®ƒä»¬ã€‚è¿™äº›ç»“æœçªå‡ºäº†EGSTalkeråœ¨å®æ—¶å¤šåª’ä½“åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08587v1">PDF</a> Main paper (6 pages). Accepted for publication by IEEE International   Conference on Systems, Man, and Cybernetics 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäº3Dé«˜æ–¯å–·ç»˜æŠ€æœ¯ï¼ˆ3DGSï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†å®æ—¶éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶EGSTalkerã€‚è¯¥æ¡†æ¶æ—¨åœ¨æé«˜é€Ÿåº¦å’Œè§†è§‰ä¿çœŸåº¦ï¼Œä»…éœ€è¦3-5åˆ†é’Ÿçš„è®­ç»ƒè§†é¢‘å³å¯åˆæˆé«˜è´¨é‡é¢éƒ¨åŠ¨ç”»ã€‚å®ƒåŒ…å«ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šé™æ€é«˜æ–¯åˆå§‹åŒ–å’ŒéŸ³é¢‘é©±åŠ¨å˜å½¢ã€‚é¦–å…ˆï¼Œä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œä¸‰å¹³é¢å’ŒKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æå–ç©ºé—´ç‰¹å¾å¹¶å»ºç«‹ç´§å‡‘çš„3Dé«˜æ–¯è¡¨ç¤ºã€‚ç„¶åï¼Œæå‡ºé«˜æ•ˆç©ºé—´éŸ³é¢‘æ³¨æ„åŠ›ï¼ˆESAAï¼‰æ¨¡å—æ¥èåˆéŸ³é¢‘å’Œç©ºé—´çº¿ç´¢ï¼ŒåŒæ—¶KANé¢„æµ‹ç›¸åº”çš„é«˜æ–¯å˜å½¢ã€‚å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEGSTalkerçš„æ¸²æŸ“è´¨é‡å’Œå”‡åŒæ­¥å‡†ç¡®æ€§å¯ä¸æœ€æ–°æŠ€æœ¯ç›¸åª²ç¾ï¼Œä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºå®ƒä»¬ã€‚è¿™çªæ˜¾äº†EGSTalkeråœ¨å®æ—¶å¤šåª’ä½“åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EGSTalkeræ˜¯ä¸€ä¸ªåŸºäº3Dé«˜æ–¯å–·ç»˜æŠ€æœ¯çš„å®æ—¶éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…å«é™æ€é«˜æ–¯åˆå§‹åŒ–å’ŒéŸ³é¢‘é©±åŠ¨å˜å½¢ä¸¤ä¸ªå…³é”®é˜¶æ®µã€‚</li>
<li>EGSTalkerä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œä¸‰å¹³é¢å’ŒKolmogorov-Arnoldç½‘ç»œæ¥æå–ç©ºé—´ç‰¹å¾å¹¶å»ºç«‹3Dé«˜æ–¯è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†é«˜æ•ˆç©ºé—´éŸ³é¢‘æ³¨æ„åŠ›æ¨¡å—æ¥èåˆéŸ³é¢‘ä¸ç©ºé—´çº¿ç´¢ã€‚</li>
<li>EGSTalkerä»…éœ€3-5åˆ†é’Ÿçš„è®­ç»ƒè§†é¢‘å³å¯åˆæˆé«˜è´¨é‡é¢éƒ¨åŠ¨ç”»ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEGSTalkerçš„æ¸²æŸ“è´¨é‡å’Œå”‡åŒæ­¥å‡†ç¡®æ€§å¯ä¸æœ€æ–°æŠ€æœ¯æ–¹æ³•ç›¸åª²ç¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ddae32babae2278ef9e8ab7fadb34d82" align="middle">
<img src="https://picx.zhimg.com/v2-d9ac13bc6dcb5beee7bfbe6e5200c5dd" align="middle">
<img src="https://picx.zhimg.com/v2-c0d5f6510dff4324afb9565ee6066dd0" align="middle">
<img src="https://picx.zhimg.com/v2-2f3a8277e28e36aae03349ebdcf0c815" align="middle">
<img src="https://picx.zhimg.com/v2-e65d60aaf632f29d2df39174db6e96a3" align="middle">
<img src="https://picx.zhimg.com/v2-e45fabfd10490432cbc639b106fc5aef" align="middle">
<img src="https://picx.zhimg.com/v2-726c52a499e0fd1215830be1e4113af0" align="middle">
<img src="https://picx.zhimg.com/v2-01c4fb226b5d3fd3d8c47516e2d4b1bb" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective"><a href="#AvatarSync-Rethinking-Talking-Head-Animation-through-Phoneme-Guided-Autoregressive-Perspective" class="headerlink" title="AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective"></a>AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective</h2><p><strong>Authors:Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</strong></p>
<p>Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. In addition, to ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text&#x2F;audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized Phoneme-Frame Causal Attention Mask to support multi-step parallel acceleration. Extensive experiments conducted on both Chinese (CMLR) and English (HDTF) datasets demonstrate that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution. </p>
<blockquote>
<p>è°ˆè¯å¤´åŠ¨ç”»ä¸»è¦å…³æ³¨ä»éŸ³é¢‘è¾“å…¥ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨è§†é¢‘ã€‚ç»§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¹‹åï¼Œæ‰©æ•£æ¨¡å‹ç”±äºå…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºä¸»æµã€‚ç„¶è€Œï¼Œæ‰©æ•£è¿‡ç¨‹æœ¬èº«çš„å›ºæœ‰å±€é™æ€§å¸¸å¸¸å¯¼è‡´å¸§é—´é—ªçƒå’Œæ¨ç†ç¼“æ…¢ï¼Œä»è€Œé™åˆ¶äº†å…¶å®é™…éƒ¨ç½²åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AvatarSyncï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºéŸ³ç´ è¡¨ç¤ºçš„è‡ªå›å½’æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•ä¸ªå‚è€ƒå›¾åƒç”Ÿæˆé€¼çœŸä¸”å¯æ§çš„è°ˆè¯å¤´åŠ¨ç”»ï¼Œç›´æ¥ç”±æ–‡æœ¬æˆ–éŸ³é¢‘è¾“å…¥é©±åŠ¨ã€‚ä¸ºäº†å‡è½»é—ªçƒå¹¶ç¡®ä¿è¿ç»­æ€§ï¼ŒAvatarSyncåˆ©ç”¨è‡ªå›å½’ç®¡é“å¢å¼ºäº†æ—¶é—´å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿å¯æ§æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†éŸ³ç´ ï¼ˆå³è¯­éŸ³å£°éŸ³çš„åŸºæœ¬å•ä½ï¼‰ï¼Œæ„å»ºäº†ä»æ–‡æœ¬&#x2F;éŸ³é¢‘åˆ°éŸ³ç´ çš„å¤šå…ƒåˆ°ä¸€å…ƒæ˜ å°„ï¼Œå®ç°äº†ç²¾ç¡®çš„éŸ³ç´ åˆ°è§†è§‰çš„å¯¹åº”ã€‚å¦å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œå°†è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨æ€è§£è€¦ï¼Œå¹¶èå…¥å®šåˆ¶çš„éŸ³ç´ å¸§å› æœæ³¨æ„åŠ›æ©ç ï¼Œä»¥æ”¯æŒå¤šæ­¥å¹¶è¡ŒåŠ é€Ÿã€‚åœ¨ä¸­æ–‡ï¼ˆCMLRï¼‰å’Œè‹±æ–‡ï¼ˆHDTFï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„è°ˆè¯å¤´åŠ¨ç”»æ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12052v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè¯­éŸ³è¾“å…¥çš„è°ˆè¯å¤´åŠ¨ç”»æŠ€æœ¯ã€‚é‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ‰©æ•£æ¨¡å‹å› å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›è€Œæˆä¸ºä¸»æµã€‚ç„¶è€Œï¼Œæ‰©æ•£è¿‡ç¨‹å›ºæœ‰çš„å±€é™æ€§å¯¼è‡´å¸§é—´é—ªçƒå’Œæ¨ç†é€Ÿåº¦æ…¢ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºAvatarSyncï¼Œä¸€ä¸ªåŸºäºè¯­éŸ³è¡¨å¾çš„è‡ªå›å½’æ¡†æ¶ï¼Œèƒ½ä»å•ä¸€å‚è€ƒå›¾åƒç”ŸæˆçœŸå®å¯æ§çš„è°ˆè¯å¤´åŠ¨ç”»ï¼Œç”±æ–‡æœ¬æˆ–éŸ³é¢‘è¾“å…¥é©±åŠ¨ã€‚é€šè¿‡è‡ªå›å½’ç®¡é“å‡è½»é—ªçƒå¹¶ç¡®ä¿è¿ç»­æ€§ã€‚å¼•å…¥éŸ³ç´ ï¼ˆè¯­éŸ³åŸºæœ¬å•ä½ï¼‰æ„å»ºæ–‡æœ¬&#x2F;éŸ³é¢‘åˆ°éŸ³ç´ çš„å¤šä¸ªä¸€å¯¹ä¸€æ˜ å°„ï¼Œå®ç°ç²¾ç¡®çš„éŸ³ç´ -è§†è§‰å¯¹é½ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥åŠ é€Ÿæ¨ç†ï¼Œå°†è¯­ä¹‰å»ºæ¨¡ä¸è§†è§‰åŠ¨æ€è§£è€¦ï¼Œå¹¶èå…¥å®šåˆ¶çš„éŸ³ç´ å¸§å› æœæ³¨æ„åŠ›æ©è†œï¼Œæ”¯æŒå¤šæ­¥å¹¶è¡ŒåŠ é€Ÿã€‚åœ¨ä¸­æ–‡ï¼ˆCMLRï¼‰å’Œè‹±æ–‡ï¼ˆHDTFï¼‰æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAvatarSyncåœ¨è§†è§‰ä¿çœŸåº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰è°ˆè¯å¤´åŠ¨ç”»æ–¹æ³•ï¼Œæä¾›å¯ä¼¸ç¼©å’Œå¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è°ˆè¯å¤´åŠ¨ç”»æŠ€æœ¯é‡ç‚¹ä»éŸ³é¢‘è¾“å…¥ç”ŸæˆçœŸå®é¢éƒ¨è§†é¢‘ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å› å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›æˆä¸ºä¸»æµï¼Œä½†å­˜åœ¨å¸§é—´é—ªçƒå’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>AvatarSyncæ˜¯ä¸€ä¸ªè‡ªå›å½’æ¡†æ¶ï¼Œèƒ½ä»å•ä¸€å‚è€ƒå›¾åƒç”ŸæˆçœŸå®å¯æ§çš„è°ˆè¯å¤´åŠ¨ç”»ï¼Œç”±æ–‡æœ¬æˆ–éŸ³é¢‘é©±åŠ¨ã€‚</li>
<li>é€šè¿‡è‡ªå›å½’ç®¡é“å’ŒéŸ³ç´ å¼•å…¥å‡è½»é—ªçƒï¼Œç¡®ä¿åŠ¨ç”»è¿ç»­æ€§å¹¶ç²¾ç¡®å¯¹é½éŸ³ç´ å’Œè§†è§‰ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥åŠ é€Ÿæ¨ç†ï¼Œè§£è€¦è¯­ä¹‰å»ºæ¨¡å’Œè§†è§‰åŠ¨æ€ã€‚</li>
<li>å®šåˆ¶çš„éŸ³ç´ å¸§å› æœæ³¨æ„åŠ›æ©è†œæ”¯æŒå¤šæ­¥å¹¶è¡ŒåŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-048d641b03e67540af17e882e55f70c6" align="middle">
<img src="https://picx.zhimg.com/v2-8077d61ade754d02cc81cb658becfc0d" align="middle">
<img src="https://picx.zhimg.com/v2-9b6c5da1fb33bc9098fcecb7a7597568" align="middle">
<img src="https://picx.zhimg.com/v2-07e6ea6fa8caed71571d12ebb34a9ea0" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Talk-Isnâ€™t-Always-Cheap-Understanding-Failure-Modes-in-Multi-Agent-Debate"><a href="#Talk-Isnâ€™t-Always-Cheap-Understanding-Failure-Modes-in-Multi-Agent-Debate" class="headerlink" title="Talk Isnâ€™t Always Cheap: Understanding Failure Modes in Multi-Agent   Debate"></a>Talk Isnâ€™t Always Cheap: Understanding Failure Modes in Multi-Agent   Debate</h2><p><strong>Authors:Andrea Wynn, Harsh Satija, Gillian Hadfield</strong></p>
<p>While multi-agent debate has been proposed as a promising strategy for improving AI reasoning ability, we find that debate can sometimes be harmful rather than helpful. Prior work has primarily focused on debates within homogeneous groups of agents, whereas we explore how diversity in model capabilities influences the dynamics and outcomes of multi-agent interactions. Through a series of experiments, we demonstrate that debate can lead to a decrease in accuracy over time - even in settings where stronger (i.e., more capable) models outnumber their weaker counterparts. Our analysis reveals that models frequently shift from correct to incorrect answers in response to peer reasoning, favoring agreement over challenging flawed reasoning. We perform additional experiments investigating various potential contributing factors to these harmful shifts - including sycophancy, social conformity, and model and task type. These results highlight important failure modes in the exchange of reasons during multi-agent debate, suggesting that naive applications of debate may cause performance degradation when agents are neither incentivised nor adequately equipped to resist persuasive but incorrect reasoning. </p>
<blockquote>
<p>è™½ç„¶å¤šæ™ºèƒ½ä½“è¾©è®ºè¢«è®¤ä¸ºæ˜¯æé«˜äººå·¥æ™ºèƒ½æ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„ç­–ç•¥ï¼Œä½†æˆ‘ä»¬å‘ç°è¾©è®ºæœ‰æ—¶å¯èƒ½æœ‰å®³è€Œéæœ‰ç›Šã€‚å…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åŒè´¨æ™ºèƒ½ä½“ç¾¤ä½“å†…çš„è¾©è®ºä¸Šï¼Œè€Œæˆ‘ä»¬æ¢ç´¢æ¨¡å‹èƒ½åŠ›çš„å¤šæ ·æ€§å¦‚ä½•å½±å“å¤šæ™ºèƒ½ä½“äº¤äº’çš„åŠ¨åŠ›å’Œç»“æœã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¾©è®ºä¼šå¯¼è‡´å‡†ç¡®æ€§éšç€æ—¶é—´çš„æ¨ç§»è€Œä¸‹é™â€”â€”å³ä½¿åœ¨æ›´å¼ºçš„æ¨¡å‹ï¼ˆå³æ›´å¼ºå¤§çš„æ¨¡å‹ï¼‰æ•°é‡è¶…è¿‡è¾ƒå¼±å¯¹æ‰‹çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹ç»å¸¸ä»æ­£ç¡®çš„ç­”æ¡ˆè½¬å‘é”™è¯¯çš„ç­”æ¡ˆæ¥å›åº”åŒä¼´çš„æ¨ç†ï¼Œæ›´å–œæ¬¢æ¥å—æœ‰ç¼ºé™·çš„æ¨ç†è€Œä¸æ„¿æ„è´¨ç–‘æŒ‘æˆ˜å®ƒã€‚æˆ‘ä»¬è¿›è¡Œäº†æ›´å¤šçš„å®éªŒæ¥è°ƒæŸ¥å¯¼è‡´è¿™äº›æœ‰å®³è½¬å˜çš„å„ç§æ½œåœ¨å› ç´ ï¼ŒåŒ…æ‹¬å¥‰æ‰¿ã€ç¤¾ä¼šä¸€è‡´æ€§å’Œæ¨¡å‹å’Œä»»åŠ¡ç±»å‹ã€‚è¿™äº›ç»“æœçªå‡ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºè¿‡ç¨‹ä¸­äº¤æµæ¨ç†çš„é‡è¦å¤±è´¥æ¨¡å¼ï¼Œè¡¨æ˜å½“æ™ºèƒ½ä½“æ—¢æ²¡æœ‰å—åˆ°æ¿€åŠ±ä¹Ÿæ²¡æœ‰å¾—åˆ°å……åˆ†å‡†å¤‡æ¥æŠµåˆ¶å…·æœ‰è¯´æœåŠ›çš„é”™è¯¯æ¨ç†æ—¶ï¼Œç®€å•åº”ç”¨è¾©è®ºå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05396v2">PDF</a> ICML MAS Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ™ºèƒ½ä½“è¾©è®ºå¯¹AIæ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œå‘ç°è¾©è®ºæœ‰æ—¶å¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ç ”ç©¶çªç ´äº†ä»¥å¾€å¯¹åŒæ„æ™ºèƒ½ä½“è¾©è®ºçš„å±€é™ï¼Œæ¢è®¨äº†æ¨¡å‹èƒ½åŠ›å¤šæ ·æ€§å¯¹å¤šæ™ºèƒ½ä½“äº¤äº’åŠ¨æ€å’Œç»“æœçš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å¼ºæ¨¡å‹æ•°é‡è¶…è¿‡å¼±æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¾©è®ºä¹Ÿå¯èƒ½å¯¼è‡´å‡†ç¡®æ€§éšæ—¶é—´é™ä½ã€‚åˆ†ææ˜¾ç¤ºï¼Œæ™ºèƒ½ä½“åœ¨å›åº”åŒè¡Œæ¨ç†æ—¶å®¹æ˜“æ”¹å˜åŸå…ˆçš„æ­£ç¡®ç­”æ¡ˆè€Œè¶‹å‘é”™è¯¯çš„ç­”æ¡ˆï¼Œä¸”åå¥½äºæ¥å—è€ŒéæŒ‘æˆ˜é”™è¯¯çš„æ¨ç†ã€‚å®éªŒè¿˜æ¢è®¨äº†åŒ…æ‹¬å¥‰æ‰¿ã€ç¤¾ä¼šä»ä¼—å¿ƒç†ä»¥åŠæ¨¡å‹å’Œä»»åŠ¡ç±»å‹ç­‰å› ç´ å¯¹æœ‰å®³å½±å“çš„ä½œç”¨ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†å¤šæ™ºèƒ½ä½“è¾©è®ºè¿‡ç¨‹ä¸­äº¤æ¢ç†ç”±çš„é‡è¦å¤±è´¥æ¨¡å¼ï¼Œæç¤ºåœ¨æ™ºèƒ½ä½“æœªèƒ½å—åˆ°æ¿€åŠ±æˆ–å……åˆ†åº”å¯¹è¯´æœæ€§ä½†é”™è¯¯çš„æ¨ç†æ—¶ï¼Œç›²ç›®åº”ç”¨è¾©è®ºå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“è¾©è®ºè™½è¢«è§†ä¸ºæå‡AIæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ï¼Œä½†æœ‰æ—¶å¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>ç ”ç©¶è€ƒå¯Ÿäº†æ¨¡å‹èƒ½åŠ›å¤šæ ·æ€§å¯¹å¤šæ™ºèƒ½ä½“è¾©è®ºçš„å½±å“ï¼Œçªç ´äº†ä»¥å¾€ç ”ç©¶çš„ä¸»è¦å±€é™ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿åœ¨å¼ºæ¨¡å‹å å¤šæ•°çš„æƒ…å†µä¸‹ï¼Œè¾©è®ºä¹Ÿå¯èƒ½å¯¼è‡´AIçš„å‡†ç¡®æ€§ä¸‹é™ã€‚</li>
<li>AIåœ¨å›åº”åŒè¡Œæ¨ç†æ—¶å®¹æ˜“æ”¹å˜ç­”æ¡ˆï¼Œæ›´å€¾å‘äºæ¥å—è€ŒéæŒ‘æˆ˜é”™è¯¯çš„æ¨ç†ã€‚</li>
<li>å®éªŒæ¢è®¨äº†å¤šç§æ½œåœ¨å› ç´ ï¼ŒåŒ…æ‹¬å¥‰æ‰¿ã€ç¤¾ä¼šä»ä¼—å¿ƒç†ç­‰å¯¹è¾©è®ºè¿‡ç¨‹ä¸­æœ‰å®³å˜åŒ–çš„å½±å“ã€‚</li>
<li>è¿™äº›å‘ç°æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„å¤±è´¥æ¨¡å¼ï¼ŒæŒ‡å‡ºäº†åœ¨æŸäº›æƒ…å†µä¸‹è¾©è®ºå¯èƒ½å¯¼è‡´AIæ€§èƒ½ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-320b5d6dd380dc1c6e99f8907eef008e" align="middle">
<img src="https://picx.zhimg.com/v2-6e8b9cee924f777dd267ad2782456b57" align="middle">
<img src="https://picx.zhimg.com/v2-09a1289c14f0375f2eba3bf188913bd0" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Talk-Less-Call-Right-Enhancing-Role-Play-LLM-Agents-with-Automatic-Prompt-Optimization-and-Role-Prompting"><a href="#Talk-Less-Call-Right-Enhancing-Role-Play-LLM-Agents-with-Automatic-Prompt-Optimization-and-Role-Prompting" class="headerlink" title="Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic   Prompt Optimization and Role Prompting"></a>Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic   Prompt Optimization and Role Prompting</h2><p><strong>Authors:Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul</strong></p>
<p>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques-character-card&#x2F;scene-contract design and strict enforcement of function calling-which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool Source code is available at <a target="_blank" rel="noopener" href="https://github.com/scb-10x/apo">https://github.com/scb-10x/apo</a> </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šæ—¨åœ¨æ¢è®¨åœ¨Commonsense Persona-grounded Dialogue Challengeï¼ˆCPDC 2025ï¼‰çš„APIèµ›é“ä¸­ï¼Œå¦‚ä½•æç¤ºå·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰®æ¼”è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†çš„æ–¹æ³•ã€‚åœ¨æ­¤åœºæ™¯ä¸­ï¼Œå¯¹è¯ä»£ç†å¾€å¾€ä¼šç”Ÿæˆè¿‡é•¿çš„è§’è‰²å†…å“åº”ï¼ˆè¯´è¯è¿‡å¤šï¼‰ï¼ŒåŒæ—¶æœªèƒ½æ ¹æ®è§’è‰²æœ‰æ•ˆåœ°ä½¿ç”¨å·¥å…·ï¼ˆè¡¨ç°ä¸è¶³ï¼‰ï¼Œä¾‹å¦‚ç”Ÿæˆä¸å­˜åœ¨çš„å‡½æ•°è°ƒç”¨æˆ–åœ¨å›ç­”é—®é¢˜ä¹‹å‰è¿›è¡Œä¸å¿…è¦çš„å·¥å…·è°ƒç”¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†å››ç§æç¤ºæ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼š1ï¼‰åŸºæœ¬è§’è‰²æç¤ºï¼Œ2ï¼‰æ”¹è¿›çš„è§’è‰²æç¤ºï¼Œ3ï¼‰è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰ï¼Œä»¥åŠ4ï¼‰åŸºäºè§„åˆ™çš„è§’è‰²æç¤ºã€‚åŸºäºè§„åˆ™çš„è§’è‰²æç¤ºï¼ˆRRPï¼‰æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œå®ƒé€šè¿‡ä¸¤ç§æ–°æŠ€æœ¯â€”â€”è§’è‰²å¡&#x2F;åœºæ™¯åˆçº¦è®¾è®¡å’Œä¸¥æ ¼çš„åŠŸèƒ½è°ƒç”¨æ‰§è¡Œï¼Œè·å¾—äº†0.571çš„æ€»ä½“å¾—åˆ†ï¼Œç›¸è¾ƒäºé›¶åŸºå‡†çº¿å¾—åˆ†0.519æœ‰æ‰€æé«˜ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸æ›´ç²¾ç»†çš„æ–¹æ³•ï¼ˆå¦‚APOï¼‰ç›¸æ¯”ï¼ŒRRPè®¾è®¡å¯ä»¥æ˜¾è‘—æé«˜è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚ä¸ºäº†æ”¯æŒæœªæ¥åœ¨å¼€å‘ä¸ªæ€§åŒ–æç¤ºæ–¹é¢çš„åŠªåŠ›ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰è¡¨ç°æœ€ä½³çš„æç¤ºå’ŒAPOå·¥å…·æºä»£ç ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scb-10x/apo%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/scb-10x/apoæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00482v2">PDF</a> EMNLP 2025 Wordplay Workshop (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æŠ¥å‘Šç ”ç©¶äº†å¦‚ä½•å¼•å¯¼å·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Commonsense Persona-grounded Dialogue Challengeï¼ˆCPDCï¼‰2025çš„APIèµ›é“ä¸­æ‰®æ¼”è§’è‰²å¯¹è¯ä»£ç†çš„æ–¹æ³•ã€‚é’ˆå¯¹å¯¹è¯ä»£ç†åœ¨è§’è‰²æ‰®æ¼”è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¯¹è¯è¿‡é•¿ã€å·¥å…·ä½¿ç”¨ä¸å½“ç­‰é—®é¢˜ï¼Œæ¢ç´¢äº†å››ç§æç¤ºæ–¹æ³•ã€‚å…¶ä¸­ï¼ŒåŸºäºè§„åˆ™çš„è§’è‰²æç¤ºï¼ˆRRPï¼‰æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œé€šè¿‡è§’è‰²å¡&#x2F;åœºæ™¯åˆçº¦è®¾è®¡å’ŒåŠŸèƒ½è°ƒç”¨çš„ä¸¥æ ¼å®æ–½ï¼Œå®ç°äº†æ•´ä½“å¾—åˆ†0.571ï¼Œç›¸è¾ƒäºé›¶åŸºçº¿æå‡äº†æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRRPè®¾è®¡èƒ½æ˜¾è‘—æå‡è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥å‘Šç ”ç©¶äº†åœ¨CPDC 2025çš„APIèµ›é“ä¸­å¼•å¯¼LLMæ‰®æ¼”è§’è‰²å¯¹è¯ä»£ç†çš„æ–¹æ³•ã€‚</li>
<li>å¯¹è¯ä»£ç†å­˜åœ¨å¯¹è¯è¿‡é•¿å’Œå·¥å…·ä½¿ç”¨ä¸å½“çš„é—®é¢˜ã€‚</li>
<li>æŠ¥å‘Šæ¢ç´¢äº†å››ç§è§£å†³ä¸Šè¿°é—®é¢˜çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºæœ¬è§’è‰²æç¤ºã€æ”¹è¿›è§’è‰²æç¤ºã€è‡ªåŠ¨æç¤ºä¼˜åŒ–å’ŒåŸºäºè§„åˆ™çš„è§’è‰²æç¤ºã€‚</li>
<li>åŸºäºè§„åˆ™çš„è§’è‰²æç¤ºï¼ˆRRPï¼‰æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œé€šè¿‡è§’è‰²å¡&#x2F;åœºæ™¯åˆçº¦è®¾è®¡å’ŒåŠŸèƒ½è°ƒç”¨çš„ä¸¥æ ¼å®æ–½ï¼Œå®ç°äº†æ•´ä½“å¾—åˆ†ä¸º0.571ã€‚</li>
<li>RRPè®¾è®¡èƒ½æ˜¾è‘—æå‡è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚</li>
<li>æŠ¥å‘Šå…¬å¼€äº†æœ€ä½³æ€§èƒ½çš„è§’è‰²æç¤ºå’Œè‡ªåŠ¨æç¤ºä¼˜åŒ–å·¥å…·æºä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a31e7ef1228f3a99283e727761c7e39" align="middle">
<img src="https://picx.zhimg.com/v2-0daee9613e67a7f32c30092e7ac05f05" align="middle">
<img src="https://picx.zhimg.com/v2-a21dcf6265072debfdd7cac965cbe424" align="middle">
<img src="https://picx.zhimg.com/v2-6e7c8ee66492659cbc6434742a1c3fbd" align="middle">
<img src="https://picx.zhimg.com/v2-146fcba28a3931abffa924c240c851c8" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AsynFusion-Towards-Asynchronous-Latent-Consistency-Models-for-Decoupled-Whole-Body-Audio-Driven-Avatars"><a href="#AsynFusion-Towards-Asynchronous-Latent-Consistency-Models-for-Decoupled-Whole-Body-Audio-Driven-Avatars" class="headerlink" title="AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled   Whole-Body Audio-Driven Avatars"></a>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled   Whole-Body Audio-Driven Avatars</h2><p><strong>Authors:Tianbao Zhang, Jian Zhao, Yuer Li, Zheng Zhu, Ping Hu, Zhaoxin Fan, Wenjun Wu, Xuelong Li</strong></p>
<p>Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>å…¨èº«éŸ³é¢‘é©±åŠ¨çš„è§’è‰²å§¿æ€å’Œè¡¨æƒ…ç”Ÿæˆå¯¹äºåˆ›å»ºé€¼çœŸçš„æ•°å­—äººç±»å¹¶å¢å¼ºäº¤äº’å¼è™šæ‹Ÿä»£ç†çš„èƒ½åŠ›æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œåœ¨è™šæ‹Ÿç°å®ã€æ•°å­—å¨±ä¹å’Œè¿œç¨‹é€šä¿¡ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç‹¬ç«‹ç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œï¼Œè¿™å¼•å…¥äº†ä¸€ä¸ªæ˜¾è‘—çš„å±€é™æ€§ï¼šé¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œå…ƒç´ ä¹‹é—´ç¼ºä¹æ— ç¼åè°ƒï¼Œå¯¼è‡´åŠ¨ç”»æ•ˆæœä¸é‚£ä¹ˆè‡ªç„¶å’Œè¿è´¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AsynFusionï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ‰©æ•£å˜å‹å™¨å®ç°å’Œè°è¡¨æƒ…å’ŒåŠ¨ä½œåˆæˆçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨åŒåˆ†æ”¯DiTæ¶æ„ä¹‹ä¸Šï¼Œå®ç°äº†é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œçš„å¹¶è¡Œç”Ÿæˆã€‚åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä½œåŒæ­¥æ¨¡å—ï¼Œä»¥ä¿ƒè¿›ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„åŒå‘ç‰¹å¾äº¤äº’ï¼Œä»¥åŠä¸€ç§å¼‚æ­¥LCMé‡‡æ ·ç­–ç•¥ï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒé«˜è´¨é‡è¾“å‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAsynFusionåœ¨ç”Ÿæˆå®æ—¶åŒæ­¥çš„å…¨èº«åŠ¨ç”»æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15058v2">PDF</a> 15pages, conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…¨éŸ³é¢‘é©±åŠ¨çš„è§’è‰²å§¿æ€ä¸è¡¨æƒ…ç”ŸæˆæŠ€æœ¯çš„é‡è¦æ€§åŠå…¶åœ¨è™šæ‹Ÿç°å®ã€æ•°å­—å¨±ä¹å’Œè¿œç¨‹é€šä¿¡ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç‹¬ç«‹ç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œï¼Œå¯¼è‡´é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œä¹‹é—´ç¼ºä¹æ— ç¼åè°ƒï¼ŒåŠ¨ç”»æ•ˆæœä¸è‡ªç„¶ä¸”è¿è´¯æ€§ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AsynFusionæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜å‹å™¨å®ç°å’Œè°çš„è¡¨æƒ…å’ŒåŠ¨ä½œåˆæˆã€‚è¯¥æ–¹æ³•åŸºäºåŒåˆ†æ”¯DiTæ¶æ„æ„å»ºï¼Œèƒ½å¤Ÿå®ç°é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œçš„å¹¶è¡Œç”Ÿæˆã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§ååŒåŒæ­¥æ¨¡å—å’Œä¸€ç§å¼‚æ­¥LCMé‡‡æ ·ç­–ç•¥æ¥ä¿ƒè¿›é¢éƒ¨å’ŒåŠ¨ä½œä¹‹é—´çš„åŒå‘ç‰¹å¾äº¤äº’å¹¶ç»´æŒé«˜è´¨é‡è¾“å‡ºåŒæ—¶é™ä½è®¡ç®—å¼€é”€ã€‚å®éªŒè¯æ˜ï¼ŒAsynFusionåœ¨ç”Ÿæˆå®æ—¶åŒæ­¥å…¨èº«åŠ¨ç”»æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚å…¶åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>éŸ³é¢‘é©±åŠ¨çš„è§’è‰²å§¿æ€ä¸è¡¨æƒ…ç”Ÿæˆåœ¨è™šæ‹Ÿç°å®ã€æ•°å­—å¨±ä¹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ä»·å€¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨é¢éƒ¨è¡¨æƒ…ä¸åŠ¨ä½œä¸åè°ƒçš„é—®é¢˜ï¼Œå¯¼è‡´åŠ¨ç”»ä¸è‡ªç„¶ä¸”è¿è´¯æ€§ä¸è¶³ã€‚</li>
<li>AsynFusionæ¡†æ¶ä½¿ç”¨æ‰©æ•£å˜å‹å™¨å®ç°å’Œè°çš„è¡¨æƒ…å’ŒåŠ¨ä½œåˆæˆï¼Œé€šè¿‡åŒåˆ†æ”¯DiTæ¶æ„å¹¶è¡Œç”Ÿæˆé¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œã€‚</li>
<li>å¼•å…¥ååŒåŒæ­¥æ¨¡å—ä¿ƒè¿›é¢éƒ¨å’ŒåŠ¨ä½œä¹‹é—´çš„åŒå‘ç‰¹å¾äº¤äº’ã€‚</li>
<li>æå‡ºä¸€ç§å¼‚æ­¥LCMé‡‡æ ·ç­–ç•¥ï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€å¹¶ç»´æŒé«˜è´¨é‡è¾“å‡ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e2254775b108fd18a8fa99ca94902de" align="middle">
<img src="https://picx.zhimg.com/v2-2e0076f7d6e605ab591fe5ab1206a080" align="middle">
<img src="https://picx.zhimg.com/v2-0907e1cf507e65acde16d35da6c6fd52" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GestureCoach-Rehearsing-for-Engaging-Talks-with-LLM-Driven-Gesture-Recommendations"><a href="#GestureCoach-Rehearsing-for-Engaging-Talks-with-LLM-Driven-Gesture-Recommendations" class="headerlink" title="GestureCoach: Rehearsing for Engaging Talks with LLM-Driven Gesture   Recommendations"></a>GestureCoach: Rehearsing for Engaging Talks with LLM-Driven Gesture   Recommendations</h2><p><strong>Authors:Ashwin Ram, Varsha Suresh, Artin Saberpour Abadian, Vera Demberg, JÃ¼rgen Steimle</strong></p>
<p>This paper introduces GestureCoach, a system designed to help speakers deliver more engaging talks by guiding them to gesture effectively during rehearsal. GestureCoach combines an LLM-driven gesture recommendation model with a rehearsal interface that proactively cues speakers to gesture appropriately. Trained on expertsâ€™ gesturing patterns from TED talks, the model consists of two modules: an emphasis proposal module, which predicts when to gesture by identifying gesture-worthy text segments in the presenter notes, and a gesture identification module, which determines what gesture to use by retrieving semantically appropriate gestures from a curated gesture database. Results of a model performance evaluation and user study (N&#x3D;30) show that the emphasis proposal module outperforms off-the-shelf LLMs in identifying suitable gesture regions, and that participants rated the majority of these predicted regions and their corresponding gestures as highly appropriate. A subsequent user study (N&#x3D;10) showed that rehearsing with GestureCoach encouraged speakers to gesture and significantly increased gesture diversity, resulting in more engaging talks. We conclude with design implications for future AI-driven rehearsal systems. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†GestureCoachç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ—¨åœ¨é€šè¿‡æŒ‡å¯¼æ¼”è®²è€…åœ¨æ’ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ‰‹åŠ¿è¾…åŠ©ï¼Œå¸®åŠ©æ¼”è®²è€…è¿›è¡Œæ›´æœ‰å¸å¼•åŠ›çš„æ¼”è®²ã€‚GestureCoachç»“åˆäº†ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ‰‹åŠ¿æ¨èæ¨¡å‹å’Œä¸€ä¸ªæ’ç»ƒç•Œé¢ï¼Œè¯¥ç•Œé¢ä¼šä¸»åŠ¨æç¤ºæ¼”è®²è€…è¿›è¡Œé€‚å½“çš„æ‰‹åŠ¿ã€‚è¯¥æ¨¡å‹ä»¥TEDæ¼”è®²ä¸­ä¸“å®¶æ‰‹åŠ¿æ¨¡å¼ä¸ºè®­ç»ƒæ•°æ®ï¼ŒåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šé‡ç‚¹ææ¡ˆæ¨¡å—ï¼Œé€šè¿‡è¯†åˆ«æ¼”è®²ç¬”è®°ä¸­å€¼å¾—åšæ‰‹åŠ¿çš„æ–‡æœ¬ç‰‡æ®µæ¥é¢„æµ‹ä½•æ—¶è¿›è¡Œæ‰‹åŠ¿ï¼›æ‰‹åŠ¿è¯†åˆ«æ¨¡å—ï¼Œé€šè¿‡ä»ç²¾é€‰çš„æ‰‹åŠ¿æ•°æ®åº“ä¸­æ£€ç´¢è¯­ä¹‰é€‚å½“çš„æ‰‹åŠ¿æ¥ç¡®å®šåº”ä½¿ç”¨ä½•ç§æ‰‹åŠ¿ã€‚æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œç”¨æˆ·ç ”ç©¶ï¼ˆN&#x3D;30ï¼‰çš„ç»“æœæ˜¾ç¤ºï¼Œé‡ç‚¹ææ¡ˆæ¨¡å—åœ¨è¯†åˆ«åˆé€‚çš„æ‰‹åŠ¿åŒºåŸŸæ–¹é¢ä¼˜äºç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”å¤§å¤šæ•°å‚ä¸è€…è®¤ä¸ºè¿™äº›é¢„æµ‹åŒºåŸŸåŠå…¶å¯¹åº”çš„æ‰‹åŠ¿é«˜åº¦åˆé€‚ã€‚éšåçš„ç”¨æˆ·ç ”ç©¶ï¼ˆN&#x3D;10ï¼‰è¡¨æ˜ï¼Œä½¿ç”¨GestureCoachè¿›è¡Œæ’ç»ƒé¼“åŠ±äº†æ¼”è®²è€…è¿›è¡Œæ‰‹åŠ¿ï¼Œå¹¶æ˜¾è‘—å¢åŠ äº†æ‰‹åŠ¿çš„å¤šæ ·æ€§ï¼Œä»è€Œäº§ç”Ÿäº†æ›´å…·å¸å¼•åŠ›çš„æ¼”è®²ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†å¯¹æœªæ¥AIé©±åŠ¨æ’ç»ƒç³»ç»Ÿçš„è®¾è®¡å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10706v2">PDF</a> Accepted at UIST 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GestureCoachç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å¼•å¯¼æ¼”è®²è€…åœ¨æ’ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ‰‹åŠ¿ï¼Œå¸®åŠ©æ¼”è®²è€…è¿›è¡Œæ›´å…·å¸å¼•åŠ›çš„æ¼”è®²ã€‚GestureCoachç»“åˆäº†LLMé©±åŠ¨çš„æ‰‹åŠ¿æ¨èæ¨¡å‹ä¸æ’ç»ƒç•Œé¢ï¼Œæå‰æç¤ºæ¼”è®²è€…è¿›è¡Œé€‚å½“çš„æ‰‹åŠ¿ã€‚è¯¥æ¨¡å‹åŸºäºTEDæ¼”è®²ä¸“å®¶çš„æ‰‹åŠ¿æ¨¡å¼è®­ç»ƒï¼ŒåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šé‡ç‚¹æç¤ºæ¨¡å—ï¼Œé€šè¿‡è¯†åˆ«å€¼å¾—æ‰‹åŠ¿çš„æ–‡æœ¬ç‰‡æ®µæ¥é¢„æµ‹ä½•æ—¶è¿›è¡Œæ‰‹åŠ¿ï¼›æ‰‹åŠ¿è¯†åˆ«æ¨¡å—ï¼Œé€šè¿‡ä»ç²¾é€‰çš„æ‰‹åŠ¿æ•°æ®åº“ä¸­æ£€ç´¢è¯­ä¹‰é€‚å½“çš„æ‰‹åŠ¿æ¥ç¡®å®šåº”ä½¿ç”¨ä½•ç§æ‰‹åŠ¿ã€‚æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œä¸€é¡¹æœ‰30åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œé‡ç‚¹æç¤ºæ¨¡å—åœ¨è¯†åˆ«åˆé€‚çš„æ‰‹åŠ¿åŒºåŸŸæ–¹é¢è¡¨ç°ä¼˜äºç°æˆçš„LLMï¼Œå¹¶ä¸”å¤§å¤šæ•°å‚ä¸è€…è®¤ä¸ºè¿™äº›é¢„æµ‹åŒºåŸŸåŠå…¶å¯¹åº”çš„æ‰‹åŠ¿éå¸¸åˆé€‚ã€‚å¦ä¸€é¡¹æœ‰10åå‚ä¸è€…çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨GestureCoachæ’ç»ƒé¼“åŠ±æ¼”è®²è€…è¿›è¡Œæ‰‹åŠ¿ï¼Œå¹¶æ˜¾è‘—å¢åŠ äº†æ‰‹åŠ¿çš„å¤šæ ·æ€§ï¼Œä»è€Œä½¿æ¼”è®²æ›´åŠ å¼•äººå…¥èƒœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GestureCoachç³»ç»Ÿæ—¨åœ¨é€šè¿‡å¼•å¯¼æ¼”è®²è€…è¿›è¡Œæœ‰æ•ˆæ‰‹åŠ¿æ¥æå‡æ¼”è®²å¸å¼•åŠ›ã€‚</li>
<li>ç³»ç»Ÿç»“åˆLLMé©±åŠ¨çš„æ‰‹åŠ¿æ¨èæ¨¡å‹å’Œæ’ç»ƒç•Œé¢ï¼Œæå‰æç¤ºæ¼”è®²è€…è¿›è¡Œæ‰‹åŠ¿ã€‚</li>
<li>æ‰‹åŠ¿æ¨èæ¨¡å‹åŒ…å«é‡ç‚¹æç¤ºæ¨¡å—å’Œæ‰‹åŠ¿è¯†åˆ«æ¨¡å—ã€‚</li>
<li>é‡ç‚¹æç¤ºæ¨¡å—èƒ½è¯†åˆ«å€¼å¾—æ‰‹åŠ¿çš„æ–‡æœ¬ç‰‡æ®µï¼Œé¢„æµ‹æœ€ä½³æ‰‹åŠ¿æ—¶æœºã€‚</li>
<li>æ‰‹åŠ¿è¯†åˆ«æ¨¡å—ä»ç²¾é€‰æ•°æ®åº“ä¸­æ£€ç´¢è¯­ä¹‰é€‚å½“çš„æ‰‹åŠ¿ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½æœ‰æ•ˆæé«˜æ¼”è®²è€…çš„æ‰‹åŠ¿ä½¿ç”¨é¢‘ç‡å’Œå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9aa628f6e95794b0c22ac87b6368e66" align="middle">
<img src="https://picx.zhimg.com/v2-b7c42a9ea9082ca194d6661c6ca46bca" align="middle">
<img src="https://picx.zhimg.com/v2-95d0f4422d4e1d4f7d197aaa1d68e145" align="middle">
<img src="https://picx.zhimg.com/v2-3d8871ef7e082caf7f2588333acdaa7f" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PASE-Phoneme-Aware-Speech-Encoder-to-Improve-Lip-Sync-Accuracy-for-Talking-Head-Synthesis"><a href="#PASE-Phoneme-Aware-Speech-Encoder-to-Improve-Lip-Sync-Accuracy-for-Talking-Head-Synthesis" class="headerlink" title="PASE: Phoneme-Aware Speech Encoder to Improve Lip Sync Accuracy for   Talking Head Synthesis"></a>PASE: Phoneme-Aware Speech Encoder to Improve Lip Sync Accuracy for   Talking Head Synthesis</h2><p><strong>Authors:Yihuan Huang, Jiajun Liu, Yanzhen Ren, Jun Xue, Wuyang Liu, Zongkun Sun</strong></p>
<p>Recent talking head synthesis works typically adopt speech features extracted from large-scale pre-trained acoustic models. However, the intrinsic many-to-many relationship between speech and lip motion causes phoneme-viseme alignment ambiguity, leading to inaccurate and unstable lips. To further improve lip sync accuracy, we propose PASE (Phoneme-Aware Speech Encoder), a novel speech representation model that bridges the gap between phonemes and visemes. PASE explicitly introduces phoneme embeddings as alignment anchors and employs a contrastive alignment module to enhance the discriminability between corresponding audio-visual pairs. In addition, a prediction and reconstruction task is designed to improve robustness under noise and partial modality absence. Experimental results show PASE significantly improves lip sync accuracy and achieves state-of-the-art performance across both NeRF- and 3DGS-based rendering frameworks, outperforming conventional methods based on acoustic features by 13.7 % and 14.2 %, respectively. Importantly, PASE can be seamlessly integrated into diverse talking head pipelines to improve the lip sync accuracy without architectural modifications. </p>
<blockquote>
<p>æœ€æ–°çš„è¯´è¯äººå¤´éƒ¨åˆæˆå·¥ä½œé€šå¸¸é‡‡ç”¨ä»å¤§è§„æ¨¡é¢„è®­ç»ƒå£°å­¦æ¨¡å‹ä¸­æå–çš„è¯­éŸ³ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¯­éŸ³å’Œå”‡éƒ¨è¿åŠ¨ä¹‹é—´å›ºæœ‰çš„å¤šå¯¹å¤šå…³ç³»å¯¼è‡´äº†éŸ³ç´ -åŠ¨ç´ å¯¹é½çš„æ¨¡ç³Šæ€§ï¼Œä»è€Œå¯¼è‡´å”‡éƒ¨ä¸å‡†ç¡®ä¸”ä¸ç¨³å®šã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å”‡éƒ¨åŒæ­¥ç²¾åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†PASEï¼ˆéŸ³ç´ æ„ŸçŸ¥è¯­éŸ³ç¼–ç å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ï¼Œèƒ½å¤Ÿå¼¥åˆéŸ³ç´ å’ŒåŠ¨ç´ ä¹‹é—´çš„é¸¿æ²Ÿã€‚PASEæ˜¾å¼å¼•å…¥éŸ³ç´ åµŒå…¥ä½œä¸ºå¯¹é½é”šç‚¹ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å¯¹é½æ¨¡å—å¢å¼ºç›¸åº”è§†å¬å¯¹çš„è¾¨åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†é¢„æµ‹å’Œé‡å»ºä»»åŠ¡ï¼Œä»¥æé«˜å™ªå£°å’Œå±€éƒ¨æ¨¡æ€ç¼ºå¤±æƒ…å†µä¸‹çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPASEæ˜¾è‘—æé«˜äº†å”‡éƒ¨åŒæ­¥ç²¾åº¦ï¼Œåœ¨åŸºäºNeRFå’Œ3DGSçš„æ¸²æŸ“æ¡†æ¶ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒåŸºäºå£°å­¦ç‰¹å¾çš„ä¼ ç»Ÿæ–¹æ³•åˆ†åˆ«æé«˜äº†13.7%å’Œ14.2%ã€‚é‡è¦çš„æ˜¯ï¼ŒPASEå¯ä»¥æ— ç¼é›†æˆåˆ°å¤šæ ·åŒ–çš„è¯´è¯äººå¤´éƒ¨ç®¡é“ä¸­ï¼Œæé«˜å”‡éƒ¨åŒæ­¥ç²¾åº¦è€Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05803v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸè¯´è¯äººå¤´éƒ¨åˆæˆç ”ç©¶é€šå¸¸é‡‡ç”¨ä»å¤§è§„æ¨¡é¢„è®­ç»ƒå£°éŸ³æ¨¡å‹ä¸­æå–çš„è¯­éŸ³ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¯­éŸ³å’Œå”‡éƒ¨è¿åŠ¨ä¹‹é—´çš„å†…åœ¨å¤šå¯¹å¤šå…³ç³»å¯¼è‡´äº†éŸ³ç´ -é¢éƒ¨åŠ¨ç´ å¯¹é½çš„æ¨¡ç³Šæ€§ï¼Œä»è€Œå¯¼è‡´å”‡éƒ¨è¡¨ç°ä¸å‡†ç¡®ä¸”ä¸ç¨³å®šã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å”‡éƒ¨åŒæ­¥ç²¾åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†PASEï¼ˆéŸ³ç´ æ„ŸçŸ¥è¯­éŸ³ç¼–ç å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ï¼Œèƒ½å¤Ÿç¼©å°éŸ³ç´ å’Œé¢éƒ¨åŠ¨ç´ ä¹‹é—´çš„å·®è·ã€‚PASEé€šè¿‡å¼•å…¥éŸ³ç´ åµŒå…¥ä½œä¸ºå¯¹é½é”šç‚¹ï¼Œå¹¶é‡‡ç”¨äº†å¯¹æ¯”å¯¹é½æ¨¡å—æ¥å¢å¼ºå¯¹åº”éŸ³é¢‘è§†è§‰å¯¹ä¹‹é—´çš„åŒºåˆ†åº¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªé¢„æµ‹å’Œé‡å»ºä»»åŠ¡ï¼Œä»¥æé«˜åœ¨å™ªå£°å’Œéƒ¨åˆ†æ¨¡æ€ç¼ºå¤±ä¸‹çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPASEæ˜¾è‘—æé«˜äº†å”‡éƒ¨åŒæ­¥ç²¾åº¦ï¼Œå¹¶åœ¨åŸºäºNeRFå’Œ3DGSçš„æ¸²æŸ“æ¡†æ¶ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºäºå£°éŸ³ç‰¹å¾çš„ä¼ ç»Ÿæ–¹æ³•åˆ†åˆ«æé«˜äº†13.7%å’Œ14.2%ã€‚é‡è¦çš„æ˜¯ï¼ŒPASEå¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§è¯´è¯äººå¤´éƒ¨ç®¡é“ä¸­ï¼Œä»¥æé«˜å”‡éƒ¨åŒæ­¥ç²¾åº¦è€Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯´è¯äººå¤´éƒ¨åˆæˆé¢ä¸´å”‡éƒ¨åŒæ­¥çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè¯­éŸ³å’Œå”‡éƒ¨è¿åŠ¨ä¹‹é—´å­˜åœ¨å¤šå¯¹å¤šçš„å…³ç³»ã€‚</li>
<li>PASEæ¨¡å‹é€šè¿‡å¼•å…¥éŸ³ç´ æ„ŸçŸ¥æœºåˆ¶æ¥ç¼©å°éŸ³ç´ å’Œé¢éƒ¨åŠ¨ç´ ä¹‹é—´çš„å·®è·ã€‚</li>
<li>PASEé‡‡ç”¨å¯¹æ¯”å¯¹é½æ¨¡å—å¢å¼ºéŸ³é¢‘è§†è§‰å¯¹çš„åŒºåˆ†åº¦ã€‚</li>
<li>PASEè®¾è®¡é¢„æµ‹å’Œé‡å»ºä»»åŠ¡ä»¥æé«˜åœ¨å™ªå£°å’Œéƒ¨åˆ†æ¨¡æ€ç¼ºå¤±ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºPASEæ˜¾è‘—æé«˜å”‡éƒ¨åŒæ­¥ç²¾åº¦ï¼Œå¹¶åœ¨ä¸åŒæ¸²æŸ“æ¡†æ¶ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>PASEå¯åœ¨å„ç§è¯´è¯äººå¤´éƒ¨ç®¡é“ä¸­é›†æˆï¼Œæé«˜å”‡éƒ¨åŒæ­¥ç²¾åº¦è€Œæ— éœ€æ”¹å˜æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9e99c24acfcfd27d5a027db1761cbf7" align="middle">
<img src="https://picx.zhimg.com/v2-4011cd7c5d0b0093cc3049e898cb7e52" align="middle">
<img src="https://picx.zhimg.com/v2-ee1750007411d3b30109bebebec20175" align="middle">
<img src="https://picx.zhimg.com/v2-677514280c46ded2ea7b30c48dcbe8c0" align="middle">
<img src="https://picx.zhimg.com/v2-2b58549a1560c8461dde18a2a53c12f3" align="middle">
<img src="https://picx.zhimg.com/v2-ca0f128bf6c3a05b2cf67fd80597d3f8" align="middle">
<img src="https://picx.zhimg.com/v2-a0f246799c0aca76dc1bb2e15d22fcac" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Tokenizing-Motion-A-Generative-Approach-for-Scene-Dynamics-Compression"><a href="#Tokenizing-Motion-A-Generative-Approach-for-Scene-Dynamics-Compression" class="headerlink" title="Tokenizing Motion: A Generative Approach for Scene Dynamics Compression"></a>Tokenizing Motion: A Generative Approach for Scene Dynamics Compression</h2><p><strong>Authors:Shanzhi Yin, Zihan Zhang, Bolin Chen, Shiqi Wang, Yan Ye</strong></p>
<p>This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-<a target="_blank" rel="noopener" href="https://github.com/xyzysz/GNVDC">https://github.com/xyzysz/GNVDC</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç”Ÿæˆå¼è§†é¢‘å‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ¥è‡ªå¸¸è§åœºæ™¯ä¸­çš„ç»†å¾®åŠ¨æ€ï¼ˆå¦‚æ‘‡æ›³çš„èŠ±æœµæˆ–æ°´ä¸Šé£˜åŠ¨çš„èˆ¹åªï¼‰æ‰€æ¨å¯¼å‡ºçš„è¿åŠ¨æ¨¡å¼å…ˆéªŒï¼Œè€Œä¸æ˜¯ä¾èµ–äºè§†é¢‘å†…å®¹å…ˆéªŒï¼ˆå¦‚äººè„¸æˆ–äººä½“ï¼‰ã€‚è¿™äº›ç´§å‡‘çš„è¿åŠ¨å…ˆéªŒçŸ¥è¯†ä¸ºå®ç°è¶…ä½æ¯”ç‰¹ç‡é€šä¿¡æä¾›äº†ä¸€ç§æ–°æ–¹æ³•ï¼ŒåŒæ—¶åœ¨å„ç§åœºæ™¯å†…å®¹ä¸­å®ç°äº†é«˜è´¨é‡é‡å»ºã€‚åœ¨ç¼–ç å™¨ç«¯ï¼Œé€šè¿‡ç¨ å¯†åˆ°ç¨€ç–çš„è½¬æ¢ï¼Œå¯ä»¥å°†è¿åŠ¨å…ˆéªŒçŸ¥è¯†ç®€åŒ–ä¸ºç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ã€‚åœ¨è§£ç å™¨ç«¯ï¼Œè¿™äº›å…ˆéªŒçŸ¥è¯†æœ‰åŠ©äºä½¿ç”¨å…ˆè¿›çš„æµé©±åŠ¨æ‰©æ•£æ¨¡å‹é‡å»ºåœºæ™¯åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‡å¤±çœŸæ€§èƒ½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨åœºæ™¯åŠ¨æ€åºåˆ—ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„ä¼ ç»Ÿè§†é¢‘ç¼–ç å¢å¼ºå‹ç¼©æ¨¡å‹ï¼ˆECMï¼‰ã€‚è¯¥é¡¹ç›®é¡µé¢å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/xyzysz/GNVDC">https://github.com/xyzysz/GNVDC</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09768v2">PDF</a> 5page, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†é¢‘å‹ç¼©æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä»å¸¸è§åœºæ™¯ä¸­çš„ç»†å¾®åŠ¨æ€ï¼ˆå¦‚æ‘‡æ›³çš„èŠ±æœµæˆ–æ°´ä¸Šæ¼‚æµçš„èˆ¹åªï¼‰å¾—å‡ºçš„è¿åŠ¨æ¨¡å¼å…ˆéªŒçŸ¥è¯†ï¼Œè€Œä¸æ˜¯ä¾èµ–äºè§†é¢‘å†…å®¹å…ˆéªŒçŸ¥è¯†ï¼ˆå¦‚äººè„¸æˆ–äººä½“ï¼‰ã€‚è¿™ç§ç´§å‡‘çš„è¿åŠ¨å…ˆéªŒçŸ¥è¯†ä¸ºå®ç°è¶…ä½æ¯”ç‰¹ç‡é€šä¿¡æä¾›äº†æ–°çš„é€”å¾„ï¼ŒåŒæ—¶åœ¨å„ç§åœºæ™¯å†…å®¹ä¸­å®ç°äº†é«˜è´¨é‡é‡å»ºã€‚ç¼–ç å™¨ç«¯å¯é€šè¿‡å¯†é›†åˆ°ç¨€ç–çš„è½¬æ¢æ¥ç®€åŒ–è¿åŠ¨å…ˆéªŒçŸ¥è¯†è¡¨ç¤ºï¼Œè€Œè§£ç å™¨ç«¯åˆ™åˆ©ç”¨å…ˆè¿›çš„æµé©±åŠ¨æ‰©æ•£æ¨¡å‹é‡å»ºåœºæ™¯åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‡å¤±çœŸæ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹¶åœ¨åœºæ™¯åŠ¨æ€åºåˆ—ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›è§†é¢‘ç¼–ç å™¨å¢å¼ºå‹ç¼©æ¨¡å‹ï¼ˆECMï¼‰ã€‚æ›´å¤šè¯¦æƒ…å¯è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/xyzysz/GNVDC%E3%80%82">https://github.com/xyzysz/GNVDCã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹è§†é¢‘å‹ç¼©æ¡†æ¶åˆ©ç”¨è¿åŠ¨æ¨¡å¼å…ˆéªŒçŸ¥è¯†ï¼Œè¿™æ˜¯ä»å¸¸è§åœºæ™¯çš„ç»†å¾®åŠ¨æ€ä¸­å¾—å‡ºçš„ã€‚</li>
<li>æ¡†æ¶å®ç°äº†è¶…ä½æ¯”ç‰¹ç‡é€šä¿¡ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡é‡å»ºã€‚</li>
<li>ç¼–ç å™¨ç«¯é€šè¿‡å¯†é›†åˆ°ç¨€ç–çš„è½¬æ¢ç®€åŒ–è¿åŠ¨å…ˆéªŒçŸ¥è¯†è¡¨ç¤ºã€‚</li>
<li>è§£ç å™¨ç«¯åˆ©ç”¨å…ˆè¿›çš„æµé©±åŠ¨æ‰©æ•£æ¨¡å‹é‡å»ºåœºæ™¯åŠ¨æ€ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®éªŒä¸­çš„ç‡å¤±çœŸæ€§èƒ½è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›è§†é¢‘ç¼–ç å™¨å¢å¼ºå‹ç¼©æ¨¡å‹ï¼ˆECMï¼‰åœ¨åœºæ™¯åŠ¨æ€åºåˆ—ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97ddd84ea94a971d01a6f0755b643e9a" align="middle">
<img src="https://picx.zhimg.com/v2-ebecf704cfdbfacf434f66076a243ba8" align="middle">
<img src="https://picx.zhimg.com/v2-161ebb8b113c8259c1a3214ef0083975" align="middle">
<img src="https://picx.zhimg.com/v2-956c009ac3781aa7d603c7e240f21d50" align="middle">
<img src="https://picx.zhimg.com/v2-9273bb693d7dc9ca2338612918f3e2a8" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-19/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a67bbd7b6fa932a2fa5e550137d2933a" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  RoboGPT-R1 Enhancing Robot Planning with Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ffb66aad3cc5db70ab7ab6f661397f08" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  OmniMotion Multimodal Motion Generation with Continuous Masked   Autoregression
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
