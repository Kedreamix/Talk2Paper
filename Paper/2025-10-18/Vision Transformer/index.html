<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Towards Generalist Intelligence in Dentistry Vision Foundation Models   for Oral and Maxillofacial Radiology">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-85b3126f3e4d8c14cccbeb1047147760~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738137&auth_key=1760738137-0-0-73b410b64a50a459b0e2ccbcfac14c92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-18-æ›´æ–°"><a href="#2025-10-18-æ›´æ–°" class="headerlink" title="2025-10-18 æ›´æ–°"></a>2025-10-18 æ›´æ–°</h1><h2 id="Towards-Generalist-Intelligence-in-Dentistry-Vision-Foundation-Models-for-Oral-and-Maxillofacial-Radiology"><a href="#Towards-Generalist-Intelligence-in-Dentistry-Vision-Foundation-Models-for-Oral-and-Maxillofacial-Radiology" class="headerlink" title="Towards Generalist Intelligence in Dentistry: Vision Foundation Models   for Oral and Maxillofacial Radiology"></a>Towards Generalist Intelligence in Dentistry: Vision Foundation Models   for Oral and Maxillofacial Radiology</h2><p><strong>Authors:Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang</strong></p>
<p>Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare. </p>
<blockquote>
<p>å£è…”é¢Œé¢æ”¾å°„å­¦åœ¨ç‰™ç§‘å¥åº·æŠ¤ç†ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†ç”±äºè®­ç»ƒæœ‰ç´ çš„ä¸“ä¸šäººå‘˜çŸ­ç¼ºï¼Œæ”¾å°„å­¦å›¾åƒè§£è¯»å—åˆ°é™åˆ¶ã€‚è™½ç„¶äººå·¥æ™ºèƒ½æ–¹æ³•å·²ç»å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰çš„ç‰™ç§‘äººå·¥æ™ºèƒ½ç³»ç»Ÿå—åˆ°å…¶å•ä¸€æ¨¡æ€å…³æ³¨ç‚¹ã€ç‰¹å®šä»»åŠ¡è®¾è®¡å’Œä¾èµ–æ˜‚è´µæ ‡æ³¨æ•°æ®çš„é™åˆ¶ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨å„ç§ä¸´åºŠåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentVFMï¼Œè¿™æ˜¯ä¸ºç‰™ç§‘è®¾è®¡çš„ç¬¬ä¸€ä»£è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ã€‚DentVFMä¸ºå„ç§ç‰™ç§‘åº”ç”¨ç”Ÿæˆä»»åŠ¡é€šç”¨çš„è§†è§‰è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨DentVistaï¼ˆä¸€ä¸ªå¤§å‹ç²¾é€‰ç‰™ç§‘æˆåƒæ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªä¸åŒåŒ»ç–—ä¸­å¿ƒçš„çº¦160ä¸‡å¤šç§æ¨¡æ€çš„æ”¾å°„å›¾åƒã€‚DentVFMåŒ…æ‹¬åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¶æ„çš„2Då’Œ3Då˜ä½“ã€‚ä¸ºäº†è§£å†³ç‰™ç§‘æ™ºèƒ½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å…«ä¸ªç‰™ç§‘ä¸“ç§‘ã€æ›´å¤šç–¾ç—…ã€æˆåƒæ–¹å¼å’Œå¹¿æ³›çš„åœ°ç†åˆ†å¸ƒã€‚DentVFMå±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„é€šç”¨æ™ºèƒ½ï¼Œè¡¨ç°å‡ºå¯¹å„ç§ç‰™ç§‘ä»»åŠ¡çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼Œå¦‚ç–¾ç—…è¯Šæ–­ã€æ²»ç–—åˆ†æã€ç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«ä»¥åŠè§£å‰–åœ°æ ‡æ£€æµ‹å’Œåˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDentVFMæ˜¾è‘—ä¼˜äºç›‘ç£å­¦ä¹ ã€è‡ªæˆ‘ç›‘ç£å­¦ä¹ å’Œå¼±ç›‘ç£å­¦ä¹ çš„åŸºçº¿ï¼Œå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€æ ‡ç­¾æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒDentVFMèƒ½å¤Ÿå®ç°è·¨æ¨¡æ€è¯Šæ–­ï¼Œåœ¨å¸¸è§„æˆåƒæ— æ³•è·å–çš„æƒ…å†µä¸‹æä¾›æ›´å¯é çš„ç»“æœï¼Œå…¶è¯Šæ–­ç»“æœç”šè‡³æ¯”ç»éªŒä¸°å¯Œçš„ç‰™åŒ»è¿˜è¦å¯é ã€‚DentVFMä¸ºç‰™ç§‘äººå·¥æ™ºèƒ½è®¾å®šäº†æ–°çš„èŒƒå¼ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯é€‚åº”å’Œæ ‡ç­¾æ•ˆç‡é«˜çš„æ¨¡å‹ï¼Œä»¥æ”¹å–„æ™ºèƒ½ç‰™ç§‘å¥åº·æŠ¤ç†ï¼Œå¹¶è§£å†³å…¨çƒå£è…”å¥åº·æŠ¤ç†ä¸­çš„å…³é”®å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14532v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    å£è…”é¢Œé¢æ”¾å°„å­¦åœ¨å£è…”å¥åº·æŠ¤ç†ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†æ”¾å°„å›¾åƒè§£è¯»å—é™äºä¸“ä¸šäººæ‰çš„çŸ­ç¼ºã€‚è™½ç„¶äººå·¥æ™ºèƒ½æ–¹æ³•å·²å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰å£è…”AIç³»ç»Ÿå—åˆ°å•ä¸€æ¨¡å¼å…³æ³¨ç‚¹ã€ç‰¹å®šä»»åŠ¡è®¾è®¡å’Œä¾èµ–æ˜‚è´µæ ‡æ³¨æ•°æ®çš„é™åˆ¶ï¼Œéš¾ä»¥åœ¨ä¸åŒä¸´åºŠæƒ…æ™¯ä¸­é€šç”¨åŒ–ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentVFMï¼Œè¿™æ˜¯é¦–æ¬¾ä¸ºç‰™ç§‘è®¾è®¡çš„è§†è§‰åŸºç¡€æ¨¡å‹å®¶æ—ï¼ˆVFMsï¼‰ã€‚DentVFMä¸ºå¹¿æ³›çš„ç‰™ç§‘åº”ç”¨ç”Ÿæˆä»»åŠ¡æ— å…³çš„è§†è§‰è¡¨å¾ï¼Œå¹¶ä½¿ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨å¤§å‹ç‰™ç§‘å½±åƒæ•°æ®é›†DentVistaä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«æ¥è‡ªä¸åŒåŒ»ç–—ä¸­å¿ƒçš„çº¦160ä¸‡å¤šç§æ¨¡æ€æ”¾å°„å›¾åƒã€‚DentVFMåŒ…æ‹¬åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¶æ„çš„äºŒç»´å’Œä¸‰ç»´å˜ä½“ã€‚ä¸ºè§£å†³ç‰™ç§‘æ™ºèƒ½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DentBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å…«ä¸ªç‰™ç§‘ä¸“ç§‘ã€æ›´å¤šç–¾ç—…ã€æˆåƒæ–¹å¼å’Œå¹¿æ³›åœ°ç†åˆ†å¸ƒçš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚DentVFMå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é€šç”¨æ™ºèƒ½ï¼Œè¯æ˜å…¶åœ¨å„ç§ç‰™ç§‘ä»»åŠ¡ï¼ˆå¦‚ç–¾ç—…è¯Šæ–­ã€æ²»ç–—åˆ†æã€ç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«ä»¥åŠè§£å‰–åœ°æ ‡æ£€æµ‹å’Œåˆ†å‰²ï¼‰ä¸Šçš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDentVFMæ˜¾è‘—ä¼˜äºç›‘ç£å¼ã€è‡ªæˆ‘ç›‘ç£å¼å’Œå¼±ç›‘ç£å¼åŸºçº¿ï¼Œæä¾›ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€æ ‡ç­¾æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒDentVFMå¯å®ç°è·¨æ¨¡æ€è¯Šæ–­ï¼Œåœ¨å¸¸è§„æˆåƒæ— æ³•ä½¿ç”¨çš„æƒ…å†µä¸‹æä¾›æ›´å¯é çš„ç»“æœã€‚DentVFMä¸ºå£è…”AIè®¾å®šäº†æ–°çš„èŒƒå¼ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯é€‚åº”å’Œæ ‡ç­¾æ•ˆç‡é«˜çš„æ¨¡å‹ï¼Œä»¥æ”¹å–„æ™ºèƒ½å£è…”å¥åº·æŠ¤ç†å¹¶è§£å†³å…¨çƒå£è…”å¥åº·æŠ¤ç†ä¸­çš„å…³é”®å·®è·ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å£è…”é¢Œé¢æ”¾å°„å­¦åœ¨ç‰™ç§‘å¥åº·æŠ¤ç†ä¸­æä¸ºé‡è¦ï¼Œä½†æ”¾å°„å›¾åƒè§£è¯»é¢ä¸´ä¸“ä¸šäººæ‰çŸ­ç¼ºçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰AIç³»ç»Ÿåœ¨ç‰™ç§‘åº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å•ä¸€æ¨¡å¼å…³æ³¨ç‚¹ã€ç‰¹å®šä»»åŠ¡è®¾è®¡å’Œä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>DentVFMæ˜¯é¦–æ¬¾ä¸ºç‰™ç§‘è®¾è®¡çš„è§†è§‰åŸºç¡€æ¨¡å‹å®¶æ—ï¼Œæ”¯æŒå¤šç§ç‰™ç§‘åº”ç”¨å¹¶å…·å¤‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>DentVFMåˆ©ç”¨å¤§å‹ç‰™ç§‘å½±åƒæ•°æ®é›†DentVistaè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«å¤šç§æ¨¡æ€çš„æ”¾å°„å›¾åƒã€‚</li>
<li>DentVFMåŒ…æ‹¬åŸºäºè§†è§‰è½¬æ¢å™¨æ¶æ„çš„äºŒç»´å’Œä¸‰ç»´æ¨¡å‹å˜ä½“ã€‚</li>
<li>DentBenchå¡«è¡¥äº†ç‰™ç§‘æ™ºèƒ½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„ç©ºç™½ï¼Œæ¶µç›–å¤šä¸ªç‰™ç§‘é¢†åŸŸã€ç–¾ç—…ã€æˆåƒæ–¹å¼å’Œåœ°ç†åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-081c465c5cf95574b7e1c3d44818df90~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738021&auth_key=1760738021-0-0-a82092894ae8ec4228465269b9118879&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Invited-Paper-BitMedViT-Ternary-Quantized-Vision-Transformer-for-Medical-AI-Assistants-on-the-Edge"><a href="#Invited-Paper-BitMedViT-Ternary-Quantized-Vision-Transformer-for-Medical-AI-Assistants-on-the-Edge" class="headerlink" title="Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for   Medical AI Assistants on the Edge"></a>Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for   Medical AI Assistants on the Edge</h2><p><strong>Authors:Mikolaj Walczak, Uttej Kallakuri, Edward Humes, Xiaomin Lin, Tinoosh Mohsenin</strong></p>
<p>Vision Transformers (ViTs) have demonstrated strong capabilities in interpreting complex medical imaging data. However, their significant computational and memory demands pose challenges for deployment in real-time, resource-constrained mobile and wearable devices used in clinical environments. We introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI assistants that perform structured analysis of medical images directly on the edge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical imaging and com- bines a training procedure with multi-query attention, preserving stability under ternary weights with low-precision activations. Furthermore, BiTMedViT employs task-aware distillation from a high-capacity teacher to recover accuracy lost due to extreme quantization. Lastly, we also present a pipeline that maps the ternarized ViTs to a custom CUDA kernel for efficient memory bandwidth utilization and latency reduction on the Jetson Orin Nano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on MedMNIST across 12 datasets, while reducing model size by 43x, memory traffic by 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that of SOTA models at 183.62 GOPs&#x2F;J on the Orin Nano. Our results demonstrate a practical and scientifically grounded route for extreme-precision medical imaging ViTs deployable on the edge, narrowing the gap between algorithmic advances and deployable clinical tools. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰åœ¨è§£è¯»å¤æ‚åŒ»å­¦æˆåƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œä¸ºåœ¨èµ„æºå—é™çš„å®æ—¶ç§»åŠ¨å’Œå¯ç©¿æˆ´è®¾å¤‡çš„éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œè¿™äº›è®¾å¤‡åœ¨ä¸´åºŠç¯å¢ƒä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†BiTMedViTï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è¾¹ç¼˜ViTç±»åˆ«ï¼Œä½œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½åŠ©ç†ï¼Œç›´æ¥åœ¨è¾¹ç¼˜å¯¹åŒ»ç–—å›¾åƒè¿›è¡Œç»“æ„åŒ–åˆ†æã€‚BiTMedViTåˆ©ç”¨é’ˆå¯¹åŒ»å­¦æˆåƒé‡èº«å®šåˆ¶çš„ä¸‰å…ƒé‡åŒ–çº¿æ€§å±‚ï¼Œå¹¶ç»“åˆå¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„è®­ç»ƒè¿‡ç¨‹ï¼Œåœ¨ä¸‰å…ƒæƒé‡ä¸‹ä¿æŒä½ç²¾åº¦æ¿€æ´»çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒBiTMedViTè¿˜é‡‡ç”¨ä»»åŠ¡æ„ŸçŸ¥è’¸é¦æ³•ï¼Œä»é«˜æ€§èƒ½æ•™å¸ˆæ¨¡å‹ä¸­æ¢å¤å› æç«¯é‡åŒ–è€ŒæŸå¤±çš„å‡†ç¡®æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæµç¨‹ï¼Œå°†ä¸‰å…ƒViTæ˜ å°„åˆ°è‡ªå®šä¹‰CUDAå†…æ ¸ï¼Œä»¥åœ¨Jetson Orin Nanoä¸Šå®ç°é«˜æ•ˆçš„å†…å­˜å¸¦å®½åˆ©ç”¨å’Œå»¶è¿Ÿé™ä½ã€‚æœ€ç»ˆï¼ŒBiTMedViTåœ¨MedMNISTçš„12ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†86%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼ˆæœ€é«˜æ°´å¹³ä¸º89%ï¼‰ï¼ŒåŒæ—¶ç¼©å°äº†æ¨¡å‹å¤§å°43å€ï¼Œå†…å­˜æµé‡å‡å°‘39å€ï¼Œå¹¶åœ¨Orin Nanoä¸Šä»¥183.62 GOPs&#x2F;Jçš„èƒ½æ•ˆå®ç°äº†16.8æ¯«ç§’çš„æ¨ç†ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†åœ¨è¾¹ç¼˜éƒ¨ç½²æç«¯ç²¾åº¦åŒ»ç–—æˆåƒViTçš„å®é™…ä¸”ç§‘å­¦çš„é€”å¾„ï¼Œç¼©å°äº†ç®—æ³•è¿›æ­¥å’Œå¯éƒ¨ç½²ä¸´åºŠå·¥å…·ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13760v1">PDF</a> Accepted at 2025 IEEE&#x2F;ACM International Conf. on Computer-Aided   Design (ICCAD) Oct. 26-30 2025, Munich, DE</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—å½±åƒå¤„ç†çš„è¾¹ç¼˜äººå·¥æ™ºèƒ½åŠ©æ‰‹â€”â€”BiTMedViTçš„è®¾è®¡ä¸å®ç°ã€‚é’ˆå¯¹èµ„æºå—é™çš„ç¯å¢ƒå¦‚ç§»åŠ¨è®¾å¤‡æˆ–å¯ç©¿æˆ´è®¾å¤‡ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç‰¹åŒ–é‡åŒ–æŠ€æœ¯å’Œä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œå®ç°äº†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šç›´æ¥è¿›è¡ŒåŒ»ç–—å›¾åƒçš„ç»“æ„åŒ–åˆ†æã€‚åŒæ—¶ï¼Œé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥è’¸é¦æŠ€æœ¯æ¢å¤å› é‡åŒ–å¯¼è‡´çš„ç²¾åº¦æŸå¤±ï¼Œå¹¶é‡‡ç”¨CUDAå†…æ ¸æ˜ å°„æŠ€æœ¯æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡å¹¶é™ä½å»¶è¿Ÿã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•æ˜¾ç¤ºï¼ŒBiTMedViTè¾¾åˆ°äº†é«˜è¯Šæ–­å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ¨¡å‹å¤§å°ã€å†…å­˜æµé‡ï¼Œå®ç°äº†é«˜æ•ˆèƒ½æºåˆ©ç”¨å’Œå¿«é€Ÿæ¨ç†ã€‚è¿™ä¸€æŠ€æœ¯çš„æå‡ºï¼Œä¸ºå°†åŒ»ç–—å½±åƒå¤„ç†é¢†åŸŸçš„ç®—æ³•è¿›å±•åº”ç”¨äºä¸´åºŠå®è·µå¼€è¾Ÿäº†åˆ‡å®æœ‰æ•ˆçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BiTMedViTæ˜¯ä¸€ç§ç”¨äºåŒ»ç–—å½±åƒå¤„ç†çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ–°å‹è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚</li>
<li>å®ƒé‡‡ç”¨ç‰¹åŒ–é‡åŒ–æŠ€æœ¯æ¥å‡å°‘è®¡ç®—éœ€æ±‚å’Œå†…å­˜å ç”¨ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒã€‚</li>
<li>é€šè¿‡å¤šæŸ¥è¯¢æ³¨æ„è®­ç»ƒç¨‹åºå’Œä»»åŠ¡æ„ŸçŸ¥è’¸é¦æŠ€æœ¯ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>CUDAå†…æ ¸æ˜ å°„æŠ€æœ¯ç”¨äºæé«˜å†…å­˜æ•ˆç‡å’Œé™ä½æ¨ç†å»¶è¿Ÿã€‚</li>
<li>BiTMedViTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°é«˜è¯Šæ–­å‡†ç¡®ç‡ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°å’Œå†…å­˜æµé‡ï¼Œå…·æœ‰é«˜æ•ˆèƒ½æºåˆ©ç”¨å’Œå¿«é€Ÿæ¨ç†æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-02903238a8b9d3faf0d086d2fc94d028~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738030&auth_key=1760738030-0-0-0da96b4a79ab4cb353967a732bf5a024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdff02d32e4248eadcad19b0837bed00~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738037&auth_key=1760738037-0-0-b2132190384ea6a1abdc94296d09840b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-538c1272a74ad996d000725aee5a2cde~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738044&auth_key=1760738044-0-0-40ae5e6852569e39193c755ce7fafa17&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b2b886d59f8f1ac6535e3ab773949074~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738051&auth_key=1760738051-0-0-c3d423caab35b0db3d828e9c4fdeaa5d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f377a4b582b1c0feeade59db2f42ce60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738059&auth_key=1760738059-0-0-549ef873af7337251e83bdf09a06ef6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e51986f6a9bb1561de578cdea3b5f591~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738066&auth_key=1760738066-0-0-5b2b599d5de27c9c728eb8a611de029f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-Scale-High-Resolution-Logarithmic-Grapher-Module-for-Efficient-Vision-GNNs"><a href="#Multi-Scale-High-Resolution-Logarithmic-Grapher-Module-for-Efficient-Vision-GNNs" class="headerlink" title="Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient   Vision GNNs"></a>Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient   Vision GNNs</h2><p><strong>Authors:Mustafa Munir, Alex Zhang, Radu Marculescu</strong></p>
<p>Vision graph neural networks (ViG) have demonstrated promise in vision tasks as a competitive alternative to conventional convolutional neural nets (CNN) and transformers (ViTs); however, common graph construction methods, such as k-nearest neighbor (KNN), can be expensive on larger images. While methods such as Sparse Vision Graph Attention (SVGA) have shown promise, SVGAâ€™s fixed step scale can lead to over-squashing and missing multiple connections to gain the same information that could be gained from a long-range link. Through this observation, we propose a new graph construction method, Logarithmic Scalable Graph Construction (LSGC) to enhance performance by limiting the number of long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and high-resolution architectures, we introduce and apply a high-resolution branch and fuse features between our high-resolution and low-resolution branches for a multi-scale high-resolution Vision GNN network. Extensive experiments show that LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification and semantic segmentation tasks. Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3% reduction in GMACs. Our work shows that leveraging long-range links in graph construction for ViGs through our proposed LSGC can exceed the performance of current state-of-the-art ViGs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mmunir127/LogViG-Official">https://github.com/mmunir127/LogViG-Official</a>. </p>
<blockquote>
<p>è§†è§‰å›¾ç¥ç»ç½‘ç»œï¼ˆViGï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­ä½œä¸ºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜å‹å™¨ï¼ˆViTï¼‰çš„ç«äº‰æ›¿ä»£æ–¹æ¡ˆå·²ç»æ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¸¸è§çš„å›¾æ„å»ºæ–¹æ³•ï¼Œå¦‚k-æœ€è¿‘é‚»ï¼ˆKNNï¼‰ï¼Œåœ¨è¾ƒå¤§å›¾åƒä¸Šå¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚è™½ç„¶ç¨€ç–è§†è§‰å›¾æ³¨æ„åŠ›ï¼ˆSVGAï¼‰ç­‰æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†SVGAçš„å›ºå®šæ­¥é•¿å°ºåº¦å¯èƒ½å¯¼è‡´è¿‡åº¦å‹ç¼©å¹¶é”™è¿‡é€šè¿‡é•¿è·ç¦»é“¾æ¥å¯ä»¥è·å¾—çš„ä¿¡æ¯çš„å¤šä¸ªè¿æ¥ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å›¾æ„å»ºæ–¹æ³•â€”â€”å¯¹æ•°å¯ä¼¸ç¼©å›¾æ„å»ºï¼ˆLSGCï¼‰ï¼Œé€šè¿‡é™åˆ¶é•¿è·ç¦»é“¾æ¥çš„æ•°é‡æ¥æé«˜æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LogViGï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨LSGCçš„æ–°å‹æ··åˆCNN-GNNæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå—åˆ°å¤šå°ºåº¦å’Œé«˜åˆ†è¾¨ç‡æ¶æ„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥å¹¶åº”ç”¨äº†ä¸€ä¸ªé«˜åˆ†è¾¨ç‡åˆ†æ”¯ï¼Œå¹¶å°†é«˜åˆ†è¾¨ç‡å’Œ low-resolution åˆ†æ”¯ä¹‹é—´çš„ç‰¹å¾èåˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªå¤šå°ºåº¦é«˜åˆ†è¾¨ç‡è§†è§‰ GNN ç½‘ç»œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLogViG åœ¨å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ã€GMACs å’Œå‚æ•°æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„ ViGã€CNN å’Œ ViT æ¶æ„ã€‚æˆ‘ä»¬çš„æœ€å°æ¨¡å‹ Ti-LogViG åœ¨ ImageNet-1K ä¸Šçš„å¹³å‡ top-1 å‡†ç¡®ç‡ä¸º 79.9%ï¼Œæ ‡å‡†å·®ä¸º 0.2%ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯” Vision GNN é«˜ 1.7%ï¼ŒåŒæ—¶å‚æ•°å‡å°‘äº† 24.3%ï¼ŒGMACs å‡å°‘äº† 35.3%ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œé€šè¿‡æˆ‘ä»¬æå‡ºçš„ LSGC åˆ©ç”¨å›¾æ„å»ºä¸­çš„é•¿è·ç¦»é“¾æ¥ï¼Œå¯ä»¥è¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„ ViG çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/mmunir127/LogViG-Official">https://github.com/mmunir127/LogViG-Official</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13740v1">PDF</a> Published in the Proceedings of the Third Learning on Graphs   Conference (LoG 2024)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LogViGï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ··åˆCNN-GNNæ¨¡å‹ï¼Œå®ƒé€šè¿‡é‡‡ç”¨å¯¹æ•°å¯ä¼¸ç¼©å›¾æ„å»ºï¼ˆLSGCï¼‰æ–¹æ³•æ¥æé«˜æ€§èƒ½ã€‚LogViGåœ¨å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰çš„ViGã€CNNå’ŒViTæ¶æ„ã€‚å…¶ä¸­æœ€å°çš„æ¨¡å‹Ti-LogViGåœ¨ImageNet-1Kä¸Šçš„å¹³å‡top-1å‡†ç¡®ç‡è¾¾åˆ°äº†79.9%ï¼Œè¾ƒVision GNNæœ‰æ›´é«˜çš„å¹³å‡å‡†ç¡®ç‡å’Œæ˜¾è‘—çš„å‚æ•°ä¸è®¡ç®—é‡å‡å°‘ã€‚è¿™è¡¨æ˜åˆ©ç”¨é•¿ç¨‹é“¾æ¥çš„å›¾æ„å»ºæ–¹æ³•å¯ä»¥é€šè¿‡LSGCè¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„ViGæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LogViGæ˜¯ä¸€ä¸ªæ–°å‹çš„æ··åˆCNN-GNNæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>LogViGæå‡ºäº†ä¸€ç§æ–°çš„å›¾æ„å»ºæ–¹æ³•â€”â€”å¯¹æ•°å¯ä¼¸ç¼©å›¾æ„å»ºï¼ˆLSGCï¼‰ï¼Œä»¥é™åˆ¶é•¿ç¨‹é“¾æ¥çš„æ•°é‡ã€‚</li>
<li>LogViGåœ¨å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„ViGã€CNNå’ŒViTæ¶æ„ã€‚</li>
<li>Ti-LogViGæ¨¡å‹åœ¨ImageNet-1Kä¸Šçš„è¡¨ç°çªå‡ºï¼Œè¾¾åˆ°äº†79.9%çš„å¹³å‡top-1å‡†ç¡®ç‡ã€‚</li>
<li>Ti-LogViGè¾ƒVision GNNæœ‰æ›´é«˜çš„å¹³å‡å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°å’Œè®¡ç®—é‡ã€‚</li>
<li>LogViGçš„å·¥ä½œè¡¨æ˜ï¼Œé€šè¿‡LSGCåˆ©ç”¨é•¿ç¨‹é“¾æ¥å¯ä»¥è¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„ViGæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-de48248aa89d3ce0552bece5b9ff7c39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738074&auth_key=1760738074-0-0-2e368b35e521cc86244024474a4097cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4093e627ea621b9c9ad912d01c5247f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738082&auth_key=1760738082-0-0-0581e4e9a39ff7c1769476bf95f78f1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65299cc416f34502e4c2e434c66578bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738089&auth_key=1760738089-0-0-7aaee8c3bc65347adfbbe39097e9d570&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8706beb91bd0460ece97519bf8e6092b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738095&auth_key=1760738095-0-0-663d35f4f2777c7b44d92287b309d213&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f70ddd73c9d998c1497c3f468a01e2d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738102&auth_key=1760738102-0-0-b4a4257d575df2e6178dde8b017330cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a35717bb4d70e4f95de96c0c31496e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738108&auth_key=1760738108-0-0-5d112009a171849dbe9886a6f55dff92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Hybrid-Explanation-Guided-Learning-for-Transformer-Based-Chest-X-Ray-Diagnosis"><a href="#Hybrid-Explanation-Guided-Learning-for-Transformer-Based-Chest-X-Ray-Diagnosis" class="headerlink" title="Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray   Diagnosis"></a>Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray   Diagnosis</h2><p><strong>Authors:Shelley Zixin Shu, Haozhe Luo, Alexander Poellinger, Mauricio Reyes</strong></p>
<p>Transformer-based deep learning models have demonstrated exceptional performance in medical imaging by leveraging attention mechanisms for feature representation and interpretability. However, these models are prone to learning spurious correlations, leading to biases and limited generalization. While human-AI attention alignment can mitigate these issues, it often depends on costly manual supervision. In this work, we propose a Hybrid Explanation-Guided Learning (H-EGL) framework that combines self-supervised and human-guided constraints to enhance attention alignment and improve generalization. The self-supervised component of H-EGL leverages class-distinctive attention without relying on restrictive priors, promoting robustness and flexibility. We validate our approach on chest X-ray classification using the Vision Transformer (ViT), where H-EGL outperforms two state-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating superior classification accuracy and generalization capability. Additionally, it produces attention maps that are better aligned with human expertise. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ·±åº¦å­¦ä¹ æ¨¡å‹é€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾è¡¨ç¤ºå’Œè§£é‡Šï¼Œå·²åœ¨åŒ»å­¦å½±åƒé¢†åŸŸå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å®¹æ˜“å­¦ä¹ å¶ç„¶å…³è”ï¼Œå¯¼è‡´åè§å’Œæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶äººç±»-äººå·¥æ™ºèƒ½æ³¨æ„åŠ›å¯¹é½å¯ä»¥ç¼“å’Œè¿™äº›é—®é¢˜ï¼Œä½†å®ƒå¸¸å¸¸ä¾èµ–äºæ˜‚è´µçš„äººå·¥ç›‘ç£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ··åˆè§£é‡Šå¼•å¯¼å­¦ä¹ ï¼ˆH-EGLï¼‰æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è‡ªæˆ‘ç›‘ç£å’Œäººç±»å¼•å¯¼çº¦æŸï¼Œä»¥æé«˜æ³¨æ„åŠ›å¯¹é½å¹¶æ”¹å–„æ³›åŒ–èƒ½åŠ›ã€‚H-EGLçš„è‡ªæˆ‘ç›‘ç£éƒ¨åˆ†åˆ©ç”¨ç±»åŒºåˆ†æ³¨æ„åŠ›ï¼Œä¸ä¾èµ–é™åˆ¶æ€§å…ˆéªŒï¼Œä¿ƒè¿›ç¨³å¥æ€§å’Œçµæ´»æ€§ã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨Vision Transformerï¼ˆViTï¼‰è¿›è¡Œèƒ¸éƒ¨Xå°„çº¿åˆ†ç±»çš„ä»»åŠ¡ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒH-EGLè¡¨ç°ä¼˜äºä¸¤ç§æœ€å…ˆè¿›çš„è§£é‡Šå¼•å¯¼å­¦ä¹ æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´é«˜çš„åˆ†ç±»ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒäº§ç”Ÿçš„æ³¨æ„åŠ›å›¾ä¸ä¸“å®¶äººç±»æ›´åŠ å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12704v1">PDF</a> Accepted by iMIMIC at MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ··åˆè§£é‡Šå¼•å¯¼å­¦ä¹ ï¼ˆH-EGLï¼‰æ¡†æ¶ï¼Œç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ å’Œäººç±»å¼•å¯¼çº¦æŸï¼Œæé«˜æ³¨æ„åŠ›å¯¹é½å’Œæ”¹è¿›äº†æ¦‚æ‹¬èƒ½åŠ›ã€‚åœ¨åŸºäºVision Transformerçš„èƒ¸éƒ¨Xå…‰åˆ†ç±»ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¡¨ç°å‡ºä¼˜äºä¸¤ç§æœ€æ–°è§£é‡Šå¼•å¯¼å­¦ä¹ ï¼ˆEGLï¼‰æ–¹æ³•çš„åˆ†ç±»å‡†ç¡®æ€§å’Œæ¦‚æ‹¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨åŒ»ç–—æˆåƒé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†å­˜åœ¨å­¦ä¹ è™šå‡å…³è”çš„é—®é¢˜ï¼Œå¯¼è‡´åè§å’Œæœ‰é™çš„æ¦‚æ‹¬èƒ½åŠ›ã€‚</li>
<li>äººæœºæ³¨æ„åŠ›å¯¹é½å¯ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†éœ€è¦æ˜‚è´µçš„äººå·¥ç›‘ç£ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ··åˆè§£é‡Šå¼•å¯¼å­¦ä¹ ï¼ˆH-EGLï¼‰æ¡†æ¶ï¼Œç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ å’Œäººç±»å¼•å¯¼çº¦æŸï¼Œä»¥å¢å¼ºæ³¨æ„åŠ›å¯¹é½å’Œæé«˜æ¦‚æ‹¬èƒ½åŠ›ã€‚</li>
<li>H-EGLæ¡†æ¶åˆ©ç”¨ç±»åŒºåˆ«æ³¨æ„åŠ›ï¼Œä¸ä¾èµ–é™åˆ¶æ€§å…ˆéªŒï¼Œä¿ƒè¿›äº†ç¨³å¥æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>åœ¨èƒ¸éƒ¨Xå…‰åˆ†ç±»ä»»åŠ¡ä¸ŠéªŒè¯äº†H-EGLæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¼˜äºä¸¤ç§æœ€æ–°EGLæ–¹æ³•ã€‚</li>
<li>H-EGLæ¡†æ¶äº§ç”Ÿçš„æ³¨æ„åŠ›å›¾ä¸äººç±»ä¸“ä¸šçŸ¥è¯†æ›´å¯¹é½ã€‚</li>
<li>H-EGLä¸ºåŒ»ç–—æˆåƒé¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1e08fdd66fa8c56c98b9b561c595de94~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738115&auth_key=1760738115-0-0-2187a3b573b230c6f1d899b1321bd87d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bad3885e191a169c3b00580668d71b67~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738123&auth_key=1760738123-0-0-56af9cddc52f69253886260c66e535ff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hybrid-Vision-Transformer-and-Quantum-Convolutional-Neural-Network-for-Image-Classification"><a href="#Hybrid-Vision-Transformer-and-Quantum-Convolutional-Neural-Network-for-Image-Classification" class="headerlink" title="Hybrid Vision Transformer and Quantum Convolutional Neural Network for   Image Classification"></a>Hybrid Vision Transformer and Quantum Convolutional Neural Network for   Image Classification</h2><p><strong>Authors:Mingzhu Wang, Yun Shang</strong></p>
<p>Quantum machine learning (QML) holds promise for computational advantage, yet progress on real-world tasks is hindered by classical preprocessing and noisy devices. We introduce ViT-QCNN-FT, a hybrid framework that integrates a fine-tuned Vision Transformer with a quantum convolutional neural network (QCNN) to compress high-dimensional images into features suited for noisy intermediate-scale quantum (NISQ) devices. By systematically probing entanglement, we show that ansatzes with uniformly distributed entanglement entropy consistently deliver superior non-local feature fusion and state-of-the-art accuracy (99.77% on CIFAR-10). Surprisingly, quantum noise emerges as a double-edged factor: in some cases, it enhances accuracy (+2.71% under amplitude damping). Strikingly, substituting the QCNN with classical counterparts of equal parameter count leads to a dramatic 29.36% drop, providing unambiguous evidence of quantum advantage. Our study establishes a principled pathway for co-designing classical and quantum architectures, pointing toward practical QML capable of tackling complex, high-dimensional learning tasks. </p>
<blockquote>
<p>é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰åœ¨è®¡ç®—æ–¹é¢å…·æœ‰å·¨å¤§çš„ä¼˜åŠ¿ï¼Œç„¶è€Œåœ¨å®é™…ä»»åŠ¡ä¸Šçš„è¿›å±•å—åˆ°ç»å…¸é¢„å¤„ç†å’Œå™ªå£°è®¾å¤‡çš„å½±å“ã€‚æˆ‘ä»¬å¼•å…¥äº†ViT-QCNN-FTè¿™ä¸€æ··åˆæ¡†æ¶ï¼Œå®ƒå°†ç»è¿‡ç²¾ç»†è®­ç»ƒçš„è§†è§‰Transformerä¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆQCNNï¼‰ç›¸ç»“åˆï¼Œå°†é«˜ç»´å›¾åƒå‹ç¼©æˆé€‚åˆå™ªå£°ä¸­é—´è§„æ¨¡é‡å­ï¼ˆNISQï¼‰è®¾å¤‡çš„ç‰¹å¾ã€‚é€šè¿‡ç³»ç»Ÿåœ°æ¢æµ‹çº ç¼ å…³ç³»ï¼Œæˆ‘ä»¬å‘ç°å…·æœ‰å‡åŒ€åˆ†å¸ƒçš„çº ç¼ ç†µçš„å‡è®¾æ–¹æ¡ˆå§‹ç»ˆèƒ½æä¾›æ›´ä¼˜è¶Šçš„éå±€éƒ¨ç‰¹å¾èåˆå’Œæœ€æ–°æŠ€æœ¯å‡†ç¡®ç‡çš„å‡†ç¡®æ€§ï¼ˆåœ¨CIFAR-10ä¸Šè¾¾åˆ°99.77%ï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé‡å­å™ªå£°å‘ˆç°å‡ºä¸¤é¢æ€§ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒèƒ½æé«˜å‡†ç¡®æ€§ï¼ˆåœ¨æŒ¯å¹…é˜»å°¼ä¸‹æé«˜2.71%ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”¨å‚æ•°æ•°é‡ç›¸åŒçš„ç»å…¸ç½‘ç»œæ›¿ä»£QCNNä¼šå¯¼è‡´å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼ˆä¸‹é™29.36%ï¼‰ï¼Œè¿™ä¸ºé‡å­ä¼˜åŠ¿æä¾›äº†ç¡®å‡¿çš„è¯æ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå…±åŒè®¾è®¡ç»å…¸å’Œé‡å­æ¶æ„å»ºç«‹äº†åŸåˆ™æ€§é€”å¾„ï¼Œæœç€èƒ½å¤Ÿåº”å¯¹å¤æ‚ã€é«˜ç»´å­¦ä¹ ä»»åŠ¡çš„å®ç”¨QMLå‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12291v1">PDF</a> </p>
<p><strong>Summary</strong><br>é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰å…·æœ‰è®¡ç®—ä¼˜åŠ¿æ½œåŠ›ï¼Œä½†åœ¨ç°å®ä»»åŠ¡ä¸Šè¿›å±•å—åˆ°ç»å…¸é¢„å¤„ç†å’Œå™ªå£°è®¾å¤‡çš„é˜»ç¢ã€‚æœ¬ç ”ç©¶å¼•å…¥ViT-QCNN-FTæ··åˆæ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒè§†è§‰å˜å‹å™¨ä¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆQCNNï¼‰çš„ç»“åˆï¼Œå°†é«˜ç»´å›¾åƒå‹ç¼©æˆé€‚åˆå™ªå£°ä¸­é—´å°ºåº¦é‡å­ï¼ˆNISQï¼‰è®¾å¤‡çš„ç‰¹å¾ã€‚é€šè¿‡ç³»ç»Ÿåœ°æ¢æµ‹çº ç¼ ï¼Œå‘ç°å…·æœ‰å‡åŒ€åˆ†å¸ƒçº ç¼ ç†µçš„ansatzåœ¨æä¾›å“è¶Šçš„éå±€éƒ¨ç‰¹å¾èåˆå’Œæœ€æ–°æŠ€æœ¯å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¸€è‡´ï¼ˆåœ¨CIFAR-10ä¸Šä¸º99.77%ï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé‡å­å™ªå£°æˆä¸ºäº†ä¸€ä¸ªåŒåˆƒå‰‘ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒæé«˜äº†å‡†ç¡®æ€§ï¼ˆ+ 2.71%åœ¨æŒ¯å¹…é˜»å°¼ä¸‹ï¼‰ã€‚æ˜¾è‘—çš„æ˜¯ï¼Œç”¨å‚æ•°æ•°é‡ç›¸ç­‰çš„ç»å…¸ç½‘ç»œæ›¿ä»£QCNNä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™29.36%ï¼Œè¿™ä¸ºé‡å­ä¼˜åŠ¿æä¾›äº†æ˜ç¡®è¯æ®ã€‚æœ¬ç ”ç©¶ä¸ºå…±åŒè®¾è®¡ç»å…¸å’Œé‡å­æ¶æ„å»ºç«‹äº†åŸåˆ™æ€§é€”å¾„ï¼Œä¸ºèƒ½å¤Ÿåº”å¯¹å¤æ‚ã€é«˜ç»´å­¦ä¹ ä»»åŠ¡çš„å®ç”¨QMLæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å­æœºå™¨å­¦ä¹ é¢ä¸´é¢„å¤„ç†å’Œå™ªå£°é—®é¢˜ã€‚</li>
<li>ViT-QCNN-FTæ··åˆæ¡†æ¶ç»“åˆäº†è§†è§‰å˜å‹å™¨å’Œé‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼Œå‹ç¼©å›¾åƒä»¥é€‚åº”NISQè®¾å¤‡ã€‚</li>
<li>çº ç¼ ç†µçš„åˆ†å¸ƒå½±å“ç‰¹å¾èåˆå’Œå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé‡å­å™ªå£°å¯ä»¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>ç”¨ç»å…¸ç½‘ç»œæ›¿ä»£QCNNä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æ­¤ç ”ç©¶è¯æ˜äº†é‡å­ä¼˜åŠ¿çš„å­˜åœ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1d11305f8439b669734a57b3fad8811b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738130&auth_key=1760738130-0-0-aadff5e984d1f986021c7cfc41256c04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85b3126f3e4d8c14cccbeb1047147760~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738137&auth_key=1760738137-0-0-73b410b64a50a459b0e2ccbcfac14c92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d87a0f3ed45e028cbd03e1fe8684ab3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738144&auth_key=1760738144-0-0-307070a28dca47dadd391fff5d515c94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Chimera-State-Space-Models-Beyond-Sequences"><a href="#Chimera-State-Space-Models-Beyond-Sequences" class="headerlink" title="Chimera: State Space Models Beyond Sequences"></a>Chimera: State Space Models Beyond Sequences</h2><p><strong>Authors:Aakash Lahoti, Tanya Marwah, Ratish Puduppully, Albert Gu</strong></p>
<p>Transformer-based deep learning methods have become the standard approach for modeling diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires inductive biasesâ€“such as position embeddings in sequences and images, or random walks in graphsâ€“to incorporate topology. However, designing such task-specific biases requires significant effort and can introduce side effects that hinder generalization. We introduce Chimera, a unified model that directly incorporates data topology in a principled way, removing the need for domain-specific biases. The key idea is that state space modelsâ€“which naturally do not require position embeddingsâ€“can be generalized to capture any graph topology. Our experiments show that Chimera achieves strong performance across language, vision, and graph domains, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph Benchmark. We further propose algorithmic optimizations to improve Chimeraâ€™s efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a linear-time recurrence; (2) for general graphs, a simple mathematical relaxation achieves Transformerâ€™s quadratic complexity without domain-specific heuristics. These results validate Chimeraâ€™s core contribution and support the idea that data topology is a powerful inductive bias across modalities. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ·±åº¦å­¦ä¹ æ–¹æ³•å·²æˆä¸ºå¯¹åºåˆ—ã€å›¾åƒå’Œå›¾å½¢ç­‰å¤šæ ·åŒ–æ•°æ®è¿›è¡Œå»ºæ¨¡çš„æ ‡å‡†æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ•°æ®è§†ä¸ºæ— åºçš„å…ƒç´ é›†åˆã€‚è¿™å¿½ç•¥äº†æ•°æ®çš„é‚»åŸŸç»“æ„æˆ–å›¾å½¢æ‹“æ‰‘ç»“æ„ï¼Œéœ€è¦å½’çº³åè§ï¼Œä¾‹å¦‚åœ¨åºåˆ—å’Œå›¾åƒä¸­çš„ä½ç½®åµŒå…¥ï¼Œæˆ–åœ¨å›¾å½¢ä¸­çš„éšæœºæ¸¸èµ°ï¼Œä»¥èå…¥æ‹“æ‰‘ç»“æ„ã€‚ç„¶è€Œï¼Œè®¾è®¡è¿™æ ·çš„ç‰¹å®šä»»åŠ¡åè§éœ€è¦å¤§é‡çš„åŠªåŠ›ï¼Œå¹¶å¯èƒ½å¼•å…¥é˜»ç¢æ³›åŒ–çš„å‰¯ä½œç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†Chimeraï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œå®ƒä»¥æœ‰åŸåˆ™çš„æ–¹å¼ç›´æ¥èå…¥æ•°æ®æ‹“æ‰‘ç»“æ„ï¼Œæ¶ˆé™¤äº†å¯¹ç‰¹å®šé¢†åŸŸçš„åè§çš„éœ€æ±‚ã€‚å…³é”®çš„æƒ³æ³•æ˜¯çŠ¶æ€ç©ºé—´æ¨¡å‹â€”â€”å®ƒä»¬è‡ªç„¶ä¸éœ€è¦ä½ç½®åµŒå…¥â€”â€”å¯ä»¥è¢«æ¨å¹¿åˆ°æ•è·ä»»ä½•å›¾å½¢æ‹“æ‰‘ç»“æ„ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨è¯­è¨€ã€è§†è§‰å’Œå›¾å½¢é¢†åŸŸï¼ŒChimeraéƒ½å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶åœ¨GLUEä¸Šçš„è¡¨ç°ä¼˜äºBERT 0.7åˆ†ï¼Œåœ¨ImageNet-1kä¸Šçš„è¡¨ç°ä¼˜äºViT 2.6%ï¼Œå¹¶åœ¨é•¿è·ç¦»å›¾å½¢åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†æ‰€æœ‰åŸºçº¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ç®—æ³•ä¼˜åŒ–ä»¥æé«˜Chimeraçš„æ•ˆç‡ï¼šï¼ˆ1ï¼‰å¯¹äºæ— ç¯å›¾ï¼Œå¯ä»¥å°†Chimeraå®ç°ä¸ºçº¿æ€§æ—¶é—´é€’å½’ï¼›ï¼ˆ2ï¼‰å¯¹äºä¸€èˆ¬å›¾å½¢ï¼Œç®€å•çš„æ•°å­¦æ¾å¼›å¯ä»¥åœ¨ä¸éœ€è¦ç‰¹å®šé¢†åŸŸå¯å‘å¼çš„æƒ…å†µä¸‹å®ç°Transformerçš„äºŒæ¬¡å¤æ‚æ€§ã€‚è¿™äº›ç»“æœéªŒè¯äº†Chimeraçš„æ ¸å¿ƒè´¡çŒ®å¹¶æ”¯æŒè¿™ä¸€æƒ³æ³•ï¼šæ•°æ®æ‹“æ‰‘æ˜¯ä¸€ä¸ªå¼ºå¤§çš„è·¨æ¨¡æ€å½’çº³åè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12111v1">PDF</a> Published in TMLR (October 2025); 22 Pages, 6 Figures, 11 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Transformeræ·±åº¦å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å¿½ç•¥äº†æ•°æ®çš„æ‹“æ‰‘ç»“æ„è€Œéœ€è¦ä¾èµ–ç‰¹å®šçš„åç½®ã€‚ä½œè€…å¼•å…¥äº†æ–°çš„æ¨¡å‹ Chimeraï¼Œèƒ½å¤Ÿç›´æ¥æ•´åˆæ•°æ®æ‹“æ‰‘ç»“æ„ï¼Œå‡å°‘äº†å¯¹ç‰¹å®šé¢†åŸŸçš„åç½®éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§é¢†åŸŸï¼ˆå¦‚è¯­è¨€ã€å›¾åƒå’Œå›¾ï¼‰ä¸­ï¼ŒChimer éƒ½å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ä½œè€…è¿˜æå‡ºäº†ç®—æ³•ä¼˜åŒ–æªæ–½ä»¥æé«˜ Chimera çš„æ•ˆç‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ•°æ®æ‹“æ‰‘ç»“æ„æ˜¯ä¸€ä¸ªå¼ºå¤§çš„è·¨æ¨¡æ€åç½®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„å…³é”®è¦ç‚¹ï¼Œä»¥å­å¼¹å½¢å¼å‘ˆç°ï¼š</p>
<ol>
<li>Transformer-basedæ·±åº¦å­¦ä¹ æ–¹æ³•ä¾èµ–äºç‰¹å®šé¢†åŸŸåç½®æ¥è®¾è®¡ä»»åŠ¡ç‰¹å®šçš„ç‰¹å¾è¡¨è¾¾æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•éœ€è¦é€šè¿‡åµŒå…¥æ–¹å¼åº”å¯¹å¤æ‚æ‹“æ‰‘æ•°æ®é›†çš„å¤æ‚æ€§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨ä¸€å®šå±€é™æ€§å’Œå¯¹å¤§è§„æ¨¡æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›éœ€æ±‚çš„ç¼ºé™·ã€‚é‰´äºæ­¤èƒŒæ™¯ä¸‹è¯ç”Ÿäº†â€œè·¨è¶Šæ—¶ç©ºåŸŸçš„è¡¨è¾¾èƒ½åŠ›æ„å»ºé—®é¢˜â€å’Œå…³äºå¦‚ä½•é€šè¿‡æ”¹è¿›ç°æœ‰æŠ€æœ¯ä»¥ç¼“è§£ç°æœ‰å±€é™çš„è®¨è®ºå’Œè€ƒè™‘æˆä¸ºäº†å­¦ç•Œçš„é‡è¦è¯¾é¢˜ã€‚åŸºäºæ­¤è¯é¢˜æå‡ºçš„è®¨è®ºæ­ç¤ºäº†ç ”ç©¶äººå‘˜å¦‚ä½•è¿½æ±‚ç®€åŒ–è¿‡ç¨‹å¹¶å®ç°æ•ˆç‡ä¼˜åŒ–æå‡çš„æŒ‘æˆ˜å’Œå†³å¿ƒã€‚ç”±äºç®€åŒ–éœ€è¦ä¸“é—¨åŒ–çš„åè§æ–¹æ³•èå…¥ç°ä»£AIç³»ç»Ÿä¸­æ˜¯ä¸€å¤§éš¾ç‚¹ã€‚è¿™ä¹Ÿæš—ç¤ºç€è¡Œä¸šæ­£é¢ä¸´ç€è¯¸å¤šæœºé‡å’ŒæŒ‘æˆ˜å¹¶å¯»æ±‚å®ç°äººå·¥æ™ºèƒ½åœ¨ç°å®ç”Ÿæ´»ä¸­çš„åº”ç”¨å’Œæ¨å¹¿çš„æ–°æ–¹æ³•ã€‚å¼•å…¥æ–°å‹æ¨¡å‹å’Œç®—æ³•æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„å¿…ç»ä¹‹è·¯ã€‚å¼•å…¥äº†ä¸€ç§æ–°å‹ç»Ÿä¸€æ¨¡å‹Chimeræ­£æ˜¯å…¶ä¸­ä¹‹ä¸€â€”â€”å®ƒå¯ä»¥å‡å°‘é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åè§è®¾è®¡æŒ‘æˆ˜éœ€æ±‚å¹¶å…·æœ‰é€šç”¨æ€§å’Œæ½œåŠ›æ¨å¹¿ä½¿ç”¨é¢†åŸŸé™åˆ¶æœ€å°‘çš„æœ‰æ•ˆAIæ¨¡å‹å…·æœ‰ä¼˜åŠ¿ä¹‹ä¸€çš„æ˜¯èƒ½å¤Ÿæ¶ˆé™¤è®¾è®¡å¤æ‚ç‰¹å¾æå–è¿‡ç¨‹çš„éœ€æ±‚å¹¶ä¸”å¯èƒ½èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³ä¸åŒé¢†åŸŸé—®é¢˜æé«˜æ¨¡å‹æ•ˆç‡å¹¶è§£å†³å¤§è§„æ¨¡æ•°æ®é›†è®¡ç®—éœ€æ±‚æŒ‘æˆ˜åŒæ—¶ä¹Ÿæœ‰å¾…äºè¿›è¡Œè¿›ä¸€æ­¥çš„æŠ€æœ¯å¼€å‘ä»¥ä¾¿è¿›è¡Œè§„æ¨¡åŒ–åº”ç”¨å’Œåœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„é›†æˆæ€§èƒ½è¯„ä¼°å’Œæ£€éªŒå…³äºè¿™é¡¹ç ”ç©¶çš„é•¿è¿œå‘å±•é¢„è®¡ä¼šå¯¹å½“å‰ç ”ç©¶äº§ç”Ÿå½±å“å…·æœ‰ç ”ç©¶é¢†åŸŸçš„æˆ˜ç•¥æ„ä¹‰å’Œæ½œåœ¨çš„é¢ è¦†æ€§è´¡çŒ®æ˜¯æœªæ¥æ™ºèƒ½åŒ–ç”Ÿæ€ç³»ç»Ÿæ¨è¿›å’ŒæŠ€æœ¯å¼€å‘çš„é‡ä¸­ä¹‹é‡ç»è¿‡ä¸æ‡ˆåŠªåŠ›å¯å®ç°ç«¯åˆ°ç«¯çš„æ”¹è¿›è½åœ°å°†ä¸ºæ‰“é€ AIç»æµç°ä»£åŒ–ç¤¾ä¼šçš„æ¨åŠ¨åˆ›æ–°åˆ›æ–°å¼€å¯æ–°çš„å‘å±•å±€é¢å…¶è§£å†³æ–¹æ¡ˆä¹Ÿéœ€è¦æ ¹æ®ä¸åŒåº”ç”¨æƒ…å†µè¿›è¡Œé€‚å½“é€‚é…ä»¥è¾¾åˆ°åœ¨ä¸åŒè¡Œä¸šä¸­çš„æœ€ä½³æ•ˆæœæœªæ¥æœ‰æœ›é€šè¿‡æŠ€æœ¯çš„ä¸æ–­è¿­ä»£å’Œå‡çº§æ¨åŠ¨AIè¡Œä¸šçš„æŒç»­å‘å±•å’Œè¿›æ­¥å¹¶ä¸ºå®é™…åº”ç”¨é¢†åŸŸæä¾›é‡è¦çš„æ”¯æ’‘å’ŒæœåŠ¡æ¨å¹¿æ›´åŠ ä¾¿æ·çš„ç”Ÿæ´»æ¨¡å¼å’Œæ¨åŠ¨æ•´ä¸ªç¤¾ä¼šè¿ˆå‘æ›´é«˜å±‚æ¬¡æ™ºèƒ½åŒ–åº”ç”¨æ°´å¹³çš„æé«˜æœªæ¥çš„å‰æ™¯å°†ä¼šå˜å¾—æ›´åŠ å¹¿é˜”ä¹Ÿæ›´åŠ å€¼å¾—æˆ‘ä»¬æœŸå¾…å’Œæ€è€ƒå°†ä¼šå¯¹æœªæ¥äººå·¥æ™ºèƒ½çš„å‘å±•äº§ç”Ÿæ·±è¿œå½±å“ã€‚ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹ï¼š</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c98350aec633c793d3594a4dff6b054e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738151&auth_key=1760738151-0-0-50743c5f5d38a053c427d76bd36bea1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e85d2a8124c331fb722d9a21ee4f8546~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738158&auth_key=1760738158-0-0-d6eaee39bcfe1ab36b5b96cc4155f217&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Explainability-of-Vision-Transformers-in-Medical-Imaging"><a href="#Evaluating-the-Explainability-of-Vision-Transformers-in-Medical-Imaging" class="headerlink" title="Evaluating the Explainability of Vision Transformers in Medical Imaging"></a>Evaluating the Explainability of Vision Transformers in Medical Imaging</h2><p><strong>Authors:Leili Barekatain, Ben Glocker</strong></p>
<p>Understanding model decisions is crucial in medical imaging, where interpretability directly impacts clinical trust and adoption. Vision Transformers (ViTs) have demonstrated state-of-the-art performance in diagnostic imaging; however, their complex attention mechanisms pose challenges to explainability. This study evaluates the explainability of different Vision Transformer architectures and pre-training strategies - ViT, DeiT, DINO, and Swin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct both quantitative and qualitative analyses on two medical imaging tasks: peripheral blood cell classification and breast ultrasound image classification. Our findings indicate that DINO combined with Grad-CAM offers the most faithful and localized explanations across datasets. Grad-CAM consistently produces class-discriminative and spatially precise heatmaps, while Gradient Attention Rollout yields more scattered activations. Even in misclassification cases, DINO with Grad-CAM highlights clinically relevant morphological features that appear to have misled the model. By improving model transparency, this research supports the reliable and explainable integration of ViTs into critical medical diagnostic workflows. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œç†è§£æ¨¡å‹å†³ç­–è‡³å…³é‡è¦ï¼Œå› ä¸ºå¯è§£é‡Šæ€§ç›´æ¥å½±å“ä¸´åºŠä¿¡ä»»åº¦å’Œé‡‡ç”¨åº¦ã€‚è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰åœ¨è¯Šæ–­æˆåƒæ–¹é¢è¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼›ç„¶è€Œï¼Œå®ƒä»¬å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ç»™è§£é‡Šæ€§å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ¢¯åº¦æ³¨æ„åŠ›å±•å¼€ï¼ˆGradient Attention Rolloutï¼‰å’ŒGrad-CAMè¯„ä¼°äº†ä¸åŒè§†è§‰å˜å‹å™¨æ¶æ„å’Œé¢„è®­ç»ƒç­–ç•¥ï¼ˆåŒ…æ‹¬ViTã€DeiTã€DINOå’ŒSwin Transformerï¼‰çš„è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤é¡¹åŒ»å­¦æˆåƒä»»åŠ¡ä¸Šè¿›è¡Œäº†å®šé‡å’Œå®šæ€§åˆ†æï¼šå¤–å‘¨è¡€ç»†èƒåˆ†ç±»å’Œä¹³è…ºè¶…å£°å›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDINOä¸Grad-CAMç›¸ç»“åˆæä¾›äº†è·¨æ•°æ®é›†çš„æœ€ä¸ºçœŸå®å’Œå±€éƒ¨åŒ–çš„è§£é‡Šã€‚Grad-CAMæŒç»­äº§ç”Ÿç±»åˆ«åˆ¤åˆ«åŠ›å’Œç©ºé—´ç²¾ç¡®çš„çƒ­å›¾ï¼Œè€Œæ¢¯åº¦æ³¨æ„åŠ›å±•å¼€äº§ç”Ÿçš„æ¿€æ´»æ›´ä¸ºåˆ†æ•£ã€‚å³ä½¿åœ¨è¯¯åˆ†ç±»çš„æƒ…å†µä¸‹ï¼ŒDINOä¸Grad-CAMä¹Ÿèƒ½çªå‡ºæ˜¾ç¤ºä¸´åºŠä¸Šç›¸å…³çš„å½¢æ€ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¼¼ä¹è¯¯å¯¼äº†æ¨¡å‹ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„é€æ˜åº¦ï¼Œæœ¬ç ”ç©¶æ”¯æŒå°†ViTså¯é ä¸”å¯è§£é‡Šåœ°é›†æˆåˆ°å…³é”®çš„åŒ»å­¦è¯Šæ–­å·¥ä½œæµç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12021v1">PDF</a> Accepted at Workshop on Interpretability of Machine Intelligence in   Medical Image Computing at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Vision Transformerï¼ˆViTï¼‰åœ¨åŒ»å­¦æˆåƒä¸­çš„è§£é‡Šæ€§ï¼Œè¯„ä¼°äº†ä¸åŒViTæ¶æ„å’Œé¢„è®­ç»ƒç­–ç•¥ï¼ˆåŒ…æ‹¬ViTã€DeiTã€DINOå’ŒSwin Transformerï¼‰çš„è§£é‡Šæ€§ã€‚é€šè¿‡Gradient Attention Rolloutå’ŒGrad-CAMè¿›è¡Œå®šé‡å’Œå®šæ€§åˆ†æï¼Œå‘ç°DINOç»“åˆGrad-CAMèƒ½æä¾›æœ€å¿ å®å’Œå±€éƒ¨åŒ–çš„è§£é‡Šã€‚æ­¤ç ”ç©¶æé«˜äº†æ¨¡å‹çš„é€æ˜åº¦ï¼Œä¸ºViTå¯é åœ°èå…¥å…³é”®åŒ»ç–—è¯Šæ–­æµç¨‹æä¾›äº†æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨åŒ»å­¦æˆåƒä¸­å±•ç°å…ˆè¿›æ€§èƒ½ï¼Œä½†å…¶å¤æ‚æ³¨æ„åŠ›æœºåˆ¶å¯¹è§£é‡Šæ€§æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ViTã€DeiTã€DINOå’ŒSwin Transformerç­‰ä¸åŒVision Transformeræ¶æ„å’Œé¢„è®­ç»ƒç­–ç•¥çš„è§£é‡Šæ€§ã€‚</li>
<li>é‡‡ç”¨Gradient Attention Rolloutå’ŒGrad-CAMè¿›è¡Œå®šé‡å’Œå®šæ€§åˆ†æã€‚</li>
<li>DINOç»“åˆGrad-CAMæä¾›è·¨æ•°æ®é›†æœ€å¿ å®å’Œå±€éƒ¨åŒ–çš„è§£é‡Šã€‚</li>
<li>Grad-CAMèƒ½ç”Ÿæˆç±»åˆ«åˆ¤åˆ«æ€§å’Œç©ºé—´ç²¾ç¡®æ€§çš„çƒ­å›¾ï¼Œè€ŒGradient Attention Rolloutäº§ç”Ÿçš„æ¿€æ´»æ›´åˆ†æ•£ã€‚</li>
<li>åœ¨è¯¯åˆ†ç±»æƒ…å†µä¸‹ï¼ŒDINOä¸Grad-CAMèƒ½çªå‡ºæ˜¾ç¤ºä¸´åºŠä¸Šç›¸å…³çš„å½¢æ€ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¼¼ä¹è¯¯å¯¼äº†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3aa5fcc104290e1312af22316a44d5a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738165&auth_key=1760738165-0-0-86fd45721a3eb7f81d9be7a1ac8d686c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c589ebe5dd5c052e835dfd3c267c0c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738172&auth_key=1760738172-0-0-c704f3093a90e96835adbffe01def2bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16ecba0e33851f66911af0af511cd3ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738179&auth_key=1760738179-0-0-4874db5ba95cb1eda387ce2b72c0d8cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7af5e3021c1a455a2f91f444bd570b6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738186&auth_key=1760738186-0-0-7ef2b4565d4f3f79a6e21505a34476cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f75dd62d54b02d9870f17e65a62b20df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738192&auth_key=1760738192-0-0-1dd63b3449abb9b3a5b4b306b41ee41e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SeeingSounds-Learning-Audio-to-Visual-Alignment-via-Text"><a href="#SeeingSounds-Learning-Audio-to-Visual-Alignment-via-Text" class="headerlink" title="SeeingSounds: Learning Audio-to-Visual Alignment via Text"></a>SeeingSounds: Learning Audio-to-Visual Alignment via Text</h2><p><strong>Authors:Simone Carnemolla, Matteo Pennisi, Chiara Russo, Simone Palazzo, Daniela Giordano, Concetto Spampinato</strong></p>
<p>We introduce SeeingSounds, a lightweight and modular framework for audio-to-image generation that leverages the interplay between audio, language, and vision-without requiring any paired audio-visual data or training on visual generative models. Rather than treating audio as a substitute for text or relying solely on audio-to-text mappings, our method performs dual alignment: audio is projected into a semantic language space via a frozen language encoder, and, contextually grounded into the visual domain using a vision-language model. This approach, inspired by cognitive neuroscience, reflects the natural cross-modal associations observed in human perception. The model operates on frozen diffusion backbones and trains only lightweight adapters, enabling efficient and scalable learning. Moreover, it supports fine-grained and interpretable control through procedural text prompt generation, where audio transformations (e.g., volume or pitch shifts) translate into descriptive prompts (e.g., â€œa distant thunderâ€) that guide visual outputs. Extensive experiments across standard benchmarks confirm that SeeingSounds outperforms existing methods in both zero-shot and supervised settings, establishing a new state of the art in controllable audio-to-visual generation. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SeeingSoundsï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºéŸ³é¢‘åˆ°å›¾åƒç”Ÿæˆçš„è½»é‡çº§å’Œæ¨¡å—åŒ–æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨éŸ³é¢‘ã€è¯­è¨€å’Œè§†è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œè€Œæ— éœ€ä»»ä½•é…å¯¹éŸ³è§†é¢‘æ•°æ®æˆ–åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯å°†éŸ³é¢‘è§†ä¸ºæ–‡æœ¬çš„æ›¿ä»£å“æˆ–ä»…ä¾èµ–äºéŸ³é¢‘åˆ°æ–‡æœ¬çš„æ˜ å°„ï¼Œè€Œæ˜¯æ‰§è¡ŒåŒé‡å¯¹é½ï¼šéŸ³é¢‘é€šè¿‡å†»ç»“çš„è¯­è¨€ç¼–ç å™¨æŠ•å°„åˆ°è¯­ä¹‰è¯­è¨€ç©ºé—´ï¼Œå¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¸Šå°†å…¶ä¸Šä¸‹æ–‡åŒ–ã€‚è¿™ç§æ–¹æ³•å—åˆ°è®¤çŸ¥ç¥ç»ç§‘å­¦çš„å¯å‘ï¼Œåæ˜ äº†äººç±»æ„ŸçŸ¥ä¸­è§‚å¯Ÿåˆ°çš„è‡ªç„¶è·¨æ¨¡æ€å…³è”ã€‚è¯¥æ¨¡å‹åœ¨å†»ç»“çš„æ‰©æ•£éª¨å¹²ç½‘ä¸Šè¿è¡Œï¼Œä»…è®­ç»ƒè½»é‡çº§é€‚é…å™¨ï¼Œå®ç°äº†é«˜æ•ˆå’Œå¯æ‰©å±•çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡ç¨‹åºæ€§æ–‡æœ¬æç¤ºç”Ÿæˆæ”¯æŒç²¾ç»†å’Œå¯è§£é‡Šçš„æ§åˆ¶ï¼Œå…¶ä¸­éŸ³é¢‘è½¬æ¢ï¼ˆä¾‹å¦‚éŸ³é‡æˆ–éŸ³è°ƒå˜åŒ–ï¼‰è½¬åŒ–ä¸ºæè¿°æ€§æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œè¿œå¤„é›·å£°â€ï¼‰ï¼Œå¼•å¯¼è§†è§‰è¾“å‡ºã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯å®ï¼ŒSeeingSoundsåœ¨é›¶æ ·æœ¬å’Œå—ç›‘ç£ç¯å¢ƒä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¯æ§çš„éŸ³é¢‘åˆ°è§†è§‰ç”Ÿæˆé¢†åŸŸæ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11738v1">PDF</a> accepted to ACM Multimedia Asia 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SeeingSoundsï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€æ¨¡å—åŒ–çš„éŸ³é¢‘åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨éŸ³é¢‘ã€è¯­è¨€å’Œè§†è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæ— éœ€ä»»ä½•é…å¯¹éŸ³è§†é¢‘æ•°æ®æˆ–è§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡å†°å†»è¯­è¨€ç¼–ç å™¨å°†éŸ³é¢‘æŠ•å½±åˆ°è¯­ä¹‰è¯­è¨€ç©ºé—´ï¼Œå¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å°†å…¶ä¸Šä¸‹æ–‡èå…¥è§†è§‰é¢†åŸŸã€‚è¯¥æ–¹æ³•å—åˆ°è®¤çŸ¥ç¥ç»ç§‘å­¦çš„å¯å‘ï¼Œåæ˜ äº†äººç±»æ„ŸçŸ¥ä¸­è§‚å¯Ÿåˆ°çš„è‡ªç„¶è·¨æ¨¡æ€å…³è”ã€‚è¯¥æ¨¡å‹åœ¨å†°å†»æ‰©æ•£ä¸»å¹²ä¸Šè¿è¡Œï¼Œåªè®­ç»ƒè½»é‡çº§é€‚é…å™¨ï¼Œå®ç°é«˜æ•ˆå’Œå¯æ‰©å±•çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒé€šè¿‡ç¨‹åºæ€§æ–‡æœ¬æç¤ºç”Ÿæˆè¿›è¡Œç²¾ç»†å’Œå¯è§£é‡Šçš„æ§åˆ¶ï¼Œå…¶ä¸­éŸ³é¢‘è½¬æ¢ï¼ˆä¾‹å¦‚éŸ³é‡æˆ–éŸ³è°ƒå˜åŒ–ï¼‰è½¬åŒ–ä¸ºæè¿°æ€§æç¤ºï¼Œå¼•å¯¼è§†è§‰è¾“å‡ºã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯å®ï¼ŒSeeingSoundsåœ¨é›¶æ ·æœ¬å’Œç›‘ç£è®¾ç½®ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¯æ§éŸ³é¢‘åˆ°è§†è§‰ç”Ÿæˆé¢†åŸŸæ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SeeingSoundsæ˜¯ä¸€ä¸ªè½»é‡çº§ã€æ¨¡å—åŒ–çš„éŸ³é¢‘åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨éŸ³é¢‘ã€è¯­è¨€å’Œè§†è§‰çš„ç›¸äº’ä½œç”¨ï¼Œæ— éœ€é…å¯¹éŸ³è§†é¢‘æ•°æ®æˆ–è§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>é€šè¿‡å†°å†»è¯­è¨€ç¼–ç å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹å®ç°éŸ³é¢‘çš„è¯­ä¹‰è¡¨ç¤ºå’Œè§†è§‰åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•å—åˆ°è®¤çŸ¥ç¥ç»ç§‘å­¦çš„å¯å‘ï¼Œåæ˜ äººç±»æ„ŸçŸ¥ä¸­çš„è‡ªç„¶è·¨æ¨¡æ€å…³è”ã€‚</li>
<li>æ¨¡å‹å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•çš„å­¦ä¹ ï¼Œåªéœ€è®­ç»ƒè½»é‡çº§é€‚é…å™¨ã€‚</li>
<li>æ”¯æŒé€šè¿‡ç¨‹åºæ€§æ–‡æœ¬æç¤ºè¿›è¡Œç²¾ç»†å’Œå¯è§£é‡Šçš„æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7bbdec426fe334caba12b5f34887ee73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738199&auth_key=1760738199-0-0-826af827136fc439f9bd841e57277240&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3408ce2a5a677fd234eca6f924dc5eab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738206&auth_key=1760738206-0-0-eebfddd1581ac4962a72597ddeac8480&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df65459bf0aa4f32054b71cf5fb7c1e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738212&auth_key=1760738212-0-0-0abf653b22d982848d36a3f8e27121a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be77f9c962ca99950f28da04e3120bbb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738219&auth_key=1760738219-0-0-f30a670b9a8de8bd86c0177a262d1c48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33844dd7e92f94bcf39c571d7f377dcc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738225&auth_key=1760738225-0-0-9e002285d3311ded856cb1af7b715efe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d947fb74ec4cbee848260142d1d9a20~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738232&auth_key=1760738232-0-0-28cfe9b107796148b4ae40d7086a4f82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-115a3057ffd90e21ffe13c73fe8cf65c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738239&auth_key=1760738239-0-0-526da33691ce057d117256b6d66cf75e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Validation-of-an-Artificial-Intelligence-Tool-for-the-Detection-of-Sperm-DNA-Fragmentation-Using-the-TUNEL-In-Situ-Hybridization-Assay"><a href="#Validation-of-an-Artificial-Intelligence-Tool-for-the-Detection-of-Sperm-DNA-Fragmentation-Using-the-TUNEL-In-Situ-Hybridization-Assay" class="headerlink" title="Validation of an Artificial Intelligence Tool for the Detection of Sperm   DNA Fragmentation Using the TUNEL In Situ Hybridization Assay"></a>Validation of an Artificial Intelligence Tool for the Detection of Sperm   DNA Fragmentation Using the TUNEL In Situ Hybridization Assay</h2><p><strong>Authors:Byron Alexander Jacobs, Aqeel Morris, Ifthakaar Shaik, Frando Lin</strong></p>
<p>Sperm DNA fragmentation (SDF) is a critical parameter in male fertility assessment that conventional semen analysis fails to evaluate. This study presents the validation of a novel artificial intelligence (AI) tool designed to detect SDF through digital analysis of phase contrast microscopy images, using the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL) assay as the gold standard reference. Utilising the established link between sperm morphology and DNA integrity, the present work proposes a morphology assisted ensemble AI model that combines image processing techniques with state-of-the-art transformer based machine learning models (GC-ViT) for the prediction of DNA fragmentation in sperm from phase contrast images. The ensemble model is benchmarked against a pure transformer <code>vision&#39; model as well as a </code>morphology-only&#96; model. Promising results show the proposed framework is able to achieve sensitivity of 60% and specificity of 75%. This non-destructive methodology represents a significant advancement in reproductive medicine by enabling real-time sperm selection based on DNA integrity for clinical diagnostic and therapeutic applications. </p>
<blockquote>
<p>ç²¾å­DNAç¢ç‰‡åŒ–ï¼ˆSDFï¼‰æ˜¯ç”·æ€§ç”Ÿè‚²èƒ½åŠ›è¯„ä¼°ä¸­çš„ä¸€ä¸ªå…³é”®å‚æ•°ï¼Œä¼ ç»Ÿçš„ç²¾æ¶²åˆ†ææ— æ³•è¿›è¡Œè¯„ä¼°ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·¥å…·çš„éªŒè¯ï¼Œè¯¥å·¥å…·é€šè¿‡ç›¸è¡¬æ˜¾å¾®é•œå›¾åƒçš„æ•°å­—åˆ†ææ¥æ£€æµ‹SDFï¼Œä»¥æœ«ç«¯è„±æ°§æ ¸è‹·é…¸è½¬ç§»é…¶dUTPç¼ºå£æœ«ç«¯æ ‡è®°ï¼ˆTUNELï¼‰æµ‹å®šæ³•ä½œä¸ºé‡‘æ ‡å‡†å‚è€ƒã€‚åˆ©ç”¨ç²¾å­å½¢æ€ä¸DNAå®Œæ•´æ€§çš„æ—¢å®šè”ç³»ï¼Œç›®å‰çš„å·¥ä½œæå‡ºäº†ä¸€ä¸ªå½¢æ€è¾…åŠ©é›†æˆAIæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å›¾åƒå¤„ç†æŠ€æœ¯ä¸æœ€æ–°çš„åŸºäºå˜å‹å™¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆGC-ViTï¼‰ï¼Œç”¨äºä»ç›¸è¡¬å›¾åƒé¢„æµ‹ç²¾å­çš„DNAç¢ç‰‡åŒ–ã€‚è¯¥é›†æˆæ¨¡å‹ä¸çº¯å˜å‹å™¨â€œè§†è§‰â€æ¨¡å‹ä»¥åŠâ€œä»…å½¢æ€â€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ä»¤äººé¼“èˆçš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°60%çš„çµæ•åº¦å’Œ75%çš„ç‰¹å¼‚åº¦ã€‚è¿™ç§éç ´åæ€§çš„æ–¹æ³•ä»£è¡¨äº†ç”Ÿæ®–åŒ»å­¦çš„ä¸€ä¸ªé‡å¤§è¿›æ­¥ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å®æ—¶ç­›é€‰åŸºäºDNAå®Œæ•´æ€§çš„ç²¾å­æ¥å®ç°ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11142v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>ç ”ç©¶éªŒè¯äº†æ–°å‹äººå·¥æ™ºèƒ½å·¥å…·è¯„ä¼°ç²¾å­DNAç¢ç‰‡åŒ–ï¼ˆSDFï¼‰çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æŠ€æœ¯é€šè¿‡ç›¸ä½å¯¹æ¯”æ˜¾å¾®é•œå›¾åƒçš„æ•°å­—åˆ†ææ¥æ£€æµ‹SDFï¼Œä»¥TUNELæ£€æµ‹ä¸ºé‡‘æ ‡å‡†ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå›¾åƒå¤„ç†å’ŒåŸºäºå˜å‹å™¨ï¼ˆGC-ViTï¼‰çš„æœ€æ–°æœºå™¨å­¦ä¹ æ¨¡å‹çš„å½¢æ€è¾…åŠ©é›†æˆAIæ¨¡å‹æ¥é¢„æµ‹ç²¾å­DNAç¢ç‰‡åŒ–ã€‚ä¸çº¯â€œè§†è§‰â€æ¨¡å‹å’Œä»…å½¢æ€æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥é›†æˆæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œæ•æ„Ÿåº¦å’Œç‰¹å¼‚åº¦åˆ†åˆ«è¾¾åˆ°äº†60%å’Œ75%ã€‚è¿™é¡¹éç ´åæ€§æ–¹æ³•ä»£è¡¨äº†ç”Ÿæ®–åŒ»å­¦é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§è¿›å±•ï¼Œèƒ½å¤ŸåŸºäºDNAå®Œæ•´æ€§è¿›è¡Œå®æ—¶ç²¾å­é€‰æ‹©ï¼Œå¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç²¾å­DNAç¢ç‰‡åŒ–ï¼ˆSDFï¼‰æ˜¯ç”·æ€§ç”Ÿè‚²èƒ½åŠ›è¯„ä¼°çš„é‡è¦å‚æ•°ï¼Œä¼ ç»Ÿç²¾æ¶²åˆ†ææ— æ³•è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶éªŒè¯äº†ä¸€ç§æ–°å‹äººå·¥æ™ºèƒ½å·¥å…·ï¼Œé€šè¿‡ç›¸ä½å¯¹æ¯”æ˜¾å¾®é•œå›¾åƒæ£€æµ‹SDFã€‚</li>
<li>é›†æˆAIæ¨¡å‹ç»“åˆäº†å›¾åƒå¤„ç†å’ŒåŸºäºå˜å‹å™¨ï¼ˆGC-ViTï¼‰çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé¢„æµ‹ç²¾å­DNAç¢ç‰‡åŒ–ã€‚</li>
<li>è¯¥æ¨¡å‹çš„æ•æ„Ÿåº¦å’Œç‰¹å¼‚åº¦åˆ†åˆ«ä¸º60%å’Œ75%ã€‚</li>
<li>æ­¤æ–¹æ³•å…·æœ‰éç ´åæ€§ï¼Œå¯¹ç”Ÿæ®–åŒ»å­¦é¢†åŸŸæœ‰é‡å¤§è¿›å±•ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯åŸºäºDNAå®Œæ•´æ€§è¿›è¡Œå®æ—¶ç²¾å­é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a25c6d8731bd956ec73aa60e608678e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738245&auth_key=1760738245-0-0-43cde1bae9613a8dd6ea4e81aa9ab1eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0fa8c3e5934fdc64412e25febf2192d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738252&auth_key=1760738252-0-0-d45b55eb9345e23082ee70879232cdd5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-47cd230affe04fca0d10f00ab30ba1f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738258&auth_key=1760738258-0-0-8b97386fce7b10b3c87fcfad50ec762d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab2de10c8bca665f2f6c3c20ac628df7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738265&auth_key=1760738265-0-0-f15c944a8917e3596f728643c903b662&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04cc5a5bb12514d46fe37f5cade8c945~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738274&auth_key=1760738274-0-0-2bc0ac04b78f9160b7f4fa77c1baae26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Scalable-Face-Security-Vision-Foundation-Model-for-Deepfake-Diffusion-and-Spoofing-Detection"><a href="#Scalable-Face-Security-Vision-Foundation-Model-for-Deepfake-Diffusion-and-Spoofing-Detection" class="headerlink" title="Scalable Face Security Vision Foundation Model for Deepfake, Diffusion,   and Spoofing Detection"></a>Scalable Face Security Vision Foundation Model for Deepfake, Diffusion,   and Spoofing Detection</h2><p><strong>Authors:Gaojian Wang, Feng Lin, Tong Wu, Zhisheng Yan, Kui Ren</strong></p>
<p>With abundant, unlabeled real faces, how can we learn robust and transferable facial representations to boost generalization across various face security tasks? We make the first attempt and propose FS-VFM, a scalable self-supervised pre-training framework, to learn fundamental representations of real face images. We introduce three learning objectives, namely 3C, that synergize masked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM to encode both local patterns and global semantics of real faces. Specifically, we formulate various facial masking strategies for MIM and devise a simple yet effective CRFR-P masking, which explicitly prompts the model to pursue meaningful intra-region Consistency and challenging inter-region Coherency. We present a reliable self-distillation mechanism that seamlessly couples MIM with ID to establish underlying local-to-global Correspondence. After pre-training, vanilla vision transformers (ViTs) serve as universal Vision Foundation Models for downstream Face Security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forensics. To efficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a lightweight plug-and-play bottleneck atop the frozen backbone with a novel real-anchor contrastive objective. Extensive experiments on 11 public benchmarks demonstrate that our FS-VFM consistently generalizes better than diverse VFMs, spanning natural and facial domains, fully, weakly, and self-supervised paradigms, small, base, and large ViT scales, and even outperforms SOTA task-specific methods, while FS-Adapter offers an excellent efficiency-performance trade-off. The code and models are available on <a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/fsvfm.html">https://fsfm-3c.github.io/fsvfm.html</a>. </p>
<blockquote>
<p>é¢å¯¹å¤§é‡æœªæ ‡è®°çš„çœŸå®äººè„¸ï¼Œæˆ‘ä»¬å¦‚ä½•å­¦ä¹ ç¨³å¥ä¸”å¯è¿ç§»çš„é¢éƒ¨è¡¨ç¤ºï¼Œä»¥ä¿ƒè¿›å„ç§é¢éƒ¨å®‰å…¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›å‘¢ï¼Ÿæˆ‘ä»¬é¦–æ¬¡å°è¯•å¹¶æå‡ºFS-VFMï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œå³3Cï¼Œå®ƒç»“åˆäº†æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹é‰´åˆ«ï¼ˆIDï¼‰ï¼Œä½¿FS-VFMèƒ½å¤Ÿç¼–ç çœŸå®äººè„¸çš„å±€éƒ¨æ¨¡å¼å’Œå…¨å±€è¯­ä¹‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºMIMåˆ¶å®šäº†å„ç§é¢éƒ¨æ©ç ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„CRFR-Pæ©ç ï¼Œå®ƒæ˜ç¡®åœ°æç¤ºæ¨¡å‹è¿½æ±‚åŒºåŸŸå†…æœ‰æ„ä¹‰çš„ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰å’ŒåŒºåŸŸé—´ååŒæ€§ï¼ˆCoherencyï¼‰ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¯é çš„è‡ªæˆ‘è’¸é¦æœºåˆ¶ï¼Œæ— ç¼ç»“åˆMIMå’ŒIDï¼Œå»ºç«‹åŸºæœ¬çš„å±€éƒ¨åˆ°å…¨å±€å¯¹åº”å…³ç³»ã€‚é¢„è®­ç»ƒåï¼Œæ™®é€šçš„è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰å¯ä½œä¸ºé¢å‘ä¸‹æ¸¸é¢éƒ¨å®‰å…¨ä»»åŠ¡çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼šè·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªä»¥åŠæœªè§æ‰©æ•£é¢éƒ¨å–è¯ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è¿ç§»é¢„è®­ç»ƒçš„FS-VFMï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºFS-Adapterï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å³æ’å³ç”¨ç“¶é¢ˆï¼Œä½äºå†»ç»“çš„ä¸»å¹²ä¹‹ä¸Šï¼Œå…·æœ‰æ–°å‹çœŸå®é”šç‚¹å¯¹æ¯”ç›®æ ‡ã€‚åœ¨11ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„FS-VFMå§‹ç»ˆæ¯”æ¶µç›–è‡ªç„¶å’Œé¢éƒ¨é¢†åŸŸçš„å„ç§VFMsæœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¶µç›–å…¨ç›‘ç£ã€å¼±ç›‘ç£å’Œè‡ªç›‘ç£èŒƒå¼ï¼Œå°å‹ã€åŸºç¡€å’Œå¤§å‹ViTè§„æ¨¡ï¼Œç”šè‡³è¶…è¶Šäº†ç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè€ŒFS-Adapteræä¾›äº†å“è¶Šçš„æ•ˆç‡æ€§èƒ½æƒè¡¡ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/fsvfm.html%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://fsfm-3c.github.io/fsvfm.htmlä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10663v1">PDF</a> 18 pages, 9 figures, project page:   <a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/fsvfm.html">https://fsfm-3c.github.io/fsvfm.html</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†FS-VFMæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ çœŸå®é¢éƒ¨å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºã€‚å¼•å…¥ä¸‰ç§å­¦ä¹ ç›®æ ‡ï¼Œå³3Cï¼ŒååŒæ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹é‰´åˆ«ï¼ˆIDï¼‰ï¼Œä½¿FS-VFMèƒ½å¤Ÿç¼–ç çœŸå®é¢éƒ¨çš„å±€éƒ¨æ¨¡å¼å’Œå…¨å±€è¯­ä¹‰ã€‚æ–‡ç« åˆ¶å®šäº†å„ç§é¢éƒ¨æ©æ¨¡ç­–ç•¥ï¼Œå¹¶æå‡ºäº†ç®€å•æœ‰æ•ˆçš„CRFR-Pæ©æ¨¡ï¼Œä¿ƒä½¿æ¨¡å‹è¿½æ±‚åŒºåŸŸå†…ä¸€è‡´æ€§åŠåŒºåŸŸé—´è¿è´¯æ€§ã€‚å»ºç«‹å¯é çš„è‡ªæˆ‘è’¸é¦æœºåˆ¶ï¼Œæ— ç¼ç»“åˆMIMå’ŒIDä»¥å»ºç«‹åŸºæœ¬çš„å±€éƒ¨åˆ°å…¨å±€å¯¹åº”å…³ç³»ã€‚é¢„è®­ç»ƒåï¼Œæ™®é€šè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰å¯ä½œä¸ºé¢å‘ä¸‹æ¸¸é¢éƒ¨å®‰å…¨ä»»åŠ¡çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ã€‚ä¸ºè¿›ä¸€æ­¥é«˜æ•ˆè½¬ç§»é¢„è®­ç»ƒçš„FS-VFMï¼Œæ–‡ç« è¿˜æå‡ºäº†FS-Adapterï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å³æ’å³ç”¨ç“¶é¢ˆï¼Œä½äºå†»ç»“çš„ä¸»å¹²ä¹‹ä¸Šï¼Œå…·æœ‰æ–°å‹çœŸå®é”šå¯¹æ¯”ç›®æ ‡ã€‚åœ¨11ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸è·¨è¶Šè‡ªç„¶å’Œé¢éƒ¨é¢†åŸŸçš„å„ç§VFMsç›¸æ¯”ï¼ŒFS-VFMå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”ä¼˜äºæœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ–¹æ³•ï¼Œè€ŒFS-Adapteråˆ™æä¾›äº†å‡ºè‰²çš„æ•ˆç‡ä¸æ€§èƒ½çš„å¹³è¡¡ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/fsvfm.html%E8%8E%B7%E5%8F%96%E3%80%82">https://fsfm-3c.github.io/fsvfm.htmlè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥FS-VFMæ¡†æ¶ï¼Œä¸€ç§å¯æ‰©å±•çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ çœŸå®é¢éƒ¨å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºã€‚</li>
<li>æå‡ºä¸‰ç§å­¦ä¹ ç›®æ ‡ï¼š3Cï¼ˆä¸€è‡´æ€§ã€è¿è´¯æ€§ã€å¯¹åº”æ€§ï¼‰ï¼Œç»“åˆMIMå’ŒIDè¿›è¡Œé¢éƒ¨è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>ä»‹ç»äº†å¤šç§é¢éƒ¨æ©æ¨¡ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯CRFR-Pæ©æ¨¡æ–¹æ³•ä»¥æé«˜å­¦ä¹ æ•ˆæœã€‚</li>
<li>å»ºç«‹è‡ªæˆ‘è’¸é¦æœºåˆ¶ä»¥ç»“åˆMIMå’ŒIDï¼Œå½¢æˆå±€éƒ¨åˆ°å…¨å±€çš„é¢éƒ¨è¡¨ç¤ºå¯¹åº”å…³ç³»ã€‚</li>
<li>é¢„è®­ç»ƒåï¼ŒViTså¯ä½œä¸ºä¸‹æ¸¸é¢éƒ¨å®‰å…¨ä»»åŠ¡çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æå‡ºFS-Adapterï¼Œä¸€ä¸ªè½»é‡çº§ç»“æ„ï¼Œç”¨äºé«˜æ•ˆè½¬ç§»é¢„è®­ç»ƒçš„FS-VFMçŸ¥è¯†åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bf2a2ca4252bda68102150e46b9dee33~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738282&auth_key=1760738282-0-0-7ffa989b93cec02328a900d5e5a786d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-786650c445a0dce28d7cc248c0fdec4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738289&auth_key=1760738289-0-0-5f975d8e806f7fdcb7d287c2a666d539&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-197b467112ff8463d1ed9cb0e6c40042~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738296&auth_key=1760738296-0-0-d0fe84ff588883fbdb20d55588d7860e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MARS-Sep-Multimodal-Aligned-Reinforced-Sound-Separation"><a href="#MARS-Sep-Multimodal-Aligned-Reinforced-Sound-Separation" class="headerlink" title="MARS-Sep: Multimodal-Aligned Reinforced Sound Separation"></a>MARS-Sep: Multimodal-Aligned Reinforced Sound Separation</h2><p><strong>Authors:Zihan Zhang, Xize Cheng, Zhennan Jiang, Dongjie Fu, Jingyuan Chen, Zhou Zhao, Tao Jin</strong></p>
<p>Universal sound separation faces a fundamental misalignment: models optimized for low-level signal metrics often produce semantically contaminated outputs, failing to suppress perceptually salient interference from acoustically similar sources. To bridge this gap, we introduce MARS-Sep, a reinforcement learning framework that reformulates separation as decision making. Instead of simply regressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy that is optimized by a clipped trust-region surrogate with entropy regularization and group-relative advantage normalization. Concretely, we sample masks from a frozen old policy, reconstruct waveforms, and update the current policy using clipped importance ratios-yielding substantially more stable and sample-efficient learning. Multimodal rewards, derived from an audio-text-vision encoder, directly incentivize semantic consistency with query prompts. We further propose a progressive alignment scheme to fine-tune this encoder, boosting its cross-modal discriminability and improving reward faithfulness. Extensive experiments on multiple benchmarks demonstrate consistent gains in Text-, Audio-, and Image-Queried separation, with notable improvements in signal metrics and semantic quality. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MARS-Sep">https://anonymous.4open.science/r/MARS-Sep</a>. Sound separation samples are available at <a target="_blank" rel="noopener" href="https://mars-sep.github.io/">https://mars-sep.github.io/</a>. </p>
<blockquote>
<p>é€šç”¨å£°éŸ³åˆ†ç¦»é¢ä¸´ä¸€ä¸ªåŸºæœ¬çš„ä¸åŒ¹é…é—®é¢˜ï¼šé’ˆå¯¹ä½çº§åˆ«ä¿¡å·æŒ‡æ ‡ä¼˜åŒ–çš„æ¨¡å‹é€šå¸¸ä¼šäº§ç”Ÿè¯­ä¹‰æ±¡æŸ“çš„è¾“å‡ºæ¥ï¼Œæ— æ³•æŠ‘åˆ¶å¬è§‰ä¸Šæ˜¾è‘—æ¥è‡ªå£°éŸ³ç›¸ä¼¼æºçš„å¹²æ‰°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MARS-Sepï¼Œè¿™æ˜¯ä¸€ç§å°†åˆ†ç¦»é‡æ–°å®šä¹‰ä¸ºå†³ç­–åˆ¶å®šçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚MARS-Sepä¸æ˜¯ç®€å•åœ°å›å½’åœ°é¢çœŸå®æ©è†œï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„å› å­åŒ–Betaæ©è†œç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡å¸¦æœ‰ç†µæ­£åˆ™åŒ–å’Œç»„ç›¸å¯¹ä¼˜åŠ¿å½’ä¸€åŒ–çš„å‰ªè¾‘ä¿¡ä»»åŒºåŸŸæ›¿ä»£ç‰©è¿›è¡Œä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»å†»ç»“çš„æ—§æ”¿ç­–ä¸­é‡‡æ ·æ©è†œï¼Œé‡å»ºæ³¢å½¢ï¼Œå¹¶ä½¿ç”¨å‰ªè¾‘åçš„é‡è¦æ€§æ¯”ç‡æ›´æ–°å½“å‰æ”¿ç­–ï¼Œä»è€Œå®ç°äº†æ›´åŠ ç¨³å®šå’Œé«˜æ•ˆçš„æ ·æœ¬å­¦ä¹ ã€‚ä»éŸ³é¢‘æ–‡æœ¬è§†è§‰ç¼–ç å™¨æ´¾ç”Ÿçš„å¤šæ¨¡å¼å¥–åŠ±ç›´æ¥æ¿€åŠ±ä¸æŸ¥è¯¢æç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ¸è¿›çš„å¯¹é½æ–¹æ¡ˆæ¥å¾®è°ƒæ­¤ç¼–ç å™¨ï¼Œæé«˜å…¶è·¨æ¨¡æ€çš„è¾¨åˆ«åŠ›å¹¶æ”¹å–„å¥–åŠ±çš„çœŸå®æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨æ–‡æœ¬ã€éŸ³é¢‘å’Œå›¾åƒæŸ¥è¯¢åˆ†ç¦»ä¸­å‡å–å¾—äº†æŒç»­çš„æ”¶ç›Šï¼Œåœ¨ä¿¡å·æŒ‡æ ‡å’Œè¯­ä¹‰è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MARS-Sep%E3%80%82%E5%A3%B0%E9%9F%B3%E5%88%86%E7%A6%BB%E6%A0%B7%E6%9C%AC%E5%8F%AF%E7%94%A8%E5%9C%A8https://mars-sep.github.io/%E3%80%82">https://anonymous.4open.science/r/MARS-Sepã€‚å£°éŸ³åˆ†ç¦»æ ·æœ¬å¯ç”¨åœ¨https://mars-sep.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10509v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ä¸ªåä¸ºMARS-Sepçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³é€šç”¨å£°éŸ³åˆ†ç¦»ä¸­çš„æ ¹æœ¬æ€§ä¸åŒ¹é…é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†å£°éŸ³åˆ†ç¦»é‡æ–°å®šä¹‰ä¸ºå†³ç­–åˆ¶å®šï¼Œè€Œéç®€å•åœ°å›å½’åœ°é¢çœŸå®æ©è†œã€‚å®ƒé€šè¿‡é‡‡ç”¨å› å­åŒ–çš„Betaæ©è†œç­–ç•¥ã€ä¼˜åŒ–çš„ä¿¡ä»»åŒºåŸŸæ›¿ä»£ç‰©ã€ç†µæ­£åˆ™åŒ–ä»¥åŠç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å½’ä¸€åŒ–ç­‰æŠ€æœ¯ï¼Œå¤§å¤§æé«˜äº†å­¦ä¹ å’Œæ ·æœ¬æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å¥–åŠ±æ–¹æ¡ˆï¼Œç›´æ¥æ¿€åŠ±æŸ¥è¯¢æç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ–‡æœ¬ã€éŸ³é¢‘å’Œå›¾åƒæŸ¥è¯¢åˆ†ç¦»æ–¹é¢å‡è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡å·æŒ‡æ ‡å’Œè¯­ä¹‰è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MARS-Sepæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³å£°éŸ³åˆ†ç¦»ä¸­çš„æ ¹æœ¬æ€§ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶å°†å£°éŸ³åˆ†ç¦»é‡æ–°å®šä¹‰ä¸ºå†³ç­–åˆ¶å®šï¼Œå¹¶é‡‡ç”¨å› å­åŒ–çš„Betaæ©è†œç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>MARS-Sepä½¿ç”¨ä¿¡ä»»åŒºåŸŸæ›¿ä»£ç‰©ã€ç†µæ­£åˆ™åŒ–å’Œç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å½’ä¸€åŒ–ç­‰æŠ€æœ¯ï¼Œæé«˜äº†å­¦ä¹ å’Œæ ·æœ¬æ•ˆç‡ã€‚</li>
<li>å¤šæ¨¡æ€å¥–åŠ±æ–¹æ¡ˆç›´æ¥æ¿€åŠ±æŸ¥è¯¢æç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºçš„æ¸è¿›å¯¹é½æ–¹æ¡ˆæé«˜äº†éŸ³é¢‘æ–‡æœ¬è§†è§‰ç¼–ç å™¨çš„è·¨æ¨¡æ€é‰´åˆ«åŠ›å’Œå¥–åŠ±çœŸå®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¡¨æ˜MARS-Sepåœ¨æ–‡æœ¬ã€éŸ³é¢‘å’Œå›¾åƒæŸ¥è¯¢åˆ†ç¦»æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c0186386a3d04d97eaadcbe475e11913~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738304&auth_key=1760738304-0-0-289c2def01d55c75d9329d0c211178a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d4a7e23bc385ff176efa36f16d1251f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738311&auth_key=1760738311-0-0-7e8bdf89ddc3bdceb772ce53c94dd8fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d6f6ed0d45070ab103237cd4d6279f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738318&auth_key=1760738318-0-0-e961c69a29523a388a80ec1f2e37e822&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cooperative-Pseudo-Labeling-for-Unsupervised-Federated-Classification"><a href="#Cooperative-Pseudo-Labeling-for-Unsupervised-Federated-Classification" class="headerlink" title="Cooperative Pseudo Labeling for Unsupervised Federated Classification"></a>Cooperative Pseudo Labeling for Unsupervised Federated Classification</h2><p><strong>Authors:Kuangpu Guo, Lijun Sheng, Yongcan Yu, Jian Liang, Zilei Wang, Ran He</strong></p>
<p>Unsupervised Federated Learning (UFL) aims to collaboratively train a global model across distributed clients without sharing data or accessing label information. Previous UFL works have predominantly focused on representation learning and clustering tasks. Recently, vision language models (e.g., CLIP) have gained significant attention for their powerful zero-shot prediction capabilities. Leveraging this advancement, classification problems that were previously infeasible under the UFL paradigm now present promising new opportunities, yet remain largely unexplored. In this paper, we extend UFL to the classification problem with CLIP for the first time and propose a novel method, \underline{\textbf{Fed}}erated \underline{\textbf{Co}}operative \underline{\textbf{P}}seudo \underline{\textbf{L}}abeling (\textbf{FedCoPL}). Specifically, clients estimate and upload their pseudo label distribution, and the server adjusts and redistributes them to avoid global imbalance among classes. Moreover, we introduce a partial prompt aggregation protocol for effective collaboration and personalization. In particular, visual prompts containing general image features are aggregated at the server, while text prompts encoding personalized knowledge are retained locally. Extensive experiments demonstrate the superior performance of our FedCoPL compared to baseline methods. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/krumpguo/FedCoPL%7D%7Bhttps://github.com/krumpguo/FedCoPL%7D">https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}</a>. </p>
<blockquote>
<p>æ— ç›‘ç£è”é‚¦å­¦ä¹ ï¼ˆUFLï¼‰æ—¨åœ¨åœ¨ä¸å…±äº«æ•°æ®æˆ–è®¿é—®æ ‡ç­¾ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒååŒè®­ç»ƒåˆ†å¸ƒå¼å®¢æˆ·ç«¯ä¸Šçš„å…¨å±€æ¨¡å‹ã€‚ä¹‹å‰çš„UFLå·¥ä½œä¸»è¦é›†ä¸­åœ¨è¡¨ç¤ºå­¦ä¹ å’Œèšç±»ä»»åŠ¡ä¸Šã€‚æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰ç”±äºå…¶å¼ºå¤§çš„é›¶æ ·æœ¬é¢„æµ‹èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚åˆ©ç”¨è¿™ä¸€è¿›å±•ï¼Œä¹‹å‰åœ¨UFLèŒƒå¼ä¸‹ä¸å¯è¡Œçš„åˆ†ç±»é—®é¢˜ç°åœ¨å‘ˆç°å‡ºæœ‰å¸Œæœ›çš„æ–°çš„æœºä¼šï¼Œä½†ä»æœ‰å¾…å¹¿æ³›æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†UFLæ‰©å±•åˆ°ä½¿ç”¨CLIPçš„åˆ†ç±»é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå³è”é‚¦åˆä½œä¼ªæ ‡ç­¾æ³•ï¼ˆFedCoPLï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œå®¢æˆ·ç«¯ä¼°è®¡å¹¶ä¸Šä¼ å…¶ä¼ªæ ‡ç­¾åˆ†å¸ƒï¼ŒæœåŠ¡å™¨è¿›è¡Œè°ƒæ•´å’Œé‡æ–°åˆ†é…ï¼Œä»¥é¿å…ç±»åˆ«ä¹‹é—´çš„å…¨å±€ä¸å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éƒ¨åˆ†æç¤ºèšåˆåè®®ï¼Œä»¥å®ç°æœ‰æ•ˆçš„åä½œå’Œä¸ªæ€§åŒ–ã€‚ç‰¹åˆ«æ˜¯ï¼ŒåŒ…å«ä¸€èˆ¬å›¾åƒç‰¹å¾çš„è§†è§‰æç¤ºåœ¨æœåŠ¡å™¨ä¸Šè¿›è¡Œèšåˆï¼Œè€Œç¼–ç ä¸ªæ€§åŒ–çŸ¥è¯†çš„æ–‡æœ¬æç¤ºåˆ™ä¿ç•™åœ¨æœ¬åœ°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„FedCoPLç›¸æ¯”åŸºçº¿æ–¹æ³•å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/krumpguo/FedCoPL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/krumpguo/FedCoPLä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10100v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>åŸºäºCLIPçš„è”é‚¦å­¦ä¹ åˆ†ç±»é—®é¢˜æ–°æ–¹æ³•â€”â€”FedCoPLã€‚è¯¥æ–¹æ³•åœ¨æ— ç›‘ç£è”é‚¦å­¦ä¹ ï¼ˆUFLï¼‰æ¡†æ¶ä¸‹å¤„ç†åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡ä¼ªæ ‡ç­¾åˆ†å¸ƒä¼°è®¡ä¸è°ƒæ•´ã€éƒ¨åˆ†æç¤ºèšåˆåè®®å®ç°æœ‰æ•ˆåä½œä¸ä¸ªæ€§åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UFLæ‰©å±•è‡³åˆ†ç±»é—®é¢˜ï¼šåˆ©ç”¨CLIPæ¨¡å‹å®ç°å¼ºå¤§çš„é›¶æ ·æœ¬é¢„æµ‹èƒ½åŠ›ï¼Œè§£å†³UFLåœ¨åˆ†ç±»é—®é¢˜ä¸Šçš„å±€é™æ€§ã€‚</li>
<li>æå‡ºFedCoPLæ–¹æ³•ï¼šåˆ©ç”¨ä¼ªæ ‡ç­¾åˆ†å¸ƒä¼°è®¡ä¸è°ƒæ•´ï¼Œé¿å…å…¨å±€ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>éƒ¨åˆ†æç¤ºèšåˆåè®®ï¼šå®ç°æœ‰æ•ˆåä½œå’Œä¸ªæ€§åŒ–ï¼Œè§†è§‰æç¤ºåœ¨æœåŠ¡å™¨èšåˆï¼Œæ–‡æœ¬æç¤ºä¿ç•™æœ¬åœ°ã€‚</li>
<li>FedCoPLæ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºåˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„åˆ†ç±»é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®éšç§ä¿æŠ¤éœ€æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸­ã€‚</li>
<li>ä»£ç å…¬å¼€å¯ç”¨ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ä¸è¿›ä¸€æ­¥æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ae9d54b5c31ea7c0eae72ef4c3dae187~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738325&auth_key=1760738325-0-0-89211b63ebc35623e8be9094e4f920d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7af07b6c3d0f8c4059710045a019abbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738332&auth_key=1760738332-0-0-29847f8371d9392b416e94b8365b13ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db36bc18febe0c6207783f83e899f0f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738338&auth_key=1760738338-0-0-c7b10febc3854e385a644f3bf994a4e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1311d864a72b3d3b07f7044dff66911a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738345&auth_key=1760738345-0-0-261465223a0885e20b40e6b0f11cbe06&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b49c1678b1248cefce86ddc61c4acd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738351&auth_key=1760738351-0-0-8ca42cde06b09d4a242a18d2feecb080&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Zero-shot-image-privacy-classification-with-Vision-Language-Models"><a href="#Zero-shot-image-privacy-classification-with-Vision-Language-Models" class="headerlink" title="Zero-shot image privacy classification with Vision-Language Models"></a>Zero-shot image privacy classification with Vision-Language Models</h2><p><strong>Authors:Alina Elena Baia, Alessio Xompero, Andrea Cavallaro</strong></p>
<p>While specialized learning-based models have historically dominated image privacy prediction, the current literature increasingly favours adopting large Vision-Language Models (VLMs) designed for generic tasks. This trend risks overlooking the performance ceiling set by purpose-built models due to a lack of systematic evaluation. To address this problem, we establish a zero-shot benchmark for image privacy classification, enabling a fair comparison. We evaluate the top-3 open-source VLMs, according to a privacy benchmark, using task-aligned prompts and we contrast their performance, efficiency, and robustness against established vision-only and multi-modal methods. Counter-intuitively, our results show that VLMs, despite their resource-intensive nature in terms of high parameter count and slower inference, currently lag behind specialized, smaller models in privacy prediction accuracy. We also find that VLMs exhibit higher robustness to image perturbations. </p>
<blockquote>
<p>åœ¨å†å²ä¸­ï¼ŒåŸºäºç‰¹å®šå­¦ä¹ çš„æ¨¡å‹ä¸€ç›´ä¸»å¯¼ç€å›¾åƒéšç§é¢„æµ‹é¢†åŸŸï¼Œä½†å½“å‰æ–‡çŒ®è¶Šæ¥è¶Šå€¾å‘äºé‡‡ç”¨é’ˆå¯¹é€šç”¨ä»»åŠ¡è®¾è®¡çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¿™ä¸€è¶‹åŠ¿å¯èƒ½ä¼šå› ç¼ºä¹ç³»ç»Ÿè¯„ä¼°è€Œå¿½ç•¥ä¸“é—¨æ„å»ºçš„æ¨¡å‹æ‰€è®¾å®šçš„æ€§èƒ½ä¸Šé™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºå›¾åƒéšç§åˆ†ç±»å»ºç«‹äº†é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ï¼Œä»¥å®ç°å…¬å¹³æ¯”è¾ƒã€‚æˆ‘ä»¬æ ¹æ®éšç§åŸºå‡†è¯„ä¼°äº†æ’åå‰ä¸‰çš„å¼€æºVLMsï¼Œé‡‡ç”¨ä»»åŠ¡å¯¹é½æç¤ºå¹¶å¯¹å…¶æ€§èƒ½ã€æ•ˆç‡å’Œç¨³å¥æ€§ä¸ç°æœ‰çš„ä»…è§†è§‰å’Œå¤šæ¨¡æ€æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœå‡ºäººæ„æ–™çš„æ˜¯ï¼Œå°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å‚æ•°æ•°é‡å’Œæ¨ç†é€Ÿåº¦æ–¹é¢èµ„æºå¯†é›†ï¼Œä½†åœ¨éšç§é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ï¼Œå®ƒä»¬ç›®å‰ä»è½åäºä¸“é—¨çš„è¾ƒå°æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å¯¹å›¾åƒæ‰°åŠ¨çš„ç¨³å¥æ€§æ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09253v1">PDF</a> 5 pages, 3 figures, 3 tables. This work has been submitted to the   ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>é‡‡ç”¨å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œå›¾åƒéšç§åˆ†ç±»çš„è¶‹åŠ¿é€æ¸æ˜æ˜¾ï¼Œä½†ç”±äºç¼ºä¹ç³»ç»Ÿè¯„ä¼°ï¼Œå®¹æ˜“å¿½è§†ä¸“ä¸šæ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•æ¥å…¬å¹³æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ï¼Œå¯¹ä¸‰ç§é¡¶çº§å¼€æºVLMsè¿›è¡Œè¯„ä¼°ï¼Œå¹¶å¯¹æ¯”å…¶æ€§èƒ½ã€æ•ˆç‡å’Œç¨³å¥æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡VLMså‚æ•°ä¼—å¤šã€æ¨ç†é€Ÿåº¦æ…¢ä¸”èµ„æºæ¶ˆè€—å¤§ï¼Œä½†åœ¨éšç§é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ç›®å‰ä»è½åäºä¸“ä¸šçš„å°å‹æ¨¡å‹ï¼Œä½†VLMså¯¹å›¾åƒæ‰°åŠ¨å…·æœ‰æ›´é«˜çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡çŒ®è¶Šæ¥è¶Šå€¾å‘äºé‡‡ç”¨ç”¨äºé€šç”¨ä»»åŠ¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œå›¾åƒéšç§åˆ†ç±»ã€‚</li>
<li>ç¼ºä¹ç³»ç»Ÿè¯„ä¼°å¯èƒ½å¯¼è‡´å¿½è§†ä¸“ä¸šæ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•æ¥å…¬å¹³æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¯¹ä¸‰ç§é¡¶çº§å¼€æºVLMsè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>VLMsåœ¨éšç§é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ç›®å‰ä»è½åäºä¸“ä¸šçš„å°å‹æ¨¡å‹ã€‚</li>
<li>VLMså¯¹å›¾åƒæ‰°åŠ¨å…·æœ‰æ›´é«˜çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-90933f5cef6a39e30ee0c6743bbfdc03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738359&auth_key=1760738359-0-0-aae69ae510c1f953661ddeff939fea99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b63dd7fadf9c5403332c226cb12779ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738367&auth_key=1760738367-0-0-013fd5eab864981b5328a12978dad9e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-407d8442979f8d04c0d895819462ca09~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738374&auth_key=1760738374-0-0-4d9f0fd66e8757ed7f7c11c86c109a51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e92ea565ca438b9682c52c77f50f2b6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738381&auth_key=1760738381-0-0-9207c606a9282decfa06baca373bfd8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-850640bd77e45cbf4858574ba321d442~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738387&auth_key=1760738387-0-0-b515ece3e4937f0494214bd21cc2f25c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Modeling-Time-Lapse-Trajectories-to-Characterize-Cranberry-Growth"><a href="#Modeling-Time-Lapse-Trajectories-to-Characterize-Cranberry-Growth" class="headerlink" title="Modeling Time-Lapse Trajectories to Characterize Cranberry Growth"></a>Modeling Time-Lapse Trajectories to Characterize Cranberry Growth</h2><p><strong>Authors:Ronan John, Anis Chihoub, Ryan Meegan, Gina Sidelli, Jeffery Neyhart, Peter Oudemans, Kristin Dana</strong></p>
<p>Change monitoring is an essential task for cranberry farming as it provides both breeders and growers with the ability to analyze growth, predict yield, and make treatment decisions. However, this task is often done manually, requiring significant time on the part of a cranberry grower or breeder. Deep learning based change monitoring holds promise, despite the caveat of hard-to-interpret high dimensional features and hand-annotations for fine-tuning. To address this gap, we introduce a method for modeling crop growth based on fine-tuning vision transformers (ViTs) using a self-supervised approach that avoids tedious image annotations. We use a two-fold pretext task (time regression and class prediction) to learn a latent space for the time-lapse evolution of plant and fruit appearance. The resulting 2D temporal tracks provide an interpretable time-series model of crop growth that can be used to: 1) predict growth over time and 2) distinguish temporal differences of cranberry varieties. We also provide a novel time-lapse dataset of cranberry fruit featuring eight distinct varieties, observed 52 times over the growing season (span of around four months), annotated with information about fungicide application, yield, and rot. Our approach is general and can be applied to other crops and applications (code and dataset can be found at <a target="_blank" rel="noopener" href="https://github/">https://github</a>. com&#x2F;ronan-39&#x2F;tlt&#x2F;). </p>
<blockquote>
<p>ç›‘æµ‹å˜åŒ–å¯¹äºè“è“ç§æ¤æ¥è¯´æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒä¸ºè‚²ç§è€…å’Œç§æ¤è€…æä¾›äº†åˆ†æç”Ÿé•¿æƒ…å†µã€é¢„æµ‹äº§é‡å’Œåšå‡ºå¤„ç†å†³å®šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä»»åŠ¡é€šå¸¸æ˜¯é€šè¿‡äººå·¥å®Œæˆçš„ï¼Œéœ€è¦è“è“ç§æ¤è€…æˆ–è‚²ç§è€…èŠ±è´¹å¤§é‡æ—¶é—´ã€‚å°½ç®¡å­˜åœ¨éš¾ä»¥è§£é‡Šçš„é«˜ç»´ç‰¹å¾å’Œå¾®è°ƒæ—¶çš„æ‰‹å·¥æ³¨é‡Šè¿™ä¸€ä¸è¶³ä¹‹å¤„ï¼Œæ·±åº¦å­¦ä¹ åœ¨å˜åŒ–ç›‘æµ‹ä¸­å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¾®è°ƒè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„å†œä½œç‰©ç”Ÿé•¿å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡è‡ªç›‘ç£æ–¹å¼é¿å…ç¹ççš„å›¾åƒæ³¨é‡Šã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤å€çš„é¢„æ–‡æœ¬ä»»åŠ¡ï¼ˆæ—¶é—´å›å½’å’Œç±»åˆ«é¢„æµ‹ï¼‰æ¥å­¦ä¹ æ¤ç‰©å’Œæœå®å¤–è§‚æ—¶é—´æ¨ç§»æ¼”å˜çš„æ½œåœ¨ç©ºé—´ã€‚æ‰€å¾—çš„äºŒç»´æ—¶é—´è½¨è¿¹æä¾›äº†å†œä½œç‰©ç”Ÿé•¿çš„å¯è§£é‡Šæ—¶é—´åºåˆ—æ¨¡å‹ï¼Œå¯ç”¨äºï¼š1ï¼‰é¢„æµ‹ç”Ÿé•¿è¶‹åŠ¿ï¼›2ï¼‰åŒºåˆ†ä¸åŒå“ç§çš„è“è“çš„æ—¶é—´å·®å¼‚ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæ–°å‹çš„æ—¶é—´æ¨ç§»è“è“æœå®æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å…«ä¸ªä¸åŒå“ç§ï¼Œåœ¨ç”Ÿé•¿å­£èŠ‚ï¼ˆå¤§çº¦å››ä¸ªæœˆçš„æ—¶é—´è·¨åº¦ï¼‰å†…è§‚å¯Ÿäº†52æ¬¡ï¼Œå¹¶æ ‡æ³¨äº†æœ‰å…³æ€èŒå‰‚åº”ç”¨ã€äº§é‡å’Œè…çƒ‚çš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šç”¨çš„ï¼Œå¯åº”ç”¨äºå…¶ä»–å†œä½œç‰©å’Œåº”ç”¨ï¼ˆä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ronan-39/tlt/%E6%89%BE%E5%88%B0%EF%BC%89%E3%80%82">https://github.com/ronan-39/tlt/æ‰¾åˆ°ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08901v1">PDF</a> Accepted to ICCV Workshops 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å˜åŒ–ç›‘æµ‹åœ¨è“è“ç§æ¤ä¸­çš„åº”ç”¨å‰æ™¯ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§ä½¿ç”¨è‡ªç›‘ç£æ–¹æ³•å¾®è°ƒè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ¨¡å‹ï¼Œç”¨äºæ¨¡æ‹Ÿä½œç‰©ç”Ÿé•¿ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤é¡¹é¢„è®­ç»ƒä»»åŠ¡ï¼ˆæ—¶é—´å›å½’å’Œç±»åˆ«é¢„æµ‹ï¼‰å­¦ä¹ æ¤ç‰©å’Œæœå®å¤–è§‚çš„æ—¶é—´åºåˆ—å˜åŒ–çš„æ½œåœ¨ç©ºé—´ï¼Œå¯æä¾›å¯è§£é‡Šçš„ä½œç‰©ç”Ÿé•¿æ—¶é—´åºåˆ—æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ä½œç‰©ç”Ÿé•¿è¿‡ç¨‹å’ŒåŒºåˆ†ä¸åŒå“ç§çš„è“è“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªæ–°å‹çš„æ—¶é—´åºåˆ—è“è“æœå®æ•°æ®é›†ï¼ŒåŒ…å«å…«ç§ä¸åŒå“ç§åœ¨ç”Ÿé•¿å­£èŠ‚å†…çš„52æ¬¡è§‚æµ‹æ•°æ®ï¼Œå¹¶é™„å¸¦äº†å…³äºæ€èŒå‰‚åº”ç”¨ã€äº§é‡å’Œè…çƒ‚ç­‰ä¿¡æ¯çš„æ³¨é‡Šã€‚è¯¥ç ”ç©¶çš„æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºå…¶ä»–ä½œç‰©å’Œåº”ç”¨åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜åŒ–ç›‘æµ‹å¯¹äºè“è“ç§æ¤è‡³å…³é‡è¦ï¼Œä½†å½“å‰çš„æ‰‹åŠ¨æ–¹æ³•è€—æ—¶è€—åŠ›ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å˜åŒ–ç›‘æµ‹ä¸­æœ‰åº”ç”¨å‰æ™¯ï¼Œä½†å­˜åœ¨éš¾ä»¥è§£é‡Šçš„é«˜ç»´ç‰¹å¾å’Œç¹ççš„å›¾åƒæ³¨é‡Šé—®é¢˜ã€‚</li>
<li>ç ”ç©¶äººå‘˜å¼•å…¥äº†ä¸€ç§åŸºäºè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªç›‘ç£æ–¹æ³•ï¼Œé¿å…äº†ç¹ççš„å›¾åƒæ³¨é‡Šã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨ä¸¤é¡¹é¢„è®­ç»ƒä»»åŠ¡æ¥å­¦ä¹ æ—¶é—´åºåˆ—å˜åŒ–çš„æ½œåœ¨ç©ºé—´ï¼Œæä¾›å¯è§£é‡Šçš„ä½œç‰©ç”Ÿé•¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å¯é¢„æµ‹ä½œç‰©ç”Ÿé•¿è¿‡ç¨‹å¹¶åŒºåˆ†ä¸åŒå“ç§çš„è“è“ã€‚</li>
<li>ç ”ç©¶äººå‘˜è¿˜æä¾›äº†ä¸€ä¸ªè“è“æœå®çš„æ—¶é—´åºåˆ—æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§å“ç§å’Œä¸°å¯Œçš„æ³¨é‡Šä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-138d803706f620499831319fd30381dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738395&auth_key=1760738395-0-0-3a63ef4eba94739d5f90333f2a5868d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77b6c934a737fd2055649e0400730144~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738403&auth_key=1760738403-0-0-6008ca3ac1b700040242be5a47c8d90f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ebcf4c3029df614a4aa3d09716dfdeb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738409&auth_key=1760738409-0-0-921359cc80dbe33098b5e5ea07c60792&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-05b71d27c75aa5628eefc5e4c85bc7a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738416&auth_key=1760738416-0-0-65de869460dea4584555b6f7b4a2bdb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b47f420d6aef1db93ab92158599a043~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738423&auth_key=1760738423-0-0-fcc3bd354c825126456280c663b6641b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3dce6e59291b0f068ba887efa71b37f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738429&auth_key=1760738429-0-0-b384be430084c37115c8a6bc6cbc87d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f4b4b1974e9aa4683531666d34c43da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738436&auth_key=1760738436-0-0-0f0fb3336c0ef5bc3a9bc1a7cb4bfcbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1071f2fbf470be3ec95642eff1a231f5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738477&auth_key=1760738477-0-0-5c30401c3575a7be8e7bf3f3e28cd538&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation"><a href="#MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation" class="headerlink" title="MedDINOv3: How to adapt vision foundation models for medical image   segmentation?"></a>MedDINOv3: How to adapt vision foundation models for medical image   segmentation?</h2><p><strong>Authors:Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang</strong></p>
<p>Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3">https://github.com/ricklisz/MedDINOv3</a>. </p>
<blockquote>
<p>åœ¨CTå’ŒMRIæ‰«æä¸­å‡†ç¡®åœ°åˆ†å‰²å™¨å®˜å’Œè‚¿ç˜¤å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨åŒ–åˆ†å‰²çš„è¿›å±•ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»ç„¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼Œåœ¨ä¸åŒæ¨¡æ€å’Œæœºæ„ä¹‹é—´ç¼ºä¹é€šç”¨æ€§ã€‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨ç™¾äº¿çº§è‡ªç„¶å›¾åƒä¸Šçš„é¢„è®­ç»ƒæä¾›äº†å¼ºå¤§ä¸”å¯è¿ç§»çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°†å…¶é€‚åº”åŒ»å­¦æˆåƒé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¤§å¤šæ•°åŸºç¡€æ¨¡å‹çš„ViTä¸»å¹²åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œä¸å¦‚ä¸“ä¸šåŒ–çš„CNNï¼›ï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„å·¨å¤§é¢†åŸŸå·®è·é™åˆ¶äº†å¯è¿ç§»æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MedDINOv3ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå°†DINOv3é€‚åº”åŒ»å­¦åˆ†å‰²ã€‚æˆ‘ä»¬é¦–å…ˆå›é¡¾äº†æ™®é€šçš„ViTsï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¶æ„ï¼Œå…·æœ‰å¤šå°ºåº¦ä»¤ç‰Œèšåˆã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨CT-3Mä¸Šè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾é€‰çš„åŒ…å«387ä¸‡å¼ è½´å‘CTåˆ‡ç‰‡çš„é›†åˆï¼Œä½¿ç”¨å¤šé˜¶æ®µçš„DINOv3é…æ–¹æ¥å­¦ä¹ ç¨³å¥çš„å¯†é›†ç‰¹å¾ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œè¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ç»Ÿä¸€ä¸»å¹²çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ricklisz/MedDINOv3æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02379v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MedDINOv3æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å°†DINOv3é€‚åº”äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡é‡æ–°å®¡è§†æ™®é€šViTå¹¶è®¾è®¡å…·æœ‰å¤šå°ºåº¦ä»¤ç‰Œèšåˆçš„ç®€å•æœ‰æ•ˆæ¶æ„ï¼Œä»¥åŠä½¿ç”¨å¤šé˜¶æ®µDINOv3é…æ–¹åœ¨CT-3Mä¸Šè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œå­¦ä¹ ç¨³å¥çš„å¯†é›†ç‰¹å¾ï¼ŒMedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå±•ç¤ºäº†ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ç»Ÿä¸€èƒŒéª¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®è¿›è¡ŒCTå’ŒMRIæ‰«æä¸­çš„å™¨å®˜å’Œè‚¿ç˜¤åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²æ—¶ä»é¢ä¸´ç‰¹å®šä»»åŠ¡æŒ‘æˆ˜ï¼Œç¼ºä¹è·¨ä¸åŒæ¨¡æ€å’Œæœºæ„çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Vision foundation modelsï¼ˆFMsï¼‰é€šè¿‡åœ¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒä¸Šçš„é¢„è®­ç»ƒï¼Œæä¾›äº†å¼ºå¤§çš„å¯è¿ç§»è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>å°†è¿™äº›æ¨¡å‹åº”ç”¨äºåŒ»å­¦æˆåƒé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šViTä¸»å¹²åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä»è½åäºä¸“ä¸šCNNï¼›è‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒä¹‹é—´çš„åŸŸå·®è·é™åˆ¶äº†æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ã€‚</li>
<li>MedDINOv3æ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡è®¾è®¡ç®€å•æœ‰æ•ˆçš„æ¶æ„å’Œè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ¥é€‚åº”åŒ»å­¦åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>MedDINOv3æ¡†æ¶åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ff28c5cb1f2aef55e210576b226bdad3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738485&auth_key=1760738485-0-0-87d676fbb34523bf2596ce8a499456bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-69eb7b8f98c17f7f1a653c65dd53dc9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738492&auth_key=1760738492-0-0-d3f866a01d14054b19f4c780c6d344b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a4306bae32e38dc7777c065bb007ab5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738499&auth_key=1760738499-0-0-954e5b4a0869b42994dae8f41edf3021&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f3f2b82643e7fec0a1229c41ce5fb1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738540&auth_key=1760738540-0-0-b72b06625e7aec1c5e737db33bfdea13&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cfe3be72dd8124edfca2d135d61d1066~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738547&auth_key=1760738547-0-0-b307ac23f07f9b888de1b5d509e8076e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Automatic-Synthesis-of-High-Quality-Triplet-Data-for-Composed-Image-Retrieval"><a href="#Automatic-Synthesis-of-High-Quality-Triplet-Data-for-Composed-Image-Retrieval" class="headerlink" title="Automatic Synthesis of High-Quality Triplet Data for Composed Image   Retrieval"></a>Automatic Synthesis of High-Quality Triplet Data for Composed Image   Retrieval</h2><p><strong>Authors:Haiwen Li, Delong Liu, Zhaohui Hou, Zhicheng Zhao, Fei Su</strong></p>
<p>As a challenging vision-language (VL) task, Composed Image Retrieval (CIR) aims to retrieve target images using multimodal (image+text) queries. Although many existing CIR methods have attained promising performance, their reliance on costly, manually labeled triplets hinders scalability and zero-shot capability. To address this issue, we propose a scalable pipeline for automatic triplet generation, along with a fully synthetic dataset named Composed Image Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a large language model (LLM) to generate diverse prompts, controlling a text-to-image generative model to produce image pairs with identical elements in each pair, which are then filtered and reorganized to form the CIRHS dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a novel CIR framework, which can accomplish global alignment and local reasoning within a broader context, enabling the model to learn more robust and informative representations. By utilizing the synthetic CIRHS dataset, CoAlign achieves outstanding zero-shot performance on three commonly used benchmarks, demonstrating for the first time the feasibility of training CIR models on a fully synthetic dataset. Furthermore, under supervised training, our method outperforms all the state-of-the-art supervised CIR approaches, validating the effectiveness of our proposed retrieval framework. The code and the CIRHS dataset will be released soon. </p>
<blockquote>
<p>ä½œä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰è¯­è¨€ï¼ˆVLï¼‰ä»»åŠ¡ï¼Œç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰æ—¨åœ¨ä½¿ç”¨å¤šæ¨¡æ€ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰æŸ¥è¯¢æ¥æ£€ç´¢ç›®æ ‡å›¾åƒã€‚å°½ç®¡è®¸å¤šç°æœ‰çš„CIRæ–¹æ³•å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¯¹æˆæœ¬é«˜æ˜‚çš„æ‰‹åŠ¨æ ‡æ³¨ä¸‰å…ƒç»„çš„ä¾èµ–é˜»ç¢äº†å…¶å¯æ‰©å±•æ€§å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºè‡ªåŠ¨ä¸‰å…ƒç»„ç”Ÿæˆçš„å¯æ‰©å±•ç®¡é“ï¼Œä»¥åŠä¸€ä¸ªåä¸ºâ€œåŸºäºé«˜è´¨é‡åˆæˆä¸‰å…ƒç»„çš„ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRHSï¼‰â€çš„å®Œå…¨åˆæˆæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç®¡é“åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå„ç§æç¤ºï¼Œæ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥äº§ç”Ÿå…·æœ‰ç›¸åŒå…ƒç´ çš„å›¾åƒå¯¹ï¼Œç„¶åå¯¹å®ƒä»¬è¿›è¡Œè¿‡æ»¤å’Œé‡ç»„ä»¥å½¢æˆCIRHSæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ··åˆä¸Šä¸‹æ–‡å¯¹é½ï¼ˆCoAlignï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„CIRæ¡†æ¶ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„èƒŒæ™¯ä¸‹å®Œæˆå…¨å±€å¯¹é½å’Œå±€éƒ¨æ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´ç¨³å¥å’Œæ›´æœ‰ç”¨çš„è¡¨ç¤ºã€‚é€šè¿‡åˆ©ç”¨åˆæˆçš„CIRHSæ•°æ®é›†ï¼ŒCoAlignåœ¨ä¸‰ä¸ªå¸¸ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œé¦–æ¬¡è¯æ˜äº†åœ¨å®Œå…¨åˆæˆçš„æ•°æ®é›†ä¸Šè®­ç»ƒCIRæ¨¡å‹çš„å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œåœ¨ç›‘ç£è®­ç»ƒä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æ‰€æœ‰æœ€æ–°çš„ç›‘ç£CIRæ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€æå‡ºçš„æ£€ç´¢æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’ŒCIRHSæ•°æ®é›†å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05970v3">PDF</a> This paper was originally submitted to ACM MM 2025 on April 12, 2025</p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€é¡¹é’ˆå¯¹ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰çš„æŒ‘æˆ˜æ€§ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§å¯æ‰©å±•åˆ°å¤§è§„æ¨¡æ•°æ®çš„è‡ªåŠ¨ä¸‰å…ƒç»„ç”Ÿæˆç®¡é“ä»¥åŠä¸€ä¸ªåä¸ºCIRHSçš„å…¨åˆæˆæ•°æ®é›†ã€‚è¯¥ç®¡é“åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šç§æç¤ºï¼Œæ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä»¥äº§ç”Ÿå…·æœ‰ç›¸åŒå…ƒç´ çš„å›¾åƒå¯¹ï¼Œå¹¶é€šè¿‡è¿‡æ»¤å’Œé‡ç»„å½¢æˆCIRHSæ•°æ®é›†ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„CIRæ¡†æ¶Hybrid Contextual Alignmentï¼ˆCoAlignï¼‰ï¼Œèƒ½åœ¨æ›´å¹¿æ³›çš„èƒŒæ™¯ä¸‹è¿›è¡Œå…¨å±€å¯¹é½å’Œå±€éƒ¨æ¨ç†ï¼Œä½¿æ¨¡å‹å­¦ä¹ æ›´ç¨³å¥å’Œæ›´å…·ä¿¡æ¯æ€§çš„è¡¨ç¤ºã€‚CoAlignåœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸Šé¦–æ¬¡å®ç°äº†å…¨åˆæˆæ•°æ®é›†è®­ç»ƒCIRæ¨¡å‹çš„å¯è¡Œæ€§ï¼Œå¹¶åœ¨ç›‘ç£è®­ç»ƒä¸‹ä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„CIRæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰æ˜¯ä¸€ä¸ªä½¿ç”¨å¤šæ¨¡æ€ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰æŸ¥è¯¢æ¥æ£€ç´¢ç›®æ ‡å›¾åƒçš„è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</li>
<li>å½“å‰CIRæ–¹æ³•ä¾èµ–æ˜‚è´µçš„æ‰‹åŠ¨æ ‡ç­¾æ•°æ®ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨ä¸‰å…ƒç»„ç”Ÿæˆç®¡é“å’Œåä¸ºCIRHSçš„å…¨åˆæˆæ•°æ®é›†ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæç¤ºï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹äº§ç”Ÿç›¸åŒå…ƒç´ çš„å›¾åƒå¯¹ã€‚</li>
<li>å¼•å…¥äº†Hybrid Contextual Alignmentï¼ˆCoAlignï¼‰æ¡†æ¶ï¼Œå®ç°å…¨å±€å¯¹é½å’Œå±€éƒ¨æ¨ç†ï¼Œæé«˜æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>CoAlignåœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é›¶æ ·æœ¬è®­ç»ƒçš„å¯è¡Œæ€§ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-400922f37e52854480e089d1da394e25~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738554&auth_key=1760738554-0-0-653716293ad0a7efc7fc63c844b9ec59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d632c13f988823112ab8b7931644ba6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738562&auth_key=1760738562-0-0-b522e254aa4201e5bb8ea6c6ec01bee1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-629f8cd70325ac841ea542cadfbd337a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738569&auth_key=1760738569-0-0-fa367a7a6665788e4210721d3ce68cd5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe0adff207f718255572ec3e8d888ddd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738576&auth_key=1760738576-0-0-75316b611d9276f393a229e79735fc66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TMT-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation"><a href="#TMT-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation" class="headerlink" title="TMT: Cross-domain Semantic Segmentation with Region-adaptive   Transferability Estimation"></a>TMT: Cross-domain Semantic Segmentation with Region-adaptive   Transferability Estimation</h2><p><strong>Authors:Enming Zhang, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Guan Wang, Yang Li, Xiaoping Zhang</strong></p>
<p>Recent advances in Vision Transformers (ViTs) have significantly advanced semantic segmentation performance. However, their adaptation to new target domains remains challenged by distribution shifts, which often disrupt global attention mechanisms. While existing global and patch-level adaptation methods offer some improvements, they overlook the spatially varying transferability inherent in different image regions. To address this, we propose the Transferable Mask Transformer (TMT), a region-adaptive framework designed to enhance cross-domain representation learning through transferability guidance. First, we dynamically partition the image into coherent regions, grouped by structural and semantic similarity, and estimates their domain transferability at a localized level. Then, we incorporate region-level transferability maps directly into the self-attention mechanism of ViTs, allowing the model to adaptively focus attention on areas with lower transferability and higher semantic uncertainty. Extensive experiments across 20 diverse cross-domain settings demonstrate that TMT not only mitigates the performance degradation typically associated with domain shift but also consistently outperforms existing approaches. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒVision Transformersï¼ˆViTsï¼‰çš„è¿›å±•å¤§å¤§æé«˜äº†è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€‚åº”æ–°ç›®æ ‡é¢†åŸŸæ—¶ä»ç„¶é¢ä¸´åˆ†å¸ƒè½¬ç§»çš„æŒ‘æˆ˜ï¼Œè¿™å¾€å¾€ä¼šç ´åå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ã€‚è™½ç„¶ç°æœ‰çš„å…¨å±€å’Œè¡¥ä¸çº§é€‚åº”æ–¹æ³•æä¾›äº†ä¸€äº›æ”¹è¿›ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†ä¸åŒå›¾åƒåŒºåŸŸå›ºæœ‰çš„ç©ºé—´å¯è½¬ç§»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Transferable Mask Transformerï¼ˆTMTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŒºåŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯è½¬ç§»æ€§æŒ‡å¯¼å¢å¼ºè·¨åŸŸè¡¨ç¤ºå­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®ç»“æ„å’Œè¯­ä¹‰ç›¸ä¼¼æ€§å°†å›¾åƒåŠ¨æ€åˆ’åˆ†ä¸ºè¿è´¯çš„åŒºåŸŸå¹¶å¯¹å…¶è¿›è¡Œåˆ†ç»„ï¼Œç„¶ååœ¨å±€éƒ¨çº§åˆ«ä¼°è®¡å®ƒä»¬çš„åŸŸå¯è½¬ç§»æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†åŒºåŸŸçº§å¯è½¬ç§»æ€§å›¾ç›´æ¥çº³å…¥ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨å¯è½¬ç§»æ€§è¾ƒä½ã€è¯­ä¹‰ä¸ç¡®å®šæ€§è¾ƒé«˜çš„åŒºåŸŸã€‚åœ¨20ä¸ªä¸åŒçš„è·¨åŸŸè®¾ç½®ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTMTä¸ä»…å‡è½»äº†ä¸åŸŸåç§»ç›¸å…³çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œè€Œä¸”æ€»ä½“ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05774v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision Transformersï¼ˆViTsï¼‰çš„æœ€æ–°è¿›å±•åœ¨è¯­ä¹‰åˆ†å‰²æ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ï¼Œä½†å…¶åœ¨é€‚åº”æ–°ç›®æ ‡é¢†åŸŸæ—¶ä»é¢ä¸´åˆ†å¸ƒè½¬ç§»çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Transferable Mask Transformerï¼ˆTMTï¼‰è¿™ä¸€åŒºåŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡è½¬ç§»æ€§æŒ‡å¯¼å¢å¼ºè·¨åŸŸè¡¨ç¤ºå­¦ä¹ ã€‚TMTé€šè¿‡åŠ¨æ€åˆ’åˆ†å›¾åƒåŒºåŸŸï¼Œå¹¶ä¼°è®¡å…¶åŸŸè½¬ç§»æ€§ï¼Œå°†åŒºåŸŸçº§è½¬ç§»å›¾ç›´æ¥èå…¥ViTsçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨è½¬ç§»æ€§è¾ƒä½ã€è¯­ä¹‰ä¸ç¡®å®šæ€§è¾ƒé«˜çš„åŒºåŸŸã€‚å®éªŒç»“æœè¯æ˜äº†TMTåœ¨è·¨åŸŸè®¾ç½®ä¸‹çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformersï¼ˆViTsï¼‰åœ¨è¯­ä¹‰åˆ†å‰²æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>è·¨åŸŸé€‚åº”é¢ä¸´åˆ†å¸ƒè½¬ç§»çš„æŒ‘æˆ˜ï¼Œå½±å“å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>ç°æœ‰å…¨å±€å’Œè¡¥ä¸çº§é€‚åº”æ–¹æ³•è™½æœ‰æ‰€æ”¹å–„ï¼Œä½†å¿½ç•¥äº†ä¸åŒå›¾åƒåŒºåŸŸå›ºæœ‰çš„ç©ºé—´å¯è½¬ç§»æ€§ã€‚</li>
<li>æå‡ºTransferable Mask Transformerï¼ˆTMTï¼‰æ¡†æ¶ï¼Œé€šè¿‡è½¬ç§»æ€§æŒ‡å¯¼å¢å¼ºè·¨åŸŸè¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>TMTé€šè¿‡åŠ¨æ€åˆ’åˆ†å›¾åƒåŒºåŸŸå¹¶ä¼°è®¡å…¶åŸŸè½¬ç§»æ€§ï¼Œå®ç°åŒºåŸŸè‡ªé€‚åº”ã€‚</li>
<li>å°†åŒºåŸŸçº§è½¬ç§»å›¾èå…¥ViTsçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæé«˜æ¨¡å‹åœ¨è½¬ç§»æ€§è¾ƒä½ã€è¯­ä¹‰ä¸ç¡®å®šæ€§è¾ƒé«˜åŒºåŸŸçš„å…³æ³¨åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c9de1c8b06b9ec12b130c98b84de6c9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738584&auth_key=1760738584-0-0-f473bf4cf9369e5fa4b1baef2aeeb73e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cb197da16f3082de8029136851de4d71~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738591&auth_key=1760738591-0-0-a1ec762c7b89c2c0c941fe07ed469399&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7d6bae20760de2efc99d4edb3073440~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738598&auth_key=1760738598-0-0-9abafbe4bb01f17f843776b986b8448d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0897eaf21eecd3ae0f112947a2440207~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738605&auth_key=1760738605-0-0-837a76d17960d1bd995389999916e834&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a13548c173150200e5f36c5bcb9f53c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738611&auth_key=1760738611-0-0-f032d57117b4ead00091829af688bc40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ChA-MAEViT-Unifying-Channel-Aware-Masked-Autoencoders-and-Multi-Channel-Vision-Transformers-for-Improved-Cross-Channel-Learning"><a href="#ChA-MAEViT-Unifying-Channel-Aware-Masked-Autoencoders-and-Multi-Channel-Vision-Transformers-for-Improved-Cross-Channel-Learning" class="headerlink" title="ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel   Vision Transformers for Improved Cross-Channel Learning"></a>ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel   Vision Transformers for Improved Cross-Channel Learning</h2><p><strong>Authors:Chau Pham, Juan C. Caicedo, Bryan A. Plummer</strong></p>
<p>Prior work using Masked Autoencoders (MAEs) typically relies on random patch masking based on the assumption that images have significant redundancies across different channels, allowing for the reconstruction of masked content using cross-channel correlations. However, this assumption does not hold in Multi-Channel Imaging (MCI), where channels may provide complementary information with minimal feature overlap. Thus, these MAEs primarily learn local structures within individual channels from patch reconstruction, failing to fully leverage cross-channel interactions and limiting their MCI effectiveness. In this paper, we present ChA-MAEViT, an MAE-based method that enhances feature learning across MCI channels via four key strategies: (1) dynamic channel-patch masking, which compels the model to reconstruct missing channels in addition to masked patches, thereby enhancing cross-channel dependencies and improving robustness to varying channel configurations; (2) memory tokens, which serve as long-term memory aids to promote information sharing across channels, addressing the challenges of reconstructing structurally diverse channels; (3) hybrid token fusion module, which merges fine-grained patch tokens with a global class token to capture richer representations; and (4) Channel-Aware Decoder, a lightweight decoder utilizes channel tokens to effectively reconstruct image patches. Experiments on satellite and microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, show that ChA-MAEViT significantly outperforms state-of-the-art MCI-ViTs by 3.0-21.5%, highlighting the importance of cross-channel interactions in MCI. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/chaudatascience/cha_mae_vit">https://github.com/chaudatascience/cha_mae_vit</a>. </p>
<blockquote>
<p>å…ˆå‰çš„å·¥ä½œé€šå¸¸ä½¿ç”¨åŸºäºå‡è®¾çš„æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEsï¼‰ï¼Œå³å›¾åƒåœ¨ä¸åŒé€šé“ä¹‹é—´å­˜åœ¨å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œå…è®¸åˆ©ç”¨è·¨é€šé“ç›¸å…³æ€§é‡å»ºæ©ç å†…å®¹ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾åœ¨å¤šé€šé“æˆåƒï¼ˆMCIï¼‰ä¸­å¹¶ä¸æˆç«‹ï¼Œå…¶ä¸­å„é€šé“å¯èƒ½æä¾›äº’è¡¥ä¿¡æ¯ï¼Œç‰¹å¾é‡å è¾ƒå°‘ã€‚å› æ­¤ï¼Œè¿™äº›MAEsä¸»è¦é€šè¿‡ä»è¡¥ä¸é‡å»ºä¸­å­¦ä¹ å•ä¸ªé€šé“å†…çš„å±€éƒ¨ç»“æ„ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è·¨é€šé“äº¤äº’ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨MCIä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMAEçš„æ–¹æ³•ChA-MAEViTï¼Œå®ƒé€šè¿‡å››ä¸ªå…³é”®ç­–ç•¥å¢å¼ºMCIé€šé“çš„ç‰¹å¾å­¦ä¹ ï¼šï¼ˆ1ï¼‰åŠ¨æ€é€šé“è¡¥ä¸æ©ç ï¼Œè¿«ä½¿æ¨¡å‹é™¤äº†æ©ç è¡¥ä¸å¤–è¿˜éœ€é‡å»ºç¼ºå¤±çš„é€šé“ï¼Œä»è€Œå¢å¼ºè·¨é€šé“ä¾èµ–æ€§å¹¶æé«˜å¯¹ä¸åŒé€šé“é…ç½®çš„ç¨³å¥æ€§ï¼›ï¼ˆ2ï¼‰å†…å­˜ä»¤ç‰Œå……å½“é•¿æœŸè®°å¿†è¾…åŠ©å·¥å…·ï¼Œä¿ƒè¿›è·¨é€šé“ä¿¡æ¯å…±äº«ï¼Œè§£å†³é‡å»ºç»“æ„å¤šæ ·é€šé“çš„æŒ‘æˆ˜ï¼›ï¼ˆ3ï¼‰æ··åˆä»¤ç‰Œèåˆæ¨¡å—ï¼Œå°†ç²¾ç»†ç²’åº¦çš„è¡¥ä¸ä»¤ç‰Œä¸å…¨å±€ç±»åˆ«ä»¤ç‰Œåˆå¹¶ï¼Œä»¥æ•è·æ›´ä¸°å¯Œçš„è¡¨ç¤ºï¼›ï¼ˆ4ï¼‰é€šé“æ„ŸçŸ¥è§£ç å™¨æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§£ç å™¨ï¼Œå®ƒåˆ©ç”¨é€šé“ä»¤ç‰Œæœ‰æ•ˆåœ°é‡å»ºå›¾åƒè¡¥ä¸ã€‚åœ¨å«æ˜Ÿå’Œæ˜¾å¾®é•œæ•°æ®é›†CHAMMIã€JUMP-CPå’ŒSo2Satä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒChA-MAEViTæ˜¾è‘—ä¼˜äºæœ€æ–°çš„MCI-ViTsï¼Œæ€§èƒ½æé«˜äº†3.0-21.5%ï¼Œçªæ˜¾äº†è·¨é€šé“äº¤äº’åœ¨MCIä¸­çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/chaudatascience/cha_mae_vit%E3%80%82">https://github.com/chaudatascience/cha_mae_vitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19331v2">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šé€šé“æˆåƒï¼ˆMCIï¼‰çš„ChA-MAEViTæ–¹æ³•ï¼Œé€šè¿‡å››ç§ç­–ç•¥å¢å¼ºè·¨é€šé“ç‰¹å¾å­¦ä¹ ï¼šåŠ¨æ€é€šé“è¡¥ä¸æ©æ¨¡æé«˜è·¨é€šé“ä¾èµ–æ€§å’Œé²æ£’æ€§ï¼›è®°å¿†ä»¤ç‰Œä¿ƒè¿›é€šé“é—´ä¿¡æ¯å…±äº«ï¼›æ··åˆä»¤ç‰Œèåˆæ¨¡å—æ•è·æ›´ä¸°å¯Œè¡¨ç¤ºï¼›ä»¥åŠé€šé“æ„ŸçŸ¥è§£ç å™¨æœ‰æ•ˆé‡å»ºå›¾åƒè¡¥ä¸ã€‚åœ¨å«æ˜Ÿå’Œæ˜¾å¾®é•œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒChA-MAEViTæ˜¾è‘—ä¼˜äºå…¶ä»–MCI-ViTsæ¨¡å‹ï¼Œçªæ˜¾è·¨é€šé“äº¤äº’çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChA-MAEViTæ˜¯ä¸€ç§åŸºäºMasked Autoencoderï¼ˆMAEï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤šé€šé“æˆåƒï¼ˆMCIï¼‰ä¸­çš„è·¨é€šé“ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>ä¼ ç»ŸMAEæ–¹æ³•ä¸»è¦å­¦ä¹ å•ä¸ªé€šé“å†…çš„å±€éƒ¨ç»“æ„ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è·¨é€šé“äº¤äº’ï¼Œåœ¨MCIä¸­çš„æœ‰æ•ˆæ€§å—é™ã€‚</li>
<li>ChA-MAEViTé€šè¿‡å››ç§å…³é”®ç­–ç•¥æé«˜MCIæ•ˆæœï¼šåŠ¨æ€é€šé“è¡¥ä¸æ©æ¨¡ã€è®°å¿†ä»¤ç‰Œã€æ··åˆä»¤ç‰Œèåˆæ¨¡å—å’Œé€šé“æ„ŸçŸ¥è§£ç å™¨ã€‚</li>
<li>åŠ¨æ€é€šé“è¡¥ä¸æ©æ¨¡å¢å¼ºè·¨é€šé“ä¾èµ–æ€§å’Œé²æ£’æ€§ï¼Œè¿«ä½¿æ¨¡å‹é‡å»ºç¼ºå¤±é€šé“ã€‚</li>
<li>è®°å¿†ä»¤ç‰Œä½œä¸ºé•¿æœŸè®°å¿†è¾…åŠ©ï¼Œä¿ƒè¿›é€šé“é—´ä¿¡æ¯å…±äº«ï¼Œè§£å†³ç»“æ„å¤šæ ·é€šé“çš„é‡æ„æŒ‘æˆ˜ã€‚</li>
<li>æ··åˆä»¤ç‰Œèåˆæ¨¡å—åˆå¹¶ç»†ç²’åº¦è¡¥ä¸ä»¤ç‰Œä¸å…¨å±€ç±»åˆ«ä»¤ç‰Œï¼Œä»¥æ•è·æ›´ä¸°å¯Œè¡¨ç¤ºã€‚</li>
<li>é€šé“æ„ŸçŸ¥è§£ç å™¨åˆ©ç”¨é€šé“ä»¤ç‰Œæœ‰æ•ˆåœ°é‡å»ºå›¾åƒè¡¥ä¸ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-186effed166808a651db1271807a6796~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738618&auth_key=1760738618-0-0-3ada1cf1a1dec5908ddcacf6365ec075&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a2f8c1ab8d1a907fa547bca98ac9a083~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738626&auth_key=1760738626-0-0-94684dc2fbed366e3afdfb14e79e0f7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16d27a004f5507ab589c45fdf102b3de~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738641&auth_key=1760738641-0-0-0e7e4e9555b984f145080e59d7f48f02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Isolated-Channel-Vision-Transformers-From-Single-Channel-Pretraining-to-Multi-Channel-Finetuning"><a href="#Isolated-Channel-Vision-Transformers-From-Single-Channel-Pretraining-to-Multi-Channel-Finetuning" class="headerlink" title="Isolated Channel Vision Transformers: From Single-Channel Pretraining to   Multi-Channel Finetuning"></a>Isolated Channel Vision Transformers: From Single-Channel Pretraining to   Multi-Channel Finetuning</h2><p><strong>Authors:Wenyi Lian, Patrick Micke, Joakim Lindblad, NataÅ¡a Sladoje</strong></p>
<p>Vision Transformers (ViTs) have achieved remarkable success in standard RGB image processing tasks. However, applying ViTs to multi-channel imaging (MCI) data, e.g., for medical and remote sensing applications, remains a challenge. In particular, MCI data often consist of layers acquired from different modalities. Directly training ViTs on such data can obscure complementary information and impair the performance. In this paper, we introduce a simple yet effective pretraining framework for large-scale MCI datasets. Our method, named Isolated Channel ViT (IC-ViT), patchifies image channels individually and thereby enables pretraining for multimodal multi-channel tasks. We show that this channel-wise patchifying is a key technique for MCI processing. More importantly, one can pretrain the IC-ViT on single channels and finetune it on downstream multi-channel datasets. This pretraining framework captures dependencies between patches as well as channels and produces robust feature representation. Experiments on various tasks and benchmarks, including JUMP-CP and CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging, show that the proposed IC-ViT delivers 4-14 percentage points of performance improvement over existing channel-adaptive approaches. Further, its efficient training makes it a suitable candidate for large-scale pretraining of foundation models on heterogeneous data. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/shermanlian/IC-ViT">https://github.com/shermanlian/IC-ViT</a>. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰åœ¨æ ‡å‡†RGBå›¾åƒå¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†ViTsåº”ç”¨äºå¤šé€šé“æˆåƒï¼ˆMCIï¼‰æ•°æ®ï¼Œä¾‹å¦‚åŒ»ç–—å’Œé¥æ„Ÿåº”ç”¨ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯ï¼ŒMCIæ•°æ®é€šå¸¸ç”±ä»ä¸åŒæ¨¡æ€è·å¾—çš„å±‚ç»„æˆã€‚ç›´æ¥åœ¨MCIæ•°æ®ä¸Šè®­ç»ƒViTså¯èƒ½ä¼šæ©ç›–äº’è¡¥ä¿¡æ¯å¹¶æŸå®³æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹å¤§è§„æ¨¡MCIæ•°æ®é›†å¼•å…¥äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„é¢„è®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åä¸ºéš”ç¦»é€šé“è§†è§‰Transformerï¼ˆIC-ViTï¼‰ï¼Œå®ƒå•ç‹¬åœ°å¯¹å›¾åƒé€šé“è¿›è¡Œåˆ’åˆ†ï¼Œä»è€Œå®ç°å¯¹å¤šæ¨¡æ€å¤šé€šé“ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚æˆ‘ä»¬è¯æ˜è¿™ç§é€šé“çº§çš„åˆ’åˆ†æ˜¯å¤šé€šé“æˆåƒå¤„ç†çš„å…³é”®æŠ€æœ¯ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå¯ä»¥åœ¨å•ä¸ªé€šé“ä¸Šé¢„è®­ç»ƒIC-ViTï¼Œå¹¶åœ¨ä¸‹æ¸¸å¤šé€šé“æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥é¢„è®­ç»ƒæ¡†æ¶æ•è·äº†è¡¥ä¸ä¹‹é—´çš„ä¾èµ–å…³ç³»ä»¥åŠé€šé“é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶äº§ç”Ÿäº†ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨å„ç§ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬ç”¨äºç»†èƒæ˜¾å¾®é•œæˆåƒçš„JUMP-CPå’ŒCHAMMIï¼Œä»¥åŠç”¨äºå«æ˜Ÿæˆåƒçš„So2Sat-LCZ42ï¼Œè¡¨æ˜æ‰€æå‡ºçš„IC-ViTåœ¨ç°æœ‰çš„é€šé“è‡ªé€‚åº”æ–¹æ³•ä¸Šå®ç°äº†4-14ä¸ªç™¾åˆ†ç‚¹çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå…¶é«˜æ•ˆçš„è®­ç»ƒä½¿å…¶æˆä¸ºåœ¨å¼‚è´¨æ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒçš„åˆé€‚å€™é€‰è€…ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shermanlian/IC-ViT">https://github.com/shermanlian/IC-ViT</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09826v2">PDF</a> Paper has been accepted by BMVC as an Oral presentation</p>
<p><strong>Summary</strong><br>å¤šé€šé“æˆåƒæ•°æ®åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œä½†å¯¹äºåœ¨å¤§å‹å¤šé€šé“æˆåƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„ViTæ¨¡å‹ä»ç„¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚è¯¥æ–‡æå‡ºä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶â€”â€”Isolated Channel Vision Transformerï¼ˆIC-ViTï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹å›¾åƒé€šé“è¿›è¡Œå•ç‹¬çš„åˆ’åˆ†ï¼ˆpatchifyingï¼‰ï¼Œå®ç°äº†å¤šæ¨¡æ€å¤šé€šé“ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ•è·patchä¹‹é—´çš„ä¾èµ–å…³ç³»ä»¥åŠé€šé“é—´çš„è”ç³»ï¼Œç”Ÿæˆç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºï¼Œä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViTs) åœ¨å¤„ç†å¤šé€šé“æˆåƒï¼ˆMCIï¼‰æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>IC-ViT æ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œé’ˆå¯¹å¤§å‹ MCI æ•°æ®é›†è®¾è®¡ã€‚</li>
<li>IC-ViT é€šè¿‡å•ç‹¬åˆ’åˆ†å›¾åƒé€šé“ï¼ˆpatchifyingï¼‰å®ç°å¤šæ¨¡æ€å¤šé€šé“ä»»åŠ¡çš„é¢„è®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒIC-ViT åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ¯”ç°æœ‰é€šé“è‡ªé€‚åº”æ–¹æ³•æ€§èƒ½æå‡4-14ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-94c1219f73951cc94787b672e5d6b586~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738648&auth_key=1760738648-0-0-8f1c17db8cfac3a77ea453ee75a2de2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-205713236fdc990e38a0af589f84c7d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738655&auth_key=1760738655-0-0-6b522d7b9fc6adfb7b3c7a8ff65a33b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8326526f7e6cd5a3a70fc1b74e22f486~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738662&auth_key=1760738662-0-0-0a48da6952adfc7f522303f5be40a6db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa43da1982a545d12dd6eb66effc1f39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738668&auth_key=1760738668-0-0-495a086590bd67cc1efa3ec354d1f3c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdea55b372be819a7a0a758cb739b141~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738675&auth_key=1760738675-0-0-f31fa3db05abbabe2031e7c1c76f53a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21c59e9e82b7eb2f46bb22ee066c8e06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738681&auth_key=1760738681-0-0-ffaf0bc397835a5e25c1798853fc43f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Open-Vocabulary-Multi-Label-Video-Classification"><a href="#Open-Vocabulary-Multi-Label-Video-Classification" class="headerlink" title="Open Vocabulary Multi-Label Video Classification"></a>Open Vocabulary Multi-Label Video Classification</h2><p><strong>Authors:Rohit Gupta, Mamshad Nayeem Rizve, Jayakrishnan Unnikrishnan, Ashish Tawari, Son Tran, Mubarak Shah, Benjamin Yao, Trishul Chilimbi</strong></p>
<p>Pre-trained vision-language models (VLMs) have enabled significant progress in open vocabulary computer vision tasks such as image classification, object detection and image segmentation. Some recent works have focused on extending VLMs to open vocabulary single label action classification in videos. However, previous methods fall short in holistic video understanding which requires the ability to simultaneously recognize multiple actions and entities e.g., objects in the video in an open vocabulary setting. We formulate this problem as open vocabulary multilabel video classification and propose a method to adapt a pre-trained VLM such as CLIP to solve this task. We leverage large language models (LLMs) to provide semantic guidance to the VLM about class labels to improve its open vocabulary performance with two key contributions. First, we propose an end-to-end trainable architecture that learns to prompt an LLM to generate soft attributes for the CLIP text-encoder to enable it to recognize novel classes. Second, we integrate a temporal modeling module into CLIPâ€™s vision encoder to effectively model the spatio-temporal dynamics of video concepts as well as propose a novel regularized finetuning technique to ensure strong open vocabulary classification performance in the video domain. Our extensive experimentation showcases the efficacy of our approach on multiple benchmark datasets. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¼€æ”¾è¯æ±‡è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ï¼‰æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ä¸€äº›æœ€æ–°ç ”ç©¶ç€çœ¼äºå°†VLMsæ‰©å±•åˆ°å¼€æ”¾è¯æ±‡çš„å•æ ‡ç­¾è§†é¢‘åŠ¨ä½œåˆ†ç±»ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ–¹æ³•åœ¨æ•´ä½“è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™éœ€è¦åŒæ—¶è¯†åˆ«å¤šä¸ªåŠ¨ä½œå’Œè§†é¢‘ä¸­çš„å®ä½“ï¼ˆä¾‹å¦‚ï¼Œåœ¨å¼€æ”¾è¯æ±‡è®¾ç½®ä¸­è¯†åˆ«è§†é¢‘ä¸­çš„å¯¹è±¡ï¼‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†é—®é¢˜å®šä½ä¸ºå¼€æ”¾è¯æ±‡çš„å¤šæ ‡ç­¾è§†é¢‘åˆ†ç±»ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä»¥é€‚åº”é¢„è®­ç»ƒçš„VLMï¼ˆå¦‚CLIPï¼‰æ¥è§£å†³æ­¤ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºVLMæä¾›å…³äºç±»åˆ«æ ‡ç­¾çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œä»¥æé«˜å…¶å¼€æ”¾è¯æ±‡æ€§èƒ½ï¼Œè¿™å¾—ç›Šäºä¸¤é¡¹å…³é”®è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¯è®­ç»ƒæ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥å­¦ä¹ æç¤ºLLMä¸ºCLIPæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆè½¯å±æ€§ï¼Œä»¥ä½¿å…¶èƒ½å¤Ÿè¯†åˆ«æ–°å‹ç±»åˆ«ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ—¶åºå»ºæ¨¡æ¨¡å—é›†æˆåˆ°CLIPçš„è§†è§‰ç¼–ç å™¨ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹è§†é¢‘æ¦‚å¿µçš„ç©ºé—´æ—¶é—´åŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹æ­£åˆ™å¾®è°ƒæŠ€æœ¯ï¼Œä»¥ç¡®ä¿åœ¨è§†é¢‘é¢†åŸŸçš„å¼€æ”¾è¯æ±‡åˆ†ç±»æ€§èƒ½å¼ºå¤§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09073v2">PDF</a> Accepted at ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¼€æ”¾è¯æ±‡è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨éœ€è¦åŒæ—¶è¯†åˆ«è§†é¢‘ä¸­çš„å¤šä¸ªåŠ¨ä½œå’Œå®ä½“ï¼ˆä¾‹å¦‚ï¼Œå¼€æ”¾è¯æ±‡è®¾ç½®ä¸­çš„è§†é¢‘ä¸­çš„å¯¹è±¡ï¼‰çš„æ•´ä½“è§†é¢‘ç†è§£æ–¹é¢ï¼Œä»¥å‰çš„æ–¹æ³•æœ‰æ‰€ä¸è¶³ã€‚æœ¬ç ”ç©¶å°†æ­¤é—®é¢˜è¡¨è¿°ä¸ºå¼€æ”¾è¯æ±‡çš„å¤šæ ‡ç­¾è§†é¢‘åˆ†ç±»é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€‚åº”é¢„è®­ç»ƒçš„VLMï¼ˆå¦‚CLIPï¼‰æ¥è§£å†³æ­¤ä»»åŠ¡çš„æ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºVLMæä¾›å…³äºç±»åˆ«æ ‡ç­¾çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œä»¥æé«˜å…¶å¼€æ”¾è¯æ±‡æ€§èƒ½ï¼Œå¹¶åšå‡ºäº†ä¸¤é¡¹é‡è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¯è®­ç»ƒæ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥å­¦ä¹ æç¤ºLLMä¸ºCLIPæ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆè½¯å±æ€§ï¼Œä»¥ä½¿å…¶èƒ½å¤Ÿè¯†åˆ«æ–°å‹ç±»åˆ«ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ—¶é—´å»ºæ¨¡æ¨¡å—é›†æˆåˆ°CLIPçš„è§†è§‰ç¼–ç å™¨ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹è§†é¢‘æ¦‚å¿µçš„ç©ºé—´æ—¶é—´åŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™å¾®è°ƒæŠ€æœ¯ï¼Œä»¥ç¡®ä¿åœ¨è§†é¢‘é¢†åŸŸçš„å¼ºå¼€æ”¾è¯æ±‡åˆ†ç±»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¼€æ”¾è¯æ±‡è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤šåŠ¨ä½œå’Œå®ä½“è¯†åˆ«çš„æ•´ä½“è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾è¯æ±‡ç¯å¢ƒä¸‹ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªæ–¹æ³•æ¥è§£å†³å¼€æ”¾è¯æ±‡çš„å¤šæ ‡ç­¾è§†é¢‘åˆ†ç±»é—®é¢˜ï¼Œå¹¶æˆåŠŸé€‚åº”äº†é¢„è®­ç»ƒçš„VLMå¦‚CLIPã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›è¯­ä¹‰æŒ‡å¯¼ï¼Œæé«˜æ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¯è®­ç»ƒæ¶æ„ï¼Œèƒ½å¤Ÿæç¤ºLLMç”Ÿæˆè½¯å±æ€§ç”¨äºè¯†åˆ«æ–°ç±»åˆ«ã€‚</li>
<li>åœ¨CLIPä¸­é›†æˆäº†æ—¶é—´å»ºæ¨¡æ¨¡å—æ¥æ•æ‰è§†é¢‘çš„ç©ºé—´æ—¶é—´åŠ¨æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.09073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4e825df320d371e05e7c2e58428c30df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738688&auth_key=1760738688-0-0-c3c89887b8fda71ce21dbcfd1e3ce39b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2647e0189b576ac64ee9e25051815328~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738696&auth_key=1760738696-0-0-301565fd4bad36b9370bb2a9bde21029&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3ca7e508dd09855cff3e2bb710641cf3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760738703&auth_key=1760738703-0-0-1f955eee061d14c43c15855f7b449da6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-18/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-9131e57d65e898131cf44f4154f5d4ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760827360&auth_key=1760827360-0-0-43a30a48882eaab0d7821289e670234e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  CoT-PL Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for   Open-Vocabulary Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-06e62343c630cc627d1133022c4bb17a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760736831&auth_key=1760736831-0-0-0a05db017bad519d502f9835cbd35a03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Shot2Tactic-Caption Multi-Scale Captioning of Badminton Videos for   Tactical Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
