<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  MetaScale Test-Time Scaling with Evolving Meta-Thoughts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ae4aa0d01a00b084fbf0a3221e0f44c7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-18-æ›´æ–°"><a href="#2025-03-18-æ›´æ–°" class="headerlink" title="2025-03-18 æ›´æ–°"></a>2025-03-18 æ›´æ–°</h1><h2 id="MetaScale-Test-Time-Scaling-with-Evolving-Meta-Thoughts"><a href="#MetaScale-Test-Time-Scaling-with-Evolving-Meta-Thoughts" class="headerlink" title="MetaScale: Test-Time Scaling with Evolving Meta-Thoughts"></a>MetaScale: Test-Time Scaling with Evolving Meta-Thoughts</h2><p><strong>Authors:Qin Liu, Wenxuan Zhou, Nan Xu, James Y. Huang, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen</strong></p>
<p>One critical challenge for large language models (LLMs) for making complex reasoning is their reliance on matching reasoning patterns from training data, instead of proactively selecting the most appropriate cognitive strategy to solve a given task. Existing approaches impose fixed cognitive structures that enhance performance in specific tasks but lack adaptability across diverse scenarios. To address this limitation, we introduce METASCALE, a test-time scaling framework based on meta-thoughts â€“ adaptive thinking strategies tailored to each task. METASCALE initializes a pool of candidate meta-thoughts, then iteratively selects and evaluates them using a multi-armed bandit algorithm with upper confidence bound selection, guided by a reward model. To further enhance adaptability, a genetic algorithm evolves high-reward meta-thoughts, refining and extending the strategy pool over time. By dynamically proposing and optimizing meta-thoughts at inference time, METASCALE improves both accuracy and generalization across a wide range of tasks. Experimental results demonstrate that MetaScale consistently outperforms standard inference approaches, achieving an 11% performance gain in win rate on Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably, METASCALE scales more effectively with increasing sampling budgets and produces more structured, expert-level responses. </p>
<blockquote>
<p>å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤æ‚æ¨ç†çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ï¼Œå®ƒä»¬ä¾èµ–äºä»è®­ç»ƒæ•°æ®ä¸­åŒ¹é…æ¨ç†æ¨¡å¼ï¼Œè€Œä¸æ˜¯ä¸»åŠ¨é€‰æ‹©æœ€é€‚å½“çš„è®¤çŸ¥ç­–ç•¥æ¥è§£å†³ç»™å®šä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•å¼ºåŠ å›ºå®šçš„è®¤çŸ¥ç»“æ„ï¼Œè¿™åœ¨ç‰¹å®šä»»åŠ¡ä¸­æé«˜äº†æ€§èƒ½ï¼Œä½†ç¼ºä¹åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†METASCALEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå…ƒæ€ç»´çš„æµ‹è¯•æ—¶ç¼©æ”¾æ¡†æ¶â€”â€”é€‚åº”æ¯ä¸ªä»»åŠ¡çš„æ€è€ƒç­–ç•¥ã€‚METASCALEé¦–å…ˆåˆå§‹åŒ–ä¸€ç»„å€™é€‰å…ƒæ€ç»´ï¼Œç„¶åä½¿ç”¨å¤šè‡‚åŒªå¾’ç®—æ³•å’Œä¸Šé™ç½®ä¿¡ç•Œé€‰æ‹©è¿›è¡Œè¿­ä»£é€‰æ‹©å’Œè¯„ä¼°ï¼Œç”±å¥–åŠ±æ¨¡å‹å¼•å¯¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é€‚åº”æ€§ï¼Œé—ä¼ ç®—æ³•å¯¹é«˜å¥–åŠ±çš„å…ƒæ€ç»´è¿›è¡Œè¿›åŒ–ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ä¸æ–­ç²¾ç‚¼å’Œæ‰©å±•ç­–ç•¥æ± ã€‚é€šè¿‡åœ¨æ¨ç†æ—¶åŠ¨æ€æå‡ºå’Œä¼˜åŒ–å…ƒæ€ç»´ï¼ŒMETASCALEæé«˜äº†å„ç§ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMetaScaleå§‹ç»ˆä¼˜äºæ ‡å‡†æ¨ç†æ–¹æ³•ï¼Œåœ¨GPT-4oçš„Arena-Hardä¸Šèµ¢å¾—äº†11%çš„æ€§èƒ½æå‡ç‡ï¼Œåœ¨é£æ ¼æ§åˆ¶ä¸‹è¶…è¶Šäº†o1-mini 0.9%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMETASCALEåœ¨å¢åŠ é‡‡æ ·é¢„ç®—æ—¶æ›´æœ‰æ•ˆåœ°æ‰©å±•ï¼Œå¹¶äº§ç”Ÿæ›´ç»“æ„åŒ–ã€ä¸“å®¶çº§çš„å›åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13447v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å®ƒä»¬ä¾èµ–äºè®­ç»ƒæ•°æ®ä¸­çš„åŒ¹é…æ¨ç†æ¨¡å¼ï¼Œè€Œä¸æ˜¯ä¸»åŠ¨é€‰æ‹©æœ€é€‚å½“çš„è®¤çŸ¥ç­–ç•¥æ¥è§£å†³ç»™å®šä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºå…ƒæ€ç»´çš„ä»»åŠ¡è‡ªé€‚åº”æ€è€ƒç­–ç•¥æ¡†æ¶METASCALEã€‚è¯¥æ¡†æ¶é€šè¿‡åˆå§‹åŒ–å€™é€‰å…ƒæ€ç»´æ± ï¼Œä½¿ç”¨å¤šè‡‚è€è™æœºç®—æ³•è¿­ä»£é€‰æ‹©å’Œè¯„ä¼°å®ƒä»¬ï¼Œå¹¶é€šè¿‡å¥–åŠ±æ¨¡å‹è¿›è¡Œå¼•å¯¼ã€‚æ­¤å¤–ï¼Œé—ä¼ ç®—æ³•å¯¹é«˜å¥–åŠ±çš„å…ƒæ€ç»´è¿›è¡Œè¿›åŒ–ï¼Œéšæ—¶é—´æ¨ç§»ä¼˜åŒ–å’Œæ‰©å±•ç­–ç•¥æ± ã€‚METASCALEåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†æ¨ç†æ–¹æ³•ç›¸æ¯”ï¼ŒMetaScaleæ€§èƒ½å¢ç›Šè¾¾11%ï¼Œå°¤å…¶åœ¨GPT-4oçš„Arena-Hardä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå¹¶è¶…è¿‡äº†o1-miniçš„é£æ ¼æ§åˆ¶è¡¨ç°ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒMETASCALEåœ¨å¢åŠ é‡‡æ ·é¢„ç®—æ—¶æ›´æœ‰æ•ˆï¼Œå¹¶èƒ½äº§ç”Ÿæ›´ç»“æ„åŒ–ã€ä¸“å®¶çº§çš„å›åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†æ—¶ä¾èµ–è®­ç»ƒæ•°æ®ä¸­çš„åŒ¹é…æ¨¡å¼ï¼Œç¼ºä¹é€‚åº”ä¸åŒä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>METASCALEæ¡†æ¶å¼•å…¥å…ƒæ€ç»´æ¦‚å¿µï¼Œä¸ºä¸åŒä»»åŠ¡æä¾›è‡ªé€‚åº”æ€è€ƒç­–ç•¥ã€‚</li>
<li>METASCALEé€šè¿‡å¤šè‡‚è€è™æœºç®—æ³•é€‰æ‹©å’Œè¯„ä¼°å€™é€‰å…ƒæ€ç»´ï¼Œå¹¶ç”±å¥–åŠ±æ¨¡å‹å¼•å¯¼ã€‚</li>
<li>é—ä¼ ç®—æ³•ç”¨äºä¼˜åŒ–å’Œæ‰©å±•é«˜å¥–åŠ±çš„å…ƒæ€ç»´ç­–ç•¥ã€‚</li>
<li>METASCALEæé«˜äº†å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMetaScaleæ€§èƒ½ä¼˜äºæ ‡å‡†æ¨ç†æ–¹æ³•ï¼Œå°¤å…¶åœ¨GPT-4oçš„Arena-Hardä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0577d94d0ef7d1a983bf77aca10e5f8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbfb60052aa38923a22dd3c00bc86cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ee9872ee999b971e1aea9ffcd2eb116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb43dc75987adc079f11a825256b9746.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2facbe895feb676018587beaee4de4dc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VideoMind-A-Chain-of-LoRA-Agent-for-Long-Video-Reasoning"><a href="#VideoMind-A-Chain-of-LoRA-Agent-for-Long-Video-Reasoning" class="headerlink" title="VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning"></a>VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning</h2><p><strong>Authors:Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou</strong></p>
<p>Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning. </p>
<blockquote>
<p>è§†é¢‘ä»¥å…¶ç‹¬ç‰¹çš„æ—¶åºç»´åº¦ä¸ºç‰¹è‰²ï¼Œè¦æ±‚ç²¾ç¡®ä¸”åŸºäºå®é™…æƒ…å¢ƒçš„ç†è§£ï¼Œç­”æ¡ˆç›´æ¥ä¸è§†è§‰ã€å¯è§£é‡Šçš„è¯æ®ç›¸å…³è”ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å¤šæ¨¡æ€æ¨ç†ï¼ˆå°¤å…¶æ˜¯è§†é¢‘ï¼‰ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoMindï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ—¶åºè§†é¢‘ç†è§£è®¾è®¡çš„æ–°å‹è§†é¢‘è¯­è¨€ä»£ç†ã€‚VideoMindåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆiï¼‰æˆ‘ä»¬ç¡®å®šäº†è§†é¢‘æ—¶åºæ¨ç†æ‰€éœ€çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶åŸºäºè§’è‰²è®¾è®¡äº†ä¸€ä¸ªä»£ç†å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåè°ƒä¸åŒè§’è‰²çš„è§„åˆ’å™¨ã€ä¸€ä¸ªç”¨äºæ—¶åºå®šä½çš„å®šä½å™¨ã€ä¸€ä¸ªè¯„ä¼°æ—¶åºé—´éš”å‡†ç¡®æ€§çš„éªŒè¯å™¨ï¼Œä»¥åŠä¸€ä¸ªç”¨äºé—®ç­”çš„å›ç­”è€…ã€‚ï¼ˆiiï¼‰ä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆè¿™äº›ä¸åŒçš„è§’è‰²ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Chain-of-LoRAç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿé€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨å®ç°æ— ç¼è§’è‰²åˆ‡æ¢ï¼ŒåŒæ—¶é¿å…å¤šä¸ªæ¨¡å‹çš„å¼€é”€ï¼Œä»è€Œåœ¨æ•ˆç‡å’Œçµæ´»æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨14ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬3ä¸ªåŸºäºå®é™…æƒ…å¢ƒçš„è§†é¢‘é—®ç­”ä»»åŠ¡ã€6ä¸ªè§†é¢‘æ—¶åºå®šä½ä»»åŠ¡å’Œ5ä¸ªä¸€èˆ¬è§†é¢‘é—®ç­”ä»»åŠ¡ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨æ¨è¿›è§†é¢‘ä»£ç†å’Œé•¿æ ¼å¼æ—¶åºæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13444v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://videomind.github.io/">https://videomind.github.io/</a></p>
<p><strong>Summary</strong><br>è§†é¢‘ç”±äºå…¶ç‹¬ç‰¹çš„æ—¶é—´ç»´åº¦ï¼Œéœ€è¦ç²¾ç¡®çš„ç†è§£ï¼Œç­”æ¡ˆå¿…é¡»ä¸è§†è§‰ã€å¯è§£é‡Šçš„è¯æ®ç›´æ¥ç›¸å…³ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å¤šæ¨¡æ€æ¨ç†ï¼ˆå°¤å…¶æ˜¯è§†é¢‘ï¼‰ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoMindï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ—¶é—´æ¥åœ°ç†è§£è§†é¢‘è€Œè®¾è®¡çš„æ–°å‹è§†é¢‘è¯­è¨€ä»£ç†ã€‚VideoMindç»“åˆäº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®šäº†è§†é¢‘æ—¶é—´æ¨ç†çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶åŸºäºè§’è‰²è®¾è®¡äº†ä¸€ä¸ªä»£ç†å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåè°ƒä¸åŒè§’è‰²çš„è§„åˆ’å™¨ã€ä¸€ä¸ªç”¨äºæ—¶é—´å®šä½çš„å®šæ ‡å™¨ã€ä¸€ä¸ªè¯„ä¼°æ—¶é—´é—´éš”å‡†ç¡®æ€§çš„éªŒè¯å™¨ä»¥åŠä¸€ä¸ªé—®ç­”è§£ç­”å™¨ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆè¿™äº›å¤šæ ·åŒ–çš„è§’è‰²ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Chain-of-LoRAç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨å®ç°æ— ç¼çš„è§’è‰²åˆ‡æ¢ï¼Œé¿å…äº†å¤šä¸ªæ¨¡å‹çš„å¼€é”€ï¼Œä»è€Œå®ç°äº†æ•ˆç‡å’Œçµæ´»æ€§çš„å¹³è¡¡ã€‚åœ¨14ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è®ºæ–‡ä¸­æœ€æ˜¾è‘—çš„ä¸ƒä¸ªè¦ç‚¹æˆ–å…³é”®æ´è§ï¼š</p>
<ol>
<li>è§†é¢‘ç”±äºå…¶ç‹¬ç‰¹çš„æ—¶é—´ç»´åº¦ï¼Œå¯¹ç†è§£æœ‰ç²¾ç¡®è¦æ±‚ï¼Œç­”æ¡ˆéœ€ä¸è§†è§‰è¯æ®ç›´æ¥ç›¸å…³ã€‚</li>
<li>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ–¹é¢æœ‰æ‰€çªç ´ï¼Œä½†å¤šæ¨¡æ€æ¨ç†å°¤å…¶æ˜¯è§†é¢‘æ–¹é¢çš„å¤šæ¨¡æ€æ¨ç†å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>å¼•å…¥äº†VideoMindè¿™ä¸€æ–°å‹è§†é¢‘è¯­è¨€ä»£ç†æ¥å¢å¼ºè§†é¢‘çš„æ—¶é—´æ„ŸçŸ¥ç†è§£èƒ½åŠ›ã€‚</li>
<li>VideoMindåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šç¡®å®šè§†é¢‘æ—¶é—´æ¨ç†çš„æ ¸å¿ƒèƒ½åŠ›å¹¶åŸºäºè§’è‰²è®¾è®¡ä»£ç†å·¥ä½œæµç¨‹ã€‚</li>
<li>VideoMindå…·æœ‰ä¸€ä¸ªè§„åˆ’å™¨æ¥åè°ƒè§’è‰²å·¥ä½œã€ä¸€ä¸ªç”¨äºæ—¶é—´å®šä½çš„å®šæ ‡å™¨ä»¥åŠä¸€ä¸ªè¯„ä¼°æ—¶é—´é—´éš”å‡†ç¡®æ€§çš„éªŒè¯å™¨ç­‰æ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„Chain-of-LoRAç­–ç•¥ä»¥å®ç°æ•ˆç‡ä¸çµæ´»æ€§çš„å¹³è¡¡ã€‚è¿™ç§ç­–ç•¥èƒ½æ— ç¼åœ°åœ¨ä¸åŒè§’è‰²é—´åˆ‡æ¢å¹¶é¿å…å¤šä¸ªæ¨¡å‹çš„å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f67a2edd5a77bf4890846d0edf04f68b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86845e5943d8d4f83c93a98953ba4ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ecd8e7a2ab1ee00fecb4e352d4a7368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0ad1e51ad6980ab765146c580ff7b3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f64f9f3ca2ee88900720c4541e3ea15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54666f24476073a8e24f97694f7e00d2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="xLSTM-7B-A-Recurrent-LLM-for-Fast-and-Efficient-Inference"><a href="#xLSTM-7B-A-Recurrent-LLM-for-Fast-and-Efficient-Inference" class="headerlink" title="xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference"></a>xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference</h2><p><strong>Authors:Maximilian Beck, Korbinian PÃ¶ppel, Phillip Lippe, Richard Kurle, Patrick M. Blies, GÃ¼nter Klambauer, Sebastian BÃ¶ck, Sepp Hochreiter</strong></p>
<p>Recent breakthroughs in solving reasoning, math and coding problems with Large Language Models (LLMs) have been enabled by investing substantial computation budgets at inference time. Therefore, inference speed is one of the most critical properties of LLM architectures, and there is a growing need for LLMs that are efficient and fast at inference. Recently, LLMs built on the xLSTM architecture have emerged as a powerful alternative to Transformers, offering linear compute scaling with sequence length and constant memory usage, both highly desirable properties for efficient inference. However, such xLSTM-based LLMs have yet to be scaled to larger models and assessed and compared with respect to inference speed and efficiency. In this work, we introduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTMâ€™s architectural benefits with targeted optimizations for fast and efficient inference. Our experiments demonstrate that xLSTM 7B achieves performance on downstream tasks comparable to other similar-sized LLMs, while providing significantly faster inference speeds and greater efficiency compared to Llama- and Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most efficient 7B LLM, offering a solution for tasks that require large amounts of test-time computation. Our work highlights xLSTMâ€™s potential as a foundational architecture for methods building on heavy use of LLM inference. Our model weights, model code and training code are open-source. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé€šè¿‡æ¨ç†æ—¶é—´çš„å¤§é‡è®¡ç®—é¢„ç®—æŠ•å…¥ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³æ¨ç†ã€æ•°å­¦å’Œç¼–ç é—®é¢˜å–å¾—äº†çªç ´ã€‚å› æ­¤ï¼Œæ¨ç†é€Ÿåº¦æˆä¸ºLLMæ¶æ„ä¸­æœ€å…³é”®çš„å±æ€§ä¹‹ä¸€ï¼Œå¯¹é«˜æ•ˆå¿«é€Ÿçš„LLMæ¨ç†éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚æœ€è¿‘ï¼ŒåŸºäºxLSTMæ¶æ„çš„LLMä½œä¸ºTransformerçš„å¼ºå¤§æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œå®ƒæä¾›çº¿æ€§è®¡ç®—æ‰©å±•å’Œæ’å®šå†…å­˜ä½¿ç”¨ï¼Œéƒ½æ˜¯å®ç°é«˜æ•ˆæ¨ç†æ‰€æœŸæœ›çš„å±æ€§ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„åŸºäºxLSTMçš„LLMå°šæœªæ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹ï¼Œå¹¶éœ€è¦é’ˆå¯¹æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡è¿›è¡Œè¯„ä¼°å’Œæ¯”è¾ƒã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†xLSTM 7Bï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰7äº¿å‚æ•°çš„LLMï¼Œå®ƒå°†xLSTMçš„æ¶æ„ä¼˜åŠ¿ä¸é’ˆå¯¹å¿«é€Ÿé«˜æ•ˆæ¨ç†çš„ä¼˜åŒ–ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒxLSTM 7Båœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸å…¶ä»–ç±»ä¼¼è§„æ¨¡çš„LLMç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´é«˜çš„æ•ˆç‡ï¼Œä¸åŸºäºLlamaå’ŒMambaçš„LLMç›¸æ¯”è¡¨ç°æ›´ä¼˜ç§€ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†xLSTM 7Bä½œä¸ºæœ€å¿«ã€æœ€é«˜æ•ˆçš„7B LLMçš„åœ°ä½ï¼Œä¸ºè§£å†³éœ€è¦å¤§é‡æµ‹è¯•æ—¶é—´è®¡ç®—çš„ä»»åŠ¡æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†xLSTMä½œä¸ºåŸºäºå¤§é‡ä½¿ç”¨LLMæ¨ç†çš„æ–¹æ³•çš„åŸºç¡€æ¶æ„çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹æƒé‡ã€æ¨¡å‹ä»£ç å’ŒåŸ¹è®­ä»£ç éƒ½æ˜¯å¼€æºçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13427v1">PDF</a> Code available at: <a target="_blank" rel="noopener" href="https://github.com/NX-AI/xlstm">https://github.com/NX-AI/xlstm</a> and   <a target="_blank" rel="noopener" href="https://github.com/NX-AI/xlstm-jax">https://github.com/NX-AI/xlstm-jax</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ã€æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜è§£å†³æ–¹é¢çš„æœ€æ–°çªç ´ï¼Œå¾—ç›Šäºåœ¨æ¨ç†æ—¶é—´æŠ•å…¥äº†å¤§é‡çš„è®¡ç®—é¢„ç®—ã€‚å› æ­¤ï¼Œæ¨ç†é€Ÿåº¦æˆä¸ºLLMæ¶æ„ä¸­æœ€å…³é”®çš„å±æ€§ä¹‹ä¸€ï¼Œå¯¹äºé«˜æ•ˆå¿«é€Ÿçš„LLMéœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚åŸºäºxLSTMæ¶æ„çš„LLMæœ€è¿‘ä½œä¸ºTransformerçš„æœ‰åŠ›æ›¿ä»£è€Œå‡ºç°ï¼Œå…·æœ‰çº¿æ€§è®¡ç®—æ‰©å±•æ€§å’Œæ’å®šå†…å­˜ä½¿ç”¨ç­‰ç†æƒ³å±æ€§ï¼Œæœ‰åˆ©äºé«˜æ•ˆæ¨ç†ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„xLSTM-based LLMå°šæœªæ‰©å±•åˆ°å¤§å‹æ¨¡å‹ï¼Œå¹¶å…³äºæ¨ç†é€Ÿåº¦å’Œæ•ˆç‡è¿›è¡Œè¯„ä¼°å’Œæ¯”è¾ƒã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†xLSTM 7Bï¼Œè¿™æ˜¯ä¸€ä¸ª7äº¿å‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†xLSTMçš„æ¶æ„ä¼˜åŠ¿ï¼Œå¹¶é’ˆå¯¹å¿«é€Ÿé«˜æ•ˆçš„æ¨ç†è¿›è¡Œäº†ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒxLSTM 7Båœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸å…¶ä»–ç±»ä¼¼è§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œä½†æ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œæ•ˆç‡æ›´é«˜ï¼Œä¸Lamaå’ŒMamba-based LLMç›¸æ¯”è¡¨ç°æ›´ä½³ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†xLSTM 7Bä½œä¸ºæœ€å¿«å’Œæœ€æœ‰æ•ˆç‡çš„7B LLMçš„åœ°ä½ï¼Œä¸ºè§£å†³éœ€è¦å¤§é‡æµ‹è¯•æ—¶é—´è®¡ç®—çš„ä»»åŠ¡æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†xLSTMä½œä¸ºå¤§é‡ä½¿ç”¨LLMæ¨ç†æ–¹æ³•çš„åŸºç¡€æ¶æ„çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹æƒé‡ã€æ¨¡å‹ä»£ç å’ŒåŸ¹è®­ä»£ç éƒ½æ˜¯å¼€æºçš„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ã€æ•°å­¦å’Œç¼–ç¨‹é—®é¢˜æ±‚è§£æ–¹é¢å–å¾—æ–°çªç ´ã€‚</li>
<li>æ¨ç†é€Ÿåº¦æ˜¯LLMæ¶æ„çš„å…³é”®å±æ€§ä¹‹ä¸€ï¼Œå¯¹é«˜æ•ˆå¿«é€Ÿçš„LLMéœ€æ±‚å¢åŠ ã€‚</li>
<li>xLSTMæ¶æ„çš„LLMä½œä¸ºTransformerçš„æœ‰åŠ›æ›¿ä»£å‡ºç°ï¼Œå…·æœ‰çº¿æ€§è®¡ç®—æ‰©å±•æ€§å’Œæ’å®šå†…å­˜ä½¿ç”¨æ€§ã€‚</li>
<li>xLSTM 7Bæ¨¡å‹ç»“åˆäº†xLSTMçš„æ¶æ„ä¼˜åŠ¿å¹¶ä¼˜åŒ–äº†æ¨ç†é€Ÿåº¦å’Œæ•ˆç‡ã€‚</li>
<li>xLSTM 7Båœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸ç±»ä¼¼è§„æ¨¡çš„å…¶ä»–LLMç›¸å½“ã€‚</li>
<li>xLSTM 7Bç›¸æ¯”å…¶ä»–LLMå…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´é«˜çš„æ•ˆç‡ã€‚</li>
<li>xLSTM 7Bçš„æ¨¡å‹æƒé‡ã€æ¨¡å‹å’Œè®­ç»ƒä»£ç éƒ½æ˜¯å¼€æºçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e5763ca8b9f2e9f84815d887ad705cc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5dbdf8546d0c16742d76124cd342865b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8d94b347b3ee84d7073c07c47a276d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae83261d68658a9a72cfcb4f1d47750a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DLPO-Towards-a-Robust-Efficient-and-Generalizable-Prompt-Optimization-Framework-from-a-Deep-Learning-Perspective"><a href="#DLPO-Towards-a-Robust-Efficient-and-Generalizable-Prompt-Optimization-Framework-from-a-Deep-Learning-Perspective" class="headerlink" title="DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization   Framework from a Deep-Learning Perspective"></a>DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization   Framework from a Deep-Learning Perspective</h2><p><strong>Authors:Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, Libo Qin</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success across diverse tasks, largely driven by well-designed prompts. However, crafting and selecting such prompts often requires considerable human effort, significantly limiting its scalability. To mitigate this, recent studies have explored automated prompt optimization as a promising solution. Despite these efforts, existing methods still face critical challenges in robustness, efficiency, and generalization. To systematically address these challenges, we first conduct an empirical analysis to identify the limitations of current reflection-based prompt optimization paradigm. Building on these insights, we propose 7 innovative approaches inspired by traditional deep learning paradigms for prompt optimization (DLPO), seamlessly integrating these concepts into text-based gradient optimization. Through these advancements, we progressively tackle the aforementioned challenges and validate our methods through extensive experimentation. We hope our study not only provides valuable guidance for future research but also offers a comprehensive understanding of the challenges and potential solutions in prompt optimization. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sfasfaffa/DLPO">https://github.com/sfasfaffa/DLPO</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ç›Šäºç²¾å¿ƒè®¾è®¡çš„æç¤ºã€‚ç„¶è€Œï¼Œåˆ¶ä½œå’Œé€‰æ‹©è¿™äº›æç¤ºé€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥åŠªåŠ›ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œè¿‘æœŸçš„ç ”ç©¶æ¢ç´¢äº†è‡ªåŠ¨æç¤ºä¼˜åŒ–ä½œä¸ºä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡ä»˜å‡ºäº†è¿™äº›åŠªåŠ›ï¼Œç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ç¨³å¥æ€§ã€æ•ˆç‡å’Œæ³›åŒ–æ–¹é¢çš„ä¸¥å³»æŒ‘æˆ˜ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹åŸºäºå½“å‰åæ€çš„æç¤ºä¼˜åŒ–èŒƒå¼çš„å±€é™æ€§è¿›è¡Œå®è¯ç ”ç©¶ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†7ç§åˆ›æ–°æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å—åˆ°ä¼ ç»Ÿæ·±åº¦å­¦ä¹ èŒƒå¼çš„å¯å‘ï¼Œç”¨äºæç¤ºä¼˜åŒ–ï¼ˆDLPOï¼‰ï¼Œæ— ç¼é›†æˆè¿™äº›æ¦‚å¿µåˆ°æ–‡æœ¬æ¢¯åº¦ä¼˜åŒ–ä¸­ã€‚é€šè¿‡è¿™äº›è¿›æ­¥ï¼Œæˆ‘ä»¬é€æ­¥è§£å†³äº†ä¸Šè¿°æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶ä¸ä»…ä¸ºæœªæ¥ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ï¼Œè€Œä¸”å…¨é¢äº†è§£äº†æç¤ºä¼˜åŒ–ä¸­çš„æŒ‘æˆ˜å’Œæ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sfasfaffa/DLPO%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/sfasfaffa/DLPOè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13413v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸå¾—ç›Šäºå·§å¦™çš„æç¤ºè®¾è®¡ï¼Œä½†è®¾è®¡é€‰æ‹©æç¤ºéœ€è¦å¤§é‡äººåŠ›ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œè¿‘æœŸç ”ç©¶å°è¯•è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç¨³å¥æ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é€šè¿‡å®è¯åˆ†æäº†å½“å‰åŸºäºåå°„çš„æç¤ºä¼˜åŒ–èŒƒå¼çš„å±€é™æ€§ï¼Œå¹¶æå‡º7ç§å—ä¼ ç»Ÿæ·±åº¦å­¦ä¹ èŒƒå¼å¯å‘çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼ˆDLPOï¼‰ï¼Œå°†å…¶æ— ç¼é›†æˆåˆ°æ–‡æœ¬æ¢¯åº¦ä¼˜åŒ–ä¸­ã€‚é€šè¿‡é€æ­¥è§£å†³å‰è¿°æŒ‘æˆ˜å¹¶é€šè¿‡å¤§é‡å®éªŒéªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæœ¬ç ”ç©¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ï¼Œå¹¶å…¨é¢ç†è§£äº†æç¤ºä¼˜åŒ–ä¸­çš„æŒ‘æˆ˜å’Œæ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäºå·§å¦™çš„æç¤ºè®¾è®¡ï¼Œä½†äººå·¥è®¾è®¡æç¤ºé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚</li>
<li>è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–æ˜¯ç¼“è§£è¿™ä¸€é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ç¨³å¥æ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å®è¯åˆ†æäº†å½“å‰åŸºäºåå°„çš„æç¤ºä¼˜åŒ–èŒƒå¼çš„å±€é™æ€§ã€‚</li>
<li>æå‡º7ç§å—ä¼ ç»Ÿæ·±åº¦å­¦ä¹ èŒƒå¼å¯å‘çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼ˆDLPOï¼‰ã€‚</li>
<li>DLPOæ–¹æ³•æ— ç¼é›†æˆåˆ°æ–‡æœ¬æ¢¯åº¦ä¼˜åŒ–ä¸­ï¼Œé€æ­¥è§£å†³å‰è¿°æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†DLPOæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9a64c333b6bd96fc115eac2770d3595.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c664b9f914fea8eacc0a4a130a19f582.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2486f3408af6d4f6ec22162c90e15567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d046b33e937f57c97c38aef9ded4f330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e788886eb79ad0390d4a2385d95f4b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baa8b595de0a199f3b0aa80a33c52f83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a269d04e369c47a342621ee9cf6bcd6a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MicroVQA-A-Multimodal-Reasoning-Benchmark-for-Microscopy-Based-Scientific-Research"><a href="#MicroVQA-A-Multimodal-Reasoning-Benchmark-for-Microscopy-Based-Scientific-Research" class="headerlink" title="MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based   Scientific Research"></a>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based   Scientific Research</h2><p><strong>Authors:James Burgess, Jeffrey J Nirschl, Laura Bravo-SÃ¡nchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, Sarina M. Hasan, Alexandra Johannesson, William D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg, Serena Yeung-Levy</strong></p>
<p>Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based &#96;RefineBotâ€™ updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jmhb/microvqa">https://huggingface.co/datasets/jmhb/microvqa</a>, and project page at <a target="_blank" rel="noopener" href="https://jmhb0.github.io/microvqa">https://jmhb0.github.io/microvqa</a>. </p>
<blockquote>
<p>ç§‘å­¦ç ”ç©¶éœ€è¦å¯¹å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¤æ‚æ¨ç†ï¼Œè¿™åœ¨ç”Ÿç‰©å­¦ä¸­å°¤å…¶æ™®éã€‚å°½ç®¡æœ€è¿‘åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©ç ”ç©¶æä¾›äº†æ”¯æŒï¼Œä½†ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä»…é’ˆå¯¹å¤§å­¦æ°´å¹³çš„éš¾åº¦ï¼Œè€Œç ”ç©¶çº§åŸºå‡†æµ‹è¯•åˆ™å¼ºè°ƒè¾ƒä½çº§åˆ«çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿œè¿œè¾¾ä¸åˆ°ç§‘å­¦å‘ç°æ‰€éœ€çš„å¤šæ¨¡æ€å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MicroVQAï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç ”ç©¶å·¥ä½œæµç¨‹ä¸­è‡³å…³é‡è¦çš„ä¸‰ç§æ¨ç†èƒ½åŠ›ï¼šä¸“å®¶å›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒæ–¹æ¡ˆæå‡ºã€‚MicroVQAåŒ…å«ç”±ç”Ÿç‰©å­¦ä¸“å®¶ç­–åˆ’çš„1042é“é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œæ¶‰åŠå¤šç§æ˜¾å¾®é•œæ¨¡æ€ï¼Œç¡®ä¿VQAæ ·æœ¬ä»£è¡¨çœŸå®çš„ç§‘å­¦å®è·µã€‚åœ¨æ„å»ºåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†çš„é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•ä¼šå¼•å‘è¯­è¨€ä¸Šçš„æ·å¾„ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡‡ç”¨æ–°çš„ä¸¤é˜¶æ®µæµç¨‹ï¼šä¼˜åŒ–çš„LLMæç¤ºå°†é—®é¢˜ç­”æ¡ˆå¯¹ç»“æ„åŒ–ä¸ºé€‰æ‹©é¢˜ï¼›ç„¶ååŸºäºä»£ç†çš„â€œRefineBotâ€å¯¹å…¶è¿›è¡Œæ›´æ–°ä»¥æ¶ˆé™¤æ·å¾„ã€‚åœ¨æœ€æ–°MLLMsä¸Šçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæœ€ä½³æ€§èƒ½è¾¾åˆ°53%ï¼›ä½¿ç”¨è¾ƒå°LLMçš„æ¨¡å‹ä»…ç•¥å¾®é€Šäºé¡¶çº§æ¨¡å‹ï¼Œè¿™è¡¨æ˜åŸºäºè¯­è¨€çš„æ¨ç†ä¸å¦‚å¤šæ¨¡æ€æ¨ç†å…·æœ‰æŒ‘æˆ˜æ€§ï¼›ä½¿ç”¨ç§‘å­¦æ–‡ç« è¿›è¡Œè°ƒæ•´å¯å¢å¼ºæ€§èƒ½ã€‚ä¸“å®¶å¯¹æ€ç»´é“¾å¼ååº”çš„åˆ†æè¡¨æ˜ï¼Œæ„ŸçŸ¥é”™è¯¯æœ€ä¸ºé¢‘ç¹ï¼Œå…¶æ¬¡æ˜¯çŸ¥è¯†é”™è¯¯ï¼Œç„¶åæ˜¯è¿‡åº¦æ¦‚æ‹¬é”™è¯¯ã€‚è¿™äº›è§è§£çªå‡ºäº†å¤šæ¨¡æ€ç§‘å­¦æ¨ç†çš„æŒ‘æˆ˜æ€§ï¼Œè¡¨æ˜MicroVQAæ˜¯æ¨åŠ¨äººå·¥æ™ºèƒ½é©±åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„æœ‰ä»·å€¼çš„èµ„æºã€‚MicroVQAå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jmhb/microvqa%E8%AE%BF%E9%97%AE%EF%BC%8C%E9%A1%B9%E7%9B%AE%E9%A1%B5%E9%9D%A2%E4%B8%BAhttps://jmhb0.github.io/microvqa%E3%80%82">https://huggingface.co/datasets/jmhb/microvqaè®¿é—®ï¼Œé¡¹ç›®é¡µé¢ä¸ºhttps://jmhb0.github.io/microvqaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13399v1">PDF</a> CVPR 2025 (Conference on Computer Vision and Pattern Recognition)   Project page at <a target="_blank" rel="noopener" href="https://jmhb0.github.io/microvqa">https://jmhb0.github.io/microvqa</a> Benchmark at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jmhb/microvqa">https://huggingface.co/datasets/jmhb/microvqa</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç§‘å­¦ç ”ç©¶ä¸­çš„å¤šæ¨¡æ€æ•°æ®æ¨ç†éœ€æ±‚ï¼ŒMicroVQAåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°å›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒæ–¹æ¡ˆæå‡ºç­‰ä¸‰é¡¹å…³é”®èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ç”±ç”Ÿç‰©å­¦ä¸“å®¶ç¼–å†™çš„åƒä½™é“é€‰æ‹©é¢˜ï¼Œæ—¨åœ¨åæ˜ çœŸå®ç§‘å­¦å®è·µä¸­çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œæ–°çš„LLMæç¤ºå’ŒRefineBotèƒ½å¤Ÿå‡å°‘è¯­è¨€æ·å¾„çš„äº§ç”Ÿã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›®å‰é¡¶å°–æ¨¡å‹çš„æ€§èƒ½ä»…è¾¾åˆ°çº¦53%ï¼Œè€Œä½¿ç”¨ç§‘å­¦æ–‡ç« è¿›è¡Œå¾®è°ƒå¯æå‡æ€§èƒ½ã€‚æ„ŸçŸ¥é”™è¯¯æ˜¯æœ€å¸¸è§çš„æŒ‘æˆ˜ã€‚MicroVQAæ˜¯æ¨è¿›AIé©±åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„é‡è¦èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MicroVQAæ˜¯ä¸€é¡¹è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œé’ˆå¯¹ç§‘å­¦ç ”ç©¶é¢†åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å®ƒåŒ…æ‹¬ç”±ç”Ÿç‰©å­¦ä¸“å®¶ç¼–åˆ¶çš„1,042é“é€‰æ‹©é¢˜ï¼Œæ¶µç›–å¤šæ ·çš„æ˜¾å¾®é•œæˆåƒæ¨¡å¼ã€‚</li>
<li>æ ‡å‡†é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•ä¼šå¯¼è‡´è¯­è¨€æ·å¾„çš„é—®é¢˜ï¼Œå› æ­¤å¼•å…¥LLMæç¤ºå’ŒRefineBotæ¥ä¼˜åŒ–é—®é¢˜è®¾è®¡ã€‚</li>
<li>å½“å‰é¡¶å°–æ¨¡å‹çš„æ€§èƒ½åœ¨MicroVQAä¸Šä»…ä¸ºçº¦53%ï¼Œæ˜¾ç¤ºå¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä½¿ç”¨ç§‘å­¦æ–‡ç« è¿›è¡Œå¾®è°ƒå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸“å®¶åˆ†ææ˜¾ç¤ºæ„ŸçŸ¥é”™è¯¯æ˜¯æœ€å¸¸è§çš„æŒ‘æˆ˜ï¼Œå…¶æ¬¡æ˜¯çŸ¥è¯†é”™è¯¯å’Œè¿‡åº¦æ³›åŒ–é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2b6442616c127f5ab9df9cb18a69354.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51b2fae3f70d0c19646a95bb8754f751.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2cde8e2a3a3375e5a860f7345d7fa8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-446268fe87532aa5e460f36bc62dd390.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Scale-Efficient-Training-for-Large-Datasets"><a href="#Scale-Efficient-Training-for-Large-Datasets" class="headerlink" title="Scale Efficient Training for Large Datasets"></a>Scale Efficient Training for Large Datasets</h2><p><strong>Authors:Qing Zhou, Junyu Gao, Qi Wang</strong></p>
<p>The rapid growth of dataset scales has been a key driver in advancing deep learning research. However, as dataset scale increases, the training process becomes increasingly inefficient due to the presence of low-value samples, including excessive redundant samples, overly challenging samples, and inefficient easy samples that contribute little to model improvement.To address this challenge, we propose Scale Efficient Training (SeTa) for large datasets, a dynamic sample pruning approach that losslessly reduces training time. To remove low-value samples, SeTa first performs random pruning to eliminate redundant samples, then clusters the remaining samples according to their learning difficulty measured by loss. Building upon this clustering, a sliding window strategy is employed to progressively remove both overly challenging and inefficient easy clusters following an easy-to-hard curriculum.We conduct extensive experiments on large-scale synthetic datasets, including ToCa, SS1M, and ST+MJ, each containing over 3 million samples.SeTa reduces training costs by up to 50% while maintaining or improving performance, with minimal degradation even at 70% cost reduction. Furthermore, experiments on various scale real datasets across various backbones (CNNs, Transformers, and Mambas) and diverse tasks (instruction tuning, multi-view stereo, geo-localization, composed image retrieval, referring image segmentation) demonstrate the powerful effectiveness and universality of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mrazhou/SeTa">https://github.com/mrazhou/SeTa</a>. </p>
<blockquote>
<p>æ•°æ®é›†è§„æ¨¡çš„å¿«é€Ÿå¢é•¿æ˜¯æ¨åŠ¨æ·±åº¦å­¦ä¹ ç ”ç©¶è¿›æ­¥çš„å…³é”®é©±åŠ¨åŠ›ã€‚ç„¶è€Œï¼Œéšç€æ•°æ®é›†è§„æ¨¡çš„å¢åŠ ï¼Œç”±äºå­˜åœ¨å¤§é‡ä½ä»·å€¼æ ·æœ¬ï¼ŒåŒ…æ‹¬è¿‡å¤šçš„å†—ä½™æ ·æœ¬ã€è¿‡äºå›°éš¾çš„æ ·æœ¬ä»¥åŠå¯¹æ¨¡å‹æ”¹è¿›è´¡çŒ®ç”šå¾®çš„é«˜æ•ˆç®€å•æ ·æœ¬ï¼Œè®­ç»ƒè¿‡ç¨‹å˜å¾—è¶Šæ¥è¶Šä½æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13385v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡æ•°æ®é›†çš„å¢é•¿æ¨åŠ¨äº†æ·±åº¦å­¦ä¹ ç ”ç©¶çš„è¿›æ­¥ï¼Œä½†å…¶è®­ç»ƒè¿‡ç¨‹å´å˜å¾—æ„ˆå‘ä½æ•ˆã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„åŠ¨æ€æ ·æœ¬è£å‰ªæ–¹æ³•â€”â€”è§„æ¨¡é«˜æ•ˆè®­ç»ƒï¼ˆSeTaï¼‰ï¼Œæ—¨åœ¨æ— æŸå‡å°‘è®­ç»ƒæ—¶é—´ã€‚é€šè¿‡éšæœºè£å‰ªæ¶ˆé™¤å†—ä½™æ ·æœ¬ï¼Œæ ¹æ®æŸå¤±æµ‹é‡å­¦ä¹ éš¾åº¦å¯¹å‰©ä½™æ ·æœ¬è¿›è¡Œèšç±»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥æ¸è¿›ç§»é™¤è¿‡äºå¤æ‚å’Œæ•ˆç‡è¾ƒä½çš„ç°‡ï¼Œéµå¾ªä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹æ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼ŒSeTaåœ¨å¤§å‹åˆæˆæ•°æ®é›†ä¸Šé™ä½äº†50%çš„è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†æ€§èƒ½ï¼Œç”šè‡³åœ¨70%çš„æˆæœ¬é™ä½ä¸‹ä»å…·æœ‰å¾®å°æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œåœ¨å„ç§è§„æ¨¡çš„ç°å®æ•°æ®é›†ã€ä¸åŒçš„éª¨å¹²ç½‘ç»œï¼ˆCNNã€Transformerå’ŒMambasï¼‰ä»¥åŠå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼ˆæŒ‡ä»¤è°ƒä¼˜ã€å¤šè§†å›¾ç«‹ä½“å£°ã€åœ°ç†å®šä½ã€ç»„åˆå›¾åƒæ£€ç´¢ã€å¼•ç”¨å›¾åƒåˆ†å‰²ï¼‰ä¸Šçš„å®éªŒå±•ç¤ºäº†è¯¥æ–¹æ³•çš„å¼ºå¤§æ•ˆæœå’Œæ™®éæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡æ•°æ®é›†çš„å¢é•¿æ¨åŠ¨äº†æ·±åº¦å­¦ä¹ ç ”ç©¶çš„è¿›æ­¥ï¼Œä½†è®­ç»ƒè¿‡ç¨‹å˜å¾—ä½æ•ˆã€‚</li>
<li>SeTaæ˜¯ä¸€ç§åŠ¨æ€æ ·æœ¬è£å‰ªæ–¹æ³•ï¼Œæ—¨åœ¨æ— æŸå‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li>
<li>SeTaé€šè¿‡éšæœºè£å‰ªæ¶ˆé™¤å†—ä½™æ ·æœ¬ï¼Œç„¶åæ ¹æ®æŸå¤±æµ‹é‡å­¦ä¹ éš¾åº¦è¿›è¡Œæ ·æœ¬èšç±»ã€‚</li>
<li>SeTaé‡‡ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥ï¼Œæ¸è¿›ç§»é™¤è¿‡äºå¤æ‚å’Œæ•ˆç‡è¾ƒä½çš„æ ·æœ¬ç°‡ã€‚</li>
<li>SeTaåœ¨å¤§å‹åˆæˆæ•°æ®é›†ä¸Šå¯é™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>SeTaåœ¨å¤šç§ç°å®æ•°æ®é›†ã€ä¸åŒéª¨å¹²ç½‘ç»œå’Œä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d5464dadcc00cc4daf50d85965304a5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee153554b7bd69cb989e33139943ee3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9744316854ddab4c0a4e6a30b9df3574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85a29b79201f0c7447800a22303aeb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-571de64d6795e251a4cba71a790cab2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a2a5889a53bc7d6c23af5b9e7e81180.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cream-of-the-Crop-Harvesting-Rich-Scalable-and-Transferable-Multi-Modal-Data-for-Instruction-Fine-Tuning"><a href="#Cream-of-the-Crop-Harvesting-Rich-Scalable-and-Transferable-Multi-Modal-Data-for-Instruction-Fine-Tuning" class="headerlink" title="Cream of the Crop: Harvesting Rich, Scalable and Transferable   Multi-Modal Data for Instruction Fine-Tuning"></a>Cream of the Crop: Harvesting Rich, Scalable and Transferable   Multi-Modal Data for Instruction Fine-Tuning</h2><p><strong>Authors:Mengyao Lyu, Yan Li, Huasong Zhong, Wenhao Yang, Hui Chen, Jungong Han, Guiguang Ding, Zhenheng Yang</strong></p>
<p>The hypothesis that pretrained large language models (LLMs) necessitate only minimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has been substantiated by recent advancements in data curation and selection research. However, their stability and generalizability are compromised due to the vulnerability to experimental setups and validation protocols, falling short of surpassing random sampling (Diddee &amp; Ippolito, 2024; Xia et al., 2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer token volume and heightened heterogeneity of data sources, amplify both the significance and complexity of data selection.   To harvest multi-modal instructional data in a robust and efficient manner, we re-define the granularity of the quality metric by decomposing it into 14 vision-language-related capabilities, and introduce multi-modal rich scorers to evaluate the capabilities of each data candidate. To promote diversity, in light of the inherent objective of the alignment stage, we take interaction style as diversity indicator and use a multi-modal rich styler to identify data instruction patterns. In doing so, our multi-modal rich scorers and styler (mmSSR) guarantee that high-scoring information is conveyed to users in diversified forms. Free from embedding-based clustering or greedy sampling, mmSSR efficiently scales to millions of data with varying budget constraints, supports customization for general or specific capability acquisition, and facilitates training-free generalization to new domains for curation. Across 10+ experimental settings, validated by 14 multi-modal benchmarks, we demonstrate consistent improvements over random sampling, baseline strategies and state-of-the-art selection methods, achieving 99.1% of full performance with only 30% of the 2.6M data. </p>
<blockquote>
<p>å‡è®¾é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µåªéœ€è¦æœ€å°é™åº¦çš„ç›‘ç£ï¼ˆZhouç­‰äººï¼Œ2024å¹´ï¼‰çš„è§‚ç‚¹ï¼Œæœ€è¿‘çš„æ•°æ®æ•´ç†å’Œé€‰æ‹©ç ”ç©¶æ–¹é¢çš„è¿›å±•å·²ç»è¯å®äº†è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œç”±äºå®¹æ˜“å—åˆ°å®éªŒè®¾ç½®å’ŒéªŒè¯åè®®çš„å½±å“ï¼Œå…¶ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›å—åˆ°æŸå®³ï¼Œæœªèƒ½è¶…è¶ŠéšæœºæŠ½æ ·ï¼ˆDiddeeå’ŒIppolitoï¼Œ2024å¹´ï¼›Xiaç­‰äººï¼Œ2024bï¼‰ã€‚åŸºäºLLMçš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰ç»“åˆäº†åºå¤§çš„æ•°æ®é‡å’Œé«˜åº¦å¼‚è´¨çš„æ•°æ®æºï¼Œæ”¾å¤§äº†æ•°æ®é€‰æ‹©çš„é‡è¦æ€§å’Œå¤æ‚æ€§ã€‚ä¸ºäº†ä»¥ç¨³å¥é«˜æ•ˆçš„æ–¹å¼æ”¶é›†å¤šæ¨¡æ€æ•™å­¦æ•°æ®ï¼Œæˆ‘ä»¬é‡æ–°å®šä¹‰äº†è´¨é‡æŒ‡æ ‡çš„ç²’åº¦ï¼Œå°†å…¶åˆ†è§£ä¸ºä¸è§†è§‰è¯­è¨€ç›¸å…³çš„14ç§èƒ½åŠ›ï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€ä¸°å¯Œè¯„åˆ†è€…æ¥è¯„ä¼°æ¯ä¸ªæ•°æ®å€™é€‰è€…çš„èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›å¤šæ ·æ€§ï¼Œè€ƒè™‘åˆ°å¯¹é½é˜¶æ®µçš„å›ºæœ‰ç›®æ ‡ï¼Œæˆ‘ä»¬ä»¥äº¤äº’é£æ ¼ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨å¤šæ¨¡æ€ä¸°å¯Œé£æ ¼å™¨æ¥è¯†åˆ«æ•°æ®æŒ‡ä»¤æ¨¡å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€ä¸°å¯Œè¯„åˆ†å™¨å’Œé£æ ¼å™¨ï¼ˆmmSSRï¼‰ç¡®ä¿ä»¥å¤šæ ·åŒ–çš„å½¢å¼å‘ç”¨æˆ·ä¼ é€’é«˜å¾—åˆ†çš„ä¿¡æ¯ã€‚æ— éœ€åŸºäºåµŒå…¥çš„èšç±»æˆ–è´ªå©ªé‡‡æ ·ï¼ŒmmSSRå¯æœ‰æ•ˆåœ°æ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªä¸åŒé¢„ç®—çº¦æŸçš„æ•°æ®ï¼Œæ”¯æŒé€šç”¨æˆ–ç‰¹å®šèƒ½åŠ›è·å–çš„å®šåˆ¶ï¼Œå¹¶ä¿ƒè¿›æ— éœ€åŸ¹è®­çš„é€šç”¨åŒ–åˆ°æ–°é¢†åŸŸè¿›è¡Œæ•´ç†ã€‚åœ¨10å¤šä¸ªå®éªŒç¯å¢ƒä¸­ï¼Œç»è¿‡14ä¸ªå¤šæ¨¡æ€åŸºå‡†éªŒè¯ï¼Œä¸éšæœºæŠ½æ ·ã€åŸºçº¿ç­–ç•¥å’Œæœ€å…ˆè¿›çš„ç­›é€‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬è¡¨ç°å‡ºäº†æŒç»­çš„ä¼˜åŠ¿ï¼Œåœ¨ä»…ä½¿ç”¨260ä¸‡æ•°æ®çš„30%çš„æƒ…å†µä¸‹å®ç°äº†99.1%çš„å®Œå…¨æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13383v1">PDF</a> update comparison with sota and analysis</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰åœ¨æ•°æ®é€‰æ‹©å’Œè¯„ä»·æ–¹é¢çš„æŒ‘æˆ˜ä¸åˆ›æ–°ã€‚ç ”ç©¶æå‡ºé€šè¿‡é‡æ–°å®šä¹‰è´¨é‡åº¦é‡çš„ç²’åº¦ï¼Œå°†å…¶åˆ†è§£ä¸º14ç§ä¸è§†è§‰è¯­è¨€ç›¸å…³çš„èƒ½åŠ›ï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€ä¸°å¯Œè¯„åˆ†å™¨æ¥è¯„ä¼°æ¯ä¸ªæ•°æ®å€™é€‰è€…çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä¸ºäº†ä¿ƒè¿›å¤šæ ·æ€§ï¼Œç ”ç©¶é‡‡ç”¨äº¤äº’é£æ ¼ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨å¤šæ¨¡æ€ä¸°å¯Œé£æ ¼å™¨æ¥è¯†åˆ«æ•°æ®æŒ‡ä»¤æ¨¡å¼ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œç ”ç©¶å¼€å‘çš„mmSSRæŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸åŒé¢„ç®—çº¦æŸä¸‹ï¼Œæœ‰æ•ˆæ‰©å±•åˆ°æ•°ç™¾ä¸‡æ•°æ®ï¼Œæ”¯æŒé€šç”¨æˆ–ç‰¹å®šèƒ½åŠ›è·å–éœ€æ±‚çš„å®šåˆ¶ï¼Œå¹¶ä¿ƒè¿›å¯¹æ–°é¢†åŸŸçš„æ— è®­ç»ƒæ³›åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸éšæœºé‡‡æ ·ã€åŸºå‡†ç­–ç•¥ä»¥åŠå…ˆè¿›çš„é€‰æ‹©æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå®éªŒè®¾ç½®å’Œ14ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸€è‡´åœ°æ›´å¥½ï¼Œä»¥ä»…ä½¿ç”¨30%çš„260ä¸‡æ•°æ®å®ç°äº†99.1%çš„å®Œå…¨æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¾®è°ƒé˜¶æ®µåªéœ€è¦æœ€å°‘çš„ç›‘ç£ã€‚</li>
<li>æ•°æ®é€‰æ‹©å’Œç ”ç©¶çš„æœ€æ–°è¿›å±•è¯å®äº†è¿™ä¸€å‡è®¾ã€‚</li>
<li>å¤šæ¨¡æ€LLMï¼ˆMLLMsï¼‰åœ¨æé«˜æ•°æ®é€‰æ‹©å’Œé‡è¦æ€§æ–¹é¢æœ‰æ›´å¤æ‚çš„éœ€æ±‚ã€‚</li>
<li>ä¸ºäº†æœ‰æ•ˆåœ°æ”¶é›†å¤šæ¨¡å¼æ•™å­¦æ•°æ®ï¼Œé‡æ–°å®šä¹‰äº†è´¨é‡åº¦é‡çš„ç²’åº¦ï¼Œå¹¶å¼•å…¥äº†å¤šæ¨¡æ€ä¸°å¯Œè¯„åˆ†å™¨ã€‚</li>
<li>äº¤äº’é£æ ¼è¢«è§†ä¸ºæ•°æ®å¤šæ ·æ€§çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ã€‚</li>
<li>mmSSRæŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸åŒé¢„ç®—çº¦æŸä¸‹æœ‰æ•ˆæ‰©å±•åˆ°å¤§é‡æ•°æ®ï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-63f6f2e4052adb839d25fa3501e9371c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-759ea6afe6d013e382fce16231d53ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae4aa0d01a00b084fbf0a3221e0f44c7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Mitigating-Visual-Forgetting-via-Take-along-Visual-Conditioning-for-Multi-modal-Long-CoT-Reasoning"><a href="#Mitigating-Visual-Forgetting-via-Take-along-Visual-Conditioning-for-Multi-modal-Long-CoT-Reasoning" class="headerlink" title="Mitigating Visual Forgetting via Take-along Visual Conditioning for   Multi-modal Long CoT Reasoning"></a>Mitigating Visual Forgetting via Take-along Visual Conditioning for   Multi-modal Long CoT Reasoning</h2><p><strong>Authors:Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVistaâ€™s test-hard subset, revealing the modelâ€™s textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»å±•ç°å‡ºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä»æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå‘å±•åˆ°é¢å‘äº§å“çš„å…ˆè¿›è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚OpenAI o1ã€‚åœ¨æˆ‘ä»¬é‡æ–°å®ç°æ­¤æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å‘ç°å¤šæ¨¡æ€ä»»åŠ¡éœ€è¦è§†è§‰è¾“å…¥ï¼ˆä¾‹å¦‚å‡ ä½•é—®é¢˜ï¼‰ï¼Œå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰åœ¨ç»´æŒå¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨æ–¹é¢é‡åˆ°å›°éš¾ã€‚æ¢å¥è¯è¯´ï¼Œéšç€æ¨ç†çš„è¿›è¡Œï¼ŒMLLMå¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨é€æ¸ä¸‹é™ï¼Œå¯¼è‡´è¿‡åº¦ä¾èµ–æ–‡æœ¬è¾“å‡ºã€‚ä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨é•¿é“¾æ¨ç†è¿‡ç¨‹ä¸­æ¶ˆé™¤å›¾åƒè¾“å…¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸­é€”æˆªæ–­ï¼Œç„¶åç§»é™¤è¾“å…¥å›¾åƒé‡æ–°å®Œæˆæ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨MathVistaçš„ç¡¬æµ‹è¯•å­é›†ä¸Šä»…è§‚å¯Ÿåˆ°çº¦2%çš„å‡†ç¡®ç‡ä¸‹é™ï¼Œè¡¨æ˜æ¨¡å‹çš„æ–‡æœ¬è¾“å‡ºä¸»å¯¼äº†åç»­çš„æ¨ç†è¿‡ç¨‹ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œéšè¡Œè§†è§‰æ¡ä»¶â€ï¼ˆTVCï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†å›¾åƒè¾“å…¥è½¬ç§»åˆ°å…³é”®çš„æ¨ç†é˜¶æ®µï¼Œå¹¶é€šè¿‡åŠ¨æ€å‰ªæå‹ç¼©å†—ä½™çš„è§†è§‰æ ‡è®°ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæ¨¡å‹åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„æ³¨æ„åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹³å‡åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ˆ+3.4%å¯¹æ¯”ä¹‹å‰çš„æœ€ä¼˜æ°´å¹³ï¼‰ï¼Œè¯æ˜äº†TVCåœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13360v1">PDF</a> The project page is available at   <a target="_blank" rel="noopener" href="https://sun-hailong.github.io/projects/TVC">https://sun-hailong.github.io/projects/TVC</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸLLMåœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„è¿›æ­¥ä»Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå‘å±•åˆ°OpenAI o1ç­‰é¢å‘äº§å“çš„å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚åœ¨é‡æ–°å®ç°æ­¤æ¨¡å‹æ—¶å‘ç°ï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼ŒMultimodal LLMsï¼ˆMLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­éš¾ä»¥ç»´æŒå¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨ï¼Œå¯¼è‡´æ–‡æœ¬è¿‡åº¦ä¾èµ–è¾“å‡ºã€‚ç ”ç©¶é€šè¿‡æˆªæ–­æ¨ç†è¿‡ç¨‹å¹¶ç§»é™¤å›¾åƒè¾“å…¥è¿›è¡Œè§‚å¯Ÿï¼Œå‘ç°ä»…åœ¨MathVistaçš„test-hardå­é›†ä¸Šå‡†ç¡®ç‡ä¸‹é™çº¦2%ï¼Œæ˜¾ç¤ºæ¨¡å‹è¾“å‡ºä¸»è¦ç”±æ–‡æœ¬é©±åŠ¨ã€‚ä¸ºæ­¤ï¼Œæå‡ºTake-along Visual Conditioningï¼ˆTVCï¼‰ç­–ç•¥ï¼Œå°†å›¾åƒè¾“å…¥è½¬ç§»åˆ°å…³é”®æ¨ç†é˜¶æ®µï¼Œå¹¶é€šè¿‡åŠ¨æ€å‰ªæå‹ç¼©å†—ä½™è§†è§‰ç¬¦å·ã€‚è¯¥æ–¹æ³•æœ‰åŠ©äºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„æ³¨æ„åŠ›ã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ˆ+3.4%ï¼‰ï¼Œè¯æ˜TVCåœ¨æé«˜å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­é¢ä¸´å¯¹è§†è§‰ä¿¡æ¯å…³æ³¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>MLLMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šè¿‡åº¦ä¾èµ–æ–‡æœ¬è¾“å‡ºã€‚</li>
<li>é€šè¿‡æˆªæ–­å¹¶ç§»é™¤å›¾åƒè¾“å…¥çš„å®éªŒè§‚å¯Ÿï¼Œå‘ç°æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨åº¦ä¸‹é™ã€‚</li>
<li>æå‡ºTake-along Visual Conditioningï¼ˆTVCï¼‰ç­–ç•¥ï¼Œå°†å›¾åƒè¾“å…¥è½¬ç§»åˆ°å…³é”®æ¨ç†é˜¶æ®µã€‚</li>
<li>TVCç­–ç•¥é€šè¿‡åŠ¨æ€å‰ªæå‹ç¼©å†—ä½™è§†è§‰ç¬¦å·ï¼Œå¸®åŠ©æ¨¡å‹ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„æ³¨æ„åŠ›ã€‚</li>
<li>åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTVCç­–ç•¥è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8725b11ba2c2d90eee3538510e4f17b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47ba6bf717776236bffe18a51fa12566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f40330e2c1b7b0d762e2e5d31c7e364.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ae23d62b0ce74f4fd5dcdcd361ef432.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e75a2e23b1d55e50b25b4c126aabe5b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a6ce182e379b6068bb7ae8f1aa4a406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de2261852e2c300a7628d88c598d44b9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Agents-Play-Thousands-of-3D-Video-Games"><a href="#Agents-Play-Thousands-of-3D-Video-Games" class="headerlink" title="Agents Play Thousands of 3D Video Games"></a>Agents Play Thousands of 3D Video Games</h2><p><strong>Authors:Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, Wei Yang</strong></p>
<p>We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTALâ€™s effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on <a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">https://zhongwen.one/projects/portal</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†PORTALï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œç”¨äºå¼€å‘èƒ½å¤Ÿé€šè¿‡è¯­è¨€æŒ‡å¯¼ç­–ç•¥ç”Ÿæˆç©æ•°åƒæ¬¾3Dè§†é¢‘æ¸¸æˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚é€šè¿‡å°†å†³ç­–é—®é¢˜è½¬å˜ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆä»¥é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰è¡¨ç¤ºçš„è¡Œä¸ºæ ‘ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿ç•™äº†æˆ˜ç•¥æ·±åº¦å’Œå¿«é€Ÿé€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ··åˆç­–ç•¥ç»“æ„ï¼Œè¯¥ç»“æ„ç»“åˆäº†åŸºäºè§„åˆ™çš„èŠ‚ç‚¹å’Œç¥ç»ç½‘ç»œç»„ä»¶ï¼Œèƒ½å¤Ÿå®ç°é«˜çº§æˆ˜ç•¥æ¨ç†å’Œç²¾ç¡®çš„ä½çº§æ§åˆ¶ã€‚é‡‡ç”¨èåˆå®šé‡æ¸¸æˆæŒ‡æ ‡å’Œè§†è§‰è¯­è¨€æ¨¡å‹åˆ†æçš„åŒé‡åé¦ˆæœºåˆ¶ï¼Œä¿ƒè¿›äº†æˆ˜æœ¯å’Œæˆ˜ç•¥å±‚é¢ç­–ç•¥çš„è¿­ä»£æ”¹è¿›ã€‚æ‰€å¾—åˆ°çš„ç­–ç•¥å¯å³æ—¶éƒ¨ç½²ã€äººç±»å¯è§£é‡Šï¼Œå¹¶èƒ½å¤Ÿè·¨ä¸åŒæ¸¸æˆç¯å¢ƒè¿›è¡Œæ¨å¹¿ã€‚å®éªŒç»“æœè¯æ˜äº†PORTALåœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»ï¼ˆFPSï¼‰æ¸¸æˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¼€å‘æ•ˆç‡ã€ç­–ç•¥æ¨å¹¿å’Œè¡Œä¸ºå¤šæ ·æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚PORTALä»£è¡¨äº†æ¸¸æˆAIå¼€å‘ä¸­çš„ä¸€é¡¹é‡å¤§è¿›å±•ï¼Œä¸ºåˆ›å»ºèƒ½å¤Ÿåœ¨æ•°åƒæ¬¾å•†ä¸šè§†é¢‘æ¸¸æˆä¸­è¿è¡Œä¸”å¼€å‘æˆæœ¬æä½çš„å¤æ‚ä»£ç†æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚æœ‰å…³3Dè§†é¢‘æ¸¸æˆçš„å®éªŒç»“æœè¯·è®¿é—® <a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">https://zhongwen.one/projects/portal</a> æŸ¥çœ‹æ•ˆæœæœ€ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¯­è¨€æŒ‡å¯¼ç­–ç•¥ç”Ÿæˆæ¡†æ¶PORTALï¼Œå®ç°äººå·¥æ™ºèƒ½åœ¨ä¸‰åƒç»´æ¸¸æˆä¸­çš„åº”ç”¨ã€‚åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œç”Ÿæˆä»¥é¢†åŸŸç‰¹å®šè¯­è¨€è¡¨ç¤ºçš„è¡Œä¸ºæ ‘ï¼Œæå‡ç­–ç•¥æ·±åº¦å’Œå¿«é€Ÿé€‚åº”æ€§ï¼ŒåŒæ—¶é™ä½è®¡ç®—è´Ÿæ‹…ã€‚å¼•å…¥æ··åˆæ”¿ç­–ç»“æ„ï¼Œç»“åˆè§„åˆ™èŠ‚ç‚¹å’Œç¥ç»ç½‘ç»œç»„ä»¶ï¼Œå®ç°é«˜å±‚æˆ˜ç•¥æ¨ç†å’Œç²¾ç¡®ä½çº§æ§åˆ¶ã€‚é€šè¿‡ç»“åˆå®šé‡æ¸¸æˆæŒ‡æ ‡å’Œè§†è§‰è¯­è¨€æ¨¡å‹åˆ†æçš„åŒåé¦ˆæœºåˆ¶ï¼Œå®ç°æˆ˜æœ¯å’Œæˆ˜ç•¥å±‚é¢çš„æ”¿ç­–è¿­ä»£æ”¹è¿›ã€‚æ”¿ç­–å³æ—¶éƒ¨ç½²ã€äººç±»å¯è§£é‡Šï¼Œå¯æ¨å¹¿è‡³ä¸åŒæ¸¸æˆç¯å¢ƒã€‚åœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆä¸­éªŒè¯äº†PORTAçš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—æé«˜å¼€å‘æ•ˆç‡ã€æ”¿ç­–æ¨å¹¿å’Œè¡Œä¸ºå¤šæ ·æ€§ã€‚è¯¦æƒ…è¯·è®¿é—® <a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">https://zhongwen.one/projects/portal</a> æŸ¥çœ‹å®éªŒç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PORTALæ˜¯ä¸€ä¸ªæ”¯æŒå¤šç§æ¸¸æˆçš„AIå¼€å‘æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¸¸æˆAIçš„å¼€å‘æ•ˆç‡å’Œå¤æ‚æ€§ã€‚</li>
<li>é€šè¿‡å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¡Œä¸ºæ ‘ã€‚</li>
<li>å¼•å…¥æ··åˆæ”¿ç­–ç»“æ„ä»¥å®ç°é«˜çº§ç­–ç•¥æ¨ç†å’Œä½çº§ç²¾ç¡®æ§åˆ¶ç›¸ç»“åˆçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c167cc28f9e4d1240446b3e2193eb448.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b0b221d0b53ecebb1c021db30418ac3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Valid-Text-to-SQL-Generation-with-Unification-based-DeepStochLog"><a href="#Valid-Text-to-SQL-Generation-with-Unification-based-DeepStochLog" class="headerlink" title="Valid Text-to-SQL Generation with Unification-based DeepStochLog"></a>Valid Text-to-SQL Generation with Unification-based DeepStochLog</h2><p><strong>Authors:Ying Jiao, Luc De Raedt, Giuseppe Marra</strong></p>
<p>Large language models have been used to translate natural language questions to SQL queries. Without hard constraints on syntax and database schema, they occasionally produce invalid queries that are not executable. These failures limit the usage of these systems in real-life scenarios. We propose a neurosymbolic framework that imposes SQL syntax and schema constraints with unification-based definite clause grammars and thus guarantees the generation of valid queries. Our framework also builds a bi-directional interface to language models to leverage their natural language understanding abilities. The evaluation results on a subset of SQL grammars show that all our output queries are valid. This work is the first step towards extending language models with unification-based grammars. We demonstrate this extension enhances the validity, execution accuracy, and ground truth alignment of the underlying language model by a large margin. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ML-KULeuven/deepstochlog-lm">https://github.com/ML-KULeuven/deepstochlog-lm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²è¢«ç”¨äºå°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘ä¸ºSQLæŸ¥è¯¢ã€‚åœ¨ä¸å—è¯­æ³•å’Œæ•°æ®åº“æ¨¡å¼ä¸¥æ ¼çº¦æŸçš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬å¶å°”ä¼šç”Ÿæˆæ— æ³•æ‰§è¡Œçš„æ— æ•ˆæŸ¥è¯¢ã€‚è¿™äº›å¤±è´¥é™åˆ¶äº†è¿™äº›ç³»ç»Ÿåœ¨ç°å®åœºæ™¯ä¸­çš„ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŸºäºç»Ÿä¸€çš„ç¡®å®šæ€§å­å¥è¯­æ³•æ¥å®æ–½SQLè¯­æ³•å’Œæ¨¡å¼çº¦æŸï¼Œä»è€Œç¡®ä¿ç”Ÿæˆæœ‰æ•ˆçš„æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜å»ºç«‹äº†ä¸€ä¸ªä¸è¯­è¨€æ¨¡å‹çš„åŒå‘æ¥å£ï¼Œä»¥åˆ©ç”¨å…¶è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚åœ¨SQLè¯­æ³•å­é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬è¾“å‡ºçš„æ‰€æœ‰æŸ¥è¯¢éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚è¿™é¡¹å·¥ä½œæ˜¯åŸºäºç»Ÿä¸€çš„è¯­æ³•æ‰©å±•è¯­è¨€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ã€‚æˆ‘ä»¬è¯æ˜è¿™ç§æ‰©å±•å¤§å¤§æé«˜äº†åŸºç¡€è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€æ‰§è¡Œå‡†ç¡®æ€§å’Œä¸çœŸå®æƒ…å†µçš„åŒ¹é…åº¦ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ML-KULeuven/deepstochlog-lm%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ML-KULeuven/deepstochlog-lmè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºå°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSQLæŸ¥è¯¢æ—¶ï¼Œå¯èƒ½ä¼šå› æ²¡æœ‰ä¸¥æ ¼çš„è¯­æ³•å’Œæ•°æ®åº“æ¨¡å¼çº¦æŸè€Œäº§ç”Ÿæ— æ•ˆçš„æŸ¥è¯¢ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºç»Ÿä¸€çš„ç¡®å®šæ€§å­å¥è¯­æ³•å¯¹SQLè¯­æ³•å’Œæ¨¡å¼è¿›è¡Œçº¦æŸï¼Œç¡®ä¿ç”Ÿæˆæœ‰æ•ˆçš„æŸ¥è¯¢ã€‚è¯¥æ¡†æ¶è¿˜æ„å»ºäº†ä¸è¯­è¨€æ¨¡å‹çš„åŒå‘æ¥å£ï¼Œåˆ©ç”¨å…¶è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è¾“å‡ºæŸ¥è¯¢å‡ä¸ºæœ‰æ•ˆã€‚è¿™é¡¹å·¥ä½œæ˜¯åœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥åŸºäºç»Ÿä¸€çš„è¯­æ³•çš„é‡è¦å°è¯•ï¼Œæ˜¾è‘—æé«˜è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€æ‰§è¡Œå‡†ç¡®æ€§å’Œä¸çœŸå®æ•°æ®çš„å¯¹é½ç¨‹åº¦ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢æ—¶å¯èƒ½äº§ç”Ÿæ— æ•ˆæŸ¥è¯¢ã€‚</li>
<li>æå‡ºçš„ç¥ç»ç¬¦å·æ¡†æ¶é€šè¿‡å¼•å…¥SQLè¯­æ³•å’Œæ¨¡å¼çº¦æŸç¡®ä¿ç”Ÿæˆæœ‰æ•ˆæŸ¥è¯¢ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºç»Ÿä¸€çš„ç¡®å®šæ€§å­å¥è¯­æ³•æ„å»ºåŒå‘æ¥å£ä¸è¯­è¨€æ¨¡å‹äº¤äº’ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºæ‰€æœ‰è¾“å‡ºæŸ¥è¯¢å‡ä¸ºæœ‰æ•ˆã€‚</li>
<li>ä¸ä¼ ç»Ÿè¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæ­¤æ¡†æ¶æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€æ‰§è¡Œå‡†ç¡®æ€§å’Œä¸çœŸå®æ•°æ®çš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶çš„å®ç°åœ¨GitHubä¸Šæœ‰å…¬å¼€çš„ä»£ç å¯ä¾›è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1ee815a9c4165ff7ddfa2bd954797508.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a839f5c0814829f297c2ccd29758afa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4d4ec580e3489fe84261c4ad0a80900.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9147722cf54332728d88dcd98564ab6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations"><a href="#Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations" class="headerlink" title="Edit Transfer: Learning Image Editing via Vision In-Context Relations"></a>Edit Transfer: Learning Image Editing via Vision In-Context Relations</h2><p><strong>Authors:Lan Chen, Qi Mao, Yuchao Gu, Mike Zheng Shou</strong></p>
<p>We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°è®¾ç½®ï¼Œåä¸ºâ€œç¼–è¾‘ä¼ è¾“â€ï¼ˆEdit Transferï¼‰ï¼Œåœ¨è¯¥è®¾ç½®ä¸­ï¼Œæ¨¡å‹ä»…ä»ä¸€ä¸ªæºç›®æ ‡ç¤ºä¾‹ä¸­å­¦ä¹ å˜æ¢ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚è™½ç„¶åŸºäºæ–‡æœ¬çš„æ–¹æ³•åœ¨é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œè¯­ä¹‰æ“ä½œæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸å¯¹äºç²¾ç¡®çš„å‡ ä½•ç»†èŠ‚ï¼ˆä¾‹å¦‚å§¿åŠ¿å’Œè§†ç‚¹å˜åŒ–ï¼‰æ„Ÿåˆ°æ£˜æ‰‹ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºå‚è€ƒçš„ç¼–è¾‘é€šå¸¸ä¾§é‡äºé£æ ¼æˆ–å¤–è§‚ï¼Œè€Œåœ¨éåˆšæ€§å˜æ¢æ–¹é¢è¡¨ç°ä¸ä½³ã€‚é€šè¿‡ä»æºç›®æ ‡å¯¹ä¸­å­¦ä¹ ç¼–è¾‘å˜æ¢ï¼ŒEdit Transfer ç¼“è§£äº†ä»…ä½¿ç”¨æ–‡æœ¬å’Œå¤–è§‚ä¸ºä¸­å¿ƒçš„å‚è€ƒçš„é™åˆ¶ã€‚æˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§åŸºäºDiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ã€‚æˆ‘ä»¬å°†ç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒæ’åˆ—æˆç»Ÿä¸€çš„å››é¢æ¿ç»„åˆï¼Œç„¶ååº”ç”¨è½»é‡çº§çš„LoRAå¾®è°ƒï¼Œä»¥ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ å¤æ‚çš„ç©ºé—´å˜æ¢ã€‚å°½ç®¡åªä½¿ç”¨äº†42ä¸ªè®­ç»ƒæ ·æœ¬ï¼ŒEdit Transferåœ¨å¤šç§éåˆšæ€§åœºæ™¯ä¸Šå¤§å¹…è¶…è¶Šäº†æœ€å…ˆè¿›çš„TIEå’ŒRIEæ–¹æ³•ï¼Œè¯æ˜äº†å°‘é‡è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13327v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”Edit Transferï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿä»å•ä¸€æºç›®æ ‡å®ä¾‹å­¦ä¹ è½¬æ¢å¹¶åº”ç”¨äºæ–°æŸ¥è¯¢å›¾åƒã€‚Edit Transferå¼¥è¡¥äº†çº¯æ–‡æœ¬å’Œå¤–è§‚å‚è€ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œé€šè¿‡ä»æºåˆ°ç›®æ ‡å¯¹æ˜¾å¼å­¦ä¹ ç¼–è¾‘è½¬æ¢æ¥å®ç°ã€‚è¯¥æŠ€æœ¯å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸Šä¸‹æ–‡å­¦ä¹ çš„å¯å‘ï¼Œå»ºç«‹åœ¨ä¸€ä¸ªåŸºäºDiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¹‹ä¸Šã€‚é€šè¿‡å°†ç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒç»„åˆæˆç»Ÿä¸€çš„å››é¢æ¿å¤åˆä½“ï¼Œç„¶ååº”ç”¨è½»é‡çº§çš„LoRAå¾®è°ƒæ¥ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ å¤æ‚çš„ç©ºé—´è½¬æ¢ã€‚å°½ç®¡åªä½¿ç”¨äº†42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä½†Edit Transferåœ¨å¤šæ ·çš„éåˆšæ€§åœºæ™¯ä¸Šå¤§å¹…è¶…è¶Šäº†æœ€æ–°çš„TIEå’ŒRIEæ–¹æ³•ï¼Œè¯æ˜äº†å°‘æ•°è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Edit TransferæŠ€æœ¯èƒ½ä»å•ä¸€æºç›®æ ‡å®ä¾‹å­¦ä¹ è½¬æ¢å¹¶åº”ç”¨äºæ–°æŸ¥è¯¢å›¾åƒã€‚</li>
<li>Edit Transferå¼¥è¡¥äº†æ–‡æœ¬æ–¹æ³•å’Œå¤–è§‚å‚è€ƒæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æŠ€æœ¯é€šè¿‡ä»æºåˆ°ç›®æ ‡çš„æ˜¾å¼å­¦ä¹ æ¥å®ç°ç¼–è¾‘è½¬æ¢ã€‚</li>
<li>Edit Transferå—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸Šä¸‹æ–‡å­¦ä¹ çš„å¯å‘ã€‚</li>
<li>è¯¥æŠ€æœ¯å»ºç«‹åœ¨ä¸€ä¸ªåŸºäºDiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¹‹ä¸Šã€‚</li>
<li>é€šè¿‡å°†ç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒç»„åˆæˆå››é¢æ¿å¤åˆä½“æ¥åº”ç”¨ç¼–è¾‘è½¬æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62abc8701cc0822cc38b0abdfc84a860.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a87e29fd01504a9ca50c9c786fed625.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be31f915090bc4a5ddf7e8b10eb97fe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f5d3a1333ed599da96a19b547d8ddf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66a3ce0dce3f9e275e16027cc270189d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76404e796fc375a36fe8e0ccf676dd2a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LIMCA-LLM-for-Automating-Analog-In-Memory-Computing-Architecture-Design-Exploration"><a href="#LIMCA-LLM-for-Automating-Analog-In-Memory-Computing-Architecture-Design-Exploration" class="headerlink" title="LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design   Exploration"></a>LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design   Exploration</h2><p><strong>Authors:Deepak Vungarala, Md Hasibul Amin, Pietro Mercati, Arnob Ghosh, Arman Roohi, Ramtin Zand, Shaahin Angizi</strong></p>
<p>Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as a promising architecture for Deep Neural Network (DNN) acceleration, offering high memory bandwidth and in-situ computation. However, the manual, knowledge-intensive design process and the lack of high-quality circuit netlists have significantly constrained design space exploration and optimization to behavioral system-level tools. In this work, we introduce LIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for automating the design and evaluation of IMC crossbar architectures. Unlike traditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated pipeline to generate and validate circuit netlists for SPICE simulations, eliminating manual intervention. LIMCA systematically explores the IMC design space by leveraging a structured dataset and LLM-based performance evaluation. Our experimental results on MNIST classification demonstrate that LIMCA successfully generates crossbar designs achieving $\geq$96% accuracy while maintaining a power consumption $\leq$3W, making this the first work in LLM-assisted IMC design space exploration. Compared to existing frameworks, LIMCA provides an automated, scalable, and hardware-aware solution, reducing design exploration time while ensuring user-constrained performance trade-offs. </p>
<blockquote>
<p>ç”µé˜»å¼äº¤å‰ç»“æ„ä¸ºå®ç°æ¨¡æ‹Ÿå†…å­˜è®¡ç®—ï¼ˆIMCï¼‰æä¾›äº†æœ‰å‰æ™¯çš„æ¶æ„ï¼Œç”¨äºåŠ é€Ÿæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œæä¾›é«˜å†…å­˜å¸¦å®½å’Œç°åœºè®¡ç®—åŠŸèƒ½ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨ã€çŸ¥è¯†å¯†é›†å‹çš„è®¾è®¡æµç¨‹ä»¥åŠç¼ºä¹é«˜è´¨é‡çš„ç”µè·¯ç½‘è¡¨ï¼Œæå¤§åœ°é™åˆ¶äº†è®¾è®¡ç©ºé—´çš„æ¢ç´¢å’Œä¼˜åŒ–ï¼Œä½¿å…¶ä»…é€‚ç”¨äºè¡Œä¸ºçº§ç³»ç»Ÿå·¥å…·ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LIMCAï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨æ¡†æ¶ï¼Œå¯è‡ªåŠ¨åŒ–IMCäº¤å‰ç»“æ„çš„è®¾è®¡è¯„ä¼°ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒLIMCAé‡‡ç”¨æ— äººå·¥é—­ç¯ï¼ˆNHILï¼‰è‡ªåŠ¨åŒ–ç®¡é“ç”Ÿæˆå’ŒéªŒè¯SPICEæ¨¡æ‹Ÿçš„ç”µè·¯ç½‘è¡¨ï¼Œæ¶ˆé™¤äº†äººå·¥å¹²é¢„ã€‚LIMCAé€šè¿‡åˆ©ç”¨ç»“æ„åŒ–æ•°æ®é›†å’ŒåŸºäºLLMçš„æ€§èƒ½è¯„ä¼°ï¼Œç³»ç»Ÿåœ°æ¢ç´¢äº†IMCè®¾è®¡ç©ºé—´ã€‚æˆ‘ä»¬åœ¨MNISTåˆ†ç±»ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLIMCAæˆåŠŸç”Ÿæˆäº¤å‰ç»“æ„è®¾è®¡ï¼Œå‡†ç¡®ç‡â‰¥96%ï¼ŒåŒæ—¶ä¿æŒåŠŸè€—â‰¤3Wï¼Œè¿™æ˜¯LLMè¾…åŠ©IMCè®¾è®¡ç©ºé—´æ¢ç´¢çš„é¦–æ¬¡å·¥ä½œã€‚ä¸ç°æœ‰æ¡†æ¶ç›¸æ¯”ï¼ŒLIMCAæä¾›äº†è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•å’Œç¡¬ä»¶æ„ŸçŸ¥çš„è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†è®¾è®¡æ¢ç´¢æ—¶é—´ï¼ŒåŒæ—¶ç¡®ä¿ç”¨æˆ·é™åˆ¶çš„æ€§èƒ½æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13301v1">PDF</a> 4 Figures, 5 Tables</p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ— äººå·¥å¹²é¢„çš„äº¤å‰æ¡è®¾è®¡æ¡†æ¶LIMCAï¼Œè‡ªåŠ¨åŒ–å®ç°å†…å­˜è®¡ç®—ï¼ˆIMCï¼‰æ¶æ„çš„è®¾è®¡å’Œä¼˜åŒ–ï¼Œæå‡ç¥ç»ç½‘ç»œåŠ é€Ÿæ€§èƒ½ã€‚åˆ©ç”¨LLMè¿›è¡Œæ€§èƒ½è¯„ä¼°ï¼Œç³»ç»Ÿæ¢ç´¢è®¾è®¡ç©ºé—´ï¼Œç”Ÿæˆç”µè·¯ç½‘è¡¨è¿›è¡ŒSPICEä»¿çœŸéªŒè¯ã€‚åœ¨MNISTåˆ†ç±»å®éªŒä¸­ï¼ŒLIMCAæˆåŠŸç”Ÿæˆå‡†ç¡®åº¦é«˜ä¸”åŠŸè€—ä½çš„äº¤å‰æ¡è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºè‡ªåŠ¨åŒ–è®¾è®¡å†…å­˜è®¡ç®—ï¼ˆIMCï¼‰æ¶æ„çš„äº¤å‰æ¡ï¼Œå®ç°æ— äººå·¥å¹²é¢„çš„è‡ªåŠ¨åŒ–è®¾è®¡ã€‚</li>
<li>LLMç”¨äºæ€§èƒ½è¯„ä¼°å’Œç³»ç»Ÿæ¢ç´¢è®¾è®¡ç©ºé—´ï¼Œæé«˜è®¾è®¡æ•ˆç‡ã€‚</li>
<li>LIMCAæ¡†æ¶é€šè¿‡ç”Ÿæˆç”µè·¯ç½‘è¡¨è¿›è¡ŒSPICEä»¿çœŸéªŒè¯ï¼Œç¡®ä¿è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>MNISTåˆ†ç±»å®éªŒè¯æ˜ï¼ŒLIMCAç”Ÿæˆçš„äº¤å‰æ¡è®¾è®¡è¾¾åˆ°96%ä»¥ä¸Šçš„å‡†ç¡®åº¦ã€‚</li>
<li>LIMCAè®¾è®¡åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†ä½åŠŸè€—ï¼ŒåŠŸç‡æ¶ˆè€—ä½äºæˆ–ç­‰äº3Wã€‚</li>
<li>ä¸ç°æœ‰æ¡†æ¶ç›¸æ¯”ï¼ŒLIMCAæä¾›è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•å’Œç¡¬ä»¶æ„ŸçŸ¥çš„è§£å†³æ–¹æ¡ˆï¼Œç¼©çŸ­äº†è®¾è®¡æ¢ç´¢æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f54d2cef49b50cec74f0e31f3f331b20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dedf746c33078c620656c377327c3e06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb44af80bc9007b3418fc62802acd11d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4952a3d29374313a9b905cec94d7df0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fb436bc188d5a6c8a5e9f05f8a083fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a04b887df89d88cdb5014b62567a6bb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5850a0522be1781d86680f0198a2313.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03eb17e25ff5d6aa60d3265517621c9f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="3DAxisPrompt-Promoting-the-3D-Grounding-and-Reasoning-in-GPT-4o"><a href="#3DAxisPrompt-Promoting-the-3D-Grounding-and-Reasoning-in-GPT-4o" class="headerlink" title="3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o"></a>3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o</h2><p><strong>Authors:Dingning Liu, Cheng Wang, Peng Gao, Renrui Zhang, Xinzhu Ma, Yuan Meng, Zhihui Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) exhibit impressive capabilities across a variety of tasks, especially when equipped with carefully designed visual prompts. However, existing studies primarily focus on logical reasoning and visual understanding, while the capability of MLLMs to operate effectively in 3D vision remains an ongoing area of exploration. In this paper, we introduce a novel visual prompting method, called 3DAxisPrompt, to elicit the 3D understanding capabilities of MLLMs in real-world scenes. More specifically, our method leverages the 3D coordinate axis and masks generated from the Segment Anything Model (SAM) to provide explicit geometric priors to MLLMs and then extend their impressive 2D grounding and reasoning ability to real-world 3D scenarios. Besides, we first provide a thorough investigation of the potential visual prompting formats and conclude our findings to reveal the potential and limits of 3D understanding capabilities in GPT-4o, as a representative of MLLMs. Finally, we build evaluation environments with four datasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various 3D tasks. Based on this, we conduct extensive quantitative and qualitative experiments, which demonstrate the effectiveness of the proposed method. Overall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can effectively perceive an objectâ€™s 3D position in real-world scenarios. Nevertheless, a single prompt engineering approach does not consistently achieve the best outcomes for all 3D tasks. This study highlights the feasibility of leveraging MLLMs for 3D vision grounding&#x2F;reasoning with prompt engineering techniques. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å½“é…å¤‡ç²¾å¿ƒè®¾è®¡çš„è§†è§‰æç¤ºæ—¶ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€»è¾‘ç†æ€§å’Œè§†è§‰ç†è§£ä¸Šï¼Œè€ŒMLLMåœ¨3Dè§†è§‰ä¸­æœ‰æ•ˆæ“ä½œçš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ­£åœ¨æ¢ç´¢çš„é¢†åŸŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰æç¤ºæ–¹æ³•ï¼Œç§°ä¸º3DAxisPromptï¼Œä»¥æ¿€å‘MLLMåœ¨ç°å®åœºæ™¯ä¸­çš„3Dç†è§£èƒ½åŠ›ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç”±Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„3Dåæ ‡è½´å’Œæ©ç ï¼Œä¸ºMLLMæä¾›æ˜ç¡®çš„å‡ ä½•å…ˆéªŒï¼Œç„¶åå°†å®ƒä»¬ä»¤äººå°è±¡æ·±åˆ»çš„2Dæ¥åœ°å’Œæ¨ç†èƒ½åŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„3Dåœºæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡è°ƒæŸ¥äº†æ½œåœ¨çš„è§†è§‰æç¤ºæ ¼å¼ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼Œä»¥æ­ç¤ºGPT-4oä½œä¸ºMLLMä»£è¡¨åœ¨3Dç†è§£æ–¹é¢çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ScanReferã€ScanNetã€FMBå’ŒnuSceneå››ä¸ªæ•°æ®é›†æ„å»ºäº†è¯„ä¼°ç¯å¢ƒï¼Œæ¶µç›–å„ç§3Dä»»åŠ¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå€ŸåŠ©3DAxisPromptçš„MLLMå¯ä»¥æœ‰æ•ˆåœ°æ„ŸçŸ¥ç°å®åœºæ™¯ä¸­ç‰©ä½“çš„3Dä½ç½®ã€‚ç„¶è€Œï¼Œå•ä¸€çš„æç¤ºå·¥ç¨‹æ–¹æ³•å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰3Dä»»åŠ¡éƒ½èƒ½å–å¾—æœ€ä½³æ•ˆæœã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯å°†MLLMåº”ç”¨äº3Dè§†è§‰æ¥åœ°&#x2F;æ¨ç†çš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13185v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨3Dè§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚é€šè¿‡å¼•å…¥åä¸º3DAxisPromptçš„æ–°å‹è§†è§‰æç¤ºæ–¹æ³•ï¼ŒMLLMsèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å±•ç°3Dç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„3Dåæ ‡è½´å’Œé®ç½©ï¼Œä¸ºMLLMsæä¾›æ˜ç¡®çš„å‡ ä½•å…ˆéªŒä¿¡æ¯ï¼Œå¹¶å°†å…¶åœ¨2Dåœºæ™¯ä¸­çš„å“è¶Šæ¨ç†èƒ½åŠ›æ‰©å±•åˆ°çœŸå®ä¸–ç•Œçš„3Dåœºæ™¯ã€‚ç ”ç©¶é€šè¿‡å¤šä¸ªæ•°æ®é›†å¯¹MLLMsçš„æ½œåŠ›è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œå¹¶å‘ç°è™½ç„¶MLLMsèƒ½å¤Ÿæœ‰æ•ˆæ„ŸçŸ¥ç‰©ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„ä¸‰ç»´ä½ç½®ï¼Œä½†å•ä¸€æç¤ºå·¥ç¨‹æ–¹æ³•å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ã€‚æ•´ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†MLLMsåœ¨ç»“åˆæç¤ºå·¥ç¨‹æŠ€æœ¯è¿›è¡Œ3Dè§†è§‰å®šä½ä¸æ¨ç†æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨é…å¤‡ç²¾å¿ƒè®¾è®¡çš„è§†è§‰æç¤ºæ—¶æ›´æ˜¯å¦‚æ­¤ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€»è¾‘ç†è§£å’Œè§†è§‰è®¤çŸ¥ä¸Šï¼Œè€ŒMLLMsåœ¨3Dè§†è§‰é¢†åŸŸçš„è¿ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæ¢ç´¢ä¸­çš„é¢†åŸŸã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è§†è§‰æç¤ºæ–¹æ³•â€”â€”3DAxisPromptï¼Œç”¨äºæ¿€å‘MLLMsåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„3Dç†è§£èƒ½åŠ›ã€‚</li>
<li>3DAxisPromptåˆ©ç”¨SAMç”Ÿæˆçš„3Dåæ ‡è½´å’Œé®ç½©ä¸ºMLLMsæä¾›å‡ ä½•å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>MLLMsèƒ½å°†å…¶åœ¨2Dåœºæ™¯ä¸­çš„ä¼˜ç§€æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°çœŸå®ä¸–ç•Œçš„3Dåœºæ™¯ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¤šä¸ªæ•°æ®é›†å¯¹MLLMsçš„æ½œåŠ›è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œå‘ç°å®ƒä»¬åœ¨æ„ŸçŸ¥ç‰©ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„ä¸‰ç»´ä½ç½®æ–¹é¢å…·æœ‰æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-181105fd11a624073c6aa7dbf89c6d62.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Triad-Empowering-LMM-based-Anomaly-Detection-with-Vision-Expert-guided-Visual-Tokenizer-and-Manufacturing-Process"><a href="#Triad-Empowering-LMM-based-Anomaly-Detection-with-Vision-Expert-guided-Visual-Tokenizer-and-Manufacturing-Process" class="headerlink" title="Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided   Visual Tokenizer and Manufacturing Process"></a>Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided   Visual Tokenizer and Manufacturing Process</h2><p><strong>Authors:Yuanze Li, Shihao Yuan, Haolin Wang, Qizhang Li, Ming Liu, Chen Xu, Guangming Shi, Wangmeng Zuo</strong></p>
<p>Although recent methods have tried to introduce large multimodal models (LMMs) into industrial anomaly detection (IAD), their generalization in the IAD field is far inferior to that for general purposes. We summarize the main reasons for this gap into two aspects. On one hand, general-purpose LMMs lack cognition of defects in the visual modality, thereby failing to sufficiently focus on defect areas. Therefore, we propose to modify the AnyRes structure of the LLaVA model, providing the potential anomalous areas identified by existing IAD models to the LMMs. On the other hand, existing methods mainly focus on identifying defects by learning defect patterns or comparing with normal samples, yet they fall short of understanding the causes of these defects. Considering that the generation of defects is closely related to the manufacturing process, we propose a manufacturing-driven IAD paradigm. An instruction-tuning dataset for IAD (InstructIAD) and a data organization approach for Chain-of-Thought with manufacturing (CoT-M) are designed to leverage the manufacturing process for IAD. Based on the above two modifications, we present Triad, a novel LMM-based method incorporating an expert-guided region-of-interest tokenizer and manufacturing process for industrial anomaly detection. Extensive experiments show that our Triad not only demonstrates competitive performance against current LMMs but also achieves further improved accuracy when equipped with manufacturing processes. Source code, training data, and pre-trained models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/tzjtatata/Triad">https://github.com/tzjtatata/Triad</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„æ–¹æ³•è¯•å›¾å°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å¼•å…¥å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰ï¼Œä½†å®ƒä»¬åœ¨IADé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›è¿œè¿œä¸å¦‚é€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬å°†é€ æˆè¿™ç§å·®è·çš„ä¸»è¦åŸå› æ€»ç»“ä¸ºä¸¤ä¸ªæ–¹é¢ã€‚ä¸€æ–¹é¢ï¼Œé€šç”¨LMMç¼ºä¹å¯¹è§†è§‰æ¨¡æ€ç¼ºé™·çš„è®¤çŸ¥ï¼Œå› æ­¤æ— æ³•å……åˆ†å…³æ³¨ç¼ºé™·åŒºåŸŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¿®æ”¹LLaVAæ¨¡å‹çš„AnyResç»“æ„ï¼Œå‘LMMæä¾›ç°æœ‰IADæ¨¡å‹è¯†åˆ«çš„æ½œåœ¨å¼‚å¸¸åŒºåŸŸã€‚å¦ä¸€æ–¹é¢ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºé€šè¿‡å­¦ä¹ ç¼ºé™·æ¨¡å¼æˆ–ä¸æ­£å¸¸æ ·æœ¬è¿›è¡Œæ¯”è¾ƒæ¥è¯†åˆ«ç¼ºé™·ï¼Œä½†å®ƒä»¬ç¼ºä¹å¯¹ç¼ºé™·åŸå› çš„ç†è§£ã€‚è€ƒè™‘åˆ°ç¼ºé™·çš„äº§ç”Ÿä¸åˆ¶é€ è¿‡ç¨‹å¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ¶é€ é©±åŠ¨å‹IADèŒƒå¼ã€‚è®¾è®¡äº†ç”¨äºIADçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ˆInstructIADï¼‰å’Œç»“åˆåˆ¶é€ çš„Chain-of-Thoughtæ•°æ®ç»„ç»‡æ–¹æ³•ï¼ˆCoT-Mï¼‰ï¼Œä»¥åˆ©ç”¨åˆ¶é€ è¿‡ç¨‹è¿›è¡ŒIADã€‚åŸºäºä¸Šè¿°ä¸¤ä¸ªä¿®æ”¹ï¼Œæˆ‘ä»¬æå‡ºäº†Triadï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLMMçš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†ä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸåˆ†è¯å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Triadä¸ä»…åœ¨ä¸å½“å‰LMMçš„ç«äº‰ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨é…å¤‡åˆ¶é€ è¿‡ç¨‹åå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æºä»£ç ã€è®­ç»ƒæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tzjtatata/Triad%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/tzjtatata/Triadä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13184v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸå°è¯•å°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¼•å…¥å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰é¢†åŸŸçš„æ–¹æ³•ï¼Œåœ¨IADé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ä¸åŠé€šç”¨é¢†åŸŸã€‚æœ¬æ–‡æ€»ç»“äº†é€ æˆè¿™ä¸€å·®è·çš„ä¸»è¦åŸå› ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚ä¸€æ–¹é¢ï¼Œé€šç”¨LMMsç¼ºä¹å¯¹è§†è§‰æ¨¡æ€ç¼ºé™·çš„è®¤çŸ¥ï¼Œæ— æ³•å……åˆ†å…³æ³¨ç¼ºé™·åŒºåŸŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¿®æ”¹LLaVAæ¨¡å‹çš„AnyResç»“æ„ï¼Œå°†ç°æœ‰IADæ¨¡å‹è¯†åˆ«çš„æ½œåœ¨å¼‚å¸¸åŒºåŸŸæä¾›ç»™LMMsã€‚å¦ä¸€æ–¹é¢ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é€šè¿‡å­¦ä¹ ç¼ºé™·æ¨¡å¼æˆ–ä¸æ­£å¸¸æ ·æœ¬è¿›è¡Œæ¯”è¾ƒæ¥è¯†åˆ«ç¼ºé™·ï¼Œä½†ç¼ºä¹å¯¹ç¼ºé™·åŸå› çš„ç†è§£ã€‚è€ƒè™‘åˆ°ç¼ºé™·çš„äº§ç”Ÿä¸åˆ¶é€ è¿‡ç¨‹å¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ¶é€ é©±åŠ¨IADèŒƒå¼ã€‚ä¸ºæ­¤è®¾è®¡äº†IADæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼ˆInstructIADï¼‰å’Œåˆ¶é€ è¿‡ç¨‹çš„CoT-Mæ•°æ®ç»„ç»‡æ–¹æ³•ã€‚åŸºäºä¸Šè¿°ä¸¤ä¸ªä¿®æ”¹ï¼Œæˆ‘ä»¬æå‡ºäº†Triadè¿™ä¸€æ–°å‹åŸºäºLMMçš„æ–¹æ³•ï¼Œç»“åˆä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸåˆ†è¯å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒTriadä¸ä»…ä¸å½“å‰LMMsç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œè€Œä¸”åœ¨é…å¤‡åˆ¶é€ è¿‡ç¨‹åå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LMMsåœ¨IADé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ä¸åŠé€šç”¨é¢†åŸŸçš„åŸå› ä¸»è¦åŒ…æ‹¬ç¼ºä¹å¯¹è§†è§‰æ¨¡æ€ç¼ºé™·çš„è®¤çŸ¥å’Œç¼ºä¹å¯¹ç¼ºé™·åŸå› çš„ç†è§£ã€‚</li>
<li>ä¸ºè§£å†³LMMså¯¹è§†è§‰æ¨¡æ€ç¼ºé™·çš„è®¤çŸ¥ä¸è¶³ï¼Œæå‡ºä¿®æ”¹LLaVAæ¨¡å‹çš„AnyResç»“æ„ï¼Œå°†æ½œåœ¨å¼‚å¸¸åŒºåŸŸä¿¡æ¯æä¾›ç»™LMMsã€‚</li>
<li>ç°æœ‰IADæ–¹æ³•ä¸»è¦å…³æ³¨ç¼ºé™·çš„è¯†åˆ«å’Œæ¯”è¾ƒï¼Œä½†ç¼ºä¹å¯¹ç¼ºé™·äº§ç”ŸåŸå› çš„ç†è§£ï¼Œä¸ºæ­¤æå‡ºåˆ¶é€ é©±åŠ¨IADèŒƒå¼ã€‚</li>
<li>ä¸ºå®ç°åˆ¶é€ é©±åŠ¨IADèŒƒå¼ï¼Œè®¾è®¡äº†InstructIADæ•°æ®é›†å’ŒCoT-Mæ•°æ®ç»„ç»‡æ–¹æ³•ï¼Œä»¥åˆ©ç”¨åˆ¶é€ è¿‡ç¨‹è¿›è¡ŒIADã€‚</li>
<li>æå‡ºTriadè¿™ä¸€æ–°å‹åŸºäºLMMçš„æ–¹æ³•ï¼Œç»“åˆä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸåˆ†è¯å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>Triadåœ¨å¤§é‡å®éªŒä¸­è¡¨ç°å‡ºä¸å½“å‰LMMsç›¸æ¯”çš„ç«äº‰åŠ›ï¼Œå¹¶ä¸”åœ¨é…å¤‡åˆ¶é€ è¿‡ç¨‹åå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdcd0b3f92f38d73583fc7f7aff64196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f007d3ceb170957429ba6cd84b3785b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3aaadf1b9bfc60f6f50f793813f8eaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77719c76765f8d455f677a2a174d1c30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-828ac609c5c15510398d5c217ba14964.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Crab-A-Unified-Audio-Visual-Scene-Understanding-Model-with-Explicit-Cooperation"><a href="#Crab-A-Unified-Audio-Visual-Scene-Understanding-Model-with-Explicit-Cooperation" class="headerlink" title="Crab: A Unified Audio-Visual Scene Understanding Model with Explicit   Cooperation"></a>Crab: A Unified Audio-Visual Scene Understanding Model with Explicit   Cooperation</h2><p><strong>Authors:Henghui Du, Guangyao Li, Chang Zhou, Chunjie Zhang, Alan Zhao, Di Hu</strong></p>
<p>In recent years, numerous tasks have been proposed to encourage model to develop specified capability in understanding audio-visual scene, primarily categorized into temporal localization, spatial localization, spatio-temporal reasoning, and pixel-level understanding. Instead, human possesses a unified understanding ability for diversified tasks. Therefore, designing an audio-visual model with general capability to unify these tasks is of great value. However, simply joint training for all tasks can lead to interference due to the heterogeneity of audiovisual data and complex relationship among tasks. We argue that this problem can be solved through explicit cooperation among tasks. To achieve this goal, we propose a unified learning method which achieves explicit inter-task cooperation from both the perspectives of data and model thoroughly. Specifically, considering the labels of existing datasets are simple words, we carefully refine these datasets and construct an Audio-Visual Unified Instruction-tuning dataset with Explicit reasoning process (AV-UIE), which clarifies the cooperative relationship among tasks. Subsequently, to facilitate concrete cooperation in learning stage, an interaction-aware LoRA structure with multiple LoRA heads is designed to learn different aspects of audiovisual data interaction. By unifying the explicit cooperation across the data and model aspect, our method not only surpasses existing unified audio-visual model on multiple tasks, but also outperforms most specialized models for certain tasks. Furthermore, we also visualize the process of explicit cooperation and surprisingly find that each LoRA head has certain audio-visual understanding ability. Code and dataset: <a target="_blank" rel="noopener" href="https://github.com/GeWu-Lab/Crab">https://github.com/GeWu-Lab/Crab</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä¸ºäº†é¼“åŠ±æ¨¡å‹å‘å±•ç‰¹å®šçš„èƒ½åŠ›ä»¥ç†è§£è§†å¬åœºæ™¯ï¼Œå·²ç»æå‡ºäº†è®¸å¤šä»»åŠ¡ï¼Œä¸»è¦å¯åˆ†ä¸ºæ—¶é—´å®šä½ã€ç©ºé—´å®šä½ã€æ—¶ç©ºæ¨ç†å’Œåƒç´ çº§ç†è§£ã€‚ç„¶è€Œï¼Œäººç±»å…·å¤‡å¯¹å„ç§ä»»åŠ¡çš„ç»Ÿä¸€ç†è§£èƒ½åŠ›ã€‚å› æ­¤ï¼Œè®¾è®¡ä¸€ç§å…·å¤‡ç»Ÿä¸€è¿™äº›ä»»åŠ¡çš„ä¸€èˆ¬èƒ½åŠ›çš„è§†å¬æ¨¡å‹æ˜¯éå¸¸æœ‰ä»·å€¼çš„ã€‚ç„¶è€Œï¼Œå¯¹æ‰€æœ‰ä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒå¯èƒ½ä¼šå¯¼è‡´ç”±äºè§†å¬æ•°æ®çš„å¼‚è´¨æ€§å’Œä»»åŠ¡ä¹‹é—´çš„å¤æ‚å…³ç³»è€Œäº§ç”Ÿçš„å¹²æ‰°ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡ä»»åŠ¡ä¹‹é—´çš„æ˜ç¡®åˆä½œæ¥è§£å†³ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å­¦ä¹ æ–¹æ³•ï¼Œä»æ•°æ®å’Œæ¨¡å‹çš„è§†è§’å®ç°ä»»åŠ¡é—´çš„æ˜ç¡®åˆä½œã€‚å…·ä½“æ¥è¯´ï¼Œè€ƒè™‘åˆ°ç°æœ‰æ•°æ®é›†çš„æ ‡ç­¾æ˜¯ç®€å•çš„å•è¯ï¼Œæˆ‘ä»¬ç²¾å¿ƒåœ°æ”¹è¿›äº†è¿™äº›æ•°æ®é›†ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªè§†å¬ç»Ÿä¸€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå…·æœ‰æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼ˆAV-UIEï¼‰ï¼Œè¯¥æ•°æ®é›†æ˜ç¡®äº†ä»»åŠ¡ä¹‹é—´çš„åˆä½œå…³ç³»ã€‚éšåï¼Œä¸ºäº†åœ¨å­¦ä¹ é˜¶æ®µä¿ƒè¿›å…·ä½“çš„åˆä½œï¼Œè®¾è®¡äº†ä¸€ç§å…·æœ‰å¤šä¸ªLoRAå¤´çš„äº¤äº’æ„ŸçŸ¥LoRAç»“æ„æ¥å­¦ä¹ è§†å¬æ•°æ®äº¤äº’çš„ä¸åŒæ–¹é¢ã€‚é€šè¿‡ç»Ÿä¸€æ•°æ®å’Œæ¨¡å‹æ–¹é¢çš„æ˜ç¡®åˆä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€è§†å¬æ¨¡å‹ï¼Œè€Œä¸”è¿˜åœ¨æŸäº›ç‰¹å®šä»»åŠ¡ä¸Šè¶…è¿‡äº†å¤§å¤šæ•°ä¸“ç”¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯è§†åŒ–äº†æ˜ç¡®åˆä½œçš„è¿‡ç¨‹ï¼Œå¹¶æƒŠè®¶åœ°å‘ç°æ¯ä¸ªLoRAå¤´éƒ½å…·æœ‰ä¸€å®šçš„è§†å¬ç†è§£èƒ½åŠ›ã€‚ä»£ç å’Œæ•°æ®é›†è¯·è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/GeWu-Lab/Crab%E3%80%82">https://github.com/GeWu-Lab/Crabã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13068v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»Ÿä¸€çš„è§†å¬å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®å’Œæ¨¡å‹ä¸¤ä¸ªå±‚é¢çš„æ˜¾å¼ä»»åŠ¡é—´åˆä½œæ¥è§£å†³è§†å¬æ•°æ®çš„å¼‚è´¨æ€§å’Œä»»åŠ¡é—´çš„å¤æ‚å…³ç³»é—®é¢˜ã€‚ä¸ºè§£å†³ç°æœ‰æ•°æ®é›†æ ‡ç­¾ç®€å•çš„é—®é¢˜ï¼Œå¯¹æ•°æ®é›†è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œæ„å»ºäº†è§†å¬ç»Ÿä¸€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼ˆAV-UIEï¼‰ï¼Œæ˜ç¡®äº†ä»»åŠ¡é—´çš„åˆä½œå…³ç³»ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§äº¤äº’æ„ŸçŸ¥çš„LoRAç»“æ„ï¼Œé…å¤‡å¤šä¸ªLoRAå¤´ï¼Œä»¥å­¦ä¹ è§†å¬æ•°æ®äº¤äº’çš„ä¸åŒæ–¹é¢ã€‚æ­¤æ–¹æ³•ä¸ä»…è¶…è¶Šäº†ç°æœ‰çš„å¤šä»»åŠ¡ç»Ÿä¸€è§†å¬æ¨¡å‹ï¼Œè€Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºå¤§å¤šæ•°ä¸“é¡¹æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†å¬å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ä¸åŒä»»åŠ¡é—´çš„æ˜¾å¼åˆä½œã€‚</li>
<li>è®ºæ–‡æ„å»ºäº†è§†å¬ç»Ÿä¸€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼ˆAV-UIEï¼‰ï¼Œä»¥è§£å†³ç°æœ‰æ•°æ®é›†æ ‡ç­¾ç®€å•çš„é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æ˜ç¡®äº†ä»»åŠ¡é—´çš„åˆä½œå…³ç³»ï¼Œæœ‰åŠ©äºè§£å†³ç”±äºè§†å¬æ•°æ®çš„å¼‚è´¨æ€§å’Œä»»åŠ¡é—´å¤æ‚å…³ç³»å¯¼è‡´çš„å¹²æ‰°é—®é¢˜ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§äº¤äº’æ„ŸçŸ¥çš„LoRAç»“æ„ï¼Œé…å¤‡å¤šä¸ªLoRAå¤´ï¼Œä»¥å­¦ä¹ è§†å¬æ•°æ®äº¤äº’çš„ä¸åŒæ–¹é¢ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…åœ¨å¤šä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€è§†å¬æ¨¡å‹ï¼Œè€Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å¤§å¤šæ•°ä¸“é¡¹æ¨¡å‹ã€‚</li>
<li>è®ºæ–‡æä¾›äº†å¯è§†åŒ–æ˜¾å¼åˆä½œè¿‡ç¨‹çš„ç»“æœï¼Œå‘ç°æ¯ä¸ªLoRAå¤´å…·æœ‰ä¸€å®šçš„éŸ³é¢‘è§†è§‰ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b020ae61fa1e2ed81b1f120f91d24fa5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e3c10028eaa7e27e317033a70d0a4ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f3753a6c09cb239cecafb09aec8f6b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5661e525b3837a8309f7970060ff833a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be4ec85f91a4dd384727838a2129ab83.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HiDe-LLaVA-Hierarchical-Decoupling-for-Continual-Instruction-Tuning-of-Multimodal-Large-Language-Model"><a href="#HiDe-LLaVA-Hierarchical-Decoupling-for-Continual-Instruction-Tuning-of-Multimodal-Large-Language-Model" class="headerlink" title="HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of   Multimodal Large Language Model"></a>HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of   Multimodal Large Language Model</h2><p><strong>Authors:Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu</strong></p>
<p>Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Our code will be public available. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒè¢«å¹¿æ³›ç”¨äºé€šè¿‡è®­ç»ƒé¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ç²¾é€‰çš„ä»»åŠ¡ç‰¹å®šæ•°æ®é›†ä¸Šæ¥æé«˜å…¶æ€§èƒ½ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£äººç±»æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œåœ¨ç°å®åœºæ™¯ä¸­åŒæ—¶æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æŒ‡ä»¤æ•°æ®é›†æ˜¯ä¸å¯è¡Œçš„ã€‚å› æ­¤ï¼Œå¯ç”¨å…·æœ‰æŒç»­æŒ‡ä»¤å¾®è°ƒåŠŸèƒ½çš„MLLMå¯¹äºä¿æŒå…¶é€‚åº”æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä»¥ç‰ºç‰²å†…å­˜æ•ˆç‡ä¸ºä»£ä»·æ¥æ¢å–æ€§èƒ½æå‡ï¼Œä»è€Œä¸¥é‡æŸå®³æ•´ä½“æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåœ¨ä¸åŒæ•°æ®é›†è®­ç»ƒæ—¶ä¸åŒæ¨¡å‹å±‚ä¹‹é—´ä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ç›¸ä¼¼æ€§çš„å˜åŒ–çš„ä»»åŠ¡ç‰¹å®šæ‰©å±•å’Œä»»åŠ¡é€šç”¨èåˆæ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•æ¥åˆç†è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æœ€å…ˆè¿›çš„ç›¸æ¯”ï¼Œæ€§èƒ½æœ‰äº†æ˜¾è‘—çš„æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12941v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡æŒ‡ä»¤å¾®è°ƒæé«˜é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œä¸­æ— æ³•åŒæ—¶æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æŒ‡ä»¤æ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ç›¸ä¼¼åº¦å˜åŒ–çš„ç‰¹å®šä»»åŠ¡æ‰©å±•å’Œé€šç”¨ä»»åŠ¡èåˆæ¡†æ¶ã€‚åŒæ—¶ï¼Œæœ¬æ–‡åˆ†æäº†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•æ¥åˆç†è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒç”¨äºæé«˜é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç°å®ä¸–ç•Œä¸­æ— æ³•åŒæ—¶æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œå› æ­¤æŒç»­æŒ‡ä»¤å¾®è°ƒå¯¹ä¿æŒæ¨¡å‹é€‚åº”æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€ä»¥ç‰ºç‰²å†…å­˜æ•ˆç‡ä¸ºä»£ä»·æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ç›¸ä¼¼åº¦å˜åŒ–çš„ç‰¹å®šä»»åŠ¡æ‰©å±•å’Œé€šç”¨ä»»åŠ¡èåˆæ¡†æ¶ã€‚</li>
<li>åˆ†æäº†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•æ¥åˆç†è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0725a7e26d8367327ded8fd74ceff0e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2e988beb69aa35c8dab9a7a08ba3800.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-000421c667ae2c20e917af4f3387a3ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58b7507d6778679c7d472da0c8e75422.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Reflect-DiT-Inference-Time-Scaling-for-Text-to-Image-Diffusion-Transformers-via-In-Context-Reflection"><a href="#Reflect-DiT-Inference-Time-Scaling-for-Text-to-Image-Diffusion-Transformers-via-In-Context-Reflection" class="headerlink" title="Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion   Transformers via In-Context Reflection"></a>Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion   Transformers via In-Context Reflection</h2><p><strong>Authors:Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, Aditya Grover</strong></p>
<p>The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach. </p>
<blockquote>
<p>æ¨è¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»è¦æ–¹æ³•ä¸€ç›´æ˜¯è®­ç»ƒæ—¶é—´å°ºåº¦è°ƒæ•´ï¼Œå³ä½¿ç”¨æ›´å¤§çš„è®¡ç®—èµ„æºåœ¨æ›´å¤šæ•°æ®ä¸Šè®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚è™½ç„¶è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œäººä»¬å¯¹æé«˜æ€§èƒ½çš„æ¨ç†æ—¶é—´å°ºåº¦è°ƒæ•´æ–¹æ³•è¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚ç›®å‰ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨ç†æ—¶é—´å°ºåº¦è°ƒæ•´ä¸»è¦å±€é™äºNé€‰æœ€ä¼˜é‡‡æ ·ï¼Œå³æ¯æ¬¡æç¤ºç”Ÿæˆå¤šä¸ªå›¾åƒï¼Œå¹¶ç”±é€‰æ‹©æ¨¡å‹é€‰æ‹©æœ€ä½³è¾“å‡ºã€‚å—è¯­è¨€é¢†åŸŸDeepSeek-R1ç­‰æ¨ç†æ¨¡å‹è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ›¿ä»£Né€‰æœ€ä¼˜é‡‡æ ·çš„æ–¹æ³•ï¼Œå³é€šè¿‡ä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£è½¬æ¢å™¨é…å¤‡ä¸Šä¸‹æ–‡å†…åå°„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†Reflect-DiTæ–¹æ³•ï¼Œå®ƒä½¿æ‰©æ•£è½¬æ¢å™¨èƒ½å¤Ÿåˆ©ç”¨ä»¥å‰ç”Ÿæˆçš„å›¾åƒçš„ä¸Šä¸‹æ–‡å†…ç¤ºä¾‹å’Œæè¿°å¿…è¦æ”¹è¿›çš„æ–‡æœ¬åé¦ˆæ¥æ”¹è¿›å…¶ç”Ÿæˆã€‚Reflect-DiTæ˜ç¡®é’ˆå¯¹éœ€è¦å¢å¼ºçš„ç‰¹å®šæ–¹é¢è°ƒæ•´å…¶ç”Ÿæˆï¼Œè€Œä¸æ˜¯è¢«åŠ¨åœ°ä¾èµ–éšæœºé‡‡æ ·å¹¶å¸Œæœ›åœ¨å°†æ¥çš„ç”Ÿæˆä¸­è·å¾—æ›´å¥½çš„ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SANA-1.0-1.6Bä½œä¸ºåŸºå‡†æ¨¡å‹ï¼ŒReflect-DiTåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æé«˜äº†ï¼ˆ+0.19ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ä»…ç”Ÿæˆ20ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹è¾¾åˆ°äº†GenEvalä¸Šçš„æœ€æ–°æœ€é«˜åˆ†æ•°0.81ï¼Œè¶…è¿‡äº†ä¹‹å‰ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆSANA-1.5-4.8Bï¼‰åœ¨Né€‰æœ€ä¼˜æ–¹æ³•ä¸‹è·å¾—æœ€é«˜åˆ†æ•°0.80ï¼ˆéœ€è¦ç”Ÿæˆ2048ä¸ªæ ·æœ¬ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12271v1">PDF</a> 17 pages, 9 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡æ€§èƒ½çš„æ–°æ–¹æ³•â€”â€”Reflect-DiTã€‚è¯¥æ–¹æ³•é€šè¿‡èµ‹äºˆæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£è½¬æ¢å™¨ä¸Šä¸‹æ–‡åæ€èƒ½åŠ›ï¼Œæ”¹è¿›äº†å½“å‰ä¸»è¦é‡‡ç”¨çš„åŸºäºå¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºçš„è®­ç»ƒæ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚Reflect-DiTå…è®¸æ‰©æ•£è½¬æ¢å™¨åˆ©ç”¨å…ˆå‰ç”Ÿæˆçš„å›¾åƒä¸Šä¸‹æ–‡ç¤ºä¾‹å’Œæè¿°å¿…è¦æ”¹è¿›çš„æ–‡æœ¬åé¦ˆæ¥ä¼˜åŒ–å…¶ç”Ÿæˆç»“æœã€‚ç›¸è¾ƒäºè¢«åŠ¨ä¾èµ–éšæœºé‡‡æ ·å’ŒæœŸæœ›æœªæ¥ç”Ÿæˆæ›´å¥½ç»“æœçš„æ–¹æ³•ï¼ŒReflect-DiTèƒ½å¤Ÿæ˜ç¡®é’ˆå¯¹éœ€è¦å¢å¼ºçš„ç‰¹å®šæ–¹é¢è°ƒæ•´å…¶ç”Ÿæˆç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReflect-DiTåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ï¼ˆä½¿ç”¨SANA-1.0-1.6Bä½œä¸ºåŸºå‡†æ¨¡å‹ï¼Œ+0.19ï¼‰ï¼Œä¸”åœ¨æ¯æç¤ºä»…ç”Ÿæˆ20ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹è¾¾åˆ°äº†GenEvalçš„æ–°çºªå½•å¾—åˆ†0.81ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€ä½³å¾—åˆ†çš„æ¨¡å‹ï¼ˆSANA-1.5-4.8Bï¼‰çš„å¾—åˆ†ï¼ˆåœ¨æœ€ä½³Næ–¹æ³•ä¸‹ç”Ÿæˆäº†é«˜è¾¾2048ä¸ªæ ·æœ¬ï¼‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºè®¡ç®—èµ„æºçš„æ¶ˆè€—ï¼Œä¿ƒä½¿äº†å¯¹æ¨ç†æ—¶é—´ç¼©æ”¾çš„å…³æ³¨ã€‚</li>
<li>æœ€ä½³Né‡‡æ ·æ˜¯ç›®å‰æ¨ç†æ—¶é—´ç¼©æ”¾çš„ä¸»è¦æ–¹æ³•ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºReflect-DiTçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ–‡æœ¬åé¦ˆå’Œä¸Šä¸‹æ–‡å›¾åƒç¤ºä¾‹æ¥ä¼˜åŒ–å›¾åƒç”Ÿæˆã€‚</li>
<li>Reflect-DiTå…è®¸æ‰©æ•£è½¬æ¢å™¨é’ˆå¯¹éœ€è¦æ”¹è¿›çš„ç‰¹å®šæ–¹é¢è¿›è¡Œæ˜ç¡®è°ƒæ•´ï¼Œè€Œä¸æ˜¯ä¾èµ–éšæœºé‡‡æ ·ã€‚</li>
<li>Reflect-DiTåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å³å¯è¾¾åˆ°æ–°çš„è®°å½•å¾—åˆ†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒReflect-DiTèƒ½å¤Ÿåœ¨ä»…ç”Ÿæˆå°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹å®ç°é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84427d29d72ffab6ddb9ce245516e1ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a16805f2bee94ac7f0e911a6c1698438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf87569c667b4adb65df42600aad1fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8aa4eacfa19128fe54e85d03f817143c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcf288e810018e17cab2c1e9e5ce4e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bf01cad07984f4c6c6a81506b5e99bf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Transformer-and-Prototype-based-Interpretable-Model-for-Contextual-Sarcasm-Detection"><a href="#A-Transformer-and-Prototype-based-Interpretable-Model-for-Contextual-Sarcasm-Detection" class="headerlink" title="A Transformer and Prototype-based Interpretable Model for Contextual   Sarcasm Detection"></a>A Transformer and Prototype-based Interpretable Model for Contextual   Sarcasm Detection</h2><p><strong>Authors:Ximing Wen, Rezvaneh Rezapour</strong></p>
<p>Sarcasm detection, with its figurative nature, poses unique challenges for affective systems designed to perform sentiment analysis. While these systems typically perform well at identifying direct expressions of emotion, they struggle with sarcasmâ€™s inherent contradiction between literal and intended sentiment. Since transformer-based language models (LMs) are known for their efficient ability to capture contextual meanings, we propose a method that leverages LMs and prototype-based networks, enhanced by sentiment embeddings to conduct interpretable sarcasm detection. Our approach is intrinsically interpretable without extra post-hoc interpretability techniques. We test our model on three public benchmark datasets and show that our model outperforms the current state-of-the-art. At the same time, the prototypical layer enhances the modelâ€™s inherent interpretability by generating explanations through similar examples in the reference time. Furthermore, we demonstrate the effectiveness of incongruity loss in the ablation study, which we construct using sentiment prototypes. </p>
<blockquote>
<p>è®½åˆºæ£€æµ‹ç”±äºå…¶æ¯”å–»æ€§è´¨ï¼Œå¯¹è®¾è®¡ç”¨äºæƒ…æ„Ÿåˆ†æçš„æƒ…æ„Ÿç³»ç»Ÿæå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚è™½ç„¶è¿™äº›ç³»ç»Ÿé€šå¸¸åœ¨è¯†åˆ«ç›´æ¥æƒ…ç»ªè¡¨è¾¾æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†è®½åˆºä¸­å­—é¢å«ä¹‰å’Œæ„å›¾æƒ…æ„Ÿä¹‹é—´çš„å†…åœ¨çŸ›ç›¾æ—¶å´æ„Ÿåˆ°å›°éš¾ã€‚ç”±äºåŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä»¥å…¶æ•æ‰ä¸Šä¸‹æ–‡å«ä¹‰çš„é«˜æ•ˆèƒ½åŠ›è€Œé—»åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¯­è¨€æ¨¡å‹å’ŒåŸºäºåŸå‹çš„ç½‘ç»œçš„æ–¹æ³•ï¼Œé€šè¿‡æƒ…æ„ŸåµŒå…¥æ¥è¿›è¡Œå¯è§£é‡Šçš„è®½åˆºæ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯å¯è§£é‡Šçš„ï¼Œä¸éœ€è¦é¢å¤–çš„åç»­è§£é‡ŠæŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚åŒæ—¶ï¼ŒåŸå‹å±‚é€šè¿‡å‚è€ƒæ—¶é—´ä¸­çš„ç›¸ä¼¼ä¾‹å­ç”Ÿæˆè§£é‡Šï¼Œæé«˜äº†æ¨¡å‹çš„å†…åœ¨å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æƒ…æ„ŸåŸå‹æ„å»ºçš„æ¶ˆèç ”ç©¶è¯æ˜äº†ä¸ä¸€è‡´æŸå¤±çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11838v1">PDF</a> 8 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè½¬åŒ–å™¨è¯­è¨€æ¨¡å‹çš„è®½åˆºæ£€æµ‹æ³•ç»“åˆäº†åŸå‹ç½‘ç»œå’Œæƒ…æ„ŸåµŒå…¥æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³æƒ…æ„Ÿåˆ†æä¸­è®½åˆºè¡¨è¾¾å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ä¸ä»…é«˜æ•ˆæ•æ‰è¯­å¢ƒæ„ä¹‰ï¼Œè€Œä¸”é€šè¿‡ç”Ÿæˆå‚è€ƒæ—¶é—´ä¸­çš„ç›¸ä¼¼ä¾‹å­æ¥è§£é‡Šæ¨¡å‹é¢„æµ‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å†…åœ¨å¯è§£é‡Šæ€§ã€‚åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åˆ©ç”¨æƒ…æ„ŸåŸå‹æ„å»ºäº†ä¸ä¸€è‡´æŸå¤±çš„æ¶ˆèç ”ç©¶ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è®½åˆºæ£€æµ‹å¯¹äºæƒ…æ„Ÿç³»ç»Ÿæ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè®½åˆºè¡¨è¾¾åœ¨å­—é¢å’Œæ„å›¾æƒ…æ„Ÿä¹‹é—´å­˜åœ¨å†…åœ¨çŸ›ç›¾ã€‚</li>
<li>è½¬åŒ–å™¨è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆæ•æ‰è¯­å¢ƒæ„ä¹‰ï¼Œæ˜¯å¤„ç†è®½åˆºæ£€æµ‹çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•ç»“åˆäº†è¯­è¨€æ¨¡å‹ã€åŸå‹ç½‘ç»œä»¥åŠæƒ…æ„ŸåµŒå…¥æŠ€æœ¯ï¼Œè¿›è¡Œå¯è§£é‡Šçš„è®½åˆºæ£€æµ‹ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>åŸå‹å±‚é€šè¿‡ç”Ÿæˆå‚è€ƒæ—¶é—´ä¸­çš„ç›¸ä¼¼ä¾‹å­æ¥è§£é‡Šæ¨¡å‹é¢„æµ‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å†…åœ¨å¯è§£é‡Šæ€§ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†ä¸ä¸€è‡´æŸå¤±çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæƒ…æ„Ÿåˆ†æé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯å¤„ç†è®½åˆºè¡¨è¾¾æ–¹é¢å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79aa69bc909bc10c5f11de6c47f8d23d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a8061af7c7f8c8a9f785d17cb6131a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ASMA-Tune-Unlocking-LLMsâ€™-Assembly-Code-Comprehension-via-Structural-Semantic-Instruction-Tuning"><a href="#ASMA-Tune-Unlocking-LLMsâ€™-Assembly-Code-Comprehension-via-Structural-Semantic-Instruction-Tuning" class="headerlink" title="ASMA-Tune: Unlocking LLMsâ€™ Assembly Code Comprehension via   Structural-Semantic Instruction Tuning"></a>ASMA-Tune: Unlocking LLMsâ€™ Assembly Code Comprehension via   Structural-Semantic Instruction Tuning</h2><p><strong>Authors:Xinyi Wang, Jiashui Wang, Peng Chen, Jinbo Su, Yanming Liu, Long Liu, Yangdong Wang, Qiyuan Chen, Kai Yun, Chunfu Jia</strong></p>
<p>Analysis and comprehension of assembly code are crucial in various applications, such as reverse engineering. However, the low information density and lack of explicit syntactic structures in assembly code pose significant challenges. Pioneering approaches with masked language modeling (MLM)-based methods have been limited by facilitating natural language interaction. While recent methods based on decoder-focused large language models (LLMs) have significantly enhanced semantic representation, they still struggle to capture the nuanced and sparse semantics in assembly code. In this paper, we propose Assembly Augmented Tuning (ASMA-Tune), an end-to-end structural-semantic instruction-tuning framework. Our approach synergizes encoder architectures with decoder-based LLMs through projector modules to enable comprehensive code understanding. Experiments show that ASMA-Tune outperforms existing benchmarks, significantly enhancing assembly code comprehension and instruction-following abilities. Our model and dataset are public at <a target="_blank" rel="noopener" href="https://github.com/wxy3596/ASMA-Tune">https://github.com/wxy3596/ASMA-Tune</a>. </p>
<blockquote>
<p>åˆ†æå’Œç†è§£æ±‡ç¼–ä»£ç åœ¨å„ç§åº”ç”¨ï¼ˆå¦‚é€†å‘å·¥ç¨‹ï¼‰ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ±‡ç¼–ä»£ç ä¸­ä¿¡æ¯å¯†åº¦ä½ä¸”ç¼ºä¹æ˜ç¡®çš„è¯­æ³•ç»“æ„ï¼Œè¿™æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚åŸºäºæ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰çš„æ–¹æ³•è™½ç„¶ä¿ƒè¿›äº†è‡ªç„¶è¯­è¨€äº¤äº’ï¼Œä½†å…¶åœ¨å®è·µä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚è™½ç„¶æœ€è¿‘åŸºäºè§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†è¯­ä¹‰è¡¨ç¤ºï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥æ•æ‰æ±‡ç¼–ä»£ç ä¸­ç»†å¾®ä¸”ç¨€ç–çš„è¯­ä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ±‡ç¼–å¢å¼ºè°ƒä¼˜â€ï¼ˆASMA-Tuneï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç»“æ„è¯­ä¹‰æŒ‡ä»¤è°ƒä¼˜æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æŠ•å½±ä»ªæ¨¡å—ååŒç¼–ç å™¨æ¶æ„å’ŒåŸºäºè§£ç å™¨çš„LLMï¼Œä»¥å®ç°å…¨é¢çš„ä»£ç ç†è§£ã€‚å®éªŒè¡¨æ˜ï¼ŒASMA-Tuneä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œæ˜¾è‘—æé«˜äº†æ±‡ç¼–ä»£ç çš„ç†è§£å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®é›†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/wxy3596/ASMA-Tune%E3%80%82">https://github.com/wxy3596/ASMA-Tuneã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11617v1">PDF</a> 19 pages, multiple figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨æ±‡ç¼–ä»£ç åˆ†ææ–¹é¢çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Assembly Augmented Tuningï¼ˆASMA-Tuneï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æŒ‡ä»¤è°ƒä¼˜æ¡†æ¶ï¼Œèåˆäº†ç¼–ç å™¨çš„æ¶æ„å’ŒåŸºäºè§£ç å™¨çš„è¯­è¨€æ¨¡å‹ã€‚ASMA-Tuneæ¡†æ¶é‡‡ç”¨æŠ•å½±æ¨¡å—å¢å¼ºäº†æ±‡ç¼–ä»£ç çš„ç»¼åˆç†è§£èƒ½åŠ›ï¼Œå®éªŒç»“æœè¯æ˜å…¶åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ•°æ®é›†å’Œæ¨¡å‹å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ±‡ç¼–ä»£ç åˆ†æå’Œç†è§£åœ¨å¤šä¸ªåº”ç”¨å¦‚é€†å‘å·¥ç¨‹ä¸­éå¸¸é‡è¦ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•é¢ä¸´æ±‡ç¼–ä»£ç ä¿¡æ¯å¯†åº¦ä½å’Œç¼ºä¹æ˜ç¡®å¥æ³•ç»“æ„çš„é—®é¢˜ã€‚</li>
<li>åŸºäºè¯­è¨€æ¨¡å‹çš„æ±‡ç¼–ä»£ç ç†è§£æ–¹æ³•å°šå¤„äºåˆçº§é˜¶æ®µï¼Œç‰¹åˆ«æ˜¯è§£ç å™¨ä¸»å¯¼çš„LLMæ¨¡å‹ï¼Œå­˜åœ¨æ•æ‰è¯­ä¹‰çš„å±€é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Assembly Augmented Tuningï¼ˆASMA-Tuneï¼‰æ¡†æ¶ï¼Œç»“åˆäº†ç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„çš„ä¼˜åŠ¿ã€‚</li>
<li>ASMA-Tuneæ¡†æ¶é€šè¿‡æŠ•å½±æ¨¡å—å®ç°å…¨é¢ä»£ç ç†è§£ï¼Œæå‡äº†æ±‡ç¼–ä»£ç çš„ç†è§£èƒ½åŠ›å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒASMA-Tuneåœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dec50f1a468781640e44d8ffcb60b9a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6e0e3f70654c026c25d127badf8fbf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e36d9a4c6f94d082ccebe9ed0fddfdf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Perceive-Understand-and-Restore-Real-World-Image-Super-Resolution-with-Autoregressive-Multimodal-Generative-Models"><a href="#Perceive-Understand-and-Restore-Real-World-Image-Super-Resolution-with-Autoregressive-Multimodal-Generative-Models" class="headerlink" title="Perceive, Understand and Restore: Real-World Image Super-Resolution with   Autoregressive Multimodal Generative Models"></a>Perceive, Understand and Restore: Real-World Image Super-Resolution with   Autoregressive Multimodal Generative Models</h2><p><strong>Authors:Hongyang Wei, Shuaizheng Liu, Chun Yuan, Lei Zhang</strong></p>
<p>By leveraging the generative priors from pre-trained text-to-image diffusion models, significant progress has been made in real-world image super-resolution (Real-ISR). However, these methods tend to generate inaccurate and unnatural reconstructions in complex and&#x2F;or heavily degraded scenes, primarily due to their limited perception and understanding capability of the input low-quality image. To address these limitations, we propose, for the first time to our knowledge, to adapt the pre-trained autoregressive multimodal model such as Lumina-mGPT into a robust Real-ISR model, namely PURE, which Perceives and Understands the input low-quality image, then REstores its high-quality counterpart. Specifically, we implement instruction tuning on Lumina-mGPT to perceive the image degradation level and the relationships between previously generated image tokens and the next token, understand the image content by generating image semantic descriptions, and consequently restore the image by generating high-quality image tokens autoregressively with the collected information. In addition, we reveal that the image token entropy reflects the image structure and present a entropy-based Top-k sampling strategy to optimize the local structure of the image during inference. Experimental results demonstrate that PURE preserves image content while generating realistic details, especially in complex scenes with multiple objects, showcasing the potential of autoregressive multimodal generative models for robust Real-ISR. The model and code will be available at <a target="_blank" rel="noopener" href="https://github.com/nonwhy/PURE">https://github.com/nonwhy/PURE</a>. </p>
<blockquote>
<p>é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼ŒçœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤æ‚å’Œ&#x2F;æˆ–ä¸¥é‡é€€åŒ–çš„åœºæ™¯ä¸­å¾€å¾€ä¼šäº§ç”Ÿä¸å‡†ç¡®å’Œä¸è‡ªç„¶çš„é‡å»ºï¼Œä¸»è¦æ˜¯ç”±äºå®ƒä»¬å¯¹è¾“å…¥çš„ä½è´¨é‡å›¾åƒçš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†é¢„è®­ç»ƒçš„è‡ªåŠ¨å›å½’å¤šæ¨¡å¼æ¨¡å‹ï¼ˆå¦‚Lumina-mGPTï¼‰æ”¹ç¼–ä¸ºç¨³å¥çš„Real-ISRæ¨¡å‹ï¼Œå³PUREæ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥å’Œç†è§£è¾“å…¥çš„ä½è´¨é‡å›¾åƒï¼Œç„¶åæ¢å¤å…¶é«˜è´¨é‡å¯¹åº”å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹Lumina-mGPTè¿›è¡Œäº†æŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æ„ŸçŸ¥å›¾åƒé€€åŒ–ç¨‹åº¦å’Œå…ˆå‰ç”Ÿæˆçš„å›¾åƒæ ‡è®°ä¸ä¸‹ä¸€ä¸ªæ ‡è®°ä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒè¯­ä¹‰æè¿°æ¥ç†è§£å›¾åƒå†…å®¹ï¼Œå¹¶ä½¿ç”¨æ”¶é›†çš„ä¿¡æ¯ä»¥è‡ªåŠ¨å›å½’çš„æ–¹å¼ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ‡è®°æ¥æ¢å¤å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å›¾åƒæ ‡è®°ç†µåæ˜ äº†å›¾åƒç»“æ„ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºç†µçš„Top-ké‡‡æ ·ç­–ç•¥ï¼Œä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼˜åŒ–å›¾åƒå±€éƒ¨ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPUREåœ¨ä¿ç•™å›¾åƒå†…å®¹çš„åŒæ—¶ç”Ÿæˆäº†é€¼çœŸçš„ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯ä¸­ï¼Œå±•ç¤ºäº†è‡ªåŠ¨å›å½’å¤šæ¨¡å¼ç”Ÿæˆæ¨¡å‹åœ¨ç¨³å¥Real-ISRä¸­çš„æ½œåŠ›ã€‚æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/nonwhy/PURE%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/nonwhy/PUREä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11073v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒï¼ŒçœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤æ‚å’Œ&#x2F;æˆ–ä¸¥é‡é€€åŒ–çš„åœºæ™¯ä¸­å¾€å¾€ä¼šç”Ÿæˆä¸å‡†ç¡®å’Œä¸è‡ªç„¶çš„é‡å»ºï¼Œä¸»è¦æ˜¯ç”±äºå…¶å¯¹è¾“å…¥ä½è´¨é‡å›¾åƒçš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›æœ‰é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†é¢„è®­ç»ƒçš„è‡ªåŠ¨å›å½’å¤šæ¨¡å¼æ¨¡å‹ï¼ˆå¦‚Lumina-mGPTï¼‰é€‚åº”ä¸ºç¨³å¥çš„Real-ISRæ¨¡å‹ï¼Œå³PUREæ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥å’Œç†è§£è¾“å…¥çš„ä½è´¨é‡å›¾åƒï¼Œç„¶åæ¢å¤å…¶é«˜è´¨é‡å¯¹åº”ç‰©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹Lumina-mGPTè¿›è¡Œäº†æŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æ„ŸçŸ¥å›¾åƒé€€åŒ–ç¨‹åº¦å’Œå…ˆå‰ç”Ÿæˆçš„å›¾åƒæ ‡è®°ä¸ä¸‹ä¸€ä¸ªæ ‡è®°ä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒè¯­ä¹‰æè¿°æ¥ç†è§£å›¾åƒå†…å®¹ï¼Œå¹¶ä½¿ç”¨æ”¶é›†çš„ä¿¡æ¯ä»¥è‡ªåŠ¨å›å½’çš„æ–¹å¼ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ‡è®°æ¥æ¢å¤å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å›¾åƒæ ‡è®°ç†µåæ˜ äº†å›¾åƒç»“æ„ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºç†µçš„Top-ké‡‡æ ·ç­–ç•¥ï¼Œä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼˜åŒ–å›¾åƒå±€éƒ¨ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPUREèƒ½å¤Ÿä¿ç•™å›¾åƒå†…å®¹å¹¶ç”Ÿæˆé€¼çœŸçš„ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯ä¸­ï¼Œå±•ç¤ºäº†è‡ªåŠ¨å›å½’å¤šæ¨¡å¼ç”Ÿæˆæ¨¡å‹åœ¨ç¨³å¥Real-ISRä¸­çš„æ½œåŠ›ã€‚æ¨¡å‹å’Œä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/nonwhy/PURE%E3%80%82">https://github.com/nonwhy/PUREã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒåœ¨Real-ISRé¢†åŸŸå–å¾—è¿›å±•ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å’Œä¸¥é‡é€€åŒ–çš„åœºæ™¯æ—¶å­˜åœ¨ä¸å‡†ç¡®å’Œä¸è‡ªç„¶çš„é‡å»ºé—®é¢˜ã€‚</li>
<li>é¦–æ¬¡å°†è‡ªåŠ¨å›å½’å¤šæ¨¡å¼æ¨¡å‹ï¼ˆå¦‚Lumina-mGPTï¼‰é€‚åº”ä¸ºReal-ISRæ¨¡å‹ï¼Œå½¢æˆPUREæ¨¡å‹ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤è°ƒæ•´æå‡æ¨¡å‹çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒå¯¹åº”ç‰©ã€‚</li>
<li>æå‡ºåŸºäºç†µçš„Top-ké‡‡æ ·ç­–ç•¥æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„å›¾åƒå±€éƒ¨ç»“æ„ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPUREæ¨¡å‹èƒ½å¤Ÿä¿ç•™å›¾åƒå†…å®¹å¹¶ç”Ÿæˆé€¼çœŸçš„ç»†èŠ‚ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84ba1531242e4c3083a32ecca85d61be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9588ee563dbcd5c5594552f5c0aca6d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29361a46ec7f8c1c890c1a67b43668d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df557009fd913f7477ee7d240e8de9fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e70d37a57d8e605234da4619c5f2d9a2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6e023b873a4405186db6379b15919ad2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  VideoMind A Chain-of-LoRA Agent for Long Video Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6ab0166324c140954021b1012ef6c3ae.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  VideoMind A Chain-of-LoRA Agent for Long Video Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
