<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-18  Realization of a Pre-Sample Photonic-based Free-Electron Modulator in   Ultrafast Transmission Electron Microscopes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ff0097c597a4034c4aef4a18d8813f90.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-18-更新"><a href="#2025-03-18-更新" class="headerlink" title="2025-03-18 更新"></a>2025-03-18 更新</h1><h2 id="Realization-of-a-Pre-Sample-Photonic-based-Free-Electron-Modulator-in-Ultrafast-Transmission-Electron-Microscopes"><a href="#Realization-of-a-Pre-Sample-Photonic-based-Free-Electron-Modulator-in-Ultrafast-Transmission-Electron-Microscopes" class="headerlink" title="Realization of a Pre-Sample Photonic-based Free-Electron Modulator in   Ultrafast Transmission Electron Microscopes"></a>Realization of a Pre-Sample Photonic-based Free-Electron Modulator in   Ultrafast Transmission Electron Microscopes</h2><p><strong>Authors:Beatrice Matilde Ferrari, Cameron James Richard Duncan, Michael Yannai, Raphael Dahan, Paolo Rosi, Irene Ostroman, Maria Giulia Bravi, Arthur Niedermayr, Tom Lenkiewicz Abudi, Yuval Adiv, Tal Fishman, Sang Tae Park, Dan Masiel, Thomas Lagrange, Fabrizio Carbone, Vincenzo Grillo, F. Javier García de Abajo, Ido Kaminer, Giovanni Maria Vanacore</strong></p>
<p>Spatial and temporal light modulation is a well-established technology that enables dynamic shaping of the phase and amplitude of optical fields, significantly enhancing the resolution and sensitivity of imaging methods. Translating this capability to electron beams is highly desirable within the framework of a transmission electron microscope (TEM) to benefit from the nanometer spatial resolution of these instruments. In this work, we report on the experimental realization of a photonic-based free-electron modulator integrated into the column of two ultrafast TEMs for pre-sample electron-beam shaping. Electron-photon interaction is employed to coherently modulate both the transverse and longitudinal components of the electron wave function, while leveraging dynamically controlled optical fields and tailored design of electron-laser-sample interaction geometry. Using energy- and momentum-resolved electron detection, we successfully reconstruct the shaped electron wave function at the TEM sample plane. These results demonstrate the ability to manipulate the electron wave function before probing the sample, paving the way for the future development of innovative imaging methods in ultrafast electron microscopy. </p>
<blockquote>
<p>空间和时间光调制是一项成熟的技术，能够动态地改变光场的相位和振幅，极大地提高了成像方法的分辨率和灵敏度。在透射电子显微镜（TEM）的框架下，将该能力应用于电子束是非常理想的，以受益于这些仪器的纳米级空间分辨率。在这项工作中，我们报告了在两辆超快透射电子显微镜的列中集成基于光子学的自由电子调制器的实验实现，用于样品前的电子束整形。利用电子-光子相互作用，相干地调制电子波函数的横向和纵向分量，同时利用动态控制的光场和定制的电子-激光-样品相互作用几何结构。通过能量和动量解析的电子检测，我们成功地在透射电子显微镜样品平面上重建了整形的电子波函数。这些结果证明了在探测样品之前操纵电子波函数的能力，为未来在超快电子显微镜中开发创新成像方法铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11313v1">PDF</a> 14 pages, 5, figures, includes supplementary information, journal   paper</p>
<p><strong>Summary</strong></p>
<p>空间和时间光调制是一项成熟的技术，能动态调整光学场的相位和振幅，显著提高成像方法的分辨率和灵敏度。将这一技术应用于透射电子显微镜（TEM）的电子束中，以利用这些仪器的纳米级空间分辨率，是非常理想的。本研究报道了将光子基自由电子调制器集成到两台超快TEM的列中进行预样本电子束形状的实验实现。电子-光子相互作用用于相干地调制电子波函数的横向和纵向分量，同时利用动态控制的光场和定制的电子-激光-样本相互作用几何设计。通过能量和动量解析的电子检测，我们在TEM样本平面上成功重建了形状的电子波函数。这些结果展示了在探测样品之前操作电子波函数的能力，为未来在超快电子显微镜中开发创新成像方法铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间和时间光调制技术能够动态调整光学场的相位和振幅，增强成像方法的分辨率和灵敏度。</li>
<li>将此技术应用于透射电子显微镜的电子束中非常理想，以利用纳米级空间分辨率。</li>
<li>实验实现了光子基自由电子调制器的集成，用于预样本电子束形状的调整。</li>
<li>通过电子-光子相互作用，可以相干地调制电子波函数的横向和纵向分量。</li>
<li>利用动态控制的光场和定制的电子-激光-样本相互作用几何设计，实现了对电子波函数的有效操控。</li>
<li>通过能量和动量解析的电子检测，成功重建了形状的电子波函数在TEM样本平面上的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7aa1c8f08f2aa62ae1c2280991ff7a09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20d8a75305d172f1d28e0debb3ffa1b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e43844e66c937408102bd4255f8effc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84f42b5ca70d218cbf811c5293099b99.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CyclePose-–-Leveraging-Cycle-Consistency-for-Annotation-Free-Nuclei-Segmentation-in-Fluorescence-Microscopy"><a href="#CyclePose-–-Leveraging-Cycle-Consistency-for-Annotation-Free-Nuclei-Segmentation-in-Fluorescence-Microscopy" class="headerlink" title="CyclePose – Leveraging Cycle-Consistency for Annotation-Free Nuclei   Segmentation in Fluorescence Microscopy"></a>CyclePose – Leveraging Cycle-Consistency for Annotation-Free Nuclei   Segmentation in Fluorescence Microscopy</h2><p><strong>Authors:Jonas Utz, Stefan Vocht, Anne Tjorven Buessen, Dennis Possart, Fabian Wagner, Mareike Thies, Mingxuan Gu, Stefan Uderhardt, Katharina Breininger</strong></p>
<p>In recent years, numerous neural network architectures specifically designed for the instance segmentation of nuclei in microscopic images have been released. These models embed nuclei-specific priors to outperform generic architectures like U-Nets; however, they require large annotated datasets, which are often not available. Generative models (GANs, diffusion models) have been used to compensate for this by synthesizing training data. These two-stage approaches are computationally expensive, as first a generative model and then a segmentation model has to be trained. We propose CyclePose, a hybrid framework integrating synthetic data generation and segmentation training. CyclePose builds on a CycleGAN architecture, which allows unpaired translation between microscopy images and segmentation masks. We embed a segmentation model into CycleGAN and leverage a cycle consistency loss for self-supervision. Without annotated data, CyclePose outperforms other weakly or unsupervised methods on two public datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose">https://github.com/jonasutz/CyclePose</a> </p>
<blockquote>
<p>近年来，针对显微图像中的细胞核实例分割，已经推出了许多专门设计的神经网络架构。这些模型嵌入细胞核特异性先验知识，以超越通用架构（如U-Nets）。然而，它们需要大量标注数据集，而这些数据通常不可用。生成模型（GANs，扩散模型）已被用于通过合成训练数据来弥补这一缺陷。这两种两阶段的方法计算成本高昂，因为首先必须训练一个生成模型，然后训练一个分割模型。我们提出了CyclePose，这是一个混合框架，融合了合成数据生成和分割训练。CyclePose基于CycleGAN架构，允许显微图像和分割掩膜之间的无配对转换。我们将分割模型嵌入到CycleGAN中，并利用循环一致性损失进行自监督。无需标注数据，CyclePose在两个公共数据集上的表现优于其他弱监督或无监督方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/jonasutz/CyclePose%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jonasutz/CyclePose找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11266v1">PDF</a> under review for MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对显微图像中细胞核实例分割的神经网络架构CyclePose。该框架集成了合成数据生成和分割训练，采用CycleGAN架构实现显微图像与分割掩膜之间的无配对转换。CyclePose在无需标注数据的情况下，在公共数据集上的表现优于其他弱监督或无监督方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近年针对显微图像中细胞核实例分割的神经网络架构逐渐增多，但通常需要大量标注数据。</li>
<li>生成模型（如GANs、扩散模型）已被用于合成训练数据，以弥补标注数据的不足。</li>
<li>两阶段方法（先训练生成模型再训练分割模型）计算成本较高。</li>
<li>CyclePose是一种集成合成数据生成和分割训练的混合框架。</li>
<li>CyclePose基于CycleGAN架构，实现显微图像与分割掩膜之间的无配对转换。</li>
<li>CyclePose在无需标注数据的情况下，在公共数据集上的表现优于其他弱监督或无监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3cfd4cdec05cc7a5549ce9aea702bf78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff0097c597a4034c4aef4a18d8813f90.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation"><a href="#Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation" class="headerlink" title="Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"></a>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</h2><p><strong>Authors:Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye</strong></p>
<p>We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a> </p>
<blockquote>
<p>我们介绍了Reangle-A-Video，这是一个从单个输入视频生成同步多视角视频的统一框架。不同于主流方法在大型4D数据集上训练多视角视频扩散模型，我们的方法将多视角视频生成任务重新构造成视频到视频的翻译，并利用可公开获取的图像和视频扩散先验。本质上，Reangle-A-Video分为两个阶段。（1）多视角运动学习：以自监督的方式同步微调图像到视频扩散转换器，从一组变形视频中提取视角不变运动。（2）多视角一致图像到图像翻译：输入视频的第一帧在推理时间跨视角一致性指导下被变形和填充到各种相机视角，生成多视角一致起始图像。对静态视角传输和动态摄像机控制的广泛实验表明，Reangle-A-Video超越了现有方法，为多视角视频生成建立了新的解决方案。我们将公开发布我们的代码和数据。项目页面：<a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09151v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
<p><strong>Summary</strong></p>
<p>基于单输入视频生成同步多视角视频的Reangle-A-Video统一框架介绍。不同于主流方法在大型4D数据集上训练多视角视频扩散模型，Reangle-A-Video将多视角视频生成任务重构为视频到视频的翻译任务，利用公众可获得的图像和视频扩散先验知识。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Reangle-A-Video是一个统一框架，可以从单个输入视频生成同步多视角视频。</li>
<li>该方法通过视频到视频的翻译任务来重构多视角视频生成，而非在大型4D数据集上训练多视角视频扩散模型。</li>
<li>利用图像和视频扩散的先验知识。</li>
<li>该框架包含两个阶段：多视角运动学习（同步微调图像到视频扩散转换器以从一组变形视频中提取视角不变的运动）和多视角一致图像到图像翻译（将输入视频的第一帧在推理时间跨视角一致性指导下变形和填充，生成多视角一致起始图像）。</li>
<li>通过静态视角传输和动态相机控制的大量实验证明，Reangle-A-Video超越了现有方法，为多视角视频生成提供了新的解决方案。</li>
<li>该研究将公开代码和数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09151">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1eff3b7dde46c0a0f2e660a0d6b19c49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-034525eeb601df4fbf9f0abc26d2d754.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b7950510b9a10e64706e857d8e34f6.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_I2I Translation/2503.09151v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Whole-Body Talking Human Animation"></a>Versatile Multimodal Controls for Whole-Body Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Minghui Yang, Ming Yang, Le Wang</strong></p>
<p>Human animation from a single reference image shall be flexible to synthesize whole-body motion for either a headshot or whole-body portrait, where the motions are readily controlled by audio signal and text prompts. This is hard for most existing methods as they only support producing pre-specified head or half-body motion aligned with audio inputs. In this paper, we propose a versatile human animation method, i.e., VersaAnimator, which generates whole-body talking human from arbitrary portrait images, not only driven by audio signal but also flexibly controlled by text prompts. Specifically, we design a text-controlled, audio-driven motion generator that produces whole-body motion representations in 3D synchronized with audio inputs while following textual motion descriptions. To promote natural smooth motion, we propose a code-pose translation module to link VAE codebooks with 2D DWposes extracted from template videos. Moreover, we introduce a multi-modal video diffusion that generates photorealistic human animation from a reference image according to both audio inputs and whole-body motion representations. Extensive experiments show that VersaAnimator outperforms existing methods in visual quality, identity preservation, and audio-lip synchronization. </p>
<blockquote>
<p>从单一参考图像进行的人物动画应当能够灵活地合成头部或全身肖像的全身运动，这些运动可以通过音频信号和文字提示轻松控制。对于大多数现有方法而言，这很难实现，因为它们仅支持生成与音频输入对齐的预设头部或半身运动。在本文中，我们提出了一种通用的人物动画方法，即VersaAnimator。它可以从任意肖像图像生成全身说话的人物，不仅由音频信号驱动，还可以灵活地由文字提示控制。具体来说，我们设计了一个文本控制、音频驱动的运动生成器，它产生与音频输入同步的全身运动表示（3D），同时遵循文本运动描述。为了促进自然流畅的运动，我们提出了一个代码姿势翻译模块，将VAE代码本与从模板视频中提取的2DDW姿势联系起来。此外，我们引入了一种多模态视频扩散方法，根据音频输入和全身运动表示从参考图像生成逼真的人物动画。大量实验表明，VersaAnimator在视觉质量、身份保留和音频-嘴唇同步方面优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v2">PDF</a> </p>
<p><strong>Summary</strong><br>基于单张参考图像的人体动画能灵活地合成全身动作，无论是头部特写还是全身肖像，这些动作都能通过音频信号和文字提示轻易控制。现有的多数方法仅支持根据音频输入生成预设的头部或半身动作。本文提出了一种通用的人体动画方法，即VersaAnimator，不仅能由音频信号驱动，还能通过文字提示灵活控制，从任意肖像图像生成全身动作。此方法设计了文本控制的音频驱动运动生成器，在三维空间中生成与音频输入同步的全身动作表示，并遵循文本动作描述以产生自然流畅的运动。此外，通过连接变自动编码器代码本与从模板视频提取的二维动态姿势，提出了代码姿势翻译模块。再结合多模态视频扩散技术，根据参考图像、音频输入和全身动作表示生成逼真的人体动画。实验表明，VersaAnimator在视觉质量、身份保留和音频唇同步方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VersaAnimator能够从单张参考图像生成全身动画。</li>
<li>该方法支持通过音频信号和文字提示控制动画。</li>
<li>VersaAnimator设计了文本控制的音频驱动运动生成器，生成与音频同步的全身动作表示。</li>
<li>通过代码姿势翻译模块连接变自动编码器代码本和二维动态姿势。</li>
<li>多模态视频扩散技术用于生成逼真的人体动画。</li>
<li>VersaAnimator在视觉质量、身份保留和音频唇同步方面表现优异。</li>
<li>当前方法相对于现有方法的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a4be268b951eb464599915f093168534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b16248ac9cc86d5b6803d40037667cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b111d2208a9e49f013d5550bffdaa200.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28fcbcf16b6a3381317a45ec3ee5082f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dfa8bd672ff361a77bf3c1459b8296e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion embeds an input reference image into arbitrary scenes as described by the text prompts, while exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion features’ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing harmonious fusion of the reference structural information and the textual semantic information. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning, all while substantially outperforming related methods in image quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. </p>
<blockquote>
<p>光学错觉隐藏图像是一种有趣的视觉感知现象，其中图像被巧妙地集成到另一幅图像中，观众无法立即发现。我们基于现成的文本到图像（T2I）扩散模型，提出了一种无需训练的文字引导图像到图像（I2I）转换框架，称为Phase-Transferred Diffusion Model（PTDiffusion），用于合成隐藏艺术图像。PTDiffusion将输入参考图像嵌入到文本提示描述的任意场景中，同时显示参考图像的隐藏视觉线索。我们方法的核心是一个即插即用的相位转移机制，该机制动态且逐步地移植扩散特征的相位谱，从去噪过程中重建参考图像，并将其采样为生成的错觉图像，实现参考结构信息和文本语义信息的和谐融合。此外，我们提出了异步相位转移，以实现灵活控制隐藏内容的可识别程度。我们的方法避免了任何模型训练和微调，同时在图像质量、文本保真度、视觉可识别度和上下文自然度方面大大优于相关方法在错觉图像合成方面的表现，这已通过广泛的质量和数量实验得到证明。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v2">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>光学错觉隐藏图像是一种有趣的视觉感知现象，其中图像被巧妙地融入另一幅画面中，观众无法立即察觉。基于现成的文本到图像（T2I）扩散模型，提出了一种无需训练、文本引导的图像到图像（I2I）转换框架——相位转移扩散模型（PTDiffusion），用于合成隐藏艺术图像。PTDiffusion可将输入参考图像嵌入到文本提示所描述的任意场景中，同时展示参考图像的隐藏视觉线索。该方法的核心是一个即插即用的相位转移机制，该机制动态且渐进地移植扩散特征的相位谱，从去噪过程中重建参考图像并采样生成错觉图像，实现参考结构信息和文本语义信息的和谐融合。此外，还提出了异步相位转移，以实现对隐藏内容辨识度的灵活控制。该方法无需任何模型训练和微调，在图像质量、文本保真度、视觉辨识度和上下文自然度方面均优于相关方法，大量定性和定量实验证明了这一点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>光学错觉隐藏图片是一种视觉感知现象，图像被巧妙地融入另一幅画面中。</li>
<li>提出了一种新的训练外文本引导的图像到图像（I2I）转换框架——相位转移扩散模型（PTDiffusion）。</li>
<li>PTDiffusion可将参考图像嵌入任意场景，同时展示其隐藏视觉线索。</li>
<li>核心机制是相位转移，该机制能够动态地重建和融合参考图像和文本提示。</li>
<li>异步相位转移可实现隐藏内容辨识度的灵活控制。</li>
<li>该方法无需模型训练和微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1439734c12e8f96afe7f6f6170e7e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eaaf2a9d0ccc30a7efbdd5937b621d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6005a17c2307fd503c9c683c80fb486c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9447fd81ce69dc56cb27f94aa559ab91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4093835e9ee8f1fac03a92910a207a22.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Visual-Modality-Prompt-for-Adapting-Vision-Language-Object-Detectors"><a href="#Visual-Modality-Prompt-for-Adapting-Vision-Language-Object-Detectors" class="headerlink" title="Visual Modality Prompt for Adapting Vision-Language Object Detectors"></a>Visual Modality Prompt for Adapting Vision-Language Object Detectors</h2><p><strong>Authors:Heitor R. Medeiros, Atif Belal, Srikanth Muralidharan, Eric Granger, Marco Pedersoli</strong></p>
<p>The zero-shot performance of object detectors degrades when tested on different modalities, such as infrared and depth. While recent work has explored image translation techniques to adapt detectors to new modalities, these methods are limited to a single modality and apply only to traditional detectors. Recently, vision-language detectors, such as YOLO-World and Grounding DINO, have shown promising zero-shot capabilities, however, they have not yet been adapted for other visual modalities. Traditional fine-tuning approaches compromise the zero-shot capabilities of the detectors. The visual prompt strategies commonly used for classification with vision-language models apply the same linear prompt translation to each image, making them less effective. To address these limitations, we propose ModPrompt, a visual prompt strategy to adapt vision-language detectors to new modalities without degrading zero-shot performance. In particular, an encoder-decoder visual prompt strategy is proposed, further enhanced by the integration of inference-friendly modality prompt decoupled residual, facilitating a more robust adaptation. Empirical benchmarking results show our method for modality adaptation on two vision-language detectors, YOLO-World and Grounding DINO, and on challenging infrared (LLVIP, FLIR) and depth (NYUv2) datasets, achieving performance comparable to full fine-tuning while preserving the model’s zero-shot capability. Code available at: <a target="_blank" rel="noopener" href="https://github.com/heitorrapela/ModPrompt">https://github.com/heitorrapela/ModPrompt</a>. </p>
<blockquote>
<p>对象检测器的零样本性能在不同的模态（如红外和深度）上进行测试时会下降。虽然近期的工作已经探索了图像翻译技术来适应新的模态检测器，但这些方法仅限于单一模态，仅适用于传统检测器。最近，视觉语言检测器，如YOLO-World和Grounding DINO，已经显示出有希望的零样本能力，然而，它们尚未适应其他视觉模态。传统的微调方法会损害检测器的零样本能力。视觉提示策略常用于与视觉语言模型一起进行分类任务，将相同的线性提示翻译应用于每个图像，使其效果较差。为了解决这些局限性，我们提出了ModPrompt，这是一种视觉提示策略，旨在适应新的模态视觉语言检测器，而不会降低零样本性能。特别是，我们提出了一种编码器-解码器视觉提示策略，通过引入推理友好的模态提示解耦残差进行增强，从而实现更稳健的适应。实证基准测试结果表明，我们的方法在两种视觉语言检测器YOLO-World和Grounding DINO上，以及在具有挑战性的红外（LLVIP、FLIR）和深度（NYUv2）数据集上进行模态适应的方法，在达到与完全微调相当的性能的同时，保持了模型的零样本能力。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/heitorrapela/ModPrompt%E3%80%82">https://github.com/heitorrapela/ModPrompt。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00622v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要探讨零镜头目标检测器在处理不同模态如红外和深度时性能下降的问题。传统方法主要通过图像翻译技术或精细调整来适应新模态，但这种方法可能会损害零镜头性能。为此，本文提出了一种名为ModPrompt的视觉提示策略，该策略可适应新模态而无需对零镜头性能进行微调。该策略使用编码器解码器结构，并加入推理友好的模态提示分离残留物，实现更稳健的适应。在两种视觉语言检测器YOLO-World和Grounding DINO上进行的实验结果表明，该方法在红外和深度数据集上的性能与完全精细调整相当，同时保持了模型的零镜头能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零镜头目标检测器在不同模态下的性能会下降。</li>
<li>传统方法通过图像翻译或精细调整来适应新模态，但可能损害零镜头性能。</li>
<li>ModPrompt策略旨在适应新模态而不损害零镜头性能。</li>
<li>ModPrompt策略采用编码器解码器结构，加入推理友好的模态提示分离残留物。</li>
<li>ModPrompt在YOLO-World和Grounding DINO等视觉语言检测器上表现良好。</li>
<li>在红外和深度数据集上的实验结果表明，ModPrompt性能与完全精细调整相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b1dcecac4c123c38d610d8b66152e8f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43530c2b3d971cfc156a0d1db4e07175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2f782715d1a8992afb8273f849f3b55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d029afb2cc36d31208df14ea79e3be62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87cb16ed6cc8ebd2363ca7670e668b21.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DVMNet-Rethinking-Relative-Pose-Estimation-for-Unseen-Objects"><a href="#DVMNet-Rethinking-Relative-Pose-Estimation-for-Unseen-Objects" class="headerlink" title="DVMNet++: Rethinking Relative Pose Estimation for Unseen Objects"></a>DVMNet++: Rethinking Relative Pose Estimation for Unseen Objects</h2><p><strong>Authors:Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann</strong></p>
<p>Determining the relative pose of a previously unseen object between two images is pivotal to the success of generalizable object pose estimation. Existing approaches typically predict 3D translation utilizing the ground-truth object bounding box and approximate 3D rotation with a large number of discrete hypotheses. This strategy makes unrealistic assumptions about the availability of ground truth and incurs a computationally expensive process of scoring each hypothesis at test time. By contrast, we rethink the problem of relative pose estimation for unseen objects by presenting a Deep Voxel Matching Network (DVMNet++). Our method computes the relative object pose in a single pass, eliminating the need for ground-truth object bounding boxes and rotation hypotheses. We achieve open-set object detection by leveraging image feature embedding and natural language understanding as reference. The detection result is then employed to approximate the translation parameters and crop the object from the query image. For rotation estimation, we map the two RGB images, i.e., reference and cropped query, to their respective voxelized 3D representations. The resulting voxels are passed through a rotation estimation module, which aligns the voxels and computes the rotation in an end-to-end fashion by solving a least-squares problem. To enhance robustness, we introduce a weighted closest voxel algorithm capable of mitigating the impact of noisy voxels. We conduct extensive experiments on the CO3D, Objaverse, LINEMOD, and LINEMOD-O datasets, demonstrating that our approach delivers more accurate relative pose estimates for novel objects at a lower computational cost compared to state-of-the-art methods. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/sailor-z/DVMNet/">https://github.com/sailor-z/DVMNet/</a>. </p>
<blockquote>
<p>确定两个图像之间先前未见对象的相对姿态对于可泛化的对象姿态估计的成功至关重要。现有方法通常利用真实对象边界框预测3D平移，并用大量离散假设来近似3D旋转。这种策略对真实可用性的假设不现实，并且在测试时对每个假设进行评分的过程计算开销很大。相比之下，我们通过提出深度体素匹配网络（DVMNet++）来重新思考未见过对象的相对姿态估计问题。我们的方法在一次传递中计算相对对象姿态，无需真实对象边界框和旋转假设。我们利用图像特征嵌入和自然语言理解为参考来实现开放集对象检测。检测结果然后用于近似平移参数并从查询图像中裁剪出对象。对于旋转估计，我们将两个RGB图像（即参考和裁剪的查询）映射到其各自的体素化3D表示。得到的体素通过一个旋转估计模块，该模块对齐体素并通过解决最小二乘问题以端到端的方式计算旋转。为了提高稳健性，我们引入了一种加权最接近体素算法，能够减轻噪声体素的影响。我们在CO3D、Objaverse、LINEMOD和LINEMOD-O数据集上进行了大量实验，结果表明我们的方法以较低的计算成本为新型对象提供更准确的相对姿态估计，与最先进的方法相比。我们的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/sailor-z/DVMNet/%E3%80%82">https://github.com/sailor-z/DVMNet/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13683v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于深度体素匹配网络（DVMNet++）的方法，用于估计未见对象在两图像间的相对姿态。该方法无需使用真实对象边界框和旋转假设，通过一次计算即可得出相对对象姿态。通过图像特征嵌入和自然语言理解实现开放集对象检测，并据此近似翻译参数并从查询图像中裁剪对象。旋转估计则通过将两个RGB图像转换为各自的体素化三维表示，通过旋转估计模块对齐体素并计算旋转。实验结果表明，该方法在CO3D、Objaverse、LINEMOD和LINEMOD-O数据集上相比最新方法具有更高的相对姿态估计准确性和更低的计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的深度体素匹配网络（DVMNet++）用于估计未见对象在两图像间的相对姿态。</li>
<li>不需要使用真实对象边界框和旋转假设，简化了计算过程。</li>
<li>通过图像特征嵌入和自然语言理解实现开放集对象检测。</li>
<li>引入了一种加权最接近体素算法，以提高方法的稳健性。</li>
<li>在多个数据集上的实验表明，该方法相比最新方法具有更高的准确性和更低的计算成本。</li>
<li>公开了代码，便于他人使用和研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13683">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c22ecb6e5b834b0e0e47d30ef557a85e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ea984154e73daa62de64d742f02362.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ada477d81f6e2a4ec7d0f9797054899d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff210a26ffe1b775b34fdb8692066814.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cde181318e7129abeda2ead1ef9f845.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bc135ad0d0def72cacde9f8a014dd998.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-03-18  Logic-in-Frames Dynamic Keyframe Search via Visual Semantic-Logical   Verification for Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.11223v3/page_5_0.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-18  TriDF Triplane-Accelerated Density Fields for Few-Shot Remote Sensing   Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
