<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b1b8c04a90717be36da770e69bba9395.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-18-æ›´æ–°"><a href="#2025-03-18-æ›´æ–°" class="headerlink" title="2025-03-18 æ›´æ–°"></a>2025-03-18 æ›´æ–°</h1><h2 id="One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation"><a href="#One-Step-Residual-Shifting-Diffusion-for-Image-Super-Resolution-via-Distillation" class="headerlink" title="One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation"></a>One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation</h2><p><strong>Authors:Daniil Selikhanovych, David Li, Aleksei Leonov, Nikita Gushchin, Sergei Kushneriuk, Alexander Filippov, Evgeny Burnaev, Iaroslav Koshelev, Alexander Korotin</strong></p>
<p>Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯èƒ½äº§ç”Ÿé«˜è´¨é‡çš„è§†è§‰æ•ˆæœï¼Œä½†éœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚å°½ç®¡å·²ç»å¼€å‘äº†å‡ ç§åŠ é€Ÿæ‰©æ•£SRæ¨¡å‹çš„æ–¹æ³•ï¼Œä½†ä¸€äº›æ–¹æ³•ï¼ˆä¾‹å¦‚SinSRï¼‰æ— æ³•äº§ç”ŸçœŸå®çš„æ„ŸçŸ¥ç»†èŠ‚ï¼Œè€Œå…¶ä»–æ–¹æ³•ï¼ˆä¾‹å¦‚OSEDiffï¼‰å¯èƒ½ä¼šè™šæ„ä¸å­˜åœ¨çš„ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RSDï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ResShiftçš„æ–°è’¸é¦æ–¹æ³•ï¼ŒResShiftæ˜¯é¡¶çº§çš„æ‰©æ•£SRæ¨¡å‹ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè®­ç»ƒå­¦ç”Ÿç½‘ç»œæ¥ç”Ÿæˆå›¾åƒï¼Œä½¿å¾—åœ¨è¿™äº›å›¾åƒä¸Šè®­ç»ƒçš„æ–°å‡ResShiftæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ç›¸ç¬¦ã€‚RSDå®ç°äº†å•æ­¥æ¢å¤ï¼Œå¹¶ä¸”å¤§å¹…åº¦è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºæˆ‘ä»¬çš„è’¸é¦æ–¹æ³•å¯ä»¥è¶…è¶ŠResShiftçš„å¦ä¸€ç§è’¸é¦æ–¹æ³•SinSRï¼Œä½¿å…¶ä¸æœ€å…ˆè¿›çš„æ‰©æ•£SRè’¸é¦æ–¹æ³•ç›¸åª²ç¾ã€‚ä¸åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„SRæ–¹æ³•ç›¸æ¯”ï¼ŒRSDäº§ç”Ÿçš„æ„ŸçŸ¥è´¨é‡å…·æœ‰ç«äº‰åŠ›ï¼Œæä¾›çš„å›¾åƒä¸é€€åŒ–è¾“å…¥å›¾åƒçš„å¯¹é½æ€§æ›´å¥½ï¼Œå¹¶ä¸”éœ€è¦çš„å‚æ•°å’ŒGPUå†…å­˜æ›´å°‘ã€‚æˆ‘ä»¬åœ¨å„ç§çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šæä¾›äº†å®éªŒç»“æœï¼ŒåŒ…æ‹¬RealSRã€RealSet65ã€DRealSRã€ImageNetå’ŒDIV2Kã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰åº”ç”¨è™½ç„¶å¯ä»¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚è™½ç„¶å·²æœ‰è‹¥å¹²åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆç»†èŠ‚ä¸çœŸå®æˆ–æ¨¡æ‹Ÿä¸å­˜åœ¨çš„ç»“æ„çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ResShiftæ¨¡å‹çš„å…¨æ–°è’¸é¦æ–¹æ³•RSDã€‚RSDè®­ç»ƒå­¦ç”Ÿç½‘ç»œç”Ÿæˆå›¾åƒï¼Œè®©æ–°å‡ResShiftæ¨¡å‹å¯¹å›¾åƒè®­ç»ƒçš„ç»“æœä¸æ•™å¸ˆæ¨¡å‹ç›¸ç¬¦ã€‚RSDå®ç°ä¸€æ­¥æ¢å¤å¹¶å¤§å¹…åº¦è¶…è¶Šæ•™å¸ˆæ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRSDè¶…è¶Šå…¶ä»–åŸºäºè’¸é¦çš„ResShiftæ¨¡å‹SinSRï¼Œä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹SRè’¸é¦æ–¹æ³•è¡¨ç°ç›¸å½“ã€‚ç›¸è¾ƒäºåŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒçš„SRæ–¹æ³•ï¼ŒRSDåœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ›´å…·ç«äº‰åŠ›ï¼Œç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆé€€åŒ–è¾“å…¥å›¾åƒï¼ŒåŒæ—¶å‚æ•°å’ŒGPUå†…å­˜éœ€æ±‚æ›´å°‘ã€‚å®éªŒåœ¨RealSRã€RealSet65ã€DRealSRã€ImageNetå’ŒDIV2Kç­‰çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡åº”ç”¨ä¸­è™½èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>ç›®å‰å­˜åœ¨çš„åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ–¹æ³•ä»å­˜åœ¨é—®é¢˜ï¼Œå¦‚ç”Ÿæˆç»†èŠ‚ä¸çœŸå®æˆ–æ¨¡æ‹Ÿä¸å­˜åœ¨çš„ç»“æ„ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹ResShiftæ¨¡å‹çš„å…¨æ–°è’¸é¦æ–¹æ³•RSDï¼Œå®ç°ä¸€æ­¥æ¢å¤å¹¶å¤§å¹…åº¦æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>RSDé€šè¿‡è®­ç»ƒå­¦ç”Ÿç½‘ç»œç”Ÿæˆå›¾åƒï¼Œä½¿æ–°å‡ResShiftæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ç›¸ç¬¦ã€‚</li>
<li>RSDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–ç›¸å…³æ¨¡å‹ï¼ŒåŒ…æ‹¬SinSRå’Œå…¶ä»–è’¸é¦æ–¹æ³•ã€‚</li>
<li>RSDç›¸è¾ƒäºåŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒçš„SRæ–¹æ³•æ›´å…·ç«äº‰åŠ›ï¼Œç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆé€€åŒ–è¾“å…¥å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a24412bf3591c4a7e10fa509b7c428c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b39c0cf99c41bbbb61f35ca8b0c62b83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3993bb76507cdfd8790b61d580fab4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a417397af1b562e441101600d926d5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generative-Gaussian-Splatting-Generating-3D-Scenes-with-Video-Diffusion-Priors"><a href="#Generative-Gaussian-Splatting-Generating-3D-Scenes-with-Video-Diffusion-Priors" class="headerlink" title="Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion   Priors"></a>Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion   Priors</h2><p><strong>Authors:Katja Schwarz, Norman Mueller, Peter Kontschieder</strong></p>
<p>Synthesizing consistent and photorealistic 3D scenes is an open problem in computer vision. Video diffusion models generate impressive videos but cannot directly synthesize 3D representations, i.e., lack 3D consistency in the generated sequences. In addition, directly training generative 3D models is challenging due to a lack of 3D training data at scale. In this work, we present Generative Gaussian Splatting (GGS) â€“ a novel approach that integrates a 3D representation with a pre-trained latent video diffusion model. Specifically, our model synthesizes a feature field parameterized via 3D Gaussian primitives. The feature field is then either rendered to feature maps and decoded into multi-view images, or directly upsampled into a 3D radiance field. We evaluate our approach on two common benchmark datasets for scene synthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model significantly improves both the 3D consistency of the generated multi-view images, and the quality of the generated 3D scenes over all relevant baselines. Compared to a similar model without 3D representation, GGS improves FID on the generated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page: <a target="_blank" rel="noopener" href="https://katjaschwarz.github.io/ggs/">https://katjaschwarz.github.io/ggs/</a> </p>
<blockquote>
<p>åˆæˆä¸€è‡´ä¸”é€¼çœŸçš„3Dåœºæ™¯æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆä»¤äººå°è±¡æ·±åˆ»çš„è§†é¢‘ï¼Œä½†æ— æ³•ç›´æ¥åˆæˆ3Dè¡¨ç¤ºï¼Œå³åœ¨ç”Ÿæˆçš„åºåˆ—ä¸­ç¼ºä¹3Dä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡çš„3Dè®­ç»ƒæ•°æ®ï¼Œç›´æ¥è®­ç»ƒç”Ÿæˆå¼3Dæ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”Ÿæˆå¼é«˜æ–¯å–·æº…ï¼ˆGGSï¼‰â€”â€”ä¸€ç§å°†3Dè¡¨ç¤ºä¸é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡3Dé«˜æ–¯åŸå§‹æ•°æ®åˆæˆç‰¹å¾åœºã€‚ç„¶åï¼Œè¯¥ç‰¹å¾åœºè¦ä¹ˆè¢«æ¸²æŸ“ä¸ºç‰¹å¾å›¾å¹¶è§£ç ä¸ºå¤šè§†å›¾å›¾åƒï¼Œè¦ä¹ˆç›´æ¥ä¸Šé‡‡æ ·ä¸º3Dè¾å°„åœºã€‚æˆ‘ä»¬åœ¨åœºæ™¯åˆæˆçš„ä¸¤ä¸ªå¸¸ç”¨åŸºå‡†æ•°æ®é›†RealEstate10Kå’ŒScanNet+ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå‘ç°æ‰€æå‡ºçš„GGSæ¨¡å‹åœ¨ç”Ÿæˆçš„å¤šè§†å›¾å›¾åƒçš„3Dä¸€è‡´æ€§å’Œç”Ÿæˆçš„3Dåœºæ™¯çš„è´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹å–„ï¼Œä¼˜äºæ‰€æœ‰ç›¸å…³åŸºå‡†ã€‚ä¸æ²¡æœ‰3Dè¡¨ç¤ºçš„ç±»ä¼¼æ¨¡å‹ç›¸æ¯”ï¼ŒGGSåœ¨RealEstate10Kå’ŒScanNet+ä¸Šå°†ç”Ÿæˆçš„3Dåœºæ™¯çš„FIDæé«˜äº†çº¦20%ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://katjaschwarz.github.io/ggs/">https://katjaschwarz.github.io/ggs/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13272v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGenerative Gaussian Splattingï¼ˆGGSï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒå°†3Dè¡¨ç¤ºä¸é¢„è®­ç»ƒçš„æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œç”¨äºåˆæˆ3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆä»¥3Dé«˜æ–¯åŸå§‹æ•°æ®ä¸ºå‚æ•°çš„ç‰¹å¾åœºï¼Œç„¶åå°†å…¶æ¸²æŸ“ä¸ºç‰¹å¾å›¾å¹¶è§£ç ä¸ºå¤šè§†è§’å›¾åƒæˆ–ç›´æ¥ä¸Šé‡‡æ ·ä¸º3Dè¾å°„åœºã€‚åœ¨RealEstate10Kå’ŒScanNet+ä¸¤ä¸ªå¸¸ç”¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒGGSæ¨¡å‹æ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„å¤šè§†è§’å›¾åƒçš„3Dä¸€è‡´æ€§å’Œç”Ÿæˆçš„3Dåœºæ™¯çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GGSæ˜¯ä¸€ç§å°†3Dè¡¨ç¤ºä¸é¢„è®­ç»ƒæ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç»“åˆçš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åˆæˆä»¥3Dé«˜æ–¯åŸå§‹æ•°æ®ä¸ºå‚æ•°çš„ç‰¹å¾åœºæ¥å®ç°3Dåœºæ™¯çš„ç”Ÿæˆã€‚</li>
<li>ç‰¹å¾åœºå¯ä»¥æ¸²æŸ“ä¸ºç‰¹å¾å›¾å¹¶è§£ç ä¸ºå¤šè§†è§’å›¾åƒæˆ–ç›´æ¥ä¸Šé‡‡æ ·ä¸º3Dè¾å°„åœºã€‚</li>
<li>åœ¨RealEstate10Kå’ŒScanNet+æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒGGSæ¨¡å‹æé«˜äº†ç”Ÿæˆçš„å¤šè§†è§’å›¾åƒçš„3Dä¸€è‡´æ€§å’Œ3Dåœºæ™¯çš„è´¨é‡ã€‚</li>
<li>ä¸æ²¡æœ‰3Dè¡¨ç¤ºçš„ç±»ä¼¼æ¨¡å‹ç›¸æ¯”ï¼ŒGGSæ¨¡å‹åœ¨ç”Ÿæˆçš„3Dåœºæ™¯ä¸Šçš„FIDå¾—åˆ†æé«˜äº†çº¦20%ã€‚</li>
<li>GGSæ¨¡å‹é€šè¿‡æ•´åˆ3Dè¡¨ç¤ºï¼Œè§£å†³äº†è§†é¢‘æ‰©æ•£æ¨¡å‹æ— æ³•ç›´æ¥åˆæˆ3Dè¡¨ç¤ºçš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b1b8c04a90717be36da770e69bba9395.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a91f4c36995ffc0029bffab21ec26582.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b20b930cb75d0495169548aaafbd94d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FlexWorld-Progressively-Expanding-3D-Scenes-for-Flexiable-View-Synthesis"><a href="#FlexWorld-Progressively-Expanding-3D-Scenes-for-Flexiable-View-Synthesis" class="headerlink" title="FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View   Synthesis"></a>FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View   Synthesis</h2><p><strong>Authors:Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, Chongxuan Li</strong></p>
<p>Generating flexible-view 3D scenes, including 360{\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\deg} rotations and zooming. Project page: <a target="_blank" rel="noopener" href="https://ml-gsai.github.io/FlexWorld">https://ml-gsai.github.io/FlexWorld</a>. </p>
<blockquote>
<p>ç”ŸæˆåŒ…å«360Â°æ—‹è½¬å’Œç¼©æ”¾åŠŸèƒ½çš„çµæ´»è§†è§’3Dåœºæ™¯ï¼Œä»å•å¼ å›¾åƒå¼€å§‹æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹3Dæ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†FlexWorldï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆçš„æ–°å‹æ¡†æ¶ï¼šï¼ˆ1ï¼‰å¼ºå¤§çš„è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»ç”±ç²—ç³™åœºæ™¯æ¸²æŸ“çš„ä¸å®Œæ•´è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªæ¸è¿›çš„æ‰©å±•è¿‡ç¨‹æ¥æ„å»ºå®Œæ•´çš„3Dåœºæ™¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œå€ŸåŠ©å…ˆè¿›çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹å’Œç²¾ç¡®çš„æ·±åº¦ä¼°è®¡è®­ç»ƒå¯¹ï¼Œæˆ‘ä»¬çš„V2Væ¨¡å‹å¯ä»¥åœ¨å¤§ç›¸æœºå§¿æ€å˜åŒ–ä¸‹ç”Ÿæˆæ–°è§†è§’ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒFlexWorldé€æ­¥ç”Ÿæˆæ–°çš„3Då†…å®¹ï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥åœºæ™¯èåˆå°†å…¶é›†æˆåˆ°å…¨å±€åœºæ™¯ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlexWorldåœ¨ç”Ÿæˆé«˜è´¨é‡æ–°è§†è§’è§†é¢‘å’Œçµæ´»è§†è§’3Dåœºæ™¯æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä»å•å¼ å›¾åƒå¼€å§‹ï¼Œåœ¨å¤šä¸ªæµè¡ŒæŒ‡æ ‡å’Œæ•°æ®é›†ä¸Šä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†ä¼˜è¶Šçš„è§†è§‰è´¨é‡ã€‚ä»å®šæ€§è§’åº¦çœ‹ï¼Œæˆ‘ä»¬å¼ºè°ƒFlexWorldå¯ä»¥ç”Ÿæˆå…·æœ‰çµæ´»è§†è§’çš„é«˜ä¿çœŸåœºæ™¯ï¼Œå¦‚360Â°æ—‹è½¬å’Œç¼©æ”¾ã€‚é¡¹ç›®é¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://ml-gsai.github.io/FlexWorld%E3%80%82]">https://ml-gsai.github.io/FlexWorldã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13265v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFlexWorldçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºä»å•ä¸€å›¾åƒç”Ÿæˆçµæ´»çš„3Dåœºæ™¯ï¼ŒåŒ…æ‹¬360Â°æ—‹è½¬å’Œç¼©æ”¾ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€æ˜¯å¼ºå¤§çš„è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»ç²—ç•¥åœºæ™¯æ¸²æŸ“çš„ä¸å®Œæ•´è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼›äºŒæ˜¯æ¸è¿›æ‰©å±•è¿‡ç¨‹ï¼Œç”¨äºæ„å»ºå®Œæ•´çš„3Dåœºæ™¯ã€‚FlexWorldèƒ½å¤Ÿåˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹å’Œç²¾ç¡®çš„æ·±åº¦ä¼°è®¡è®­ç»ƒå¯¹ï¼Œç”Ÿæˆæ–°å‹è§†è§’å›¾åƒï¼Œå¹¶åœ¨å¤§å‹ç›¸æœºå§¿æ€å˜åŒ–ä¸‹è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒFlexWorldé€æ­¥ç”Ÿæˆæ–°çš„3Då†…å®¹ï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥åœºæ™¯èåˆå°†å…¶èå…¥å…¨å±€åœºæ™¯ã€‚å®éªŒè¯æ˜ï¼ŒFlexWorldåœ¨ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’è§†é¢‘å’Œçµæ´»çš„3Dåœºæ™¯æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¤šä¸ªæµè¡Œçš„æŒ‡æ ‡å’Œæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„è§†è§‰è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlexWorldæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç”¨äºä»å•ä¸€å›¾åƒç”Ÿæˆçµæ´»çš„3Dåœºæ™¯ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šè§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ‰©æ•£æ¨¡å‹å’Œæ¸è¿›æ‰©å±•è¿‡ç¨‹ã€‚</li>
<li>V2Væ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹å’Œæ·±åº¦ä¼°è®¡è®­ç»ƒå¯¹ï¼Œèƒ½ç”Ÿæˆæ–°å‹è§†è§’å›¾åƒï¼Œåº”å¯¹å¤§å‹ç›¸æœºå§¿æ€å˜åŒ–ã€‚</li>
<li>æ¸è¿›æ‰©å±•è¿‡ç¨‹ç”¨äºæ„å»ºå®Œæ•´çš„3Dåœºæ™¯ï¼Œé€æ­¥ç”Ÿæˆæ–°çš„3Då†…å®¹å¹¶èå…¥å…¨å±€åœºæ™¯ã€‚</li>
<li>FlexWorldç”Ÿæˆçš„é«˜è´¨é‡æ–°è§†è§’è§†é¢‘å’Œçµæ´»çš„3Dåœºæ™¯åœ¨å¤šä¸ªæŒ‡æ ‡å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>FlexWorldç”Ÿæˆçš„åœºæ™¯å…·æœ‰é«˜ä¿çœŸåº¦ï¼Œæ”¯æŒ360Â°æ—‹è½¬å’Œç¼©æ”¾ç­‰çµæ´»è§†å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9512a10faf2502add1744e5d8e716d01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5611d516e934281ddf5b311e6e7b6b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fa7443ececa6211f1dfbdef7644363b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d5d2532a18f471e4018584f4c9e3c6d.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2503.13265v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MedLoRD-A-Medical-Low-Resource-Diffusion-Model-for-High-Resolution-3D-CT-Image-Synthesis"><a href="#MedLoRD-A-Medical-Low-Resource-Diffusion-Model-for-High-Resolution-3D-CT-Image-Synthesis" class="headerlink" title="MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D   CT Image Synthesis"></a>MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D   CT Image Synthesis</h2><p><strong>Authors:Marvin Seyfarth, Salman Ul Hassan Dar, Isabelle Ayx, Matthias Alexander Fink, Stefan O. Schoenberg, Hans-Ulrich Kauczor, Sandy Engelhardt</strong></p>
<p>Advancements in AI for medical imaging offer significant potential. However, their applications are constrained by the limited availability of data and the reluctance of medical centers to share it due to patient privacy concerns. Generative models present a promising solution by creating synthetic data as a substitute for real patient data. However, medical images are typically high-dimensional, and current state-of-the-art methods are often impractical for computational resource-constrained healthcare environments. These models rely on data sub-sampling, raising doubts about their feasibility and real-world applicability. Furthermore, many of these models are evaluated on quantitative metrics that alone can be misleading in assessing the image quality and clinical meaningfulness of the generated images. To address this, we introduce MedLoRD, a generative diffusion model designed for computational resource-constrained environments. MedLoRD is capable of generating high-dimensional medical volumes with resolutions up to 512$\times$512$\times$256, utilizing GPUs with only 24GB VRAM, which are commonly found in standard desktop workstations. MedLoRD is evaluated across multiple modalities, including Coronary Computed Tomography Angiography and Lung Computed Tomography datasets. Extensive evaluations through radiological evaluation, relative regional volume analysis, adherence to conditional masks, and downstream tasks show that MedLoRD generates high-fidelity images closely adhering to segmentation mask conditions, surpassing the capabilities of current state-of-the-art generative models for medical image synthesis in computational resource-constrained environments. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒæ–¹é¢çš„è¿›æ­¥å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶åº”ç”¨å—åˆ°æ•°æ®æœ‰é™ä»¥åŠåŒ»ç–—ä¸­å¿ƒå› æ‚£è€…éšç§æ‹…å¿§è€Œä¸æ„¿å…±äº«æ•°æ®çš„åˆ¶çº¦ã€‚ç”Ÿæˆæ¨¡å‹é€šè¿‡åˆ›å»ºåˆæˆæ•°æ®ä½œä¸ºçœŸå®æ‚£è€…æ•°æ®çš„æ›¿ä»£å“ï¼Œå‘ˆç°äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒåŒ»å­¦å›¾åƒé€šå¸¸æ˜¯é«˜ç»´çš„ï¼Œå½“å‰å…ˆè¿›çš„æŠ€æœ¯æ–¹æ³•åœ¨è®¡ç®—èµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒä¸­é€šå¸¸ä¸åˆ‡å®é™…ã€‚è¿™äº›æ¨¡å‹ä¾èµ–äºæ•°æ®å­é‡‡æ ·ï¼Œäººä»¬å¯¹å®ƒä»¬çš„å¯è¡Œæ€§å’Œç°å®åº”ç”¨å¯è¡Œæ€§æå‡ºè´¨ç–‘ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹çš„è¯„ä¼°å¾€å¾€ä¾èµ–äºå®šé‡æŒ‡æ ‡ï¼Œè¿™å•ç‹¬è¯„ä¼°ç”Ÿæˆçš„å›¾åƒçš„è´¨é‡å’Œä¸´åºŠæ„ä¹‰å¯èƒ½æ˜¯è¯¯å¯¼çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedLoRDï¼Œè¿™æ˜¯ä¸€ç§ä¸ºè®¡ç®—èµ„æºå—é™ç¯å¢ƒè®¾è®¡çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ã€‚MedLoRDèƒ½å¤Ÿç”Ÿæˆé«˜ç»´åŒ»å­¦ä½“ç§¯æ•°æ®ï¼Œåˆ†è¾¨ç‡é«˜è¾¾512Ã—512Ã—256ï¼Œä»…åˆ©ç”¨24GB VRAMçš„GPUï¼Œè¿™åœ¨æ ‡å‡†å°å¼å·¥ä½œç«™ä¸­å¾ˆå¸¸è§ã€‚MedLoRDåœ¨å¤šæ¨¡æ€è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å† çŠ¶åŠ¨è„‰è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½±å’Œè‚ºéƒ¨è®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®é›†ã€‚é€šè¿‡æ”¾å°„å­¦è¯„ä¼°ã€ç›¸å¯¹åŒºåŸŸä½“ç§¯åˆ†æã€éµå¾ªæ¡ä»¶æ©è†œå’Œä¸‹æ¸¸ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMedLoRDç”Ÿæˆçš„å›¾åƒé«˜åº¦ä¿çœŸï¼Œç´§å¯†éµå¾ªåˆ†å‰²æ©è†œæ¡ä»¶ï¼Œè¶…è¶Šäº†å½“å‰å…ˆè¿›ç”Ÿæˆæ¨¡å‹åœ¨è®¡ç®—èµ„æºå—é™ç¯å¢ƒä¸­åŒ»å­¦å›¾åƒåˆæˆçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13211v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºæ‰©æ•£æ¨¡å‹çš„MedLoRDæ–¹æ³•ä¸ºè§£å†³åŒ»ç–—æˆåƒä¸­æ•°æ®å—é™åŠéšç§ä¿æŠ¤é—®é¢˜æä¾›äº†æ–°çš„å¸Œæœ›ã€‚è¯¥æ–¹æ³•èƒ½åœ¨è®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒä¸‹ç”Ÿæˆé«˜è´¨é‡åŒ»å­¦å›¾åƒï¼Œåˆ†è¾¨ç‡é«˜è¾¾512Ã—512Ã—256ï¼Œåªéœ€åˆ©ç”¨å¸¸è§çš„æ¡Œé¢å·¥ä½œç«™é…å¤‡çš„GPUï¼ˆ24GB VRAMï¼‰ã€‚ç»è¿‡å¤šæ¨¡æ€è¯„ä¼°ï¼ŒåŒ…æ‹¬å† çŠ¶åŠ¨è„‰è®¡ç®—æœºæ–­å±‚æ‰«æå’Œè‚ºéƒ¨è®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®é›†ï¼Œè¯æ˜MedLoRDç”Ÿæˆçš„é«˜ä¿çœŸå›¾åƒç´§å¯†è´´åˆåˆ†å‰²æ©è†œæ¡ä»¶ï¼Œè¶…è¶Šå½“å‰åŒç±»æ¨¡å‹çš„èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»ç–—æˆåƒä¸­AIçš„è¿›å±•å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æ•°æ®å¯ç”¨æ€§å’Œéšç§ä¿æŠ¤é™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹é€šè¿‡åˆ›å»ºåˆæˆæ•°æ®ä½œä¸ºçœŸå®æ‚£è€…æ•°æ®çš„æ›¿ä»£å“æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>åŒ»ç–—å›¾åƒæ˜¯é«˜ç»´åº¦çš„ï¼Œå½“å‰å…ˆè¿›çš„æ–¹æ³•å¯¹äºè®¡ç®—èµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒæ¥è¯´é€šå¸¸ä¸åˆ‡å®é™…ã€‚</li>
<li>MedLoRDä½œä¸ºä¸€ç§ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—èµ„æºå—é™ç¯å¢ƒä¸‹çš„åŒ»ç–—å›¾åƒç”Ÿæˆé—®é¢˜ã€‚</li>
<li>MedLoRDèƒ½å¤Ÿåœ¨æ ‡å‡†æ¡Œé¢å·¥ä½œç«™å¸¸è§çš„GPUï¼ˆ24GB VRAMï¼‰ä¸Šç”Ÿæˆé«˜è´¨é‡åŒ»å­¦å›¾åƒï¼Œåˆ†è¾¨ç‡é«˜è¾¾512Ã—512Ã—256ã€‚</li>
<li>MedLoRDåœ¨å¤šæ¨¡æ€è¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼ŒåŒ…æ‹¬å† çŠ¶åŠ¨è„‰å’Œè‚ºéƒ¨è®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¤šé¡¹è¯„ä¼°è¯æ˜ï¼ŒMedLoRDç”Ÿæˆçš„å›¾åƒè´¨é‡é«˜ã€ä¸´åºŠæ„ä¹‰é‡å¤§ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3c5ee40173c9254d6c2b96c25771d03a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec95067df4881e71bd373acd2e7a6026.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ed47a2e0408d4fd26053f17a056c5c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4420b6224fcf8d3524836cf76d464ffa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-989bab139bb7639dd0dab6dc5333460a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Patient-specific-radiomic-feature-selection-with-reconstructed-healthy-persona-of-knee-MR-images"><a href="#Patient-specific-radiomic-feature-selection-with-reconstructed-healthy-persona-of-knee-MR-images" class="headerlink" title="Patient-specific radiomic feature selection with reconstructed healthy   persona of knee MR images"></a>Patient-specific radiomic feature selection with reconstructed healthy   persona of knee MR images</h2><p><strong>Authors:Yaxi Chen, Simin Ni, Aleksandra Ivanova, Shaheer U. Saeed, Rikin Hargunani, Jie Huang, Chaozong Liu, Yipeng Hu</strong></p>
<p>Classical radiomic features have been designed to describe image appearance and intensity patterns. These features are directly interpretable and readily understood by radiologists. Compared with end-to-end deep learning (DL) models, lower dimensional parametric models that use such radiomic features offer enhanced interpretability but lower comparative performance in clinical tasks. In this study, we propose an approach where a standard logistic regression model performance is substantially improved by learning to select radiomic features for individual patients, from a pool of candidate features. This approach has potentials to maintain the interpretability of such approaches while offering comparable performance to DL. We also propose to expand the feature pool by generating a patient-specific healthy persona via mask-inpainting using a denoising diffusion model trained on healthy subjects. Such a pathology-free baseline feature set allows further opportunity in novel feature discovery and improved condition classification. We demonstrate our method on multiple clinical tasks of classifying general abnormalities, anterior cruciate ligament tears, and meniscus tears. Experimental results demonstrate that our approach achieved comparable or even superior performance than state-of-the-art DL approaches while offering added interpretability by using radiomic features extracted from images and supplemented by generating healthy personas. Example clinical cases are discussed in-depth to demonstrate the intepretability-enabled utilities such as human-explainable feature discovery and patient-specific location&#x2F;view selection. These findings highlight the potentials of the combination of subject-specific feature selection with generative models in augmenting radiomic analysis for more interpretable decision-making. The codes are available at: <a target="_blank" rel="noopener" href="https://github.com/YaxiiC/RadiomicsPersona.git">https://github.com/YaxiiC/RadiomicsPersona.git</a> </p>
<blockquote>
<p>ç»å…¸æ”¾å°„å­¦ç‰¹å¾è¢«è®¾è®¡ç”¨äºæè¿°å›¾åƒçš„å¤–è§‚å’Œå¼ºåº¦æ¨¡å¼ã€‚è¿™äº›ç‰¹å¾æ˜¯ç›´æ¥å¯è§£é‡Šçš„ï¼Œå¾ˆå®¹æ˜“è¢«æ”¾å°„ç§‘åŒ»ç”Ÿç†è§£ã€‚ä¸ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨è¿™äº›æ”¾å°„å­¦ç‰¹å¾çš„ä½ç»´å‚æ•°æ¨¡å‹æä¾›å¢å¼ºçš„å¯è§£é‡Šæ€§ï¼Œä½†åœ¨ä¸´åºŠä»»åŠ¡ä¸­çš„æ¯”è¾ƒæ€§èƒ½è¾ƒä½ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå³é€šè¿‡ä»å€™é€‰ç‰¹å¾æ± ä¸­å­¦ä¹ é€‰æ‹©é’ˆå¯¹ä¸ªåˆ«æ‚£è€…çš„æ”¾å°„å­¦ç‰¹å¾æ¥æ˜¾è‘—æ”¹è¿›æ ‡å‡†é€»è¾‘å›å½’æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•æœ‰æ½œåŠ›ä¿æŒæ­¤ç±»æ–¹æ³•çš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶æä¾›ä¸æ·±åº¦å­¦ä¹ ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æè®®é€šè¿‡åˆ©ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å¯¹å¥åº·å—è¯•è€…è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æ©è†œä¿®å¤æ³•ç”Ÿæˆæ‚£è€…ç‰¹å®šçš„å¥åº·äººæ ¼æ¥æ‰©å±•ç‰¹å¾æ± ã€‚è¿™æ ·çš„æ— ç—…å˜åŸºå‡†ç‰¹å¾é›†ä¸ºè¿›ä¸€æ­¥å‘ç°æ–°ç‰¹å¾å’Œæ”¹è¿›çŠ¶å†µåˆ†ç±»æä¾›äº†æ›´å¤šæœºä¼šã€‚æˆ‘ä»¬åœ¨åˆ†ç±»ä¸€èˆ¬å¼‚å¸¸ã€å‰äº¤å‰éŸ§å¸¦æ’•è£‚å’ŒåŠæœˆæ¿æ’•è£‚ç­‰å¤šä¸ªä¸´åºŠä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€æ–°æ·±åº¦å­¦ä¹ æŠ€æœ¯ç›¸æ¯”å–å¾—äº†ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡æå–å›¾åƒä¸­çš„æ”¾å°„å­¦ç‰¹å¾å’Œè¡¥å……ç”Ÿæˆå¥åº·äººæ ¼ï¼Œå¢åŠ äº†å¯è§£é‡Šæ€§ã€‚æ·±å…¥è®¨è®ºäº†ç¤ºä¾‹ç—…ä¾‹ï¼Œä»¥å±•ç¤ºå¯è§£é‡Šæ€§åŠŸèƒ½çš„å®ç”¨æ€§ï¼Œå¦‚äººç±»å¯è§£é‡Šçš„ç‰¹å¾å‘ç°ã€æ‚£è€…ç‰¹å®šä½ç½®&#x2F;è§†å›¾é€‰æ‹©ç­‰ã€‚è¿™äº›å‘ç°çªå‡ºäº†ç»“åˆä¸»ä½“ç‰¹å®šç‰¹å¾é€‰æ‹©å’Œç”Ÿæˆæ¨¡å‹åœ¨å¢å¼ºæ”¾å°„å­¦åˆ†æä¸­çš„æ½œåŠ›ï¼Œä»¥åšå‡ºæ›´å…·å¯è§£é‡Šæ€§çš„å†³ç­–ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/YaxiiC/RadiomicsPersona.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YaxiiC/RadiomicsPersona.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13131v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé€»è¾‘å›å½’æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ‚£è€…ä¸ªä½“åŒ–æ”¾å°„å­¦ç‰¹å¾æ¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ï¼Œç‰¹å¾æ± å¾—åˆ°æ‰©å±•ï¼Œé€šè¿‡åˆ©ç”¨å¥åº·å—è¯•è€…çš„å»å™ªæ‰©æ•£æ¨¡å‹è¿›è¡Œé®ç½©å¡«å……æ¥ç”Ÿæˆæ‚£è€…ç‰¹å®šçš„å¥åº·äººæ ¼ã€‚è¯¥æ–¹æ³•åœ¨ç»´æŒæ¨¡å‹å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼Œå¹¶åœ¨åˆ†ç±»ä¸€èˆ¬å¼‚å¸¸ã€å‰äº¤å‰éŸ§å¸¦æ’•è£‚å’ŒåŠæœˆæ¿æ’•è£‚ç­‰å¤šä¸ªä¸´åºŠä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»å…¸æ”¾å°„å­¦ç‰¹å¾ç”¨äºæè¿°å›¾åƒå¤–è§‚å’Œå¼ºåº¦æ¨¡å¼ï¼Œå¯ç›´æ¥è¢«æ”¾å°„ç§‘åŒ»ç”Ÿè§£è¯»ã€‚</li>
<li>ä¸ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼Œä½ç»´å‚æ•°æ¨¡å‹è™½ç„¶æ€§èƒ½è¾ƒä½ï¼Œä½†å…·æœ‰æ›´é«˜çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ”¹è¿›é€»è¾‘å›å½’æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©ä¸ªä½“åŒ–çš„æ”¾å°„å­¦ç‰¹å¾æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨å¥åº·å—è¯•è€…çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ‚£è€…ç‰¹å®šçš„å¥åº·äººæ ¼æ¥æ‰©å±•ç‰¹å¾æ± ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç»´æŒæ¨¡å‹å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªä¸´åºŠä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬åˆ†ç±»ä¸€èˆ¬å¼‚å¸¸ã€å‰äº¤å‰éŸ§å¸¦æ’•è£‚å’ŒåŠæœˆæ¿æ’•è£‚ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ecec381e49048bf6c82be5964c27ddb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39cf059fd78a805a9eaab41eb6d42d82.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TFDM-Time-Variant-Frequency-Based-Point-Cloud-Diffusion-with-Mamba"><a href="#TFDM-Time-Variant-Frequency-Based-Point-Cloud-Diffusion-with-Mamba" class="headerlink" title="TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba"></a>TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba</h2><p><strong>Authors:Jiaxu Liu, Li Li, Hubert P. H. Shum, Toby P. Breckon</strong></p>
<p>Diffusion models currently demonstrate impressive performance over various generative tasks. Recent work on image diffusion highlights the strong capabilities of Mamba (state space models) due to its efficient handling of long-range dependencies and sequential data modeling. Unfortunately, joint consideration of state space models with 3D point cloud generation remains limited. To harness the powerful capabilities of the Mamba model for 3D point cloud generation, we propose a novel diffusion framework containing dual latent Mamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The DM-Block apply a space-filling curve to reorder points into sequences suitable for Mamba state-space modeling, while operating in a latent space to mitigate the computational overhead that arises from direct 3D data processing. Meanwhile, the TF-Encoder takes advantage of the ability of the diffusion model to refine fine details in later recovery stages by prioritizing key points within the U-Net architecture. This frequency-based mechanism ensures enhanced detail quality in the final stages of generation. Experimental results on the ShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art performance (ShapeNet-v2: 0.14% on 1-NNA-Abs50 EMD and 57.90% on COV EMD) on certain metrics for specific categories while reducing computational parameters and inference time by up to 10$\times$ and 9$\times$, respectively. Source code is available in Supplementary Materials and will be released upon accpetance. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ç›®å‰åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚å…³äºå›¾åƒæ‰©æ•£çš„æœ€æ–°å·¥ä½œçªå‡ºäº†Mambaï¼ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰çš„å¼ºå¤§èƒ½åŠ›ï¼Œå¾—ç›Šäºå…¶é«˜æ•ˆå¤„ç†é•¿ç¨‹ä¾èµ–å…³ç³»å’Œåºåˆ—æ•°æ®å»ºæ¨¡ã€‚ç„¶è€Œï¼Œå°†çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸3Dç‚¹äº‘ç”Ÿæˆç›¸ç»“åˆçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚ä¸ºäº†åˆ©ç”¨Mambaæ¨¡å‹åœ¨3Dç‚¹äº‘ç”Ÿæˆæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¡†æ¶ï¼ŒåŒ…å«åŒæ½œä¼Mambaå—ï¼ˆDM-Blockï¼‰å’Œæ—¶é—´å˜é‡é¢‘ç‡ç¼–ç å™¨ï¼ˆTF-Encoderï¼‰ã€‚DM-Blockåº”ç”¨ç©ºé—´å¡«å……æ›²çº¿å¯¹ç‚¹è¿›è¡Œæ’åºï¼Œç”Ÿæˆé€‚åˆMambaçŠ¶æ€ç©ºé—´å»ºæ¨¡çš„åºåˆ—ï¼ŒåŒæ—¶åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œä»¥å‡è½»ç›´æ¥å¤„ç†3Dæ•°æ®æ‰€äº§ç”Ÿçš„è®¡ç®—å¼€é”€ã€‚åŒæ—¶ï¼ŒTF-Encoderåˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨åæœŸæ¢å¤é˜¶æ®µç»†åŒ–ç»†èŠ‚çš„èƒ½åŠ›ï¼Œåœ¨U-Netæ¶æ„å†…ä¼˜å…ˆå¤„ç†å…³é”®ç‚¹ã€‚è¿™ç§åŸºäºé¢‘ç‡çš„æœºåˆ¶ç¡®ä¿äº†ç”Ÿæˆæœ€ç»ˆé˜¶æ®µçš„ç»†èŠ‚è´¨é‡å¾—åˆ°æé«˜ã€‚åœ¨ShapeNet-v2æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰¹å®šç±»åˆ«çš„æŸäº›æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ˆShapeNet-v2ï¼šåœ¨1-NNA-Abs50 EMDä¸Šä¸º0.14%ï¼Œåœ¨COV EMDä¸Šä¸º57.90%ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—å‚æ•°å’Œæ¨ç†æ—¶é—´ï¼Œåˆ†åˆ«é«˜è¾¾10å€å’Œ9å€ã€‚æºä»£ç åŒ…å«åœ¨è¡¥å……ææ–™ä¸­ï¼Œæ¥å—åå°†äºˆä»¥å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13004v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ‰©æ•£æ–¹é¢çš„èƒ½åŠ›ã€‚é’ˆå¯¹ä¸‰ç»´ç‚¹äº‘ç”Ÿæˆï¼Œæå‡ºä¸€ç§æ–°å‹æ‰©æ•£æ¡†æ¶ï¼ŒåŒ…å«åŒæ½œæ€Mambaå—ï¼ˆDM-Blockï¼‰å’Œæ—¶å˜é¢‘ç‡ç¼–ç å™¨ï¼ˆTF-Encoderï¼‰ã€‚DM-Blockåˆ©ç”¨ç©ºé—´å¡«å……æ›²çº¿å°†ç‚¹é‡æ–°æ’åºï¼Œé€‚åˆMambaçŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´æ“ä½œä»¥å‡è½»ç›´æ¥å¤„ç†3Dæ•°æ®çš„è®¡ç®—è´Ÿæ‹…ã€‚TF-Encoderåˆ™åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨åæœŸæ¢å¤é˜¶æ®µç»†åŒ–ç»†èŠ‚çš„èƒ½åŠ›ï¼Œä¼˜å…ˆå¤„ç†U-Netæ¶æ„ä¸­çš„å…³é”®ç‚¹ã€‚åœ¨ShapeNet-v2æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç‰¹å®šç±»åˆ«æŒ‡æ ‡çš„æŸäº›åº¦é‡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å‚æ•°å’Œæ¨ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒæ‰©æ•£æ–¹é¢ã€‚</li>
<li>é’ˆå¯¹ä¸‰ç»´ç‚¹äº‘ç”Ÿæˆï¼Œæå‡ºäº†åŒ…å«DM-Blockå’ŒTF-Encoderçš„æ–°å‹æ‰©æ•£æ¡†æ¶ã€‚</li>
<li>DM-Blockåˆ©ç”¨ç©ºé—´å¡«å……æ›²çº¿å¤„ç†ç‚¹äº‘æ•°æ®ï¼Œé€‚åˆMambaçŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´å‡è½»è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>TF-Encoderåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç»†èŠ‚ä¼˜åŒ–èƒ½åŠ›ï¼Œä¼˜å…ˆå¤„ç†å…³é”®ç‚¹ã€‚</li>
<li>åœ¨ShapeNet-v2æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è®¡ç®—å‚æ•°å’Œæ¨ç†æ—¶é—´ä¸Šæœ‰æ‰€ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec14a412d2cc99ca1a2cce0ec83a3895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-700275f5d1f72f50e1afaa8daac88ed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec449e66eb462848fb054a80955e497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4065d3ea0ca72a7485fb7d35b7613741.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-438059f357edbb9852fb9ec2e371ad43.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unlock-Pose-Diversity-Accurate-and-Efficient-Implicit-Keypoint-based-Spatiotemporal-Diffusion-for-Audio-driven-Talking-Portrait"><a href="#Unlock-Pose-Diversity-Accurate-and-Efficient-Implicit-Keypoint-based-Spatiotemporal-Diffusion-for-Audio-driven-Talking-Portrait" class="headerlink" title="Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based   Spatiotemporal Diffusion for Audio-driven Talking Portrait"></a>Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based   Spatiotemporal Diffusion for Audio-driven Talking Portrait</h2><p><strong>Authors:Chaolong Yang, Kai Yao, Yuyao Yan, Chenru Jiang, Weiguang Zhao, Jie Sun, Guangliang Cheng, Yifei Zhang, Bin Dong, Kaizhu Huang</strong></p>
<p>Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/chaolongy/KDTalker">https://github.com/chaolongy/KDTalker</a>. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„å•å›¾åƒè¯´è¯è‚–åƒç”Ÿæˆåœ¨è™šæ‹Ÿç°å®ã€æ•°å­—äººç±»åˆ›å»ºå’Œç”µå½±åˆ¶ä½œä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åˆ†ä¸ºåŸºäºå…³é”®ç‚¹çš„æ–¹æ³•å’ŒåŸºäºå›¾åƒçš„æ–¹æ³•ã€‚åŸºäºå…³é”®ç‚¹çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿ç•™äººç‰©èº«ä»½ï¼Œä½†ç”±äº3Då¯å˜å½¢æ¨¡å‹çš„å›ºå®šç‚¹é™åˆ¶ï¼Œå¾ˆéš¾æ•æ‰é¢éƒ¨ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿç”Ÿæˆç½‘ç»œåœ¨æœ‰é™æ•°æ®é›†ä¸Šå»ºç«‹éŸ³é¢‘å’Œå…³é”®ç‚¹ä¹‹é—´çš„å› æœå…³ç³»é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´å§¿åŠ¿å¤šæ ·æ€§è¾ƒä½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºå›¾åƒçš„æ–¹æ³•ä½¿ç”¨æ‰©æ•£ç½‘ç»œç”Ÿæˆå…·æœ‰å„ç§ç»†èŠ‚çš„é«˜è´¨é‡è‚–åƒï¼Œä½†ä¼šäº§ç”Ÿèº«ä»½å¤±çœŸå’Œæ˜‚è´µçš„è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†KDTalkerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆæ— ç›‘ç£éšå¼3Då…³é”®ç‚¹ä¸æ—¶ç©ºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ã€‚åˆ©ç”¨æ— ç›‘ç£éšå¼3Då…³é”®ç‚¹ï¼ŒKDTalkeré€‚åº”é¢éƒ¨ä¿¡æ¯å¯†åº¦ï¼Œä½¿æ‰©æ•£è¿‡ç¨‹èƒ½å¤Ÿçµæ´»åœ°æ¨¡æ‹Ÿå„ç§å¤´éƒ¨å§¿åŠ¿å¹¶æ•æ‰é¢éƒ¨ç»†èŠ‚ã€‚å®šåˆ¶çš„æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†å‡†ç¡®çš„å”‡éƒ¨åŒæ­¥ï¼Œäº§ç”Ÿæ—¶é—´ä¸Šè¿è´¯ã€é«˜è´¨é‡åŠ¨ç”»ï¼ŒåŒæ—¶æé«˜è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKDTalkeråœ¨å”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§ã€å¤´éƒ¨å§¿åŠ¿å¤šæ ·æ€§å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chaolongy/KDTalker%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chaolongy/KDTalkeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12963v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKDTalkerçš„éŸ³é¢‘é©±åŠ¨çš„å•å›¾åƒè¯´è¯è‚–åƒç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ— ç›‘ç£éšå¼3Då…³é”®ç‚¹åŠæ—¶ç©ºæ‰©æ•£æ¨¡å‹ã€‚å®ƒé‡‡ç”¨éšå¼å…³é”®ç‚¹ï¼Œæé«˜é¢éƒ¨ä¿¡æ¯å¯†åº¦ï¼Œä½¿æ‰©æ•£è¿‡ç¨‹èƒ½å¤Ÿçµæ´»æ¨¡æ‹Ÿå„ç§å¤´éƒ¨å§¿æ€å’Œæ•æ‰é¢éƒ¨ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå®šåˆ¶çš„æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†å‡†ç¡®çš„å”‡åŒæ­¥ï¼Œç”Ÿæˆäº†æ—¶é—´è¿è´¯çš„é«˜è´¨é‡åŠ¨ç”»ï¼ŒåŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚KDTalkeråœ¨å”‡åŒæ­¥å‡†ç¡®æ€§ã€å¤´éƒ¨å§¿æ€å¤šæ ·æ€§å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KDTalkeræ˜¯é¦–ä¸ªç»“åˆæ— ç›‘ç£éšå¼3Då…³é”®ç‚¹å’Œæ—¶ç©ºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºéŸ³é¢‘é©±åŠ¨çš„å•å›¾åƒè¯´è¯è‚–åƒç”Ÿæˆã€‚</li>
<li>éšå¼å…³é”®ç‚¹æŠ€æœ¯æé«˜äº†é¢éƒ¨ä¿¡æ¯å¯†åº¦ï¼Œä½¿æ‰©æ•£è¿‡ç¨‹èƒ½å¤Ÿçµæ´»æ¨¡æ‹Ÿå„ç§å¤´éƒ¨å§¿æ€å’Œæ•æ‰é¢éƒ¨ç»†èŠ‚ã€‚</li>
<li>å®šåˆ¶çš„æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†å‡†ç¡®çš„å”‡åŒæ­¥ï¼Œå¢å¼ºäº†åŠ¨ç”»çš„çœŸå®æ„Ÿã€‚</li>
<li>KDTalkerç”Ÿæˆäº†æ—¶é—´è¿è´¯çš„é«˜è´¨é‡åŠ¨ç”»ï¼ŒåŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§ã€å¤´éƒ¨å§¿æ€å¤šæ ·æ€§å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</li>
<li>KDTalkeræ¡†æ¶å¯ç”¨äºè™šæ‹Ÿç°å®ã€æ•°å­—äººç‰©åˆ›å»ºå’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a889d7e910f61352d3fa643e52b58e4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5203dc0be2d3da64f568e4872272ea2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49aaa97d719087b1bfb1a2ae82312e86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9262304a8a2a125837231f0f70d031f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VasTSD-Learning-3D-Vascular-Tree-state-Space-Diffusion-Model-for-Angiography-Synthesis"><a href="#VasTSD-Learning-3D-Vascular-Tree-state-Space-Diffusion-Model-for-Angiography-Synthesis" class="headerlink" title="VasTSD: Learning 3D Vascular Tree-state Space Diffusion Model for   Angiography Synthesis"></a>VasTSD: Learning 3D Vascular Tree-state Space Diffusion Model for   Angiography Synthesis</h2><p><strong>Authors:Zhifeng Wang, Renjiao Yi, Xin Wen, Chenyang Zhu, Kai Xu</strong></p>
<p>Angiography imaging is a medical imaging technique that enhances the visibility of blood vessels within the body by using contrast agents. Angiographic images can effectively assist in the diagnosis of vascular diseases. However, contrast agents may bring extra radiation exposure which is harmful to patients with health risks. To mitigate these concerns, in this paper, we aim to automatically generate angiography from non-angiographic inputs, by leveraging and enhancing the inherent physical properties of vascular structures. Previous methods relying on 2D slice-based angiography synthesis struggle with maintaining continuity in 3D vascular structures and exhibit limited effectiveness across different imaging modalities. We propose VasTSD, a 3D vascular tree-state space diffusion model to synthesize angiography from 3D non-angiographic volumes, with a novel state space serialization approach that dynamically constructs vascular tree topologies, integrating these with a diffusion-based generative model to ensure the generation of anatomically continuous vasculature in 3D volumes. A pre-trained vision embedder is employed to construct vascular state space representations, enabling consistent modeling of vascular structures across multiple modalities. Extensive experiments on various angiographic datasets demonstrate the superiority of VasTSD over prior works, achieving enhanced continuity of blood vessels in synthesized angiographic synthesis for multiple modalities and anatomical regions. </p>
<blockquote>
<p>è¡€ç®¡é€ å½±æˆåƒæ˜¯ä¸€ç§åˆ©ç”¨é€ å½±å‰‚æé«˜ä½“å†…è¡€ç®¡å¯è§åº¦çš„åŒ»å­¦æˆåƒæŠ€æœ¯ã€‚è¡€ç®¡é€ å½±å›¾åƒå¯ä»¥æœ‰æ•ˆåœ°è¾…åŠ©è¡€ç®¡ç–¾ç—…çš„è¯Šæ–­ã€‚ç„¶è€Œï¼Œé€ å½±å‰‚å¯èƒ½ä¼šå¸¦æ¥é¢å¤–çš„è¾å°„æš´éœ²ï¼Œå¯¹å…·æœ‰å¥åº·é£é™©çš„æ‚£è€…æœ‰å®³ã€‚ä¸ºäº†ç¼“è§£è¿™äº›æ‹…å¿§ï¼Œæœ¬æ–‡æ—¨åœ¨åˆ©ç”¨å¹¶å¢å¼ºè¡€ç®¡ç»“æ„çš„å›ºæœ‰ç‰©ç†ç‰¹æ€§ï¼Œä»éè¡€ç®¡é€ å½±è¾“å…¥ä¸­è‡ªåŠ¨ç”Ÿæˆè¡€ç®¡é€ å½±ã€‚ä»¥å¾€ä¾èµ–äº2Dåˆ‡ç‰‡çš„è¡€ç®¡é€ å½±åˆæˆæ–¹æ³•åœ¨ç»´æŒ3Dè¡€ç®¡ç»“æ„è¿ç»­æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”åœ¨ä¸åŒæˆåƒæ¨¡å¼ä¹‹é—´çš„æœ‰æ•ˆæ€§æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†VasTSDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä»3Déè¡€ç®¡é€ å½±ä½“ç§¯åˆæˆè¡€ç®¡é€ å½±çš„3Dè¡€ç®¡æ ‘çŠ¶æ€ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„çŠ¶æ€ç©ºé—´åºåˆ—åŒ–æ–¹æ³•ï¼ŒåŠ¨æ€æ„å»ºè¡€ç®¡æ ‘æ‹“æ‰‘ç»“æ„ï¼Œå¹¶å°†å…¶ä¸åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œç¡®ä¿åœ¨3Dä½“ç§¯ä¸­ç”Ÿæˆè§£å‰–è¿ç»­çš„è¡€ç®¡ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰åµŒå…¥å™¨æ¥æ„å»ºè¡€ç®¡çŠ¶æ€ç©ºé—´è¡¨ç¤ºï¼Œå®ç°åœ¨å¤šä¸ªæ¨¡æ€ä¸‹å¯¹è¡€ç®¡ç»“æ„çš„ä¸€è‡´å»ºæ¨¡ã€‚åœ¨å„ç§è¡€ç®¡é€ å½±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVasTSDä¼˜äºå…ˆå‰çš„å·¥ä½œï¼Œåœ¨å¤šä¸ªæ¨¡æ€å’Œè§£å‰–åŒºåŸŸçš„åˆæˆè¡€ç®¡é€ å½±ä¸­å®ç°äº†å¢å¼ºçš„è¡€ç®¡è¿ç»­æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12758v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¡€ç®¡é€ å½±æˆåƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨é€ å½±å‰‚å¢å¼ºä½“å†…è¡€ç®¡çš„å¯è§†æ€§ã€‚ç„¶è€Œï¼Œé€ å½±å‰‚å¯èƒ½å¸¦æ¥é¢å¤–çš„è¾å°„æš´éœ²ï¼Œå¯¹æ‚£è€…å¥åº·æ„æˆé£é™©ã€‚ä¸ºå‡è½»è¿™ä¸€æ‹…å¿§ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ©ç”¨å’Œæå‡è¡€ç®¡ç»“æ„çš„å›ºæœ‰ç‰©ç†ç‰¹æ€§ï¼Œä»éè¡€ç®¡é€ å½±è¾“å…¥ä¸­è‡ªåŠ¨ç”Ÿæˆè¡€ç®¡é€ å½±ã€‚é’ˆå¯¹ä»¥å¾€æ–¹æ³•åœ¨ç»´æŒä¸‰ç»´è¡€ç®¡ç»“æ„è¿ç»­æ€§æ–¹é¢çš„ä¸è¶³ï¼Œä»¥åŠåœ¨ä¸åŒæˆåƒæ¨¡å¼ä¹‹é—´æ•ˆæœçš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVasTSDçš„ä¸‰ç»´è¡€ç®¡æ ‘æ€ç©ºé—´æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸€ç§æ–°çš„æ€ç©ºé—´åºåˆ—åŒ–æ–¹æ³•åŠ¨æ€æ„å»ºè¡€ç®¡æ ‘æ‹“æ‰‘ç»“æ„ï¼Œå¹¶ä¸åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œç¡®ä¿åœ¨ä¸‰ç»´ä½“ç§¯ä¸­ç”Ÿæˆè§£å‰–è¿ç»­çš„è¡€ç®¡ã€‚é‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰åµŒå…¥å™¨æ„å»ºè¡€ç®¡çŠ¶æ€ç©ºé—´è¡¨ç¤ºï¼Œå®ç°äº†è·¨å¤šæ¨¡æ€çš„è¡€ç®¡ç»“æ„ä¸€è‡´å»ºæ¨¡ã€‚åœ¨å¤šä¸ªè¡€ç®¡é€ å½±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVasTSDç›¸è¾ƒäºä»¥å‰çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œåˆæˆçš„è¡€ç®¡é€ å½±ä¸­è¡€ç®¡è¿ç»­æ€§å¢å¼ºï¼Œé€‚ç”¨äºå¤šç§æ¨¡æ€å’Œè§£å‰–åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡€ç®¡é€ å½±æˆåƒæŠ€æœ¯åˆ©ç”¨é€ å½±å‰‚å¢å¼ºè¡€ç®¡å¯è§†åŒ–ï¼Œæœ‰åŠ©äºè¯Šæ–­è¡€ç®¡ç–¾ç—…ï¼Œä½†é€ å½±å‰‚å¯èƒ½å¸¦æ¥é¢å¤–çš„è¾å°„é£é™©ã€‚</li>
<li>ä»¥å¾€æ–¹æ³•ä¸»è¦ä¾èµ–äºŒç»´åˆ‡ç‰‡åŸºç¡€çš„è¡€ç®¡é€ å½±åˆæˆï¼Œéš¾ä»¥ç»´æŒä¸‰ç»´è¡€ç®¡ç»“æ„çš„è¿ç»­æ€§ï¼Œä¸”åœ¨å¤šæ¨¡æ€æˆåƒä¹‹é—´æ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºäº†VasTSDæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸‰ç»´è¡€ç®¡æ ‘æ€ç©ºé—´æ‰©æ•£çš„æ¨¡å‹ï¼Œç”¨äºä»ä¸‰ç»´éè¡€ç®¡é€ å½±ä½“ç§¯ä¸­åˆæˆè¡€ç®¡é€ å½±ã€‚</li>
<li>VasTSDé‡‡ç”¨æ–°çš„æ€ç©ºé—´åºåˆ—åŒ–æ–¹æ³•æ„å»ºè¡€ç®¡æ ‘æ‹“æ‰‘ç»“æ„ï¼Œç¡®ä¿ç”Ÿæˆçš„è¡€ç®¡åœ¨ä¸‰ç»´ä½“ç§¯ä¸­è§£å‰–è¿ç»­ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆé¢„è®­ç»ƒçš„è§†è§‰åµŒå…¥å™¨ï¼Œå®ç°è·¨å¤šæ¨¡æ€çš„è¡€ç®¡ç»“æ„ä¸€è‡´å»ºæ¨¡ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVasTSDåœ¨åˆæˆçš„è¡€ç®¡é€ å½±ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„è¿ç»­æ€§ï¼Œé€‚ç”¨äºå¤šç§æˆåƒæ¨¡æ€å’Œè§£å‰–åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9458891a063037278995ba01bf0b9585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-727e6459c316ee55f0bb1bf3ed9339be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dad495abdb8aca8853deb156ddaeb066.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-733e5968523a4da1b20d5b7d8879065e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LATINO-PRO-LAtent-consisTency-INverse-sOlver-with-PRompt-Optimization"><a href="#LATINO-PRO-LAtent-consisTency-INverse-sOlver-with-PRompt-Optimization" class="headerlink" title="LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization"></a>LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization</h2><p><strong>Authors:Alessio Spagnoletti, Jean Prost, AndrÃ©s Almansa, Nicolas Papadakis, Marcelo Pereyra</strong></p>
<p>Text-to-image latent diffusion models (LDMs) have recently emerged as powerful generative models with great potential for solving inverse problems in imaging. However, leveraging such models in a Plug &amp; Play (PnP), zero-shot manner remains challenging because it requires identifying a suitable text prompt for the unknown image of interest. Also, existing text-to-image PnP approaches are highly computationally expensive. We herein address these challenges by proposing a novel PnP inference paradigm specifically designed for embedding generative models within stochastic inverse solvers, with special attention to Latent Consistency Models (LCMs), which distill LDMs into fast generators. We leverage our framework to propose LAtent consisTency INverse sOlver (LATINO), the first zero-shot PnP framework to solve inverse problems with priors encoded by LCMs. Our conditioning mechanism avoids automatic differentiation and reaches SOTA quality in as little as 8 neural function evaluations. As a result, LATINO delivers remarkably accurate solutions and is significantly more memory and computationally efficient than previous approaches. We then embed LATINO within an empirical Bayesian framework that automatically calibrates the text prompt from the observed measurements by marginal maximum likelihood estimation. Extensive experiments show that prompt self-calibration greatly improves estimation, allowing LATINO with PRompt Optimization to define new SOTAs in image reconstruction quality and computational efficiency. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æœ€è¿‘ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹å‡ºç°ï¼Œåœ¨è§£å†³æˆåƒä¸­çš„é€†é—®é¢˜ä¸Šå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»¥Plug &amp; Playï¼ˆPnPï¼‰å³æ’å³ç”¨ã€é›¶å°„å‡»çš„æ–¹å¼åˆ©ç”¨è¿™äº›æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™éœ€è¦ä¸ºæœªçŸ¥çš„æ„Ÿå…´è¶£å›¾åƒç¡®å®šåˆé€‚çš„æ–‡æœ¬æç¤ºã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒPnPæ–¹æ³•è®¡ç®—é‡éå¸¸å¤§ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ–°å‹PnPæ¨ç†èŒƒå¼æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥èŒƒå¼ä¸“é—¨è®¾è®¡ç”¨äºå°†ç”Ÿæˆæ¨¡å‹åµŒå…¥éšæœºé€†æ±‚è§£å™¨ä¸­ï¼Œç‰¹åˆ«å…³æ³¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰ï¼Œå®ƒå°†LDMæç‚¼ä¸ºå¿«é€Ÿç”Ÿæˆå™¨ã€‚æˆ‘ä»¬åˆ©ç”¨æˆ‘ä»¬çš„æ¡†æ¶æå‡ºäº†æ½œåœ¨ä¸€è‡´æ€§é€†æ±‚è§£å™¨ï¼ˆLATINOï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé›¶å°„å‡»PnPæ¡†æ¶ï¼Œä½¿ç”¨LCMsç¼–ç çš„å…ˆéªŒçŸ¥è¯†æ¥è§£å†³é€†é—®é¢˜ã€‚æˆ‘ä»¬çš„è°ƒèŠ‚æœºåˆ¶é¿å…äº†è‡ªåŠ¨å¾®åˆ†ï¼Œå¹¶åœ¨ä»…8ä¸ªç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å› æ­¤ï¼ŒLATINOæä¾›äº†éå¸¸å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨å†…å­˜å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚ç„¶åæˆ‘ä»¬å°†LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶ä¸­ï¼Œé€šè¿‡è¾¹é™…æœ€å¤§ä¼¼ç„¶ä¼°è®¡è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºæ¥è‡ªè§‚å¯Ÿåˆ°çš„æµ‹é‡å€¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæç¤ºè‡ªæˆ‘æ ¡å‡†æå¤§åœ°æé«˜äº†ä¼°è®¡å€¼ï¼Œä½¿å¸¦æœ‰æç¤ºä¼˜åŒ–çš„LATINOåœ¨å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å®šä¹‰äº†æ–°çš„æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12615v1">PDF</a> 27 pages, 20 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨è§£å†³æˆåƒä¸­çš„é€†é—®é¢˜ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»¥Plug &amp; Playï¼ˆPnPï¼‰æ–¹å¼è¿›è¡Œé›¶æ ·æœ¬åº”ç”¨ä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä¸ºæœªçŸ¥å›¾åƒé€‰æ‹©åˆé€‚çš„æ–‡æœ¬æç¤ºã€‚ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒPnPæ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºä¸€ç§ä¸ºåµŒå…¥ç”Ÿæˆæ¨¡å‹çš„éšæœºé€†æ±‚è§£å™¨è®¾è®¡çš„PnPæ¨ç†èŒƒå¼ï¼Œç‰¹åˆ«å…³æ³¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰ï¼Œå®ƒå°†LDMsç®€åŒ–ä¸ºå¿«é€Ÿç”Ÿæˆå™¨ã€‚æˆ‘ä»¬åˆ©ç”¨è¯¥æ¡†æ¶æå‡ºLAtento consisTency INverse sOlverï¼ˆLATINOï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé›¶æ ·æœ¬PnPæ¡†æ¶ï¼Œä½¿ç”¨LCMsç¼–ç çš„å…ˆéªŒçŸ¥è¯†è§£å†³é€†é—®é¢˜ã€‚å…¶è°ƒèŠ‚æœºåˆ¶é¿å…äº†è‡ªåŠ¨å¾®åˆ†ï¼Œåœ¨ä»…8ä¸ªç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ä¸­è¾¾åˆ°æœ€æ–°è´¨é‡ã€‚LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶ï¼Œé€šè¿‡è¾¹ç¼˜æœ€å¤§ä¼¼ç„¶ä¼°è®¡è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºï¼Œä»è€Œæé«˜ä¼°ç®—è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œå¸¦æœ‰æç¤ºä¼˜åŒ–çš„LATINOåœ¨å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å®šä¹‰æ–°æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMså…·æœ‰è§£å†³æˆåƒé€†é—®é¢˜çš„æ½œåŠ›ã€‚</li>
<li>PnPæ–¹å¼åº”ç”¨LDMså…·æœ‰æŒ‘æˆ˜æ€§å’Œé«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>æå‡ºä¸€ç§é’ˆå¯¹ç”Ÿæˆæ¨¡å‹çš„PnPæ¨ç†èŒƒå¼ï¼Œç‰¹åˆ«å…³æ³¨LCMsã€‚</li>
<li>å¼•å…¥LATINOæ¡†æ¶ï¼Œèƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹è§£å†³é€†é—®é¢˜å¹¶è¾¾åˆ°æœ€æ–°è´¨é‡ã€‚</li>
<li>LATINOè°ƒèŠ‚æœºåˆ¶é¿å…è‡ªåŠ¨å¾®åˆ†ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶ï¼Œå¯è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºï¼Œæé«˜ä¼°ç®—å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå¸¦æœ‰æç¤ºä¼˜åŒ–çš„LATINOåœ¨å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¾¾åˆ°æ–°çš„æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38d44774197a9a77f663fdd11a05723d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fae853abc8a89172ff4242d904ac6df0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fced14eb1950a2cd92b6692440d2f7d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35d41d2fc3f20d66ad6da36e6bd68353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f79a163360f80b92d0e60e9ac16f3532.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BalancedDPO-Adaptive-Multi-Metric-Alignment"><a href="#BalancedDPO-Adaptive-Multi-Metric-Alignment" class="headerlink" title="BalancedDPO: Adaptive Multi-Metric Alignment"></a>BalancedDPO: Adaptive Multi-Metric Alignment</h2><p><strong>Authors:Dipesh Tamboli, Souradip Chakraborty, Aditya Malusare, Biplab Banerjee, Amrit Singh Bedi, Vaneet Aggarwal</strong></p>
<p>Text-to-image (T2I) diffusion models have made remarkable advancements, yet aligning them with diverse preferences remains a persistent challenge. Current methods often optimize single metrics or depend on narrowly curated datasets, leading to overfitting and limited generalization across key visual quality metrics. We present BalancedDPO, a novel extension of Direct Preference Optimization (DPO) that addresses these limitations by simultaneously aligning T2I diffusion models with multiple metrics, including human preference, CLIP score, and aesthetic quality. Our key novelty lies in aggregating consensus labels from diverse metrics in the preference distribution space as compared to existing reward mixing approaches, enabling robust and scalable multi-metric alignment while maintaining the simplicity of the standard DPO pipeline that we refer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD datasets show that BalancedDPO achieves state-of-the-art results, outperforming existing approaches across all major metrics. BalancedDPO improves the average win rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD, respectively, from the DiffusionDPO. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å¦‚ä½•å°†å…¶ä¸å¤šç§åå¥½è¿›è¡ŒåŒ¹é…ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ä¼˜åŒ–å•ä¸€æŒ‡æ ‡æˆ–ä¾èµ–äºç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ï¼Œå¯¼è‡´åœ¨å…³é”®è§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šè¿‡æ‹Ÿåˆå’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†BalancedDPOï¼Œå®ƒæ˜¯Direct Preference Optimizationï¼ˆDPOï¼‰çš„æ–°æ‰©å±•ï¼Œé€šè¿‡åŒæ—¶è°ƒæ•´T2Iæ‰©æ•£æ¨¡å‹ä¸åŒ…æ‹¬äººç±»åå¥½ã€CLIPåˆ†æ•°å’Œç¾å­¦è´¨é‡åœ¨å†…çš„å¤šä¸ªæŒ‡æ ‡ï¼Œè§£å†³äº†è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºï¼Œä¸ç°æœ‰çš„å¥–åŠ±æ··åˆæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨åå¥½åˆ†å¸ƒç©ºé—´ä¸­èšåˆæ¥è‡ªä¸åŒæŒ‡æ ‡çš„å…±è¯†æ ‡ç­¾ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¿æŒæ ‡å‡†DPOç®¡é“ç®€å•æ€§çš„åŒæ—¶ï¼Œå®ç°ç¨³å¥å’Œå¯æ‰©å±•çš„å¤šæŒ‡æ ‡åŒ¹é…ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºBalancedDPOã€‚æˆ‘ä»¬åœ¨Pick-a-Picã€PartiPromptå’ŒHPDæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒBalancedDPOè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨æ‰€æœ‰ä¸»è¦æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚BalancedDPOåœ¨Pick-a-picã€PartiPromptå’ŒHPDä¸Šåˆ†åˆ«æé«˜äº†DiffusionDPOçš„å¹³å‡èƒœç‡15%ã€7.1%å’Œ10.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12575v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æœ€æ–°ç ”ç©¶æˆæœBalancedDPOã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–å•ä¸€åº¦é‡æ ‡å‡†æˆ–æœ‰é™æ•°æ®é›†å¯¼è‡´çš„å±€é™æ€§é—®é¢˜ï¼Œå®ƒé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•çš„æ”¹è¿›ï¼Œèƒ½å¤Ÿåœ¨åå¥½åˆ†å¸ƒç©ºé—´ä¸­ä»å¤šç§åº¦é‡æ ‡å‡†ä¸­æ±‡é›†å…±è¯†æ ‡ç­¾ï¼Œå®ç°ç¨³å¥ä¸”å¯æ‰©å±•çš„å¤šåº¦é‡æ ‡å‡†å¯¹é½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒBalancedDPOåœ¨ä¸»è¦æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œç›¸å¯¹äºDiffusionDPOå¹³å‡æé«˜äº†çº¦15%ã€7.1%å’Œ10.3%çš„èƒœç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„ä¸»è¦å‘ç°ç‚¹ï¼š</p>
<ul>
<li>BalancedDPOæ˜¯ä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ä¸å¤šç§åå¥½å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„æ”¹è¿›æ–¹æ³•ï¼ŒBalancedDPOèƒ½åœ¨åå¥½åˆ†å¸ƒç©ºé—´ä¸­æ±‡é›†å¤šç§åº¦é‡æ ‡å‡†çš„å…±è¯†æ ‡ç­¾ã€‚</li>
<li>BalancedDPOå®ç°äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„å¤šåº¦é‡æ ‡å‡†å¯¹é½ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–å•ä¸€åº¦é‡æ ‡å‡†æˆ–æœ‰é™æ•°æ®é›†çš„å±€é™æ€§ã€‚</li>
<li>BalancedDPOåœ¨å„ç§æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœå‡ä¸ºä¸šç•Œæœ€ä½³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99ab0120dc92ba72b09882780a1460ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0ea4a947e34e08baed05afccdac56e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c9c998c14dbc473891e9013b4ae4b36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d91fb7a12de486f8d19667b6299a23a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64642e75bef5bacec039c8ee5b43d44f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Segment-Any-Quality-Images-with-Generative-Latent-Space-Enhancement"><a href="#Segment-Any-Quality-Images-with-Generative-Latent-Space-Enhancement" class="headerlink" title="Segment Any-Quality Images with Generative Latent Space Enhancement"></a>Segment Any-Quality Images with Generative Latent Space Enhancement</h2><p><strong>Authors:Guangqian Guo, Yoong Guo, Xuehui Yu, Wenbo Li, Yaoxing Wang, Shan Gao</strong></p>
<p>Despite their success, Segment Anything Models (SAMs) experience significant performance drops on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Specifically, we adapt the concept of latent diffusion to SAM-based segmentation frameworks and perform the generative diffusion process in the latent space of SAM to reconstruct high-quality representation, thereby improving segmentation. Additionally, we introduce two techniques to improve compatibility between the pre-trained diffusion model and the segmentation framework. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. We also construct the LQSeg dataset with a greater diversity of degradation types and levels for training and evaluating the model. Extensive experiments demonstrate that GleSAM significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM also performs well on unseen degradations, underscoring the versatility of our approach and dataset. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†Segment Anything Modelsï¼ˆSAMï¼‰åœ¨ä¸¥é‡é€€åŒ–ã€ä½è´¨é‡çš„å›¾åƒä¸Šä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GleSAMã€‚å®ƒåˆ©ç”¨ç”Ÿæˆæ½œåœ¨ç©ºé—´å¢å¼ºæŠ€æœ¯æ¥æé«˜å¯¹ä½è´¨é‡å›¾åƒçš„ç¨³å¥æ€§ï¼Œä»è€Œå®ç°å„ç§å›¾åƒè´¨é‡çš„æ³›åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€‚åº”åŸºäºSAMçš„åˆ†å‰²æ¡†æ¶çš„æ½œåœ¨æ‰©æ•£æ¦‚å¿µï¼Œå¹¶åœ¨SAMçš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆæ‰©æ•£è¿‡ç¨‹ï¼Œä»¥é‡å»ºé«˜è´¨é‡è¡¨ç¤ºï¼Œä»è€Œæé«˜åˆ†å‰²æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§æŠ€æœ¯æ¥æé«˜é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œåˆ†å‰²æ¡†æ¶ä¹‹é—´çš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºé¢„è®­ç»ƒçš„SAMå’ŒSAM2ï¼Œå¹¶ä¸”åªéœ€è¦å¾ˆå°‘çš„å¯å­¦ä¹ å‚æ•°ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LQSegæ•°æ®é›†ï¼ŒåŒ…å«æ›´å¤šç±»å‹å’Œç¨‹åº¦çš„é€€åŒ–ç±»å‹ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGleSAMåœ¨å¤æ‚é€€åŒ–æƒ…å†µä¸‹æ˜¾è‘—æé«˜åˆ†å‰²ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒå¯¹æ¸…æ™°å›¾åƒçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGleSAMåœ¨æœªè§è¿‡çš„é€€åŒ–æƒ…å†µä¸‹ä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•å’Œæ•°æ®é›†çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12507v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨ä½è´¨é‡å›¾åƒä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGleSAMï¼Œåˆ©ç”¨ç”Ÿæˆå¼æ½œåœ¨ç©ºé—´å¢å¼ºæŠ€æœ¯æé«˜æ¨¡å‹å¯¹ä½è´¨é‡å›¾åƒçš„ç¨³å¥æ€§ï¼Œå®ç°è·¨ä¸åŒå›¾åƒè´¨é‡çš„æ³›åŒ–ã€‚æˆ‘ä»¬åœ¨SAMåˆ†å‰²æ¡†æ¶ä¸­å¼•å…¥æ½œåœ¨æ‰©æ•£æ¦‚å¿µï¼Œåœ¨SAMçš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆå¼æ‰©æ•£è¿‡ç¨‹ï¼Œé‡å»ºé«˜è´¨é‡è¡¨ç¤ºï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥ä¸¤ç§æŠ€æœ¯ï¼Œæé«˜é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åˆ†å‰²æ¡†æ¶çš„å…¼å®¹æ€§ã€‚GleSAMå¯åº”ç”¨äºé¢„è®­ç»ƒSAMå’ŒSAM2ï¼Œä»…éœ€è¦å°‘é‡é¢å¤–çš„å­¦ä¹ å‚æ•°ï¼Œå®ç°é«˜æ•ˆä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LQSegæ•°æ®é›†ï¼ŒåŒ…å«æ›´å¤šç±»å‹çš„é™è§£å’Œæ°´å¹³ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒGleSAMåœ¨å¤æ‚é™è§£æƒ…å†µä¸‹æ˜¾è‘—æé«˜åˆ†å‰²ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒå¯¹æ¸…æ™°å›¾åƒçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„é™è§£æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹åœ¨ä½è´¨é‡å›¾åƒä¸Šæ€§èƒ½å—é™ã€‚</li>
<li>GleSAMé€šè¿‡åˆ©ç”¨ç”Ÿæˆå¼æ½œåœ¨ç©ºé—´å¢å¼ºæŠ€æœ¯æé«˜æ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>GleSAMå¼•å…¥æ½œåœ¨æ‰©æ•£æ¦‚å¿µè‡³SAMåˆ†å‰²æ¡†æ¶ä¸­ã€‚</li>
<li>åœ¨SAMçš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆå¼æ‰©æ•£è¿‡ç¨‹ä»¥é‡å»ºé«˜è´¨é‡è¡¨ç¤ºã€‚</li>
<li>GleSAMå¼•å…¥ä¸¤ç§æŠ€æœ¯æå‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åˆ†å‰²æ¡†æ¶çš„å…¼å®¹æ€§ã€‚</li>
<li>GleSAMå¯åº”ç”¨äºé¢„è®­ç»ƒSAMå’ŒSAM2ï¼Œä¸”éœ€è¦å°‘é‡é¢å¤–å­¦ä¹ å‚æ•°ã€‚</li>
<li>LQSegæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼ŒåŒ…å«å¤šç§é™è§£ç±»å‹å’Œæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68abeafead4c9bb6876877eaa65037bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0414849a04b48aaa678bd9f1f780670a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b109fbb71561ac596f90c44975b057.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7873b85c13c50dccf3b1dbb7e8e3a723.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SING-Semantic-Image-Communications-using-Null-Space-and-INN-Guided-Diffusion-Models"><a href="#SING-Semantic-Image-Communications-using-Null-Space-and-INN-Guided-Diffusion-Models" class="headerlink" title="SING: Semantic Image Communications using Null-Space and INN-Guided   Diffusion Models"></a>SING: Semantic Image Communications using Null-Space and INN-Guided   Diffusion Models</h2><p><strong>Authors:Jiakang Chen, Selim F. Yilmaz, Di You, Pier Luigi Dragotti, Deniz GÃ¼ndÃ¼z</strong></p>
<p>Joint source-channel coding systems based on deep neural networks (DeepJSCC) have recently demonstrated remarkable performance in wireless image transmission. Existing methods primarily focus on minimizing distortion between the transmitted image and the reconstructed version at the receiver, often overlooking perceptual quality. This can lead to severe perceptual degradation when transmitting images under extreme conditions, such as low bandwidth compression ratios (BCRs) and low signal-to-noise ratios (SNRs). In this work, we propose SING, a novel two-stage JSCC framework that formulates the recovery of high-quality source images from corrupted reconstructions as an inverse problem. Depending on the availability of information about the DeepJSCC encoder&#x2F;decoder and the channel at the receiver, SING can either approximate the stochastic degradation as a linear transformation, or leverage invertible neural networks (INNs) for precise modeling. Both approaches enable the seamless integration of diffusion models into the reconstruction process, enhancing perceptual quality. Experimental results demonstrate that SING outperforms DeepJSCC and other approaches, delivering superior perceptual quality even under extremely challenging conditions, including scenarios with significant distribution mismatches between the training and test data. </p>
<blockquote>
<p>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„è”åˆæºä¿¡é“ç¼–ç ç³»ç»Ÿï¼ˆDeepJSCCï¼‰åœ¨æ— çº¿å›¾åƒä¼ è¾“ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æœ€å°åŒ–ä¼ è¾“å›¾åƒå’Œæ¥æ”¶ç«¯é‡å»ºç‰ˆæœ¬ä¹‹é—´çš„å¤±çœŸï¼Œå¾€å¾€å¿½ç•¥äº†æ„ŸçŸ¥è´¨é‡ã€‚è¿™åœ¨æç«¯æ¡ä»¶ä¸‹ä¼ è¾“å›¾åƒæ—¶ï¼Œå¦‚ä½å¸¦å®½å‹ç¼©æ¯”ï¼ˆBCRï¼‰å’Œä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡çš„æ„ŸçŸ¥è´¨é‡ä¸‹é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SINGï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µJSCCæ¡†æ¶ï¼Œå®ƒå°†ä»æŸåçš„é‡å»ºä¸­æ¢å¤é«˜è´¨é‡æºå›¾åƒè¡¨è¿°ä¸ºä¸€ä¸ªåé—®é¢˜ã€‚æ ¹æ®æ¥æ”¶ç«¯å…³äºDeepJSCCç¼–ç å™¨&#x2F;è§£ç å™¨å’Œä¿¡é“çš„ä¿¡æ¯å¯ç”¨æ€§ï¼ŒSINGå¯ä»¥å°†éšæœºé€€åŒ–è¿‘ä¼¼ä¸ºçº¿æ€§å˜æ¢ï¼Œæˆ–è€…åˆ©ç”¨å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNsï¼‰è¿›è¡Œç²¾ç¡®å»ºæ¨¡ã€‚ä¸¤ç§æ–¹æ³•éƒ½èƒ½æ— ç¼åœ°å°†æ‰©æ•£æ¨¡å‹é›†æˆåˆ°é‡å»ºè¿‡ç¨‹ä¸­ï¼Œæé«˜æ„ŸçŸ¥è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æå…·æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼ŒåŒ…æ‹¬è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„æƒ…å†µï¼ŒSINGä¹Ÿä¼˜äºDeepJSCCå’Œå…¶ä»–æ–¹æ³•ï¼Œæä¾›å“è¶Šçš„æ„ŸçŸ¥è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12484v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„è”åˆæºä¿¡é“ç¼–ç ç³»ç»Ÿï¼ˆDeepJSCCï¼‰åœ¨æ— çº¿å›¾åƒä¼ è¾“ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä¼ è¾“å›¾åƒä¸æ¥æ”¶ç«¯é‡å»ºç‰ˆæœ¬ä¹‹é—´çš„å¤±çœŸæœ€å°åŒ–ï¼Œå¿½è§†äº†æ„ŸçŸ¥è´¨é‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µJSCCæ¡†æ¶SINGï¼Œå°†ä»æŸåçš„é‡å»ºä¸­æ¢å¤é«˜è´¨é‡æºå›¾åƒè¡¨è¿°ä¸ºé€†é—®é¢˜ã€‚æ ¹æ®æ¥æ”¶å™¨å¯¹DeepJSCCç¼–ç å™¨&#x2F;è§£ç å™¨å’Œé€šé“ä¿¡æ¯çš„å¯ç”¨æ€§ï¼ŒSINGå¯ä»¥å°†éšæœºé€€åŒ–è¿‘ä¼¼ä¸ºçº¿æ€§å˜æ¢ï¼Œæˆ–åˆ©ç”¨å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNsï¼‰è¿›è¡Œç²¾ç¡®å»ºæ¨¡ã€‚ä¸¤ç§æ–¹æ³•éƒ½èƒ½æ— ç¼é›†æˆæ‰©æ•£æ¨¡å‹åˆ°é‡å»ºè¿‡ç¨‹ä¸­ï¼Œæé«˜æ„ŸçŸ¥è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æå…·æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼ŒSINGçš„è¡¨ç°ä¼˜äºDeepJSCCå’Œå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepJSCCåœ¨æ— çº¿å›¾åƒä¼ è¾“ä¸­æœ‰å“è¶Šæ€§èƒ½è¡¨ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒä¼ è¾“å¤±çœŸï¼Œå¿½è§†äº†æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>æå‡ºçš„æ–°å‹æ¡†æ¶SINGå°†å›¾åƒæ¢å¤è¡¨è¿°ä¸ºé€†é—®é¢˜ï¼Œä»¥æé«˜æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>æ ¹æ®ä¿¡æ¯çš„å¯ç”¨æ€§ï¼ŒSINGå¯ä»¥é‡‡ç”¨çº¿æ€§å˜æ¢æˆ–å¯é€†ç¥ç»ç½‘ç»œå»ºæ¨¡ã€‚</li>
<li>SINGé€šè¿‡é›†æˆæ‰©æ•£æ¨¡å‹å¢å¼ºé‡å»ºè¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSINGåœ¨å¤šç§æ¡ä»¶ä¸‹è¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬ä½å¸¦å®½å‹ç¼©æ¯”å’Œä½ä¿¡å™ªæ¯”ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c87a039aef7db7dae93a962bc98f572.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-234b51a3229ce916b891af5c216adac3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682850aa6d4ce5f7c8a14addbcea5d13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4cec5f13435e495ca81e56e3e6b11ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c708c575c64b81c66aa1a460e3ff1cc4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Harnessing-Frequency-Spectrum-Insights-for-Image-Copyright-Protection-Against-Diffusion-Models"><a href="#Harnessing-Frequency-Spectrum-Insights-for-Image-Copyright-Protection-Against-Diffusion-Models" class="headerlink" title="Harnessing Frequency Spectrum Insights for Image Copyright Protection   Against Diffusion Models"></a>Harnessing Frequency Spectrum Insights for Image Copyright Protection   Against Diffusion Models</h2><p><strong>Authors:Zhenguang Liu, Chao Shuai, Shaojing Fan, Ziping Dong, Jinwu Hu, Zhongjie Ba, Kui Ren</strong></p>
<p>Diffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce \emph{CoprGuard}, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–°è§†è§’å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬ä¾èµ–äºå¤§é‡ã€å¤šæ ·ä¸”é€šå¸¸ä¸å¯è¿½æº¯çš„Webæ•°æ®é›†ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹å›¾åƒç‰ˆæƒä¿æŠ¤çš„ç´§è¿«å…³æ³¨ã€‚å½“å‰çš„æ–¹æ³•åœ¨å¯é åœ°è¯†åˆ«æœªç»æˆæƒçš„å›¾åƒä½¿ç”¨æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­éš¾ä»¥å®ç°é€šç”¨åŒ–ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ•°æ®é›†ä¸­åŒ…å«æ¥è‡ªå¤šä¸ªæºå¤´çš„å›¾åƒæ—¶ï¼Œåªæœ‰å°‘æ•°å¯è¯†åˆ«çš„ï¼ˆæ°´å°æˆ–ä¸­æ¯’ï¼‰æ ·æœ¬ä¼šå¯¼è‡´å…¶å¤±æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æ–°çš„è¯æ®ï¼Œè¯æ˜æ‰©æ•£ç”Ÿæˆçš„å›¾åƒå¿ å®åœ°ä¿ç•™äº†å…¶è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡å±æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰è°±ç‰¹å¾ä¸Šã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoprGuardï¼Œè¿™æ˜¯ä¸€ä¸ªç¨³å¥çš„é¢‘ç‡åŸŸæ°´å°æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­é˜²æ­¢å›¾åƒè¢«æœªç»æˆæƒçš„ä½¿ç”¨ã€‚CoprGuardå¯¹æŠ—ä¸€ç³»åˆ—æ¨¡å‹è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œä»ç®€å•çš„æ‰©æ•£æ¨¡å‹åˆ°å¤æ‚çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ°´å°å›¾åƒä»…å 1%æ—¶ä¹Ÿèƒ½ä¿æŒç¨³å¥ã€‚è¿™ç§ç¨³å¥ä¸”é€šç”¨çš„æ–¹æ³•ä½¿å†…å®¹æ‰€æœ‰è€…åœ¨AIé©±åŠ¨å›¾åƒç”Ÿæˆçš„æ—¶ä»£èƒ½å¤Ÿä¿æŠ¤å…¶çŸ¥è¯†äº§æƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11071v2">PDF</a> Received by CVPR 2025 (10 pages, 11 figures)</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨æ–°å‹è§†å›¾åˆæˆä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶ä¾èµ–äºå¤§é‡å¤šæ ·ä¸”éš¾ä»¥è¿½æº¯çš„Webæ•°æ®é›†ï¼Œå¼•å‘äº†å…³äºå›¾åƒç‰ˆæƒä¿æŠ¤çš„ç´§è¿«å…³æ³¨ã€‚å½“å‰æ–¹æ³•éš¾ä»¥åœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­æ™®éè¯†åˆ«æœªç»æˆæƒçš„å›¾åƒä½¿ç”¨ï¼Œä¸”åœ¨è®­ç»ƒæ•°æ®é›†ä¸­åŒ…å«å¤šä¸ªæ¥æºçš„å›¾åƒæ—¶ï¼Œå› å¯è¯†åˆ«çš„æ°´å°æˆ–ä¸­æ¯’æ ·æœ¬è¾ƒå°‘è€Œå¤±æ•ˆã€‚æœ¬æ–‡æå‡ºæ‰©æ•£ç”Ÿæˆå›¾åƒå¿ å®ä¿ç•™å…¶è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰è°±ç‰¹å¾ä¸Šçš„ä½“ç°ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoprGuardï¼Œä¸€ä¸ªç¨³å¥çš„é¢‘ç‡åŸŸæ°´å°æ¡†æ¶ï¼Œç”¨äºä¿æŠ¤æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒä¸­çš„å›¾åƒå…å—æœªç»æˆæƒçš„ä½¿ç”¨ã€‚CoprGuardå¯¹ä»ç®€å•çš„æ‰©æ•£æ¨¡å‹åˆ°å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç­‰ä¸€ç³»åˆ—æ¨¡å‹å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œå³ä½¿åœ¨æ°´å°å›¾åƒä»…å è®­ç»ƒæ•°æ®é›†çš„1%æ—¶ä»å…·æœ‰ç¨³å¥æ€§ã€‚è¿™ä¸€ç¨³å¥ä¸”é€šç”¨çš„æ–¹æ³•èµ‹äºˆäº†å†…å®¹æ‰€æœ‰è€…åœ¨AIé©±åŠ¨å›¾åƒç”Ÿæˆæ—¶ä»£ä¿æŠ¤å…¶çŸ¥è¯†äº§æƒçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†å›¾åˆæˆä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†å­˜åœ¨å›¾åƒç‰ˆæƒä¿æŠ¤çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥åœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­è¯†åˆ«æœªç»æˆæƒçš„å›¾åƒä½¿ç”¨ã€‚</li>
<li>æ‰©æ•£ç”Ÿæˆå›¾åƒä¿ç•™è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰è°±ç‰¹å¾ä¸Šã€‚</li>
<li>å¼•å…¥CoprGuardæ¡†æ¶ï¼ŒåŸºäºé¢‘ç‡åŸŸæ°´å°ä¿æŠ¤å›¾åƒå…å—æœªç»æˆæƒä½¿ç”¨ã€‚</li>
<li>CoprGuardå¯¹å„ç§æ‰©æ•£æ¨¡å‹æœ‰æ•ˆï¼ŒåŒ…æ‹¬ä»ç®€å•çš„åˆ°å¤æ‚çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</li>
<li>CoprGuardåœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ°´å°å›¾åƒå æ¯”å¾ˆä½æ—¶ä»å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-674a6a5518523850dcbcb2323ac20cbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99824011a096f7df0daa5b8b12f9034e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a6b31bbf90cf495357a862bcf283701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bf7796588e9a9bee164cf022a2e92ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67a350a0e331867666e1762c6cd8cef6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation"><a href="#Reangle-A-Video-4D-Video-Generation-as-Video-to-Video-Translation" class="headerlink" title="Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"></a>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</h2><p><strong>Authors:Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye</strong></p>
<p>We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Reangle-A-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªä»å•ä¸ªè¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ä¸»æµæ–¹æ³•åœ¨å¤§å‹4Dæ•°æ®é›†ä¸Šè®­ç»ƒå¤šè§†è§’è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å¤šè§†è§’è§†é¢‘ç”Ÿæˆä»»åŠ¡é‡æ–°å®šä½ä¸ºè§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘ï¼Œåˆ©ç”¨å…¬å¼€å¯ç”¨çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒã€‚æœ¬è´¨ä¸Šï¼ŒReangle-A-Videoåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯â€œå¤šè§†è§’è¿åŠ¨å­¦ä¹ â€ï¼šä»¥è‡ªç›‘ç£çš„æ–¹å¼åŒæ­¥å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œä»ä¸€ç»„å˜å½¢è§†é¢‘ä¸­æç‚¼å‡ºè§†è§’ä¸å˜çš„è¿åŠ¨ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯â€œå¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘â€ï¼šè¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§åœ¨æ¨ç†æ—¶é—´è·¨è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ä¸‹è¢«å˜å½¢å’Œå¡«å……åˆ°å„ç§ç›¸æœºè§†è§’ï¼Œä½¿ç”¨DUSt3Rç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„é¦–å¸§å›¾åƒã€‚åœ¨é™æ€è§†è§’è½¬æ¢å’ŒåŠ¨æ€ç›¸æœºæ§åˆ¶æ–¹é¢çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReangle-A-Videoè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤šè§†è§’è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09151v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hyeonho99.github.io/reangle-a-video/">https://hyeonho99.github.io/reangle-a-video/</a></p>
<p><strong>Summary</strong></p>
<p>Reangle-A-Videoæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»¥ä»å•ä¸ªè¾“å…¥è§†é¢‘ç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘ã€‚å®ƒé‡‡ç”¨è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘æ–¹å¼ï¼Œåˆ©ç”¨å…¬å¼€å¯ç”¨çš„å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œå°†å¤šè§†è§’è§†é¢‘ç”Ÿæˆä»»åŠ¡é‡æ–°æ„å»ºã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šå¤šè§†è§’è¿åŠ¨å­¦ä¹ å’Œå¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚é€šè¿‡åŒæ­¥å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œä»ä¸€ç»„å˜å½¢çš„è§†é¢‘ä¸­æç‚¼å‡ºè§†å›¾ä¸å˜çš„è¿åŠ¨ï¼›å¹¶åœ¨æ¨ç†æ—¶é—´äº¤å‰è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ä¸‹ï¼Œå°†è¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§å˜å½¢å¹¶å¡«å……ä¸ºå„ç§ç›¸æœºè§†è§’ï¼Œç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„å¼€å§‹å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒReangle-A-Videoåœ¨é™æ€è§†è§’è½¬æ¢å’ŒåŠ¨æ€ç›¸æœºæ§åˆ¶æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤šè§’åº¦è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Reangle-A-Videoæ˜¯ä¸€ä¸ªç”ŸæˆåŒæ­¥å¤šè§†è§’è§†é¢‘çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»å•ä¸ªè¾“å…¥è§†é¢‘è¿›è¡Œç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨è§†é¢‘åˆ°è§†é¢‘çš„ç¿»è¯‘æ–¹å¼ï¼Œåˆ©ç”¨å›¾åƒå’Œè§†é¢‘æ‰©æ•£å…ˆéªŒã€‚</li>
<li>Reangle-A-VideoåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šå¤šè§†è§’è¿åŠ¨å­¦ä¹ å’Œå¤šè§†è§’ä¸€è‡´å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚</li>
<li>é€šè¿‡åŒæ­¥å¾®è°ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ï¼Œä»å˜å½¢çš„è§†é¢‘ä¸­æç‚¼è§†å›¾ä¸å˜çš„è¿åŠ¨ã€‚</li>
<li>åœ¨æ¨ç†æ—¶é—´äº¤å‰è§†è§’ä¸€è‡´æ€§æŒ‡å¯¼ä¸‹ï¼Œç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„å¼€å§‹å›¾åƒã€‚</li>
<li>Reangle-A-Videoåœ¨é™æ€è§†è§’è½¬æ¢å’ŒåŠ¨æ€ç›¸æœºæ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1eff3b7dde46c0a0f2e660a0d6b19c49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-034525eeb601df4fbf9f0abc26d2d754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c42602c21b81f4a9b9c1e75bf0491a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6b7950510b9a10e64706e857d8e34f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc5663c575eb9a60228af9158e38ef4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PLADIS-Pushing-the-Limits-of-Attention-in-Diffusion-Models-at-Inference-Time-by-Leveraging-Sparsity"><a href="#PLADIS-Pushing-the-Limits-of-Attention-in-Diffusion-Models-at-Inference-Time-by-Leveraging-Sparsity" class="headerlink" title="PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference   Time by Leveraging Sparsity"></a>PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference   Time by Leveraging Sparsity</h2><p><strong>Authors:Kwanyoung Kim, Byeongsu Sim</strong></p>
<p>Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net&#x2F;Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution. See Our project page : <a target="_blank" rel="noopener" href="https://cubeyoung.github.io/pladis-proejct/">https://cubeyoung.github.io/pladis-proejct/</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åˆ©ç”¨æŒ‡å¯¼æŠ€æœ¯ï¼ˆå¦‚æ— éœ€åˆ†ç±»å™¨æŒ‡å¯¼ï¼‰ç”Ÿæˆé«˜è´¨é‡çš„æ¡ä»¶æ ·æœ¬ï¼Œå¹¶å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œä½¿å…¶ä¸æŒ‡å¯¼è’¸é¦æ¨¡å‹ä¸å…¼å®¹ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¾èµ–äºå¯å‘å¼æ–¹æ³•ï¼Œéœ€è¦è¯†åˆ«ç›®æ ‡å±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºPLADISï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æ¥æå‡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆU-Net&#x2F;Transformerï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å±‚ä¸­çš„softmaxåŠå…¶ç¨€ç–å¯¹åº”ç‰©æ¥æ¨æ–­æŸ¥è¯¢-é”®ç›¸å…³æ€§ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–NFEã€‚é€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›çš„å™ªå£°é²æ£’æ€§ï¼Œæˆ‘ä»¬çš„PLADISé‡Šæ”¾äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›ï¼Œä½¿å®ƒä»¬åœ¨ä»¥å‰æŒ£æ‰çš„é¢†åŸŸæœ‰äº†æ–°çš„å‘ç°ï¼Œå¹¶åœ¨æ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”æ™®éé€‚ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚è¯¦è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cubeyoung.github.io/pladis-proejct/">https://cubeyoung.github.io/pladis-proejct/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07677v2">PDF</a> 29 pages, 19 figures, project page :   <a target="_blank" rel="noopener" href="https://cubeyoung.github.io/pladis-proejct/">https://cubeyoung.github.io/pladis-proejct/</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åˆ©ç”¨æŒ‡å¯¼æŠ€æœ¯å¦‚éåˆ†ç±»æŒ‡å¯¼ï¼ˆCFGï¼‰ç”Ÿæˆé«˜è´¨é‡çš„æ¡ä»¶æ ·æœ¬ï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰ï¼Œè¿™ä¸æŒ‡å¯¼è’¸é¦æ¨¡å‹ä¸å…¼å®¹ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¾èµ–äºå¯å‘å¼æ–¹æ³•ï¼Œéœ€è¦è¯†åˆ«ç›®æ ‡å±‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œåä¸ºPLADISï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æ¥æå‡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆU-Net&#x2F;Transformerï¼‰ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ‰©å±•äº†äº¤å‰æ³¨æ„åŠ›å±‚ä¸­çš„æŸ¥è¯¢-é”®ç›¸å…³æ€§ï¼Œä½¿ç”¨softmaxåŠå…¶ç¨€ç–å¯¹åº”ç‰©ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–NFEsã€‚é€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›çš„å™ªå£°é²æ£’æ€§ï¼ŒPLADISé‡Šæ”¾äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›ï¼Œä½¿å®ƒä»¬åœ¨ä»¥å‰æŒ£æ‰çš„é¢†åŸŸæœ‰äº†æ–°çš„å‘ç°ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ•ˆæœã€‚å®ƒä¸æŒ‡å¯¼æŠ€æœ¯æ— ç¼é›†æˆï¼ŒåŒ…æ‹¬æŒ‡å¯¼è’¸é¦æ¨¡å‹ã€‚å¤§é‡çš„å®éªŒæ˜¾ç¤ºï¼Œå®ƒåœ¨æ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½æ–¹é¢æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”æ™®éé€‚ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡ä½¿ç”¨æŒ‡å¯¼æŠ€æœ¯å¦‚éåˆ†ç±»æŒ‡å¯¼ï¼ˆCFGï¼‰ç”Ÿæˆé«˜è´¨é‡æ¡ä»¶æ ·æœ¬ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å…¼å®¹æ€§å’Œæ•ˆç‡é—®é¢˜ï¼Œéœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç¥ç»åŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰ï¼Œå¹¶ä¸”ä¾èµ–äºå¯å‘å¼æ–¹æ³•æ¥è¯†åˆ«ç›®æ ‡å±‚ã€‚</li>
<li>PLADISæ–¹æ³•é€šè¿‡åˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›æå‡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆU-Net&#x2F;Transformerï¼‰ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–NFEsã€‚</li>
<li>PLADISåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­é‡Šæ”¾äº†æ½œåœ¨èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ä»¥å‰è¡¨ç°ä¸ä½³çš„é¢†åŸŸå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>PLADISæ–¹æ³•ä¸å„ç§æŒ‡å¯¼æŠ€æœ¯æ— ç¼é›†æˆï¼ŒåŒ…æ‹¬æŒ‡å¯¼è’¸é¦æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPLADISåœ¨æ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>PLADISæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”æ™®éé€‚ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-beb1ba3116552caef77fe07ad1f64914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8a30b32789cc3fe4adb4b49c716343b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ba24f8d5fd1105e23ce0deb6a96fc24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a49886bd2016f4866c72c8004f94249.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2503.07677v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TraSCE-Trajectory-Steering-for-Concept-Erasure"><a href="#TraSCE-Trajectory-Steering-for-Concept-Erasure" class="headerlink" title="TraSCE: Trajectory Steering for Concept Erasure"></a>TraSCE: Trajectory Steering for Concept Erasure</h2><p><strong>Authors:Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, Yuki Mitsufuji</strong></p>
<p>Recent advancements in text-to-image diffusion models have brought them to the public spotlight, becoming widely accessible and embraced by everyday users. However, these models have been shown to generate harmful content such as not-safe-for-work (NSFW) images. While approaches have been proposed to erase such abstract concepts from the models, jail-breaking techniques have succeeded in bypassing such safety measures. In this paper, we propose TraSCE, an approach to guide the diffusion trajectory away from generating harmful content. Our approach is based on negative prompting, but as we show in this paper, a widely used negative prompting strategy is not a complete solution and can easily be bypassed in some corner cases. To address this issue, we first propose using a specific formulation of negative prompting instead of the widely used one. Furthermore, we introduce a localized loss-based guidance that enhances the modified negative prompting technique by steering the diffusion trajectory. We demonstrate that our proposed method achieves state-of-the-art results on various benchmarks in removing harmful content, including ones proposed by red teams, and erasing artistic styles and objects. Our proposed approach does not require any training, weight modifications, or training data (either image or prompt), making it easier for model owners to erase new concepts. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä½¿å®ƒä»¬å—åˆ°å…¬ä¼—å…³æ³¨ï¼Œæ—¥å¸¸ç”¨æˆ·ä¹Ÿå¯ä»¥å¹¿æ³›è®¿é—®å’Œä½¿ç”¨å®ƒä»¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å·²æ˜¾ç¤ºå‡ºä¼šç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œä¾‹å¦‚ä¸é€‚åˆå·¥ä½œåœºæ‰€ï¼ˆNSFWï¼‰çš„å›¾åƒã€‚è™½ç„¶æœ‰äººæå‡ºä»æ¨¡å‹ä¸­åˆ é™¤æ­¤ç±»æŠ½è±¡æ¦‚å¿µçš„æ–¹æ³•ï¼Œä½†è¶Šç‹±æŠ€æœ¯å·²æˆåŠŸç»•è¿‡è¿™äº›å®‰å…¨æªæ–½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºTraSCEæ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æ‰©æ•£è½¨è¿¹è¿œç¦»ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè´Ÿæç¤ºï¼Œä½†æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ‰€ç¤ºï¼Œå¹¿æ³›ä½¿ç”¨çš„è´Ÿæç¤ºç­–ç•¥å¹¶ä¸æ˜¯å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨æŸäº›ç‰¹å®šæƒ…å†µä¸‹å¾ˆå®¹æ˜“è¢«ç»•è¿‡ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºä½¿ç”¨ç‰¹å®šçš„è´Ÿæç¤ºå½¢å¼ï¼Œè€Œä¸æ˜¯å¹¿æ³›ä½¿ç”¨çš„å½¢å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå±€éƒ¨æŸå¤±çš„æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æ‰©æ•£è½¨è¿¹æ¥å¢å¼ºæ”¹è¿›åçš„è´Ÿæç¤ºæŠ€æœ¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒ…æ‹¬çº¢é˜Ÿæå‡ºçš„å„ç§åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠæ¶ˆé™¤è‰ºæœ¯é£æ ¼å’Œç‰©ä½“ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸éœ€è¦ä»»ä½•è®­ç»ƒã€æƒé‡ä¿®æ”¹æˆ–è®­ç»ƒæ•°æ®ï¼ˆæ— è®ºæ˜¯å›¾åƒè¿˜æ˜¯æç¤ºï¼‰ï¼Œè¿™ä½¿å¾—æ¨¡å‹æ‰€æœ‰è€…æ›´å®¹æ˜“æ¶ˆé™¤æ–°æ¦‚å¿µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07658v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬æ‰©æ•£æ¨¡å‹æœ€æ–°è¿›å±•å¹¿å—å…¬ä¼—å…³æ³¨ï¼Œä½†ç”Ÿæˆæœ‰å®³å†…å®¹ï¼ˆå¦‚ä¸é€‚åˆå·¥ä½œç¯å¢ƒçš„å›¾åƒï¼‰çš„é—®é¢˜ä¹Ÿéšä¹‹æ˜¾ç°ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾ä»æ¨¡å‹ä¸­æ¶ˆé™¤æ­¤ç±»æŠ½è±¡æ¦‚å¿µï¼Œä½†å­˜åœ¨è¶Šç‹±æŠ€æœ¯ç»•è¿‡è¿™äº›å®‰å…¨æªæ–½ã€‚æœ¬æ–‡æå‡ºTraSCEæ–¹æ³•ï¼Œé€šè¿‡è´Ÿæç¤ºå¼•å¯¼æ‰©æ•£è½¨è¿¹é¿å…ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚æˆ‘ä»¬å‘ç°å¹¿æ³›ä½¿ç”¨çš„è´Ÿæç¤ºç­–ç•¥å¹¶ä¸å®Œå–„ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å®¹æ˜“è¢«ç»•è¿‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç‰¹å®šçš„è´Ÿæç¤ºå½¢å¼ï¼Œå¹¶å¼•å…¥åŸºäºå±€éƒ¨æŸå¤±çš„æŒ‡å¯¼ï¼Œå¢å¼ºä¿®æ”¹åçš„è´Ÿæç¤ºæŠ€æœ¯ï¼Œå¼•å¯¼æ‰©æ•£è½¨è¿¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹ã€æ¶ˆé™¤è‰ºæœ¯é£æ ¼å’Œç‰©ä½“ç­‰æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒã€æƒé‡ä¿®æ”¹æˆ–è®­ç»ƒæ•°æ®ï¼ˆæ— è®ºæ˜¯å›¾åƒè¿˜æ˜¯æç¤ºï¼‰ï¼Œä½¿å¾—æ¨¡å‹æ‰€æœ‰è€…æ›´å®¹æ˜“æ¶ˆé™¤æ–°æ¦‚å¿µã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬æ‰©æ•£æ¨¡å‹è¿‘æœŸè¿›å±•å¼•å‘å…¬ä¼—å…³æ³¨ï¼Œä½†ç”Ÿæˆæœ‰å®³å†…å®¹çš„é—®é¢˜çªæ˜¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾ä»æ¨¡å‹ä¸­æ¶ˆé™¤æœ‰å®³å†…å®¹ï¼Œä½†å­˜åœ¨æŠ€æœ¯èƒ½å¤Ÿç»•è¿‡è¿™äº›å®‰å…¨æªæ–½ã€‚</li>
<li>TraSCEæ–¹æ³•é€šè¿‡è´Ÿæç¤ºå¼•å¯¼æ‰©æ•£è½¨è¿¹ï¼Œé¿å…ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚</li>
<li>å¹¿æ³›ä½¿ç”¨çš„è´Ÿæç¤ºç­–ç•¥å¹¶ä¸å®Œå–„ï¼Œéœ€è¦åœ¨ç‰¹å®šæƒ…å†µä¸‹è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>TraSCEä½¿ç”¨ç‰¹å®šçš„è´Ÿæç¤ºå½¢å¼ï¼Œå¹¶ç»“åˆå±€éƒ¨æŸå¤±æŒ‡å¯¼å¢å¼ºæŠ€æœ¯æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜TraSCEæ–¹æ³•åœ¨å»é™¤æœ‰å®³å†…å®¹æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2412.07658v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2412.07658v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2412.07658v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2412.07658v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2412.07658v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Video-Depth-without-Video-Models"><a href="#Video-Depth-without-Video-Models" class="headerlink" title="Video Depth without Video Models"></a>Video Depth without Video Models</h2><p><strong>Authors:Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, Konrad Schindler</strong></p>
<p>Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io. </p>
<blockquote>
<p>è§†é¢‘æ·±åº¦ä¼°è®¡é€šè¿‡æ¨æ–­æ¯ä¸€å¸§çš„å¯†é›†æ·±åº¦ï¼Œå°†å•ç›®è§†é¢‘ç‰‡æ®µæå‡åˆ°3Dã€‚éšç€å¤§å‹åŸºç¡€æ¨¡å‹å’Œåˆæˆè®­ç»ƒæ•°æ®çš„å…´èµ·ï¼Œå•å›¾åƒæ·±åº¦ä¼°è®¡æ–¹é¢çš„æœ€æ–°è¿›å±•æ¿€å‘äº†äººä»¬å¯¹è§†é¢‘æ·±åº¦çš„é‡æ–°å…³æ³¨ã€‚ç„¶è€Œï¼Œç›²ç›®åœ°å°†å•å›¾åƒæ·±åº¦ä¼°è®¡å™¨åº”ç”¨äºè§†é¢‘çš„æ¯ä¸€å¸§ï¼Œä¼šå¿½è§†æ—¶é—´çš„è¿ç»­æ€§ï¼Œè¿™ä¸ä»…ä¼šå¯¼è‡´é—ªçƒï¼Œè¿˜å¯èƒ½åœ¨ç›¸æœºè¿åŠ¨å¯¼è‡´æ·±åº¦èŒƒå›´çªç„¶å˜åŒ–æ—¶å‡ºç°é—®é¢˜ã€‚ä¸€ä¸ªæ˜æ˜¾ä¸”åŸºäºåŸåˆ™çš„è§£å†³æ–¹æ¡ˆæ˜¯å»ºç«‹è§†é¢‘åŸºç¡€æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹ä¹Ÿæœ‰å…¶è‡ªèº«çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬æ˜‚è´µçš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€ä¸å®Œç¾çš„3Dä¸€è‡´æ€§ä»¥åŠå›ºå®šé•¿åº¦ï¼ˆçŸ­ï¼‰è¾“å‡ºçš„æ‹¼æ¥ç¨‹åºã€‚æˆ‘ä»¬é€€åä¸€æ­¥ï¼Œå±•ç¤ºäº†å¦‚ä½•å°†å•å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è½¬åŒ–ä¸ºæœ€å…ˆè¿›çš„è§†é¢‘æ·±åº¦ä¼°è®¡å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€”â€”RollingDepthï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆiï¼‰å¤šå¸§æ·±åº¦ä¼°è®¡å™¨ï¼Œå®ƒæ¥æºäºå•å›¾åƒLDMï¼Œå°†éå¸¸çŸ­çš„è§†é¢‘ç‰‡æ®µï¼ˆé€šå¸¸ä¸ºä¸‰å¸§ï¼‰æ˜ å°„åˆ°æ·±åº¦ç‰‡æ®µã€‚ï¼ˆiiï¼‰ä¸€ä¸ªç¨³å¥çš„ã€åŸºäºä¼˜åŒ–çš„æ³¨å†Œç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿæœ€ä½³åœ°å°†ä»ä¸åŒå¸§ç‡é‡‡æ ·çš„æ·±åº¦ç‰‡æ®µé‡æ–°ç»„åˆæˆä¸€è‡´çš„è§†é¢‘ã€‚RollingDepthèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é•¿è¾¾æ•°ç™¾å¸§çš„é•¿è§†é¢‘ï¼Œå¹¶æä¾›æ¯”ä¸“ç”¨è§†é¢‘æ·±åº¦ä¼°è®¡å™¨å’Œé«˜æ€§èƒ½å•å¸§æ¨¡å‹æ›´å‡†ç¡®çš„æ·±åº¦è§†é¢‘ã€‚é¡¹ç›®é¡µé¢ï¼šrollingdepth.github.ioã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19189v2">PDF</a> Project page: rollingdepth.github.io</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†é¢‘æ·±åº¦ä¼°ç®—æŠ€æœ¯ï¼Œå°†å•çœ¼è§†é¢‘å‰ªè¾‘æå‡ä¸ºä¸‰ç»´æ•ˆæœï¼Œé€šè¿‡æ¯ä¸€å¸§æ¨æ–­å‡ºå¯†é›†æ·±åº¦ä¿¡æ¯ã€‚è¿‘æœŸéšç€å¤§å‹åŸºç¡€æ¨¡å‹å’Œåˆæˆè®­ç»ƒæ•°æ®çš„å…´èµ·ï¼Œå•å›¾åƒæ·±åº¦ä¼°ç®—æŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œå¼•å‘äº†äººä»¬å¯¹è§†é¢‘æ·±åº¦çš„é‡æ–°å…³æ³¨ã€‚ç„¶è€Œï¼Œç®€å•åœ°å°†å•å›¾åƒæ·±åº¦ä¼°ç®—å™¨åº”ç”¨äºè§†é¢‘çš„æ¯ä¸€å¸§ä¼šå¿½è§†æ—¶é—´çš„è¿ç»­æ€§ï¼Œè¿™ä¸ä»…ä¼šå¯¼è‡´é—ªçƒï¼Œè¿˜å¯èƒ½åœ¨ç›¸æœºè¿åŠ¨å¯¼è‡´æ·±åº¦èŒƒå›´çªç„¶å˜åŒ–æ—¶å¤±æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å•å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è½¬åŒ–ä¸ºæœ€å…ˆè¿›çš„è§†é¢‘æ·±åº¦ä¼°ç®—å™¨çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€”â€”RollingDepthï¼ŒåŒ…å«ä¸¤ä¸ªä¸»è¦æˆåˆ†ï¼šï¼ˆiï¼‰ä»å•å›¾åƒLDMæ´¾ç”Ÿçš„å¤šå¸§æ·±åº¦ä¼°ç®—å™¨ï¼Œå®ƒå°†éå¸¸çŸ­çš„è§†é¢‘ç‰‡æ®µï¼ˆé€šå¸¸ä¸ºä¸‰å¸§ï¼‰æ˜ å°„åˆ°æ·±åº¦ç‰‡æ®µï¼›ï¼ˆiiï¼‰ä¸€ç§ç¨³å¥çš„ã€åŸºäºä¼˜åŒ–çš„æ³¨å†Œç®—æ³•ï¼Œå®ƒä»¥æœ€ä½³æ–¹å¼å°†ä¸åŒå¸§ç‡é‡‡æ ·çš„æ·±åº¦ç‰‡æ®µé‡æ–°ç»„åˆæˆä¸€è‡´çš„è§†é¢‘ã€‚RollingDepthèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é•¿è¾¾æ•°ç™¾å¸§çš„è§†é¢‘ï¼Œå¹¶æä¾›æ¯”ä¸“ç”¨è§†é¢‘æ·±åº¦ä¼°ç®—å™¨å’Œé«˜æ€§èƒ½å•å¸§æ¨¡å‹æ›´å‡†ç¡®çš„æ·±åº¦è§†é¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ·±åº¦ä¼°ç®—æŠ€æœ¯èƒ½å¤Ÿå°†å•çœ¼è§†é¢‘è½¬åŒ–ä¸ºä¸‰ç»´æ•ˆæœï¼Œé€šè¿‡æ¯ä¸€å¸§çš„å¯†é›†æ·±åº¦ä¿¡æ¯æ¨æ–­å®ç°ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸­å•å›¾åƒæ·±åº¦ä¼°ç®—å·²å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç®€å•åº”ç”¨å•å›¾åƒæ·±åº¦ä¼°ç®—äºè§†é¢‘å¤„ç†ä¼šå¿½ç•¥æ—¶é—´è¿ç»­æ€§ï¼Œé€ æˆé—ªçƒæˆ–æ·±åº¦èŒƒå›´çªå˜æ—¶å¤±æ•ˆçš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å°†å•å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è½¬åŒ–ä¸ºè§†é¢‘æ·±åº¦ä¼°ç®—å™¨çš„æ–¹æ³•â€”â€”RollingDepthã€‚</li>
<li>RollingDepthåŒ…å«å¤šå¸§æ·±åº¦ä¼°ç®—å™¨å’ŒåŸºäºä¼˜åŒ–çš„æ³¨å†Œç®—æ³•ä¸¤ä¸ªä¸»è¦æˆåˆ†ã€‚</li>
<li>å¤šå¸§æ·±åº¦ä¼°ç®—å™¨èƒ½å°†çŸ­è§†é¢‘ç‰‡æ®µæ˜ å°„åˆ°æ·±åº¦ç‰‡æ®µã€‚</li>
<li>åŸºäºä¼˜åŒ–çš„æ³¨å†Œç®—æ³•èƒ½ç¡®ä¿ä¸åŒå¸§ç‡é‡‡æ ·çš„æ·±åº¦ç‰‡æ®µç»„åˆæˆä¸€è‡´çš„è§†é¢‘ï¼Œæé«˜å¤„ç†é•¿è§†é¢‘çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚RollingDepthç›¸æ¯”å…¶ä»–æ–¹æ³•èƒ½æä¾›æ›´å‡†ç¡®çš„æ·±åº¦è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.19189v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.19189v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.19189v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.19189v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Classifier-Free-Guidance-inside-the-Attraction-Basin-May-Cause-Memorization"><a href="#Classifier-Free-Guidance-inside-the-Attraction-Basin-May-Cause-Memorization" class="headerlink" title="Classifier-Free Guidance inside the Attraction Basin May Cause   Memorization"></a>Classifier-Free Guidance inside the Attraction Basin May Cause   Memorization</h2><p><strong>Authors:Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, Yuki Mitsufuji</strong></p>
<p>Diffusion models are prone to exactly reproduce images from the training data. This exact reproduction of the training data is concerning as it can lead to copyright infringement and&#x2F;or leakage of privacy-sensitive information. In this paper, we present a novel perspective on the memorization phenomenon and propose a simple yet effective approach to mitigate it. We argue that memorization occurs because of an attraction basin in the denoising process which steers the diffusion trajectory towards a memorized image. However, this can be mitigated by guiding the diffusion trajectory away from the attraction basin by not applying classifier-free guidance until an ideal transition point occurs from which classifier-free guidance is applied. This leads to the generation of non-memorized images that are high in image quality and well-aligned with the conditioning mechanism. To further improve on this, we present a new guidance technique, opposite guidance, that escapes the attraction basin sooner in the denoising process. We demonstrate the existence of attraction basins in various scenarios in which memorization occurs, and we show that our proposed approach successfully mitigates memorization. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å®¹æ˜“ç›´æ¥å¤åˆ¶è®­ç»ƒæ•°æ®ä¸­çš„å›¾åƒã€‚è¿™ç§å¯¹è®­ç»ƒæ•°æ®çš„ç²¾ç¡®å¤åˆ¶ä¼šå¼•å‘ç‰ˆæƒä¾µçŠ¯å’Œ&#x2F;æˆ–éšç§æ•æ„Ÿä¿¡æ¯æ³„éœ²çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»æ–°çš„è§’åº¦çœ‹å¾…è®°å¿†ç°è±¡ï¼Œå¹¶æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„ç¼“è§£æ–¹æ³•ã€‚æˆ‘ä»¬è®¤ä¸ºè®°å¿†çš„äº§ç”Ÿæ˜¯ç”±äºå»å™ªè¿‡ç¨‹ä¸­çš„å¸å¼•å­ç›†åœ°ä½¿å¾—æ‰©æ•£è½¨è¿¹æœå‘è®°å¿†ä¸­çš„å›¾åƒã€‚ç„¶è€Œï¼Œå¯ä»¥é€šè¿‡åœ¨è¾¾åˆ°ç†æƒ³çš„è¿‡æ¸¡ç‚¹ä¹‹å‰ä¸åº”ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œå°†æ‰©æ•£è½¨è¿¹ä»å¸å¼•å­ç›†åœ°ä¸­ç§»å¼€ï¼Œä»è€Œç¼“è§£è¿™ä¸€é—®é¢˜ã€‚è¿™å¯¼è‡´äº†ç”Ÿæˆéè®°å¿†å›¾åƒï¼Œå›¾åƒè´¨é‡é«˜ä¸”ä¸è°ƒèŠ‚æœºåˆ¶å»åˆè‰¯å¥½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼•å¯¼æŠ€æœ¯â€”â€”åå‘å¼•å¯¼ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å»å™ªè¿‡ç¨‹ä¸­æ›´å¿«åœ°æ‘†è„±å¸å¼•å­ç›†åœ°ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨å„ç§è®°å¿†ç°è±¡å‘ç”Ÿçš„åœºæ™¯ä¸­å¸å¼•å­ç›†åœ°çš„å­˜åœ¨ï¼Œå¹¶è¯æ˜æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æˆåŠŸåœ°ç¼“è§£äº†è®°å¿†ç°è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16738v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºæ‰©æ•£æ¨¡å‹ä¼šé‡ç°è®­ç»ƒæ•°æ®ä¸­çš„å›¾åƒï¼Œè¿™å¯èƒ½å¯¼è‡´ç‰ˆæƒä¾µçŠ¯å’Œéšç§æ³„éœ²ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„è§†è§’æ¥è§£é‡Šè¿™ä¸€ç°è±¡ï¼Œå¹¶å»ºè®®äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥å‡è½»å…¶å½±å“ã€‚æ–‡ç« è®¤ä¸ºè®°å¿†ç°è±¡æ˜¯ç”±äºå»å™ªè¿‡ç¨‹ä¸­çš„å¸å¼•ç›†åœ°é€ æˆçš„ï¼Œå¯ä»¥é€šè¿‡ä¸åº”ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ç›´è‡³ç†æƒ³çš„è¿‡æ¸¡ç‚¹ï¼Œç„¶ååº”ç”¨å®ƒæ¥å¼•å¯¼æ‰©æ•£è½¨è¿¹è¿œç¦»å¸å¼•ç›†åœ°ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡å¯¼æŠ€æœ¯ï¼Œå³åå‘æŒ‡å¯¼ï¼Œå®ƒèƒ½æ›´æ—©åœ°ä»å»å™ªè¿‡ç¨‹ä¸­è·³å‡ºå¸å¼•ç›†åœ°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå‡è½»äº†è®°å¿†ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¼šé‡ç°è®­ç»ƒæ•°æ®ä¸­çš„å›¾åƒï¼Œè¿™å¯èƒ½ä¼šå¼•å‘ç‰ˆæƒå’Œéšç§é—®é¢˜ã€‚</li>
<li>è®°å¿†ç°è±¡çš„äº§ç”Ÿæ˜¯å› ä¸ºå»å™ªè¿‡ç¨‹ä¸­çš„å¸å¼•ç›†åœ°å¯¼è‡´æ‰©æ•£è½¨è¿¹å¯¼å‘è®°å¿†ä¸­çš„å›¾åƒã€‚</li>
<li>é€šè¿‡ä¸åº”ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ç›´è‡³ç†æƒ³çš„è¿‡æ¸¡ç‚¹ï¼Œç„¶ååº”ç”¨å®ƒæ¥å¼•å¯¼æ‰©æ•£è½¨è¿¹è¿œç¦»å¸å¼•ç›†åœ°ï¼Œå¯ä»¥å‡è½»è®°å¿†ç°è±¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡å¯¼æŠ€æœ¯â€”â€”åå‘æŒ‡å¯¼ï¼Œèƒ½æ›´æ—©åœ°ä»å»å™ªè¿‡ç¨‹ä¸­è·³å‡ºå¸å¼•ç›†åœ°ã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†å¸å¼•ç›†åœ°çš„å­˜åœ¨ï¼Œä»¥åŠè®°å¿†ç°è±¡å‘ç”Ÿæ—¶çš„å„ç§æƒ…æ™¯ã€‚</li>
<li>æœ¬æ–‡çš„æ–¹æ³•æˆåŠŸå‡è½»äº†æ‰©æ•£æ¨¡å‹çš„è®°å¿†ç°è±¡ï¼Œç”Ÿæˆçš„å›¾åƒè´¨é‡é«˜ä¸”ä¸æ¡ä»¶æœºåˆ¶å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.16738v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.16738v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.16738v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.16738v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2411.16738v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Believing-is-Seeing-Unobserved-Object-Detection-using-Generative-Models"><a href="#Believing-is-Seeing-Unobserved-Object-Detection-using-Generative-Models" class="headerlink" title="Believing is Seeing: Unobserved Object Detection using Generative Models"></a>Believing is Seeing: Unobserved Object Detection using Generative Models</h2><p><strong>Authors:Subhransu S. Bhattacharjee, Dylan Campbell, Rahul Shome</strong></p>
<p>Can objects that are not visible in an image â€“ but are in the vicinity of the camera â€“ be detected? This study introduces the novel tasks of 2D, 2.5D and 3D unobserved object detection for predicting the location of nearby objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to address this task, including 2D and 3D diffusion models and vision-language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that capture different aspects of performance. Our empirical evaluation on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the use of generative models for the unobserved object detection task. </p>
<blockquote>
<p>åœ¨å›¾åƒä¸­æ— æ³•çœ‹åˆ°ä½†ä½äºç›¸æœºé™„è¿‘çš„å¯¹è±¡èƒ½è¢«æ£€æµ‹å‡ºæ¥å—ï¼Ÿæœ¬ç ”ç©¶ä»‹ç»äº†äºŒç»´ã€äºŒç»´åŠå’Œä¸‰ç»´éè§‚æµ‹å¯¹è±¡æ£€æµ‹çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨é¢„æµ‹è¢«é®æŒ¡æˆ–ä½äºå›¾åƒæ¡†å¤–çš„é™„è¿‘å¯¹è±¡çš„ä½ç½®ã€‚æˆ‘ä»¬é€‚åº”äº†å‡ ç§æœ€æ–°çš„é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹æ¥è§£å†³æ­¤ä»»åŠ¡ï¼ŒåŒ…æ‹¬äºŒç»´å’Œä¸‰ç»´æ‰©æ•£æ¨¡å‹ä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†è¿™äº›æ¨¡å‹å¯ä»¥ç”¨äºæ¨æ–­æœªç›´æ¥è§‚å¯Ÿåˆ°çš„å¯¹è±¡çš„å­˜åœ¨ã€‚ä¸ºäº†è¯„ä¼°æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€å¥—æŒ‡æ ‡ï¼Œä»¥æ•æ‰æ€§èƒ½çš„ä¸åŒæ–¹é¢ã€‚æˆ‘ä»¬åœ¨RealEstate10kå’ŒNYU Depth v2æ•°æ®é›†ä¸Šçš„å®¤å†…åœºæ™¯å®è¯è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œéè§‚æµ‹å¯¹è±¡æ£€æµ‹ä»»åŠ¡æ˜¯å¯è¡Œçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05869v3">PDF</a> IEEE&#x2F;CVF Computer Vision and Pattern Recognition 2025; 22 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†2Dã€2.5Då’Œ3Dæœªè§‚æµ‹ç‰©ä½“æ£€æµ‹çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨é¢„æµ‹å›¾åƒä¸­ä¸å¯è§ä½†ç›¸æœºé™„è¿‘ç‰©ä½“çš„ä½ç½®ã€‚ç ”ç©¶é‡‡ç”¨äº†å‡ ç§å…ˆè¿›çš„é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹æ¥å®Œæˆæ­¤ä»»åŠ¡ï¼ŒåŒ…æ‹¬2Då’Œ3Dæ‰©æ•£æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶è¯æ˜å®ƒä»¬å¯ç”¨äºæ¨æ–­æœªç›´æ¥è§‚å¯Ÿåˆ°çš„ç‰©ä½“çš„å­˜åœ¨ã€‚è¯¥ç ”ç©¶è¿˜æå‡ºäº†ç”¨äºè¯„ä¼°æ­¤ä»»åŠ¡æ€§èƒ½çš„æŒ‡æ ‡ï¼Œå¹¶åœ¨RealEstate10kå’ŒNYU Depth v2æ•°æ®é›†ä¸Šçš„å®¤å†…åœºæ™¯è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œè¯æ˜äº†ç”Ÿæˆæ¨¡å‹åœ¨æœªè§‚æµ‹ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†é¢„æµ‹å›¾åƒä¸­æœªè§‚æµ‹ç‰©ä½“ä½ç½®çš„æ–°ä»»åŠ¡ï¼Œè¿™äº›ç‰©ä½“å¯èƒ½å› é®æŒ¡æˆ–ä½äºå›¾åƒèŒƒå›´ä¹‹å¤–è€Œä¸å¯è§ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ¨æ–­æœªç›´æ¥è§‚å¯Ÿåˆ°çš„ç‰©ä½“çš„å­˜åœ¨ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°è¯¥ä»»åŠ¡æ€§èƒ½ï¼Œæå‡ºäº†å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ï¼Œä»ä¸åŒè§’åº¦è¡¡é‡æ¨¡å‹è¡¨ç°ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨RealEstate10kå’ŒNYU Depth v2æ•°æ®é›†è¿›è¡Œå®è¯è¯„ä¼°ï¼Œå±•ç¤ºäº†ç”Ÿæˆæ¨¡å‹åœ¨æœªè§‚æµ‹ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°åœ¨å®¤å†…åœºæ™¯ä¸­è¿›è¡Œæ­¤ç±»æ£€æµ‹å…·æœ‰ä¸€å®šçš„å¯è¡Œæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æ¨åŠ¨äº†ç”Ÿæˆæ¨¡å‹åœ¨æœªè§‚æµ‹ç‰©ä½“æ£€æµ‹é¢†åŸŸçš„è¿›ä¸€æ­¥åº”ç”¨å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2410.05869v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2410.05869v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2410.05869v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Diffusion Models/2410.05869v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5d6a422a9a5048b5eea40aafa607f945.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  MusicInfuser Making Video Diffusion Listen and Dance
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_NeRF/2503.13347v1/page_0_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  TriDF Triplane-Accelerated Density Fields for Few-Shot Remote Sensing   Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
