<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-03-18  HoloGest Decoupled Diffusion and Motion Priors for Generating   Holisticly Expressive Co-speech Gestures">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-13d4d5a743b2f3aea19e1c5b6fd00fb3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-18-更新"><a href="#2025-03-18-更新" class="headerlink" title="2025-03-18 更新"></a>2025-03-18 更新</h1><h2 id="HoloGest-Decoupled-Diffusion-and-Motion-Priors-for-Generating-Holisticly-Expressive-Co-speech-Gestures"><a href="#HoloGest-Decoupled-Diffusion-and-Motion-Priors-for-Generating-Holisticly-Expressive-Co-speech-Gestures" class="headerlink" title="HoloGest: Decoupled Diffusion and Motion Priors for Generating   Holisticly Expressive Co-speech Gestures"></a>HoloGest: Decoupled Diffusion and Motion Priors for Generating   Holisticly Expressive Co-speech Gestures</h2><p><strong>Authors:Yongkang Cheng, Shaoli Huang</strong></p>
<p>Animating virtual characters with holistic co-speech gestures is a challenging but critical task. Previous systems have primarily focused on the weak correlation between audio and gestures, leading to physically unnatural outcomes that degrade the user experience. To address this problem, we introduce HoleGest, a novel neural network framework based on decoupled diffusion and motion priors for the automatic generation of high-quality, expressive co-speech gestures. Our system leverages large-scale human motion datasets to learn a robust prior with low audio dependency and high motion reliance, enabling stable global motion and detailed finger movements. To improve the generation efficiency of diffusion-based models, we integrate implicit joint constraints with explicit geometric and conditional constraints, capturing complex motion distributions between large strides. This integration significantly enhances generation speed while maintaining high-quality motion. Furthermore, we design a shared embedding space for gesture-transcription text alignment, enabling the generation of semantically correct gesture actions. Extensive experiments and user feedback demonstrate the effectiveness and potential applications of our model, with our method achieving a level of realism close to the ground truth, providing an immersive user experience. Our code, model, and demo are are available at <a target="_blank" rel="noopener" href="https://cyk990422.github.io/HoloGest.github.io/">https://cyk990422.github.io/HoloGest.github.io/</a>. </p>
<blockquote>
<p>用整体的共语手势来驱动虚拟角色动画是一项充满挑战但至关重要的任务。以前的系统主要关注音频和手势之间微弱的关联，导致物理上不太自然的结果，从而降低了用户体验。为了解决这一问题，我们引入了HoleGest，这是一个基于解耦扩散和运动先验知识的新型神经网络框架，用于自动生成高质量、富有表现力的共语手势。我们的系统利用大规模人类运动数据集来学习一个稳健的先验模型，该模型具有较低的音频依赖性和较高的运动依赖性，能够实现稳定的全局运动和精细的手指动作。为了提高基于扩散模型的生成效率，我们将隐式关节约束与显式几何和条件约束相结合，在大步之间捕捉复杂的运动分布。这种结合显著提高了生成速度，同时保持了高质量的运动。此外，我们为手势转录文本对齐设计了一个共享嵌入空间，能够实现语义上正确的手势动作生成。大量的实验和用户反馈证明了我们模型的有效性和潜在应用，我们的方法达到了接近真实水平的逼真度，提供了沉浸式的用户体验。我们的代码、模型和演示作品可在<a target="_blank" rel="noopener" href="https://cyk990422.github.io/HoloGest.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://cyk990422.github.io/HoloGest.github.io/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13229v1">PDF</a> Accepted by 3DV 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于解耦扩散和运动先验的神经网络框架HoleGest，用于自动生成高质量、表达性强的协同语音手势。该系统利用大规模人类运动数据集学习稳健的先验，实现稳定的全局运动和精细的手指动作，解决了音频与手势之间弱相关性的问题，提高了虚拟角色动画的自然性和用户沉浸感。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HoleGest是一个基于解耦扩散和运动先验的神经网络框架，用于自动生成高质量协同语音手势。</li>
<li>系统利用大规模人类运动数据集学习稳健的先验，实现稳定的全局运动和精细的手指动作。</li>
<li>HoleGest通过整合隐式关节约束、显式几何约束和条件约束，提高了扩散模型的生成效率，同时保持了高质量的运动。</li>
<li>设计了共享嵌入空间用于手势转录文本对齐，生成语义上正确的手势动作。</li>
<li>系统解决了音频与手势之间弱相关性的问题，提高了虚拟角色动画的自然性。</li>
<li>广泛的实验和用户反馈证明了该模型的有效性和潜在应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13229">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-36357c191fa5d4f2d6b8db362ec6380a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a65cdd7a551641c3773a4d6e68fc22c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks"><a href="#FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks" class="headerlink" title="FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks"></a>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks</h2><p><strong>Authors:Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</strong></p>
<p>The current dominant approach for neural speech enhancement relies on purely-supervised deep learning using simulated pairs of far-field noisy-reverberant speech (mixtures) and clean speech. However, these trained models often exhibit limited generalizability to real-recorded mixtures. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a novel framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce a novel evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods. </p>
<blockquote>
<p>当前神经网络语音增强的主流方法主要依赖于使用模拟的远距离噪声混响语音（混合物）和清洁语音的配对进行纯监督深度学习。然而，这些训练好的模型对于真实记录的混合物往往表现出有限的泛化能力。为了解决这一问题，本研究致力于直接对真实混合物进行增强模型的训练。具体来说，我们重新审视单通道远距离到近距离语音增强（FNSE）任务，主要关注现实世界的数据特征，如低信噪比（SNR）、高混响和中高频衰减。我们提出了FNSE-SBGAN这一新型框架，它结合了基于Schrodinger Bridge（SB）的扩散模型与生成对抗网络（GANs）。我们的方法在各种指标和主观评估方面取得了最先进的性能，相较于远距离信号，字符错误率（CER）降低了高达14.58%。实验结果表明，FNSE-SBGAN保持了较高的主观质量，为现实世界的远距离语音增强建立了新的基准。此外，我们还引入了一种新的评估框架，利用时间频域中的矩阵秩分析，为模型性能提供了系统的见解，揭示了不同生成方法的优缺点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12936v1">PDF</a> 13 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于真实混合物的语音增强模型训练问题，提出了一种新的FNSE-SBGAN框架，结合了基于Schrodinger Bridge的扩散模型和生成对抗网络。该框架在各项指标和主观评价上取得了最优性能，相比远场信号降低了高达14.58%的字符错误率，同时保留了优质的主观音质，为真实远场语音增强设立了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前神经网络语音增强主要依赖纯监督深度学习，使用模拟的远场带噪语音和干净语音对进行训练，但模型在真实录制混合物上的泛化能力有限。</li>
<li>为解决此问题，本研究直接对真实混合物进行增强模型训练。</li>
<li>研究重点为单通道远场到近场语音增强任务，针对低信噪比、高回声和中高频衰减的真实世界数据。</li>
<li>提出了一种新的FNSE-SBGAN框架，结合了Schrodinger Bridge扩散模型和生成对抗网络，实现了各项最优性能。</li>
<li>FNSE-SBGAN框架显著降低了字符错误率，同时保持了优质的主观音质。</li>
<li>引入了一种新的评价框架，利用时频域矩阵秩分析，为模型性能提供了系统洞察，揭示了不同生成方法的优势和劣势。</li>
<li>研究结果为真实远场语音增强设立了新的基准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-110ffbb6afae5560bd36e2fcc0a27d66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28509a8d0e22b7bef313698b352f01c1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Prosody-Enhanced-Acoustic-Pre-training-and-Acoustic-Disentangled-Prosody-Adapting-for-Movie-Dubbing"><a href="#Prosody-Enhanced-Acoustic-Pre-training-and-Acoustic-Disentangled-Prosody-Adapting-for-Movie-Dubbing" class="headerlink" title="Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody   Adapting for Movie Dubbing"></a>Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody   Adapting for Movie Dubbing</h2><p><strong>Authors:Zhedong Zhang, Liang Li, Chenggang Yan, Chunshan Liu, Anton van den Hengel, Yuankai Qi</strong></p>
<p>Movie dubbing describes the process of transforming a script into speech that aligns temporally and emotionally with a given movie clip while exemplifying the speaker’s voice demonstrated in a short reference audio clip. This task demands the model bridge character performances and complicated prosody structures to build a high-quality video-synchronized dubbing track. The limited scale of movie dubbing datasets, along with the background noise inherent in audio data, hinder the acoustic modeling performance of trained models. To address these issues, we propose an acoustic-prosody disentangled two-stage method to achieve high-quality dubbing generation with precise prosody alignment. First, we propose a prosody-enhanced acoustic pre-training to develop robust acoustic modeling capabilities. Then, we freeze the pre-trained acoustic system and design a disentangled framework to model prosodic text features and dubbing style while maintaining acoustic quality. Additionally, we incorporate an in-domain emotion analysis module to reduce the impact of visual domain shifts across different movies, thereby enhancing emotion-prosody alignment. Extensive experiments show that our method performs favorably against the state-of-the-art models on two primary benchmarks. The demos are available at <a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/">https://zzdoog.github.io/ProDubber/</a>. </p>
<blockquote>
<p>电影配音是将剧本转化为与给定电影片段在时间和情感上相匹配的语音的过程，同时展示在简短参考音频片段中演示的说话人的声音。这项任务要求模型在角色表演和复杂的韵律结构之间建立联系，以构建高质量的与视频同步的配音轨道。电影配音数据集规模的有限性，以及音频数据所固有的背景噪音，阻碍了训练模型的声学建模性能。为了解决这些问题，我们提出了一种声学-韵律分解的两阶段方法，以实现高质量配音生成与精确韵律对齐。首先，我们提出一种韵律增强声学预训练方法，以发展稳健的声学建模能力。然后，我们冻结预训练的声学系统，设计一个分离的框架来建模韵律文本特征和配音风格，同时保持声学质量。此外，我们引入了一个领域内的情感分析模块，以减少不同电影之间视觉领域变化的影响，从而增强情感-韵律对齐。大量实验表明，我们的方法在两个主要基准测试上的表现优于最先进的模型。演示请访问：<a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/%E3%80%82">https://zzdoog.github.io/ProDubber/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12042v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了电影配音的过程，即将剧本转化为与电影片段相匹配的声音，同时展示参考音频片段中的说话人的声音。电影配音任务需要模型在角色表演和复杂的韵律结构之间建立联系，以创建高质量的与视频同步的配音轨道。针对电影配音数据集规模有限及音频数据中的背景噪音问题，提出了一个声学韵律解耦的两阶段方法，以实现高质量的配音生成和精确的节奏对齐。首先，通过提出韵律增强的声学预训练来开发稳健的声学建模能力。然后，冻结预训练的声学系统，设计一个解耦框架来建模文本特征、配音风格和保持声学质量。此外，还融入了领域内的情感分析模块，以减少不同电影之间视觉域变化的影响，从而提高情感韵律的对齐度。实验证明，该方法在两个主要基准测试上的表现均优于现有模型。演示地址：<a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>电影配音是将剧本转化为与电影片段相匹配的声音的过程，需考虑角色表演和复杂韵律结构。</li>
<li>现有的电影配音数据集规模有限，且音频数据中的背景噪音影响声学建模性能。</li>
<li>提出一个声学韵律解耦的两阶段方法，以实现高质量的配音生成和精确的节奏对齐。</li>
<li>引入韵律增强的声学预训练来增强模型的声学建模能力。</li>
<li>通过冻结预训练的声学系统并设计解耦框架，同时建模文本特征、配音风格和保持声学质量。</li>
<li>融入领域内的情感分析模块以减少不同电影视觉域变化的影响，提高情感韵律的对齐度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-41ae576be778ad1c16dc76f8bbb09676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53311922658be8ead1ffbba037090584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3614debffb60ad043d0ef7e07834a70.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MMS-LLaMA-Efficient-LLM-based-Audio-Visual-Speech-Recognition-with-Minimal-Multimodal-Speech-Tokens"><a href="#MMS-LLaMA-Efficient-LLM-based-Audio-Visual-Speech-Recognition-with-Minimal-Multimodal-Speech-Tokens" class="headerlink" title="MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with   Minimal Multimodal Speech Tokens"></a>MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with   Minimal Multimodal Speech Tokens</h2><p><strong>Authors:Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early av-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.74% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%. </p>
<blockquote>
<p>视听语音识别（AVSR）通过结合听觉和视觉信息，在嘈杂环境中实现了稳健的语音识别。然而，基于大型语言模型（LLM）的AVSR系统由于LLM处理视听语音的高时间分辨率而产生了较高的计算成本。在这项工作中，我们引入了一个高效的多模态语音LLM框架，该框架在保留基本语言内容的同时，最小化令牌长度。我们的方法采用早期av融合模块进行流线型特征集成，一个视听语音Q-Former，根据输入持续时间动态分配令牌，以及一种精细化的查询分配策略，配备一个语速预测器，根据每个音频样本的语速调整令牌分配。在LRS3数据集上的大量实验表明，我们的方法以0.74%的字错误率（WER）达到了最先进的性能，同时每秒仅使用3.5个令牌。此外，我们的方法不仅将令牌使用量减少了86%，与以前的多模态语音LLM框架相比，还提高了计算效率，减少了35.7%的浮点运算（FLOPs）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11315v1">PDF</a> The code and models are available   <a target="_blank" rel="noopener" href="https://github.com/JeongHun0716/MMS-LLaMA">https://github.com/JeongHun0716/MMS-LLaMA</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了音频视觉语音识别（AVSR）在噪声环境下的稳健语音识别技术，结合了听觉和视觉信息。针对大型语言模型（LLM）在AVSR系统中处理高时空分辨率音频视觉语音时的高计算成本问题，提出了一种高效的多模态语音LLM框架。该框架通过最小化令牌长度同时保留基本语言内容、采用早期av融合模块进行功能整合、音频视觉语音Q-Former动态分配令牌以及使用精细查询分配策略与语速预测器调整每个音频样本的令牌分配，以实现性能优化。在LRS3数据集上的实验表明，该方法在词错误率为0.74%的情况下实现了最先进的性能，并且每秒仅使用3.5个令牌。此外，该方法不仅将令牌使用量减少了86%，而且通过降低浮点运算次数提高了计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频视觉语音识别（AVSR）结合了听觉和视觉信息，提高了在噪声环境下的语音识别稳健性。</li>
<li>大型语言模型（LLM）处理高时空分辨率音频视觉语音时计算成本高昂。</li>
<li>高效多模态语音LLM框架旨在优化性能，减少计算成本。</li>
<li>该框架采用早期av融合、音频视觉语音Q-Former和查询分配策略与语速预测器的组合。</li>
<li>在LRS3数据集上的实验表明，该方法实现了先进的性能，词错误率低至0.74%。</li>
<li>与之前的多模态语音LLM框架相比，该方法显著减少了令牌使用量，并提高了计算效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-14a67d7a6fdf39339dafa56183355397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a1e8b4d32bb175890274e72787256d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d9bc42503fef211f8602bdd7878602a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13d4d5a743b2f3aea19e1c5b6fd00fb3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Joint-Training-And-Decoding-for-Multilingual-End-to-End-Simultaneous-Speech-Translation"><a href="#Joint-Training-And-Decoding-for-Multilingual-End-to-End-Simultaneous-Speech-Translation" class="headerlink" title="Joint Training And Decoding for Multilingual End-to-End Simultaneous   Speech Translation"></a>Joint Training And Decoding for Multilingual End-to-End Simultaneous   Speech Translation</h2><p><strong>Authors:Wuwei Huang, Renren Jin, Wen Zhang, Jian Luan, Bin Wang, Deyi Xiong</strong></p>
<p>Recent studies on end-to-end speech translation(ST) have facilitated the exploration of multilingual end-to-end ST and end-to-end simultaneous ST. In this paper, we investigate end-to-end simultaneous speech translation in a one-to-many multilingual setting which is closer to applications in real scenarios. We explore a separate decoder architecture and a unified architecture for joint synchronous training in this scenario. To further explore knowledge transfer across languages, we propose an asynchronous training strategy on the proposed unified decoder architecture. A multi-way aligned multilingual end-to-end ST dataset was curated as a benchmark testbed to evaluate our methods. Experimental results demonstrate the effectiveness of our models on the collected dataset. Our codes and data are available at: <a target="_blank" rel="noopener" href="https://github.com/XiaoMi/TED-MMST">https://github.com/XiaoMi/TED-MMST</a>. </p>
<blockquote>
<p>最近关于端到端语音翻译（ST）的研究促进了多语言端到端ST和端到端同步ST的探索。在本文中，我们在更接近实际应用场景的多种语言到多种语言的设置中，研究了端到端的实时语音翻译。我们在这个场景中探索了单独的解码器架构和联合同步训练的统一架构。为了进一步研究跨语言的知识转移，我们在统一的解码器架构上提出了异步训练策略。作为评估方法的基准测试平台，我们筛选了一个多路对齐的多语言端到端ST数据集。实验结果表明，我们的模型在收集的数据集上是有效的。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/XiaoMi/TED-MMST%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XiaoMi/TED-MMST上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11080v1">PDF</a> ICASSP 2023</p>
<p><strong>Summary</strong></p>
<p>本文研究了端到端的多语言同步语音识别翻译技术，在真实场景应用中更具有实际意义。探讨了分别解码架构和联合同步训练统一架构的使用，并提出了异步训练策略来提升跨语言知识迁移。研究使用多语种对齐的多语种端到端语音识别翻译数据集作为评估模型性能的基准测试平台。实验结果表明模型的有效性。相关代码和数据可在GitHub上获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了端到端的多语言同步语音识别翻译技术，适用于真实场景应用。</li>
<li>分别探讨了分别解码架构和统一架构在联合同步训练中的应用。</li>
<li>提出了异步训练策略来提升跨语言知识迁移。</li>
<li>使用多语种对齐的多语种端到端语音识别翻译数据集作为评估模型性能的基准测试平台。</li>
<li>实验结果证明了模型的有效性。</li>
<li>相关代码和数据可在GitHub上获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5b98c993a8617fdd44d0209e67687eab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c2e547eb5e640d49456173b4b61e713.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41aaa90d9e230b8f40ef56a14f386ea6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-506bf90957f546c1d53dc16f4aa57736.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EmoDiffusion-Enhancing-Emotional-3D-Facial-Animation-with-Latent-Diffusion-Models"><a href="#EmoDiffusion-Enhancing-Emotional-3D-Facial-Animation-with-Latent-Diffusion-Models" class="headerlink" title="EmoDiffusion: Enhancing Emotional 3D Facial Animation with Latent   Diffusion Models"></a>EmoDiffusion: Enhancing Emotional 3D Facial Animation with Latent   Diffusion Models</h2><p><strong>Authors:Yixuan Zhang, Qing Chang, Yuxi Wang, Guang Chen, Zhaoxiang Zhang, Junran Peng</strong></p>
<p>Speech-driven 3D facial animation seeks to produce lifelike facial expressions that are synchronized with the speech content and its emotional nuances, finding applications in various multimedia fields. However, previous methods often overlook emotional facial expressions or fail to disentangle them effectively from the speech content. To address these challenges, we present EmoDiffusion, a novel approach that disentangles different emotions in speech to generate rich 3D emotional facial expressions. Specifically, our method employs two Variational Autoencoders (VAEs) to separately generate the upper face region and mouth region, thereby learning a more refined representation of the facial sequence. Unlike traditional methods that use diffusion models to connect facial expression sequences with audio inputs, we perform the diffusion process in the latent space. Furthermore, we introduce an Emotion Adapter to evaluate upper face movements accurately. Given the paucity of 3D emotional talking face data in the animation industry, we capture facial expressions under the guidance of animation experts using LiveLinkFace on an iPhone. This effort results in the creation of an innovative 3D blendshape emotional talking face dataset (3D-BEF) used to train our network. Extensive experiments and perceptual evaluations validate the effectiveness of our approach, confirming its superiority in generating realistic and emotionally rich facial animations. </p>
<blockquote>
<p>语音驱动的3D面部动画旨在产生与语音内容及其情感细微差别同步的逼真面部表情，在多媒体领域具有广泛的应用。然而，传统方法往往会忽略情感面部表情或无法有效地从语音内容中将其分离出来。为了应对这些挑战，我们提出了EmoDiffusion这一新方法，它通过分解语音中的不同情感来生成丰富的3D情感面部表情。具体来说，我们的方法采用两个变分自动编码器（VAEs）分别生成面部上区域和嘴巴区域，从而学习更精细的面部表情序列表示。不同于传统方法使用扩散模型将面部表情序列与音频输入连接起来，我们在潜在空间执行扩散过程。此外，我们引入了一个情感适配器来准确评估面部上部的运动。鉴于动画行业缺乏3D情感对话面部数据，我们在动画专家的指导下使用iPhone上的LiveLinkFace捕获面部表情，成功创建了一个创新的3D混相情绪对话面部数据集（3D-BEF），用于训练我们的网络。大量的实验和感知评估验证了我们的方法的有效性，证实其在生成真实且情感丰富的面部动画方面的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11028v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了Speech-driven 3D面部动画面临的挑战及其应用于多媒体领域的价值。为解决以往方法忽视情感面部表情或无法有效从语音内容中分离情感的问题，提出一种名为EmoDiffusion的新方法。该方法利用变分自编码器（VAEs）分别生成面部上区和口部区域，并在潜在空间进行扩散过程，从而更精细地表示面部序列。此外，引入情感适配器准确评估面部上区的运动。为解决动画行业中缺乏3D情感说话面部数据的问题，使用LiveLinkFace在iPhone上采集面部表情数据，创建创新的3D blendshape情感说话面部数据集（3D-BEF）用于训练网络。实验和感知评估验证了该方法的优越性，能够在生成真实且情感丰富的面部动画方面表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech-driven 3D面部动画旨在实现与语音内容及其情感细微差别同步的逼真面部表情。</li>
<li>现有方法忽视了情感面部表情或无法有效地从语音内容中分离情感。</li>
<li>EmoDiffusion是一种新颖的方法，它通过变分自编码器（VAEs）生成面部上区和口部区域，以更精细的方式表示面部序列。</li>
<li>EmoDiffusion在潜在空间进行扩散过程，并引入情感适配器以更准确地评估面部上区的运动。</li>
<li>为训练网络，创建了3D blendshape情感说话面部数据集（3D-BEF），该数据集是通过使用LiveLinkFace在iPhone上采集面部表情数据制成的。</li>
<li>广泛实验证明了EmoDiffusion方法的有效性，能够生成真实且情感丰富的面部动画。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11028">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-58e853a905eceb5428ab9a863737421b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f254f40f8a1ce87ba5cea036d281732.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SPES-Spectrogram-Perturbation-for-Explainable-Speech-to-Text-Generation"><a href="#SPES-Spectrogram-Perturbation-for-Explainable-Speech-to-Text-Generation" class="headerlink" title="SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation"></a>SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation</h2><p><strong>Authors:Dennis Fucci, Marco Gaido, Beatrice Savoldi, Matteo Negri, Mauro Cettolo, Luisa Bentivogli</strong></p>
<p>Spurred by the demand for interpretable models, research on eXplainable AI for language technologies has experienced significant growth, with feature attribution methods emerging as a cornerstone of this progress. While prior work in NLP explored such methods for classification tasks and textual applications, explainability intersecting generation and speech is lagging, with existing techniques failing to account for the autoregressive nature of state-of-the-art models and to provide fine-grained, phonetically meaningful explanations. We address this gap by introducing Spectrogram Perturbation for Explainable Speech-to-text Generation (SPES), a feature attribution technique applicable to sequence generation tasks with autoregressive models. SPES provides explanations for each predicted token based on both the input spectrogram and the previously generated tokens. Extensive evaluation on speech recognition and translation demonstrates that SPES generates explanations that are faithful and plausible to humans. </p>
<blockquote>
<p>受到可解释模型需求的影响，语言技术中的可解释人工智能研究经历了显著增长，特征归因方法已成为这一进步的核心。虽然自然语言处理中的先前工作已经探索了分类任务和文本应用中的这些方法，但解释能力与生成和语音的交集滞后，现有技术未能考虑到最先进模型的自回归性质，也无法提供精细粒度和音素意义上的解释。我们通过引入用于解释语音到文本生成的频谱扰动（SPES）来解决这一差距，SPES是一种适用于自回归模型的序列生成任务的特征归因技术。SPES基于输入频谱图和先前生成的标记为每个预测标记提供解释。在语音识别和翻译方面的广泛评估表明，SPES生成的解释对人类来说是忠实和可信的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01710v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了可解释的语音识别技术的最新进展。随着对可解释模型的需求增长，解释性人工智能在语言技术领域的研究已经取得了显著进展，特征归因方法已成为这一进展的核心。尽管先前的工作在NLP中探索了这些方法用于分类任务和文本应用，但解释性与生成和语音的交叉部分仍然滞后。为解决这一差距，本文引入了Spectrogram Perturbation for Explainable Speech-to-text Generation（SPES），这是一种适用于具有自回归模型的序列生成任务的特征归因技术。SPES基于输入频谱图和先前生成的标记为每个预测标记提供解释。在语音识别和翻译方面的广泛评估表明，SPES生成的解释对人类来说是忠实和可信的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>解释性人工智能在语言技术领域的研究正在增长，特征归因方法已成为核心。</li>
<li>在NLP中，尽管分类任务和文本应用的解释性研究已经取得进展，但生成任务和语音方面的解释性研究仍然滞后。</li>
<li>现有技术未能充分考虑到先进模型的自回归性质，无法提供精细粒度的、具有语音意义的解释。</li>
<li>引入Spectrogram Perturbation for Explainable Speech-to-text Generation（SPES）以填补空白。</li>
<li>SPES是一种特征归因技术，适用于具有自回归模型的序列生成任务。</li>
<li>SPES为每个预测标记提供基于输入频谱图和先前生成标记的解释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19ec1ad3e82783d1339087f9e2f6d756.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fad9c52f77e2a353b861740679bde4d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Personalized-Speech-Emotion-Recognition-in-Human-Robot-Interaction-using-Vision-Transformers"><a href="#Personalized-Speech-Emotion-Recognition-in-Human-Robot-Interaction-using-Vision-Transformers" class="headerlink" title="Personalized Speech Emotion Recognition in Human-Robot Interaction using   Vision Transformers"></a>Personalized Speech Emotion Recognition in Human-Robot Interaction using   Vision Transformers</h2><p><strong>Authors:Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa</strong></p>
<p>Emotions are an essential element in verbal communication, so understanding individuals’ affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT&#x2F;BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs. </p>
<blockquote>
<p>情感是口头交流的重要组成部分，因此在人机互动（HRI）中理解个体的情感变得至关重要。本文研究了视觉转换器模型在人机互动中的语音情感识别（SER）应用，重点关注使用ViT（视觉转换器）和BEiT（图像转换器的BERT预训练方法）管道。研究的重点是通过对基准数据集进行微调并利用集成方法，使SER模型适应个体语音特征。为此，我们从与NAO机器人进行伪自然对话的不同人类受试者中收集音频数据。然后我们对基于ViT和BEiT的模型进行微调，并在来自参与者的未见语音样本上测试这些模型。结果表明，在基准数据集上微调视觉转换器，然后使用已经微调过的模型或集成ViT&#x2F;BEiT模型，在识别四种主要情绪（中性、快乐、悲伤和愤怒）时，针对个人的分类准确率最高，与微调普通ViTs或BEITs相比效果更佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10687v3">PDF</a> This work has been accepted for the IEEE Robotics and Automation   Letters (RA-L)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了将视觉转换器模型应用于人机互动中的语音情感识别。研究采用ViT和BEiT模型，通过对基准数据集进行微调并利用集成方法，实现对个体语音特性的通用化。通过对与NAO机器人进行伪自然对话的音频数据进行收集、微调模型并测试，发现对四种主要情绪——中性、快乐、悲伤和愤怒——的识别中，相较于微调普通的ViT或BEiT模型，使用已微调过的模型或集成ViT&#x2F;BEiT模型可获得更高的个体分类准确率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感在人际交流中的重要性，特别是在人机互动中理解个体的情感变得至关重要。</li>
<li>研究采用视觉转换器模型（ViT和BEiT）进行语音情感识别。</li>
<li>通过微调基准数据集上的模型，实现对个体语音特性的通用化。</li>
<li>集成方法可以提高语音情感识别的准确率。</li>
<li>使用了与NAO机器人进行伪自然对话的音频数据来测试模型。</li>
<li>研究发现，使用已微调过的模型或集成ViT&#x2F;BEiT模型在识别四种主要情绪时表现更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d2724482341270ae8a08431985f7aa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1df52457354ebe35a7c2d95f86f95549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6718c798826cf4e5e49657d6f619c0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6817f7d46662b07115c15784608602d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac915e3876c60b287ac49530c98cf191.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef02565806f4dd911d10d5d0a1222d0c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-based-speaker-diarization-correction-A-generalizable-approach"><a href="#LLM-based-speaker-diarization-correction-A-generalizable-approach" class="headerlink" title="LLM-based speaker diarization correction: A generalizable approach"></a>LLM-based speaker diarization correction: A generalizable approach</h2><p><strong>Authors:Georgios Efstathiadis, Vijay Yadav, Anzar Abbas</strong></p>
<p>Speaker diarization is necessary for interpreting conversations transcribed using automated speech recognition (ASR) tools. Despite significant developments in diarization methods, diarization accuracy remains an issue. Here, we investigate the use of large language models (LLMs) for diarization correction as a post-processing step. LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations. The ability of the models to improve diarization accuracy in a holdout dataset from the Fisher corpus as well as an independent dataset was measured. We report that fine-tuned LLMs can markedly improve diarization accuracy. However, model performance is constrained to transcripts produced using the same ASR tool as the transcripts used for fine-tuning, limiting generalizability. To address this constraint, an ensemble model was developed by combining weights from three separate models, each fine-tuned using transcripts from a different ASR tool. The ensemble model demonstrated better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable. We have made the weights of these models publicly available on HuggingFace at <a target="_blank" rel="noopener" href="https://huggingface.co/bklynhlth">https://huggingface.co/bklynhlth</a>. </p>
<blockquote>
<p>说话人识别是对使用自动语音识别（ASR）工具转录的对话进行解释的必要步骤。尽管识别方法有了显著的发展，但识别准确性仍然是一个问题。在这里，我们研究了大型语言模型（LLM）在作为后处理步骤的识别校正中的应用。这些LLM使用Fisher语料库（一个包含大量转录对话的数据集）进行了微调。我们测量了这些模型在提高Fisher语料库中的保留数据集以及独立数据集的识别准确性方面的能力。我们报告说，经过精细调整的LLM可以显著提高识别准确性。然而，模型的性能仅限于使用与微调时使用的相同ASR工具产生的转录本，从而限制了其通用性。为了解决这一限制，我们通过结合三个单独模型的权重，开发了一个集成模型，每个模型都使用来自不同ASR工具的转录本进行微调。集成模型的表现优于每个ASR特定模型的表现，这表明可能实现了通用和独立于ASR的方法。我们已在HuggingFace上公开了这些模型的权重：<a target="_blank" rel="noopener" href="https://huggingface.co/bklynhlth">https://huggingface.co/bklynhlth</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04927v3">PDF</a> </p>
<p><strong>Summary</strong><br>     使用大型语言模型（LLM）进行语音转文本的后处理步骤可以显著改善分置准确度。在研究中，通过对Fisher语料库的大型数据集进行微调训练LLM模型后，对其在Fisher语料库中的独立数据集和其他独立数据集上的表现进行了评估。报告指出，精细训练的LLM可以显著提高分置准确性，但模型性能仅限于使用与微调过程中相同的语音识别工具生成的转录文本，限制了其泛化能力。为解决此限制，通过结合三个独立模型的权重开发了集成模型，每个模型针对不同的语音识别工具进行微调。集成模型表现出优于单一ASR特定模型的性能，这表明可以实现一种通用且独立于ASR的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）可以用于语音分置修正后处理，能显著提高分置准确度。</li>
<li>LLM通过微调训练来适应特定的语音数据集。</li>
<li>LLM的模型性能受限于与微调过程中使用的语音识别工具相同的转录文本。</li>
<li>集成模型通过将不同ASR工具生成的转录文本合并来优化模型性能。</li>
<li>集成模型表现优于单一ASR特定模型。</li>
<li>集成模型的权重已经公开提供在HuggingFace平台上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04927">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d909a192b4904c0193f8eac3aeb2f13b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20829a7d4959acd9a01dbc1a8047a7a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347d68cb6424d9e08e3906e5857ab652.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="P-SpikeSSM-Harnessing-Probabilistic-Spiking-State-Space-Models-for-Long-Range-Dependency-Tasks"><a href="#P-SpikeSSM-Harnessing-Probabilistic-Spiking-State-Space-Models-for-Long-Range-Dependency-Tasks" class="headerlink" title="P-SpikeSSM: Harnessing Probabilistic Spiking State Space Models for   Long-Range Dependency Tasks"></a>P-SpikeSSM: Harnessing Probabilistic Spiking State Space Models for   Long-Range Dependency Tasks</h2><p><strong>Authors:Malyaban Bal, Abhronil Sengupta</strong></p>
<p>Spiking neural networks (SNNs) are posited as a computationally efficient and biologically plausible alternative to conventional neural architectures, with their core computational framework primarily using the leaky integrate-and-fire (LIF) neuron model. However, the limited hidden state representation of LIF neurons, characterized by a scalar membrane potential, and sequential spike generation process, poses challenges for effectively developing scalable spiking models to address long-range dependencies in sequence learning tasks. In this study, we develop a scalable probabilistic spiking learning framework for long-range dependency tasks leveraging the fundamentals of state space models. Unlike LIF neurons that rely on the deterministic Heaviside function for a sequential process of spike generation, we introduce a SpikeSampler layer that samples spikes stochastically based on an SSM-based neuronal model while allowing parallel computations. To address non-differentiability of the spiking operation and enable effective training, we also propose a surrogate function tailored for the stochastic nature of the SpikeSampler layer. To enhance inter-neuron communication, we introduce the SpikeMixer block, which integrates spikes from neuron populations in each layer. This is followed by a ClampFuse layer, incorporating a residual connection to capture complex dependencies, enabling scalability of the model. Our models attain state-of-the-art performance among SNN models across diverse long-range dependency tasks, encompassing the Long Range Arena benchmark, permuted sequential MNIST, and the Speech Command dataset and demonstrate sparse spiking pattern highlighting its computational efficiency. </p>
<blockquote>
<p>脉冲神经网络（SNNs）被提出作为传统神经网络架构的计算效率高且生物上合理的替代方案，其主要的计算框架主要使用泄漏积分和发射（LIF）神经元模型。然而，LIF神经元的隐藏状态表示有限，其特征在于标量膜电位和顺序脉冲生成过程，这为开发可扩展的脉冲模型以处理序列学习任务中的长距离依赖关系带来了挑战。在这项研究中，我们针对长距离依赖任务开发了一个可扩展的概率脉冲学习框架，该框架利用状态空间模型的基本原理。不同于依赖确定性海维赛德函数进行脉冲生成序列过程的LIF神经元，我们引入了SpikeSampler层，该层基于SSM神经元模型的随机性进行脉冲采样，同时允许并行计算。为了解决脉冲操作的不可微性和实现有效的训练，我们还为SpikeSampler层的随机性量身定制了替代函数。为了增强神经元之间的通信，我们引入了SpikeMixer块，该块整合了每层神经元群体的脉冲。随后是ClampFuse层，它结合了残差连接以捕捉复杂的依赖关系，使模型的可扩展性得以增强。我们的模型在不同类型的长距离依赖任务中达到了脉冲神经网络模型的最新性能水平，包括Long Range Arena基准测试、排列顺序的MNIST和语音命令数据集，并显示出稀疏的脉冲模式，突显了其计算效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02923v5">PDF</a> Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于状态空间模型（SSM）的概率性脉冲学习框架，用于解决序列学习中的长程依赖问题。该研究引入了SpikeSampler层以随机采样脉冲，并利用SSM神经元模型实现并行计算。为解决脉冲操作不可微分的问题，研究提出了针对SpikeSampler层随机性的替代函数。此外，还引入了SpikeMixer块以增强神经元间的通信，并通过ClampFuse层结合残差连接捕捉复杂依赖关系，实现模型的扩展性。该模型在多种长程依赖任务上达到了脉冲神经网络模型的最新性能水平，包括Long Range Arena基准测试、顺序MNIST和语音命令数据集，并展示了其计算效率高的稀疏脉冲模式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SNNs作为一种计算高效且生物上可行的神经网络架构，面临长程依赖问题的挑战。</li>
<li>引入基于状态空间模型的SpikeSampler层以随机采样脉冲并实现并行计算。</li>
<li>提出替代函数以解决脉冲操作不可微分的问题，确保有效的训练。</li>
<li>SpikeMixer块增强了神经元间的通信，而ClampFuse层则通过结合残差连接捕捉复杂依赖关系。</li>
<li>模型在多种长程依赖任务上实现了最新性能水平，包括Long Range Arena基准测试等。</li>
<li>模型展示了其计算效率高的稀疏脉冲模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02923">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a55b665fd2de54e37283260742d81573.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c04c92946710cf83ec7b358e012aa86b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c7f6ef84b4251bd688f1f479c3887a61.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-03-18  SyncDiff Diffusion-based Talking Head Synthesis with Bottlenecked   Temporal Visual Prior for Improved Synchronization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62d448d16cf04bb54473cb20f5e239c1.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-03-18  Efficient Multimodal 3D Object Detector via Instance-Level Contrastive   Distillation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23251k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
