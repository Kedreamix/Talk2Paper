<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  HoloGest Decoupled Diffusion and Motion Priors for Generating   Holisticly Expressive Co-speech Gestures">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-13d4d5a743b2f3aea19e1c5b6fd00fb3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    39 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-18-æ›´æ–°"><a href="#2025-03-18-æ›´æ–°" class="headerlink" title="2025-03-18 æ›´æ–°"></a>2025-03-18 æ›´æ–°</h1><h2 id="HoloGest-Decoupled-Diffusion-and-Motion-Priors-for-Generating-Holisticly-Expressive-Co-speech-Gestures"><a href="#HoloGest-Decoupled-Diffusion-and-Motion-Priors-for-Generating-Holisticly-Expressive-Co-speech-Gestures" class="headerlink" title="HoloGest: Decoupled Diffusion and Motion Priors for Generating   Holisticly Expressive Co-speech Gestures"></a>HoloGest: Decoupled Diffusion and Motion Priors for Generating   Holisticly Expressive Co-speech Gestures</h2><p><strong>Authors:Yongkang Cheng, Shaoli Huang</strong></p>
<p>Animating virtual characters with holistic co-speech gestures is a challenging but critical task. Previous systems have primarily focused on the weak correlation between audio and gestures, leading to physically unnatural outcomes that degrade the user experience. To address this problem, we introduce HoleGest, a novel neural network framework based on decoupled diffusion and motion priors for the automatic generation of high-quality, expressive co-speech gestures. Our system leverages large-scale human motion datasets to learn a robust prior with low audio dependency and high motion reliance, enabling stable global motion and detailed finger movements. To improve the generation efficiency of diffusion-based models, we integrate implicit joint constraints with explicit geometric and conditional constraints, capturing complex motion distributions between large strides. This integration significantly enhances generation speed while maintaining high-quality motion. Furthermore, we design a shared embedding space for gesture-transcription text alignment, enabling the generation of semantically correct gesture actions. Extensive experiments and user feedback demonstrate the effectiveness and potential applications of our model, with our method achieving a level of realism close to the ground truth, providing an immersive user experience. Our code, model, and demo are are available at <a target="_blank" rel="noopener" href="https://cyk990422.github.io/HoloGest.github.io/">https://cyk990422.github.io/HoloGest.github.io/</a>. </p>
<blockquote>
<p>ç”¨æ•´ä½“çš„å…±è¯­æ‰‹åŠ¿æ¥é©±åŠ¨è™šæ‹Ÿè§’è‰²åŠ¨ç”»æ˜¯ä¸€é¡¹å……æ»¡æŒ‘æˆ˜ä½†è‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚ä»¥å‰çš„ç³»ç»Ÿä¸»è¦å…³æ³¨éŸ³é¢‘å’Œæ‰‹åŠ¿ä¹‹é—´å¾®å¼±çš„å…³è”ï¼Œå¯¼è‡´ç‰©ç†ä¸Šä¸å¤ªè‡ªç„¶çš„ç»“æœï¼Œä»è€Œé™ä½äº†ç”¨æˆ·ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†HoleGestï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§£è€¦æ‰©æ•£å’Œè¿åŠ¨å…ˆéªŒçŸ¥è¯†çš„æ–°å‹ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€å¯Œæœ‰è¡¨ç°åŠ›çš„å…±è¯­æ‰‹åŠ¿ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåˆ©ç”¨å¤§è§„æ¨¡äººç±»è¿åŠ¨æ•°æ®é›†æ¥å­¦ä¹ ä¸€ä¸ªç¨³å¥çš„å…ˆéªŒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰è¾ƒä½çš„éŸ³é¢‘ä¾èµ–æ€§å’Œè¾ƒé«˜çš„è¿åŠ¨ä¾èµ–æ€§ï¼Œèƒ½å¤Ÿå®ç°ç¨³å®šçš„å…¨å±€è¿åŠ¨å’Œç²¾ç»†çš„æ‰‹æŒ‡åŠ¨ä½œã€‚ä¸ºäº†æé«˜åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ•ˆç‡ï¼Œæˆ‘ä»¬å°†éšå¼å…³èŠ‚çº¦æŸä¸æ˜¾å¼å‡ ä½•å’Œæ¡ä»¶çº¦æŸç›¸ç»“åˆï¼Œåœ¨å¤§æ­¥ä¹‹é—´æ•æ‰å¤æ‚çš„è¿åŠ¨åˆ†å¸ƒã€‚è¿™ç§ç»“åˆæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¿åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºæ‰‹åŠ¿è½¬å½•æ–‡æœ¬å¯¹é½è®¾è®¡äº†ä¸€ä¸ªå…±äº«åµŒå…¥ç©ºé—´ï¼Œèƒ½å¤Ÿå®ç°è¯­ä¹‰ä¸Šæ­£ç¡®çš„æ‰‹åŠ¿åŠ¨ä½œç”Ÿæˆã€‚å¤§é‡çš„å®éªŒå’Œç”¨æˆ·åé¦ˆè¯æ˜äº†æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ½œåœ¨åº”ç”¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æ¥è¿‘çœŸå®æ°´å¹³çš„é€¼çœŸåº¦ï¼Œæä¾›äº†æ²‰æµ¸å¼çš„ç”¨æˆ·ä½“éªŒã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ¼”ç¤ºä½œå“å¯åœ¨<a target="_blank" rel="noopener" href="https://cyk990422.github.io/HoloGest.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://cyk990422.github.io/HoloGest.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13229v1">PDF</a> Accepted by 3DV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè§£è€¦æ‰©æ•£å’Œè¿åŠ¨å…ˆéªŒçš„ç¥ç»ç½‘ç»œæ¡†æ¶HoleGestï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€è¡¨è¾¾æ€§å¼ºçš„ååŒè¯­éŸ³æ‰‹åŠ¿ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è§„æ¨¡äººç±»è¿åŠ¨æ•°æ®é›†å­¦ä¹ ç¨³å¥çš„å…ˆéªŒï¼Œå®ç°ç¨³å®šçš„å…¨å±€è¿åŠ¨å’Œç²¾ç»†çš„æ‰‹æŒ‡åŠ¨ä½œï¼Œè§£å†³äº†éŸ³é¢‘ä¸æ‰‹åŠ¿ä¹‹é—´å¼±ç›¸å…³æ€§çš„é—®é¢˜ï¼Œæé«˜äº†è™šæ‹Ÿè§’è‰²åŠ¨ç”»çš„è‡ªç„¶æ€§å’Œç”¨æˆ·æ²‰æµ¸æ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HoleGestæ˜¯ä¸€ä¸ªåŸºäºè§£è€¦æ‰©æ•£å’Œè¿åŠ¨å…ˆéªŒçš„ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ååŒè¯­éŸ³æ‰‹åŠ¿ã€‚</li>
<li>ç³»ç»Ÿåˆ©ç”¨å¤§è§„æ¨¡äººç±»è¿åŠ¨æ•°æ®é›†å­¦ä¹ ç¨³å¥çš„å…ˆéªŒï¼Œå®ç°ç¨³å®šçš„å…¨å±€è¿åŠ¨å’Œç²¾ç»†çš„æ‰‹æŒ‡åŠ¨ä½œã€‚</li>
<li>HoleGesté€šè¿‡æ•´åˆéšå¼å…³èŠ‚çº¦æŸã€æ˜¾å¼å‡ ä½•çº¦æŸå’Œæ¡ä»¶çº¦æŸï¼Œæé«˜äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¿åŠ¨ã€‚</li>
<li>è®¾è®¡äº†å…±äº«åµŒå…¥ç©ºé—´ç”¨äºæ‰‹åŠ¿è½¬å½•æ–‡æœ¬å¯¹é½ï¼Œç”Ÿæˆè¯­ä¹‰ä¸Šæ­£ç¡®çš„æ‰‹åŠ¿åŠ¨ä½œã€‚</li>
<li>ç³»ç»Ÿè§£å†³äº†éŸ³é¢‘ä¸æ‰‹åŠ¿ä¹‹é—´å¼±ç›¸å…³æ€§çš„é—®é¢˜ï¼Œæé«˜äº†è™šæ‹Ÿè§’è‰²åŠ¨ç”»çš„è‡ªç„¶æ€§ã€‚</li>
<li>å¹¿æ³›çš„å®éªŒå’Œç”¨æˆ·åé¦ˆè¯æ˜äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ½œåœ¨åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-36357c191fa5d4f2d6b8db362ec6380a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a65cdd7a551641c3773a4d6e68fc22c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks"><a href="#FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks" class="headerlink" title="FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks"></a>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks</h2><p><strong>Authors:Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</strong></p>
<p>The current dominant approach for neural speech enhancement relies on purely-supervised deep learning using simulated pairs of far-field noisy-reverberant speech (mixtures) and clean speech. However, these trained models often exhibit limited generalizability to real-recorded mixtures. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a novel framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce a novel evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods. </p>
<blockquote>
<p>å½“å‰ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºçš„ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºä½¿ç”¨æ¨¡æ‹Ÿçš„è¿œè·ç¦»å™ªå£°æ··å“è¯­éŸ³ï¼ˆæ··åˆç‰©ï¼‰å’Œæ¸…æ´è¯­éŸ³çš„é…å¯¹è¿›è¡Œçº¯ç›‘ç£æ·±åº¦å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹äºçœŸå®è®°å½•çš„æ··åˆç‰©å¾€å¾€è¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶è‡´åŠ›äºç›´æ¥å¯¹çœŸå®æ··åˆç‰©è¿›è¡Œå¢å¼ºæ¨¡å‹çš„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†å•é€šé“è¿œè·ç¦»åˆ°è¿‘è·ç¦»è¯­éŸ³å¢å¼ºï¼ˆFNSEï¼‰ä»»åŠ¡ï¼Œä¸»è¦å…³æ³¨ç°å®ä¸–ç•Œçš„æ•°æ®ç‰¹å¾ï¼Œå¦‚ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€é«˜æ··å“å’Œä¸­é«˜é¢‘è¡°å‡ã€‚æˆ‘ä»¬æå‡ºäº†FNSE-SBGANè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†åŸºäºSchrodinger Bridgeï¼ˆSBï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºè¿œè·ç¦»ä¿¡å·ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰é™ä½äº†é«˜è¾¾14.58%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFNSE-SBGANä¿æŒäº†è¾ƒé«˜çš„ä¸»è§‚è´¨é‡ï¼Œä¸ºç°å®ä¸–ç•Œçš„è¿œè·ç¦»è¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨æ—¶é—´é¢‘åŸŸä¸­çš„çŸ©é˜µç§©åˆ†æï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†ç³»ç»Ÿçš„è§è§£ï¼Œæ­ç¤ºäº†ä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12936v1">PDF</a> 13 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºçœŸå®æ··åˆç‰©çš„è¯­éŸ³å¢å¼ºæ¨¡å‹è®­ç»ƒé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„FNSE-SBGANæ¡†æ¶ï¼Œç»“åˆäº†åŸºäºSchrodinger Bridgeçš„æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚è¯¥æ¡†æ¶åœ¨å„é¡¹æŒ‡æ ‡å’Œä¸»è§‚è¯„ä»·ä¸Šå–å¾—äº†æœ€ä¼˜æ€§èƒ½ï¼Œç›¸æ¯”è¿œåœºä¿¡å·é™ä½äº†é«˜è¾¾14.58%çš„å­—ç¬¦é”™è¯¯ç‡ï¼ŒåŒæ—¶ä¿ç•™äº†ä¼˜è´¨çš„ä¸»è§‚éŸ³è´¨ï¼Œä¸ºçœŸå®è¿œåœºè¯­éŸ³å¢å¼ºè®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºä¸»è¦ä¾èµ–çº¯ç›‘ç£æ·±åº¦å­¦ä¹ ï¼Œä½¿ç”¨æ¨¡æ‹Ÿçš„è¿œåœºå¸¦å™ªè¯­éŸ³å’Œå¹²å‡€è¯­éŸ³å¯¹è¿›è¡Œè®­ç»ƒï¼Œä½†æ¨¡å‹åœ¨çœŸå®å½•åˆ¶æ··åˆç‰©ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶ç›´æ¥å¯¹çœŸå®æ··åˆç‰©è¿›è¡Œå¢å¼ºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>ç ”ç©¶é‡ç‚¹ä¸ºå•é€šé“è¿œåœºåˆ°è¿‘åœºè¯­éŸ³å¢å¼ºä»»åŠ¡ï¼Œé’ˆå¯¹ä½ä¿¡å™ªæ¯”ã€é«˜å›å£°å’Œä¸­é«˜é¢‘è¡°å‡çš„çœŸå®ä¸–ç•Œæ•°æ®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„FNSE-SBGANæ¡†æ¶ï¼Œç»“åˆäº†Schrodinger Bridgeæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œå®ç°äº†å„é¡¹æœ€ä¼˜æ€§èƒ½ã€‚</li>
<li>FNSE-SBGANæ¡†æ¶æ˜¾è‘—é™ä½äº†å­—ç¬¦é”™è¯¯ç‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜è´¨çš„ä¸»è§‚éŸ³è´¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶ï¼Œåˆ©ç”¨æ—¶é¢‘åŸŸçŸ©é˜µç§©åˆ†æï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†ç³»ç»Ÿæ´å¯Ÿï¼Œæ­ç¤ºäº†ä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚</li>
<li>ç ”ç©¶ç»“æœä¸ºçœŸå®è¿œåœºè¯­éŸ³å¢å¼ºè®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-110ffbb6afae5560bd36e2fcc0a27d66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28509a8d0e22b7bef313698b352f01c1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Prosody-Enhanced-Acoustic-Pre-training-and-Acoustic-Disentangled-Prosody-Adapting-for-Movie-Dubbing"><a href="#Prosody-Enhanced-Acoustic-Pre-training-and-Acoustic-Disentangled-Prosody-Adapting-for-Movie-Dubbing" class="headerlink" title="Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody   Adapting for Movie Dubbing"></a>Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody   Adapting for Movie Dubbing</h2><p><strong>Authors:Zhedong Zhang, Liang Li, Chenggang Yan, Chunshan Liu, Anton van den Hengel, Yuankai Qi</strong></p>
<p>Movie dubbing describes the process of transforming a script into speech that aligns temporally and emotionally with a given movie clip while exemplifying the speakerâ€™s voice demonstrated in a short reference audio clip. This task demands the model bridge character performances and complicated prosody structures to build a high-quality video-synchronized dubbing track. The limited scale of movie dubbing datasets, along with the background noise inherent in audio data, hinder the acoustic modeling performance of trained models. To address these issues, we propose an acoustic-prosody disentangled two-stage method to achieve high-quality dubbing generation with precise prosody alignment. First, we propose a prosody-enhanced acoustic pre-training to develop robust acoustic modeling capabilities. Then, we freeze the pre-trained acoustic system and design a disentangled framework to model prosodic text features and dubbing style while maintaining acoustic quality. Additionally, we incorporate an in-domain emotion analysis module to reduce the impact of visual domain shifts across different movies, thereby enhancing emotion-prosody alignment. Extensive experiments show that our method performs favorably against the state-of-the-art models on two primary benchmarks. The demos are available at <a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/">https://zzdoog.github.io/ProDubber/</a>. </p>
<blockquote>
<p>ç”µå½±é…éŸ³æ˜¯å°†å‰§æœ¬è½¬åŒ–ä¸ºä¸ç»™å®šç”µå½±ç‰‡æ®µåœ¨æ—¶é—´å’Œæƒ…æ„Ÿä¸Šç›¸åŒ¹é…çš„è¯­éŸ³çš„è¿‡ç¨‹ï¼ŒåŒæ—¶å±•ç¤ºåœ¨ç®€çŸ­å‚è€ƒéŸ³é¢‘ç‰‡æ®µä¸­æ¼”ç¤ºçš„è¯´è¯äººçš„å£°éŸ³ã€‚è¿™é¡¹ä»»åŠ¡è¦æ±‚æ¨¡å‹åœ¨è§’è‰²è¡¨æ¼”å’Œå¤æ‚çš„éŸµå¾‹ç»“æ„ä¹‹é—´å»ºç«‹è”ç³»ï¼Œä»¥æ„å»ºé«˜è´¨é‡çš„ä¸è§†é¢‘åŒæ­¥çš„é…éŸ³è½¨é“ã€‚ç”µå½±é…éŸ³æ•°æ®é›†è§„æ¨¡çš„æœ‰é™æ€§ï¼Œä»¥åŠéŸ³é¢‘æ•°æ®æ‰€å›ºæœ‰çš„èƒŒæ™¯å™ªéŸ³ï¼Œé˜»ç¢äº†è®­ç»ƒæ¨¡å‹çš„å£°å­¦å»ºæ¨¡æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å£°å­¦-éŸµå¾‹åˆ†è§£çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œä»¥å®ç°é«˜è´¨é‡é…éŸ³ç”Ÿæˆä¸ç²¾ç¡®éŸµå¾‹å¯¹é½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºä¸€ç§éŸµå¾‹å¢å¼ºå£°å­¦é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥å‘å±•ç¨³å¥çš„å£°å­¦å»ºæ¨¡èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬å†»ç»“é¢„è®­ç»ƒçš„å£°å­¦ç³»ç»Ÿï¼Œè®¾è®¡ä¸€ä¸ªåˆ†ç¦»çš„æ¡†æ¶æ¥å»ºæ¨¡éŸµå¾‹æ–‡æœ¬ç‰¹å¾å’Œé…éŸ³é£æ ¼ï¼ŒåŒæ—¶ä¿æŒå£°å­¦è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢†åŸŸå†…çš„æƒ…æ„Ÿåˆ†ææ¨¡å—ï¼Œä»¥å‡å°‘ä¸åŒç”µå½±ä¹‹é—´è§†è§‰é¢†åŸŸå˜åŒ–çš„å½±å“ï¼Œä»è€Œå¢å¼ºæƒ…æ„Ÿ-éŸµå¾‹å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æ¼”ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/%E3%80%82">https://zzdoog.github.io/ProDubber/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12042v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”µå½±é…éŸ³çš„è¿‡ç¨‹ï¼Œå³å°†å‰§æœ¬è½¬åŒ–ä¸ºä¸ç”µå½±ç‰‡æ®µç›¸åŒ¹é…çš„å£°éŸ³ï¼ŒåŒæ—¶å±•ç¤ºå‚è€ƒéŸ³é¢‘ç‰‡æ®µä¸­çš„è¯´è¯äººçš„å£°éŸ³ã€‚ç”µå½±é…éŸ³ä»»åŠ¡éœ€è¦æ¨¡å‹åœ¨è§’è‰²è¡¨æ¼”å’Œå¤æ‚çš„éŸµå¾‹ç»“æ„ä¹‹é—´å»ºç«‹è”ç³»ï¼Œä»¥åˆ›å»ºé«˜è´¨é‡çš„ä¸è§†é¢‘åŒæ­¥çš„é…éŸ³è½¨é“ã€‚é’ˆå¯¹ç”µå½±é…éŸ³æ•°æ®é›†è§„æ¨¡æœ‰é™åŠéŸ³é¢‘æ•°æ®ä¸­çš„èƒŒæ™¯å™ªéŸ³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå£°å­¦éŸµå¾‹è§£è€¦çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œä»¥å®ç°é«˜è´¨é‡çš„é…éŸ³ç”Ÿæˆå’Œç²¾ç¡®çš„èŠ‚å¥å¯¹é½ã€‚é¦–å…ˆï¼Œé€šè¿‡æå‡ºéŸµå¾‹å¢å¼ºçš„å£°å­¦é¢„è®­ç»ƒæ¥å¼€å‘ç¨³å¥çš„å£°å­¦å»ºæ¨¡èƒ½åŠ›ã€‚ç„¶åï¼Œå†»ç»“é¢„è®­ç»ƒçš„å£°å­¦ç³»ç»Ÿï¼Œè®¾è®¡ä¸€ä¸ªè§£è€¦æ¡†æ¶æ¥å»ºæ¨¡æ–‡æœ¬ç‰¹å¾ã€é…éŸ³é£æ ¼å’Œä¿æŒå£°å­¦è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†é¢†åŸŸå†…çš„æƒ…æ„Ÿåˆ†ææ¨¡å—ï¼Œä»¥å‡å°‘ä¸åŒç”µå½±ä¹‹é—´è§†è§‰åŸŸå˜åŒ–çš„å½±å“ï¼Œä»è€Œæé«˜æƒ…æ„ŸéŸµå¾‹çš„å¯¹é½åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æ¼”ç¤ºåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå½±é…éŸ³æ˜¯å°†å‰§æœ¬è½¬åŒ–ä¸ºä¸ç”µå½±ç‰‡æ®µç›¸åŒ¹é…çš„å£°éŸ³çš„è¿‡ç¨‹ï¼Œéœ€è€ƒè™‘è§’è‰²è¡¨æ¼”å’Œå¤æ‚éŸµå¾‹ç»“æ„ã€‚</li>
<li>ç°æœ‰çš„ç”µå½±é…éŸ³æ•°æ®é›†è§„æ¨¡æœ‰é™ï¼Œä¸”éŸ³é¢‘æ•°æ®ä¸­çš„èƒŒæ™¯å™ªéŸ³å½±å“å£°å­¦å»ºæ¨¡æ€§èƒ½ã€‚</li>
<li>æå‡ºä¸€ä¸ªå£°å­¦éŸµå¾‹è§£è€¦çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œä»¥å®ç°é«˜è´¨é‡çš„é…éŸ³ç”Ÿæˆå’Œç²¾ç¡®çš„èŠ‚å¥å¯¹é½ã€‚</li>
<li>å¼•å…¥éŸµå¾‹å¢å¼ºçš„å£°å­¦é¢„è®­ç»ƒæ¥å¢å¼ºæ¨¡å‹çš„å£°å­¦å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„å£°å­¦ç³»ç»Ÿå¹¶è®¾è®¡è§£è€¦æ¡†æ¶ï¼ŒåŒæ—¶å»ºæ¨¡æ–‡æœ¬ç‰¹å¾ã€é…éŸ³é£æ ¼å’Œä¿æŒå£°å­¦è´¨é‡ã€‚</li>
<li>èå…¥é¢†åŸŸå†…çš„æƒ…æ„Ÿåˆ†ææ¨¡å—ä»¥å‡å°‘ä¸åŒç”µå½±è§†è§‰åŸŸå˜åŒ–çš„å½±å“ï¼Œæé«˜æƒ…æ„ŸéŸµå¾‹çš„å¯¹é½åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-41ae576be778ad1c16dc76f8bbb09676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53311922658be8ead1ffbba037090584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3614debffb60ad043d0ef7e07834a70.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MMS-LLaMA-Efficient-LLM-based-Audio-Visual-Speech-Recognition-with-Minimal-Multimodal-Speech-Tokens"><a href="#MMS-LLaMA-Efficient-LLM-based-Audio-Visual-Speech-Recognition-with-Minimal-Multimodal-Speech-Tokens" class="headerlink" title="MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with   Minimal Multimodal Speech Tokens"></a>MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with   Minimal Multimodal Speech Tokens</h2><p><strong>Authors:Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early av-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.74% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰é€šè¿‡ç»“åˆå¬è§‰å’Œè§†è§‰ä¿¡æ¯ï¼Œåœ¨å˜ˆæ‚ç¯å¢ƒä¸­å®ç°äº†ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ã€‚ç„¶è€Œï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„AVSRç³»ç»Ÿç”±äºLLMå¤„ç†è§†å¬è¯­éŸ³çš„é«˜æ—¶é—´åˆ†è¾¨ç‡è€Œäº§ç”Ÿäº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé«˜æ•ˆçš„å¤šæ¨¡æ€è¯­éŸ³LLMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¿ç•™åŸºæœ¬è¯­è¨€å†…å®¹çš„åŒæ—¶ï¼Œæœ€å°åŒ–ä»¤ç‰Œé•¿åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ—©æœŸavèåˆæ¨¡å—è¿›è¡Œæµçº¿å‹ç‰¹å¾é›†æˆï¼Œä¸€ä¸ªè§†å¬è¯­éŸ³Q-Formerï¼Œæ ¹æ®è¾“å…¥æŒç»­æ—¶é—´åŠ¨æ€åˆ†é…ä»¤ç‰Œï¼Œä»¥åŠä¸€ç§ç²¾ç»†åŒ–çš„æŸ¥è¯¢åˆ†é…ç­–ç•¥ï¼Œé…å¤‡ä¸€ä¸ªè¯­é€Ÿé¢„æµ‹å™¨ï¼Œæ ¹æ®æ¯ä¸ªéŸ³é¢‘æ ·æœ¬çš„è¯­é€Ÿè°ƒæ•´ä»¤ç‰Œåˆ†é…ã€‚åœ¨LRS3æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥0.74%çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ¯ç§’ä»…ä½¿ç”¨3.5ä¸ªä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å°†ä»¤ç‰Œä½¿ç”¨é‡å‡å°‘äº†86%ï¼Œä¸ä»¥å‰çš„å¤šæ¨¡æ€è¯­éŸ³LLMæ¡†æ¶ç›¸æ¯”ï¼Œè¿˜æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå‡å°‘äº†35.7%çš„æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11315v1">PDF</a> The code and models are available   <a target="_blank" rel="noopener" href="https://github.com/JeongHun0716/MMS-LLaMA">https://github.com/JeongHun0716/MMS-LLaMA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å¥è¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼Œç»“åˆäº†å¬è§‰å’Œè§†è§‰ä¿¡æ¯ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨AVSRç³»ç»Ÿä¸­å¤„ç†é«˜æ—¶ç©ºåˆ†è¾¨ç‡éŸ³é¢‘è§†è§‰è¯­éŸ³æ—¶çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€è¯­éŸ³LLMæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–ä»¤ç‰Œé•¿åº¦åŒæ—¶ä¿ç•™åŸºæœ¬è¯­è¨€å†…å®¹ã€é‡‡ç”¨æ—©æœŸavèåˆæ¨¡å—è¿›è¡ŒåŠŸèƒ½æ•´åˆã€éŸ³é¢‘è§†è§‰è¯­éŸ³Q-FormeråŠ¨æ€åˆ†é…ä»¤ç‰Œä»¥åŠä½¿ç”¨ç²¾ç»†æŸ¥è¯¢åˆ†é…ç­–ç•¥ä¸è¯­é€Ÿé¢„æµ‹å™¨è°ƒæ•´æ¯ä¸ªéŸ³é¢‘æ ·æœ¬çš„ä»¤ç‰Œåˆ†é…ï¼Œä»¥å®ç°æ€§èƒ½ä¼˜åŒ–ã€‚åœ¨LRS3æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯é”™è¯¯ç‡ä¸º0.74%çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ¯ç§’ä»…ä½¿ç”¨3.5ä¸ªä»¤ç‰Œã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸ä»…å°†ä»¤ç‰Œä½¿ç”¨é‡å‡å°‘äº†86%ï¼Œè€Œä¸”é€šè¿‡é™ä½æµ®ç‚¹è¿ç®—æ¬¡æ•°æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†å¬è§‰å’Œè§†è§‰ä¿¡æ¯ï¼Œæé«˜äº†åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«ç¨³å¥æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†é«˜æ—¶ç©ºåˆ†è¾¨ç‡éŸ³é¢‘è§†è§‰è¯­éŸ³æ—¶è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>é«˜æ•ˆå¤šæ¨¡æ€è¯­éŸ³LLMæ¡†æ¶æ—¨åœ¨ä¼˜åŒ–æ€§èƒ½ï¼Œå‡å°‘è®¡ç®—æˆæœ¬ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨æ—©æœŸavèåˆã€éŸ³é¢‘è§†è§‰è¯­éŸ³Q-Formerå’ŒæŸ¥è¯¢åˆ†é…ç­–ç•¥ä¸è¯­é€Ÿé¢„æµ‹å™¨çš„ç»„åˆã€‚</li>
<li>åœ¨LRS3æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯é”™è¯¯ç‡ä½è‡³0.74%ã€‚</li>
<li>ä¸ä¹‹å‰çš„å¤šæ¨¡æ€è¯­éŸ³LLMæ¡†æ¶ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨é‡ï¼Œå¹¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14a67d7a6fdf39339dafa56183355397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a1e8b4d32bb175890274e72787256d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d9bc42503fef211f8602bdd7878602a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13d4d5a743b2f3aea19e1c5b6fd00fb3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Joint-Training-And-Decoding-for-Multilingual-End-to-End-Simultaneous-Speech-Translation"><a href="#Joint-Training-And-Decoding-for-Multilingual-End-to-End-Simultaneous-Speech-Translation" class="headerlink" title="Joint Training And Decoding for Multilingual End-to-End Simultaneous   Speech Translation"></a>Joint Training And Decoding for Multilingual End-to-End Simultaneous   Speech Translation</h2><p><strong>Authors:Wuwei Huang, Renren Jin, Wen Zhang, Jian Luan, Bin Wang, Deyi Xiong</strong></p>
<p>Recent studies on end-to-end speech translation(ST) have facilitated the exploration of multilingual end-to-end ST and end-to-end simultaneous ST. In this paper, we investigate end-to-end simultaneous speech translation in a one-to-many multilingual setting which is closer to applications in real scenarios. We explore a separate decoder architecture and a unified architecture for joint synchronous training in this scenario. To further explore knowledge transfer across languages, we propose an asynchronous training strategy on the proposed unified decoder architecture. A multi-way aligned multilingual end-to-end ST dataset was curated as a benchmark testbed to evaluate our methods. Experimental results demonstrate the effectiveness of our models on the collected dataset. Our codes and data are available at: <a target="_blank" rel="noopener" href="https://github.com/XiaoMi/TED-MMST">https://github.com/XiaoMi/TED-MMST</a>. </p>
<blockquote>
<p>æœ€è¿‘å…³äºç«¯åˆ°ç«¯è¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰çš„ç ”ç©¶ä¿ƒè¿›äº†å¤šè¯­è¨€ç«¯åˆ°ç«¯STå’Œç«¯åˆ°ç«¯åŒæ­¥STçš„æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨æ›´æ¥è¿‘å®é™…åº”ç”¨åœºæ™¯çš„å¤šç§è¯­è¨€åˆ°å¤šç§è¯­è¨€çš„è®¾ç½®ä¸­ï¼Œç ”ç©¶äº†ç«¯åˆ°ç«¯çš„å®æ—¶è¯­éŸ³ç¿»è¯‘ã€‚æˆ‘ä»¬åœ¨è¿™ä¸ªåœºæ™¯ä¸­æ¢ç´¢äº†å•ç‹¬çš„è§£ç å™¨æ¶æ„å’Œè”åˆåŒæ­¥è®­ç»ƒçš„ç»Ÿä¸€æ¶æ„ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç ”ç©¶è·¨è¯­è¨€çš„çŸ¥è¯†è½¬ç§»ï¼Œæˆ‘ä»¬åœ¨ç»Ÿä¸€çš„è§£ç å™¨æ¶æ„ä¸Šæå‡ºäº†å¼‚æ­¥è®­ç»ƒç­–ç•¥ã€‚ä½œä¸ºè¯„ä¼°æ–¹æ³•çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸€ä¸ªå¤šè·¯å¯¹é½çš„å¤šè¯­è¨€ç«¯åˆ°ç«¯STæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šæ˜¯æœ‰æ•ˆçš„ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiaoMi/TED-MMST%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XiaoMi/TED-MMSTä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11080v1">PDF</a> ICASSP 2023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç«¯åˆ°ç«¯çš„å¤šè¯­è¨€åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘æŠ€æœ¯ï¼Œåœ¨çœŸå®åœºæ™¯åº”ç”¨ä¸­æ›´å…·æœ‰å®é™…æ„ä¹‰ã€‚æ¢è®¨äº†åˆ†åˆ«è§£ç æ¶æ„å’Œè”åˆåŒæ­¥è®­ç»ƒç»Ÿä¸€æ¶æ„çš„ä½¿ç”¨ï¼Œå¹¶æå‡ºäº†å¼‚æ­¥è®­ç»ƒç­–ç•¥æ¥æå‡è·¨è¯­è¨€çŸ¥è¯†è¿ç§»ã€‚ç ”ç©¶ä½¿ç”¨å¤šè¯­ç§å¯¹é½çš„å¤šè¯­ç§ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«ç¿»è¯‘æ•°æ®é›†ä½œä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚å®éªŒç»“æœè¡¨æ˜æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†ç«¯åˆ°ç«¯çš„å¤šè¯­è¨€åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘æŠ€æœ¯ï¼Œé€‚ç”¨äºçœŸå®åœºæ™¯åº”ç”¨ã€‚</li>
<li>åˆ†åˆ«æ¢è®¨äº†åˆ†åˆ«è§£ç æ¶æ„å’Œç»Ÿä¸€æ¶æ„åœ¨è”åˆåŒæ­¥è®­ç»ƒä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†å¼‚æ­¥è®­ç»ƒç­–ç•¥æ¥æå‡è·¨è¯­è¨€çŸ¥è¯†è¿ç§»ã€‚</li>
<li>ä½¿ç”¨å¤šè¯­ç§å¯¹é½çš„å¤šè¯­ç§ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«ç¿»è¯‘æ•°æ®é›†ä½œä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨GitHubä¸Šè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b98c993a8617fdd44d0209e67687eab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c2e547eb5e640d49456173b4b61e713.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41aaa90d9e230b8f40ef56a14f386ea6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-506bf90957f546c1d53dc16f4aa57736.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EmoDiffusion-Enhancing-Emotional-3D-Facial-Animation-with-Latent-Diffusion-Models"><a href="#EmoDiffusion-Enhancing-Emotional-3D-Facial-Animation-with-Latent-Diffusion-Models" class="headerlink" title="EmoDiffusion: Enhancing Emotional 3D Facial Animation with Latent   Diffusion Models"></a>EmoDiffusion: Enhancing Emotional 3D Facial Animation with Latent   Diffusion Models</h2><p><strong>Authors:Yixuan Zhang, Qing Chang, Yuxi Wang, Guang Chen, Zhaoxiang Zhang, Junran Peng</strong></p>
<p>Speech-driven 3D facial animation seeks to produce lifelike facial expressions that are synchronized with the speech content and its emotional nuances, finding applications in various multimedia fields. However, previous methods often overlook emotional facial expressions or fail to disentangle them effectively from the speech content. To address these challenges, we present EmoDiffusion, a novel approach that disentangles different emotions in speech to generate rich 3D emotional facial expressions. Specifically, our method employs two Variational Autoencoders (VAEs) to separately generate the upper face region and mouth region, thereby learning a more refined representation of the facial sequence. Unlike traditional methods that use diffusion models to connect facial expression sequences with audio inputs, we perform the diffusion process in the latent space. Furthermore, we introduce an Emotion Adapter to evaluate upper face movements accurately. Given the paucity of 3D emotional talking face data in the animation industry, we capture facial expressions under the guidance of animation experts using LiveLinkFace on an iPhone. This effort results in the creation of an innovative 3D blendshape emotional talking face dataset (3D-BEF) used to train our network. Extensive experiments and perceptual evaluations validate the effectiveness of our approach, confirming its superiority in generating realistic and emotionally rich facial animations. </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨äº§ç”Ÿä¸è¯­éŸ³å†…å®¹åŠå…¶æƒ…æ„Ÿç»†å¾®å·®åˆ«åŒæ­¥çš„é€¼çœŸé¢éƒ¨è¡¨æƒ…ï¼Œåœ¨å¤šåª’ä½“é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¼šå¿½ç•¥æƒ…æ„Ÿé¢éƒ¨è¡¨æƒ…æˆ–æ— æ³•æœ‰æ•ˆåœ°ä»è¯­éŸ³å†…å®¹ä¸­å°†å…¶åˆ†ç¦»å‡ºæ¥ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†EmoDiffusionè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ†è§£è¯­éŸ³ä¸­çš„ä¸åŒæƒ…æ„Ÿæ¥ç”Ÿæˆä¸°å¯Œçš„3Dæƒ…æ„Ÿé¢éƒ¨è¡¨æƒ…ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤ä¸ªå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰åˆ†åˆ«ç”Ÿæˆé¢éƒ¨ä¸ŠåŒºåŸŸå’Œå˜´å·´åŒºåŸŸï¼Œä»è€Œå­¦ä¹ æ›´ç²¾ç»†çš„é¢éƒ¨è¡¨æƒ…åºåˆ—è¡¨ç¤ºã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨æ‰©æ•£æ¨¡å‹å°†é¢éƒ¨è¡¨æƒ…åºåˆ—ä¸éŸ³é¢‘è¾“å…¥è¿æ¥èµ·æ¥ï¼Œæˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´æ‰§è¡Œæ‰©æ•£è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæƒ…æ„Ÿé€‚é…å™¨æ¥å‡†ç¡®è¯„ä¼°é¢éƒ¨ä¸Šéƒ¨çš„è¿åŠ¨ã€‚é‰´äºåŠ¨ç”»è¡Œä¸šç¼ºä¹3Dæƒ…æ„Ÿå¯¹è¯é¢éƒ¨æ•°æ®ï¼Œæˆ‘ä»¬åœ¨åŠ¨ç”»ä¸“å®¶çš„æŒ‡å¯¼ä¸‹ä½¿ç”¨iPhoneä¸Šçš„LiveLinkFaceæ•è·é¢éƒ¨è¡¨æƒ…ï¼ŒæˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªåˆ›æ–°çš„3Dæ··ç›¸æƒ…ç»ªå¯¹è¯é¢éƒ¨æ•°æ®é›†ï¼ˆ3D-BEFï¼‰ï¼Œç”¨äºè®­ç»ƒæˆ‘ä»¬çš„ç½‘ç»œã€‚å¤§é‡çš„å®éªŒå’Œæ„ŸçŸ¥è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯å®å…¶åœ¨ç”ŸæˆçœŸå®ä¸”æƒ…æ„Ÿä¸°å¯Œçš„é¢éƒ¨åŠ¨ç”»æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11028v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†Speech-driven 3Dé¢éƒ¨åŠ¨ç”»é¢ä¸´çš„æŒ‘æˆ˜åŠå…¶åº”ç”¨äºå¤šåª’ä½“é¢†åŸŸçš„ä»·å€¼ã€‚ä¸ºè§£å†³ä»¥å¾€æ–¹æ³•å¿½è§†æƒ…æ„Ÿé¢éƒ¨è¡¨æƒ…æˆ–æ— æ³•æœ‰æ•ˆä»è¯­éŸ³å†…å®¹ä¸­åˆ†ç¦»æƒ…æ„Ÿçš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºEmoDiffusionçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åˆ†åˆ«ç”Ÿæˆé¢éƒ¨ä¸ŠåŒºå’Œå£éƒ¨åŒºåŸŸï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œæ›´ç²¾ç»†åœ°è¡¨ç¤ºé¢éƒ¨åºåˆ—ã€‚æ­¤å¤–ï¼Œå¼•å…¥æƒ…æ„Ÿé€‚é…å™¨å‡†ç¡®è¯„ä¼°é¢éƒ¨ä¸ŠåŒºçš„è¿åŠ¨ã€‚ä¸ºè§£å†³åŠ¨ç”»è¡Œä¸šä¸­ç¼ºä¹3Dæƒ…æ„Ÿè¯´è¯é¢éƒ¨æ•°æ®çš„é—®é¢˜ï¼Œä½¿ç”¨LiveLinkFaceåœ¨iPhoneä¸Šé‡‡é›†é¢éƒ¨è¡¨æƒ…æ•°æ®ï¼Œåˆ›å»ºåˆ›æ–°çš„3D blendshapeæƒ…æ„Ÿè¯´è¯é¢éƒ¨æ•°æ®é›†ï¼ˆ3D-BEFï¼‰ç”¨äºè®­ç»ƒç½‘ç»œã€‚å®éªŒå’Œæ„ŸçŸ¥è¯„ä¼°éªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿåœ¨ç”ŸæˆçœŸå®ä¸”æƒ…æ„Ÿä¸°å¯Œçš„é¢éƒ¨åŠ¨ç”»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech-driven 3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨å®ç°ä¸è¯­éŸ³å†…å®¹åŠå…¶æƒ…æ„Ÿç»†å¾®å·®åˆ«åŒæ­¥çš„é€¼çœŸé¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†æƒ…æ„Ÿé¢éƒ¨è¡¨æƒ…æˆ–æ— æ³•æœ‰æ•ˆåœ°ä»è¯­éŸ³å†…å®¹ä¸­åˆ†ç¦»æƒ…æ„Ÿã€‚</li>
<li>EmoDiffusionæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ç”Ÿæˆé¢éƒ¨ä¸ŠåŒºå’Œå£éƒ¨åŒºåŸŸï¼Œä»¥æ›´ç²¾ç»†çš„æ–¹å¼è¡¨ç¤ºé¢éƒ¨åºåˆ—ã€‚</li>
<li>EmoDiffusionåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥æƒ…æ„Ÿé€‚é…å™¨ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°é¢éƒ¨ä¸ŠåŒºçš„è¿åŠ¨ã€‚</li>
<li>ä¸ºè®­ç»ƒç½‘ç»œï¼Œåˆ›å»ºäº†3D blendshapeæƒ…æ„Ÿè¯´è¯é¢éƒ¨æ•°æ®é›†ï¼ˆ3D-BEFï¼‰ï¼Œè¯¥æ•°æ®é›†æ˜¯é€šè¿‡ä½¿ç”¨LiveLinkFaceåœ¨iPhoneä¸Šé‡‡é›†é¢éƒ¨è¡¨æƒ…æ•°æ®åˆ¶æˆçš„ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜äº†EmoDiffusionæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿç”ŸæˆçœŸå®ä¸”æƒ…æ„Ÿä¸°å¯Œçš„é¢éƒ¨åŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58e853a905eceb5428ab9a863737421b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f254f40f8a1ce87ba5cea036d281732.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SPES-Spectrogram-Perturbation-for-Explainable-Speech-to-Text-Generation"><a href="#SPES-Spectrogram-Perturbation-for-Explainable-Speech-to-Text-Generation" class="headerlink" title="SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation"></a>SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation</h2><p><strong>Authors:Dennis Fucci, Marco Gaido, Beatrice Savoldi, Matteo Negri, Mauro Cettolo, Luisa Bentivogli</strong></p>
<p>Spurred by the demand for interpretable models, research on eXplainable AI for language technologies has experienced significant growth, with feature attribution methods emerging as a cornerstone of this progress. While prior work in NLP explored such methods for classification tasks and textual applications, explainability intersecting generation and speech is lagging, with existing techniques failing to account for the autoregressive nature of state-of-the-art models and to provide fine-grained, phonetically meaningful explanations. We address this gap by introducing Spectrogram Perturbation for Explainable Speech-to-text Generation (SPES), a feature attribution technique applicable to sequence generation tasks with autoregressive models. SPES provides explanations for each predicted token based on both the input spectrogram and the previously generated tokens. Extensive evaluation on speech recognition and translation demonstrates that SPES generates explanations that are faithful and plausible to humans. </p>
<blockquote>
<p>å—åˆ°å¯è§£é‡Šæ¨¡å‹éœ€æ±‚çš„å½±å“ï¼Œè¯­è¨€æŠ€æœ¯ä¸­çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ç ”ç©¶ç»å†äº†æ˜¾è‘—å¢é•¿ï¼Œç‰¹å¾å½’å› æ–¹æ³•å·²æˆä¸ºè¿™ä¸€è¿›æ­¥çš„æ ¸å¿ƒã€‚è™½ç„¶è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…ˆå‰å·¥ä½œå·²ç»æ¢ç´¢äº†åˆ†ç±»ä»»åŠ¡å’Œæ–‡æœ¬åº”ç”¨ä¸­çš„è¿™äº›æ–¹æ³•ï¼Œä½†è§£é‡Šèƒ½åŠ›ä¸ç”Ÿæˆå’Œè¯­éŸ³çš„äº¤é›†æ»åï¼Œç°æœ‰æŠ€æœ¯æœªèƒ½è€ƒè™‘åˆ°æœ€å…ˆè¿›æ¨¡å‹çš„è‡ªå›å½’æ€§è´¨ï¼Œä¹Ÿæ— æ³•æä¾›ç²¾ç»†ç²’åº¦å’ŒéŸ³ç´ æ„ä¹‰ä¸Šçš„è§£é‡Šã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ç”¨äºè§£é‡Šè¯­éŸ³åˆ°æ–‡æœ¬ç”Ÿæˆçš„é¢‘è°±æ‰°åŠ¨ï¼ˆSPESï¼‰æ¥è§£å†³è¿™ä¸€å·®è·ï¼ŒSPESæ˜¯ä¸€ç§é€‚ç”¨äºè‡ªå›å½’æ¨¡å‹çš„åºåˆ—ç”Ÿæˆä»»åŠ¡çš„ç‰¹å¾å½’å› æŠ€æœ¯ã€‚SPESåŸºäºè¾“å…¥é¢‘è°±å›¾å’Œå…ˆå‰ç”Ÿæˆçš„æ ‡è®°ä¸ºæ¯ä¸ªé¢„æµ‹æ ‡è®°æä¾›è§£é‡Šã€‚åœ¨è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ–¹é¢çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSPESç”Ÿæˆçš„è§£é‡Šå¯¹äººç±»æ¥è¯´æ˜¯å¿ å®å’Œå¯ä¿¡çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01710v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯è§£é‡Šçš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„æœ€æ–°è¿›å±•ã€‚éšç€å¯¹å¯è§£é‡Šæ¨¡å‹çš„éœ€æ±‚å¢é•¿ï¼Œè§£é‡Šæ€§äººå·¥æ™ºèƒ½åœ¨è¯­è¨€æŠ€æœ¯é¢†åŸŸçš„ç ”ç©¶å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹å¾å½’å› æ–¹æ³•å·²æˆä¸ºè¿™ä¸€è¿›å±•çš„æ ¸å¿ƒã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œåœ¨NLPä¸­æ¢ç´¢äº†è¿™äº›æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡å’Œæ–‡æœ¬åº”ç”¨ï¼Œä½†è§£é‡Šæ€§ä¸ç”Ÿæˆå’Œè¯­éŸ³çš„äº¤å‰éƒ¨åˆ†ä»ç„¶æ»åã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¼•å…¥äº†Spectrogram Perturbation for Explainable Speech-to-text Generationï¼ˆSPESï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºå…·æœ‰è‡ªå›å½’æ¨¡å‹çš„åºåˆ—ç”Ÿæˆä»»åŠ¡çš„ç‰¹å¾å½’å› æŠ€æœ¯ã€‚SPESåŸºäºè¾“å…¥é¢‘è°±å›¾å’Œå…ˆå‰ç”Ÿæˆçš„æ ‡è®°ä¸ºæ¯ä¸ªé¢„æµ‹æ ‡è®°æä¾›è§£é‡Šã€‚åœ¨è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘æ–¹é¢çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSPESç”Ÿæˆçš„è§£é‡Šå¯¹äººç±»æ¥è¯´æ˜¯å¿ å®å’Œå¯ä¿¡çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£é‡Šæ€§äººå·¥æ™ºèƒ½åœ¨è¯­è¨€æŠ€æœ¯é¢†åŸŸçš„ç ”ç©¶æ­£åœ¨å¢é•¿ï¼Œç‰¹å¾å½’å› æ–¹æ³•å·²æˆä¸ºæ ¸å¿ƒã€‚</li>
<li>åœ¨NLPä¸­ï¼Œå°½ç®¡åˆ†ç±»ä»»åŠ¡å’Œæ–‡æœ¬åº”ç”¨çš„è§£é‡Šæ€§ç ”ç©¶å·²ç»å–å¾—è¿›å±•ï¼Œä½†ç”Ÿæˆä»»åŠ¡å’Œè¯­éŸ³æ–¹é¢çš„è§£é‡Šæ€§ç ”ç©¶ä»ç„¶æ»åã€‚</li>
<li>ç°æœ‰æŠ€æœ¯æœªèƒ½å……åˆ†è€ƒè™‘åˆ°å…ˆè¿›æ¨¡å‹çš„è‡ªå›å½’æ€§è´¨ï¼Œæ— æ³•æä¾›ç²¾ç»†ç²’åº¦çš„ã€å…·æœ‰è¯­éŸ³æ„ä¹‰çš„è§£é‡Šã€‚</li>
<li>å¼•å…¥Spectrogram Perturbation for Explainable Speech-to-text Generationï¼ˆSPESï¼‰ä»¥å¡«è¡¥ç©ºç™½ã€‚</li>
<li>SPESæ˜¯ä¸€ç§ç‰¹å¾å½’å› æŠ€æœ¯ï¼Œé€‚ç”¨äºå…·æœ‰è‡ªå›å½’æ¨¡å‹çš„åºåˆ—ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>SPESä¸ºæ¯ä¸ªé¢„æµ‹æ ‡è®°æä¾›åŸºäºè¾“å…¥é¢‘è°±å›¾å’Œå…ˆå‰ç”Ÿæˆæ ‡è®°çš„è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19ec1ad3e82783d1339087f9e2f6d756.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fad9c52f77e2a353b861740679bde4d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Personalized-Speech-Emotion-Recognition-in-Human-Robot-Interaction-using-Vision-Transformers"><a href="#Personalized-Speech-Emotion-Recognition-in-Human-Robot-Interaction-using-Vision-Transformers" class="headerlink" title="Personalized Speech Emotion Recognition in Human-Robot Interaction using   Vision Transformers"></a>Personalized Speech Emotion Recognition in Human-Robot Interaction using   Vision Transformers</h2><p><strong>Authors:Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa</strong></p>
<p>Emotions are an essential element in verbal communication, so understanding individualsâ€™ affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT&#x2F;BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs. </p>
<blockquote>
<p>æƒ…æ„Ÿæ˜¯å£å¤´äº¤æµçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå› æ­¤åœ¨äººæœºäº’åŠ¨ï¼ˆHRIï¼‰ä¸­ç†è§£ä¸ªä½“çš„æƒ…æ„Ÿå˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è½¬æ¢å™¨æ¨¡å‹åœ¨äººæœºäº’åŠ¨ä¸­çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨ä½¿ç”¨ViTï¼ˆè§†è§‰è½¬æ¢å™¨ï¼‰å’ŒBEiTï¼ˆå›¾åƒè½¬æ¢å™¨çš„BERTé¢„è®­ç»ƒæ–¹æ³•ï¼‰ç®¡é“ã€‚ç ”ç©¶çš„é‡ç‚¹æ˜¯é€šè¿‡å¯¹åŸºå‡†æ•°æ®é›†è¿›è¡Œå¾®è°ƒå¹¶åˆ©ç”¨é›†æˆæ–¹æ³•ï¼Œä½¿SERæ¨¡å‹é€‚åº”ä¸ªä½“è¯­éŸ³ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»ä¸NAOæœºå™¨äººè¿›è¡Œä¼ªè‡ªç„¶å¯¹è¯çš„ä¸åŒäººç±»å—è¯•è€…ä¸­æ”¶é›†éŸ³é¢‘æ•°æ®ã€‚ç„¶åæˆ‘ä»¬å¯¹åŸºäºViTå’ŒBEiTçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æ¥è‡ªå‚ä¸è€…çš„æœªè§è¯­éŸ³æ ·æœ¬ä¸Šæµ‹è¯•è¿™äº›æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå¾®è°ƒè§†è§‰è½¬æ¢å™¨ï¼Œç„¶åä½¿ç”¨å·²ç»å¾®è°ƒè¿‡çš„æ¨¡å‹æˆ–é›†æˆViT&#x2F;BEiTæ¨¡å‹ï¼Œåœ¨è¯†åˆ«å››ç§ä¸»è¦æƒ…ç»ªï¼ˆä¸­æ€§ã€å¿«ä¹ã€æ‚²ä¼¤å’Œæ„¤æ€’ï¼‰æ—¶ï¼Œé’ˆå¯¹ä¸ªäººçš„åˆ†ç±»å‡†ç¡®ç‡æœ€é«˜ï¼Œä¸å¾®è°ƒæ™®é€šViTsæˆ–BEITsç›¸æ¯”æ•ˆæœæ›´ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10687v3">PDF</a> This work has been accepted for the IEEE Robotics and Automation   Letters (RA-L)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†è§†è§‰è½¬æ¢å™¨æ¨¡å‹åº”ç”¨äºäººæœºäº’åŠ¨ä¸­çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€‚ç ”ç©¶é‡‡ç”¨ViTå’ŒBEiTæ¨¡å‹ï¼Œé€šè¿‡å¯¹åŸºå‡†æ•°æ®é›†è¿›è¡Œå¾®è°ƒå¹¶åˆ©ç”¨é›†æˆæ–¹æ³•ï¼Œå®ç°å¯¹ä¸ªä½“è¯­éŸ³ç‰¹æ€§çš„é€šç”¨åŒ–ã€‚é€šè¿‡å¯¹ä¸NAOæœºå™¨äººè¿›è¡Œä¼ªè‡ªç„¶å¯¹è¯çš„éŸ³é¢‘æ•°æ®è¿›è¡Œæ”¶é›†ã€å¾®è°ƒæ¨¡å‹å¹¶æµ‹è¯•ï¼Œå‘ç°å¯¹å››ç§ä¸»è¦æƒ…ç»ªâ€”â€”ä¸­æ€§ã€å¿«ä¹ã€æ‚²ä¼¤å’Œæ„¤æ€’â€”â€”çš„è¯†åˆ«ä¸­ï¼Œç›¸è¾ƒäºå¾®è°ƒæ™®é€šçš„ViTæˆ–BEiTæ¨¡å‹ï¼Œä½¿ç”¨å·²å¾®è°ƒè¿‡çš„æ¨¡å‹æˆ–é›†æˆViT&#x2F;BEiTæ¨¡å‹å¯è·å¾—æ›´é«˜çš„ä¸ªä½“åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿåœ¨äººé™…äº¤æµä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äººæœºäº’åŠ¨ä¸­ç†è§£ä¸ªä½“çš„æƒ…æ„Ÿå˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è§†è§‰è½¬æ¢å™¨æ¨¡å‹ï¼ˆViTå’ŒBEiTï¼‰è¿›è¡Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€‚</li>
<li>é€šè¿‡å¾®è°ƒåŸºå‡†æ•°æ®é›†ä¸Šçš„æ¨¡å‹ï¼Œå®ç°å¯¹ä¸ªä½“è¯­éŸ³ç‰¹æ€§çš„é€šç”¨åŒ–ã€‚</li>
<li>é›†æˆæ–¹æ³•å¯ä»¥æé«˜è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®ç‡ã€‚</li>
<li>ä½¿ç”¨äº†ä¸NAOæœºå™¨äººè¿›è¡Œä¼ªè‡ªç„¶å¯¹è¯çš„éŸ³é¢‘æ•°æ®æ¥æµ‹è¯•æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨å·²å¾®è°ƒè¿‡çš„æ¨¡å‹æˆ–é›†æˆViT&#x2F;BEiTæ¨¡å‹åœ¨è¯†åˆ«å››ç§ä¸»è¦æƒ…ç»ªæ—¶è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d2724482341270ae8a08431985f7aa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1df52457354ebe35a7c2d95f86f95549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6718c798826cf4e5e49657d6f619c0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6817f7d46662b07115c15784608602d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac915e3876c60b287ac49530c98cf191.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef02565806f4dd911d10d5d0a1222d0c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-based-speaker-diarization-correction-A-generalizable-approach"><a href="#LLM-based-speaker-diarization-correction-A-generalizable-approach" class="headerlink" title="LLM-based speaker diarization correction: A generalizable approach"></a>LLM-based speaker diarization correction: A generalizable approach</h2><p><strong>Authors:Georgios Efstathiadis, Vijay Yadav, Anzar Abbas</strong></p>
<p>Speaker diarization is necessary for interpreting conversations transcribed using automated speech recognition (ASR) tools. Despite significant developments in diarization methods, diarization accuracy remains an issue. Here, we investigate the use of large language models (LLMs) for diarization correction as a post-processing step. LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations. The ability of the models to improve diarization accuracy in a holdout dataset from the Fisher corpus as well as an independent dataset was measured. We report that fine-tuned LLMs can markedly improve diarization accuracy. However, model performance is constrained to transcripts produced using the same ASR tool as the transcripts used for fine-tuning, limiting generalizability. To address this constraint, an ensemble model was developed by combining weights from three separate models, each fine-tuned using transcripts from a different ASR tool. The ensemble model demonstrated better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable. We have made the weights of these models publicly available on HuggingFace at <a target="_blank" rel="noopener" href="https://huggingface.co/bklynhlth">https://huggingface.co/bklynhlth</a>. </p>
<blockquote>
<p>è¯´è¯äººè¯†åˆ«æ˜¯å¯¹ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å·¥å…·è½¬å½•çš„å¯¹è¯è¿›è¡Œè§£é‡Šçš„å¿…è¦æ­¥éª¤ã€‚å°½ç®¡è¯†åˆ«æ–¹æ³•æœ‰äº†æ˜¾è‘—çš„å‘å±•ï¼Œä½†è¯†åˆ«å‡†ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½œä¸ºåå¤„ç†æ­¥éª¤çš„è¯†åˆ«æ ¡æ­£ä¸­çš„åº”ç”¨ã€‚è¿™äº›LLMä½¿ç”¨Fisherè¯­æ–™åº“ï¼ˆä¸€ä¸ªåŒ…å«å¤§é‡è½¬å½•å¯¹è¯çš„æ•°æ®é›†ï¼‰è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬æµ‹é‡äº†è¿™äº›æ¨¡å‹åœ¨æé«˜Fisherè¯­æ–™åº“ä¸­çš„ä¿ç•™æ•°æ®é›†ä»¥åŠç‹¬ç«‹æ•°æ®é›†çš„è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æŠ¥å‘Šè¯´ï¼Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„LLMå¯ä»¥æ˜¾è‘—æé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„æ€§èƒ½ä»…é™äºä½¿ç”¨ä¸å¾®è°ƒæ—¶ä½¿ç”¨çš„ç›¸åŒASRå·¥å…·äº§ç”Ÿçš„è½¬å½•æœ¬ï¼Œä»è€Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆä¸‰ä¸ªå•ç‹¬æ¨¡å‹çš„æƒé‡ï¼Œå¼€å‘äº†ä¸€ä¸ªé›†æˆæ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½ä½¿ç”¨æ¥è‡ªä¸åŒASRå·¥å…·çš„è½¬å½•æœ¬è¿›è¡Œå¾®è°ƒã€‚é›†æˆæ¨¡å‹çš„è¡¨ç°ä¼˜äºæ¯ä¸ªASRç‰¹å®šæ¨¡å‹çš„è¡¨ç°ï¼Œè¿™è¡¨æ˜å¯èƒ½å®ç°äº†é€šç”¨å’Œç‹¬ç«‹äºASRçš„æ–¹æ³•ã€‚æˆ‘ä»¬å·²åœ¨HuggingFaceä¸Šå…¬å¼€äº†è¿™äº›æ¨¡å‹çš„æƒé‡ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/bklynhlth">https://huggingface.co/bklynhlth</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04927v3">PDF</a> </p>
<p><strong>Summary</strong><br>     ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¯­éŸ³è½¬æ–‡æœ¬çš„åå¤„ç†æ­¥éª¤å¯ä»¥æ˜¾è‘—æ”¹å–„åˆ†ç½®å‡†ç¡®åº¦ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œé€šè¿‡å¯¹Fisherè¯­æ–™åº“çš„å¤§å‹æ•°æ®é›†è¿›è¡Œå¾®è°ƒè®­ç»ƒLLMæ¨¡å‹åï¼Œå¯¹å…¶åœ¨Fisherè¯­æ–™åº“ä¸­çš„ç‹¬ç«‹æ•°æ®é›†å’Œå…¶ä»–ç‹¬ç«‹æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ã€‚æŠ¥å‘ŠæŒ‡å‡ºï¼Œç²¾ç»†è®­ç»ƒçš„LLMå¯ä»¥æ˜¾è‘—æé«˜åˆ†ç½®å‡†ç¡®æ€§ï¼Œä½†æ¨¡å‹æ€§èƒ½ä»…é™äºä½¿ç”¨ä¸å¾®è°ƒè¿‡ç¨‹ä¸­ç›¸åŒçš„è¯­éŸ³è¯†åˆ«å·¥å…·ç”Ÿæˆçš„è½¬å½•æ–‡æœ¬ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œé€šè¿‡ç»“åˆä¸‰ä¸ªç‹¬ç«‹æ¨¡å‹çš„æƒé‡å¼€å‘äº†é›†æˆæ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹é’ˆå¯¹ä¸åŒçš„è¯­éŸ³è¯†åˆ«å·¥å…·è¿›è¡Œå¾®è°ƒã€‚é›†æˆæ¨¡å‹è¡¨ç°å‡ºä¼˜äºå•ä¸€ASRç‰¹å®šæ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜å¯ä»¥å®ç°ä¸€ç§é€šç”¨ä¸”ç‹¬ç«‹äºASRçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ç”¨äºè¯­éŸ³åˆ†ç½®ä¿®æ­£åå¤„ç†ï¼Œèƒ½æ˜¾è‘—æé«˜åˆ†ç½®å‡†ç¡®åº¦ã€‚</li>
<li>LLMé€šè¿‡å¾®è°ƒè®­ç»ƒæ¥é€‚åº”ç‰¹å®šçš„è¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>LLMçš„æ¨¡å‹æ€§èƒ½å—é™äºä¸å¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„è¯­éŸ³è¯†åˆ«å·¥å…·ç›¸åŒçš„è½¬å½•æ–‡æœ¬ã€‚</li>
<li>é›†æˆæ¨¡å‹é€šè¿‡å°†ä¸åŒASRå·¥å…·ç”Ÿæˆçš„è½¬å½•æ–‡æœ¬åˆå¹¶æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é›†æˆæ¨¡å‹è¡¨ç°ä¼˜äºå•ä¸€ASRç‰¹å®šæ¨¡å‹ã€‚</li>
<li>é›†æˆæ¨¡å‹çš„æƒé‡å·²ç»å…¬å¼€æä¾›åœ¨HuggingFaceå¹³å°ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d909a192b4904c0193f8eac3aeb2f13b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20829a7d4959acd9a01dbc1a8047a7a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347d68cb6424d9e08e3906e5857ab652.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="P-SpikeSSM-Harnessing-Probabilistic-Spiking-State-Space-Models-for-Long-Range-Dependency-Tasks"><a href="#P-SpikeSSM-Harnessing-Probabilistic-Spiking-State-Space-Models-for-Long-Range-Dependency-Tasks" class="headerlink" title="P-SpikeSSM: Harnessing Probabilistic Spiking State Space Models for   Long-Range Dependency Tasks"></a>P-SpikeSSM: Harnessing Probabilistic Spiking State Space Models for   Long-Range Dependency Tasks</h2><p><strong>Authors:Malyaban Bal, Abhronil Sengupta</strong></p>
<p>Spiking neural networks (SNNs) are posited as a computationally efficient and biologically plausible alternative to conventional neural architectures, with their core computational framework primarily using the leaky integrate-and-fire (LIF) neuron model. However, the limited hidden state representation of LIF neurons, characterized by a scalar membrane potential, and sequential spike generation process, poses challenges for effectively developing scalable spiking models to address long-range dependencies in sequence learning tasks. In this study, we develop a scalable probabilistic spiking learning framework for long-range dependency tasks leveraging the fundamentals of state space models. Unlike LIF neurons that rely on the deterministic Heaviside function for a sequential process of spike generation, we introduce a SpikeSampler layer that samples spikes stochastically based on an SSM-based neuronal model while allowing parallel computations. To address non-differentiability of the spiking operation and enable effective training, we also propose a surrogate function tailored for the stochastic nature of the SpikeSampler layer. To enhance inter-neuron communication, we introduce the SpikeMixer block, which integrates spikes from neuron populations in each layer. This is followed by a ClampFuse layer, incorporating a residual connection to capture complex dependencies, enabling scalability of the model. Our models attain state-of-the-art performance among SNN models across diverse long-range dependency tasks, encompassing the Long Range Arena benchmark, permuted sequential MNIST, and the Speech Command dataset and demonstrate sparse spiking pattern highlighting its computational efficiency. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰è¢«æå‡ºä½œä¸ºä¼ ç»Ÿç¥ç»ç½‘ç»œæ¶æ„çš„è®¡ç®—æ•ˆç‡é«˜ä¸”ç”Ÿç‰©ä¸Šåˆç†çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…¶ä¸»è¦çš„è®¡ç®—æ¡†æ¶ä¸»è¦ä½¿ç”¨æ³„æ¼ç§¯åˆ†å’Œå‘å°„ï¼ˆLIFï¼‰ç¥ç»å…ƒæ¨¡å‹ã€‚ç„¶è€Œï¼ŒLIFç¥ç»å…ƒçš„éšè—çŠ¶æ€è¡¨ç¤ºæœ‰é™ï¼Œå…¶ç‰¹å¾åœ¨äºæ ‡é‡è†œç”µä½å’Œé¡ºåºè„‰å†²ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿™ä¸ºå¼€å‘å¯æ‰©å±•çš„è„‰å†²æ¨¡å‹ä»¥å¤„ç†åºåˆ—å­¦ä¹ ä»»åŠ¡ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹é•¿è·ç¦»ä¾èµ–ä»»åŠ¡å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¦‚ç‡è„‰å†²å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹çš„åŸºæœ¬åŸç†ã€‚ä¸åŒäºä¾èµ–ç¡®å®šæ€§æµ·ç»´èµ›å¾·å‡½æ•°è¿›è¡Œè„‰å†²ç”Ÿæˆåºåˆ—è¿‡ç¨‹çš„LIFç¥ç»å…ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†SpikeSamplerå±‚ï¼Œè¯¥å±‚åŸºäºSSMç¥ç»å…ƒæ¨¡å‹çš„éšæœºæ€§è¿›è¡Œè„‰å†²é‡‡æ ·ï¼ŒåŒæ—¶å…è®¸å¹¶è¡Œè®¡ç®—ã€‚ä¸ºäº†è§£å†³è„‰å†²æ“ä½œçš„ä¸å¯å¾®æ€§å’Œå®ç°æœ‰æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬è¿˜ä¸ºSpikeSamplerå±‚çš„éšæœºæ€§é‡èº«å®šåˆ¶äº†æ›¿ä»£å‡½æ•°ã€‚ä¸ºäº†å¢å¼ºç¥ç»å…ƒä¹‹é—´çš„é€šä¿¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpikeMixerå—ï¼Œè¯¥å—æ•´åˆäº†æ¯å±‚ç¥ç»å…ƒç¾¤ä½“çš„è„‰å†²ã€‚éšåæ˜¯ClampFuseå±‚ï¼Œå®ƒç»“åˆäº†æ®‹å·®è¿æ¥ä»¥æ•æ‰å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œä½¿æ¨¡å‹çš„å¯æ‰©å±•æ€§å¾—ä»¥å¢å¼ºã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒç±»å‹çš„é•¿è·ç¦»ä¾èµ–ä»»åŠ¡ä¸­è¾¾åˆ°äº†è„‰å†²ç¥ç»ç½‘ç»œæ¨¡å‹çš„æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬Long Range ArenaåŸºå‡†æµ‹è¯•ã€æ’åˆ—é¡ºåºçš„MNISTå’Œè¯­éŸ³å‘½ä»¤æ•°æ®é›†ï¼Œå¹¶æ˜¾ç¤ºå‡ºç¨€ç–çš„è„‰å†²æ¨¡å¼ï¼Œçªæ˜¾äº†å…¶è®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02923v5">PDF</a> Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„æ¦‚ç‡æ€§è„‰å†²å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³åºåˆ—å­¦ä¹ ä¸­çš„é•¿ç¨‹ä¾èµ–é—®é¢˜ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†SpikeSamplerå±‚ä»¥éšæœºé‡‡æ ·è„‰å†²ï¼Œå¹¶åˆ©ç”¨SSMç¥ç»å…ƒæ¨¡å‹å®ç°å¹¶è¡Œè®¡ç®—ã€‚ä¸ºè§£å†³è„‰å†²æ“ä½œä¸å¯å¾®åˆ†çš„é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†é’ˆå¯¹SpikeSamplerå±‚éšæœºæ€§çš„æ›¿ä»£å‡½æ•°ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†SpikeMixerå—ä»¥å¢å¼ºç¥ç»å…ƒé—´çš„é€šä¿¡ï¼Œå¹¶é€šè¿‡ClampFuseå±‚ç»“åˆæ®‹å·®è¿æ¥æ•æ‰å¤æ‚ä¾èµ–å…³ç³»ï¼Œå®ç°æ¨¡å‹çš„æ‰©å±•æ€§ã€‚è¯¥æ¨¡å‹åœ¨å¤šç§é•¿ç¨‹ä¾èµ–ä»»åŠ¡ä¸Šè¾¾åˆ°äº†è„‰å†²ç¥ç»ç½‘ç»œæ¨¡å‹çš„æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬Long Range ArenaåŸºå‡†æµ‹è¯•ã€é¡ºåºMNISTå’Œè¯­éŸ³å‘½ä»¤æ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†å…¶è®¡ç®—æ•ˆç‡é«˜çš„ç¨€ç–è„‰å†²æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SNNsä½œä¸ºä¸€ç§è®¡ç®—é«˜æ•ˆä¸”ç”Ÿç‰©ä¸Šå¯è¡Œçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œé¢ä¸´é•¿ç¨‹ä¾èµ–é—®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„SpikeSamplerå±‚ä»¥éšæœºé‡‡æ ·è„‰å†²å¹¶å®ç°å¹¶è¡Œè®¡ç®—ã€‚</li>
<li>æå‡ºæ›¿ä»£å‡½æ•°ä»¥è§£å†³è„‰å†²æ“ä½œä¸å¯å¾®åˆ†çš„é—®é¢˜ï¼Œç¡®ä¿æœ‰æ•ˆçš„è®­ç»ƒã€‚</li>
<li>SpikeMixerå—å¢å¼ºäº†ç¥ç»å…ƒé—´çš„é€šä¿¡ï¼Œè€ŒClampFuseå±‚åˆ™é€šè¿‡ç»“åˆæ®‹å·®è¿æ¥æ•æ‰å¤æ‚ä¾èµ–å…³ç³»ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šç§é•¿ç¨‹ä¾èµ–ä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬Long Range ArenaåŸºå‡†æµ‹è¯•ç­‰ã€‚</li>
<li>æ¨¡å‹å±•ç¤ºäº†å…¶è®¡ç®—æ•ˆç‡é«˜çš„ç¨€ç–è„‰å†²æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a55b665fd2de54e37283260742d81573.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c04c92946710cf83ec7b358e012aa86b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c7f6ef84b4251bd688f1f479c3887a61.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  SyncDiff Diffusion-based Talking Head Synthesis with Bottlenecked   Temporal Visual Prior for Improved Synchronization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62d448d16cf04bb54473cb20f5e239c1.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Efficient Multimodal 3D Object Detector via Instance-Level Contrastive   Distillation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23251k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
