<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-03-18  VideoMind A Chain-of-LoRA Agent for Long Video Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6e023b873a4405186db6379b15919ad2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    46 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-18-更新"><a href="#2025-03-18-更新" class="headerlink" title="2025-03-18 更新"></a>2025-03-18 更新</h1><h2 id="VideoMind-A-Chain-of-LoRA-Agent-for-Long-Video-Reasoning"><a href="#VideoMind-A-Chain-of-LoRA-Agent-for-Long-Video-Reasoning" class="headerlink" title="VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning"></a>VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning</h2><p><strong>Authors:Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou</strong></p>
<p>Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning. </p>
<blockquote>
<p>视频以其独特的时间维度为特色，要求精确且基于情境的理解，答案直接与视觉、可解释的证据相关联。尽管大型语言模型在推理能力方面取得了重大突破，但多模态推理——尤其是视频推理——仍然未得到充分探索。在这项工作中，我们引入了VideoMind，这是一个为基于时间情境的视频理解而设计的新型视频语言代理。VideoMind有两个关键的创新点：（一）我们确定了视频时间推理的必要能力，并基于角色设计了一个代理工作流程，包括一个协调不同角色的规划器、一个用于时间定位的定位器、一个评估时间间隔准确性的验证器，以及一个用于问题回答的解答器。（二）为了有效地整合这些不同的角色，我们提出了一种新颖的Chain-of-LoRA策略，通过轻量级的LoRA适配器实现无缝的角色切换，同时避免使用多个模型带来的开销，从而在效率和灵活性之间取得平衡。在14个公开基准测试上的广泛实验表明，我们的代理在多种视频理解任务上实现了最先进的性能，其中包括3项基于情境的视频问答任务、6项视频时间定位任务和5项通用视频问答任务，这凸显了其在推进视频代理和长格式时间推理方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13444v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://videomind.github.io/">https://videomind.github.io/</a></p>
<p><strong>Summary</strong>：视频具有独特的时间维度，需要精确的理解。尽管大型语言模型在推理能力方面取得了重大突破，但多模态推理，尤其是对视频的多模态推理，仍然未得到深入研究。在这项研究中，我们推出了VideoMind，这是一种用于时间定位视频理解的新型视频语言代理。VideoMind有两个关键的创新点：一是确定了视频时间推理的必要能力，并基于角色设计了一个代理工作流程；二是提出了高效的Chain-of-LoRA策略，可以在不牺牲效率的情况下灵活切换角色。在多个公共基准测试上的实验表明，VideoMind在多种视频理解任务上达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>视频理解需要精确的时间定位，大型语言模型在多模态推理方面仍需提升。</li>
<li>VideoMind是一种新型的视频语言代理，用于时间定位的视频理解。</li>
<li>VideoMind包含基于角色的代理工作流程，其中包括规划器、定位器、验证器和问答器四个关键角色。</li>
<li>引入了一种新的Chain-of-LoRA策略，以平衡效率和灵活性，实现无缝的角色切换。</li>
<li>VideoMind在多个公共基准测试上取得了最先进的性能表现。</li>
<li>VideoMind对于提升视频代理和长时间推理的效能有显著影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13444">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Agent/2503.13444v1/page_0_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86845e5943d8d4f83c93a98953ba4ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ecd8e7a2ab1ee00fecb4e352d4a7368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0ad1e51ad6980ab765146c580ff7b3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f64f9f3ca2ee88900720c4541e3ea15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54666f24476073a8e24f97694f7e00d2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Agents-Play-Thousands-of-3D-Video-Games"><a href="#Agents-Play-Thousands-of-3D-Video-Games" class="headerlink" title="Agents Play Thousands of 3D Video Games"></a>Agents Play Thousands of 3D Video Games</h2><p><strong>Authors:Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, Wei Yang</strong></p>
<p>We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL’s effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on <a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">https://zhongwen.one/projects/portal</a> . </p>
<blockquote>
<p>我们推出了PORTAL，这是一个新型框架，用于开发能够通过语言指导策略生成玩数千款3D视频游戏的人工智能代理。通过将决策问题转化为语言建模任务，我们的方法利用大型语言模型（LLM）来生成以领域特定语言（DSL）表示的行为树。这种方法消除了传统强化学习方法的计算负担，同时保留了战略深度和快速适应性。我们的框架引入了一种混合策略结构，结合了基于规则的节点和神经网络组件，既可实现高级战略推理，又能实现精确的低级控制。采用双反馈机制结合定量游戏指标和视觉语言模型分析，促进了战术和战略层面的策略迭代改进。生成的策略可即时部署、人类可解释，并能跨不同游戏环境进行推广。实验结果证明了PORTAL在数千款第一人称射击游戏（FPS）中的有效性，与传统方法相比，在开发效率、策略推广和行为多样性方面都有显著提高。PORTAL代表了游戏AI开发中的重大进展，为创建能够在数千款商业视频游戏中运行且开发成本较低的复杂代理提供了实用解决方案。关于3D视频游戏的实验结果，建议访问<a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%E3%80%82">https://zhongwen.one/projects/portal进行查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于语言指导策略生成的理念，提出了PORTAL这一新型人工智能代理开发框架，该框架能支持数千款3D视频游戏。通过把决策问题转化为语言建模任务，利用大型语言模型生成行为树，实现策略生成。此框架结合了规则节点和神经网络组件的混合策略结构，实现了高级战略推理和精确低级控制。实验结果显示，PORTAL在FPS游戏的多个环境中表现优越，显著提高开发效率、策略泛化和行为多样性。详情请访问<a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PORTAL框架通过语言指导策略生成，支持数千款3D视频游戏。</li>
<li>利用大型语言模型（LLMs）将决策问题转化为语言建模任务，生成行为树。</li>
<li>框架采用混合策略结构，结合规则节点和神经网络组件，实现战略和精确控制。</li>
<li>通过双反馈机制结合游戏指标和视觉语言模型分析，改进策略。</li>
<li>实验结果显示，PORTAL在FPS游戏中表现出高开发效率、策略泛化和行为多样性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c167cc28f9e4d1240446b3e2193eb448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b0b221d0b53ecebb1c021db30418ac3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Goal2Story-A-Multi-Agent-Fleet-based-on-Privately-Enabled-sLLMs-for-Impacting-Mapping-on-Requirements-Elicitation"><a href="#Goal2Story-A-Multi-Agent-Fleet-based-on-Privately-Enabled-sLLMs-for-Impacting-Mapping-on-Requirements-Elicitation" class="headerlink" title="Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for   Impacting Mapping on Requirements Elicitation"></a>Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for   Impacting Mapping on Requirements Elicitation</h2><p><strong>Authors:Xinkai Zou, Yan Liu, Xiongbo Shi, Chen Yang</strong></p>
<p>As requirements drift with rapid iterations, agile development becomes the dominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet challenging task in agile project development due to its heavy tangling with adaptive planning and efficient collaboration. Recently, AI agents have shown promising ability in supporting requirements analysis by saving significant time and effort for stakeholders. However, current research mainly focuses on functional RE, and research works have not been reported bridging the long journey from goal to user stories. Moreover, considering the cost of LLM facilities and the need for data and idea protection, privately hosted small-sized LLM should be further utilized in RE. To address these challenges, we propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM) framework while merely using cost-effective sLLMs for goal-driven RE. Moreover, we introduce a StorySeek dataset that contains over 1,000 user stories (USs) with corresponding goals and project context information, as well as the semi-automatic dataset construction method. For evaluation, we proposed two metrics: Factuality Hit Rate (FHR) to measure consistency between the generated USs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate the quality of the generated USs. Experimental results demonstrate that Goal2Story outperforms the baseline performance of the Super-Agent adopting powerful LLMs, while also showcasing the performance improvements in key metrics brought by CoT and Agent Profile to Goal2Story, as well as its exploration in identifying latent needs. </p>
<blockquote>
<p>随着需求的快速迭代变化，敏捷开发已成为主流的开发模式。在敏捷项目开发中，以目标驱动的需求采集（RE）是至关重要且充满挑战的任务，因为它与适应性规划和高效协作密切相关。最近，AI代理人在支持需求分析中表现出了令人瞩目的能力，为利益相关者节省了大量时间和精力。然而，当前的研究主要集中在功能RE上，尚未有研究报告填补从目标到用户故事的漫长旅程中的空白。此外，考虑到大型语言模型（LLM）设施的成本以及数据和思想保护的需求，应进一步利用私有托管的小型LLM进行RE。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13279v1">PDF</a> </p>
<p><strong>Summary</strong><br>    敏捷开发逐渐成为主流范式，要求快速迭代的需求驱动软件开发。目标驱动的需求采集（RE）是敏捷项目开发中的关键任务，但充满挑战。AI代理人在需求分析中显示出潜力，可节省利益相关者的时间和努力。然而，当前研究主要集中在功能RE上，尚未有报道跨越从目标到用户故事的漫长旅程。为解决挑战，我们提出Goal2Story，采用影响映射框架的多代理团队，仅使用成本效益高的sLLM进行目标驱动RE。我们还介绍了包含超过1000个用户故事的数据集和半自动数据集构建方法。实验结果表明，Goal2Story优于采用强大LLM的超级代理的基线性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>敏捷开发成为主流，需求快速迭代，目标驱动的需求采集（RE）在敏捷项目开发中至关重要。</li>
<li>AI代理在需求分析和用户故事生成方面显示出潜力，可以节省利益相关者的时间和努力。</li>
<li>当前研究主要集中在功能性RE上，缺乏从目标到用户故事的桥梁。</li>
<li>Goal2Story是一个采用多代理团队和影响力映射框架的解决方案，旨在解决RE中的挑战。</li>
<li>Goal2Story使用成本效益高的sLLM，而不是大型语言模型（LLM），以降低成本并保护数据。</li>
<li>介绍了StorySeek数据集和半自动数据集构建方法，用于评估用户故事的质量和一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13279">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f655c469c74d6473b62a4a9e187e1c6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ef71cfd9a555b0dbfd6ff4230546197.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2865fb19868fad7ad190b8925cc79c84.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Knowledge-Aware-Iterative-Retrieval-for-Multi-Agent-Systems"><a href="#Knowledge-Aware-Iterative-Retrieval-for-Multi-Agent-Systems" class="headerlink" title="Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"></a>Knowledge-Aware Iterative Retrieval for Multi-Agent Systems</h2><p><strong>Authors:Seyoung Song</strong></p>
<p>We introduce a novel large language model (LLM)-driven agent framework, which iteratively refines queries and filters contextual evidence by leveraging dynamically evolving knowledge. A defining feature of the system is its decoupling of external sources from an internal knowledge cache that is progressively updated to guide both query generation and evidence selection. This design mitigates bias-reinforcement loops and enables dynamic, trackable search exploration paths, thereby optimizing the trade-off between exploring diverse information and maintaining accuracy through autonomous agent decision-making. Our approach is evaluated on a broad range of open-domain question answering benchmarks, including multi-step tasks that mirror real-world scenarios where integrating information from multiple sources is critical, especially given the vulnerabilities of LLMs that lack explicit reasoning or planning capabilities. The results show that the proposed system not only outperforms single-step baselines regardless of task difficulty but also, compared to conventional iterative retrieval methods, demonstrates pronounced advantages in complex tasks through precise evidence-based reasoning and enhanced efficiency. The proposed system supports both competitive and collaborative sharing of updated context, enabling multi-agent extension. The benefits of multi-agent configurations become especially prominent as task difficulty increases. The number of convergence steps scales with task difficulty, suggesting cost-effective scalability. </p>
<blockquote>
<p>我们引入了一种新型的大型语言模型（LLM）驱动的智能代理框架，该框架通过利用动态演化的知识来迭代优化查询和过滤上下文证据。该系统的特点是将其外部来源与内部知识缓存相分离，内部知识缓存会逐步更新，以指导查询生成和证据选择。这种设计减轻了偏见加强循环，并实现了动态、可追踪的搜索探索路径，从而优化了探索多样信息与通过自主代理决策保持准确性之间的权衡。我们的方法在一系列开放域问答基准测试上进行了评估，包括多步骤任务，这些任务反映了现实世界场景，其中从多个源整合信息至关重要，尤其是鉴于缺乏明确推理或规划能力的大型语言模型的脆弱性。结果表明，所提出的系统不仅在各种任务难度上都优于单步骤基线，而且与传统迭代检索方法相比，在复杂任务中通过精确的基于证据的推理和效率提升表现出了显著优势。所提出的系统支持更新的上下文的竞争性和协作性共享，能够实现多智能体扩展。随着任务难度的增加，多智能体配置的优势变得尤为突出。收敛步骤的数量随任务难度的增加而增加，这表明其具有成本效益的可扩展性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种新型的大型语言模型驱动的智能代理框架，它通过利用不断演变的外部知识来迭代优化查询和过滤上下文证据。该系统通过将外部数据源与内部知识缓存相分离的设计，避免了偏见强化循环，实现了动态、可追踪的搜索探索路径，从而在探索多样信息和保持准确性之间取得了平衡。通过广泛的开放域问答基准测试评估，包括模拟现实世界中需要从多个来源整合信息的多步骤任务，结果表明该系统不仅在任务难度不同的情况下表现出超越单步骤基准的性能，而且在复杂任务中与传统的迭代检索方法相比也显示出显著优势。此外，该系统支持竞争性和协作性的共享更新上下文，可实现多智能体扩展，随着任务难度的增加，多智能体配置的优势变得尤为突出。该系统的成本效益也较高，收敛步骤的数量随任务难度的增加而增加。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种新型的大型语言模型驱动的智能代理框架。</li>
<li>通过利用动态演变的外部知识来迭代优化查询和过滤上下文证据。</li>
<li>通过将外部数据源与内部知识缓存分离的设计避免了偏见强化循环。</li>
<li>该系统可实现动态、可追踪的搜索探索路径，平衡探索多样信息和保持准确性。</li>
<li>在广泛的开放域问答基准测试中表现出超越单步骤基准的性能。</li>
<li>在复杂任务中显示出显著优势，通过精确的证据推理提高效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ea735dcf82fa92f07a2f7cd4e89aef79.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DAgent-A-Relational-Database-Driven-Data-Analysis-Report-Generation-Agent"><a href="#DAgent-A-Relational-Database-Driven-Data-Analysis-Report-Generation-Agent" class="headerlink" title="DAgent: A Relational Database-Driven Data Analysis Report Generation   Agent"></a>DAgent: A Relational Database-Driven Data Analysis Report Generation   Agent</h2><p><strong>Authors:Wenyi Xu, Yuren Mao, Xiaolu Zhang, Chao Zhang, Xuemei Dong, Mengfei Zhang, Jun Zhou, Yunjun Gao</strong></p>
<p>Relational database-driven data analysis (RDB-DA) report generation, which aims to generate data analysis reports after querying relational databases, has been widely applied in fields such as finance and healthcare. Typically, these tasks are manually completed by data scientists, making the process very labor-intensive and showing a clear need for automation. Although existing methods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human dependency, they cannot handle complex analytical tasks that require multi-step reasoning, cross-table associations, and synthesizing insights into reports. Moreover, there is no dataset available for developing automatic RDB-DA report generation. To fill this gap, this paper proposes an LLM agent system for RDB-DA report generation tasks, dubbed DAgent; moreover, we construct a benchmark for automatic data analysis report generation, which includes a new dataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools, and memory modules to decompose natural language questions into logically independent sub-queries, accurately retrieve key information from relational databases, and generate analytical reports that meet the requirements of completeness, correctness, and conciseness through multi-step reasoning and effective data integration. Experimental analysis on the DA-Dataset demonstrates that DAgent’s superiority in retrieval performance and analysis report generation quality, showcasing its strong potential for tackling complex database analysis report generation tasks. </p>
<blockquote>
<p>关系数据库驱动的数据分析（RDB-DA）报告生成广泛应用于金融、医疗等领域，其目标是在查询关系数据库后生成数据分析报告。通常，这些任务由数据科学家手动完成，使得流程非常劳动密集，并显示出明显的自动化需求。尽管已有方法（例如表格问答或文本到SQL）被提出以减少对人工的依赖，但它们无法处理复杂的分析任务，这些任务需要多步骤推理、跨表关联和将见解综合成报告。而且，目前尚无可用于开发自动RDB-DA报告生成的数据集。为了填补这一空白，本文提出了一个用于RDB-DA报告生成任务的大型语言模型（LLM）代理系统，称为DAgent；此外，我们构建了一个用于自动数据分析报告生成的标准基准，其中包括新的数据集DA-Dataset和评估指标。DAgent集成了规划、工具、记忆模块，能够将自然语言问题分解为逻辑上独立的子查询，准确从关系数据库中检索关键信息，并通过多步骤推理和有效的数据集成生成符合完整性、正确性和简洁性要求的分析报告。在DA-Dataset上的实验分析证明了DAgent在检索性能和分析报告生成质量上的优越性，展示了其处理复杂数据库分析报告生成任务的强大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13269v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种面向关系数据库数据分析报告生成的LLM代理系统——DAgent。针对现有方法的不足，DAgent通过整合规划、工具和记忆模块，能够处理复杂的多步推理、跨表关联和报告综合洞察力的任务。为填补自动RDB-DA报告生成领域的空白，本文还构建了一个基准测试，包括新的数据集DA-Dataset和评估指标。实验分析表明，DAgent在检索性能和分析报告生成质量方面具有显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>关系数据库驱动的数据分析报告生成（RDB-DA）在财务和医疗等领域有广泛应用，但现有方法无法处理复杂的分析任务。</li>
<li>目前缺乏针对自动RDB-DA报告生成的数据集。</li>
<li>DAgent是一个面向RDB-DA报告生成的LLM代理系统，通过整合规划、工具和记忆模块，能够处理复杂的多步推理和跨表关联任务。</li>
<li>DAgent生成的分析报告要求完整性、正确性和简洁性。</li>
<li>DA-Dataset是为自动数据分析报告生成而构建的新数据集。</li>
<li>实验分析表明，DAgent在检索性能和分析报告生成质量方面表现出卓越的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9b29889ecaa0a528d6b83c8e64e7556.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fda211450446696f993bc2d33629d7f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MAP-Evaluation-and-Multi-Agent-Enhancement-of-Large-Language-Models-for-Inpatient-Pathways"><a href="#MAP-Evaluation-and-Multi-Agent-Enhancement-of-Large-Language-Models-for-Inpatient-Pathways" class="headerlink" title="MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for   Inpatient Pathways"></a>MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for   Inpatient Pathways</h2><p><strong>Authors:Zhen Chen, Zhihao Peng, Xusheng Liang, Cheng Wang, Peigan Liang, Linsheng Zeng, Minjie Ju, Yixuan Yuan</strong></p>
<p>Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited research focused on artificial intelligence (AI) inpatient pathways systems, due to the lack of large-scale inpatient datasets. Moreover, existing medical benchmarks typically concentrated on medical question-answering and examinations, ignoring the multifaceted nature of clinical decision-making in inpatient settings. To address these gaps, we first developed the Inpatient Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database, encompassing 51,274 cases across nine triage departments and 17 major disease categories alongside 16 standardized treatment options. Then, we proposed the Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways with three clinical agents, including a triage agent managing the patient admission, a diagnosis agent serving as the primary decision maker at the department, and a treatment agent providing treatment plans. Additionally, our MAP framework includes a chief agent overseeing the inpatient pathways to guide and promote these three clinician agents. Extensive experiments showed our MAP improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant clinical compliance, outperforming three board-certified clinicians by 10%-12%, establishing a foundation for inpatient pathways systems. </p>
<blockquote>
<p>住院路径需要根据全面的患者信息进行复杂的临床决策，这对临床医生提出了巨大的挑战。尽管医疗应用中的大型语言模型（LLM）有所进展，但由于缺乏大规模的住院患者数据集，关于人工智能（AI）住院路径系统的研究有限。此外，现有的医疗基准测试通常集中在医疗问题回答和考试上，忽视了住院环境中临床决策的多元化性质。为了解决这些差距，我们首先从MIMIC-IV数据库开发住院路径决策支持（IPDS）基准测试，涵盖51274例跨越九个急诊部门和17个主要疾病类别的病例，以及16种标准化治疗方案。接着，我们提出了多智能体住院路径（MAP）框架，通过三个智能体完成住院路径，包括管理患者入院的首诊智能体、作为部门主要决策者的诊断智能体、以及提供治疗计划的医疗智能体。此外，我们的MAP框架还包括一个主管住院路径的首席智能体，以指导和促进这三个临床智能体的协作。大量实验表明，与最先进的LLM华图GPT2-13B相比，我们的MAP提高了25.10%的诊断准确率。值得注意的是，我们的MAP表现出显著的临床合规性，比三名专业认证的临床医生高出10%-12%，为住院路径系统奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13205v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>基于患者全面信息的住院路径需要复杂的临床决策，对临床医生提出了重大挑战。针对大型语言模型在医疗应用中的发展，本研究开发了住院路径决策支持（IPDS）基准测试，并提出了多智能体住院路径（MAP）框架以应对挑战。该框架包括接诊智能体、诊断智能体和治疗智能体三大智能体。通过试验显示，与最先进的HuatuoGPT2-13B大型语言模型相比，该框架的诊断准确率提高了25.10%，并在模拟实际环境中展现出良好的临床合规性，超越了三位资深临床医生的诊断准确率约达到了约百分之十至十二。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>住院路径需要基于全面的患者信息的复杂临床决策。</li>
<li>大型语言模型在医疗应用中有一定的优势，但在住院路径系统中相关研究仍有限。</li>
<li>缺乏大规模住院数据集限制了AI在住院路径的应用。</li>
<li>开发住院路径决策支持基准测试，如IPDS基准测试是解决上述挑战的重要一步。</li>
<li>MAP框架由接诊智能体、诊断智能体和治疗智能体三大智能体组成，通过分工合作实现高效的住院路径管理。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c39294afb4295677f8760adb44c0eb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-305dbfea79f4780685946cf14e10f04c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6997176d7e7957d8c9c6f01ea09568e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33832616a409866126917e8aa04d03a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-653337cbb360d5a4a37587ecc1fbf707.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Better-Sample-Efficiency-in-Multi-Agent-Reinforcement-Learning-via-Exploration"><a href="#Towards-Better-Sample-Efficiency-in-Multi-Agent-Reinforcement-Learning-via-Exploration" class="headerlink" title="Towards Better Sample Efficiency in Multi-Agent Reinforcement Learning   via Exploration"></a>Towards Better Sample Efficiency in Multi-Agent Reinforcement Learning   via Exploration</h2><p><strong>Authors:Amir Baghi, Jens Sjölund, Joakim Bergdahl, Linus Gisslén, Alessandro Sestini</strong></p>
<p>Multi-agent reinforcement learning has shown promise in learning cooperative behaviors in team-based environments. However, such methods often demand extensive training time. For instance, the state-of-the-art method TiZero takes 40 days to train high-quality policies for a football environment. In this paper, we hypothesize that better exploration mechanisms can improve the sample efficiency of multi-agent methods. We propose two different approaches for better exploration in TiZero: a self-supervised intrinsic reward and a random network distillation bonus. Additionally, we introduce architectural modifications to the original algorithm to enhance TiZero’s computational efficiency. We evaluate the sample efficiency of these approaches through extensive experiments. Our results show that random network distillation improves training sample efficiency by 18.8% compared to the original TiZero. Furthermore, we evaluate the qualitative behavior of the models produced by both variants against a heuristic AI, with the self-supervised reward encouraging possession and random network distillation leading to a more offensive performance. Our results highlights the applicability of our random network distillation variant in practical settings. Lastly, due to the nature of the proposed method, we acknowledge its use beyond football simulation, especially in environments with strong multi-agent and strategic aspects. </p>
<blockquote>
<p>多智能体强化学习在团队环境中的学习合作行为方面显示出潜力。然而，这些方法通常需要大量的训练时间。例如，目前最先进的方法TiZero在足球环境中训练高质量的策略需要40天。在本文中，我们假设更好的探索机制可以提高多智能体方法的样本效率。我们为TiZero提出了两种更好的探索方法：自我监督的内在奖励和随机网络蒸馏奖励。此外，我们对原始算法进行了架构修改，以提高TiZero的计算效率。我们通过大量的实验评估了这些方法的样本效率。结果表明，与原始TiZero相比，随机网络蒸馏将训练样本效率提高了18.8%。此外，我们将这两种变体产生的模型与启发式AI进行了定性行为评估，自我监督奖励鼓励控球，而随机网络蒸馏则导致更进攻性的表现。我们的结果突出了随机网络蒸馏变体在实际应用中的适用性。最后，由于所提方法的特点，我们认识到其不仅适用于足球模拟，尤其适用于具有强烈多智能体和战略方面的环境。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13077v1">PDF</a> 8 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本论文针对多智能体强化学习在团队环境中的合作行为学习进行了探索，发现现有方法训练时间长。为此，提出两种改进探索机制的方法，即自监督内在奖励和随机网络蒸馏奖励，并对原有算法进行架构修改以提高计算效率。实验结果显示，随机网络蒸馏提高了样本效率，并且所提出的方法在足球模拟环境中表现出更好的进攻性能。此外，该方法还可应用于其他具有强多智能体和战略方面的环境。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习在团队环境中学习合作行为具有潜力，但训练时间长。</li>
<li>提出两种改进探索机制的方法：自监督内在奖励和随机网络蒸馏奖励。</li>
<li>架构修改提高了原有算法的计算效率。</li>
<li>随机网络蒸馏提高了样本效率，相较于原始方法提升了18.8%。</li>
<li>自监督奖励鼓励持有球权，而随机网络蒸馏导致更进攻性的表现。</li>
<li>方法在足球模拟环境中表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2e549fa383dcb3453a5e7dd2439db2a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2afa8acacf44bac1d2411612b184ec31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7761dd6732d34e8aaa58facb8f002394.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5010547669abe772c9d3a55c621f4b44.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Identifying-Cooperative-Personalities-in-Multi-agent-Contexts-through-Personality-Steering-with-Representation-Engineering"><a href="#Identifying-Cooperative-Personalities-in-Multi-agent-Contexts-through-Personality-Steering-with-Representation-Engineering" class="headerlink" title="Identifying Cooperative Personalities in Multi-agent Contexts through   Personality Steering with Representation Engineering"></a>Identifying Cooperative Personalities in Multi-agent Contexts through   Personality Steering with Representation Engineering</h2><p><strong>Authors:Kenneth J. K. Ong, Lye Jia Jun, Hieu Minh “Jord” Nguyen, Seong Hah Cho, Natalia Pérez-Campanero Antolín</strong></p>
<p>As Large Language Models (LLMs) gain autonomous capabilities, their coordination in multi-agent settings becomes increasingly important. However, they often struggle with cooperation, leading to suboptimal outcomes. Inspired by Axelrod’s Iterated Prisoner’s Dilemma (IPD) tournaments, we explore how personality traits influence LLM cooperation. Using representation engineering, we steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and analyze their impact on IPD decision-making. Our results show that higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation, highlighting both the potential and limitations of personality-based steering for aligning AI agents. </p>
<blockquote>
<p>随着大型语言模型（LLM）获得自主能力，它们在多智能体环境中的协调变得愈发重要。然而，它们在合作方面经常遇到困难，导致结果不尽如人意。受艾克塞尔罗德重复囚徒困境（IPD）锦标赛的启发，我们探索了人格特质如何影响LLM合作。我们使用表征工程来引导LLM中的五大特质（例如，友善性、尽责性），并分析它们对IPD决策制定的影响。我们的结果表明，较高的友善性和尽责性可以提高合作水平，但也会增加被利用的风险，从而突出了基于人格引导在使AI智能体协同方面的潜力和局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12722v1">PDF</a> Poster, Technical AI Safety Conference 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在获得自主能力后，在多智能体环境中的协作变得尤为重要。然而，它们在合作方面经常遇到困难，导致结果不尽如人意。本研究受到阿克塞尔罗德的重复囚徒困境（IPD）锦标赛的启发，探讨了人格特质如何影响LLM的合作。通过表征工程，我们引导五大人格特质（例如，友善性、尽责性）在LLM中，并分析它们对IPD决策制定的影响。结果表明，较高的友善性和尽责性有助于提高合作能力，但同时也增加了被利用的风险，突显了基于人格引导在人工智能代理中的潜力和局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在多智能体环境中的协作至关重要。</li>
<li>LLM在合作方面面临挑战，常常导致结果不尽如人意。</li>
<li>人格特质影响LLM在IPD决策制定中的合作行为。</li>
<li>使用表征工程来引导LLM的五大人格特质（友善性、尽责性等）。</li>
<li>高友善性和尽责性能够提高LLM的合作能力。</li>
<li>然而，提高友善性和尽责性的LLM更容易受到利用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12722">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4aa0ac66632414975d75421ae11bc5ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1ca884335c9a35df8276ce30258b39d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2741cfff886cf1c24d8f1f3bee689762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cb44632131f116b68a9142d795786f2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AI-Agents-Evolution-Architecture-and-Real-World-Applications"><a href="#AI-Agents-Evolution-Architecture-and-Real-World-Applications" class="headerlink" title="AI Agents: Evolution, Architecture, and Real-World Applications"></a>AI Agents: Evolution, Architecture, and Real-World Applications</h2><p><strong>Authors:Naveen Krishnan</strong></p>
<p>This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems. </p>
<blockquote>
<p>本文探讨了人工智能代理的演变、架构和实际应用程序，从早期基于规则的形式发展到现代先进的系统，这些系统集成了大型语言模型，并拥有专门的感知、规划和工具使用模块。本文强调理论基础和实际应用部署，回顾了关键代理范式，讨论了当前评估基准的局限性，并提出了一个全面的评估框架，该框架平衡了任务的有效性、效率、鲁棒性和安全性。分析了在企业、个人助理和特殊领域的应用，并深入探讨了未来研究方向，以便建立更加灵活和适应性更强的人工智能代理系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12687v1">PDF</a> 52 pages, 4 figures, comprehensive survey and analysis of AI agent   evolution, architecture, evaluation frameworks, and applications</p>
<p><strong>Summary</strong><br>人工智能代理人的演变、架构和实际应用从早期的规则基础形态到现代集成了大型语言模型的专业系统得到了详尽的探讨。该论文强调了理论基础的现实应用部署，回顾了关键代理范式，讨论了当前评估基准的局限性，并提出了一个平衡任务有效性、效率、健壮性和安全性的全面评估框架。关于在企业、个人助理和专门领域的应用进行了深入分析，并洞察了未来更具弹性和适应性的AI代理人系统的研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能代理人从早期的规则基础形态进化到现代集成大型语言模型的系统。</li>
<li>论文探讨了AI代理人的架构演变及其实践应用。</li>
<li>论文强调AI代理人的理论基础和现实世界部署的重要性。</li>
<li>论文回顾了关键的AI代理人范式。</li>
<li>当前评估基准存在局限性，需要更全面、平衡的评估框架。</li>
<li>论文分析了AI代理人在企业、个人助理和专门领域的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-60ad82153afb709a5bcaf394396f51e5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Being-0-A-Humanoid-Robotic-Agent-with-Vision-Language-Models-and-Modular-Skills"><a href="#Being-0-A-Humanoid-Robotic-Agent-with-Vision-Language-Models-and-Modular-Skills" class="headerlink" title="Being-0: A Humanoid Robotic Agent with Vision-Language Models and   Modular Skills"></a>Being-0: A Humanoid Robotic Agent with Vision-Language Models and   Modular Skills</h2><p><strong>Authors:Haoqi Yuan, Yu Bai, Yuhui Fu, Bohan Zhou, Yicheng Feng, Xinrun Xu, Yi Zhan, Börje F. Karlsson, Zongqing Lu</strong></p>
<p>Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM’s embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0’s effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit <a target="_blank" rel="noopener" href="https://beingbeyond.github.io/being-0">https://beingbeyond.github.io/being-0</a>. </p>
<blockquote>
<p>构建能够在真实世界中的实体任务中实现人类水平性能的自主机器人代理，是人类型机器人研究的终极目标。最近的进展在高层次的认知与基础模型（FMs）以及人类机器人的低级技能发展方面都取得了重大进展。然而，直接将这两个组件结合通常会导致长期任务的鲁棒性和效率较差，以及不同模块的延迟差异。我们引入了Being-0，这是一个层次化的代理框架，它整合了基础模型和一个模块化技能库。基础模型处理高级认知任务，如指令理解、任务规划和推理，而技能库则提供稳定的运动和灵活的操控能力以实现低级控制。为了填补这些层次之间的鸿沟，我们提出了一种新型连接器模块，它由轻量级视觉语言模型（VLM）驱动。连接器通过翻译基于语言的计划为可操作的技能命令，并动态协调运动和操控能力，从而提升基础模型的实体能力。除了基础模型外，所有组件都部署在低成本的内载计算设备上，Being-0在全尺寸的人类机器人上实现了高效的实时性能，该机器人配备了灵活的双手和主动视觉。在大规模室内环境中的大量实验证明了Being-0在解决复杂、长期的导航和操控任务方面的有效性。更多详情和视频请访问：<a target="_blank" rel="noopener" href="https://beingbeyond.github.io/being-0%E3%80%82">https://beingbeyond.github.io/being-0。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12533v1">PDF</a> </p>
<p><strong>Summary</strong><br>实现自主机器人实现人类水平的真实世界任务仍是机器人研究的重要目标。近期发展高级认知的Foundation Models和低级技能的humanoid机器人已取得进展，但两者结合时仍面临鲁棒性和效率问题。我们推出Being-0，一种集成Foundation Model和模块化技能库的分层智能体框架，借助视觉语言模型来弥合高认知和低控制之间的差距。该框架可实现高效实时性能，在配备灵巧手和主动视觉的全尺寸人形机器人上展示有效完成复杂长期任务的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Building human-level performance in real-world tasks remains a key goal in humanoid robot research.</li>
<li>Recent advances have been made in both high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots.</li>
<li>Direct combination of FM and low-level skills often leads to poor robustness and efficiency due to compounding errors and module latency.</li>
<li>Being-0 introduces a hierarchical agent framework that integrates FM with a modular skill library, enhancing high-level cognitive tasks and low-level control.</li>
<li>The Connector module, powered by a lightweight vision-language model (VLM), bridges the gap between high and low levels, translating language-based plans into actionable skill commands and coordinating locomotion and manipulation.</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12533">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-016338fe0afbbe9e58269982cb278c80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e023b873a4405186db6379b15919ad2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d68ef2c5998b5230fdab2ddaaab48d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Long-Video-Audio-Synthesis-with-Multi-Agent-Collaboration"><a href="#Long-Video-Audio-Synthesis-with-Multi-Agent-Collaboration" class="headerlink" title="Long-Video Audio Synthesis with Multi-Agent Collaboration"></a>Long-Video Audio Synthesis with Multi-Agent Collaboration</h2><p><strong>Authors:Yehang Zhang, Xinli Xu, Xiaojie Xu, Li Liu, Yingcong Chen</strong></p>
<p>Video-to-audio synthesis, which generates synchronized audio for visual content, critically enhances viewer immersion and narrative coherence in film and interactive media. However, video-to-audio dubbing for long-form content remains an unsolved challenge due to dynamic semantic shifts, temporal misalignment, and the absence of dedicated datasets. While existing methods excel in short videos, they falter in long scenarios (e.g., movies) due to fragmented synthesis and inadequate cross-scene consistency. We propose LVAS-Agent, a novel multi-agent framework that emulates professional dubbing workflows through collaborative role specialization. Our approach decomposes long-video synthesis into four steps including scene segmentation, script generation, sound design and audio synthesis. Central innovations include a discussion-correction mechanism for scene&#x2F;script refinement and a generation-retrieval loop for temporal-semantic alignment. To enable systematic evaluation, we introduce LVAS-Bench, the first benchmark with 207 professionally curated long videos spanning diverse scenarios. Experiments demonstrate superior audio-visual alignment over baseline methods. Project page: <a target="_blank" rel="noopener" href="https://lvas-agent.github.io/">https://lvas-agent.github.io</a> </p>
<blockquote>
<p>视频到音频的合成技术为视觉内容生成同步音频，极大地增强了观众在电影和交互媒体中的沉浸感和叙事连贯性。然而，由于动态语义转换、时间错位以及缺乏专用数据集，长内容的视频到音频配音仍然是一个未解决的难题。现有方法在短视频上表现优异，但在长场景（如电影）中由于合成片段化和跨场景连贯性不足而表现不佳。我们提出了LVAS-Agent，一种新型多智能体框架，通过协作角色专业化模拟专业配音工作流程。我们的方法将长视频合成分解成四个步骤，包括场景分割、剧本生成、声音设计和音频合成。主要创新点包括用于场景&#x2F;剧本精细化的讨论校正机制和用于时间语义对齐的生成检索循环。为了进行系统评估，我们引入了LVAS-Bench，这是第一个包含207个专业策划的长视频、涵盖各种场景的基准测试。实验表明，该方法在音频视觉对齐方面优于基线方法。项目页面：<a target="_blank" rel="noopener" href="https://lvas-agent.github.io/">https://lvas-agent.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10719v2">PDF</a> </p>
<p><strong>Summary</strong><br>视频转音频合成技术为视觉内容生成同步音频，显著增强了电影和交互式媒体中的观众沉浸感和叙事连贯性。然而，对于长格式内容的视频转音频配音仍然存在挑战，如动态语义变化、时间不对齐和缺乏专用数据集。现有方法在短视频中表现优异，但在长场景（如电影）中因合成片段化和跨场景一致性不足而表现不佳。我们提出LVAS-Agent，一种新型多代理框架，通过协作角色专业化模拟专业配音工作流程。我们的方法将长视频合成分解为四个步骤，包括场景分割、剧本生成、声音设计和音频合成。主要创新包括用于场景&#x2F;剧本精细化的讨论校正机制和用于时间语义对齐的生成检索循环。为了进行系统评估，我们引入了LVAS-Bench，第一个包含207个专业策划的长视频、涵盖各种场景的标准基准测试。实验证明，该方法在视听对齐方面优于基准方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频转音频合成技术增强观众沉浸感和叙事连贯性。</li>
<li>长格式内容视频转音频配音面临动态语义变化、时间不对齐和缺乏数据集等挑战。</li>
<li>现有方法在长场景合成中表现不足，存在合成片段化和跨场景一致性差的问题。</li>
<li>LVAS-Agent是一种多代理框架，模拟专业配音工作流程，包括场景分割、剧本生成、声音设计和音频合成四个步骤。</li>
<li>LVAS-Agent主要创新包括讨论校正机制和生成检索循环，用于优化场景&#x2F;剧本和时空语义对齐。</li>
<li>为评估视频转音频合成技术，引入了LVAS-Bench标准基准测试，包含多种场景的长视频素材。</li>
<li>实验表明，LVAS-Agent在视听对齐方面相比基准方法有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10719">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-397e16897300cc100dc2659dd55e3e51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b219c06545e17e2f9a7896bd4118c62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd43362e17c963c21a04ff630df7453f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2bfbc97f7c2d588e909a74a86260474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b988299b3f31e9d55bfb64edbcb68f8b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Scaling-Large-Language-Model-based-Multi-Agent-Collaboration"><a href="#Scaling-Large-Language-Model-based-Multi-Agent-Collaboration" class="headerlink" title="Scaling Large Language Model-based Multi-Agent Collaboration"></a>Scaling Large Language Model-based Multi-Agent Collaboration</h2><p><strong>Authors:Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun</strong></p>
<p>Recent breakthroughs in large language model-driven autonomous agents have revealed that multi-agent collaboration often surpasses each individual through collective reasoning. Inspired by the neural scaling law–increasing neurons enhances performance, this study explores whether the continuous addition of collaborative agents can yield similar benefits. Technically, we utilize directed acyclic graphs to organize agents into a multi-agent collaboration network (MacNet), upon which their interactive reasoning is topologically orchestrated for autonomous task solving. Extensive evaluations reveal that it effectively supports collaboration among over a thousand agents, with irregular topologies outperforming regular ones. We also identify a collaborative scaling law–the overall performance follows a logistic growth pattern as agents scale, with collaborative emergence occurring earlier than traditional neural emergence. We speculate this may be because scaling agents catalyzes their multidimensional considerations during interactive reflection and refinement, thereby producing more comprehensive artifacts. The code is available at <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/ChatDev/tree/macnet">https://github.com/OpenBMB/ChatDev/tree/macnet</a>. </p>
<blockquote>
<p>近期大型语言模型驱动的自适应代理的重大突破表明，多代理协作通常通过集体推理超越个体。本研究受神经可伸缩定律的启发——增加神经元可以提高性能，探索连续添加协作代理是否能产生类似的好处。技术上，我们使用有向无环图来组织代理形成一个多代理协作网络（MacNet），在此基础上，他们的交互推理被拓扑协调用于自主任务解决。广泛的评估表明，它有效地支持了上千个代理之间的协作，不规则拓扑的性能优于规则拓扑。我们还确定了一个协作扩展定律——随着代理的扩展，总体性能遵循逻辑增长模式，协作涌现比传统的神经涌现更早出现。我们推测这可能是因为扩展代理在交互反思和细化过程中催化了他们的多维考虑，从而产生了更全面的结果。代码可在<a target="_blank" rel="noopener" href="https://github.com/OpenBMB/ChatDev/tree/macnet">https://github.com/OpenBMB/ChatDev/tree/macnet</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07155v3">PDF</a> Accepted to ICLR-2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型驱动的自适应代理的最新突破表明，多代理协作通常通过集体推理超越个体表现。本研究受神经元规模定律启发——增加神经元数量能提高性能，探讨了持续添加协作代理是否能带来类似效益。我们利用有向无环图来组织一个多代理协作网络（MacNet），在这个网络上，他们的互动推理被拓扑地协调用于自主任务解决。评估显示，该网络有效支持超过一千个代理的协作，不规则拓扑优于规则拓扑。我们还发现了协作规模定律——随着代理的扩展，总体性能遵循逻辑增长模式，协作涌现比传统神经涌现更早发生。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多代理协作通过集体推理超越个体表现。</li>
<li>利用有向无环图构建多代理协作网络（MacNet）。</li>
<li>MacNet有效支持超过一千个代理的协作。</li>
<li>不规则拓扑在代理协作中表现优于规则拓扑。</li>
<li>发现了协作规模定律，总体性能随代理扩展而遵循逻辑增长模式。</li>
<li>协作涌现比传统神经涌现更早发生。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f3b2e62f717414137500fafad9a7bdbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13767a2602b68de874417b9b5e162439.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f158181361eb30ff9f009b5f1e9373d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73847743c922dfeb542efc19c104dfa5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-16ac621e5356ed6f81ab09edccc21589.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-03-18  New Trends for Modern Machine Translation with Large Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae4aa0d01a00b084fbf0a3221e0f44c7.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-03-18  MetaScale Test-Time Scaling with Evolving Meta-Thoughts
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
