<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  U2AD Uncertainty-based Unsupervised Anomaly Detection Framework for   Detecting T2 Hyperintensity in MRI Spinal Cord">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12094v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-14-æ›´æ–°"><a href="#2025-05-14-æ›´æ–°" class="headerlink" title="2025-05-14 æ›´æ–°"></a>2025-05-14 æ›´æ–°</h1><h2 id="U2AD-Uncertainty-based-Unsupervised-Anomaly-Detection-Framework-for-Detecting-T2-Hyperintensity-in-MRI-Spinal-Cord"><a href="#U2AD-Uncertainty-based-Unsupervised-Anomaly-Detection-Framework-for-Detecting-T2-Hyperintensity-in-MRI-Spinal-Cord" class="headerlink" title="U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for   Detecting T2 Hyperintensity in MRI Spinal Cord"></a>U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for   Detecting T2 Hyperintensity in MRI Spinal Cord</h2><p><strong>Authors:Qi Zhang, Xiuyuan Chen, Ziyi He, Kun Wang, Lianming Wu, Hongxing Shen, Jianqi Sun</strong></p>
<p>T2 hyperintensities in spinal cord MR images are crucial biomarkers for conditions such as degenerative cervical myelopathy. However, current clinical diagnoses primarily rely on manual evaluation. Deep learning methods have shown promise in lesion detection, but most supervised approaches are heavily dependent on large, annotated datasets. Unsupervised anomaly detection (UAD) offers a compelling alternative by eliminating the need for abnormal data annotations. However, existing UAD methods rely on curated normal datasets and their performance frequently deteriorates when applied to clinical datasets due to domain shifts. We propose an Uncertainty-based Unsupervised Anomaly Detection framework, termed U2AD, to address these limitations. Unlike traditional methods, U2AD is designed to be trained and tested within the same clinical dataset, following a â€œmask-and-reconstructionâ€ paradigm built on a Vision Transformer-based architecture. We introduce an uncertainty-guided masking strategy to resolve task conflicts between normal reconstruction and anomaly detection to achieve an optimal balance. Specifically, we employ a Monte-Carlo sampling technique to estimate reconstruction uncertainty mappings during training. By iteratively optimizing reconstruction training under the guidance of both epistemic and aleatoric uncertainty, U2AD reduces overall reconstruction variance while emphasizing regions. Experimental results demonstrate that U2AD outperforms existing supervised and unsupervised methods in patient-level identification and segment-level localization tasks. This framework establishes a new benchmark for incorporating uncertainty guidance into UAD, highlighting its clinical utility in addressing domain shifts and task conflicts in medical image anomaly detection. Our code is available: <a target="_blank" rel="noopener" href="https://github.com/zhibaishouheilab/U2AD">https://github.com/zhibaishouheilab/U2AD</a> </p>
<blockquote>
<p>åœ¨è„Šé«“æ ¸ç£å…±æŒ¯å›¾åƒä¸­ï¼ŒT2é«˜ä¿¡å·å¼ºåº¦æ˜¯é€€è¡Œæ€§è„‘è„Šé«“ç—…å˜ç­‰ç–¾ç—…çš„å…³é”®ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ç„¶è€Œï¼Œç›®å‰çš„ä¸´åºŠè¯Šæ–­ä¸»è¦ä¾èµ–äºäººå·¥è¯„ä¼°ã€‚æ·±åº¦å­¦ä¹ åœ¨ç—…ç¶æ£€æµ‹æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¤§å¤šæ•°ç›‘ç£æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡æ¶ˆé™¤å¯¹å¼‚å¸¸æ•°æ®æ ‡æ³¨çš„éœ€è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„UADæ–¹æ³•ä¾èµ–äºç²¾é€‰çš„æ­£å¸¸æ•°æ®é›†ï¼Œå½“åº”ç”¨äºä¸´åºŠæ•°æ®é›†æ—¶ï¼Œç”±äºé¢†åŸŸå·®å¼‚ï¼Œå…¶æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºä¸ç¡®å®šæ€§çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œç§°ä¸ºU2ADã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒU2ADè¢«è®¾è®¡æˆåœ¨åŒä¸€ä¸ªä¸´åºŠæ•°æ®é›†ä¸­è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œéµå¾ªä¸€ä¸ªåŸºäºè§†è§‰Transformeræ¶æ„çš„â€œæ©è†œå’Œé‡å»ºâ€èŒƒå¼ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”±ä¸ç¡®å®šæ€§å¼•å¯¼çš„æ©è†œç­–ç•¥ï¼Œä»¥è§£å†³æ­£å¸¸é‡å»ºå’Œå¼‚å¸¸æ£€æµ‹ä¹‹é—´çš„ä»»åŠ¡å†²çªï¼Œä»¥å®ç°æœ€ä½³å¹³è¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·æŠ€æœ¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼°è®¡é‡å»ºä¸ç¡®å®šæ€§æ˜ å°„ã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–åœ¨è®¤è¯†è®ºå’Œå¶ç„¶ä¸ç¡®å®šæ€§æŒ‡å¯¼ä¸‹çš„é‡å»ºè®­ç»ƒï¼ŒU2ADé™ä½äº†æ•´ä½“é‡å»ºæ–¹å·®ï¼ŒåŒæ—¶å¼ºè°ƒäº†åŒºåŸŸé‡è¦æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒU2ADåœ¨æ‚£è€…çº§åˆ«è¯†åˆ«å’Œåˆ†æ®µçº§åˆ«å®šä½ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ºå°†ä¸ç¡®å®šæ€§æŒ‡å¯¼èå…¥UADå»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œçªæ˜¾å…¶åœ¨è§£å†³åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹ä¸­çš„é¢†åŸŸå·®å¼‚å’Œä»»åŠ¡å†²çªçš„ä¸´åºŠå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/zhibaishouheilab/U2AD">https://github.com/zhibaishouheilab/U2AD</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13400v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¡†æ¶U2ADï¼Œç”¨äºè§£å†³è„Šé«“æ ¸ç£å…±æŒ¯å›¾åƒä¸­T2é«˜ä¿¡å·ç—…ç¶æ£€æµ‹çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ— éœ€å¼‚å¸¸æ•°æ®æ ‡æ³¨ï¼Œé€šè¿‡â€œé®ç½©ä¸é‡å»ºâ€æ¨¡å¼è®­ç»ƒï¼Œå¼•å…¥ä¸ç¡®å®šæ€§å¼•å¯¼ç­–ç•¥æ¥è§£å†³æ­£å¸¸é‡å»ºä¸å¼‚å¸¸æ£€æµ‹çš„ä»»åŠ¡å†²çªã€‚å®éªŒç»“æœè¯æ˜U2ADåœ¨æ‚£è€…çº§åˆ«å’Œåˆ†æ®µçº§åˆ«çš„å®šä½ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°å‡ºå…¶åœ¨åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹ä¸­è§£å†³é¢†åŸŸåç§»å’Œä»»åŠ¡å†²çªçš„ä¸´åºŠå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2é«˜ä¿¡å·æ˜¯è„Šé«“ç—…å˜çš„å…³é”®ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œç›®å‰ä¸»è¦é€šè¿‡æ‰‹åŠ¨è¯„ä¼°è¿›è¡Œä¸´åºŠè¯Šæ–­ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨ç—…ç¶æ£€æµ‹ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¤§å¤šæ•°ç›‘ç£æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰æä¾›äº†ä¸€ç§ä¸éœ€è¦å¼‚å¸¸æ•°æ®æ ‡æ³¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰UADæ–¹æ³•ä¾èµ–äºç²¾é€‰çš„æ­£å¸¸æ•°æ®é›†ï¼Œåœ¨åº”ç”¨äºä¸´åºŠæ•°æ®é›†æ—¶æ€§èƒ½ä¼šä¸‹é™ï¼Œå­˜åœ¨é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>æå‡ºçš„U2ADæ¡†æ¶é€šè¿‡ç»“åˆä¸ç¡®å®šæ€§æŒ‡å¯¼çš„é®ç½©ç­–ç•¥å’Œâ€œé®ç½©ä¸é‡å»ºâ€æ¨¡å¼æ¥è§£å†³é¢†åŸŸåç§»å’Œä»»åŠ¡å†²çªã€‚</li>
<li>U2ADä½¿ç”¨Monte-Carloé‡‡æ ·æŠ€æœ¯ä¼°è®¡é‡å»ºä¸ç¡®å®šæ€§æ˜ å°„ï¼Œé€šè¿‡ä¼˜åŒ–é‡å»ºè®­ç»ƒæ¥å‡å°‘æ•´ä½“é‡å»ºæ–¹å·®å¹¶å¼ºè°ƒå…³é”®åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13400v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13400v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13400v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13400v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Scale-Efficient-Training-for-Large-Datasets"><a href="#Scale-Efficient-Training-for-Large-Datasets" class="headerlink" title="Scale Efficient Training for Large Datasets"></a>Scale Efficient Training for Large Datasets</h2><p><strong>Authors:Qing Zhou, Junyu Gao, Qi Wang</strong></p>
<p>The rapid growth of dataset scales has been a key driver in advancing deep learning research. However, as dataset scale increases, the training process becomes increasingly inefficient due to the presence of low-value samples, including excessive redundant samples, overly challenging samples, and inefficient easy samples that contribute little to model improvement.To address this challenge, we propose Scale Efficient Training (SeTa) for large datasets, a dynamic sample pruning approach that losslessly reduces training time. To remove low-value samples, SeTa first performs random pruning to eliminate redundant samples, then clusters the remaining samples according to their learning difficulty measured by loss. Building upon this clustering, a sliding window strategy is employed to progressively remove both overly challenging and inefficient easy clusters following an easy-to-hard curriculum.We conduct extensive experiments on large-scale synthetic datasets, including ToCa, SS1M, and ST+MJ, each containing over 3 million samples.SeTa reduces training costs by up to 50% while maintaining or improving performance, with minimal degradation even at 70% cost reduction. Furthermore, experiments on various scale real datasets across various backbones (CNNs, Transformers, and Mambas) and diverse tasks (instruction tuning, multi-view stereo, geo-localization, composed image retrieval, referring image segmentation) demonstrate the powerful effectiveness and universality of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mrazhou/SeTa">https://github.com/mrazhou/SeTa</a>. </p>
<blockquote>
<p>æ•°æ®çš„å¿«é€Ÿå¢é•¿æ˜¯æ¨åŠ¨æ·±åº¦å­¦ä¹ ç ”ç©¶å‘å±•çš„å…³é”®é©±åŠ¨åŠ›ã€‚ç„¶è€Œï¼Œéšç€æ•°æ®é›†è§„æ¨¡çš„å¢åŠ ï¼Œç”±äºå­˜åœ¨å¤§é‡ä½ä»·å€¼æ ·æœ¬ï¼ŒåŒ…æ‹¬è¿‡å¤šçš„å†—ä½™æ ·æœ¬ã€è¿‡äºæŒ‘æˆ˜çš„æ ·æœ¬ä»¥åŠå‡ ä¹æ²¡æœ‰æ”¹è¿›æ¨¡å‹æ•ˆç‡çš„æ˜“å¤„ç†æ ·æœ¬ï¼Œè®­ç»ƒè¿‡ç¨‹å˜å¾—è¶Šæ¥è¶Šä½æ•ˆã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸ºå¤§å‹æ•°æ®é›†æå‡ºäº†Scale Efficient Trainingï¼ˆSeTaï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€æ ·æœ¬ä¿®å‰ªæ–¹æ³•ï¼Œå¯æ— æŸåœ°å‡å°‘è®­ç»ƒæ—¶é—´ã€‚ä¸ºäº†å»é™¤ä½ä»·å€¼æ ·æœ¬ï¼ŒSeTaé¦–å…ˆæ‰§è¡Œéšæœºä¿®å‰ªä»¥æ¶ˆé™¤å†—ä½™æ ·æœ¬ï¼Œç„¶åæ ¹æ®æŸå¤±è¡¡é‡å…¶å­¦ä¹ éš¾åº¦å¯¹å‰©ä½™æ ·æœ¬è¿›è¡Œèšç±»ã€‚åœ¨æ­¤èšç±»çš„åŸºç¡€ä¸Šï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥é€æ­¥å»é™¤è¿‡äºæŒ‘æˆ˜å’Œæ•ˆç‡ä½çš„å®¹æ˜“é›†ç¾¤ï¼Œéµå¾ªä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹å®‰æ’ã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬ToCaã€SS1Må’ŒST+MJï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½åŒ…å«è¶…è¿‡3ç™¾ä¸‡ä¸ªæ ·æœ¬ã€‚SeTaåœ¨ä¿æŒæˆ–æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®­ç»ƒæˆæœ¬é™ä½äº†é«˜è¾¾50%ï¼Œå³ä½¿åœ¨å‡å°‘70%çš„æˆæœ¬æ—¶ä¹Ÿèƒ½ä¿æŒæœ€å°çš„æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å„ç§è§„æ¨¡çš„çœŸå®æ•°æ®é›†ã€å„ç§ä¸»å¹²ç½‘ç»œï¼ˆCNNã€Transformerå’ŒMambasï¼‰ä»¥åŠä¸åŒä»»åŠ¡ï¼ˆæŒ‡ä»¤è°ƒæ•´ã€å¤šè§†å›¾ç«‹ä½“ã€åœ°ç†å®šä½ã€ç»„åˆå›¾åƒæ£€ç´¢ã€å¼•ç”¨å›¾åƒåˆ†å‰²ï¼‰çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„å¼ºå¤§æ•ˆæœå’Œé€šç”¨æ€§ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/mrazhou/SeTa%E3%80%82">https://github.com/mrazhou/SeTaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13385v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong><br>     éšç€æ•°æ®é›†è§„æ¨¡çš„å¿«é€Ÿå¢é•¿ï¼Œä½ä»·å€¼æ ·æœ¬çš„å­˜åœ¨ä½¿å¾—è®­ç»ƒè¿‡ç¨‹è¶Šæ¥è¶Šä½æ•ˆã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§æ•°æ®é›†çš„Scale Efficient Trainingï¼ˆSeTaï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€æ ·æœ¬ä¿®å‰ªæ¥æ— æŸå‡å°‘è®­ç»ƒæ—¶é—´ã€‚é¦–å…ˆè¿›è¡Œéšæœºä¿®å‰ªä»¥æ¶ˆé™¤å†—ä½™æ ·æœ¬ï¼Œç„¶åæ ¹æ®æŸå¤±æµ‹é‡å­¦ä¹ éš¾åº¦å¯¹å‰©ä½™æ ·æœ¬è¿›è¡Œèšç±»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥é€æ­¥å»é™¤è¿‡äºå›°éš¾å’Œä¸é«˜æ•ˆçš„ç®€å•é›†ç¾¤ï¼Œéµå¾ªç”±æ˜“åˆ°éš¾çš„è¯¾ç¨‹ã€‚SeTaåœ¨å¤§å‹åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå‡å°‘äº†50%çš„è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†æ€§èƒ½ï¼Œå³ä½¿åœ¨70%çš„æˆæœ¬å‡å°‘ä¸‹ä¹Ÿå‡ ä¹æ²¡æœ‰é€€åŒ–ã€‚æ­¤å¤–ï¼Œåœ¨ä¸åŒè§„æ¨¡çš„çœŸå®æ•°æ®é›†ã€å„ç§éª¨å¹²ç½‘ç»œï¼ˆCNNã€Transformerå’ŒMambasï¼‰å’Œå¤šæ ·ä»»åŠ¡ï¼ˆæŒ‡ä»¤è°ƒä¼˜ã€å¤šè§†å›¾ç«‹ä½“ã€åœ°ç†å®šä½ã€ç»„åˆå›¾åƒæ£€ç´¢ã€å¼•ç”¨å›¾åƒåˆ†å‰²ï¼‰ä¸Šçš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é›†è§„æ¨¡çš„å¿«é€Ÿå¢é•¿æ˜¯æ¨åŠ¨æ·±åº¦å­¦ä¹ ç ”ç©¶è¿›æ­¥çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
<li>éšç€æ•°æ®é›†è§„æ¨¡çš„å¢åŠ ï¼Œè®­ç»ƒè¿‡ç¨‹å› ä½ä»·å€¼æ ·æœ¬çš„å­˜åœ¨è€Œå˜å¾—è¶Šæ¥è¶Šä½æ•ˆã€‚</li>
<li>SeTaæ˜¯ä¸€ç§é’ˆå¯¹å¤§æ•°æ®é›†çš„åŠ¨æ€æ ·æœ¬ä¿®å‰ªæ–¹æ³•ï¼Œèƒ½å¤Ÿæ— æŸå‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li>
<li>SeTaé€šè¿‡éšæœºä¿®å‰ªæ¶ˆé™¤å†—ä½™æ ·æœ¬ï¼Œç„¶åæ ¹æ®æŸå¤±æµ‹é‡å­¦ä¹ éš¾åº¦è¿›è¡Œæ ·æœ¬èšç±»ã€‚</li>
<li>SeTaé‡‡ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥é€æ­¥å»é™¤è¿‡äºå›°éš¾å’Œä¸é«˜æ•ˆçš„ç®€å•é›†ç¾¤ï¼Œéµå¾ªç”±æ˜“åˆ°éš¾çš„è®­ç»ƒè¯¾ç¨‹ã€‚</li>
<li>åœ¨å¤§å‹åˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSeTaèƒ½å‡å°‘50%çš„è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13385v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13385v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13385v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13385v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13385v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13385v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sampling-Innovation-Based-Adaptive-Compressive-Sensing"><a href="#Sampling-Innovation-Based-Adaptive-Compressive-Sensing" class="headerlink" title="Sampling Innovation-Based Adaptive Compressive Sensing"></a>Sampling Innovation-Based Adaptive Compressive Sensing</h2><p><strong>Authors:Zhifu Tian, Tao Hu, Chaoyang Niu, Di Wu, Shu Wang</strong></p>
<p>Scene-aware Adaptive Compressive Sensing (ACS) has attracted significant interest due to its promising capability for efficient and high-fidelity acquisition of scene images. ACS typically prescribes adaptive sampling allocation (ASA) based on previous samples in the absence of ground truth. However, when confronting unknown scenes, existing ACS methods often lack accurate judgment and robust feedback mechanisms for ASA, thus limiting the high-fidelity sensing of the scene. In this paper, we introduce a Sampling Innovation-Based ACS (SIB-ACS) method that can effectively identify and allocate sampling to challenging image reconstruction areas, culminating in high-fidelity image reconstruction. An innovation criterion is proposed to judge ASA by predicting the decrease in image reconstruction error attributable to sampling increments, thereby directing more samples towards regions where the reconstruction error diminishes significantly. A sampling innovation-guided multi-stage adaptive sampling (AS) framework is proposed, which iteratively refines the ASA through a multi-stage feedback process. For image reconstruction, we propose a Principal Component Compressed Domain Network (PCCD-Net), which efficiently and faithfully reconstructs images under AS scenarios. Extensive experiments demonstrate that the proposed SIB-ACS method significantly outperforms the state-of-the-art methods in terms of image reconstruction fidelity and visual effects. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/giant-pandada/SIB-ACS_CVPR2025">https://github.com/giant-pandada/SIB-ACS_CVPR2025</a>. </p>
<blockquote>
<p>åœºæ™¯æ„ŸçŸ¥è‡ªé€‚åº”å‹ç¼©æ„ŸçŸ¥ï¼ˆACSï¼‰å› å…¶å¯¹åœºæ™¯å›¾åƒé«˜æ•ˆé«˜ä¿çœŸé‡‡é›†çš„æ½œåŠ›è€Œå¤‡å—å…³æ³¨ã€‚ACSé€šå¸¸åŸºäºå…ˆå‰çš„æ ·æœ¬è¿›è¡Œè‡ªé€‚åº”é‡‡æ ·åˆ†é…ï¼ˆASAï¼‰ï¼Œä½†åœ¨ç¼ºä¹çœŸå®ä¾æ®çš„æƒ…å†µä¸‹ä¼šé¢ä¸´æŒ‘æˆ˜ã€‚å½“é¢å¯¹æœªçŸ¥åœºæ™¯æ—¶ï¼Œç°æœ‰çš„ACSæ–¹æ³•å¾€å¾€ç¼ºä¹å‡†ç¡®çš„åˆ¤æ–­å’Œç¨³å¥çš„ASAåé¦ˆæœºåˆ¶ï¼Œä»è€Œé™åˆ¶äº†åœºæ™¯çš„é«˜ä¿çœŸæ„ŸçŸ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºé‡‡æ ·åˆ›æ–°çš„ACSï¼ˆSIB-ACSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«å’Œåˆ†é…é‡‡æ ·åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾åƒé‡å»ºåŒºåŸŸï¼Œä»è€Œå®ç°é«˜ä¿çœŸå›¾åƒé‡å»ºã€‚æå‡ºäº†ä¸€ä¸ªåˆ›æ–°æ ‡å‡†æ¥åˆ¤æ–­ASAï¼Œé€šè¿‡é¢„æµ‹é‡‡æ ·å¢é‡å¯¼è‡´çš„å›¾åƒé‡å»ºè¯¯å·®å‡å°‘æ¥è¯„ä¼°ï¼Œä»è€Œå°†æ›´å¤šæ ·æœ¬å¯¼å‘é‡å»ºè¯¯å·®æ˜¾è‘—å‡å°‘çš„åŒºåŸŸã€‚æå‡ºäº†ä¸€ç§é‡‡æ ·åˆ›æ–°å¼•å¯¼çš„å¤šé˜¶æ®µè‡ªé€‚åº”é‡‡æ ·ï¼ˆASï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤šé˜¶æ®µåé¦ˆè¿‡ç¨‹è¿­ä»£ä¼˜åŒ–ASAã€‚å¯¹äºå›¾åƒé‡å»ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸»æˆåˆ†å‹ç¼©åŸŸç½‘ç»œï¼ˆPCCD-Netï¼‰ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿåœ¨ASåœºæ™¯ä¸‹é«˜æ•ˆå¿ å®åœ°é‡å»ºå›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SIB-ACSæ–¹æ³•åœ¨å›¾åƒé‡å»ºä¿çœŸåº¦å’Œè§†è§‰æ•ˆæœæ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/giant-pandada/SIB-ACS_CVPR2025%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/giant-pandada/SIB-ACS_CVPR2025è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13241v1">PDF</a> CVPR2025 accepted</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé‡‡æ ·åˆ›æ–°çš„è‡ªé€‚åº”å‹ç¼©æ„ŸçŸ¥ï¼ˆSIB-ACSï¼‰æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶åˆ†é…é‡‡æ ·åˆ°å›¾åƒé‡å»ºçš„å›°éš¾åŒºåŸŸï¼Œä»è€Œå®ç°é«˜ä¿çœŸå›¾åƒé‡å»ºã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹é‡‡æ ·å¢é‡å¯¹å›¾åƒé‡å»ºè¯¯å·®çš„å‡å°æ¥åˆ¤æ–­è‡ªé€‚åº”é‡‡æ ·åˆ†é…ï¼ˆASAï¼‰ï¼Œå¹¶å¼•å¯¼æ›´å¤šæ ·æœ¬åˆ°é‡å»ºè¯¯å·®æ˜¾è‘—å‡å°çš„åŒºåŸŸã€‚åŒæ—¶ï¼Œæå‡ºäº†é‡‡æ ·åˆ›æ–°å¼•å¯¼çš„å¤šé˜¶æ®µè‡ªé€‚åº”é‡‡æ ·æ¡†æ¶å’Œä¸»æˆåˆ†å‹ç¼©åŸŸç½‘ç»œï¼ˆPCCD-Netï¼‰ï¼Œåœ¨è‡ªé€‚åº”é‡‡æ ·åœºæ™¯ä¸‹å®ç°é«˜æ•ˆå’ŒçœŸå®çš„å›¾åƒé‡å»ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„å›¾åƒé‡å»ºä¿çœŸåº¦å’Œè§†è§‰æ•ˆæœæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SIB-ACSæ–¹æ³•èƒ½æœ‰æ•ˆåº”å¯¹æœªçŸ¥åœºæ™¯çš„å›¾åƒé‡å»ºï¼Œé€šè¿‡é‡‡æ ·åˆ›æ–°æœºåˆ¶è¯†åˆ«å¹¶åˆ†é…é‡‡æ ·åˆ°å›¾åƒé‡å»ºçš„å…³é”®åŒºåŸŸã€‚</li>
<li>å¼•å…¥åˆ›æ–°æ ‡å‡†æ¥åˆ¤æ–­ASAï¼Œé€šè¿‡é¢„æµ‹é‡‡æ ·å¢é‡å¯¹å›¾åƒé‡å»ºè¯¯å·®çš„å½±å“æ¥æŒ‡å¯¼é‡‡æ ·åˆ†é…ã€‚</li>
<li>æå‡ºäº†å¤šé˜¶æ®µè‡ªé€‚åº”é‡‡æ ·æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£åé¦ˆè¿‡ç¨‹ä¼˜åŒ–ASAã€‚</li>
<li>é‡‡ç”¨PCCD-Netè¿›è¡Œå›¾åƒé‡å»ºï¼Œèƒ½åœ¨è‡ªé€‚åº”é‡‡æ ·æ¡ä»¶ä¸‹å®ç°é«˜æ•ˆä¸”çœŸå®çš„å›¾åƒé‡å»ºã€‚</li>
<li>SIB-ACSæ–¹æ³•æ˜¾è‘—æé«˜äº†å›¾åƒé‡å»ºçš„ä¿çœŸåº¦å’Œè§†è§‰æ•ˆæœã€‚</li>
<li>æä¾›äº†ä»£ç å…¬å¼€é“¾æ¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13241v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13241v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13241v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13241v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MedLoRD-A-Medical-Low-Resource-Diffusion-Model-for-High-Resolution-3D-CT-Image-Synthesis"><a href="#MedLoRD-A-Medical-Low-Resource-Diffusion-Model-for-High-Resolution-3D-CT-Image-Synthesis" class="headerlink" title="MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D   CT Image Synthesis"></a>MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D   CT Image Synthesis</h2><p><strong>Authors:Marvin Seyfarth, Salman Ul Hassan Dar, Isabelle Ayx, Matthias Alexander Fink, Stefan O. Schoenberg, Hans-Ulrich Kauczor, Sandy Engelhardt</strong></p>
<p>Advancements in AI for medical imaging offer significant potential. However, their applications are constrained by the limited availability of data and the reluctance of medical centers to share it due to patient privacy concerns. Generative models present a promising solution by creating synthetic data as a substitute for real patient data. However, medical images are typically high-dimensional, and current state-of-the-art methods are often impractical for computational resource-constrained healthcare environments. These models rely on data sub-sampling, raising doubts about their feasibility and real-world applicability. Furthermore, many of these models are evaluated on quantitative metrics that alone can be misleading in assessing the image quality and clinical meaningfulness of the generated images. To address this, we introduce MedLoRD, a generative diffusion model designed for computational resource-constrained environments. MedLoRD is capable of generating high-dimensional medical volumes with resolutions up to 512$\times$512$\times$256, utilizing GPUs with only 24GB VRAM, which are commonly found in standard desktop workstations. MedLoRD is evaluated across multiple modalities, including Coronary Computed Tomography Angiography and Lung Computed Tomography datasets. Extensive evaluations through radiological evaluation, relative regional volume analysis, adherence to conditional masks, and downstream tasks show that MedLoRD generates high-fidelity images closely adhering to segmentation mask conditions, surpassing the capabilities of current state-of-the-art generative models for medical image synthesis in computational resource-constrained environments. </p>
<blockquote>
<p>åŒ»ç–—å½±åƒäººå·¥æ™ºèƒ½çš„è¿›æ­¥æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶åº”ç”¨å—é™äºæ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼Œä»¥åŠåŒ»ç–—ä¸­å¿ƒç”±äºæ‚£è€…éšç§æ‹…å¿§è€Œä¸æ„¿å…±äº«æ•°æ®ã€‚ç”Ÿæˆæ¨¡å‹é€šè¿‡åˆ›å»ºåˆæˆæ•°æ®ä½œä¸ºçœŸå®æ‚£è€…æ•°æ®çš„æ›¿ä»£å“ï¼Œå‘ˆç°å‡ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒåŒ»ç–—å›¾åƒé€šå¸¸æ˜¯é«˜ç»´çš„ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•å¯¹äºè®¡ç®—èµ„æºå—é™çš„åŒ»ç–—å«ç”Ÿç¯å¢ƒæ¥è¯´é€šå¸¸ä¸åˆ‡å®é™…ã€‚è¿™äº›æ¨¡å‹ä¾èµ–äºæ•°æ®å­é‡‡æ ·ï¼Œäººä»¬å¯¹å®ƒä»¬çš„å¯è¡Œæ€§å’Œç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§æŒæ€€ç–‘æ€åº¦ã€‚æ­¤å¤–ï¼Œè®¸å¤šè¿™äº›æ¨¡å‹çš„è¯„ä¼°æ˜¯åŸºäºå®šé‡æŒ‡æ ‡ï¼Œä½†ä»…ä»…ä¾é è¿™äº›æŒ‡æ ‡å¯èƒ½ä¼šè¯¯åˆ¤ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œä¸´åºŠé‡è¦æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedLoRDï¼Œè¿™æ˜¯ä¸€ç§ä¸ºè®¡ç®—èµ„æºå—é™ç¯å¢ƒè®¾è®¡çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ã€‚MedLoRDèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨24GB VRAMçš„GPUçš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆåˆ†è¾¨ç‡ä¸ºé«˜è¾¾512Ã—512Ã—256çš„é«˜ç»´åŒ»ç–—ä½“ç§¯å›¾åƒï¼Œè¿™äº›GPUåœ¨æ ‡å‡†å°å¼å·¥ä½œç«™ä¸­å¾ˆå¸¸è§ã€‚MedLoRDåœ¨å¤šæ¨¡å¼æ€ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å† çŠ¶åŠ¨è„‰è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½±å’Œè‚ºéƒ¨è®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®é›†ã€‚é€šè¿‡æ”¾å°„å­¦è¯„ä¼°ã€ç›¸å¯¹åŒºåŸŸä½“ç§¯åˆ†æã€éµå¾ªæ¡ä»¶æ©è†œå’Œä¸‹æ¸¸ä»»åŠ¡çš„å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒMedLoRDç”Ÿæˆçš„å›¾åƒé«˜åº¦é€¼çœŸï¼Œç´§å¯†éµå¾ªåˆ†å‰²æ©è†œæ¡ä»¶ï¼Œè¶…è¶Šäº†å½“å‰å…ˆè¿›åŒ»ç–—å›¾åƒåˆæˆç”Ÿæˆæ¨¡å‹åœ¨è®¡ç®—èµ„æºå—é™ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13211v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å…ˆè¿›çš„AIåŒ»ç–—å½±åƒæŠ€æœ¯å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å—é™äºæ•°æ®å¯ç”¨æ€§å’ŒåŒ»ç–—ä¸­å¿ƒå› éšç§æ‹…å¿§ä¸æ„¿å…±äº«æ•°æ®ã€‚ç”Ÿæˆæ¨¡å‹é€šè¿‡åˆ›å»ºåˆæˆæ•°æ®ä½œä¸ºçœŸå®æ‚£è€…æ•°æ®çš„æ›¿ä»£å“å±•ç°å‡ºè§£å†³æ­¤é—®é¢˜çš„å‰æ™¯ã€‚ç„¶è€Œï¼ŒåŒ»ç–—å›¾åƒé€šå¸¸å…·æœ‰é«˜ç»´åº¦æ€§ï¼Œç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•å¯¹äºè®¡ç®—èµ„æºæœ‰é™çš„åŒ»ç–—ç¯å¢ƒæ¥è¯´ä¸å¤ªå®ç”¨ã€‚å®ƒä»¬ä¾èµ–äºæ•°æ®å­é‡‡æ ·ï¼Œå¼•å‘å¯¹å…¶å¯è¡Œæ€§å’Œå®é™…åº”ç”¨èƒ½åŠ›çš„è´¨ç–‘ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ¨¡å‹çš„è¯„ä¼°ä»…ä¾èµ–å®šé‡æŒ‡æ ‡ï¼Œè¿™å¯èƒ½æ— æ³•å‡†ç¡®è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œä¸´åºŠæ„ä¹‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedLoRDï¼Œä¸€ç§é€‚ç”¨äºè®¡ç®—èµ„æºå—é™ç¯å¢ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ã€‚MedLoRDèƒ½å¤Ÿåˆ©ç”¨ä»…æœ‰24GB VRAMçš„GPUç”Ÿæˆé«˜åˆ†è¾¨ç‡ï¼ˆé«˜è¾¾512Ã—512Ã—256ï¼‰çš„åŒ»ç–—ä½“ç§¯å›¾åƒï¼Œè¿™äº›GPUåœ¨æ ‡å‡†å°å¼å·¥ä½œç«™ä¸­å¾ˆå¸¸è§ã€‚MedLoRDåœ¨å¤šæ¨¡æ€ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å† çŠ¶åŠ¨è„‰è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½±å’Œè‚ºéƒ¨è®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®é›†ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ”¾å°„å­¦è¯„ä¼°ã€ç›¸å¯¹åŒºåŸŸä½“ç§¯åˆ†æã€éµå¾ªæ¡ä»¶æ©è†œå’Œä¸‹æ¸¸ä»»åŠ¡ç­‰ï¼Œè¯æ˜MedLoRDç”Ÿæˆçš„å›¾åƒå…·æœ‰é«˜ä¿çœŸåº¦ï¼Œç´§å¯†éµå¾ªåˆ†å‰²æ©è†œæ¡ä»¶ï¼Œè¶…è¶Šäº†å½“å‰å…ˆè¿›ç”Ÿæˆæ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åŒ»ç–—å›¾åƒåˆæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨åŒ»ç–—å½±åƒé¢†åŸŸçš„è¿›å±•å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å—é™äºæ•°æ®å¯ç”¨æ€§å’Œéšç§ä¿æŠ¤é—®é¢˜ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹é€šè¿‡åˆ›å»ºåˆæˆæ•°æ®ä¸ºè§£å†³æ•°æ®éšç§é—®é¢˜æä¾›äº†å‰æ™¯ã€‚</li>
<li>åŒ»ç–—å›¾åƒçš„é«˜ç»´åº¦æ€§ä½¿å¾—ç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•åœ¨è®¡ç®—èµ„æºæœ‰é™çš„åŒ»ç–—ç¯å¢ƒä¸­åº”ç”¨å—é™ã€‚</li>
<li>ç°æœ‰æ¨¡å‹è¿‡åº¦ä¾èµ–æ•°æ®å­é‡‡æ ·ï¼Œå¼•å‘å¯¹å…¶å¯è¡Œæ€§å’Œå®é™…åº”ç”¨èƒ½åŠ›çš„è´¨ç–‘ã€‚</li>
<li>è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„æŒ‡æ ‡éœ€è¦ç»¼åˆè€ƒè™‘å®šé‡å’Œä¸´åºŠæ„ä¹‰çš„è¯„ä¼°ã€‚</li>
<li>MedLoRDæ˜¯ä¸€ç§é€‚ç”¨äºè®¡ç®—èµ„æºå—é™ç¯å¢ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜åˆ†è¾¨ç‡åŒ»ç–—ä½“ç§¯å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13211v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13211v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13211v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13211v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13211v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-zero-shot-learning-in-medical-imaging-integrating-clip-with-advanced-techniques-for-improved-chest-x-ray-analysis"><a href="#Enhancing-zero-shot-learning-in-medical-imaging-integrating-clip-with-advanced-techniques-for-improved-chest-x-ray-analysis" class="headerlink" title="Enhancing zero-shot learning in medical imaging: integrating clip with   advanced techniques for improved chest x-ray analysis"></a>Enhancing zero-shot learning in medical imaging: integrating clip with   advanced techniques for improved chest x-ray analysis</h2><p><strong>Authors:Prakhar Bhardwaj, Sheethal Bhat, Andreas Maier</strong></p>
<p>Due to the large volume of medical imaging data, advanced AI methodologies are needed to assist radiologists in diagnosing thoracic diseases from chest X-rays (CXRs). Existing deep learning models often require large, labeled datasets, which are scarce in medical imaging due to the time-consuming and expert-driven annotation process. In this paper, we extend the existing approach to enhance zero-shot learning in medical imaging by integrating Contrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo), resulting in our proposed model, MoCoCLIP. Our method addresses challenges posed by class-imbalanced and unlabeled datasets, enabling improved detection of pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset demonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model, achieving relative improvement of approximately 6.5%. Furthermore, on the CheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance, achieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC, highlighting its enhanced generalization capabilities on unseen data. </p>
<blockquote>
<p>ç”±äºåŒ»å­¦æˆåƒæ•°æ®é‡å¤§ï¼Œéœ€è¦å…ˆè¿›çš„AIæ–¹æ³•æ¥è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿä»èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRsï¼‰è¯Šæ–­èƒ¸éƒ¨ç–¾ç—…ã€‚ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡æœ‰æ ‡ç­¾çš„æ•°æ®é›†ï¼Œä½†ç”±äºè€—æ—¶ä¸”ä¾èµ–ä¸“å®¶æ ‡æ³¨çš„è¿‡ç¨‹ï¼ŒåŒ»å­¦æˆåƒä¸­è¿™ç§æ•°æ®é›†éå¸¸ç¨€ç¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ•´åˆContrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰å’ŒMomentum Contrastï¼ˆMoCoï¼‰ï¼Œæ”¹è¿›äº†ç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†åŒ»å­¦æˆåƒä¸­çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä»è€Œæå‡ºäº†æˆ‘ä»¬çš„æ¨¡å‹MoCoCLIPã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†ç”±ç±»åˆ«ä¸å¹³è¡¡å’Œæ— æ ‡ç­¾æ•°æ®é›†å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä½¿è‚ºéƒ¨ç—…ç†æ£€æµ‹å¾—ä»¥æ”¹è¿›ã€‚åœ¨NIH ChestXray14æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMoCoCLIPä¼˜äºæœ€æ–°çš„CheXZeroæ¨¡å‹ï¼Œç›¸å¯¹æ”¹è¿›ç‡çº¦ä¸º6.5%ã€‚æ­¤å¤–ï¼Œåœ¨CheXpertæ•°æ®é›†ä¸Šï¼ŒMoCoCLIPå±•ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹³å‡AUCä¸º0.750ï¼Œé«˜äºCheXZeroçš„0.746 AUCï¼Œçªæ˜¾å‡ºå…¶åœ¨æœªè§æ•°æ®ä¸Šçš„å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13134v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•å¤„ç†å¤§é‡åŒ»å­¦å›¾åƒæ•°æ®æ—¶ï¼Œå› éœ€è¦æ ‡æ³¨çš„æ•°æ®é›†è¾ƒä¸ºç¨€ç¼ºï¼Œç°æœ‰æ¨¡å‹åœ¨è¯Šæ–­èƒ¸éƒ¨ç–¾ç—…æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºContrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰å’ŒMomentum Contrastï¼ˆMoCoï¼‰ç»“åˆçš„é›¶æ ·æœ¬å­¦ä¹ å¢å¼ºæ–¹æ³•ï¼Œå³MoCoCLIPæ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè§£å†³ç±»ä¸å¹³è¡¡å’Œæ— æ ‡ç­¾æ•°æ®é›†çš„é—®é¢˜ï¼Œæé«˜äº†è‚ºéƒ¨ç—…ç†çš„æ£€æµ‹èƒ½åŠ›ã€‚åœ¨NIH ChestXray14æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMoCoCLIPç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„CheXZeroæ¨¡å‹æœ‰çº¦6.5%çš„ç›¸å¯¹æ”¹è¿›ã€‚åœ¨CheXpertæ•°æ®é›†ä¸Šï¼ŒMoCoCLIPå±•ç°å‡ºæ›´å‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹³å‡AUCè¾¾åˆ°0.750ï¼Œé«˜äºCheXZeroæ¨¡å‹çš„0.746 AUCï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æœªè§æ•°æ®ä¸Šçš„ä¼˜ç§€æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå¤§æ•°æ®èƒŒæ™¯ä¸‹ï¼ŒAIæ–¹æ³•è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè¯Šæ–­èƒ¸éƒ¨ç–¾ç—…çš„éœ€æ±‚è¿«åˆ‡ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹å› æ ‡æ³¨æ•°æ®é›†ç¨€ç¼ºé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºç»“åˆContrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰å’ŒMomentum Contrastï¼ˆMoCoï¼‰çš„MoCoCLIPæ¨¡å‹ï¼Œå¢å¼ºé›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>MoCoCLIPæ¨¡å‹èƒ½è§£å†³ç±»ä¸å¹³è¡¡å’Œæ— æ ‡ç­¾æ•°æ®é›†é—®é¢˜ï¼Œæé«˜è‚ºéƒ¨ç—…ç†æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>NIH ChestXray14æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒMoCoCLIPç›¸å¯¹æ”¹è¿›çº¦6.5%ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>åœ¨CheXpertæ•°æ®é›†ä¸Šï¼ŒMoCoCLIPå±•ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹³å‡AUCé«˜äºCheXZeroæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13134v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13134v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13134v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13134v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13134v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HiMTok-Learning-Hierarchical-Mask-Tokens-for-Image-Segmentation-with-Large-Multimodal-Model"><a href="#HiMTok-Learning-Hierarchical-Mask-Tokens-for-Image-Segmentation-with-Large-Multimodal-Model" class="headerlink" title="HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with   Large Multimodal Model"></a>HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with   Large Multimodal Model</h2><p><strong>Authors:Tao Wang, Changxu Cheng, Lingfeng Wang, Senda Chen, Wuyue Zhao</strong></p>
<p>The remarkable performance of large multimodal models (LMMs) has attracted significant interest from the image segmentation community. To align with the next-token-prediction paradigm, current LMM-driven segmentation methods either use object boundary points to represent masks or introduce special segmentation tokens, whose hidden states are decoded by a segmentation model requiring the original image as input. However, these approaches often suffer from inadequate mask representation and complex architectures, limiting the potential of LMMs. In this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which represents segmentation masks with up to 32 tokens and eliminates the need for the original image during mask de-tokenization. HiMTok allows for compact and coarse-to-fine mask representations, aligning well with the LLM next-token-prediction paradigm and facilitating the direct acquisition of segmentation capabilities. We develop a 3-stage training recipe for progressive learning of segmentation and visual capabilities, featuring a hierarchical mask loss for effective coarse-to-fine learning. Additionally, we enable bidirectional information flow, allowing conversion between bounding boxes and mask tokens to fully leverage multi-task training potential. Extensive experiments demonstrate that our method achieves state-of-the-art performance across various segmentation tasks,while also enhancing visual grounding and maintaining overall visual understanding. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºè‰²æ€§èƒ½å¼•èµ·äº†å›¾åƒåˆ†å‰²ç•Œçš„æå¤§å…´è¶£ã€‚ä¸ºäº†ä¸ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼ç›¸ä¸€è‡´ï¼Œå½“å‰çš„LMMé©±åŠ¨çš„åˆ†å‰²æ–¹æ³•è¦ä¹ˆä½¿ç”¨å¯¹è±¡è¾¹ç•Œç‚¹æ¥è¡¨ç¤ºæ©è†œï¼Œè¦ä¹ˆå¼•å…¥ç‰¹æ®Šçš„åˆ†å‰²ä»¤ç‰Œï¼Œå…¶éšè—çŠ¶æ€ç”±éœ€è¦åŸå§‹å›¾åƒä½œä¸ºè¾“å…¥çš„åˆ†å‰²æ¨¡å‹è§£ç ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨æ©è†œè¡¨ç¤ºä¸è¶³å’Œæ¶æ„å¤æ‚çš„é—®é¢˜ï¼Œé™åˆ¶äº†LMMçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚æ©è†œä»¤ç‰ŒåŒ–å™¨ï¼ˆHiMTokï¼‰ï¼Œå®ƒå¯ä»¥ç”¨é«˜è¾¾32ä¸ªä»¤ç‰Œæ¥è¡¨ç¤ºåˆ†å‰²æ©è†œï¼Œå¹¶åœ¨æ©è†œåä»¤ç‰ŒåŒ–è¿‡ç¨‹ä¸­æ¶ˆé™¤äº†å¯¹åŸå§‹å›¾åƒçš„éœ€æ±‚ã€‚HiMTokå…è®¸ç´§å‡‘å’Œç”±ç²—åˆ°ç»†çš„æ©è†œè¡¨ç¤ºï¼Œå¾ˆå¥½åœ°ä¸LLMçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼ç›¸ä¸€è‡´ï¼Œå¹¶ä¿ƒè¿›äº†ç›´æ¥è·å–åˆ†å‰²èƒ½åŠ›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13026v1">PDF</a> technical report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå±‚æ¬¡æ©è†œä»¤ç‰ŒåŒ–ï¼ˆHiMTokï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æœ€å¤š32ä¸ªä»¤ç‰Œè¡¨ç¤ºåˆ†å‰²æ©è†œï¼Œæ— éœ€åŸå§‹å›¾åƒå³å¯è¿›è¡Œæ©è†œå»ä»¤ç‰ŒåŒ–ã€‚HiMTokæ”¯æŒç´§å‡‘ä¸”ä»ç²—åˆ°ç»†çš„æ©è†œè¡¨ç¤ºï¼Œä¸LLMçš„next-tokené¢„æµ‹èŒƒå¼å¯¹é½ï¼Œä¿ƒè¿›ç›´æ¥è·å–åˆ†å‰²èƒ½åŠ›ã€‚é€šè¿‡åˆ†é˜¶æ®µè®­ç»ƒé…æ–¹å’Œå±‚æ¬¡æ©è†œæŸå¤±ï¼Œå®ç°åˆ†å‰²å’Œè§†è§‰èƒ½åŠ›çš„æ¸è¿›å­¦ä¹ ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯ç”¨è¾¹ç•Œæ¡†å’Œæ©è†œä»¤ç‰Œä¹‹é—´çš„åŒå‘ä¿¡æ¯æµï¼Œå……åˆ†åˆ©ç”¨å¤šä»»åŠ¡è®­ç»ƒçš„æ½œåŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼ŒåŒæ—¶æé«˜è§†è§‰å®šä½èƒ½åŠ›å¹¶ä¿æŒæ•´ä½“è§†è§‰ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>å½“å‰LMMé©±åŠ¨çš„åˆ†å‰²æ–¹æ³•å¸¸ä½¿ç”¨å¯¹è±¡è¾¹ç•Œç‚¹æˆ–ç‰¹æ®Šåˆ†å‰²ä»¤ç‰Œè¡¨ç¤ºæ©è†œï¼Œä½†å­˜åœ¨ä¸è¶³ã€‚</li>
<li>HiMTokæ–¹æ³•æå‡ºåˆ©ç”¨æœ€å¤š32ä¸ªä»¤ç‰Œè¡¨ç¤ºåˆ†å‰²æ©è†œï¼Œæ— éœ€åŸå§‹å›¾åƒè¿›è¡Œå»ä»¤ç‰ŒåŒ–ã€‚</li>
<li>HiMTokæ”¯æŒç´§å‡‘ä¸”ä»ç²—åˆ°ç»†çš„æ©è†œè¡¨ç¤ºï¼Œä¸LLMçš„next-tokené¢„æµ‹èŒƒå¼å¯¹é½ã€‚</li>
<li>3é˜¶æ®µè®­ç»ƒé…æ–¹å’Œå±‚æ¬¡æ©è†œæŸå¤±ç”¨äºæ¸è¿›å­¦ä¹ åˆ†å‰²å’Œè§†è§‰èƒ½åŠ›ã€‚</li>
<li>åŒå‘ä¿¡æ¯æµæŠ€æœ¯ç”¨äºå……åˆ†åˆ©ç”¨å¤šä»»åŠ¡è®­ç»ƒçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13026v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13026v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13026v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13026v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13026v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Test-Time-Domain-Generalization-via-Universe-Learning-A-Multi-Graph-Matching-Approach-for-Medical-Image-Segmentation"><a href="#Test-Time-Domain-Generalization-via-Universe-Learning-A-Multi-Graph-Matching-Approach-for-Medical-Image-Segmentation" class="headerlink" title="Test-Time Domain Generalization via Universe Learning: A Multi-Graph   Matching Approach for Medical Image Segmentation"></a>Test-Time Domain Generalization via Universe Learning: A Multi-Graph   Matching Approach for Medical Image Segmentation</h2><p><strong>Authors:Xingguo Lv, Xingbo Dong, Liwen Wang, Jiewen Yang, Lei Zhao, Bin Pu, Zhe Jin, Xuejun Li</strong></p>
<p>Despite domain generalization (DG) has significantly addressed the performance degradation of pre-trained models caused by domain shifts, it often falls short in real-world deployment. Test-time adaptation (TTA), which adjusts a learned model using unlabeled test data, presents a promising solution. However, most existing TTA methods struggle to deliver strong performance in medical image segmentation, primarily because they overlook the crucial prior knowledge inherent to medical images. To address this challenge, we incorporate morphological information and propose a framework based on multi-graph matching. Specifically, we introduce learnable universe embeddings that integrate morphological priors during multi-source training, along with novel unsupervised test-time paradigms for domain adaptation. This approach guarantees cycle-consistency in multi-matching while enabling the model to more effectively capture the invariant priors of unseen data, significantly mitigating the effects of domain shifts. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches on two medical image segmentation benchmarks for both multi-source and single-source domain generalization tasks. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/Yore0/TTDG-MGM">https://github.com/Yore0/TTDG-MGM</a>. </p>
<blockquote>
<p>å°½ç®¡é¢†åŸŸæ³›åŒ–ï¼ˆDGï¼‰å·²ç»æ˜¾è‘—è§£å†³äº†é¢„è®­ç»ƒæ¨¡å‹å› é¢†åŸŸå·®å¼‚å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­å¸¸å¸¸å­˜åœ¨ä¸è¶³ã€‚æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ˜¯ä¸€ç§ä½¿ç”¨æ— æ ‡ç­¾æµ‹è¯•æ•°æ®è°ƒæ•´å·²å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•ï¼Œå±•ç°å‡ºäº†ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„TTAæ–¹æ³•åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­å¾ˆéš¾æä¾›å¼ºå¤§çš„æ€§èƒ½ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬å¿½ç•¥äº†åŒ»ç–—å›¾åƒæ‰€å›ºæœ‰çš„å…³é”®å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬èå…¥äº†å½¢æ€ä¿¡æ¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå¤šå›¾åŒ¹é…ï¼ˆMGMï¼‰çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å­¦ä¹ çš„å®‡å®™åµŒå…¥ï¼ˆuniverse embeddingsï¼‰ï¼Œåœ¨å¤šæºè®­ç»ƒè¿‡ç¨‹ä¸­æ•´åˆå½¢æ€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨äº†æ–°å‹çš„æ— ç›‘ç£æµ‹è¯•æ—¶é—´èŒƒå¼æ¥è¿›è¡Œé¢†åŸŸé€‚åº”ã€‚è¿™ç§æ–¹æ³•ä¿è¯äº†å¤šåŒ¹é…ä¸­çš„å¾ªç¯ä¸€è‡´æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•è·æœªè§æ•°æ®çš„æ’å®šå…ˆéªŒçŸ¥è¯†ï¼Œæ˜¾è‘—å‡è½»äº†é¢†åŸŸå·®å¼‚çš„å½±å“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ç§åŒ»ç–—å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ— è®ºæ˜¯å¤šæºè¿˜æ˜¯å•æºé¢†åŸŸæ³›åŒ–ä»»åŠ¡ï¼Œéƒ½è¶…è¿‡äº†å…¶ä»–å…ˆè¿›çš„æ–¹æ³•ã€‚æºä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Yore0/TTDG-MGM%E3%80%82">https://github.com/Yore0/TTDG-MGMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13012v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ‘˜è¦ä»‹ç»äº†å°½ç®¡åŸŸæ³›åŒ–ï¼ˆDGï¼‰åœ¨è§£å†³é¢„è®­ç»ƒæ¨¡å‹å› åŸŸåç§»å¯¼è‡´çš„æ€§èƒ½ä¸‹é™æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­ä»å­˜åœ¨ä¸è¶³ã€‚æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTTAï¼‰ä½œä¸ºä¸€ç§ä½¿ç”¨æ— æ ‡ç­¾æµ‹è¯•æ•°æ®è¿›è¡Œæ¨¡å‹è°ƒæ•´çš„æ–¹æ³•ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†å¸Œæœ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„TTAæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬å¿½ç•¥äº†åŒ»å­¦å›¾åƒä¸­å›ºæœ‰çš„å…³é”®å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡ç»“åˆäº†å½¢æ€ä¿¡æ¯ï¼Œå¹¶æå‡ºäº†åŸºäºå¤šå›¾åŒ¹é…ï¼ˆMGMï¼‰çš„æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„å®‡å®™åµŒå…¥ï¼ˆuniverse embeddingsï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæºè®­ç»ƒè¿‡ç¨‹ä¸­èå…¥äº†å½¢æ€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨äº†æ–°å‹çš„æ— ç›‘ç£æµ‹è¯•æ—¶é—´èŒƒå¼è¿›è¡ŒåŸŸé€‚åº”ã€‚æ­¤æ–¹æ³•ä¿è¯äº†å¤šåŒ¹é…ä¸­çš„å¾ªç¯ä¸€è‡´æ€§ï¼Œä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ•è·æœªè§æ•°æ®çš„å›ºæœ‰å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œæ˜¾è‘—å‡è½»äº†åŸŸåç§»çš„å½±å“ã€‚åœ¨ä¸¤é¡¹åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå¯¹äºå¤šæºå’Œå•æºåŸŸæ³›åŒ–ä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å‡ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚ç›¸å…³æºä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Yore0/TTDG-MGM">https://github.com/Yore0/TTDG-MGM</a> ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTTAï¼‰æ˜¯è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­åŸŸæ³›åŒ–é—®é¢˜çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰çš„TTAæ–¹æ³•å¿½ç•¥åŒ»å­¦å›¾åƒä¸­çš„å…³é”®å…ˆéªŒçŸ¥è¯†ï¼Œè¿™åœ¨ç°å®åº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šå›¾åŒ¹é…ï¼ˆMGMï¼‰çš„æ¡†æ¶ï¼Œç»“åˆå½¢æ€ä¿¡æ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¯å­¦ä¹ çš„å®‡å®™åµŒå…¥ï¼Œä»¥åœ¨å¤šæºè®­ç»ƒè¿‡ç¨‹ä¸­èå…¥å½¢æ€å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é‡‡ç”¨æ–°å‹çš„æ— ç›‘ç£æµ‹è¯•æ—¶é—´èŒƒå¼è¿›è¡ŒåŸŸé€‚åº”ï¼Œç¡®ä¿å¾ªç¯ä¸€è‡´æ€§å¹¶æœ‰æ•ˆæ•è·æœªè§æ•°æ®çš„å›ºæœ‰å…ˆéªŒã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸¤é¡¹åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
<li>æä¾›æºä»£ç é“¾æ¥ä¾›ç ”ç©¶è€…å’Œå¼€å‘è€…å‚è€ƒå’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13012v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.13012v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UniReg-Foundation-Model-for-Controllable-Medical-Image-Registration"><a href="#UniReg-Foundation-Model-for-Controllable-Medical-Image-Registration" class="headerlink" title="UniReg: Foundation Model for Controllable Medical Image Registration"></a>UniReg: Foundation Model for Controllable Medical Image Registration</h2><p><strong>Authors:Zi Li, Jianpeng Zhang, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, Zeli Chen, Xianghua Ye, Le Lu, Dakai Jin</strong></p>
<p>Learning-based medical image registration has achieved performance parity with conventional methods while demonstrating a substantial advantage in computational efficiency. However, learning-based registration approaches lack generalizability across diverse clinical scenarios, requiring the laborious development of multiple isolated networks for specific registration tasks, e.g., inter-&#x2F;intra-subject registration or organ-specific alignment. % To overcome this limitation, we propose \textbf{UniReg}, the first interactive foundation model for medical image registration, which combines the precision advantages of task-specific learning methods with the generalization of traditional optimization methods. Our key innovation is a unified framework for diverse registration scenarios, achieved through a conditional deformation field estimation within a unified registration model. This is realized through a dynamic learning paradigm that explicitly encodes: (1) anatomical structure priors, (2) registration type constraints (inter&#x2F;intra-subject), and (3) instance-specific features, enabling the generation of scenario-optimal deformation fields. % Through comprehensive experiments encompassing $90$ anatomical structures at different body regions, our UniReg model demonstrates comparable performance with contemporary state-of-the-art methodologies while achieving ~50% reduction in required training iterations relative to the conventional learning-based paradigm. This optimization contributes to a significant reduction in computational resources, such as training time. Code and model will be available. </p>
<blockquote>
<p>åŸºäºå­¦ä¹ çš„åŒ»å­¦å›¾åƒé…å‡†æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå·²ç»è¾¾åˆ°ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¹³è¡¡ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒåŸºäºå­¦ä¹ çš„é…å‡†æ–¹æ³•åœ¨ä¸´åºŠåœºæ™¯çš„é€šç”¨æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé’ˆå¯¹ç‰¹å®šçš„é…å‡†ä»»åŠ¡ï¼ˆå¦‚è·¨å—è¯•è€…æˆ–è·¨åŒä¸€å—è¯•è€…å†…çš„é…å‡†æˆ–ç‰¹å®šå™¨å®˜çš„å¯¹é½ï¼‰éœ€è¦è´¹åŠ›åœ°å¼€å‘å¤šä¸ªç‹¬ç«‹çš„ç½‘ç»œã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†UniRegï¼Œè¿™æ˜¯é¦–ä¸ªåŒ»å­¦å›¾åƒé…å‡†çš„äº¤äº’å¼åŸºç¡€æ¨¡å‹ï¼Œå®ƒå°†ä»»åŠ¡ç‰¹å®šå­¦ä¹ æ–¹æ³•çš„ç²¾ç¡®ä¼˜åŠ¿ä¸ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•çš„é€šç”¨æ€§ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åœ¨äºä¸ºå„ç§é…å‡†åœºæ™¯æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¿™æ˜¯é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„é…å‡†æ¨¡å‹å†…çš„æ¡ä»¶å˜å½¢åœºä¼°è®¡æ¥å®ç°çš„ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªåŠ¨æ€å­¦ä¹ èŒƒå¼å®ç°çš„ï¼Œè¯¥èŒƒå¼æ˜¾å¼åœ°ç¼–ç ï¼šï¼ˆ1ï¼‰è§£å‰–ç»“æ„å…ˆéªŒçŸ¥è¯†ï¼Œï¼ˆ2ï¼‰é…å‡†ç±»å‹çº¦æŸï¼ˆè·¨å—è¯•è€…æˆ–åŒä¸€å—è¯•è€…å†…ï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰ç‰¹å®šå®ä¾‹çš„ç‰¹å¾ï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆåœºæ™¯ä¼˜åŒ–çš„å˜å½¢åœºã€‚é€šè¿‡å¯¹æ¶µç›–ä¸åŒèº«ä½“åŒºåŸŸçš„90ä¸ªè§£å‰–ç»“æ„è¿›è¡Œå…¨é¢å®éªŒï¼Œæˆ‘ä»¬çš„UniRegæ¨¡å‹å±•ç¤ºå‡ºäº†ä¸å½“ä»£æœ€æ–°æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç›¸å¯¹äºä¼ ç»Ÿçš„åŸºäºå­¦ä¹ çš„èŒƒå¼å®ç°äº†çº¦50%çš„è®­ç»ƒè¿­ä»£å‡å°‘ã€‚è¿™ä¸€ä¼˜åŒ–ä¸ºè®¡ç®—èµ„æºçš„ä½¿ç”¨å¸¦æ¥äº†æ˜¾è‘—çš„å‡å°‘ï¼Œå¦‚è®­ç»ƒæ—¶é—´ã€‚ä»£ç å’Œæ¨¡å‹å°†å¯ä¾›ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12868v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å­¦ä¹ åŸºäºåŒ»å­¦å›¾åƒæ³¨å†Œçš„æ¨¡å‹å·²å®ç°äº†ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½ç›¸å½“ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒåŸºäºå­¦ä¹ çš„æ³¨å†Œæ–¹æ³•åœ¨ä¸´åºŠåœºæ™¯ä¸­çš„é€šç”¨æ€§è¾ƒå·®ã€‚ä¸ºå…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†UniRegï¼Œé¦–ä¸ªäº¤äº’å¼åŒ»å­¦å›¾åƒæ³¨å†ŒåŸºç¡€æ¨¡å‹ï¼Œç»“åˆäº†ä»»åŠ¡ç‰¹å®šå­¦ä¹ æ–¹æ³•çš„ç²¾ç¡®ä¼˜åŠ¿ä¸ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡ç»Ÿä¸€æ³¨å†Œæ¨¡å‹å†…çš„æ¡ä»¶å˜å½¢åœºä¼°è®¡ï¼Œå®ç°å¤šç§æ³¨å†Œåœºæ™¯çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¿™é€šè¿‡åŠ¨æ€å­¦ä¹ èŒƒå¼å®ç°ï¼Œæ˜¾å¼ç¼–ç 1ï¼‰è§£å‰–ç»“æ„å…ˆéªŒçŸ¥è¯†ï¼Œ2ï¼‰æ³¨å†Œç±»å‹çº¦æŸï¼ˆè·¨&#x2F;å†…ä¸»ä½“ï¼‰ï¼Œä»¥åŠ3ï¼‰å®ä¾‹ç‰¹å®šç‰¹å¾ï¼Œä»¥ç”Ÿæˆåœºæ™¯æœ€ä¼˜çš„å˜å½¢åœºã€‚å®éªŒè¡¨æ˜ï¼ŒUniRegæ¨¡å‹ä¸å½“å‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”å…·æœ‰ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç›¸å¯¹äºä¼ ç»Ÿå­¦ä¹ èŒƒå¼çš„è®­ç»ƒè¿­ä»£ä¸­å®ç°äº†çº¦50%çš„å‡å°‘ï¼Œæœ‰åŠ©äºå‡å°‘è®¡ç®—èµ„æºï¼Œå¦‚è®­ç»ƒæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ åŸºäºåŒ»å­¦å›¾åƒæ³¨å†Œçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå·²ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡æœ‰ä¼˜åŠ¿ã€‚</li>
<li>åŸºäºå­¦ä¹ çš„æ³¨å†Œæ–¹æ³•åœ¨ä¸´åºŠåœºæ™¯ä¸­çš„é€šç”¨æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>UniRegæ˜¯é¦–ä¸ªäº¤äº’å¼åŒ»å­¦å›¾åƒæ³¨å†ŒåŸºç¡€æ¨¡å‹ï¼Œç»“åˆäº†ä»»åŠ¡ç‰¹å®šå­¦ä¹ æ–¹æ³•å’Œä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•çš„ä¼˜ç‚¹ã€‚</li>
<li>UniRegé€šè¿‡æ¡ä»¶å˜å½¢åœºä¼°è®¡å®ç°å¤šç§æ³¨å†Œåœºæ™¯çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>åŠ¨æ€å­¦ä¹ èŒƒå¼ç»“åˆè§£å‰–ç»“æ„å…ˆéªŒçŸ¥è¯†ã€æ³¨å†Œç±»å‹çº¦æŸå’Œå®ä¾‹ç‰¹å®šç‰¹å¾ã€‚</li>
<li>UniRegæ¨¡å‹ä¸å½“å‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”å…·æœ‰ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†è®­ç»ƒè¿­ä»£æ¬¡æ•°å’Œè®¡ç®—èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12868v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12868v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12868v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Adaptive-Transformer-Attention-and-Multi-Scale-Fusion-for-Spine-3D-Segmentation"><a href="#Adaptive-Transformer-Attention-and-Multi-Scale-Fusion-for-Spine-3D-Segmentation" class="headerlink" title="Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D   Segmentation"></a>Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D   Segmentation</h2><p><strong>Authors:Yanlin Xiang, Qingyuan He, Ting Xu, Ran Hao, Jiacheng Hu, Hanchao Zhang</strong></p>
<p>This study proposes a 3D semantic segmentation method for the spine based on the improved SwinUNETR to improve segmentation accuracy and robustness. Aiming at the complex anatomical structure of spinal images, this paper introduces a multi-scale fusion mechanism to enhance the feature extraction capability by using information of different scales, thereby improving the recognition accuracy of the model for the target area. In addition, the introduction of the adaptive attention mechanism enables the model to dynamically adjust the attention to the key area, thereby optimizing the boundary segmentation effect. The experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net + Transformer, the model of this study has achieved significant improvements in mIoU, mDice, and mAcc indicators, and has better segmentation performance. The ablation experiment further verifies the effectiveness of the proposed improved method, proving that multi-scale fusion and adaptive attention mechanism have a positive effect on the segmentation task. Through the visualization analysis of the inference results, the model can better restore the real anatomical structure of the spinal image. Future research can further optimize the Transformer structure and expand the data scale to improve the generalization ability of the model. This study provides an efficient solution for the task of medical image segmentation, which is of great significance to intelligent medical image analysis. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›å‹SwinUNETRçš„è„ŠæŸ±3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œä»¥æé«˜åˆ†å‰²ç²¾åº¦å’Œç¨³å¥æ€§ã€‚é’ˆå¯¹è„ŠæŸ±å›¾åƒå¤æ‚çš„è§£å‰–ç»“æ„ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦èåˆæœºåˆ¶ï¼Œåˆ©ç”¨ä¸åŒå°ºåº¦çš„ä¿¡æ¯å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹ç›®æ ‡åŒºåŸŸçš„è¯†åˆ«ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå¼•å…¥è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€è°ƒæ•´å¯¹å…³é”®åŒºåŸŸçš„æ³¨æ„åŠ›ï¼Œä»è€Œä¼˜åŒ–è¾¹ç•Œåˆ†å‰²æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸3D CNNã€3D U-Netå’Œ3D U-Net+Transformerç›¸æ¯”ï¼Œæœ¬ç ”ç©¶çš„æ¨¡å‹åœ¨mIoUã€mDiceå’ŒmAccæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå…·æœ‰æ›´å¥½çš„åˆ†å‰²æ€§èƒ½ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†æ‰€æ”¹è¿›æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å¤šå°ºåº¦èåˆå’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶å¯¹åˆ†å‰²ä»»åŠ¡å…·æœ‰ç§¯æå½±å“ã€‚é€šè¿‡å¯¹æ¨ç†ç»“æœçš„å¯è§†åŒ–åˆ†æï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ¢å¤è„ŠæŸ±å›¾åƒçš„çœŸå®è§£å‰–ç»“æ„ã€‚æœªæ¥ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–Transformerç»“æ„å¹¶æ‰©å¤§æ•°æ®è§„æ¨¡ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æ™ºèƒ½åŒ»å­¦å›¾åƒåˆ†æå…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹è„ŠæŸ±å›¾åƒçš„å¤æ‚è§£å‰–ç»“æ„ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›å‹SwinUNETRçš„3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥å¤šå°ºåº¦èåˆæœºåˆ¶å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç‰¹å¾æå–èƒ½åŠ›å’Œæ¨¡å‹å¯¹ç›®æ ‡åŒºåŸŸçš„è¯†åˆ«ç²¾åº¦ï¼Œä¼˜åŒ–äº†è¾¹ç•Œåˆ†å‰²æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäº3D CNNã€3D U-Netå’Œ3D U-Net + Transformeræ¨¡å‹ï¼Œåœ¨mIoUã€mDiceå’ŒmAccæŒ‡æ ‡ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ï¼Œå…·æœ‰æ›´å¥½çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›å‹SwinUNETRçš„3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œé’ˆå¯¹è„ŠæŸ±å›¾åƒçš„å¤æ‚è§£å‰–ç»“æ„è¿›è¡Œè®¾è®¡ã€‚</li>
<li>å¼•å…¥å¤šå°ºåº¦èåˆæœºåˆ¶ï¼Œåˆ©ç”¨ä¸åŒå°ºåº¦çš„ä¿¡æ¯å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½åŠ¨æ€è°ƒæ•´å¯¹å…³é”®åŒºåŸŸçš„æ³¨æ„åŠ›ï¼Œä¼˜åŒ–è¾¹ç•Œåˆ†å‰²æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨mIoUã€mDiceå’ŒmAccæŒ‡æ ‡ä¸Šè¾ƒå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ¶ˆèå®éªŒéªŒè¯äº†å¤šå°ºåº¦èåˆå’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶å¯¹åˆ†å‰²ä»»åŠ¡çš„ç§¯æå½±å“ã€‚</li>
<li>é€šè¿‡å¯è§†åŒ–åˆ†ææ¨ç†ç»“æœï¼Œè¯æ˜è¯¥æ¨¡å‹èƒ½æ›´å¥½åœ°æ¢å¤çœŸå®çš„è„ŠæŸ±å›¾åƒè§£å‰–ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12853v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12853v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12853v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mixed-granularity-Implicit-Representation-for-Continuous-Hyperspectral-Compressive-Reconstruction"><a href="#Mixed-granularity-Implicit-Representation-for-Continuous-Hyperspectral-Compressive-Reconstruction" class="headerlink" title="Mixed-granularity Implicit Representation for Continuous Hyperspectral   Compressive Reconstruction"></a>Mixed-granularity Implicit Representation for Continuous Hyperspectral   Compressive Reconstruction</h2><p><strong>Authors:Jianan Li, Huan Chen, Wangcai Zhao, Rui Chen, Tingfa Xu</strong></p>
<p>Hyperspectral Images (HSIs) are crucial across numerous fields but are hindered by the long acquisition times associated with traditional spectrometers. The Coded Aperture Snapshot Spectral Imaging (CASSI) system mitigates this issue through a compression technique that accelerates the acquisition process. However, reconstructing HSIs from compressed data presents challenges due to fixed spatial and spectral resolution constraints. This study introduces a novel method using implicit neural representation for continuous hyperspectral image reconstruction. We propose the Mixed Granularity Implicit Representation (MGIR) framework, which includes a Hierarchical Spectral-Spatial Implicit Encoder for efficient multi-scale implicit feature extraction. This is complemented by a Mixed-Granularity Local Feature Aggregator that adaptively integrates local features across scales, combined with a decoder that merges coordinate information for precise reconstruction. By leveraging implicit neural representations, the MGIR framework enables reconstruction at any desired spatial-spectral resolution, significantly enhancing the flexibility and adaptability of the CASSI system. Extensive experimental evaluations confirm that our model produces reconstructed images at arbitrary resolutions and matches state-of-the-art methods across varying spectral-spatial compression ratios. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/chh11/MGIR">https://github.com/chh11/MGIR</a>. </p>
<blockquote>
<p>é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰åœ¨å¤šä¸ªé¢†åŸŸéƒ½è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿå…‰è°±ä»ªç›¸å…³çš„é•¿æ—¶é—´é‡‡é›†è¿‡ç¨‹é™åˆ¶äº†å…¶å‘å±•ã€‚ç¼–ç å­”å¾„å¿«ç…§å…‰è°±æˆåƒï¼ˆCASSIï¼‰ç³»ç»Ÿé€šè¿‡ä¸€ç§å‹ç¼©æŠ€æœ¯è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä»è€ŒåŠ é€Ÿäº†é‡‡é›†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œä»å‹ç¼©æ•°æ®ä¸­é‡å»ºé«˜å…‰è°±å›¾åƒé¢ä¸´ç€å›ºå®šçš„ç©ºé—´åˆ†è¾¨ç‡å’Œå…‰è°±åˆ†è¾¨ç‡çº¦æŸæ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ä½¿ç”¨éšå¼ç¥ç»è¡¨ç¤ºè¿›è¡Œè¿ç»­é«˜å…‰è°±å›¾åƒé‡å»ºçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†æ··åˆç²’åº¦éšå¼è¡¨ç¤ºï¼ˆMGIRï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ†å±‚è°±ç©ºé—´éšå¼ç¼–ç å™¨ï¼Œç”¨äºé«˜æ•ˆçš„å¤šå°ºåº¦éšç‰¹å¾æå–ã€‚è¿™å¾—åˆ°äº†æ··åˆç²’åº¦å±€éƒ¨ç‰¹å¾èšåˆå™¨çš„è¡¥å……ï¼Œè¯¥èšåˆå™¨è‡ªé€‚åº”åœ°é›†æˆäº†è·¨å°ºåº¦çš„å±€éƒ¨ç‰¹å¾ï¼Œå¹¶ç»“åˆè§£ç å™¨åˆå¹¶åæ ‡ä¿¡æ¯è¿›è¡Œç²¾ç¡®é‡å»ºã€‚é€šè¿‡åˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ŒMGIRæ¡†æ¶èƒ½å¤Ÿä»¥æ‰€éœ€çš„ä»»ä½•ç©ºé—´å…‰è°±åˆ†è¾¨ç‡è¿›è¡Œé‡å»ºï¼Œæ˜¾è‘—å¢å¼ºäº†CASSIç³»ç»Ÿçš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»æ„åˆ†è¾¨ç‡ä¸‹ç”Ÿæˆé‡å»ºå›¾åƒï¼Œå¹¶åœ¨å„ç§å…‰è°±ç©ºé—´å‹ç¼©æ¯”ç‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/chh11/MGIR%E5%8F%91%E6%98%BE%E3%80%82">https://github.com/chh11/MGIRå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12783v1">PDF</a> Accepted by TNNLS</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨éšç¥ç»è¡¨ç¤ºè¿›è¡Œè¿ç»­é«˜å…‰è°±å›¾åƒé‡å»ºçš„æ–°æ–¹æ³•ã€‚è¯¥ç ”ç©¶æå‡ºäº†åä¸ºMixed Granularity Implicit Representationï¼ˆMGIRï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚è°±ç©ºé—´éšç¼–ç å™¨å’Œæ··åˆç²’åº¦å±€éƒ¨ç‰¹å¾èšåˆå™¨ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šå°ºåº¦éšç‰¹å¾æå–å’Œè‡ªé€‚åº”å±€éƒ¨ç‰¹å¾é›†æˆã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿé‡å»ºä»»æ„æ‰€éœ€çš„ç©ºé—´å…‰è°±åˆ†è¾¨ç‡çš„é«˜å…‰è°±å›¾åƒï¼Œæ˜¾è‘—æé«˜äº†CASSIç³»ç»Ÿçš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç¥ç»è¡¨ç¤ºç”¨äºé«˜å…‰è°±å›¾åƒé‡å»ºã€‚</li>
<li>MGIRæ¡†æ¶åŒ…å«åˆ†å±‚è°±ç©ºé—´éšç¼–ç å™¨å’Œæ··åˆç²’åº¦å±€éƒ¨ç‰¹å¾èšåˆå™¨ã€‚</li>
<li>MGIRèƒ½å¤Ÿå®ç°å¤šå°ºåº¦éšç‰¹å¾æå–å’Œè‡ªé€‚åº”å±€éƒ¨ç‰¹å¾é›†æˆã€‚</li>
<li>MGIRæ¡†æ¶èƒ½å¤Ÿé‡å»ºä»»æ„æ‰€éœ€çš„ç©ºé—´å…‰è°±åˆ†è¾¨ç‡çš„é«˜å…‰è°±å›¾åƒã€‚</li>
<li>MGIRé€šè¿‡åˆ©ç”¨åæ ‡ä¿¡æ¯æé«˜äº†é‡å»ºç²¾åº¦ã€‚</li>
<li>å®éªŒè¯„ä¼°è¯æ˜ï¼ŒMGIRæ¨¡å‹èƒ½å¤Ÿåœ¨ä»»æ„åˆ†è¾¨ç‡ä¸‹é‡å»ºå›¾åƒï¼Œå¹¶åœ¨ä¸åŒçš„è°±ç©ºé—´å‹ç¼©æ¯”ç‡ä¸‹è¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12783v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12783v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12783v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12783v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Deep-Learning-for-Multiclass-Breast-Cancer-Classification-via-Misprediction-Risk-Analysis"><a href="#Adaptive-Deep-Learning-for-Multiclass-Breast-Cancer-Classification-via-Misprediction-Risk-Analysis" class="headerlink" title="Adaptive Deep Learning for Multiclass Breast Cancer Classification via   Misprediction Risk Analysis"></a>Adaptive Deep Learning for Multiclass Breast Cancer Classification via   Misprediction Risk Analysis</h2><p><strong>Authors:Gul Sheeraz, Qun Chen, Liu Feiyu, Zhou Fengjin MD</strong></p>
<p>Breast cancer remains one of the leading causes of cancer-related deaths worldwide. Early detection is crucial for improving patient outcomes, yet the diagnostic process is often complex and prone to inconsistencies among pathologists. Computer-aided diagnostic approaches have significantly enhanced breast cancer detection, particularly in binary classification (benign vs. malignant). However, these methods face challenges in multiclass classification, leading to frequent mispredictions. In this work, we propose a novel adaptive learning approach for multiclass breast cancer classification using H&amp;E-stained histopathology images. First, we introduce a misprediction risk analysis framework that quantifies and ranks the likelihood of an image being mislabeled by a classifier. This framework leverages an interpretable risk model that requires only a small number of labeled samples for training. Next, we present an adaptive learning strategy that fine-tunes classifiers based on the specific characteristics of a given dataset. This approach minimizes misprediction risk, allowing the classifier to adapt effectively to the target workload. We evaluate our proposed solutions on real benchmark datasets, demonstrating that our risk analysis framework more accurately identifies mispredictions compared to existing methods. Furthermore, our adaptive learning approach significantly improves the performance of state-of-the-art deep neural network classifiers. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚æ—©æœŸå‘ç°å¯¹äºæ”¹å–„æ‚£è€…ç»“æœè‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œè¯Šæ–­è¿‡ç¨‹å¾€å¾€å¤æ‚ï¼Œç—…ç†å­¦å®¶ä¹‹é—´å®¹æ˜“å‡ºç°ä¸ä¸€è‡´ã€‚è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹æ³•å·²ç»å¤§å¤§æé«˜äº†ä¹³è…ºç™Œçš„æ£€æµ‹æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨äºŒåˆ†ç±»ï¼ˆè‰¯æ€§ä¸æ¶æ€§ï¼‰ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤šç±»åˆ†ç±»ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´é¢‘ç¹çš„é”™è¯¯é¢„æµ‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨H&amp;EæŸ“è‰²ç»„ç»‡ç—…ç†å­¦å›¾åƒè¿›è¡Œå¤šç±»ä¹³è…ºç™Œåˆ†ç±»çš„æ–°å‹è‡ªé€‚åº”å­¦ä¹ æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯¯é¢„æµ‹é£é™©åˆ†ææ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥é‡åŒ–å’Œæ’åºå›¾åƒè¢«åˆ†ç±»å™¨é”™è¯¯æ ‡è®°çš„å¯èƒ½æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†ä¸€ä¸ªå¯è§£é‡Šçš„é£é™©æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…éœ€è¦å°‘é‡æ ‡è®°æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç»™å®šæ•°æ®é›†ç‰¹å®šç‰¹æ€§çš„è‡ªé€‚åº”å­¦ä¹ ç­–ç•¥ï¼Œå¯¹åˆ†ç±»å™¨è¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•é™ä½äº†è¯¯é¢„æµ‹çš„é£é™©ï¼Œä½¿åˆ†ç±»å™¨èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é€‚åº”ç›®æ ‡å·¥ä½œé‡ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„è§£å†³æ–¹æ¡ˆï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é£é™©åˆ†ææ¡†æ¶æ¯”ç°æœ‰æ–¹æ³•æ›´å‡†ç¡®åœ°è¯†åˆ«äº†è¯¯é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è‡ªé€‚åº”å­¦ä¹ æ–¹æ³•æ˜¾è‘—æé«˜äº†æœ€å…ˆè¿›çš„æ·±åº¦ç¥ç»ç½‘ç»œåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12778v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è‡ªé€‚åº”å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºåŸºäºH&amp;EæŸ“è‰²ç»„ç»‡ç—…ç†å›¾åƒçš„ä¹³è…ºç™Œå¤šåˆ†ç±»è¯Šæ–­ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªè¯¯é¢„æµ‹é£é™©åˆ†ææ¡†æ¶å’Œä¸€ä¸ªè‡ªé€‚åº”å­¦ä¹ ç­–ç•¥ã€‚é£é™©åˆ†ææ¡†æ¶èƒ½å¤Ÿé‡åŒ–å¹¶æ’åºå›¾åƒè¢«åˆ†ç±»å™¨è¯¯åˆ¤çš„æ¦‚ç‡ï¼Œåªéœ€å°‘é‡æ ·æœ¬å³å¯è®­ç»ƒã€‚è‡ªé€‚åº”å­¦ä¹ ç­–ç•¥åˆ™æ ¹æ®ç‰¹å®šæ•°æ®é›†çš„ç‰¹ç‚¹å¾®è°ƒåˆ†ç±»å™¨ï¼Œæœ€å°åŒ–è¯¯é¢„æµ‹é£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ›´å‡†ç¡®åœ°è¯†åˆ«è¯¯é¢„æµ‹ï¼Œæé«˜ç°æœ‰æ·±åº¦ç¥ç»ç½‘ç»œåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸæ£€æµ‹å¯¹äºæ”¹å–„æ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹æ³•å·²ç»æ˜¾è‘—æé«˜äº†ä¹³è…ºç™Œæ£€æµ‹èƒ½åŠ›ï¼Œä½†åœ¨å¤šåˆ†ç±»æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è‡ªé€‚åº”å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¤šç±»åˆ«ä¹³è…ºç™Œåˆ†ç±»ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªè¯¯é¢„æµ‹é£é™©åˆ†ææ¡†æ¶ï¼Œèƒ½å¤Ÿé‡åŒ–å›¾åƒè¢«è¯¯åˆ¤çš„æ¦‚ç‡ã€‚</li>
<li>é£é™©åˆ†ææ¡†æ¶åªéœ€è¦å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰é«˜åº¦çš„å®ç”¨æ€§ã€‚</li>
<li>è‡ªé€‚åº”å­¦ä¹ ç­–ç•¥èƒ½å¤ŸåŸºäºæ•°æ®é›†çš„ç‰¹ç‚¹å¾®è°ƒåˆ†ç±»å™¨ï¼Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12778v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12778v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Humanoids-in-Hospitals-A-Technical-Study-of-Humanoid-Surrogates-for-Dexterous-Medical-Interventions"><a href="#Humanoids-in-Hospitals-A-Technical-Study-of-Humanoid-Surrogates-for-Dexterous-Medical-Interventions" class="headerlink" title="Humanoids in Hospitals: A Technical Study of Humanoid Surrogates for   Dexterous Medical Interventions"></a>Humanoids in Hospitals: A Technical Study of Humanoid Surrogates for   Dexterous Medical Interventions</h2><p><strong>Authors:Soofiyan Atar, Xiao Liang, Calvin Joyce, Florian Richter, Wood Ricardo, Charles Goldberg, Preetham Suresh, Michael Yip</strong></p>
<p>The increasing demand for healthcare workers, driven by aging populations and labor shortages, presents a significant challenge for hospitals. Humanoid robots have the potential to alleviate these pressures by leveraging their human-like dexterity and adaptability to assist in medical procedures. This work conducted an exploratory study on the feasibility of humanoid robots performing direct clinical tasks through teleoperation. A bimanual teleoperation system was developed for the Unitree G1 Humanoid Robot, integrating high-fidelity pose tracking, custom grasping configurations, and an impedance controller to safely and precisely manipulate medical tools. The system is evaluated in seven diverse medical procedures, including physical examinations, emergency interventions, and precision needle tasks. Our results demonstrate that humanoid robots can successfully replicate critical aspects of human medical assessments and interventions, with promising quantitative performance in ventilation and ultrasound-guided tasks. However, challenges remain, including limitations in force output for procedures requiring high strength and sensor sensitivity issues affecting clinical accuracy. This study highlights the potential and current limitations of humanoid robots in hospital settings and lays the groundwork for future research on robotic healthcare integration. </p>
<blockquote>
<p>éšç€äººå£è€é¾„åŒ–å’ŒåŠ³åŠ¨åŠ›çŸ­ç¼ºçš„åŠ å‰§ï¼Œå¯¹åŒ»ç–—å·¥ä½œè€…çš„éœ€æ±‚ä¸æ–­å¢åŠ ï¼Œè¿™ç»™åŒ»é™¢å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ç±»äººæœºå™¨äººå…·æœ‰ç¼“è§£è¿™äº›å‹åŠ›çš„æ½œåŠ›ï¼Œå®ƒä»¬å¯ä»¥åˆ©ç”¨ç±»äººçš„çµå·§æ€§å’Œé€‚åº”æ€§æ¥è¾…åŠ©åŒ»ç–—ç¨‹åºã€‚æœ¬ç ”ç©¶é€šè¿‡é¥æ“ä½œå¯¹äººå½¢æœºå™¨äººæ‰§è¡Œç›´æ¥ä¸´åºŠä»»åŠ¡çš„å¯èƒ½æ€§è¿›è¡Œäº†æ¢ç´¢æ€§ç ”ç©¶ã€‚ä¸ºUnitree G1äººå½¢æœºå™¨äººå¼€å‘äº†ä¸€ç§åŒæ‰‹é¥æ“ä½œç³»ç»Ÿï¼Œé›†æˆäº†é«˜ä¿çœŸå§¿æ€è·Ÿè¸ªã€è‡ªå®šä¹‰æŠ“æ¡é…ç½®å’Œé˜»æŠ—æ§åˆ¶å™¨ï¼Œä»¥å®‰å…¨ç²¾ç¡®åœ°æ“ä½œåŒ»ç–—å·¥å…·ã€‚è¯¥ç³»ç»Ÿåœ¨ä¸ƒç§ä¸åŒçš„åŒ»ç–—ç¨‹åºä¸­è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ä½“æ ¼æ£€æŸ¥ã€ç´§æ€¥å¹²é¢„å’Œç²¾ç¡®é’ˆæœ¯ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œäººå½¢æœºå™¨äººèƒ½å¤ŸæˆåŠŸå¤åˆ¶äººç±»åŒ»ç–—è¯„ä¼°å’Œå¹²é¢„çš„å…³é”®æ–¹é¢ï¼Œåœ¨é€šæ°”å’Œè¶…å£°å¼•å¯¼ä»»åŠ¡ä¸­çš„å®šé‡æ€§èƒ½å…·æœ‰å‰æ™¯ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹äºéœ€è¦é«˜å¼ºåº¦çš„ç¨‹åºè€Œè¨€åŠ›é‡è¾“å‡ºçš„å±€é™æ€§ä»¥åŠå½±å“ä¸´åºŠå‡†ç¡®æ€§çš„ä¼ æ„Ÿå™¨çµæ•åº¦é—®é¢˜ã€‚è¯¥ç ”ç©¶çªå‡ºäº†åŒ»é™¢ç¯å¢ƒä¸­äººå½¢æœºå™¨äººçš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æœºå™¨äººåŒ»ç–—ä¿å¥æ•´åˆçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12725v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>äººå£è€é¾„åŒ–åŠåŠ³åŠ¨åŠ›çŸ­ç¼ºå¯¹åŒ»é™¢æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚äººå½¢æœºå™¨äººå…·å¤‡äººç±»èˆ¬çš„çµå·§æ€§å’Œé€‚åº”æ€§ï¼Œå¯ååŠ©æ‰§è¡ŒåŒ»ç–—ç¨‹åºï¼Œç¼“è§£å‹åŠ›ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†é€šè¿‡é¥æ§æ“ä½œäººå½¢æœºå™¨äººæ‰§è¡Œç›´æ¥ä¸´åºŠä»»åŠ¡çš„å¯èƒ½æ€§ã€‚ä¸ºUnitree G1äººå½¢æœºå™¨äººå¼€å‘äº†ä¸€ç§åŒæ‰‹åŠ¨é¥æ§æ“ä½œç³»ç»Ÿï¼Œé›†æˆäº†é«˜ä¿çœŸå§¿æ€è¿½è¸ªã€è‡ªå®šä¹‰æŠ“æ¡é…ç½®å’Œé˜»æŠ—æ§åˆ¶å™¨ï¼Œå¯å®‰å…¨ç²¾ç¡®åœ°æ“ä½œåŒ»ç–—å·¥å…·ã€‚ç³»ç»Ÿç»è¿‡ä¸ƒç§ä¸åŒçš„åŒ»ç–—ç¨‹åºçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ä½“æ£€ã€ç´§æ€¥å¹²é¢„å’Œç²¾å‡†é’ˆåˆºä»»åŠ¡ç­‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œäººå½¢æœºå™¨äººåœ¨å…³é”®åŒ»ç–—è¯„ä¼°æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå°¤å…¶æ˜¯åœ¨é€šæ°”å’Œè¶…å£°å¼•å¯¼ä»»åŠ¡ä¸­çš„å®šé‡è¡¨ç°ä»¤äººé¼“èˆã€‚ç„¶è€Œä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚åŠ›é‡è¾“å‡ºå—é™å½±å“éœ€è¦é«˜å¼ºåº¦çš„ç¨‹åºæ‰§è¡ŒåŠä¼ æ„Ÿå™¨çµæ•åº¦é—®é¢˜å½±å“ä¸´åºŠå‡†ç¡®æ€§ç­‰ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†äººå½¢æœºå™¨äººåœ¨åŒ»é™¢ç¯å¢ƒä¸­çš„æ½œåŠ›åŠå½“å‰å±€é™æ€§ï¼Œä¸ºæœªæ¥æœºå™¨äººåŒ»ç–—ä¿å¥èåˆç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå£è€é¾„åŒ–å’ŒåŠ³åŠ¨åŠ›çŸ­ç¼ºå¯¹åŒ»é™¢é€ æˆäº†æŒ‘æˆ˜ã€‚</li>
<li>äººå½¢æœºå™¨äººå…·å¤‡äººç±»èˆ¬çš„çµå·§æ€§å’Œé€‚åº”æ€§ï¼Œå¯ç”¨äºååŠ©åŒ»ç–—ç¨‹åºã€‚</li>
<li>ç ”ç©¶é€šè¿‡é¥æ§æ“ä½œäººå½¢æœºå™¨äººæ‰§è¡Œç›´æ¥ä¸´åºŠä»»åŠ¡çš„å¯èƒ½æ€§ã€‚</li>
<li>ä¸ºUnitree G1äººå½¢æœºå™¨äººå¼€å‘çš„åŒæ‰‹åŠ¨é¥æ§æ“ä½œç³»ç»Ÿé›†æˆäº†é«˜ä¿çœŸå§¿æ€è¿½è¸ªã€è‡ªå®šä¹‰æŠ“æ¡é…ç½®å’Œé˜»æŠ—æ§åˆ¶å™¨ã€‚</li>
<li>ç³»ç»Ÿç»è¿‡å¤šç§åŒ»ç–—ç¨‹åºè¯„ä¼°ï¼ŒåŒ…æ‹¬ä½“æ£€ã€ç´§æ€¥å¹²é¢„ç­‰ï¼Œè¡¨ç°è‰¯å¥½ã€‚</li>
<li>äººå½¢æœºå™¨äººåœ¨å…³é”®åŒ»ç–—è¯„ä¼°æ–¹é¢å–å¾—æ˜¾è‘—æˆæœï¼Œå°¤å…¶æ˜¯åœ¨é€šæ°”å’Œè¶…å£°å¼•å¯¼ä»»åŠ¡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12725v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12725v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12725v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12725v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12725v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12725v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Continual-Learning-driven-Model-for-Accurate-and-Generalizable-Segmentation-of-Clinically-Comprehensive-and-Fine-grained-Whole-body-Anatomies-in-CT"><a href="#A-Continual-Learning-driven-Model-for-Accurate-and-Generalizable-Segmentation-of-Clinically-Comprehensive-and-Fine-grained-Whole-body-Anatomies-in-CT" class="headerlink" title="A Continual Learning-driven Model for Accurate and Generalizable   Segmentation of Clinically Comprehensive and Fine-grained Whole-body   Anatomies in CT"></a>A Continual Learning-driven Model for Accurate and Generalizable   Segmentation of Clinically Comprehensive and Fine-grained Whole-body   Anatomies in CT</h2><p><strong>Authors:Dazhou Guo, Zhanghexuan Ji, Yanzhou Su, Dandan Zheng, Heng Guo, Puyang Wang, Ke Yan, Yirui Wang, Qinji Yu, Zi Li, Minfeng Xu, Jianfeng Zhang, Haoshen Li, Jia Ge, Tsung-Ying Ho, Bing-Shen Huang, Tashan Ai, Kuaile Zhao, Na Shen, Qifeng Wang, Yun Bian, Tingyu Wu, Peng Du, Hua Zhang, Feng-Ming Kong, Alan L. Yuille, Cher Heng Tan, Chunyan Miao, Perry J. Pickhardt, Senxiang Yan, Ronald M. Summers, Le Lu, Dakai Jin, Xianghua Ye</strong></p>
<p>Precision medicine in the quantitative management of chronic diseases and oncology would be greatly improved if the Computed Tomography (CT) scan of any patient could be segmented, parsed and analyzed in a precise and detailed way. However, there is no such fully annotated CT dataset with all anatomies delineated for training because of the exceptionally high manual cost, the need for specialized clinical expertise, and the time required to finish the task. To this end, we proposed a novel continual learning-driven CT model that can segment complete anatomies presented using dozens of previously partially labeled datasets, dynamically expanding its capacity to segment new ones without compromising previously learned organ knowledge. Existing multi-dataset approaches are not able to dynamically segment new anatomies without catastrophic forgetting and would encounter optimization difficulty or infeasibility when segmenting hundreds of anatomies across the whole range of body regions. Our single unified CT segmentation model, CL-Net, can highly accurately segment a clinically comprehensive set of 235 fine-grained whole-body anatomies. Composed of a universal encoder, multiple optimized and pruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and 16 private high-quality partially labeled CT datasets of various vendors, different contrast phases, and pathologies. Extensive evaluation demonstrates that CL-Net consistently outperforms the upper limit of an ensemble of 36 specialist nnUNets trained per dataset with the complexity of 5% model size and significantly surpasses the segmentation accuracy of recent leading Segment Anything-style medical image foundation models by large margins. Our continual learning-driven CL-Net model would lay a solid foundation to facilitate many downstream tasks of oncology and chronic diseases using the most widely adopted CT imaging. </p>
<blockquote>
<p>åœ¨æ…¢æ€§ç–¾ç—…çš„å®šé‡ç®¡ç†å’Œè‚¿ç˜¤å­¦ä¸­ï¼Œå¦‚æœæ‚£è€…è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰èƒ½å¤Ÿç²¾ç¡®ä¸”è¯¦ç»†åœ°åˆ†å‰²ã€è§£æå’Œåˆ†æï¼Œé‚£ä¹ˆç²¾å‡†åŒ»ç–—å°†å¤§å¤§æé«˜ã€‚ç„¶è€Œï¼Œç”±äºæ²¡æœ‰å®Œå…¨æ ‡æ³¨çš„CTæ•°æ®é›†æ¥æ ‡æ³¨æ‰€æœ‰è§£å‰–ç»“æ„ä»¥ä¾›è®­ç»ƒï¼ŒåŸå› åœ¨äºæ‰‹åŠ¨æˆæœ¬æé«˜ã€éœ€è¦ä¸“ä¸šä¸´åºŠç»éªŒä»¥åŠå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ—¶é—´ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æŒç»­å­¦ä¹ é©±åŠ¨çš„CTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä½¿ç”¨æ•°åä¸ªå…ˆå‰éƒ¨åˆ†æ ‡è®°çš„æ•°æ®é›†æ¥å‘ˆç°å®Œæ•´çš„è§£å‰–ç»“æ„ï¼Œå¹¶åŠ¨æ€æ‰©å±•å…¶åˆ†å‰²æ–°è§£å‰–ç»“æ„çš„èƒ½åŠ›ï¼Œè€Œä¸ä¼šæŸå®³ä»¥å‰å­¦ä¹ çš„å™¨å®˜çŸ¥è¯†ã€‚ç°æœ‰çš„å¤šæ•°æ®é›†æ–¹æ³•æ— æ³•åŠ¨æ€åˆ†å‰²æ–°çš„è§£å‰–ç»“æ„ï¼Œè€Œä¸ä¼šå‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œå¹¶ä¸”åœ¨åˆ†å‰²æ•°ç™¾ä¸ªè§£å‰–ç»“æ„æ—¶ï¼Œä¼šåœ¨æ•´ä¸ªèº«ä½“åŒºåŸŸèŒƒå›´å†…é‡åˆ°ä¼˜åŒ–å›°éš¾æˆ–ä¸å¯è¡Œçš„æƒ…å†µã€‚æˆ‘ä»¬çš„å•ä¸€ç»Ÿä¸€CTåˆ†å‰²æ¨¡å‹CL-Netå¯ä»¥é«˜åº¦å‡†ç¡®åœ°åˆ†å‰²ä¸´åºŠä¸Šå…¨é¢çš„235ä¸ªç²¾ç»†å…¨èº«è§£å‰–ç»“æ„ã€‚CL-Netç”±é€šç”¨ç¼–ç å™¨ã€å¤šä¸ªä¼˜åŒ–å’Œä¿®å‰ªçš„è§£ç å™¨ç»„æˆï¼Œä½¿ç”¨æ¥è‡ª20ä¸ªå…¬å…±å’Œ16ä¸ªç§æœ‰é«˜è´¨é‡éƒ¨åˆ†æ ‡è®°CTæ•°æ®é›†çš„13952ä¸ªCTæ‰«æè¿›è¡Œå¼€å‘ï¼Œè¿™äº›æ•°æ®é›†æ¥è‡ªä¸åŒçš„ä¾›åº”å•†ã€å¯¹æ¯”é˜¶æ®µå’Œç—…ç†æƒ…å†µã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼Œè¯æ˜CL-Netå§‹ç»ˆè¶…è¿‡æ¯ä¸ªæ•°æ®é›†è®­ç»ƒçš„36ä¸ªä¸“å®¶nnUNetsç»„åˆçš„ä¸Šé™ï¼Œæ¨¡å‹å¤§å°å¤æ‚åº¦ä¸º5%ï¼Œå¹¶ä¸”å¤§å¤§è¶…è¿‡äº†æœ€è¿‘é¢†å…ˆçš„Segment Anythingé£æ ¼çš„åŒ»å­¦å›¾åƒåŸºç¡€æ¨¡å‹çš„åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬æŒç»­å­¦ä¹ é©±åŠ¨çš„CL-Netæ¨¡å‹å°†ä¸ºä½¿ç”¨æœ€å¹¿æ³›é‡‡ç”¨çš„CTæˆåƒçš„è‚¿ç˜¤å­¦å’Œæ…¢æ€§ç—…ä¸‹æ¸¸ä»»åŠ¡å¥ å®šåšå®åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12698v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æŒç»­å­¦ä¹ é©±åŠ¨çš„CTæ¨¡å‹ï¼ˆCL-Netï¼‰ï¼Œè¯¥æ¨¡å‹å¯ç²¾ç¡®åˆ†å‰²ä¸´åºŠå…¨é¢çš„235ç§ç²¾ç»†å…¨èº«è§£å‰–å­¦ç»“æ„ã€‚é€šè¿‡åˆ©ç”¨å¤šç§éƒ¨åˆ†æ ‡è®°çš„CTæ•°æ®é›†ï¼ŒCL-Netèƒ½å¤ŸåŠ¨æ€æ‰©å±•å…¶åˆ†å‰²æ–°è§£å‰–å­¦ç»“æ„çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¸é—å¿˜å…ˆå‰å­¦ä¹ çš„å™¨å®˜çŸ¥è¯†ã€‚ä¸ä¼ ç»Ÿçš„å¤šæ•°æ®é›†æ–¹æ³•å’Œå…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒCL-Netåœ¨åˆ†å‰²ç²¾åº¦å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CL-Netæ¨¡å‹èƒ½å¤Ÿç²¾ç¡®åˆ†å‰²ä¸´åºŠå…¨é¢çš„235ç§ç²¾ç»†å…¨èº«è§£å‰–å­¦ç»“æ„ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨å¤šç§éƒ¨åˆ†æ ‡è®°çš„CTæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°åŠ¨æ€æ‰©å±•åˆ†å‰²æ–°è§£å‰–å­¦ç»“æ„çš„èƒ½åŠ›ã€‚</li>
<li>CL-Neté‡‡ç”¨æŒç»­å­¦ä¹ é©±åŠ¨çš„æ–¹æ³•ï¼Œåœ¨åˆ†å‰²æ–°è§£å‰–å­¦ç»“æ„æ—¶ä¸ä¼šé—å¿˜å…ˆå‰å­¦ä¹ çš„å™¨å®˜çŸ¥è¯†ã€‚</li>
<li>ä¸ä¼ ç»Ÿå¤šæ•°æ®é›†æ–¹æ³•å’Œå…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒCL-Netåœ¨åˆ†å‰²æ€§èƒ½ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>CL-Netæ¨¡å‹ç”±é€šç”¨ç¼–ç å™¨ã€å¤šä¸ªä¼˜åŒ–å’Œä¿®å‰ªçš„è§£ç å™¨ç»„æˆã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨äº†å¤§é‡æ¥è‡ªå…¬å…±å’Œç§æœ‰æ¥æºçš„é«˜è´¨é‡éƒ¨åˆ†æ ‡è®°CTæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12698v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12698v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LATINO-PRO-LAtent-consisTency-INverse-sOlver-with-PRompt-Optimization"><a href="#LATINO-PRO-LAtent-consisTency-INverse-sOlver-with-PRompt-Optimization" class="headerlink" title="LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization"></a>LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization</h2><p><strong>Authors:Alessio Spagnoletti, Jean Prost, AndrÃ©s Almansa, Nicolas Papadakis, Marcelo Pereyra</strong></p>
<p>Text-to-image latent diffusion models (LDMs) have recently emerged as powerful generative models with great potential for solving inverse problems in imaging. However, leveraging such models in a Plug &amp; Play (PnP), zero-shot manner remains challenging because it requires identifying a suitable text prompt for the unknown image of interest. Also, existing text-to-image PnP approaches are highly computationally expensive. We herein address these challenges by proposing a novel PnP inference paradigm specifically designed for embedding generative models within stochastic inverse solvers, with special attention to Latent Consistency Models (LCMs), which distill LDMs into fast generators. We leverage our framework to propose LAtent consisTency INverse sOlver (LATINO), the first zero-shot PnP framework to solve inverse problems with priors encoded by LCMs. Our conditioning mechanism avoids automatic differentiation and reaches SOTA quality in as little as 8 neural function evaluations. As a result, LATINO delivers remarkably accurate solutions and is significantly more memory and computationally efficient than previous approaches. We then embed LATINO within an empirical Bayesian framework that automatically calibrates the text prompt from the observed measurements by marginal maximum likelihood estimation. Extensive experiments show that prompt self-calibration greatly improves estimation, allowing LATINO with PRompt Optimization to define new SOTAs in image reconstruction quality and computational efficiency. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰æœ€è¿‘ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹å‡ºç°ï¼Œåœ¨è§£å†³æˆåƒä¸­çš„é€†é—®é¢˜æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»¥Plug &amp; Playï¼ˆPnPï¼‰å³æ’å³ç”¨ã€é›¶å°„å‡»çš„æ–¹å¼åˆ©ç”¨è¿™äº›æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™éœ€è¦ä¸ºæ„Ÿå…´è¶£çš„æœªçŸ¥å›¾åƒç¡®å®šåˆé€‚çš„æ–‡æœ¬æç¤ºã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒçš„PnPæ–¹æ³•è®¡ç®—é‡å·¨å¤§ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ–°å‹PnPæ¨ç†èŒƒå¼æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥èŒƒå¼ä¸“é—¨è®¾è®¡ç”¨äºå°†ç”Ÿæˆæ¨¡å‹åµŒå…¥éšæœºé€†æ±‚è§£å™¨ä¸­ï¼Œç‰¹åˆ«å…³æ³¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰ï¼Œå®ƒå°†LDMsè’¸é¦ä¸ºå¿«é€Ÿç”Ÿæˆå™¨ã€‚æˆ‘ä»¬åˆ©ç”¨æˆ‘ä»¬çš„æ¡†æ¶æå‡ºäº†æ½œåœ¨ä¸€è‡´æ€§é€†æ±‚è§£å™¨ï¼ˆLATINOï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé›¶å°„å‡»PnPæ¡†æ¶ï¼Œç”¨äºä½¿ç”¨ç”±LCMsç¼–ç çš„å…ˆéªŒçŸ¥è¯†è§£å†³é€†é—®é¢˜ã€‚æˆ‘ä»¬çš„è°ƒèŠ‚æœºåˆ¶é¿å…äº†è‡ªåŠ¨å¾®åˆ†ï¼Œå¹¶åœ¨ä»…è¿›è¡Œ8æ¬¡ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å› æ­¤ï¼ŒLATINOæä¾›äº†éå¸¸å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨å†…å­˜å’Œè®¡ç®—æ–¹é¢æ¯”ä»¥å‰çš„æ–¹æ³•æ›´åŠ é«˜æ•ˆã€‚ç„¶åæˆ‘ä»¬å°†LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶ä¸­ï¼Œé€šè¿‡è¾¹é™…æœ€å¤§ä¼¼ç„¶ä¼°è®¡è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæç¤ºè‡ªæˆ‘æ ¡å‡†æå¤§åœ°æé«˜äº†ä¼°ç®—ç²¾åº¦ï¼Œä½¿å¾—å¸¦æœ‰æç¤ºä¼˜åŒ–çš„LATINOåœ¨å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å®šä¹‰äº†æ–°çš„æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12615v1">PDF</a> 27 pages, 20 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„æ–°å‹Plug &amp; Playï¼ˆPnPï¼‰æ¨ç†èŒƒå¼ï¼Œç”¨äºè§£å†³å›¾åƒé‡å»ºä¸­çš„é€†é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§æ–°çš„æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMsï¼‰åµŒå…¥æ–¹æ³•ï¼Œå¹¶å°†LDMsè½¬åŒ–ä¸ºå¿«é€Ÿç”Ÿæˆå™¨ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºLAtento consisTency INverse sOlverï¼ˆLATINOï¼‰çš„é›¶æ ·æœ¬PnPæ¡†æ¶ï¼Œä½¿ç”¨æ¡ä»¶æœºåˆ¶é¿å…è‡ªåŠ¨å¾®åˆ†ï¼Œè¾¾åˆ°ä»…éœ€å°‘é‡ç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°å³å¯è·å¾—æœ€ä¼˜è´¨é‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å°†LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶ä¸­ï¼Œé€šè¿‡è¾¹é™…æœ€å¤§ä¼¼ç„¶ä¼°è®¡è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºï¼Œå®ç°è‡ªæˆ‘æ ¡å‡†æç¤ºä¼˜åŒ–ã€‚è¿™ä¸€æ–°æ–¹æ³•æ˜¾è‘—æé«˜å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LDMså±•ç°å‡ºè§£å†³æˆåƒé€†é—®é¢˜çš„å·¨å¤§æ½œåŠ›ï¼Œä½†Plug &amp; Playæ–¹å¼çš„åº”ç”¨å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹PnPæ¨ç†èŒƒå¼ï¼Œé’ˆå¯¹ç”Ÿæˆæ¨¡å‹åµŒå…¥éšæœºé€†æ±‚è§£å™¨ï¼Œç‰¹åˆ«å…³æ³¨LCMsã€‚</li>
<li>LATINOæ¡†æ¶æ˜¯é¦–ä¸ªé›¶æ ·æœ¬PnPæ¡†æ¶ï¼Œèƒ½è§£å†³ç”±LCMsç¼–ç å…ˆéªŒçš„é€†é—®é¢˜ã€‚</li>
<li>æ¡ä»¶æœºåˆ¶é¿å…è‡ªåŠ¨å¾®åˆ†ï¼Œè¾¾åˆ°é«˜è´¨é‡åŒæ—¶å‡å°‘ç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚</li>
<li>LATINOåµŒå…¥ç»éªŒè´å¶æ–¯æ¡†æ¶ï¼Œé€šè¿‡è¾¹é™…æœ€å¤§ä¼¼ç„¶ä¼°è®¡è‡ªåŠ¨æ ¡å‡†æ–‡æœ¬æç¤ºã€‚</li>
<li>æç¤ºè‡ªæˆ‘æ ¡å‡†ä¼˜åŒ–æ˜¾è‘—æé«˜å›¾åƒé‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12615v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12615v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12615v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12615v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12615v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AstroSat-CZTI-searches-for-hard-X-ray-prompt-emission-from-Fast-Radio-Bursts"><a href="#AstroSat-CZTI-searches-for-hard-X-ray-prompt-emission-from-Fast-Radio-Bursts" class="headerlink" title="AstroSat-CZTI searches for hard X-ray prompt emission from Fast Radio   Bursts"></a>AstroSat-CZTI searches for hard X-ray prompt emission from Fast Radio   Bursts</h2><p><strong>Authors:G. Waratkar, M. Dixit, S. P. Tendulkar, V. Bhalerao, D. Bhattacharya, S. Vadawale</strong></p>
<p>Fast Radio Bursts (FRBs) are short-duration, highly-energetic extragalactic radio transients with unclear origins &amp; emission mechanisms. Despite extensive multi-wavelength searches, no credible X-ray or other prompt electromagnetic counterparts have been found for extragalactic FRBs. We present results from a comprehensive search for such prompt X-ray counterparts using AstroSat-CZTI which has been actively detecting other high-energy fast transients like Gamma-ray bursts (GRBs). We undertook a systematic search in AstroSat-CZTI data for hard X-ray transients temporally &amp; spatially coincident with 578 FRBs, and found no X-ray counterparts. We estimate flux upper limits for these events and convert them to upper limits on X-ray-to-radio fluence ratios. Further, we utilize the redshifts derived from the dispersion measures of these FRBs to compare their isotropic luminosities with those of GRBs, providing insights into potential similarities between these two classes of transients. Finally, we explore the prospects for X-ray counterpart detections using other current and upcoming X-ray monitors, including Fermi-GBM, Swift-BAT, SVOM-ECLAIRs, and Daksha, in the era of next-generation FRB detection facilities such as CHIME, DSA-2000, CHORD, and BURSTT. Our results highlight that highly sensitive X-ray monitors with large sky coverage, like Daksha, will provide the best opportunities to detect X-ray counterparts of bright FRBs. </p>
<blockquote>
<p>å¿«é€Ÿå°„ç”µæš´ï¼ˆFRBsï¼‰æ˜¯çŸ­æš‚ä¸”èƒ½é‡æé«˜çš„æ²³å¤–å°„ç”µç¬å˜ï¼Œå…¶èµ·æºå’Œå‘å°„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚å°½ç®¡è¿›è¡Œäº†å¹¿æ³›çš„å¤šæ³¢é•¿æœç´¢ï¼Œä½†å°šæœªå‘ç°æ²³å¤–FRBçš„å¯ä¿¡Xå°„çº¿æˆ–å…¶ä»–å³æ—¶ç”µç£å¯¹åº”ç‰©ã€‚æˆ‘ä»¬åˆ©ç”¨AstroSat-CZTIæä¾›äº†å…³äºæ­¤ç±»å³æ—¶Xå°„çº¿å¯¹åº”ç‰©çš„å…¨é¢æœç´¢çš„ç»“æœï¼Œè¯¥ä»ªå™¨ä¸€ç›´åœ¨ç§¯ææ£€æµ‹å…¶ä»–é«˜èƒ½å¿«é€Ÿç¬å˜ï¼Œå¦‚ä¼½é©¬å°„çº¿æš´ï¼ˆGRBsï¼‰ã€‚æˆ‘ä»¬å¯¹AstroSat-CZTIæ•°æ®è¿›è¡Œäº†ç³»ç»Ÿæœç´¢ï¼Œå¯»æ‰¾ä¸578ä¸ªFRBåœ¨æ—¶é—´ä¸Šå’Œç©ºé—´ä¸Šç›¸å»åˆçš„ç¡¬Xå°„çº¿ç¬å˜ï¼Œä½†æ²¡æœ‰å‘ç°Xå°„çº¿å¯¹åº”ç‰©ã€‚æˆ‘ä»¬ä¼°è®¡äº†è¿™äº›äº‹ä»¶çš„æµé‡ä¸Šé™ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºXå°„çº¿ä¸å°„ç”µæµé‡æ¯”çš„ä¸Šé™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨è¿™äº›FRBçš„è‰²æ•£æµ‹é‡å¾—å‡ºçš„çº¢ç§»æ¥æ¯”è¾ƒå®ƒä»¬çš„ç­‰å‘å…‰åº¦ä¸GRBçš„å…‰åº¦ï¼Œäº†è§£è¿™ä¸¤ç±»ç¬å˜ä¹‹é—´çš„æ½œåœ¨ç›¸ä¼¼æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½¿ç”¨å½“å‰çš„å’Œå…¶ä»–å³å°†æ¨å‡ºçš„Xå°„çº¿ç›‘æµ‹å™¨ï¼ˆåŒ…æ‹¬è´¹ç±³GBMã€Swift BATã€SVOM ECLAIRså’Œè¾¾å¡ï¼‰æ¥æ£€æµ‹Xå°„çº¿å¯¹åº”ç‰©çš„å¯èƒ½æ€§ï¼Œè¿™æ˜¯åœ¨ä¸‹ä¸€ä¸ªä¸–ä»£çš„FRBæ£€æµ‹è®¾æ–½çš„æ—¶ä»£ï¼Œå¦‚CHIMEã€DSA-2000ã€CHORDå’ŒBURSTTç­‰ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œåƒè¾¾å¡è¿™æ ·å…·æœ‰é«˜çµæ•åº¦å’Œå¤§å¤©åŒºçš„Xå°„çº¿ç›‘æµ‹å™¨å°†ä¸ºæ£€æµ‹æ˜äº®FRBçš„Xå°„çº¿å¯¹åº”ç‰©æä¾›æœ€ä½³æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12452v1">PDF</a> 13 pages, 5 figures, 3 tables. Submitted to Journal of Astrophysics   and Astronomy. Comments welcome!</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¿«é€Ÿå°„ç”µæš´ï¼ˆFRBsï¼‰çš„Xå°„çº¿å¯¹åº”ç‰©ã€‚è™½ç„¶FRBsæ˜¯çŸ­æš‚ä¸”é«˜èƒ½çš„å°„ç”µç¬å˜ï¼Œä½†å¯¹å…¶èµ·æºå’Œå‘å°„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚ä½œè€…å¯¹AstroSat-CZTIè¿›è¡Œäº†å…¨é¢çš„æœç´¢ï¼Œå¯»æ‰¾ä¸FRBsç›¸å¯¹åº”çš„å³æ—¶Xå°„çº¿å¯¹åº”ç‰©ï¼Œä½†æœªå‘ç°ä»»ä½•Xå°„çº¿å¯¹åº”ç‰©ã€‚ä½œè€…ä¼°è®¡äº†è¿™äº›äº‹ä»¶çš„æµé‡ä¸Šé™ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºXå°„çº¿ä¸å°„ç”µæ³¢æµé‡æ¯”çš„ä¸Šé™ã€‚æ­¤å¤–ï¼Œä½œè€…åˆ©ç”¨è¿™äº›FRBçš„è‰²æ•£æµ‹é‡å¾—åˆ°çš„çº¢ç§»æ¥æ¯”è¾ƒå®ƒä»¬çš„ç­‰å®¹å‘å…‰é‡ä¸GRBçš„ç­‰å®¹å‘å…‰é‡ï¼Œä»¥æ­ç¤ºè¿™ä¸¤ç§ç¬æ€äº‹ä»¶ä¹‹é—´çš„æ½œåœ¨ç›¸ä¼¼æ€§ã€‚æœ€åï¼Œä½œè€…æ¢è®¨äº†ä½¿ç”¨å½“å‰å’Œæœªæ¥Xå°„çº¿ç›‘æµ‹å™¨ï¼ˆå¦‚Fermi-GBMã€Swift-BATç­‰ï¼‰æ£€æµ‹Xå°„çº¿å¯¹åº”ç‰©çš„å¯èƒ½æ€§ï¼Œå¼ºè°ƒäº†ä¸‹ä¸€ä»£FRBæ£€æµ‹è®¾æ–½å¦‚CHIMEç­‰å°†ä¸ºæ£€æµ‹FRBsçš„Xå°„çº¿å¯¹åº”ç‰©æä¾›æœ€ä½³æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FRBsæ˜¯çŸ­æš‚ä¸”é«˜èƒ½çš„å°„ç”µç¬å˜ï¼Œå…¶èµ·æºå’Œå‘å°„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚</li>
<li>é€šè¿‡AstroSat-CZTIæœªå‘ç°ä¸FRBsç›¸å¯¹åº”çš„å³æ—¶Xå°„çº¿å¯¹åº”ç‰©ã€‚</li>
<li>ä¼°è®¡äº†FRBsçš„æµé‡ä¸Šé™ï¼Œå¹¶è½¬åŒ–ä¸ºXå°„çº¿ä¸å°„ç”µæ³¢æµæµé‡æ¯”çš„ä¸Šé™ã€‚</li>
<li>åˆ©ç”¨çº¢ç§»æ¯”è¾ƒFRBså’ŒGRBçš„ç­‰å®¹å‘å…‰é‡ï¼Œæ¢è®¨ä¸¤è€…ä¹‹é—´çš„æ½œåœ¨ç›¸ä¼¼æ€§ã€‚</li>
<li>å½“å‰å’Œæœªæ¥Xå°„çº¿ç›‘æµ‹å™¨åœ¨æ£€æµ‹FRBsçš„Xå°„çº¿å¯¹åº”ç‰©æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸‹ä¸€ä»£FRBæ£€æµ‹è®¾æ–½å°†æä¾›æ£€æµ‹FRBsçš„Xå°„çº¿å¯¹åº”ç‰©çš„æœ€ä½³æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12452v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12452v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12452v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Causality-Inspired-Model-for-Intima-Media-Thickening-Assessment-in-Ultrasound-Videos"><a href="#A-Causality-Inspired-Model-for-Intima-Media-Thickening-Assessment-in-Ultrasound-Videos" class="headerlink" title="A Causality-Inspired Model for Intima-Media Thickening Assessment in   Ultrasound Videos"></a>A Causality-Inspired Model for Intima-Media Thickening Assessment in   Ultrasound Videos</h2><p><strong>Authors:Shuo Gao, Jingyang Zhang, Jun Xue, Meng Yang, Yang Chen, Guangquan Zhou</strong></p>
<p>Carotid atherosclerosis represents a significant health risk, with its early diagnosis primarily dependent on ultrasound-based assessments of carotid intima-media thickening. However, during carotid ultrasound screening, significant view variations cause style shifts, impairing content cues related to thickening, such as lumen anatomy, which introduces spurious correlations that hinder assessment. Therefore, we propose a novel causal-inspired method for assessing carotid intima-media thickening in frame-wise ultrasound videos, which focuses on two aspects: eliminating spurious correlations caused by style and enhancing causal content correlations. Specifically, we introduce a novel Spurious Correlation Elimination (SCE) module to remove non-causal style effects by enforcing prediction invariance with style perturbations. Simultaneously, we propose a Causal Equivalence Consolidation (CEC) module to strengthen causal content correlation through adversarial optimization during content randomization. Simultaneously, we design a Causal Transition Augmentation (CTA) module to ensure smooth causal flow by integrating an auxiliary pathway with text prompts and connecting it through contrastive learning. The experimental results on our in-house carotid ultrasound video dataset achieved an accuracy of 86.93%, demonstrating the superior performance of the proposed method. Code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/xielaobanyy/causal-imt%7D%7Bhttps://github.com/xielaobanyy/causal-imt%7D">https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}</a>. </p>
<blockquote>
<p>é¢ˆåŠ¨è„‰åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–æ˜¯ä¸€ä¸ªé‡è¦çš„å¥åº·é£é™©ã€‚å…¶æ—©æœŸè¯Šæ–­ä¸»è¦ä¾èµ–äºåŸºäºè¶…å£°çš„é¢ˆåŠ¨è„‰å†…è†œä¸­å±‚åšåº¦è¯„ä¼°ã€‚ç„¶è€Œï¼Œåœ¨é¢ˆåŠ¨è„‰è¶…å£°ç­›æŸ¥è¿‡ç¨‹ä¸­ï¼Œè§†å›¾å˜åŒ–è¾ƒå¤§å¯¼è‡´é£æ ¼å˜åŒ–ï¼Œä¸åšåº¦ç›¸å…³çš„å†…å®¹çº¿ç´¢å—æŸï¼Œå¦‚è…”é“ç»“æ„ï¼Œè¿™å¼•å…¥äº†è™šå‡å…³è”ï¼Œå¦¨ç¢è¯„ä¼°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å› æœå¯å‘æ–¹æ³•æ¥è¯„ä¼°å¸§çº§è¶…å£°è§†é¢‘ä¸­çš„é¢ˆåŠ¨è„‰å†…è†œä¸­å±‚åšåº¦ï¼Œè¯¥æ–¹æ³•ä¸»è¦å…³æ³¨ä¸¤ä¸ªæ–¹é¢ï¼šæ¶ˆé™¤ç”±é£æ ¼å¼•èµ·çš„è™šå‡å…³è”ï¼Œå¹¶å¢å¼ºå› æœå†…å®¹å…³è”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è™šå‡å…³è”æ¶ˆé™¤ï¼ˆSCEï¼‰æ¨¡å—ï¼Œé€šè¿‡å¼ºåˆ¶é¢„æµ‹ä¸å˜æ€§å’Œé£æ ¼æ‰°åŠ¨æ¥æ¶ˆé™¤éå› æœé£æ ¼æ•ˆåº”ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœç­‰ä»·å·©å›ºï¼ˆCECï¼‰æ¨¡å—ï¼Œé€šè¿‡å†…å®¹éšæœºåŒ–è¿‡ç¨‹ä¸­çš„å¯¹æŠ—ä¼˜åŒ–æ¥åŠ å¼ºå› æœå†…å®¹å…³è”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå› æœè¿‡æ¸¡å¢å¼ºï¼ˆCTAï¼‰æ¨¡å—ï¼Œé€šè¿‡è¾…åŠ©è·¯å¾„çš„æ–‡æœ¬æç¤ºå’Œå¯¹æ¯”å­¦ä¹ æ¥ç¡®ä¿æµç•…çš„å› æœæµç¨‹ã€‚åœ¨æˆ‘ä»¬å†…éƒ¨çš„é¢ˆåŠ¨è„‰è¶…å£°è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¾¾åˆ°äº†86.93%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xielaobanyy/causal-imt%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xielaobanyy/causal-imtè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12418v1">PDF</a> 10 pages, 5 figures, conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå› æœç†è®ºçš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°é¢ˆåŠ¨è„‰å†…è†œä¸­å±‚å¢åšã€‚è¯¥æ–¹æ³•é€šè¿‡æ¶ˆé™¤éå› æœé£æ ¼æ•ˆåº”ã€å¢å¼ºå› æœå†…å®¹å…³è”å’Œç¡®ä¿å¹³æ»‘çš„å› æœæµç¨‹ï¼Œæé«˜äº†è¶…å£°è§†é¢‘ä¸­é¢ˆåŠ¨è„‰å†…è†œä¸­å±‚å¢åšè¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢ˆåŠ¨è„‰ç²¥æ ·ç¡¬åŒ–çš„æ—©æœŸè¯Šæ–­ä¸»è¦ä¾èµ–äºåŸºäºè¶…å£°çš„é¢ˆåŠ¨è„‰å†…è†œä¸­å±‚å¢åšè¯„ä¼°ã€‚</li>
<li>ç°æœ‰è¶…å£°ç­›æŸ¥ä¸­ï¼Œè§†å›¾å˜åŒ–å¼•èµ·çš„é£æ ¼è½¬å˜ä¼šæŸå®³ä¸å¢åšç›¸å…³çš„å†…å®¹çº¿ç´¢ï¼Œå¦‚è…”é“ç»“æ„ï¼Œä»è€Œäº§ç”Ÿè™šå‡å…³è”ï¼Œå½±å“è¯„ä¼°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå› æœç†è®ºçš„æ–¹æ³•ï¼Œä¸“æ³¨äºè¯„ä¼°é¢ˆåŠ¨è„‰å†…è†œä¸­å±‚å¢åšï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šæ¶ˆé™¤è™šå‡å…³è”ã€å¼ºåŒ–å› æœå†…å®¹å…³è”å’Œç¡®ä¿å¹³æ»‘çš„å› æœæµç¨‹ã€‚</li>
<li>é€šè¿‡éå› æœé£æ ¼æ•ˆåº”æ¶ˆé™¤æ¨¡å—ï¼ˆSCEï¼‰ï¼Œåˆ©ç”¨é¢„æµ‹ä¸å˜æ€§é€šè¿‡é£æ ¼æ‰°åŠ¨æ¥å»é™¤éå› æœé£æ ¼å½±å“ã€‚</li>
<li>é€šè¿‡å› æœç­‰ä»·å·©å›ºæ¨¡å—ï¼ˆCECï¼‰ï¼Œåˆ©ç”¨å¯¹æŠ—ä¼˜åŒ–å’Œå†…å®¹éšæœºåŒ–æ¥åŠ å¼ºå› æœå†…å®¹å…³è”ã€‚</li>
<li>é€šè¿‡å› æœè½¬æ¢å¢å¼ºæ¨¡å—ï¼ˆCTAï¼‰ï¼Œç»“åˆæ–‡æœ¬æç¤ºå’Œå¯¹æ¯”å­¦ä¹ ï¼Œç¡®ä¿æµç•…çš„å› æœæµç¨‹ã€‚</li>
<li>åœ¨è‡ªæœ‰é¢ˆåŠ¨è„‰è¶…å£°è§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒè¾¾åˆ°äº†86.93%çš„å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºæ‰€æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12418v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12418v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12418v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12418v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Novel-Double-Pruning-method-for-Imbalanced-Data-using-Information-Entropy-and-Roulette-Wheel-Selection-for-Breast-Cancer-Diagnosis"><a href="#A-Novel-Double-Pruning-method-for-Imbalanced-Data-using-Information-Entropy-and-Roulette-Wheel-Selection-for-Breast-Cancer-Diagnosis" class="headerlink" title="A Novel Double Pruning method for Imbalanced Data using Information   Entropy and Roulette Wheel Selection for Breast Cancer Diagnosis"></a>A Novel Double Pruning method for Imbalanced Data using Information   Entropy and Roulette Wheel Selection for Breast Cancer Diagnosis</h2><p><strong>Authors:Soufiane Bacha, Huansheng Ning, Belarbi Mostefa, Doreen Sebastian Sarwatt, Sahraoui Dhelim</strong></p>
<p>Accurate illness diagnosis is vital for effective treatment and patient safety. Machine learning models are widely used for cancer diagnosis based on historical medical data. However, data imbalance remains a major challenge, leading to hindering classifier performance and reliability. The SMOTEBoost method addresses this issue by generating synthetic data to balance the dataset, but it may overlook crucial overlapping regions near the decision boundary and can produce noisy samples. This paper proposes RE-SMOTEBoost, an enhanced version of SMOTEBoost, designed to overcome these limitations. Firstly, RE-SMOTEBoost focuses on generating synthetic samples in overlapping regions to better capture the decision boundary using roulette wheel selection. Secondly, it incorporates a filtering mechanism based on information entropy to reduce noise, and borderline cases and improve the quality of generated data. Thirdly, we introduce a double regularization penalty to control the synthetic samples proximity to the decision boundary and avoid class overlap. These enhancements enable higher-quality oversampling of the minority class, resulting in a more balanced and effective training dataset. The proposed method outperforms existing state-of-the-art techniques when evaluated on imbalanced datasets. Compared to the top-performing sampling algorithms, RE-SMOTEBoost demonstrates a notable improvement of 3.22% in accuracy and a variance reduction of 88.8%. These results indicate that the proposed model offers a solid solution for medical settings, effectively overcoming data scarcity and severe imbalance caused by limited samples, data collection difficulties, and privacy constraints. </p>
<blockquote>
<p>å‡†ç¡®è¯Šæ–­ç–¾ç—…å¯¹äºæœ‰æ•ˆæ²»ç–—å’Œæ‚£è€…å®‰å…¨è‡³å…³é‡è¦ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹åŸºäºå†å²åŒ»ç–—æ•°æ®è¢«å¹¿æ³›ç”¨äºç™Œç—‡è¯Šæ–­ã€‚ç„¶è€Œï¼Œæ•°æ®ä¸å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå¯¼è‡´åˆ†ç±»å™¨æ€§èƒ½å’Œå¯é æ€§å—åˆ°é˜»ç¢ã€‚SMOTEBoostæ–¹æ³•é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æ¥å¹³è¡¡æ•°æ®é›†ï¼Œä»¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†å®ƒå¯èƒ½ä¼šå¿½ç•¥å†³ç­–è¾¹ç•Œé™„è¿‘çš„å…³é”®é‡å åŒºåŸŸå¹¶äº§ç”Ÿå™ªå£°æ ·æœ¬ã€‚æœ¬æ–‡æå‡ºäº†RE-SMOTEBoostï¼Œè¿™æ˜¯SMOTEBoostçš„å¢å¼ºç‰ˆï¼Œæ—¨åœ¨å…‹æœè¿™äº›å±€é™æ€§ã€‚é¦–å…ˆï¼ŒRE-SMOTEBoostä¸“æ³¨äºåœ¨é‡å åŒºåŸŸç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œä»¥æ›´å¥½åœ°ä½¿ç”¨è½®ç›˜é€‰æ‹©æ³•æ•æ‰å†³ç­–è¾¹ç•Œã€‚å…¶æ¬¡ï¼Œå®ƒåŸºäºä¿¡æ¯ç†µå¼•å…¥äº†ä¸€ç§è¿‡æ»¤æœºåˆ¶ï¼Œä»¥å‡å°‘å™ªå£°ã€è¾¹ç•Œæ¡ˆä¾‹å¹¶æé«˜ç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¼•å…¥åŒé‡æ­£åˆ™åŒ–æƒ©ç½šæ¥æ§åˆ¶åˆæˆæ ·æœ¬æ¥è¿‘å†³ç­–è¾¹ç•Œçš„ç¨‹åº¦ï¼Œå¹¶é¿å…ç±»é‡å ã€‚è¿™äº›å¢å¼ºåŠŸèƒ½å®ç°äº†å¯¹å°‘æ•°ç±»çš„æ›´é«˜è´¨é‡è¿‡é‡‡æ ·ï¼Œä»è€Œå¾—åˆ°æ›´å¹³è¡¡å’Œæœ‰æ•ˆçš„è®­ç»ƒæ•°æ®é›†ã€‚å½“åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚ä¸è¡¨ç°æœ€ä½³çš„é‡‡æ ·ç®—æ³•ç›¸æ¯”ï¼ŒRE-SMOTEBooståœ¨å‡†ç¡®åº¦ä¸Šå®ç°äº†3.22%çš„æ˜¾è‘—æé«˜ï¼Œå¹¶é™ä½äº†88.8%çš„æ–¹å·®ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºæ¨¡å‹ä¸ºåŒ»ç–—ç¯å¢ƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆå…‹æœå› æ ·æœ¬æœ‰é™ã€æ•°æ®é‡‡é›†å›°éš¾å’Œéšç§çº¦æŸè€Œå¯¼è‡´çš„æ•°æ®ç¨€ç¼ºå’Œä¸¥é‡ä¸å¹³è¡¡é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12239v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>RE-SMOTEBoostæ–¹æ³•æ˜¯ä¸ºäº†è§£å†³æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­é¢ä¸´çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜è€Œæå‡ºçš„ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆåˆæˆæ ·æœ¬ã€è¿‡æ»¤æœºåˆ¶å’ŒåŒé‡æ­£åˆ™åŒ–æƒ©ç½šç­‰æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œä»¥æé«˜å°‘æ•°ç±»æ•°æ®çš„è¿‡é‡‡æ ·è´¨é‡ï¼Œä»è€Œå¾—åˆ°æ›´å¹³è¡¡å’Œæœ‰æ•ˆçš„è®­ç»ƒæ•°æ®é›†ã€‚åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå‡†ç¡®ç‡æé«˜3.22%ï¼Œæ–¹å·®é™ä½88.8%ã€‚è¿™ä¸ºåŒ»å­¦ç¯å¢ƒä¸­çš„æ•°æ®ç¨€ç¼ºå’Œä¸¥é‡ä¸å¹³è¡¡é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®ä¸å¹³è¡¡æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç–¾ç—…è¯Šæ–­ä¸­é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå½±å“åˆ†ç±»å™¨çš„æ€§èƒ½å’Œå¯é æ€§ã€‚</li>
<li>SMOTEBoostæ–¹æ³•é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æ¥å¹³è¡¡æ•°æ®é›†ï¼Œä½†å¯èƒ½å¿½ç•¥å†³ç­–è¾¹ç•Œé™„è¿‘çš„é‡å åŒºåŸŸå¹¶äº§ç”Ÿå™ªå£°æ ·æœ¬ã€‚</li>
<li>RE-SMOTEBoostæ˜¯SMOTEBoostçš„å¢å¼ºç‰ˆæœ¬ï¼Œä¸“æ³¨äºåœ¨é‡å åŒºåŸŸç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œä»¥æ›´å¥½åœ°æ•è·å†³ç­–è¾¹ç•Œã€‚</li>
<li>RE-SMOTEBoostå¼•å…¥åŸºäºä¿¡æ¯ç†µçš„è¿‡æ»¤æœºåˆ¶ï¼Œä»¥å‡å°‘å™ªéŸ³ã€è¾¹ç•Œæ¡ˆä¾‹å¹¶æé«˜ç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚</li>
<li>åŒé‡æ­£åˆ™åŒ–æƒ©ç½šæ§åˆ¶åˆæˆæ ·æœ¬ä¸å†³ç­–è¾¹ç•Œçš„æ¥è¿‘åº¦ï¼Œé¿å…ç±»é‡å ã€‚</li>
<li>RE-SMOTEBooståœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå‡†ç¡®ç‡å’Œæ–¹å·®å‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12239v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12239v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Adaptive-Label-Correction-for-Robust-Medical-Image-Segmentation-with-Noisy-Labels"><a href="#Adaptive-Label-Correction-for-Robust-Medical-Image-Segmentation-with-Noisy-Labels" class="headerlink" title="Adaptive Label Correction for Robust Medical Image Segmentation with   Noisy Labels"></a>Adaptive Label Correction for Robust Medical Image Segmentation with   Noisy Labels</h2><p><strong>Authors:Chengxuan Qian, Kai Han, Siqi Ma, Chongwen Lyu, Zhenlong Yuan, Jun Chen, Zhe Liu</strong></p>
<p>Deep learning has shown remarkable success in medical image analysis, but its reliance on large volumes of high-quality labeled data limits its applicability. While noisy labeled data are easier to obtain, directly incorporating them into training can degrade model performance. To address this challenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC) self-ensemble framework for robust medical image segmentation with noisy labels. The framework leverages the Mean Teacher architecture to ensure consistent learning under noise perturbations. It includes an adaptive label refinement mechanism that dynamically captures and weights differences across multiple disturbance versions to enhance the quality of noisy labels. Additionally, a sample-level uncertainty-based label selection algorithm is introduced to prioritize high-confidence samples for network updates, mitigating the impact of noisy annotations. Consistency learning is integrated to align the predictions of the student and teacher networks, further enhancing model robustness. Extensive experiments on two public datasets demonstrate the effectiveness of the proposed framework, showing significant improvements in segmentation performance. By fully exploiting the strengths of the Mean Teacher structure, the ALC framework effectively processes noisy labels, adapts to challenging scenarios, and achieves competitive results compared to state-of-the-art methods. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†å…¶ä¾èµ–äºå¤§é‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ç‰¹æ€§é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚è™½ç„¶è·å–å¸¦å™ªå£°çš„æ ‡æ³¨æ•°æ®æ›´å®¹æ˜“ï¼Œä½†ç›´æ¥å°†å…¶çº³å…¥è®­ç»ƒä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMean Teacherçš„è‡ªé€‚åº”æ ‡ç­¾æ ¡æ­£ï¼ˆALCï¼‰è‡ªé›†æˆæ¡†æ¶ï¼Œç”¨äºç¨³å¥çš„åŒ»å­¦å›¾åƒåˆ†å‰²å™ªå£°æ ‡ç­¾ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Mean Teacheræ¶æ„ï¼Œç¡®ä¿åœ¨å™ªå£°æ‰°åŠ¨ä¸‹å®ç°ä¸€è‡´å­¦ä¹ ã€‚å®ƒåŒ…å«ä¸€ä¸ªè‡ªé€‚åº”æ ‡ç­¾ä¼˜åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŠ¨æ€æ•è·å¹¶æƒè¡¡å¤šä¸ªæ‰°åŠ¨ç‰ˆæœ¬ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥æé«˜å™ªå£°æ ‡ç­¾çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºæ ·æœ¬çº§åˆ«ä¸ç¡®å®šæ€§çš„æ ‡ç­¾é€‰æ‹©ç®—æ³•ï¼Œä»¥ä¼˜å…ˆå¤„ç†é«˜ç½®ä¿¡åº¦æ ·æœ¬è¿›è¡Œç½‘ç»œæ›´æ–°ï¼Œå‡è½»å™ªå£°æ³¨é‡Šçš„å½±å“ã€‚ä¸€è‡´æ€§å­¦ä¹ è¢«æ•´åˆä»¥å¯¹é½å­¦ç”Ÿå’Œæ•™å¸ˆç½‘ç»œçš„é¢„æµ‹ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºåˆ†å‰²æ€§èƒ½æœ‰æ˜¾è‘—æé«˜ã€‚é€šè¿‡å……åˆ†åˆ©ç”¨Mean Teacherç»“æ„çš„ä¼˜åŠ¿ï¼ŒALCæ¡†æ¶æœ‰æ•ˆåœ°å¤„ç†äº†å™ªå£°æ ‡ç­¾ï¼Œé€‚åº”äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼Œå¹¶ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶ä¾èµ–äºå¤§é‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„åº”ç”¨èŒƒå›´å—é™ã€‚ä¸ºè§£å†³ä½¿ç”¨å¸¦å™ªå£°æ ‡ç­¾æ•°æ®ç›´æ¥è®­ç»ƒä¼šå¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMean Teacherçš„è‡ªé€‚åº”æ ‡ç­¾æ ¡æ­£ï¼ˆALCï¼‰è‡ªé›†æˆæ¡†æ¶ï¼Œç”¨äºç¨³å¥åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Mean Teacheræ¶æ„ç¡®ä¿åœ¨å™ªå£°æ‰°åŠ¨ä¸‹çš„å­¦ä¹ ä¸€è‡´æ€§ï¼Œå¹¶åŒ…å«è‡ªé€‚åº”æ ‡ç­¾ä¼˜åŒ–æœºåˆ¶ï¼ŒåŠ¨æ€æ•æ‰å¹¶æƒè¡¡ä¸åŒæ‰°åŠ¨ç‰ˆæœ¬ä¹‹é—´çš„å·®å¼‚ï¼Œæé«˜å™ªå£°æ ‡ç­¾çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºæ ·æœ¬çº§åˆ«ä¸ç¡®å®šæ€§çš„æ ‡ç­¾é€‰æ‹©ç®—æ³•ï¼Œä¼˜å…ˆé€‰å–é«˜ç½®ä¿¡åº¦æ ·æœ¬è¿›è¡Œç½‘ç»œæ›´æ–°ï¼Œå‡å°‘å™ªå£°æ ‡æ³¨çš„å½±å“ã€‚é›†æˆä¸€è‡´æ€§å­¦ä¹ ä»¥å¯¹é½å­¦ç”Ÿå’Œæ•™å¸ˆç½‘ç»œçš„é¢„æµ‹ç»“æœï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚åˆ©ç”¨Mean Teacherç»“æ„çš„ä¼˜åŠ¿ï¼ŒALCæ¡†æ¶æœ‰æ•ˆå¤„ç†å™ªå£°æ ‡ç­¾ï¼Œé€‚åº”å¤æ‚åœºæ™¯ï¼Œå¹¶ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”å–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨å—é™äºæ ‡æ³¨æ•°æ®çš„è´¨é‡å’Œæ•°é‡ã€‚</li>
<li>å™ªå£°æ ‡ç­¾ç›´æ¥ç”¨äºè®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºåŸºäºMean Teacherçš„è‡ªé€‚åº”æ ‡ç­¾æ ¡æ­£ï¼ˆALCï¼‰è‡ªé›†æˆæ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨Mean Teacheræ¶æ„ç¡®ä¿å™ªå£°æ‰°åŠ¨ä¸‹çš„å­¦ä¹ ä¸€è‡´æ€§ã€‚</li>
<li>è‡ªé€‚åº”æ ‡ç­¾ä¼˜åŒ–æœºåˆ¶æé«˜å™ªå£°æ ‡ç­¾è´¨é‡ã€‚</li>
<li>åŸºäºæ ·æœ¬çº§åˆ«ä¸ç¡®å®šæ€§çš„æ ‡ç­¾é€‰æ‹©ç®—æ³•ä¼˜å…ˆé€‰å–é«˜ç½®ä¿¡åº¦æ ·æœ¬è¿›è¡Œç½‘ç»œæ›´æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12218v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12218v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12218v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12218v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12218v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12218v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Box-Enhancing-Remote-Sensing-Image-Segmentation-with-Freehand-Sketches"><a href="#Breaking-the-Box-Enhancing-Remote-Sensing-Image-Segmentation-with-Freehand-Sketches" class="headerlink" title="Breaking the Box: Enhancing Remote Sensing Image Segmentation with   Freehand Sketches"></a>Breaking the Box: Enhancing Remote Sensing Image Segmentation with   Freehand Sketches</h2><p><strong>Authors:Ying Zang, Yuncan Gao, Jiangi Zhang, Yuangi Hu, Runlong Cao, Lanyun Zhu, Qi Zhu, Deyi Ji, Renjun Xu, Tianrun Chen</strong></p>
<p>This work advances zero-shot interactive segmentation for remote sensing imagery through three key contributions. First, we propose a novel sketch-based prompting method, enabling users to intuitively outline objects, surpassing traditional point or box prompts. Second, we introduce LTL-Sensing, the first dataset pairing human sketches with remote sensing imagery, setting a benchmark for future research. Third, we present LTL-Net, a model featuring a multi-input prompting transport module tailored for freehand sketches. Extensive experiments show our approach significantly improves segmentation accuracy and robustness over state-of-the-art methods like SAM, fostering more intuitive human-AI collaboration in remote sensing analysis and enhancing its applications. </p>
<blockquote>
<p>æœ¬æ–‡é€šè¿‡ä¸‰ä¸ªä¸»è¦è´¡çŒ®æ¨åŠ¨äº†é¥æ„Ÿå›¾åƒé›¶æ ·æœ¬äº¤äº’åˆ†å‰²æŠ€æœ¯çš„å‘å±•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºè‰å›¾æç¤ºæ–¹æ³•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°æç»˜å‡ºç›®æ ‡å¯¹è±¡ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç‚¹æˆ–æ¡†æç¤ºã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†LTL-Sensingæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†äººç±»è‰å›¾å’Œé¥æ„Ÿå›¾åƒé…å¯¹çš„æ•°æ®é›†ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶è®¾å®šäº†åŸºå‡†ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†LTL-Netæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰å¤šè¾“å…¥æç¤ºä¼ è¾“æ¨¡å—ï¼Œä¸“ä¸ºè‡ªç”±æ‰‹ç»˜è‰å›¾è®¾è®¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºSAMç­‰å…ˆè¿›æ–¹æ³•ï¼Œåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œç¨³å¥æ€§ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œä¿ƒè¿›äº†é¥æ„Ÿåˆ†æä¸­äººæœºåä½œçš„ç›´è§‰æ€§ï¼Œå¹¶å¢å¼ºäº†å…¶åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12191v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€é¡¹é’ˆå¯¹é¥æ„Ÿå½±åƒé›¶æ ·æœ¬äº¤äº’åˆ†å‰²æŠ€æœ¯çš„æ–°è¿›å±•ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šæå‡ºä¸€ç§åŸºäºè‰å›¾çš„æ–°å‹æç¤ºæ–¹æ³•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°æç»˜ç‰©ä½“ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç‚¹æˆ–æ¡†æç¤ºï¼›å¼•å…¥LTL-Sensingæ•°æ®é›†ï¼Œé¦–æ¬¡å°†äººç±»è‰å›¾ä¸é¥æ„Ÿå½±åƒé…å¯¹ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºå‡†ï¼›æå‡ºLTL-Netæ¨¡å‹ï¼Œå…·å¤‡é’ˆå¯¹è‡ªç”±æ‰‹ç»˜è‰å›¾çš„è·¨è¾“å…¥æç¤ºä¼ è¾“æ¨¡å—ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²ç²¾åº¦å’Œç¨³å¥æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯å¦‚SAMï¼Œæ¨åŠ¨äº†é¥æ„Ÿåˆ†æä¸­äººç±»ä¸AIçš„åˆä½œæ›´ä¸ºç›´è§‚ï¼Œå¹¶ä¸°å¯Œäº†å…¶åº”ç”¨åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºè‰å›¾çš„æ–°å‹æç¤ºæ–¹æ³•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç›´è§‚æç»˜ç‰©ä½“ã€‚</li>
<li>å¼•å…¥äº†LTL-Sensingæ•°æ®é›†ï¼Œä¸ºè‰å›¾ä¸é¥æ„Ÿå½±åƒçš„ç»“åˆæä¾›åŸºå‡†ã€‚</li>
<li>å¼€å‘LTL-Netæ¨¡å‹ï¼Œå…·å¤‡è·¨è¾“å…¥æç¤ºä¼ è¾“æ¨¡å—ï¼Œé€‚åº”è‡ªç”±æ‰‹ç»˜è‰å›¾ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦å’Œç¨³å¥æ€§ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯å¦‚SAMã€‚</li>
<li>ä¿ƒè¿›äº†é¥æ„Ÿåˆ†æä¸­äººç±»ä¸AIçš„åˆä½œæ›´åŠ ç›´è§‚ã€‚</li>
<li>ä¸°å¯Œäº†é¥æ„Ÿåˆ†æçš„åº”ç”¨åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12191v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12191v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12191v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12191v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12191v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="E-SAM-Training-Free-Segment-Every-Entity-Model"><a href="#E-SAM-Training-Free-Segment-Every-Entity-Model" class="headerlink" title="E-SAM: Training-Free Segment Every Entity Model"></a>E-SAM: Training-Free Segment Every Entity Model</h2><p><strong>Authors:Weiming Zhang, Dingwen Xiao, Lei Chen, Lin Wang</strong></p>
<p>Entity Segmentation (ES) aims at identifying and segmenting distinct entities within an image without the need for predefined class labels. This characteristic makes ES well-suited to open-world applications with adaptation to diverse and dynamically changing environments, where new and previously unseen entities may appear frequently. Existing ES methods either require large annotated datasets or high training costs, limiting their scalability and adaptability. Recently, the Segment Anything Model (SAM), especially in its Automatic Mask Generation (AMG) mode, has shown potential for holistic image segmentation. However, it struggles with over-segmentation and under-segmentation, making it less effective for ES. In this paper, we introduce E-SAM, a novel training-free framework that exhibits exceptional ES capability. Specifically, we first propose Multi-level Mask Generation (MMG) that hierarchically processes SAMâ€™s AMG outputs to generate reliable object-level masks while preserving fine details at other levels. Entity-level Mask Refinement (EMR) then refines these object-level masks into accurate entity-level masks. That is, it separates overlapping masks to address the redundancy issues inherent in SAMâ€™s outputs and merges similar masks by evaluating entity-level consistency. Lastly, Under-Segmentation Refinement (USR) addresses under-segmentation by generating additional high-confidence masks fused with EMR outputs to produce the final ES map. These three modules are seamlessly optimized to achieve the best ES without additional training overhead. Extensive experiments demonstrate that E-SAM achieves state-of-the-art performance compared to prior ES methods, demonstrating a significant improvement by +30.1 on benchmark metrics. </p>
<blockquote>
<p>å®ä½“åˆ†å‰²ï¼ˆESï¼‰æ—¨åœ¨è¯†åˆ«å’Œåˆ†å‰²å›¾åƒä¸­çš„ä¸åŒå®ä½“ï¼Œè€Œæ— éœ€é¢„å…ˆå®šä¹‰çš„ç±»åˆ«æ ‡ç­¾ã€‚è¿™ä¸€ç‰¹ç‚¹ä½¿å¾—ESéå¸¸é€‚åˆäºé€‚åº”å¤šæ ·åŒ–å’ŒåŠ¨æ€å˜åŒ–ç¯å¢ƒçš„å¼€æ”¾ä¸–ç•Œåº”ç”¨ç¨‹åºï¼Œå…¶ä¸­å¯èƒ½å‡ºç°æ–°çš„å’Œä¹‹å‰æœªè§è¿‡çš„å®ä½“ã€‚ç°æœ‰çš„ESæ–¹æ³•è¦ä¹ˆéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè¦ä¹ˆè®­ç»ƒæˆæœ¬é«˜ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚æœ€è¿‘ï¼Œå°¤å…¶æ˜¯å…¶è‡ªåŠ¨è’™ç‰ˆç”Ÿæˆï¼ˆAMGï¼‰æ¨¡å¼çš„â€œä»»ä½•äº‹ç‰©åˆ†å‰²æ¨¡å‹â€ï¼ˆSAMï¼‰å·²æ˜¾ç¤ºå‡ºæ•´ä½“å›¾åƒåˆ†å‰²çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒåœ¨è¿‡åˆ†å‰²å’Œä¸è¶³åˆ†å‰²æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™ä½¿å¾—å®ƒå¯¹ESçš„æ•ˆåŠ›é™ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†E-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ— éœ€è®­ç»ƒçš„è®­ç»ƒæ¡†æ¶ï¼Œè¡¨ç°å‡ºå“è¶Šçš„ESèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºå¤šå±‚æ¬¡è’™ç‰ˆç”Ÿæˆï¼ˆMMGï¼‰ï¼Œå®ƒåˆ†å±‚å¤„ç†SAMçš„AMGè¾“å‡ºæ¥ç”Ÿæˆå¯é çš„ç‰©ä½“çº§è’™ç‰ˆï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–çº§åˆ«çš„ç²¾ç»†ç»†èŠ‚ã€‚ç„¶åï¼Œå®ä½“çº§è’™ç‰ˆç»†åŒ–ï¼ˆEMRï¼‰å°†è¿™äº›ç‰©ä½“çº§è’™ç‰ˆç»†åŒ–ä¸ºå‡†ç¡®çš„å®ä½“çº§è’™ç‰ˆã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒåˆ†ç¦»é‡å çš„è’™ç‰ˆï¼Œè§£å†³SAMè¾“å‡ºä¸­å›ºæœ‰çš„å†—ä½™é—®é¢˜ï¼Œå¹¶é€šè¿‡è¯„ä¼°å®ä½“çº§ä¸€è‡´æ€§æ¥åˆå¹¶ç›¸ä¼¼çš„è’™ç‰ˆã€‚æœ€åï¼Œä¸è¶³åˆ†å‰²ç»†åŒ–ï¼ˆUSRï¼‰è§£å†³ä¸è¶³åˆ†å‰²é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆä¸EMRè¾“å‡ºèåˆçš„é«˜ç½®ä¿¡åº¦è’™ç‰ˆæ¥ç”Ÿæˆæœ€ç»ˆçš„ESåœ°å›¾ã€‚è¿™ä¸‰ä¸ªæ¨¡å—æ— ç¼ä¼˜åŒ–ï¼Œä»¥åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–è®­ç»ƒå¼€é”€çš„æƒ…å†µä¸‹å®ç°æœ€ä½³ESã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å…ˆå‰çš„ESæ–¹æ³•ç›¸æ¯”ï¼ŒE-SAMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨åŸºå‡†æŒ‡æ ‡ä¸Šå®ç°äº†+30.1çš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12094v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹æ¡†æ¶E-SAMï¼Œç”¨äºå®ç°å®ä½“åˆ†å‰²ï¼ˆESï¼‰ã€‚å®ƒé€šè¿‡å¤šå±‚æ¬¡æ©è†œç”Ÿæˆï¼ˆMMGï¼‰ã€å®ä½“çº§æ©è†œç»†åŒ–ï¼ˆEMRï¼‰å’Œæ¬ åˆ†å‰²ç»†åŒ–ï¼ˆUSRï¼‰ä¸‰ä¸ªæ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†SAMåœ¨å®ä½“åˆ†å‰²ä¸­é‡åˆ°çš„è¿‡åº¦åˆ†å‰²å’Œæ¬ åˆ†å‰²é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒE-SAMåœ¨åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>E-SAMæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå®ä½“åˆ†å‰²ï¼ˆESï¼‰ï¼Œé€‚åº”äºå¼€æ”¾ä¸–ç•Œåº”ç”¨ã€‚</li>
<li>E-SAMé€šè¿‡å¤šå±‚æ¬¡æ©è†œç”Ÿæˆï¼ˆMMGï¼‰ç”Ÿæˆå¯é çš„å¯¹è±¡çº§æ©è†œï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–çº§åˆ«çš„ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>å®ä½“çº§æ©è†œç»†åŒ–ï¼ˆEMRï¼‰æ¨¡å—ç”¨äºå°†å¯¹è±¡çº§æ©è†œç»†åŒ–ä¸ºå‡†ç¡®çš„å®ä½“çº§æ©è†œï¼Œè§£å†³äº†SAMè¾“å‡ºä¸­çš„å†—ä½™é—®é¢˜ã€‚</li>
<li>æ¬ åˆ†å‰²ç»†åŒ–ï¼ˆUSRï¼‰æ¨¡å—è§£å†³äº†SAMçš„æ¬ åˆ†å‰²é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆé¢å¤–çš„é«˜ç½®ä¿¡åº¦æ©è†œä¸EMRè¾“å‡ºèåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„ESåœ°å›¾ã€‚</li>
<li>E-SAMé€šè¿‡è¿™ä¸‰ä¸ªæ¨¡å—çš„ååŒå·¥ä½œï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒå¼€é”€ï¼Œå³å¯å®ç°æœ€ä½³çš„å®ä½“åˆ†å‰²ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒE-SAMåœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–ESæ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12094v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12094v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12094v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12094v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12094v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0e8d199658f9832bcaa9874e192322f2.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  AC-Reason Towards Theory-Guided Actual Causality Reasoning with Large   Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-14/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a48bf04883db03ace735c930f3da0584.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  Less is More Improving Motion Diffusion Models with Sparse Keyframes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
