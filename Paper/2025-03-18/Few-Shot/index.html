<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  TriDF Triplane-Accelerated Density Fields for Few-Shot Remote Sensing   Novel View Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.11223v3/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-18-æ›´æ–°"><a href="#2025-03-18-æ›´æ–°" class="headerlink" title="2025-03-18 æ›´æ–°"></a>2025-03-18 æ›´æ–°</h1><h2 id="TriDF-Triplane-Accelerated-Density-Fields-for-Few-Shot-Remote-Sensing-Novel-View-Synthesis"><a href="#TriDF-Triplane-Accelerated-Density-Fields-for-Few-Shot-Remote-Sensing-Novel-View-Synthesis" class="headerlink" title="TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing   Novel View Synthesis"></a>TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing   Novel View Synthesis</h2><p><strong>Authors:Jiaming Kang, Keyan Chen, Zhengxia Zou, Zhenwei Shi</strong></p>
<p>Remote sensing novel view synthesis (NVS) offers significant potential for 3D interpretation of remote sensing scenes, with important applications in urban planning and environmental monitoring. However, remote sensing scenes frequently lack sufficient multi-view images due to acquisition constraints. While existing NVS methods tend to overfit when processing limited input views, advanced few-shot NVS methods are computationally intensive and perform sub-optimally in remote sensing scenes. This paper presents TriDF, an efficient hybrid 3D representation for fast remote sensing NVS from as few as 3 input views. Our approach decouples color and volume density information, modeling them independently to reduce the computational burden on implicit radiance fields and accelerate reconstruction. We explore the potential of the triplane representation in few-shot NVS tasks by mapping high-frequency color information onto this compact structure, and the direct optimization of feature planes significantly speeds up convergence. Volume density is modeled as continuous density fields, incorporating reference features from neighboring views through image-based rendering to compensate for limited input data. Additionally, we introduce depth-guided optimization based on point clouds, which effectively mitigates the overfitting problem in few-shot NVS. Comprehensive experiments across multiple remote sensing scenes demonstrate that our hybrid representation achieves a 30x speed increase compared to NeRF-based methods, while simultaneously improving rendering quality metrics over advanced few-shot methods (7.4% increase in PSNR, 12.2% in SSIM, and 18.7% in LPIPS). The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/kanehub/TriDF">https://github.com/kanehub/TriDF</a> </p>
<blockquote>
<p>é¥æ„Ÿæ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰åœ¨ä¸‰ç»´é¥æ„Ÿåœºæ™¯è§£è¯»æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œåœ¨åŸå¸‚è§„åˆ’å’Œç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºé‡‡é›†é™åˆ¶ï¼Œé¥æ„Ÿåœºæ™¯é€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„å¤šè§†è§’å›¾åƒã€‚ç°æœ‰çš„NVSæ–¹æ³•åœ¨å¤„ç†æœ‰é™è¾“å…¥è§†å›¾æ—¶å®¹æ˜“äº§ç”Ÿè¿‡åº¦æ‹Ÿåˆï¼Œè€Œå…ˆè¿›çš„å°‘è§†è§’NVSæ–¹æ³•è®¡ç®—é‡å¤§ï¼Œåœ¨é¥æ„Ÿåœºæ™¯ä¸­çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚æœ¬æ–‡æå‡ºäº†TriDFï¼Œä¸€ç§é«˜æ•ˆçš„æ··åˆä¸‰ç»´è¡¨ç¤ºæ–¹æ³•ï¼Œä»…ä»å°‘æ•°ï¼ˆè‡³å°‘3ä¸ªï¼‰è¾“å…¥è§†è§’å¿«é€Ÿè¿›è¡Œé¥æ„ŸNVSã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é¢œè‰²å’Œä½“ç§¯å¯†åº¦ä¿¡æ¯è§£è€¦ï¼Œç‹¬ç«‹å»ºæ¨¡ï¼Œä»¥é™ä½å¯¹éšè¾å°„åœºçš„è®¡ç®—è´Ÿæ‹…å¹¶åŠ é€Ÿé‡å»ºã€‚æˆ‘ä»¬æ¢ç´¢äº†triplaneè¡¨ç¤ºåœ¨å°‘è§†è§’NVSä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå°†é«˜é¢‘é¢œè‰²ä¿¡æ¯æ˜ å°„åˆ°è¿™ä¸ªç´§å‡‘ç»“æ„ä¸Šï¼Œç‰¹å¾å¹³é¢çš„ç›´æ¥ä¼˜åŒ–æ˜¾è‘—åŠ é€Ÿäº†æ”¶æ•›ã€‚ä½“ç§¯å¯†åº¦è¢«å»ºæ¨¡ä¸ºè¿ç»­å¯†åº¦åœºï¼Œé€šè¿‡åŸºäºå›¾åƒæ¸²æŸ“èå…¥é‚»è¿‘è§†å›¾çš„å‚è€ƒç‰¹å¾æ¥å¼¥è¡¥æœ‰é™çš„è¾“å…¥æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºç‚¹äº‘çš„æ·±åº¦å¼•å¯¼ä¼˜åŒ–ï¼Œæœ‰æ•ˆç¼“è§£äº†å°‘è§†è§’NVSä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚åœ¨å¤šä¸ªé¥æ„Ÿåœºæ™¯çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ··åˆè¡¨ç¤ºæ–¹æ³•ç›¸å¯¹äºåŸºäºNeRFçš„æ–¹æ³•å®ç°äº†30å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨æ¸²æŸ“è´¨é‡æŒ‡æ ‡ä¸Šæ”¹è¿›äº†å…ˆè¿›çš„å°‘è§†è§’æ–¹æ³•ï¼ˆPSNRæé«˜7.4%ï¼ŒSSIMæé«˜12.2%ï¼ŒLPIPSæé«˜18.7%ï¼‰ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/kanehub/TriDF">https://github.com/kanehub/TriDF</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13347v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ··åˆä¸‰ç»´è¡¨ç¤ºæ–¹æ³•TriDFï¼Œç”¨äºä»ä»…3ä¸ªè¾“å…¥è§†å›¾å¿«é€Ÿè¿›è¡Œé¥æ„Ÿå½±åƒçš„æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰ã€‚è¯¥æ–¹æ³•å°†é¢œè‰²å’Œä½“ç§¯å¯†åº¦ä¿¡æ¯è§£è€¦ï¼Œç‹¬ç«‹å»ºæ¨¡ï¼Œé™ä½éšè¾å°„åœºçš„è®¡ç®—è´Ÿæ‹…å¹¶åŠ é€Ÿé‡å»ºã€‚åœ¨å°‘é‡å°„å‡»çš„NVSä»»åŠ¡ä¸­æ¢ç´¢äº†triplaneè¡¨ç¤ºçš„æ½œåŠ›ï¼Œå°†é«˜é¢‘é¢œè‰²ä¿¡æ¯æ˜ å°„åˆ°æ­¤ç´§å‡‘ç»“æ„ä¸Šï¼Œå¹¶ç›´æ¥ä¼˜åŒ–ç‰¹å¾å¹³é¢ä»¥åŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚ä½“ç§¯å¯†åº¦è¢«å»ºæ¨¡ä¸ºè¿ç»­å¯†åº¦åœºï¼Œé€šè¿‡åŸºäºå›¾åƒçš„æ¸²æŸ“èå…¥é‚»è¿‘è§†å›¾çš„å‚è€ƒç‰¹å¾æ¥å¼¥è¡¥æœ‰é™çš„è¾“å…¥æ•°æ®ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†åŸºäºç‚¹äº‘çš„æ·±åº¦æŒ‡å¯¼ä¼˜åŒ–ï¼Œæœ‰æ•ˆç¼“è§£äº†å°‘é‡å°„å‡»NVSä¸­çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œä¸NeRFæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ··åˆè¡¨ç¤ºæ–¹æ³•å®ç°äº†30å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨æ¸²æŸ“è´¨é‡æŒ‡æ ‡ä¸Šä¹Ÿæœ‰æ‰€æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TriDFæ˜¯ä¸€ç§ç”¨äºé¥æ„Ÿå½±åƒçš„æ–°å‹è§†å›¾åˆæˆçš„æ··åˆä¸‰ç»´è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿä»æå°‘çš„è¾“å…¥è§†å›¾ï¼ˆä»…3ä¸ªï¼‰è¿›è¡Œå¿«é€Ÿåˆæˆã€‚</li>
<li>TriDFé€šè¿‡å°†é¢œè‰²å’Œä½“ç§¯å¯†åº¦ä¿¡æ¯è§£è€¦ï¼Œé™ä½äº†è®¡ç®—çš„å¤æ‚æ€§å¹¶åŠ é€Ÿäº†é‡å»ºè¿‡ç¨‹ã€‚</li>
<li>TriDFåˆ©ç”¨triplaneè¡¨ç¤ºæ³•å¤„ç†é«˜é¢‘é¢œè‰²ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ç›´æ¥ä¼˜åŒ–ç‰¹å¾å¹³é¢æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>ä½“ç§¯å¯†åº¦è¢«å»ºæ¨¡ä¸ºè¿ç»­å¯†åº¦åœºï¼Œå¹¶é€šè¿‡èå…¥é‚»è¿‘è§†å›¾çš„å‚è€ƒç‰¹å¾æ¥å¼¥è¡¥æ•°æ®ä¸è¶³ã€‚</li>
<li>å¼•å…¥çš„æ·±åº¦æŒ‡å¯¼ä¼˜åŒ–æŠ€æœ¯åŸºäºç‚¹äº‘ï¼Œèƒ½æœ‰æ•ˆè§£å†³å°‘é‡è¾“å…¥è§†å›¾ä¸‹çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTriDFç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æœ‰ç€æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼ˆ30å€ï¼‰ï¼ŒåŒæ—¶åœ¨æ¸²æŸ“è´¨é‡ä¸Šä¹Ÿæœ‰æ‰€æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de5763c23488b75945dff8e54b72ebc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ae1e2fcd3a619d4a1838fbf95ec192.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53fc4defc489c0fd06f36997926a3d32.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations"><a href="#Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations" class="headerlink" title="Edit Transfer: Learning Image Editing via Vision In-Context Relations"></a>Edit Transfer: Learning Image Editing via Vision In-Context Relations</h2><p><strong>Authors:Lan Chen, Qi Mao, Yuchao Gu, Mike Zheng Shou</strong></p>
<p>We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°è®¾ç½®ï¼Œåä¸ºâ€œç¼–è¾‘ä¼ è¾“â€ï¼ˆEdit Transferï¼‰ï¼Œåœ¨è¯¥è®¾ç½®ä¸­ï¼Œæ¨¡å‹ä»…ä»ä¸€ä¸ªæºç›®æ ‡ç¤ºä¾‹ä¸­å­¦ä¹ è½¬æ¢ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚è™½ç„¶åŸºäºæ–‡æœ¬çš„æ–¹æ³•åœ¨é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œè¯­ä¹‰æ“ä½œæ–¹é¢è¡¨ç°å“è¶Šï¼Œä½†å®ƒä»¬é€šå¸¸å¯¹ç²¾ç¡®çš„å‡ ä½•ç»†èŠ‚ï¼ˆä¾‹å¦‚å§¿åŠ¿å’Œè§†ç‚¹å˜åŒ–ï¼‰æ„Ÿåˆ°å›°æƒ‘ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºå‚è€ƒçš„ç¼–è¾‘é€šå¸¸ä¾§é‡äºæ ·å¼æˆ–å¤–è§‚ï¼Œè€Œåœ¨éåˆšæ€§è½¬æ¢æ–¹é¢è¡¨ç°ä¸ä½³ã€‚é€šè¿‡ä»æºç›®æ ‡å¯¹ä¸­å­¦ä¹ ç¼–è¾‘è½¬æ¢ï¼ŒEdit Transfer ç¼“è§£äº†ä»…ä½¿ç”¨æ–‡æœ¬å’Œå¤–è§‚ä¸­å¿ƒå‚è€ƒçš„é™åˆ¶ã€‚æˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºDiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ã€‚æˆ‘ä»¬å°†ç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒæ’åˆ—æˆç»Ÿä¸€çš„å››é¢æ¿åˆæˆå›¾åƒï¼Œç„¶ååº”ç”¨è½»é‡çº§çš„LoRAå¾®è°ƒæ¥ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ å¤æ‚çš„ç©ºé—´è½¬æ¢ã€‚å°½ç®¡åªä½¿ç”¨äº†42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä½†Edit Transferåœ¨å¤šç§éåˆšæ€§åœºæ™¯ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°çš„TIEå’ŒRIEæ–¹æ³•ï¼Œè¯æ˜äº†å°‘é‡è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13327v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹ç¼–è¾‘è½¬ç§»æŠ€æœ¯è¢«æå‡ºï¼Œè¯¥æŠ€æœ¯é€šè¿‡ä»å•ä¸€æºç›®æ ‡ç¤ºä¾‹ä¸­å­¦ä¹ è½¬æ¢å¹¶åº”ç”¨äºæ–°æŸ¥è¯¢å›¾åƒã€‚è¯¥æŠ€æœ¯å¼¥è¡¥äº†çº¯æ–‡æœ¬å’ŒåŸºäºå¤–è§‚å‚è€ƒç¼–è¾‘æ–¹æ³•çš„ä¸è¶³ï¼Œå¯æ˜¾å¼åœ°å­¦ä¹ æºåˆ°ç›®æ ‡çš„ç¼–è¾‘è½¬æ¢ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ å¯å‘ï¼Œç ”ç©¶æå‡ºåŸºäºè§†è§‰å…³ç³»çš„ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ï¼Œå»ºç«‹äºæ–‡æœ¬åˆ°å›¾åƒçš„DiTæ¨¡å‹ä¹‹ä¸Šã€‚ä½¿ç”¨è½»é‡çº§LoRAå¾®è°ƒæ•è·æ¥è‡ªæœ€å°æ ·æœ¬çš„å¤æ‚ç©ºé—´è½¬æ¢ã€‚å°½ç®¡åªä½¿ç”¨äº†42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä½†ç¼–è¾‘è½¬ç§»æŠ€æœ¯åœ¨å¤šç§éåˆšæ€§åœºæ™¯ä¸Šå¤§å¹…è¶…è¶Šäº†æœ€å…ˆè¿›çš„TIEå’ŒRIEæ–¹æ³•ï¼Œå±•ç¤ºäº†å°‘æ•°è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼–è¾‘è½¬ç§»æŠ€æœ¯èƒ½ä»å•ä¸€æºç›®æ ‡ç¤ºä¾‹ä¸­å­¦ä¹ è½¬æ¢å¹¶åº”ç”¨äºæ–°æŸ¥è¯¢å›¾åƒã€‚</li>
<li>è¯¥æŠ€æœ¯å…‹æœäº†æ–‡æœ¬æ–¹æ³•å’Œå‚è€ƒåŸºäºç¼–è¾‘æ–¹æ³•çš„å±€é™æ€§ï¼Œå¯è¿›è¡Œç²¾ç¡®çš„å‡ ä½•ç»†èŠ‚è½¬æ¢ã€‚</li>
<li>è§†è§‰å…³ç³»çš„ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼å—å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ å¯å‘ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨DiTæ¨¡å‹ä¸ºåŸºç¡€ï¼Œç»“åˆç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒè¿›è¡Œç»Ÿä¸€å¤„ç†ã€‚</li>
<li>é€šè¿‡è½»é‡çº§LoRAå¾®è°ƒï¼Œè¯¥æŠ€æœ¯èƒ½æ•è·å¤æ‚ç©ºé—´è½¬æ¢ï¼Œä»…ä½¿ç”¨å°‘é‡æ ·æœ¬ã€‚</li>
<li>ç¼–è¾‘è½¬ç§»æŠ€æœ¯åœ¨å¤šç§éåˆšæ€§åœºæ™¯ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„TIEå’ŒRIEæ–¹æ³•ã€‚</li>
<li>è¯¥æŠ€æœ¯å±•ç¤ºäº†å°‘æ•°è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62abc8701cc0822cc38b0abdfc84a860.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a87e29fd01504a9ca50c9c786fed625.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be31f915090bc4a5ddf7e8b10eb97fe9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f5d3a1333ed599da96a19b547d8ddf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66a3ce0dce3f9e275e16027cc270189d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76404e796fc375a36fe8e0ccf676dd2a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DivCon-NeRF-Generating-Augmented-Rays-with-Diversity-and-Consistency-for-Few-shot-View-Synthesis"><a href="#DivCon-NeRF-Generating-Augmented-Rays-with-Diversity-and-Consistency-for-Few-shot-View-Synthesis" class="headerlink" title="DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency   for Few-shot View Synthesis"></a>DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency   for Few-shot View Synthesis</h2><p><strong>Authors:Ingyun Lee, Jae Won Jang, Seunghyeon Seo, Nojun Kwak</strong></p>
<p>Neural Radiance Field (NeRF) has shown remarkable performance in novel view synthesis but requires many multiview images, making it impractical for few-shot scenarios. Ray augmentation was proposed to prevent overfitting for sparse training data by generating additional rays. However, existing methods, which generate augmented rays only near the original rays, produce severe floaters and appearance distortion due to limited viewpoints and inconsistent rays obstructed by nearby obstacles and complex surfaces. To address these problems, we propose DivCon-NeRF, which significantly enhances both diversity and consistency. It employs surface-sphere augmentation, which preserves the distance between the original camera and the predicted surface point. This allows the model to compare the order of high-probability surface points and filter out inconsistent rays easily without requiring the exact depth. By introducing inner-sphere augmentation, DivCon-NeRF randomizes angles and distances for diverse viewpoints, further increasing diversity. Consequently, our method significantly reduces floaters and visual distortions, achieving state-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code will be publicly available. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†è§’åˆæˆä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†éœ€è¦å¤šè§†è§’å›¾åƒï¼Œå¯¹äºå°æ ·æœ¬åœºæ™¯æ¥è¯´å¹¶ä¸å®ç”¨ã€‚å…‰çº¿å¢å¼ºæ³•é€šè¿‡ç”Ÿæˆé¢å¤–å…‰çº¿æ¥é˜²æ­¢ç¨€ç–è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…åœ¨åŸå§‹å…‰çº¿é™„è¿‘ç”Ÿæˆå¢å¼ºå…‰çº¿ï¼Œç”±äºæœ‰é™çš„è§†è§’å’Œä¸ä¸€è‡´çš„å°„çº¿è¢«é™„è¿‘éšœç¢ç‰©å’Œå¤æ‚è¡¨é¢é˜»æŒ¡ï¼Œå¯¼è‡´å‡ºç°ä¸¥é‡çš„æ¼‚æµ®ç‰©å’Œå¤–è§‚å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºDivCon-NeRFï¼Œå®ƒæ˜¾è‘—æé«˜äº†å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚å®ƒé‡‡ç”¨æ›²é¢çƒä½“å¢å¼ºæ³•ï¼Œä¿ç•™äº†åŸå§‹ç›¸æœºå’Œé¢„æµ‹è¡¨é¢ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ¯”è¾ƒé«˜æ¦‚ç‡è¡¨é¢ç‚¹çš„é¡ºåºï¼Œå¹¶è½»æ¾è¿‡æ»¤å‡ºä¸ä¸€è‡´çš„å°„çº¿ï¼Œè€Œæ— éœ€ç²¾ç¡®æ·±åº¦ã€‚é€šè¿‡å¼•å…¥å†…éƒ¨çƒä½“å¢å¼ºæ³•ï¼ŒDivCon-NeRFéšæœºåŒ–è§’åº¦å’Œè·ç¦»ä»¥å®ç°ä¸åŒè§†è§’ï¼Œè¿›ä¸€æ­¥å¢åŠ å¤šæ ·æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¼‚æµ®ç‰©å’Œè§†è§‰å¤±çœŸï¼Œåœ¨Blenderã€LLFFå’ŒDTUæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12947v1">PDF</a> 11 pages, 6 figures</p>
<p><strong>Summary</strong><br>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†è§’åˆæˆä¸­è¡¨ç°å“è¶Šï¼Œä½†éœ€è¦å¤šè§†è§’å›¾åƒï¼Œéš¾ä»¥å®ç°å°‘é‡åœºæ™¯çš„åº”ç”¨ã€‚é’ˆå¯¹ç¨€ç–è®­ç»ƒæ•°æ®æ˜“äº§ç”Ÿçš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡ºäº†å°„çº¿å¢å¼ºæ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…åœ¨æ¥è¿‘åŸå§‹å°„çº¿æ—¶ç”Ÿæˆå¢å¼ºå°„çº¿ï¼Œç”±äºæœ‰é™çš„è§†è§’å’Œå¤æ‚çš„è¡¨é¢é®æŒ¡å¯¼è‡´ä¸ä¸€è‡´çš„å°„çº¿ï¼Œä»è€Œäº§ç”Ÿä¸¥é‡çš„æµ®ç‚¹å’Œå¤–è§‚å¤±çœŸé—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DivCon-NeRFï¼Œæ˜¾è‘—æé«˜äº†å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚å®ƒé‡‡ç”¨çƒé¢å¢å¼ºæŠ€æœ¯ï¼Œä¿ç•™åŸå§‹ç›¸æœºä¸é¢„æµ‹è¡¨é¢ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚é€šè¿‡å¼•å…¥å†…çƒé¢å¢å¼ºæŠ€æœ¯ï¼ŒDivCon-NeRFéšæœºæ”¹å˜è§’åº¦å’Œè·ç¦»ï¼Œå®ç°å¤šæ ·çš„è§†è§’ï¼Œè¿›ä¸€æ­¥æé«˜å¤šæ ·æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æµ®ç‚¹å’Œè§†è§‰å¤±çœŸï¼Œåœ¨Blenderã€LLFFå’ŒDTUæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFåœ¨æ–°å‹è§†è§’åˆæˆä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†éœ€è¦å¤šè§†è§’å›¾åƒï¼Œéš¾ä»¥å®ç°å°‘é‡åœºæ™¯åº”ç”¨ã€‚</li>
<li>ç°æœ‰å°„çº¿å¢å¼ºæ–¹æ³•ä¸»è¦åœ¨æ¥è¿‘åŸå§‹å°„çº¿æ—¶ç”Ÿæˆå¢å¼ºå°„çº¿ï¼Œå­˜åœ¨æµ®ç‚¹å’Œå¤–è§‚å¤±çœŸé—®é¢˜ã€‚</li>
<li>DivCon-NeRFé€šè¿‡é‡‡ç”¨çƒé¢å¢å¼ºæŠ€æœ¯æé«˜å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>DivCon-NeRFä¿ç•™åŸå§‹ç›¸æœºä¸é¢„æµ‹è¡¨é¢ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œæ˜“äºè¿‡æ»¤ä¸ä¸€è‡´çš„å°„çº¿ã€‚</li>
<li>é€šè¿‡å¼•å…¥å†…çƒé¢å¢å¼ºæŠ€æœ¯ï¼ŒDivCon-NeRFéšæœºæ”¹å˜è§’åº¦å’Œè·ç¦»ï¼Œå®ç°å¤šæ ·çš„è§†è§’ã€‚</li>
<li>DivCon-NeRFæ˜¾è‘—å‡å°‘äº†æµ®ç‚¹å’Œè§†è§‰å¤±çœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f402c1d14d63d3aaae2236382d9495ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cf57e39a512e5853aacf9aea11470fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0d224a0561a182331b9f2c1f7e9408c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TLAC-Two-stage-LMM-Augmented-CLIP-for-Zero-Shot-Classification"><a href="#TLAC-Two-stage-LMM-Augmented-CLIP-for-Zero-Shot-Classification" class="headerlink" title="TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification"></a>TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification</h2><p><strong>Authors:Ans Munir, Faisal Z. Qureshi, Muhammad Haris Khan, Mohsen Ali</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot performance on image classification. However, state-of-the-art methods often rely on fine-tuning techniques like prompt learning and adapter-based tuning to optimize CLIPâ€™s performance. The necessity for fine-tuning significantly limits CLIPâ€™s adaptability to novel datasets and domains. This requirement mandates substantial time and computational resources for each new dataset. To overcome this limitation, we introduce simple yet effective training-free approaches, Single-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC), that leverages powerful Large Multimodal Models (LMMs), such as Gemini, for image classification. The proposed methods leverages the capabilities of pre-trained LMMs, allowing for seamless adaptation to diverse datasets and domains without the need for additional training. Our approaches involve prompting the LMM to identify objects within an image. Subsequently, the CLIP text encoder determines the image class by identifying the dataset class with the highest semantic similarity to the LLM predicted object. We evaluated our models on 11 base-to-novel datasets and they achieved superior accuracy on 9 of these, including benchmarks like ImageNet, SUN397 and Caltech101, while maintaining a strictly training-free paradigm. Our overall accuracy of 83.44% surpasses the previous state-of-the-art few-shot methods by a margin of 6.75%. Our method achieved 83.6% average accuracy across 13 datasets, a 9.7% improvement over the previous 73.9% state-of-the-art for training-free approaches. Our method improves domain generalization, with a 3.6% gain on ImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot methods. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å›¾åƒåˆ†ç±»æ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¾®è°ƒæŠ€æœ¯ï¼Œå¦‚åŸºäºæç¤ºçš„å­¦ä¹ å’ŒåŸºäºé€‚é…å™¨çš„è°ƒæ•´ï¼Œä»¥ä¼˜åŒ–CLIPçš„æ€§èƒ½ã€‚å¯¹å¾®è°ƒçš„å¿…è¦æ€§æ˜¾è‘—é™åˆ¶äº†CLIPå¯¹æ–°æ•°æ®é›†å’Œé¢†åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚é’ˆå¯¹æ¯ä¸ªæ–°æ•°æ®é›†ï¼Œè¿™ç§è¦æ±‚éƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç®€å•è€Œæœ‰æ•ˆçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œå³å•é˜¶æ®µLMMå¢å¼ºCLIPï¼ˆSLACï¼‰å’Œä¸¤é˜¶æ®µLMMå¢å¼ºCLIPï¼ˆTLACï¼‰ï¼Œå®ƒä»¬åˆ©ç”¨å¼ºå¤§çš„å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œå¦‚Geminiï¼Œè¿›è¡Œå›¾åƒåˆ†ç±»ã€‚æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒLMMçš„èƒ½åŠ›ï¼Œå…è®¸æ— ç¼é€‚åº”å„ç§æ•°æ®é›†å’Œé¢†åŸŸï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠæç¤ºLMMè¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ã€‚éšåï¼ŒCLIPæ–‡æœ¬ç¼–ç å™¨é€šè¿‡è¯†åˆ«ä¸LLMé¢„æµ‹å¯¹è±¡è¯­ä¹‰ç›¸ä¼¼æ€§æœ€é«˜çš„æ•°æ®é›†ç±»åˆ«æ¥ç¡®å®šå›¾åƒç±»åˆ«ã€‚æˆ‘ä»¬åœ¨11ä¸ªåŸºç¡€åˆ°æ–°é¢–çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œåœ¨å…¶ä¸­çš„9ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜è¶Šçš„å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬ImageNetã€SUN397å’ŒCaltech101ç­‰åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒæ—¶ä¿æŒäº†ä¸¥æ ¼çš„æ— è®­ç»ƒèŒƒå¼ã€‚æˆ‘ä»¬çš„æ€»ä½“å‡†ç¡®ç‡83.44%è¶…è¶Šäº†ä¹‹å‰æœ€å…ˆè¿›çš„å°‘æ ·æœ¬æ–¹æ³•ï¼Œæé«˜äº†6.75%ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨13ä¸ªæ•°æ®é›†ä¸Šå¹³å‡å‡†ç¡®ç‡ä¸º83.6%ï¼Œæ¯”ä¹‹å‰æ— è®­ç»ƒæ–¹æ³•çš„73.9%æé«˜äº†9.7%ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ImageNetV2ä¸Šæé«˜äº†3.6%ï¼Œåœ¨ImageNet-Sä¸Šæé«˜äº†16.96%ï¼Œåœ¨ImageNet-Rä¸Šæé«˜äº†12.59%ï¼Œè¶…è¿‡äº†å…ˆå‰çš„å°‘æ ·æœ¬æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12206v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºCLIPæ¨¡å‹çš„ä¸¤ç§è®­ç»ƒå…è´¹æ–¹æ³•â€”â€”å•é˜¶æ®µLMMå¢å¼ºCLIPï¼ˆSLACï¼‰å’ŒåŒé˜¶æ®µLMMå¢å¼ºCLIPï¼ˆTLACï¼‰ï¼Œç”¨äºå›¾åƒåˆ†ç±»ã€‚è¿™ä¸¤ç§æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„å¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œå¦‚Geminiï¼Œé€šè¿‡æç¤ºæ¨¡å‹è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ï¼Œå¹¶åˆ©ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨ç¡®å®šä¸LLMé¢„æµ‹å¯¹è±¡è¯­ä¹‰ç›¸ä¼¼æ€§æœ€é«˜çš„æ•°æ®é›†ç±»åˆ«æ¥è¿›è¡Œæ— ç¼é€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œé¢†åŸŸã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨åŸºå‡†åˆ°æ–°é¢–æ•°æ®é›†çš„æµ‹è¯•ä¸­ï¼ŒSLACå’ŒTLACåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜äº†é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦è¿›è¡Œå¾®è°ƒä»¥é€‚åº”æ–°æ•°æ®é›†å’Œé¢†åŸŸã€‚</li>
<li>æå‡ºSLACå’ŒTLACä¸¤ç§è®­ç»ƒå…è´¹æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆLMMsï¼‰è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚</li>
<li>SLACå’ŒTLACé€šè¿‡æç¤ºLMMè¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ï¼Œå¹¶é€šè¿‡CLIPæ–‡æœ¬ç¼–ç å™¨ç¡®å®šå›¾åƒç±»åˆ«ï¼Œå®ç°æ— ç¼é€‚åº”ä¸åŒæ•°æ®é›†å’Œé¢†åŸŸã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆå¦‚ImageNet, SUN397, Caltech101ï¼‰ä¸Šï¼ŒSLACå’ŒTLACå®ç°äº†è¶…è¶Šå…ˆå‰å°‘æ ·æœ¬æ–¹æ³•çš„é«˜å‡†ç¡®ç‡ã€‚</li>
<li>SLACå’ŒTLACçš„æ€»ä½“å‡†ç¡®ç‡è¾¾åˆ°äº†83.44%ï¼Œç›¸è¾ƒäºå…ˆå‰çš„å°‘æ ·æœ¬æ–¹æ³•æé«˜äº†6.75%ã€‚</li>
<li>ä¸å…ˆå‰çš„è®­ç»ƒå…è´¹æ–¹æ³•ç›¸æ¯”ï¼ŒSLACå’ŒTLACåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†9.7%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-656fb7104a6d17016e455ee6b5522c18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baf6c7422ae7d74a41396b2094621291.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d2bcf6b36a92397912bba437e7421fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db9a5fc1ea03ada5f4635981bb442cfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2f84e29cfe183ac9b495e2c52677d4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f26aed848ec81f6576372f0af5878c6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-the-LLM-Accessibility-Divide-Performance-Fairness-and-Cost-of-Closed-versus-Open-LLMs-for-Automated-Essay-Scoring"><a href="#Bridging-the-LLM-Accessibility-Divide-Performance-Fairness-and-Cost-of-Closed-versus-Open-LLMs-for-Automated-Essay-Scoring" class="headerlink" title="Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost   of Closed versus Open LLMs for Automated Essay Scoring"></a>Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost   of Closed versus Open LLMs for Automated Essay Scoring</h2><p><strong>Authors:Kezia Oketch, John P. Lalor, Yi Yang, Ahmed Abbasi</strong></p>
<p>Closed large language models (LLMs) such as GPT-4 have set state-of-the-art results across a number of NLP tasks and have become central to NLP and machine learning (ML)-driven solutions. Closed LLMsâ€™ performance and wide adoption has sparked considerable debate about their accessibility in terms of availability, cost, and transparency. In this study, we perform a rigorous comparative analysis of nine leading LLMs, spanning closed, open, and open-source LLM ecosystems, across text assessment and generation tasks related to automated essay scoring. Our findings reveal that for few-shot learning-based assessment of human generated essays, open LLMs such as Llama 3 and Qwen2.5 perform comparably to GPT-4 in terms of predictive performance, with no significant differences in disparate impact scores when considering age- or race-related fairness. Moreover, Llama 3 offers a substantial cost advantage, being up to 37 times more cost-efficient than GPT-4. For generative tasks, we find that essays generated by top open LLMs are comparable to closed LLMs in terms of their semantic composition&#x2F;embeddings and ML assessed scores. Our findings challenge the dominance of closed LLMs and highlight the democratizing potential of open LLMs, suggesting they can effectively bridge accessibility divides while maintaining competitive performance and fairness. </p>
<blockquote>
<p>å°é—­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4å·²åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶æˆä¸ºNLPå’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰é©±åŠ¨è§£å†³æ–¹æ¡ˆçš„æ ¸å¿ƒã€‚å°é—­LLMçš„æ€§èƒ½å’Œå¹¿æ³›é‡‡ç”¨å¼•å‘äº†å…³äºå…¶åœ¨å¯ç”¨æ€§ã€æˆæœ¬å’Œé€æ˜åº¦æ–¹é¢çš„å¯è®¿é—®æ€§çš„æ¿€çƒˆè¾©è®ºã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¹ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼å¯¹æ¯”åˆ†æï¼Œæ¶µç›–äº†å°é—­ã€å¼€æ”¾å’Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿï¼Œæ¶‰åŠä¸è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ç›¸å…³çš„æ–‡æœ¬è¯„ä¼°å’Œç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåœ¨åŸºäºå°‘æ ·æœ¬å­¦ä¹ çš„å¯¹äººç±»ç”Ÿæˆä½œæ–‡è¿›è¡Œè¯„ä¼°æ—¶ï¼Œå¼€æ”¾çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama 3å’ŒQwen 2.5ï¼‰åœ¨é¢„æµ‹æ€§èƒ½æ–¹é¢ä¸GPT-4è¡¨ç°ç›¸å½“ã€‚åœ¨è€ƒè™‘ä¸å¹´é¾„æˆ–ç§æ—ç›¸å…³çš„å…¬å¹³æ€§æ—¶ï¼Œä¸åŒå½±å“åˆ†æ•°çš„å·®å¼‚å¹¶ä¸æ˜¾è‘—ã€‚æ­¤å¤–ï¼ŒLlama 3æä¾›äº†æ˜¾è‘—çš„æˆæœ¬ä¼˜åŠ¿ï¼Œé«˜è¾¾GPT-4çš„37å€æˆæœ¬æ•ˆç›Šã€‚å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œæˆ‘ä»¬å‘ç°é¡¶çº§å¼€æ”¾çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä½œæ–‡åœ¨è¯­ä¹‰ç»„æˆ&#x2F;åµŒå…¥å’Œæœºå™¨å­¦ä¹ è¯„ä¼°åˆ†æ•°æ–¹é¢ä¸å°é—­çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†å°é—­å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿæ²»åœ°ä½ï¼Œå¹¶çªå‡ºäº†å¼€æ”¾å¤§å‹è¯­è¨€æ¨¡å‹çš„æ°‘ä¸»åŒ–æ½œåŠ›ï¼Œè¡¨æ˜å®ƒä»¬å¯ä»¥æœ‰æ•ˆåœ°ç¼©å°å¯è®¿é—®æ€§å·®è·ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§èƒ½å’Œå…¬å¹³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11827v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œæˆä¸ºNLPå’Œæœºå™¨å­¦ä¹ é©±åŠ¨è§£å†³æ–¹æ¡ˆçš„æ ¸å¿ƒã€‚æœ¬ç ”ç©¶å¯¹ä¹ç§é¢†å…ˆçš„LLMè¿›è¡Œäº†ä¸¥æ ¼çš„æ¯”è¾ƒåˆ†æï¼Œå‘ç°å¯¹äºåŸºäºå°‘æ ·æœ¬å­¦ä¹ çš„å¯¹äººç±»ç”Ÿæˆæ–‡ç« è¿›è¡Œè¯„ä¼°çš„ä»»åŠ¡ï¼Œå¼€æºLLMå¦‚Llama 3å’ŒQwen2.5ä¸GPT-4è¡¨ç°ç›¸å½“ï¼Œå¹¶ä¸”åœ¨è€ƒè™‘å¹´é¾„å’Œç§æ—å…¬å¹³æ€§çš„å½±å“è¯„åˆ†ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚Llama 3åœ¨æˆæœ¬ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæˆæœ¬æ•ˆç›Šé«˜è¾¾GPT-4çš„37å€ã€‚å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œé¡¶çº§å¼€æºLLMç”Ÿæˆçš„æ–‡ç« åœ¨è¯­ä¹‰ç»„æˆå’Œæœºå™¨å­¦ä¹ è¯„ä¼°åˆ†æ•°æ–¹é¢ä¸å°é—­LLMç›¸å½“ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜äº†å°é—­LLMçš„ä¸»å¯¼åœ°ä½ï¼Œå¹¶çªå‡ºäº†å¼€æºLLMçš„æ°‘ä¸»åŒ–æ½œåŠ›ï¼Œè¡¨æ˜å®ƒä»¬å¯ä»¥æœ‰æ•ˆåœ°ç¼©å°å·®è·å¹¶ç»´æŒç«äº‰æ€§èƒ½å’Œå…¬å¹³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨NLPä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œæˆä¸ºæ ¸å¿ƒè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å°é—­LLMå¦‚GPT-4åœ¨æ€§èƒ½å’Œå¹¿æ³›é‡‡ç”¨æ–¹é¢å¼•å‘å…³äºå…¶å¯è®¿é—®æ€§ï¼ˆåŒ…æ‹¬å¯ç”¨æ€§ã€æˆæœ¬å’Œé€æ˜åº¦ï¼‰çš„è®¨è®ºã€‚</li>
<li>å¯¹äºåŸºäºå°‘æ ·æœ¬å­¦ä¹ çš„äººç±»ç”Ÿæˆæ–‡ç« è¯„ä¼°ï¼Œå¼€æºLLMå¦‚Llama 3å’ŒQwen2.5ä¸GPT-4è¡¨ç°ç›¸å½“ã€‚</li>
<li>åœ¨å…¬å¹³æ€§çš„å½±å“è¯„åˆ†ä¸Šï¼Œå¼€æºLLMä¸GPT-4æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>Llama 3åœ¨æˆæœ¬ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸å¯¹äºGPT-4æ›´ä¸ºç»æµå®æƒ ã€‚</li>
<li>å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œé¡¶çº§å¼€æºLLMç”Ÿæˆçš„æ–‡ç« ä¸å°é—­LLMåœ¨è¯­ä¹‰ç»„æˆå’Œæœºå™¨å­¦ä¹ è¯„ä¼°åˆ†æ•°æ–¹é¢ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4348ab39ddd114d71bf76b315c08982f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2503085fe573fe0562f3caa10000674.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2503.11827v1/page_3_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de2cb22c12de05e776037cf024ee2a13.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-Few-Shot-Adaptation-of-Vision-Language-Models-in-Two-Stages"><a href="#Rethinking-Few-Shot-Adaptation-of-Vision-Language-Models-in-Two-Stages" class="headerlink" title="Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages"></a>Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages</h2><p><strong>Authors:Matteo Farina, Massimiliano Mancini, Giovanni Iacca, Elisa Ricci</strong></p>
<p>An old-school recipe for training a classifier is to (i) learn a good feature extractor and (ii) optimize a linear layer atop. When only a handful of samples are available per category, as in Few-Shot Adaptation (FSA), data are insufficient to fit a large number of parameters, rendering the above impractical. This is especially true with large pre-trained Vision-Language Models (VLMs), which motivated successful research at the intersection of Parameter-Efficient Fine-tuning (PEFT) and FSA. In this work, we start by analyzing the learning dynamics of PEFT techniques when trained on few-shot data from only a subset of categories, referred to as the &#96;&#96;baseâ€™â€™ classes. We show that such dynamics naturally splits into two distinct phases: (i) task-level feature extraction and (ii) specialization to the available concepts. To accommodate this dynamic, we then depart from prompt- or adapter-based methods and tackle FSA differently. Specifically, given a fixed computational budget, we split it to (i) learn a task-specific feature extractor via PEFT and (ii) train a linear classifier on top. We call this scheme Two-Stage Few-Shot Adaptation (2SFS). Differently from established methods, our scheme enables a novel form of selective inference at a category level, i.e., at test time, only novel categories are embedded by the adapted text encoder, while embeddings of base categories are available within the classifier. Results with fixed hyperparameters across two settings, three backbones, and eleven datasets, show that 2SFS matches or surpasses the state-of-the-art, while established methods degrade significantly across settings. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„è®­ç»ƒåˆ†ç±»å™¨çš„é£Ÿè°±æ˜¯ï¼ˆiï¼‰å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾æå–å™¨ï¼ˆiiï¼‰ä¼˜åŒ–å…¶é¡¶éƒ¨çš„çº¿æ€§å±‚ã€‚åœ¨æ¯æ¬¡ç±»åˆ«åªæœ‰å°‘æ•°å‡ ä¸ªæ ·æœ¬å¯ç”¨çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚å°æ ·æœ¬é€‚åº”ï¼ˆFSAï¼‰ï¼Œæ•°æ®ä¸è¶³ä»¥é€‚åº”å¤§é‡å‚æ•°ï¼Œè¿™ä½¿å¾—ä¸Šè¿°æ–¹æ³•å˜å¾—ä¸åˆ‡å®é™…ã€‚è¿™åœ¨å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­å°¤å…¶å¦‚æ­¤ï¼Œè¿™æ¿€å‘äº†å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰å’Œå°æ ·æœ¬é€‚åº”ï¼ˆFSAï¼‰äº¤å‰é¢†åŸŸçš„æˆåŠŸç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æPEFTæŠ€æœ¯åœ¨ä»…å¯¹å°‘æ•°ç±»åˆ«æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶çš„å­¦ä¹ åŠ¨æ€ï¼Œè¿™äº›ç±»åˆ«è¢«ç§°ä¸ºâ€œåŸºæœ¬â€ç±»åˆ«ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§åŠ¨æ€è‡ªç„¶åœ°åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆiï¼‰ä»»åŠ¡çº§ç‰¹å¾æå–å’Œï¼ˆiiï¼‰é’ˆå¯¹å¯ç”¨æ¦‚å¿µçš„ä¸“é—¨åŒ–ã€‚ä¸ºäº†é€‚åº”è¿™ç§åŠ¨æ€ï¼Œæˆ‘ä»¬éšåæ”¾å¼ƒäº†åŸºäºæç¤ºæˆ–é€‚é…å™¨çš„æ–¹æ³•ï¼Œå¹¶ä»¥ä¸åŒçš„æ–¹å¼å¤„ç†FSAã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å›ºå®šçš„è®¡ç®—é¢„ç®—ä¸‹ï¼Œæˆ‘ä»¬å°†é¢„ç®—åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šï¼ˆiï¼‰é€šè¿‡PEFTå­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ç‰¹å¾æå–å™¨ï¼Œï¼ˆiiï¼‰åœ¨é¡¶éƒ¨è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ã€‚æˆ‘ä»¬ç§°è¿™ç§æ–¹æ¡ˆä¸ºä¸¤é˜¶æ®µå°æ ·æœ¬é€‚åº”ï¼ˆ2SFSï¼‰ã€‚ä¸åŒäºç°æœ‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆèƒ½å¤Ÿåœ¨ç±»åˆ«çº§åˆ«å®ç°æ–°çš„é€‰æ‹©æ€§æ¨æ–­ï¼Œå³æµ‹è¯•æ—¶åªæœ‰æ–°ç±»åˆ«è¢«é€‚åº”çš„æ–‡æœ¬ç¼–ç å™¨åµŒå…¥è¡¨ç¤ºï¼Œè€ŒåŸºæœ¬ç±»åˆ«çš„åµŒå…¥è¡¨ç¤ºå­˜åœ¨äºåˆ†ç±»å™¨ä¸­ã€‚åœ¨ä¸¤ä¸ªè®¾ç½®ã€ä¸‰ä¸ªéª¨å¹²ç½‘ç»œå’Œåä¸€ä¸ªæ•°æ®é›†ä¸Šçš„å›ºå®šè¶…å‚æ•°ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æŠ€æœ¯å¯ä»¥å®ç°ä¸ä¹‹åŒ¹é…æˆ–è¶…è¶Šçš„ç²¾åº¦è¡¨ç°ã€‚ç„¶è€Œä¼ ç»Ÿçš„æŠ€æœ¯ä¼šéšç€ç¯å¢ƒçš„æ”¹å˜è€Œå‘ç”Ÿæ˜¾è‘—çš„é€€åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11609v1">PDF</a> Camera-ready version for CVPR 2025 (w&#x2F; SuppMat, 23 pages)</p>
<p><strong>Summary</strong><br>     åœ¨Few-Shotåœºæ™¯ä¸‹ï¼Œä¼ ç»Ÿçš„è®­ç»ƒåˆ†ç±»å™¨æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸å°‘æ ·æœ¬é€‚åº”ï¼ˆFSAï¼‰çš„ç»“åˆæ–¹æ³•ã€‚æœ¬æ–‡åˆ†æäº†åŸºäºPEFTæŠ€æœ¯åœ¨ä»…ä½¿ç”¨éƒ¨åˆ†ç±»åˆ«å°‘æ ·æœ¬æ•°æ®æ—¶çš„å­¦ä¹ åŠ¨æ€ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µå°‘æ ·æœ¬é€‚åº”ï¼ˆ2SFSï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œå…ˆå­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ç‰¹å¾æå–å™¨ï¼Œå†åœ¨å…¶ä¸Šè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œæ­¤æ–¹æ³•èƒ½å®ç°å¯¹æ–°ç±»åˆ«çš„é€‰æ‹©æ€§æ¨æ–­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ2SFSåœ¨ä¸åŒè®¾ç½®ã€ä¸åŒä¸»å¹²ç½‘ç»œåŠå¤šä¸ªæ•°æ®é›†ä¸Šå‡è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨Few-Shotåœºæ™¯ä¸‹ï¼Œä¼ ç»Ÿè®­ç»ƒåˆ†ç±»å™¨æ–¹æ³•å› æ•°æ®ä¸è¶³è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸å°‘æ ·æœ¬é€‚åº”ï¼ˆFSAï¼‰çš„ç»“åˆæ˜¯è§£å†³è¯¥é—®é¢˜çš„ä¸€ä¸ªç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>æœ¬æ–‡åˆ†æäº†ä½¿ç”¨PEFTåœ¨ä»…éƒ¨åˆ†ç±»åˆ«å°‘æ ·æœ¬æ•°æ®æ—¶çš„å­¦ä¹ åŠ¨æ€ï¼Œå¹¶å‘ç°å­˜åœ¨ä¸¤ä¸ªæ˜æ˜¾é˜¶æ®µï¼šä»»åŠ¡çº§ç‰¹å¾æå–å’Œé’ˆå¯¹å¯ç”¨æ¦‚å¿µçš„ä¸“é—¨åŒ–ã€‚</li>
<li>åŸºäºä¸Šè¿°åˆ†æï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µå°‘æ ·æœ¬é€‚åº”ï¼ˆ2SFSï¼‰æ–¹æ³•ï¼Œèƒ½åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œåˆ†é˜¶æ®µè¿›è¡Œç‰¹å¾æå–å’Œçº¿æ€§åˆ†ç±»å™¨çš„è®­ç»ƒã€‚</li>
<li>2SFSæ–¹æ³•å®ç°äº†å¯¹æ–°ç±»åˆ«çš„é€‰æ‹©æ€§æ¨æ–­ï¼Œå³æµ‹è¯•æ—¶ä»…å¯¹æ–°ç±»åˆ«è¿›è¡ŒåµŒå…¥ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œ2SFSåœ¨ä¸åŒè®¾ç½®ã€ä¸åŒä¸»å¹²ç½‘ç»œåŠå¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>2SFSæ–¹æ¡ˆå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤„ç†å°‘é‡æ ·æœ¬æ•°æ®çš„åœºæ™¯ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0ac6566659e58f29a820ceef60de020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb6a1effb8309d33590be96e3d07323c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95224a4912c49b75136212ea98e55a7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8eb1baac8b3c622fb9881c04641bb240.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Watch-and-Learn-Leveraging-Expert-Knowledge-and-Language-for-Surgical-Video-Understanding"><a href="#Watch-and-Learn-Leveraging-Expert-Knowledge-and-Language-for-Surgical-Video-Understanding" class="headerlink" title="Watch and Learn: Leveraging Expert Knowledge and Language for Surgical   Video Understanding"></a>Watch and Learn: Leveraging Expert Knowledge and Language for Surgical   Video Understanding</h2><p><strong>Authors:David Gastager, Ghazal Ghazaei, Constantin Patsch</strong></p>
<p>Automated surgical workflow analysis is crucial for education, research, and clinical decision-making, but the lack of annotated datasets hinders the development of accurate and comprehensive workflow analysis solutions. We introduce a novel approach for addressing the sparsity and heterogeneity of annotated training data inspired by the human learning procedure of watching experts and understanding their explanations. Our method leverages a video-language model trained on alignment, denoising, and generative tasks to learn short-term spatio-temporal and multimodal representations. A task-specific temporal model is then used to capture relationships across entire videos. To achieve comprehensive video-language understanding in the surgical domain, we introduce a data collection and filtering strategy to construct a large-scale pretraining dataset from educational YouTube videos. We then utilize parameter-efficient fine-tuning by projecting downstream task annotations from publicly available surgical datasets into the language domain. Extensive experiments in two surgical domains demonstrate the effectiveness of our approach, with performance improvements of up to 7% in phase segmentation tasks, 8% in zero-shot phase segmentation, and comparable capabilities to fully-supervised models in few-shot settings. Harnessing our modelâ€™s capabilities for long-range temporal localization and text generation, we present the first comprehensive solution for dense video captioning (DVC) of surgical videos, addressing this task despite the absence of existing DVC datasets in the surgical domain. We introduce a novel approach to surgical workflow understanding that leverages video-language pretraining, large-scale video pretraining, and optimized fine-tuning. Our method improves performance over state-of-the-art techniques and enables new downstream tasks for surgical video understanding. </p>
<blockquote>
<p>è‡ªåŠ¨æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æåœ¨åŒ»ç–—æ•™è‚²ã€ç ”ç©¶å’Œä¸´åºŠå†³ç­–åˆ¶å®šä¸­è‡³å…³é‡è¦ï¼Œä½†ç¼ºä¹æ ‡æ³¨æ•°æ®é›†é˜»ç¢äº†å‡†ç¡®å…¨é¢çš„å·¥ä½œæµç¨‹åˆ†æè§£å†³æ–¹æ¡ˆçš„å¼€å‘ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³æ ‡æ³¨è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºå’Œå¼‚è´¨æ€§ï¼Œè¯¥æ–¹æ³•å—åˆ°äººç±»é€šè¿‡è§‚å¯Ÿä¸“å®¶å¹¶ç†è§£å…¶è§£é‡Šçš„å­¦ä¹ è¿‡ç¨‹çš„å¯å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åœ¨æ’åˆ—ã€å»å™ªå’Œç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒçš„è§†å¬è¯­è¨€æ¨¡å‹æ¥å­¦ä¹ çŸ­æœŸæ—¶ç©ºå’Œå¤šæ¨¡æ€è¡¨ç¤ºã€‚ç„¶åï¼Œä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ—¶åºæ¨¡å‹æ¥æ•æ‰æ•´ä¸ªè§†é¢‘çš„å…³ç³»ã€‚ä¸ºäº†å®ç°æ‰‹æœ¯é¢†åŸŸçš„å…¨é¢è§†é¢‘è¯­è¨€ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ•°æ®æ”¶é›†å’Œè¿‡æ»¤ç­–ç•¥ï¼Œä»æ•™è‚²YouTubeè§†é¢‘ä¸­æ„å»ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¬å¼€å¯ç”¨çš„æ‰‹æœ¯æ•°æ®é›†çš„ä»»åŠ¡æ ‡æ³¨æŠ•å½±åˆ°è¯­è¨€é¢†åŸŸï¼Œå®ç°äº†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚åœ¨ä¸¤ä¸ªæ‰‹æœ¯é¢†åŸŸçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œåœ¨é˜¶æ®µåˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½æé«˜äº†é«˜è¾¾7%ï¼Œåœ¨é›¶æ ·æœ¬é˜¶æ®µåˆ†å‰²ä¸­çš„æ€§èƒ½æé«˜äº†8%ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬ç¯å¢ƒä¸­ä¸å®Œå…¨ç›‘ç£çš„æ¨¡å‹èƒ½åŠ›ç›¸å½“ã€‚åˆ©ç”¨æˆ‘ä»¬æ¨¡å‹çš„é•¿ç¨‹æ—¶åºå®šä½å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºæ‰‹æœ¯è§†é¢‘çš„å¯†é›†è§†é¢‘å­—å¹•ï¼ˆDVCï¼‰æä¾›äº†ç¬¬ä¸€ä¸ªå…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼Œå°½ç®¡æ‰‹æœ¯é¢†åŸŸç¼ºä¹ç°æœ‰çš„DVCæ•°æ®é›†ï¼Œæˆ‘ä»¬ä»è§£å†³äº†æ­¤ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ©ç”¨è§†å¬è¯­è¨€é¢„è®­ç»ƒã€å¤§è§„æ¨¡è§†é¢‘é¢„è®­ç»ƒå’Œä¼˜åŒ–å¾®è°ƒçš„æ–°æ–¹æ³•æ¥è¿›è¡Œæ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†æœ€å…ˆè¿›æŠ€æœ¯çš„æ€§èƒ½ï¼Œå¹¶ä¸ºæ‰‹æœ¯è§†é¢‘ç†è§£å¯ç”¨äº†æ–°çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11392v1">PDF</a> 14 pages main manuscript with 3 figures; 6 pages supplementary   material with 3 figures. To be presented at International Conference on   Information Processing in Computer-Assisted Interventions (IPCAI 2025). To be   published in International Journal of Computer Assisted Radiology and Surgery   (IJCARS)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†ææ–¹æ³•ï¼Œè§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äººç±»é€šè¿‡è§‚å¯Ÿä¸“å®¶å¹¶ç†è§£å…¶è§£é‡Šçš„å­¦ä¹ è¿‡ç¨‹ï¼Œåˆ©ç”¨è§†é¢‘è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ çŸ­æœŸæ—¶ç©ºå’Œå¤šåª’ä½“è¡¨ç¤ºã€‚é€šè¿‡ç‰¹å®šä»»åŠ¡çš„æ—¶é—´æ¨¡å‹æ•æ‰æ•´ä¸ªè§†é¢‘çš„å…³è”ã€‚ä¸ºåœ¨æ‰‹æœ¯é¢†åŸŸå®ç°å…¨é¢çš„è§†é¢‘è¯­è¨€ç†è§£ï¼Œä»æ•™è‚²YouTubeè§†é¢‘ä¸­æ„å»ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ã€‚åˆ©ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå°†å…¬å¼€æ‰‹æœ¯æ•°æ®é›†çš„ä»»åŠ¡æ ‡æ³¨æŠ•å½±åˆ°è¯­è¨€é¢†åŸŸã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰‹æœ¯è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæé«˜äº†é˜¶æ®µåˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ï¼Œå…·å¤‡é›¶æ ·æœ¬é˜¶æ®µåˆ†å‰²èƒ½åŠ›ï¼Œå¹¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰ä¸å…¨ç›‘ç£æ¨¡å‹ç›¸å½“çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ‰‹æœ¯è§†é¢‘å¯†é›†è§†é¢‘å­—å¹•ï¼ˆDVCï¼‰ä»»åŠ¡çš„é¦–ä¸ªå…¨é¢è§£å†³æ–¹æ¡ˆã€‚æ€»ä¹‹ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç»“åˆè§†é¢‘è¯­è¨€é¢„è®­ç»ƒã€å¤§è§„æ¨¡è§†é¢‘é¢„è®­ç»ƒå’Œä¼˜åŒ–å¾®è°ƒçš„æ–°æ–¹æ³•ï¼Œæé«˜æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†ææ€§èƒ½ï¼Œä¸ºæ‰‹æœ¯è§†é¢‘ç†è§£å¼€å¯æ–°çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°å‹æ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†ææ–¹æ³•ï¼Œè§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§çš„é—®é¢˜ã€‚</li>
<li>å€Ÿé‰´äººç±»å­¦ä¹ æœºåˆ¶ï¼Œåˆ©ç”¨è§†é¢‘è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡ç‰¹å®šä»»åŠ¡çš„æ—¶é—´æ¨¡å‹æ•æ‰è§†é¢‘ä¸­çš„é•¿æœŸå’ŒçŸ­æœŸå…³ç³»ã€‚</li>
<li>ä»æ•™è‚²YouTubeè§†é¢‘ä¸­æ„å»ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå®ç°å…¨é¢çš„è§†é¢‘è¯­è¨€ç†è§£ã€‚</li>
<li>é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>åœ¨æ‰‹æœ¯è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·å¤‡é˜¶æ®µåˆ†å‰²ã€é›¶æ ·æœ¬å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bae6d8b10389321eb371236d5e57c533.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dbe8bf3e031183d7b9eaec63ebb3ac5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e54ead2c466b09b370449c728181b6da.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Optimizing-Large-Language-Models-for-Detecting-Symptoms-of-Comorbid-Depression-or-Anxiety-in-Chronic-Diseases-Insights-from-Patient-Messages"><a href="#Optimizing-Large-Language-Models-for-Detecting-Symptoms-of-Comorbid-Depression-or-Anxiety-in-Chronic-Diseases-Insights-from-Patient-Messages" class="headerlink" title="Optimizing Large Language Models for Detecting Symptoms of Comorbid   Depression or Anxiety in Chronic Diseases: Insights from Patient Messages"></a>Optimizing Large Language Models for Detecting Symptoms of Comorbid   Depression or Anxiety in Chronic Diseases: Insights from Patient Messages</h2><p><strong>Authors:Jiyeong Kim, Stephen P. Ma, Michael L. Chen, Isaac R. Galatzer-Levy, John Torous, Peter J. van Roessel, Christopher Sharp, Michael A. Pfeffer, Carolyn I. Rodriguez, Eleni Linos, Jonathan H. Chen</strong></p>
<p>Patients with diabetes are at increased risk of comorbid depression or anxiety, complicating their management. This study evaluated the performance of large language models (LLMs) in detecting these symptoms from secure patient messages. We applied multiple approaches, including engineered prompts, systemic persona, temperature adjustments, and zero-shot and few-shot learning, to identify the best-performing model and enhance performance. Three out of five LLMs demonstrated excellent performance (over 90% of F-1 and accuracy), with Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot approach. While LLMs showed promise in binary classification and handling complex metrics like Patient Health Questionnaire-4, inconsistencies in challenging cases warrant further real-life assessment. The findings highlight the potential of LLMs to assist in timely screening and referrals, providing valuable empirical knowledge for real-world triage systems that could improve mental health care for patients with chronic diseases. </p>
<blockquote>
<p>ç³–å°¿ç—…æ‚£è€…å¹¶å‘æŠ‘éƒç—‡æˆ–ç„¦è™‘ç—‡çš„é£é™©å¢åŠ ï¼Œè¿™ä½¿å…¶ç®¡ç†å˜å¾—æ›´åŠ å¤æ‚ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šè¿‡å®‰å…¨çš„æ‚£è€…ä¿¡æ¯ä¸­æ£€æµ‹è¿™äº›ç—‡çŠ¶æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬äººå·¥æç¤ºã€ç³»ç»Ÿæ€§äººæ ¼ç‰¹å¾ã€æ¸©åº¦è°ƒæ•´å’Œé›¶æ ·æœ¬ä»¥åŠå°æ ·æœ¬å­¦ä¹ ï¼Œä»¥è¯†åˆ«æ€§èƒ½æœ€ä½³çš„æ¨¡å‹å¹¶æé«˜å…¶æ€§èƒ½ã€‚åœ¨äº”æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œæœ‰ä¸‰æ¬¾è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ˆF-1å€¼å’Œå‡†ç¡®ç‡å‡è¶…è¿‡90%ï¼‰ï¼Œå…¶ä¸­Llama 3.1 405Båœ¨é›¶æ ·æœ¬æ–¹æ³•ä¸­è¾¾åˆ°ä¸¤é¡¹æŒ‡æ ‡çš„å‡†ç¡®ç‡å‡è¾¾93%ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äºŒå…ƒåˆ†ç±»å’Œåº”å¯¹æ‚£è€…å¥åº·é—®å·-4ç­‰å¤æ‚æŒ‡æ ‡æ–¹é¢æ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ï¼Œä½†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç—…ä¾‹ä¸­å­˜åœ¨çš„ä¸ä¸€è‡´æ€§éœ€è¦è¿›ä¸€æ­¥è¿›è¡Œå®é™…è¯„ä¼°ã€‚ç ”ç©¶ç»“æœçªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠæ—¶ç­›æŸ¥å’Œè½¬è¯Šæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºçœŸå®ä¸–ç•Œçš„åˆ†çº§ç³»ç»Ÿæä¾›äº†å®è´µçš„å®è¯çŸ¥è¯†ï¼Œå¯èƒ½æœ‰åŠ©äºæ”¹å–„æ…¢æ€§æ‚£è€…çš„ç²¾ç¥å«ç”Ÿä¿å¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11384v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ç³–å°¿ç—…æ‚£è€…å¹¶å‘ç—‡æŠ‘éƒæˆ–ç„¦è™‘ç—‡çŠ¶æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚é€šè¿‡å·¥ç¨‹æç¤ºã€ç³»ç»Ÿæ€§äººæ ¼ã€æ¸©åº¦è°ƒæ•´ä»¥åŠé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ç­‰æ–¹æ³•ï¼Œä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ˆF-1å’Œå‡†ç¡®ç‡è¶…è¿‡90%ï¼‰ï¼Œå…¶ä¸­Llama 3.1 405Båœ¨é›¶æ ·æœ¬æ–¹æ³•ä¸‹è¾¾åˆ°93%ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äºŒå…ƒåˆ†ç±»å’Œå¤„ç†å¤æ‚æŒ‡æ ‡å¦‚æ‚£è€…å¥åº·é—®å·-4æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤æ‚æƒ…å†µä¸‹çš„ä¸ä¸€è‡´æ€§ä»éœ€è¦è¿›ä¸€æ­¥åœ¨å®é™…ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ã€‚è¯¥å‘ç°çªæ˜¾å‡ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠæ—¶ç­›æŸ¥å’Œè½¬è¯Šæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºçœŸå®ä¸–ç•Œä¸­çš„åˆ†çº§ç³»ç»Ÿæä¾›äº†å®è´µçš„å®è¯çŸ¥è¯†ï¼Œæœ‰æœ›æ”¹å–„æ…¢æ€§æ‚£è€…çš„ç²¾ç¥å¥åº·æŠ¤ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³–å°¿ç—…æ‚£è€…é¢ä¸´å¹¶å‘æŠ‘éƒæˆ–ç„¦è™‘çš„é£é™©å¢åŠ ï¼Œç®¡ç†å˜å¾—å¤æ‚ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹æ‚£è€…å¹¶å‘ç—‡æŠ‘éƒæˆ–ç„¦è™‘ç—‡çŠ¶æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>å¤šç§æ–¹æ³•è¢«åº”ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å·¥ç¨‹æç¤ºã€ç³»ç»Ÿæ€§äººæ ¼ã€æ¸©åº¦è°ƒæ•´ã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…¶ä¸­Llama 3.1 405Båœ¨é›¶æ ·æœ¬æ–¹æ³•ä¸‹è¾¾åˆ°93%çš„F-1å’Œå‡†ç¡®ç‡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äºŒå…ƒåˆ†ç±»å’Œå¤„ç†å¤æ‚æŒ‡æ ‡å¦‚æ‚£è€…å¥åº·é—®å·-4æ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>åœ¨å¤æ‚æƒ…å¢ƒä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥åœ¨å®é™…ç¯å¢ƒä¸­è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39d465fd9ba8306dc9410c8112ad2de9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Take-Things-Out-of-Context-Attention-Intervention-for-Enhancing-Chain-of-Thought-Reasoning-in-Large-Language-Models"><a href="#Donâ€™t-Take-Things-Out-of-Context-Attention-Intervention-for-Enhancing-Chain-of-Thought-Reasoning-in-Large-Language-Models" class="headerlink" title="Donâ€™t Take Things Out of Context: Attention Intervention for Enhancing   Chain-of-Thought Reasoning in Large Language Models"></a>Donâ€™t Take Things Out of Context: Attention Intervention for Enhancing   Chain-of-Thought Reasoning in Large Language Models</h2><p><strong>Authors:Shaotian Yan, Chen Shen, Wenxiao Wang, Liang Xie, Junjie Liu, Jieping Ye</strong></p>
<p>Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning capabilities of large language models (LLMs), functioning as a whole to guide these models in generating reasoning steps toward final answers. However, we observe that isolated segments, words, or tokens within CoT demonstrations can unexpectedly disrupt the generation process of LLMs. The model may overly concentrate on certain local information present in the demonstration, introducing irrelevant noise into the reasoning process and potentially leading to incorrect answers. In this paper, we investigate the underlying mechanism of CoT through dynamically tracing and manipulating the inner workings of LLMs at each output step, which demonstrates that tokens exhibiting specific attention characteristics are more likely to induce the model to take things out of context; these tokens directly attend to the hidden states tied with prediction, without substantial integration of non-local information. Building upon these insights, we propose a Few-shot Attention Intervention method (FAI) that dynamically analyzes the attention patterns of demonstrations to accurately identify these tokens and subsequently make targeted adjustments to the attention weights to effectively suppress their distracting effect on LLMs. Comprehensive experiments across multiple benchmarks demonstrate consistent improvements over baseline methods, with a remarkable 5.91% improvement on the AQuA dataset, further highlighting the effectiveness of FAI. </p>
<blockquote>
<p>Few-shot Chain-of-Thought (CoT) æŠ€æœ¯æå¤§åœ°å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½œä¸ºä¸€ä¸ªæ•´ä½“å¼•å¯¼è¿™äº›æ¨¡å‹ç”Ÿæˆæ¨ç†æ­¥éª¤ä»¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°CoTæ¼”ç¤ºä¸­çš„å­¤ç«‹ç‰‡æ®µã€å•è¯æˆ–ä»¤ç‰Œå¯èƒ½ä¼šæ„å¤–åœ°ç ´åLLMçš„ç”Ÿæˆè¿‡ç¨‹ã€‚æ¨¡å‹å¯èƒ½ä¼šè¿‡åº¦å…³æ³¨æ¼”ç¤ºä¸­å‡ºç°çš„æŸäº›å±€éƒ¨ä¿¡æ¯ï¼Œå°†æ— å…³å™ªéŸ³å¼•å…¥æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å¯èƒ½å¯¼è‡´å¾—å‡ºé”™è¯¯ç­”æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åŠ¨æ€è¿½è¸ªå’Œæ“æ§LLMçš„æ¯ä¸ªè¾“å‡ºæ­¥éª¤çš„å†…åœ¨å·¥ä½œåŸç†æ¥æ¢ç©¶CoTçš„å†…åœ¨æœºåˆ¶ã€‚æ¼”ç¤ºè¡¨æ˜ï¼Œè¡¨ç°å‡ºç‰¹å®šæ³¨æ„åŠ›ç‰¹å¾çš„ä»¤ç‰Œæ›´æœ‰å¯èƒ½å¯¼è‡´æ¨¡å‹è„±ç¦»ä¸Šä¸‹æ–‡ï¼›è¿™äº›ä»¤ç‰Œç›´æ¥å…³æ³¨ä¸é¢„æµ‹ç›¸å…³çš„éšè—çŠ¶æ€ï¼Œè€Œéå±€éƒ¨ä¿¡æ¯çš„å®è´¨æ€§æ•´åˆã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Few-shot Attention Interventionï¼ˆFAIï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŠ¨æ€åˆ†ææ¼”ç¤ºçš„æ³¨æ„åŠ›æ¨¡å¼æ¥å‡†ç¡®è¯†åˆ«è¿™äº›ä»¤ç‰Œï¼Œéšåå¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ï¼Œä»è€Œæœ‰æ•ˆåœ°æŠ‘åˆ¶å®ƒä»¬å¯¹LLMçš„å¹²æ‰°æ•ˆæœã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¸€è‡´çš„æ”¹è¿›æ•ˆæœï¼Œåœ¨AQuAæ•°æ®é›†ä¸Šçš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼Œæé«˜äº†5.91%ï¼Œè¿›ä¸€æ­¥çªå‡ºäº†FAIçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11154v1">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼ŒFew-shot Chain-of-Thoughtï¼ˆCoTï¼‰æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å‘ç°CoTæ¼”ç¤ºä¸­çš„å­¤ç«‹ç‰‡æ®µã€å•è¯æˆ–ä»¤ç‰Œå¯èƒ½ä¼šæ„å¤–å¹²æ‰°LLMçš„ç”Ÿæˆè¿‡ç¨‹ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Few-shot Attention Interventionï¼ˆFAIï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€åˆ†ææ¼”ç¤ºçš„å…³æ³¨æ¨¡å¼æ¥è¯†åˆ«é—®é¢˜ä»¤ç‰Œï¼Œå¹¶å¯¹å…³æ³¨æƒé‡è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´ï¼Œä»è€Œæœ‰æ•ˆæŠ‘åˆ¶å…¶å¯¹LLMçš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAIæ–¹æ³•åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¯¹åŸºçº¿æ–¹æ³•çš„ä¸€è‡´æ”¹è¿›ï¼Œå°¤å…¶åœ¨AQuAæ•°æ®é›†ä¸Šçš„æ”¹è¿›æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot Chain-of-Thought (CoT) å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CoTæ¼”ç¤ºä¸­çš„å­¤ç«‹ç‰‡æ®µã€å•è¯æˆ–ä»¤ç‰Œå¯èƒ½ä¼šå¹²æ‰°LLMçš„ç”Ÿæˆã€‚</li>
<li>ä»¤ç‰Œå¯èƒ½ä¼šè¿‡åº¦å…³æ³¨æ¼”ç¤ºä¸­çš„å±€éƒ¨ä¿¡æ¯ï¼Œå¼•å…¥æ— å…³å™ªå£°å¹¶å¯¼è‡´é”™è¯¯ç­”æ¡ˆã€‚</li>
<li>é€šè¿‡åŠ¨æ€è¿½è¸ªå’Œæ“æ§LLMçš„å†…éƒ¨å·¥ä½œï¼Œå‘ç°ç‰¹å®šå…³æ³¨ç‰¹æ€§çš„ä»¤ç‰Œæ›´å®¹æ˜“å¯¼è‡´æ¨¡å‹è„±ç¦»ä¸Šä¸‹æ–‡ã€‚</li>
<li>Few-shot Attention Intervention (FAI) æ–¹æ³•é€šè¿‡åˆ†ææ¼”ç¤ºçš„å…³æ³¨æ¨¡å¼æ¥è¯†åˆ«é—®é¢˜ä»¤ç‰Œã€‚</li>
<li>FAIæ–¹æ³•èƒ½æœ‰æ•ˆè°ƒæ•´å…³æ³¨æƒé‡ï¼ŒæŠ‘åˆ¶é—®é¢˜ä»¤ç‰Œçš„å¹²æ‰°æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-adefe08d42e623160de4d13166029c01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e62ec43a9eac91517c3083914508ea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e96ae3c85a0c225aa5deced986747fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d1c6349bb57f09d28b5715ec204639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f20f662c132e000c06de75afa601f6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edfa17a998e40b960dbb65a191a012d6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Riemannian-Geometric-based-Meta-Learning"><a href="#Riemannian-Geometric-based-Meta-Learning" class="headerlink" title="Riemannian Geometric-based Meta Learning"></a>Riemannian Geometric-based Meta Learning</h2><p><strong>Authors:JuneYoung Park, YuMi Lee, Tae-Joon Kim, Jang-Hwan Choi</strong></p>
<p>Meta-learning, or â€œlearning to learn,â€ aims to enable models to quickly adapt to new tasks with minimal data. While traditional methods like Model-Agnostic Meta-Learning (MAML) optimize parameters in Euclidean space, they often struggle to capture complex learning dynamics, particularly in few-shot learning scenarios. To address this limitation, we propose Stiefel-MAML, which integrates Riemannian geometry by optimizing within the Stiefel manifold, a space that naturally enforces orthogonality constraints. By leveraging the geometric structure of the Stiefel manifold, we improve parameter expressiveness and enable more efficient optimization through Riemannian gradient calculations and retraction operations. We also introduce a novel kernel-based loss function defined on the Stiefel manifold, further enhancing the modelâ€™s ability to explore the parameter space. Experimental results on benchmark datasetsâ€“including Omniglot, Mini-ImageNet, FC-100, and CUBâ€“demonstrate that Stiefel-MAML consistently outperforms traditional MAML, achieving superior performance across various few-shot learning tasks. Our findings highlight the potential of Riemannian geometry to enhance meta-learning, paving the way for future research on optimizing over different geometric structures. </p>
<blockquote>
<p>å…ƒå­¦ä¹ ï¼Œæˆ–ç§°â€œå­¦ä¹ å¦‚ä½•å­¦ä¹ â€ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨æœ€å°‘çš„æ•°æ®å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚è™½ç„¶åƒæ¨¡å‹æ— å…³å…ƒå­¦ä¹ ï¼ˆMAMLï¼‰è¿™æ ·çš„ä¼ ç»Ÿæ–¹æ³•ä¼šåœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´å†…ä¼˜åŒ–å‚æ•°ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥æ•æ‰å¤æ‚çš„å­¦ä¹ åŠ¨æ€ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Stiefel-MAMLï¼Œå®ƒé€šè¿‡Stiefelæµå½¢å†…çš„ä¼˜åŒ–æ¥æ•´åˆé»æ›¼å‡ ä½•ï¼ŒStiefelæµå½¢æ˜¯ä¸€ä¸ªè‡ªç„¶å¼ºåˆ¶æ‰§è¡Œæ­£äº¤çº¦æŸçš„ç©ºé—´ã€‚é€šè¿‡åˆ©ç”¨Stiefelæµå½¢çš„å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬æé«˜äº†å‚æ•°çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡é»æ›¼æ¢¯åº¦è®¡ç®—å’Œå›ç¼©æ“ä½œå®ç°äº†æ›´æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åœ¨Stiefelæµå½¢ä¸Šå®šä¹‰çš„æ–°å‹æ ¸å‡½æ•°æŸå¤±å‡½æ•°ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹æ¢ç´¢å‚æ•°ç©ºé—´çš„èƒ½åŠ›ã€‚åœ¨Omniglotã€Mini-ImageNetã€FC-100å’ŒCUBç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒStiefel-MAMLå§‹ç»ˆä¼˜äºä¼ ç»ŸMAMLï¼Œåœ¨å„ç§å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†é»æ›¼å‡ ä½•åœ¨å¢å¼ºå…ƒå­¦ä¹ æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥åœ¨ä¸åŒå‡ ä½•ç»“æ„ä¸Šè¿›è¡Œä¼˜åŒ–çš„ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10993v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¨¡å‹çš„å…ƒå­¦ä¹ åœ¨é¢ä¸´å¤æ‚å­¦ä¹ åŠ¨æ€æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å¸¸å¸¸å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Stiefel-MAMLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥é»æ›¼å‡ ä½•ä¼˜åŒ–Stiefelæµå½¢å†…çš„å‚æ•°ï¼Œæé«˜äº†å‚æ•°çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡é»æ›¼æ¢¯åº¦è®¡ç®—å’Œå›ç¼©æ“ä½œå®ç°äº†æ›´æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨Stiefelæµå½¢ä¸Šå®šä¹‰äº†ä¸€ç§æ–°å‹æ ¸æŸå¤±å‡½æ•°ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹æ¢ç´¢å‚æ•°ç©ºé—´çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStiefel-MAMLåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºä¼ ç»ŸMAMLï¼Œåœ¨å„ç§å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚è¿™ä¸ºé»æ›¼å‡ ä½•åœ¨å…ƒå­¦ä¹ ä¸­çš„åº”ç”¨æä¾›äº†æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥åœ¨ä¸åŒå‡ ä½•ç»“æ„ä¸Šçš„ä¼˜åŒ–ç ”ç©¶å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å…ƒå­¦ä¹ èƒ½å¤Ÿä½¿å¾—æ¨¡å‹å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚MAMLåœ¨ä¼˜åŒ–å‚æ•°æ—¶ä¸»è¦åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´è¿›è¡Œï¼Œéš¾ä»¥æ•æ‰å¤æ‚çš„åŠ¨æ€å­¦ä¹ ã€‚</li>
<li>Stiefel-MAMLé€šè¿‡å¼•å…¥é»æ›¼å‡ ä½•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¼˜åŒ–Stiefelæµå½¢å†…çš„å‚æ•°è¡¨è¾¾ï¼Œä½¿å¾—å‚æ•°æ›´æœ‰è¡¨ç°åŠ›ã€‚</li>
<li>åˆ©ç”¨é»æ›¼å‡ ä½•çš„ç»“æ„ç‰¹æ€§ï¼Œé€šè¿‡é»æ›¼æ¢¯åº¦å’Œå›ç¼©æ“ä½œå®ç°æ›´æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚</li>
<li>Stiefel-MAMLè¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ ¸æŸå¤±å‡½æ•°ï¼Œå®šä¹‰åœ¨Stiefelæµå½¢ä¸Šï¼Œå¢å¼ºäº†æ¨¡å‹æ¢ç´¢å‚æ•°ç©ºé—´çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒStiefel-MAMLåœ¨å„ç§å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºä¼ ç»ŸMAMLã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bfbe865f8b0a33586761fb53287698c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bff183b9f9cd2b3e73c1717cf827c44e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06a5b66d0d8e4968d3d2fa93035fd9bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebde5b34dc619d69f727547104c04a7f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RI3D-Few-Shot-Gaussian-Splatting-With-Repair-and-Inpainting-Diffusion-Priors"><a href="#RI3D-Few-Shot-Gaussian-Splatting-With-Repair-and-Inpainting-Diffusion-Priors" class="headerlink" title="RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion   Priors"></a>RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion   Priors</h2><p><strong>Authors:Avinash Paliwal, Xilong Zhou, Wei Ye, Jinhui Xiong, Rakesh Ranjan, Nima Khademi Kalantari</strong></p>
<p>In this paper, we propose RI3D, a novel 3DGS-based approach that harnesses the power of diffusion models to reconstruct high-quality novel views given a sparse set of input images. Our key contribution is separating the view synthesis process into two tasks of reconstructing visible regions and hallucinating missing regions, and introducing two personalized diffusion models, each tailored to one of these tasks. Specifically, one model (â€˜repairâ€™) takes a rendered image as input and predicts the corresponding high-quality image, which in turn is used as a pseudo ground truth image to constrain the optimization. The other model (â€˜inpaintingâ€™) primarily focuses on hallucinating details in unobserved areas. To integrate these models effectively, we introduce a two-stage optimization strategy: the first stage reconstructs visible areas using the repair model, and the second stage reconstructs missing regions with the inpainting model while ensuring coherence through further optimization. Moreover, we augment the optimization with a novel Gaussian initialization method that obtains per-image depth by combining 3D-consistent and smooth depth with highly detailed relative depth. We demonstrate that by separating the process into two tasks and addressing them with the repair and inpainting models, we produce results with detailed textures in both visible and missing regions that outperform state-of-the-art approaches on a diverse set of scenes with extremely sparse inputs. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RI3Dï¼Œè¿™æ˜¯ä¸€ç§åŸºäº3DGSçš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½ï¼Œä»ç¨€ç–çš„è¾“å…¥å›¾åƒé›†åˆé‡å»ºé«˜è´¨é‡çš„æ–°è§†è§’ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯å°†è§†å›¾åˆæˆè¿‡ç¨‹åˆ†ä¸ºé‡å»ºå¯è§åŒºåŸŸå’Œå¹»æƒ³ç¼ºå¤±åŒºåŸŸä¸¤ä¸ªä»»åŠ¡ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªé’ˆå¯¹è¿™äº›ä»»åŠ¡é‡èº«å®šåˆ¶çš„ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œä¸€ä¸ªæ¨¡å‹ï¼ˆç§°ä¸ºâ€œä¿®å¤â€ï¼‰ä»¥æ¸²æŸ“å›¾åƒä¸ºè¾“å…¥ï¼Œé¢„æµ‹ç›¸åº”çš„é«˜è´¨é‡å›¾åƒï¼Œåè€…åˆè¢«ç”¨ä½œä¼ªçœŸå®å›¾åƒæ¥çº¦æŸä¼˜åŒ–ã€‚å¦ä¸€ä¸ªæ¨¡å‹ï¼ˆç§°ä¸ºâ€œè¡¥å…¨â€ï¼‰ä¸»è¦å…³æ³¨æœªè§‚å¯ŸåŒºåŸŸçš„ç»†èŠ‚å¹»æƒ³ã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆè¿™äº›æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨ä¿®å¤æ¨¡å‹é‡å»ºå¯è§åŒºåŸŸï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨è¡¥å…¨æ¨¡å‹é‡å»ºç¼ºå¤±åŒºåŸŸï¼ŒåŒæ—¶é€šè¿‡è¿›ä¸€æ­¥ä¼˜åŒ–ç¡®ä¿è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°é¢–çš„é«˜æ–¯åˆå§‹åŒ–æ–¹æ³•æ¥å¢å¼ºä¼˜åŒ–ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç»“åˆ3Dä¸€è‡´æ€§å’Œå¹³æ»‘æ·±åº¦ä¸é«˜åº¦è¯¦ç»†çš„ç›¸å¯¹æ·±åº¦æ¥è·å¾—æ¯å¼ å›¾åƒçš„æ·±åº¦ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡å°†è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä»»åŠ¡å¹¶ç”¨ä¿®å¤å’Œè¡¥å…¨æ¨¡å‹æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å¯è§å’Œç¼ºå¤±åŒºåŸŸéƒ½äº§ç”Ÿäº†è¯¦ç»†çš„çº¹ç†ï¼Œåœ¨å…·æœ‰æç«¯ç¨€ç–è¾“å…¥çš„å¤šç§åœºæ™¯ä¸Šè¶…è¶Šäº†æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10860v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://people.engr.tamu.edu/nimak/Papers/RI3D">https://people.engr.tamu.edu/nimak/Papers/RI3D</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/avinashpaliwal/RI3D">https://github.com/avinashpaliwal/RI3D</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäº3DGSçš„RI3Dæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»ç¨€ç–è¾“å…¥å›¾åƒé›†ä¸­é‡å»ºé«˜è´¨é‡çš„æ–°è§†è§’ã€‚ä¸»è¦è´¡çŒ®åœ¨äºå°†è§†å›¾åˆæˆè¿‡ç¨‹åˆ†ä¸ºé‡å»ºå¯è§åŒºåŸŸå’Œæ¨æµ‹ç¼ºå¤±åŒºåŸŸä¸¤ä¸ªä»»åŠ¡ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªé’ˆå¯¹è¿™äº›ä»»åŠ¡çš„ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ã€‚ä¸€ä¸ºâ€œä¿®å¤â€æ¨¡å‹ï¼Œä»¥æ¸²æŸ“å›¾åƒä¸ºè¾“å…¥é¢„æµ‹é«˜è´¨é‡å›¾åƒï¼Œç”¨ä½œä¼ªçœŸå®å›¾åƒçº¦æŸä¼˜åŒ–ï¼›å¦ä¸€ä¸ºâ€œè¡¥å…¨â€æ¨¡å‹ï¼Œä¸“æ³¨äºæ¨æµ‹æœªè§‚æµ‹åŒºåŸŸçš„ç»†èŠ‚ã€‚é€šè¿‡ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥æ•´åˆè¿™äº›æ¨¡å‹ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨ä¿®å¤æ¨¡å‹é‡å»ºå¯è§åŒºåŸŸï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨è¡¥å…¨æ¨¡å‹é‡å»ºç¼ºå¤±åŒºåŸŸï¼Œå¹¶é€šè¿‡è¿›ä¸€æ­¥ä¼˜åŒ–ç¡®ä¿ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆ3Dä¸€è‡´æ€§å’Œå¹³æ»‘æ·±åº¦ä¸ç›¸å¯¹æ·±åº¦çš„é«˜ç»†èŠ‚ï¼Œå¯¹ä¼˜åŒ–è¿›è¡Œäº†å¢å¼ºã€‚è¯¥æ–¹æ³•å°†è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä»»åŠ¡å¹¶åˆ†åˆ«å¤„ç†ï¼Œç”Ÿæˆçš„ç»“æœåœ¨å¯è§å’Œç¼ºå¤±åŒºåŸŸéƒ½å…·æœ‰è¯¦ç»†çš„çº¹ç†ï¼Œä¸”åœ¨æç«¯ç¨€ç–è¾“å…¥çš„æƒ…å†µä¸‹ä»ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†RI3Dæ–¹æ³•ï¼ŒåŸºäº3DGSï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»ç¨€ç–è¾“å…¥å›¾åƒé‡å»ºæ–°è§†è§’ã€‚</li>
<li>å°†è§†å›¾åˆæˆè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä»»åŠ¡ï¼šé‡å»ºå¯è§åŒºåŸŸå’Œæ¨æµ‹ç¼ºå¤±åŒºåŸŸã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ï¼šä¿®å¤æ¨¡å‹ç”¨äºé‡å»ºå¯è§åŒºåŸŸï¼Œè¡¥å…¨æ¨¡å‹ç”¨äºæ¨æµ‹æœªè§‚æµ‹åŒºåŸŸçš„ç»†èŠ‚ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µé‡å»ºå¯è§åŒºåŸŸï¼Œç¬¬äºŒé˜¶æ®µé‡å»ºç¼ºå¤±åŒºåŸŸå¹¶ç¡®ä¿ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆ3Dä¸€è‡´æ€§ã€å¹³æ»‘æ·±åº¦ä¸ç›¸å¯¹æ·±åº¦çš„é«˜ç»†èŠ‚ï¼Œå¢å¼ºäº†ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>æ–¹æ³•åœ¨å¯è§å’Œç¼ºå¤±åŒºåŸŸéƒ½ç”Ÿæˆäº†å…·æœ‰è¯¦ç»†çº¹ç†çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ad02d6a754ab39521e5d66be7b0161c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e62dd0b80364630f1076d2a7a7134fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50da380aa2bdfd7676f8ed89a3bdbcfb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MoMa-A-Modular-Deep-Learning-Framework-for-Material-Property-Prediction"><a href="#MoMa-A-Modular-Deep-Learning-Framework-for-Material-Property-Prediction" class="headerlink" title="MoMa: A Modular Deep Learning Framework for Material Property Prediction"></a>MoMa: A Modular Deep Learning Framework for Material Property Prediction</h2><p><strong>Authors:Botian Wang, Yawen Ouyang, Yaohui Li, Yiqun Wang, Haorui Cui, Jianbing Zhang, Xiaonan Wang, Wei-Ying Ma, Hao Zhou</strong></p>
<p>Deep learning methods for material property prediction have been widely explored to advance materials discovery. However, the prevailing pre-train then fine-tune paradigm often fails to address the inherent diversity and disparity of material tasks. To overcome these challenges, we introduce MoMa, a Modular framework for Materials that first trains specialized modules across a wide range of tasks and then adaptively composes synergistic modules tailored to each downstream scenario. Evaluation across 17 datasets demonstrates the superiority of MoMa, with a substantial 14% average improvement over the strongest baseline. Few-shot and continual learning experiments further highlight MoMaâ€™s potential for real-world applications. Pioneering a new paradigm of modular material learning, MoMa will be open-sourced to foster broader community collaboration. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨ææ–™å±æ€§é¢„æµ‹æ–¹é¢çš„åº”ç”¨å·²å¾—åˆ°å¹¿æ³›æ¢ç´¢ï¼Œä»¥æ¨åŠ¨ææ–™å‘ç°çš„å‘å±•ã€‚ç„¶è€Œï¼Œæµè¡Œçš„é¢„è®­ç»ƒç„¶åå¾®è°ƒçš„æ¨¡å¼å¾€å¾€æ— æ³•è§£å†³ææ–™ä»»åŠ¡çš„å†…åœ¨å¤šæ ·æ€§å’Œå·®å¼‚ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MoMaï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºææ–™çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œå®ƒé¦–å…ˆåœ¨å„ç§ä»»åŠ¡ä¸Šè®­ç»ƒä¸“ä¸šåŒ–çš„æ¨¡å—ï¼Œç„¶åè‡ªé€‚åº”åœ°ç»„åˆé’ˆå¯¹æ¯ä¸ªä¸‹æ¸¸åœºæ™¯é‡èº«å®šåˆ¶çš„ååŒæ¨¡å—ã€‚åœ¨17ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†MoMaçš„ä¼˜è¶Šæ€§ï¼Œç›¸è¾ƒäºæœ€å¼ºçš„åŸºçº¿æ¨¡å‹ï¼Œå…¶å¹³å‡æå‡äº†14%ã€‚å°æ ·æœ¬å’ŒæŒç»­å­¦ä¹ çš„å®éªŒè¿›ä¸€æ­¥çªå‡ºäº†MoMaåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä½œä¸ºæ¨¡å—åŒ–ææ–™å­¦ä¹ çš„æ–°èŒƒå¼å¼€åˆ›è€…ï¼ŒMoMaå°†å¼€æºä»¥ä¿ƒè¿›æ›´å¹¿æ³›çš„ç¤¾åŒºåˆä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15483v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨ææ–™å±æ€§é¢„æµ‹æ–¹é¢çš„åº”ç”¨å·²å¾—åˆ°å¹¿æ³›æ¢ç´¢ï¼Œæ¨åŠ¨äº†ææ–™å‘ç°çš„å‘å±•ã€‚ç„¶è€Œï¼Œå½“å‰æµè¡Œçš„é¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å¼éš¾ä»¥åº”å¯¹ææ–™ä»»åŠ¡çš„å†…åœ¨å¤šæ ·æ€§å’Œå·®å¼‚æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MoMaæ¨¡å—åŒ–ææ–™å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–å…ˆè®­ç»ƒå¤šç§ä»»åŠ¡çš„ä¸“ç”¨æ¨¡å—ï¼Œç„¶åè‡ªé€‚åº”ç»„åˆååŒæ¨¡å—ä»¥é€‚åº”å„ç§ä¸‹æ¸¸åœºæ™¯ã€‚åœ¨17ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜MoMaä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹³å‡æ”¹è¿›å¹…åº¦è¾¾14%ã€‚æ­¤å¤–ï¼ŒMoMaåœ¨å°æ ·æœ¬å’ŒæŒç»­å­¦ä¹ å®éªŒä¸­çš„è¡¨ç°è¿›ä¸€æ­¥å‡¸æ˜¾äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚MoMaå¼€åˆ›äº†ä¸€ç§æ–°çš„æ¨¡å—åŒ–ææ–™å­¦ä¹ èŒƒå¼ï¼Œå¹¶å°†å¼€æºä»¥ä¿ƒè¿›æ›´å¹¿æ³›çš„ç¤¾åŒºåˆä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ææ–™å±æ€§é¢„æµ‹ä¸­çš„åº”ç”¨ä¿ƒè¿›äº†ææ–™å‘ç°çš„å‘å±•ã€‚</li>
<li>å½“å‰é¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å¼éš¾ä»¥åº”å¯¹ææ–™ä»»åŠ¡çš„å¤šæ ·æ€§å’Œå·®å¼‚æ€§ã€‚</li>
<li>MoMaæ¡†æ¶é€šè¿‡è®­ç»ƒä¸“ç”¨æ¨¡å—å¹¶è‡ªé€‚åº”ç»„åˆååŒæ¨¡å—æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>MoMaåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹³å‡æ”¹è¿›å¹…åº¦è¾¾14%ã€‚</li>
<li>MoMaåœ¨å°æ ·æœ¬å’ŒæŒç»­å­¦ä¹ å®éªŒä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>MoMaå¼€åˆ›äº†ä¸€ç§æ–°çš„æ¨¡å—åŒ–ææ–™å­¦ä¹ èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-526ce08eed2310b728d9970868bb3ac7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fc390130116b6925fa6a461008e3fe4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e773a0fbd0487f6e1ff3ee8efbbabdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1ab815da1904d1b42bbe91cc91854c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e6a71e1a88d8a1da2d46e96a4b21883.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay"><a href="#Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay" class="headerlink" title="Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay"></a>Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay</h2><p><strong>Authors:Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, Xiangyang Ji</strong></p>
<p>Foundation models have revolutionized general-purpose problem-solving, offering rapid task adaptation through pretraining, meta-training, and finetuning. Recent crucial advances in these paradigms reveal the importance of challenging task prioritized sampling to enhance adaptation robustness under distribution shifts. However, ranking task difficulties over iteration as a preliminary step typically requires exhaustive task evaluation, which is practically unaffordable in computation and data-annotation. This study provides a novel perspective to illuminate the possibility of leveraging the dual importance of adaptation robustness and learning efficiency, particularly in scenarios where task evaluation is risky or costly, such as iterative agent-environment interactions for robotic policy evaluation or computationally intensive inference steps for finetuning foundation models. Firstly, we introduce Model Predictive Task Sampling (MPTS), a framework that bridges the task space and adaptation risk landscape, providing a theoretical foundation for robust active task sampling. MPTS employs a generative model to characterize the episodic optimization process and predicts task-specific adaptation risk via posterior inference. The resulting risk learner amortizes the costly evaluation of task adaptation performance and provably approximates task difficulty rankings. MPTS seamlessly integrates into zero-shot, few-shot, and supervised finetuning settings. Empirically, we conduct extensive experiments in pattern recognition using foundation models and sequential decision-making. Our results demonstrate that MPTS significantly enhances adaptation robustness for tail or out-of-distribution (OOD) tasks and improves learning efficiency compared to state-of-the-art (SOTA) methods. The code is available at the project site <a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS">https://github.com/thu-rllab/MPTS</a>. </p>
<blockquote>
<p>æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•å·²ç»å½»åº•æ”¹å˜äº†é€šç”¨é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œé€šè¿‡é¢„è®­ç»ƒã€å…ƒè®­ç»ƒå’Œå¾®è°ƒå®ç°äº†å¿«é€Ÿä»»åŠ¡é€‚åº”ã€‚è¿™äº›æ¨¡å¼ä¸­çš„æœ€æ–°å…³é”®è¿›å±•æ­ç¤ºäº†ä¼˜å…ˆé‡‡æ ·æŒ‘æˆ˜æ€§ä»»åŠ¡åœ¨å¢å¼ºé€‚åº”ç¨³å¥æ€§æ–¹é¢çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒè½¬ç§»çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œä½œä¸ºåˆæ­¥æ­¥éª¤ï¼Œåœ¨è¿­ä»£è¿‡ç¨‹ä¸­æŒ‰ä»»åŠ¡éš¾åº¦æ’åºé€šå¸¸éœ€è¦å…¨é¢çš„ä»»åŠ¡è¯„ä¼°ï¼Œè¿™åœ¨è®¡ç®—å’Œæ•°æ®æ ‡æ³¨æ–¹é¢å®é™…ä¸Šæ˜¯æ— æ³•æ‰¿å—çš„ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œé˜æ˜äº†é€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡è¯„ä¼°å…·æœ‰é£é™©æˆ–æˆæœ¬é«˜æ˜‚çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚åœ¨æœºå™¨äººç­–ç•¥è¯„ä¼°ä¸­çš„ä»£ç†ç¯å¢ƒäº¤äº’è¿­ä»£æˆ–åœ¨å¾®è°ƒåŸºç¡€æ¨¡å‹ä¸­çš„è®¡ç®—å¯†é›†å‹æ¨ç†æ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿æ¥ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚çš„æ¡†æ¶ï¼Œä¸ºç¨³å¥çš„æ´»åŠ¨ä»»åŠ¡é‡‡æ ·æä¾›äº†ç†è®ºåŸºç¡€ã€‚MPTSä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ¥åˆ»ç”»é˜¶æ®µæ€§ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡åéªŒæ¨æ–­é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©ã€‚ç»“æœé£é™©å­¦ä¹ è€…å‡å°‘äº†æ˜‚è´µçš„ä»»åŠ¡é€‚åº”æ€§èƒ½è¯„ä¼°ï¼Œå¹¶èƒ½å¤Ÿè¯æ˜è¿‘ä¼¼ä»»åŠ¡éš¾åº¦æ’åã€‚MPTSæ— ç¼é›†æˆåˆ°é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œç›‘ç£å¾®è°ƒç¯å¢ƒä¸­ã€‚å®è¯æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„æ¨¡å¼è¯†åˆ«å’Œåºåˆ—å†³ç­–åˆ¶å®šæ–¹é¢è¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒMPTSæ˜¾è‘—æé«˜äº†å°¾éƒ¨æˆ–è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰ä»»åŠ¡çš„é€‚åº”ç¨³å¥æ€§ï¼Œå¹¶æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-rllab/MPTSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11039v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰æ¡†æ¶é€šè¿‡ç»“åˆä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ï¼Œä¸ºé²æ£’çš„æ´»åŠ¨ä»»åŠ¡é‡‡æ ·æä¾›äº†ç†è®ºåŸºç¡€ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¥è¡¨å¾ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡åéªŒæ¨ç†é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©ï¼Œä»è€Œé™ä½äº†ä»»åŠ¡é€‚åº”æ€§èƒ½è¯„ä¼°çš„æˆæœ¬ï¼Œå¹¶è¿‘ä¼¼åœ°å®Œæˆä»»åŠ¡éš¾åº¦æ’åã€‚MPTSæ— ç¼é›†æˆåˆ°é›¶æ¬¡ã€å°‘æ¬¡å’Œç›‘æ§å¾®è°ƒè®¾ç½®ä¸­ï¼Œé€šè¿‡ç»éªŒéªŒè¯ï¼Œåœ¨åŸºç¡€æ¨¡å‹çš„æ¨¡å¼è¯†åˆ«å’Œåºåˆ—å†³ç­–åˆ¶å®šæ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒMPTSæ˜¾è‘—æé«˜äº†å¯¹å°¾ç«¯æˆ–è¶…å‡ºèŒƒå›´ï¼ˆOODï¼‰ä»»åŠ¡çš„é€‚åº”ç¨³å¥æ€§ï¼Œå¹¶ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰æ¡†æ¶ï¼Œè¿æ¥ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆæ¨¡å‹è¡¨å¾ä¼˜åŒ–è¿‡ç¨‹ï¼Œé¢„æµ‹ä»»åŠ¡ç‰¹å®šé€‚åº”é£é™©ã€‚</li>
<li>åéªŒæ¨ç†é™ä½ä»»åŠ¡é€‚åº”æ€§èƒ½è¯„ä¼°æˆæœ¬ï¼Œè¿‘ä¼¼å®Œæˆä»»åŠ¡éš¾åº¦æ’åã€‚</li>
<li>MPTSæ¡†æ¶é€‚ç”¨äºé›¶æ¬¡ã€å°‘æ¬¡å’Œç›‘æ§å¾®è°ƒè®¾ç½®ã€‚</li>
<li>åœ¨æ¨¡å¼è¯†åˆ«å’Œåºåˆ—å†³ç­–åˆ¶å®šçš„å®éªŒä¸­ï¼ŒMPTSæ˜¾è‘—æé«˜é€‚åº”ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b7250502f1788f51bdb2b4c51975d419.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2501.11039v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2501.11039v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2501.11039v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Switch-a-View-Few-Shot-View-Selection-Learned-from-Edited-Videos"><a href="#Switch-a-View-Few-Shot-View-Selection-Learned-from-Edited-Videos" class="headerlink" title="Switch-a-View: Few-Shot View Selection Learned from Edited Videos"></a>Switch-a-View: Few-Shot View Selection Learned from Edited Videos</h2><p><strong>Authors:Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Kristen Grauman</strong></p>
<p>We introduce SWITCH-A-VIEW, a model that learns to automatically select the viewpoint to display at each timepoint when creating a how-to video. The key insight of our approach is how to train such a model from unlabeled â€“ but human-edited â€“ video samples. We pose a pretext task that pseudo-labels segments in the training videos for their primary viewpoint (egocentric or exocentric), and then discovers the patterns between the visual and spoken content in a how-to video on the one hand and its view-switch moments on the other hand. Armed with this predictor, our model can be applied to new multi-view video settings for orchestrating which viewpoint should be displayed when, even when such settings come with limited labels. We demonstrate our idea on a variety of real-world videos from HowTo100M and Ego-Exo4D, and rigorously validate its advantages. Project: <a target="_blank" rel="noopener" href="https://vision.cs.utexas.edu/projects/switch_a_view/">https://vision.cs.utexas.edu/projects/switch_a_view/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SWITCH-A-VIEWæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å­¦ä¹ åœ¨åˆ›å»ºå¦‚ä½•åˆ¶ä½œè§†é¢‘æ—¶è‡ªåŠ¨é€‰æ‹©åœ¨æ¯ä¸ªæ—¶é—´ç‚¹æ˜¾ç¤ºå“ªä¸ªè§†è§’ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºå¦‚ä½•ä»æ— äººæ ‡æ³¨ä½†ç»è¿‡äººå·¥ç¼–è¾‘çš„è§†é¢‘æ ·æœ¬ä¸­è®­ç»ƒæ­¤ç±»æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢„è®¾ä»»åŠ¡ï¼Œå¯¹è®­ç»ƒè§†é¢‘ä¸­çš„ç‰‡æ®µè¿›è¡Œä¼ªæ ‡ç­¾æ ‡æ³¨ï¼Œä»¥ç¡®å®šå…¶ä¸»è¦è§†è§’ï¼ˆç¬¬ä¸€äººç§°æˆ–ç¬¬ä¸‰äººç§°è§†è§’ï¼‰ï¼Œç„¶åå‘ç°å¦‚ä½•åœ¨åˆ¶ä½œå¦‚ä½•åˆ¶ä½œè§†é¢‘æ—¶ï¼Œä¸€æ–¹é¢å…¶è§†è§‰å’Œè¯­éŸ³å†…å®¹ä¹‹é—´ä¸å¦ä¸€æ–¹é¢è§†è§’åˆ‡æ¢æ—¶åˆ»ä¹‹é—´çš„æ¨¡å¼ã€‚æœ‰äº†è¿™ä¸ªé¢„æµ‹å™¨ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åº”ç”¨äºæ–°çš„å¤šè§†è§’è§†é¢‘è®¾ç½®ï¼Œä»¥åè°ƒä½•æ—¶æ˜¾ç¤ºå“ªä¸ªè§†è§’ï¼Œå³ä½¿åœ¨å¸¦æœ‰æœ‰é™æ ‡ç­¾çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åœ¨HowTo100Må’ŒEgo-Exo4Dç­‰ç°å®ä¸–ç•Œçš„è§†é¢‘ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æƒ³æ³•ï¼Œå¹¶ä¸¥æ ¼éªŒè¯äº†å…¶ä¼˜åŠ¿ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://vision.cs.utexas.edu/projects/switch_a_view/%E3%80%82">https://vision.cs.utexas.edu/projects/switch_a_view/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18386v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>SWITCH-A-VIEWæ¨¡å‹å¯ä»¥è‡ªåŠ¨é€‰æ‹©åˆ›å»ºå¦‚ä½•æ“ä½œè§†é¢‘æ—¶æ¯ä¸ªæ—¶é—´ç‚¹çš„è§†è§’ã€‚è¯¥æ¨¡å‹çš„å…³é”®åœ¨äºå¦‚ä½•ä»æ— äººæ ‡æ³¨ä½†ç»è¿‡äººå·¥ç¼–è¾‘çš„è§†é¢‘æ ·æœ¬ä¸­è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªé¢„è®¾ä»»åŠ¡ï¼Œå¯¹è®­ç»ƒè§†é¢‘çš„ä¸»è¦è§†è§’ï¼ˆç¬¬ä¸€äººç§°æˆ–ç¬¬ä¸‰äººç§°è§†è§’ï¼‰è¿›è¡Œä¼ªæ ‡ç­¾æ ‡æ³¨ï¼Œç„¶åå‘ç°è§†é¢‘ä¸­çš„è§†è§‰å’Œè¯­éŸ³å†…å®¹ä¸åˆ‡æ¢è§†è§’çš„æ—¶åˆ»ä¹‹é—´çš„æ¨¡å¼ã€‚å€ŸåŠ©æ­¤é¢„æµ‹å™¨ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åº”ç”¨äºæ–°çš„å¤šè§†è§’è§†é¢‘è®¾ç½®ï¼Œä»¥åè°ƒä½•æ—¶æ˜¾ç¤ºå“ªä¸ªè§†è§’ï¼Œå³ä½¿åœ¨æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„è§†é¢‘ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æƒ³æ³•ï¼Œå¹¶ä¸¥æ ¼éªŒè¯äº†å…¶ä¼˜åŠ¿ã€‚æ›´å¤šä¿¡æ¯å‚è§ï¼š<a target="_blank" rel="noopener" href="https://vision.cs.utexas.edu/projects/switch_a_view/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SWITCH-A-VIEWæ¨¡å‹èƒ½è‡ªåŠ¨é€‰æ‹©åˆ›å»ºå¦‚ä½•æ“ä½œè§†é¢‘çš„æœ€ä½³è§†è§’ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä»æ— äººæ ‡æ³¨ä½†ç»è¿‡äººå·¥ç¼–è¾‘çš„è§†é¢‘æ ·æœ¬ä¸­å­¦ä¹ æ¥å®ç°è¿™ä¸€åŠŸèƒ½ã€‚</li>
<li>é€šè¿‡é¢„è®¾ä»»åŠ¡å¯¹è®­ç»ƒè§†é¢‘çš„ä¸»è¦è§†è§’è¿›è¡Œä¼ªæ ‡ç­¾æ ‡æ³¨ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿå‘ç°è§†é¢‘ä¸­çš„è§†è§‰å’Œè¯­éŸ³å†…å®¹ä¸è§†è§’åˆ‡æ¢æ—¶åˆ»ä¹‹é—´çš„æ¨¡å¼ã€‚</li>
<li>è¯¥æ¨¡å‹é€‚ç”¨äºæ–°çš„å¤šè§†è§’è§†é¢‘è®¾ç½®ï¼Œå³ä½¿æ ‡ç­¾æœ‰é™ä¹Ÿèƒ½æœ‰æ•ˆè¿è¡Œã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„è§†é¢‘ä¸Šè¿›è¡Œäº†æ¨¡å‹å±•ç¤ºï¼ŒéªŒè¯äº†å…¶ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2412.18386v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2412.18386v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2412.18386v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="vesselFM-A-Foundation-Model-for-Universal-3D-Blood-Vessel-Segmentation"><a href="#vesselFM-A-Foundation-Model-for-Universal-3D-Blood-Vessel-Segmentation" class="headerlink" title="vesselFM: A Foundation Model for Universal 3D Blood Vessel Segmentation"></a>vesselFM: A Foundation Model for Universal 3D Blood Vessel Segmentation</h2><p><strong>Authors:Bastian Wittmann, Yannick Wattenberg, Tamaz Amiranashvili, Suprosanna Shit, Bjoern Menze</strong></p>
<p>Segmenting 3D blood vessels is a critical yet challenging task in medical image analysis. This is due to significant imaging modality-specific variations in artifacts, vascular patterns and scales, signal-to-noise ratios, and background tissues. These variations, along with domain gaps arising from varying imaging protocols, limit the generalization of existing supervised learning-based methods, requiring tedious voxel-level annotations for each dataset separately. While foundation models promise to alleviate this limitation, they typically fail to generalize to the task of blood vessel segmentation, posing a unique, complex problem. In this work, we present vesselFM, a foundation model designed specifically for the broad task of 3D blood vessel segmentation. Unlike previous models, vesselFM can effortlessly generalize to unseen domains. To achieve zero-shot generalization, we train vesselFM on three heterogeneous data sources: a large, curated annotated dataset, data generated by a domain randomization scheme, and data sampled from a flow matching-based generative model. Extensive evaluations show that vesselFM outperforms state-of-the-art medical image segmentation foundation models across four (pre-)clinically relevant imaging modalities in zero-, one-, and few-shot scenarios, therefore providing a universal solution for 3D blood vessel segmentation. </p>
<blockquote>
<p>å¯¹3Dè¡€ç®¡è¿›è¡Œåˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­ä¸€ä¸ªå…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è¿™æ˜¯å› ä¸ºä¼ªå½±ã€è¡€ç®¡æ¨¡å¼ä¸å°ºåº¦ã€ä¿¡å™ªæ¯”ä»¥åŠèƒŒæ™¯ç»„ç»‡ç­‰æ–¹é¢çš„æˆåƒæ¨¡æ€ç‰¹å®šå˜åŒ–æ˜¾è‘—ã€‚è¿™äº›å˜åŒ–ä»¥åŠç”±ä¸åŒæˆåƒåè®®äº§ç”Ÿçš„é¢†åŸŸå·®è·ï¼Œé™åˆ¶äº†ç°æœ‰åŸºäºç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä¸€èˆ¬åŒ–ï¼Œéœ€è¦ä¸ºæ¯ä¸ªæ•°æ®é›†å•ç‹¬è¿›è¡Œç¹ççš„ä½“ç´ çº§æ³¨é‡Šã€‚è™½ç„¶åŸºç¡€æ¨¡å‹æœ‰æœ›ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•æ³›åŒ–åˆ°è¡€ç®¡åˆ†å‰²ä»»åŠ¡ï¼Œå½¢æˆäº†ä¸€ä¸ªç‹¬ç‰¹ä¸”å¤æ‚çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“ä¸ºå¹¿æ³›çš„3Dè¡€ç®¡åˆ†å‰²ä»»åŠ¡è®¾è®¡çš„åŸºç¡€æ¨¡å‹vesselFMã€‚ä¸ä»¥å‰çš„æ¨¡å‹ä¸åŒï¼ŒvesselFMå¯ä»¥è½»æ¾æ³›åŒ–åˆ°æœªè§è¿‡çš„é¢†åŸŸã€‚ä¸ºäº†å®ç°é›¶æ ·æœ¬æ³›åŒ–ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªå¼‚è´¨æ•°æ®æºä¸Šè®­ç»ƒvesselFMï¼šä¸€ä¸ªå¤§å‹ç²¾é€‰æ³¨é‡Šæ•°æ®é›†ã€ç”±åŸŸéšæœºåŒ–æ–¹æ¡ˆç”Ÿæˆçš„æ•°æ®ï¼Œä»¥åŠåŸºäºæµåŒ¹é…çš„ç”Ÿæˆæ¨¡å‹é‡‡æ ·çš„æ•°æ®ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬ã€ä¸€æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­ï¼ŒvesselFMåœ¨å››ç§ï¼ˆä¸´åºŠï¼‰ç›¸å…³çš„æˆåƒæ¨¡æ€ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œå› æ­¤ä¸º3Dè¡€ç®¡åˆ†å‰²æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17386v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä¸»è¦æ¢è®¨åŒ»å­¦å›¾åƒåˆ†æä¸­ä¸‰ç»´è¡€ç®¡åˆ†å‰²çš„é‡è¦æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚æ–‡ç« æŒ‡å‡ºç”±äºæˆåƒæ¨¡æ€ç‰¹å¼‚æ€§é€ æˆçš„å„ç§å·®å¼‚ï¼Œå¦‚ä¼ªå½±ã€è¡€ç®¡æ¨¡å¼ã€å°ºåº¦ã€ä¿¡å™ªæ¯”å’ŒèƒŒæ™¯ç»„ç»‡ç­‰å·®å¼‚ï¼Œä»¥åŠä¸åŒæˆåƒåè®®é€ æˆçš„é¢†åŸŸå·®è·ï¼Œé™åˆ¶äº†ç°æœ‰åŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•çš„æ¨å¹¿ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºvesselFMçš„ä¸“ç”¨åŸºç¡€æ¨¡å‹ï¼Œç”¨äºè§£å†³å¹¿æ³›çš„è¡€ç®¡åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡è®­ç»ƒä¸‰ç§ä¸åŒçš„æ•°æ®æºå®ç°é›¶æ ·æœ¬æ³›åŒ–ï¼šå¤§å‹æ³¨é‡Šæ•°æ®é›†ã€é€šè¿‡é¢†åŸŸéšæœºåŒ–æ–¹æ¡ˆç”Ÿæˆçš„æ•°æ®ä»¥åŠåŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„æ•°æ®æ ·æœ¬ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼ŒvesselFMå‡ä¼˜äºæœ€æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œä¸ºä¸‰ç»´è¡€ç®¡åˆ†å‰²æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¸‰ç»´è¡€ç®¡åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å…³é”®ä»»åŠ¡ï¼Œä½†å­˜åœ¨å¤šç§æŒ‘æˆ˜ã€‚</li>
<li>æˆåƒæ¨¡æ€å·®å¼‚å¯¼è‡´çš„æ•°æ®å¤šæ ·æ€§é™åˆ¶äº†ç°æœ‰ç›‘ç£å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºvesselFMçš„åŸºç¡€æ¨¡å‹ï¼Œé’ˆå¯¹å¹¿æ³›çš„è¡€ç®¡åˆ†å‰²ä»»åŠ¡è¿›è¡Œä¸“é—¨è®¾è®¡ã€‚</li>
<li>vesselFMæ¨¡å‹é€šè¿‡è®­ç»ƒä¸‰ç§ä¸åŒçš„æ•°æ®æºå®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å‡ä¼˜äºå…¶ä»–å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.17386v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.17386v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.17386v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.17386v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.17386v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.17386v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.17386v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Transfer-Learning-for-Video-language-Foundation-Models"><a href="#Efficient-Transfer-Learning-for-Video-language-Foundation-Models" class="headerlink" title="Efficient Transfer Learning for Video-language Foundation Models"></a>Efficient Transfer Learning for Video-language Foundation Models</h2><p><strong>Authors:Haoxing Chen, Zizheng Huang, Yan Hong, Yanshuo Wang, Zhongcai Lyu, Zhuoer Xu, Jun Lan, Zhangxuan Gu</strong></p>
<p>Pre-trained vision-language models provide a robust foundation for efficient transfer learning across various downstream tasks. In the field of video action recognition, mainstream approaches often introduce additional modules to capture temporal information. Although the additional modules increase the capacity of model, enabling it to better capture video-specific inductive biases, existing methods typically introduce a substantial number of new parameters and are prone to catastrophic forgetting of previously acquired generalizable knowledge. In this paper, we propose a parameter-efficient Multi-modal Spatio-Temporal Adapter (MSTA) to enhance the alignment between textual and visual representations, achieving a balance between generalizable knowledge and task-specific adaptation. Furthermore, to mitigate over-fitting and enhance generalizability, we introduce a spatio-temporal description-guided consistency constraint.This constraint involves providing template inputs (e.g., â€œa video of {\textbf{cls}}â€œ) to the trainable language branch and LLM-generated spatio-temporal descriptions to the pre-trained language branch, enforcing output consistency between the branches. This approach reduces overfitting to downstream tasks and enhances the distinguishability of the trainable branch within the spatio-temporal semantic space. We evaluate the effectiveness of our approach across four tasks: zero-shot transfer, few-shot learning, base-to-novel generalization, and fully-supervised learning. Compared to many state-of-the-art methods, our MSTA achieves outstanding performance across all evaluations, while using only 2-7% of the trainable parameters in the original model. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„é«˜æ•ˆè¿ç§»å­¦ä¹ æä¾›äº†åšå®çš„åŸºç¡€ã€‚åœ¨è§†é¢‘åŠ¨ä½œè¯†åˆ«é¢†åŸŸï¼Œä¸»æµæ–¹æ³•é€šå¸¸å¼•å…¥é¢å¤–çš„æ¨¡å—æ¥æ•è·æ—¶é—´ä¿¡æ¯ã€‚è™½ç„¶é¢å¤–çš„æ¨¡å—å¢åŠ äº†æ¨¡å‹çš„å®¹é‡ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†é¢‘ç‰¹å®šçš„å½’çº³åè§ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸å¼•å…¥äº†å¤§é‡æ–°çš„å‚æ•°ï¼Œå¹¶å®¹æ˜“é—å¿˜å…ˆå‰è·å¾—çš„å¯æ³›åŒ–çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚æ•°æœ‰æ•ˆçš„å¤šæ¨¡æ€æ—¶ç©ºé€‚é…å™¨ï¼ˆMSTAï¼‰ï¼Œä»¥æé«˜æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„å¯¹é½ï¼Œåœ¨å¯æ³›åŒ–çŸ¥è¯†å’Œä»»åŠ¡ç‰¹å®šé€‚åº”ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£è¿‡æ‹Ÿåˆå¹¶å¢å¼ºæ³›åŒ–æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶ç©ºæè¿°å¼•å¯¼çš„ä¸€è‡´æ€§çº¦æŸã€‚è¯¥çº¦æŸæ¶‰åŠå‘å¯è®­ç»ƒçš„è¯­è¨€åˆ†æ”¯æä¾›æ¨¡æ¿è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸€ä¸ªåŒ…å«{\textbf{cls}}çš„è§†é¢‘â€ï¼‰ï¼Œå¹¶å‘é¢„è®­ç»ƒçš„è¯­è¨€åˆ†æ”¯æä¾›LLMç”Ÿæˆçš„æ—¶ç©ºæè¿°ï¼Œä»¥å¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯ä¹‹é—´çš„è¾“å‡ºä¸€è‡´æ€§ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†è¿‡åº¦æ‹Ÿåˆä¸‹æ¸¸ä»»åŠ¡çš„æƒ…å†µï¼Œå¹¶å¢å¼ºäº†å¯è®­ç»ƒåˆ†æ”¯åœ¨æ—¶ç©ºè¯­ä¹‰ç©ºé—´ä¸­çš„å¯åŒºåˆ†æ€§ã€‚æˆ‘ä»¬åœ¨å››é¡¹ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šé›¶æ ·æœ¬è¿ç§»ã€å°æ ·ä¾‹å­¦ä¹ ã€ä»åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ä»¥åŠå®Œå…¨ç›‘ç£å­¦ä¹ ã€‚ä¸è®¸å¤šæœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„MSTAåœ¨æ‰€æœ‰è¯„ä¼°ä¸­éƒ½å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒæ—¶åªä½¿ç”¨äº†åŸå§‹æ¨¡å‹ä¸­2-7%çš„å¯è®­ç»ƒå‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11223v3">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†é«˜æ•ˆçš„è¿ç§»å­¦ä¹ åŸºç¡€ã€‚åœ¨è§†é¢‘åŠ¨ä½œè¯†åˆ«é¢†åŸŸï¼Œä¸»æµæ–¹æ³•é€šè¿‡å¼•å…¥é¢å¤–æ¨¡å—æ¥æ•è·æ—¶åºä¿¡æ¯ï¼Œä½†ä¼šå¼•å…¥å¤§é‡æ–°å‚æ•°å¹¶å®¹æ˜“é—å¿˜å…ˆå‰è·å¾—çš„å¯æ¨å¹¿çŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºä¸€ç§å‚æ•°é«˜æ•ˆçš„è·¨æ¨¡æ€æ—¶ç©ºé€‚é…å™¨ï¼ˆMSTAï¼‰ï¼Œå¢å¼ºæ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„å¯¹é½ï¼Œå®ç°é€šç”¨çŸ¥è¯†å’Œä»»åŠ¡ç‰¹å®šé€‚åº”ä¹‹é—´çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æ—¶ç©ºæè¿°å¼•å¯¼çš„ä¸€è‡´æ€§çº¦æŸæ¥ç¼“è§£è¿‡æ‹Ÿåˆå¹¶å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚è¯¥çº¦æŸé€šè¿‡å‘å¯è®­ç»ƒçš„è¯­è¨€åˆ†æ”¯æä¾›æ¨¡æ¿è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸€ä¸ªåŒ…å«{\textbf{cls}}çš„è§†é¢‘â€ï¼‰ï¼Œå¹¶å‘é¢„è®­ç»ƒçš„è¯­è¨€åˆ†æ”¯æä¾›LLMç”Ÿæˆçš„æ—¶ç©ºæè¿°ï¼Œæ¥å¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯ä¹‹é—´çš„è¾“å‡ºä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•å‡å°‘äº†ä¸‹æ¸¸ä»»åŠ¡çš„è¿‡æ‹Ÿåˆï¼Œå¢å¼ºäº†å¯è®­ç»ƒåˆ†æ”¯åœ¨æ—¶ç©ºè¯­ä¹‰ç©ºé—´ä¸­çš„å¯åŒºåˆ†æ€§ã€‚åœ¨å››é¡¹ä»»åŠ¡ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šé›¶æ ·æœ¬è¿ç§»ã€å°æ ·å­¦ä¹ ã€åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ä»¥åŠå…¨ç›‘ç£å­¦ä¹ ã€‚ä¸è®¸å¤šå…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„MSTAåœ¨æ‰€æœ‰è¯„ä¼°ä¸­éƒ½å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°ä»…ä¸ºåŸå§‹æ¨¡å‹çš„2-7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸ºå¤šç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†è¿ç§»å­¦ä¹ çš„åŸºç¡€ã€‚</li>
<li>ä¸»æµè§†é¢‘åŠ¨ä½œè¯†åˆ«æ–¹æ³•é€šè¿‡å¼•å…¥é¢å¤–æ¨¡å—æ¥æ•è·æ—¶åºä¿¡æ¯ï¼Œä½†ä¼šå¢åŠ æ¨¡å‹å¤æ‚æ€§å’Œå¿˜è®°å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>æå‡ºçš„MSTAå¢å¼ºæ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºçš„å¯¹é½ï¼Œå¹³è¡¡é€šç”¨çŸ¥è¯†å’Œä»»åŠ¡ç‰¹å®šé€‚åº”ã€‚</li>
<li>å¼•å…¥æ—¶ç©ºæè¿°å¼•å¯¼çš„ä¸€è‡´æ€§çº¦æŸï¼Œé€šè¿‡å¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯è¾“å‡ºä¸€è‡´æ€§æ¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MSTAä½¿ç”¨å‚æ•°é«˜æ•ˆï¼Œä»…ä½¿ç”¨åŸå§‹æ¨¡å‹çš„2-7%å‚æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è¿ç§»ã€å°æ ·å­¦ä¹ ã€åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ä»¥åŠå…¨ç›‘ç£å­¦ä¹ ç­‰å››é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.11223v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.11223v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.11223v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2411.11223v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SPORTU-A-Comprehensive-Sports-Understanding-Benchmark-for-Multimodal-Large-Language-Models"><a href="#SPORTU-A-Comprehensive-Sports-Understanding-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal   Large Language Models"></a>SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal   Large Language Models</h2><p><strong>Authors:Haotian Xia, Zhengbang Yang, Junbo Zou, Rhys Tracy, Yuqing Wang, Chi Lu, Christopher Lai, Yanjun He, Xun Shao, Zhuoqing Xie, Yuan-fang Wang, Weining Shen, Hanjie Chen</strong></p>
<p>Multimodal Large Language Models (MLLMs) are advancing the ability to reason about complex sports scenarios by integrating textual and visual information. To comprehensively evaluate their capabilities, we introduce SPORTU, a benchmark designed to assess MLLMs across multi-level sports reasoning tasks. SPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice questions with human-annotated explanations for rule comprehension and strategy understanding. This component focuses on testing modelsâ€™ ability to reason about sports solely through question-answering (QA), without requiring visual inputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7 different sports and 12,048 QA pairs, designed to assess multi-level reasoning, from simple sports recognition to complex tasks like foul detection and rule application. We evaluate four prevalent LLMs mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text part. We evaluate four LLMs using few-shot learning and chain-of-thought (CoT) prompting on SPORTU-text. GPT-4o achieves the highest accuracy of 71%, but still falls short of human-level performance, highlighting room for improvement in rule comprehension and reasoning. The evaluation for the SPORTU-video part includes 7 proprietary and 6 open-source MLLMs. Experiments show that models fall short on hard tasks that require deep reasoning and rule-based understanding. Claude-3.5-Sonnet performs the best with only 52.6% accuracy on the hard task, showing large room for improvement. We hope that SPORTU will serve as a critical step toward evaluating modelsâ€™ capabilities in sports understanding and reasoning. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œæé«˜äº†å¯¹å¤æ‚ä½“è‚²åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°å…¶èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†SPORTUåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°MLLMsåœ¨å¤šçº§åˆ«ä½“è‚²æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚SPORTUåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šSPORTU-textå’ŒSPORTU-videoã€‚SPORTU-textç”±äººç±»æ³¨é‡Šè§£é‡Šçš„è§„åˆ™ç†è§£å’Œç­–ç•¥ç†è§£çš„900é“é€‰æ‹©é¢˜ç»„æˆï¼Œä¾§é‡äºæµ‹è¯•æ¨¡å‹ä»…é€šè¿‡é—®ç­”ï¼ˆQAï¼‰å¯¹ä½“è‚²è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œæ— éœ€è§†è§‰è¾“å…¥ï¼›SPORTU-videoåŒ…å«è·¨è¶Š7ç§ä¸åŒè¿åŠ¨çš„æ…¢é€Ÿè¿åŠ¨è§†é¢‘å‰ªè¾‘ä»¥åŠç”¨äºè¯„ä¼°å¤šçº§åˆ«æ¨ç†çš„ç­”é¢˜é…å¯¹å…±è®¡ä¸€ä¸‡ä½™æ¬¡é—®ç­”ã€‚ä¸ºäº†æµ‹è¯•æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæˆ‘ä»¬åœ¨SPORTU-textéƒ¨åˆ†ä¸»è¦ä½¿ç”¨åŸºäºå°‘æ ·æœ¬å­¦ä¹ æ¨¡å¼çš„æ€è€ƒé“¾ï¼ˆCoTï¼‰æç¤ºã€‚æˆ‘ä»¬åœ¨SPORTU-textä¸Šè¯„ä¼°äº†å››ä¸ªLLMsçš„è¡¨ç°ï¼Œå…¶ä¸­GPT-4oå–å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡71%ï¼Œä½†ä»æœªè¾¾åˆ°äººç±»æ°´å¹³è¡¨ç°ï¼Œè¿™çªæ˜¾å‡ºåœ¨è§„åˆ™ç†è§£å’Œæ¨ç†æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚å¯¹äºSPORTU-videoéƒ¨åˆ†çš„è¯„ä¼°åŒ…æ‹¬ä½¿ç”¨ä¸“æœ‰å’Œå¼€æºçš„MLLMsè¿›è¡Œå®éªŒã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤„ç†éœ€è¦æ·±åº¦æ¨ç†å’ŒåŸºäºè§„åˆ™ç†è§£çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸è¶³ã€‚å…¶ä¸­ï¼ŒClaude-3.5-Sonnetåœ¨å›°éš¾ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ€ä½³æ•ˆæœï¼Œä½†ä¹Ÿä»…å®ç°äº†ä»¤äººå¤±æœ›ä¸”æ˜¾ç¤ºå­˜åœ¨è¾ƒå¤§æå‡ç©ºé—´çš„å‡†ç¡®æ€§æ°´å¹³ä»…ä¸ºåŠæ•°å¤šå‡ ä»½ã€‚æˆ‘ä»¬å¸Œæœ›SPORTUèƒ½æˆä¸ºè¯„ä¼°æ¨¡å‹åœ¨ä½“è‚²ç†è§£å’Œæ¨ç†èƒ½åŠ›æ–¹é¢èƒ½åŠ›çš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08474v4">PDF</a> ICLR 2025 Poster</p>
<p><strong>Summary</strong></p>
<pre><code>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œæé«˜äº†å¯¹å¤æ‚ä½“è‚²åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚ä¸ºå…¨é¢è¯„ä¼°å…¶æ€§èƒ½ï¼Œæ¨å‡ºäº†SPORTUåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°MLLMsåœ¨ä¸åŒæ°´å¹³çš„ä½“è‚²æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚SPORTUåŒ…æ‹¬ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šSPORTU-textï¼ŒåŒ…å«900é“å¸¦æœ‰äººç±»æ³¨é‡Šçš„è§£é‡Šçš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹ä»…å‡­é—®ç­”ï¼ˆQAï¼‰è¿›è¡Œä½“è‚²è§„åˆ™ç†è§£å’Œç­–ç•¥ç†è§£çš„èƒ½åŠ›ï¼Œæ— éœ€è§†è§‰è¾“å…¥ï¼›SPORTU-videoï¼ŒåŒ…å«7ç§ä¸åŒä½“è‚²è¿åŠ¨çš„1701ä¸ªæ…¢åŠ¨ä½œè§†é¢‘å‰ªè¾‘å’Œ12048ä¸ªé—®ç­”å¯¹ï¼Œæ—¨åœ¨è¯„ä¼°ä»ç®€å•çš„ä½“è‚²è¯†åˆ«åˆ°å¤æ‚çš„å¦‚çŠ¯è§„æ£€æµ‹å’Œè§„åˆ™åº”ç”¨ç­‰å¤šçº§æ¨ç†èƒ½åŠ›ã€‚å¯¹å››æ¬¾æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸»è¦é‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºã€‚GPT-4oåœ¨SPORTU-textéƒ¨åˆ†è¾¾åˆ°äº†71%çš„æœ€é«˜å‡†ç¡®ç‡ï¼Œä½†ä»æœªè¾¾åˆ°äººç±»æ°´å¹³ï¼Œè¡¨æ˜åœ¨è§„åˆ™ç†è§£å’Œæ¨ç†æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚SPORTU-videoéƒ¨åˆ†çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨éœ€è¦æ·±åº¦æ¨ç†å’Œè§„åˆ™ç†è§£çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ã€‚æœ€å¥½çš„æ¨¡å‹Claude-3.5-Sonnetåœ¨ç¡¬ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡åªæœ‰52.6%ï¼Œä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚å¸Œæœ›SPORTUèƒ½æˆä¸ºè¯„ä¼°æ¨¡å‹åœ¨ä½“è‚²ç†è§£å’Œæ¨ç†èƒ½åŠ›æ–¹é¢èƒ½åŠ›çš„é‡è¦ä¸€æ­¥ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èåˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œæå‡ä½“è‚²åœºæ™¯ç†è§£ã€‚</li>
<li>SPORTUåŸºå‡†æµ‹è¯•ç”¨äºå…¨é¢è¯„ä¼°MLLMsåœ¨ä½“è‚²æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>SPORTUåŒ…å«SPORTU-textå’ŒSPORTU-videoä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«ä¾§é‡æ–‡æœ¬å’Œè§†è§‰ä½“è‚²æ¨ç†ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨LLMsè¯„ä¼°ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>GPT-4oåœ¨SPORTU-textçš„å‡†ç¡®ç‡æœ€é«˜ï¼Œä½†è§„åˆ™ç†è§£å’Œæ¨ç†æ–¹é¢ä»å¾…æé«˜ã€‚</li>
<li>æ¨¡å‹åœ¨SPORTU-videoçš„ç¡¬ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œå°¤å…¶æ˜¯æ·±åº¦æ¨ç†å’Œè§„åˆ™ç†è§£æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2410.08474v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2410.08474v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2410.08474v4/page_3_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CLIPâ€™s-Visual-Embedding-Projector-is-a-Few-shot-Cornucopia"><a href="#CLIPâ€™s-Visual-Embedding-Projector-is-a-Few-shot-Cornucopia" class="headerlink" title="CLIPâ€™s Visual Embedding Projector is a Few-shot Cornucopia"></a>CLIPâ€™s Visual Embedding Projector is a Few-shot Cornucopia</h2><p><strong>Authors:Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick PÃ©rez, Raoul de Charette</strong></p>
<p>We consider the problem of adapting a contrastively pretrained vision-language model like CLIP (Radford et al., 2021) for few-shot classification. The literature addresses this problem by learning a linear classifier of the frozen visual features, optimizing word embeddings, or learning external feature adapters. We introduce an alternative way for few-shot CLIP adaptation without adding â€˜â€™externalâ€™â€™ parameters to optimize. We find that simply fine-tuning the embedding projection matrix of the vision encoder leads to better performance than all baselines. Furthermore, we show that regularizing training with the distance between the fine-tuned and pretrained matrices adds reliability for adapting CLIP, making the results stable across different learning rates in the â€˜â€™validation-freeâ€™â€™ setting. This simple approach, coined ProLIP, yields state-of-the-art performance on 11 few-shot classification benchmarks, few-shot cross-dataset transfer, domain generalization, and base-to-new class generalization. We also show that ProLIP significantly outperforms prompt tuning when extended to another task of test-time adaptation, while being one order of magnitude faster to train. Code will be made available at: <a target="_blank" rel="noopener" href="https://github.com/astra-vision/ProLIP">https://github.com/astra-vision/ProLIP</a> . </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘å°†å¯¹æ¯”é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ¨¡å‹ï¼ˆå¦‚CLIPï¼ŒRadfordç­‰äººï¼ˆ2021ï¼‰ï¼‰åº”ç”¨äºå°æ ·æœ¬æ–‡æœ¬åˆ†ç±»çš„é—®é¢˜ã€‚æ–‡çŒ®ä¸­é’ˆå¯¹è¯¥é—®é¢˜æå‡ºäº†ä¸€äº›æ–¹æ³•ï¼Œä¾‹å¦‚å­¦ä¹ å›ºå®šè§†è§‰ç‰¹å¾çš„çº¿æ€§åˆ†ç±»å™¨ï¼Œä¼˜åŒ–è¯åµŒå…¥ï¼Œæˆ–å­¦ä¹ å¤–éƒ¨ç‰¹å¾é€‚é…å™¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€æ·»åŠ é¢å¤–çš„å‚æ•°è¿›è¡Œä¼˜åŒ–çš„å°‘æ•°æ ·æœ¬CLIPé€‚åº”æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ä»…ä»…å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„åµŒå…¥æŠ•å½±çŸ©é˜µå°±å¯ä»¥è¾¾åˆ°è¶…è¶Šæ‰€æœ‰åŸºçº¿æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†é€šè¿‡å¾®è°ƒä¸é¢„è®­ç»ƒçŸ©é˜µä¹‹é—´çš„è·ç¦»æ¥æ­£åˆ™è®­ç»ƒï¼Œå¯ä»¥å¢åŠ CLIPçš„é€‚åº”æ€§å¯é æ€§ï¼Œåœ¨æ— éœ€éªŒè¯çš„ç¯å¢ƒä¸­ï¼Œä½¿ç»“æœåœ¨ä¸åŒçš„å­¦ä¹ ç‡ä¹‹é—´ä¿æŒç¨³å®šã€‚è¿™ç§ç®€å•çš„æ–¹æ³•è¢«ç§°ä¸ºProLIPï¼Œåœ¨åŒ…æ‹¬å°‘æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ã€å°‘æ ·æœ¬è·¨æ•°æ®é›†è¿ç§»ã€åŸŸæ³›åŒ–å’ŒåŸºç¡€åˆ°æ–°ç±»åˆ«æ³›åŒ–åœ¨å†…çš„å¤šç§ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†åœ¨æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ä»»åŠ¡ä¸Šæ‰©å±•æ—¶ï¼ŒProLIPæ˜æ˜¾ä¼˜äºæç¤ºå¾®è°ƒï¼ˆprompt tuningï¼‰ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šå¿«äº†ä¸€ä¸ªæ•°é‡çº§ã€‚ä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/astra-vision/ProLIP%E3%80%82">https://github.com/astra-vision/ProLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05270v3">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åŸºäºå¯¹æ¯”é¢„è®­ç»ƒçš„å¤šæ¨¡æ€ï¼ˆè§†è§‰å’Œè¯­è¨€ï¼‰æ¨¡å‹CLIPçš„å¾®è°ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è°ƒæ•´è§†è§‰ç¼–ç å™¨çš„åµŒå…¥æŠ•å½±çŸ©é˜µè¿›è¡Œå°‘æ ·æœ¬é€‚åº”ï¼Œæ— éœ€æ·»åŠ é¢å¤–çš„å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå°‘æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨è·¨æ•°æ®é›†è¿ç§»ã€é¢†åŸŸæ³›åŒ–å’ŒåŸºç¡€åˆ°æ–°ç±»åˆ«æ³›åŒ–æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æµ‹è¯•æ—¶é€‚åº”ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæç¤ºè°ƒæ•´æ–¹æ³•ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦å¿«ä¸€ä¸ªæ•°é‡çº§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä½¿ç”¨å¯¹æ¯”é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹CLIPè¿›è¡Œå°‘æ ·æœ¬é€‚åº”çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´è§†è§‰ç¼–ç å™¨çš„åµŒå…¥æŠ•å½±çŸ©é˜µè¿›è¡Œå°‘æ ·æœ¬é€‚åº”ï¼Œæ— éœ€æ·»åŠ é¢å¤–çš„å‚æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªå°‘æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯´æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨è·¨æ•°æ®é›†è¿ç§»ã€é¢†åŸŸæ³›åŒ–å’ŒåŸºç¡€åˆ°æ–°ç±»åˆ«æ³›åŒ–æ–¹é¢ï¼Œè¯¥æ–¹æ³•éƒ½è¡¨ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ä¸æç¤ºè°ƒæ•´æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æµ‹è¯•æ—¶é€‚åº”ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•çš„è®­ç»ƒé€Ÿåº¦è¾ƒå¿«ï¼Œè¾¾åˆ°äº†ä¸€ä¸ªæ•°é‡çº§çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2410.05270v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2410.05270v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2410.05270v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2410.05270v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Disentanglement-of-Content-and-Style-via-Variance-Invariance-Constraints"><a href="#Unsupervised-Disentanglement-of-Content-and-Style-via-Variance-Invariance-Constraints" class="headerlink" title="Unsupervised Disentanglement of Content and Style via   Variance-Invariance Constraints"></a>Unsupervised Disentanglement of Content and Style via   Variance-Invariance Constraints</h2><p><strong>Authors:Yuxuan Wu, Ziyu Wang, Bhiksha Raj, Gus Xia</strong></p>
<p>We contribute an unsupervised method that effectively learns disentangled content and style representations from sequences of observations. Unlike most disentanglement algorithms that rely on domain-specific labels or knowledge, our method is based on the insight of domain-general statistical differences between content and style â€“ content varies more among different fragments within a sample but maintains an invariant vocabulary across data samples, whereas style remains relatively invariant within a sample but exhibits more significant variation across different samples. We integrate such inductive bias into an encoder-decoder architecture and name our method after V3 (variance-versus-invariance). Experimental results show that V3 generalizes across multiple domains and modalities, successfully learning disentangled content and style representations, such as pitch and timbre from music audio, digit and color from images of hand-written digits, and action and character appearance from simple animations. V3 demonstrates strong disentanglement performance compared to existing unsupervised methods, along with superior out-of-distribution generalization under few-shot adaptation compared to supervised counterparts. Lastly, symbolic-level interpretability emerges in the learned content codebook, forging a near one-to-one alignment between machine representation and human knowledge. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»è§‚å¯Ÿåºåˆ—ä¸­å­¦ä¹ åˆ†ç¦»çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºã€‚ä¸åŒäºå¤§å¤šæ•°ä¾èµ–äºç‰¹å®šé¢†åŸŸæ ‡ç­¾æˆ–çŸ¥è¯†çš„è§£çº ç¼ ç®—æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŸºäºå†…å®¹å’Œé£æ ¼ä¹‹é—´çš„ä¸€èˆ¬ç»Ÿè®¡å·®å¼‚çš„è§è§£â€”â€”å†…å®¹åœ¨æ ·æœ¬å†…çš„ä¸åŒç‰‡æ®µä¹‹é—´å˜åŒ–æ›´å¤§ï¼Œä½†åœ¨æ•°æ®æ ·æœ¬ä¹‹é—´ä¿æŒä¸å˜çš„è¯æ±‡è¡¨ï¼Œè€Œé£æ ¼åœ¨æ ·æœ¬å†…ä¿æŒç›¸å¯¹ä¸å˜ï¼Œä½†åœ¨ä¸åŒæ ·æœ¬ä¹‹é—´è¡¨ç°å‡ºæ›´å¤§çš„å˜åŒ–ã€‚æˆ‘ä»¬å°†è¿™ç§å½’çº³åè§èå…¥ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­ï¼Œå¹¶å‘½åæˆ‘ä»¬çš„æ–¹æ³•ä¸ºV3ï¼ˆæ–¹å·®ä¸ä¸å˜æ€§ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒV3å¯ä»¥è·¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡å¼è¿›è¡Œæ¨å¹¿ï¼ŒæˆåŠŸå­¦ä¹ è§£çº ç¼ çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºï¼Œä¾‹å¦‚éŸ³ä¹éŸ³é¢‘ä¸­çš„éŸ³é«˜å’ŒéŸ³è‰²ï¼Œå›¾åƒä¸­çš„æ‰‹å†™æ•°å­—ä¸­çš„æ•°å­—å’Œé¢œè‰²ï¼Œä»¥åŠç®€å•åŠ¨ç”»ä¸­çš„åŠ¨ä½œå’Œè§’è‰²å¤–è§‚ã€‚ä¸ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒV3è¡¨ç°å‡ºå¼ºå¤§çš„è§£çº ç¼ æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å°‘é‡é€‚åº”çš„æƒ…å†µä¸‹å…·æœ‰å‡ºè‰²çš„è·¨åˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºæœ‰ç›‘ç£çš„åŒç±»æ–¹æ³•ã€‚æœ€åï¼Œåœ¨å­¦ä¹ åˆ°çš„å†…å®¹ä»£ç æœ¬ä¸­å‡ºç°äº†ç¬¦å·å±‚é¢çš„å¯è§£é‡Šæ€§ï¼Œå®ç°äº†æœºå™¨è¡¨ç¤ºå’Œäººç±»çŸ¥è¯†ä¹‹é—´çš„ä¸€å¯¹ä¸€å¯¹åº”å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03824v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€ç›‘ç£çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»åºåˆ—è§‚å¯Ÿä¸­å­¦ä¹ åˆ†ç¦»çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºã€‚ä¸å…¶ä»–ä¾èµ–äºç‰¹å®šé¢†åŸŸæ ‡ç­¾æˆ–çŸ¥è¯†çš„è§£çº ç¼ ç®—æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•åŸºäºå†…å®¹å’Œé£æ ¼ä¹‹é—´çš„ä¸€èˆ¬ç»Ÿè®¡å·®å¼‚æ¥åŒºåˆ†ä¸¤è€…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸå­¦ä¹ è§£çº ç¼ çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºï¼Œå¦‚éŸ³ä¹éŸ³é¢‘ä¸­çš„éŸ³é«˜å’ŒéŸ³è‰²ã€æ‰‹å†™æ•°å­—å›¾åƒä¸­çš„æ•°å­—å’Œé¢œè‰²ä»¥åŠç®€å•åŠ¨ç”»ä¸­çš„åŠ¨ä½œå’Œè§’è‰²å¤–è§‚ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è§£çº ç¼ æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬é€‚åº”æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„è¡¨ç°ã€‚æœ€åï¼Œæ‰€å­¦ä¹ çš„å†…å®¹ä»£ç ç°¿å…·æœ‰ç¬¦å·çº§çš„å¯è§£é‡Šæ€§ï¼Œå®ç°äº†æœºå™¨è¡¨ç¤ºå’Œäººç±»çŸ¥è¯†ä¹‹é—´è¿‘ä¹ä¸€ä¸€å¯¹åº”çš„å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•åŸºäºæ— éœ€ç›‘ç£çš„æ–¹å¼å­¦ä¹ å†…å®¹å’Œé£æ ¼çš„åˆ†ç¦»è¡¨ç¤ºã€‚</li>
<li>å†…å®¹å’Œé£æ ¼çš„å®šä¹‰åŸºäºä¸€èˆ¬ç»Ÿè®¡å·®å¼‚ï¼Œè€Œéç‰¹å®šé¢†åŸŸçš„æ ‡ç­¾æˆ–çŸ¥è¯†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡æ€ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è§£çº ç¼ æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬é€‚åº”æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„è¡¨ç°ï¼Œä¼˜äºç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>å­¦ä¹ åˆ°çš„å†…å®¹ä»£ç ç°¿å…·æœ‰ç¬¦å·çº§å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.03824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2407.03824v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2407.03824v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2407.03824v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2407.03824v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Residual-MPPI-Online-Policy-Customization-for-Continuous-Control"><a href="#Residual-MPPI-Online-Policy-Customization-for-Continuous-Control" class="headerlink" title="Residual-MPPI: Online Policy Customization for Continuous Control"></a>Residual-MPPI: Online Policy Customization for Continuous Control</h2><p><strong>Authors:Pengcheng Wang, Chenran Li, Catherine Weaver, Kenta Kawamoto, Masayoshi Tomizuka, Chen Tang, Wei Zhan</strong></p>
<p>Policies developed through Reinforcement Learning (RL) and Imitation Learning (IL) have shown great potential in continuous control tasks, but real-world applications often require adapting trained policies to unforeseen requirements. While fine-tuning can address such needs, it typically requires additional data and access to the original training metrics and parameters. In contrast, an online planning algorithm, if capable of meeting the additional requirements, can eliminate the necessity for extensive training phases and customize the policy without knowledge of the original training scheme or task. In this work, we propose a generic online planning algorithm for customizing continuous-control policies at the execution time, which we call Residual-MPPI. It can customize a given prior policy on new performance metrics in few-shot and even zero-shot online settings, given access to the prior action distribution alone. Through our experiments, we demonstrate that the proposed Residual-MPPI algorithm can accomplish the few-shot&#x2F;zero-shot online policy customization task effectively, including customizing the champion-level racing agent, Gran Turismo Sophy (GT Sophy) 1.0, in the challenging car racing scenario, Gran Turismo Sport (GTS) environment. Code for MuJoCo experiments is included in the supplementary and will be open-sourced upon acceptance. Demo videos and code are available on our website: <a target="_blank" rel="noopener" href="https://sites.google.com/view/residual-mppi">https://sites.google.com/view/residual-mppi</a>. </p>
<blockquote>
<p>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰åˆ¶å®šçš„ç­–ç•¥åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°å®ä¸–ç•Œçš„åº”ç”¨é€šå¸¸éœ€è¦é€‚åº”æœªæ›¾é¢„è§çš„è¦æ±‚ã€‚è™½ç„¶å¾®è°ƒå¯ä»¥è§£å†³è¿™ç§éœ€æ±‚ï¼Œä½†å®ƒé€šå¸¸éœ€è¦é¢å¤–çš„æ•°æ®å’Œè®¿é—®åŸå§‹è®­ç»ƒæŒ‡æ ‡å’Œå‚æ•°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœåœ¨çº¿è§„åˆ’ç®—æ³•èƒ½å¤Ÿæ»¡è¶³é¢å¤–çš„è¦æ±‚ï¼Œå®ƒå¯ä»¥æ¶ˆé™¤å¯¹å¤§é‡è®­ç»ƒé˜¶æ®µçš„å¿…è¦ï¼Œå¹¶åœ¨ä¸äº†è§£åŸå§‹è®­ç»ƒæ–¹æ¡ˆæˆ–ä»»åŠ¡çš„æƒ…å†µä¸‹å®šåˆ¶ç­–ç•¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨è¿è¡Œæ—¶ä¸ºè¿ç»­æ§åˆ¶ç­–ç•¥å®šåˆ¶çš„é€šç”¨åœ¨çº¿è§„åˆ’ç®—æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºResidual-MPPIã€‚å®ƒå¯ä»¥åœ¨å°‘æ•°é•œå¤´ç”šè‡³é›¶é•œå¤´åœ¨çº¿è®¾ç½®ä¸‹ï¼Œä»…é€šè¿‡äº†è§£å…ˆå‰çš„è¡ŒåŠ¨åˆ†å¸ƒæ¥å®šåˆ¶æ–°çš„æ€§èƒ½æŒ‡æ ‡çš„ç»™å®šå…ˆå‰ç­–ç•¥ã€‚é€šè¿‡æˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„Residual-MPPIç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°å®Œæˆå°‘æ•°é•œå¤´&#x2F;é›¶é•œå¤´åœ¨çº¿ç­–ç•¥å®šåˆ¶ä»»åŠ¡ï¼ŒåŒ…æ‹¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„èµ›è½¦åœºæ™¯Gran Turismo Sportï¼ˆGTSï¼‰ç¯å¢ƒä¸­å®šåˆ¶å† å†›çº§èµ›è½¦ä»£ç†Gran Turismo Sophyï¼ˆGT Sophyï¼‰1.0ã€‚MuJoCoå®éªŒçš„ä»£ç åŒ…å«åœ¨è¡¥å……ææ–™ä¸­ï¼Œå¹¶åœ¨æ¥å—åå…¬å¼€æºä»£ç ã€‚æ¼”ç¤ºè§†é¢‘å’Œä»£ç å¯åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/residual-mppi%E3%80%82">https://sites.google.com/view/residual-mppiã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00898v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰çš„ç­–ç•¥åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ç°å®åº”ç”¨æ—¶å¾€å¾€éœ€è¦é€‚åº”æœªæ›¾é¢„è§çš„éœ€æ±‚ã€‚è™½ç„¶å¾®è°ƒå¯ä»¥è§£å†³è¿™äº›éœ€æ±‚ï¼Œä½†å®ƒé€šå¸¸éœ€è¦é¢å¤–çš„æ•°æ®å’ŒåŸå§‹è®­ç»ƒæŒ‡æ ‡å’Œå‚æ•°çš„è®¿é—®æƒé™ã€‚ç›¸åï¼Œå¦‚æœåœ¨çº¿è§„åˆ’ç®—æ³•èƒ½å¤Ÿæ»¡è¶³è¿™äº›é¢å¤–è¦æ±‚ï¼Œå®ƒå¯ä»¥çœå»æ¼«é•¿çš„è®­ç»ƒé˜¶æ®µï¼Œå¹¶åœ¨ä¸äº†è§£åŸå§‹è®­ç»ƒæ–¹æ¡ˆæˆ–ä»»åŠ¡çš„æƒ…å†µä¸‹å®šåˆ¶ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºä¸€ç§åœ¨çº¿è§„åˆ’ç®—æ³•ï¼Œåä¸ºResidual-MPPIï¼Œå¯åœ¨æ‰§è¡Œæ—¶é—´å®šåˆ¶è¿ç»­æ§åˆ¶ç­–ç•¥ï¼Œè¯¥ç®—æ³•å¯ä»¥åœ¨å°‘æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬åœ¨çº¿è®¾ç½®ä¸‹ï¼Œä»…é€šè¿‡å…ˆå‰è¡ŒåŠ¨åˆ†å¸ƒæ¥å®Œæˆå¯¹æ–°æ€§èƒ½æŒ‡æ ‡çš„å®šåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒResidual-MPPIç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°å®Œæˆåœ¨çº¿ç­–ç•¥å®šåˆ¶ä»»åŠ¡ï¼ŒåŒ…æ‹¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„èµ›è½¦åœºæ™¯Gran Turismo Sportï¼ˆGTSï¼‰ç¯å¢ƒä¸­å®šåˆ¶å† å†›çº§èµ›è½¦ä»£ç†Gran Turismo Sophyï¼ˆGT Sophyï¼‰ 1.0ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ çš„ç­–ç•¥åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>åœ¨ç°å®åº”ç”¨ä¸­ï¼Œéœ€è¦é€‚åº”è®­ç»ƒçš„ç­–ç•¥ä»¥æ»¡è¶³æœªé¢„è§çš„éœ€æ±‚ã€‚</li>
<li>ç›¸å¯¹äºå¾®è°ƒæ–¹æ³•ï¼Œåœ¨çº¿è§„åˆ’ç®—æ³•å¯ä»¥åœ¨ä¸äº†è§£åŸå§‹è®­ç»ƒæ–¹æ¡ˆæˆ–ä»»åŠ¡çš„æƒ…å†µä¸‹å®šåˆ¶ç­–ç•¥ï¼Œå¹¶çœå»å¤§é‡è®­ç»ƒæ—¶é—´ã€‚</li>
<li>Residual-MPPIæ˜¯ä¸€ç§åœ¨çº¿è§„åˆ’ç®—æ³•ï¼Œå¯åœ¨æ‰§è¡Œæ—¶é—´å¯¹è¿ç»­æ§åˆ¶ç­–ç•¥è¿›è¡Œå®šåˆ¶ã€‚</li>
<li>Residual-MPPIç®—æ³•åœ¨å°‘æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬åœ¨çº¿è®¾ç½®ä¸‹ï¼Œèƒ½å¤Ÿä»…é€šè¿‡å…ˆå‰è¡ŒåŠ¨åˆ†å¸ƒå®Œæˆå¯¹æ–°æ€§èƒ½æŒ‡æ ‡çš„å®šåˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜Residual-MPPIç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°å®Œæˆåœ¨çº¿ç­–ç•¥å®šåˆ¶ä»»åŠ¡ï¼ŒåŒ…æ‹¬åœ¨æŒ‘æˆ˜æ€§çš„èµ›è½¦åœºæ™¯ä¸­å®šåˆ¶é«˜çº§ä»£ç†ã€‚</li>
<li>ä»£ç å°†åœ¨æ¥å—åå¼€æºï¼Œæ¼”ç¤ºè§†é¢‘å’Œä»£ç å¯åœ¨ç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2407.00898v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2407.00898v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Few-Shot/2407.00898v5/page_3_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ff0097c597a4034c4aef4a18d8813f90.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Realization of a Pre-Sample Photonic-based Free-Electron Modulator in   Ultrafast Transmission Electron Microscopes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-16ac621e5356ed6f81ab09edccc21589.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  New Trends for Modern Machine Translation with Large Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
