<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Towards Scalable Foundation Model for Multi-modal and Hyperspectral   Geospatial Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ac915e3876c60b287ac49530c98cf191.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-18-æ›´æ–°"><a href="#2025-03-18-æ›´æ–°" class="headerlink" title="2025-03-18 æ›´æ–°"></a>2025-03-18 æ›´æ–°</h1><h2 id="Towards-Scalable-Foundation-Model-for-Multi-modal-and-Hyperspectral-Geospatial-Data"><a href="#Towards-Scalable-Foundation-Model-for-Multi-modal-and-Hyperspectral-Geospatial-Data" class="headerlink" title="Towards Scalable Foundation Model for Multi-modal and Hyperspectral   Geospatial Data"></a>Towards Scalable Foundation Model for Multi-modal and Hyperspectral   Geospatial Data</h2><p><strong>Authors:Haozhe Si, Yuxuan Wan, Minh Do, Deepak Vasisht, Han Zhao, Hendrik F. Hamann</strong></p>
<p>Geospatial raster (imagery) data, such as that collected by satellite-based imaging systems at different times and spectral bands, hold immense potential for enabling a wide range of high-impact applications. This potential stems from the rich information that is spatially and temporally contextualized across multiple channels and sensing modalities. Recent work has adapted existing self-supervised learning approaches for such geospatial data. However, they fall short of scalable model architectures, leading to inflexibility and computational inefficiencies when faced with an increasing number of channels and modalities. To address these limitations, we introduce Low-rank Efficient Spatial-Spectral Vision Transformer (LESS ViT) with three key innovations: i) the LESS Attention Block that approximates high-dimensional spatial-spectral attention through Kroneckerâ€™s product of the low-dimensional spatial and spectral attention components; ii) the Continuous Positional-Channel Embedding Layer that preserves both spatial and spectral continuity and physical characteristics of each patch; and iii) the Perception Field Mask that exploits local spatial dependencies by constraining attention to neighboring patches. To evaluate the proposed innovations, we construct a benchmark, GFM-Bench, which serves as a comprehensive benchmark for such geospatial raster data. We pretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with integrated positional and channel masking strategies. Experimental results demonstrate that our proposed method surpasses current state-of-the-art multi-modal geospatial foundation models, achieving superior performance with less computation and fewer parameters. The flexibility and extensibility of our framework make it a promising direction for future geospatial data analysis tasks that involve a wide range of modalities and channels. </p>
<blockquote>
<p>åœ°ç†ç©ºé—´æ …æ ¼ï¼ˆå½±åƒï¼‰æ•°æ®ï¼Œå¦‚ç”±åŸºäºå«æ˜Ÿçš„æˆåƒç³»ç»Ÿåœ¨ä¸åŒæ—¶é—´å’Œå…‰è°±æ³¢æ®µæ‰€æ”¶é›†çš„ï¼Œå¯¹äºå®ç°ä¸€ç³»åˆ—é«˜å½±å“åŠ›åº”ç”¨å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚è¿™ç§æ½œåŠ›æ¥æºäºå¤šä¸ªé€šé“å’Œæ„ŸçŸ¥æ¨¡å¼ä¹‹é—´åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šçš„ä¸°å¯Œä¿¡æ¯ä¸Šä¸‹æ–‡ã€‚è¿‘æœŸçš„å·¥ä½œå·²ç»å¯¹ç°æœ‰é’ˆå¯¹æ­¤ç±»åœ°ç†ç©ºé—´æ•°æ®çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†æ”¹è¿›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç¼ºä¹å¯æ‰©å±•çš„æ¨¡å‹æ¶æ„ï¼Œåœ¨é¢å¯¹æ—¥ç›Šå¢é•¿çš„é€šé“å’Œæ¨¡å¼æ•°é‡æ—¶ï¼Œä¼šå¯¼è‡´çµæ´»æ€§ä¸è¶³å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½ç§©é«˜æ•ˆç©ºé—´å…‰è°±è§†è§‰è½¬æ¢å™¨ï¼ˆLESS ViTï¼‰ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šä¸€ï¼‰LESSæ³¨æ„åŠ›å—é€šè¿‡å…‹ç½—å†…å…‹ä¹˜ç§¯è¿‘ä¼¼é«˜ç»´ç©ºé—´å…‰è°±æ³¨æ„åŠ›ï¼Œè¯¥ä¹˜ç§¯æ˜¯ä½ç»´ç©ºé—´å’Œå…‰è°±æ³¨æ„åŠ›ç»„ä»¶çš„ä¹˜ç§¯ï¼›äºŒï¼‰è¿ç»­ä½ç½®é€šé“åµŒå…¥å±‚ä¿ç•™äº†ç©ºé—´å…‰è°±çš„è¿ç»­æ€§ä»¥åŠæ¯ä¸ªè¡¥ä¸çš„ç‰©ç†ç‰¹å¾ï¼›ä¸‰ï¼‰æ„ŸçŸ¥åœºæ©è†œé€šè¿‡é™åˆ¶æ³¨æ„åŠ›åœ¨ç›¸é‚»è¡¥ä¸ä¸Šï¼Œåˆ©ç”¨å±€éƒ¨ç©ºé—´ä¾èµ–æ€§ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬æ„å»ºäº†åŸºå‡†æµ‹è¯•GFM-Benchï¼Œå®ƒæ˜¯æ­¤ç±»åœ°ç†ç©ºé—´æ …æ ¼æ•°æ®çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨å…·æœ‰é›†æˆä½ç½®å’Œé€šé“æ©è”½ç­–ç•¥çš„é«˜å…‰è°±æ©è”½è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶å¯¹LESS ViTè¿›è¡Œé¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¶…è¶Šäº†å½“å‰çš„å¤šæ¨¡æ€åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œåœ¨è¾ƒå°‘çš„è®¡ç®—å’Œå‚æ•°ä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¡†æ¶çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºæ¶‰åŠå¹¿æ³›æ¨¡å¼å’Œé€šé“çš„æœªæ¥åœ°ç†ç©ºé—´æ•°æ®åˆ†æä»»åŠ¡çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12843v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åœ°ç†ç©ºé—´æ …æ ¼ï¼ˆå½±åƒï¼‰æ•°æ®çš„æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šé€šé“å’Œå¤šæ¨¡æ€æ•°æ®æ—¶çš„ä¸çµæ´»æ€§å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæå‡ºLow-rank Efficient Spatial-Spectral Vision Transformerï¼ˆLESS ViTï¼‰ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šLESS Attention Blockã€Continuous Positional-Channel Embedding Layerå’ŒPerception Field Maskã€‚é€šè¿‡æ„å»ºGFM-BenchåŸºå‡†æµ‹è¯•é›†å’Œé‡‡ç”¨Hyperspectral Masked Autoencoderæ¡†æ¶è¿›è¡Œé¢„è®­ç»ƒï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºå½“å‰çš„å¤šæ¨¡æ€åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„æ€§èƒ½å’Œæ›´å°‘çš„è®¡ç®—åŠå‚æ•°éœ€æ±‚ã€‚è¯¥æ¡†æ¶å…·æœ‰çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºæ¶‰åŠå¤šç§æ¨¡æ€å’Œé€šé“çš„æœªæ¥åœ°ç†ç©ºé—´æ•°æ®åˆ†æä»»åŠ¡æä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°ç†ç©ºé—´æ …æ ¼æ•°æ®å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæ”¯æŒå¤šç§é«˜å½±å“åŠ›åº”ç”¨ã€‚</li>
<li>ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤šé€šé“å’Œå¤šæ¨¡æ€çš„åœ°ç†ç©ºé—´æ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„Low-rank Efficient Spatial-Spectral Vision Transformerï¼ˆLESS ViTï¼‰åŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šLESS Attention Blockã€Continuous Positional-Channel Embedding Layerå’ŒPerception Field Maskã€‚</li>
<li>GFM-BenchåŸºå‡†æµ‹è¯•é›†ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨Hyperspectral Masked Autoencoderæ¡†æ¶è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå½“å‰çš„å¤šæ¨¡æ€åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb774ffaf18e91725bf1b838fb82a2c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8f647b7c1a0b4821d4c61bd1df4a6f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04b887bbbec6e8b01a61e2473d0ef80d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5251540e6cb1895cea05027eb8859bb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Head-to-Tail-Towards-Balanced-Representation-in-Large-Vision-Language-Models-through-Adaptive-Data-Calibration"><a href="#From-Head-to-Tail-Towards-Balanced-Representation-in-Large-Vision-Language-Models-through-Adaptive-Data-Calibration" class="headerlink" title="From Head to Tail: Towards Balanced Representation in Large   Vision-Language Models through Adaptive Data Calibration"></a>From Head to Tail: Towards Balanced Representation in Large   Vision-Language Models through Adaptive Data Calibration</h2><p><strong>Authors:Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng</strong></p>
<p>Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an $\textbf{A}$daptive $\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$) and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç»“åˆè§†è§‰ç†è§£ä¸è¯­è¨€ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†æˆåŠŸï¼ŒLVLMçš„è®­ç»ƒæ•°æ®ä»ç„¶é¢ä¸´é•¿å°¾ï¼ˆLTï¼‰é—®é¢˜ï¼Œå³æ•°æ®åˆ†å¸ƒæåº¦ä¸å¹³è¡¡ã€‚ä¹‹å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨ä¼ ç»Ÿçš„VLMæ¶æ„ï¼Œä¾‹å¦‚CLIPæˆ–ViTï¼Œä»¥åŠç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚è¯†åˆ«å’Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œå¯¹äºLVLMï¼ˆä¾‹å¦‚LLaVAï¼‰å’Œæ›´ä¸€èˆ¬çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚è§†è§‰é—®ç­”å’Œè§†è§‰æ¨ç†ï¼‰çš„æ¢ç´¢ä»ç„¶ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹LVLMä¸­çš„LTé—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶ç¡®å®šäº†ä¸¤ä¸ªæ ¸å¿ƒåŸå› ï¼šå¤´éƒ¨æ¦‚å¿µçš„è¿‡åº¦è¡¨ç¤ºå’Œå°¾éƒ¨æ¦‚å¿µçš„è¡¨ç¤ºä¸è¶³ã€‚åŸºäºä¸Šè¿°è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”æ•°æ®ç»†åŒ–æ¡†æ¶ï¼ˆADRï¼‰ï¼Œå®ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ•°æ®å†å¹³è¡¡ï¼ˆDRï¼‰å’Œæ•°æ®åˆæˆï¼ˆDSï¼‰ã€‚åœ¨DRé˜¶æ®µï¼Œæˆ‘ä»¬æ ¹æ®å®ä½“åˆ†å¸ƒè‡ªé€‚åº”åœ°é‡æ–°å¹³è¡¡å†—ä½™æ•°æ®ï¼Œè€Œåœ¨DSé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œç¨€ç¼ºå›¾åƒæ¥è¡¥å……è¡¨ç¤ºä¸è¶³çš„éƒ¨åˆ†ã€‚é€šè¿‡åä¸€ä¸ªåŸºå‡†çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºçš„ADRæœ‰æ•ˆåœ°ç¼“è§£äº†è®­ç»ƒæ•°æ®ä¸­çš„é•¿å°¾é—®é¢˜ï¼Œåœ¨ä¸å¢åŠ è®­ç»ƒæ•°æ®é‡çš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹æé«˜äº†LLaVA 1.5çš„å¹³å‡æ€§èƒ½4.36%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12821v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ·±å…¥æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é¢ä¸´çš„Long-Tailï¼ˆLTï¼‰é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºå…¶ä¸¤å¤§æ ¸å¿ƒåŸå› ï¼šå¤´éƒ¨æ¦‚å¿µçš„è¿‡åº¦è¡¨ç¤ºå’Œå°¾éƒ¨æ¦‚å¿µçš„è¡¨ç¤ºä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”æ•°æ®ä¼˜åŒ–æ¡†æ¶ï¼ˆADRï¼‰ï¼ŒåŒ…æ‹¬æ•°æ®å†å¹³è¡¡ï¼ˆDRï¼‰å’Œæ•°æ®åˆæˆï¼ˆDSï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚é€šè¿‡å…¨é¢è¯„ä¼°ï¼ŒADRæ¡†æ¶æœ‰æ•ˆåœ°ç¼“è§£äº†è®­ç»ƒæ•°æ®ä¸­çš„é•¿å°¾é—®é¢˜ï¼Œæé«˜äº†LLaVA 1.5çš„å¹³å‡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsè™½åœ¨è§†è§‰ç†è§£å’Œè¯­è¨€ç”Ÿæˆç»“åˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è®­ç»ƒæ•°æ®ä»é¢ä¸´Long-Tailï¼ˆLTï¼‰é—®é¢˜ï¼Œå³æ•°æ®åˆ†å¸ƒæåº¦ä¸å‡è¡¡ã€‚</li>
<li>LTé—®é¢˜çš„ä¸¤å¤§æ ¸å¿ƒåŸå› æ˜¯å¤´éƒ¨æ¦‚å¿µçš„è¿‡åº¦è¡¨ç¤ºå’Œå°¾éƒ¨æ¦‚å¿µçš„è¡¨ç¤ºä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”æ•°æ®ä¼˜åŒ–æ¡†æ¶ï¼ˆADRï¼‰ï¼ŒåŒ…æ‹¬æ•°æ®å†å¹³è¡¡ï¼ˆDRï¼‰å’Œæ•°æ®åˆæˆï¼ˆDSï¼‰ä¸¤ä¸ªé˜¶æ®µï¼Œä»¥è§£å†³LTé—®é¢˜ã€‚</li>
<li>åœ¨DRé˜¶æ®µï¼Œæ ¹æ®å®ä½“åˆ†å¸ƒè‡ªé€‚åº”åœ°é‡æ–°å¹³è¡¡æ•°æ®ã€‚</li>
<li>åœ¨DSé˜¶æ®µï¼Œåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œç¨€ç¼ºå›¾åƒæ¥è¡¥å……è¡¨ç¤ºä¸è¶³çš„éƒ¨åˆ†ã€‚</li>
<li>é€šè¿‡å¯¹åä¸€ä¸ªåŸºå‡†çš„å…¨é¢è¯„ä¼°ï¼ŒADRæ¡†æ¶æœ‰æ•ˆåœ°ç¼“è§£äº†è®­ç»ƒæ•°æ®ä¸­çš„é•¿å°¾é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5ef2a5932c0a7a6429fc5c83f71e77f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f91ebc8f654dff73331136090f401c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b73bf18f8ffd9cdbf8a8688234a1e2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e4ba0692f50c329b5560452c3ae0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f0a1d88fb45e2de2f241a352008db2a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LangDA-Building-Context-Awareness-via-Language-for-Domain-Adaptive-Semantic-Segmentation"><a href="#LangDA-Building-Context-Awareness-via-Language-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="LangDA: Building Context-Awareness via Language for Domain Adaptive   Semantic Segmentation"></a>LangDA: Building Context-Awareness via Language for Domain Adaptive   Semantic Segmentation</h2><p><strong>Authors:Chang Liu, Bavesh Balaji, Saad Hossain, C Thomas, Kwei-Herng Lai, Raviteja Vemulapalli, Alexander Wong, Sirisha Rambhatla</strong></p>
<p>Unsupervised domain adaptation for semantic segmentation (DASS) aims to transfer knowledge from a label-rich source domain to a target domain with no labels. Two key approaches in DASS are (1) vision-only approaches using masking or multi-resolution crops, and (2) language-based approaches that use generic class-wise prompts informed by target domain (e.g. â€œa {snowy} photo of a {class}â€). However, the former is susceptible to noisy pseudo-labels that are biased to the source domain. The latter does not fully capture the intricate spatial relationships of objects â€“ key for dense prediction tasks. To this end, we propose LangDA. LangDA addresses these challenges by, first, learning contextual relationships between objects via VLM-generated scene descriptions (e.g. â€œa pedestrian is on the sidewalk, and the street is lined with buildings.â€). Second, LangDA aligns the entire image features with text representation of this context-aware scene caption and learns generalized representations via text. With this, LangDA sets the new state-of-the-art across three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and 3.9%. </p>
<blockquote>
<p>æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ï¼ˆDASSï¼‰æ—¨åœ¨å°†ä»æ ‡ç­¾ä¸°å¯Œçš„æºåŸŸè½¬ç§»çŸ¥è¯†åˆ°æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸã€‚DASSä¸­çš„ä¸¤ç§ä¸»è¦æ–¹æ³•æ˜¯ï¼ˆ1ï¼‰ä»…ä½¿ç”¨è§†è§‰çš„æ–¹æ³•ï¼Œé€šè¿‡é®æŒ¡æˆ–å¤šåˆ†è¾¨ç‡è£å‰ªï¼Œä»¥åŠï¼ˆ2ï¼‰åŸºäºè¯­è¨€çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”±ç›®æ ‡åŸŸæŒ‡å¯¼çš„é€šç”¨ç±»åˆ«æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œä¸€ä¸ª{é›ªè¦†ç›–çš„}ç…§ç‰‡ä¸­çš„{ç±»åˆ«}â€ï¼‰ã€‚ç„¶è€Œï¼Œå‰è€…å®¹æ˜“å—åˆ°åå‘æºåŸŸçš„å™ªå£°ä¼ªæ ‡ç­¾çš„å½±å“ã€‚åè€…åˆ™æ²¡æœ‰å®Œå…¨æ•æ‰åˆ°ç‰©ä½“çš„å¤æ‚ç©ºé—´å…³ç³»ï¼Œè¿™å¯¹äºå¯†é›†é¢„æµ‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LangDAã€‚LangDAé€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šé¦–å…ˆï¼Œé€šè¿‡VLMç”Ÿæˆçš„åœºæ™¯æè¿°å­¦ä¹ ç‰©ä½“ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œè¡Œäººåœ¨äººè¡Œé“ä¸Šï¼Œè¡—é“ä¸¤æ—æ˜¯å»ºç­‘ç‰©ã€‚â€ï¼‰ï¼›å…¶æ¬¡ï¼ŒLangDAå°†æ•´å¹…å›¾åƒçš„ç‰¹å¾ä¸è¿™ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åœºæ™¯å­—å¹•çš„æ–‡æœ¬è¡¨ç¤ºå¯¹é½ï¼Œå¹¶é€šè¿‡æ–‡æœ¬å­¦ä¹ é€šç”¨è¡¨ç¤ºã€‚å› æ­¤ï¼ŒLangDAåœ¨ä¸‰ä¸ªDASSåŸºå‡†æµ‹è¯•ä¸­åˆ›ä¸‹äº†æ–°çš„æœ€é«˜çºªå½•ï¼Œè¾ƒç°æœ‰æ–¹æ³•æé«˜äº†2.6%ã€1.4%å’Œ3.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12780v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†æ— ç›‘ç£åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ï¼ˆDASSï¼‰çš„ä»»åŠ¡æ˜¯å°†ä¸°å¯Œçš„æ ‡ç­¾æºåŸŸçŸ¥è¯†è¿ç§»åˆ°æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸä¸­ã€‚æ–‡ä¸­æå‡ºä¸¤ç§å…³é”®æ–¹æ³•ï¼šä»…è§†è§‰çš„æ–¹æ³•å’ŒåŸºäºè¯­è¨€çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œä»…è§†è§‰çš„æ–¹æ³•æ˜“å—æºåŸŸå™ªå£°ä¼ªæ ‡ç­¾çš„å½±å“ï¼Œè€ŒåŸºäºè¯­è¨€çš„æ–¹æ³•æ— æ³•å®Œå…¨æ•æ‰å¯¹è±¡çš„å¤æ‚ç©ºé—´å…³ç³»ï¼Œè¿™å¯¹å¯†é›†é¢„æµ‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LangDAæ–¹æ³•ã€‚å®ƒé€šè¿‡åˆ©ç”¨VLMç”Ÿæˆçš„åœºæ™¯æè¿°æ¥å­¦ä¹ å¯¹è±¡ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼Œå¹¶é€šè¿‡ä¸æ•´ä¸ªå›¾åƒç‰¹å¾çš„æ–‡æœ¬è¡¨ç¤ºå¯¹é½æ¥å­¦ä¹ é€šç”¨è¡¨ç¤ºã€‚å› æ­¤ï¼ŒLangDAåœ¨ä¸‰ä¸ªDASSåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåˆ†åˆ«æ¯”ç°æœ‰æ–¹æ³•é«˜å‡º2.6%ã€1.4%å’Œ3.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DASSæ—¨åœ¨ä»æ ‡ç­¾ä¸°å¯Œçš„æºåŸŸè¿ç§»åˆ°æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸã€‚</li>
<li>å½“å‰ä¸¤ç§æ–¹æ³•â€”â€”è§†è§‰æ–¹æ³•å’ŒåŸºäºè¯­è¨€çš„æ–¹æ³•â€”â€”å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è§†è§‰æ–¹æ³•æ˜“å—æºåŸŸå™ªå£°ä¼ªæ ‡ç­¾å½±å“ã€‚</li>
<li>åŸºäºè¯­è¨€çš„æ–¹æ³•æ— æ³•å®Œå…¨æ•æ‰å¯¹è±¡çš„å¤æ‚ç©ºé—´å…³ç³»ã€‚</li>
<li>LangDAæ–¹æ³•é€šè¿‡å­¦ä¹ å¯¹è±¡é—´çš„ä¸Šä¸‹æ–‡å…³ç³»æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>LangDAåˆ©ç”¨VLMç”Ÿæˆçš„åœºæ™¯æè¿°ä¸å›¾åƒç‰¹å¾å¯¹é½æ¥å­¦ä¹ é€šç”¨è¡¨ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aab16040eeaac4b996bbd20110d477d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de99814e308d18fc7dbd25e40af5f5bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d4626f0e1eaeb9ed5feafde9ce39536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6164ae8ef17f158281afd49f2ab28055.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0c287d2e8d7269fcdae729628e74211.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4370b435d4781fc032656305414f4a7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43fea0b7b91b0afe9f37dddfcdcf5537.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Scaling-Semantic-Categories-Investigating-the-Impact-on-Vision-Transformer-Labeling-Performance"><a href="#Scaling-Semantic-Categories-Investigating-the-Impact-on-Vision-Transformer-Labeling-Performance" class="headerlink" title="Scaling Semantic Categories: Investigating the Impact on Vision   Transformer Labeling Performance"></a>Scaling Semantic Categories: Investigating the Impact on Vision   Transformer Labeling Performance</h2><p><strong>Authors:Anthony Lamelas, Harrison Muchnic</strong></p>
<p>This study explores the impact of scaling semantic categories on the image classification performance of vision transformers (ViTs). In this specific case, the CLIP server provided by Jina AI is used for experimentation. The research hypothesizes that as the number of ground truth and artificially introduced semantically equivalent categories increases, the labeling accuracy of ViTs improves until a theoretical maximum or limit is reached. A wide variety of image datasets were chosen to test this hypothesis. These datasets were processed through a custom function in Python designed to evaluate the modelâ€™s accuracy, with adjustments being made to account for format differences between datasets. By exponentially introducing new redundant categories, the experiment assessed accuracy trends until they plateaued, decreased, or fluctuated inconsistently. The findings show that while semantic scaling initially increases model performance, the benefits diminish or reverse after surpassing a critical threshold, providing insight into the limitations and possible optimization of category labeling strategies for ViTs. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©å±•è¯­ä¹‰ç±»åˆ«å¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰å›¾åƒåˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚åœ¨è¿™ä¸ªç‰¹å®šæ¡ˆä¾‹ä¸­ï¼Œå®éªŒä½¿ç”¨çš„æ˜¯Jina AIæä¾›çš„CLIPæœåŠ¡å™¨ã€‚ç ”ç©¶å‡è®¾éšç€çœŸå®å’Œäººä¸ºå¼•å…¥çš„è¯­ä¹‰ç­‰ä»·ç±»åˆ«çš„æ•°é‡å¢åŠ ï¼ŒViTsçš„æ ‡ç­¾ç²¾åº¦ä¼šæé«˜ï¼Œç›´åˆ°è¾¾åˆ°ç†è®ºä¸Šçš„æœ€å¤§å€¼æˆ–æé™ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸ªå‡è®¾ï¼Œé€‰æ‹©äº†å„ç§å„æ ·çš„å›¾åƒæ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†é€šè¿‡Pythonä¸­çš„è‡ªå®šä¹‰å‡½æ•°è¿›è¡Œå¤„ç†ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¹¶å¯¹æ•°æ®é›†æ ¼å¼å·®å¼‚è¿›è¡Œäº†è°ƒæ•´ã€‚é€šè¿‡æŒ‡æ•°æ–¹å¼å¼•å…¥æ–°çš„å†—ä½™ç±»åˆ«ï¼Œå®éªŒè¯„ä¼°äº†å‡†ç¡®æ€§è¶‹åŠ¿ï¼Œç›´åˆ°å®ƒä»¬è¾¾åˆ°å¹³ç¨³çŠ¶æ€ã€ä¸‹é™æˆ–æ³¢åŠ¨ä¸ä¸€è‡´ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¯­ä¹‰ç¼©æ”¾æœ€åˆæé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨è¶…è¿‡ä¸´ç•Œé˜ˆå€¼åï¼Œè¿™äº›å¥½å¤„ä¼šå‡å°‘æˆ–æ¶ˆå¤±ï¼Œè¿™ä¸ºç†è§£ViTsç±»åˆ«æ ‡ç­¾ç­–ç•¥çš„é™åˆ¶å’Œå¯èƒ½çš„ä¼˜åŒ–æä¾›äº†æ´è§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12617v1">PDF</a> 4 pages, 7 figures, submitted to CVPR (feedback pending)</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©å±•è¯­ä¹‰ç±»åˆ«å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å›¾åƒåˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‡è®¾éšç€åœ°é¢çœŸå®å’Œäººä¸ºå¼•å…¥çš„è¯­ä¹‰ä¸Šç­‰ä»·ç±»åˆ«çš„æ•°é‡å¢åŠ ï¼ŒViTsçš„æ ‡ç­¾ç²¾åº¦ä¼šæé«˜ï¼Œç›´åˆ°è¾¾åˆ°ç†è®ºä¸Šçš„æœ€å¤§å€¼æˆ–é™åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶è¯­ä¹‰ç¼©æ”¾æœ€åˆèƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨è¶…è¿‡æŸä¸ªä¸´ç•Œé˜ˆå€¼åï¼Œå…¶ä¼˜åŠ¿ä¼šå‡å¼±æˆ–é€†è½¬ï¼Œè¿™ä¸ºViTsç±»åˆ«æ ‡æ³¨ç­–ç•¥çš„å±€é™æ€§å’Œå¯èƒ½ä¼˜åŒ–æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†è¯­ä¹‰ç±»åˆ«è§„æ¨¡å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å›¾åƒåˆ†ç±»æ€§èƒ½çš„å½±å“ã€‚</li>
<li>å®éªŒä½¿ç”¨äº†Jina AIæä¾›çš„CLIPæœåŠ¡å™¨ã€‚</li>
<li>ç ”ç©¶å‡è®¾éšç€è¯­ä¹‰ç­‰ä»·ç±»åˆ«çš„å¢åŠ ï¼ŒViTsçš„æ ‡ç­¾ç²¾åº¦ä¼šæé«˜ï¼Œç›´è‡³è¾¾åˆ°æŸä¸€ç†è®ºæé™ã€‚</li>
<li>ä½¿ç”¨äº†å¤šç§å›¾åƒæ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼Œå¹¶é€šè¿‡è‡ªå®šä¹‰çš„Pythonå‡½æ•°æ¥è¯„ä¼°æ¨¡å‹ç²¾åº¦ã€‚</li>
<li>å®éªŒé€šè¿‡å¼•å…¥æ–°çš„å†—ä½™ç±»åˆ«æ¥è¯„ä¼°ç²¾åº¦è¶‹åŠ¿ï¼Œç›´è‡³å…¶è¾¾åˆ°å¹³ç¨³ã€ä¸‹é™æˆ–æ³¢åŠ¨ä¸ä¸€è‡´çš„çŠ¶æ€ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè¯­ä¹‰ç¼©æ”¾è™½ç„¶æœ€åˆèƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†è¶…è¿‡æŸä¸ªé˜ˆå€¼åæ•ˆæœå‡å¼±æˆ–åè½¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb090a17190e61c2945d1c80cf1fb41f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aef00b2233bf0123886548bdcd05d9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5fbdc0a822e43b7545fd408194122c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a17ef2fed6677152a583eb50c9bf194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7226ae0629ff70cdd4ad35dbe4bb990.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="APLA-A-Simple-Adaptation-Method-for-Vision-Transformers"><a href="#APLA-A-Simple-Adaptation-Method-for-Vision-Transformers" class="headerlink" title="APLA: A Simple Adaptation Method for Vision Transformers"></a>APLA: A Simple Adaptation Method for Vision Transformers</h2><p><strong>Authors:Moein Sorkhei, Emir Konuk, Kevin Smith, Christos Matsoukas</strong></p>
<p>Existing adaptation techniques typically require architectural modifications or added parameters, leading to high computational costs and complexity. We introduce Attention Projection Layer Adaptation (APLA), a simple approach to adapt vision transformers (ViTs) without altering the architecture or adding parameters. Through a systematic analysis, we find that the layer immediately after the attention mechanism is crucial for adaptation. By updating only this projection layer, or even just a random subset of this layerâ€™s weights, APLA achieves state-of-the-art performance while reducing GPU memory usage by up to 52.63% and training time by up to 43.0%, with no extra cost at inference. Across 46 datasets covering a variety of tasks including scene classification, medical imaging, satellite imaging, and fine-grained classification, APLA consistently outperforms 17 other leading adaptation methods, including full fine-tuning, on classification, segmentation, and detection tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MoeinSorkhei/APLA">https://github.com/MoeinSorkhei/APLA</a>. </p>
<blockquote>
<p>ç°æœ‰çš„è‡ªé€‚åº”æŠ€æœ¯é€šå¸¸éœ€è¦ä¿®æ”¹æ¶æ„æˆ–å¢åŠ å‚æ•°ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜ä¸”å¤æ‚æ€§å¢åŠ ã€‚æˆ‘ä»¬å¼•å…¥äº†æ³¨æ„åŠ›æŠ•å½±å±‚è‡ªé€‚åº”ï¼ˆAPLAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ”¹å˜æ¶æ„æˆ–å¢åŠ å‚æ•°å³å¯é€‚åº”è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„ç®€å•æ–¹æ³•ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬å‘ç°æ³¨æ„åŠ›æœºåˆ¶ä¹‹åçš„é‚£ä¸€å±‚å¯¹äºè‡ªé€‚åº”è‡³å…³é‡è¦ã€‚é€šè¿‡ä»…æ›´æ–°æ­¤æŠ•å½±å±‚ï¼Œç”šè‡³åªæ˜¯æ›´æ–°è¯¥å±‚æƒé‡çš„éšæœºå­é›†ï¼ŒAPLAåœ¨é™ä½GPUå†…å­˜ä½¿ç”¨ç‡é«˜è¾¾52.63%å’Œè®­ç»ƒæ—¶é—´é«˜è¾¾43.0%çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”æ¨ç†é˜¶æ®µæ— éœ€é¢å¤–æˆæœ¬ã€‚åœ¨æ¶µç›–åœºæ™¯åˆ†ç±»ã€åŒ»å­¦å½±åƒã€å«æ˜Ÿæˆåƒå’Œç»†ç²’åº¦åˆ†ç±»ç­‰ä»»åŠ¡çš„46ä¸ªæ•°æ®é›†ä¸Šï¼ŒAPLAåœ¨åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºåŒ…æ‹¬å®Œå…¨å¾®è°ƒåœ¨å†…çš„å…¶ä»–17ç§ä¸»æµè‡ªé€‚åº”æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/MoeinSorkhei/APLA%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/MoeinSorkhei/APLAè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11335v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAttention Projection Layer Adaptationï¼ˆAPLAï¼‰çš„æ–¹æ³•ï¼Œå¯åœ¨ä¸æ”¹å˜æ¶æ„æˆ–å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹é€‚åº”è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ã€‚é€šè¿‡æ›´æ–°æ³¨æ„åŠ›æœºåˆ¶åçš„å…³é”®å±‚æˆ–è¯¥å±‚æƒé‡çš„éšæœºå­é›†ï¼ŒAPLAå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†GPUå†…å­˜ä½¿ç”¨ç‡å’Œè®­ç»ƒæ—¶é—´ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒAPLAåœ¨åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºå…¶ä»–17ç§ä¸»æµé€‚åº”æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>APLAæ˜¯ä¸€ç§é€‚åº”è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„æ–°æ–¹æ³•ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–å¢åŠ å‚æ•°ã€‚</li>
<li>é€šè¿‡å¯¹æ³¨æ„åŠ›æœºåˆ¶åçš„å…³é”®å±‚è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œå‘ç°è¯¥å±‚å¯¹äºé€‚åº”éå¸¸é‡è¦ã€‚</li>
<li>é€šè¿‡æ›´æ–°æ­¤å…³é”®æŠ•å½±å±‚æˆ–å…¶éƒ¨åˆ†æƒé‡ï¼ŒAPLAå¯å®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>APLAæ˜¾è‘—é™ä½äº†GPUå†…å­˜ä½¿ç”¨ç‡å’Œè®­ç»ƒæ—¶é—´ã€‚</li>
<li>APLAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºå…¶ä»–ä¸»æµé€‚åº”æ–¹æ³•ã€‚</li>
<li>APLAçš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-047f7cceb27e06d81fdaa22b95612326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea7108820f671bc0860b8cc37dc7461d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4539b129ebaa1457839527c8614859cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d839673fa04edbe0a58fcee79e522331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac0cf94e8f6e282dbb86327a6551b107.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ChatGPT-Encounters-Morphing-Attack-Detection-Zero-Shot-MAD-with-Multi-Modal-Large-Language-Models-and-General-Vision-Models"><a href="#ChatGPT-Encounters-Morphing-Attack-Detection-Zero-Shot-MAD-with-Multi-Modal-Large-Language-Models-and-General-Vision-Models" class="headerlink" title="ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with   Multi-Modal Large Language Models and General Vision Models"></a>ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with   Multi-Modal Large Language Models and General Vision Models</h2><p><strong>Authors:Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, Christoph Busch</strong></p>
<p>Face Recognition Systems (FRS) are increasingly vulnerable to face-morphing attacks, prompting the development of Morphing Attack Detection (MAD) algorithms. However, a key challenge in MAD lies in its limited generalizability to unseen data and its lack of explainability-critical for practical application environments such as enrolment stations and automated border control systems. Recognizing that most existing MAD algorithms rely on supervised learning paradigms, this work explores a novel approach to MAD using zero-shot learning leveraged on Large Language Models (LLMs). We propose two types of zero-shot MAD algorithms: one leveraging general vision models and the other utilizing multimodal LLMs. For general vision models, we address the MAD task by computing the mean support embedding of an independent support set without using morphed images. For the LLM-based approach, we employ the state-of-the-art GPT-4 Turbo API with carefully crafted prompts. To evaluate the feasibility of zero-shot MAD and the effectiveness of the proposed methods, we constructed a print-scan morph dataset featuring various unseen morphing algorithms, simulating challenging real-world application scenarios. Experimental results demonstrated notable detection accuracy, validating the applicability of zero-shot learning for MAD tasks. Additionally, our investigation into LLM-based MAD revealed that multimodal LLMs, such as ChatGPT, exhibit remarkable generalizability to untrained MAD tasks. Furthermore, they possess a unique ability to provide explanations and guidance, which can enhance transparency and usability for end-users in practical applications. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ç³»ç»Ÿï¼ˆFRSï¼‰è¶Šæ¥è¶Šå®¹æ˜“å—åˆ°äººè„¸å˜å½¢æ”»å‡»çš„å½±å“ï¼Œè¿™ä¿ƒä½¿äº†å˜å½¢æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰ç®—æ³•çš„å‘å±•ã€‚ç„¶è€Œï¼ŒMADé¢ä¸´çš„å…³é”®æŒ‘æˆ˜åœ¨äºå…¶æœ‰é™çš„æœªè§æ•°æ®æ³›åŒ–èƒ½åŠ›å’Œç¼ºä¹è§£é‡Šæ€§ï¼Œè¿™åœ¨å…¥å­¦æ³¨å†Œç«™å’Œè‡ªåŠ¨åŒ–è¾¹å¢ƒæ§åˆ¶ç³»ç»Ÿç­‰å®é™…åº”ç”¨ç¯å¢ƒä¸­è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10937v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººè„¸è¯†åˆ«ç³»ç»Ÿï¼ˆFRSï¼‰è¶Šæ¥è¶Šå®¹æ˜“å—åˆ°äººè„¸è¯†åˆ«ç¯¡æ”¹æ”»å‡»çš„å½±å“ï¼Œå› æ­¤å‡ºç°äº†ç¯¡æ”¹æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰ç®—æ³•çš„å‘å±•ã€‚ç„¶è€Œï¼ŒMADé¢ä¸´çš„å…³é”®æŒ‘æˆ˜åœ¨äºå…¶å¯¹æ–°æ•°æ®çš„æœ‰é™æ³›åŒ–èƒ½åŠ›ä»¥åŠç¼ºä¹è§£é‡Šæ€§ï¼Œè¿™åœ¨å…¥ç«™ç«™å’Œè‡ªåŠ¨åŒ–è¾¹å¢ƒæ§åˆ¶ç³»ç»Ÿç­‰å®é™…åº”ç”¨ç¯å¢ƒä¸­è‡³å…³é‡è¦ã€‚é‰´äºå¤§å¤šæ•°ç°æœ‰MADç®—æ³•éƒ½ä¾èµ–äºç›‘ç£å­¦ä¹ æ¨¡å¼ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬å­¦ä¹ æ¥è¿›è¡ŒMADçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§é›¶æ ·æœ¬MADç®—æ³•ï¼šä¸€ç§åˆ©ç”¨é€šç”¨è§†è§‰æ¨¡å‹ï¼Œå¦ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€LLMã€‚å¯¹äºé€šç”¨è§†è§‰æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—ç‹¬ç«‹æ”¯æŒé›†çš„å‡å€¼æ”¯æŒåµŒå…¥æ¥è§£å†³MADä»»åŠ¡ï¼Œè€Œæ— éœ€ä½¿ç”¨ç»è¿‡ä¿®æ”¹çš„å›¾åƒã€‚å¯¹äºåŸºäºLLMçš„æ–¹æ³•ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€å…ˆè¿›çš„GPT-4 Turbo APIå¹¶ç²¾å¿ƒè®¾è®¡äº†æç¤ºã€‚ä¸ºäº†è¯„ä¼°é›¶æ ·æœ¬MADçš„å¯è¡Œæ€§å’Œæ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«å„ç§æœªè§è¿‡çš„ç¯¡æ”¹ç®—æ³•çš„æ‰“å°æ‰«æå½¢æ€æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿå…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•Œåº”ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜æ£€æµ‹ç²¾åº¦æ˜¾è‘—ï¼ŒéªŒè¯äº†é›¶æ ·æœ¬å­¦ä¹ åœ¨MADä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹åŸºäºLLMçš„MADçš„ç ”ç©¶è¡¨æ˜ï¼Œå¦‚ChatGPTç­‰å¤šæ¨¡æ€LLMåœ¨æœªç»è®­ç»ƒçš„MADä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„å¯æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å…·æœ‰å¯¹æœªè®­ç»ƒç”¨æˆ·æä¾›è§£é‡Šå’ŒæŒ‡å¯¼çš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œè¿™å¯ä»¥å¢å¼ºå®é™…åº”ç”¨ä¸­çš„é€æ˜åº¦å’Œç”¨æˆ·å‹å¥½æ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«ç³»ç»Ÿï¼ˆFRSï¼‰é¢ä¸´äººè„¸è¯†åˆ«ç¯¡æ”¹æ”»å‡»çš„é£é™©ï¼Œéœ€è¦å‘å±•ç¯¡æ”¹æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰ç®—æ³•ã€‚</li>
<li>MADç®—æ³•é¢ä¸´å¯¹æ–°æ•°æ®æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬å­¦ä¹ æ¥è¿›è¡ŒMADçš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨é€šç”¨è§†è§‰æ¨¡å‹å’Œå¤šæ¨¡æ€LLMå®ç°äº†ä¸¤ç§é›¶æ ·æœ¬MADç®—æ³•ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†é›¶æ ·æœ¬å­¦ä¹ åœ¨MADä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¤šæ¨¡æ€LLMå¦‚ChatGPTåœ¨æœªç»è®­ç»ƒçš„MADä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LLMså…·æœ‰ä¸ºç”¨æˆ·æä¾›è§£é‡Šå’ŒæŒ‡å¯¼çš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œå¢å¼ºå®é™…åº”ç”¨ä¸­çš„é€æ˜åº¦å’Œç”¨æˆ·å‹å¥½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b1862bd2ff514a07c63406dafe8c71e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec86cdb6d80085054128327967c72c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b187e68033a45f1b3fdeedadac4eb76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea6fea4cfcd52470cf84b3d4ae9dc56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccaef5bf8d0da14a152c68ed29fa3e8d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Team-NYCU-at-Defactify4-Robust-Detection-and-Source-Identification-of-AI-Generated-Images-Using-CNN-and-CLIP-Based-Models"><a href="#Team-NYCU-at-Defactify4-Robust-Detection-and-Source-Identification-of-AI-Generated-Images-Using-CNN-and-CLIP-Based-Models" class="headerlink" title="Team NYCU at Defactify4: Robust Detection and Source Identification of   AI-Generated Images Using CNN and CLIP-Based Models"></a>Team NYCU at Defactify4: Robust Detection and Source Identification of   AI-Generated Images Using CNN and CLIP-Based Models</h2><p><strong>Authors:Tsan-Tsung Yang, I-Wei Chen, Kuan-Ting Chen, Shang-Hsuan Chiang, Wen-Chih Peng</strong></p>
<p>With the rapid advancement of generative AI, AI-generated images have become increasingly realistic, raising concerns about creativity, misinformation, and content authenticity. Detecting such images and identifying their source models has become a critical challenge in ensuring the integrity of digital media. This paper tackles the detection of AI-generated images and identifying their source models using CNN and CLIP-ViT classifiers. For the CNN-based classifier, we leverage EfficientNet-B0 as the backbone and feed with RGB channels, frequency features, and reconstruction errors, while for CLIP-ViT, we adopt a pretrained CLIP image encoder to extract image features and SVM to perform classification. Evaluated on the Defactify 4 dataset, our methods demonstrate strong performance in both tasks, with CLIP-ViT showing superior robustness to image perturbations. Compared to baselines like AEROBLADE and OCC-CLIP, our approach achieves competitive results. Notably, our method ranked Top-3 overall in the Defactify 4 competition, highlighting its effectiveness and generalizability. All of our implementations can be found in <a target="_blank" rel="noopener" href="https://github.com/uuugaga/Defactify_4">https://github.com/uuugaga/Defactify_4</a> </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¼AIçš„è¿…é€Ÿå‘å±•ï¼ŒAIç”Ÿæˆçš„å›¾åƒè¶Šæ¥è¶Šé€¼çœŸï¼Œå¼•å‘äº†å…³äºåˆ›é€ åŠ›ã€è¯¯å¯¼ä¿¡æ¯å’Œå†…å®¹çœŸå®æ€§çš„æ‹…å¿§ã€‚æ£€æµ‹æ­¤ç±»å›¾åƒå¹¶è¯†åˆ«å…¶æºæ¨¡å‹ï¼Œå·²æˆä¸ºç¡®ä¿æ•°å­—åª’ä½“å®Œæ•´æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡é‡‡ç”¨CNNå’ŒCLIP-ViTåˆ†ç±»å™¨æ£€æµ‹AIç”Ÿæˆçš„å›¾åƒå¹¶è¯†åˆ«å…¶æºæ¨¡å‹ã€‚å¯¹äºåŸºäºCNNçš„åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬åˆ©ç”¨EfficientNet-B0ä½œä¸ºéª¨å¹²ç½‘ï¼Œå¹¶è¾“å…¥RGBé€šé“ã€é¢‘ç‡ç‰¹å¾å’Œé‡å»ºè¯¯å·®ï¼›è€Œå¯¹äºCLIP-ViTï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„CLIPå›¾åƒç¼–ç å™¨æå–å›¾åƒç‰¹å¾ï¼Œå¹¶ä½¿ç”¨SVMè¿›è¡Œåˆ†ç±»ã€‚åœ¨Defactify 4æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒCLIP-ViTå¯¹å›¾åƒæ‰°åŠ¨è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§ã€‚ä¸AEROBLADEå’ŒOCC-CLIPç­‰åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Defactify 4ç«èµ›ä¸­ååˆ—å‰èŒ…ï¼Œå‡¸æ˜¾äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚æ‰€æœ‰å®ç°å‡å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/uuugaga/Defactify_4%E3%80%82">https://github.com/uuugaga/Defactify_4ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10718v1">PDF</a> </p>
<p><strong>Summary</strong><br>AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸æ¨¡å‹è¯†åˆ«æˆä¸ºæ•°å­—åª’ä½“å®Œæ•´æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡ä½¿ç”¨CNNå’ŒCLIP-ViTåˆ†ç±»å™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡EfficientNet-B0ä½œä¸ºCNNåˆ†ç±»å™¨çš„éª¨å¹²å¹¶ç»“åˆRGBé€šé“ã€é¢‘ç‡ç‰¹å¾å’Œé‡å»ºè¯¯å·®æ¥æ£€æµ‹AIç”Ÿæˆçš„å›¾åƒï¼›é‡‡ç”¨é¢„è®­ç»ƒçš„CLIPå›¾åƒç¼–ç å™¨æå–å›¾åƒç‰¹å¾å¹¶ä½¿ç”¨SVMè¿›è¡Œæ¨¡å‹è¯†åˆ«ã€‚åœ¨Defactify 4æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œè¡¨ç°ä¼˜ç§€ï¼Œæºç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIç”Ÿæˆçš„å›¾åƒæé«˜äº†æ£€æµ‹å…¶çœŸå®æ€§çš„æŒ‘æˆ˜ï¼Œéœ€å…³æ³¨æ•°å­—åª’ä½“å®Œæ•´æ€§ã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨CNNå’ŒCLIP-ViTåˆ†ç±»å™¨è§£å†³AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸æ¨¡å‹è¯†åˆ«é—®é¢˜ã€‚</li>
<li>EfficientNet-B0ä½œä¸ºCNNåˆ†ç±»å™¨çš„éª¨å¹²ï¼Œç»“åˆå¤šç§ç‰¹å¾è¿›è¡Œæ£€æµ‹ã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒçš„CLIPå›¾åƒç¼–ç å™¨æå–å›¾åƒç‰¹å¾ï¼Œå¹¶ä½¿ç”¨SVMè¿›è¡Œåˆ†ç±»ã€‚</li>
<li>åœ¨Defactify 4æ•°æ®é›†ä¸Šè¯„ä¼°è¡¨ç°ä¼˜ç§€ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ–¹æ³•åœ¨Defactify 4ç«èµ›ä¸­ä½åˆ—å‰ä¸‰ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e213e1d597036903f7d7c425b84de60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-249a89a65b19dae4c5a6d245cca944c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e169d37500c1f2f7dc725eec2cd94b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e84ba8cdc11e89ab77bb933ccac6fb37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb64ea2bef4475c4fc68f469635d397.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e48fc906393c65584c488ab8c8bdb82.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Treble-Counterfactual-VLMs-A-Causal-Approach-to-Hallucination"><a href="#Treble-Counterfactual-VLMs-A-Causal-Approach-to-Hallucination" class="headerlink" title="Treble Counterfactual VLMs: A Causal Approach to Hallucination"></a>Treble Counterfactual VLMs: A Causal Approach to Hallucination</h2><p><strong>Authors:Shawn Li, Jiashu Qu, Yuxiao Zhou, Yuehan Qin, Tiankai Yang, Yue Zhao</strong></p>
<p>Vision-Language Models (VLMs) have advanced multi-modal tasks like image captioning, visual question answering, and reasoning. However, they often generate hallucinated outputs inconsistent with the visual context or prompt, limiting reliability in critical applications like autonomous driving and medical imaging. Existing studies link hallucination to statistical biases, language priors, and biased feature learning but lack a structured causal understanding. In this work, we introduce a causal perspective to analyze and mitigate hallucination in VLMs. We hypothesize that hallucination arises from unintended direct influences of either the vision or text modality, bypassing proper multi-modal fusion. To address this, we construct a causal graph for VLMs and employ counterfactual analysis to estimate the Natural Direct Effect (NDE) of vision, text, and their cross-modal interaction on the output. We systematically identify and mitigate these unintended direct effects to ensure that responses are primarily driven by genuine multi-modal fusion. Our approach consists of three steps: (1) designing structural causal graphs to distinguish correct fusion pathways from spurious modality shortcuts, (2) estimating modality-specific and cross-modal NDE using perturbed image representations, hallucinated text embeddings, and degraded visual inputs, and (3) implementing a test-time intervention module to dynamically adjust the modelâ€™s dependence on each modality. Experimental results demonstrate that our method significantly reduces hallucination while preserving task performance, providing a robust and interpretable framework for improving VLM reliability. To enhance accessibility and reproducibility, our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/TREE985/Treble-Counterfactual-VLMs">https://github.com/TREE985/Treble-Counterfactual-VLMs</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæ¨ç†ç­‰è·¨æ¨¡å¼ä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸äº§ç”Ÿä¸è§†è§‰ä¸Šä¸‹æ–‡æˆ–æç¤ºä¸ä¸€è‡´çš„å¹»è§‰è¾“å‡ºï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨è‡ªåŠ¨é©¾é©¶å’ŒåŒ»å­¦å½±åƒç­‰å…³é”®åº”ç”¨ä¸­çš„å¯é æ€§ã€‚ç°æœ‰ç ”ç©¶å°†å¹»è§‰ä¸ç»Ÿè®¡åè§ã€è¯­è¨€å…ˆéªŒçŸ¥è¯†å’Œæœ‰åç‰¹å¾å­¦ä¹ è”ç³»èµ·æ¥ï¼Œä½†ç¼ºä¹ç»“æ„æ€§çš„å› æœç†è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å› æœè§’åº¦æ¥åˆ†æå¹¶ç¼“è§£VLMsä¸­çš„å¹»è§‰é—®é¢˜ã€‚æˆ‘ä»¬å‡è®¾å¹»è§‰çš„äº§ç”Ÿæºäºè§†è§‰æˆ–æ–‡æœ¬æ¨¡æ€çš„æ„å¤–ç›´æ¥å½±å“ï¼Œè¿™äº›å½±å“ç»•è¿‡äº†é€‚å½“çš„è·¨æ¨¡å¼èåˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºVLMsæ„å»ºäº†å› æœå›¾ï¼Œå¹¶åº”ç”¨åäº‹å®åˆ†ææ¥ä¼°è®¡è§†è§‰ã€æ–‡æœ¬åŠå…¶è·¨æ¨¡å¼äº¤äº’å¯¹è¾“å‡ºçš„è‡ªç„¶ç›´æ¥å½±å“ï¼ˆNDEï¼‰ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯†åˆ«å’Œç¼“è§£äº†è¿™äº›æ„å¤–çš„ç›´æ¥å½±å“ï¼Œä»¥ç¡®ä¿å“åº”ä¸»è¦ç”±çœŸæ­£çš„è·¨æ¨¡å¼èåˆé©±åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆ1ï¼‰è®¾è®¡ç»“æ„å› æœå›¾ï¼Œä»¥åŒºåˆ†æ­£ç¡®çš„èåˆé€”å¾„å’Œè™šå‡çš„æ¨¡å¼æ·å¾„ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨å—å¹²æ‰°çš„å›¾åƒè¡¨ç¤ºã€å¹»è§‰æ–‡æœ¬åµŒå…¥å’Œé€€åŒ–è§†è§‰è¾“å…¥æ¥ä¼°è®¡æ¨¡æ€ç‰¹å®šå’Œè·¨æ¨¡æ€çš„NDEï¼›ï¼ˆ3ï¼‰å®ç°ä¸€ä¸ªæµ‹è¯•æ—¶é—´å¹²é¢„æ¨¡å—ï¼Œä»¥åŠ¨æ€è°ƒæ•´æ¨¡å‹å¯¹å„ä¸ªæ¨¡æ€çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘å¹»è§‰çš„åŒæ—¶ä¿æŒäº†ä»»åŠ¡æ€§èƒ½ï¼Œä¸ºæå‡VLMå¯é æ€§æä¾›äº†ä¸€ä¸ªç¨³å¥ä¸”å¯è§£é‡Šæ€§çš„æ¡†æ¶ã€‚ä¸ºäº†å¢å¼ºå¯è®¿é—®æ€§å’Œå¯é‡å¤æ€§ï¼Œæˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/TREE985/Treble-Counterfactual-VLMs%E3%80%82">https://github.com/TREE985/Treble-Counterfactual-VLMsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06169v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„å¹»è§†é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ç”Ÿæˆçš„è¾“å‡ºå¸¸ä¸è§†è§‰ä¸Šä¸‹æ–‡æˆ–æç¤ºä¸ç¬¦ã€‚ä½œè€…å¼•å…¥å› æœè§†è§’æ¥åˆ†æå¹¶ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ„å»ºå› æœå›¾å¹¶ä¼°è®¡å„æ¨¡æ€å¯¹è¾“å‡ºçš„ç›´æ¥å½±å“ï¼Œç³»ç»Ÿåœ°è¯†åˆ«å’Œç¼“è§£è¿™äº›ä¸å¿…è¦çš„ç›´æ¥æ•ˆåº”ï¼Œç¡®ä¿å“åº”ä¸»è¦ç”±çœŸæ­£çš„å¤šæ¨¡æ€èåˆé©±åŠ¨ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬è®¾è®¡ç»“æ„å› æœå›¾ã€ä¼°è®¡æ¨¡æ€ç‰¹å®šå’Œè·¨æ¨¡æ€çš„è‡ªç„¶ç›´æ¥æ•ˆåº”ï¼Œä»¥åŠå®æ–½æµ‹è¯•æ—¶é—´å¹²é¢„æ¨¡å—ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨å‡å°‘å¹»è§†çš„åŒæ—¶ä¿ç•™äº†ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­é¢ä¸´å¹»è§†é—®é¢˜ï¼Œå³è¾“å‡ºä¸è§†è§‰ä¸Šä¸‹æ–‡æˆ–æç¤ºä¸ä¸€è‡´ã€‚</li>
<li>å¹»è§†é—®é¢˜å¯èƒ½ä¸ç»Ÿè®¡åè§ã€è¯­è¨€å…ˆéªŒå’Œåå·®ç‰¹å¾å­¦ä¹ æœ‰å…³ã€‚</li>
<li>æœ¬æ–‡ä»å› æœè§’åº¦åˆ†æäº†å¹»è§†çš„äº§ç”ŸåŸå› ï¼Œè®¤ä¸ºå…¶æºäºè§†è§‰æˆ–æ–‡æœ¬æ¨¡æ€çš„æ„å¤–ç›´æ¥å½±å“ï¼Œç»•è¿‡äº†é€‚å½“çš„å¤šæ¨¡æ€èåˆã€‚</li>
<li>é€šè¿‡æ„å»ºå› æœå›¾å’Œæ‰§è¡Œåäº‹å®åˆ†æï¼Œä¼°è®¡å„æ¨¡æ€å¯¹è¾“å‡ºçš„è‡ªç„¶ç›´æ¥æ•ˆåº”ï¼ˆNDEï¼‰ã€‚</li>
<li>ç³»ç»Ÿåœ°è¯†åˆ«å’Œç¼“è§£è¿™äº›ä¸å¿…è¦çš„ç›´æ¥æ•ˆåº”ï¼Œç¡®ä¿å“åº”ä¸»è¦ç”±çœŸæ­£çš„å¤šæ¨¡æ€èåˆé©±åŠ¨ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•åŒ…æ‹¬è®¾è®¡ç»“æ„å› æœå›¾ã€ä¼°è®¡æ¨¡æ€ç‰¹å®šå’Œè·¨æ¨¡æ€çš„NDEï¼Œä»¥åŠå®æ–½æµ‹è¯•æ—¶é—´å¹²é¢„æ¨¡å—æ¥åŠ¨æ€è°ƒæ•´æ¨¡å‹å¯¹å„æ¨¡æ€çš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a5ec792c809c69d9acf0067ad970d508.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97ab751ffc7194fb136b6ecfa7d7e4b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e65941cea72f7b971a030c4255ffc7ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a976974b649322a0f7a475381457b5f0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning"><a href="#Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning" class="headerlink" title="Visual Adaptive Prompting for Compositional Zero-Shot Learning"></a>Visual Adaptive Prompting for Compositional Zero-Shot Learning</h2><p><strong>Authors:Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh</strong></p>
<p>Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å­¦ä¹ è§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„è”åˆè¡¨ç¤ºæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºç”¨äºç»„åˆé›¶å°„å‡»å­¦ä¹ ï¼ˆCZSLï¼‰ç­‰ä»»åŠ¡çš„æœ‰åŠ›å·¥å…·ã€‚CZSLè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°è®­ç»ƒæœŸé—´æœªæ˜ç¡®é‡åˆ°çš„æ–°è§†è§‰åŸå§‹æ•°æ®çš„ç»„åˆï¼Œä¾‹å¦‚å±æ€§å’Œå¯¹è±¡ã€‚å…³äºCZSLçš„æœ€è¿‘æç¤ºå·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥ï¼Œé€šå¸¸ä½¿ç”¨ä¸ä¼šåœ¨å˜åŒ–çš„è§†è§‰ä¸Šä¸‹æ–‡ä¸­æ”¹å˜çš„é™æ€æç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ•æ‰å˜åŒ–çš„è§†è§‰ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¾§é‡äºæ–‡æœ¬é€‚åº”ï¼Œè€Œä¸æ˜¯åˆ©ç”¨è§†è§‰ç‰¹å¾è¿›è¡Œç»„åˆæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰è‡ªé€‚åº”æç¤ºç³»ç»Ÿï¼ˆVAPSï¼‰ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºå­˜å‚¨åº“å’ŒåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æœºåˆ¶ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¡†æ¶å†…å»ºç«‹è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€è§†è§‰æç¤ºå­˜å‚¨åº“æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®å›¾åƒè§†è§‰ç‰¹å¾é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å’Œå¯¹è±¡æç¤ºã€‚æˆ‘ä»¬æå‡ºçš„ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ ä¸€ä¸ªæ›´å…·æ³›åŒ–èƒ½åŠ›çš„åµŒå…¥ç©ºé—´ã€‚åœ¨ä¸‰ç§CZSLåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œæ— è®ºæ˜¯åœ¨å°é—­è¿˜æ˜¯å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ï¼Œéƒ½å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20292v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è”åˆè¡¨ç¤ºè§†è§‰å’Œæ–‡æœ¬æ•°æ®æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ï¼ˆCZSLï¼‰ä»»åŠ¡ä¸­ã€‚è¿‘æœŸå…³äºCZSLçš„æç¤ºæ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥ä¿®æ”¹ï¼Œé€šå¸¸ä½¿ç”¨ä¸éšè§†è§‰ä¸Šä¸‹æ–‡æ”¹å˜çš„é™æ€æç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éš¾ä»¥å……åˆ†æ•æ‰å˜åŒ–çš„è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå› ä¸ºä»–ä»¬å…³æ³¨çš„æ˜¯æ–‡æœ¬é€‚åº”è€Œä¸æ˜¯åˆ©ç”¨è§†è§‰ç‰¹å¾è¿›è¡Œç»„åˆæ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰è‡ªé€‚åº”æç¤ºç³»ç»Ÿï¼ˆVAPSï¼‰ï¼Œè¯¥ç³»ç»Ÿåœ¨VLMsçš„æ¡†æ¶ä¸‹ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºä»“åº“å’ŒåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æœºåˆ¶æ¥å¼¥åˆè¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€è§†è§‰æç¤ºä»“åº“æœºåˆ¶ï¼Œæ ¹æ®å›¾åƒè§†è§‰ç‰¹å¾é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å’Œå¯¹è±¡æç¤ºã€‚æˆ‘ä»¬çš„ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ ä¸€ä¸ªæ›´å…·é€šç”¨æ€§çš„åµŒå…¥ç©ºé—´ã€‚åœ¨ä¸‰ä¸ªCZSLåŸºå‡†æµ‹è¯•ä¸­çš„å°é—­å’Œå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹è¿›è¡Œçš„å®éªŒå‡å–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMså·²ç»å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›åœ¨å­¦ä¹ è§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„è”åˆè¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨CZSLä»»åŠ¡ä¸­ã€‚</li>
<li>è¿‘æœŸå…³äºCZSLçš„æç¤ºæ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥ä¿®æ”¹ï¼Œä½†éš¾ä»¥æ•æ‰å˜åŒ–çš„è§†è§‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†è§†è§‰è‡ªé€‚åº”æç¤ºç³»ç»Ÿï¼ˆVAPSï¼‰ã€‚</li>
<li>VAPSåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºä»“åº“å’ŒåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æœºåˆ¶æ¥å¼¥åˆè¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ã€‚</li>
<li>VAPSé€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å’Œå¯¹è±¡æç¤ºæ¥é€‚åº”è§†è§‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ ä¸€ä¸ªæ›´å…·é€šç”¨æ€§çš„åµŒå…¥ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d1983c921b7ab009f3a61e03fb17e1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fef2eab66368550060014f7cb7d0a735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-766cdad7fa24ba448d036b48ce9790df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f4c706091bcfe9ac755cc2ca42bbcb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35f0371debde75c6ece6f5931410997d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits modelsâ€™ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰éšç€æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•è€Œè¿›æ­¥ï¼Œé€šè¿‡å„ç§å­¦ä¹ æ–¹æ¡ˆå®ç°äº†è¶…å‡ºé¢„å®šç±»åˆ«çš„åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®­ç»ƒæ–¹æ³•ä¸ºå¤„ç†æœªè§æ•°æ®æä¾›äº†å¯æ‰©å±•ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯OVSSçš„å…³é”®ç›®æ ‡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼šåœ¨OVSSçš„æŒ‘æˆ˜ç¯å¢ƒä¸­ï¼ŒåŸºäºä»»æ„æŸ¥è¯¢æç¤ºå¯¹å¤æ‚å¯¹è±¡è¿›è¡Œåˆ†å‰²æ—¶ï¼Œç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚è¿™ä¸€ç–å¿½é™åˆ¶äº†æ¨¡å‹åœ¨å¯¹è±¡å†…éƒ¨åˆ†ç»„è¯­ä¹‰ä¸€è‡´å…ƒç´ çš„èƒ½åŠ›ï¼Œå¹¶ç²¾ç¡®åœ°å°†å…¶æ˜ å°„åˆ°ç”¨æˆ·å®šä¹‰çš„ä»»æ„ç±»åˆ«ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é€šè¿‡èå…¥å›¾åƒå†…çš„å¯¹è±¡çº§åˆ«ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥å…‹æœè¿™ä¸€é™åˆ¶çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡è’¸é¦è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œä½¿å¾—è¯­ä¹‰ä¸€è‡´çš„ç»„ä»¶èƒ½å¤Ÿå½¢æˆå•ä¸ªå¯¹è±¡æ©è†œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é›¶å°„å¯¹è±¡å­˜åœ¨å¯èƒ½æ€§æ¥ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œä»¥ç¡®ä¿ä¸å›¾åƒä¸­è¡¨ç¤ºçš„å…·ä½“å¯¹è±¡å‡†ç¡®å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰çš„æœ€æ–°ç ”ç©¶ï¼Œé€šè¿‡å¼•å…¥å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­å¯¹ä»»æ„æŸ¥è¯¢æç¤ºçš„åˆ†å‰²é™åˆ¶ã€‚è¯¥ç ”ç©¶é€šè¿‡è’¸é¦è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œå¹¶æ”¹è¿›äº†æ–‡æœ¬åµŒå…¥ï¼Œç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚è¯¥æ–¹æ³•å®ç°äº†è·¨å¤šä¸ªæ•°æ®é›†çš„å“è¶Šæ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰èƒ½å¤Ÿé€šè¿‡æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œæ›´å¹¿æ³›çš„åˆ†å‰²ï¼ŒåŒ…æ‹¬é¢„å®šä¹‰ç±»åˆ«ä¹‹å¤–çš„å†…å®¹ã€‚</li>
<li>è®­ç»ƒå…è´¹çš„æ–¹æ³•ä¸ºå¤„ç†æœªè§æ•°æ®æä¾›äº†å¯æ‰©å±•å’Œæ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯OVSSçš„å…³é”®ç›®æ ‡ä¹‹ä¸€ã€‚</li>
<li>å½“å‰å·¥ä½œä¸­ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ï¼Œåœ¨åŸºäºä»»æ„æŸ¥è¯¢æç¤ºçš„OVSSç¯å¢ƒä¸­åˆ†å‰²å¤æ‚å¯¹è±¡æ—¶ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚</li>
<li>æ–°å‹æ–¹æ³•é€šè¿‡æ•´åˆå›¾åƒä¸­çš„å¯¹è±¡çº§åˆ«ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è’¸é¦è§†è§‰åŸºç¡€æ¨¡å‹çš„é¢‘è°±é©±åŠ¨ç‰¹å¾åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„ä¸€è‡´æ€§ã€‚</li>
<li>ç ”ç©¶è¿˜æ”¹è¿›äº†æ–‡æœ¬åµŒå…¥ï¼Œé€šè¿‡é›¶é•œå¤´å¯¹è±¡å­˜åœ¨å¯èƒ½æ€§æ¥ç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a4674b24fe322761820fa176c1eb16e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b17542025b8df1633d8f8c54cbb60b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e98ebfbdecc978c059d2cb079af8da.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2411.17150v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Personalized-Speech-Emotion-Recognition-in-Human-Robot-Interaction-using-Vision-Transformers"><a href="#Personalized-Speech-Emotion-Recognition-in-Human-Robot-Interaction-using-Vision-Transformers" class="headerlink" title="Personalized Speech Emotion Recognition in Human-Robot Interaction using   Vision Transformers"></a>Personalized Speech Emotion Recognition in Human-Robot Interaction using   Vision Transformers</h2><p><strong>Authors:Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa</strong></p>
<p>Emotions are an essential element in verbal communication, so understanding individualsâ€™ affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT&#x2F;BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs. </p>
<blockquote>
<p>æƒ…æ„Ÿæ˜¯å£å¤´äº¤æµçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå› æ­¤åœ¨äººæœºäº’åŠ¨ï¼ˆHRIï¼‰ä¸­ç†è§£ä¸ªä½“çš„æƒ…æ„Ÿå˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è½¬æ¢å™¨æ¨¡å‹ï¼ˆå³ViTï¼ˆè§†è§‰è½¬æ¢å™¨ï¼‰å’ŒBeitï¼ˆå›¾åƒè½¬æ¢å™¨çš„BERTé¢„è®­ç»ƒï¼‰ï¼‰åœ¨äººæœºäº’åŠ¨ä¸­çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åº”ç”¨ã€‚é‡ç‚¹æ˜¯é€šè¿‡åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå¾®è°ƒè¿™äº›æ¨¡å‹å¹¶åˆ©ç”¨é›†æˆæ–¹æ³•ï¼Œä½¿SERæ¨¡å‹é€‚åº”ä¸ªäººçš„è¯­éŸ³ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»ä¸NAOæœºå™¨äººè¿›è¡Œä¼ªè‡ªç„¶å¯¹è¯çš„ä¸åŒäººç±»å—è¯•è€…æ”¶é›†äº†éŸ³é¢‘æ•°æ®ã€‚ç„¶åæˆ‘ä»¬å¯¹åŸºäºViTå’ŒBeitçš„æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨æ¥è‡ªå‚ä¸è€…çš„æœªè§è¯­éŸ³æ ·æœ¬ä¸Šæµ‹è¯•äº†è¿™äº›æ¨¡å‹ã€‚åœ¨ç»“æœä¸­ï¼Œæˆ‘ä»¬æ˜¾ç¤ºï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå¾®è°ƒè§†è§‰è½¬æ¢å™¨ï¼Œç„¶åä½¿ç”¨å·²ç»å¾®è°ƒè¿‡çš„è¿™äº›æ¨¡å‹æˆ–é›†æˆViT&#x2F;BEiTæ¨¡å‹ï¼Œåœ¨è¯†åˆ«å››ç§ä¸»è¦æƒ…ç»ªï¼ˆä¸­æ€§ã€å¿«ä¹ã€æ‚²ä¼¤å’Œæ„¤æ€’ï¼‰æ—¶ï¼Œé’ˆå¯¹ä¸ªäººè·å¾—äº†æœ€é«˜çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œè¿™ä¸å¾®è°ƒæ™®é€šViTsæˆ–BEITsç›¸æ¯”æ•ˆæœæ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10687v3">PDF</a> This work has been accepted for the IEEE Robotics and Automation   Letters (RA-L)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æƒ…ç»ªåœ¨äººæœºäº’åŠ¨ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è§†è§‰Transformeræ¨¡å‹ï¼ˆå¦‚ViTå’ŒBEiTï¼‰è¿›è¡Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ã€‚é€šè¿‡å¯¹åŸºå‡†æ•°æ®é›†è¿›è¡Œå¾®è°ƒå¹¶é‡‡ç”¨é›†æˆæ–¹æ³•ï¼Œå®ç°äº†é’ˆå¯¹ä¸ªäººè¯­éŸ³ç‰¹å¾çš„æ¨¡å‹é€šç”¨åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¯¹è§†è§‰Transformeræ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶åœ¨è¯†åˆ«å››ç§ä¸»è¦æƒ…ç»ªï¼ˆä¸­æ€§ã€å¿«ä¹ã€æ‚²ä¼¤å’Œæ„¤æ€’ï¼‰æ—¶ï¼Œä½¿ç”¨å·²è°ƒæ¨¡å‹æˆ–é›†æˆViT&#x2F;BEiTæ¨¡å‹å¯è·å¾—æœ€é«˜çš„ä¸ªä½“åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…ç»ªåœ¨äººæœºäº’åŠ¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>è§†è§‰Transformeræ¨¡å‹ï¼ˆViTå’ŒBEiTï¼‰è¢«åº”ç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ã€‚</li>
<li>é€šè¿‡å¯¹åŸºå‡†æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†é’ˆå¯¹ä¸ªäººè¯­éŸ³ç‰¹å¾çš„æ¨¡å‹é€šç”¨åŒ–ã€‚</li>
<li>æ”¶é›†æ¥è‡ªä¸NAOæœºå™¨äººè¿›è¡Œä¼ªè‡ªç„¶å¯¹è¯çš„ä¸åŒäººç±»çš„éŸ³é¢‘æ•°æ®ã€‚</li>
<li>è§†è§‰Transformeræ¨¡å‹åœ¨è¯†åˆ«å››ç§ä¸»è¦æƒ…ç»ªï¼ˆä¸­æ€§ã€å¿«ä¹ã€æ‚²ä¼¤å’Œæ„¤æ€’ï¼‰æ—¶è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
<li>å·²è°ƒæ¨¡å‹æˆ–é›†æˆViT&#x2F;BEiTæ¨¡å‹åœ¨ä¸ªä½“åˆ†ç±»å‡†ç¡®ç‡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d2724482341270ae8a08431985f7aa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1df52457354ebe35a7c2d95f86f95549.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6718c798826cf4e5e49657d6f619c0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6817f7d46662b07115c15784608602d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac915e3876c60b287ac49530c98cf191.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef02565806f4dd911d10d5d0a1222d0c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rethinking-model-prototyping-through-the-MedMNIST-dataset-collection"><a href="#Rethinking-model-prototyping-through-the-MedMNIST-dataset-collection" class="headerlink" title="Rethinking model prototyping through the MedMNIST+ dataset collection"></a>Rethinking model prototyping through the MedMNIST+ dataset collection</h2><p><strong>Authors:Sebastian Doerrich, Francesco Di Salvo, Julius Brockmann, Christian Ledig</strong></p>
<p>The integration of deep learning based systems in clinical practice is often impeded by challenges rooted in limited and heterogeneous medical datasets. In addition, the field has increasingly prioritized marginal performance gains on a few, narrowly scoped benchmarks over clinical applicability, slowing down meaningful algorithmic progress. This trend often results in excessive fine-tuning of existing methods on selected datasets rather than fostering clinically relevant innovations. In response, this work introduces a comprehensive benchmark for the MedMNIST+ dataset collection, designed to diversify the evaluation landscape across several imaging modalities, anatomical regions, classification tasks and sample sizes. We systematically reassess commonly used Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) architectures across distinct medical datasets, training methodologies, and input resolutions to validate and refine existing assumptions about model effectiveness and development. Our findings suggest that computationally efficient training schemes and modern foundation models offer viable alternatives to costly end-to-end training. Additionally, we observe that higher image resolutions do not consistently improve performance beyond a certain threshold. This highlights the potential benefits of using lower resolutions, particularly in prototyping stages, to reduce computational demands without sacrificing accuracy. Notably, our analysis reaffirms the competitiveness of CNNs compared to ViTs, emphasizing the importance of comprehending the intrinsic capabilities of different architectures. Finally, by establishing a standardized evaluation framework, we aim to enhance transparency, reproducibility, and comparability within the MedMNIST+ dataset collection. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus">https://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus</a> . </p>
<blockquote>
<p>å°†æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ•´åˆåˆ°ä¸´åºŠå®è·µç»å¸¸ä¼šå—åˆ°æœ‰é™çš„åŒ»å­¦æ•°æ®é›†å’Œå¼‚æ„æ€§çš„æŒ‘æˆ˜æ‰€é˜»ç¢ã€‚æ­¤å¤–ï¼Œè¯¥é¢†åŸŸè¶Šæ¥è¶Šé‡è§†åœ¨ç‹­çª„èŒƒå›´åŸºå‡†æµ‹è¯•ä¸­è·å¾—çš„ä¸€äº›å¾®ä¸è¶³é“çš„æ€§èƒ½å¢ç›Šï¼Œè€Œå¿½ç•¥äº†å…¶åœ¨ä¸´åºŠæ²»ç–—ä¸­çš„åº”ç”¨ä»·å€¼ï¼Œå¯¼è‡´ç®—æ³•çš„æœ‰æ•ˆè¿›å±•å˜å¾—ç¼“æ…¢ã€‚è¿™ä¸€è¶‹åŠ¿é€šå¸¸å¯¼è‡´åœ¨é€‰å®šæ•°æ®é›†ä¸Šå¯¹ç°æœ‰æ–¹æ³•çš„è¿‡åº¦å¾®è°ƒï¼Œè€Œä¸æ˜¯ä¿ƒè¿›ä¸ä¸´åºŠå®è·µç›¸å…³çš„åˆ›æ–°ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œå¼•å…¥äº†é’ˆå¯¹MedMNIST+æ•°æ®é›†é›†åˆçš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡å¤šç§æˆåƒæ¨¡å¼ã€è§£å‰–åŒºåŸŸã€åˆ†ç±»ä»»åŠ¡å’Œæ ·æœ¬é‡æ¥ä¸°å¯Œè¯„ä¼°æ™¯è§‚ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°é‡æ–°è¯„ä¼°äº†å¸¸ç”¨çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¶æ„åœ¨ä¸åŒåŒ»å­¦æ•°æ®é›†ã€è®­ç»ƒæ–¹æ³•å’Œè¾“å…¥åˆ†è¾¨ç‡ä¸Šçš„è¡¨ç°ï¼Œä»¥éªŒè¯å’Œç²¾ç‚¼å…³äºæ¨¡å‹æœ‰æ•ˆæ€§å’Œå‘å±•çš„ç°æœ‰å‡è®¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®¡ç®—æ•ˆç‡é«˜çš„è®­ç»ƒæ–¹æ¡ˆå’Œç°ä»£åŸºç¡€æ¨¡å‹ä¸ºæ˜‚è´µçš„ç«¯åˆ°ç«¯è®­ç»ƒæä¾›äº†å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å›¾åƒåˆ†è¾¨ç‡å¹¶ä¸æ€»æ˜¯åœ¨è¶…è¿‡ä¸€å®šé˜ˆå€¼åå°±èƒ½æŒç»­æé«˜æ€§èƒ½ã€‚è¿™çªå‡ºäº†ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡çš„æ½œåœ¨ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸå‹å¼€å‘é˜¶æ®µï¼Œä»¥å‡å°‘è®¡ç®—éœ€æ±‚è€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åˆ†æå†æ¬¡è¯æ˜äº†CNNä¸ViTç›¸æ¯”çš„ç«äº‰åŠ›ï¼Œå¼ºè°ƒäº†ç†è§£ä¸åŒæ¶æ„çš„å†…åœ¨èƒ½åŠ›çš„é‡è¦æ€§ã€‚æœ€åï¼Œé€šè¿‡å»ºç«‹ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬æ—¨åœ¨æé«˜MedMNIST+æ•°æ®é›†é›†åˆä¸­çš„é€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå¯æ¯”æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus">https://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15786v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŒ»ç–—æ•°æ®é›†æœ‰é™å’Œå¼‚è´¨æ€§é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€å¥—å…¨é¢çš„åŸºå‡†æµ‹è¯•æ ‡å‡†ï¼Œç”¨äºè¯„ä¼°MedMNIST+æ•°æ®é›†é›†åˆï¼Œæ¶µç›–å¤šç§æˆåƒæ¨¡å¼ã€è§£å‰–åŒºåŸŸã€åˆ†ç±»ä»»åŠ¡å’Œæ ·æœ¬é‡ã€‚æ–‡ç« è¯„ä¼°äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¶æ„åœ¨åŒ»ç–—æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶å‘ç°è®¡ç®—æ•ˆç‡é«˜çš„è®­ç»ƒæ–¹æ¡ˆå’Œç°ä»£åŒ–åŸºç¡€æ¨¡å‹å…·æœ‰å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« æŒ‡å‡ºæ›´é«˜çš„å›¾åƒåˆ†è¾¨ç‡å¹¶ä¸æ€»æ˜¯èƒ½æé«˜æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒåœ¨åŸå‹åˆ¶ä½œé˜¶æ®µä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡ä»¥å‡å°‘è®¡ç®—éœ€æ±‚è€Œä¸ç‰ºç‰²å‡†ç¡®æ€§çš„æ½œåŠ›ã€‚æœ€åï¼Œæ–‡ç« é€šè¿‡å»ºç«‹ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜MedMNIST+æ•°æ®é›†é›†åˆçš„é€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå¯æ¯”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ¨ä¸´åºŠåº”ç”¨é¢ä¸´åŒ»ç–—æ•°æ®é›†æœ‰é™å’Œå¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>MedMNIST+æ•°æ®é›†è¯„ä¼°æ ‡å‡†è¢«æå‡ºä»¥å…¨é¢è¯„ä¼°ä¸åŒæˆåƒæ¨¡å¼ã€è§£å‰–åŒºåŸŸã€åˆ†ç±»ä»»åŠ¡å’Œæ ·æœ¬é‡çš„è¡¨ç°ã€‚</li>
<li>å¯¹CNNå’ŒViTæ¶æ„çš„è¯„ä¼°å‘ç°è®¡ç®—æ•ˆç‡é«˜çš„è®­ç»ƒæ–¹æ¡ˆå’Œç°ä»£åŒ–åŸºç¡€æ¨¡å‹å…·æœ‰å¯è¡Œæ€§ã€‚</li>
<li>æ›´é«˜çš„å›¾åƒåˆ†è¾¨ç‡å¹¶ä¸ä¸€å®šæé«˜æ€§èƒ½ï¼Œä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡åœ¨åŸå‹åˆ¶ä½œé˜¶æ®µå…·æœ‰æ½œåŠ›ã€‚</li>
<li>CNNä¸ViTçš„ç«äº‰æ€§åˆ†æå¼ºè°ƒäº†ç†è§£ä¸åŒæ¶æ„å†…åœ¨èƒ½åŠ›çš„é‡è¦æ€§ã€‚</li>
<li>æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶æé«˜äº†MedMNIST+æ•°æ®é›†é›†åˆçš„é€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå¯æ¯”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2404.15786v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2404.15786v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2404.15786v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2404.15786v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain"><a href="#Masked-LoGoNet-Fast-and-Accurate-3D-Image-Analysis-for-Medical-Domain" class="headerlink" title="Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain"></a>Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain</h2><p><strong>Authors:Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath</strong></p>
<p>Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. The method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNetâ€™s superior performance in both inference time and accuracy. </p>
<blockquote>
<p>æ ‡å‡†ç°ä»£æœºå™¨å­¦ä¹ æˆåƒæ–¹æ³•åœ¨éŸ³ä¹åº”ç”¨åœ¨åŒ»ç–—é¢†åŸŸä¸Šé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ•°æ®é›†å»ºè®¾æˆæœ¬é«˜æ˜‚å¯¼è‡´äº†å¯ç”¨æ ‡æ³¨è®­ç»ƒæ•°æ®çš„æ•°é‡å—é™ã€‚æ­¤å¤–ï¼Œåœ¨å®é™…éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç”¨äºæ¯æ—¥å¤„ç†å¤§é‡æ•°æ®ï¼Œç»™åŒ»ç–—æœºæ„å¸¦æ¥äº†é«˜æ˜‚çš„ç»´æŠ¤æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåä¸ºLoGoNetï¼Œå®ƒé‡‡ç”¨å®šåˆ¶çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æ¥ç¼“è§£è¿™äº›æŒ‘æˆ˜ã€‚LoGoNetåœ¨ä¸€ä¸ªUå‹æ¶æ„ä¸­é›†æˆäº†ä¸€ç§æ–°å‹ç‰¹å¾æå–å™¨ï¼Œå€ŸåŠ©å¤§å†…æ ¸æ³¨æ„åŠ›ï¼ˆLKAï¼‰å’ŒåŒç¼–ç ç­–ç•¥æ¥å·§å¦™åœ°æ•è·é•¿çŸ­è·ç¦»ç‰¹å¾ä¾èµ–å…³ç³»ã€‚è¿™ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œåè€…ä¾èµ–äºå¢åŠ ç½‘ç»œå®¹é‡æ¥æå‡ç‰¹å¾æå–èƒ½åŠ›ã€‚æ¨¡å‹ä¸­ç»“åˆçš„æ–°å‹æŠ€æœ¯ç‰¹åˆ«æœ‰ç›ŠäºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œè€ƒè™‘åˆ°å­¦ä¹ å¤æ‚ä¸”é€šå¸¸ä¸è§„åˆ™çš„å™¨å®˜å½¢çŠ¶ï¼ˆå¦‚è„¾è„ï¼‰çš„éš¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸‰ç»´å›¾åƒæå‡ºäº†ä¸€ç§æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æ¥å¼¥è¡¥å¤§å‹æ ‡æ³¨æ•°æ®é›†çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•åœ¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å†…ç»“åˆäº†æ©ç å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶ä¸Vision Transformerï¼ˆViTï¼‰å’ŒCNNæ¨¡å‹å…¼å®¹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ï¼ˆå³BTCVå’ŒMSDï¼‰çš„å¤šä¸ªä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸å…«ç§æœ€æ–°æ¨¡å‹çš„åŸºå‡†å¯¹æ¯”æµ‹è¯•çªæ˜¾äº†LoGoNetåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®æ€§æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06190v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶æ„LoGoNetï¼Œé…åˆå®šåˆ¶çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»¥è§£å†³åŒ»å­¦åº”ç”¨ä¸­ç°ä»£æœºå™¨å­¦ä¹ æˆåƒæ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚LoGoNeté‡‡ç”¨Uå‹æ¶æ„å¹¶ç»“åˆå¤§å‹å†…æ ¸æ³¨æ„åŠ›æœºåˆ¶ä¸åŒé‡ç¼–ç ç­–ç•¥ï¼Œèƒ½æ•æ‰é•¿çŸ­è·ç¦»ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œå¼¥è¡¥äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¤æ‚çš„å™¨å®˜å½¢çŠ¶å­¦ä¹ éš¾é¢˜ï¼Œå¦‚è„¾è„ï¼Œè¯¥æ¨¡å‹å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚åŒæ—¶ï¼Œä¸ºå¼¥è¡¥ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´å›¾åƒçš„æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆäº†æ©è”½å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶å…¼å®¹Vision Transformerå’ŒCNNæ¨¡å‹ã€‚åœ¨æ ‡å‡†æ•°æ®é›†BTCVå’ŒMSDä¸Šçš„å¤šé¡¹ä»»åŠ¡å®éªŒä¸­ï¼Œä¸å…«ç§å…ˆè¿›æ¨¡å‹çš„åŸºå‡†å¯¹æ¯”ç»“æœæ˜¾ç¤ºï¼ŒLoGoNetåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®åº¦ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoGoNetæ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†Uå‹æ¶æ„ã€å¤§å‹å†…æ ¸æ³¨æ„åŠ›æœºåˆ¶å’ŒåŒé‡ç¼–ç ç­–ç•¥ï¼Œèƒ½æœ‰æ•ˆæ•æ‰åŒ»å­¦å›¾åƒä¸­çš„ç‰¹å¾ä¾èµ–å…³ç³»ã€‚</li>
<li>LoGoNeté€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è§£å†³åŒ»å­¦åº”ç”¨ä¸­æ•°æ®é›†çš„æ„å»ºæˆæœ¬é«˜å’Œæ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¤æ‚çš„å™¨å®˜å½¢çŠ¶å­¦ä¹ éš¾é¢˜ï¼Œå¦‚è„¾è„ï¼ŒLoGoNetå…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´å›¾åƒçš„æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆäº†æ©è”½å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…¼å®¹Vision Transformerå’ŒCNNæ¨¡å‹ï¼Œå¯å¹¿æ³›åº”ç”¨äºå¤šç§åŒ»å­¦å›¾åƒä»»åŠ¡ã€‚</li>
<li>åœ¨æ ‡å‡†æ•°æ®é›†BTCVå’ŒMSDä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoGoNetåœ¨æ¨ç†æ—¶é—´å’Œå‡†ç¡®åº¦ä¸Šå‡ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.06190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2402.06190v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2402.06190v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2402.06190v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2402.06190v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2402.06190v2/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_Vision Transformer/2402.06190v2/page_5_2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c6eff8ced9348e87b7694e15f4077cad.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Beyond RGB Adaptive Parallel Processing for RAW Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bc135ad0d0def72cacde9f8a014dd998.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Logic-in-Frames Dynamic Keyframe Search via Visual Semantic-Logical   Verification for Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
