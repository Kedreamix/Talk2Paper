<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Beyond RGB Adaptive Parallel Processing for RAW Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c6eff8ced9348e87b7694e15f4077cad.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-18-æ›´æ–°"><a href="#2025-03-18-æ›´æ–°" class="headerlink" title="2025-03-18 æ›´æ–°"></a>2025-03-18 æ›´æ–°</h1><h2 id="Beyond-RGB-Adaptive-Parallel-Processing-for-RAW-Object-Detection"><a href="#Beyond-RGB-Adaptive-Parallel-Processing-for-RAW-Object-Detection" class="headerlink" title="Beyond RGB: Adaptive Parallel Processing for RAW Object Detection"></a>Beyond RGB: Adaptive Parallel Processing for RAW Object Detection</h2><p><strong>Authors:Shani Gamrian, Hila Barel, Feiran Li, Masakazu Yoshimura, Daisuke Iso</strong></p>
<p>Object detection models are typically applied to standard RGB images processed through Image Signal Processing (ISP) pipelines, which are designed to enhance sensor-captured RAW images for human vision. However, these ISP functions can lead to a loss of critical information that may be essential in optimizing for computer vision tasks, such as object detection. In this work, we introduce Raw Adaptation Module (RAM), a module designed to replace the traditional ISP, with parameters optimized specifically for RAW object detection. Inspired by the parallel processing mechanisms of the human visual system, RAM departs from existing learned ISP methods by applying multiple ISP functions in parallel rather than sequentially, allowing for a more comprehensive capture of image features. These processed representations are then fused in a specialized module, which dynamically integrates and optimizes the information for the target task. This novel approach not only leverages the full potential of RAW sensor data but also enables task-specific pre-processing, resulting in superior object detection performance. Our approach outperforms RGB-based methods and achieves state-of-the-art results across diverse RAW image datasets under varying lighting conditions and dynamic ranges. </p>
<blockquote>
<p>å¯¹è±¡æ£€æµ‹æ¨¡å‹é€šå¸¸åº”ç”¨äºé€šè¿‡å›¾åƒä¿¡å·å¤„ç†ï¼ˆISPï¼‰ç®¡é“å¤„ç†çš„æ ‡å‡†RGBå›¾åƒï¼Œè¿™äº›ç®¡é“æ—¨åœ¨å¢å¼ºä¸ºå¢å¼ºäººç±»è§†è§‰è€Œæ•è·çš„ä¼ æ„Ÿå™¨RAWå›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›ISPåŠŸèƒ½å¯èƒ½å¯¼è‡´ä¸¢å¤±å¯¹äºä¼˜åŒ–è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡æ£€æµ‹ï¼‰è‡³å…³é‡è¦çš„å…³é”®ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸå§‹é€‚é…æ¨¡å—ï¼ˆRAMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ›¿ä»£ä¼ ç»ŸISPçš„æ¨¡å—ï¼Œå…¶å‚æ•°ä¸“ä¸ºRAWå¯¹è±¡æ£€æµ‹ä¼˜åŒ–ã€‚å—äººç±»è§†è§‰ç³»ç»Ÿå¹¶è¡Œå¤„ç†æœºåˆ¶çš„å¯å‘ï¼ŒRAMé€šè¿‡ä¸ç°æœ‰å­¦ä¹ ISPæ–¹æ³•ä¸åŒçš„æ–¹å¼åº”ç”¨å¤šä¸ªISPåŠŸèƒ½å¹¶è¡Œè€Œéé¡ºåºåœ°åº”ç”¨ï¼Œä»è€Œå¯ä»¥æ›´å…¨é¢åœ°æ•è·å›¾åƒç‰¹å¾ã€‚è¿™äº›å¤„ç†åçš„è¡¨ç¤ºå½¢å¼éšååœ¨ä¸€ä¸ªä¸“ç”¨æ¨¡å—ä¸­è¿›è¡Œèåˆï¼Œè¯¥æ¨¡å—åŠ¨æ€é›†æˆå¹¶ä¼˜åŒ–é’ˆå¯¹ç›®æ ‡ä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™ç§æ–°é¢–çš„æ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨äº†RAWä¼ æ„Ÿå™¨çš„å…¨éƒ¨æ½œåŠ›ï¼Œè¿˜å®ç°äº†é’ˆå¯¹ä»»åŠ¡çš„é¢„å¤„ç†ï¼Œä»è€Œå®ç°äº†å“è¶Šçš„å¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§RAWå›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºåŸºäºRGBçš„æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒçš„ç…§æ˜æ¡ä»¶å’ŒåŠ¨æ€èŒƒå›´å†…è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13163v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ä¼ ç»Ÿå›¾åƒä¿¡å·å¤„ç†ï¼ˆISPï¼‰ç®¡é“ä¸­åº”ç”¨äºæ ‡å‡†RGBå›¾åƒçš„å¯¹è±¡æ£€æµ‹æ¨¡å‹å¯èƒ½ä¼šä¸¢å¤±å…³é”®ä¿¡æ¯ï¼Œå½±å“è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ä¼˜åŒ–ã€‚å› æ­¤ï¼Œç ”ç©¶æå‡ºäº†Raw Adaptation Moduleï¼ˆRAMï¼‰æ¨¡å—ï¼Œä»¥æ›¿ä»£ä¼ ç»Ÿçš„ISPï¼Œå¹¶é’ˆå¯¹RAWå¯¹è±¡æ£€æµ‹è¿›è¡Œä¼˜åŒ–ã€‚RAMæ¨¡å—é€šè¿‡å¹¶è¡Œåº”ç”¨å¤šä¸ªISPåŠŸèƒ½ï¼Œä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸­æå–æ›´å¤šç‰¹å¾ä¿¡æ¯ï¼Œå¹¶åŠ¨æ€é›†æˆå’Œä¼˜åŒ–è¿™äº›ä¿¡æ¯ä»¥å®Œæˆç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨äº†RAWä¼ æ„Ÿå™¨æ•°æ®çš„æ½œåŠ›ï¼Œè¿˜å®ç°äº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é¢„å¤„ç†ï¼Œä»è€Œè·å¾—å“è¶Šçš„å¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¼ ç»ŸISPç®¡é“è®¾è®¡ä¸»è¦é¢å‘äººç±»è§†è§‰ä¼˜åŒ–ï¼Œå¯èƒ½ä¸¢å¤±å…³é”®ä¿¡æ¯ï¼Œå½±å“è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†Raw Adaptation Moduleï¼ˆRAMï¼‰æ¨¡å—ï¼Œæ›¿ä»£ä¼ ç»ŸISPï¼Œé’ˆå¯¹RAWå¯¹è±¡æ£€æµ‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>RAMæ¨¡å—é€šè¿‡å¹¶è¡Œåº”ç”¨å¤šä¸ªISPåŠŸèƒ½ï¼Œæå–æ›´å¤šå›¾åƒç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>RAMæ¨¡å—é‡‡ç”¨åŠ¨æ€é›†æˆå’Œä¼˜åŒ–ä¿¡æ¯çš„æ–¹å¼ï¼Œä»¥å®Œæˆç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨RAWä¼ æ„Ÿå™¨æ•°æ®çš„æ½œåŠ›ã€‚</li>
<li>å®ç°é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é¢„å¤„ç†ï¼Œæé«˜å¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7394f80225ae5bb50e276675aae71925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cfe9cc655d2977b8ab37a0437867f52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-222fdb7b4191212eb804d0c42f4933b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab2a30c80a4125818eeb2ec489f24e13.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UncTrack-Reliable-Visual-Object-Tracking-with-Uncertainty-Aware-Prototype-Memory-Network"><a href="#UncTrack-Reliable-Visual-Object-Tracking-with-Uncertainty-Aware-Prototype-Memory-Network" class="headerlink" title="UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware   Prototype Memory Network"></a>UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware   Prototype Memory Network</h2><p><strong>Authors:Siyuan Yao, Yang Guo, Yanyang Yan, Wenqi Ren, Xiaochun Cao</strong></p>
<p>Transformer-based trackers have achieved promising success and become the dominant tracking paradigm due to their accuracy and efficiency. Despite the substantial progress, most of the existing approaches tackle object tracking as a deterministic coordinate regression problem, while the target localization uncertainty has been greatly overlooked, which hampers trackersâ€™ ability to maintain reliable target state prediction in challenging scenarios. To address this issue, we propose UncTrack, a novel uncertainty-aware transformer tracker that predicts the target localization uncertainty and incorporates this uncertainty information for accurate target state inference. Specifically, UncTrack utilizes a transformer encoder to perform feature interaction between template and search images. The output features are passed into an uncertainty-aware localization decoder (ULD) to coarsely predict the corner-based localization and the corresponding localization uncertainty. Then the localization uncertainty is sent into a prototype memory network (PMN) to excavate valuable historical information to identify whether the target state prediction is reliable or not. To enhance the template representation, the samples with high confidence are fed back into the prototype memory bank for memory updating, making the tracker more robust to challenging appearance variations. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ManOfStory/UncTrack">https://github.com/ManOfStory/UncTrack</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„è·Ÿè¸ªå™¨ç”±äºå…¶å‡†ç¡®æ€§å’Œé«˜æ•ˆæ€§è€Œå–å¾—äº†æœ‰å‰æ™¯çš„æˆåŠŸï¼Œå¹¶æˆä¸ºä¸»å¯¼è·Ÿè¸ªèŒƒå¼ã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½å°†ç›®æ ‡è·Ÿè¸ªè§†ä¸ºç¡®å®šæ€§åæ ‡å›å½’é—®é¢˜ï¼Œè€Œç›®æ ‡å®šä½çš„ä¸ç¡®å®šæ€§å´è¢«å¤§å¤§å¿½è§†äº†ï¼Œè¿™é˜»ç¢äº†è·Ÿè¸ªå™¨åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ä¿æŒå¯é çš„ç›®æ ‡çŠ¶æ€é¢„æµ‹çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UncTrackï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥Transformerè·Ÿè¸ªå™¨ï¼Œå®ƒé¢„æµ‹ç›®æ ‡å®šä½çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å°†è¿™ç§ä¸ç¡®å®šæ€§ä¿¡æ¯ç”¨äºå‡†ç¡®çš„ç›®æ ‡çŠ¶æ€æ¨æ–­ã€‚å…·ä½“æ¥è¯´ï¼ŒUncTrackåˆ©ç”¨Transformerç¼–ç å™¨æ‰§è¡Œæ¨¡æ¿å›¾åƒå’Œæœç´¢å›¾åƒä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚è¾“å‡ºç‰¹å¾è¢«ä¼ é€’åˆ°ä¸ç¡®å®šæ€§æ„ŸçŸ¥å®šä½è§£ç å™¨ï¼ˆULDï¼‰ä¸­ï¼Œä»¥ç²—ç•¥é¢„æµ‹åŸºäºè§’ç‚¹çš„å®šä½å’Œç›¸åº”çš„å®šä½ä¸ç¡®å®šæ€§ã€‚ç„¶åï¼Œå®šä½ä¸ç¡®å®šæ€§è¢«å‘é€åˆ°åŸå‹è®°å¿†ç½‘ç»œï¼ˆPMNï¼‰ä¸­ï¼Œä»¥æŒ–æ˜æœ‰ä»·å€¼çš„å†å²ä¿¡æ¯æ¥ç¡®å®šç›®æ ‡çŠ¶æ€é¢„æµ‹æ˜¯å¦å¯é ã€‚ä¸ºäº†æé«˜æ¨¡æ¿è¡¨ç¤ºèƒ½åŠ›ï¼Œé«˜ç½®ä¿¡åº¦çš„æ ·æœ¬è¢«åé¦ˆåˆ°åŸå‹è®°å¿†åº“ä¸­è¿›è¡Œè®°å¿†æ›´æ–°ï¼Œä½¿è·Ÿè¸ªå™¨å¯¹æŒ‘æˆ˜æ€§çš„å¤–è§‚å˜åŒ–æ›´åŠ é²æ£’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/ManOfStory/UncTrack%E3%80%82">https://github.com/ManOfStory/UncTrackã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12888v1">PDF</a> 14 pages,11 figures,references added</p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºTransformerçš„è·Ÿè¸ªå™¨å·²æˆä¸ºç›®æ ‡è·Ÿè¸ªçš„ä¸»æµèŒƒå¼ï¼Œå› å…¶å‡†ç¡®æ€§å’Œé«˜æ•ˆæ€§è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šå°†ç›®æ ‡è·Ÿè¸ªè§†ä¸ºç¡®å®šæ€§åæ ‡å›å½’é—®é¢˜ï¼Œå¿½ç•¥äº†ç›®æ ‡å®šä½çš„ä¸ç¡®å®šæ€§ï¼Œè¿™åœ¨æŒ‘æˆ˜åœºæ™¯ä¸‹é™åˆ¶äº†è·Ÿè¸ªå™¨çš„å¯é ç›®æ ‡çŠ¶æ€é¢„æµ‹èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UncTrackï¼Œä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥Transformerè·Ÿè¸ªå™¨ï¼Œå®ƒé¢„æµ‹ç›®æ ‡å®šä½çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶ç»“åˆè¿™ä¸€ä¸ç¡®å®šæ€§ä¿¡æ¯è¿›è¡Œå‡†ç¡®çš„ç›®æ ‡çŠ¶æ€æ¨æ–­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformerè·Ÿè¸ªå™¨å·²æˆä¸ºä¸»æµï¼Œå› ä¸ºå®ƒä»¬æ—¢å‡†ç¡®åˆé«˜æ•ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç¡®å®šæ€§åæ ‡å›å½’ï¼Œå¿½ç•¥äº†ç›®æ ‡å®šä½çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>UncTrackæ˜¯ä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥Transformerè·Ÿè¸ªå™¨ã€‚</li>
<li>UncTrackåˆ©ç”¨å˜å‹å™¨ç¼–ç å™¨è¿›è¡Œæ¨¡æ¿å’Œæœç´¢å›¾åƒä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚</li>
<li>UncTrackæœ‰ä¸€ä¸ªä¸ç¡®å®šæ€§æ„ŸçŸ¥å®šä½è§£ç å™¨ï¼ˆULDï¼‰ï¼Œå¯ç²—ç•¥é¢„æµ‹åŸºäºè§’è½çš„å®šä½å’Œç›¸åº”çš„å®šä½ä¸ç¡®å®šæ€§ã€‚</li>
<li>UncTrackä½¿ç”¨åŸå‹è®°å¿†ç½‘ç»œï¼ˆPMNï¼‰æ¥æŒ–æ˜æœ‰ä»·å€¼çš„å†å²ä¿¡æ¯ï¼Œä»¥ç¡®å®šç›®æ ‡çŠ¶æ€é¢„æµ‹çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12888">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15d16fb3c054ece30e6dac1816d707ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42ec37ba5a057fd0b5bd983cbbe0f9cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-836d87a9aea8209b7134a1487ad43267.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LangDA-Building-Context-Awareness-via-Language-for-Domain-Adaptive-Semantic-Segmentation"><a href="#LangDA-Building-Context-Awareness-via-Language-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="LangDA: Building Context-Awareness via Language for Domain Adaptive   Semantic Segmentation"></a>LangDA: Building Context-Awareness via Language for Domain Adaptive   Semantic Segmentation</h2><p><strong>Authors:Chang Liu, Bavesh Balaji, Saad Hossain, C Thomas, Kwei-Herng Lai, Raviteja Vemulapalli, Alexander Wong, Sirisha Rambhatla</strong></p>
<p>Unsupervised domain adaptation for semantic segmentation (DASS) aims to transfer knowledge from a label-rich source domain to a target domain with no labels. Two key approaches in DASS are (1) vision-only approaches using masking or multi-resolution crops, and (2) language-based approaches that use generic class-wise prompts informed by target domain (e.g. â€œa {snowy} photo of a {class}â€). However, the former is susceptible to noisy pseudo-labels that are biased to the source domain. The latter does not fully capture the intricate spatial relationships of objects â€“ key for dense prediction tasks. To this end, we propose LangDA. LangDA addresses these challenges by, first, learning contextual relationships between objects via VLM-generated scene descriptions (e.g. â€œa pedestrian is on the sidewalk, and the street is lined with buildings.â€). Second, LangDA aligns the entire image features with text representation of this context-aware scene caption and learns generalized representations via text. With this, LangDA sets the new state-of-the-art across three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and 3.9%. </p>
<blockquote>
<p>æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ï¼ˆDASSï¼‰æ—¨åœ¨å°†ä»æ ‡ç­¾ä¸°å¯Œçš„æºåŸŸè½¬ç§»çš„çŸ¥è¯†åº”ç”¨åˆ°æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸã€‚DASSä¸­çš„ä¸¤ç§ä¸»è¦æ–¹æ³•æ˜¯ï¼ˆ1ï¼‰ä»…ä½¿ç”¨è§†è§‰çš„æ–¹æ³•ï¼Œé‡‡ç”¨é®æŒ¡æˆ–å¤šåˆ†è¾¨ç‡è£å‰ªï¼Œä»¥åŠï¼ˆ2ï¼‰åŸºäºè¯­è¨€çš„æ–¹æ³•ï¼Œä½¿ç”¨ç”±ç›®æ ‡åŸŸä¿¡æ¯æŒ‡å¯¼çš„é€šç”¨ç±»åˆ«æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œä¸€ä¸ª{snowy}çš„{class}çš„ç…§ç‰‡â€ï¼‰ã€‚ç„¶è€Œï¼Œå‰è€…å®¹æ˜“å—åˆ°åå‘æºåŸŸçš„å™ªå£°ä¼ªæ ‡ç­¾çš„å½±å“ã€‚åè€…åˆ™æ— æ³•å®Œå…¨æ•è·å¯¹è±¡çš„å¤æ‚ç©ºé—´å…³ç³»ï¼Œè¿™åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LangDAæ–¹æ³•ã€‚LangDAé€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šé¦–å…ˆï¼Œé€šè¿‡VLMç”Ÿæˆçš„åœºæ™¯æè¿°å­¦ä¹ å¯¹è±¡ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œè¡Œäººåœ¨äººè¡Œé“ä¸Šï¼Œè¡—é“ä¸¤æ—éƒ½æ˜¯å»ºç­‘ã€‚â€ï¼‰ï¼›å…¶æ¬¡ï¼ŒLangDAå°†æ•´å¹…å›¾åƒçš„ç‰¹å¾ä¸è¿™ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åœºæ™¯å­—å¹•çš„æ–‡æœ¬è¡¨ç¤ºå¯¹é½ï¼Œå¹¶é€šè¿‡æ–‡æœ¬å­¦ä¹ é€šç”¨è¡¨ç¤ºã€‚å› æ­¤ï¼ŒLangDAåœ¨ä¸‰ä¸ªDASSåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•åˆ†åˆ«æé«˜äº†2.6%ã€1.4%å’Œ3.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12780v1">PDF</a> </p>
<p><strong>Summary</strong><br>    DASSé‡‡ç”¨æ— ç›‘ç£åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œå°†æ ‡ç­¾ä¸°å¯Œçš„æºåŸŸçŸ¥è¯†è¿ç§»åˆ°æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å™ªå£°ä¼ªæ ‡ç­¾åå‘æºåŸŸä»¥åŠæ— æ³•å®Œå…¨æ•æ‰å¯¹è±¡é—´å¤æ‚ç©ºé—´å…³ç³»çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºLangDAæ–¹æ³•ã€‚LangDAé€šè¿‡å­¦ä¹ å¯¹è±¡é—´çš„ä¸Šä¸‹æ–‡å…³ç³»å¹¶åˆ©ç”¨VLMç”Ÿæˆçš„åœºæ™¯æè¿°æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä¾‹å¦‚â€œè¡Œäººåœ¨äººè¡Œé“ä¸Šï¼Œè¡—é“ä¸¤æ—æœ‰å»ºç­‘ç‰©ã€‚â€æ­¤å¤–ï¼ŒLangDAå°†æ•´å¹…å›¾åƒçš„ç‰¹å¾ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åœºæ™¯å­—å¹•æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œå¹¶é€šè¿‡æ–‡æœ¬å­¦ä¹ é€šç”¨è¡¨ç¤ºã€‚è¿™ä½¿LangDAåœ¨ä¸‰ä¸ªDASSåŸºå‡†æµ‹è¯•ä¸­å‡åˆ›ä¸‹æ–°é«˜ï¼Œè¾ƒç°æœ‰æ–¹æ³•æé«˜äº†2.6%ã€1.4%å’Œ3.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DASSæ—¨åœ¨å®ç°æ— ç›‘ç£åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œå°†çŸ¥è¯†ä»æ ‡ç­¾ä¸°å¯Œçš„æºåŸŸè¿ç§»åˆ°æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸã€‚</li>
<li>å½“å‰ä¸¤å¤§å…³é”®æ–¹æ³•åˆ†åˆ«ä¸ºä»…ä¾èµ–è§†è§‰çš„æ–¹æ³•å’ŒåŸºäºè¯­è¨€çš„æ–¹æ³•ã€‚è§†è§‰æ–¹æ³•æ˜“å—å™ªå£°ä¼ªæ ‡ç­¾å½±å“ä¸”æ˜“åå‘æºåŸŸï¼Œè€ŒåŸºäºè¯­è¨€çš„æ–¹æ³•åˆ™æ— æ³•å®Œå…¨æ•æ‰å¯¹è±¡çš„å¤æ‚ç©ºé—´å…³ç³»ã€‚</li>
<li>LangDAæ–¹æ³•é€šè¿‡ç»“åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å®ƒåˆ©ç”¨VLMç”Ÿæˆçš„åœºæ™¯æè¿°æ¥å­¦ä¹ å¯¹è±¡é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ã€‚</li>
<li>LangDAé€šè¿‡å°†æ•´å¹…å›¾åƒçš„ç‰¹å¾ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åœºæ™¯å­—å¹•æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œå®ç°å›¾åƒå’Œæ–‡æœ¬çš„åŒ¹é…å­¦ä¹ ã€‚</li>
<li>LangDAæ–¹æ³•åœ¨ä¸‰ä¸ªDASSåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>LangDAæ–¹æ³•æé«˜äº†è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aab16040eeaac4b996bbd20110d477d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de99814e308d18fc7dbd25e40af5f5bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d4626f0e1eaeb9ed5feafde9ce39536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6164ae8ef17f158281afd49f2ab28055.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0c287d2e8d7269fcdae729628e74211.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4370b435d4781fc032656305414f4a7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43fea0b7b91b0afe9f37dddfcdcf5537.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Advanced-AI-based-Object-Detection-Models-for-Pavement-Marking-Quality-Assessment-during-Daytime"><a href="#Comparative-Analysis-of-Advanced-AI-based-Object-Detection-Models-for-Pavement-Marking-Quality-Assessment-during-Daytime" class="headerlink" title="Comparative Analysis of Advanced AI-based Object Detection Models for   Pavement Marking Quality Assessment during Daytime"></a>Comparative Analysis of Advanced AI-based Object Detection Models for   Pavement Marking Quality Assessment during Daytime</h2><p><strong>Authors:Gian Antariksa, Rohit Chakraborty, Shriyank Somvanshi, Subasish Das, Mohammad Jalayer, Deep Rameshkumar Patel, David Mills</strong></p>
<p>Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings. </p>
<blockquote>
<p>åˆ©ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œè§†è§‰å¯¹è±¡æ£€æµ‹åœ¨è®¡ç®—æœºè§†è§‰ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œåœ¨äº¤é€šå·¥ç¨‹ä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚æœ¬æ–‡é‡ç‚¹ç ”ç©¶åœ¨ç™½å¤©åˆ©ç”¨You Only Look Onceï¼ˆYOLOï¼‰æ¨¡å‹æ£€æµ‹è·¯é¢æ ‡è®°è´¨é‡ï¼Œåˆ©ç”¨å…¶å…ˆè¿›çš„æ¶æ„ç‰¹æ€§è¿›è¡Œç²¾ç¡®å®æ—¶çš„è¯„ä¼°ï¼Œä»¥æé«˜é“è·¯å®‰å…¨ã€‚åˆ©ç”¨æ–°æ³½è¥¿å·çš„å›¾åƒæ•°æ®ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨äº†ä¸‰ç§YOLOv8å˜ä½“ï¼šYOLOv8mã€YOLOv8nå’ŒYOLOv8xã€‚è¿™äº›æ¨¡å‹çš„è¯„ä¼°åŸºäºå®ƒä»¬å¯¹è·¯é¢æ ‡è®°è¿›è¡Œè‰¯å¥½ã€ä¸­ç­‰å’Œä¸è‰¯èƒ½è§åº¦åˆ†ç±»çš„é¢„æµ‹ç²¾åº¦ã€‚ç»“æœè¡¨æ˜ï¼ŒYOLOv8nåœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œå¯¹äºå…·æœ‰è‰¯å¥½èƒ½è§åº¦çš„å¯¹è±¡å®ç°äº†æœ€é«˜çš„å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ï¼Œå¹¶åœ¨å„ç§äº¤å¹¶æ¯”ï¼ˆIoUï¼‰é˜ˆå€¼ä¸‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶é€šè¿‡æä¾›ä¸€ç§è‡ªåŠ¨åŒ–ã€å‡†ç¡®çš„æ–¹æ³•æ¥å¯¹è·¯é¢æ ‡è®°è´¨é‡è¿›è¡Œè¯„ä¼°ï¼Œæé«˜äº†äº¤é€šè¿è¾“å®‰å…¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11008v2">PDF</a> 6 pages, 3 figures, accepted at IEEE CAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨æ·±åº¦å­¦ä¹ ä¸­çš„YOLOæ¨¡å‹å¯¹æ—¥é—´è·¯é¢æ ‡è®°è´¨é‡è¿›è¡Œæ£€æµ‹ç ”ç©¶ï¼Œä»¥æé«˜é“è·¯å®‰å…¨ã€‚é€šè¿‡å¯¹æ–°æ³½è¥¿å·çš„å›¾åƒæ•°æ®è¿›è¡Œå®éªŒï¼Œå‘ç°YOLOv8næ¨¡å‹åœ¨åˆ†ç±»è·¯é¢æ ‡è®°ä¸ºè‰¯å¥½ã€ä¸­ç­‰å’Œä¸è‰¯èƒ½è§åº¦ç±»åˆ«æ—¶ï¼Œå®ç°äº†æœ€é«˜çš„å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰é˜ˆå€¼ä¸‹è¡¨ç°ç¨³å¥ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜è·¯é¢æ ‡è®°è´¨é‡çš„è‡ªåŠ¨è¯„ä¼°æä¾›äº†ä¸€ç§å‡†ç¡®çš„æ–¹æ³•ï¼Œå¢å¼ºäº†äº¤é€šè¿è¾“çš„å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>YOLOæ¨¡å‹è¢«ç”¨äºæ£€æµ‹æ—¥é—´è·¯é¢æ ‡è®°è´¨é‡ï¼Œä»¥å¢å¼ºé“è·¯å®‰å…¨ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†YOLOv8çš„ä¸‰ä¸ªå˜ä½“ï¼šYOLOv8mã€YOLOv8nå’ŒYOLOv8xã€‚</li>
<li>YOLOv8næ¨¡å‹åœ¨åˆ†ç±»è·¯é¢æ ‡è®°çš„é¢„æµ‹ç²¾åº¦ä¸Šè¡¨ç°æœ€ä½³ï¼Œå®ç°äº†æœ€é«˜çš„å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨æ–°æ³½è¥¿å·çš„å›¾åƒæ•°æ®è¿›è¡Œå®éªŒã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ£€æµ‹ä¸åŒèƒ½è§åº¦çš„è·¯é¢æ ‡è®°æ—¶è¡¨ç°å‡ºç¨³å¥æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨è¯„ä¼°è·¯é¢æ ‡è®°è´¨é‡æä¾›äº†ä¸€ç§å‡†ç¡®çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11008">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a2f95e5b2e10dfd669ba5b43292651d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf9792f01a0fc688bcb8bc10a5c728fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a54cf4e1e26a5846f63b0f583df163.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a548e1d842fc24ed0c7405fcb461181b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dca2a4b949f5ba6c64dfe72239252725.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b8a1c62d8b5b5429faafa585821413.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Cyclic-Contrastive-Knowledge-Transfer-for-Open-Vocabulary-Object-Detection"><a href="#Cyclic-Contrastive-Knowledge-Transfer-for-Open-Vocabulary-Object-Detection" class="headerlink" title="Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object   Detection"></a>Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object   Detection</h2><p><strong>Authors:Chuhan Zhang, Chaoyang Zhu, Pingcheng Dong, Long Chen, Dong Zhang</strong></p>
<p>In pursuit of detecting unstinted objects that extend beyond predefined categories, prior arts of open-vocabulary object detection (OVD) typically resort to pretrained vision-language models (VLMs) for base-to-novel category generalization. However, to mitigate the misalignment between upstream image-text pretraining and downstream region-level perception, additional supervisions are indispensable, eg, image-text pairs or pseudo annotations generated via self-training strategies. In this work, we propose CCKT-Det trained without any extra supervision. The proposed framework constructs a cyclic and dynamic knowledge transfer from language queries and visual region features extracted from VLMs, which forces the detector to closely align with the visual-semantic space of VLMs. Specifically, 1) we prefilter and inject semantic priors to guide the learning of queries, and 2) introduce a regional contrastive loss to improve the awareness of queries on novel objects. CCKT-Det can consistently improve performance as the scale of VLMs increases, all while requiring the detector at a moderate level of computation overhead. Comprehensive experimental results demonstrate that our method achieves performance gain of +2.9% and +10.2% AP50 over previous state-of-the-arts on the challenging COCO benchmark, both without and with a stronger teacher model. The code is provided at <a target="_blank" rel="noopener" href="https://github.com/ZCHUHan/CCKT-Det">https://github.com/ZCHUHan/CCKT-Det</a>. </p>
<blockquote>
<p>åœ¨è¿½æ±‚æ£€æµ‹æ— é™åˆ¶å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡è¶…å‡ºé¢„å®šç±»åˆ«çš„æƒ…å†µä¸‹ï¼Œå¼€æ”¾è¯æ±‡è¡¨å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰çš„ç°æœ‰æŠ€æœ¯é€šå¸¸ä¾èµ–äºé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒåŸºç¡€åˆ°æ–°é¢–ç±»åˆ«çš„æ¨å¹¿ã€‚ç„¶è€Œï¼Œä¸ºäº†å‡è½»ä¸Šæ¸¸å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒå’Œä¸‹æ¸¸åŒºåŸŸçº§åˆ«æ„ŸçŸ¥ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œé¢å¤–çš„ç›‘ç£æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡è‡ªè®­ç»ƒç­–ç•¥ç”Ÿæˆçš„å›¾åƒæ–‡æœ¬å¯¹æˆ–ä¼ªæ³¨é‡Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€ä»»ä½•é¢å¤–ç›‘ç£çš„CCKT-Detã€‚æ‰€æå‡ºçš„æ¡†æ¶æ„å»ºäº†ä»è¯­è¨€æŸ¥è¯¢å’Œä»VLMæå–çš„è§†è§‰åŒºåŸŸç‰¹å¾ä¹‹é—´çš„å¾ªç¯å’ŒåŠ¨æ€çŸ¥è¯†è½¬ç§»ï¼Œè¿™è¿«ä½¿æ£€æµ‹å™¨ç´§å¯†åœ°å¯¹åº”äºVLMçš„è§†è§‰è¯­ä¹‰ç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰æˆ‘ä»¬é¢„è¿‡æ»¤å¹¶æ³¨å…¥è¯­ä¹‰å…ˆéªŒæ¥æŒ‡å¯¼æŸ¥è¯¢å­¦ä¹ ï¼Œ2ï¼‰å¼•å…¥åŒºåŸŸå¯¹æ¯”æŸå¤±æ¥æé«˜æŸ¥è¯¢å¯¹æ–°å¯¹è±¡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚éšç€VLMè§„æ¨¡çš„å¢åŠ ï¼ŒCCKT-Detå¯ä»¥æŒç»­æé«˜æ€§èƒ½ï¼ŒåŒæ—¶åªéœ€è¦é€‚åº¦çš„æ£€æµ‹å™¨è®¡ç®—å¼€é”€ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„COCOåŸºå‡†æµ‹è¯•ä¸Šï¼Œç›¸è¾ƒäºä¹‹å‰çš„æœ€ä¼˜æ°´å¹³ï¼Œæœ‰æ— æ›´å¼ºçš„æ•™å¸ˆæ¨¡å‹åˆ†åˆ«å®ç°äº†+2.9%å’Œ+10.2%çš„AP50æ€§èƒ½æå‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZCHUHan/CCKT-Det%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZCHUHan/CCKT-Detæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11005v1">PDF</a> 10 pages, 5 figures, Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ— éœ€é¢å¤–ç›‘ç£çš„CCKT-Detæ¡†æ¶ï¼Œç”¨äºå¼€æ”¾è¯æ±‡è¡¨å¯¹è±¡æ£€æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå¾ªç¯å’ŒåŠ¨æ€çŸ¥è¯†è½¬ç§»ï¼Œä»è¯­è¨€æŸ¥è¯¢åˆ°è§†è§‰åŒºåŸŸç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œä¿ƒè¿›æ£€æµ‹å™¨ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­ä¹‰ç©ºé—´å¯¹é½ã€‚é€šè¿‡é¢„ç­›é€‰å’Œæ³¨å…¥è¯­ä¹‰å…ˆéªŒæ¥å¼•å¯¼æŸ¥è¯¢å­¦ä¹ ï¼Œå¼•å…¥åŒºåŸŸå¯¹æ¯”æŸå¤±æé«˜æŸ¥è¯¢å¯¹æœªçŸ¥ç‰©ä½“çš„è¯†åˆ«èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨ä¸åŒè§„æ¨¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸ŠæŒç»­æé«˜æ€§èƒ½ï¼ŒåŒæ—¶åªéœ€é€‚åº¦çš„è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CCKT-Detåˆ©ç”¨å¾ªç¯å’ŒåŠ¨æ€çŸ¥è¯†è½¬ç§»ï¼Œå®ç°è¯­è¨€æŸ¥è¯¢åˆ°è§†è§‰åŒºåŸŸç‰¹å¾çš„è®­ç»ƒï¼Œä¿ƒè¿›æ£€æµ‹å™¨ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è§†è§‰è¯­ä¹‰ç©ºé—´å¯¹é½ã€‚</li>
<li>é€šè¿‡é¢„ç­›é€‰å’Œæ³¨å…¥è¯­ä¹‰å…ˆéªŒï¼Œå¼•å¯¼æŸ¥è¯¢å­¦ä¹ ï¼Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥åŒºåŸŸå¯¹æ¯”æŸå¤±ï¼Œæé«˜æ£€æµ‹å™¨å¯¹æœªçŸ¥ç‰©ä½“çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>CCKT-Detå¯åœ¨ä¸åŒè§„æ¨¡çš„VLMsä¸ŠæŒç»­æé«˜æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒCCKT-Detåœ¨COCOåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚<br>6.CCKT-Detçš„è®¡ç®—å¼€é”€é€‚ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-574a8ed0010aeca2bb6bd9a766735fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53f7289550afffe64a2fbaffc09ba83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6dad6363a1ebbfb2f9eb47258ffab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f35ec1971db6907184083f874be5745.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HeightFormer-Learning-Height-Prediction-in-Voxel-Features-for-Roadside-Vision-Centric-3D-Object-Detection-via-Transformer"><a href="#HeightFormer-Learning-Height-Prediction-in-Voxel-Features-for-Roadside-Vision-Centric-3D-Object-Detection-via-Transformer" class="headerlink" title="HeightFormer: Learning Height Prediction in Voxel Features for Roadside   Vision Centric 3D Object Detection via Transformer"></a>HeightFormer: Learning Height Prediction in Voxel Features for Roadside   Vision Centric 3D Object Detection via Transformer</h2><p><strong>Authors:Zhang Zhang, Chao Sun, Chao Yue, Da Wen, Yujie Chen, Tianze Wang, Jianghao Leng</strong></p>
<p>Roadside vision centric 3D object detection has received increasing attention in recent years. It expands the perception range of autonomous vehicles, enhances the road safety. Previous methods focused on predicting per-pixel height rather than depth, making significant gains in roadside visual perception. While it is limited by the perspective property of near-large and far-small on image features, making it difficult for network to understand real dimension of objects in the 3D world. BEV features and voxel features present the real distribution of objects in 3D world compared to the image features. However, BEV features tend to lose details due to the lack of explicit height information, and voxel features are computationally expensive. Inspired by this insight, an efficient framework learning height prediction in voxel features via transformer is proposed, dubbed HeightFormer. It groups the voxel features into local height sequences, and utilize attention mechanism to obtain height distribution prediction. Subsequently, the local height sequences are reassembled to generate accurate 3D features. The proposed method is applied to two large-scale roadside benchmarks, DAIR-V2X-I and Rope3D. Extensive experiments are performed and the HeightFormer outperforms the state-of-the-art methods in roadside vision centric 3D object detection task. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä»¥é“è·¯ä¸ºä¸­å¿ƒçš„3Dç‰©ä½“æ£€æµ‹å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å®ƒæ‰©å±•äº†è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æ„ŸçŸ¥èŒƒå›´ï¼Œæé«˜äº†é“è·¯å®‰å…¨æ€§ã€‚ä¹‹å‰çš„æ–¹æ³•ä¾§é‡äºé¢„æµ‹åƒç´ é«˜åº¦è€Œéæ·±åº¦ï¼Œåœ¨è·¯è¾¹è§†è§‰æ„ŸçŸ¥æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå›¾åƒç‰¹å¾çš„è¿‘å¤§è¿œå°çš„é€è§†å±æ€§ï¼Œç½‘ç»œéš¾ä»¥ç†è§£ç‰©ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„çœŸå®å°ºå¯¸ã€‚ä¸å›¾åƒç‰¹å¾ç›¸æ¯”ï¼ŒBEVç‰¹å¾å’Œä½“ç´ ç‰¹å¾å±•ç¤ºäº†ç‰©ä½“åœ¨çœŸå®ä¸–ç•Œä¸­çš„çœŸå®åˆ†å¸ƒã€‚ç„¶è€Œï¼ŒBEVç‰¹å¾ç”±äºç¼ºä¹æ˜ç¡®çš„é«˜åº¦ä¿¡æ¯è€Œå®¹æ˜“ä¸¢å¤±ç»†èŠ‚ï¼Œè€Œä½“ç´ ç‰¹å¾çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å—è¿™ä¸€è§è§£çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶æ¥å­¦ä¹ é€šè¿‡Transformerè¿›è¡Œä½“ç´ ç‰¹å¾çš„é«˜åº¦é¢„æµ‹ï¼Œç§°ä¸ºHeightFormerã€‚å®ƒå°†ä½“ç´ ç‰¹å¾åˆ†ç»„ä¸ºå±€éƒ¨é«˜åº¦åºåˆ—ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶è·å¾—é«˜åº¦åˆ†å¸ƒé¢„æµ‹ã€‚éšåï¼Œè¿™äº›å±€éƒ¨é«˜åº¦åºåˆ—è¢«é‡æ–°ç»„åˆä»¥ç”Ÿæˆå‡†ç¡®çš„3Dç‰¹å¾ã€‚æ‰€æå‡ºçš„æ–¹æ³•åº”ç”¨äºä¸¤ä¸ªå¤§è§„æ¨¡çš„è·¯è¾¹åŸºå‡†æµ‹è¯•DAIR-V2X-Iå’ŒRope3Dã€‚è¿›è¡Œäº†å¤§é‡çš„å®éªŒï¼ŒHeightFormeråœ¨è·¯è¾¹è§†è§‰ä¸ºä¸­å¿ƒçš„3Dç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è·¯è¾¹è§†è§‰ä¸ºä¸­å¿ƒçš„3Dç›®æ ‡æ£€æµ‹é—®é¢˜ï¼Œæå‡ºäº†HeightFormeræ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶é¢„æµ‹ç‰©ä½“é«˜åº¦åºåˆ—ä¿¡æ¯æ¥è§£å†³å½“å‰æ–¹æ³•ä¸­BEVç‰¹å¾ç¼ºå¤±ç»†èŠ‚å’Œå›¾åƒç‰¹å¾å—é€è§†æ•ˆåº”å½±å“çš„é—®é¢˜ã€‚HeightFormerå°†ä½“ç´ ç‰¹å¾åˆ†ä¸ºå±€éƒ¨é«˜åº¦åºåˆ—å¹¶ç”Ÿæˆå‡†ç¡®çš„ä¸‰ç»´ç‰¹å¾ï¼Œåœ¨å¤§å‹è·¯è¾¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¯è¾¹è§†è§‰ä¸ºä¸­å¿ƒçš„3Dç›®æ ‡æ£€æµ‹é—®é¢˜å—åˆ°å…³æ³¨ï¼Œæ—¨åœ¨æ‰©å¤§è‡ªä¸»è½¦è¾†çš„æ„ŸçŸ¥èŒƒå›´å¹¶æé«˜é“è·¯å®‰å…¨æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é¢„æµ‹åƒç´ é«˜åº¦ï¼Œå­˜åœ¨é€è§†æ•ˆåº”é—®é¢˜ï¼Œå¯¼è‡´ç½‘ç»œéš¾ä»¥ç†è§£çœŸå®çš„ä¸‰ç»´ä¸–ç•Œä¸­çš„ç‰©ä½“å°ºå¯¸ã€‚</li>
<li>BEVç‰¹å¾å’Œä½“ç´ ç‰¹å¾èƒ½å±•ç¤ºç‰©ä½“çš„çœŸå®ä¸‰ç»´åˆ†å¸ƒï¼Œä½†BEVç‰¹å¾æ˜“ä¸¢å¤±ç»†èŠ‚ï¼Œä½“ç´ ç‰¹å¾è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>HeightFormeræ¡†æ¶é€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶é¢„æµ‹é«˜åº¦åˆ†å¸ƒæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>HeightFormerå°†ä½“ç´ ç‰¹å¾åˆ†ç»„ä¸ºå±€éƒ¨é«˜åº¦åºåˆ—ï¼Œç„¶åé‡æ–°ç»„è£…ä»¥ç”Ÿæˆå‡†ç¡®çš„ä¸‰ç»´ç‰¹å¾ã€‚</li>
<li>HeightFormeråœ¨å¤§å‹è·¯è¾¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95d58b6b727e883b359db6e682f6909d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a54ae8769241c1871edd100858b7de44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d127ed994668b56267c527b382d44d61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-052e046480e3a74be3f037a11091f414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4230a9a2b60ae2aa72b96957ead9bbe8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits modelsâ€™ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰éšç€æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•è€Œè¿›æ­¥ï¼Œé€šè¿‡å„ç§å­¦ä¹ æ–¹æ¡ˆå®ç°äº†è¶…å‡ºé¢„å®šç±»åˆ«çš„åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— è®­ç»ƒæ–¹æ³•ä¸ºå¤„ç†æœªè§æ•°æ®æä¾›äº†å¯æ‰©å±•ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯OVSSçš„å…³é”®ç›®æ ‡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼šåœ¨åŸºäºä»»æ„æŸ¥è¯¢æç¤ºçš„OVSSçš„å¤æ‚ç¯å¢ƒä¸­ï¼Œå¯¹å¤æ‚å¯¹è±¡è¿›è¡Œåˆ†å‰²æ—¶ç¼ºä¹å¯¹è±¡çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ã€‚è¿™ä¸€ç–å¿½é™åˆ¶äº†æ¨¡å‹åœ¨å¯¹è±¡å†…ç»„åˆè¯­ä¹‰ä¸€è‡´å…ƒç´ çš„èƒ½åŠ›ï¼Œå¹¶å‡†ç¡®åœ°å°†å®ƒä»¬æ˜ å°„åˆ°ç”¨æˆ·å®šä¹‰çš„ä»»æ„ç±»åˆ«ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é€šè¿‡ç»“åˆå›¾åƒä¸­çš„å¯¹è±¡çº§ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥å…‹æœè¿™ä¸€é™åˆ¶çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å°†ä»è§†è§‰åŸºç¡€æ¨¡å‹æç‚¼çš„è°±é©±åŠ¨ç‰¹å¾è’¸é¦åˆ°è§†è§‰ç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¢å¼ºäº†å¯¹è±¡å†…éƒ¨çš„è¿è´¯æ€§ï¼Œä½¿è¯­ä¹‰ä¸€è‡´çš„ç»„ä»¶èƒ½å¤Ÿå½¢æˆå•ä¸ªå¯¹è±¡æ©ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é›¶ç›®æ ‡å¯¹è±¡å­˜åœ¨å¯èƒ½æ€§æ¥ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œä»¥ç¡®ä¿ä¸å›¾åƒä¸­è¡¨ç¤ºçš„ç‰¹å®šå¯¹è±¡çš„å‡†ç¡®å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨å¯¹è±¡çº§çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†å¦‚ä½•åˆ©ç”¨ç‰©ä½“çº§åˆ«çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥æå‡Open-Vocabulary Semantic Segmentationæ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿæ¨¡å‹ä¸­åœ¨åˆ†å‰²å¤æ‚å¯¹è±¡æ—¶ç¼ºä¹å¯¹ç‰©ä½“çº§åˆ«ä¸Šä¸‹æ–‡è€ƒè™‘çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆå›¾åƒä¸­çš„ç‰©ä½“çº§åˆ«ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œå¢å¼ºæ¨¡å‹å¯¹ç‰©ä½“å†…éƒ¨çš„ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°è¯­ä¹‰ä¸Šè¿è´¯çš„ç»„ä»¶å½¢æˆå•ä¸ªç‰©ä½“æ©è†œã€‚åŒæ—¶ï¼Œé€šè¿‡æ”¹è¿›æ–‡æœ¬åµŒå…¥å’Œé›¶æ ·æœ¬ç‰©ä½“å­˜åœ¨æ¦‚ç‡ï¼Œç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šç‰©ä½“çš„å‡†ç¡®å¯¹é½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-Vocabulary Semantic Segmentation (OVSS) å…è®¸é€šè¿‡ä¸åŒçš„å­¦ä¹ æ–¹æ¡ˆè¿›è¡Œè¶…è¶Šé¢„è®¾ç±»åˆ«çš„åˆ†å‰²ã€‚</li>
<li>è®­ç»ƒæ— å…³çš„æ–¹æ³•ä¸ºå¤„ç†æœªè§æ•°æ®æä¾›äº†å¯æ‰©å±•ã€æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯OVSSçš„å…³é”®ç›®æ ‡ã€‚</li>
<li>å½“å‰å·¥ä½œä¸­ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯ç¼ºä¹ç‰©ä½“çº§åˆ«çš„ä¸Šä¸‹æ–‡è€ƒè™‘ï¼Œåœ¨åŸºäºä»»æ„æŸ¥è¯¢æç¤ºçš„OVSSç¯å¢ƒä¸­åˆ†å‰²å¤æ‚å¯¹è±¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ–°çš„æ–¹æ³•é€šè¿‡èåˆå›¾åƒä¸­çš„ç‰©ä½“çº§åˆ«ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¢å¼ºæ¨¡å‹å¯¹ç‰©ä½“å†…éƒ¨çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°è¯­ä¹‰ä¸Šè¿è´¯çš„ç»„ä»¶å½¢æˆå•ä¸ªç‰©ä½“æ©è†œï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ”¹è¿›æ–‡æœ¬åµŒå…¥å’Œé›¶æ ·æœ¬ç‰©ä½“å­˜åœ¨æ¦‚ç‡ï¼Œç¡®ä¿ä¸å›¾åƒä¸­ç‰¹å®šç‰©ä½“çš„å‡†ç¡®å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a4674b24fe322761820fa176c1eb16e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b17542025b8df1633d8f8c54cbb60b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e98ebfbdecc978c059d2cb079af8da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf6cd30b1479ae00d8615ab98575a118.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Believing-is-Seeing-Unobserved-Object-Detection-using-Generative-Models"><a href="#Believing-is-Seeing-Unobserved-Object-Detection-using-Generative-Models" class="headerlink" title="Believing is Seeing: Unobserved Object Detection using Generative Models"></a>Believing is Seeing: Unobserved Object Detection using Generative Models</h2><p><strong>Authors:Subhransu S. Bhattacharjee, Dylan Campbell, Rahul Shome</strong></p>
<p>Can objects that are not visible in an image â€“ but are in the vicinity of the camera â€“ be detected? This study introduces the novel tasks of 2D, 2.5D and 3D unobserved object detection for predicting the location of nearby objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to address this task, including 2D and 3D diffusion models and vision-language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that capture different aspects of performance. Our empirical evaluation on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the use of generative models for the unobserved object detection task. </p>
<blockquote>
<p>åœ¨å›¾åƒä¸­ä¸å¯è§ä½†ä½äºç›¸æœºé™„è¿‘çš„å¯¹è±¡èƒ½å¦è¢«æ£€æµ‹å‡ºæ¥ï¼Ÿæœ¬ç ”ç©¶å¼•å…¥äº†ç”¨äºé¢„æµ‹è¢«é®æŒ¡æˆ–ä½äºå›¾åƒå¸§å¤–çš„é™„è¿‘å¯¹è±¡ä½ç½®çš„äºŒç»´ã€2.5ç»´å’Œä¸‰ç»´æœªè§‚æµ‹å¯¹è±¡æ£€æµ‹çš„æ–°ä»»åŠ¡ã€‚æˆ‘ä»¬é€‚åº”äº†å‡ ç§æœ€å…ˆè¿›çš„é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹æ¥å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬äºŒç»´å’Œä¸‰ç»´æ‰©æ•£æ¨¡å‹ä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶è¡¨æ˜å®ƒä»¬å¯ç”¨äºæ¨æ–­æœªç›´æ¥è§‚å¯Ÿåˆ°çš„å¯¹è±¡çš„å­˜åœ¨ã€‚ä¸ºäº†è¯„ä¼°è¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€å¥—èƒ½å¤Ÿåæ˜ ä¸åŒæ€§èƒ½æ–¹é¢çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬åœ¨RealEstate10kå’ŒNYU Depth v2æ•°æ®é›†ä¸Šçš„å®¤å†…åœºæ™¯å®è¯è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œæœªè§‚æµ‹å¯¹è±¡æ£€æµ‹ä»»åŠ¡æ˜¯å¯è¡Œçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05869v3">PDF</a> IEEE&#x2F;CVF Computer Vision and Pattern Recognition 2025; 22 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶ä¸å¯è§ç‰©ä½“çš„æ£€æµ‹é—®é¢˜ï¼Œæ¢ç´¢äº†2Dã€2.5Då’Œ3Dæœªè§‚æµ‹ç‰©ä½“æ£€æµ‹çš„æ–°ä»»åŠ¡ã€‚ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ŒåŒ…æ‹¬2Då’Œ3Dæ‰©æ•£æ¨¡å‹ä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†åœ¨æœªç›´æ¥è§‚æµ‹ç‰©ä½“çš„æƒ…å†µä¸‹è¿›è¡Œæ¨æ–­çš„èƒ½åŠ›ã€‚ä¸ºè¯„ä¼°æ­¤ä»»åŠ¡æ€§èƒ½ï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—è¯„ä»·æŒ‡æ ‡ã€‚åœ¨RealEstate10kå’ŒNYU Depth v2æ•°æ®é›†ä¸Šçš„å®¤å†…åœºæ™¯å®è¯è¯„ä¼°ç»“æœè¯æ˜äº†ç”Ÿæˆæ¨¡å‹åœ¨æœªè§‚æµ‹ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æœªè§‚æµ‹ç‰©ä½“æ£€æµ‹çš„æ–°ä»»åŠ¡ï¼Œæ¢ç´¢é¢„æµ‹å›¾åƒä¸­ä¸å¯è§ç‰©ä½“çš„ä½ç½®ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>æå‡ºä¸€å¥—æ€§èƒ½è¯„ä»·æŒ‡æ ‡ä»¥è¯„ä¼°æœªè§‚æµ‹ç‰©ä½“æ£€æµ‹ä»»åŠ¡çš„æ•ˆæœã€‚</li>
<li>ç ”ç©¶æ¶µç›–ä»äºŒç»´åˆ°ä¸‰ç»´çš„å¤šç§æ£€æµ‹åœºæ™¯ã€‚</li>
<li>å®è¯è¯„ä¼°è¯æ˜äº†ç”Ÿæˆæ¨¡å‹åœ¨æœªè§‚æµ‹ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå±•ç¤ºäº†æ¨æ–­æœªç›´æ¥è§‚æµ‹ç‰©ä½“å­˜åœ¨çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a7d9f616fe9c6f5fbc7cbe6a77e59d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6eff8ced9348e87b7694e15f4077cad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a85ca8415f5e032cf76849d28e8c5b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e3f4f85b9de671f8231fd675f9522b5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SimPLR-A-Simple-and-Plain-Transformer-for-Efficient-Object-Detection-and-Segmentation"><a href="#SimPLR-A-Simple-and-Plain-Transformer-for-Efficient-Object-Detection-and-Segmentation" class="headerlink" title="SimPLR: A Simple and Plain Transformer for Efficient Object Detection   and Segmentation"></a>SimPLR: A Simple and Plain Transformer for Efficient Object Detection   and Segmentation</h2><p><strong>Authors:Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek</strong></p>
<p>The ability to detect objects in images at varying scales has played a pivotal role in the design of modern object detectors. Despite considerable progress in removing hand-crafted components and simplifying the architecture with transformers, multi-scale feature maps and pyramid designs remain a key factor for their empirical success. In this paper, we show that shifting the multiscale inductive bias into the attention mechanism can work well, resulting in a plain detector &#96;SimPLRâ€™ whose backbone and detection head are both non-hierarchical and operate on single-scale features. We find through our experiments that SimPLR with scale-aware attention is plain and simple architecture, yet competitive with multi-scale vision transformer alternatives. Compared to the multi-scale and single-scale state-of-the-art, our model scales better with bigger capacity (self-supervised) models and more pre-training data, allowing us to report a consistently better accuracy and faster runtime for object detection, instance segmentation, as well as panoptic segmentation. Code is released at <a target="_blank" rel="noopener" href="https://github.com/kienduynguyen/SimPLR">https://github.com/kienduynguyen/SimPLR</a>. </p>
<blockquote>
<p>åœ¨ç°ä»£ç›®æ ‡æ£€æµ‹å™¨è®¾è®¡ä¸­ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒå°ºåº¦ä¸Šæ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å°½ç®¡åœ¨å»é™¤æ‰‹å·¥ç»„ä»¶ã€ç®€åŒ–æ¶æ„æ–¹é¢ä»¥åŠä½¿ç”¨transformeræ–¹é¢å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†å¤šå°ºåº¦ç‰¹å¾æ˜ å°„å’Œé‡‘å­—å¡”è®¾è®¡ä»ç„¶æ˜¯å…¶ç»éªŒæˆåŠŸçš„å…³é”®å› ç´ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å°†å¤šå°ºåº¦å½’çº³åç½®è½¬ç§»åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­å¯ä»¥å¾ˆå¥½åœ°å·¥ä½œï¼Œä»è€Œå¾—åˆ°äº†ä¸€ä¸ªçº¯æ£€æµ‹å™¨SimPLRï¼Œå…¶ä¸»å¹²å’Œæ£€æµ‹å¤´éƒ½æ˜¯éåˆ†å±‚çš„ï¼Œå¹¶åœ¨å•å°ºåº¦ç‰¹å¾ä¸Šè¿è¡Œã€‚æˆ‘ä»¬é€šè¿‡å®éªŒå‘ç°ï¼Œå¸¦æœ‰å°ºåº¦æ„ŸçŸ¥æ³¨æ„åŠ›çš„SimPLRæ˜¯ä¸€ä¸ªç®€å•æ˜äº†çš„æ¶æ„ï¼Œä½†ä¸å¤šå°ºåº¦è§†è§‰transformeræ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ä»å…·æœ‰ç«äº‰åŠ›ã€‚ä¸å¤šå°ºåº¦å’Œå•å°ºåº¦æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ›´å¤§å®¹é‡ï¼ˆè‡ªç›‘ç£ï¼‰çš„æ¨¡å‹å’Œæ›´å¤šé¢„è®­ç»ƒæ•°æ®çš„æ”¯æŒä¸‹è¡¨ç°æ›´å¥½ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²æ–¹é¢æŠ¥å‘Šæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´å¿«çš„è¿è¡Œæ—¶é—´ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/kienduynguyen/SimPLR%E3%80%82">https://github.com/kienduynguyen/SimPLRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05920v4">PDF</a> In Proceeding of TMLRâ€™2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†ç›®æ ‡æ£€æµ‹ä¸­çš„å¤šå°ºåº¦ç‰¹å¾å¤„ç†é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•SimPLRï¼Œé€šè¿‡å°†å¤šå°ºåº¦å½’çº³åè§è½¬ç§»åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå®ç°äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ£€æµ‹å™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒSimPLRæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨æ›´å¤§å®¹é‡å’Œæ›´å¤šé¢„è®­ç»ƒæ•°æ®æ—¶ã€‚ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šå°ºåº¦ç‰¹å¾å¤„ç†åœ¨ç°ä»£ç›®æ ‡æ£€æµ‹å™¨è®¾è®¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>SimPLRé€šè¿‡å°†å¤šå°ºåº¦å½’çº³åè§èå…¥æ³¨æ„åŠ›æœºåˆ¶æ¥æ”¹è¿›ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>SimPLRæ˜¯ä¸€ä¸ªç®€å•ä¸”æœ‰æ•ˆçš„éå±‚æ¬¡ç»“æ„æ£€æµ‹å™¨ï¼Œå¯åœ¨å•å°ºåº¦ç‰¹å¾ä¸Šæ“ä½œã€‚</li>
<li>å®éªŒè¡¨æ˜SimPLRä¸å¤šå°ºåº¦è§†è§‰è½¬æ¢å™¨ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>SimPLRåœ¨æ›´å¤§çš„å®¹é‡æ¨¡å‹å’Œæ›´å¤šé¢„è®­ç»ƒæ•°æ®ä¸‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>SimPLRåœ¨ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.05920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-88d4060ce5636e9f6ce41a7dfc0e1eda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21730a3a41fec4508adf177cd7f0f2ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64996aa22a4f986ba5921adf438142e6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f62149865cc2ffeee76e3d8ee468d7e0.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Deepfake Detection of Face Images based on a Convolutional Neural   Network
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ac915e3876c60b287ac49530c98cf191.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  Towards Scalable Foundation Model for Multi-modal and Hyperspectral   Geospatial Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
