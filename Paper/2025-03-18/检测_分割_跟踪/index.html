<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-03-18  Beyond RGB Adaptive Parallel Processing for RAW Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c6eff8ced9348e87b7694e15f4077cad.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-18-更新"><a href="#2025-03-18-更新" class="headerlink" title="2025-03-18 更新"></a>2025-03-18 更新</h1><h2 id="Beyond-RGB-Adaptive-Parallel-Processing-for-RAW-Object-Detection"><a href="#Beyond-RGB-Adaptive-Parallel-Processing-for-RAW-Object-Detection" class="headerlink" title="Beyond RGB: Adaptive Parallel Processing for RAW Object Detection"></a>Beyond RGB: Adaptive Parallel Processing for RAW Object Detection</h2><p><strong>Authors:Shani Gamrian, Hila Barel, Feiran Li, Masakazu Yoshimura, Daisuke Iso</strong></p>
<p>Object detection models are typically applied to standard RGB images processed through Image Signal Processing (ISP) pipelines, which are designed to enhance sensor-captured RAW images for human vision. However, these ISP functions can lead to a loss of critical information that may be essential in optimizing for computer vision tasks, such as object detection. In this work, we introduce Raw Adaptation Module (RAM), a module designed to replace the traditional ISP, with parameters optimized specifically for RAW object detection. Inspired by the parallel processing mechanisms of the human visual system, RAM departs from existing learned ISP methods by applying multiple ISP functions in parallel rather than sequentially, allowing for a more comprehensive capture of image features. These processed representations are then fused in a specialized module, which dynamically integrates and optimizes the information for the target task. This novel approach not only leverages the full potential of RAW sensor data but also enables task-specific pre-processing, resulting in superior object detection performance. Our approach outperforms RGB-based methods and achieves state-of-the-art results across diverse RAW image datasets under varying lighting conditions and dynamic ranges. </p>
<blockquote>
<p>对象检测模型通常应用于通过图像信号处理（ISP）管道处理的标准RGB图像，这些管道旨在增强为增强人类视觉而捕获的传感器RAW图像。然而，这些ISP功能可能导致丢失对于优化计算机视觉任务（如对象检测）至关重要的关键信息。在这项工作中，我们引入了原始适配模块（RAM），这是一个旨在替代传统ISP的模块，其参数专为RAW对象检测优化。受人类视觉系统并行处理机制的启发，RAM通过与现有学习ISP方法不同的方式应用多个ISP功能并行而非顺序地应用，从而可以更全面地捕获图像特征。这些处理后的表示形式随后在一个专用模块中进行融合，该模块动态集成并优化针对目标任务的信息。这种新颖的方法不仅充分利用了RAW传感器的全部潜力，还实现了针对任务的预处理，从而实现了卓越的对象检测性能。我们的方法在多种RAW图像数据集上表现出优于基于RGB的方法，并在不同的照明条件和动态范围内达到了最新技术成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13163v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了在传统图像信号处理（ISP）管道中应用于标准RGB图像的对象检测模型可能会丢失关键信息，影响计算机视觉任务的优化。因此，研究提出了Raw Adaptation Module（RAM）模块，以替代传统的ISP，并针对RAW对象检测进行优化。RAM模块通过并行应用多个ISP功能，从原始传感器数据中提取更多特征信息，并动态集成和优化这些信息以完成目标检测任务。该方法不仅充分利用了RAW传感器数据的潜力，还实现了针对特定任务的预处理，从而获得卓越的对象检测性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>传统ISP管道设计主要面向人类视觉优化，可能丢失关键信息，影响计算机视觉任务的性能。</li>
<li>提出了Raw Adaptation Module（RAM）模块，替代传统ISP，针对RAW对象检测进行优化。</li>
<li>RAM模块通过并行应用多个ISP功能，提取更多图像特征信息。</li>
<li>RAM模块采用动态集成和优化信息的方式，以完成目标检测任务。</li>
<li>该方法充分利用RAW传感器数据的潜力。</li>
<li>实现针对特定任务的预处理，提高对象检测性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13163">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7394f80225ae5bb50e276675aae71925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cfe9cc655d2977b8ab37a0437867f52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-222fdb7b4191212eb804d0c42f4933b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab2a30c80a4125818eeb2ec489f24e13.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UncTrack-Reliable-Visual-Object-Tracking-with-Uncertainty-Aware-Prototype-Memory-Network"><a href="#UncTrack-Reliable-Visual-Object-Tracking-with-Uncertainty-Aware-Prototype-Memory-Network" class="headerlink" title="UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware   Prototype Memory Network"></a>UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware   Prototype Memory Network</h2><p><strong>Authors:Siyuan Yao, Yang Guo, Yanyang Yan, Wenqi Ren, Xiaochun Cao</strong></p>
<p>Transformer-based trackers have achieved promising success and become the dominant tracking paradigm due to their accuracy and efficiency. Despite the substantial progress, most of the existing approaches tackle object tracking as a deterministic coordinate regression problem, while the target localization uncertainty has been greatly overlooked, which hampers trackers’ ability to maintain reliable target state prediction in challenging scenarios. To address this issue, we propose UncTrack, a novel uncertainty-aware transformer tracker that predicts the target localization uncertainty and incorporates this uncertainty information for accurate target state inference. Specifically, UncTrack utilizes a transformer encoder to perform feature interaction between template and search images. The output features are passed into an uncertainty-aware localization decoder (ULD) to coarsely predict the corner-based localization and the corresponding localization uncertainty. Then the localization uncertainty is sent into a prototype memory network (PMN) to excavate valuable historical information to identify whether the target state prediction is reliable or not. To enhance the template representation, the samples with high confidence are fed back into the prototype memory bank for memory updating, making the tracker more robust to challenging appearance variations. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ManOfStory/UncTrack">https://github.com/ManOfStory/UncTrack</a>. </p>
<blockquote>
<p>基于Transformer的跟踪器由于其准确性和高效性而取得了有前景的成功，并成为主导跟踪范式。尽管取得了重大进展，但大多数现有方法都将目标跟踪视为确定性坐标回归问题，而目标定位的不确定性却被大大忽视了，这阻碍了跟踪器在具有挑战性的场景中保持可靠的目标状态预测的能力。为了解决这一问题，我们提出了UncTrack，这是一种新型的不确定性感知Transformer跟踪器，它预测目标定位的不确定性，并将这种不确定性信息用于准确的目标状态推断。具体来说，UncTrack利用Transformer编码器执行模板图像和搜索图像之间的特征交互。输出特征被传递到不确定性感知定位解码器（ULD）中，以粗略预测基于角点的定位和相应的定位不确定性。然后，定位不确定性被发送到原型记忆网络（PMN）中，以挖掘有价值的历史信息来确定目标状态预测是否可靠。为了提高模板表示能力，高置信度的样本被反馈到原型记忆库中进行记忆更新，使跟踪器对挑战性的外观变化更加鲁棒。大量实验表明，我们的方法优于其他最先进的方法。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/ManOfStory/UncTrack%E3%80%82">https://github.com/ManOfStory/UncTrack。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12888v1">PDF</a> 14 pages,11 figures,references added</p>
<p><strong>Summary</strong>：<br>基于Transformer的跟踪器已成为目标跟踪的主流范式，因其准确性和高效性而受到广泛关注。然而，现有方法大多将目标跟踪视为确定性坐标回归问题，忽略了目标定位的不确定性，这在挑战场景下限制了跟踪器的可靠目标状态预测能力。为解决这一问题，我们提出了UncTrack，一种新型的不确定性感知Transformer跟踪器，它预测目标定位的不确定性，并结合这一不确定性信息进行准确的目标状态推断。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Transformer跟踪器已成为主流，因为它们既准确又高效。</li>
<li>现有方法主要关注确定性坐标回归，忽略了目标定位的不确定性。</li>
<li>UncTrack是一种新型的不确定性感知Transformer跟踪器。</li>
<li>UncTrack利用变压器编码器进行模板和搜索图像之间的特征交互。</li>
<li>UncTrack有一个不确定性感知定位解码器（ULD），可粗略预测基于角落的定位和相应的定位不确定性。</li>
<li>UncTrack使用原型记忆网络（PMN）来挖掘有价值的历史信息，以确定目标状态预测的可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12888">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-15d16fb3c054ece30e6dac1816d707ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42ec37ba5a057fd0b5bd983cbbe0f9cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-836d87a9aea8209b7134a1487ad43267.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LangDA-Building-Context-Awareness-via-Language-for-Domain-Adaptive-Semantic-Segmentation"><a href="#LangDA-Building-Context-Awareness-via-Language-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="LangDA: Building Context-Awareness via Language for Domain Adaptive   Semantic Segmentation"></a>LangDA: Building Context-Awareness via Language for Domain Adaptive   Semantic Segmentation</h2><p><strong>Authors:Chang Liu, Bavesh Balaji, Saad Hossain, C Thomas, Kwei-Herng Lai, Raviteja Vemulapalli, Alexander Wong, Sirisha Rambhatla</strong></p>
<p>Unsupervised domain adaptation for semantic segmentation (DASS) aims to transfer knowledge from a label-rich source domain to a target domain with no labels. Two key approaches in DASS are (1) vision-only approaches using masking or multi-resolution crops, and (2) language-based approaches that use generic class-wise prompts informed by target domain (e.g. “a {snowy} photo of a {class}”). However, the former is susceptible to noisy pseudo-labels that are biased to the source domain. The latter does not fully capture the intricate spatial relationships of objects – key for dense prediction tasks. To this end, we propose LangDA. LangDA addresses these challenges by, first, learning contextual relationships between objects via VLM-generated scene descriptions (e.g. “a pedestrian is on the sidewalk, and the street is lined with buildings.”). Second, LangDA aligns the entire image features with text representation of this context-aware scene caption and learns generalized representations via text. With this, LangDA sets the new state-of-the-art across three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and 3.9%. </p>
<blockquote>
<p>无监督领域自适应语义分割（DASS）旨在将从标签丰富的源域转移的知识应用到无标签的目标域。DASS中的两种主要方法是（1）仅使用视觉的方法，采用遮挡或多分辨率裁剪，以及（2）基于语言的方法，使用由目标域信息指导的通用类别提示（例如，“一个{snowy}的{class}的照片”）。然而，前者容易受到偏向源域的噪声伪标签的影响。后者则无法完全捕获对象的复杂空间关系，这在密集预测任务中至关重要。为此，我们提出了LangDA方法。LangDA通过以下方式解决这些挑战：首先，通过VLM生成的场景描述学习对象之间的上下文关系（例如，“行人在人行道上，街道两旁都是建筑。”）；其次，LangDA将整幅图像的特征与这种上下文感知的场景字幕的文本表示对齐，并通过文本学习通用表示。因此，LangDA在三个DASS基准测试中均达到最新水平，相较于现有方法分别提高了2.6%、1.4%和3.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12780v1">PDF</a> </p>
<p><strong>Summary</strong><br>    DASS采用无监督域自适应技术，将标签丰富的源域知识迁移到无标签的目标域。现有方法存在噪声伪标签偏向源域以及无法完全捕捉对象间复杂空间关系的问题。为此，提出LangDA方法。LangDA通过学习对象间的上下文关系并利用VLM生成的场景描述来解决这些问题，例如“行人在人行道上，街道两旁有建筑物。”此外，LangDA将整幅图像的特征与上下文感知的场景字幕文本表示进行对齐，并通过文本学习通用表示。这使LangDA在三个DASS基准测试中均创下新高，较现有方法提高了2.6%、1.4%和3.9%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DASS旨在实现无监督域自适应技术，将知识从标签丰富的源域迁移到无标签的目标域。</li>
<li>当前两大关键方法分别为仅依赖视觉的方法和基于语言的方法。视觉方法易受噪声伪标签影响且易偏向源域，而基于语言的方法则无法完全捕捉对象的复杂空间关系。</li>
<li>LangDA方法通过结合视觉和语言信息来解决上述问题。它利用VLM生成的场景描述来学习对象间的上下文关系。</li>
<li>LangDA通过将整幅图像的特征与上下文感知的场景字幕文本表示进行对齐，实现图像和文本的匹配学习。</li>
<li>LangDA方法在三个DASS基准测试中表现优异，显著优于现有方法。</li>
<li>LangDA方法提高了语义分割的准确性和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12780">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aab16040eeaac4b996bbd20110d477d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de99814e308d18fc7dbd25e40af5f5bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d4626f0e1eaeb9ed5feafde9ce39536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6164ae8ef17f158281afd49f2ab28055.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0c287d2e8d7269fcdae729628e74211.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4370b435d4781fc032656305414f4a7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43fea0b7b91b0afe9f37dddfcdcf5537.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Advanced-AI-based-Object-Detection-Models-for-Pavement-Marking-Quality-Assessment-during-Daytime"><a href="#Comparative-Analysis-of-Advanced-AI-based-Object-Detection-Models-for-Pavement-Marking-Quality-Assessment-during-Daytime" class="headerlink" title="Comparative Analysis of Advanced AI-based Object Detection Models for   Pavement Marking Quality Assessment during Daytime"></a>Comparative Analysis of Advanced AI-based Object Detection Models for   Pavement Marking Quality Assessment during Daytime</h2><p><strong>Authors:Gian Antariksa, Rohit Chakraborty, Shriyank Somvanshi, Subasish Das, Mohammad Jalayer, Deep Rameshkumar Patel, David Mills</strong></p>
<p>Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings. </p>
<blockquote>
<p>利用深度学习进行视觉对象检测在计算机视觉中起着至关重要的作用，在交通工程中有广泛的应用。本文重点研究在白天利用You Only Look Once（YOLO）模型检测路面标记质量，利用其先进的架构特性进行精确实时的评估，以提高道路安全。利用新泽西州的图像数据，本研究采用了三种YOLOv8变体：YOLOv8m、YOLOv8n和YOLOv8x。这些模型的评估基于它们对路面标记进行良好、中等和不良能见度分类的预测精度。结果表明，YOLOv8n在准确性和计算效率之间达到了最佳平衡，对于具有良好能见度的对象实现了最高的平均精度（mAP），并在各种交并比（IoU）阈值下表现出稳健的性能。本研究通过提供一种自动化、准确的方法来对路面标记质量进行评估，提高了交通运输安全。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11008v2">PDF</a> 6 pages, 3 figures, accepted at IEEE CAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文利用深度学习中的YOLO模型对日间路面标记质量进行检测研究，以提高道路安全。通过对新泽西州的图像数据进行实验，发现YOLOv8n模型在分类路面标记为良好、中等和不良能见度类别时，实现了最高的平均精度（mAP），并且在不同的交并比（IoU）阈值下表现稳健。该研究为提高路面标记质量的自动评估提供了一种准确的方法，增强了交通运输的安全性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>YOLO模型被用于检测日间路面标记质量，以增强道路安全。</li>
<li>研究使用了YOLOv8的三个变体：YOLOv8m、YOLOv8n和YOLOv8x。</li>
<li>YOLOv8n模型在分类路面标记的预测精度上表现最佳，实现了最高的平均精度（mAP）。</li>
<li>研究使用新泽西州的图像数据进行实验。</li>
<li>该模型在检测不同能见度的路面标记时表现出稳健性能。</li>
<li>该研究为自动评估路面标记质量提供了一种准确的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11008">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a2f95e5b2e10dfd669ba5b43292651d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf9792f01a0fc688bcb8bc10a5c728fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a54cf4e1e26a5846f63b0f583df163.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a548e1d842fc24ed0c7405fcb461181b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dca2a4b949f5ba6c64dfe72239252725.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b8a1c62d8b5b5429faafa585821413.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Cyclic-Contrastive-Knowledge-Transfer-for-Open-Vocabulary-Object-Detection"><a href="#Cyclic-Contrastive-Knowledge-Transfer-for-Open-Vocabulary-Object-Detection" class="headerlink" title="Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object   Detection"></a>Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object   Detection</h2><p><strong>Authors:Chuhan Zhang, Chaoyang Zhu, Pingcheng Dong, Long Chen, Dong Zhang</strong></p>
<p>In pursuit of detecting unstinted objects that extend beyond predefined categories, prior arts of open-vocabulary object detection (OVD) typically resort to pretrained vision-language models (VLMs) for base-to-novel category generalization. However, to mitigate the misalignment between upstream image-text pretraining and downstream region-level perception, additional supervisions are indispensable, eg, image-text pairs or pseudo annotations generated via self-training strategies. In this work, we propose CCKT-Det trained without any extra supervision. The proposed framework constructs a cyclic and dynamic knowledge transfer from language queries and visual region features extracted from VLMs, which forces the detector to closely align with the visual-semantic space of VLMs. Specifically, 1) we prefilter and inject semantic priors to guide the learning of queries, and 2) introduce a regional contrastive loss to improve the awareness of queries on novel objects. CCKT-Det can consistently improve performance as the scale of VLMs increases, all while requiring the detector at a moderate level of computation overhead. Comprehensive experimental results demonstrate that our method achieves performance gain of +2.9% and +10.2% AP50 over previous state-of-the-arts on the challenging COCO benchmark, both without and with a stronger teacher model. The code is provided at <a target="_blank" rel="noopener" href="https://github.com/ZCHUHan/CCKT-Det">https://github.com/ZCHUHan/CCKT-Det</a>. </p>
<blockquote>
<p>在追求检测无限制对象，这些对象超出预定类别的情况下，开放词汇表对象检测（OVD）的现有技术通常依赖于预训练的视觉语言模型（VLM）进行基础到新颖类别的推广。然而，为了减轻上游图像文本预训练和下游区域级别感知之间的不匹配，额外的监督是不可或缺的，例如，通过自训练策略生成的图像文本对或伪注释。在这项工作中，我们提出了无需任何额外监督的CCKT-Det。所提出的框架构建了从语言查询和从VLM提取的视觉区域特征之间的循环和动态知识转移，这迫使检测器紧密地对应于VLM的视觉语义空间。具体来说，1）我们预过滤并注入语义先验来指导查询学习，2）引入区域对比损失来提高查询对新对象的感知能力。随着VLM规模的增加，CCKT-Det可以持续提高性能，同时只需要适度的检测器计算开销。综合实验结果表明，我们的方法在具有挑战性的COCO基准测试上，相较于之前的最优水平，有无更强的教师模型分别实现了+2.9%和+10.2%的AP50性能提升。代码可在<a target="_blank" rel="noopener" href="https://github.com/ZCHUHan/CCKT-Det%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZCHUHan/CCKT-Det找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11005v1">PDF</a> 10 pages, 5 figures, Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong><br>     本文提出一种无需额外监督的CCKT-Det框架，用于开放词汇表对象检测。该框架通过构建循环和动态知识转移，从语言查询到视觉区域特征进行训练，促进检测器与视觉语言模型的视觉语义空间对齐。通过预筛选和注入语义先验来引导查询学习，引入区域对比损失提高查询对未知物体的识别能力。实验表明，该框架能在不同规模的视觉语言模型上持续提高性能，同时只需适度的计算开销。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CCKT-Det利用循环和动态知识转移，实现语言查询到视觉区域特征的训练，促进检测器与视觉语言模型（VLMs）的视觉语义空间对齐。</li>
<li>通过预筛选和注入语义先验，引导查询学习，提高检测性能。</li>
<li>引入区域对比损失，提高检测器对未知物体的识别能力。</li>
<li>CCKT-Det可在不同规模的VLMs上持续提高性能。</li>
<li>与现有技术相比，CCKT-Det在COCO基准测试上实现了显著的性能提升。<br>6.CCKT-Det的计算开销适中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-574a8ed0010aeca2bb6bd9a766735fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53f7289550afffe64a2fbaffc09ba83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6dad6363a1ebbfb2f9eb47258ffab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f35ec1971db6907184083f874be5745.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HeightFormer-Learning-Height-Prediction-in-Voxel-Features-for-Roadside-Vision-Centric-3D-Object-Detection-via-Transformer"><a href="#HeightFormer-Learning-Height-Prediction-in-Voxel-Features-for-Roadside-Vision-Centric-3D-Object-Detection-via-Transformer" class="headerlink" title="HeightFormer: Learning Height Prediction in Voxel Features for Roadside   Vision Centric 3D Object Detection via Transformer"></a>HeightFormer: Learning Height Prediction in Voxel Features for Roadside   Vision Centric 3D Object Detection via Transformer</h2><p><strong>Authors:Zhang Zhang, Chao Sun, Chao Yue, Da Wen, Yujie Chen, Tianze Wang, Jianghao Leng</strong></p>
<p>Roadside vision centric 3D object detection has received increasing attention in recent years. It expands the perception range of autonomous vehicles, enhances the road safety. Previous methods focused on predicting per-pixel height rather than depth, making significant gains in roadside visual perception. While it is limited by the perspective property of near-large and far-small on image features, making it difficult for network to understand real dimension of objects in the 3D world. BEV features and voxel features present the real distribution of objects in 3D world compared to the image features. However, BEV features tend to lose details due to the lack of explicit height information, and voxel features are computationally expensive. Inspired by this insight, an efficient framework learning height prediction in voxel features via transformer is proposed, dubbed HeightFormer. It groups the voxel features into local height sequences, and utilize attention mechanism to obtain height distribution prediction. Subsequently, the local height sequences are reassembled to generate accurate 3D features. The proposed method is applied to two large-scale roadside benchmarks, DAIR-V2X-I and Rope3D. Extensive experiments are performed and the HeightFormer outperforms the state-of-the-art methods in roadside vision centric 3D object detection task. </p>
<blockquote>
<p>近年来，以道路为中心的3D物体检测得到了越来越多的关注。它扩展了自动驾驶车辆的感知范围，提高了道路安全性。之前的方法侧重于预测像素高度而非深度，在路边视觉感知方面取得了重大进展。然而，由于图像特征的近大远小的透视属性，网络难以理解物体在现实世界中的真实尺寸。与图像特征相比，BEV特征和体素特征展示了物体在真实世界中的真实分布。然而，BEV特征由于缺乏明确的高度信息而容易丢失细节，而体素特征的计算成本较高。受这一见解的启发，提出了一个高效的框架来学习通过Transformer进行体素特征的高度预测，称为HeightFormer。它将体素特征分组为局部高度序列，并利用注意力机制获得高度分布预测。随后，这些局部高度序列被重新组合以生成准确的3D特征。所提出的方法应用于两个大规模的路边基准测试DAIR-V2X-I和Rope3D。进行了大量的实验，HeightFormer在路边视觉为中心的3D物体检测任务中优于现有技术方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注路边视觉为中心的3D目标检测问题，提出了HeightFormer框架，通过利用注意力机制预测物体高度序列信息来解决当前方法中BEV特征缺失细节和图像特征受透视效应影响的问题。HeightFormer将体素特征分为局部高度序列并生成准确的三维特征，在大型路边基准测试中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>路边视觉为中心的3D目标检测问题受到关注，旨在扩大自主车辆的感知范围并提高道路安全性。</li>
<li>传统方法主要预测像素高度，存在透视效应问题，导致网络难以理解真实的三维世界中的物体尺寸。</li>
<li>BEV特征和体素特征能展示物体的真实三维分布，但BEV特征易丢失细节，体素特征计算成本高。</li>
<li>HeightFormer框架通过利用注意力机制预测高度分布来解决上述问题。</li>
<li>HeightFormer将体素特征分组为局部高度序列，然后重新组装以生成准确的三维特征。</li>
<li>HeightFormer在大型路边基准测试中表现优越，优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-95d58b6b727e883b359db6e682f6909d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a54ae8769241c1871edd100858b7de44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d127ed994668b56267c527b382d44d61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-052e046480e3a74be3f037a11091f414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4230a9a2b60ae2aa72b96957ead9bbe8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation"><a href="#Distilling-Spectral-Graph-for-Object-Context-Aware-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation"></a>Distilling Spectral Graph for Object-Context Aware Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Chanyoung Kim, Dayun Ju, Woojung Han, Ming-Hsuan Yang, Seong Jae Hwang</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits models’ ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets. </p>
<blockquote>
<p>开放词汇语义分割（OVSS）随着最近的视觉语言模型（VLMs）的发展而进步，通过各种学习方案实现了超出预定类别的分割。值得注意的是，无训练方法为处理未见数据提供了可扩展、易于部署的解决方案，这是OVSS的关键目标。然而，一个关键问题依然存在：在基于任意查询提示的OVSS的复杂环境中，对复杂对象进行分割时缺乏对象级别的上下文考虑。这一疏忽限制了模型在对象内组合语义一致元素的能力，并准确地将它们映射到用户定义的任意类别。在这项工作中，我们介绍了一种通过结合图像中的对象级上下文知识来克服这一限制的新方法。具体来说，我们的模型通过将从视觉基础模型提炼的谱驱动特征蒸馏到视觉编码器的注意力机制中，增强了对象内部的连贯性，使语义一致的组件能够形成单个对象掩码。此外，我们还通过零目标对象存在可能性来优化文本嵌入，以确保与图像中表示的特定对象的准确对齐。通过利用对象级的上下文知识，我们提出的方法在多个数据集上实现了最先进的性能，并具有较强的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17150v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本描述了如何利用物体级别的上下文知识来提升Open-Vocabulary Semantic Segmentation模型的性能。为了解决传统模型中在分割复杂对象时缺乏对物体级别上下文考虑的局限性，提出了一种新的方法。该方法通过融合图像中的物体级别上下文知识，增强模型对物体内部的一致性，从而实现语义上连贯的组件形成单个物体掩膜。同时，通过改进文本嵌入和零样本物体存在概率，确保与图像中特定物体的准确对齐。该方法在多个数据集上取得了最先进的性能，并具有较强的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-Vocabulary Semantic Segmentation (OVSS) 允许通过不同的学习方案进行超越预设类别的分割。</li>
<li>训练无关的方法为处理未见数据提供了可扩展、易于部署的解决方案，这是OVSS的关键目标。</li>
<li>当前工作中一个关键问题是缺乏物体级别的上下文考虑，在基于任意查询提示的OVSS环境中分割复杂对象时存在局限性。</li>
<li>新的方法通过融合图像中的物体级别上下文知识来解决这个问题，增强模型对物体内部的一致性。</li>
<li>该方法实现语义上连贯的组件形成单个物体掩膜，提高了模型的性能。</li>
<li>通过改进文本嵌入和零样本物体存在概率，确保与图像中特定物体的准确对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17150">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a4674b24fe322761820fa176c1eb16e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b17542025b8df1633d8f8c54cbb60b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0bdf034384f8da4962c37811fdb7ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e98ebfbdecc978c059d2cb079af8da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf6cd30b1479ae00d8615ab98575a118.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Believing-is-Seeing-Unobserved-Object-Detection-using-Generative-Models"><a href="#Believing-is-Seeing-Unobserved-Object-Detection-using-Generative-Models" class="headerlink" title="Believing is Seeing: Unobserved Object Detection using Generative Models"></a>Believing is Seeing: Unobserved Object Detection using Generative Models</h2><p><strong>Authors:Subhransu S. Bhattacharjee, Dylan Campbell, Rahul Shome</strong></p>
<p>Can objects that are not visible in an image – but are in the vicinity of the camera – be detected? This study introduces the novel tasks of 2D, 2.5D and 3D unobserved object detection for predicting the location of nearby objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to address this task, including 2D and 3D diffusion models and vision-language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that capture different aspects of performance. Our empirical evaluation on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the use of generative models for the unobserved object detection task. </p>
<blockquote>
<p>在图像中不可见但位于相机附近的对象能否被检测出来？本研究引入了用于预测被遮挡或位于图像帧外的附近对象位置的二维、2.5维和三维未观测对象检测的新任务。我们适应了几种最先进的预训练生成模型来完成这项任务，包括二维和三维扩散模型以及视觉语言模型，并表明它们可用于推断未直接观察到的对象的存在。为了评估这项任务，我们提出了一套能够反映不同性能方面的指标。我们在RealEstate10k和NYU Depth v2数据集上的室内场景实证评估结果表明，使用生成模型进行未观测对象检测任务是可行的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05869v3">PDF</a> IEEE&#x2F;CVF Computer Vision and Pattern Recognition 2025; 22 pages</p>
<p><strong>Summary</strong></p>
<p>本文研究不可见物体的检测问题，探索了2D、2.5D和3D未观测物体检测的新任务。研究利用预训练的生成模型进行预测，包括2D和3D扩散模型以及视觉语言模型，并展示了在未直接观测物体的情况下进行推断的能力。为评估此任务性能，研究提出了一套评价指标。在RealEstate10k和NYU Depth v2数据集上的室内场景实证评估结果证明了生成模型在未观测物体检测任务中的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入未观测物体检测的新任务，探索预测图像中不可见物体的位置。</li>
<li>利用预训练的生成模型（包括扩散模型和视觉语言模型）进行预测。</li>
<li>提出一套性能评价指标以评估未观测物体检测任务的效果。</li>
<li>研究涵盖从二维到三维的多种检测场景。</li>
<li>实证评估证明了生成模型在未观测物体检测任务中的有效性。</li>
<li>研究结果展示了推断未直接观测物体存在的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a7d9f616fe9c6f5fbc7cbe6a77e59d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6eff8ced9348e87b7694e15f4077cad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a85ca8415f5e032cf76849d28e8c5b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e3f4f85b9de671f8231fd675f9522b5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SimPLR-A-Simple-and-Plain-Transformer-for-Efficient-Object-Detection-and-Segmentation"><a href="#SimPLR-A-Simple-and-Plain-Transformer-for-Efficient-Object-Detection-and-Segmentation" class="headerlink" title="SimPLR: A Simple and Plain Transformer for Efficient Object Detection   and Segmentation"></a>SimPLR: A Simple and Plain Transformer for Efficient Object Detection   and Segmentation</h2><p><strong>Authors:Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek</strong></p>
<p>The ability to detect objects in images at varying scales has played a pivotal role in the design of modern object detectors. Despite considerable progress in removing hand-crafted components and simplifying the architecture with transformers, multi-scale feature maps and pyramid designs remain a key factor for their empirical success. In this paper, we show that shifting the multiscale inductive bias into the attention mechanism can work well, resulting in a plain detector &#96;SimPLR’ whose backbone and detection head are both non-hierarchical and operate on single-scale features. We find through our experiments that SimPLR with scale-aware attention is plain and simple architecture, yet competitive with multi-scale vision transformer alternatives. Compared to the multi-scale and single-scale state-of-the-art, our model scales better with bigger capacity (self-supervised) models and more pre-training data, allowing us to report a consistently better accuracy and faster runtime for object detection, instance segmentation, as well as panoptic segmentation. Code is released at <a target="_blank" rel="noopener" href="https://github.com/kienduynguyen/SimPLR">https://github.com/kienduynguyen/SimPLR</a>. </p>
<blockquote>
<p>在现代目标检测器设计中，能够在不同尺度上检测图像中的物体起着至关重要的作用。尽管在去除手工组件、简化架构方面以及使用transformer方面取得了很大进展，但多尺度特征映射和金字塔设计仍然是其经验成功的关键因素。在本文中，我们展示了将多尺度归纳偏置转移到注意力机制中可以很好地工作，从而得到了一个纯检测器SimPLR，其主干和检测头都是非分层的，并在单尺度特征上运行。我们通过实验发现，带有尺度感知注意力的SimPLR是一个简单明了的架构，但与多尺度视觉transformer替代方案相比仍具有竞争力。与多尺度和单尺度最新技术相比，我们的模型在更大容量（自监督）的模型和更多预训练数据的支持下表现更好，这使我们能够在目标检测、实例分割和全景分割方面报告更高的准确性和更快的运行时间。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/kienduynguyen/SimPLR%E3%80%82">https://github.com/kienduynguyen/SimPLR。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05920v4">PDF</a> In Proceeding of TMLR’2025</p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了目标检测中的多尺度特征处理问题，提出了一种新的方法SimPLR，通过将多尺度归纳偏见转移到注意力机制中，实现了一种简单而有效的检测器。实验表明，SimPLR模型在目标检测、实例分割和全景分割任务上表现优异，特别是当使用更大容量和更多预训练数据时。代码已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多尺度特征处理在现代目标检测器设计中扮演重要角色。</li>
<li>SimPLR通过将多尺度归纳偏见融入注意力机制来改进目标检测。</li>
<li>SimPLR是一个简单且有效的非层次结构检测器，可在单尺度特征上操作。</li>
<li>实验表明SimPLR与多尺度视觉转换器相比具有竞争力。</li>
<li>SimPLR在更大的容量模型和更多预训练数据下表现更佳。</li>
<li>SimPLR在目标检测、实例分割和全景分割任务上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.05920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-88d4060ce5636e9f6ce41a7dfc0e1eda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21730a3a41fec4508adf177cd7f0f2ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64996aa22a4f986ba5921adf438142e6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f62149865cc2ffeee76e3d8ee468d7e0.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-03-18  Deepfake Detection of Face Images based on a Convolutional Neural   Network
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ac915e3876c60b287ac49530c98cf191.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-03-18  Towards Scalable Foundation Model for Multi-modal and Hyperspectral   Geospatial Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
