<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  VideoMind A Chain-of-LoRA Agent for Long Video Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6ab0166324c140954021b1012ef6c3ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-18-æ›´æ–°"><a href="#2025-03-18-æ›´æ–°" class="headerlink" title="2025-03-18 æ›´æ–°"></a>2025-03-18 æ›´æ–°</h1><h2 id="VideoMind-A-Chain-of-LoRA-Agent-for-Long-Video-Reasoning"><a href="#VideoMind-A-Chain-of-LoRA-Agent-for-Long-Video-Reasoning" class="headerlink" title="VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning"></a>VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning</h2><p><strong>Authors:Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou</strong></p>
<p>Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning. </p>
<blockquote>
<p>è§†é¢‘ä»¥å…¶ç‹¬ç‰¹çš„æ—¶é—´ç»´åº¦è€Œè‘—ç§°ï¼Œéœ€è¦ç²¾ç¡®ä¸”åŸºäºæƒ…å¢ƒçš„ç†è§£ï¼Œç­”æ¡ˆéœ€ç›´æ¥ä¸å¯è§†åŒ–ã€å¯è§£é‡Šçš„è¯æ®ç›¸å…³è”ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼Œä½†å¤šæ¨¡æ€æ¨ç†ï¼ˆç‰¹åˆ«æ˜¯é’ˆå¯¹è§†é¢‘ï¼‰ä»ç„¶æœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoMindï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºåŸºäºæ—¶é—´æƒ…å¢ƒçš„è§†é¢‘ç†è§£è®¾è®¡çš„æ–°å‹è§†é¢‘è¯­è¨€ä»£ç†ã€‚VideoMindæœ‰ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆiï¼‰æˆ‘ä»¬ç¡®å®šäº†è§†é¢‘æ—¶é—´æ¨ç†çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶åŸºäºè§’è‰²è®¾è®¡äº†ä¸€ä¸ªä»£ç†å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåè°ƒä¸åŒè§’è‰²çš„è§„åˆ’å™¨ã€ä¸€ä¸ªç”¨äºæ—¶é—´å®šä½çš„å®šä½å™¨ã€ä¸€ä¸ªè¯„ä¼°æ—¶é—´é—´éš”å‡†ç¡®æ€§çš„éªŒè¯å™¨ï¼Œä»¥åŠä¸€ä¸ªç”¨äºé—®ç­”çš„å›ç­”è€…ã€‚ï¼ˆiiï¼‰ä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆè¿™äº›å¤šæ ·åŒ–çš„è§’è‰²ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Chain-of-LoRAç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨å®ç°æ— ç¼è§’è‰²åˆ‡æ¢ï¼ŒåŒæ—¶é¿å…ä½¿ç”¨å¤šä¸ªæ¨¡å‹å¸¦æ¥çš„å¼€é”€ï¼Œä»è€Œåœ¨æ•ˆç‡å’Œçµæ´»æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨14ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨å¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬3ä¸ªåŸºäºæƒ…å¢ƒçš„è§†é¢‘é—®ç­”ä»»åŠ¡ã€6ä¸ªè§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡å’Œ5ä¸ªé€šç”¨è§†é¢‘é—®ç­”ä»»åŠ¡ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨æ¨è¿›è§†é¢‘ä»£ç†å’Œé•¿æ ¼å¼æ—¶é—´æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13444v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://videomind.github.io/">https://videomind.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è§†é¢‘è¯­è¨€ä»£ç†â€”â€”VideoMindï¼Œä¸“ä¸ºæ—¶é—´åŸºç¡€è§†é¢‘ç†è§£è€Œè®¾è®¡ã€‚å®ƒåŒ…å«äº†ä¸¤ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯ç¡®å®šäº†è§†é¢‘æ—¶é—´æ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œå¹¶åŸºäºè¿™äº›èƒ½åŠ›è®¾è®¡äº†è§’è‰²åŒ–çš„ä»£ç†å·¥ä½œæµç¨‹ï¼›äºŒæ˜¯æå‡ºäº†Chain-of-LoRAç­–ç•¥ï¼Œé«˜æ•ˆæ•´åˆä¸åŒè§’è‰²ï¼Œå®ç°è½»æ¾çš„è§’è‰²åˆ‡æ¢ã€‚VideoMindåœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨æ¨è¿›è§†é¢‘ä»£ç†å’Œé•¿å½¢å¼æ—¶é—´æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”±äºå…¶ç‹¬ç‰¹çš„æ—¶é—´ç»´åº¦ï¼Œéœ€è¦ç²¾ç¡®çš„æ—¶é—´åŸºç¡€ç†è§£ï¼Œç­”æ¡ˆéœ€ç›´æ¥ä¸è§†è§‰ã€å¯è§£é‡Šçš„è¯æ®ç›¸å…³è”ã€‚</li>
<li>VideoMindæ˜¯ä¸€ç§æ–°å‹è§†é¢‘è¯­è¨€ä»£ç†ï¼Œä¸“ä¸ºæ—¶é—´åŸºç¡€è§†é¢‘ç†è§£è€Œè®¾è®¡ã€‚</li>
<li>VideoMindåŒ…å«ä¸¤ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼šè§’è‰²åŒ–çš„ä»£ç†å·¥ä½œæµç¨‹å’ŒChain-of-LoRAç­–ç•¥ã€‚</li>
<li>è§’è‰²åŒ–çš„ä»£ç†å·¥ä½œæµç¨‹åŒ…æ‹¬è§„åˆ’ã€æ—¶é—´å®šä½ã€æ—¶é—´åŒºé—´ç²¾åº¦è¯„ä¼°å’Œé—®é¢˜å›ç­”ç­‰å¤šä¸ªè§’è‰²ã€‚</li>
<li>Chain-of-LoRAç­–ç•¥èƒ½å¤Ÿé«˜æ•ˆæ•´åˆä¸åŒè§’è‰²ï¼Œå®ç°è½»æ¾çš„è§’è‰²åˆ‡æ¢ï¼ŒåŒæ—¶ä¿æŒæ•ˆç‡å’Œçµæ´»æ€§ã€‚</li>
<li>VideoMindåœ¨å¤šä¸ªè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬åŸºäºæ—¶é—´çš„è§†é¢‘é—®ç­”ã€è§†é¢‘æ—¶é—´å®šä½å’Œä¸€èˆ¬è§†é¢‘é—®ç­”ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f67a2edd5a77bf4890846d0edf04f68b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86845e5943d8d4f83c93a98953ba4ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ecd8e7a2ab1ee00fecb4e352d4a7368.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0ad1e51ad6980ab765146c580ff7b3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f64f9f3ca2ee88900720c4541e3ea15e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54666f24476073a8e24f97694f7e00d2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MicroVQA-A-Multimodal-Reasoning-Benchmark-for-Microscopy-Based-Scientific-Research"><a href="#MicroVQA-A-Multimodal-Reasoning-Benchmark-for-Microscopy-Based-Scientific-Research" class="headerlink" title="MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based   Scientific Research"></a>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based   Scientific Research</h2><p><strong>Authors:James Burgess, Jeffrey J Nirschl, Laura Bravo-SÃ¡nchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus G. Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, Sarina M. Hasan, Alexandra Johannesson, William D. Leineweber, Malvika G Nair, Ridhi Yarlagadda, Connor Zuraski, Wah Chiu, Sarah Cohen, Jan N. Hansen, Manuel D Leonetti, Chad Liu, Emma Lundberg, Serena Yeung-Levy</strong></p>
<p>Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based &#96;RefineBotâ€™ updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jmhb/microvqa">https://huggingface.co/datasets/jmhb/microvqa</a>, and project page at <a target="_blank" rel="noopener" href="https://jmhb0.github.io/microvqa">https://jmhb0.github.io/microvqa</a>. </p>
<blockquote>
<p>ç§‘å­¦ç ”ç©¶éœ€è¦å¯¹å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¤æ‚çš„æ¨ç†ï¼Œè¿™åœ¨ç”Ÿç‰©å­¦ä¸­å°¤ä¸ºæ™®éã€‚å°½ç®¡è¿‘å¹´æ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨AIè¾…åŠ©ç ”ç©¶æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä»…é’ˆå¯¹å¤§å­¦æ°´å¹³çš„éš¾åº¦ï¼Œè€Œç ”ç©¶çº§åŸºå‡†æµ‹è¯•åˆ™å¼ºè°ƒä½çº§åˆ«çš„æ„ŸçŸ¥ï¼Œæ— æ³•è¾¾åˆ°ç§‘å­¦å‘ç°æ‰€éœ€çš„å¤šæ¨¡æ€å¤æ‚æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MicroVQAï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç ”ç©¶å·¥ä½œæµç¨‹ä¸­è‡³å…³é‡è¦çš„ä¸‰ç§æ¨ç†èƒ½åŠ›ï¼šä¸“å®¶å›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒæ–¹æ¡ˆæå‡ºã€‚MicroVQAåŒ…å«ç”±ç”Ÿç‰©å­¦ä¸“å®¶è·¨å¤šç§æ˜¾å¾®é•œæ¨¡å¼æ•´ç†çš„1042é“é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œç¡®ä¿VQAæ ·æœ¬ä»£è¡¨çœŸå®çš„ç§‘å­¦å®è·µã€‚åœ¨æ„å»ºåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†çš„é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•ä¼šå¯¼è‡´è¯­è¨€ä¸Šçš„æ·å¾„ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡‡ç”¨æ–°çš„ä¸¤é˜¶æ®µæµç¨‹ï¼šä¼˜åŒ–çš„LLMæç¤ºç»“æ„å°†é—®é¢˜å’Œç­”æ¡ˆé…å¯¹æˆé€‰æ‹©é¢˜ï¼›ç„¶ååŸºäºä»£ç†çš„RefineBotæ›´æ–°å®ƒä»¬ä»¥æ¶ˆé™¤æ·å¾„ã€‚åœ¨æœ€æ–°çš„MLLMä¸Šçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæœ€é«˜æ€§èƒ½è¾¾åˆ°53%ï¼›ä½¿ç”¨è¾ƒå°LLMçš„æ¨¡å‹æ€§èƒ½ç•¥ä½äºé¡¶çº§æ¨¡å‹ï¼Œè¿™è¡¨æ˜åŸºäºè¯­è¨€çš„æ¨ç†æ¯”å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜æ€§è¾ƒå°ï¼›é€šè¿‡ç§‘å­¦æ–‡ç« è°ƒæ•´å¯å¢å¼ºæ€§èƒ½ã€‚ä¸“å®¶å¯¹æ€ç»´é“¾çš„å“åº”åˆ†ææ˜¾ç¤ºï¼Œæ„ŸçŸ¥é”™è¯¯æœ€ä¸ºé¢‘ç¹ï¼Œå…¶æ¬¡æ˜¯çŸ¥è¯†é”™è¯¯ï¼Œç„¶åæ˜¯è¿‡åº¦æ¦‚æ‹¬é”™è¯¯ã€‚è¿™äº›è§è§£çªå‡ºäº†å¤šæ¨¡æ€ç§‘å­¦æ¨ç†çš„æŒ‘æˆ˜ï¼Œè¡¨æ˜MicroVQAæ˜¯æ¨åŠ¨AIé©±åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„æœ‰ä»·å€¼çš„èµ„æºã€‚MicroVQAå¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jmhb/microvqa%E8%8E%B7%E5%8F%96%EF%BC%8C%E9%A1%B9%E7%9B%AE%E9%A1%B5%E9%9D%A2%E4%B8%BAhttps://jmhb0.github.io/microvqa%E3%80%82">https://huggingface.co/datasets/jmhb/microvqaè·å–ï¼Œé¡¹ç›®é¡µé¢ä¸ºhttps://jmhb0.github.io/microvqaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13399v1">PDF</a> CVPR 2025 (Conference on Computer Vision and Pattern Recognition)   Project page at <a target="_blank" rel="noopener" href="https://jmhb0.github.io/microvqa">https://jmhb0.github.io/microvqa</a> Benchmark at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jmhb/microvqa">https://huggingface.co/datasets/jmhb/microvqa</a></p>
<p><strong>Summary</strong><br>     ä¸ºå¡«è¡¥ç§‘å­¦ç ”ç©¶ä¸­çš„å¤šæ¨¡æ€æ¨ç†éœ€æ±‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ä¹‹é—´çš„å·®è·ï¼Œæå‡ºMicroVQAåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç”Ÿç‰©å­¦ç ”ç©¶ä¸­çš„ä¸‰å¤§å…³é”®æ¨ç†èƒ½åŠ›ï¼šä¸“ä¸šå›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒææ¡ˆã€‚MicroVQAåŒ…å«ç”±ç”Ÿç‰©å­¦ä¸“å®¶è·¨å¤šç§æ˜¾å¾®é•œæ¨¡å¼ç­–åˆ’çš„1042é“é€‰æ‹©é¢˜ï¼Œä»¥ç¡®ä¿è§†è§‰é—®ç­”æ ·æœ¬èƒ½çœŸå®åæ˜ ç§‘å­¦å®è·µã€‚ç ”ç©¶ä¸­å‘ç°æ ‡å‡†é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•ä¼šå¼•å…¥è¯­è¨€æ·å¾„ï¼Œå› æ­¤ç ”å‘äº†ä¸€ä¸ªåŒ…å«ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºç»“æ„å’ŒåŸºäºä»£ç†çš„RefineBotçš„é€‰æ‹©é¢˜ä¼˜åŒ–ä¸¤é˜¶æ®µç®¡é“ã€‚åœ¨æœ€æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæœ€ä½³æ€§èƒ½ä¸º53%ï¼Œä¸”è¾ƒå°çš„è¯­è¨€æ¨¡å‹æ€§èƒ½ç•¥ä½äºé¡¶çº§æ¨¡å‹ï¼Œè¯´æ˜è¯­è¨€æ¨ç†è¾ƒä¹‹äºå¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜æ€§è¾ƒå°ã€‚é€šè¿‡ä¸“å®¶åˆ†æå‘ç°æ„ŸçŸ¥é”™è¯¯æœ€ä¸ºé¢‘ç¹ï¼Œå…¶æ¬¡æ˜¯çŸ¥è¯†é”™è¯¯å’Œè¿‡åº¦æ³›åŒ–é”™è¯¯ã€‚è¿™äº›è§è§£çªæ˜¾äº†å¤šæ¨¡æ€ç§‘å­¦æ¨ç†çš„å›°éš¾ï¼Œå¹¶è¯æ˜äº†MicroVQAæ˜¯æ¨åŠ¨äººå·¥æ™ºèƒ½é©±åŠ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MicroVQAå¡«è¡¥äº†ç§‘å­¦ç ”ç©¶ä¸­å¤šæ¨¡æ€æ¨ç†éœ€æ±‚çš„åŸºå‡†æµ‹è¯•ç©ºç™½ã€‚</li>
<li>MicroVQAæ—¨åœ¨è¯„ä¼°ç”Ÿç‰©å­¦ç ”ç©¶ä¸­çš„ä¸‰å¤§å…³é”®æ¨ç†èƒ½åŠ›ï¼šä¸“ä¸šå›¾åƒç†è§£ã€å‡è®¾ç”Ÿæˆå’Œå®éªŒææ¡ˆã€‚</li>
<li>MicroVQAåŒ…å«ç”±ç”Ÿç‰©å­¦ä¸“å®¶åœ¨ä¸åŒæ˜¾å¾®é•œæ¨¡å¼ä¸‹ç­–åˆ’çš„1042é“é€‰æ‹©é¢˜ã€‚</li>
<li>æ ‡å‡†é€‰æ‹©é¢˜ç”Ÿæˆæ–¹æ³•å­˜åœ¨å¼•å…¥è¯­è¨€æ·å¾„çš„é—®é¢˜ï¼Œå› æ­¤ç ”å‘äº†ä¸¤é˜¶æ®µç®¡é“è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºæœ€ä½³æ€§èƒ½ä¸º53%ï¼Œä¸”è¾ƒå°çš„è¯­è¨€æ¨¡å‹æ€§èƒ½ç•¥ä½äºé¡¶çº§æ¨¡å‹ã€‚</li>
<li>è¯­è¨€æ¨ç†ç›¸è¾ƒäºå¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜æ€§è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f2b6442616c127f5ab9df9cb18a69354.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51b2fae3f70d0c19646a95bb8754f751.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2cde8e2a3a3375e5a860f7345d7fa8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-446268fe87532aa5e460f36bc62dd390.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TimeZero-Temporal-Video-Grounding-with-Reasoning-Guided-LVLM"><a href="#TimeZero-Temporal-Video-Grounding-with-Reasoning-Guided-LVLM" class="headerlink" title="TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM"></a>TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM</h2><p><strong>Authors:Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, Qin Jin</strong></p>
<p>We introduce TimeZero, a reasoning-guided LVLM designed for the temporal video grounding (TVG) task. This task requires precisely localizing relevant video segments within long videos based on a given language query. TimeZero tackles this challenge by extending the inference process, enabling the model to reason about video-language relationships solely through reinforcement learning. To evaluate the effectiveness of TimeZero, we conduct experiments on two benchmarks, where TimeZero achieves state-of-the-art performance on Charades-STA. Code is available at <a target="_blank" rel="noopener" href="https://github.com/www-Ye/TimeZero">https://github.com/www-Ye/TimeZero</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†TimeZeroï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ—¶åºè§†é¢‘å®šä½ï¼ˆTVGï¼‰ä»»åŠ¡è®¾è®¡çš„æ¨ç†å¼•å¯¼å‹LVVMã€‚æ­¤ä»»åŠ¡éœ€è¦åœ¨ç»™å®šè¯­è¨€æŸ¥è¯¢çš„åŸºç¡€ä¸Šï¼Œåœ¨é•¿è§†é¢‘ä¸­ç²¾ç¡®å®šä½ç›¸å…³è§†é¢‘ç‰‡æ®µã€‚TimeZeroé€šè¿‡æ‰©å±•æ¨ç†è¿‡ç¨‹æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹è§†é¢‘è¯­è¨€å…³ç³»è¿›è¡Œæ¨ç†ã€‚ä¸ºäº†è¯„ä¼°TimeZeroçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå…¶ä¸­TimeZeroåœ¨Charades-STAä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/www-Ye/TimeZero%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/www-Ye/TimeZeroæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13377v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/www-Ye/TimeZero">https://github.com/www-Ye/TimeZero</a></p>
<p><strong>Summary</strong></p>
<p>TimeZeroæ˜¯ä¸€ç§é’ˆå¯¹æ—¶åºè§†é¢‘å®šä½ï¼ˆTVGï¼‰ä»»åŠ¡çš„æ¨ç†å¼•å¯¼LVLMã€‚è¯¥ä»»åŠ¡éœ€è¦åœ¨é•¿è§†é¢‘ä¸­ç²¾ç¡®å®šä½ä¸ç»™å®šè¯­è¨€æŸ¥è¯¢ç›¸å…³çš„è§†é¢‘ç‰‡æ®µã€‚TimeZeroé€šè¿‡æ‰©å±•æ¨ç†è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹è§†é¢‘è¯­è¨€å…³ç³»è¿›è¡Œæ¨ç†ï¼Œæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨Charades-STAç­‰ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒTimeZeroçš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/www-Ye/TimeZero">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeZeroæ˜¯ä¸€ç§ä¸“ä¸ºæ—¶åºè§†é¢‘å®šä½ï¼ˆTVGï¼‰ä»»åŠ¡è®¾è®¡çš„æ¨ç†å¼•å¯¼LVLMã€‚</li>
<li>TVGä»»åŠ¡éœ€è¦é•¿è§†é¢‘ä¸­æ ¹æ®ç»™å®šçš„è¯­è¨€æŸ¥è¯¢ç²¾ç¡®å®šä½ç›¸å…³è§†é¢‘ç‰‡æ®µã€‚</li>
<li>TimeZeroé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨ç†ï¼Œæ‰©å±•äº†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æ¥å¤„ç†è¿™ä¸€ä»»åŠ¡ã€‚</li>
<li>TimeZeroåœ¨Charades-STAåŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€æ–°æ°´å¹³çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>TimeZeroæ¨¡å‹å…·å¤‡å¼ºå¤§çš„è§†é¢‘è¯­è¨€å…³ç³»å¤„ç†èƒ½åŠ›ã€‚</li>
<li>TimeZeroçš„ä»£ç å·²ç»å…¬å¼€ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-338bc62d4f9ddae2afadb9de18f428cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e710b9b14e3a6076ea2154e6cfd488c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ab0166324c140954021b1012ef6c3ae.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mitigating-Visual-Forgetting-via-Take-along-Visual-Conditioning-for-Multi-modal-Long-CoT-Reasoning"><a href="#Mitigating-Visual-Forgetting-via-Take-along-Visual-Conditioning-for-Multi-modal-Long-CoT-Reasoning" class="headerlink" title="Mitigating Visual Forgetting via Take-along Visual Conditioning for   Multi-modal Long CoT Reasoning"></a>Mitigating Visual Forgetting via Take-along Visual Conditioning for   Multi-modal Long CoT Reasoning</h2><p><strong>Authors:Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVistaâ€™s test-hard subset, revealing the modelâ€™s textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å·²ç»è¯æ˜äº†å…¶å¢å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä»æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå‘å±•åˆ°é¢å‘äº§å“çš„é«˜çº§è§£å†³æ–¹æ¡ˆï¼Œå¦‚OpenAI o1ã€‚åœ¨æˆ‘ä»¬é‡æ–°å®ç°æ­¤æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¤šæ¨¡æ€ä»»åŠ¡éœ€è¦è§†è§‰è¾“å…¥ï¼ˆä¾‹å¦‚å‡ ä½•é—®é¢˜ï¼‰ï¼Œå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰åœ¨ç»´æŒå¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨ä¸Šé‡åˆ°å›°éš¾ã€‚æ¢å¥è¯è¯´ï¼Œéšç€æ¨ç†çš„è¿›è¡Œï¼ŒMLLMå¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨é€æ¸ä¸‹é™ï¼Œå¯¼è‡´è¿‡åº¦ä¾èµ–æ–‡æœ¬è¾“å‡ºã€‚ä¸ºäº†è°ƒæŸ¥è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨é•¿é“¾æ¨ç†è¿‡ç¨‹ä¸­æ¶ˆé™¤å›¾åƒè¾“å…¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸­é€”æˆªæ–­ï¼Œç„¶åç§»é™¤è¾“å…¥å›¾åƒé‡æ–°å®Œæˆæ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨MathVistaçš„ç¡¬æµ‹è¯•å­é›†ä¸Šè§‚å¯Ÿåˆ°ä»…çº¦2%çš„å‡†ç¡®ç‡ä¸‹é™ï¼Œè¿™è¡¨æ˜æ¨¡å‹çš„æ–‡æœ¬è¾“å‡ºä¸»å¯¼äº†åç»­çš„æ¨ç†è¿‡ç¨‹ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæºå¸¦å¼è§†è§‰æ¡ä»¶â€ï¼ˆTVCï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†å›¾åƒè¾“å…¥è½¬ç§»åˆ°å…³é”®çš„æ¨ç†é˜¶æ®µï¼Œå¹¶é€šè¿‡åŠ¨æ€ä¿®å‰ªå‹ç¼©å†—ä½™çš„è§†è§‰ä»¤ç‰Œã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæ¨¡å‹åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„æ³¨æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°å¹³å‡æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆæ¯”ä¹‹å‰çš„æœ€ä¼˜æ€§èƒ½é«˜å‡º3.4%ï¼‰ï¼Œè¯æ˜äº†TVCåœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13360v1">PDF</a> The project page is available at   <a target="_blank" rel="noopener" href="https://sun-hailong.github.io/projects/TVC">https://sun-hailong.github.io/projects/TVC</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä»é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå‘å±•åˆ°é¢å‘äº§å“çš„OpenAIç­‰é«˜çº§è§£å†³æ–¹æ¡ˆã€‚åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ—¶é¢ä¸´æ³¨æ„åŠ›é€æ¸ä¸§å¤±çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œç§»é™¤å›¾åƒè¾“å…¥å¯¹é•¿é“¾æ¨ç†è¿‡ç¨‹ä»…é€ æˆçº¦2%çš„å‡†ç¡®ç‡ä¸‹é™ï¼Œè¡¨æ˜æ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬è¾“å‡ºè¿›è¡Œåç»­æ¨ç†ã€‚å› æ­¤ï¼Œæå‡ºä¸€ç§åä¸ºâ€œå¸¦è§†è§‰æ¡ä»¶â€ï¼ˆTVCï¼‰çš„ç­–ç•¥ï¼Œå°†å›¾åƒè¾“å…¥è½¬ç§»åˆ°å…³é”®æ¨ç†é˜¶æ®µï¼Œå¹¶é€šè¿‡åŠ¨æ€ä¿®å‰ªå‹ç¼©å†—ä½™è§†è§‰æ ‡è®°ã€‚æ­¤æ–¹æ³•æœ‰åŠ©äºæ¨¡å‹åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„æ³¨æ„åŠ›ï¼Œå¹¶åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—å¹³å‡æœ€ä½³æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ—¶å­˜åœ¨æ³¨æ„åŠ›é€æ¸ä¸§å¤±çš„é—®é¢˜ã€‚</li>
<li>ç§»é™¤å›¾åƒè¾“å…¥å¯¹é•¿é“¾æ¨ç†è¿‡ç¨‹çš„å½±å“è¾ƒå°ï¼Œè¡¨æ˜æ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬è¾“å‡ºã€‚</li>
<li>æå‡ºå¸¦è§†è§‰æ¡ä»¶ï¼ˆTVCï¼‰çš„ç­–ç•¥ï¼Œå°†å›¾åƒè¾“å…¥è½¬ç§»åˆ°å…³é”®æ¨ç†é˜¶æ®µã€‚</li>
<li>é€šè¿‡åŠ¨æ€ä¿®å‰ªå‹ç¼©å†—ä½™è§†è§‰æ ‡è®°ï¼Œæœ‰åŠ©äºæ¨¡å‹ä¿æŒå¯¹è§†è§‰æˆåˆ†çš„æ³¨æ„åŠ›ã€‚</li>
<li>TVCç­–ç•¥åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¹³å‡æœ€ä½³æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8725b11ba2c2d90eee3538510e4f17b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47ba6bf717776236bffe18a51fa12566.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f40330e2c1b7b0d762e2e5d31c7e364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ae23d62b0ce74f4fd5dcdcd361ef432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e75a2e23b1d55e50b25b4c126aabe5b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a6ce182e379b6068bb7ae8f1aa4a406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de2261852e2c300a7628d88c598d44b9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Agents-Play-Thousands-of-3D-Video-Games"><a href="#Agents-Play-Thousands-of-3D-Video-Games" class="headerlink" title="Agents Play Thousands of 3D Video Games"></a>Agents Play Thousands of 3D Video Games</h2><p><strong>Authors:Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, Wei Yang</strong></p>
<p>We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTALâ€™s effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on <a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">https://zhongwen.one/projects/portal</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PORTALï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºå¼€å‘èƒ½å¤Ÿé€šè¿‡è¯­è¨€æŒ‡å¯¼ç­–ç•¥ç”Ÿæˆç©æ•°åƒæ¬¾3Dè§†é¢‘æ¸¸æˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚é€šè¿‡å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆä»¥é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰è¡¨ç¤ºçš„è¡Œä¸ºæ ‘ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒäº†æˆ˜ç•¥æ·±åº¦å’Œå¿«é€Ÿé€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ··åˆç­–ç•¥ç»“æ„ï¼Œç»“åˆäº†åŸºäºè§„åˆ™çš„èŠ‚ç‚¹å’Œç¥ç»ç½‘ç»œç»„ä»¶ï¼Œèƒ½å¤Ÿå®ç°é«˜çº§æˆ˜ç•¥æ¨ç†å’Œç²¾ç¡®çš„ä½çº§æ§åˆ¶ã€‚é‡‡ç”¨ç»“åˆå®šé‡æ¸¸æˆæŒ‡æ ‡å’Œè§†è§‰è¯­è¨€æ¨¡å‹åˆ†æçš„åŒé‡åé¦ˆæœºåˆ¶ï¼Œä¿ƒè¿›æˆ˜æœ¯å’Œæˆ˜ç•¥å±‚é¢çš„ç­–ç•¥è¿­ä»£æ”¹è¿›ã€‚å¾—åˆ°çš„ç­–ç•¥å¯ç«‹å³éƒ¨ç½²ã€äººç±»å¯è§£é‡Šï¼Œå¹¶èƒ½å¤Ÿåœ¨å„ç§æ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œæ¨å¹¿ã€‚å®éªŒç»“æœè¯æ˜äº†PORTALåœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆï¼ˆFPSï¼‰ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¼€å‘æ•ˆç‡ã€ç­–ç•¥æ¨å¹¿å’Œè¡Œä¸ºå¤šæ ·æ€§æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚PORTALåœ¨æ¸¸æˆäººå·¥æ™ºèƒ½å¼€å‘æ–¹é¢ä»£è¡¨äº†é‡å¤§è¿›å±•ï¼Œä¸ºåˆ›å»ºèƒ½å¤Ÿåœ¨æ•°åƒæ¬¾å•†ä¸šè§†é¢‘æ¸¸æˆä¸­æ“ä½œä¸”å¼€å‘æˆæœ¬è¾ƒä½çš„å¤æ‚ä»£ç†æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚å…³äº3Dè§†é¢‘æ¸¸æˆçš„å®éªŒç»“æœï¼Œè¯·è®¿é—® <a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">https://zhongwen.one/projects/portal</a> æŸ¥çœ‹æœ€ä½³æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¯­è¨€æŒ‡å¯¼ç­–ç•¥ç”Ÿæˆï¼ŒPORTALæ¡†æ¶èƒ½å¤Ÿå®ç°äººå·¥æ™ºèƒ½ä»£ç†åœ¨æ•°åƒæ¬¾ä¸‰ç»´è§†é¢‘æ¸¸æˆä¸­çš„æ¸¸æˆèƒ½åŠ›ã€‚é€šè¿‡å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¡Œä¸ºæ ‘ï¼Œå®ç°æˆ˜ç•¥æ·±åº¦å’Œå¿«é€Ÿé€‚åº”æ€§çš„å¹³è¡¡ã€‚è¯¥æ¡†æ¶å¼•å…¥æ··åˆæ”¿ç­–ç»“æ„ï¼Œç»“åˆè§„åˆ™èŠ‚ç‚¹å’Œç¥ç»ç½‘ç»œç»„ä»¶ï¼Œå®ç°é«˜çº§æˆ˜ç•¥æ¨ç†å’Œç²¾ç¡®ä½çº§æ§åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„å¼€å‘æ•ˆç‡æå‡ã€æ”¿ç­–æ¨å¹¿å’Œè¡Œä¸ºå¤šæ ·æ€§ä¼˜åŠ¿ã€‚æ›´å¤šå®éªŒç»“æœè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://zhongwen.one/projects/portal">https://zhongwen.one/projects/portal</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PORTALæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå¯ä»¥å¼€å‘èƒ½åœ¨æ•°åƒæ¬¾ä¸‰ç»´è§†é¢‘æ¸¸æˆä¸­è¿›è¡Œæ¸¸æˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚</li>
<li>é€šè¿‡å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¡Œä¸ºæ ‘ã€‚</li>
<li>å¼•å…¥æ··åˆæ”¿ç­–ç»“æ„ï¼Œç»“åˆè§„åˆ™èŠ‚ç‚¹å’Œç¥ç»ç½‘ç»œç»„ä»¶ï¼Œå®ç°æˆ˜ç•¥å’Œç²¾ç¡®æ§åˆ¶çš„å¹³è¡¡ã€‚</li>
<li>é‡‡ç”¨åŒåé¦ˆæœºåˆ¶ï¼Œç»“åˆå®šé‡æ¸¸æˆæŒ‡æ ‡å’Œè§†è§‰è¯­è¨€æ¨¡å‹åˆ†æï¼Œä¿ƒè¿›æˆ˜æœ¯å’Œç­–ç•¥å±‚é¢çš„æ”¿ç­–æ”¹è¿›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨FPSæ¸¸æˆä¸­æ˜¾è‘—æå‡å¼€å‘æ•ˆç‡ã€æ”¿ç­–é€šç”¨æ€§å’Œè¡Œä¸ºå¤šæ ·æ€§ã€‚</li>
<li>ç”Ÿæˆçš„æ”¿ç­–å…·æœ‰å³æ—¶éƒ¨ç½²æ€§ã€äººç±»å¯è§£é‡Šæ€§å’Œè·¨ä¸åŒæ¸¸æˆç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c167cc28f9e4d1240446b3e2193eb448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b0b221d0b53ecebb1c021db30418ac3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-Complex-Reasoning-with-Dynamic-Prompt-Corruption-A-soft-prompt-Optimization-Approach"><a href="#Improving-Complex-Reasoning-with-Dynamic-Prompt-Corruption-A-soft-prompt-Optimization-Approach" class="headerlink" title="Improving Complex Reasoning with Dynamic Prompt Corruption: A soft   prompt Optimization Approach"></a>Improving Complex Reasoning with Dynamic Prompt Corruption: A soft   prompt Optimization Approach</h2><p><strong>Authors:Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye</strong></p>
<p>Prompt-tuning (PT) for large language models (LLMs) can facilitate the performance on various conventional NLP tasks with significantly fewer trainable parameters. However, our investigation reveals that PT provides limited improvement and may even degrade the primitive performance of LLMs on complex reasoning tasks. Such a phenomenon suggests that soft prompts can positively impact certain instances while negatively affecting others, particularly during the later phases of reasoning. To address these challenges, We first identify an information accumulation within the soft prompts. Through detailed analysis, we demonstrate that this phenomenon is often accompanied by erroneous information flow patterns in the deeper layers of the model, which ultimately lead to incorrect reasoning outcomes. we propose a novel method called \textbf{D}ynamic \textbf{P}rompt \textbf{C}orruption (DPC) to take better advantage of soft prompts in complex reasoning tasks, which dynamically adjusts the influence of soft prompts based on their impact on the reasoning process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic Trigger measures the impact of soft prompts, identifying whether beneficial or detrimental. Then, Dynamic Corruption mitigates the negative effects of soft prompts by selectively masking key tokens that interfere with the reasoning process. We validate the proposed approach through extensive experiments on various LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can consistently enhance the performance of PT, achieving 4%-8% accuracy gains compared to vanilla prompt tuning, highlighting the effectiveness of our approach and its potential to enhance complex reasoning in LLMs. </p>
<blockquote>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æç¤ºå¾®è°ƒï¼ˆPTï¼‰å¯ä»¥ä¾¿äºåœ¨å„ç§ä¼ ç»Ÿçš„NLPä»»åŠ¡ä¸­ä½¿ç”¨æ˜¾è‘—æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼ŒPTæä¾›çš„æ”¹è¿›éå¸¸æœ‰é™ï¼Œç”šè‡³å¯èƒ½ä¼šé™ä½LLMåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šçš„åŸå§‹æ€§èƒ½ã€‚è¿™ç§ç°è±¡è¡¨æ˜ï¼Œè½¯æç¤ºå¯èƒ½ä¼šå¯¹æŸäº›å®ä¾‹äº§ç”Ÿç§¯æå½±å“ï¼Œè€Œå¯¹å…¶ä»–å®ä¾‹äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†çš„åæœŸé˜¶æ®µã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šäº†è½¯æç¤ºä¸­çš„ä¿¡æ¯ç§¯ç´¯ã€‚é€šè¿‡è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬è¯æ˜è¿™ç§ç°è±¡é€šå¸¸ä¼´éšç€æ¨¡å‹æ·±å±‚ä¸­é”™è¯¯çš„ä¿¡æ¯æµæ¨¡å¼ï¼Œæœ€ç»ˆå¯¼è‡´é”™è¯¯çš„æ¨ç†ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€æç¤ºè…è´¥ï¼ˆDPCï¼‰çš„æ–°æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨è½¯æç¤ºè¿›è¡Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•ä¼šæ ¹æ®è½¯æç¤ºå¯¹æ¨ç†è¿‡ç¨‹çš„å½±å“è¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚å…·ä½“æ¥è¯´ï¼ŒDPCåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåŠ¨æ€è§¦å‘å’ŒåŠ¨æ€è…è´¥ã€‚é¦–å…ˆï¼ŒåŠ¨æ€è§¦å‘ä¼šè¡¡é‡è½¯æç¤ºçš„å½±å“ï¼Œåˆ¤æ–­å…¶æ˜¯æœ‰ç›Šè¿˜æ˜¯æœ‰å®³ã€‚ç„¶åï¼ŒåŠ¨æ€è…è´¥é€šè¿‡æœ‰é€‰æ‹©åœ°æ©ç›–å¹²æ‰°æ¨ç†è¿‡ç¨‹çš„å…³é”®ä»¤ç‰Œæ¥å‡è½»è½¯æç¤ºçš„è´Ÿé¢å½±å“ã€‚æˆ‘ä»¬é€šè¿‡åœ¨å„ç§LLMå’Œæ¨ç†ä»»åŠ¡ï¼ˆåŒ…æ‹¬GSM8Kã€MATHå’ŒAQuAï¼‰ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDPCå¯ä»¥æŒç»­æé«˜PTçš„æ€§èƒ½ï¼Œä¸ç®€å•çš„æç¤ºè°ƒæ•´ç›¸æ¯”ï¼Œå®ç°äº†4%-8%çš„å‡†ç¡®ç‡æå‡ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨å¢å¼ºLLMå¤æ‚æ¨ç†æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13208v1">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¾®è°ƒè½¯æç¤ºå¯ä¼˜åŒ–å„ç§å¸¸è§„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„è¡¨ç°ã€‚ä½†ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œå¾®è°ƒè½¯æç¤ºæä¾›çš„æ”¹è¿›æœ‰é™ï¼Œç”šè‡³å¯èƒ½é™ä½åŸå§‹æ€§èƒ½ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€æç¤ºè…è´¥ï¼ˆDPCï¼‰æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è½¯æç¤ºå¯¹æ¨ç†è¿‡ç¨‹çš„å½±å“ï¼Œæœ‰æ•ˆæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ–¹æ³•çš„ä¸¤ä¸ªé˜¶æ®µåˆ†åˆ«ä¸ºåŠ¨æ€è§¦å‘å’ŒåŠ¨æ€è…è´¥ï¼Œé€šè¿‡è¡¡é‡è½¯æç¤ºçš„å½±å“ï¼Œé€‰æ‹©æ€§å±è”½å¹²æ‰°æ¨ç†è¿‡ç¨‹çš„å…³é”®ä»¤ç‰Œæ¥å‡è½»è½¯æç¤ºçš„è´Ÿé¢å½±å“ã€‚åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯æç¤ºå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¸¸è§„NLPä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜åŒ–æœ‰ç§¯æä½œç”¨ï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ•ˆæœæœ‰é™æˆ–å¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œè½¯æç¤ºå¯èƒ½å¯¼è‡´ä¿¡æ¯ç§¯ç´¯å¹¶ä¼´éšé”™è¯¯çš„ä¿¡æ¯æµæ¨¡å¼ã€‚</li>
<li>åŠ¨æ€æç¤ºè…è´¥ï¼ˆDPCï¼‰æ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´è½¯æç¤ºçš„å½±å“æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…æ‹¬åŠ¨æ€è§¦å‘å’ŒåŠ¨æ€è…è´¥ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>åŠ¨æ€è§¦å‘ç”¨äºè¡¡é‡è½¯æç¤ºçš„å½±å“ï¼Œåˆ¤æ–­å…¶æ˜¯å¦æœ‰åˆ©äºæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>åŠ¨æ€è…è´¥é€šè¿‡é€‰æ‹©æ€§å±è”½å¹²æ‰°æ¨ç†è¿‡ç¨‹çš„å…³é”®ä»¤ç‰Œæ¥å‡è½»è½¯æç¤ºçš„è´Ÿé¢å½±å“ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDPCæ–¹æ³•èƒ½æ˜¾è‘—æå‡å¾®è°ƒè½¯æç¤ºçš„æ€§èƒ½ï¼Œä¸å¸¸è§„æ–¹æ³•ç›¸æ¯”ï¼Œå‡†ç¡®ç‡æå‡4%-8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d6efbba8f5889ca3c542b569b3e1de1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8657227c8346d1cdd3b15fe3a5c97daf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03e5fe464dd5fbcfab49e0f57c343495.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c131079371e09ef2ff021af59f1ffc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea74a04e94b791c29dc562267148d22.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="3DAxisPrompt-Promoting-the-3D-Grounding-and-Reasoning-in-GPT-4o"><a href="#3DAxisPrompt-Promoting-the-3D-Grounding-and-Reasoning-in-GPT-4o" class="headerlink" title="3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o"></a>3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o</h2><p><strong>Authors:Dingning Liu, Cheng Wang, Peng Gao, Renrui Zhang, Xinzhu Ma, Yuan Meng, Zhihui Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) exhibit impressive capabilities across a variety of tasks, especially when equipped with carefully designed visual prompts. However, existing studies primarily focus on logical reasoning and visual understanding, while the capability of MLLMs to operate effectively in 3D vision remains an ongoing area of exploration. In this paper, we introduce a novel visual prompting method, called 3DAxisPrompt, to elicit the 3D understanding capabilities of MLLMs in real-world scenes. More specifically, our method leverages the 3D coordinate axis and masks generated from the Segment Anything Model (SAM) to provide explicit geometric priors to MLLMs and then extend their impressive 2D grounding and reasoning ability to real-world 3D scenarios. Besides, we first provide a thorough investigation of the potential visual prompting formats and conclude our findings to reveal the potential and limits of 3D understanding capabilities in GPT-4o, as a representative of MLLMs. Finally, we build evaluation environments with four datasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various 3D tasks. Based on this, we conduct extensive quantitative and qualitative experiments, which demonstrate the effectiveness of the proposed method. Overall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can effectively perceive an objectâ€™s 3D position in real-world scenarios. Nevertheless, a single prompt engineering approach does not consistently achieve the best outcomes for all 3D tasks. This study highlights the feasibility of leveraging MLLMs for 3D vision grounding&#x2F;reasoning with prompt engineering techniques. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å½“é…å¤‡ç²¾å¿ƒè®¾è®¡çš„è§†è§‰æç¤ºæ—¶ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€»è¾‘æ¨ç†å’Œè§†è§‰ç†è§£ä¸Šï¼Œè€ŒMLLMsåœ¨3Dè§†è§‰ä¸­æœ‰æ•ˆæ“ä½œçš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ­£åœ¨æ¢ç´¢çš„é¢†åŸŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰æç¤ºæ–¹æ³•ï¼Œç§°ä¸º3DAxisPromptï¼Œä»¥æ¿€å‘MLLMsåœ¨ç°å®åœºæ™¯ä¸­çš„3Dç†è§£èƒ½åŠ›ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç”±Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„3Dåæ ‡è½´å’Œæ©ç ï¼Œä¸ºMLLMsæä¾›æ˜ç¡®çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†ï¼Œç„¶åå°†å®ƒä»¬ä»¤äººå°è±¡æ·±åˆ»çš„2Då®šä½å’Œæ¨ç†èƒ½åŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„3Dåœºæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡ç ”ç©¶äº†æ½œåœ¨çš„è§†è§‰æç¤ºæ ¼å¼ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼Œä»¥æ­ç¤ºGPT-4oç­‰MLLMsçš„3Dç†è§£èƒ½åŠ›çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ScanReferã€ScanNetã€FMBå’ŒnuSceneå››ä¸ªæ•°æ®é›†æ„å»ºäº†è¯„ä¼°ç¯å¢ƒï¼Œæ¶µç›–å„ç§3Dä»»åŠ¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå€ŸåŠ©3DAxisPromptï¼ŒMLLMså¯ä»¥æœ‰æ•ˆåœ°æ„ŸçŸ¥ç°å®åœºæ™¯ä¸­ç‰©ä½“çš„3Dä½ç½®ã€‚ç„¶è€Œï¼Œå•ä¸€çš„æç¤ºå·¥ç¨‹æ–¹æ³•å¹¶ä¸æ€»æ˜¯å¯¹æ‰€æœ‰3Dä»»åŠ¡éƒ½èƒ½å–å¾—æœ€ä½³æ•ˆæœã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯è¿›è¡ŒMLLMsçš„3Dè§†è§‰å®šä½&#x2F;æ¨ç†çš„å¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13185v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º3DAxisPromptçš„æ–°å‹è§†è§‰æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æ¿€å‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç°å®åœºæ™¯ä¸­çš„3Dç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”±Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„3Dåæ ‡è½´å’Œæ©ç ï¼Œä¸ºMLLMsæä¾›æ˜ç¡®çš„å‡ ä½•å…ˆéªŒä¿¡æ¯ï¼Œå¹¶å°†å…¶åœ¨2Dåœºæ™¯ä¸­çš„å‡ºè‰²åŸºç¡€çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„3Dåœºæ™¯ã€‚æ–‡ç« è¿˜å¯¹è§†è§‰æç¤ºæ ¼å¼è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼Œæ­ç¤ºäº†GPT-4oç­‰MLLMsåœ¨3Dç†è§£èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚é€šè¿‡æ„å»ºåŒ…å«å¤šç§3Dä»»åŠ¡çš„è¯„ä»·ç¯å¢ƒï¼Œè¿›è¡Œå®šé‡å’Œå®šæ€§çš„å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†MLLMså€ŸåŠ©3DAxisPromptæœ‰æ•ˆæ„ŸçŸ¥ç‰©ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸‰ç»´ä½ç½®çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå•ä¸€çš„æç¤ºå·¥ç¨‹æ–¹æ³•å¹¶ä¸æ€»æ˜¯å¯¹æ‰€æœ‰3Dä»»åŠ¡äº§ç”Ÿæœ€ä½³æ•ˆæœã€‚è¯¥ç ”ç©¶çªæ˜¾äº†åˆ©ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯å°†MLLMsç”¨äº3Dè§†è§‰æ¥åœ°&#x2F;æ¨ç†çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨3Dè§†è§‰ç†è§£æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»æ˜¯ä¸€ä¸ªæ­£åœ¨æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸º3DAxisPromptçš„æ–°å‹è§†è§‰æç¤ºæ–¹æ³•ï¼Œåˆ©ç”¨3Dåæ ‡è½´å’Œæ©ç æ¥æ¿€å‘MLLMsçš„3Dç†è§£èƒ½åŠ›ã€‚</li>
<li>å€ŸåŠ©Segment Anything Modelï¼ˆSAMï¼‰ï¼Œä¸ºMLLMsæä¾›æ˜ç¡®çš„å‡ ä½•å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æ„å»ºåŒ…å«å¤šç§3Dä»»åŠ¡çš„è¯„ä»·ç¯å¢ƒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>MLLMsèƒ½æœ‰æ•ˆæ„ŸçŸ¥ç‰©ä½“åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸‰ç»´ä½ç½®ã€‚</li>
<li>å•ä¸€çš„æç¤ºå·¥ç¨‹æ–¹æ³•å¹¶ä¸æ€»æ˜¯å¯¹æ‰€æœ‰3Dä»»åŠ¡äº§ç”Ÿæœ€ä½³æ•ˆæœï¼Œéœ€è¦é’ˆå¯¹ä¸åŒä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-181105fd11a624073c6aa7dbf89c6d62.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Triad-Empowering-LMM-based-Anomaly-Detection-with-Vision-Expert-guided-Visual-Tokenizer-and-Manufacturing-Process"><a href="#Triad-Empowering-LMM-based-Anomaly-Detection-with-Vision-Expert-guided-Visual-Tokenizer-and-Manufacturing-Process" class="headerlink" title="Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided   Visual Tokenizer and Manufacturing Process"></a>Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided   Visual Tokenizer and Manufacturing Process</h2><p><strong>Authors:Yuanze Li, Shihao Yuan, Haolin Wang, Qizhang Li, Ming Liu, Chen Xu, Guangming Shi, Wangmeng Zuo</strong></p>
<p>Although recent methods have tried to introduce large multimodal models (LMMs) into industrial anomaly detection (IAD), their generalization in the IAD field is far inferior to that for general purposes. We summarize the main reasons for this gap into two aspects. On one hand, general-purpose LMMs lack cognition of defects in the visual modality, thereby failing to sufficiently focus on defect areas. Therefore, we propose to modify the AnyRes structure of the LLaVA model, providing the potential anomalous areas identified by existing IAD models to the LMMs. On the other hand, existing methods mainly focus on identifying defects by learning defect patterns or comparing with normal samples, yet they fall short of understanding the causes of these defects. Considering that the generation of defects is closely related to the manufacturing process, we propose a manufacturing-driven IAD paradigm. An instruction-tuning dataset for IAD (InstructIAD) and a data organization approach for Chain-of-Thought with manufacturing (CoT-M) are designed to leverage the manufacturing process for IAD. Based on the above two modifications, we present Triad, a novel LMM-based method incorporating an expert-guided region-of-interest tokenizer and manufacturing process for industrial anomaly detection. Extensive experiments show that our Triad not only demonstrates competitive performance against current LMMs but also achieves further improved accuracy when equipped with manufacturing processes. Source code, training data, and pre-trained models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/tzjtatata/Triad">https://github.com/tzjtatata/Triad</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„æ–¹æ³•å°è¯•å°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å¼•å…¥å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰ï¼Œä½†å®ƒä»¬åœ¨IADé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›è¿œè¿œä¸å¦‚é€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬å°†é€ æˆè¿™ç§å·®è·çš„ä¸»è¦åŸå› æ€»ç»“ä¸ºä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œé€šç”¨å‹LMMå¯¹è§†è§‰æ¨¡æ€ä¸­çš„ç¼ºé™·ç¼ºä¹è®¤çŸ¥ï¼Œå› æ­¤æ— æ³•å……åˆ†å…³æ³¨ç¼ºé™·åŒºåŸŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¿®æ”¹LLaVAæ¨¡å‹çš„AnyResç»“æ„ï¼Œå°†ç°æœ‰IADæ¨¡å‹è¯†åˆ«çš„æ½œåœ¨å¼‚å¸¸åŒºåŸŸæä¾›ç»™LMMã€‚å¦ä¸€æ–¹é¢ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºé€šè¿‡å­¦ä¹ ç¼ºé™·æ¨¡å¼æˆ–ä¸æ­£å¸¸æ ·æœ¬è¿›è¡Œæ¯”è¾ƒæ¥è¯†åˆ«ç¼ºé™·ï¼Œä½†å®ƒä»¬æ— æ³•ç†è§£ç¼ºé™·çš„åŸå› ã€‚è€ƒè™‘åˆ°ç¼ºé™·çš„äº§ç”Ÿä¸åˆ¶é€ è¿‡ç¨‹å¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥åˆ¶é€ ä¸ºé©±åŠ¨çš„IADèŒƒå¼ã€‚è®¾è®¡äº†ç”¨äºIADçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†InstructIADå’Œç”¨äºChain-of-Thoughtçš„åˆ¶é€ æ•°æ®ç»„ç»‡æ–¹æ³•CoT-Mï¼Œä»¥åˆ©ç”¨åˆ¶é€ è¿‡ç¨‹è¿›è¡ŒIADã€‚åŸºäºä»¥ä¸Šä¸¤ä¸ªä¿®æ”¹ï¼Œæˆ‘ä»¬æå‡ºäº†Triadï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLMMçš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†ä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸæ ‡è®°å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Triadä¸ä»…ä¸å½“å‰çš„LMMç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œè€Œä¸”åœ¨é…å¤‡åˆ¶é€ è¿‡ç¨‹åè¿˜å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æºä»£ç ã€è®­ç»ƒæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tzjtatata/Triad%E4%B8%8A%E5%BC%80%E6%9C%89%E5%AE%9D%E5%B9%BF%E5%A4%AF%E7%9A%84%E7%BB%BC%E5%90%88%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/tzjtatata/Triadä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13184v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸå°è¯•å°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¼•å…¥å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰é¢†åŸŸçš„æ–¹æ³•è™½ç„¶æœ‰æ‰€è¿›å±•ï¼Œä½†å…¶æ³›åŒ–èƒ½åŠ›ç›¸è¾ƒäºé€šç”¨é¢†åŸŸä»æœ‰æ‰€ä¸è¶³ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæ–‡ç« ä»ä¸¤ä¸ªæ–¹é¢è¿›è¡Œäº†æ€»ç»“ä¸åˆ†æã€‚é¦–å…ˆï¼Œé€šç”¨çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰æ¨¡æ€ä¸Šå¯¹ç¼ºé™·çš„è®¤çŸ¥ä¸è¶³ï¼Œæ— æ³•å……åˆ†å…³æ³¨ç¼ºé™·åŒºåŸŸã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºå¯¹LLaVAæ¨¡å‹çš„AnyResç»“æ„è¿›è¡Œä¿®æ”¹ï¼Œå°†ç°æœ‰IADæ¨¡å‹è¯†åˆ«çš„æ½œåœ¨å¼‚å¸¸åŒºåŸŸæä¾›ç»™LMMsã€‚å…¶æ¬¡ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å­¦ä¹ ç¼ºé™·æ¨¡å¼æˆ–ä¸æ­£å¸¸æ ·æœ¬è¿›è¡Œæ¯”è¾ƒæ¥è¯†åˆ«ç¼ºé™·ï¼Œç¼ºä¹å¯¹ç¼ºé™·äº§ç”ŸåŸå› çš„æ·±å…¥ç†è§£ã€‚è€ƒè™‘åˆ°ç¼ºé™·çš„äº§ç”Ÿä¸åˆ¶é€ è¿‡ç¨‹å¯†åˆ‡ç›¸å…³ï¼Œæ–‡ç« æå‡ºäº†ä»¥åˆ¶é€ è¿‡ç¨‹é©±åŠ¨çš„IADèŒƒå¼ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†é’ˆå¯¹IADçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†InstructIADï¼Œä»¥åŠé€‚ç”¨äºåˆ¶é€ è¿‡ç¨‹çš„Chain-of-Thoughtæ•°æ®ç»„ç»‡æ–¹æ³•ï¼ˆCoT-Mï¼‰ã€‚åŸºäºä¸Šè¿°ä¸¤ä¸ªä¿®æ”¹ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºLMMçš„æ–°æ–¹æ³•Triadï¼Œç»“åˆä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸåˆ†è¯å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒTriadä¸ä»…ä¸å½“å‰LMMsç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œè€Œä¸”åœ¨é…å¤‡åˆ¶é€ è¿‡ç¨‹åå‡†ç¡®ç‡è¿›ä¸€æ­¥æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
<li>é€šç”¨LMMsåœ¨è§†è§‰æ¨¡æ€ä¸Šå¯¹ç¼ºé™·è®¤çŸ¥ä¸è¶³ï¼Œéœ€è¦å…³æ³¨ç¼ºé™·åŒºåŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç¼ºé™·çš„è¯†åˆ«å’Œæ¯”è¾ƒï¼Œä½†ç¼ºä¹å¯¹ç¼ºé™·äº§ç”ŸåŸå› çš„ç†è§£ã€‚</li>
<li>ç¼ºé™·äº§ç”Ÿä¸åˆ¶é€ è¿‡ç¨‹å¯†åˆ‡ç›¸å…³ï¼Œæå‡ºä»¥åˆ¶é€ è¿‡ç¨‹é©±åŠ¨çš„IADèŒƒå¼ã€‚</li>
<li>è®¾è®¡äº†InstructIADæ•°æ®é›†å’ŒCoT-Mæ•°æ®ç»„ç»‡æ–¹æ³•ä»¥åˆ©ç”¨åˆ¶é€ è¿‡ç¨‹è¿›è¡ŒIADã€‚</li>
<li>åŸºäºä¸Šè¿°æ”¹è¿›ï¼Œæå‡ºäº†LMMsä¸ºåŸºç¡€çš„æ–°æ–¹æ³•Triadï¼Œç»“åˆä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸåˆ†è¯å™¨å’Œåˆ¶é€ è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bdcd0b3f92f38d73583fc7f7aff64196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f007d3ceb170957429ba6cd84b3785b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3aaadf1b9bfc60f6f50f793813f8eaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77719c76765f8d455f677a2a174d1c30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-828ac609c5c15510398d5c217ba14964.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Free-form-language-based-robotic-reasoning-and-grasping"><a href="#Free-form-language-based-robotic-reasoning-and-grasping" class="headerlink" title="Free-form language-based robotic reasoning and grasping"></a>Free-form language-based robotic reasoning and grasping</h2><p><strong>Authors:Runyu Jiao, Alice Fasoli, Francesco Giuliari, Matteo Bortolon, Sergio Povoli, Guofeng Mei, Yiming Wang, Fabio Poiesi</strong></p>
<p>Performing robotic grasping from a cluttered bin based on human instructions is a challenging task, as it requires understanding both the nuances of free-form language and the spatial relationships between objects. Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have demonstrated remarkable reasoning capabilities across both text and images. But can they truly be used for this task in a zero-shot setting? And what are their limitations? In this paper, we explore these research questions via the free-form language-based robotic grasping task, and propose a novel method, FreeGrasp, leveraging the pre-trained VLMsâ€™ world knowledge to reason about human instructions and object spatial arrangements. Our method detects all objects as keypoints and uses these keypoints to annotate marks on images, aiming to facilitate GPT-4oâ€™s zero-shot spatial reasoning. This allows our method to determine whether a requested object is directly graspable or if other objects must be grasped and removed first. Since no existing dataset is specifically designed for this task, we introduce a synthetic dataset FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated instructions and ground-truth grasping sequences. We conduct extensive analyses with both FreeGraspData and real-world validation with a gripper-equipped robotic arm, demonstrating state-of-the-art performance in grasp reasoning and execution. Project website: <a target="_blank" rel="noopener" href="https://tev-fbk.github.io/FreeGrasp/">https://tev-fbk.github.io/FreeGrasp/</a>. </p>
<blockquote>
<p>æ‰§è¡ŒåŸºäºäººç±»æŒ‡ä»¤çš„ä»æ‚ä¹±ç®±ä¸­æŠ“å–ç‰©ä½“çš„æœºå™¨äººä»»åŠ¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒéœ€è¦ç†è§£è‡ªç”±å½¢å¼è¯­è¨€çš„ç»†å¾®å·®åˆ«ä»¥åŠç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚åœ¨webçº§æ•°æ®ä¸Šè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚GPT-4oï¼Œåœ¨æ–‡æœ¬å’Œå›¾åƒæ–¹é¢éƒ½è¡¨ç°å‡ºäº†æƒŠäººçš„æ¨ç†èƒ½åŠ›ã€‚ä½†å®ƒä»¬çœŸçš„å¯ä»¥ç”¨äºé›¶æ ·æœ¬è®¾ç½®ä¸­çš„è¿™ä¸ªä»»åŠ¡å—ï¼Ÿå®ƒä»¬åˆå­˜åœ¨å“ªäº›å±€é™æ€§å‘¢ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºè‡ªç”±å½¢å¼è¯­è¨€çš„æœºå™¨äººæŠ“å–ä»»åŠ¡æ¥æ¢ç´¢è¿™äº›ç ”ç©¶é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•FreeGraspï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„VLMsçš„ä¸–ç•ŒçŸ¥è¯†æ¥æ¨ç†äººç±»æŒ‡ä»¤å’Œç‰©ä½“çš„ç©ºé—´å¸ƒå±€ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ£€æµ‹æ‰€æœ‰ç‰©ä½“ä½œä¸ºå…³é”®ç‚¹ï¼Œå¹¶ä½¿ç”¨è¿™äº›å…³é”®ç‚¹åœ¨å›¾åƒä¸Šè¿›è¡Œæ ‡æ³¨ï¼Œæ—¨åœ¨ä¿ƒè¿›GPT-4oçš„é›¶æ ·æœ¬ç©ºé—´æ¨ç†ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç¡®å®šæ‰€è¯·æ±‚çš„å¯¹è±¡æ˜¯å¦å¯ä»¥ç›´æ¥æŠ“å–ï¼Œæˆ–è€…æ˜¯å¦å¿…é¡»å…ˆæŠ“å–å¹¶ç§»é™¤å…¶ä»–å¯¹è±¡ã€‚ç”±äºæ²¡æœ‰ç°æœ‰çš„æ•°æ®é›†æ˜¯ä¸“é—¨ä¸ºè¿™é¡¹ä»»åŠ¡è®¾è®¡çš„ï¼Œæˆ‘ä»¬é€šè¿‡æ‰©å±•MetaGraspNetV2æ•°æ®é›†ï¼Œå¼•å…¥äº†åŒ…å«äººç±»æ³¨é‡ŠæŒ‡ä»¤å’ŒçœŸå®æŠ“å–åºåˆ—çš„åˆæˆæ•°æ®é›†FreeGraspDataã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„FreeGraspDataåˆ†æä»¥åŠä¸é…å¤‡å¤¹å…·çš„æœºå™¨äººæ‰‹è‡‚çš„çœŸå®ä¸–ç•ŒéªŒè¯ï¼Œåœ¨æŠ“å–æ¨ç†å’Œæ‰§è¡Œæ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://tev-fbk.github.io/FreeGrasp/%E3%80%82">https://tev-fbk.github.io/FreeGrasp/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13082v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://tev-fbk.github.io/FreeGrasp/">https://tev-fbk.github.io/FreeGrasp/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æ¢ç´¢äº†åœ¨æ‚ä¹±æ— ç« çš„ç®±å­ä¸­è¿›è¡ŒåŸºäºäººç±»æŒ‡ä»¤çš„æœºå™¨äººæŠ“å–ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è·¨æ–‡æœ¬å’Œå›¾åƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚GPT-4oï¼Œæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•FreeGraspã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨VLMsçš„ä¸–ç•ŒçŸ¥è¯†ç†è§£äººç±»æŒ‡ä»¤å’Œç‰©ä½“ç©ºé—´å¸ƒå±€ï¼Œé€šè¿‡æ£€æµ‹æ‰€æœ‰ç‰©ä½“ä½œä¸ºå…³é”®ç‚¹å¹¶åœ¨å›¾åƒä¸Šè¿›è¡Œæ ‡æ³¨ï¼Œä»¥ä¿ƒè¿›GPT-4oçš„é›¶æ ·æœ¬ç©ºé—´æ¨ç†ã€‚æ–°æ–¹æ³•èƒ½å¤Ÿåœ¨æ— ç‰¹å®šæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åˆæˆçš„FreeGraspDataæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ç°å®ä¸–ç•Œçš„éªŒè¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æ¢ç´¢äº†åŸºäºäººç±»æŒ‡ä»¤ä»æ‚ä¹±æ— ç« çš„ç®±å­ä¸­è¿›è¡Œæœºå™¨äººæŠ“å–ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚GPT-4oè¿›è¡Œæœºå™¨äººæŠ“å–çš„æ–°æ–¹æ³•FreeGraspã€‚</li>
<li>FreeGraspæ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨VLMsçš„ä¸–ç•ŒçŸ¥è¯†ç†è§£äººç±»æŒ‡ä»¤å’Œç‰©ä½“ç©ºé—´å¸ƒå±€ã€‚</li>
<li>é€šè¿‡æ£€æµ‹ç‰©ä½“ä½œä¸ºå…³é”®ç‚¹å¹¶åœ¨å›¾åƒä¸Šæ ‡æ³¨ï¼Œä¿ƒè¿›GPT-4oçš„é›¶æ ·æœ¬ç©ºé—´æ¨ç†ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†ä¸€ä¸ªåˆæˆçš„FreeGraspDataæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¹¶éªŒè¯FreeGraspæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>FreeGraspæ–¹æ³•åœ¨æ¡æŒæ¨ç†å’Œæ‰§è¡Œæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4d8a1cf88eed10a0bcf6e5cae2b0b0d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ea619a875f3fc9f8aea99f0dec3bd96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b5e53f386479505164337e9b9e9010.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55594f3f74c77ff6f34b85a51eb3b0fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d48cecbedb5e899c488494baed89811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a503ef4b9ef28a893571e0e6e86d4157.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Crab-A-Unified-Audio-Visual-Scene-Understanding-Model-with-Explicit-Cooperation"><a href="#Crab-A-Unified-Audio-Visual-Scene-Understanding-Model-with-Explicit-Cooperation" class="headerlink" title="Crab: A Unified Audio-Visual Scene Understanding Model with Explicit   Cooperation"></a>Crab: A Unified Audio-Visual Scene Understanding Model with Explicit   Cooperation</h2><p><strong>Authors:Henghui Du, Guangyao Li, Chang Zhou, Chunjie Zhang, Alan Zhao, Di Hu</strong></p>
<p>In recent years, numerous tasks have been proposed to encourage model to develop specified capability in understanding audio-visual scene, primarily categorized into temporal localization, spatial localization, spatio-temporal reasoning, and pixel-level understanding. Instead, human possesses a unified understanding ability for diversified tasks. Therefore, designing an audio-visual model with general capability to unify these tasks is of great value. However, simply joint training for all tasks can lead to interference due to the heterogeneity of audiovisual data and complex relationship among tasks. We argue that this problem can be solved through explicit cooperation among tasks. To achieve this goal, we propose a unified learning method which achieves explicit inter-task cooperation from both the perspectives of data and model thoroughly. Specifically, considering the labels of existing datasets are simple words, we carefully refine these datasets and construct an Audio-Visual Unified Instruction-tuning dataset with Explicit reasoning process (AV-UIE), which clarifies the cooperative relationship among tasks. Subsequently, to facilitate concrete cooperation in learning stage, an interaction-aware LoRA structure with multiple LoRA heads is designed to learn different aspects of audiovisual data interaction. By unifying the explicit cooperation across the data and model aspect, our method not only surpasses existing unified audio-visual model on multiple tasks, but also outperforms most specialized models for certain tasks. Furthermore, we also visualize the process of explicit cooperation and surprisingly find that each LoRA head has certain audio-visual understanding ability. Code and dataset: <a target="_blank" rel="noopener" href="https://github.com/GeWu-Lab/Crab">https://github.com/GeWu-Lab/Crab</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä¸ºäº†é¼“åŠ±æ¨¡å‹å‘å±•ç†è§£éŸ³é¢‘è§†è§‰åœºæ™¯çš„èƒ½åŠ›ï¼Œå·²ç»æå‡ºäº†è®¸å¤šä»»åŠ¡ï¼Œä¸»è¦å¯åˆ†ä¸ºæ—¶é—´å®šä½ã€ç©ºé—´å®šä½ã€æ—¶ç©ºæ¨ç†å’Œåƒç´ çº§ç†è§£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å…·å¤‡å¯¹å„ç§ä»»åŠ¡çš„ç»Ÿä¸€ç†è§£èƒ½åŠ›ã€‚å› æ­¤ï¼Œè®¾è®¡ä¸€ç§å…·å¤‡ç»Ÿä¸€è¿™äº›ä»»åŠ¡çš„ä¸€èˆ¬èƒ½åŠ›çš„è§†å¬æ¨¡å‹æ˜¯éå¸¸æœ‰ä»·å€¼çš„ã€‚ç„¶è€Œï¼Œç®€å•åœ°å¯¹æ‰€æœ‰ä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒå¯èƒ½ä¼šå¯¼è‡´ç”±äºè§†å¬æ•°æ®çš„å¼‚è´¨æ€§å’Œä»»åŠ¡ä¹‹é—´çš„å¤æ‚å…³ç³»å¯¼è‡´çš„å¹²æ‰°ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡ä»»åŠ¡ä¹‹é—´çš„æ˜¾å¼åˆä½œæ¥è§£å†³ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å­¦ä¹ æ–¹æ³•ï¼Œä»æ•°æ®å’Œæ¨¡å‹çš„è§†è§’å®ç°ä»»åŠ¡çš„æ˜¾å¼åˆä½œã€‚å…·ä½“æ¥è¯´ï¼Œè€ƒè™‘åˆ°ç°æœ‰æ•°æ®é›†çš„æ ‡ç­¾æ˜¯ç®€å•çš„å•è¯ï¼Œæˆ‘ä»¬ç²¾å¿ƒåœ°ç»†åŒ–äº†è¿™äº›æ•°æ®é›†ï¼Œæ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰æ˜ç¡®æ¨ç†è¿‡ç¨‹çš„è§†å¬ç»Ÿä¸€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ˆAV-UIEï¼‰ï¼Œè¯¥æ•°æ®é›†æ˜ç¡®äº†ä»»åŠ¡ä¹‹é—´çš„åˆä½œå…³ç³»ã€‚éšåï¼Œä¸ºäº†åœ¨å­¦ä¹ é˜¶æ®µä¿ƒè¿›å…·ä½“çš„åˆä½œï¼Œè®¾è®¡äº†ä¸€ç§å…·æœ‰å¤šä¸ªLoRAå¤´çš„ä¿¡æ¯äº¤äº’æ„ŸçŸ¥LoRAç»“æ„ã€‚é€šè¿‡ç»Ÿä¸€æ•°æ®å’Œæ¨¡å‹æ–¹é¢çš„æ˜¾å¼åˆä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€è§†å¬æ¨¡å‹ï¼Œè€Œä¸”è¿˜åœ¨æŸäº›ç‰¹å®šä»»åŠ¡ä¸Šè¶…è¿‡äº†å¤§å¤šæ•°ä¸“ç”¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯è§†åŒ–äº†æ˜¾å¼åˆä½œçš„è¿‡ç¨‹ï¼Œå¹¶æƒŠè®¶åœ°å‘ç°æ¯ä¸ªLoRAå¤´éƒ½å…·æœ‰ä¸€å®šçš„éŸ³é¢‘è§†è§‰ç†è§£èƒ½åŠ›ã€‚ä»£ç å’Œæ•°æ®é›†åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/GeWu-Lab/Crab">https://github.com/GeWu-Lab/Crab</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13068v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†å¬å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®å±‚é¢çš„ç»†åŒ–å¤„ç†å’Œæ¨¡å‹å±‚é¢çš„åˆ›æ–°è®¾è®¡ï¼Œå®ç°äº†ä¸åŒè§†å¬ä»»åŠ¡é—´çš„æ˜¾å¼åˆä½œã€‚é€šè¿‡æ„å»ºAV-UIEæ•°æ®é›†å’Œå¼•å…¥äº¤äº’æ„ŸçŸ¥çš„LoRAç»“æ„ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è§†å¬æ¨¡å‹ï¼Œç”šè‡³åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¿‡äº†ä¸“é—¨çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†å¬åœºæ™¯ç†è§£çš„ä»»åŠ¡ä¸»è¦åŒ…æ‹¬æ—¶é—´å®šä½ã€ç©ºé—´å®šä½ã€æ—¶ç©ºæ¨ç†å’Œåƒç´ çº§ç†è§£ã€‚</li>
<li>å•çº¯è”åˆè®­ç»ƒæ‰€æœ‰ä»»åŠ¡ä¼šå› è§†å¬æ•°æ®çš„å¼‚è´¨æ€§å’Œä»»åŠ¡é—´å¤æ‚å…³ç³»å¯¼è‡´å¹²æ‰°ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®å’Œæ¨¡å‹ä¸¤ä¸ªå±‚é¢çš„æ˜¾å¼ä»»åŠ¡åˆä½œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ„å»ºäº†AV-UIEæ•°æ®é›†ï¼Œæ˜ç¡®äº†ä»»åŠ¡é—´çš„åˆä½œå…³ç³»ã€‚</li>
<li>å¼•å…¥äº†äº¤äº’æ„ŸçŸ¥çš„LoRAç»“æ„ï¼Œè®¾è®¡äº†å¤šä¸ªLoRAå¤´æ¥å­¦ä¹ è§†å¬æ•°æ®çš„ä¸åŒäº¤äº’æ–¹é¢ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…åœ¨å¤šä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è§†å¬æ¨¡å‹ï¼Œè€Œä¸”åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¿‡äº†ä¸“ä¸šæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b020ae61fa1e2ed81b1f120f91d24fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e3c10028eaa7e27e317033a70d0a4ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f3753a6c09cb239cecafb09aec8f6b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5661e525b3837a8309f7970060ff833a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be4ec85f91a4dd384727838a2129ab83.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Multi-Stage-Framework-with-Taxonomy-Guided-Reasoning-for-Occupation-Classification-Using-Large-Language-Models"><a href="#A-Multi-Stage-Framework-with-Taxonomy-Guided-Reasoning-for-Occupation-Classification-Using-Large-Language-Models" class="headerlink" title="A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation   Classification Using Large Language Models"></a>A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation   Classification Using Large Language Models</h2><p><strong>Authors:Palakorn Achananuparp, Ee-Peng Lim</strong></p>
<p>Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show significant improvements in classification accuracy. Furthermore, we demonstrate the frameworkâ€™s adaptability for multi-label skill classification. Our results indicate that the framework outperforms existing LLM-based methods, offering a practical and scalable solution for occupation classification and related tasks across LLMs. </p>
<blockquote>
<p>è‡ªåŠ¨ä½¿ç”¨æ ‡å‡†åŒ–èŒä¸šå¯¹ä½œä¸šæ•°æ®è¿›è¡Œæ³¨é‡Šï¼Œè¿™åœ¨èŒä¸šåˆ†ç±»ä¸­è¢«ç§°ä¸ºåˆ†ç±»ï¼Œå¯¹äºåŠ³åŠ¨åŠ›å¸‚åœºåˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç¨€ç¼ºå’Œæ‰‹åŠ¨æ³¨é‡Šçš„æŒ‘æˆ˜ï¼Œè¿™ä¸€ä»»åŠ¡å¾€å¾€å—é˜»ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è€Œå…·æœ‰æ½œåŠ›ï¼Œä½†å…¶å¯¹èŒä¸šåˆ†ç±»çŸ¥è¯†çš„äº†è§£ä»ç„¶ä¸æ˜ç¡®ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°LLMç”Ÿæˆç²¾ç¡®åˆ†ç±»å®ä½“çš„èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«æ¨ç†ã€æ£€ç´¢å’Œé‡æ–°æ’åºé˜¶æ®µçš„å¤šé˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†åˆ†ç±»æŒ‡å¯¼çš„æ¨ç†ç¤ºä¾‹ï¼Œé€šè¿‡å¯¹é½è¾“å‡ºå’Œåˆ†ç±»çŸ¥è¯†æ¥æé«˜æ€§èƒ½ã€‚åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œåˆ†ç±»ç²¾åº¦æœ‰äº†æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤šæ ‡ç­¾æŠ€èƒ½åˆ†ç±»æ–¹é¢çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨èŒä¸šåˆ†ç±»å’Œç›¸å…³ä»»åŠ¡æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„åŸºäºLLMçš„æ–¹æ³•ï¼Œä¸ºè·¨LLMçš„èŒä¸šåˆ†ç±»å’Œç›¸å…³ä»»åŠ¡æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ </p>
</blockquote>
<p>é’ˆå¯¹ä¸Šè¿°æ–‡æœ¬ï¼Œå…¶ç¿»è¯‘ä¸ºï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12989v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è‡ªåŠ¨æ ‡æ³¨èŒä¸šæ•°æ®å¹¶è¿›è¡Œæ ‡å‡†åŒ–èŒä¸šåˆ†ç±»å¯¹äºåŠ³åŠ¨åŠ›å¸‚åœºåˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç¨€ç¼ºæ€§å’Œæ‰‹åŠ¨æ ‡æ³¨çš„æŒ‘æˆ˜ï¼Œæ­¤ä»»åŠ¡å¾€å¾€å—é˜»ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”±äºå…¶ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è€Œå…·æœ‰æ½œåŠ›ï¼Œä½†å…¶å¯¹èŒä¸šåˆ†ç±»çŸ¥è¯†çš„äº†è§£å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†LLMsç”Ÿæˆç²¾ç¡®åˆ†ç±»æœ¯è¯­çš„èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒäº†å…¶å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«æ¨ç†ã€æ£€ç´¢å’Œé‡æ–°æ’åºé˜¶æ®µçš„å¤šé˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåˆ†ç±»çŸ¥è¯†æŒ‡å¯¼çš„æ¨ç†ç¤ºä¾‹æ¥æé«˜æ€§èƒ½ï¼Œä½¿è¾“å‡ºä¸åˆ†ç±»çŸ¥è¯†ç›¸ç¬¦ã€‚å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºåˆ†ç±»ç²¾åº¦æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤šé‡æ ‡ç­¾æŠ€èƒ½åˆ†ç±»ä¸­çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨èŒä¸šåˆ†ç±»å’Œç›¸å…³ä»»åŠ¡æ–¹é¢ä¼˜äºç°æœ‰çš„LLMæ–¹æ³•ï¼Œä¸ºè·¨LLMsæä¾›å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªåŠ¨æ ‡æ³¨èŒä¸šæ•°æ®å¹¶è¿›è¡Œæ ‡å‡†åŒ–èŒä¸šåˆ†ç±»æ˜¯åŠ³åŠ¨åŠ›å¸‚åœºåˆ†æçš„å…³é”®ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èŒä¸šåˆ†ç±»ä¸Šå…·æœ‰æ½œåŠ›ï¼Œä½†éœ€æé«˜å…¶å¯¹äºèŒä¸šåˆ†ç±»çŸ¥è¯†çš„äº†è§£ã€‚</li>
<li>LLMsåœ¨ç”Ÿæˆç²¾ç¡®åˆ†ç±»æœ¯è¯­æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„å¤šé˜¶æ®µæ¡†æ¶åŒ…æ‹¬æ¨ç†ã€æ£€ç´¢å’Œé‡æ–°æ’åºé˜¶æ®µï¼Œæ—¨åœ¨æé«˜LLMsåœ¨èŒä¸šåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶é›†æˆäº†åˆ†ç±»çŸ¥è¯†æŒ‡å¯¼çš„æ¨ç†ç¤ºä¾‹ï¼Œä»¥å¯¹é½è¾“å‡ºä¸åˆ†ç±»çŸ¥è¯†ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†åˆ†ç±»ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a6ba3c3718a68533063a6ca221f2d4d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ef8c905f3c28f5089cc5109d4dc9acc.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_R1_Reasoning/2503.12989v1/page_2_1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f57a92e75ebfc4dd90dd654bbbdb290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-048b831da829ed2e1f8421c4953661ba.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Aligning-Vision-to-Language-Text-Free-Multimodal-Knowledge-Graph-Construction-for-Enhanced-LLMs-Reasoning"><a href="#Aligning-Vision-to-Language-Text-Free-Multimodal-Knowledge-Graph-Construction-for-Enhanced-LLMs-Reasoning" class="headerlink" title="Aligning Vision to Language: Text-Free Multimodal Knowledge Graph   Construction for Enhanced LLMs Reasoning"></a>Aligning Vision to Language: Text-Free Multimodal Knowledge Graph   Construction for Enhanced LLMs Reasoning</h2><p><strong>Authors:Junming Liu, Siyuan Meng, Yanting Gao, Song Mao, Pinlong Cai, Guohang Yan, Yirong Chen, Zilin Bian, Botian Shi, Ding Wang</strong></p>
<p>Multimodal reasoning in Large Language Models (LLMs) struggles with incomplete knowledge and hallucination artifacts, challenges that textual Knowledge Graphs (KGs) only partially mitigate due to their modality isolation. While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal understanding, their practical construction is impeded by semantic narrowness of manual text annotations and inherent noise in visual-semantic entity linkages. In this paper, we propose Vision-align-to-Language integrated Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances LLMs reasoning through cross-modal information supplementation. Specifically, we cascade pre-trained Vision-Language Models (VLMs) to align image features with text, transforming them into descriptions that encapsulate image-specific information. Furthermore, we developed a cross-modal similarity verification mechanism to quantify semantic consistency, effectively filtering out noise introduced during feature alignment. Even without manually annotated image captions, the refined descriptions alone suffice to construct the MMKG. Compared to conventional MMKGs construction paradigms, our approach achieves substantial storage efficiency gains while maintaining direct entity-to-image linkage capability. Experimental results on multimodal reasoning tasks demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art models. Our code is published at <a target="_blank" rel="noopener" href="https://github.com/Wings-Of-Disaster/VaLiK">https://github.com/Wings-Of-Disaster/VaLiK</a>. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œå¤„ç†ä¸å®Œæ•´çŸ¥è¯†å’Œå¹»è§‰æ•ˆåº”é¢ä¸´å›°éš¾ã€‚ç”±äºæ¨¡æ€éš”ç¦»çš„åŸå› ï¼Œæ–‡æœ¬çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰åªèƒ½éƒ¨åˆ†ç¼“è§£è¿™äº›æŒ‘æˆ˜ã€‚è™½ç„¶å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆMMKGï¼‰æœ‰æœ›å¢å¼ºè·¨æ¨¡æ€ç†è§£ï¼Œä½†å…¶å®é™…æ„å»ºå—åˆ°æ‰‹åŠ¨æ–‡æœ¬æ³¨é‡Šçš„è¯­ä¹‰ç‹­çª„å’Œè§†è§‰è¯­ä¹‰å®ä½“é“¾æ¥ä¸­çš„å›ºæœ‰å™ªå£°çš„é˜»ç¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Vision-align-to-Languageé›†æˆçŸ¥è¯†å›¾è°±ï¼ˆVaLiKï¼‰è¿™ä¸€æ„å»ºMMKGçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€ä¿¡æ¯è¡¥å……å¢å¼ºLLMæ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çº§è”èµ·æ¥ï¼Œå°†å›¾åƒç‰¹å¾ä¸æ–‡æœ¬å¯¹é½ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºåŒ…å«å›¾åƒç‰¹å®šä¿¡æ¯çš„æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è·¨æ¨¡æ€ç›¸ä¼¼æ€§éªŒè¯æœºåˆ¶ï¼Œä»¥é‡åŒ–è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæœ‰æ•ˆåœ°è¿‡æ»¤å‡ºåœ¨ç‰¹å¾å¯¹é½è¿‡ç¨‹ä¸­å¼•å…¥çš„å™ªå£°ã€‚å³ä½¿æ²¡æœ‰æ‰‹åŠ¨æ³¨é‡Šçš„å›¾åƒæ ‡é¢˜ï¼Œç»è¿‡æç‚¼çš„æè¿°ä¹Ÿè¶³ä»¥æ„å»ºMMKGã€‚ä¸ä¼ ç»Ÿçš„MMKGæ„å»ºèŒƒå¼ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒç›´æ¥å®ä½“åˆ°å›¾åƒé“¾æ¥èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å·¨å¤§çš„å­˜å‚¨æ•ˆç‡æå‡ã€‚åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡VaLiKå¢å¼ºçš„LLMæ€§èƒ½ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Wings-Of-Disaster/VaLiK%E3%80%82">https://github.com/Wings-Of-Disaster/VaLiKã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12972v1">PDF</a> 14 pages, 7 figures, 6 tables</p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ¨¡æ€æ¨ç†é¢ä¸´çŸ¥è¯†ä¸å®Œæ•´å’Œå¹»è§‰ä¼ªåƒçš„é—®é¢˜ï¼Œæ–‡æœ¬çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç”±äºæ¨¡æ€éš”ç¦»åªèƒ½éƒ¨åˆ†ç¼“è§£è¿™äº›é—®é¢˜ã€‚è™½ç„¶å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆMMKGï¼‰æ‰¿è¯ºå¢å¼ºè·¨æ¨¡æ€ç†è§£ï¼Œä½†å…¶å®é™…æ„å»ºå—åˆ°æ‰‹åŠ¨æ–‡æœ¬æ³¨é‡Šçš„è¯­ä¹‰ç‹­çª„æ€§å’Œè§†è§‰è¯­ä¹‰å®ä½“é“¾æ¥ä¸­çš„å›ºæœ‰å™ªå£°çš„é˜»ç¢ã€‚æœ¬æ–‡æå‡ºVision-align-to-Languageé›†æˆçŸ¥è¯†å›¾è°±ï¼ˆVaLiKï¼‰æ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€ä¿¡æ¯è¡¥å……å¢å¼ºLLMæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çº§è”ï¼Œä»¥å°†å›¾åƒç‰¹å¾ä¸æ–‡æœ¬å¯¹é½ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåŒ…å«å›¾åƒç‰¹å®šä¿¡æ¯çš„æè¿°ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§è·¨æ¨¡æ€ç›¸ä¼¼æ€§éªŒè¯æœºåˆ¶ï¼Œä»¥é‡åŒ–è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæœ‰æ•ˆåœ°è¿‡æ»¤å‡ºåœ¨ç‰¹å¾å¯¹é½è¿‡ç¨‹ä¸­å¼•å…¥çš„å™ªå£°ã€‚å³ä½¿åœ¨æ²¡æœ‰æ‰‹åŠ¨æ³¨é‡Šçš„å›¾åƒæ ‡é¢˜çš„æƒ…å†µä¸‹ï¼Œç»è¿‡ä¼˜åŒ–çš„æè¿°ä¹Ÿè¶³ä»¥æ„å»ºMMKGã€‚ä¸å¸¸è§„MMKGæ„å»ºèŒƒå¼ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç›´æ¥å®ä½“åˆ°å›¾åƒé“¾æ¥èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å·¨å¤§çš„å­˜å‚¨æ•ˆç‡æå‡ã€‚åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºVaLiKçš„LLMä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å¤šæ¨¡æ€æ¨ç†é¢ä¸´çŸ¥è¯†ä¸å®Œæ•´å’Œå¹»è§‰ä¼ªåƒçš„æŒ‘æˆ˜ã€‚</li>
<li>æ–‡æœ¬çŸ¥è¯†å›¾è°±åªèƒ½éƒ¨åˆ†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶æ¨¡æ€éš”ç¦»ã€‚</li>
<li>VaLiKæ˜¯ä¸€ç§æ„å»ºå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€ä¿¡æ¯è¡¥å……å¢å¼ºLLMæ¨ç†ã€‚</li>
<li>VaLiKä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å°†å›¾åƒç‰¹å¾ä¸æ–‡æœ¬å¯¹é½ã€‚</li>
<li>VaLiKå¼€å‘äº†ä¸€ç§è·¨æ¨¡æ€ç›¸ä¼¼æ€§éªŒè¯æœºåˆ¶ï¼Œä»¥è¿‡æ»¤æ‰ç‰¹å¾å¯¹é½è¿‡ç¨‹ä¸­çš„å™ªå£°ã€‚</li>
<li>å³ä½¿åœ¨æ²¡æœ‰æ‰‹åŠ¨æ³¨é‡Šçš„å›¾åƒæ ‡é¢˜çš„æƒ…å†µä¸‹ï¼ŒVaLiKä¹Ÿèƒ½æ„å»ºMMKGã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96fba9632e75cdb7e07c66943839a3fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39e7136572f4c18ad5ff2bdc1bf31dbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1696eb334ca305000eb6a4e81b64ab70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-651e33caf46251d9f9c696839eb93637.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="R1-VL-Learning-to-Reason-with-Multimodal-Large-Language-Models-via-Step-wise-Group-Relative-Policy-Optimization"><a href="#R1-VL-Learning-to-Reason-with-Multimodal-Large-Language-Models-via-Step-wise-Group-Relative-Policy-Optimization" class="headerlink" title="R1-VL: Learning to Reason with Multimodal Large Language Models via   Step-wise Group Relative Policy Optimization"></a>R1-VL: Learning to Reason with Multimodal Large Language Models via   Step-wise Group Relative Policy Optimization</h2><p><strong>Authors:Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, Dacheng Tao</strong></p>
<p>Recent studies generally enhance MLLMsâ€™ reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMsâ€™ reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶é€šå¸¸é€šè¿‡åœ¨é«˜è´¨é‡çš„æ€ç»´é“¾æ¨ç†æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå¢å¼ºMLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¾€å¾€å¯¼è‡´æ¨¡å‹ä»…ä»…æ˜¯æ¨¡ä»¿æˆåŠŸçš„æ¨ç†è·¯å¾„ï¼Œè€Œæ²¡æœ‰ç†è§£é”™è¯¯çš„æ¨ç†è·¯å¾„æ˜¯ä»€ä¹ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡ä¸ä»…ä»…æ˜¯è¢«åŠ¨åœ°æ¨¡ä»¿ç§¯æçš„æ¨ç†è·¯å¾„æ¥æå‡MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†é€æ­¥ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆStepGRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç®€å•ã€æœ‰æ•ˆå’Œå¯†é›†çš„é€æ­¥å¥–åŠ±ï¼Œä½¿MLLMsèƒ½å¤Ÿè‡ªæˆ‘æå‡æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒStepGRPOå¼•å…¥äº†ä¸¤é¡¹åŸºäºè§„åˆ™çš„æ–°æ¨ç†å¥–åŠ±ï¼šé€æ­¥æ¨ç†å‡†ç¡®æ€§å¥–åŠ±ï¼ˆStepRARï¼‰å’Œé€æ­¥æ¨ç†æœ‰æ•ˆæ€§å¥–åŠ±ï¼ˆStepRVRï¼‰ã€‚StepRARé€šè¿‡è½¯å…³é”®æ­¥éª¤åŒ¹é…æŠ€æœ¯å¥–åŠ±åŒ…å«å¿…è¦ä¸­é—´æ¨ç†æ­¥éª¤çš„æ¨ç†è·¯å¾„ï¼Œè€ŒStepRVRåˆ™å¥–åŠ±éµå¾ªç»“æ„è‰¯å¥½å’Œé€»è¾‘ä¸€è‡´çš„æ¨ç†è¿‡ç¨‹çš„æ¨ç†è·¯å¾„ï¼Œé€šè¿‡æ¨ç†çš„å®Œæ•´æ€§å’Œé€»è¾‘è¯„ä¼°ç­–ç•¥æ¥å®ç°ã€‚é€šè¿‡æå‡ºçš„StepGRPOï¼Œæˆ‘ä»¬æ¨å‡ºäº†R1-VLç³»åˆ—MLLMsï¼Œåœ¨é€æ­¥æ¨ç†æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡ä¸€ç§åä¸ºStepGRPOçš„æ–°å‹åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¢å¼ºMLLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ç§åŸºäºè§„åˆ™çš„æ¨ç†å¥–åŠ±ï¼šé€æ­¥æ¨ç†å‡†ç¡®æ€§å¥–åŠ±ï¼ˆStepRARï¼‰å’Œé€æ­¥æ¨ç†æœ‰æ•ˆæ€§å¥–åŠ±ï¼ˆStepRVRï¼‰ã€‚é€šè¿‡é€æ­¥å¥–åŠ±æœºåˆ¶ï¼ŒMLLMsèƒ½å¤Ÿé€šè¿‡ç®€å•ã€æœ‰æ•ˆä¸”å¯†é›†çš„é€æ­¥å¥–åŠ±è‡ªæˆ‘æå‡æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯è¢«åŠ¨æ¨¡ä»¿æˆåŠŸçš„æ¨ç†è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç°çŠ¶ï¼šå½“å‰ç ”ç©¶ä¸»è¦é€šè¿‡åœ¨é«˜è´¨é‡æ€ç»´é“¾æ•°æ®ä¸Šç›‘ç£å¾®è°ƒå¢å¼ºMLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™ç§æ–¹å¼å¾€å¾€å¯¼è‡´æ¨¡å‹åªæ˜¯æ¨¡ä»¿æˆåŠŸçš„æ¨ç†è·¯å¾„ï¼Œç¼ºä¹å¯¹é”™è¯¯æ¨ç†è·¯å¾„çš„ç†è§£ã€‚</li>
<li>ç›®æ ‡ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è¶…è¶Šè¢«åŠ¨æ¨¡ä»¿æ­£å‘æ¨ç†è·¯å¾„ï¼ŒçœŸæ­£æå‡MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•ï¼šå¼•å…¥æ–°å‹åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶StepGRPOï¼Œé€šè¿‡ç®€å•ã€æœ‰æ•ˆä¸”å¯†é›†çš„é€æ­¥å¥–åŠ±ï¼Œä½¿MLLMsèƒ½å¤Ÿè‡ªæˆ‘æå‡æ¨ç†èƒ½åŠ›ã€‚</li>
<li>StepGRPOç‰¹ç‚¹ï¼šå¼•å…¥ä¸¤ç§åŸºäºè§„åˆ™çš„æ¨ç†å¥–åŠ±â€”â€”StepRARå’ŒStepRVRã€‚</li>
<li>StepRARä½œç”¨ï¼šå¥–åŠ±åŒ…å«å¿…è¦ä¸­é—´æ¨ç†æ­¥éª¤çš„æ¨ç†è·¯å¾„ï¼Œé€šè¿‡è½¯å…³é”®æ­¥éª¤åŒ¹é…æŠ€æœ¯å®ç°ã€‚</li>
<li>StepRVRä½œç”¨ï¼šå¥–åŠ±éµå¾ªç»“æ„è‰¯å¥½ã€é€»è¾‘ä¸€è‡´çš„æ¨ç†è¿‡ç¨‹çš„è·¯å¾„ï¼Œé€šè¿‡æ¨ç†å®Œæ•´æ€§å’Œé€»è¾‘è¯„ä¼°ç­–ç•¥å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7d56db10e73598eb1fe3a364c3bcd33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd001e15cea01a72fcb7a392e2daef33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95c0bb725ffaa84364f8758daa761026.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b791d626a468513380d40ab03f3fddff.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks"><a href="#FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks" class="headerlink" title="FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks"></a>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks</h2><p><strong>Authors:Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</strong></p>
<p>The current dominant approach for neural speech enhancement relies on purely-supervised deep learning using simulated pairs of far-field noisy-reverberant speech (mixtures) and clean speech. However, these trained models often exhibit limited generalizability to real-recorded mixtures. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a novel framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce a novel evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods. </p>
<blockquote>
<p>å½“å‰ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºçš„ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºä½¿ç”¨æ¨¡æ‹Ÿçš„è¿œè·ç¦»å™ªå£°æ··å“è¯­éŸ³ï¼ˆæ··åˆç‰©ï¼‰å’Œæ¸…æ´è¯­éŸ³çš„é…å¯¹è¿›è¡Œçº¯ç›‘ç£æ·±åº¦å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨çœŸå®å½•éŸ³æ··åˆç‰©ä¸Šçš„æ³›åŒ–èƒ½åŠ›å¾€å¾€æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†ç›´æ¥åœ¨çœŸå®æ··åˆç‰©ä¸Šè®­ç»ƒå¢å¼ºæ¨¡å‹çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†å•é€šé“è¿œè·ç¦»åˆ°è¿‘è·ç¦»è¯­éŸ³å¢å¼ºï¼ˆFNSEï¼‰ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨ç°å®ä¸–ç•Œæ•°æ®ï¼Œå…¶ç‰¹ç‚¹ä¸ºä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€é«˜æ··å“ã€ä»¥åŠä¸­é«˜é¢‘è¡°å‡ã€‚æˆ‘ä»¬æå‡ºäº†FNSE-SBGANè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†åŸºäºSchrodinger Bridgeï¼ˆSBï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸è¿œè·ç¦»ä¿¡å·ç›¸æ¯”ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰é™ä½äº†é«˜è¾¾14.58%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFNSE-SBGANä¿æŒäº†å“è¶Šçš„ä¸»è§‚è´¨é‡ï¼Œä¸ºç°å®ä¸–ç•Œä¸­çš„è¿œè·ç¦»è¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨æ—¶é¢‘åŸŸçš„çŸ©é˜µç§©åˆ†æï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†ç³»ç»Ÿçš„è§è§£ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12936v1">PDF</a> 13 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºçœŸå®æ··åˆç‰©çš„è¯­éŸ³å¢å¼ºæ¨¡å‹è®­ç»ƒé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä½ä¿¡å™ªæ¯”ã€é«˜æ··å“å’Œä¸­é«˜é¢‘è¡°å‡çš„å®é™…æƒ…å†µã€‚æå‡ºFNSE-SBGANæ¡†æ¶ï¼Œç»“åˆåŸºäºSchrodinger Bridgeçš„æ‰©æ•£æ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œå®ç°å‰æ²¿æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½å­—ç¬¦é”™è¯¯ç‡ã€‚åŒæ—¶ï¼Œå¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨æ—¶é¢‘åŸŸçŸ©é˜µç§©åˆ†æï¼Œæ·±å…¥æ´å¯Ÿæ¨¡å‹æ€§èƒ½ï¼Œæ­ç¤ºä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºä¸»è¦ä¾èµ–æ¨¡æ‹Ÿçš„è¿œè·ç¦»å«æ··å“è¯­éŸ³å’Œå¹²å‡€è¯­éŸ³çš„é…å¯¹è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒï¼Œä½†æ¨¡å‹åœ¨çœŸå®å½•éŸ³æ··åˆç‰©ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ç ”ç©¶è€…å¼€å§‹æ¢ç´¢ç›´æ¥åœ¨çœŸå®æ··åˆç‰©ä¸Šè®­ç»ƒè¯­éŸ³å¢å¼ºæ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>FNSE-SBGANæ¡†æ¶ç»“åˆäº†Schrodinger Bridgeæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œå®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶æ˜¾è‘—é™ä½äº†å­—ç¬¦é”™è¯¯ç‡ï¼Œè¾¾åˆ°æœ€é«˜è¾¾14.58%çš„é™ä½ã€‚</li>
<li>FNSE-SBGANä¿ç•™äº†å‡ºè‰²çš„ä¸»è§‚éŸ³è´¨ï¼Œä¸ºçœŸå®ä¸–ç•Œçš„è¿œè·ç¦»è¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</li>
<li>å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨æ—¶é¢‘åŸŸçŸ©é˜µç§©åˆ†æï¼Œæä¾›äº†å¯¹æ¨¡å‹æ€§èƒ½çš„æ·±å…¥ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-110ffbb6afae5560bd36e2fcc0a27d66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28509a8d0e22b7bef313698b352f01c1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Lifelong-Reinforcement-Learning-with-Similarity-Driven-Weighting-by-Large-Models"><a href="#Lifelong-Reinforcement-Learning-with-Similarity-Driven-Weighting-by-Large-Models" class="headerlink" title="Lifelong Reinforcement Learning with Similarity-Driven Weighting by   Large Models"></a>Lifelong Reinforcement Learning with Similarity-Driven Weighting by   Large Models</h2><p><strong>Authors:Zhiyi Huang, Xiaohan Shan, Jianmin Li</strong></p>
<p>Lifelong Reinforcement Learning (LRL) holds significant potential for addressing sequential tasks, but it still faces considerable challenges. A key difficulty lies in effectively preventing catastrophic forgetting and facilitating knowledge transfer while maintaining reliable decision-making performance across subsequent tasks in dynamic environments. To tackle this, we propose a novel framework, SDW (Similarity-Driven Weighting Framework), which leverages large-language-model-generated dynamic functions to precisely control the training process. The core of SDW lies in two functions pre-generated by large models: the task similarity function and the weight computation function. The task similarity function extracts multidimensional features from task descriptions to quantify the similarities and differences between tasks in terms of states, actions, and rewards. The weight computation function dynamically generates critical training parameters based on the similarity information, including the proportion of old task data stored in the Replay Buffer and the strategy consistency weight in the loss function, enabling an adaptive balance between learning new tasks and transferring knowledge from previous tasks. By generating function code offline prior to training, rather than relying on large-model inference during the training process, the SDW framework reduces computational overhead while maintaining efficiency in sequential task scenarios. Experimental results on Atari and MiniHack sequential tasks demonstrate that SDW significantly outperforms existing lifelong reinforcement learning methods. </p>
<blockquote>
<p>ç»ˆèº«å¼ºåŒ–å­¦ä¹ ï¼ˆLRLï¼‰åœ¨å¤„ç†åºåˆ—ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ä¸»è¦å›°éš¾åœ¨äºå¦‚ä½•åœ¨åŠ¨æ€ç¯å¢ƒä¸­æœ‰æ•ˆé˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œä¿ƒè¿›çŸ¥è¯†è¿ç§»ï¼ŒåŒæ—¶ä¿æŒåç»­ä»»åŠ¡çš„å¯é å†³ç­–æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶SDWï¼ˆSimilarity-Driven Weighting Frameworkï¼‰ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åŠ¨æ€å‡½æ•°æ¥ç²¾ç¡®æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ã€‚SDWçš„æ ¸å¿ƒåœ¨äºç”±å¤§å‹æ¨¡å‹é¢„å…ˆç”Ÿæˆçš„ä¸¤ä¸ªå‡½æ•°ï¼šä»»åŠ¡ç›¸ä¼¼åº¦å‡½æ•°å’Œæƒé‡è®¡ç®—å‡½æ•°ã€‚ä»»åŠ¡ç›¸ä¼¼åº¦å‡½æ•°ä»ä»»åŠ¡æè¿°ä¸­æå–å¤šç»´ç‰¹å¾ï¼Œä»¥é‡åŒ–ä»»åŠ¡åœ¨çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±æ–¹é¢çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚ã€‚æƒé‡è®¡ç®—å‡½æ•°æ ¹æ®ç›¸ä¼¼åº¦ä¿¡æ¯åŠ¨æ€ç”Ÿæˆå…³é”®è®­ç»ƒå‚æ•°ï¼ŒåŒ…æ‹¬å­˜å‚¨åœ¨å›æ”¾ç¼“å†²åŒºä¸­çš„æ—§ä»»åŠ¡æ•°æ®æ¯”ä¾‹å’ŒæŸå¤±å‡½æ•°ä¸­çš„ç­–ç•¥ä¸€è‡´æ€§æƒé‡ï¼Œä»è€Œåœ¨å­¦ä¹ æ–°ä»»åŠ¡å’Œä»ä»¥å‰çš„ä»»åŠ¡ä¸­è¿ç§»çŸ¥è¯†ä¹‹é—´å®ç°è‡ªé€‚åº”å¹³è¡¡ã€‚SDWæ¡†æ¶åœ¨è®­ç»ƒå‰ç¦»çº¿ç”Ÿæˆå‡½æ•°ä»£ç ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾èµ–å¤§å‹æ¨¡å‹çš„æ¨ç†ï¼Œä»è€Œåœ¨ä¿æŒåºåˆ—ä»»åŠ¡åœºæ™¯æ•ˆç‡çš„åŒæ—¶å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚åœ¨Atariå’ŒMiniHackåºåˆ—ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSDWæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç»ˆèº«å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12923v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç»ˆèº«å¼ºåŒ–å­¦ä¹ ï¼ˆLRLï¼‰åœ¨å¤„ç†åºåˆ—ä»»åŠ¡æ—¶çš„æ½œåŠ›ä¸æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SDWï¼ˆSimilarity-Driven Weighting Frameworkï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åŠ¨æ€å‡½æ•°ç²¾ç¡®æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ï¼Œé€šè¿‡ä»»åŠ¡ç›¸ä¼¼åº¦å‡½æ•°å’Œæƒé‡è®¡ç®—å‡½æ•°ä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼Œå®ç°ä»»åŠ¡é—´çš„ç›¸ä¼¼åº¦é‡åŒ–åŠè®­ç»ƒå‚æ•°çš„åŠ¨æ€è°ƒæ•´ã€‚SDWæ¡†æ¶åœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œä¿æŒäº†å¤„ç†åºåˆ—ä»»åŠ¡æ—¶çš„æ•ˆç‡ï¼Œå¹¶åœ¨Atariå’ŒMiniHackçš„åºåˆ—ä»»åŠ¡å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»ˆèº«å¼ºåŒ–å­¦ä¹ ï¼ˆLRLï¼‰åœ¨å¤„ç†åºåˆ—ä»»åŠ¡æ—¶é¢ä¸´é˜²æ­¢ç¾éš¾æ€§é—å¿˜å’Œä¿ƒè¿›çŸ¥è¯†è¿ç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>SDWæ¡†æ¶é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åŠ¨æ€å‡½æ•°ç²¾ç¡®æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>ä»»åŠ¡ç›¸ä¼¼åº¦å‡½æ•°ä»ä»»åŠ¡æè¿°ä¸­æå–å¤šç»´ç‰¹å¾ï¼Œé‡åŒ–ä»»åŠ¡é—´çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚ã€‚</li>
<li>æƒé‡è®¡ç®—å‡½æ•°æ ¹æ®ç›¸ä¼¼åº¦ä¿¡æ¯åŠ¨æ€ç”Ÿæˆå…³é”®è®­ç»ƒå‚æ•°ã€‚</li>
<li>SDWæ¡†æ¶åœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œä¿æŒäº†å¤„ç†åºåˆ—ä»»åŠ¡æ—¶çš„æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSDWæ¡†æ¶åœ¨Atariå’ŒMiniHackçš„åºåˆ—ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç»ˆèº«å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f493a8016902af72cafdceef970d74e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74d1fd77b62c4a27579b846c1939977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a0484c41f468d54ea6df633a8248ddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-431c96b6d97ee13df292b19caffc357e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5806ef0150b6f956cb125ba6166d6989.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeepPerception-Advancing-R1-like-Cognitive-Visual-Perception-in-MLLMs-for-Knowledge-Intensive-Visual-Grounding"><a href="#DeepPerception-Advancing-R1-like-Cognitive-Visual-Perception-in-MLLMs-for-Knowledge-Intensive-Visual-Grounding" class="headerlink" title="DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs   for Knowledge-Intensive Visual Grounding"></a>DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs   for Knowledge-Intensive Visual Grounding</h2><p><strong>Authors:Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun</strong></p>
<p>Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08% accuracy improvements on KVG-Bench and exhibiting +4.60% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at <a target="_blank" rel="noopener" href="https://github.com/thunlp/DeepPerception">https://github.com/thunlp/DeepPerception</a>. </p>
<blockquote>
<p>äººç±»ä¸“å®¶æ“…é•¿é€šè¿‡åˆ©ç”¨é¢†åŸŸçŸ¥è¯†æ¥ç»†åŒ–æ„ŸçŸ¥ç‰¹å¾ï¼Œè¿›è¡Œç²¾ç»†åŒ–çš„è§†è§‰è¾¨åˆ«ï¼Œè¿™æ˜¯å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ä»æ¬ å‘å±•çš„èƒ½åŠ›ã€‚å°½ç®¡æ‹¥æœ‰å¤§é‡çš„ä¸“å®¶çº§çŸ¥è¯†ï¼ŒMLLMsåœ¨å°†æ¨ç†èå…¥è§†è§‰æ„ŸçŸ¥æ—¶ä»æ„Ÿåˆ°å›°éš¾ï¼Œç»å¸¸äº§ç”Ÿç›´æ¥çš„å›åº”ï¼Œè€Œæ²¡æœ‰è¿›è¡Œæ›´æ·±å…¥çš„åˆ†æã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŸ¥è¯†å¯†é›†å‹è§†è§‰å®šä½ï¼ˆKVGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦ç²¾ç»†æ„ŸçŸ¥å’Œç‰¹å®šé¢†åŸŸçŸ¥è¯†æ•´åˆçš„æ–°å‹è§†è§‰å®šä½ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹KVGçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepPerceptionï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºå‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰è®¤çŸ¥è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆç®¡é“ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ã€ä¸çŸ¥è¯†å¯¹é½çš„è®­ç»ƒæ ·æœ¬ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒç”¨äºè®¤çŸ¥æ¨ç†çš„æ¶æ„å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ„ŸçŸ¥ä¸è®¤çŸ¥ååŒä½œç”¨ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†KVG-Benchæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–10ä¸ªé¢†åŸŸçš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«1300ä¸ªæ‰‹åŠ¨æ•´ç†çš„æµ‹è¯•æ¡ˆä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepPerceptionæ˜¾è‘—ä¼˜äºç›´æ¥å¾®è°ƒæ–¹æ³•ï¼Œåœ¨KVG-Benchä¸Šå‡†ç¡®ç‡æé«˜8.08%ï¼Œåœ¨è·¨åŸŸæ¦‚æ‹¬æ–¹é¢æ¯”åŸºçº¿æ–¹æ³•é«˜å‡º4.60%ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å°†è®¤çŸ¥è¿‡ç¨‹èå…¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ï¼Œä¸ºç±»äººè§†è§‰æ„ŸçŸ¥å’Œå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚æ•°æ®ã€ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/thunlp/DeepPerception%E3%80%82">https://github.com/thunlp/DeepPerceptionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12797v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†ç²’åº¦è§†è§‰è¾¨è¯†å’Œé¢†åŸŸçŸ¥è¯†æ•´åˆæ–¹é¢çš„èƒ½åŠ›æ¬ ç¼ºã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæå‡ºäº†çŸ¥è¯†å¯†é›†å‹è§†è§‰å®šä½ï¼ˆKVGï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†DeepPerceptionæ¨¡å‹ä»¥å¢å¼ºå…¶è®¤çŸ¥è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“å’Œä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ¨¡å‹å®ç°äº†åœ¨KVGä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepPerceptionåœ¨KVG-Benchæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†8.08%ï¼Œå¹¶ä¸”åœ¨è·¨åŸŸæ³›åŒ–æ–¹é¢ä¹Ÿè¡¨ç°å‡ºä¼˜äºåŸºå‡†æ–¹æ³•çš„èƒ½åŠ›ã€‚ç ”ç©¶å¼ºè°ƒäº†å°†è®¤çŸ¥è¿‡ç¨‹èå…¥MLLMsçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æŒ‡æ˜äº†æ–°æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†ç²’åº¦è¾¨è¯†å’Œé¢†åŸŸçŸ¥è¯†æ•´åˆæ–¹é¢ã€‚</li>
<li>çŸ¥è¯†å¯†é›†å‹è§†è§‰å®šä½ï¼ˆKVGï¼‰æ˜¯ä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡ç²¾ç»†ç²’åº¦æ„ŸçŸ¥å’Œé¢†åŸŸçŸ¥è¯†æ•´åˆèƒ½åŠ›ã€‚</li>
<li>DeepPerceptionæ¨¡å‹é€šè¿‡å¼•å…¥è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“ï¼Œç”Ÿæˆé«˜è´¨é‡çš„çŸ¥è¯†å¯¹é½è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒè¿›è¡Œè®¤çŸ¥æ¨ç†è„šæ‰‹æ¶çš„æ„å»ºå’Œå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ„ŸçŸ¥ä¸è®¤çŸ¥çš„ååŒã€‚</li>
<li>DeepPerceptionåœ¨KVG-Benchæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”å±•ç°å‡ºä¼˜äºåŸºå‡†æ–¹æ³•çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å°†è®¤çŸ¥è¿‡ç¨‹èå…¥MLLMsçš„é‡è¦æ€§ï¼Œä»¥å®ç°æ›´äººæ€§åŒ–çš„è§†è§‰æ„ŸçŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e12eff23d92142f13e2a7e552cb79b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0abf3d2f81927ca777a7ff2ed0fd6f0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e85c3bb8522f0b769e3e764b1ed0306.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46ee98e74615c9b0c1f9af86a04707dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc816bae649f7f0a3f6f6c28a9408352.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RAG-RL-Advancing-Retrieval-Augmented-Generation-via-RL-and-Curriculum-Learning"><a href="#RAG-RL-Advancing-Retrieval-Augmented-Generation-via-RL-and-Curriculum-Learning" class="headerlink" title="RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum   Learning"></a>RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum   Learning</h2><p><strong>Authors:Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Julia Hockenmaier, Tong Zhang</strong></p>
<p>Recent research highlights the challenges retrieval models face in retrieving useful contexts and the limitations of generation models in effectively utilizing those contexts in retrieval-augmented generation (RAG) settings. To address these challenges, we introduce RAG-RL, the first reasoning language model (RLM) specifically trained for RAG. RAG-RL demonstrates that stronger answer generation models can identify relevant contexts within larger sets of retrieved information â€“ thereby alleviating the burden on retrievers â€“ while also being able to utilize those contexts more effectively. Moreover, we show that curriculum design in the reinforcement learning (RL) post-training process is a powerful approach to enhancing model performance. We benchmark our method on two open-domain question-answering datasets and achieve state-of-the-art results, surpassing previous SOTA generative reader models. In addition, we offers empirical insights into various curriculum learning strategies, providing a deeper understanding of their impact on model performance. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒäº†æ£€ç´¢æ¨¡å‹åœ¨æ£€ç´¢æœ‰ç”¨ä¸Šä¸‹æ–‡æ—¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä»¥åŠç”Ÿæˆæ¨¡å‹åœ¨å¢å¼ºæ£€ç´¢ï¼ˆRAGï¼‰è®¾ç½®ä¸­æœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¸Šä¸‹æ–‡æ—¶çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RAG-RLï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºRAGè®­ç»ƒçš„é¦–ä¸ªæ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMï¼‰ã€‚RAG-RLè¯æ˜ï¼Œæ›´å¼ºå¤§çš„ç­”æ¡ˆç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿåœ¨æ£€ç´¢çš„å¤§é‡ä¿¡æ¯ä¸­è¯†åˆ«å‡ºç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œå‡è½»æ£€ç´¢å™¨çš„è´Ÿæ‹…ï¼ŒåŒæ—¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè¯¾ç¨‹è®¾è®¡çš„å¢å¼ºæ¨¡å‹æ€§èƒ½çš„å¼ºå¤§æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³ç”Ÿæˆå¼é˜…è¯»æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å…³äºå„ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„å®è¯è§è§£ï¼Œä¸ºæ·±å…¥äº†è§£å®ƒä»¬å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æä¾›äº†æ›´æ·±çš„äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12759v1">PDF</a> 11 Pages, 3 Figures, Preprint</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸç ”ç©¶å‘ç°æ£€ç´¢æ¨¡å‹åœ¨è·å–æœ‰ç”¨ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç”Ÿæˆæ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è®¾ç½®ä¸­æœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¸Šä¸‹æ–‡ä¹Ÿæœ‰é™åˆ¶ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“ä¸ºRAGè®¾è®¡çš„æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMï¼‰RAG-RLã€‚RAG-RLè¡¨æ˜ï¼Œæ›´å¼ºå¤§çš„ç­”æ¡ˆç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿåœ¨æ£€ç´¢åˆ°çš„æ›´å¤§ä¿¡æ¯é›†ä¸­è¯†åˆ«ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä»è€Œå‡è½»æ£€ç´¢å™¨çš„è´Ÿæ‹…ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè¯¾ç¨‹è®¾è®¡çš„å¢å¼ºæ¨¡å‹æ€§èƒ½çš„å¼ºå¤§æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå–å¾—äº†è¶…è¶Šä¹‹å‰æœ€ä½³ç”Ÿæˆé˜…è¯»æ¨¡å‹çš„æœ€æ–°ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å…³äºå„ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„ç»éªŒè§è§£ï¼Œä»¥æ·±å…¥äº†è§£å®ƒä»¬å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ£€ç´¢æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹åœ¨RAGè®¾ç½®ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³ã€‚</li>
<li>RAG-RLæ˜¯é¦–ä¸ªä¸“ä¸ºRAGè®¾è®¡çš„æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMï¼‰ï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«ç›¸å…³ä¸Šä¸‹æ–‡å¹¶å‡è½»æ£€ç´¢å™¨çš„è´Ÿæ‹…ã€‚</li>
<li>RAG-RLåœ¨åè®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè¯¾ç¨‹è®¾è®¡ï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>RAG-RLåœ¨å¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³ç”Ÿæˆé˜…è¯»æ¨¡å‹ã€‚</li>
<li>RAG-RLæä¾›äº†å…³äºè¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„ç»éªŒè§è§£ã€‚</li>
<li>è¯¾ç¨‹è®¾è®¡ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½å…·æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-359497c66967a5978030d6aee40e27b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6f98dde5e6d68c9a41127a524349f44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a9632c38840b0daf8ad05100299856a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a820573b7911e6dc8aad73c2cfbcd00f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a9e3b419bbe07a4c9be3d13edc6de7f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AI-Agents-Evolution-Architecture-and-Real-World-Applications"><a href="#AI-Agents-Evolution-Architecture-and-Real-World-Applications" class="headerlink" title="AI Agents: Evolution, Architecture, and Real-World Applications"></a>AI Agents: Evolution, Architecture, and Real-World Applications</h2><p><strong>Authors:Naveen Krishnan</strong></p>
<p>This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ä»£ç†äººçš„å‘å±•æ¼”å˜ã€æ¶æ„å’Œå®é™…åº”ç”¨ï¼Œä»æ—©æœŸçš„åŸºäºè§„åˆ™çš„å½¢å¼åˆ°ç°ä»£å¤æ‚ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ç”¨äºæ„ŸçŸ¥ã€è§„åˆ’å’Œå·¥å…·ä½¿ç”¨çš„ä¸“ç”¨æ¨¡å—ç›¸ç»“åˆã€‚æœ¬æ–‡æ—¢å¼ºè°ƒç†è®ºåŸºç¡€ï¼Œåˆå…³æ³¨ç°å®åº”ç”¨ï¼Œå›é¡¾äº†å…³é”®ä»£ç†èŒƒå¼ï¼Œè®¨è®ºäº†å½“å‰è¯„ä¼°åŸºå‡†çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¹³è¡¡äº†ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€ç¨³å¥æ€§å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡åˆ†æäº†åœ¨ä¼ä¸šã€ä¸ªäººåŠ©ç†å’Œç‰¹æ®Šé¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨å»ºç«‹æ›´å…·å¼¹æ€§å’Œé€‚åº”æ€§çš„äººå·¥æ™ºèƒ½ä»£ç†ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12687v1">PDF</a> 52 pages, 4 figures, comprehensive survey and analysis of AI agent   evolution, architecture, evaluation frameworks, and applications</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†äººå·¥æ™ºèƒ½ä»£ç†äººçš„æ¼”å˜ã€æ¶æ„å’Œå®é™…åº”ç”¨ï¼Œä»æ—©æœŸçš„è§„åˆ™åŸºç¡€å½¢å¼åˆ°ç°ä»£é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€è§„åˆ’å’Œå·¥å…·ä½¿ç”¨ç­‰ä¸“ç”¨æ¨¡å—ã€‚æ–‡ç« å¼ºè°ƒç†è®ºåŸºç¡€å’Œå®é™…åº”ç”¨éƒ¨ç½²ï¼Œå›é¡¾äº†å…³é”®ä»£ç†èŒƒå¼ï¼Œè®¨è®ºäº†å½“å‰è¯„ä¼°åŸºå‡†çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¹³è¡¡äº†ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€ç¨³å¥æ€§å’Œå®‰å…¨æ€§ã€‚æ–‡ç« è¿˜åˆ†æäº†åœ¨ä¼ä¸šã€ä¸ªäººåŠ©ç†å’Œç‰¹æ®Šé¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æ´å¯Ÿäº†æœªæ¥æ›´å…·å¼¹æ€§å’Œé€‚åº”æ€§çš„äººå·¥æ™ºèƒ½ä»£ç†äººç³»ç»Ÿçš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººå·¥æ™ºèƒ½ä»£ç†äººçš„æ¼”å˜ï¼šä»æ—©æœŸçš„è§„åˆ™åŸºç¡€å½¢å¼åˆ°ç°ä»£é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚ç³»ç»Ÿã€‚</li>
<li>äººå·¥æ™ºèƒ½ä»£ç†äººçš„æ¶æ„ï¼šåŒ…æ‹¬æ„ŸçŸ¥ã€è§„åˆ’å’Œå·¥å…·ä½¿ç”¨ç­‰æ¨¡å—ã€‚</li>
<li>äººå·¥æ™ºèƒ½ä»£ç†äººçš„å®é™…åº”ç”¨ï¼šæ¶‰åŠä¼ä¸šã€ä¸ªäººåŠ©ç†å’Œç‰¹æ®Šé¢†åŸŸã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäººå·¥æ™ºèƒ½ä»£ç†äººçš„ç†è®ºåŸºç¡€å’Œå®é™…åº”ç”¨éƒ¨ç½²ã€‚</li>
<li>å½“å‰è¯„ä¼°åŸºå‡†çš„å±€é™æ€§åŠå…¨é¢è¯„ä¼°æ¡†æ¶çš„ä»‹ç»ï¼ŒåŒ…æ‹¬ä»»åŠ¡æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€ç¨³å¥æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>è®ºæ–‡å¯¹å…³é”®ä»£ç†èŒƒå¼çš„å›é¡¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60ad82153afb709a5bcaf394396f51e5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Formally-Reason-as-Abstract-Interpreters-for-Program-Analysis"><a href="#Can-LLMs-Formally-Reason-as-Abstract-Interpreters-for-Program-Analysis" class="headerlink" title="Can LLMs Formally Reason as Abstract Interpreters for Program Analysis?"></a>Can LLMs Formally Reason as Abstract Interpreters for Program Analysis?</h2><p><strong>Authors:Jacqueline L. Mitchell, Brian Hyeongseok Kim, Chenyu Zhou, Chao Wang</strong></p>
<p>LLMs have demonstrated impressive capabilities in code generation and comprehension, but their potential in being able to perform program analysis in a formal, automatic manner remains under-explored. To that end, we systematically investigate whether LLMs can reason about programs using a program analysis framework called abstract interpretation. We prompt LLMs to follow two different strategies, denoted as Compositional and Fixed Point Equation, to formally reason in the style of abstract interpretation, which has never been done before to the best of our knowledge. We validate our approach using state-of-the-art LLMs on 22 challenging benchmark programs from the Software Verification Competition (SV-COMP) 2019 dataset, widely used in program analysis. Our results show that our strategies are able to elicit abstract interpretation-based reasoning in the tested models, but LLMs are susceptible to logical errors, especially while interpreting complex program structures, as well as general hallucinations. This highlights key areas for improvement in the formal reasoning capabilities of LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆå’Œç†è§£æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ­£å¼ã€è‡ªåŠ¨æ‰§è¡Œç¨‹åºåˆ†ææ–¹é¢çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†LLMsæ˜¯å¦å¯ä»¥ä½¿ç”¨åä¸ºæŠ½è±¡è§£é‡Šçš„ç¨‹åºåˆ†ææ¡†æ¶æ¥æ¨ç†ç¨‹åºã€‚æˆ‘ä»¬æç¤ºLLMsé‡‡ç”¨ä¸¤ç§ä¸åŒç­–ç•¥ï¼Œå³ç»„åˆç­–ç•¥å’Œä¸åŠ¨ç‚¹æ–¹ç¨‹ç­–ç•¥ï¼Œä»¥æŠ½è±¡è§£é‡Šçš„æ–¹å¼è¿›è¡Œæ­£å¼æ¨ç†ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯å‰æ‰€æœªæœ‰çš„ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„LLMséªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæµ‹è¯•äº†æ¥è‡ªè½¯ä»¶éªŒè¯ç«èµ›ï¼ˆSV-COMPï¼‰2019æ•°æ®é›†çš„22ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ç¨‹åºï¼Œå¹¿æ³›åº”ç”¨äºç¨‹åºåˆ†æé¢†åŸŸã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç­–ç•¥èƒ½å¤Ÿåœ¨æµ‹è¯•æ¨¡å‹ä¸­æ¿€å‘åŸºäºæŠ½è±¡è§£é‡Šçš„æ¨ç†ï¼Œä½†LLMså®¹æ˜“çŠ¯é€»è¾‘é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£é‡Šå¤æ‚çš„ç¨‹åºç»“æ„æ—¶ï¼Œä»¥åŠå‡ºç°ä¸€èˆ¬æ€§çš„å¹»è§‰ã€‚è¿™çªå‡ºäº†æé«˜LLMså½¢å¼æ¨ç†èƒ½åŠ›çš„é‡è¦æ”¹è¿›é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆå’Œç†è§£æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å…¶ä»¥æ­£å¼ã€è‡ªåŠ¨çš„æ–¹å¼è¿›è¡Œç¨‹åºåˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†äº†è§£ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶LLMsæ˜¯å¦èƒ½åˆ©ç”¨æŠ½è±¡è§£é‡Šçš„ç¨‹åºåˆ†ææ¡†æ¶è¿›è¡Œç¨‹åºæ¨ç†ã€‚é€šè¿‡é‡‡ç”¨ç»„åˆç­–ç•¥å’Œä¸åŠ¨ç‚¹æ–¹ç¨‹ç­–ç•¥ï¼Œæœ¬æ–‡é¦–æ¬¡åˆ©ç”¨æŠ½è±¡è§£é‡Šé£æ ¼å¯¹LLMsè¿›è¡Œæ­£å¼æ¨ç†ç ”ç©¶ã€‚é€šè¿‡ä½¿ç”¨å‰æ²¿çš„LLMså’Œå¤§é‡æ¥è‡ªè½¯ä»¶éªŒè¯ç«èµ›ï¼ˆSV-COMPï¼‰2019æ•°æ®é›†çš„åŸºå‡†ç¨‹åºè¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¿™ä¸¤ç§ç­–ç•¥èƒ½å¤Ÿæ¿€å‘åŸºäºæŠ½è±¡è§£é‡Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†LLMsåœ¨å¤„ç†å¤æ‚ç¨‹åºç»“æ„å’Œé€»è¾‘é”™è¯¯æ—¶æ˜“å‡ºç°é”™è¯¯ï¼Œä»¥åŠå‡ºç°ä¸€èˆ¬æ€§çš„å¹»è§‰ã€‚è¿™ä¸ºæå‡LLMsçš„æ­£å¼æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„æ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„ä»£ç ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ç¨‹åºåˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å°è¯•åˆ©ç”¨æŠ½è±¡è§£é‡Šæ¡†æ¶æ¥ç ”ç©¶LLMsè¿›è¡Œç¨‹åºæ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç»„åˆç­–ç•¥å’Œä¸åŠ¨ç‚¹æ–¹ç¨‹ç­–ç•¥ï¼ŒLLMsèƒ½å¤Ÿè¡¨ç°å‡ºåŸºäºæŠ½è±¡è§£é‡Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤„ç†å¤æ‚ç¨‹åºç»“æ„å’Œé€»è¾‘é”™è¯¯æ—¶ï¼ŒLLMså®¹æ˜“å‡ºç°é”™è¯¯ã€‚</li>
<li>LLMsåœ¨ç¨‹åºåˆ†æä¸­çš„è¡¨ç°å­˜åœ¨æ”¹è¿›çš„ç©ºé—´å’Œå¿…è¦æ€§ã€‚</li>
<li>æŠ½è±¡è§£é‡Šæ¡†æ¶åœ¨æå‡LLMsçš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee1686fa63459da7f4bc2f55ada8bafc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39e66299437e42b26cf6641cc503448a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89cfaf4f010393dc07ce173038b196c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-671ca3e22401ee5798ee6139eed4fd8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a5471a7767cae5b5ddbacbcf977ade6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Focusing-Robot-Open-Ended-Reinforcement-Learning-Through-Usersâ€™-Purposes"><a href="#Focusing-Robot-Open-Ended-Reinforcement-Learning-Through-Usersâ€™-Purposes" class="headerlink" title="Focusing Robot Open-Ended Reinforcement Learning Through Usersâ€™ Purposes"></a>Focusing Robot Open-Ended Reinforcement Learning Through Usersâ€™ Purposes</h2><p><strong>Authors:Emilio Cartoni, Gianluca Cioccolini, Gianluca Baldassarre</strong></p>
<p>Open-Ended Learning (OEL) autonomous robots can acquire new skills and knowledge through direct interaction with their environment, relying on mechanisms such as intrinsic motivations and self-generated goals to guide learning processes. OEL robots are highly relevant for applications as they can autonomously leverage acquired knowledge to perform tasks beneficial to human users in unstructured environments, addressing challenges unforeseen at design time. However, OEL robots face a significant limitation: their openness may lead them to waste time learning information that is irrelevant to tasks desired by specific users. Here, we propose a solution called <code>Purpose-Directed Open-Ended Learning&#39; (POEL), based on the novel concept of </code>purposeâ€™ introduced in previous work. A purpose specifies what users want the robot to achieve. The key insight of this work is that purpose can focus OEL on learning self-generated classes of tasks that, while unknown during autonomous learning (as typical in OEL), involve objects relevant to the purpose. This concept is operationalised in a novel robot architecture capable of receiving a human purpose through speech-to-text, analysing the scene to identify objects, and using a Large Language Model to reason about which objects are purpose-relevant. These objects are then used to bias OEL exploration towards their spatial proximity and to self-generate rewards that favour interactions with them. The solution is tested in a simulated scenario where a camera-arm-gripper robot interacts freely with purpose-related and distractor objects. For the first time, the results demonstrate the potential advantages of purpose-focused OEL over state-of-the-art OEL methods, enabling robots to handle unstructured environments while steering their learning toward knowledge acquisition relevant to users. </p>
<blockquote>
<p>å¼€æ”¾å¼å­¦ä¹ ï¼ˆOELï¼‰è‡ªä¸»æœºå™¨äººèƒ½å¤Ÿé€šè¿‡ä¸å…¶ç¯å¢ƒçš„ç›´æ¥äº’åŠ¨è·å¾—æ–°çš„æŠ€èƒ½å’ŒçŸ¥è¯†ï¼Œä¾èµ–äºå†…åœ¨åŠ¨åŠ›å’Œè‡ªæˆ‘ç”Ÿæˆç›®æ ‡ç­‰æœºåˆ¶æ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚OELæœºå™¨äººåœ¨åº”ç”¨ä¸Šå…·æœ‰å¾ˆé«˜çš„ç°å®æ„ä¹‰ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥è‡ªä¸»åœ°åˆ©ç”¨æ‰€è·å¾—çš„çŸ¥è¯†ï¼Œåœ¨ç»“æ„ä¸è‰¯çš„ç¯å¢ƒä¸­æ‰§è¡Œå¯¹äººç±»ç”¨æˆ·æœ‰ç›Šçš„ä»»åŠ¡ï¼Œåº”å¯¹è®¾è®¡æ—¶æ— æ³•é¢„è§åˆ°çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒOELæœºå™¨äººé¢ä¸´ä¸€ä¸ªé‡å¤§å±€é™ï¼šå®ƒä»¬çš„å¼€æ”¾æ€§å¯èƒ½å¯¼è‡´å®ƒä»¬æµªè´¹æ—¶é—´å­¦ä¹ ç‰¹å®šç”¨æˆ·ä¸éœ€è¦çš„ä»»åŠ¡çš„ç›¸å…³ä¿¡æ¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œç›®çš„å¯¼å‘çš„å¼€æ”¾å¼å­¦ä¹ â€ï¼ˆPOELï¼‰çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯åŸºäºå…ˆå‰å·¥ä½œä¸­å¼•å…¥çš„â€œç›®çš„â€è¿™ä¸€æ–°æ¦‚å¿µã€‚ç›®çš„æŒ‡å®šäº†ç”¨æˆ·å¸Œæœ›æœºå™¨äººå®ç°çš„ç›®æ ‡ã€‚è¿™é¡¹å·¥ä½œçš„å…³é”®è§è§£æ˜¯ï¼Œç›®çš„å¯ä»¥å¼•å¯¼OELå­¦ä¹ è‡ªæˆ‘ç”Ÿæˆçš„ç±»åˆ«ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è™½ç„¶åœ¨è‡ªä¸»å­¦ä¹ ä¸­æœªçŸ¥ï¼ˆå¦‚å…¸å‹çš„OELï¼‰ï¼Œä½†æ¶‰åŠä¸ç›®çš„ç›¸å…³çš„å¯¹è±¡ã€‚è¿™ä¸ªæ¦‚å¿µåœ¨ä¸€ä¸ªèƒ½å¤Ÿæ¥æ”¶äººç±»ç›®çš„å¹¶é€šè¿‡è¯­éŸ³åˆ°æ–‡æœ¬çš„æ–°å‹æœºå™¨äººæ¶æ„ä¸­å¾—ä»¥å®æ–½ã€‚è¯¥æ¶æ„èƒ½å¤Ÿåˆ†æåœºæ™¯ä»¥è¯†åˆ«ç‰©ä½“ï¼Œå¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ¨æ–­å“ªäº›ç‰©ä½“ä¸ç›®çš„ç›¸å…³ã€‚è¿™äº›ç‰©ä½“éšåè¢«ç”¨æ¥åå‘OELæ¢ç´¢å®ƒä»¬çš„ç©ºé—´é‚»è¿‘æ€§å¹¶è‡ªæˆ‘ç”Ÿæˆå¥–åŠ±ï¼Œä»¥é¼“åŠ±ä¸å®ƒä»¬çš„äº’åŠ¨ã€‚è¯¥è§£å†³æ–¹æ¡ˆåœ¨ä¸€ä¸ªæ¨¡æ‹Ÿåœºæ™¯ä¸­è¿›è¡Œäº†æµ‹è¯•ï¼Œå…¶ä¸­æ‘„åƒæœºæ‰‹è‡‚æŠ“æ‰‹æœºå™¨äººå¯ä»¥è‡ªç”±åœ°ä¸ä¸ç›®çš„ç›¸å…³å’Œæ— å…³çš„å¯¹è±¡è¿›è¡Œäº’åŠ¨ã€‚ç»“æœé¦–æ¬¡å±•ç¤ºäº†æœ‰ç›®çš„å¯¼å‘çš„OELç›¸å¯¹äºæœ€å…ˆè¿›çš„OELæ–¹æ³•çš„æ½œåœ¨ä¼˜åŠ¿ï¼Œä½¿æœºå™¨äººåœ¨å¤„ç†ç»“æ„ä¸è‰¯ç¯å¢ƒæ—¶èƒ½å¤Ÿå¼•å¯¼å…¶å­¦ä¹ ï¼Œä»è€Œè·å–ä¸ç”¨æˆ·ç›¸å…³çš„çŸ¥è¯†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12579v1">PDF</a> 4 pages, 2 figures, accepted at RLDM 2025</p>
<p><strong>Summary</strong><br>     å¼€æ”¾å­¦ä¹ ï¼ˆOELï¼‰è‡ªä¸»æœºå™¨äººå¯é€šè¿‡ä¸ç¯å¢ƒç›´æ¥äº’åŠ¨è·å–æ–°æŠ€èƒ½ä¸çŸ¥è¯†ï¼Œä¾èµ–å†…åœ¨åŠ¨æœºå’Œè‡ªæˆ‘ç”Ÿæˆç›®æ ‡ç­‰æœºåˆ¶æ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚å¯¹äºåº”ç”¨è€Œè¨€ï¼ŒOELæœºå™¨äººé«˜åº¦ç›¸å…³ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥è‡ªä¸»åˆ©ç”¨è·å–çš„çŸ¥è¯†åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­æ‰§è¡Œå¯¹äººç±»ç”¨æˆ·æœ‰ç›Šçš„ä»»åŠ¡ï¼Œè§£å†³è®¾è®¡æ—¶æ— æ³•é¢„è§åˆ°çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒOELæœºå™¨äººé¢ä¸´ä¸€ä¸ªé‡å¤§å±€é™ï¼šå…¶å¼€æ”¾æ€§å¯èƒ½å¯¼è‡´å®ƒä»¬æµªè´¹æ—¶é—´å­¦ä¹ å¯¹ç‰¹å®šç”¨æˆ·ä»»åŠ¡æ— å…³çš„ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå…ˆå‰å·¥ä½œä¸­å¼•å…¥çš„â€œç›®çš„â€æ¦‚å¿µçš„è§£å†³æ–¹æ¡ˆï¼Œå³ç›®çš„å¯¼å‘å¼€æ”¾å­¦ä¹ ï¼ˆPOELï¼‰ã€‚ç›®çš„æŒ‡å®šäº†ç”¨æˆ·å¸Œæœ›æœºå™¨äººå®ç°çš„ç›®æ ‡ã€‚å…³é”®æ´å¯ŸåŠ›åœ¨äºï¼Œç›®çš„å¯ä»¥å¼•å¯¼OELä¸“æ³¨äºå­¦ä¹ è‡ªæˆ‘ç”Ÿæˆçš„ä¸ç›®çš„ç›¸å…³çš„ä»»åŠ¡ç±»åˆ«ï¼Œè¿™äº›ä»»åŠ¡åœ¨è‡ªä¸»å­¦ä¹ ä¸­æ˜¯æœªçŸ¥çš„ï¼Œä½†æ¶‰åŠä¸ç›®çš„ç›¸å…³çš„å¯¹è±¡ã€‚æˆ‘ä»¬åœ¨æ–°å‹æœºå™¨äººæ¶æ„ä¸­å®ç°äº†è¿™ä¸€æ¦‚å¿µï¼Œè¯¥æ¶æ„èƒ½å¤Ÿé€šè¿‡è¯­éŸ³åˆ°æ–‡æœ¬æ¥æ”¶äººç±»ç›®çš„ï¼Œåˆ†æåœºæ™¯ä»¥è¯†åˆ«å¯¹è±¡ï¼Œå¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ¨æ–­å“ªäº›å¯¹è±¡æ˜¯ç›®çš„ç›¸å…³çš„ã€‚è¿™äº›å¯¹è±¡ç„¶åç”¨äºåå‘OELæ¢ç´¢å…¶ç©ºé—´é‚»è¿‘æ€§å¹¶è‡ªæˆ‘ç”Ÿæˆå¥–åŠ±ï¼Œä»¥æœ‰åˆ©äºä¸å…¶çš„äº’åŠ¨ã€‚è§£å†³æ–¹æ¡ˆåœ¨æ¨¡æ‹Ÿåœºæ™¯ä¸­è¿›è¡Œäº†æµ‹è¯•ï¼Œå…¶ä¸­æ‘„åƒæœºæœºæ¢°è‡‚æŠ“å–æœºå™¨äººè‡ªç”±åœ°ä¸ç›®çš„ç›¸å…³ç‰©ä½“å’Œå¹²æ‰°ç‰©ä½“äº’åŠ¨ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°OELæ–¹æ³•ç›¸æ¯”ï¼Œä»¥ç›®çš„ä¸ºä¸­å¿ƒçš„OELå…·æœ‰æ½œåœ¨ä¼˜åŠ¿ï¼Œä½¿æœºå™¨äººåœ¨å¤„ç†éç»“æ„åŒ–ç¯å¢ƒæ—¶èƒ½å¤Ÿå°†å­¦ä¹ å¯¼å‘ä¸ç”¨æˆ·ç›¸å…³çš„çŸ¥è¯†è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾å­¦ä¹ ï¼ˆOELï¼‰è‡ªä¸»æœºå™¨äººèƒ½é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨æ¥ä¹ å¾—æ–°æŠ€èƒ½å’ŒçŸ¥è¯†ã€‚</li>
<li>OELæœºå™¨äººå…·æœ‰é«˜åº¦è‡ªä¸»æ€§ï¼Œå¯ä»¥åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ‰§è¡Œå¯¹äººç±»æœ‰ç›Šçš„ä»»åŠ¡ã€‚</li>
<li>OELæœºå™¨äººå¯èƒ½å› å¼€æ”¾æ€§è€Œå­¦ä¹ æ— å…³ä¿¡æ¯ï¼Œå¯¼è‡´æ—¶é—´æµªè´¹ã€‚</li>
<li>æå‡ºç›®çš„å¯¼å‘å¼€æ”¾å­¦ä¹ ï¼ˆPOELï¼‰è§£å†³æ–¹æ¡ˆï¼ŒåŸºäºç”¨æˆ·å®šä¹‰çš„ç›®æ ‡æ¥å¼•å¯¼æœºå™¨äººçš„å­¦ä¹ ã€‚</li>
<li>POELèƒ½å¤Ÿå¼•å¯¼æœºå™¨äººå­¦ä¹ è‡ªæˆ‘ç”Ÿæˆçš„ä¸ç›®çš„ç›¸å…³çš„ä»»åŠ¡ç±»åˆ«ã€‚</li>
<li>æ–°å‹æœºå™¨äººæ¶æ„é€šè¿‡è¯†åˆ«ç›®çš„ç›¸å…³ç‰©ä½“æ¥å®æ–½POELï¼Œå¹¶æ®æ­¤åå‘æ¢ç´¢å’Œç”Ÿæˆå¥–åŠ±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6fcb546645935f8552e1a7eb97637f27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ada3d4666d7c8aa0be46c151a980d84c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10173ff137b020bfff9fc1ceb52ec343.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-18/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae4aa0d01a00b084fbf0a3221e0f44c7.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  MetaScale Test-Time Scaling with Evolving Meta-Thoughts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be9edefd2530b672c1b1316199272d95.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-16  Quenching and recovery of persistent X-ray emission during a superburst   in 4U 1820$-$30
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13453.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
