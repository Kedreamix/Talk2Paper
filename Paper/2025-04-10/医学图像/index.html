<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  HRMedSeg Unlocking High-resolution Medical Image segmentation via   Memory-efficient Attention Modeling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7b9cbee8aede2cf6c6cba8321a8a4016.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-10-æ›´æ–°"><a href="#2025-04-10-æ›´æ–°" class="headerlink" title="2025-04-10 æ›´æ–°"></a>2025-04-10 æ›´æ–°</h1><h2 id="HRMedSeg-Unlocking-High-resolution-Medical-Image-segmentation-via-Memory-efficient-Attention-Modeling"><a href="#HRMedSeg-Unlocking-High-resolution-Medical-Image-segmentation-via-Memory-efficient-Attention-Modeling" class="headerlink" title="HRMedSeg: Unlocking High-resolution Medical Image segmentation via   Memory-efficient Attention Modeling"></a>HRMedSeg: Unlocking High-resolution Medical Image segmentation via   Memory-efficient Attention Modeling</h2><p><strong>Authors:Qing Xu, Zhenye Lou, Chenxin Li, Xiangjian He, Rong Qu, Tesema Fiseha Berhanu, Yi Wang, Wenting Duan, Zhen Chen</strong></p>
<p>High-resolution segmentation is critical for precise disease diagnosis by extracting micro-imaging information from medical images. Existing transformer-based encoder-decoder frameworks have demonstrated remarkable versatility and zero-shot performance in medical segmentation. While beneficial, they usually require huge memory costs when handling large-size segmentation mask predictions, which are expensive to apply to real-world scenarios. To address this limitation, we propose a memory-efficient framework for high-resolution medical image segmentation, called HRMedSeg. Specifically, we first devise a lightweight gated vision transformer (LGViT) as our image encoder to model long-range dependencies with linear complexity. Then, we design an efficient cross-multiscale decoder (ECM-Decoder) to generate high-resolution segmentation masks. Moreover, we utilize feature distillation during pretraining to unleash the potential of our proposed model. Extensive experiments reveal that HRMedSeg outperforms state-of-the-arts in diverse high-resolution medical image segmentation tasks. In particular, HRMedSeg uses only 0.59GB GPU memory per batch during fine-tuning, demonstrating low training costs. Besides, when HRMedSeg meets the Segment Anything Model (SAM), our HRMedSegSAM takes 0.61% parameters of SAM-H. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/HRMedSeg">https://github.com/xq141839/HRMedSeg</a>. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡åˆ†å‰²æ˜¯ä»åŒ»å­¦å›¾åƒä¸­æå–å¾®è§‚æˆåƒä¿¡æ¯ï¼Œè¿›è¡Œç²¾ç¡®ç–¾ç—…è¯Šæ–­çš„å…³é”®ã€‚ç°æœ‰çš„åŸºäºè½¬æ¢å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶åœ¨åŒ»å­¦åˆ†å‰²ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„é€šç”¨æ€§å’Œé›¶æ ·æœ¬æ€§èƒ½ã€‚è™½ç„¶æœ‰ç›Šï¼Œä½†åœ¨å¤„ç†å¤§å°ºå¯¸åˆ†å‰²æ©è†œé¢„æµ‹æ—¶ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å·¨å¤§çš„å†…å­˜æˆæœ¬ï¼Œéš¾ä»¥åº”ç”¨äºç°å®åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„å†…å­˜é«˜æ•ˆæ¡†æ¶ï¼Œç§°ä¸ºHRMedSegã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§è½»é‡å‹é—¨æ§è§†è§‰è½¬æ¢å™¨ï¼ˆLGViTï¼‰ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œä»¥çº¿æ€§å¤æ‚åº¦å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé«˜æ•ˆçš„è·¨å¤šå°ºåº¦è§£ç å™¨ï¼ˆECM-Decoderï¼‰æ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡åˆ†å‰²æ©è†œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç‰¹å¾è’¸é¦æ¥é‡Šæ”¾æˆ‘ä»¬æå‡ºæ¨¡å‹çš„æ½œåŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHRMedSegåœ¨å¤šç§é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯ï¼ŒHRMedSegåœ¨å¾®è°ƒæœŸé—´æ¯æ‰¹ä»…ä½¿ç”¨0.59GBçš„GPUå†…å­˜ï¼Œæ˜¾ç¤ºå‡ºè¾ƒä½çš„è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œå½“HRMedSegé‡åˆ°Segment Anything Modelï¼ˆSAMï¼‰æ—¶ï¼Œæˆ‘ä»¬çš„HRMedSegSAMä»…ä½¿ç”¨SAM-Hçš„0.61%å‚æ•°ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xq141839/HRMedSeg%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xq141839/HRMedSegä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06205v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>    æå‡ºä¸€ç§é«˜æ•ˆã€å†…å­˜ä¼˜åŒ–ã€ç”¨äºé«˜è§£æåº¦åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ¡†æ¶HRMedSegã€‚é‡‡ç”¨è½»é‡åŒ–é—¨æ§è§†è§‰è½¬æ¢å™¨LGViTå»ºæ¨¡é•¿æœŸä¾èµ–å…³ç³»ï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆè·¨å¤šå°ºåº¦è§£ç å™¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡åˆ†å‰²æ©è†œã€‚ç‰¹å¾è’¸é¦åœ¨é¢„è®­ç»ƒä¸­çš„åº”ç”¨æé«˜äº†æ¨¡å‹æ½œåŠ›ã€‚HRMedSegåœ¨é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨å¾®è°ƒæ—¶ä»…å ç”¨0.59GBçš„GPUå†…å­˜ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡åˆ†å‰²å¯¹äºç²¾ç¡®ç–¾ç—…è¯Šæ–­è‡³å…³é‡è¦ï¼Œéœ€è¦ä»åŒ»å­¦å›¾åƒä¸­æå–å¾®è§‚æˆåƒä¿¡æ¯ã€‚</li>
<li>ç°æœ‰åŸºäºè½¬æ¢å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å·²åœ¨åŒ»å­¦åˆ†å‰²ä¸­å±•ç°å‡ºå“è¶Šçš„å¤šåŠŸèƒ½æ€§å’Œé›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>è¿™äº›æ¡†æ¶åœ¨å¤„ç†å¤§å‹åˆ†å‰²æ©è†œé¢„æµ‹æ—¶å†…å­˜æˆæœ¬é«˜æ˜‚ï¼Œå®é™…åº”ç”¨ä¸­å—é™ã€‚</li>
<li>æå‡ºä¸€ç§å†…å­˜é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶HRMedSegï¼Œç”¨äºé«˜è§£æåº¦åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>HRMedSegé‡‡ç”¨è½»é‡åŒ–é—¨æ§è§†è§‰è½¬æ¢å™¨LGViTï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦çš„é•¿æœŸä¾èµ–å…³ç³»å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>HRMedSegè®¾è®¡äº†ä¸€ç§é«˜æ•ˆè·¨å¤šå°ºåº¦è§£ç å™¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡åˆ†å‰²æ©è†œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bfd15669571dbace2f06e4104acd8fe6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a400a61f11267c01b740ae27d44d179b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89b8a32613085d90045f18ee9adcf85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b6138cc7cbca7d4f53cd1714441d253.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25f19b4f45137c53b09a8b4e6bbd9823.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09cfbafa479fe8eebac5d41693264f88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0269ead2ccd0a316bb4dbdf10a8aa187.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b05418d13f6e7e8c0bc6677b49b6882.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rethinking-the-Nested-U-Net-Approach-Enhancing-Biomarker-Segmentation-with-Attention-Mechanisms-and-Multiscale-Feature-Fusion"><a href="#Rethinking-the-Nested-U-Net-Approach-Enhancing-Biomarker-Segmentation-with-Attention-Mechanisms-and-Multiscale-Feature-Fusion" class="headerlink" title="Rethinking the Nested U-Net Approach: Enhancing Biomarker Segmentation   with Attention Mechanisms and Multiscale Feature Fusion"></a>Rethinking the Nested U-Net Approach: Enhancing Biomarker Segmentation   with Attention Mechanisms and Multiscale Feature Fusion</h2><p><strong>Authors:Saad Wazir, Daeyoung Kim</strong></p>
<p>Identifying biomarkers in medical images is vital for a wide range of biotech applications. However, recent Transformer and CNN based methods often struggle with variations in morphology and staining, which limits their feature extraction capabilities. In medical image segmentation, where data samples are often limited, state-of-the-art (SOTA) methods improve accuracy by using pre-trained encoders, while end-to-end approaches typically fall short due to difficulties in transferring multiscale features effectively between encoders and decoders. To handle these challenges, we introduce a nested UNet architecture that captures both local and global context through Multiscale Feature Fusion and Attention Mechanisms. This design improves feature integration from encoders, highlights key channels and regions, and restores spatial details to enhance segmentation performance. Our method surpasses SOTA approaches, as evidenced by experiments across four datasets and detailed ablation studies. Code: <a target="_blank" rel="noopener" href="https://github.com/saadwazir/ReN-UNet">https://github.com/saadwazir/ReN-UNet</a> </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒä¸­è¯†åˆ«ç”Ÿç‰©æ ‡å¿—ç‰©å¯¹äºå¹¿æ³›çš„ç”Ÿç‰©æŠ€æœ¯åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæœ€è¿‘åŸºäºTransformerå’ŒCNNçš„æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹å½¢æ€å’ŒæŸ“è‰²æ–¹é¢çš„å˜åŒ–ï¼Œè¿™é™åˆ¶äº†å…¶ç‰¹å¾æå–èƒ½åŠ›ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œç”±äºæ•°æ®æ ·æœ¬é€šå¸¸æœ‰é™ï¼Œæœ€å…ˆè¿›çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨æ¥æé«˜å‡†ç¡®æ€§ï¼Œè€Œç«¯åˆ°ç«¯çš„æ–¹æ³•é€šå¸¸å› åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´æœ‰æ•ˆåœ°ä¼ é€’å¤šå°ºåº¦ç‰¹å¾æ–¹é¢é‡åˆ°å›°éš¾è€Œè¡¨ç°ä¸è¶³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åµŒå¥—UNetæ¶æ„ï¼Œå®ƒé€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆå’Œæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚è¿™ç§è®¾è®¡æ”¹è¿›äº†ç¼–ç å™¨çš„ç‰¹å¾é›†æˆï¼Œçªå‡ºäº†å…³é”®é€šé“å’ŒåŒºåŸŸï¼Œå¹¶æ¢å¤äº†ç©ºé—´ç»†èŠ‚ä»¥å¢å¼ºåˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œé€šè¿‡å››ä¸ªæ•°æ®é›†çš„å®éªŒå’Œè¯¦ç»†çš„æ¶ˆèç ”ç©¶å¯ä»¥è¯æ˜è¿™ä¸€ç‚¹ã€‚ä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/saadwazir/ReN-UNet">https://github.com/saadwazir/ReN-UNet</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06158v1">PDF</a> Published in the Proceedings of the 2024 International Conference on   Medical Imaging and Computer-Aided Diagnosis (MICAD 2024), Lecture Notes in   Electrical Engineering (LNEE), Volume 1372, Springer Nature, Singapore</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­çš„ç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«å¯¹äºç”Ÿç‰©æŠ€æœ¯åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµæ–¹æ³•åœ¨å¤„ç†å½¢æ€å­¦å’ŒæŸ“è‰²å·®å¼‚æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé™åˆ¶äº†ç‰¹å¾æå–èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åµŒå¥—UNetæ¶æ„ï¼Œé€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆå’Œæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜äº†ç‰¹å¾é›†æˆèƒ½åŠ›ï¼Œå¢å¼ºäº†åˆ†å‰²æ€§èƒ½ã€‚è¯¥æ–¹æ³•å·²è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œç»è¿‡å››ä¸ªæ•°æ®é›†çš„å®éªŒå’Œè¯¦ç»†çš„æ¶ˆèç ”ç©¶éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­çš„ç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«åœ¨ç”Ÿç‰©æŠ€æœ¯åº”ç”¨ä¸­æä¸ºå…³é”®ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†å½¢æ€å­¦å’ŒæŸ“è‰²å·®å¼‚æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åµŒå¥—UNetæ¶æ„ä»¥å¤„ç†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è¯¥æ¶æ„é€šè¿‡å¤šå°ºåº¦ç‰¹å¾èåˆå’Œæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>è¯¥è®¾è®¡æé«˜äº†ç‰¹å¾é›†æˆèƒ½åŠ›ï¼Œå¼ºåŒ–äº†å…³é”®é€šé“å’ŒåŒºåŸŸçš„æ ‡è¯†ã€‚</li>
<li>é€šè¿‡æ¢å¤ç©ºé—´ç»†èŠ‚ï¼Œå¢å¼ºäº†åˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c1c5a368fe15079959e4c92aefdefd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dacffadfe05c27216dc8522f407baf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70c10b645ddcfb362b6bbf968ff499e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6bcaf2ee5b817c6337906c766d8e3bd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MCAT-Visual-Query-Based-Localization-of-Standard-Anatomical-Clips-in-Fetal-Ultrasound-Videos-Using-Multi-Tier-Class-Aware-Token-Transformer"><a href="#MCAT-Visual-Query-Based-Localization-of-Standard-Anatomical-Clips-in-Fetal-Ultrasound-Videos-Using-Multi-Tier-Class-Aware-Token-Transformer" class="headerlink" title="MCAT: Visual Query-Based Localization of Standard Anatomical Clips in   Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer"></a>MCAT: Visual Query-Based Localization of Standard Anatomical Clips in   Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer</h2><p><strong>Authors:Divyanshu Mishra, Pramit Saha, He Zhao, Netzahualcoyotl Hernandez-Cruz, Olga Patey, Aris Papageorghiou, J. Alison Noble</strong></p>
<p>Accurate standard plane acquisition in fetal ultrasound (US) videos is crucial for fetal growth assessment, anomaly detection, and adherence to clinical guidelines. However, manually selecting standard frames is time-consuming and prone to intra- and inter-sonographer variability. Existing methods primarily rely on image-based approaches that capture standard frames and then classify the input frames across different anatomies. This ignores the dynamic nature of video acquisition and its interpretation. To address these challenges, we introduce Multi-Tier Class-Aware Token Transformer (MCAT), a visual query-based video clip localization (VQ-VCL) method, to assist sonographers by enabling them to capture a quick US sweep. By then providing a visual query of the anatomy they wish to analyze, MCAT returns the video clip containing the standard frames for that anatomy, facilitating thorough screening for potential anomalies. We evaluate MCAT on two ultrasound video datasets and a natural image VQ-VCL dataset based on Ego4D. Our model outperforms state-of-the-art methods by 10% and 13% mIoU on the ultrasound datasets and by 5.35% mIoU on the Ego4D dataset, using 96% fewer tokens. MCATâ€™s efficiency and accuracy have significant potential implications for public health, especially in low- and middle-income countries (LMICs), where it may enhance prenatal care by streamlining standard plane acquisition, simplifying US-based screening, diagnosis and allowing sonographers to examine more patients. </p>
<blockquote>
<p>åœ¨èƒå„¿è¶…å£°ï¼ˆUSï¼‰è§†é¢‘ä¸­å‡†ç¡®è·å–æ ‡å‡†å¹³é¢å¯¹äºèƒå„¿ç”Ÿé•¿è¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹ä»¥åŠéµå¾ªä¸´åºŠæŒ‡å—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨é€‰æ‹©æ ‡å‡†å¸§æ—¢è€—æ—¶åˆå®¹æ˜“å‡ºç°æ“ä½œè€…å†…éƒ¨å’Œæ“ä½œè€…ä¹‹é—´çš„å·®å¼‚ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–åŸºäºå›¾åƒçš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¼šæ•è·æ ‡å‡†å¸§å¹¶å¯¹è¾“å…¥å¸§è¿›è¡Œè·¨ä¸åŒè§£å‰–ç»“æ„çš„åˆ†ç±»ã€‚è¿™å¿½ç•¥äº†è§†é¢‘é‡‡é›†çš„åŠ¨æ€æ€§ä»¥åŠå…¶è§£é‡Šã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå±‚æ¬¡ç±»åˆ«æ„ŸçŸ¥ä»¤ç‰Œè½¬æ¢å™¨ï¼ˆMCATï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§†è§‰æŸ¥è¯¢çš„è§†é¢‘å‰ªè¾‘å®šä½ï¼ˆVQ-VCLï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨ååŠ©è¶…å£°åŒ»ç”Ÿè¿›è¡Œå¿«é€Ÿè¶…å£°æ‰«æã€‚é€šè¿‡æä¾›ä»–ä»¬å¸Œæœ›åˆ†æçš„è§£å‰–ç»“æ„çš„è§†è§‰æŸ¥è¯¢ï¼ŒMCATè¿”å›åŒ…å«è¯¥è§£å‰–ç»“æ„æ ‡å‡†å¸§çš„è§†é¢‘å‰ªè¾‘ï¼Œä¾¿äºå½»åº•ç­›æŸ¥æ½œåœ¨å¼‚å¸¸ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªè¶…å£°è§†é¢‘æ•°æ®é›†å’Œä¸€ä¸ªåŸºäºEgo4Dçš„è‡ªç„¶å›¾åƒVQ-VCLæ•°æ®é›†ä¸Šè¯„ä¼°äº†MCATã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¶…å£°æ•°æ®é›†ä¸Šçš„mIoUåˆ†åˆ«é«˜å‡ºæœ€æ–°æ–¹æ³•10%å’Œ13%ï¼Œåœ¨Ego4Dæ•°æ®é›†ä¸Šçš„mIoUé«˜å‡º5.35%ï¼ŒåŒæ—¶ä½¿ç”¨çš„ä»¤ç‰Œå‡å°‘äº†96%ã€‚MCATçš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§å¯¹å…¬å…±å«ç”Ÿå…·æœ‰é‡å¤§æ½œåœ¨å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ä½æ”¶å…¥å›½å®¶ï¼ˆLMICsï¼‰ï¼Œå®ƒå¯èƒ½é€šè¿‡ç®€åŒ–æ ‡å‡†å¹³é¢é‡‡é›†ã€åŸºäºè¶…å£°çš„ç­›æŸ¥å’Œè¯Šæ–­ï¼Œä½¿è¶…å£°åŒ»ç”Ÿèƒ½å¤Ÿæ£€æŸ¥æ›´å¤šæ‚£è€…ï¼Œä»è€ŒåŠ å¼ºäº§å‰æŠ¤ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06088v1">PDF</a> Accepted in AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMulti-Tier Class-Aware Token Transformerï¼ˆMCATï¼‰çš„è§†è§‰æŸ¥è¯¢è§†é¢‘å‰ªè¾‘å®šä½æ–¹æ³•ï¼Œç”¨äºè¾…åŠ©èƒå„¿è¶…å£°æ‰«æã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè¿…é€Ÿæ•æ‰è¶…å£°æ‰«æè¿‡ç¨‹ï¼Œé€šè¿‡æä¾›è§£å‰–ç»“æ„çš„è§†è§‰æŸ¥è¯¢ï¼Œè¿”å›åŒ…å«æ ‡å‡†å¸§çš„è§†é¢‘å‰ªè¾‘ï¼Œä¾¿äºåŒ»ç”Ÿè¿›è¡Œæ½œåœ¨å¼‚å¸¸çš„å…¨é¢ç­›æŸ¥ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒå„¿è¶…å£°æ ‡å‡†å¹³é¢é‡‡é›†çš„é‡è¦æ€§ï¼šå¯¹äºèƒå„¿ç”Ÿé•¿è¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹ä»¥åŠéµå¾ªä¸´åºŠæŒ‡å—è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„é—®é¢˜ï¼šä¸»è¦ä¾èµ–å›¾åƒæ–¹æ³•æ•æ‰æ ‡å‡†å¸§ï¼Œå¹¶åˆ†ç±»è¾“å…¥å¸§ï¼Œå¿½ç•¥äº†è§†é¢‘é‡‡é›†çš„åŠ¨æ€æ€§å’Œè§£è¯»ã€‚</li>
<li>MCATæ–¹æ³•ä»‹ç»ï¼šé‡‡ç”¨è§†è§‰æŸ¥è¯¢è§†é¢‘å‰ªè¾‘å®šä½æŠ€æœ¯ï¼Œè¾…åŠ©åŒ»ç”Ÿå¿«é€Ÿæ•æ‰è¶…å£°æ‰«æè¿‡ç¨‹ã€‚</li>
<li>MCATçš„ä¼˜åŠ¿ï¼šæ ¹æ®è§£å‰–ç»“æ„æä¾›è§†è§‰æŸ¥è¯¢ï¼Œè¿”å›åŒ…å«æ ‡å‡†å¸§çš„è§†é¢‘å‰ªè¾‘ï¼Œä¾¿äºæ½œåœ¨å¼‚å¸¸çš„å…¨é¢ç­›æŸ¥ã€‚</li>
<li>MCATçš„è¯„ä¼°ï¼šåœ¨è¶…å£°è§†é¢‘æ•°æ®é›†å’Œè‡ªç„¶å›¾åƒVQ-VCLæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰ã€‚</li>
<li>MCATçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼šä½¿ç”¨è¾ƒå°‘çš„ä»¤ç‰Œæ•°é‡ï¼ˆ96%ï¼‰ï¼Œå…·æœ‰é«˜æ•ˆçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d4a84ba9dff01bc98fba1a86d7b243e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ad879b8f1f518153e80d4d3ac850252.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-102a8c7cc2899ad6349ed2d104a45533.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-122875f80e5a192f699f810a75735662.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-455f2a4ef1bdae75e1ff65a251f61ad2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-950883b977d9963420888ac87c8753db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b758f022908a84475191616de75a6f02.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Latent-Multimodal-Reconstruction-for-Misinformation-Detection"><a href="#Latent-Multimodal-Reconstruction-for-Misinformation-Detection" class="headerlink" title="Latent Multimodal Reconstruction for Misinformation Detection"></a>Latent Multimodal Reconstruction for Misinformation Detection</h2><p><strong>Authors:Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, Panagiotis C. Petrantonakis</strong></p>
<p>Multimodal misinformation, such as miscaptioned images, where captions misrepresent an imageâ€™s origin, context, or meaning, poses a growing challenge in the digital age. To support fact-checkers, researchers have been focusing on creating datasets and developing methods for multimodal misinformation detection (MMD). Due to the scarcity of large-scale annotated MMD datasets, recent studies leverage synthetic training data via out-of-context image-caption pairs or named entity manipulations; altering names, dates, and locations. However, these approaches often produce simplistic misinformation that fails to reflect real-world complexity, limiting the robustness of detection models trained on them. Meanwhile, despite recent advancements, Large Vision-Language Models (LVLMs) remain underutilized for generating diverse, realistic synthetic training data for MMD. To address this gap, we introduce â€œMisCaption This!â€, a training dataset comprising LVLM-generated miscaptioned images. Additionally, we introduce â€œLatent Multimodal Reconstructionâ€ (LAMAR), a network trained to reconstruct the embeddings of truthful captions, providing a strong auxiliary signal to the detection process. To optimize LAMAR, we explore different training strategies (end-to-end training and large-scale pre-training) and integration approaches (direct, mask, gate, and attention). Extensive experiments show that models trained on â€œMisCaption This!â€ generalize better on real-world misinformation, while LAMAR sets new state-of-the-art on both NewsCLIPpings and VERITE benchmarks; highlighting the potential of LVLM-generated data and reconstruction-based approaches for advancing MMD. We release our code at: <a target="_blank" rel="noopener" href="https://github.com/stevejpapad/miscaptioned-image-reconstruction">https://github.com/stevejpapad/miscaptioned-image-reconstruction</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯ï¼Œå¦‚è¯¯æ ‡æ³¨çš„å›¾åƒï¼Œå…¶ä¸­çš„æ ‡é¢˜é”™è¯¯åœ°ä»£è¡¨äº†å›¾åƒçš„åŸå‡ºå¤„ã€ä¸Šä¸‹æ–‡æˆ–æ„ä¹‰ï¼Œåœ¨æ•°å­—æ—¶ä»£æ„æˆäº†ä¸€ä¸ªæ—¥ç›Šä¸¥å³»çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æ”¯æŒäº‹å®æ ¸æŸ¥äººå‘˜ï¼Œç ”ç©¶äººå‘˜ä¸€ç›´ä¸“æ³¨äºåˆ›å»ºæ•°æ®é›†å¹¶å¼€å‘å¤šæ¨¡æ€è¯¯æ£€æ£€æµ‹ï¼ˆMMDï¼‰æ–¹æ³•ã€‚ç”±äºå¤§è§„æ¨¡æ ‡æ³¨çš„MMDæ•°æ®é›†ç¨€ç¼ºï¼Œæœ€è¿‘çš„ç ”ç©¶é€šè¿‡è„±ç¦»ä¸Šä¸‹æ–‡çš„å›¾åƒæ ‡é¢˜å¯¹æˆ–å‘½åå®ä½“æ“ä½œï¼ˆå¦‚æ›´æ”¹åç§°ã€æ—¥æœŸå’Œåœ°ç‚¹ï¼‰æ¥åˆ©ç”¨åˆæˆè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿç®€å•çš„é”™è¯¯ä¿¡æ¯ï¼Œæ— æ³•åæ˜ ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ï¼Œé™åˆ¶äº†åœ¨è¿™äº›æ•°æ®ä¸Šè®­ç»ƒçš„æ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§ã€‚å°½ç®¡æœ€è¿‘æœ‰è¯¸å¤šè¿›å±•ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨ç”Ÿæˆå¤šæ ·ä¸”ç°å®çš„åˆæˆè®­ç»ƒæ•°æ®æ–¹é¢ä»ç„¶ä½¿ç”¨ä¸è¶³ï¼Œç”¨äºMMDã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†â€œMisCaption Thisï¼â€è®­ç»ƒæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç”±LVLMç”Ÿæˆçš„è¯¯æ ‡æ³¨å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†â€œæ½œåœ¨å¤šæ¨¡æ€é‡å»ºâ€ï¼ˆLAMARï¼‰ç½‘ç»œï¼Œè¯¥ç½‘ç»œç»è¿‡è®­ç»ƒä»¥é‡å»ºçœŸå®æ ‡é¢˜çš„åµŒå…¥ï¼Œä¸ºæ£€æµ‹è¿‡ç¨‹æä¾›å¼ºå¤§çš„è¾…åŠ©ä¿¡å·ã€‚ä¸ºäº†ä¼˜åŒ–LAMARï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸åŒçš„è®­ç»ƒç­–ç•¥ï¼ˆç«¯åˆ°ç«¯è®­ç»ƒå’Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼‰å’Œé›†æˆæ–¹æ³•ï¼ˆç›´æ¥ã€æ©ç ã€é—¨æ§å’Œæ³¨æ„åŠ›ï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨â€œMisCaption Thisï¼â€ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ç°å®ä¸–ç•Œé”™è¯¯ä¿¡æ¯ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼Œè€ŒLAMARåœ¨NewsCLIPpingså’ŒVERITEåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œçªæ˜¾äº†LVLMç”Ÿæˆæ•°æ®å’Œé‡å»ºæ–¹æ³•åœ¨å¤šæ¨¡æ€è¯¯æ£€æ£€æµ‹ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/stevejpapad/miscaptioned-image-reconstruction">https://github.com/stevejpapad/miscaptioned-image-reconstruction</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06010v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚è¯¯æ ‡æ³¨çš„å›¾åƒï¼‰å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»¥åŠé’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜çš„å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹ï¼ˆMMDï¼‰é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚ç ”ç©¶ä¸­ï¼Œå› ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨çš„MMDæ•°æ®é›†ï¼Œç ”ç©¶è€…é€šè¿‡åˆæˆè®­ç»ƒæ•°æ®çš„æ–¹å¼è¿›è¡Œç ”ç©¶ï¼Œå¦‚åˆ©ç”¨è„±ç¦»ä¸Šä¸‹æ–‡å…³ç³»çš„å›¾åƒ-å­—å¹•é…å¯¹æˆ–å‘½åå®ä½“æ“æ§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•äº§ç”Ÿçš„é”™è¯¯ä¿¡æ¯è¿‡äºç®€å•ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†â€œMisCaption This!â€è®­ç»ƒæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç”±å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ç”Ÿæˆçš„è¯¯æ ‡æ³¨å›¾åƒã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†â€œæ½œåœ¨å¤šæ¨¡æ€é‡å»ºâ€ï¼ˆLAMARï¼‰ç½‘ç»œï¼Œç”¨äºè®­ç»ƒæ£€æµ‹è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œâ€œMisCaption This!â€æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨ç°å®ä¸–ç•Œé”™è¯¯ä¿¡æ¯ä¸Šçš„æ³›åŒ–æ€§èƒ½æ›´å¥½ï¼Œè€ŒLAMARç½‘ç»œåœ¨æ–°CLASCLIPpingså’ŒVERITEåŸºå‡†æµ‹è¯•ä¸Šå‡è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚è¿™è¡¨æ˜LVLMç”Ÿæˆçš„æ•°æ®å’Œé‡å»ºæ–¹æ³•åœ¨å¤šæ¨¡æ€è¯¯ä¿¡æ¯ä¼ æ’­æ–¹é¢æœ‰ç€å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬æå–å‡ºçš„ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚è¯¯æ ‡æ³¨çš„å›¾åƒï¼‰å·²æˆä¸ºæ•°å­—æ—¶ä»£çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è€…æ­£åœ¨å¼€å‘å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯æ£€æµ‹ï¼ˆMMDï¼‰æŠ€æœ¯ä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>ç”±äºç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨çš„MMDæ•°æ®é›†ï¼Œåˆæˆè®­ç»ƒæ•°æ®æˆä¸ºç ”ç©¶é‡ç‚¹ã€‚</li>
<li>å½“å‰æ–¹æ³•äº§ç”Ÿçš„é”™è¯¯ä¿¡æ¯è¿‡äºç®€å•ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</li>
<li>â€œMisCaption This!â€æ•°æ®é›†ç”±å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ç”Ÿæˆçš„è¯¯æ ‡æ³¨å›¾åƒç»„æˆï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†æ–°æ–¹å‘ã€‚</li>
<li>å¼•å…¥â€œæ½œåœ¨å¤šæ¨¡æ€é‡å»ºâ€ï¼ˆLAMARï¼‰ç½‘ç»œè¿›è¡Œè®­ç»ƒæ£€æµ‹è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b9cbee8aede2cf6c6cba8321a8a4016.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f16f51a0bcf1ef95e77d2d6df11a9fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b14387b7cd1a5a4c6f879742240e5e83.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Optical-and-X-ray-timing-analysis-of-the-2018-2020-outburst-and-rebrightening-of-the-black-hole-transient-MAXI-J1820-070"><a href="#Optical-and-X-ray-timing-analysis-of-the-2018-2020-outburst-and-rebrightening-of-the-black-hole-transient-MAXI-J1820-070" class="headerlink" title="Optical and X-ray timing analysis of the 2018-2020 outburst and   rebrightening of the black-hole transient MAXI J1820+070"></a>Optical and X-ray timing analysis of the 2018-2020 outburst and   rebrightening of the black-hole transient MAXI J1820+070</h2><p><strong>Authors:M. Fiori, L. Zampieri, A. Burtovoi, G. Naletto, P. Ochner, U. Munari, F. Manzini, A. Vagnozzi, E. A. Barsukova, M. A. Burlak, V. P. Goranski, N. P. Ikonnikova, N. A. Katysheva, E. G. Sheyanov, S. Yu. Shugarov, A. V. Zharova, A. M. Zubareva, S. E. Motta</strong></p>
<p>We report the results of a comprehensive analysis of the multiwavelength (in optical and X-rays) and multitimescale (from months to tenths of a second) variability of the 2018-2020 outburst of the black hole transient MAXI J1820+070. During the first outburst episode, a detailed analysis of the optical photometry shows a periodicity that evolves over time and stabilises at a frequency of $1.4517(1)$ $1&#x2F;d$ ($\sim0.5%$ longer than the orbital period). This super-orbital modulation is also seen in the X-rays for a few days soon after the transition to the high-soft state. We also observed optical Quasi-Periodic Oscillations (QPOs), which correspond to some of the QPOs observed in X-rays at three different epochs when the source was in the low-hard state. In two epochs, optical QPOs with a centroid consistent with half the frequency of the most prominent X-ray QPO can be seen. If the lowest modulation frequency is the one observed in the optical, the characteristic precession frequency of MAXI J1820+070 is lower than that inferred from the &#96;fundamentalâ€™ QPO in the X-rays. Assuming that QPOs can be generated by Lense-Thirring precession, we calculate the spin of the black hole in the case where the fundamental precession frequency is tracked by the optical emission. We find a relatively slowly spinning black hole with a spin parameter $\lesssim 0.15$. The super-orbital optical and X-ray modulations observed after the disappearance of the QPOs may be triggered by the self-irradiation of the outer disc by a standard inner disc truncated at a few gravitational radii. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†å¯¹é»‘æ´ç¬æ€MAXI J1820+070åœ¨2018-2020å¹´çˆ†å‘æœŸçš„å¤šæ³¢é•¿ï¼ˆå…‰å­¦å’ŒXå°„çº¿ï¼‰å’Œå¤šå°ºåº¦ï¼ˆä»å‡ ä¸ªæœˆåˆ°ååˆ†ä¹‹ä¸€ç§’ï¼‰å˜åŒ–çš„ç»¼åˆåˆ†æç»“æœã€‚åœ¨ç¬¬ä¸€æ¬¡çˆ†å‘æœŸé—´ï¼Œå¯¹å…‰å­¦å…‰åº¦è®¡æ•°æ®çš„è¯¦ç»†åˆ†ææ˜¾ç¤ºäº†ä¸€ä¸ªéšæ—¶é—´æ¼”åŒ–å¹¶ç¨³å®šåœ¨$ 1.4517ï¼ˆ1ï¼‰\frac{1}{d}$é¢‘ç‡ï¼ˆæ¯”è½¨é“å‘¨æœŸé•¿çº¦0.5%ï¼‰çš„è¶…è½¨é“è°ƒåˆ¶ã€‚è¿™ç§è¶…è½¨é“è°ƒåˆ¶åœ¨è½¬åˆ°é«˜è½¯æ€åçš„å‡ å¤©å†…ä¹Ÿåœ¨Xå°„çº¿ä¸Šè§‚å¯Ÿåˆ°ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°å…‰å­¦å‡†å‘¨æœŸæŒ¯è¡ï¼ˆQPOsï¼‰ï¼Œè¿™ä¸æºå¤„äºä½ç¡¬æ€æ—¶åœ¨ä¸åŒæ—¶æœŸè§‚å¯Ÿåˆ°çš„ä¸‰æ¬¡Xå°„çº¿QPOsç›¸å¯¹åº”ã€‚åœ¨ä¸¤ä¸ªæ—¶æœŸä¸­ï¼Œå…‰å­¦QPOçš„ä¸­å¿ƒé¢‘ç‡ä¸æœ€çªå‡ºçš„Xå°„çº¿QPOé¢‘ç‡çš„ä¸€åŠä¸€è‡´ã€‚å¦‚æœæœ€ä½è°ƒåˆ¶é¢‘ç‡æ˜¯åœ¨å…‰å­¦ä¸­è§‚å¯Ÿåˆ°çš„ï¼Œé‚£ä¹ˆMAXI J1820+070çš„ç‰¹å¾è¿›åŠ¨é¢‘ç‡ä½äºä»Xå°„çº¿ä¸­çš„â€œåŸºæœ¬â€QPOæ¨æ–­å‡ºçš„é¢‘ç‡ã€‚å‡è®¾QPOså¯ä»¥ç”±Lense-Thirringè¿›åŠ¨äº§ç”Ÿï¼Œæˆ‘ä»¬è®¡ç®—äº†é»‘æ´è‡ªè½¬çš„æƒ…å†µï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºæœ¬è¿›åŠ¨é¢‘ç‡ç”±å…‰å­¦å‘å°„è·Ÿè¸ªã€‚æˆ‘ä»¬å‘ç°ä¸€ä¸ªç›¸å¯¹ç¼“æ…¢æ—‹è½¬çš„é»‘æ´ï¼Œå…¶è‡ªè½¬å‚æ•°â‰¤0.15ã€‚åœ¨QPOæ¶ˆå¤±åè§‚å¯Ÿåˆ°çš„å…‰å­¦å’ŒXå°„çº¿è¶…è½¨é“è°ƒåˆ¶å¯èƒ½ç”±å¤–ç›˜çš„è‡ªæˆ‘ç…§å°„è§¦å‘ï¼Œæ ‡å‡†å†…ç›˜è¢«æˆªæ–­åœ¨å‡ ä¸ªå¼•åŠ›åŠå¾„å†…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06001v1">PDF</a> 18 pages, 18 figures, 6 tables. Accepted for publication in A&amp;A</p>
<p><strong>Summary</strong><br>     å¯¹é»‘æ´ç¬æ€MAXI J1820+070çš„2018-2020å¹´çˆ†å‘è¿›è¡Œäº†å…¨é¢çš„å¤šæ³¢é•¿ï¼ˆå…‰å­¦å’ŒXå°„çº¿ï¼‰å’Œå¤šå°ºåº¦ï¼ˆä»å‡ ä¸ªæœˆåˆ°ååˆ†ä¹‹ä¸€ç§’ï¼‰å˜åŒ–åˆ†æã€‚åœ¨é¦–æ¬¡çˆ†å‘æœŸé—´ï¼Œå…‰å­¦å…‰åº¦è®¡çš„è¯¦ç»†åˆ†ææ˜¾ç¤ºå‘¨æœŸæ€§éšæ—¶é—´æ¼”å˜ï¼Œæœ€ç»ˆç¨³å®šåœ¨æ¯æ—¥é¢‘ç‡çš„$1.4517(1)$ï¼Œæ¯”è½¨é“å‘¨æœŸé•¿çº¦$0.5%$ã€‚è¶…è½¨é“è°ƒåˆ¶ä¹Ÿåœ¨Xå°„çº¿ä¸ŠæŒç»­äº†å‡ å¤©ã€‚è§‚å¯Ÿåˆ°å…‰å­¦å‡†å‘¨æœŸæŒ¯è¡ï¼ˆQPOsï¼‰ä¸æºå¤„äºä½ç¡¬æ€æ—¶Xå°„çº¿ä¸­çš„ä¸‰æ¬¡ä¸åŒæ—¶æœŸçš„QPOç›¸å¯¹åº”ã€‚åœ¨ä¸¤ä¸ªæ—¶æœŸä¸­ï¼Œå…‰å­¦QPOçš„ä¸­å¿ƒé¢‘ç‡ä¸æœ€çªå‡ºçš„Xå°„çº¿QPOçš„ä¸€åŠé¢‘ç‡ä¸€è‡´ã€‚å‡è®¾æœ€ä½è°ƒåˆ¶é¢‘ç‡åœ¨å…‰å­¦ä¸Šè¢«è§‚å¯Ÿåˆ°ï¼ŒMAXI J1820+070çš„ç‰¹å¾è¿›åŠ¨é¢‘ç‡ä½äºXå°„çº¿ä¸­çš„åŸºæœ¬QPOæ‰€æ¨æ–­çš„é¢‘ç‡ã€‚å‡è®¾QPOsæ˜¯ç”±Lense-Thirringè¿›åŠ¨äº§ç”Ÿçš„ï¼Œæˆ‘ä»¬è®¡ç®—äº†è·Ÿè¸ªå…‰å­¦å‘å°„çš„åŸºæœ¬è¿›åŠ¨é¢‘ç‡æƒ…å†µä¸‹çš„é»‘æ´è‡ªè½¬ã€‚å‘ç°äº†ä¸€ä¸ªç›¸å¯¹è¾ƒæ…¢çš„è‡ªè½¬é»‘æ´ï¼Œè‡ªè½¬å‚æ•°$\leqslant 0.15$ã€‚å…‰å­¦å’ŒXå°„çº¿çš„è¶…è½¨é“è°ƒåˆ¶å¯èƒ½åœ¨QPOæ¶ˆå¤±åè¢«è§¦å‘ï¼Œå¯èƒ½æ˜¯ç”±äºæ ‡å‡†å†…ç›˜è¢«æˆªæ–­åœ¨å‡ ä¸ªå¼•åŠ›åŠå¾„æ—¶å¯¹å¤–ç›˜çš„è‡ªç…§ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAXI J1820+070é»‘æ´ç¬æ€åœ¨2018-2020å¹´çš„çˆ†å‘ä¸­è¡¨ç°å‡ºå¤šæ³¢é•¿å’Œå¤šå°ºåº¦çš„å˜åŒ–ç‰¹æ€§ã€‚</li>
<li>åœ¨é¦–æ¬¡çˆ†å‘æœŸé—´ï¼Œè§‚å¯Ÿåˆ°å…‰å­¦å…‰åº¦å‘¨æœŸæ€§éšæ—¶é—´æ¼”å˜å¹¶ç¨³å®šåœ¨ä¸è½¨é“å‘¨æœŸæœ‰è½»å¾®å·®å¼‚çš„é¢‘ç‡ä¸Šã€‚</li>
<li>åœ¨æŸäº›æ—¶æ®µè§‚å¯Ÿåˆ°å…‰å­¦å’ŒXå°„çº¿ä¸­çš„è¶…è½¨é“è°ƒåˆ¶ã€‚</li>
<li>å…‰å­¦å‡†å‘¨æœŸæŒ¯è¡ï¼ˆQPOsï¼‰ä¸Xå°„çº¿ä¸­çš„QPOå­˜åœ¨å…³è”ï¼Œç‰¹åˆ«æ˜¯åœ¨æºå¤„äºä½ç¡¬æ€æ—¶ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…‰å­¦QPOçš„ä¸­å¿ƒé¢‘ç‡ä¸Xå°„çº¿QPOçš„ä¸€åŠé¢‘ç‡ç›¸ç¬¦ã€‚</li>
<li>åŸºäºLense-Thirringè¿›åŠ¨ç†è®ºï¼Œæ¨æ–­å‡ºè¯¥é»‘æ´çš„è‡ªè½¬é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a9bc9715399f4553638ba7e90ed142a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ae018dcfeb698a06392fd36f301dd72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e98ba47f624168a2c563be235b642d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd2792523ea5c6df20cbf13ba1c365f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db8f6b0e094250012d0edde73e0c794e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Ambiguous-Image-Segmentation"><a href="#Diffusion-Based-Ambiguous-Image-Segmentation" class="headerlink" title="Diffusion Based Ambiguous Image Segmentation"></a>Diffusion Based Ambiguous Image Segmentation</h2><p><strong>Authors:Jakob LÃ¸nborg Christensen, Morten Rieger Hannemose, Anders Bjorholm Dahl, Vedrana Andersen Dahl</strong></p>
<p>Medical image segmentation often involves inherent uncertainty due to variations in expert annotations. Capturing this uncertainty is an important goal and previous works have used various generative image models for the purpose of representing the full distribution of plausible expert ground truths. In this work, we explore the design space of diffusion models for generative segmentation, investigating the impact of noise schedules, prediction types, and loss weightings. Notably, we find that making the noise schedule harder with input scaling significantly improves performance. We conclude that x- and v-prediction outperform epsilon-prediction, likely because the diffusion process is in the discrete segmentation domain. Many loss weightings achieve similar performance as long as they give enough weight to the end of the diffusion process. We base our experiments on the LIDC-IDRI lung lesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we introduce a randomly cropped variant of the LIDC-IDRI dataset that is better suited for uncertainty in image segmentation. Our model also achieves SOTA in this harder setting. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ç»å¸¸å› ä¸ºä¸“å®¶æ ‡æ³¨çš„å˜å¼‚è€Œæ¶‰åŠå›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚æ•æ‰è¿™ç§ä¸ç¡®å®šæ€§æ˜¯ä¸€ä¸ªé‡è¦ç›®æ ‡ï¼Œä¹‹å‰çš„ç ”ç©¶å·²ç»ä½¿ç”¨å„ç§ç”Ÿæˆå›¾åƒæ¨¡å‹æ¥è¡¨ç¤ºä¸“å®¶çœŸå®æ ‡æ³¨çš„å…¨åˆ†å¸ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç”Ÿæˆåˆ†å‰²æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼Œç ”ç©¶äº†å™ªå£°æ—¶é—´è¡¨ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡çš„å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡è¾“å…¥ç¼©æ”¾ä½¿å™ªå£°æ—¶é—´è¡¨æ›´åŠ å›°éš¾å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œxé¢„æµ‹å’Œvé¢„æµ‹çš„è¡¨ç°ä¼˜äºÎµé¢„æµ‹ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‰©æ•£è¿‡ç¨‹å¤„äºç¦»æ•£åˆ†å‰²é¢†åŸŸã€‚åªè¦å¯¹æ‰©æ•£è¿‡ç¨‹çš„æœ«å°¾ç»™äºˆè¶³å¤Ÿçš„é‡è§†ï¼Œè®¸å¤šæŸå¤±æƒé‡éƒ½èƒ½å®ç°ç›¸ä¼¼çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒåŸºäºLIDC-IDRIè‚ºç—…å˜æ•°æ®é›†ï¼Œå¹¶å–å¾—äº†æœ€æ–°(SOTA)çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†LIDC-IDRIæ•°æ®é›†çš„éšæœºè£å‰ªç‰ˆæœ¬ï¼Œæ›´é€‚åˆäºå›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™ä¸ªæ›´å›°éš¾çš„è®¾ç½®ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05977v1">PDF</a> Accepted at SCIA25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ç”Ÿæˆæ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼Œç ”ç©¶äº†å™ªå£°å®‰æ’ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒåŠ å¤§è¾“å…¥è§„æ¨¡çš„å™ªå£°å®‰æ’èƒ½æé«˜æ€§èƒ½ï¼Œx-å’Œv-é¢„æµ‹ä¼˜äºepsilon-é¢„æµ‹ï¼Œè¿™å¯èƒ½æ˜¯ç¦»æ•£åˆ†å‰²é¢†åŸŸä¸­çš„æ‰©æ•£è¿‡ç¨‹æ‰€è‡´ã€‚å¤šç§æŸå¤±æƒé‡åœ¨ç»™äºˆè¶³å¤Ÿé‡è§†æ‰©æ•£è¿‡ç¨‹ç»“æŸæ—¶éƒ½èƒ½è¾¾åˆ°ç›¸ä¼¼æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ–‡ç« åœ¨LIDC-IDRIè‚ºç—…å˜æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†æ›´é€‚åˆå›¾åƒåˆ†å‰²ä¸ç¡®å®šæ€§çš„LIDC-IDRIæ•°æ®é›†çš„éšæœºè£å‰ªç‰ˆæœ¬ï¼Œæ¨¡å‹åœ¨è¯¥æ›´å›°éš¾çš„ç¯å¢ƒä¸‹ä¹Ÿè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç”±äºä¸“å®¶æ ‡æ³¨å·®å¼‚å­˜åœ¨å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºç”Ÿæˆå›¾åƒåˆ†å‰²æ¨¡å‹çš„è®¾è®¡ç©ºé—´æ¢ç´¢ä¸­ï¼Œç ”ç©¶äº†å™ªå£°å®‰æ’ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡çš„å½±å“ã€‚</li>
<li>åŠ å¤§è¾“å…¥è§„æ¨¡çš„å™ªå£°å®‰æ’èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>x-å’Œv-é¢„æµ‹ä¼˜äºepsilon-é¢„æµ‹ï¼Œè¿™å¯èƒ½ä¸ç¦»æ•£åˆ†å‰²é¢†åŸŸçš„æ‰©æ•£è¿‡ç¨‹æœ‰å…³ã€‚</li>
<li>å¤šç§æŸå¤±æƒé‡åœ¨é‡è§†æ‰©æ•£è¿‡ç¨‹ç»“æŸæ—¶éƒ½èƒ½è·å¾—è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>å®éªŒåŸºäºLIDC-IDRIè‚ºç—…å˜æ•°æ®é›†ï¼Œè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6170fc41063aa1b1104c59a8d656712b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6de7ffaa87d20e553c841ce75a2cb731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4512d984a41c79504eec109e680c6028.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CTI-Unet-Cascaded-Threshold-Integration-for-Improved-U-Net-Segmentation-of-Pathology-Images"><a href="#CTI-Unet-Cascaded-Threshold-Integration-for-Improved-U-Net-Segmentation-of-Pathology-Images" class="headerlink" title="CTI-Unet: Cascaded Threshold Integration for Improved U-Net Segmentation   of Pathology Images"></a>CTI-Unet: Cascaded Threshold Integration for Improved U-Net Segmentation   of Pathology Images</h2><p><strong>Authors:Mingyang Zhu, Yuqiu Liang, Jiacheng Wang</strong></p>
<p>Chronic kidney disease (CKD) is a growing global health concern, necessitating precise and efficient image analysis to aid diagnosis and treatment planning. Automated segmentation of kidney pathology images plays a central role in facilitating clinical workflows, yet conventional segmentation models often require delicate threshold tuning. This paper proposes a novel \textit{Cascaded Threshold-Integrated U-Net (CTI-Unet)} to overcome the limitations of single-threshold segmentation. By sequentially integrating multiple thresholded outputs, our approach can reconcile noise suppression with the preservation of finer structural details. Experiments on the challenging KPIs2024 dataset demonstrate that CTI-Unet outperforms state-of-the-art architectures such as nnU-Net, Swin-Unet, and CE-Net, offering a robust and flexible framework for kidney pathology image segmentation. </p>
<blockquote>
<p>æ…¢æ€§è‚¾è„ç—…ï¼ˆCKDï¼‰æ˜¯å…¨çƒæ—¥ç›Šå…³æ³¨çš„å¥åº·é—®é¢˜ï¼Œéœ€è¦ç²¾ç¡®é«˜æ•ˆçš„å›¾åƒåˆ†ææ¥è¾…åŠ©è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ã€‚è‚¾è„ç—…ç†å›¾åƒçš„è‡ªåŠ¨åˆ†å‰²åœ¨ä¿ƒè¿›ä¸´åºŠå·¥ä½œæµç¨‹ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œä½†ä¼ ç»Ÿçš„åˆ†å‰²æ¨¡å‹é€šå¸¸éœ€è¦ç²¾ç»†çš„é˜ˆå€¼è°ƒæ•´ã€‚æœ¬æ–‡é’ˆå¯¹å•ä¸€é˜ˆå€¼åˆ†å‰²çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„çº§è”é˜ˆå€¼é›†æˆU-Netï¼ˆCTI-Unetï¼‰ã€‚é€šè¿‡é¡ºåºé›†æˆå¤šä¸ªé˜ˆå€¼è¾“å‡ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨æŠ‘åˆ¶å™ªå£°çš„åŒæ—¶ä¿ç•™æ›´ç²¾ç»†çš„ç»“æ„ç»†èŠ‚ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„KPIs2024æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCTI-Unetä¼˜äºæœ€æ–°æ¶æ„ï¼Œå¦‚nnU-Netã€Swin-Unetå’ŒCE-Netï¼Œä¸ºè‚¾è„ç—…ç†å›¾åƒåˆ†å‰²æä¾›äº†ç¨³å¥çµæ´»æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05640v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„çº§è”é˜ˆå€¼é›†æˆU-Netï¼ˆCTI-Unetï¼‰æ–¹æ³•ï¼Œç”¨äºå…‹æœå•ä¸€é˜ˆå€¼åˆ†å‰²åœ¨è‚¾è„ç—…ç†å›¾åƒåˆ†æä¸­çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æŒ‰é¡ºåºé›†æˆå¤šä¸ªé˜ˆå€¼è¾“å‡ºï¼Œæ—¢æŠ‘åˆ¶äº†å™ªå£°åˆä¿ç•™äº†æ›´ç²¾ç»†çš„ç»“æ„ç»†èŠ‚ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„KPIs2024æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCTI-Unetä¼˜äºnnU-Netã€Swin-Unetå’ŒCE-Netç­‰å½“å‰å…ˆè¿›æ¶æ„ï¼Œä¸ºè‚¾è„ç—…ç†å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ä¸ªç¨³å¥å’Œçµæ´»çš„åˆ†ææ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ…¢æ€§è‚¾è„ç–¾ç—…ï¼ˆCKDï¼‰æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„å¥åº·é—®é¢˜ï¼Œéœ€è¦ç²¾ç¡®å’Œé«˜æ•ˆçš„å›¾åƒåˆ†ææ¥è¾…åŠ©è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ã€‚</li>
<li>è‡ªåŠ¨åŒ–è‚¾è„ç—…ç†å›¾åƒåˆ†å‰²åœ¨ä¿ƒè¿›ä¸´åºŠå·¥ä½œæµç¨‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ä¼ ç»Ÿåˆ†å‰²æ¨¡å‹é€šå¸¸éœ€è¦ç²¾ç»†çš„é˜ˆå€¼è°ƒæ•´ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>CTI-Uneté€šè¿‡çº§è”é›†æˆå¤šä¸ªé˜ˆå€¼è¾“å‡ºï¼Œå…‹æœäº†å•ä¸€é˜ˆå€¼åˆ†å‰²çš„ç¼ºç‚¹ã€‚</li>
<li>CTI-Unetæ–¹æ³•èƒ½åœ¨æŠ‘åˆ¶å™ªå£°çš„åŒæ—¶ä¿ç•™æ›´ç²¾ç»†çš„ç»“æ„ç»†èŠ‚ã€‚</li>
<li>åœ¨KPIs2024æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCTI-Unetæ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-438aeb356b8eb285800cbfa0d6c58dfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc30a499c9bf76cb5e12066a06493d7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa0ea24eb627f1aecee12ecc2f26ac1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e510b6cb850e6626228833e2d1147144.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Maternal-and-Fetal-Health-Status-Assessment-by-Using-Machine-Learning-on-Optical-3D-Body-Scans"><a href="#Maternal-and-Fetal-Health-Status-Assessment-by-Using-Machine-Learning-on-Optical-3D-Body-Scans" class="headerlink" title="Maternal and Fetal Health Status Assessment by Using Machine Learning on   Optical 3D Body Scans"></a>Maternal and Fetal Health Status Assessment by Using Machine Learning on   Optical 3D Body Scans</h2><p><strong>Authors:Ruting Cheng, Yijiang Zheng, Boyuan Feng, Chuhui Qiu, Zhuoxin Long, Joaquin A. Calderon, Xiaoke Zhang, Jaclyn M. Phillips, James K. Hahn</strong></p>
<p>Monitoring maternal and fetal health during pregnancy is crucial for preventing adverse outcomes. While tests such as ultrasound scans offer high accuracy, they can be costly and inconvenient. Telehealth and more accessible body shape information provide pregnant women with a convenient way to monitor their health. This study explores the potential of 3D body scan data, captured during the 18-24 gestational weeks, to predict adverse pregnancy outcomes and estimate clinical parameters. We developed a novel algorithm with two parallel streams which are used for extract body shape features: one for supervised learning to extract sequential abdominal circumference information, and another for unsupervised learning to extract global shape descriptors, alongside a branch for demographic data.   Our results indicate that 3D body shape can assist in predicting preterm labor, gestational diabetes mellitus (GDM), gestational hypertension (GH), and in estimating fetal weight. Compared to other machine learning models, our algorithm achieved the best performance, with prediction accuracies exceeding 88% and fetal weight estimation accuracy of 76.74% within a 10% error margin, outperforming conventional anthropometric methods by 22.22%. </p>
<blockquote>
<p>ç›‘æµ‹å­•å¦‡å’Œèƒå„¿åœ¨å¦Šå¨ æœŸé—´çš„å¥åº·çŠ¶å†µå¯¹äºé¢„é˜²ä¸è‰¯ç»“æœè‡³å…³é‡è¦ã€‚è™½ç„¶è¶…å£°æ‰«æç­‰æµ‹è¯•æä¾›äº†å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¯èƒ½æˆæœ¬é«˜æ˜‚ä¸”ä½¿ç”¨ä¸ä¾¿ã€‚è¿œç¨‹åŒ»ç–—å’Œæ›´æ˜“äºè·å–çš„èº«ä½“å½¢æ€ä¿¡æ¯ä¸ºå­•å¦‡æä¾›äº†ä¸€ç§ä¾¿æ·çš„å¥åº·ç›‘æµ‹æ–¹å¼ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¦Šå¨ 18-24å‘¨æœŸé—´é‡‡é›†çš„3Dèº«ä½“æ‰«ææ•°æ®åœ¨é¢„æµ‹ä¸è‰¯å¦Šå¨ ç»“å±€å’Œä¼°è®¡ä¸´åºŠå‚æ•°æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•æœ‰ä¸¤ä¸ªå¹¶è¡Œæµç”¨äºæå–èº«ä½“å½¢æ€ç‰¹å¾ï¼šä¸€ä¸ªç”¨äºç›‘ç£å­¦ä¹ ä»¥æå–è¿ç»­çš„è…¹å›´ä¿¡æ¯ï¼Œå¦ä¸€ä¸ªç”¨äºæ— ç›‘ç£å­¦ä¹ ä»¥æå–å…¨å±€å½¢çŠ¶æè¿°ç¬¦ï¼Œä»¥åŠä¸€ä¸ªç”¨äºäººå£ç»Ÿè®¡å­¦æ•°æ®çš„åˆ†æ”¯ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸‰ç»´èº«ä½“å½¢æ€æœ‰åŠ©äºé¢„æµ‹æ—©äº§ã€å¦Šå¨ æœŸç³–å°¿ç—…ï¼ˆGDMï¼‰ã€å¦Šå¨ æœŸé«˜è¡€å‹ï¼ˆGHï¼‰ï¼Œå¹¶æœ‰åŠ©äºä¼°è®¡èƒå„¿ä½“é‡ã€‚ä¸å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç®—æ³•æ€§èƒ½æœ€ä½³ï¼Œé¢„æµ‹å‡†ç¡®ç‡è¶…è¿‡88%ï¼Œèƒå„¿ä½“é‡ä¼°è®¡è¯¯å·®åœ¨10%ä»¥å†…è¾¾åˆ°76.74%ï¼Œæ¯”ä¼ ç»Ÿäººä½“æµ‹é‡æ–¹æ³•é«˜å‡º22.22%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05627v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨å­•æœŸ18-24å‘¨è·å–çš„3Dèº«ä½“æ‰«ææ•°æ®é¢„æµ‹ä¸è‰¯å¦Šå¨ ç»“å±€å’Œä¼°è®¡ä¸´åºŠå‚æ•°çš„å¯èƒ½æ€§ã€‚å¼€å‘äº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ æå–èº«ä½“å½¢çŠ¶ç‰¹å¾ï¼Œå¹¶è®¾ç«‹åˆ†æ”¯å¤„ç†äººå£ç»Ÿè®¡æ•°æ®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œ3Dèº«ä½“å½¢çŠ¶æœ‰åŠ©äºé¢„æµ‹æ—©äº§ã€å¦Šå¨ æœŸç³–å°¿ç—…å’Œå¦Šå¨ æœŸé«˜è¡€å‹ç­‰ç–¾ç—…ï¼Œå¹¶å‡†ç¡®ä¼°è®¡èƒå„¿ä½“é‡ï¼Œå…¶æ€§èƒ½ä¼˜äºå…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dèº«ä½“æ‰«ææ•°æ®åœ¨é¢„æµ‹ä¸è‰¯å¦Šå¨ ç»“å±€å’Œä¼°è®¡ä¸´åºŠå‚æ•°æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§ç»“åˆç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„ç®—æ³•æ¥æå–èº«ä½“å½¢çŠ¶ç‰¹å¾ã€‚</li>
<li>3Dèº«ä½“å½¢çŠ¶å¯ç”¨äºé¢„æµ‹æ—©äº§ã€å¦Šå¨ æœŸç³–å°¿ç—…å’Œå¦Šå¨ æœŸé«˜è¡€å‹ç­‰ç–¾ç—…ã€‚</li>
<li>ç®—æ³•çš„é¢„æµ‹å‡†ç¡®ç‡è¶…è¿‡88%ï¼Œèƒå„¿ä½“é‡ä¼°è®¡å‡†ç¡®ç‡åœ¨è¯¯å·®èŒƒå›´10%å†…è¾¾åˆ°76.74%ã€‚</li>
<li>ä¸ä¼ ç»Ÿäººç±»å­¦æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•æ€§èƒ½ä¼˜è¶Šï¼Œå‡†ç¡®ç‡æé«˜äº†22.22%ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨æ›´ä¾¿æ·çš„æ–¹å¼ç›‘æµ‹æ¯èƒå¥åº·æä¾›äº†å¯èƒ½æ€§ï¼Œé™ä½äº†ä¸è‰¯å¦Šå¨ ç»“å±€çš„é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fa1b6381704ad90b2500ca44b22302e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91397107a0496e595f3caf97def08823.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5144f5e71a2f21d2857fe3e55b352e54.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Falcon-Fractional-Alternating-Cut-with-Overcoming-Minima-in-Unsupervised-Segmentation"><a href="#Falcon-Fractional-Alternating-Cut-with-Overcoming-Minima-in-Unsupervised-Segmentation" class="headerlink" title="Falcon: Fractional Alternating Cut with Overcoming Minima in   Unsupervised Segmentation"></a>Falcon: Fractional Alternating Cut with Overcoming Minima in   Unsupervised Segmentation</h2><p><strong>Authors:Xiao Zhang, Xiangyu Han, Xiwen Lai, Yao Sun, Pei Zhang, Konrad Kording</strong></p>
<p>Todayâ€™s unsupervised image segmentation algorithms often segment suboptimally. Modern graph-cut based approaches rely on high-dimensional attention maps from Transformer-based foundation models, typically employing a relaxed Normalized Cut solved recursively via the Fiedler vector (the eigenvector of the second smallest eigenvalue). Consequently, they still lag behind supervised methods in both mask generation speed and segmentation accuracy. We present a regularized fractional alternating cut (Falcon), an optimization-based K-way Normalized Cut without relying on recursive eigenvector computations, achieving substantially improved speed and accuracy. Falcon operates in two stages: (1) a fast K-way Normalized Cut solved by extending into a fractional quadratic transformation, with an alternating iterative procedure and regularization to avoid local minima; and (2) refinement of the resulting masks using complementary low-level information, producing high-quality pixel-level segmentations. Experiments show that Falcon not only surpasses existing state-of-the-art methods by an average of 2.5% across six widely recognized benchmarks (reaching up to 4.3% improvement on Cityscapes), but also reduces runtime by around 30% compared to prior graph-based approaches. These findings demonstrate that the semantic information within foundation-model attention can be effectively harnessed by a highly parallelizable graph cut framework. Consequently, Falcon can narrow the gap between unsupervised and supervised segmentation, enhancing scalability in real-world applications and paving the way for dense prediction-based vision pre-training in various downstream tasks. The code is released in <a target="_blank" rel="noopener" href="https://github.com/KordingLab/Falcon">https://github.com/KordingLab/Falcon</a>. </p>
<blockquote>
<p>ä»Šå¤©çš„æ— ç›‘ç£å›¾åƒåˆ†å‰²ç®—æ³•é€šå¸¸åˆ†å‰²æ•ˆæœå¹¶ä¸ç†æƒ³ã€‚ç°ä»£åŸºäºå›¾å‰²çš„æ–¹æ³•ä¾èµ–äºåŸºäºTransformerçš„åŸºç¡€æ¨¡å‹äº§ç”Ÿçš„é«˜ç»´æ³¨æ„åŠ›å›¾ï¼Œé€šå¸¸é‡‡ç”¨æ¾å¼›çš„å½’ä¸€åŒ–åˆ‡å‰²ï¼ˆé€šè¿‡Fiedlerå‘é‡è§£å†³ç‰¹å¾å‘é‡çš„äºŒæ¬¡æœ€å°ç‰¹å¾å€¼ï¼‰ï¼Œä½†å®ƒä»¬åœ¨æ©è†œç”Ÿæˆé€Ÿåº¦å’Œåˆ†å‰²å‡†ç¡®æ€§æ–¹é¢ä»ç„¶è½åäºæœ‰ç›‘ç£æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ­£åˆ™åŒ–åˆ†æ•°äº¤æ›¿åˆ‡å‰²ï¼ˆFalconï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¼˜åŒ–çš„K-æ–¹å¼å½’ä¸€åŒ–åˆ‡å‰²ï¼Œæ— éœ€ä¾èµ–é€’å½’ç‰¹å¾å‘é‡è®¡ç®—ï¼Œå®ç°äº†é€Ÿåº¦å’Œå‡†ç¡®æ€§çš„å®è´¨æ€§æå‡ã€‚Falconåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯é€šè¿‡æ‰©å±•ä¸ºåˆ†æ•°äºŒæ¬¡å˜æ¢å¿«é€Ÿè§£å†³K-æ–¹å¼å½’ä¸€åŒ–åˆ‡å‰²é—®é¢˜ï¼Œé‡‡ç”¨äº¤æ›¿è¿­ä»£ç¨‹åºå’Œæ­£åˆ™åŒ–ä»¥é¿å…å±€éƒ¨æœ€å°å€¼ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯åˆ©ç”¨è¡¥å……çš„åº•å±‚ä¿¡æ¯å¯¹å¾—åˆ°çš„æ©è†œè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œäº§ç”Ÿé«˜è´¨é‡çš„åƒç´ çº§åˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼ŒFalconä¸ä»…åœ¨å…­ä¸ªå¹¿æ³›è®¤å¯çš„åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ˆåœ¨Cityscapesä¸Šæœ€é«˜è¾¾åˆ°4.3%çš„æå‡ï¼‰ï¼Œè€Œä¸”ä¸ä¹‹å‰åŸºäºå›¾çš„çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¿è¡Œæ—¶é—´ç¼©çŸ­äº†çº¦30%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé«˜åº¦å¯å¹¶è¡ŒåŒ–çš„å›¾å‰²æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨åŸºç¡€æ¨¡å‹æ³¨æ„åŠ›ä¸­çš„è¯­ä¹‰ä¿¡æ¯ã€‚å› æ­¤ï¼ŒFalconå¯ä»¥ç¼©å°æ— ç›‘ç£å’Œæœ‰ç›‘ç£åˆ†å‰²ä¹‹é—´çš„å·®è·ï¼Œå¢å¼ºåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¯†é›†é¢„æµ‹è§†è§‰é¢„è®­ç»ƒé“ºå¹³é“è·¯ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/KordingLab/Falcon%E3%80%82">https://github.com/KordingLab/Falconã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05613v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºä¼˜åŒ–çš„K-wayå½’ä¸€åŒ–åˆ‡å‰²æ–¹æ³•â€”â€”Falconï¼Œæ— éœ€ä¾èµ–é€’å½’ç‰¹å¾å‘é‡è®¡ç®—ï¼Œå®ç°äº†å›¾åƒåˆ†å‰²çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§çš„æ˜¾è‘—æå‡ã€‚Falconåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¿«é€ŸK-wayå½’ä¸€åŒ–åˆ‡å‰²å’ŒåŸºäºä½çº§åˆ«ä¿¡æ¯çš„ç²¾ç»†ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFalconåœ¨å…­ä¸ªå¹¿æ³›è®¤å¯çš„åŸºå‡†æµ‹è¯•ä¸­å¹³å‡è¶…è¿‡ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•2.5%ï¼Œå¹¶åœ¨Cityscapesä¸Šæœ€é«˜æå‡è¾¾åˆ°4.3%ï¼ŒåŒæ—¶ä¸å‰ç½®å›¾çš„æ–¹æ³•ç›¸æ¯”è¿è¡Œæ—¶é—´ç¼©çŸ­äº†çº¦30%ã€‚è¯¥ä»£ç å·²å‘å¸ƒåœ¨KordingLab&#x2F;Falconä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°ä»£å›¾åƒåˆ†å‰²ç®—æ³•ä»é¢ä¸´åˆ†å‰²æ•ˆæœæ¬ ä½³çš„é—®é¢˜ï¼Œå­˜åœ¨ä¼˜åŒ–çš„éœ€æ±‚ã€‚</li>
<li>Falconæ˜¯ä¸€ç§ä¼˜åŒ–å‹å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨æå‡é€Ÿåº¦å’Œå‡†ç¡®åº¦ï¼Œæ˜¯é¦–ä¸ªä¸ä½¿ç”¨é€’å½’ç‰¹å¾å‘é‡è®¡ç®—çš„K-wayå½’ä¸€åŒ–åˆ‡å‰²æ–¹æ³•ã€‚</li>
<li>FalconåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå¿«é€ŸK-wayå½’ä¸€åŒ–åˆ‡å‰²å’ŒåŸºäºä½çº§åˆ«ä¿¡æ¯çš„ç²¾ç»†ä¼˜åŒ–ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡åƒç´ çº§åˆ«çš„åˆ†å‰²ç»“æœã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFalconåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨Cityscapesä¸Šçš„æå‡æ˜¾è‘—ã€‚</li>
<li>Falconç¼©å°äº†æ— ç›‘ç£å’Œæœ‰ç›‘ç£å›¾åƒåˆ†å‰²ä¹‹é—´çš„å·®è·ï¼Œå¢å¼ºäº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚è¿™ä¸ºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¯†é›†é¢„æµ‹å¼è§†è§‰é¢„è®­ç»ƒå¼€è¾Ÿäº†é“è·¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a62ec3868ba2d5dff70dcfa2506f5b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89b258407206eeea36d627f55038b3d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0db24625cb3479c817f6bbc871c1f94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14964d4ba3d8d9383f6139c271b90c07.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Class-Imbalance-Correction-for-Improved-Universal-Lesion-Detection-and-Tagging-in-CT"><a href="#Class-Imbalance-Correction-for-Improved-Universal-Lesion-Detection-and-Tagging-in-CT" class="headerlink" title="Class Imbalance Correction for Improved Universal Lesion Detection and   Tagging in CT"></a>Class Imbalance Correction for Improved Universal Lesion Detection and   Tagging in CT</h2><p><strong>Authors:Peter D. Erickson, Tejas Sudharshan Mathai, Ronald M. Summers</strong></p>
<p>Radiologists routinely detect and size lesions in CT to stage cancer and assess tumor burden. To potentially aid their efforts, multiple lesion detection algorithms have been developed with a large public dataset called DeepLesion (32,735 lesions, 32,120 CT slices, 10,594 studies, 4,427 patients, 8 body part labels). However, this dataset contains missing measurements and lesion tags, and exhibits a severe imbalance in the number of lesions per label category. In this work, we utilize a limited subset of DeepLesion (6%, 1331 lesions, 1309 slices) containing lesion annotations and body part label tags to train a VFNet model to detect lesions and tag them. We address the class imbalance by conducting three experiments: 1) Balancing data by the body part labels, 2) Balancing data by the number of lesions per patient, and 3) Balancing data by the lesion size. In contrast to a randomly sampled (unbalanced) data subset, our results indicated that balancing the body part labels always increased sensitivity for lesions &gt;&#x3D; 1cm for classes with low data quantities (Bone: 80% vs. 46%, Kidney: 77% vs. 61%, Soft Tissue: 70% vs. 60%, Pelvis: 83% vs. 76%). Similar trends were seen for three other models tested (FasterRCNN, RetinaNet, FoveaBox). Balancing data by lesion size also helped the VFNet model improve recalls for all classes in contrast to an unbalanced dataset. We also provide a structured reporting guideline for a <code>Lesions&#39;&#39; subsection to be entered into the </code>Findingsâ€™â€™ section of a radiology report. To our knowledge, we are the first to report the class imbalance in DeepLesion, and have taken data-driven steps to address it in the context of joint lesion detection and tagging. </p>
<blockquote>
<p>æ”¾å°„ç§‘åŒ»ç”Ÿé€šå¸¸åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­æ£€æµ‹å’Œå®šä½ç—…ç¶ï¼Œä»¥è¿›è¡Œç™Œç—‡åˆ†æœŸå’Œè‚¿ç˜¤è´Ÿæ‹…è¯„ä¼°ã€‚ä¸ºäº†å¯èƒ½åœ°è¾…åŠ©ä»–ä»¬çš„åŠªåŠ›ï¼Œå·²ç»å¼€å‘å‡ºäº†å¤šç§ç—…ç¶æ£€æµ‹ç®—æ³•ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå¤§å‹å…¬å…±æ•°æ®é›†å«åšDeepLesionï¼ˆåŒ…å«32,735ä¸ªç—…ç¶ã€32,120å¼ CTåˆ‡ç‰‡ã€10,594æ¬¡ç ”ç©¶ã€4,427åæ‚£è€…å’Œ8ä¸ªèº«ä½“éƒ¨ä½æ ‡ç­¾ï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ•°æ®é›†å­˜åœ¨æµ‹é‡ç¼ºå¤±å’Œç—…ç¶æ ‡ç­¾ç¼ºå¤±çš„é—®é¢˜ï¼Œå¹¶ä¸”æ¯ä¸ªæ ‡ç­¾ç±»åˆ«ä¸­çš„ç—…ç¶æ•°é‡å­˜åœ¨ä¸¥é‡ä¸å¹³è¡¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†DeepLesionçš„ä¸€ä¸ªæœ‰é™å­é›†ï¼ˆ6%ï¼Œ1331ä¸ªç—…ç¶ï¼Œ1309å¼ åˆ‡ç‰‡ï¼‰ï¼Œå…¶ä¸­åŒ…å«ç—…ç¶æ³¨é‡Šå’Œèº«ä½“éƒ¨ä½æ ‡ç­¾ï¼Œä»¥è®­ç»ƒVFNetæ¨¡å‹è¿›è¡Œç—…ç¶æ£€æµ‹å’Œæ ‡è®°ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰é¡¹å®éªŒæ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼š1ï¼‰æŒ‰èº«ä½“éƒ¨ä½æ ‡ç­¾å¹³è¡¡æ•°æ®ï¼Œ2ï¼‰æŒ‰æ¯ä¸ªæ‚£è€…çš„ç—…ç¶æ•°é‡å¹³è¡¡æ•°æ®ï¼Œ3ï¼‰æŒ‰ç—…ç¶å¤§å°å¹³è¡¡æ•°æ®ã€‚ä¸éšæœºé‡‡æ ·ï¼ˆä¸å¹³è¡¡ï¼‰çš„æ•°æ®å­é›†ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¹³è¡¡èº«ä½“éƒ¨ä½æ ‡ç­¾å§‹ç»ˆæé«˜äº†å¯¹äºæ•°æ®é‡å°‘ä¸”ç—…ç¶å°ºå¯¸å¤§äºç­‰äº1å˜ç±³çš„ç±»åˆ«çš„æ•æ„Ÿæ€§ï¼ˆå¦‚éª¨ï¼š80ï¼…å¯¹46ï¼…ï¼Œè‚¾è„ï¼š77ï¼…å¯¹61ï¼…ï¼Œè½¯ç»„ç»‡ï¼š70ï¼…å¯¹60ï¼…ï¼Œéª¨ç›†ï¼š83ï¼…å¯¹76ï¼…ï¼‰ã€‚å…¶ä»–ä¸‰ä¸ªç»è¿‡æµ‹è¯•çš„æ¨¡å‹ï¼ˆFasterRCNNã€RetinaNetã€FoveaBoxï¼‰ä¹Ÿå‘ˆç°å‡ºç±»ä¼¼è¶‹åŠ¿ã€‚ä¸ä¸å¹³è¡¡çš„æ•°æ®é›†ç›¸æ¯”ï¼Œé€šè¿‡ç—…ç¶å¤§å°å¹³è¡¡æ•°æ®ä¹Ÿæœ‰åŠ©äºVFNetæ¨¡å‹æé«˜æ‰€æœ‰ç±»åˆ«çš„å¬å›ç‡ã€‚æˆ‘ä»¬è¿˜ä¸ºæ”¾å°„æŠ¥å‘Šâ€œå‘ç°â€éƒ¨åˆ†ä¸­çš„â€œç—…ç¶â€å­éƒ¨åˆ†æä¾›äº†ç»“æ„åŒ–æŠ¥å‘ŠæŒ‡å—ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€æ‰¹æŠ¥å‘ŠDeepLesionä¸­ç±»åˆ«ä¸å¹³è¡¡å¹¶é‡‡å–äº†æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥åº”å¯¹å…¶åœ¨è”åˆç—…ç¶æ£€æµ‹å’Œæ ‡è®°ä¸Šä¸‹æ–‡ä¸­çš„é—®é¢˜çš„å›¢é˜Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05591v1">PDF</a> Published at MICCAI MILLAND Workshop 2022</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨DeepLesionæ•°æ®é›†çš„ä¸€ä¸ªæœ‰é™å­é›†è®­ç»ƒäº†VFNetæ¨¡å‹ï¼Œç”¨äºæ£€æµ‹å¹¶æ ‡æ³¨ç—…å˜ã€‚é’ˆå¯¹æ•°æ®é›†å­˜åœ¨çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡å¹³è¡¡èº«ä½“éƒ¨ä½æ ‡ç­¾ã€æŒ‰æ‚£è€…ç—…å˜æ•°é‡å’Œç—…å˜å¤§å°å¹³è¡¡æ•°æ®ä¸‰ç§å®éªŒæ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹çš„æ•æ„Ÿæ€§ã€‚åŒæ—¶ï¼Œæä¾›äº†ä¸€ç§ç»“æ„åŒ–æŠ¥å‘ŠæŒ‡å—ï¼Œç”¨äºåœ¨æ”¾å°„å­¦æŠ¥å‘Šçš„â€œå‘ç°â€éƒ¨åˆ†æ·»åŠ â€œç—…å˜â€å­éƒ¨åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨DeepLesionæ•°æ®é›†çš„æœ‰é™å­é›†è®­ç»ƒäº†VFNetæ¨¡å‹è¿›è¡Œç—…å˜æ£€æµ‹å’Œæ ‡æ³¨ã€‚</li>
<li>å‘ç°äº†æ•°æ®é›†ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†ä¸‰ç§å®éªŒæ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¹³è¡¡èº«ä½“éƒ¨ä½æ ‡ç­¾çš„æ–¹æ³•ï¼Œå¯¹ä½æ•°æ®é‡çš„ç±»åˆ«ï¼ˆå¦‚éª¨éª¼ã€è‚¾è„ã€è½¯ç»„ç»‡ã€éª¨ç›†ï¼‰æé«˜äº†å¤§äºç­‰äº1å˜ç±³çš„ç—…å˜çš„æ•æ„Ÿæ€§ã€‚</li>
<li>å…¶ä»–ä¸‰ç§æ¨¡å‹ï¼ˆFasterRCNNã€RetinaNetã€FoveaBoxï¼‰ä¹Ÿæ˜¾ç¤ºå‡ºç±»ä¼¼è¶‹åŠ¿ã€‚</li>
<li>é€šè¿‡å¹³è¡¡ç—…å˜å¤§å°çš„æ•°æ®ï¼Œæœ‰åŠ©äºæé«˜æ‰€æœ‰ç±»åˆ«çš„å¬å›ç‡ã€‚</li>
<li>æä¾›äº†ä¸€ç§ç»“æ„åŒ–æŠ¥å‘ŠæŒ‡å—ï¼Œç”¨äºæ”¾å°„æŠ¥å‘Šçš„â€œå‘ç°â€éƒ¨åˆ†ä¸­çš„â€œç—…å˜â€å­éƒ¨åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-989763ade904d6f838d84d18a0b08f0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a26ed03ece60db2e95fe8393bba79ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55efb63b047f8e5e3fc283aea4112409.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Lightweight-Large-Vision-language-Model-for-Multimodal-Medical-Images"><a href="#A-Lightweight-Large-Vision-language-Model-for-Multimodal-Medical-Images" class="headerlink" title="A Lightweight Large Vision-language Model for Multimodal Medical Images"></a>A Lightweight Large Vision-language Model for Multimodal Medical Images</h2><p><strong>Authors:Belal Alsinglawi, Chris McCarthy, Sara Webb, Christopher Fluke, Navid Toosy Saidy</strong></p>
<p>Medical Visual Question Answering (VQA) enhances clinical decision-making by enabling systems to interpret medical images and answer clinical queries. However, developing efficient, high-performance VQA models is challenging due to the complexity of medical imagery and diverse modalities. In this paper, we introduce a lightweight, multimodal VQA model integrating BiomedCLIP for image feature extraction and LLaMA-3 for text processing. Designed for medical VQA tasks, our model achieves state-of-the-art performance on the OmniMedVQA dataset. With approximately 8 billion parameters, it requires only two NVIDIA 40 GB A100 GPUs, demonstrating superior efficiency over larger models. Our results show 73.4% accuracy for open-end questions, surpassing existing models and validating its potential for real-world medical applications. Key contributions include a specialized multimodal VQA model, a resource-efficient architecture, and strong performance in answering open-ended clinical questions. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é€šè¿‡ä½¿ç³»ç»Ÿè§£è¯»åŒ»å­¦å›¾åƒå’Œå›ç­”ä¸´åºŠé—®é¢˜ï¼Œä»è€Œæé«˜äº†ä¸´åºŠå†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºåŒ»å­¦å›¾åƒçš„å¤æ‚æ€§å’Œå¤šç§æ¨¡æ€ï¼Œå¼€å‘é«˜æ•ˆã€é«˜æ€§èƒ½çš„VQAæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªè½»é‡çº§çš„ã€å¤šæ¨¡æ€çš„VQAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é›†æˆäº†BiomedCLIPè¿›è¡Œå›¾åƒç‰¹å¾æå–å’ŒLLaMA-3è¿›è¡Œæ–‡æœ¬å¤„ç†ã€‚è¯¥æ¨¡å‹é’ˆå¯¹åŒ»ç–—VQAä»»åŠ¡è®¾è®¡ï¼Œåœ¨OmniMedVQAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹å¤§çº¦æœ‰8äº¿ä¸ªå‚æ•°ï¼Œä»…éœ€è¦ä¸¤ä¸ªNVIDIA 40GB A100 GPUï¼Œæ˜¾ç¤ºå‡ºæ¯”å¤§å‹æ¨¡å‹æ›´é«˜çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¼€æ”¾æ€§é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†73.4%ï¼Œè¶…è¿‡äº†ç°æœ‰æ¨¡å‹ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨ç°å®ä¸–ç•ŒåŒ»ç–—åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä¸“é—¨çš„å¤šæ¨¡æ€VQAæ¨¡å‹ã€èµ„æºé«˜æ•ˆçš„æ¶æ„ï¼Œä»¥åŠåœ¨å›ç­”å¼€æ”¾çš„ä¸´åºŠé—®é¢˜æ–¹é¢çš„å¼ºåŠ²è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05575v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æŠ€æœ¯åœ¨ä¸´åºŠå†³ç­–ä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å¤šæ¨¡æ€VQAæ¨¡å‹ï¼Œç»“åˆäº†BiomedCLIPè¿›è¡Œå›¾åƒç‰¹å¾æå–å’ŒLLaMA-3è¿›è¡Œæ–‡æœ¬å¤„ç†ã€‚è¯¥æ¨¡å‹é’ˆå¯¹åŒ»å­¦VQAä»»åŠ¡è®¾è®¡ï¼Œåœ¨OmniMedVQAæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä»…éœ€ä¸¤ä¸ªNVIDIA 40 GB A100 GPUï¼Œå‚æ•°çº¦ä¸º8äº¿ï¼Œå±•ç°å‡ºè¾ƒé«˜çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨å¼€æ”¾æ€§é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†73.4%ï¼Œè¶…è¿‡äº†ç°æœ‰æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•ŒåŒ»å­¦åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æŠ€æœ¯èƒ½å¤Ÿæé«˜ä¸´åºŠå†³ç­–æ°´å¹³ï¼Œé€šè¿‡è§£é‡ŠåŒ»å­¦å›¾åƒå’Œå›ç­”ä¸´åºŠé—®é¢˜æ¥å®ç°ã€‚</li>
<li>å¼€å‘é«˜æ•ˆã€é«˜æ€§èƒ½çš„VQAæ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯åŒ»å­¦å›¾åƒçš„å¤æ‚æ€§å’Œå¤šç§æ¨¡æ€ã€‚</li>
<li>ä»‹ç»çš„è½»é‡çº§å¤šæ¨¡æ€VQAæ¨¡å‹ç»“åˆäº†BiomedCLIPè¿›è¡Œå›¾åƒç‰¹å¾æå–å’ŒLLaMA-3è¿›è¡Œæ–‡æœ¬å¤„ç†ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨OmniMedVQAæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‚æ•°è¾ƒå°‘ä¸”è®¡ç®—æ•ˆç‡è¾ƒé«˜ï¼Œåªéœ€ä¸¤ä¸ªNVIDIA 40 GB A100 GPUsã€‚</li>
<li>æ¨¡å‹åœ¨å¼€æ”¾æ€§é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä¸º73.4%ï¼Œè¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰æ½œåŠ›åº”ç”¨äºçœŸå®ä¸–ç•Œçš„åŒ»å­¦åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edda39bf663984d47c8baf1673b21520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82559f256df9fe1daa65aada31baa688.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7401ffe75b075d97229a7a64efe3929d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd54a0b1fca3dd81c0f261016640566d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39aed02e4b1f778523ea2c8cefcae6df.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Core-Excited-States-of-Linear-and-Bent-Uranyl-Complexes-Insights-from-High-Energy-Resolution-X-ray-Spectroscopy-and-Relativistic-Quantum-Chemistry"><a href="#Core-Excited-States-of-Linear-and-Bent-Uranyl-Complexes-Insights-from-High-Energy-Resolution-X-ray-Spectroscopy-and-Relativistic-Quantum-Chemistry" class="headerlink" title="Core-Excited States of Linear and Bent Uranyl Complexes: Insights from   High-Energy Resolution X-ray Spectroscopy and Relativistic Quantum Chemistry"></a>Core-Excited States of Linear and Bent Uranyl Complexes: Insights from   High-Energy Resolution X-ray Spectroscopy and Relativistic Quantum Chemistry</h2><p><strong>Authors:Wilken Aldair Misael, Lucia Amidani, Juliane MÃ¤rz, Elena F. Bazarkina, Kristina O. Kvashnina, ValÃ©rie Vallet, AndrÃ© Severo Pereira Gomes</strong></p>
<p>Advanced X-ray spectroscopic techniques are widely recognized as state-of-the-art tools for probing the electronic structure, bonding, and chemical environments of the heaviest elements in the periodic table. In this study, we employ X-ray absorption near-edge structure measurements in high-energy resolution fluorescence detection (HERFD-XANES) mode to investigate the core states arising from excitations out of the U 3d${_{3&#x2F;2}}$ (M$_4$ edge) levels for molecular complexes in which the uranyl moiety deviates from linearity to varying degrees, and in particular systems containing the UO$_2$Cl$_2$ group such as UO$_2$Cl$_2$.n(H$_2$O) and UO$_2$Cl$_2$(phen)$_2$, which in the latter case exhibits a pronounced O-U-O bending angle. These U M$_4$ edge HERFD-XANES spectra are compared to those of other linear (Cs$_2$UO$_2$Cl$_4$) or pseudo-linear ([UO$_2$(NO$_3$)$_2$.n(H$_2$O)]) uranyl complexes. This evaluation is complemented by ab initio relativistic quantum chemistry simulations using 2-component Time-Dependent Density Functional Theory (TD-DFT) with the CAM-B3LYP functional, employing the Tamm-Dancoff approximation (2c-TDA). Our 2c-TDA simulations show modest deviations from the HERFD-XANES data, with peak splittings differing by less than 1 eV from experimental values. These core-excited states were further characterized by Natural Transition Orbital (NTO) analysis. Overall, our results highlight the influence of equatorial ligands on the spectroscopic signatures, particularly pronounced in UO$_2$Cl$_2$(phen)$<em>2$, where the U 3d${</em>{3&#x2F;2}} \rightarrow$ $5f$ $\sigma{_u}^{*}$ satellite transition appears at lower energies compared to the other systems studied. </p>
<blockquote>
<p>é«˜çº§Xå°„çº¿å…‰è°±æŠ€æœ¯è¢«å…¬è®¤ä¸ºæ˜¯æ¢æµ‹å‘¨æœŸè¡¨ä¸­é‡å…ƒç´ çš„ç”µå­ç»“æ„ã€é”®åˆå’ŒåŒ–å­¦ç¯å¢ƒçš„æœ€æ–°å·¥å…·ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨é«˜èƒ½é‡åˆ†è¾¨ç‡è§å…‰æ£€æµ‹ï¼ˆHERFDï¼‰æ¨¡å¼ä¸‹çš„Xå°„çº¿å¸æ”¶è¿‘è¾¹ç»“æ„æµ‹é‡æŠ€æœ¯ï¼Œç ”ç©¶åç¦»çº¿æ€§çš„é“€é…°åˆ†å­å¤åˆç‰©çš„æ ¸å¿ƒçŠ¶æ€ï¼Œç‰¹åˆ«æ˜¯å«æœ‰UO2Cl2åŸºå›¢çš„ä½“ç³»ï¼Œå¦‚UO2Cl2.nï¼ˆH2Oï¼‰å’ŒUO2Cl2ï¼ˆphenï¼‰2ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„O-U-Oå¼¯æ›²è§’ã€‚æˆ‘ä»¬å°†è¿™äº›UM4è¾¹ç¼˜çš„HERFD-XANESå…‰è°±ä¸å…¶ä»–çº¿æ€§ï¼ˆå¦‚Cs2UO2Cl4ï¼‰æˆ–ä¼ªçº¿æ€§ï¼ˆå¦‚UO2ï¼ˆNO3ï¼‰2.nï¼ˆH2Oï¼‰ï¼‰çš„é“€é…°å¤åˆç‰©çš„å…‰è°±è¿›è¡Œäº†æ¯”è¾ƒã€‚è¿™ä¸€è¯„ä¼°è¾…ä»¥åŸºäºæ—¶é—´çš„ä»å¤´è®¡ç®—ç›¸å¯¹è®ºé‡å­åŒ–å­¦æ¨¡æ‹Ÿï¼Œé‡‡ç”¨å«æ—¶å¯†åº¦æ³›å‡½ç†è®ºï¼ˆTD-DFTï¼‰çš„CAM-B3LYPåŠŸèƒ½ï¼Œé‡‡ç”¨Tamm-Dancoffè¿‘ä¼¼ï¼ˆ2c-TDAï¼‰ã€‚æˆ‘ä»¬çš„2c-TDAæ¨¡æ‹Ÿä¸HERFD-XANESæ•°æ®æœ‰é€‚åº¦åå·®ï¼Œå³°å€¼åˆ†è£‚ä¸å®éªŒå€¼ç›¸å·®ä¸åˆ°1ç”µå­ä¼ç‰¹ã€‚è¿™äº›æ ¸å¿ƒæ¿€å‘æ€è¿›ä¸€æ­¥é€šè¿‡è‡ªç„¶è¿‡æ¸¡è½¨é“ï¼ˆNTOï¼‰åˆ†æè¡¨å¾ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœçªå‡ºäº†èµ¤é“é…ä½“å¯¹å…‰è°±ç‰¹å¾çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨UO2Cl2ï¼ˆphenï¼‰2ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œå…¶ä¸­U 3d 3&#x2F;2 â†’ 5f Ïƒu *å«æ˜Ÿè½¬æ¢å‡ºç°åœ¨æ¯”å…¶ä»–ç³»ç»Ÿæ›´ä½çš„èƒ½é‡å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05542v1">PDF</a> 27 pages, 9 figures, 3 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨é«˜èƒ½é‡åˆ†è¾¨ç‡è§å…‰æ£€æµ‹ï¼ˆHERFD-XANESï¼‰æ¨¡å¼ä¸‹çš„Xå°„çº¿å¸æ”¶è¿‘è¾¹ç»“æ„æµ‹é‡æŠ€æœ¯ï¼Œæ¢ç©¶ä¸åŒç¨‹åº¦åç¦»çº¿æ€§çš„é“€é…°åˆ†å­å¤åˆç‰©çš„æ ¸å¿ƒæ€ï¼Œç‰¹åˆ«æ˜¯å¯¹å«æœ‰UO2Cl2åŸºå›¢çš„ä½“ç³»ï¼Œå¦‚UO2Cl2Â·n(H2O)å’ŒUO2Cl2(phen)2ã€‚é€šè¿‡ä¸çº¿æ€§æˆ–ä¼ªçº¿æ€§é“€é…°å¤åˆç‰©çš„æ¯”è¾ƒï¼Œç»“åˆä»å¤´è®¡ç®—çš„ç›¸å¯¹è®ºé‡å­åŒ–å­¦æ¨¡æ‹Ÿï¼Œæœ¬ç ”ç©¶æ­ç¤ºäº†èµ¤é“é…ä½“å¯¹å…‰è°±ç‰¹å¾çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨UO2Cl2(phen)2ä½“ç³»ä¸­ï¼ŒU 3d{3&#x2F;2}â†’$5f$Ïƒu*å«æ˜Ÿè¿‡æ¸¡åœ¨è¾ƒä½èƒ½é‡å¤„å‡ºç°ä¸å…¶ä»–ç³»ç»Ÿç›¸æ¯”æ›´ä¸ºæ˜æ˜¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨HERFD-XANESæ¨¡å¼æ¢ç©¶äº†ä¸åŒç¨‹åº¦åç¦»çº¿æ€§çš„é“€é…°åˆ†å­å¤åˆç‰©çš„æ ¸å¿ƒæ€ã€‚</li>
<li>UO2Cl2åŠå…¶ç›¸å…³å¤åˆç‰©ä½“ç³»çš„å…‰è°±ç‰¹æ€§è¢«è¯¦ç»†ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å«æœ‰å¼¯æ›²O-U-Oè§’çš„UO2Cl2(phen)2ä½“ç³»ã€‚</li>
<li>é€šè¿‡ä¸å…¶ä»–çº¿æ€§æˆ–ä¼ªçº¿æ€§é“€é…°å¤åˆç‰©çš„æ¯”è¾ƒï¼Œæ­ç¤ºäº†èµ¤é“é…ä½“å¯¹å…‰è°±ç‰¹å¾çš„é‡è¦å½±å“ã€‚</li>
<li>ä½¿ç”¨äº†ä»å¤´è®¡ç®—çš„ç›¸å¯¹è®ºé‡å­åŒ–å­¦æ¨¡æ‹Ÿæ¥è¾…åŠ©åˆ†æå’Œè§£é‡Šå®éªŒç»“æœã€‚</li>
<li>2c-TDAæ¨¡æ‹Ÿä¸HERFD-XANESæ•°æ®æœ‰é€‚åº¦åå·®ï¼Œå³°å€¼åˆ†è£‚ä¸å®éªŒå€¼å·®å¼‚å°äº1ç”µå­ä¼ç‰¹ã€‚</li>
<li>è‡ªç„¶è·ƒè¿è½¨é“ï¼ˆNTOï¼‰åˆ†æè¿›ä¸€æ­¥è¡¨å¾äº†æ ¸å¿ƒæ¿€å‘æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-829922004f0f93bcb96c3e1cd289e067.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Studying-Image-Diffusion-Features-for-Zero-Shot-Video-Object-Segmentation"><a href="#Studying-Image-Diffusion-Features-for-Zero-Shot-Video-Object-Segmentation" class="headerlink" title="Studying Image Diffusion Features for Zero-Shot Video Object   Segmentation"></a>Studying Image Diffusion Features for Zero-Shot Video Object   Segmentation</h2><p><strong>Authors:Thanos Delatolas, Vicky Kalogeiton, Dim P. Papadopoulos</strong></p>
<p>This paper investigates the use of large-scale diffusion models for Zero-Shot Video Object Segmentation (ZS-VOS) without fine-tuning on video data or training on any image segmentation data. While diffusion models have demonstrated strong visual representations across various tasks, their direct application to ZS-VOS remains underexplored. Our goal is to find the optimal feature extraction process for ZS-VOS by identifying the most suitable time step and layer from which to extract features. We further analyze the affinity of these features and observe a strong correlation with point correspondences. Through extensive experiments on DAVIS-17 and MOSE, we find that diffusion models trained on ImageNet outperform those trained on larger, more diverse datasets for ZS-VOS. Additionally, we highlight the importance of point correspondences in achieving high segmentation accuracy, and we yield state-of-the-art results in ZS-VOS. Finally, our approach performs on par with models trained on expensive image segmentation datasets. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹åœ¨æ— å¾®è°ƒè§†é¢‘æ•°æ®æˆ–ä»»ä½•å›¾åƒåˆ†å‰²æ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç”¨äºé›¶æ ·æœ¬è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆZS-VOSï¼‰çš„åº”ç”¨ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ZS-VOSçš„ç›´æ¥åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ZS-VOSçš„æœ€ä½³ç‰¹å¾æå–è¿‡ç¨‹ï¼Œé€šè¿‡ç¡®å®šæœ€åˆé€‚çš„æ—¶é—´æ­¥é•¿å’Œå±‚æ¥æå–ç‰¹å¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†è¿™äº›ç‰¹å¾çš„äº²å’ŒåŠ›ï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶ä¸ç‚¹å¯¹åº”å…³ç³»çš„å¼ºçƒˆç›¸å…³æ€§ã€‚åœ¨DAVIS-17å’ŒMOSEçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ImageNetä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨ZS-VOSæ–¹é¢çš„è¡¨ç°ä¼˜äºåœ¨æ›´å¤§ã€æ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼ºè°ƒäº†å®ç°é«˜åˆ†å‰²å‡†ç¡®ç‡çš„ç‚¹å¯¹åº”å…³ç³»çš„é‡è¦æ€§ï¼Œå¹¶åœ¨ZS-VOSä¸­å–å¾—äº†æœ€æ–°ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸åœ¨æ˜‚è´µçš„å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05468v1">PDF</a> Accepted to CVPRW2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆZS-VOSï¼‰ä¸­çš„åº”ç”¨ï¼Œæ— éœ€å¯¹è§†é¢‘æ•°æ®è¿›è¡Œå¾®è°ƒæˆ–ä»»ä½•å›¾åƒåˆ†å‰²æ•°æ®çš„è®­ç»ƒã€‚ç ”ç©¶ç›®æ ‡æ˜¯æ‰¾åˆ°é€‚ç”¨äºZS-VOSçš„æœ€ä½³ç‰¹å¾æå–è¿‡ç¨‹ï¼Œå¹¶è§‚å¯Ÿåˆ°ç‰¹å¾ä¹‹é—´çš„äº²å’ŒåŠ›ä¸ç‚¹å¯¹åº”å…³ç³»ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨DAVIS-17å’ŒMOSEä¸Šï¼Œä½¿ç”¨ImageNetè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨ZS-VOSä¸Šçš„è¡¨ç°ä¼˜äºåœ¨æ›´å¤§ã€æ›´å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚åŒæ—¶ï¼Œç‚¹å¯¹åº”å…³ç³»å¯¹äºå®ç°é«˜åˆ†å‰²ç²¾åº¦è‡³å…³é‡è¦ï¼Œå¹¶ä¸”æœ¬æ–‡æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ZS-VOSç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¢«åº”ç”¨äºé›¶æ ·æœ¬è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆZS-VOSï¼‰ï¼Œæ— éœ€å¯¹è§†é¢‘æ•°æ®è¿›è¡Œå¾®è°ƒæˆ–è®­ç»ƒå›¾åƒåˆ†å‰²æ•°æ®ã€‚</li>
<li>ç ”ç©¶æ‰¾åˆ°é€‚ç”¨äºZS-VOSçš„æœ€ä½³ç‰¹å¾æå–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€‰æ‹©é€‚å½“çš„æ—¶é—´æ­¥é•¿å’Œå±‚æ¥æå–ç‰¹å¾ã€‚</li>
<li>è§‚å¯Ÿåˆ°æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾ä¸ç‚¹å¯¹åº”å…³ç³»ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚</li>
<li>åœ¨DAVIS-17å’ŒMOSEä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ImageNetè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨ZS-VOSä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ç‚¹å¯¹åº”å…³ç³»å¯¹äºå®ç°é«˜åˆ†å‰²ç²¾åº¦è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ZS-VOSç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d016d82726907510a3ef651b318f3db2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e2578f0da1ebfdcbf9dc4510c57a91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7866760aa12e58057dcc885e7691de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36fcb105bb87e0fab00abc0e7d99eaad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1bb14e4e9837e168f6f1b48ee00ec6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Biomechanical-Constraints-Assimilation-in-Deep-Learning-Image-Registration-Application-to-sliding-and-locally-rigid-deformations"><a href="#Biomechanical-Constraints-Assimilation-in-Deep-Learning-Image-Registration-Application-to-sliding-and-locally-rigid-deformations" class="headerlink" title="Biomechanical Constraints Assimilation in Deep-Learning Image   Registration: Application to sliding and locally rigid deformations"></a>Biomechanical Constraints Assimilation in Deep-Learning Image   Registration: Application to sliding and locally rigid deformations</h2><p><strong>Authors:Ziad Kheil, Soleakhena Ken, Laurent Risser</strong></p>
<p>Regularization strategies in medical image registration often take a one-size-fits-all approach by imposing uniform constraints across the entire image domain. Yet biological structures are anything but regular. Lacking structural awareness, these strategies may fail to consider a panoply of spatially inhomogeneous deformation properties, which would faithfully account for the biomechanics of soft and hard tissues, especially in poorly contrasted structures.   To bridge this gap, we propose a learning-based image registration approach in which the inferred deformation properties can locally adapt themselves to trained biomechanical characteristics. Specifically, we first enforce in the training process local rigid displacements, shearing motions or pseudo-elastic deformations using regularization losses inspired from the field of solid-mechanics. We then show on synthetic and real 3D thoracic and abdominal images that these mechanical properties of different nature are well generalized when inferring the deformations between new image pairs. Our approach enables neural-networks to infer tissue-specific deformation patterns directly from input images, ensuring mechanically plausible motion. These networks preserve rigidity within hard tissues while allowing controlled sliding in regions where tissues naturally separate, more faithfully capturing physiological motion. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Kheil-Z/biomechanical_DLIR">https://github.com/Kheil-Z/biomechanical_DLIR</a> . </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒé…å‡†ä¸­çš„æ­£åˆ™åŒ–ç­–ç•¥é€šå¸¸é‡‡ç”¨ä¸€åˆ€åˆ‡çš„æ–¹æ³•ï¼Œå¯¹æ•´ä¸ªå›¾åƒåŸŸæ–½åŠ ç»Ÿä¸€çš„çº¦æŸã€‚ç„¶è€Œï¼Œç”Ÿç‰©ç»“æ„å´è¿œéå¦‚æ­¤ã€‚ç”±äºç¼ºä¹ç»“æ„æ„è¯†ï¼Œè¿™äº›ç­–ç•¥å¯èƒ½æ— æ³•è€ƒè™‘åˆ°ä¸€ç³»åˆ—ç©ºé—´éå‡åŒ€å˜å½¢ç‰¹æ€§ï¼Œæ— æ³•çœŸå®åæ˜ è½¯ç»„ç»‡å’Œç¡¬ç»„ç»‡çš„ç”Ÿç‰©åŠ›å­¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹æ¯”åº¦è¾ƒå·®çš„ç»“æ„ä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„å›¾åƒé…å‡†æ–¹æ³•ï¼Œå…¶ä¸­æ¨æ–­çš„å˜å½¢ç‰¹æ€§å¯ä»¥å±€éƒ¨é€‚åº”äºè®­ç»ƒå¾—åˆ°çš„ç”Ÿç‰©åŠ›å­¦ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ä»å›ºä½“åŠ›å­¦é¢†åŸŸè·å¾—å¯å‘çš„æ­£åˆ™åŒ–æŸå¤±ï¼Œå¼ºåˆ¶å®æ–½å±€éƒ¨åˆšæ€§ä½ç§»ã€å‰ªåˆ‡è¿åŠ¨æˆ–ä¼ªå¼¹æ€§å˜å½¢ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®çš„3Dèƒ¸éƒ¨å’Œè…¹éƒ¨å›¾åƒä¸Šå±•ç¤ºï¼Œå½“æ¨æ–­æ–°å›¾åƒå¯¹ä¹‹é—´çš„å˜å½¢æ—¶ï¼Œè¿™äº›ä¸åŒæ€§è´¨çš„æœºæ¢°ç‰¹æ€§å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿç›´æ¥ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­å‡ºç»„ç»‡ç‰¹å®šçš„å˜å½¢æ¨¡å¼ï¼Œç¡®ä¿æœºæ¢°ä¸Šå¯è¡Œçš„è¿åŠ¨ã€‚è¿™äº›ç½‘ç»œåœ¨ç¡¬ç»„ç»‡å†…ä¿æŒåˆšæ€§ï¼ŒåŒæ—¶åœ¨ç»„ç»‡è‡ªç„¶åˆ†ç¦»çš„åŒºåŸŸå…è®¸å—æ§æ»‘åŠ¨ï¼Œæ›´çœŸå®åœ°æ•æ‰ç”Ÿç†è¿åŠ¨ã€‚ä»£ç å…¬å¼€å¯ç”¨åœ¨ <a target="_blank" rel="noopener" href="https://github.com/Kheil-Z/biomechanical_DLIR%E3%80%82">https://github.com/Kheil-Z/biomechanical_DLIRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå­¦ä¹ çš„åŒ»å­¦å›¾åƒæ³¨å†Œæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå±€éƒ¨é€‚åº”è®­ç»ƒå¾—åˆ°çš„ç”Ÿç‰©åŠ›å­¦ç‰¹æ€§ï¼Œä»¥å¼¥è¡¥ä¼ ç»Ÿæ³¨å†Œç­–ç•¥åœ¨åº”å¯¹ç”Ÿç‰©ç»“æ„å¤æ‚æ€§æ—¶çš„ä¸è¶³ã€‚é€šè¿‡å¼•å…¥å›ºä½“åŠ›å­¦é¢†åŸŸçš„æ­£åˆ™åŒ–æŸå¤±ï¼Œè¯¥æ–¹æ³•å¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ–½åŠ å±€éƒ¨åˆšæ€§ä½ç§»ã€å‰ªåˆ‡è¿åŠ¨æˆ–ä¼ªå¼¹æ€§å˜å½¢ã€‚åœ¨æ–°å›¾åƒå¯¹çš„å˜å½¢æ¨æ–­ä¸­ï¼Œä¸åŒæ€§è´¨çš„æœºæ¢°ç‰¹æ€§å¾—åˆ°è‰¯å¥½æ³›åŒ–ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿç›´æ¥ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­å‡ºç»„ç»‡ç‰¹å®šçš„å˜å½¢æ¨¡å¼ï¼Œç¡®ä¿æœºæ¢°è¿åŠ¨çš„å¯ä¿¡æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸåŒ»å­¦å›¾åƒæ³¨å†Œç­–ç•¥å¸¸é‡‡ç”¨å…¨å±€ç»Ÿä¸€çº¦æŸï¼Œç¼ºä¹ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>ç”Ÿç‰©ç»“æ„å…·æœ‰ç©ºé—´éå‡åŒ€å˜å½¢ç‰¹æ€§ï¼Œéœ€è€ƒè™‘è½¯ç»„ç»‡å’Œç¡¬ç»„ç»‡çš„ç”Ÿç‰©åŠ›å­¦ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå­¦ä¹ çš„å›¾åƒæ³¨å†Œæ–¹æ³•ï¼Œå¯å±€éƒ¨é€‚åº”ç”Ÿç‰©åŠ›å­¦ç‰¹æ€§ã€‚</li>
<li>å¼•å…¥å›ºä½“åŠ›å­¦é¢†åŸŸçš„æ­£åˆ™åŒ–æŸå¤±ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å±€éƒ¨å˜å½¢çº¦æŸã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®çš„3Dèƒ¸è…¹éƒ¨å›¾åƒä¸ŠéªŒè¯ï¼Œä¸åŒæ€§è´¨çš„æœºæ¢°ç‰¹æ€§åœ¨æ¨æ–­æ–°å›¾åƒå¯¹å˜å½¢æ—¶å…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç¥ç»ç½‘ç»œèƒ½å¤Ÿç›´æ¥ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­å‡ºç»„ç»‡ç‰¹å®šçš„å˜å½¢æ¨¡å¼ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿è¯ç¡¬ç»„ç»‡åˆšæ€§çš„åŒæ—¶ï¼Œå…è®¸åœ¨å¤©ç„¶ç»„ç»‡åˆ†ç¦»åŒºåŸŸè¿›è¡Œå—æ§æ»‘åŠ¨ï¼Œæ›´çœŸå®åœ°æ•æ‰ç”Ÿç†è¿åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dfe91cd7ea02b0c2e19781c7c32e39b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d490191eab0b5c160e8d77fd8d6847a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6551bb4952e77b0d6a75680b6f1ee526.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-226ff9b5e354041a1af06c2fd88b64e8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Multi-Scale-Feature-Fusion-Framework-Integrating-Frequency-Domain-and-Cross-View-Attention-for-Dual-View-X-ray-Security-Inspections"><a href="#A-Multi-Scale-Feature-Fusion-Framework-Integrating-Frequency-Domain-and-Cross-View-Attention-for-Dual-View-X-ray-Security-Inspections" class="headerlink" title="A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and   Cross-View Attention for Dual-View X-ray Security Inspections"></a>A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and   Cross-View Attention for Dual-View X-ray Security Inspections</h2><p><strong>Authors:Shilong Hong, Yanzhou Zhou, Weichao Xu</strong></p>
<p>With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray equipment is widely deployed, it struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose an innovative multi-scale interactive feature fusion framework tailored for dual-view X-ray security inspection image classification. The framework comprises three core modules: the Frequency Domain Interaction Module (FDIM) enhances frequency-domain features through Fourier transform; the Multi-Scale Cross-View Feature Enhancement (MSCFE) leverages cross-view attention mechanisms to strengthen feature interactions; and the Convolutional Attention Fusion Module (CAFM) efficiently fuses features by integrating channel attention with depthwise-separable convolutions. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches across multiple backbone architectures, particularly excelling in complex scenarios with occlusions and object stacking. </p>
<blockquote>
<p>éšç€ç°ä»£äº¤é€šè¿è¾“ç³»ç»Ÿçš„å¿«é€Ÿå‘å±•å’Œç‰©æµé‡çš„æŒ‡æ•°çº§å¢é•¿ï¼ŒåŸºäºæ™ºèƒ½Xå°„çº¿çš„å®‰å…¨æ£€æŸ¥ç³»ç»Ÿåœ¨å…¬å…±å®‰å…¨ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚è™½ç„¶å•è§†å›¾Xå°„çº¿è®¾å¤‡å·²å¹¿æ³›éƒ¨ç½²ï¼Œä½†ç”±äºå…¶å¼ºçƒˆçš„è§†è§’ä¾èµ–æ€§å’Œç‰¹å¾è¡¨ç¤ºçš„ä¸è¶³ï¼Œå®ƒåœ¨å¤æ‚çš„å †å åœºæ™¯ä¸­éš¾ä»¥å‡†ç¡®è¯†åˆ«è¿ç¦å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒè§†å›¾Xå°„çº¿å®‰å…¨æ£€æŸ¥å›¾åƒåˆ†ç±»çš„å¤šå°ºåº¦äº¤äº’ç‰¹å¾èåˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šé¢‘åŸŸäº¤äº’æ¨¡å—ï¼ˆFDIMï¼‰é€šè¿‡å‚…é‡Œå¶å˜æ¢å¢å¼ºé¢‘åŸŸç‰¹å¾ï¼›å¤šå°ºåº¦è·¨è§†å›¾ç‰¹å¾å¢å¼ºï¼ˆMSCFEï¼‰åˆ©ç”¨è·¨è§†å›¾æ³¨æ„åŠ›æœºåˆ¶åŠ å¼ºç‰¹å¾äº¤äº’ï¼›å·ç§¯æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆCAFMï¼‰é€šè¿‡ç»“åˆé€šé“æ³¨æ„åŠ›å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¥æœ‰æ•ˆåœ°èåˆç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ä¸»å¹²æ¶æ„ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œå°¤å…¶åœ¨å­˜åœ¨é®æŒ¡å’Œç‰©ä½“å †å çš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01710v3">PDF</a> I did not obtain permission from the other authors, especially the   corresponding author, to submit this manuscript, so I respectfully request   its withdrawal</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹åŒè§†è§’Xå…‰å®‰æ£€å›¾åƒåˆ†ç±»çš„å¤šå°ºåº¦äº¤äº’ç‰¹å¾èåˆæ¡†æ¶ï¼Œä»¥è§£å†³å•è§†è§’Xå…‰è®¾å¤‡åœ¨å¤æ‚å †å åœºæ™¯ä¸­éš¾ä»¥å‡†ç¡®è¯†åˆ«è¿ç¦å“çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šé¢‘ç‡åŸŸäº¤äº’æ¨¡å—ï¼ˆFDIMï¼‰ã€å¤šå°ºåº¦è·¨è§†å›¾ç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆMSCFEï¼‰å’Œå·ç§¯æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆCAFMï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸»å¹²æ¶æ„ä¸Šä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå°¤å…¶åœ¨é®æŒ¡å’Œå¯¹è±¡å †å çš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¬å…±äº¤é€šå®‰æ£€ä¸­ï¼Œæ™ºèƒ½Xå…‰å®‰æ£€ç³»ç»Ÿå‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
<li>å•è§†è§’Xå…‰è®¾å¤‡åœ¨å¤æ‚å †å åœºæ™¯ä¸‹è¯†åˆ«è¿ç¦å“å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºäº†å¤šå°ºåº¦äº¤äº’ç‰¹å¾èåˆæ¡†æ¶ï¼ŒåŒ…æ‹¬FDIMã€MSCFEå’ŒCAFMä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚</li>
<li>FDIMé€šè¿‡å‚…é‡Œå¶å˜æ¢å¢å¼ºé¢‘ç‡åŸŸç‰¹å¾ã€‚</li>
<li>MSCFEåˆ©ç”¨è·¨è§†å›¾æ³¨æ„åŠ›æœºåˆ¶åŠ å¼ºç‰¹å¾äº¤äº’ã€‚</li>
<li>CAFMé€šè¿‡ç»“åˆé€šé“æ³¨æ„åŠ›å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¥æœ‰æ•ˆèåˆç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5eb516231478871f03084ca3ec88e0f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75099c9c1686c1d9aedb2081cf14d487.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-242ae69af9df44e8bfdbbf59fac7af91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a96735b420779f155f9b37050f06327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5593da8868927c82e7fb7429a2276bd6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Transfer-Learning-Strategies-for-Pathological-Foundation-Models-A-Systematic-Evaluation-in-Brain-Tumor-Classification"><a href="#Transfer-Learning-Strategies-for-Pathological-Foundation-Models-A-Systematic-Evaluation-in-Brain-Tumor-Classification" class="headerlink" title="Transfer Learning Strategies for Pathological Foundation Models: A   Systematic Evaluation in Brain Tumor Classification"></a>Transfer Learning Strategies for Pathological Foundation Models: A   Systematic Evaluation in Brain Tumor Classification</h2><p><strong>Authors:Ken Enda, Yoshitaka Oda, Zen-ichi Tanei, Kenichi Satoh, Hiroaki Motegi, Terasaka Shunsuke, Shigeru Yamaguchi, Takahiro Ogawa, Wang Lei, Masumi Tsuda, Shinya Tanaka</strong></p>
<p>Foundation models pretrained on large-scale pathology datasets have shown promising results across various diagnostic tasks. Here, we present a systematic evaluation of transfer learning strategies for brain tumor classification using these models. We analyzed 254 cases comprising five major tumor types: glioblastoma, astrocytoma, oligodendroglioma, primary central nervous system lymphoma, and metastatic tumors. Comparing state-of-the-art foundation models with conventional approaches, we found that foundation models demonstrated robust classification performance with as few as 10 patches per case, despite the traditional assumption that extensive per-case image sampling is necessary. Furthermore, our evaluation revealed that simple transfer learning strategies like linear probing were sufficient, while fine-tuning often degraded model performance. These findings suggest a paradigm shift from â€œtraining encoders on extensive pathological dataâ€ to â€œquerying pre-trained encoders with labeled datasetsâ€, providing practical implications for implementing AI-assisted diagnosis in clinical pathology. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡ç—…ç†å­¦æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹ï¼ˆfoundation modelsï¼‰åœ¨å„ç§è¯Šæ–­ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æœ‰å‰æ™¯çš„ç»“æœã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬é’ˆå¯¹ä½¿ç”¨è¿™äº›æ¨¡å‹çš„è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ï¼Œå¯¹è¿ç§»å­¦ä¹ ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿçš„è¯„ä¼°ã€‚æˆ‘ä»¬åˆ†æäº†åŒ…å«äº”ç§ä¸»è¦è‚¿ç˜¤ç±»å‹çš„254ä¸ªç—…ä¾‹ï¼šèƒ¶è´¨æ¯ç»†èƒç˜¤ã€æ˜Ÿå½¢ç»†èƒç˜¤ã€å°‘æèƒ¶è´¨ç»†èƒç˜¤ã€åŸå‘æ€§ä¸­æ¢ç¥ç»ç³»ç»Ÿæ·‹å·´ç˜¤å’Œè½¬ç§»æ€§è‚¿ç˜¤ã€‚å°†æœ€å‰æ²¿çš„åŸºçŸ³æ¨¡å‹ä¸ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå¯¹æ¯”ï¼Œæˆ‘ä»¬å‘ç°åŸºçŸ³æ¨¡å‹åœ¨ä»…å¯¹æ¯ä¸ªç—…ä¾‹ä½¿ç”¨å°‘é‡ï¼ˆå¦‚æ¯ä¸ªç—…ä¾‹ä»…ä½¿ç”¨10ä¸ªæ ·æœ¬ï¼‰çš„æƒ…å†µä¸‹ï¼Œå°±è¡¨ç°å‡ºäº†ç¨³å¥çš„åˆ†ç±»æ€§èƒ½ï¼Œå°½ç®¡ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºæ¯ä¸ªç—…ä¾‹éƒ½éœ€è¦è¿›è¡Œå¤§é‡çš„å›¾åƒé‡‡æ ·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è¯„ä¼°è¿˜è¡¨æ˜ï¼Œç®€å•çš„è¿ç§»å­¦ä¹ ç­–ç•¥ï¼ˆå¦‚çº¿æ€§æ¢æµ‹ï¼‰å°±å·²è¶³å¤Ÿæœ‰æ•ˆï¼Œè€Œå¾®è°ƒå¾€å¾€ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›å‘ç°æ ‡å¿—ç€ä»â€œåœ¨å¤§é‡ç—…ç†å­¦æ•°æ®ä¸Šè®­ç»ƒç¼–ç å™¨â€è½¬å‘â€œç”¨å¸¦æ ‡ç­¾çš„æ•°æ®é›†æŸ¥è¯¢é¢„è®­ç»ƒç¼–ç å™¨â€ï¼Œè¿™ä¸ºåœ¨ä¸´åºŠç—…ç†å­¦ä¸­å®ç°äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­æä¾›äº†å®é™…å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11014v2">PDF</a> 25 pages, 7 figures</p>
<p><strong>æ€»ç»“</strong></p>
<p>é¢„è®­ç»ƒçš„å¤§å‹ç—…ç†å­¦æ•°æ®é›†ä¸Šçš„åŸºç¡€æ¨¡å‹åœ¨å¤šç§è¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç»“æœã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†ç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»çš„åŸºç¡€æ¨¡å‹çš„è¿ç§»å­¦ä¹ ç­–ç•¥ã€‚æˆ‘ä»¬åˆ†æäº†åŒ…å«äº”ç§ä¸»è¦è‚¿ç˜¤ç±»å‹çš„254ä¸ªç—…ä¾‹ï¼šèƒ¶è´¨æ¯ç»†èƒç˜¤ã€æ˜Ÿå½¢ç»†èƒç˜¤ã€å°‘çªèƒ¶è´¨ç»†èƒç˜¤ã€åŸå‘æ€§ä¸­æ¢ç¥ç»ç³»ç»Ÿæ·‹å·´ç˜¤å’Œè½¬ç§»æ€§è‚¿ç˜¤ã€‚é€šè¿‡å¯¹æ¯”æœ€å‰æ²¿çš„åŸºç¡€æ¨¡å‹ä¸ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°åŸºç¡€æ¨¡å‹åœ¨ä»…ä½¿ç”¨æ¯ç—…ä¾‹10ä¸ªè¡¥ä¸çš„æƒ…å†µä¸‹å°±è¡¨ç°å‡ºäº†å¼ºå¤§çš„åˆ†ç±»æ€§èƒ½ï¼Œå°½ç®¡ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºæ¯ä¸ªç—…ä¾‹çš„å›¾åƒé‡‡æ ·éœ€è¦å¹¿æ³›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ç®€å•çš„è¿ç§»å­¦ä¹ ç­–ç•¥å¦‚çº¿æ€§æ¢æµ‹å°±å·²è¶³å¤Ÿï¼Œè€Œå¾®è°ƒå¾€å¾€ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ä»â€œåœ¨å¤§é‡ç—…ç†æ•°æ®ä¸Šè®­ç»ƒç¼–ç å™¨â€è½¬å‘â€œç”¨å¸¦æ ‡ç­¾çš„æ•°æ®é›†æŸ¥è¯¢é¢„è®­ç»ƒç¼–ç å™¨â€ï¼Œä¸ºåœ¨ä¸´åºŠç—…ç†å­¦ä¸­å®ç°äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­æä¾›äº†å®é™…å¯ç¤ºã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹åœ¨ç—…ç†å­¦è¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”äº†å‰æ²¿çš„åŸºç¡€æ¨¡å‹ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ŒåŸºç¡€æ¨¡å‹åœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åœ¨å°‘é‡å›¾åƒæ ·æœ¬ä¸‹ä¾ç„¶è¡¨ç°å‡ºè‰¯å¥½çš„åˆ†ç±»æ€§èƒ½ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿè§‚å¿µã€‚</li>
<li>ç®€å•çš„è¿ç§»å­¦ä¹ ç­–ç•¥å¦‚çº¿æ€§æ¢æµ‹å·²ç»è¶³å¤Ÿæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¿ç§»å­¦ä¹ ç­–ç•¥çš„è¯„ä¼°ç»“æœè¡¨æ˜å¾®è°ƒå¹¶ä¸æ€»æ˜¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åŸºç¡€æ¨¡å‹çš„è¯„ä¼°ç»“æœå¯¹åŒ»å­¦å›¾åƒè¯Šæ–­ä¸­çš„AIåº”ç”¨å…·æœ‰å®é™…æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c956b42591620b0962a63f2358ddca4e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="How-Well-Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments"><a href="#How-Well-Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments" class="headerlink" title="How Well Can Modern LLMs Act as Agent Cores in Radiology Environments?"></a>How Well Can Modern LLMs Act as Agent Cores in Radiology Environments?</h2><p><strong>Authors:Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>We introduce RadA-BenchPlat, an evaluation platform that benchmarks the performance of large language models (LLMs) act as agent cores in radiology environments using 2,200 radiologist-verified synthetic patient records covering six anatomical regions, five imaging modalities, and 2,200 disease scenarios, resulting in 24,200 question-answer pairs that simulate diverse clinical situations. The platform also defines ten categories of tools for agent-driven task solving and evaluates seven leading LLMs, revealing that while models like Claude-3.7-Sonnet can achieve a 67.1% task completion rate in routine settings, they still struggle with complex task understanding and tool coordination, limiting their capacity to serve as the central core of automated radiology systems. By incorporating four advanced prompt engineering strategiesâ€“where prompt-backpropagation and multi-agent collaboration contributed 16.8% and 30.7% improvements, respectivelyâ€“the performance for complex tasks was enhanced by 48.2% overall. Furthermore, automated tool building was explored to improve robustness, achieving a 65.4% success rate, thereby offering promising insights for the future integration of fully automated radiology applications into clinical practice. All of our code and data are openly available at <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench">https://github.com/MAGIC-AI4Med/RadABench</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†RadA-BenchPlatè¯„ä¼°å¹³å°ï¼Œè¯¥å¹³å°ä½¿ç”¨ç»è¿‡æ”¾å°„ç§‘åŒ»ç”ŸéªŒè¯çš„2200ä»½åˆæˆæ‚£è€…è®°å½•ï¼Œæ¶µç›–å…­ä¸ªè§£å‰–åŒºåŸŸã€äº”ç§æˆåƒæ–¹å¼å’Œ2200ç§ç–¾ç—…åœºæ™¯ï¼Œç”Ÿæˆäº†æ¨¡æ‹Ÿå¤šç§ä¸´åºŠæƒ…å¢ƒçš„24,200ä¸ªé—®ç­”å¯¹ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­ä½œä¸ºä»£ç†æ ¸å¿ƒçš„æ€§èƒ½ã€‚è¯¥å¹³å°è¿˜å®šä¹‰äº†åç±»å·¥å…·ç”¨äºä»£ç†é©±åŠ¨çš„ä»»åŠ¡è§£å†³ï¼Œå¹¶è¯„ä¼°äº†ä¸ƒæ¬¾é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶åƒClaude-3.7-Sonnetè¿™æ ·çš„æ¨¡å‹åœ¨æ—¥å¸¸ç¯å¢ƒä¸­çš„ä»»åŠ¡å®Œæˆç‡å¯ä»¥è¾¾åˆ°67.1%ï¼Œä½†åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ç†è§£å’Œå·¥å…·åè°ƒæ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œéš¾ä»¥ä½œä¸ºè‡ªåŠ¨åŒ–æ”¾å°„ç³»ç»Ÿçš„æ ¸å¿ƒã€‚é€šè¿‡é‡‡ç”¨å››ç§å…ˆè¿›çš„æç¤ºå·¥ç¨‹ç­–ç•¥â€”â€”å…¶ä¸­æç¤ºåå‘ä¼ æ’­å’Œå¤šä»£ç†åä½œåˆ†åˆ«è´¡çŒ®äº†16.8%å’Œ30.7%çš„æ”¹è¿›â€”â€”å¤æ‚ä»»åŠ¡çš„æ€§èƒ½æ€»ä½“æé«˜äº†48.2%ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¹å–„ç¨³å¥æ€§ï¼Œè¿˜æ¢ç´¢äº†è‡ªåŠ¨åŒ–å·¥å…·æ„å»ºï¼Œå–å¾—äº†65.4%çš„æˆåŠŸç‡ï¼Œä¸ºæœªæ¥å…¨è‡ªåŠ¨æ”¾å°„å­¦åº”ç”¨èå…¥ä¸´åºŠå®è·µæä¾›äº†æœ‰å‰æ™¯çš„è§è§£ã€‚æˆ‘ä»¬çš„æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench%E4%B8%8A%E3%80%82">https://github.com/MAGIC-AI4Med/RadABenchä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09529v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šRadA-BenchPlatæ˜¯ä¸€ä¸ªè¯„ä»·å¹³å°ï¼Œé€šè¿‡æ¨¡æ‹ŸåŒ»ç”Ÿåœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­çš„è¡Œä¸ºï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚å¹³å°ä½¿ç”¨åˆæˆæ‚£è€…è®°å½•æ¨¡æ‹Ÿä¸´åºŠæƒ…å¢ƒï¼Œå¯¹LLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æ¢è®¨äº†æå‡å¤æ‚ä»»åŠ¡ç†è§£å’Œå·¥å…·åè°ƒç­‰é—®é¢˜çš„ç­–ç•¥ã€‚ç ”ç©¶è¿˜å‘ç°ä¸€äº›æ¨¡å‹å¦‚Claude-3.7-Sonnetè¡¨ç°è‰¯å¥½ï¼Œä½†ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç ”ç©¶æ•°æ®ä»£ç å‡å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RadA-BenchPlatå¹³å°ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚</li>
<li>å¹³å°ä½¿ç”¨åˆæˆæ‚£è€…è®°å½•æ¨¡æ‹Ÿä¸´åºŠæƒ…å¢ƒè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¹³å°å®šä¹‰åç±»å·¥å…·ç”¨äºä»£ç†ä»»åŠ¡è§£å†³ï¼Œå¹¶è¯„ä¼°äº†ä¸ƒæ¬¾é¢†å…ˆçš„LLMã€‚</li>
<li>Claude-3.7-Sonnetç­‰æ¨¡å‹åœ¨å¸¸è§„ç¯å¢ƒä¸‹ä»»åŠ¡å®Œæˆç‡è¾ƒé«˜ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ç†è§£å’Œå·¥å…·åè°ƒæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨å››ç§å…ˆè¿›çš„æç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œå¤æ‚ä»»åŠ¡æ€§èƒ½æœ‰æ‰€æå‡ï¼Œå…¶ä¸­æç¤ºåå‘ä¼ æ’­å’Œå¤šä»£ç†åä½œåˆ†åˆ«è´¡çŒ®äº†16.8%å’Œ30.7%çš„æ”¹è¿›ã€‚</li>
<li>æ¢è®¨äº†è‡ªåŠ¨åŒ–å·¥å…·å»ºè®¾çš„é²æ£’æ€§ï¼Œå¹¶å–å¾—äº†65.4%çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e10b5eeeedf6eb51e69c4c8d338cbe49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b70ad4a7ff799314230f6243d7532a84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0835bfb7adb77f937b9d67027191d29.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Respiratory-Differencing-Enhancing-Pulmonary-Thermal-Ablation-Evaluation-Through-Pre-and-Intra-Operative-Image-Fusion"><a href="#Respiratory-Differencing-Enhancing-Pulmonary-Thermal-Ablation-Evaluation-Through-Pre-and-Intra-Operative-Image-Fusion" class="headerlink" title="Respiratory Differencing: Enhancing Pulmonary Thermal Ablation   Evaluation Through Pre- and Intra-Operative Image Fusion"></a>Respiratory Differencing: Enhancing Pulmonary Thermal Ablation   Evaluation Through Pre- and Intra-Operative Image Fusion</h2><p><strong>Authors:Wan Li, Wei Li, Moheng Rong, Yutao Rao, Hui Tang, Yudong Zhang, Feng Wang</strong></p>
<p>CT image-guided thermal ablation is widely used for lung cancer treatment; however, follow-up data indicate that physiciansâ€™ subjective assessments of intraoperative images often overestimate the ablation effect, potentially leading to incomplete treatment. To address these challenges, we developed \textit{Respiratory Differencing}, a novel intraoperative CT image assistance system aimed at improving ablation evaluation. The system first segments tumor regions in preoperative CT images and then employs a multi-stage registration process to align these images with corresponding intraoperative or postoperative images, compensating for respiratory deformations and treatment-induced changes. This system provides two key outputs to help physicians evaluate intraoperative ablation. First, differential images are generated by subtracting the registered preoperative images from the intraoperative ones, allowing direct visualization and quantitative comparison of pre- and post-treatment differences. These differential images enable physicians to assess the relative positions of the tumor and ablation zones, even when the tumor is no longer visible in post-ablation images, thus improving the subjective evaluation of ablation effectiveness. Second, the system provides a quantitative metric that measures the discrepancies between the tumor area and the treatment zone, offering a numerical assessment of the overall efficacy of ablation.This pioneering system compensates for complex lung deformations and integrates pre- and intra-operative imaging data, enhancing quality control in cancer ablation treatments. A follow-up study involving 35 clinical cases demonstrated that our system significantly outperforms traditional subjective assessments in identifying under-ablation cases during or immediately after treatment, highlighting its potential to improve clinical decision-making and patient outcomes. </p>
<blockquote>
<p>CTå›¾åƒå¼•å¯¼çš„çƒ­æ¶ˆèå¹¿æ³›åº”ç”¨äºè‚ºç™Œæ²»ç–—ã€‚ç„¶è€Œï¼Œéšè®¿æ•°æ®è¡¨æ˜ï¼ŒåŒ»ç”Ÿå¯¹æœ¯ä¸­å›¾åƒçš„ä¸»è§‚è¯„ä¼°å¾€å¾€ä¼šé«˜ä¼°æ¶ˆèæ•ˆæœï¼Œå¯èƒ½å¯¼è‡´æ²»ç–—ä¸å®Œå…¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ã€Šå‘¼å¸å·®å¼‚ã€‹è¿™ä¸€æ–°å‹æœ¯ä¸­CTå›¾åƒè¾…åŠ©ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æ¶ˆèè¯„ä¼°æ°´å¹³ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆåˆ†å‰²æœ¯å‰CTå›¾åƒä¸­çš„è‚¿ç˜¤åŒºåŸŸï¼Œç„¶åé‡‡ç”¨å¤šé˜¶æ®µé…å‡†è¿‡ç¨‹å°†è¿™äº›å›¾åƒä¸ç›¸åº”çš„æœ¯ä¸­æˆ–æœ¯åå›¾åƒå¯¹é½ï¼Œä»¥è¡¥å¿å‘¼å¸å˜å½¢å’Œæ²»ç–—å¼•èµ·çš„å˜åŒ–ã€‚è¯¥ç³»ç»Ÿæä¾›ä¸¤ä¸ªå…³é”®è¾“å‡ºï¼Œå¸®åŠ©åŒ»ç”Ÿè¯„ä¼°æœ¯ä¸­æ¶ˆèæƒ…å†µã€‚é¦–å…ˆï¼Œé€šè¿‡å‡å»é…å‡†çš„æœ¯å‰å›¾åƒï¼Œäº§ç”Ÿå·®åˆ†å›¾åƒï¼Œä½¿åŒ»ç”Ÿèƒ½å¤Ÿç›´æ¥å¯è§†åŒ–å’Œå®šé‡æ¯”è¾ƒæ²»ç–—å‰åå·®å¼‚ã€‚è¿™äº›å·®åˆ†å›¾åƒä½¿åŒ»ç”Ÿèƒ½å¤Ÿè¯„ä¼°è‚¿ç˜¤å’Œæ¶ˆèåŒºçš„ç›¸å¯¹ä½ç½®ï¼Œå³ä½¿åœ¨æ¶ˆèåå›¾åƒä¸­è‚¿ç˜¤ä¸å†å¯è§ï¼Œä¹Ÿèƒ½æé«˜æ¶ˆèæ•ˆæœçš„ä¸»è§‚è¯„ä¼°ã€‚å…¶æ¬¡ï¼Œè¯¥ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå®šé‡æŒ‡æ ‡ï¼Œæµ‹é‡è‚¿ç˜¤åŒºåŸŸä¸æ²»ç–—åŒºåŸŸä¹‹é—´çš„å·®å¼‚ï¼Œå¯¹æ¶ˆèçš„æ€»ä½“æ•ˆæœè¿›è¡Œæ•°å€¼è¯„ä¼°ã€‚è¿™ä¸€å¼€åˆ›æ€§çš„ç³»ç»Ÿèƒ½å¤Ÿè¡¥å¿è‚ºéƒ¨å¤æ‚çš„å˜å½¢ï¼Œæ•´åˆæœ¯å‰å’Œæœ¯ä¸­çš„æˆåƒæ•°æ®ï¼Œæé«˜ç™Œç—‡æ¶ˆèæ²»ç–—çš„è´¨é‡æ§åˆ¶ã€‚å¯¹35ä¾‹ä¸´åºŠç—…ä¾‹çš„éšè®¿ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨æ²»ç–—è¿‡ç¨‹æˆ–æ²»ç–—ç»“æŸåè¯†åˆ«æ¶ˆèä¸è¶³ç—…ä¾‹æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¸»è§‚è¯„ä¼°ï¼Œçªæ˜¾å…¶åœ¨æ”¹è¿›ä¸´åºŠå†³ç­–å’Œæ‚£è€…é¢„åæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04299v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æœ¯ä¸­CTå›¾åƒè¾…åŠ©ç³»ç»Ÿâ€”â€”Respiratory Differencingï¼Œç”¨äºæ”¹è¿›æ¶ˆèè¯„ä¼°ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†å‰²è‚¿ç˜¤åŒºåŸŸå¹¶å¯¹å…¶è¿›è¡Œå¤šé˜¶æ®µé…å‡†ï¼Œå°†æœ¯å‰CTå›¾åƒä¸æœ¯ä¸­æˆ–æœ¯åå›¾åƒå¯¹é½ï¼Œä»¥è¡¥å¿å‘¼å¸å˜å½¢å’Œæ²»ç–—å¼•èµ·çš„å˜åŒ–ã€‚è¯¥ç³»ç»Ÿæä¾›å·®åˆ†å›¾åƒå’Œå®šé‡æŒ‡æ ‡ï¼Œå¸®åŠ©åŒ»ç”Ÿè¯„ä¼°æ¶ˆèæ•ˆæœï¼Œå¹¶æ˜¾è‘—æé«˜åœ¨è¯†åˆ«æ¬ æ¶ˆèç—…ä¾‹æ–¹é¢çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTå›¾åƒå¼•å¯¼çš„çƒ­æ¶ˆèæ˜¯æ²»ç–—è‚ºç™Œçš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†åŒ»ç”Ÿå¯¹æœ¯ä¸­å›¾åƒçš„ä¸»è§‚è¯„ä¼°å¯èƒ½ä¼šé«˜ä¼°æ¶ˆèæ•ˆæœï¼Œå¯¼è‡´æ²»ç–—ä¸å®Œå…¨ã€‚</li>
<li>æå‡ºäº†æ–°å‹æœ¯ä¸­CTå›¾åƒè¾…åŠ©ç³»ç»Ÿâ€”â€”Respiratory Differencingï¼Œæ—¨åœ¨æ”¹è¿›æ¶ˆèè¯„ä¼°ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡åˆ†å‰²è‚¿ç˜¤åŒºåŸŸå¹¶è¿›è¡Œå¤šé˜¶æ®µé…å‡†ï¼Œå°†æœ¯å‰CTå›¾åƒä¸æœ¯ä¸­æˆ–æœ¯åå›¾åƒå¯¹é½ã€‚</li>
<li>ç³»ç»Ÿæä¾›å·®åˆ†å›¾åƒï¼Œé€šè¿‡ä»æœ¯ä¸­å›¾åƒä¸­å‡å»å·²é…å‡†çš„æœ¯å‰å›¾åƒï¼Œç›´æ¥å¯è§†åŒ–å¹¶å®šé‡æ¯”è¾ƒæ²»ç–—å‰åå·®å¼‚ï¼Œå¸®åŠ©åŒ»ç”Ÿè¯„ä¼°æ¶ˆèæ•ˆæœã€‚</li>
<li>ç³»ç»Ÿè¿˜æä¾›å®šé‡æŒ‡æ ‡ï¼Œæµ‹é‡è‚¿ç˜¤åŒºåŸŸä¸æ²»ç–—åŒºåŸŸä¹‹é—´çš„å·®å¼‚ï¼Œå…¨é¢è¯„ä¼°æ¶ˆèæ•ˆæœã€‚</li>
<li>è¯¥ç³»ç»Ÿå¯è¡¥å¿è‚ºéƒ¨å¤æ‚å˜å½¢ï¼Œæ•´åˆæœ¯å‰å’Œæœ¯ä¸­çš„å½±åƒæ•°æ®ï¼Œæé«˜ç™Œç—‡æ¶ˆèæ²»ç–—çš„è´¨é‡æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-895bd1b2b1a6bbcba7868932eda5b1dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71722e1bd7a27b1dd460ba926e8289c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8694707ece8f1571db92a029cf5b8c2e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Efficient-and-Accurate-Pneumonia-Detection-Using-a-Novel-Multi-Scale-Transformer-Approach"><a href="#Efficient-and-Accurate-Pneumonia-Detection-Using-a-Novel-Multi-Scale-Transformer-Approach" class="headerlink" title="Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale   Transformer Approach"></a>Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale   Transformer Approach</h2><p><strong>Authors:Alireza Saber, Pouria Parhami, Alimohammad Siahkarzadeh, Mansoor Fateh, Amirreza Fateh</strong></p>
<p>Pneumonia, a prevalent respiratory infection, remains a leading cause of morbidity and mortality worldwide, particularly among vulnerable populations. Chest X-rays serve as a primary tool for pneumonia detection; however, variations in imaging conditions and subtle visual indicators complicate consistent interpretation. Automated tools can enhance traditional methods by improving diagnostic reliability and supporting clinical decision-making. In this study, we propose a novel multi-scale transformer approach for pneumonia detection that integrates lung segmentation and classification into a unified framework. Our method introduces a lightweight transformer-enhanced TransUNet for precise lung segmentation, achieving a Dice score of 95.68% on the â€œChest X-ray Masks and Labelsâ€ dataset with fewer parameters than traditional transformers. For classification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101) to extract multi-scale feature maps, which are then processed through a modified transformer module to enhance pneumonia detection. This integration of multi-scale feature extraction and lightweight transformer modules ensures robust performance, making our method suitable for resource-constrained clinical environments. Our approach achieves 93.75% accuracy on the â€œKermanyâ€ dataset and 96.04% accuracy on the â€œCohenâ€ dataset, outperforming existing methods while maintaining computational efficiency. This work demonstrates the potential of multi-scale transformer architectures to improve pneumonia diagnosis, offering a scalable and accurate solution to global healthcare challenges.â€<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia">https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia</a>â€œ </p>
<blockquote>
<p>è‚ºç‚æ˜¯ä¸€ç§å¸¸è§çš„å‘¼å¸é“æ„ŸæŸ“ï¼Œä»ç„¶æ˜¯å…¨çƒå‘ç—…å’Œæ­»äº¡çš„ä¸»è¦åŸå› ï¼Œç‰¹åˆ«æ˜¯åœ¨è„†å¼±äººç¾¤ä¸­ã€‚èƒ¸éƒ¨Xå°„çº¿æ˜¯æ£€æµ‹è‚ºç‚çš„ä¸»è¦å·¥å…·ï¼›ç„¶è€Œï¼Œæˆåƒæ¡ä»¶çš„å·®å¼‚å’Œå¾®å¦™çš„è§†è§‰æŒ‡æ ‡ä½¿ä¸€è‡´çš„è§£è¯»å˜å¾—å¤æ‚ã€‚è‡ªåŠ¨åŒ–å·¥å…·å¯ä»¥é€šè¿‡æé«˜è¯Šæ–­çš„å¯é æ€§å’Œæ”¯æŒä¸´åºŠå†³ç­–æ¥å¢å¼ºä¼ ç»Ÿæ–¹æ³•ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦å˜å‹å™¨æ–¹æ³•ï¼Œç”¨äºè‚ºç‚æ£€æµ‹ï¼Œè¯¥æ–¹æ³•å°†è‚ºéƒ¨åˆ†å‰²å’Œåˆ†ç±»æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„å˜å‹å™¨å¢å¼ºå‹TransUNetè¿›è¡Œç²¾ç¡®çš„è‚ºéƒ¨åˆ†å‰²ï¼Œåœ¨â€œèƒ¸éƒ¨Xå°„çº¿æ©è†œå’Œæ ‡ç­¾â€æ•°æ®é›†ä¸Šå®ç°äº†95.68%çš„Diceå¾—åˆ†ï¼ŒåŒæ—¶å‚æ•°å°‘äºä¼ ç»Ÿå˜å‹å™¨ã€‚å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„ResNetæ¨¡å‹ï¼ˆResNet-50å’ŒResNet-101ï¼‰æå–å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œç„¶åé€šè¿‡æ”¹è¿›åçš„å˜å‹å™¨æ¨¡å—è¿›è¡Œå¤„ç†ï¼Œä»¥æé«˜è‚ºç‚æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚å¤šå°ºåº¦ç‰¹å¾æå–å’Œè½»å‹å˜å‹å™¨æ¨¡å—çš„é›†æˆç¡®ä¿äº†ç¨³å¥çš„æ€§èƒ½ï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨â€œKermanyâ€æ•°æ®é›†ä¸Šå®ç°äº†93.75%çš„å‡†ç¡®ç‡ï¼Œåœ¨â€œCohenâ€æ•°æ®é›†ä¸Šå®ç°äº†96.04%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å¤šå°ºåº¦å˜å‹å™¨æ¶æ„åœ¨è‚ºç‚è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œä¸ºå…¨çƒåŒ»ç–—ä¿å¥æŒ‘æˆ˜æä¾›äº†å¯ä¼¸ç¼©å’Œå‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ç›¸å…³ä»£ç ä»“åº“åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia">https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04290v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šå°ºåº¦Transformerçš„æ–¹æ³•ç”¨äºè‚ºç‚æ£€æµ‹ï¼Œç»“åˆè‚ºéƒ¨åˆ†å‰²å’Œåˆ†ç±»ã€‚é‡‡ç”¨è½»é‡çº§Transformerå¢å¼ºçš„TransUNetè¿›è¡Œç²¾ç¡®è‚ºéƒ¨åˆ†å‰²ï¼Œå®ç°â€œChest X-ray Masks and Labelsâ€æ•°æ®é›†ä¸Šçš„Diceåˆ†æ•°è¾¾95.68%ã€‚åˆ†ç±»æ–¹é¢ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ResNetæ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œå†é€šè¿‡æ”¹è‰¯çš„Transformeræ¨¡å—å¢å¼ºè‚ºç‚æ£€æµ‹ã€‚æ­¤æ–¹æ³•é€‚åˆèµ„æºæœ‰é™çš„ä¸´åºŠç¯å¢ƒï¼Œå¯¹â€œKermanyâ€å’Œâ€œCohenâ€æ•°æ®é›†çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾93.75%å’Œ96.04%ã€‚å±•ç°äº†å¤šå°ºåº¦Transformeræ¶æ„åœ¨è‚ºç‚è¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç‚ä»æ˜¯å…¨çƒä¸»è¦çš„ç–¾ç—…åŸå› ï¼Œç‰¹åˆ«æ˜¯åœ¨è„†å¼±äººç¾¤ä¸­ã€‚</li>
<li>èƒ¸Xå…‰æ£€æŸ¥æ˜¯è‚ºç‚æ£€æµ‹çš„ä¸»è¦å·¥å…·ï¼Œä½†å­˜åœ¨è§£è¯»ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>è‡ªåŠ¨åŒ–å·¥å…·èƒ½æå‡è¯Šæ–­çš„å¯é æ€§å’Œä¸´åºŠå†³ç­–æ”¯æŒã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¤šå°ºåº¦Transformerçš„è‚ºç‚æ£€æµ‹æ–¹æ³•ï¼Œæ•´åˆè‚ºéƒ¨åˆ†å‰²å’Œåˆ†ç±»ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§Transformerçš„TransUNetè¿›è¡Œç²¾ç¡®è‚ºéƒ¨åˆ†å‰²ï¼Œå®ç°é«˜Diceåˆ†æ•°ã€‚</li>
<li>åˆ©ç”¨ResNetæ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œé€šè¿‡æ”¹è‰¯çš„Transformeræ¨¡å—å¢å¼ºè‚ºç‚æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8d9869bd0a62597de52c254db35951b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b1c4753f68410fa042c9ab6248403a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RaDialog-A-Large-Vision-Language-Model-for-Radiology-Report-Generation-and-Conversational-Assistance"><a href="#RaDialog-A-Large-Vision-Language-Model-for-Radiology-Report-Generation-and-Conversational-Assistance" class="headerlink" title="RaDialog: A Large Vision-Language Model for Radiology Report Generation   and Conversational Assistance"></a>RaDialog: A Large Vision-Language Model for Radiology Report Generation   and Conversational Assistance</h2><p><strong>Authors:Chantal Pellegrini, Ege Ã–zsoy, Benjamin Busam, Nassir Navab, Matthias Keicher</strong></p>
<p>Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: <a target="_blank" rel="noopener" href="https://github.com/ChantalMP/RaDialog">https://github.com/ChantalMP/RaDialog</a>. </p>
<blockquote>
<p>èƒ½å¤Ÿé’ˆå¯¹ç»™å®šçš„åŒ»å­¦å›¾åƒç”Ÿæˆå¹¶è®¨è®ºä¸´åºŠä¸Šæ­£ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šçš„å¯¹è°ˆå¼äººå·¥æ™ºèƒ½å·¥å…·å…·æœ‰æ”¹å˜æ”¾å°„å­¦çš„æ½œåŠ›ã€‚è¿™æ ·çš„äººæœºå¾ªç¯æ”¾å°„å­¦åŠ©ç†å¯ä»¥ä¿ƒè¿›åä½œè¯Šæ–­è¿‡ç¨‹ï¼Œä»è€ŒèŠ‚çœæ—¶é—´å¹¶æé«˜æŠ¥å‘Šè´¨é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†RaDialogï¼Œè¿™æ˜¯é¦–ä¸ªå½»åº•è¯„ä¼°å¹¶å…¬å¼€å¯ç”¨çš„ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œäº¤äº’å¼å¯¹è¯çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚RaDialogæœ‰æ•ˆåœ°å°†è§†è§‰å›¾åƒç‰¹å¾ã€ç»“æ„åŒ–ç—…ç†å‘ç°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåœ¨ä¸€èµ·ï¼ŒåŒæ—¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ¥é€‚åº”ä¸“ä¸šé¢†åŸŸã€‚ä¸ºäº†ä¿æŒåº•å±‚LLMçš„å¯¹è¯èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºèƒ¸éƒ¨Xå°„çº¿æ”¾å°„å­¦ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªç»¼åˆçš„ã€åŠè‡ªåŠ¨æ ‡æ³¨çš„ã€ä»¥å›¾åƒä¸ºåŸºç¡€çš„æŒ‡ä»¤æ•°æ®é›†ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ä¸´åºŠæ­£ç¡®æ€§ï¼Œå¹¶åœ¨äº¤äº’å¼ä»»åŠ¡ï¼ˆå¦‚çº æ­£æŠ¥å‘Šå’Œå›ç­”é—®é¢˜ï¼‰ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œè¿™å¯ä»¥ä½œä¸ºä¸´åºŠå¯¹è¯ç³»ç»Ÿçš„åŸºç¡€æ­¥éª¤ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨githubä¸Šè·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/ChantalMP/RaDialog%E3%80%82">https://github.com/ChantalMP/RaDialogã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.18681v2">PDF</a> improved version accepted at MIDL 2025:   <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=trUvr1gSNI">https://openreview.net/pdf?id=trUvr1gSNI</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾å…·æœ‰æ½œåŠ›æ”¹å˜æ”¾å°„å­¦é¢†åŸŸçš„å¯¹è¯å¼äººå·¥æ™ºèƒ½å·¥å…·â€”â€”RaDialogã€‚å®ƒèƒ½å¤ŸåŸºäºåŒ»å­¦å›¾åƒç”Ÿæˆå¹¶è®¨è®ºä¸´åºŠæ­£ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šï¼Œä»è€ŒååŠ©è¯Šæ–­è¿‡ç¨‹ï¼ŒèŠ‚çœæ—¶é—´å¹¶æé«˜æŠ¥å‘Šè´¨é‡ã€‚RaDialogæœ‰æ•ˆåœ°æ•´åˆäº†è§†è§‰å›¾åƒç‰¹å¾ã€ç»“æ„åŒ–ç—…ç†å‘ç°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒé€‚åº”ä¸“ä¸šé¢†åŸŸã€‚åŒæ—¶ï¼Œä¸ºäº†ä¿æŒåº•å±‚è¯­è¨€æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿æ”¾å°„å­¦ä»»åŠ¡çš„å›¾åƒåŸºç¡€æŒ‡ä»¤æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é‡‡ç”¨åŠè‡ªåŠ¨æ ‡æ³¨æ–¹å¼ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼ŒRaDialogåœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†ä¸´åºŠæ­£ç¡®æ€§çš„æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨äº¤äº’å¼ä»»åŠ¡ï¼ˆå¦‚çº æ­£æŠ¥å‘Šå’Œå›ç­”é—®é¢˜ï¼‰ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RaDialogæ˜¯ä¸€æ¬¾ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œäº¤äº’å¼å¯¹è¯çš„å·¥å…·ï¼Œå…·æœ‰æ½œåŠ›æ”¹å˜æ”¾å°„å­¦é¢†åŸŸã€‚</li>
<li>RaDialogèƒ½å¤ŸåŸºäºåŒ»å­¦å›¾åƒç”Ÿæˆä¸´åºŠæ­£ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šï¼Œä¿ƒè¿›è¯Šæ–­è¿‡ç¨‹çš„åä½œï¼Œæé«˜æŠ¥å‘Šè´¨é‡å’ŒèŠ‚çœæ—¶é—´ã€‚</li>
<li>RaDialogé€šè¿‡æ•´åˆè§†è§‰å›¾åƒç‰¹å¾ã€ç»“æ„åŒ–ç—…ç†å‘ç°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†åœ¨æ”¾å°„å­¦é¢†åŸŸçš„ä¸“ä¸šåŒ–åº”ç”¨ã€‚</li>
<li>ä¸ºäº†ä¿æŒåº•å±‚è¯­è¨€æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿æ”¾å°„å­¦ä»»åŠ¡çš„å›¾åƒåŸºç¡€æŒ‡ä»¤æ•°æ®é›†ã€‚</li>
<li>RaDialogåœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†ä¸´åºŠæ­£ç¡®æ€§çš„æœ€æ–°æ°´å¹³ï¼Œè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>RaDialogåœ¨äº¤äº’å¼ä»»åŠ¡ï¼ˆå¦‚çº æ­£æŠ¥å‘Šå’Œå›ç­”é—®é¢˜ï¼‰ä¸­å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.18681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25da5051fe3b3f5d71acc6a89ad2b668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5994f21186ef6e56c077da0072236ae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01555b7dfed8235f6109d23434371969.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc3535867ac007c42eeb8c876d4883ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e0ff2855e928a0808809a1dccb77d14.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6b10bf32f3d183a2ed5108ce824dce80.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  AVENet Disentangling Features by Approximating Average Features for   Voice Conversion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5ed045cc4a6f90da5808658bd4410315.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Transfer between Modalities with MetaQueries
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19211.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
