<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Hogwild! Inference Parallel LLM Generation via Concurrent Attention">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-10-æ›´æ–°"><a href="#2025-04-10-æ›´æ–°" class="headerlink" title="2025-04-10 æ›´æ–°"></a>2025-04-10 æ›´æ–°</h1><h2 id="Hogwild-Inference-Parallel-LLM-Generation-via-Concurrent-Attention"><a href="#Hogwild-Inference-Parallel-LLM-Generation-via-Concurrent-Attention" class="headerlink" title="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"></a>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</h2><p><strong>Authors:Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh</strong></p>
<p>Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM â€œworkersâ€ in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while â€œseeingâ€ each otherâ€™s partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with â€œinstantâ€ access to each otherâ€™s generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºé€šè¿‡é«˜çº§æ¨ç†ã€é•¿å½¢å¼å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨æ¥å¤„ç†æ—¥ç›Šå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚è§£å†³è¿™äº›ä»»åŠ¡é€šå¸¸æ¶‰åŠé•¿æ—¶é—´çš„æ¨ç†è®¡ç®—ã€‚åœ¨äººç±»çš„é—®é¢˜è§£å†³ä¸­ï¼ŒåŠ é€Ÿå·¥ä½œçš„å¸¸è§ç­–ç•¥æ˜¯åä½œï¼šå°†é—®é¢˜åˆ†ä¸ºå­ä»»åŠ¡ï¼ŒåŒæ—¶æ¢ç´¢ä¸åŒçš„ç­–ç•¥ç­‰ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMä¹Ÿå¯ä»¥é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶è¿›è¡Œå¹¶è¡Œæ“ä½œï¼Œå¦‚æŠ•ç¥¨æœºåˆ¶æˆ–å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„ç‹¬ç«‹å­ä»»åŠ¡çš„æ˜¾å¼åˆ›å»ºã€‚ç„¶è€Œï¼Œè¿™äº›æ¡†æ¶å¯èƒ½å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„ä»»åŠ¡ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å…¶é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸åŒçš„è®¾è®¡æ€è·¯ï¼šæˆ‘ä»¬å¹¶è¡Œè¿è¡ŒLLMâ€œå·¥ä½œè€…â€ï¼Œå…è®¸å®ƒä»¬é€šè¿‡å®æ—¶æ›´æ–°çš„å…³æ³¨ç¼“å­˜è¿›è¡ŒåŒæ­¥ï¼Œå¹¶æç¤ºè¿™äº›å·¥ä½œè€…å†³å®šå¦‚ä½•æœ€ä½³åœ°åä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å®ä¾‹ä¸ºæ‰‹å¤´çš„é—®é¢˜åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼ŒåŒæ—¶â€œçœ‹åˆ°â€å½¼æ­¤åœ¨å¹¶è¡Œç¼“å­˜ä¸­çš„éƒ¨åˆ†è¿›åº¦ã€‚æˆ‘ä»¬é€šè¿‡Hogwildï¼å®ç°è¿™ç§æ–¹æ³•ï¼šä¸€ä¸ªå¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå…¶ä¸­åŒä¸€ä¸ªLLMçš„å¤šä¸ªå®ä¾‹å¯ä»¥å¹¶è¡Œè¿è¡Œå¹¶ä½¿ç”¨ç›¸åŒçš„å…³æ³¨ç¼“å­˜ï¼Œå¹¶ä¸”å¯ä»¥å³æ—¶è®¿é—®å½¼æ­¤ç”Ÿæˆçš„ä»¤ç‰Œã€‚Hogwildï¼æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æ¥é¿å…é‡æ–°è®¡ç®—ï¼ŒåŒæ—¶æé«˜å¹¶è¡Œç¡¬ä»¶åˆ©ç”¨ç‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°ä»£å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨å…±äº«é”®å€¼ç¼“å­˜çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06261v1">PDF</a> Preprint, work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é«˜çº§æ¨ç†ã€é•¿æ–‡æœ¬å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨ï¼Œèƒ½å¤Ÿå¤„ç†æ—¥ç›Šå¤æ‚çš„ä»»åŠ¡ã€‚é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶ï¼Œå¦‚æŠ•ç¥¨æœºåˆ¶å’Œç‹¬ç«‹å­ä»»åŠ¡çš„åˆ›å»ºï¼ŒLLMå¯ä»¥å¹¶è¡Œæ“ä½œä»¥åŠ å¿«å·¥ä½œé€Ÿåº¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ³•ï¼Œå³å¹¶è¡Œè¿è¡ŒLLMâ€œå·¥ä½œè€…â€ï¼Œé€šè¿‡åŒæ­¥æ›´æ–°çš„å…³æ³¨ç¼“å­˜æ¥æç¤ºè¿™äº›å·¥ä½œè€…å†³å®šæœ€ä½³çš„åä½œæ–¹å¼ã€‚è¿™ç§æ–¹æ³•å…è®¸å®ä¾‹ä¸ºæ‰‹å¤´é—®é¢˜åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼ŒåŒæ—¶â€œæŸ¥çœ‹â€å½¼æ­¤åœ¨å¹¶å‘ç¼“å­˜ä¸­çš„éƒ¨åˆ†è¿›åº¦ã€‚æœ¬ç ”ç©¶é€šè¿‡éœæ ¼å¨å°”ï¼ˆHo wildcardï¼‰æ¨ç†å®ç°è¿™ä¸€æ–¹æ³•ï¼šä¸€ç§å¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå…·æœ‰ç›¸åŒçš„å…³æ³¨ç¼“å­˜çš„å¤šä¸ªLLMå®ä¾‹å¹¶è¡Œè¿è¡Œï¼Œå¯å³æ—¶è®¿é—®å½¼æ­¤ç”Ÿæˆçš„ä»¤ç‰Œã€‚éœæ ¼å¨å°”æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æ¥æé«˜å¹¶è¡Œç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œé¿å…é‡æ–°è®¡ç®—ã€‚ç ”ç©¶å‘ç°ï¼Œç°ä»£å…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMå¯ä»¥åœ¨å…±äº«é”®å€¼ç¼“å­˜çš„æƒ…å†µä¸‹è¿›è¡Œæ¨æ–­ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é«˜çº§æ¨ç†ã€é•¿æ–‡æœ¬å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨ã€‚</li>
<li>é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶ï¼ŒLLMå¯ä»¥å¹¶è¡Œæ“ä½œä»¥åŠ å¿«ä»»åŠ¡å®Œæˆé€Ÿåº¦ã€‚</li>
<li>æ–°è®¾è®¡æ–¹æ³•å…è®¸LLMå®ä¾‹åˆ¶å®šé’ˆå¯¹ç‰¹å®šé—®é¢˜çš„åä½œç­–ç•¥ï¼Œå¹¶é€šè¿‡å…³æ³¨ç¼“å­˜è¿›è¡ŒåŒæ­¥ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸å®ä¾‹åœ¨å¹¶è¡Œç¯å¢ƒä¸­â€œæŸ¥çœ‹â€å½¼æ­¤çš„è¿›åº¦ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨éœæ ¼å¨å°”æ¨ç†å®ç°å¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œæé«˜äº†ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚</li>
<li>éœæ ¼å¨å°”æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥æŠ€æœ¯ï¼Œé¿å…äº†é‡æ–°è®¡ç®—çš„éœ€è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-defc5823af4167516d325db284872c06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7764d42ae79aa59b4b917e5ba5203cef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7fc796185d70e9a1911cd57646adc91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbff8ce93b27a62a7a23c173c58ee62a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FEABench-Evaluating-Language-Models-on-Multiphysics-Reasoning-Ability"><a href="#FEABench-Evaluating-Language-Models-on-Multiphysics-Reasoning-Ability" class="headerlink" title="FEABench: Evaluating Language Models on Multiphysics Reasoning Ability"></a>FEABench: Evaluating Language Models on Multiphysics Reasoning Ability</h2><p><strong>Authors:Nayantara Mudur, Hao Cui, Subhashini Venugopalan, Paul Raccuglia, Michael P. Brenner, Peter Norgaard</strong></p>
<p>Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMsâ€™ reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at <a target="_blank" rel="noopener" href="https://github.com/google/feabench">https://github.com/google/feabench</a> </p>
<blockquote>
<p>æ„å»ºç²¾ç¡®æ¨¡æ‹Ÿç°å®ä¸–ç•Œå¹¶è°ƒç”¨æ•°å€¼æ±‚è§£å™¨æ¥è§£å†³å®šé‡é—®é¢˜æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„å¿…å¤‡è¦æ±‚ã€‚æˆ‘ä»¬æ¨å‡ºFEABenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†æ¨¡æ‹Ÿå¹¶è§£å†³ç‰©ç†ã€æ•°å­¦å’Œå·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œä½¿ç”¨æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰ã€‚æˆ‘ä»¬å¼•å…¥å…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°è¿›è¡Œæ¨ç†ï¼Œå¹¶åˆ©ç”¨COMSOL MultiphysicsÂ®æœ‰é™å…ƒåˆ†æè½¯ä»¶æ¥è®¡ç®—æœºç­”æ¡ˆï¼Œä»¥ç ”ç©¶LLMè§£å†³è¿™äº›ç«¯åˆ°ç«¯é—®é¢˜çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ä»£ç†ï¼Œè¯¥ä»£ç†å…·å¤‡é€šè¿‡è½¯ä»¶åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ä¸ä¹‹äº¤äº’çš„èƒ½åŠ›ï¼Œæ£€æŸ¥å…¶è¾“å‡ºå¹¶ä½¿ç”¨å·¥å…·åœ¨å¤šæ¬¡è¿­ä»£ä¸­æ”¹è¿›å…¶è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„ç­–ç•¥ç”Ÿæˆå¯æ‰§è¡ŒAPIè°ƒç”¨çš„æ¬¡æ•°è¾¾åˆ°88%ã€‚èƒ½å¤ŸæˆåŠŸäº¤äº’å¹¶æ“ä½œæœ‰é™å…ƒåˆ†æè½¯ä»¶ä»¥è§£å†³æˆ‘ä»¬åŸºå‡†æµ‹è¯•ä¸­é—®é¢˜çš„LLMï¼Œå°†æ¨åŠ¨å·¥ç¨‹è‡ªåŠ¨åŒ–é¢†åŸŸçš„è¾¹ç•Œã€‚è·å–è¿™ç§èƒ½åŠ›å°†ä½¿LLMçš„æ¨ç†èƒ½åŠ›ä¸æ•°å€¼æ±‚è§£å™¨çš„ç²¾åº¦ç›¸ç»“åˆï¼Œä¿ƒè¿›å¼€å‘èƒ½å¤Ÿè§£å†³ç°å®ä¸–ç•Œå¤æ‚é—®é¢˜çš„è‡ªä¸»ç³»ç»Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/google/feabench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/google/feabenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06260v1">PDF</a> 39 pages. Accepted at the NeurIPS 2024 Workshops on Mathematical   Reasoning and AI and Open-World Agents</p>
<p><strong>Summary</strong></p>
<p>åŸºäºçœŸå®ä¸–ç•Œçš„ç²¾ç¡®æ¨¡æ‹Ÿå’Œæ•°å€¼æ±‚è§£åœ¨ç§‘å­¦ä¸å·¥ç¨‹ä¸­æ˜¯æ ¸å¿ƒè¦æ±‚ã€‚æˆ‘ä»¬æå‡ºFEABenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠå…¶ä»£ç†äººåœ¨æœ‰é™å…ƒç´ åˆ†æï¼ˆFEAï¼‰åŸºç¡€ä¸Šæ¨¡æ‹Ÿå¹¶è§£å†³ç‰©ç†ã€æ•°å­¦å’Œå·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€å¥—å…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°é—®é¢˜å¹¶æ“ä½œCOMSOL Multiphysicsè½¯ä»¶è®¡ç®—ç­”æ¡ˆæ¥è¯„ä¼°LLMç«¯åˆ°ç«¯çš„è§£å†³é—®é¢˜èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ä»£ç†ï¼Œå®ƒé€šè¿‡åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ä¸è½¯ä»¶äº¤äº’ï¼Œæ£€æŸ¥è¾“å‡ºå¹¶ä½¿ç”¨å·¥å…·æ”¹å–„å¤šæ¬¡è¿­ä»£çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æœ€ä½³ç­–ç•¥ç”Ÿæˆå¯æ‰§è¡ŒAPIè°ƒç”¨çš„å‡†ç¡®ç‡ä¸º88%ã€‚èƒ½æˆåŠŸæ“ä½œFEAè½¯ä»¶è§£å†³æ­¤ç±»é—®é¢˜çš„LLMåŸºå‡†æµ‹è¯•å°†æ¨åŠ¨å·¥ç¨‹è‡ªåŠ¨åŒ–å‰æ²¿ã€‚è·å¾—æ­¤èƒ½åŠ›å°†ä½¿LLMçš„æ¨ç†èƒ½åŠ›é€šè¿‡æ•°å€¼æ±‚è§£å™¨ç²¾ç¡®åº¦çš„åŠ æŒè€Œå‘å±•ï¼Œæ¨åŠ¨è‡ªä¸»ç³»ç»Ÿè§£å†³ç°å®ä¸–ç•Œå¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/google/feabench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/google/feabenchè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºFEABenchä½œä¸ºè¯„ä¼°LLMæ¨¡æ‹Ÿå’Œå·¥ç¨‹é—®é¢˜è§£å†³èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å…¨é¢è¯„ä¼°LLMå¤„ç†ç‰©ç†ã€æ•°å­¦å’Œå·¥ç¨‹é—®é¢˜èƒ½åŠ›çš„æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°é—®é¢˜å¹¶æ“ä½œCOMSOL Multiphysicsè½¯ä»¶è®¡ç®—ç­”æ¡ˆï¼Œè¯„ä¼°LLMçš„ç«¯åˆ°ç«¯é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†èƒ½ä¸FEAè½¯ä»¶è¿›è¡Œäº¤äº’çš„è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œé€šè¿‡APIè¿›è¡Œæ“ä½œå¹¶æ”¹å–„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æœ€ä½³ç­–ç•¥ç”Ÿæˆå¯æ‰§è¡ŒAPIè°ƒç”¨çš„å‡†ç¡®ç‡ä¸º88%ã€‚</li>
<li>LLMsæœ‰èƒ½åŠ›æ“ä½œFEAè½¯ä»¶è§£å†³é—®é¢˜å°†æ¨åŠ¨å·¥ç¨‹è‡ªåŠ¨åŒ–çš„å‘å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0e6eb3a08e2c216f7adb37ea59031ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e009ee066f8bace188cbd33c6115987.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fffaff2da821dec3273000a5a18c2ea3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f6f59f186d32a73947d11cb7fb7a983.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-869c11750ebea92b48b2b043ad5fdd18.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Transfer-between-Modalities-with-MetaQueries"><a href="#Transfer-between-Modalities-with-MetaQueries" class="headerlink" title="Transfer between Modalities with MetaQueries"></a>Transfer between Modalities with MetaQueries</h2><p><strong>Authors:Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, Saining Xie</strong></p>
<p>Unified multimodal models aim to integrate understanding (text output) and generation (pixel output), but aligning these different modalities within a single architecture often demands complex training recipes and careful data balancing. We introduce MetaQueries, a set of learnable queries that act as an efficient interface between autoregressive multimodal LLMs (MLLMs) and diffusion models. MetaQueries connects the MLLMâ€™s latents to the diffusion decoder, enabling knowledge-augmented image generation by leveraging the MLLMâ€™s deep understanding and reasoning capabilities. Our method simplifies training, requiring only paired image-caption data and standard diffusion objectives. Notably, this transfer is effective even when the MLLM backbone remains frozen, thereby preserving its state-of-the-art multimodal understanding capabilities while achieving strong generative performance. Additionally, our method is flexible and can be easily instruction-tuned for advanced applications such as image editing and subject-driven generation. </p>
<blockquote>
<p>ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹æ—¨åœ¨æ•´åˆç†è§£å’Œç”Ÿæˆï¼ˆåˆ†åˆ«è¾“å‡ºæ–‡æœ¬å’Œåƒç´ ï¼‰ï¼Œä½†åœ¨å•ä¸€æ¶æ„å†…å¯¹é½è¿™äº›ä¸åŒæ¨¡æ€é€šå¸¸è¦æ±‚å¤æ‚çš„è®­ç»ƒé…æ–¹å’Œè°¨æ…çš„æ•°æ®å¹³è¡¡ã€‚æˆ‘ä»¬å¼•å…¥äº†MetaQueriesï¼Œè¿™æ˜¯ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢ï¼Œä½œä¸ºè‡ªå›å½’å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆæ¥å£ã€‚MetaQuerieså°†MLLMçš„æ½œåœ¨ç©ºé—´è¿æ¥åˆ°æ‰©æ•£è§£ç å™¨ï¼Œåˆ©ç”¨MLLMçš„æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ç°çŸ¥è¯†å¢å¼ºå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒï¼Œåªéœ€é…å¯¹å›¾åƒæ ‡é¢˜æ•°æ®å’Œæ ‡å‡†æ‰©æ•£ç›®æ ‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿MLLMéª¨å¹²ä¿æŒå†»ç»“çŠ¶æ€ï¼Œè¿™ç§è½¬ç§»ä¾ç„¶æœ‰æ•ˆï¼Œä»è€Œä¿ç•™äº†å…¶æœ€å…ˆè¿›çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶å®ç°äº†å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çµæ´»ï¼Œå¯è½»æ¾è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒä»¥ç”¨äºé«˜çº§åº”ç”¨ï¼Œå¦‚å›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06256v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://xichenpan.com/metaquery">https://xichenpan.com/metaquery</a></p>
<p><strong>Summary</strong></p>
<p>MetaQueriesé€šè¿‡ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢ï¼Œå®ç°äº†è‡ªåŠ¨å›å½’å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆæ¥å£è¿æ¥ã€‚å®ƒç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œåªéœ€é…å¯¹å›¾åƒå’Œå­—å¹•æ•°æ®ä»¥åŠæ ‡å‡†æ‰©æ•£ç›®æ ‡å³å¯ã€‚MetaQuerieså°†MLLMçš„æ½œåœ¨ç©ºé—´ä¸æ‰©æ•£è§£ç å™¨è¿æ¥èµ·æ¥ï¼Œå®ç°äº†çŸ¥è¯†å¢å¼ºçš„å›¾åƒç”Ÿæˆï¼Œæ—¢ä¿ç•™äº†MLLMçš„æœ€å…ˆè¿›çš„å¤šæ¨¡æ€ç†è§£åŠ›ï¼Œåˆå®ç°äº†å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚æ­¤æ–¹æ³•çµæ´»ï¼Œæ˜“äºæŒ‡ä»¤å¾®è°ƒï¼Œå¯åº”ç”¨äºå›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆç­‰é«˜çº§åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaQuerieså®ç°å¤šæ¨¡æ€æ¨¡å‹çš„ç»Ÿä¸€ï¼Œé€šè¿‡æ¥å£è¿æ¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>MetaQueriesç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œåªéœ€è¦å›¾åƒå’Œå­—å¹•æ•°æ®ä»¥åŠæ ‡å‡†æ‰©æ•£ç›®æ ‡ã€‚</li>
<li>MetaQueriesèƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›å®ç°çŸ¥è¯†å¢å¼ºçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ç•™äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£åŠ›ï¼Œå¹¶å®ç°äº†å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>MetaQueriesæ–¹æ³•çµæ´»ï¼Œå¯ä»¥è½»æ¾åœ°é€‚åº”ä¸åŒçš„æŒ‡ä»¤å¾®è°ƒä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆç­‰é«˜çº§åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34c0bdb455721760acdffaf8afa68fd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bd7083f5108116326a8d0d4ddb5ce14.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56008693de8fb99b52c62bf6f3e4a3f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a06893fb108d01abee6e848089fed5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dc37d9fdb8b36c2945169b15509e894.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LExT-Towards-Evaluating-Trustworthiness-of-Natural-Language-Explanations"><a href="#LExT-Towards-Evaluating-Trustworthiness-of-Natural-Language-Explanations" class="headerlink" title="LExT: Towards Evaluating Trustworthiness of Natural Language   Explanations"></a>LExT: Towards Evaluating Trustworthiness of Natural Language   Explanations</h2><p><strong>Authors:Krithi Shailya, Shreya Rajpal, Gokul S Krishnan, Balaraman Ravindran</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT) (The code and set up to reproduce our experiments are publicly available at <a target="_blank" rel="noopener" href="https://github.com/cerai-iitm/LExT">https://github.com/cerai-iitm/LExT</a>). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸçš„é›†æˆåº¦ä¸æ–­æé«˜ï¼Œå·²ç»æå‡ºäº†å¤šç§ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šçš„æ–¹æ³•ã€‚è¿™äº›è§£é‡Šå¯¹äºæé«˜æ¨¡å‹çš„è§£é‡Šæ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ç­‰é€æ˜åº¦å’Œå¯é æ€§è‡³å…³é‡è¦çš„æ•æ„Ÿé¢†åŸŸã€‚é‰´äºLLMç”Ÿæˆè§£é‡ŠåŠå…¶å·²çŸ¥çš„é—®é¢˜ï¼Œè¶Šæ¥è¶Šéœ€è¦å¯é çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šã€‚è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡å¦‚BLEUå’ŒROUGEèƒ½å¤Ÿæ•æ‰è¯­æ³•å’Œè¯­ä¹‰çš„å‡†ç¡®æ€§ï¼Œä½†å¿½ç•¥äº†äº‹å®å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œå¿ å®æ€§ç­‰å…¶ä»–å…³é”®æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶æ¥é‡åŒ–è‡ªç„¶è¯­è¨€è§£é‡Šçš„ä¿¡ä»»åº¦ï¼Œå¹³è¡¡åˆç†æ€§å’Œå¿ å®æ€§ï¼Œä»¥å¾—å‡ºå…¨é¢çš„è¯­è¨€è§£é‡Šä¿¡ä»»åº¦å¾—åˆ†ï¼ˆLExTï¼‰ï¼ˆæˆ‘ä»¬çš„å®éªŒä»£ç å’Œè®¾ç½®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cerai-iitm/LExT%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%EF%BC%89%E3%80%82%E5%88%A9%E7%94%A8%E6%88%91%E4%BB%AC%E7%9A%84%E9%A2%86%E5%9F%9F%E9%80%9A%E7%94%A8%E6%A1%86%E6%9E%B6%E5%BA%94%E7%94%A8%E4%BA%8E%E5%8C%BB%E7%96%97%E9%A2%86%E5%9F%9F%E4%BD%BF%E7%94%A8%E5%85%AC%E5%85%B1%E5%8C%BB%E5%AD%A6%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%E5%85%AD%E4%B8%AA%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E4%BA%86%E8%AF%84%E4%BC%B0%EF%BC%8C%E5%8C%85%E6%8B%AC%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E7%BB%93%E6%9E%9C%E8%AF%81%E6%98%8E%E4%BA%86%E5%AE%83%E4%BB%AC%E5%9C%A8%E7%94%9F%E6%88%90%E5%8F%AF%E4%BF%A1%E8%A7%A3%E9%87%8A%E6%96%B9%E9%9D%A2%E7%9A%84%E6%98%BE%E8%91%97%E5%B7%AE%E5%BC%82%E3%80%82%E9%80%9A%E8%BF%87%E6%AF%94%E8%BE%83%E8%BF%99%E4%BA%9B%E8%A7%A3%E9%87%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E8%A7%82%E5%AF%9F%E5%88%B0%E4%B8%80%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E7%8E%B0%E8%B1%A1%EF%BC%8C%E5%A6%82%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BF%A0%E5%AE%9E%E6%80%A7%E4%B8%8D%E4%B8%80%E8%87%B4%E4%BB%A5%E5%8F%8A%E5%AE%83%E4%BB%AC%E5%80%BE%E5%90%91%E4%BA%8E%E4%BC%98%E4%BA%8E%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E3%80%82%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%BC%BA%E8%B0%83%E4%BA%86%E4%BD%BF%E7%94%A8%E9%87%8F%E8%BA%AB%E5%AE%9A%E5%88%B6%E7%9A%84%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6%E6%9D%A5%E8%AF%84%E4%BC%B0%E6%95%8F%E6%84%9F%E9%A2%86%E5%9F%9F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%A7%A3%E9%87%8A%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%8C%E4%B8%BA%E6%94%B9%E5%96%84%E5%8C%BB%E7%96%97%E4%BF%9D%E5%81%A5%E9%A2%86%E5%9F%9F%E5%8F%8A%E4%BB%A5%E5%A4%96%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E4%BF%A1%E5%BA%A6%E5%92%8C%E9%80%8F%E6%98%8E%E5%BA%A6%E6%8F%90%E4%BE%9B%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/cerai-iitm/LExTä¸Šå…¬å¼€è·å¾—ï¼‰ã€‚åˆ©ç”¨æˆ‘ä»¬çš„é¢†åŸŸé€šç”¨æ¡†æ¶åº”ç”¨äºåŒ»ç–—é¢†åŸŸä½¿ç”¨å…¬å…±åŒ»å­¦æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹å…­ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹å’Œé€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¯æ˜äº†å®ƒä»¬åœ¨ç”Ÿæˆå¯ä¿¡è§£é‡Šæ–¹é¢çš„æ˜¾è‘—å·®å¼‚ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›è§£é‡Šï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€äº›æœ‰è¶£çš„ç°è±¡ï¼Œå¦‚é€šç”¨æ¨¡å‹çš„å¿ å®æ€§ä¸ä¸€è‡´ä»¥åŠå®ƒä»¬å€¾å‘äºä¼˜äºç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œè¿›ä¸€æ­¥å¼ºè°ƒäº†ä½¿ç”¨é‡èº«å®šåˆ¶çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æ•æ„Ÿé¢†åŸŸè‡ªç„¶è¯­è¨€è§£é‡Šçš„é‡è¦æ€§ï¼Œä¸ºæ”¹å–„åŒ»ç–—ä¿å¥é¢†åŸŸåŠä»¥å¤–çš„è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦å’Œé€æ˜åº¦æä¾›äº†åŸºç¡€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06227v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šæ–¹é¢çš„ä¸åŒæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ä¿å¥ç­‰æ•æ„Ÿé¢†åŸŸçš„é‡è¦æ€§ã€‚æå‡ºä¸€ä¸ªé€šç”¨çš„æ¡†æ¶æ¥é‡åŒ–è‡ªç„¶è¯­è¨€è§£é‡Šçš„ä¿¡ä»»åº¦ï¼Œå¹³è¡¡å¯ä¿¡åº¦ä¸å¿ å®åº¦ï¼Œä»¥å¾—å‡ºå…¨é¢çš„è¯­è¨€è§£é‡Šä¿¡ä»»åº¦å¾—åˆ†ï¼ˆLExTï¼‰ã€‚åº”ç”¨è¯¥æ¡†æ¶äºåŒ»ç–—é¢†åŸŸï¼Œè¯„ä¼°äº†å…­ä¸ªæ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨ç”Ÿæˆå¯ä¿¡è§£é‡Šæ–¹é¢çš„èƒ½åŠ›å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚å¼ºè°ƒä½¿ç”¨å®šåˆ¶è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æ•æ„Ÿé¢†åŸŸè‡ªç„¶è¯­è¨€è§£é‡Šçš„é‡è¦æ€§ï¼Œä¸ºæé«˜è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥ç­‰é¢†åŸŸçš„å¯ä¿¡åº¦å’Œé€æ˜åº¦å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šæ–¹é¢å·²æå‡ºå¤šç§æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—ä¿å¥ç­‰æ•æ„Ÿé¢†åŸŸçš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºä¸€ä¸ªé€šç”¨æ¡†æ¶æ¥é‡åŒ–è‡ªç„¶è¯­è¨€è§£é‡Šçš„ä¿¡ä»»åº¦ï¼ŒåŒ…æ‹¬å¯ä¿¡åº¦å’Œå¿ å®åº¦ä¸¤ä¸ªæ–¹é¢ã€‚</li>
<li>å…¬å¼€äº†å®éªŒä»£ç å’Œè®¾ç½®ï¼Œå¯é‡ç°ç ”ç©¶ã€‚</li>
<li>åº”ç”¨è¯¥æ¡†æ¶äºåŒ»ç–—é¢†åŸŸï¼Œè¯„ä¼°å…­ä¸ªæ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ç‰¹å®šé¢†åŸŸå’Œé€šç”¨æ¨¡å‹ã€‚</li>
<li>å‘ç°ä¸åŒæ¨¡å‹åœ¨ç”Ÿæˆå¯ä¿¡è§£é‡Šæ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>é€šç”¨æ¨¡å‹åœ¨å¿ å®åº¦æ–¹é¢å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œå¹¶å€¾å‘äºåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç‰¹å®šé¢†åŸŸçš„ç²¾ç»†è°ƒæ•´æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5a8a94723b19a72470377a529f2a7e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-318bdf7596a6042e19f6a6b6445501eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f7f39042b589f7036d3e92b4e968e55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70c6738828c4233999d66b028fd071ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f65a8a5982c05341d9b7282f0ea1d5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a74f71ff73ba8895a8e3785a5a3403e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Encoder-Decoder-Gemma-Improving-the-Quality-Efficiency-Trade-Off-via-Adaptation"><a href="#Encoder-Decoder-Gemma-Improving-the-Quality-Efficiency-Trade-Off-via-Adaptation" class="headerlink" title="Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via   Adaptation"></a>Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via   Adaptation</h2><p><strong>Authors:Biao Zhang, Fedor Moiseev, Joshua Ainslie, Paul Suganthan, Min Ma, Surya Bhupatiraju, Fede Lebron, Orhan Firat, Armand Joulin, Zhe Dong</strong></p>
<p>While decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization&#x2F;optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\sim$7% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $&gt;$3%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research. </p>
<blockquote>
<p>è™½ç„¶åªæœ‰è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†åœ¨å®é™…åº”ç”¨ç¨‹åºä¸­ï¼Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç”±äºå…¶æ¨ç†æ•ˆç‡å’Œæ›´ä¸°å¯Œçš„ç¼–ç å™¨è¡¨ç¤ºè€Œä»ç„¶è¢«å¹¿æ³›é‡‡ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªæ–°é—®é¢˜ï¼šå°†é¢„è®­ç»ƒçš„å•è§£ç å™¨LLMé€‚åº”åˆ°ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œæ—¨åœ¨ç»“åˆè¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œå®ç°æ›´æœ‰åˆ©çš„è´¨é‡æ•ˆç‡æƒè¡¡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§é€‚åº”ä¸ä»…ç»§æ‰¿äº†å•è§£ç å™¨LLMçš„èƒ½åŠ›ï¼Œè€Œä¸”ä¸ä»å¤´å¼€å§‹é¢„è®­ç»ƒç›¸æ¯”ï¼Œè¿˜å‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚æˆ‘ä»¬ä¸¥æ ¼æ¢ç´¢äº†ä¸åŒçš„é¢„è®­ç»ƒç›®æ ‡ä»¥åŠå‚æ•°åˆå§‹åŒ–å’Œä¼˜åŒ–æŠ€æœ¯ã€‚åŸºäºGemma 2ï¼ˆ2Bå’Œ9Bï¼‰å’Œä¸€ç³»åˆ—æ–°é¢„è®­ç»ƒçš„mT5è§„æ¨¡æ¨¡å‹ï¼ˆæœ€å¤šè¾¾1.6Bï¼‰çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†é€‚åº”æ€§å’Œç¼–ç å™¨-è§£ç å™¨LLMçš„ä¼˜åŠ¿ã€‚åœ¨ç±»ä¼¼çš„æ¨ç†é¢„ç®—ä¸‹ï¼Œç¼–ç å™¨-è§£ç å™¨LLMçš„é¢„è®­ç»ƒæ€§èƒ½ç›¸å½“ï¼ˆé€šå¸¸æ›´å¥½ï¼‰ï¼Œä½†å¾®è°ƒæ€§èƒ½æ˜æ˜¾ä¼˜äºå…¶å•è§£ç å™¨å¯¹åº”æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒGemma 2B-2Båœ¨æŒ‡ä»¤è°ƒæ•´åæ¯”Gemma 2Bé«˜å‡ºçº¦7%ã€‚ç¼–ç å™¨-è§£ç å™¨é€‚åº”è¿˜å…è®¸çµæ´»ç»„åˆä¸åŒå¤§å°çš„æ¨¡å‹ï¼Œå…¶ä¸­Gemma 9B-2Bæ˜¾è‘—è¶…è¿‡Gemma 2B-2Bè¶…è¿‡3%ã€‚é€‚åº”çš„ç¼–ç å™¨è¡¨ç¤ºåœ¨SuperGLUEä¸Šä¹Ÿäº§ç”Ÿäº†æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ£€æŸ¥ç‚¹ä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06225v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œå°½ç®¡è§£ç å™¨æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å› å…¶æ¨ç†æ•ˆç‡å’Œä¸°å¯Œçš„ç¼–ç å™¨è¡¨å¾è€Œåœ¨å®é™…åº”ç”¨ä¸­ä»å¹¿æ³›é‡‡ç”¨ã€‚æœ¬æ–‡ç ”ç©¶äº†å°†é¢„è®­ç»ƒè§£ç å™¨æ¨¡å‹é€‚åº”åˆ°ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„æ–°é—®é¢˜ï¼Œæ—¨åœ¨ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œå®ç°æ›´ä¼˜è´¨é«˜æ•ˆã€‚å®éªŒè¯æ˜ï¼Œé€‚åº”ç­–ç•¥ä¸ä»…ç»§æ‰¿äº†è§£ç å™¨æ¨¡å‹çš„èƒ½åŠ›ï¼Œè€Œä¸”ç›¸è¾ƒäºä»å¤´å¼€å§‹é¢„è®­ç»ƒï¼Œå‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚é€šè¿‡åœ¨ä¸åŒé¢„è®­ç»ƒç›®æ ‡ã€å‚æ•°åˆå§‹åŒ–&#x2F;ä¼˜åŒ–æŠ€æœ¯ä¸Šçš„ä¸¥æ ¼æ¢ç´¢ï¼Œä»¥åŠåŸºäºGemma 2å’Œä¸€ç³»åˆ—æ–°é¢„è®­ç»ƒmT5è§„æ¨¡æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œå±•ç¤ºäº†é€‚åº”ç­–ç•¥çš„æœ‰æ•ˆæ€§ä»¥åŠç¼–ç å™¨-è§£ç å™¨LLMçš„ä¼˜åŠ¿ã€‚åœ¨ç›¸åŒæ¨ç†é¢„ç®—ä¸‹ï¼Œç¼–ç å™¨-è§£ç å™¨LLMçš„é¢„è®­ç»ƒæ€§èƒ½ç›¸å½“ï¼ˆé€šå¸¸æ›´å¥½ï¼‰ï¼Œå¾®è°ƒæ€§èƒ½æ˜¾è‘—ä¼˜äºè§£ç å™¨æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒGemma 2B-2Båœ¨æŒ‡ä»¤è°ƒæ•´åçš„æ€§èƒ½æ¯”Gemma 2Bé«˜å‡ºçº¦7ï¼…ã€‚æ­¤å¤–ï¼Œç¼–ç å™¨-è§£ç å™¨é€‚åº”ç­–ç•¥è¿˜å…è®¸çµæ´»ç»„åˆä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œå…¶ä¸­Gemma 9B-2Bçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºGemma 2B-2Bè¶…è¿‡3ï¼…ã€‚é€‚åº”çš„ç¼–ç å™¨è¡¨å¾åœ¨SuperGLUEä¸Šä¹Ÿå–å¾—äº†æ›´å¥½çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹åœ¨æ¨ç†æ•ˆç‡å’Œä¸°å¯Œçš„ç¼–ç å™¨è¡¨å¾æ–¹é¢ä»å…·æœ‰ä¼˜åŠ¿ï¼Œå› æ­¤åœ¨ç°å®åº”ç”¨ä¸­è¢«å¹¿æ³›é‡‡ç”¨ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†å°†é¢„è®­ç»ƒçš„è§£ç å™¨æ¨¡å‹é€‚åº”åˆ°ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„æ–°é—®é¢˜ã€‚</li>
<li>é€‚åº”ç­–ç•¥ä¸ä»…ç»§æ‰¿äº†è§£ç å™¨æ¨¡å‹çš„èƒ½åŠ›ï¼Œè€Œä¸”å‡å°‘äº†ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„è®¡ç®—éœ€æ±‚ã€‚</li>
<li>é€šè¿‡å¤šç§é¢„è®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†é€‚åº”ç­–ç•¥çš„æœ‰æ•ˆæ€§ä»¥åŠç¼–ç å™¨-è§£ç å™¨LLMçš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨ç›¸åŒçš„æ¨ç†é¢„ç®—ä¸‹ï¼Œç¼–ç å™¨-è§£ç å™¨LLMçš„é¢„è®­ç»ƒå’Œå¾®è°ƒæ€§èƒ½å‡ä¼˜äºè§£ç å™¨æ¨¡å‹ã€‚</li>
<li>é€‚åº”çš„ç¼–ç å™¨è¡¨å¾åœ¨SuperGLUEä»»åŠ¡ä¸Šå–å¾—äº†æ›´å¥½çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef109f3f71e49a1b2180127568e3a606.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1aa1beadefaa3b4b2dc9dbbe250d4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ca4eb20f64b0116dfb984f93c731fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b69d42b27a7696cbe98e7344bfe11c7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-058754a3475195d9109a7e1cf7a9af09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e146b4d888f7602c9e98b1cb3f4b83db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ca882c3d22a9f42faacc13814afba5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="From-128K-to-4M-Efficient-Training-of-Ultra-Long-Context-Large-Language-Models"><a href="#From-128K-to-4M-Efficient-Training-of-Ultra-Long-Context-Large-Language-Models" class="headerlink" title="From 128K to 4M: Efficient Training of Ultra-Long Context Large Language   Models"></a>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language   Models</h2><p><strong>Authors:Chejian Xu, Wei Ping, Peng Xu, Zihan Liu, Boxin Wang, Mohammad Shoeybi, Bo Li, Bryan Catanzaro</strong></p>
<p>Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and multimodal data. In this work, we introduce a efficient training recipe for building ultra-long context LLMs from aligned instruct model, pushing the boundaries of context lengths from 128K to 1M, 2M, and 4M tokens. Our approach leverages efficient continued pretraining strategies to extend the context window and employs effective instruction tuning to maintain the instruction-following and reasoning abilities. Our UltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves state-of-the-art performance across a diverse set of long-context benchmarks. Importantly, models trained with our approach maintain competitive performance on standard benchmarks, demonstrating balanced improvements for both long and short context tasks. We further provide an in-depth analysis of key design choices, highlighting the impacts of scaling strategies and data composition. Our findings establish a robust framework for efficiently scaling context lengths while preserving general model capabilities. We release all model weights at: <a target="_blank" rel="noopener" href="https://ultralong.github.io/">https://ultralong.github.io/</a>. </p>
<blockquote>
<p>é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›å¯¹äºå¤šç§åº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬æ–‡æ¡£å’Œè§†é¢‘ç†è§£ã€ä¸Šä¸‹æ–‡å†…å­¦ä¹ å’Œæ¨ç†æ—¶é—´ç¼©æ”¾ç­‰ã€‚æ‰€æœ‰è¿™äº›åº”ç”¨éƒ½éœ€è¦æ¨¡å‹å¤„ç†å’Œæ¨ç†é•¿æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®åºåˆ—ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒé…æ–¹ï¼Œç”¨äºä»å¯¹é½çš„æŒ‡ä»¤æ¨¡å‹ä¸­æ„å»ºè¶…é•¿ä¸Šä¸‹æ–‡LLMï¼Œå°†ä¸Šä¸‹æ–‡é•¿åº¦è¾¹ç•Œä»128Kæ¨è‡³1Mã€2Må’Œ4Mä»¤ç‰Œã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é«˜æ•ˆçš„æŒç»­é¢„è®­ç»ƒç­–ç•¥æ¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶é‡‡ç”¨æœ‰æ•ˆçš„æŒ‡ä»¤è°ƒæ•´æ¥ä¿æŒæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„UltraLong-8Bï¼ŒåŸºäºLlama3.1-Instructæ„å»ºï¼Œé‡‡ç”¨æˆ‘ä»¬çš„é…æ–¹ï¼Œåœ¨å¤šç§é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œé‡‡ç”¨æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šä¿æŒç«äº‰åŠ›ï¼Œè¡¨æ˜åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡å’ŒçŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šå‡å–å¾—äº†å¹³è¡¡æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æ·±å…¥åˆ†æäº†å…³é”®è®¾è®¡é€‰æ‹©ï¼Œé‡ç‚¹ä»‹ç»äº†æ‰©å±•ç­–ç•¥å’Œæ•°æ®ç»„åˆçš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªç¨³å¥çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦åŒæ—¶ä¿ç•™é€šç”¨æ¨¡å‹èƒ½åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒçš„æ‰€æœ‰æ¨¡å‹æƒé‡ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://ultralong.github.io/]">https://ultralong.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06214v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¶…é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›åœ¨ä¼—å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå¦‚æ–‡æ¡£å’Œè§†é¢‘ç†è§£ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ¨ç†ã€æ¨ç†æ—¶é—´ç¼©æ”¾ç­‰ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆï¼Œç”¨äºæ„å»ºè¶…é•¿ä¸Šä¸‹æ–‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†ä¸Šä¸‹æ–‡é•¿åº¦è¾¹ç•Œä»128Kæ¨è‡³1Mã€2Må’Œ4Mä»¤ç‰Œã€‚è¯¥ç ”ç©¶é‡‡ç”¨é«˜æ•ˆçš„æŒç»­é¢„è®­ç»ƒç­–ç•¥æ¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶é‡‡ç”¨æœ‰æ•ˆçš„æŒ‡ä»¤å¾®è°ƒæ¥ä¿æŒæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚åŸºäºLlama3.1-Instructæ„å»ºçš„UltraLong-8Bæ¨¡å‹åœ¨ä¸åŒè¶…é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºåœ¨é•¿çŸ­æœŸä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šçš„å‡è¡¡æ”¹è¿›ã€‚ç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†å…³é”®è®¾è®¡é€‰æ‹©ï¼Œçªå‡ºäº†æ‰©å±•ç­–ç•¥å’Œæ•°æ®ç»„åˆçš„å½±å“ã€‚ç ”ç©¶ä¸ºæœ‰æ•ˆæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦åŒæ—¶ä¿ç•™ä¸€èˆ¬æ¨¡å‹èƒ½åŠ›æä¾›äº†ç¨³å¥æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†è¶…é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›åœ¨å„ç§åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ„å»ºè¶…é•¿ä¸Šä¸‹æ–‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡é«˜æ•ˆæŒç»­é¢„è®­ç»ƒç­–ç•¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ã€‚</li>
<li>é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒä¿æŒæ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŸºäºLlama3.1-Instructæ„å»ºçš„UltraLong-8Bæ¨¡å‹åœ¨è¶…é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå®ç°äº†é•¿çŸ­æœŸä¸Šä¸‹æ–‡ä»»åŠ¡çš„å‡è¡¡æ”¹è¿›ã€‚</li>
<li>æ·±å…¥åˆ†æå…³é”®è®¾è®¡é€‰æ‹©ï¼Œæ˜ç¡®äº†æ‰©å±•ç­–ç•¥å’Œæ•°æ®ç»„åˆçš„å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c706e19b84420da79d4af4550adb987b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76c189547299e244abb977c9f558d73c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ecea6b249f4be7c4acfbb2c543af6f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TxGemma-Efficient-and-Agentic-LLMs-for-Therapeutics"><a href="#TxGemma-Efficient-and-Agentic-LLMs-for-Therapeutics" class="headerlink" title="TxGemma: Efficient and Agentic LLMs for Therapeutics"></a>TxGemma: Efficient and Agentic LLMs for Therapeutics</h2><p><strong>Authors:Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi</strong></p>
<p>Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanityâ€™s Last Exam benchmark (Chemistry &amp; Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high). </p>
<blockquote>
<p>æ²»ç–—å¼€å‘æ˜¯ä¸€é¡¹æˆæœ¬é«˜æ˜‚ã€é£é™©æé«˜çš„å·¥ä½œï¼Œå¸¸å¸¸é¢ä¸´é«˜å¤±è´¥ç‡çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TxGemmaï¼Œè¿™æ˜¯ä¸€å¥—é«˜æ•ˆã€é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½å¤Ÿè¿›è¡Œæ²»ç–—å±æ€§é¢„æµ‹ä»¥åŠäº¤äº’å¼æ¨ç†å’Œè§£é‡Šã€‚ä¸åŒäºç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼ŒTxGemmaèƒ½å¤Ÿç»¼åˆæ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œåœ¨æ²»ç–—å¼€å‘ç®¡é“ä¸­å¹¿æ³›åº”ç”¨ã€‚è¯¥å¥—ä»¶åŒ…æ‹¬å‚æ•°æ¨¡å‹2Bã€9Bå’Œ27Bï¼Œåœ¨å°åˆ†å­ã€è›‹ç™½è´¨ã€æ ¸é…¸ã€ç–¾ç—…å’Œç»†èƒç³»çš„ç»¼åˆæ•°æ®é›†ä¸Šå¾®è°ƒè‡ªGemma-2ã€‚åœ¨66é¡¹æ²»ç–—å¼€å‘ä»»åŠ¡ä¸­ï¼ŒTxGemmaåœ¨64é¡¹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºæˆ–ç›¸å½“äºæœ€å…ˆè¿›çš„é€šç”¨æ¨¡å‹ï¼ˆåœ¨45é¡¹ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ï¼‰ï¼Œå¹¶ä¸”åœ¨é’ˆå¯¹ä¸“ä¸šæ¨¡å‹çš„50é¡¹ä»»åŠ¡ä¸­ï¼ˆåœ¨26é¡¹ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ï¼‰è¡¨ç°è‰¯å¥½ã€‚å¯¹TxGemmaæ¨¡å‹è¿›è¡Œä¸‹æ¸¸æ²»ç–—ä»»åŠ¡çš„å¾®è°ƒï¼Œå¦‚ä¸´åºŠè¯•éªŒä¸è‰¯äº‹ä»¶é¢„æµ‹ï¼Œéœ€è¦çš„è®­ç»ƒæ•°æ®å°‘äºå¯¹åŸºç¡€LLMçš„å¾®è°ƒï¼Œè¿™ä½¿å¾—TxGemmaé€‚åˆæ•°æ®æœ‰é™çš„åº”ç”¨ã€‚é™¤äº†è¿™äº›é¢„æµ‹èƒ½åŠ›ä¹‹å¤–ï¼ŒTxGemmaè¿˜å…·å¤‡ä¼šè¯æ¨¡å‹ï¼Œèƒ½å¤Ÿå¼¥åˆé€šç”¨LLMå’Œä¸“ç”¨å±æ€§é¢„æµ‹å™¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¿™ä½¿å¾—ç§‘å­¦å®¶èƒ½å¤Ÿä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’ï¼ŒåŸºäºåˆ†å­ç»“æ„æä¾›æœºæ¢°æ¨ç†ï¼Œå¹¶è¿›è¡Œç§‘å­¦è®¨è®ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†ç”±Gemini 2.5é©±åŠ¨çš„é€šç”¨æ²»ç–—å‰‚ç³»ç»ŸAgentic-Txï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œæ¨ç†ã€è¡ŒåŠ¨ã€ç®¡ç†å„ç§å·¥ä½œæµç¨‹å¹¶è·å–å¤–éƒ¨é¢†åŸŸçŸ¥è¯†ã€‚Agentic-Txåœ¨Humanityâ€™s Last ExamåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å…ˆå‰çš„é¢†å…ˆæ¨¡å‹ï¼Œåœ¨Chemistry &amp; Biologyæ–¹é¢ç›¸å¯¹äºo3-miniï¼ˆé«˜ï¼‰æé«˜äº†52.3%ï¼Œåœ¨GPQAï¼ˆChemistryï¼‰ä¸Šæé«˜äº†26.7%ï¼Œå¹¶åœ¨ChemBench-Preferenceå’ŒChemBench-Miniä¸Šåˆ†åˆ«æé«˜äº†6.3%å’Œ2.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06196v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>TxGemmaæ˜¯ä¸€å¥—é«˜æ•ˆã€é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½å¤Ÿé¢„æµ‹æ²»ç–—å±æ€§å¹¶å®ç°äº’åŠ¨æ¨ç†å’Œè§£é‡Šæ€§ï¼Œä¸ºè§£å†³æ²»ç–—å¼€å‘çš„é«˜æˆæœ¬å’Œé«˜é£é™©é—®é¢˜æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚è¯¥å¥—ä»¶åŒ…æ‹¬é’ˆå¯¹ä¸åŒéœ€æ±‚çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å„ç§æ¥æºåˆæˆä¿¡æ¯ï¼Œä»è€Œåœ¨æ•´ä¸ªæ²»ç–—å¼€å‘æµç¨‹ä¸­å¹¿æ³›åº”ç”¨ã€‚åœ¨66é¡¹æ²»ç–—å¼€å‘ä»»åŠ¡ä¸­ï¼ŒTxGemmaåœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„åº”ç”¨ä¸­ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·å¤‡ä¼šè¯æ¨¡å‹åŠŸèƒ½ï¼Œä½¿ç§‘å­¦å®¶èƒ½å¤Ÿä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤æµï¼ŒåŸºäºåˆ†å­ç»“æ„æä¾›é¢„æµ‹æœºåˆ¶æ¨ç†å¹¶å‚ä¸ç§‘å­¦è®¨è®ºã€‚åŸºäºè¿™äº›åŠŸèƒ½ï¼Œè¿›ä¸€æ­¥å¼•å…¥äº†Agentic-Txç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç”±Gemini 2.5é©±åŠ¨ï¼Œèƒ½å¤Ÿè¿›è¡Œæ¨ç†ã€è¡ŒåŠ¨ã€ç®¡ç†å„ç§å·¥ä½œæµç¨‹å¹¶è·å–å¤–éƒ¨é¢†åŸŸçŸ¥è¯†ã€‚å®ƒåœ¨Humanityçš„Last ExamåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ol>
<li>TxGemmaæ˜¯ä¸€ç§ç”¨äºæ²»ç–—å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹å¥—ä»¶ï¼Œå…·æœ‰é¢„æµ‹æ²»ç–—å±æ€§çš„èƒ½åŠ›ã€‚</li>
<li>TxGemmaèƒ½å¤Ÿåˆæˆå„ç§æ¥æºçš„ä¿¡æ¯ï¼Œé€‚ç”¨äºæ•´ä¸ªæ²»ç–—å¼€å‘æµç¨‹ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>åœ¨å¤šé¡¹æ²»ç–—å¼€å‘ä»»åŠ¡ä¸­ï¼ŒTxGemmaè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„åº”ç”¨ä¸­ã€‚</li>
<li>TxGemmaå…·å¤‡ä¼šè¯æ¨¡å‹åŠŸèƒ½ï¼Œå…è®¸ç§‘å­¦å®¶ä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤æµï¼Œæä¾›åŸºäºåˆ†å­ç»“æ„çš„é¢„æµ‹æœºåˆ¶æ¨ç†ã€‚</li>
<li>Agentic-Txç³»ç»Ÿç”±Gemini 2.5é©±åŠ¨ï¼Œå…·å¤‡æ¨ç†ã€è¡ŒåŠ¨ã€ç®¡ç†å·¥ä½œæµç¨‹å’Œè·å–å¤–éƒ¨é¢†åŸŸçŸ¥è¯†çš„èƒ½åŠ›ã€‚</li>
<li>Agentic-Txç³»ç»Ÿåœ¨Humanityçš„Last ExamåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
<li>TxGemmaå’ŒAgentic-Txä¸ºè§£å†³æ²»ç–—å¼€å‘çš„é«˜æˆæœ¬å’Œé£é™©æä¾›äº†æœ‰æ•ˆçš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-112b46d89b5a6ab9ace41fc3b3a027c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-009f0e0b9eb524cf328a00c7c326d849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0636d1d182202dd898a1391b966d3dc0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="V-MAGE-A-Game-Evaluation-Framework-for-Assessing-Visual-Centric-Capabilities-in-Multimodal-Large-Language-Models"><a href="#V-MAGE-A-Game-Evaluation-Framework-for-Assessing-Visual-Centric-Capabilities-in-Multimodal-Large-Language-Models" class="headerlink" title="V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric   Capabilities in Multimodal Large Language Models"></a>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric   Capabilities in Multimodal Large Language Models</h2><p><strong>Authors:Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, Lijuan Wang</strong></p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CSU-JPG/V-MAGE">https://github.com/CSU-JPG/V-MAGE</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥å·²ç»æ¨åŠ¨åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œéšç€è¯„ä¼°ä»é™æ€æ•°æ®é›†è½¬å‘å¼€æ”¾ä¸–ç•ŒåŠ¨æ€ç¯å¢ƒï¼Œå½“å‰çš„æ¸¸æˆåŸºå‡†æµ‹è¯•ä»ç„¶ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ— æ³•è¯„ä¼°ç°å®ä¸–ç•Œå†³ç­–æ‰€éœ€çš„å¤šæ ·åŒ–æ¨ç†æŠ€èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰ä¸­å¿ƒå¤šé‡èƒ½åŠ›æ¸¸æˆè¯„ä¼°ï¼ˆV-MAGEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¸¸æˆçš„è®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°MLLMçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚V-MAGEåŒ…å«äº”ä¸ªä¸åŒçš„æ¸¸æˆå’Œè¶…è¿‡ä¸‰åä¸ªæ‰‹å·¥åˆ¶ä½œçš„å…³å¡ï¼Œæµ‹è¯•æ¨¡å‹çš„æ ¸å¿ƒè§†è§‰æŠ€èƒ½ï¼Œå¦‚å®šä½ã€è½¨è¿¹è·Ÿè¸ªã€è®¡æ—¶å’Œè§†è§‰è®°å¿†ï¼Œä»¥åŠæ›´é«˜å±‚æ¬¡çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚é•¿æœŸè§„åˆ’å’Œå®¡æ…è€ƒè™‘ã€‚æˆ‘ä»¬ä½¿ç”¨V-MAGEè¯„ä¼°é¢†å…ˆçš„MLLMï¼Œæ­ç¤ºå®ƒä»¬åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æ‰€æœ‰æ¸¸æˆç¯å¢ƒä¸­ï¼Œæ ¹æ®åŸƒæ´›è¯„çº§æ¯”è¾ƒç»“æœï¼Œé¡¶çº§è¡¨ç°çš„MLLMä¸äººç±»ç›¸æ¯”è¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹æ‰€çŠ¯çš„å„ç§ç±»å‹çš„æ„ŸçŸ¥é”™è¯¯ï¼Œå¹¶ä»ä»£ç†ä¸­å¿ƒè§†è§’æå‡ºäº†æ½œåœ¨çš„æ”¹è¿›é€”å¾„ï¼Œå¦‚æ”¹è¿›ä»£ç†ç­–ç•¥å’Œè§£å†³æ„ŸçŸ¥ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CSU-JPG/V-MAGE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CSU-JPG/V-MAGEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06148v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œéšç€è¯„ä¼°ä»é™æ€æ•°æ®é›†è½¬å‘å¼€æ”¾ä¸–ç•ŒåŠ¨æ€ç¯å¢ƒï¼Œå½“å‰çš„æ¸¸æˆåŸºå‡†æµ‹è¯•ä»ç„¶ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ— æ³•è¯„ä¼°ç°å®ä¸–ç•Œå†³ç­–æ‰€éœ€çš„å„ç§æ¨ç†æŠ€èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šèƒ½åŠ›æ¸¸æˆè¯„ä¼°ï¼ˆV-MAGEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚V-MAGEåŒ…å«äº”ä¸ªä¸åŒæ¸¸æˆå’Œä¸‰åå¤šä¸ªå®šåˆ¶çº§åˆ«ï¼Œæµ‹è¯•æ¨¡å‹åœ¨æ ¸å¿ƒè§†è§‰æŠ€èƒ½ï¼ˆå¦‚å®šä½ã€è½¨è¿¹è·Ÿè¸ªã€å®šæ—¶å’Œè§†è§‰è®°å¿†ï¼‰ä»¥åŠé«˜çº§æ¨ç†ï¼ˆå¦‚é•¿æœŸè§„åˆ’å’Œæƒè¡¡ï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨V-MAGEè¯„ä¼°äº†é¢†å…ˆçš„MLLMsï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æ‰€æœ‰æ¸¸æˆç¯å¢ƒä¸­ï¼Œæ ¹æ®åŸƒæ´›è¯„çº§æ¯”è¾ƒï¼Œè¡¨ç°æœ€ä½³çš„MLLMä¸äººç±»ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†å…³é”®çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹çŠ¯çš„å„ç§ç±»å‹çš„æ„ŸçŸ¥é”™è¯¯ï¼Œå¹¶ä»æ™ºèƒ½ä½“ä¸­å¿ƒçš„è§’åº¦æå‡ºäº†æ”¹è¿›å»ºè®®ï¼Œå¦‚æ”¹è¿›æ™ºèƒ½ä½“ç­–ç•¥å’Œè§£å†³æ„ŸçŸ¥ä¸å‡†ç¡®é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>å½“å‰æ¸¸æˆåŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è§†è§‰æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¼•å…¥V-MAGEæ¡†æ¶ï¼Œé€šè¿‡æ¸¸æˆç¯å¢ƒè¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>V-MAGEåŒ…å«å¤šä¸ªæ¸¸æˆå’Œçº§åˆ«ï¼Œæ¶µç›–æ ¸å¿ƒè§†è§‰æŠ€èƒ½å’Œé«˜çº§æ¨ç†ã€‚</li>
<li>é¢†å…ˆçš„MLLMsåœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MLLMsä¸äººç±»ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-353ca58a8b6465aa1188389faf91d3ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a2147ac2368a45a888631a5db1acade.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5601e6c5597ef4da775d2caaae60cf7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d77b9acc422b80dbac14e9393f9eafe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb6c151107fff219b5a6c4568ee96243.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Multi-Sense-Embeddings-for-Language-Models-and-Knowledge-Distillation"><a href="#Multi-Sense-Embeddings-for-Language-Models-and-Knowledge-Distillation" class="headerlink" title="Multi-Sense Embeddings for Language Models and Knowledge Distillation"></a>Multi-Sense Embeddings for Language Models and Knowledge Distillation</h2><p><strong>Authors:Qitong Wang, Mohammed J. Zaki, Georgios Kollias, Vasileios Kalantzis</strong></p>
<p>Transformer-based large language models (LLMs) rely on contextual embeddings which generate different (continuous) representations for the same token depending on its surrounding context. Nonetheless, words and tokens typically have a limited number of senses (or meanings). We propose multi-sense embeddings as a drop-in replacement for each token in order to capture the range of their uses in a language. To construct a sense embedding dictionary, we apply a clustering algorithm to embeddings generated by an LLM and consider the cluster centers as representative sense embeddings. In addition, we propose a novel knowledge distillation method that leverages the sense dictionary to learn a smaller student model that mimics the senses from the much larger base LLM model, offering significant space and inference time savings, while maintaining competitive performance. Via thorough experiments on various benchmarks, we showcase the effectiveness of our sense embeddings and knowledge distillation approach. We share our code at <a target="_blank" rel="noopener" href="https://github.com/Qitong-Wang/SenseDict">https://github.com/Qitong-Wang/SenseDict</a> </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–äºä¸Šä¸‹æ–‡åµŒå…¥ï¼Œå®ƒä¸ºåŒä¸€æ ‡è®°ç”Ÿæˆä¸åŒçš„ï¼ˆè¿ç»­ï¼‰è¡¨ç¤ºå½¢å¼ï¼Œè¿™å–å†³äºå…¶å‘¨å›´çš„ä¸Šä¸‹æ–‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå•è¯å’Œæ ‡è®°é€šå¸¸å…·æœ‰æœ‰é™æ•°é‡çš„å«ä¹‰ï¼ˆæˆ–æ„ä¹‰ï¼‰ã€‚ä¸ºäº†æ•æ‰è¯­è¨€ä¸­ä½¿ç”¨æ ‡è®°çš„èŒƒå›´ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šä¹‰åµŒå…¥ä½œä¸ºæ¯ä¸ªæ ‡è®°çš„å³æ’å³ç”¨æ›¿ä»£å“ã€‚ä¸ºäº†æ„å»ºæ„ä¹‰åµŒå…¥å­—å…¸ï¼Œæˆ‘ä»¬åº”ç”¨èšç±»ç®—æ³•å¯¹LLMç”Ÿæˆçš„åµŒå…¥è¿›è¡Œèšç±»ï¼Œå¹¶å°†èšç±»ä¸­å¿ƒä½œä¸ºä»£è¡¨æ€§çš„æ„ä¹‰åµŒå…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ©ç”¨æ„ä¹‰å­—å…¸è¿›è¡ŒçŸ¥è¯†è’¸é¦çš„æ–¹æ³•ï¼Œå­¦ä¹ ä¸€ä¸ªè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ï¼Œä»æ›´å¤§çš„åŸºç¡€LLMæ¨¡å‹ä¸­æ¨¡ä»¿æ„ä¹‰ï¼Œåœ¨èŠ‚çœå¤§é‡ç©ºé—´å’Œæ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬é€šè¿‡åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ„ä¹‰åµŒå…¥å’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å°†ä»£ç å…±äº«åœ¨ <a target="_blank" rel="noopener" href="https://github.com/Qitong-Wang/SenseDict">https://github.com/Qitong-Wang/SenseDict</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06036v1">PDF</a> 16 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–ä¸Šä¸‹æ–‡åµŒå…¥æ¥ç”Ÿæˆä¸åŒè¯­å¢ƒä¸‹ç›¸åŒæ ‡è®°ï¼ˆtokenï¼‰çš„è¿ç»­è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå•è¯å’Œæ ‡è®°é€šå¸¸åªæœ‰æœ‰é™æ•°é‡çš„æ„ä¹‰ï¼ˆæˆ–å«ä¹‰ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†å¤šä¹‰è¯åµŒå…¥ä½œä¸ºä¸€ç§æ›¿æ¢æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰ä¸€ç§è¯­è¨€ä¸­çš„å¤šç§ä½¿ç”¨æ–¹å¼ã€‚åŒæ—¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå¤šä¹‰è¯åµŒå…¥çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åº”ç”¨èšç±»ç®—æ³•ç”Ÿæˆä¸€ä¸ªè¯æ±‡å«ä¹‰è¯å…¸ã€‚è¿™ç§æ–°çš„å­¦ç”Ÿæ¨¡å‹åœ¨å¤§å‹åŸºç¡€æ¨¡å‹çš„æ”¯æŒä¸‹å­¦ä¼šäº†æ„ŸçŸ¥ä¸åŒçš„å«ä¹‰ï¼Œåœ¨ç»´æŒç«äº‰åŠ›æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†ç©ºé—´å’Œæ¨ç†æ—¶é—´æ¶ˆè€—ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§å¤šä¹‰è¯åµŒå…¥å’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•éå¸¸æœ‰æ•ˆã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Qitong-Wang/SenseDict">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¾èµ–ä¸Šä¸‹æ–‡åµŒå…¥ç”Ÿæˆä¸åŒè¯­å¢ƒä¸‹çš„æ ‡è®°è¡¨ç¤ºã€‚</li>
<li>æå‡ºå¤šä¹‰è¯åµŒå…¥ä½œä¸ºè§£å†³æ ‡è®°æœ‰é™å«ä¹‰çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡èšç±»ç®—æ³•ç”Ÿæˆè¯æ±‡å«ä¹‰è¯å…¸ï¼Œä»£è¡¨ä¸åŒçš„è¯æ±‡å«ä¹‰ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨è¯æ±‡å«ä¹‰è¯å…¸å­¦ä¹ å°å‹å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨æ˜¾è‘—å‡å°‘ç©ºé—´å’Œæ¨ç†æ—¶é—´çš„åŒæ—¶ç»´æŒæ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜å¤šä¹‰è¯åµŒå…¥å’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5389ce7614a666909fabc12819846305.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b362bd99623d7ce655e587d7670ab588.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6f214e161767fd68b0ebdeffe0407df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22156e07eeb7b458e4cf8b046d41e220.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53096362cdf7ecd715ecc8a474f0584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfd7dca332fb306b87e964b51526ac25.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Llama-3-Nanda-10B-Chat-An-Open-Generative-Large-Language-Model-for-Hindi"><a href="#Llama-3-Nanda-10B-Chat-An-Open-Generative-Large-Language-Model-for-Hindi" class="headerlink" title="Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for   Hindi"></a>Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for   Hindi</h2><p><strong>Authors:Monojit Choudhury, Shivam Chauhan, Rocktim Jyoti Das, Dhruv Sahnan, Xudong Han, Haonan Li, Aaryamonvikram Singh, Alok Anil Jadhav, Utkarsh Agarwal, Mukund Choudhary, Debopriyo Banerjee, Fajri Koto, Junaid Bhat, Awantika Shukla, Samujjwal Ghosh, Samta Kamboj, Onkar Pandit, Lalit Pradhan, Rahul Pal, Sunil Sahu, Soundar Doraiswamy, Parvez Mullah, Ali El Filali, Neha Sengupta, Gokul Ramakrishnan, Rituraj Joshi, Gurpreet Gosal, Avraham Sheinin, Natalia Vassilieva, Preslav Nakov</strong></p>
<p>Developing high-quality large language models (LLMs) for moderately resourced languages presents unique challenges in data availability, model adaptation, and evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a state-of-the-art Hindi-centric instruction-tuned generative LLM, designed to push the boundaries of open-source Hindi language models. Built upon Llama-3-8B, Nanda incorporates continuous pre-training with expanded transformer blocks, leveraging the Llama Pro methodology. A key challenge was the limited availability of high-quality Hindi text data; we addressed this through rigorous data curation, augmentation, and strategic bilingual training, balancing Hindi and English corpora to optimize cross-linguistic knowledge transfer. With 10 billion parameters, Nanda stands among the top-performing open-source Hindi and multilingual models of similar scale, demonstrating significant advantages over many existing models. We provide an in-depth discussion of training strategies, fine-tuning techniques, safety alignment, and evaluation metrics, demonstrating how these approaches enabled Nanda to achieve state-of-the-art results. By open-sourcing Nanda, we aim to advance research in Hindi LLMs and support a wide range of real-world applications across academia, industry, and public services. </p>
<blockquote>
<p>å¼€å‘é’ˆå¯¹ä¸­ç­‰èµ„æºè¯­è¨€çš„é«˜è´¨é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°æ®å¯ç”¨æ€§ã€æ¨¡å‹é€‚åº”æ€§å’Œè¯„ä¼°æ–¹é¢å­˜åœ¨ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†Llama-3-Nanda-10B-Chatï¼Œç®€ç§°Nandaï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å°åœ°è¯­ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è°ƒæ•´ç”Ÿæˆå¼LLMçš„æœ€æ–°æŠ€æœ¯ï¼Œæ—¨åœ¨æ¨åŠ¨å¼€æºå°åœ°è¯­è¯­è¨€æ¨¡å‹çš„è¾¹ç•Œã€‚åŸºäºLlama-3-8Bï¼ŒNandaç»“åˆäº†ä½¿ç”¨Llama Proæ–¹æ³•çš„è¿ç»­é¢„è®­ç»ƒå’Œæ‰©å±•çš„Transformerå—ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯é«˜è´¨é‡å°åœ°è¯­æ–‡æœ¬æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼›æˆ‘ä»¬é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†ã€å¢å¼ºå’Œæˆ˜ç•¥åŒè¯­åŸ¹è®­è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå¹³è¡¡å°åœ°è¯­å’Œè‹±è¯­è¯­æ–™åº“ä»¥ä¼˜åŒ–è·¨è¯­è¨€çŸ¥è¯†è½¬ç§»ã€‚å…·æœ‰10äº¿ä¸ªå‚æ•°ï¼ŒNandaåœ¨åŒç±»å¼€æºå°åœ°è¯­å’Œå¤šè¯­è¨€æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºå‡ºå¯¹è®¸å¤šç°æœ‰æ¨¡å‹çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æˆ‘ä»¬æ·±å…¥è®¨è®ºäº†è®­ç»ƒç­–ç•¥ã€å¾®è°ƒæŠ€æœ¯ã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå±•ç¤ºäº†è¿™äº›æ–¹æ³•å¦‚ä½•å¸®åŠ©Nandaå®ç°æœ€æ–°æŠ€æœ¯æˆæœã€‚é€šè¿‡å¼€æºNandaï¼Œæˆ‘ä»¬æ—¨åœ¨æ¨åŠ¨å°åœ°è¯­LLMçš„ç ”ç©¶ï¼Œå¹¶æ”¯æŒå­¦æœ¯ã€è¡Œä¸šå’Œå…¬å…±æœåŠ¡ä¸­çš„å¹¿æ³›å®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06011v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ä¸­ç­‰èµ„æºè¯­è¨€çš„ä¼˜è´¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€å‘é¢ä¸´æ•°æ®å¯ç”¨æ€§ã€æ¨¡å‹é€‚åº”æ€§å’Œè¯„ä¼°æ–¹é¢çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†ä»¥å°åœ°è¯­ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è°ƒä¼˜ç”Ÿæˆå¼LLMâ€”â€”Nandaï¼Œæ—¨åœ¨æ¨åŠ¨å¼€æºå°åœ°è¯­è¯­è¨€æ¨¡å‹çš„è¾¹ç•Œã€‚Nandaä»¥Llama-3-8Bä¸ºåŸºç¡€ï¼Œé‡‡ç”¨Llama Proæ–¹æ³•ï¼Œè¿›è¡Œè¿ç»­é¢„è®­ç»ƒï¼Œå¹¶æ‰©å±•äº†transformerå—ã€‚è§£å†³é«˜è´¨é‡å°åœ°è¯­æ–‡æœ¬æ•°æ®æœ‰é™çš„å…³é”®æŒ‘æˆ˜åœ¨äºï¼Œæˆ‘ä»¬é€šè¿‡ä¸¥æ ¼çš„æ•°æ®é‡‡é›†ã€å¢å¼ºå’Œæˆ˜ç•¥åŒè¯­åŸ¹è®­æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå¹³è¡¡å°åœ°è¯­å’Œè‹±è¯­è¯­æ–™åº“ä»¥ä¼˜åŒ–è·¨è¯­è¨€çŸ¥è¯†è½¬ç§»ã€‚Nandaçš„å‚æ•°è§„æ¨¡è¾¾åˆ°10äº¿ï¼Œæˆä¸ºè¡¨ç°æœ€ä½³çš„å¼€æºå°åœ°è¯­å’Œå¤šè¯­ç§æ¨¡å‹ä¹‹ä¸€ï¼Œç›¸å¯¹äºè®¸å¤šç°æœ‰æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æœ¬æ–‡æ·±å…¥è®¨è®ºäº†è®­ç»ƒç­–ç•¥ã€å¾®è°ƒæŠ€æœ¯ã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå±•ç¤ºäº†è¿™äº›æ–¹æ³•å¦‚ä½•å¸®åŠ©Nandaå®ç°æœ€ä½³ç»“æœã€‚é€šè¿‡å¼€æºNandaï¼Œæˆ‘ä»¬æ—¨åœ¨æ¨åŠ¨å°åœ°è¯­LLMçš„ç ”ç©¶ï¼Œæ”¯æŒå­¦æœ¯ã€è¡Œä¸šå’Œå…¬å…±æœåŠ¡çš„å¹¿æ³›åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Nandaæ˜¯ä¸€ä¸ªä»¥å°åœ°è¯­ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è°ƒä¼˜ç”Ÿæˆå¼LLMï¼Œå»ºç«‹åœ¨Llama-3-8BåŸºç¡€ä¸Šï¼Œæ‹¥æœ‰10äº¿å‚æ•°è§„æ¨¡ã€‚</li>
<li>Nandaé€šè¿‡è¿ç»­é¢„è®­ç»ƒä¸æ‰©å±•çš„transformerå—ï¼Œé‡‡ç”¨Llama Proæ–¹æ³•è®¾è®¡ã€‚</li>
<li>é«˜è´¨é‡å°åœ°è¯­æ–‡æœ¬æ•°æ®æœ‰é™æ˜¯å¼€å‘ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œé€šè¿‡ä¸¥æ ¼çš„æ•°æ®é‡‡é›†ã€å¢å¼ºå’ŒåŒè¯­åŸ¹è®­æ¥åº”å¯¹ã€‚</li>
<li>Nandaåœ¨å¼€æºå°åœ°è¯­å’Œå¤šè¯­ç§æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œç›¸å¯¹äºè®¸å¤šç°æœ‰æ¨¡å‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>Nandaçš„è®­ç»ƒç­–ç•¥ã€å¾®è°ƒæŠ€æœ¯ã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°æŒ‡æ ‡è¿›è¡Œäº†æ·±å…¥è®¨è®ºã€‚</li>
<li>å¼€æºNandaæ—¨åœ¨æ¨åŠ¨å°åœ°è¯­LLMçš„ç ”ç©¶ï¼Œå¹¶å¹¿æ³›æ”¯æŒå­¦æœ¯ã€è¡Œä¸šå’Œå…¬å…±æœåŠ¡åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96857ab56dcedbf6a55637c2e16a8cdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26d3f5b11339514580384d3602e4accc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30bae77143c31c69ae8f69f772bce9ed.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Optuna-vs-Code-Llama-Are-LLMs-a-New-Paradigm-for-Hyperparameter-Tuning"><a href="#Optuna-vs-Code-Llama-Are-LLMs-a-New-Paradigm-for-Hyperparameter-Tuning" class="headerlink" title="Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?"></a>Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?</h2><p><strong>Authors:Roman Kochnev, Arash Torabi Goodarzi, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte</strong></p>
<p>Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of using large language models (LLMs) for hyperparameter optimization by employing a fine-tuned version of Code Llama. Through parameter-efficient fine-tuning using LoRA, we adapt the LLM to generate accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional methods such as Optuna, which rely on exhaustive trials, the proposed approach achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our approach highlights that LLM-based optimization not only matches state-of-the-art methods like Tree-structured Parzen Estimators but also accelerates the tuning process. This positions LLMs as a promising alternative to conventional optimization techniques, particularly for rapid experimentation. Furthermore, the ability to generate hyperparameters in a single inference step makes this method particularly well-suited for resource-constrained environments such as edge devices and mobile applications, where computational efficiency is paramount. The results confirm that LLMs, beyond their efficiency, offer substantial time savings and comparable stability, underscoring their value in advancing machine learning workflows. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research. </p>
<blockquote>
<p>æœ€ä¼˜è¶…å‚æ•°çš„é€‰æ‹©å¯¹äºæœ€å¤§åŒ–ç¥ç»ç½‘ç»œæ€§èƒ½è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹å¤æ‚åº¦ä¸æ–­å¢é•¿çš„æƒ…å†µä¸‹ã€‚æœ¬ç ”ç©¶é€šè¿‡é‡‡ç”¨ç»è¿‡ç²¾ç»†è°ƒæ•´çš„Code Llamaç‰ˆæœ¬ï¼Œæ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–çš„å¯è¡Œæ€§ã€‚é€šè¿‡LoRAå®ç°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œæˆ‘ä»¬ä½¿LLMèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹å„ç§ç¥ç»ç½‘ç»œæ¶æ„å‡†ç¡®ä¸”é«˜æ•ˆçš„è¶…å‚æ•°æ¨èã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚Optunaï¼‰ä¸åŒï¼ŒOptunaä¾èµ–äºè¯¦å°½çš„è¯•éªŒï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰æ–¹é¢å–å¾—äº†å…·æœ‰ç«äº‰åŠ›æˆ–æ›´é«˜çš„ç»“æœï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„æ–¹æ³•çªæ˜¾äº†åŸºäºLLMçš„ä¼˜åŒ–ä¸ä»…ä¸æœ€æ–°çš„æ–¹æ³•ï¼ˆå¦‚æ ‘ç»“æ„Parzenä¼°è®¡å™¨ï¼‰ç›¸åŒ¹é…ï¼Œè€Œä¸”è¿˜åŠ é€Ÿäº†è°ƒæ•´è¿‡ç¨‹ã€‚è¿™ä¸ºLLMåœ¨å¸¸è§„ä¼˜åŒ–æŠ€æœ¯ä¸­ä½œä¸ºæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¿«é€Ÿå®éªŒæ–¹é¢æä¾›äº†åœ°ä½ã€‚æ­¤å¤–ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªæ¨ç†æ­¥éª¤ä¸­ç”Ÿæˆè¶…å‚æ•°çš„æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒï¼Œå¦‚è¾¹ç¼˜è®¾å¤‡å’Œç§»åŠ¨åº”ç”¨ç¨‹åºï¼Œå…¶ä¸­è®¡ç®—æ•ˆç‡è‡³å…³é‡è¦ã€‚ç»“æœè¯å®ï¼Œé™¤äº†æé«˜æ•ˆç‡å¤–ï¼ŒLLMè¿˜èƒ½æä¾›å¯è§‚çš„æ—¶é—´èŠ‚çœå’Œç›¸å½“çš„ç¨³å®šæ€§ï¼Œçªæ˜¾å…¶åœ¨æ¨è¿›æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸­çš„ä»·å€¼ã€‚æ‰€æœ‰ç”Ÿæˆçš„è¶…å‚æ•°éƒ½åŒ…å«åœ¨LEMURç¥ç»ç½‘ç»œï¼ˆNNï¼‰æ•°æ®é›†ä¸­ï¼Œè¯¥æ•°æ®é›†å…¬å¼€å¯ç”¨ï¼Œå¹¶ä½œä¸ºè¶…å‚æ•°ä¼˜åŒ–ç ”ç©¶çš„ä¸€ä¸ªå¼€æºåŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç¥ç»ç½‘ç»œè¶…å‚æ•°ä¼˜åŒ–çš„å¯è¡Œæ€§ã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´Code Llamaç‰ˆæœ¬ï¼Œç»“åˆLoRAå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼ŒLLMèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹å¤šç§ç¥ç»ç½‘ç»œæ¶æ„çš„å‡†ç¡®ä¸”é«˜æ•ˆçš„è¶…å‚æ•°æ¨èã€‚è¯¥æ–¹æ³•ä¸ä¼ ç»Ÿçš„Optunaç­‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰æ–¹é¢è¡¨ç°å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚LLMä¼˜åŒ–ä¸ä»…åŒ¹é…äº†å½“å‰å…ˆè¿›æŠ€æœ¯ï¼Œå¦‚æ ‘ç»“æ„Parzenä¼°è®¡å™¨ï¼Œè¿˜åŠ é€Ÿäº†è°ƒæ•´è¿‡ç¨‹ï¼Œç‰¹åˆ«é€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡å’Œç§»åŠ¨åº”ç”¨ç­‰èµ„æºå—é™çš„ç¯å¢ƒã€‚LLMåœ¨æ¨è¿›æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹æ–¹é¢å±•ç°å‡ºäº†ä»·å€¼ï¼Œä¸ä»…æ•ˆç‡é«˜ï¼Œè¿˜èƒ½èŠ‚çœå¤§é‡æ—¶é—´å¹¶ä¿æŒç¨³å®šæ€§ã€‚ç”Ÿæˆçš„è¶…å‚æ•°å·²åŒ…å«åœ¨LEMURç¥ç»ç½‘ç»œæ•°æ®é›†ä¸­ï¼Œè¯¥æ•°æ®é›†å…¬å¼€å¯ç”¨ï¼Œä¸ºè¶…å‚æ•°ä¼˜åŒ–ç ”ç©¶æä¾›äº†å¼€æºåŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºç¥ç»ç½‘ç»œè¶…å‚æ•°ä¼˜åŒ–ï¼Œå…·æœ‰æ½œåŠ›æ›¿ä»£ä¼ ç»Ÿä¼˜åŒ–æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è°ƒæ•´Code Llamaå¹¶ç»“åˆLoRAæŠ€æœ¯ï¼ŒLLMèƒ½ç”Ÿæˆé’ˆå¯¹å¤šç§ç¥ç»ç½‘ç»œæ¶æ„çš„è¶…å‚æ•°æ¨èã€‚</li>
<li>LLMæ–¹æ³•åœ¨å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè®¡ç®—å¼€é”€è¾ƒå°ã€‚</li>
<li>LLMä¼˜åŒ–ä¸ä»…åŒ¹é…äº†å½“å‰å…ˆè¿›æŠ€æœ¯ï¼Œè¿˜èƒ½åŠ é€Ÿè¶…å‚æ•°è°ƒæ•´è¿‡ç¨‹ã€‚</li>
<li>LLMæ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒï¼Œå¦‚è¾¹ç¼˜è®¾å¤‡å’Œç§»åŠ¨åº”ç”¨ã€‚</li>
<li>LLMåœ¨æ¨è¿›æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹æ–¹é¢å…·æœ‰é«˜æ•ˆã€çœæ—¶å’Œç¨³å®šçš„ä¼˜ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c84bc3908e36be61950994be9b9bd9f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6682a4982597852796bcf484d75d8a96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c858780b7ddbb83d8eddb650e590c9ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2931d71f5fd89bd2ed58bd642844599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be0260789b3d9962df4fab7dd2a36fce.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NativQA-Framework-Enabling-LLMs-with-Native-Local-and-Everyday-Knowledge"><a href="#NativQA-Framework-Enabling-LLMs-with-Native-Local-and-Everyday-Knowledge" class="headerlink" title="NativQA Framework: Enabling LLMs with Native, Local, and Everyday   Knowledge"></a>NativQA Framework: Enabling LLMs with Native, Local, and Everyday   Knowledge</h2><p><strong>Authors:Firoj Alam, Md Arid Hasan, Sahinur Rahman Laskar, Mucahid Kutlu, Shammur Absar Chowdhury</strong></p>
<p>The rapid advancement of large language models (LLMs) has raised concerns about cultural bias, fairness, and their applicability in diverse linguistic and underrepresented regional contexts. To enhance and benchmark the capabilities of LLMs, there is a need to develop large-scale resources focused on multilingual, local, and cultural contexts. In this study, we propose a framework, NativQA, that can seamlessly construct large-scale, culturally and regionally aligned QA datasets in native languages. The framework utilizes user-defined seed queries and leverages search engines to collect location-specific, everyday information. It has been evaluated across 39 locations in 24 countries and in 7 languages, ranging from extremely low-resource to high-resource languages, which resulted over 300K Question Answer (QA) pairs. The developed resources can be used for LLM benchmarking and further fine-tuning. The framework has been made publicly available for the community (<a target="_blank" rel="noopener" href="https://gitlab.com/nativqa/nativqa-framework">https://gitlab.com/nativqa/nativqa-framework</a>). </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹æ–‡åŒ–åè§ã€å…¬å¹³æ€§åŠå…¶åœ¨å¤šæ ·åŒ–å’Œä»£è¡¨æ€§ä¸è¶³çš„åŒºåŸŸè¯­å¢ƒä¸­çš„é€‚ç”¨æ€§çš„å…³æ³¨ã€‚ä¸ºäº†å¢å¼ºå’Œè¯„ä¼°LLMçš„èƒ½åŠ›ï¼Œéœ€è¦å¼€å‘ä¸“æ³¨äºå¤šè¯­è¨€ã€æœ¬åœ°å’Œæ–‡åŒ–èƒŒæ™¯çš„å¤§å‹èµ„æºã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºNativQAçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ— ç¼æ„å»ºå¤§è§„æ¨¡ã€æ–‡åŒ–å’ŒåŒºåŸŸå¯¹é½çš„æœ¬åœ°è¯­è¨€é—®ç­”æ•°æ®é›†ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”¨æˆ·å®šä¹‰çš„ç§å­æŸ¥è¯¢ï¼Œå¹¶å€ŸåŠ©æœç´¢å¼•æ“æ”¶é›†ç‰¹å®šåœ°ç‚¹çš„æ—¥å¸¸ä¿¡æ¯ã€‚å®ƒå·²åœ¨24ä¸ªå›½å®¶çš„39ä¸ªåœ°ç‚¹å’Œ7ç§è¯­è¨€ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†ä»æåº¦ä½èµ„æºåˆ°é«˜èµ„æºè¯­è¨€çš„å„ç§æƒ…å†µï¼Œäº§ç”Ÿäº†è¶…è¿‡30ä¸‡ç»„é—®ç­”å¯¹ã€‚æ‰€å¼€å‘çš„èµ„æºå¯ç”¨äºLLMåŸºå‡†æµ‹è¯•å’Œè¿›ä¸€æ­¥çš„å¾®è°ƒã€‚è¯¥æ¡†æ¶å·²å‘å…¬ä¼—æä¾›ï¼Œä¾›ç¤¾åŒºä½¿ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://gitlab.com/nativqa/nativqa-framework%EF%BC%89%E3%80%82">https://gitlab.com/nativqa/nativqa-frameworkï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05995v1">PDF</a> LLMs, Native, Multilingual, Language Diversity, Contextual   Understanding, Minority Languages, Culturally Informed, Foundation Models,   Large Language Models</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å…³äºæ–‡åŒ–åè§ã€å…¬å¹³æ€§ä»¥åŠå…¶åœ¨å¤šæ ·åŒ–å’Œä»£è¡¨æ€§ä¸è¶³çš„è¯­å¢ƒä¸­é€‚ç”¨æ€§çš„å…³æ³¨ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºNativQAçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè½»æ¾æ„å»ºå¤§è§„æ¨¡ã€ä¸æ–‡åŒ–å’Œåœ°åŒºç›¸é€‚åº”çš„é—®ç­”æ•°æ®é›†ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”¨æˆ·å®šä¹‰çš„ç§å­æŸ¥è¯¢å’Œæœç´¢å¼•æ“æ”¶é›†ç‰¹å®šåœ°ç‚¹çš„æ—¥å¸¸ä¿¡æ¯ã€‚è¯¥æ¡†æ¶å·²åœ¨24ä¸ªå›½å®¶çš„39ä¸ªåœ°ç‚¹å’Œ7ç§è¯­è¨€ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†ä»æåº¦ä½èµ„æºåˆ°é«˜èµ„æºè¯­è¨€çš„ä¸åŒæƒ…å†µï¼Œäº§ç”Ÿäº†è¶…è¿‡30ä¸‡ç»„é—®ç­”å¯¹ã€‚æ‰€å¼€å‘çš„èµ„æºå¯ç”¨äºLLMåŸºå‡†æµ‹è¯•å’Œè¿›ä¸€æ­¥å¾®è°ƒã€‚è¯¥æ¡†æ¶å·²å‘å…¬ä¼—å¼€æ”¾ä½¿ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://gitlab.com/nativqa/nativqa-framework%EF%BC%89%E3%80%82">https://gitlab.com/nativqa/nativqa-frameworkï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•å¼•å‘äº†å¯¹æ–‡åŒ–åè§å’Œå…¬å¹³æ€§çš„å…³æ³¨ã€‚</li>
<li>åœ¨å¤šæ ·åŒ–å’Œä»£è¡¨æ€§ä¸è¶³çš„è¯­å¢ƒä¸­ï¼ŒLLMçš„é€‚ç”¨æ€§å—åˆ°æŒ‘æˆ˜ã€‚</li>
<li>NativQAæ¡†æ¶èƒ½å¤Ÿæ„å»ºä¸æ–‡åŒ–å’Œåœ°åŒºç›¸é€‚åº”çš„å¤§å‹é—®ç­”æ•°æ®é›†ã€‚</li>
<li>NativQAæ¡†æ¶åˆ©ç”¨ç”¨æˆ·å®šä¹‰çš„ç§å­æŸ¥è¯¢å’Œæœç´¢å¼•æ“æ”¶é›†ç‰¹å®šåœ°ç‚¹çš„æ—¥å¸¸ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¡†æ¶å·²åœ¨å…¨çƒå¤šä¸ªå›½å®¶å’Œè¯­è¨€ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œäº§ç”Ÿäº†å¤§é‡é—®ç­”å¯¹æ•°æ®ã€‚</li>
<li>å¼€å‘çš„èµ„æºå¯ç”¨äºLLMçš„åŸºå‡†æµ‹è¯•å’Œè¿›ä¸€æ­¥å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45b38ea89530346f8138f43baf1ebf72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8eaa6596bd4462ad73c980b42263ed0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities"><a href="#An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities" class="headerlink" title="An Empirical Study of GPT-4o Image Generation Capabilities"></a>An Empirical Study of GPT-4o Image Generation Capabilities</h2><p><strong>Authors:Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi</strong></p>
<p>The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4oâ€™s image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆé¢†åŸŸçš„æ™¯è§‚å·²ç»è¿…é€Ÿæ¼”å˜ï¼Œä»æ—©æœŸçš„åŸºäºGANçš„æ–¹æ³•åˆ°æ‰©æ•£æ¨¡å‹ï¼Œå†åˆ°æœ€è¿‘çš„å¯»æ±‚ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´æ¡¥æ¢çš„ç»Ÿä¸€ç”Ÿæˆæ¶æ„ã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå°¤å…¶æ˜¯GPT-4oï¼Œå·²ç»è¯æ˜äº†é«˜ä¿çœŸåº¦å¤šåª’ä½“ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä½†å…¶æ¶æ„è®¾è®¡ä»ç„¶ç¥ç§˜ä¸”æœªå…¬å¸ƒã€‚è¿™å¼•å‘äº†äººä»¬å…³äºå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ˜¯å¦å·²æˆåŠŸåœ°é›†æˆåˆ°è¿™äº›æ–¹æ³•çš„ç»Ÿä¸€æ¡†æ¶ä¸­çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå°†å…¶ä¸é¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†å››ä¸ªä¸»è¦ç±»åˆ«ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆï¼Œæ¶‰åŠè¶…è¿‡20é¡¹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶å°†å…¶å®šä½åœ¨æ›´å¹¿æ³›çš„ç”Ÿæˆæ¨¡å‹æ¼”å˜ä¸­ã€‚é€šè¿‡è¿™é¡¹è°ƒæŸ¥ï¼Œæˆ‘ä»¬ä¸ºæœªæ¥çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†æ¶æ„è®¾è®¡å’Œæ•°æ®æ‰©å±•çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05979v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¯¹å…¶è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œå¹¶ä¸å¼€æºå’Œå•†ä¸šé¢†å…ˆæ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚è¯„ä¼°æ¶µç›–æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆç­‰å››ä¸ªä¸»è¦ç±»åˆ«ï¼Œè¶…è¿‡20é¡¹ä»»åŠ¡ã€‚åˆ†æå¼ºè°ƒäº†GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶å°†å…¶ç½®äºç”Ÿæˆæ¨¡å‹æ›´å¹¿æ³›çš„æ¼”å˜ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oåœ¨å›¾åƒç”Ÿæˆæ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é«˜ä¿çœŸåº¦å¤šåª’ä½“ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>GPT-4oçš„æ¶æ„è®¾è®¡ä¸­èåˆäº†æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆï¼Œè¡¨æ˜ç»Ÿä¸€ç”Ÿæˆæ¶æ„çš„æ½œåŠ›ã€‚</li>
<li>GPT-4oåœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆã€‚</li>
<li>ä¸å…¶ä»–å¼€æºå’Œå•†ä¸šæ¨¡å‹ç›¸æ¯”ï¼ŒGPT-4oå…·æœ‰ä¸€å®šçš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹çš„é¢†åŸŸå†…å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</li>
<li>æ¶æ„è®¾è®¡å’Œæ•°æ®è§„æ¨¡å¯¹ç”Ÿæˆæ¨¡å‹çš„å‘å±•èµ·ç€é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ada19c845658d6086bd8f06224ef8f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d8b70545b43ecb6ffa3d082185a785b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-653e211f893e469876b0aff041185c8b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="STAGE-Stemmed-Accompaniment-Generation-through-Prefix-Based-Conditioning"><a href="#STAGE-Stemmed-Accompaniment-Generation-through-Prefix-Based-Conditioning" class="headerlink" title="STAGE: Stemmed Accompaniment Generation through Prefix-Based   Conditioning"></a>STAGE: Stemmed Accompaniment Generation through Prefix-Based   Conditioning</h2><p><strong>Authors:Giorgio Strano, Chiara Ballanti, Donato Crisostomi, Michele Mancusi, Luca Cosmo, Emanuele RodolÃ </strong></p>
<p>Recent advances in generative models have made it possible to create high-quality, coherent music, with some systems delivering production-level output.Yet, most existing models focus solely on generating music from scratch, limiting their usefulness for musicians who want to integrate such models into a human, iterative composition workflow.In this paper we introduce STAGE, our STemmed Accompaniment GEneration model, fine-tuned from the state-of-the-art MusicGen to generate single-stem instrumental accompaniments conditioned on a given mixture. Inspired by instruction-tuning methods for language models, we extend the transformerâ€™s embedding matrix with a context token, enabling the model to attend to a musical context through prefix-based conditioning.Compared to the baselines, STAGE yields accompaniments that exhibit stronger coherence with the input mixture, higher audio quality, and closer alignment with textual prompts.Moreover, by conditioning on a metronome-like track, our framework naturally supports tempo-constrained generation, achieving state-of-the-art alignment with the target rhythmic structureâ€“all without requiring any additional tempo-specific module.As a result, STAGE offers a practical, versatile tool for interactive music creation that can be readily adopted by musicians in real-world workflows. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹çš„æ–°è¿›å±•ä½¿å¾—åˆ›å»ºé«˜è´¨é‡ã€è¿è´¯çš„éŸ³ä¹æˆä¸ºå¯èƒ½ï¼Œä¸€äº›ç³»ç»Ÿç”šè‡³èƒ½å¤Ÿäº§ç”Ÿä¸“ä¸šçº§åˆ«çš„è¾“å‡ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¸“æ³¨äºä»å¤´å¼€å§‹ç”ŸæˆéŸ³ä¹ï¼Œè¿™é™åˆ¶äº†å¸Œæœ›å°†æ­¤ç±»æ¨¡å‹æ•´åˆåˆ°äººç±»è¿­ä»£ä½œæ›²å·¥ä½œæµç¨‹ä¸­çš„éŸ³ä¹å®¶çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†STAGEï¼Œæˆ‘ä»¬çš„åŸºäºèŒçš„ä¼´å¥ç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäºæœ€å…ˆè¿›çš„MusicGenè¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆç»™å®šæ··åˆç‰©çš„å•èŒä¹å™¨ä¼´å¥ã€‚å—è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬é€šè¿‡ä¸Šä¸‹æ–‡æ ‡è®°æ‰©å±•äº†å˜å‹å™¨çš„åµŒå…¥çŸ©é˜µï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡åŸºäºå‰ç¼€çš„æ¡ä»¶æ¥å…³æ³¨éŸ³ä¹ä¸Šä¸‹æ–‡ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼ŒSTAGEç”Ÿæˆçš„ä¼´å¥ä¸è¾“å…¥æ··åˆç‰©çš„è¿è´¯æ€§æ›´å¼ºï¼ŒéŸ³é¢‘è´¨é‡æ›´é«˜ï¼Œä¸æ–‡æœ¬æç¤ºçš„å¯¹é½ç¨‹åº¦æ›´é«˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŸºäºèŠ‚æ‹å™¨è½¨è¿¹çš„æ¡ä»¶è®¾ç½®ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è‡ªç„¶åœ°æ”¯æŒèŠ‚å¥çº¦æŸç”Ÿæˆï¼Œå®ç°äº†ä¸ç›®æ ‡èŠ‚å¥ç»“æ„çš„ä¸€æµå¯¹é½â€”â€”æ‰€æœ‰è¿™ä¸€åˆ‡éƒ½ä¸éœ€è¦ä»»ä½•é¢å¤–çš„ç‰¹å®šèŠ‚å¥çš„æ¨¡å—ã€‚å› æ­¤ï¼ŒSTAGEä¸ºäº¤äº’å¼éŸ³ä¹åˆ›ä½œæä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å¤šåŠŸèƒ½å·¥å…·ï¼Œå¯ä»¥è¢«éŸ³ä¹å®¶åœ¨ç°å®å·¥ä½œæµç¨‹ä¸­è½»æ¾é‡‡ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05690v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹æŠ€æœ¯å·²èƒ½åˆ¶ä½œå‡ºé«˜è´¨é‡ã€è¿è´¯çš„éŸ³ä¹ï¼ŒæŸäº›ç³»ç»Ÿç”šè‡³èƒ½äº§å‡ºä¸“ä¸šçº§åˆ«çš„ä½œå“ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¤§å¤šä¸“æ³¨äºä»é›¶å¼€å§‹ç”ŸæˆéŸ³ä¹ï¼Œè¿™å¯¹äºå¸Œæœ›å°†è¿™ç±»æ¨¡å‹èå…¥ä¸ªäººåˆ›ä½œæµç¨‹çš„ä¹æ‰‹è€Œè¨€ï¼Œå®ç”¨æ€§æœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†STAGEæ¨¡å‹ï¼Œå®ƒåœ¨MusicGençš„åŸºç¡€ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œç”¨äºç”Ÿæˆç»™å®šæ—‹å¾‹çš„å•ä¹å™¨ä¼´å¥ã€‚å—è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒæ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬é€šè¿‡æ‰©å±•è½¬æ¢å™¨åµŒå…¥çŸ©é˜µæ·»åŠ äº†è¯­å¢ƒä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½é€šè¿‡å‰ç¼€æ¡ä»¶å…³æ³¨éŸ³ä¹è¯­å¢ƒã€‚ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼ŒSTAGEç”Ÿæˆçš„ä¼´å¥ä¸è¾“å…¥æ—‹å¾‹æ›´è¿è´¯ã€éŸ³é¢‘è´¨é‡æ›´é«˜ã€ä¸æ–‡å­—æç¤ºæ›´å»åˆã€‚æ­¤å¤–ï¼Œé€šè¿‡ä»¥èŠ‚æ‹å™¨èˆ¬çš„è½¨è¿¹ä¸ºæ¡ä»¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½è‡ªç„¶åœ°è¿›è¡ŒèŠ‚å¥çº¦æŸç”Ÿæˆï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„èŠ‚å¥ç‰¹å®šæ¨¡å—å³å¯å®ç°å¯¹ç›®æ ‡èŠ‚å¥ç»“æ„çš„æœ€ä½³å¯¹é½ã€‚å› æ­¤ï¼ŒSTAGEä¸ºäº¤äº’å¼éŸ³ä¹åˆ›ä½œæä¾›äº†ä¸€ä¸ªå®ç”¨ã€å¤šåŠŸèƒ½å·¥å…·ï¼Œä¹æ‰‹å¯è½»æ¾å°†å…¶èå…¥å®é™…åˆ›ä½œæµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹èƒ½åˆ¶ä½œå‡ºé«˜è´¨é‡ã€è¿è´¯çš„éŸ³ä¹ï¼Œä¸”éƒ¨åˆ†ç³»ç»Ÿå·²å¯è¾¾ä¸“ä¸šçº§åˆ«ã€‚</li>
<li>ç°æœ‰éŸ³ä¹ç”Ÿæˆæ¨¡å‹å¤§å¤šä»…èƒ½ä»é›¶å¼€å§‹ç”ŸæˆéŸ³ä¹ï¼Œå¯¹ä¹æ‰‹è€Œè¨€å®ç”¨æ€§æœ‰é™ã€‚</li>
<li>STAGEæ¨¡å‹åœ¨MusicGenåŸºç¡€ä¸Šå¾®è°ƒï¼Œå¯ç”Ÿæˆç»™å®šæ—‹å¾‹çš„å•ä¹å™¨ä¼´å¥ã€‚</li>
<li>STAGEé€šè¿‡æ‰©å±•è½¬æ¢å™¨åµŒå…¥çŸ©é˜µæ·»åŠ è¯­å¢ƒä»¤ç‰Œï¼Œä½¿æ¨¡å‹å…³æ³¨éŸ³ä¹è¯­å¢ƒã€‚</li>
<li>STAGEç”Ÿæˆçš„ä¼´å¥ä¸è¾“å…¥æ—‹å¾‹æ›´è¿è´¯ï¼ŒéŸ³é¢‘è´¨é‡æ›´é«˜ï¼Œä¸æ–‡å­—æç¤ºæ›´å»åˆã€‚</li>
<li>STAGEèƒ½ä»¥èŠ‚æ‹å™¨èˆ¬çš„è½¨è¿¹ä¸ºæ¡ä»¶è¿›è¡ŒèŠ‚å¥çº¦æŸç”Ÿæˆï¼Œå®ç°å¯¹ç›®æ ‡èŠ‚å¥ç»“æ„çš„æœ€ä½³å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-79301d2e5f224b4607a5edca2831f746.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d20ceef31ac5f7ebd985f18d938fa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d7ee14a5404d19e655608f82e46228c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d84691c7200e7602459b6b2a7fa477ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d291bc873cbce5295c82cf82fc65c0d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecd4bfe0bcc82a404d9500c62a8e820c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TAGC-Optimizing-Gradient-Communication-in-Distributed-Transformer-Training"><a href="#TAGC-Optimizing-Gradient-Communication-in-Distributed-Transformer-Training" class="headerlink" title="TAGC: Optimizing Gradient Communication in Distributed Transformer   Training"></a>TAGC: Optimizing Gradient Communication in Distributed Transformer   Training</h2><p><strong>Authors:Igor Polyakov, Alexey Dukhanov, Egor Spirin</strong></p>
<p>The increasing complexity of large language models (LLMs) necessitates efficient training strategies to mitigate the high computational costs associated with distributed training. A significant bottleneck in this process is gradient synchronization across multiple GPUs, particularly in the zero-redundancy parallelism mode. In this paper, we introduce Transformer-Aware Gradient Compression (TAGC), an optimized gradient compression algorithm designed specifically for transformer-based models. TAGC extends the lossless homomorphic compression method by adapting it for sharded models and incorporating transformer-specific optimizations, such as layer-selective compression and dynamic sparsification. Our experimental results demonstrate that TAGC accelerates training by up to 15% compared to the standard Fully Sharded Data Parallel (FSDP) approach, with minimal impact on model quality. We integrate TAGC into the PyTorch FSDP framework, the implementation is publicly available at <a target="_blank" rel="noopener" href="https://github.com/ipolyakov/TAGC">https://github.com/ipolyakov/TAGC</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ€§ä¸æ–­å¢åŠ ï¼Œéœ€è¦æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥æ¥ç¼“è§£ä¸åˆ†å¸ƒå¼è®­ç»ƒç›¸å…³çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªä¸»è¦çš„ç“¶é¢ˆæ˜¯è·¨å¤šä¸ªGPUçš„æ¢¯åº¦åŒæ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶å†—ä½™å¹¶è¡Œæ¨¡å¼ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Transformeræ„ŸçŸ¥æ¢¯åº¦å‹ç¼©ï¼ˆTAGCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºåŸºäºtransformerçš„æ¨¡å‹è®¾è®¡çš„ä¼˜åŒ–æ¢¯åº¦å‹ç¼©ç®—æ³•ã€‚TAGCé€šè¿‡å¯¹æ— æŸåŒæ€å‹ç¼©æ–¹æ³•è¿›è¡Œæ‰©å±•ï¼Œé€šè¿‡å°†å…¶é€‚é…äºåˆ†ç‰‡æ¨¡å‹å¹¶å¼•å…¥é’ˆå¯¹transformerçš„ä¼˜åŒ–ï¼Œå¦‚å±‚é€‰æ‹©å‹ç¼©å’ŒåŠ¨æ€ç¨€ç–åŒ–ï¼Œä»è€Œå®ç°å¯¹å®ƒçš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†çš„å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDPï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒTAGCå¯ä»¥åŠ é€Ÿè®­ç»ƒè¾¾15%ï¼Œè€Œå¯¹æ¨¡å‹è´¨é‡çš„å½±å“æœ€å°ã€‚æˆ‘ä»¬å°†TAGCé›†æˆåˆ°PyTorch FSDPæ¡†æ¶ä¸­ï¼Œå®ç°å…¬å¼€å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/ipolyakov/TAGC%E3%80%82">https://github.com/ipolyakov/TAGCã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05638v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜æ˜‚é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTransformer-Aware Gradient Compressionï¼ˆTAGCï¼‰çš„ä¼˜åŒ–æ¢¯åº¦å‹ç¼©ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡é€‚åº”æ— æŸåŒæ€å‹ç¼©æ–¹æ³•ï¼Œé’ˆå¯¹åˆ†ç‰‡æ¨¡å‹è¿›è¡Œç‰¹å®šä¼˜åŒ–ï¼Œå¹¶å¼•å…¥å¦‚å±‚é€‰æ‹©å‹ç¼©å’ŒåŠ¨æ€ç¨€ç–åŒ–ç­‰é’ˆå¯¹transformeræ¨¡å‹çš„ä¼˜åŒ–æ‰‹æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAGCç›¸è¾ƒäºæ ‡å‡†çš„Fully Sharded Data Parallelï¼ˆFSDPï¼‰æ–¹æ³•ï¼Œèƒ½å¤ŸåŠ é€Ÿè®­ç»ƒé«˜è¾¾15%ï¼ŒåŒæ—¶å¯¹æ¨¡å‹è´¨é‡å½±å“æå°ã€‚è¯¥ç®—æ³•å·²é›†æˆåˆ°PyTorchçš„FSDPæ¡†æ¶ä¸­ï¼Œå¹¶å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒé¢ä¸´é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œéœ€è¦é«˜æ•ˆè®­ç»ƒç­–ç•¥ã€‚</li>
<li>æ¢¯åº¦åŒæ­¥æ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„ä¸»è¦ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶å†—ä½™å¹¶è¡Œæ¨¡å¼ä¸‹ã€‚</li>
<li>Transformer-Aware Gradient Compressionï¼ˆTAGCï¼‰æ˜¯ä¸€ç§é’ˆå¯¹åŸºäºtransformerçš„æ¨¡å‹çš„ä¼˜åŒ–æ¢¯åº¦å‹ç¼©ç®—æ³•ã€‚</li>
<li>TAGCé€šè¿‡é€‚åº”æ— æŸåŒæ€å‹ç¼©æ–¹æ³•ï¼Œå¹¶é’ˆå¯¹åˆ†ç‰‡æ¨¡å‹è¿›è¡Œç‰¹å®šä¼˜åŒ–ã€‚</li>
<li>TAGCå¼•å…¥å±‚é€‰æ‹©å‹ç¼©å’ŒåŠ¨æ€ç¨€ç–åŒ–ç­‰é’ˆå¯¹transformeræ¨¡å‹çš„ä¼˜åŒ–æ‰‹æ®µã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTAGCèƒ½å¤ŸåŠ é€Ÿè®­ç»ƒé«˜è¾¾15%ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29c3431be623a9ea04a1ad9b3570c6cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b402f05bea98533d6fc10b44aeadb264.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e69c3721c900a6ca39df8ab77d5177e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e0a2e4863b59214c5b7de0d942a0170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c7e49b74e2f8e64d50ef2f951d3bf37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c433b3f5bc9ccc5dc40a2b83d4ef7da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-033c2db9c3a06401dd5c21a8dc8d1530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4693ca7dfd692ff5a5d92bd9aba20449.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6946fa3505d07a252479d989dc58cc36.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning"><a href="#Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning" class="headerlink" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning"></a>Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, Jiawei Han</strong></p>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆï¼Œè·å–å¤–éƒ¨çŸ¥è¯†å’Œæœ€æ–°ä¿¡æ¯è‡³å…³é‡è¦ã€‚å°½ç®¡å¯ä»¥é€šè¿‡æç¤ºå…·æœ‰æ¨ç†èƒ½åŠ›çš„é«˜çº§LLMåœ¨æ¨ç†æ—¶ä½¿ç”¨æœç´¢å¼•æ“æ¥è¿›è¡Œæ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆï¼Œä½†ç”±äºLLMå¯èƒ½æ— æ³•å®Œå…¨æŒæ¡å¦‚ä½•ä¸æœç´¢å¼•æ“è¿›è¡Œæœ€ä½³äº¤äº’çš„æ–¹æ³•ï¼Œå› æ­¤é€šå¸¸å¹¶ä¸ç†æƒ³ã€‚æœ¬æ–‡ä»‹ç»äº†Search-R1ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨ç†æ¡†æ¶çš„æ‰©å±•ï¼Œè¯¥æ¡†æ¶ä½¿LLMèƒ½å¤Ÿåœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢ï¼Œå¹¶å®æ—¶æ£€ç´¢ã€‚Search-R1é€šè¿‡å¤šè½®æœç´¢äº¤äº’ä¼˜åŒ–LLMçš„æ¨ç†è½¨è¿¹ï¼Œåˆ©ç”¨æ£€ç´¢ä»¤ç‰Œå±è”½è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒä»¥åŠç®€å•çš„åŸºäºç»“æœå¥–åŠ±å‡½æ•°ã€‚åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼ŒSearch-R1åœ¨å„ç§RAGåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æé«˜äº†41%ï¼ˆQwen2.5-7Bï¼‰å’Œ20%ï¼ˆQwen2.5-3Bï¼‰ã€‚æœ¬æ–‡è¿˜è¿›ä¸€æ­¥æä¾›äº†å…³äºRLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨æ€çš„å®è¯è§è§£ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä½äº<a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1%E3%80%82">https://github.com/PeterGriffinJin/Search-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09516v3">PDF</a> 31 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Search-R1ï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¨ç†æ¡†æ¶çš„æ‰©å±•ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢ï¼Œè¿›è¡Œå®æ—¶æ£€ç´¢ã€‚Search-R1é€šè¿‡å¤šè½®æœç´¢äº¤äº’ä¼˜åŒ–LLMçš„æ¨ç†è½¨è¿¹ï¼Œåˆ©ç”¨æ£€ç´¢ä»¤ç‰Œå±è”½æŠ€æœ¯è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒï¼Œå¹¶é‡‡ç”¨åŸºäºç»“æœçš„ç®€å•å¥–åŠ±å‡½æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒSearch-R1åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå„ç§RAGåŸºå‡†æµ‹è¯•ï¼Œæé«˜äº†41%ï¼ˆQwen2.5-7Bï¼‰å’Œ20%ï¼ˆQwen2.5-3Bï¼‰çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Search-R1æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ¨ç†æ¡†æ¶ä¸­çš„ä¸€é¡¹æ‰©å±•åº”ç”¨ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æœç´¢èƒ½åŠ›ã€‚</li>
<li>LLMèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼Œè¿›è¡Œå®æ—¶æ£€ç´¢ï¼Œè¿™æœ‰åŠ©äºæé«˜æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>Search-R1é‡‡ç”¨å¤šè½®æœç´¢äº¤äº’æ¥ä¼˜åŒ–LLMçš„æ¨ç†è½¨è¿¹ï¼Œè¿™æ„å‘³ç€æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›ã€‚</li>
<li>é€šè¿‡æ£€ç´¢ä»¤ç‰Œå±è”½æŠ€æœ¯ï¼ŒSearch-R1å®ç°äº†ç¨³å®šçš„RLè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºç»“æœçš„ç®€å•å¥–åŠ±å‡½æ•°æ˜¯Search-R1çš„å¦ä¸€ä¸ªå…³é”®ç‰¹ç‚¹ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å¹¶å¼•å¯¼æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSearch-R1åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2d557fd71b904b6d8a465233c77d0be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af27f02acfaa2b3e1c7056aedffd6780.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Efficient-Response-Generation-Strategy-Selection-for-Fine-Tuning-Large-Language-Models-Through-Self-Aligned-Perplexity"><a href="#Efficient-Response-Generation-Strategy-Selection-for-Fine-Tuning-Large-Language-Models-Through-Self-Aligned-Perplexity" class="headerlink" title="Efficient Response Generation Strategy Selection for Fine-Tuning Large   Language Models Through Self-Aligned Perplexity"></a>Efficient Response Generation Strategy Selection for Fine-Tuning Large   Language Models Through Self-Aligned Perplexity</h2><p><strong>Authors:Xuan Ren, Qi Chen, Lingqiao Liu</strong></p>
<p>Fine-tuning large language models (LLMs) typically relies on producing large sets of input-output pairs. Yet for a given question, there can be many valid outputs. In practice, these outputs are often derived by distilling knowledge from teacher models, and they can vary depending on the specific teacher model or prompting strategy employed. Recent findings show that how these training outputs are generated can significantly affect the performance of the fine-tuned model, raising an important question: how do we pick the best data generation method from among numerous possibilities? Rather than exhaustively training and evaluating on each candidate, this paper proposes a scalable approximate method that assesses a small subset of generated data to estimate its suitability for a specific target LLM. Our central idea is that effective outputs should be familiar to the target LLM. While previous work measures familiarity with perplexity, we find that perplexity might be suboptimal in characterizing â€˜familiarityâ€™ through theoretical analysis and practical observations. To address this, we introduce self-aligned perplexity, a novel metric capturing how closely candidate outputs adhere to the target LLMâ€™s own style and reasoning patterns. In this way, we can identify the most effective generation strategy on a small sample, then apply it to produce the complete training set. We demonstrate that training on data generated by the chosen method yields significant improvements across diverse reasoning-focused benchmarks. </p>
<blockquote>
<p>å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒé€šå¸¸ä¾èµ–äºç”Ÿæˆå¤§é‡çš„è¾“å…¥è¾“å‡ºå¯¹ã€‚ç„¶è€Œï¼Œå¯¹äºç»™å®šçš„é—®é¢˜ï¼Œå¯èƒ½æœ‰å¤šç§æœ‰æ•ˆçš„è¾“å‡ºã€‚å®é™…ä¸Šï¼Œè¿™äº›è¾“å‡ºé€šå¸¸æ˜¯é€šè¿‡ä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼çŸ¥è¯†è€Œå¾—åˆ°çš„ï¼Œå¹¶ä¸”å®ƒä»¬ä¼šæ ¹æ®æ‰€ä½¿ç”¨çš„å…·ä½“æ•™å¸ˆæ¨¡å‹æˆ–æç¤ºç­–ç•¥è€Œå˜åŒ–ã€‚æœ€æ–°ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›è®­ç»ƒè¾“å‡ºçš„ç”Ÿæˆæ–¹å¼ä¼šæ˜¾è‘—å½±å“å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™å°±å¼•å‘äº†ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•åœ¨ä¼—å¤šå¯èƒ½æ€§ä¸­æŒ‘é€‰å‡ºæœ€ä½³çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Ÿæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„è¿‘ä¼¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨è¯„ä¼°ä¸€å°éƒ¨åˆ†ç”Ÿæˆçš„æ•°æ®ï¼Œä»¥ä¼°ç®—å…¶å¯¹äºç‰¹å®šç›®æ ‡LLMçš„é€‚ç”¨æ€§ï¼Œè€Œä¸æ˜¯å…¨é¢åœ°å¯¹æ¯ä¸ªå€™é€‰æ–¹æ³•è¿›è¡ŒåŸ¹è®­å’Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œæœ‰æ•ˆçš„è¾“å‡ºåº”è¯¥ä¸ºç›®æ ‡LLMæ‰€ç†Ÿæ‚‰ã€‚è™½ç„¶ä»¥å‰çš„å·¥ä½œé€šè¿‡å›°æƒ‘åº¦æ¥è¡¡é‡ç†Ÿæ‚‰ç¨‹åº¦ï¼Œä½†æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡ç†è®ºåˆ†æå’Œå®é™…è§‚å¯Ÿï¼Œå›°æƒ‘åº¦å¯èƒ½åœ¨æè¿°â€œç†Ÿæ‚‰åº¦â€æ–¹é¢å¹¶éæœ€ä½³é€‰é¡¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæˆ‘å¯¹é½å›°æƒ‘åº¦è¿™ä¸€æ–°å‹æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡èƒ½å¤Ÿç´§å¯†æ•æ‰å€™é€‰è¾“å‡ºæ˜¯å¦ç¬¦åˆç›®æ ‡LLMè‡ªèº«çš„é£æ ¼å’Œæ¨ç†æ¨¡å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å°æ ·æœ¬ä¸Šç¡®å®šæœ€æœ‰æ•ˆçš„ç”Ÿæˆç­–ç•¥ï¼Œç„¶åå°†å…¶åº”ç”¨äºç”Ÿæˆå®Œæ•´çš„è®­ç»ƒé›†ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨æ‰€é€‰æ–¹æ³•ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨å¤šç§ä¾§é‡äºæ¨ç†çš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11779v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒé€šå¸¸ä¾èµ–äºç”Ÿæˆå¤§é‡çš„è¾“å…¥è¾“å‡ºå¯¹ã€‚å¯¹äºç»™å®šçš„é—®é¢˜ï¼Œå­˜åœ¨å¤šç§æœ‰æ•ˆçš„è¾“å‡ºã€‚å®è·µä¸­ï¼Œè¿™äº›è¾“å‡ºå¾€å¾€é€šè¿‡æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è’¸é¦å¾—åˆ°ï¼Œå¹¶ä¼šå› ç‰¹å®šçš„æ•™å¸ˆæ¨¡å‹æˆ–æç¤ºç­–ç•¥çš„ä¸åŒè€Œæœ‰æ‰€å˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„è¿‘ä¼¼æ–¹æ³•ï¼Œé€šè¿‡è¯„ä¼°ä¸€å°éƒ¨åˆ†ç”Ÿæˆçš„æ•°æ®æ¥ä¼°è®¡å…¶å¯¹äºç‰¹å®šç›®æ ‡LLMçš„é€‚å®œæ€§ã€‚æœ‰æ•ˆçš„è¾“å‡ºåº”è¯¥å¯¹äºç›®æ ‡LLMæ¥è¯´æ˜¯ç†Ÿæ‚‰çš„ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è·µè§‚å¯Ÿå‘ç°ï¼Œæœ¬æ–‡å¼•å…¥äº†è‡ªæˆ‘å¯¹é½å›°æƒ‘åº¦è¿™ä¸€æ–°æŒ‡æ ‡æ¥è¡¡é‡å€™é€‰è¾“å‡ºå¯¹ç›®æ ‡LLMé£æ ¼å’Œæ¨ç†æ¨¡å¼çš„è´´åˆç¨‹åº¦ï¼Œä»è€Œè¯†åˆ«å‡ºæœ€æœ‰æ•ˆçš„ç”Ÿæˆç­–ç•¥ï¼Œå¹¶å°†å…¶åº”ç”¨äºå®Œæ•´çš„è®­ç»ƒé›†ç”Ÿæˆä¸­ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨æ‰€é€‰æ–¹æ³•ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒä¾èµ–äºç”Ÿæˆå¤§é‡çš„è¾“å…¥è¾“å‡ºå¯¹ã€‚</li>
<li>è¿™äº›è¾“å‡ºå¾€å¾€é€šè¿‡æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è’¸é¦å¾—åˆ°ï¼Œå¹¶ä¼šå› ä¸åŒçš„æ•™å¸ˆæ¨¡å‹æˆ–æç¤ºç­–ç•¥è€Œæœ‰æ‰€å˜åŒ–ã€‚</li>
<li>æœ‰æ•ˆè¾“å‡ºçš„ç‰¹ç‚¹æ˜¯å¯¹äºç›®æ ‡LLMæ¥è¯´æ˜¯ç†Ÿæ‚‰çš„ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ å€™é€‰è¾“å‡ºçš„â€œç†Ÿæ‚‰åº¦â€ï¼Œå› æ­¤å¼•å…¥äº†è‡ªæˆ‘å¯¹é½å›°æƒ‘åº¦è¿™ä¸€æ–°æŒ‡æ ‡æ¥è¡¡é‡å…¶ä¸ç›®æ ‡LLMé£æ ¼å’Œæ¨ç†æ¨¡å¼çš„è´´åˆç¨‹åº¦ã€‚</li>
<li>é€šè¿‡ç†è®ºåˆ†æå’Œå®è·µè§‚å¯Ÿå‘ç°è‡ªæˆ‘å¯¹é½å›°æƒ‘åº¦çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>é€šè¿‡è¯†åˆ«æœ€æœ‰æ•ˆçš„ç”Ÿæˆç­–ç•¥å¹¶å°†å…¶åº”ç”¨äºå®Œæ•´çš„è®­ç»ƒé›†ç”Ÿæˆä¸­ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f13f92ff478b0596aae14a9a9bc0d542.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e824b88a69f91a6a08a7614e32bd1f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ce37888dd0b151f135173ec712c7aa8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Expertized-Caption-Auto-Enhancement-for-Video-Text-Retrieval"><a href="#Expertized-Caption-Auto-Enhancement-for-Video-Text-Retrieval" class="headerlink" title="Expertized Caption Auto-Enhancement for Video-Text Retrieval"></a>Expertized Caption Auto-Enhancement for Video-Text Retrieval</h2><p><strong>Authors:Baoyao Yang, Junxiang Chen, Wanyun Li, Wenbin Yao, Yang Zhou</strong></p>
<p>Video-text retrieval has been stuck in the information mismatch caused by personalized and inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders an effective cross-modal representation alignment, resulting in ambiguous retrieval results. Although text rewriting methods have been proposed to broaden text expressions, the modality gap remains significant, as the text representation space is hardly expanded with insufficient semantic enrichment.Instead, this paper turns to enhancing visual presentation, bridging video expression closer to textual representation via caption generation and thereby facilitating video-text matching.While multimodal large language models (mLLM) have shown a powerful capability to convert video content into text, carefully crafted prompts are essential to ensure the reasonableness and completeness of the generated captions. Therefore, this paper proposes an automatic caption enhancement method that improves expression quality and mitigates empiricism in augmented captions through self-learning.Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, further exploring the utilization potential of caption augmentation.Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/CaryXiang/ECA4VTR">https://github.com/CaryXiang/ECA4VTR</a>. </p>
<blockquote>
<p>è§†é¢‘æ–‡æœ¬æ£€ç´¢ä¸€ç›´é¢ä¸´ç€å› è§†é¢‘ä¸ªæ€§åŒ–åŠæ–‡æœ¬æè¿°ä¸è¶³è€Œå¯¼è‡´çš„ä¿¡æ¯ä¸åŒ¹é…é—®é¢˜ã€‚ä¸¤ç§æ¨¡å¼ä¹‹é—´å­˜åœ¨å·¨å¤§çš„ä¿¡æ¯é¸¿æ²Ÿï¼Œé˜»ç¢äº†æœ‰æ•ˆçš„è·¨æ¨¡æ€è¡¨ç¤ºå¯¹é½ï¼Œå¯¼è‡´æ£€ç´¢ç»“æœæ¨¡ç³Šä¸æ¸…ã€‚è™½ç„¶æå‡ºäº†æ–‡æœ¬é‡å†™æ–¹æ³•æ¥æ‰©å±•æ–‡æœ¬è¡¨è¾¾ï¼Œä½†ç”±äºè¯­ä¹‰ä¸°å¯Œä¸è¶³ï¼Œæ–‡æœ¬è¡¨ç¤ºç©ºé—´å‡ ä¹æ— æ³•æ‰©å±•ï¼Œæ¨¡æ€å·®è·ä»ç„¶å­˜åœ¨ã€‚</p>
</blockquote>
<p>ç›¸åï¼Œæœ¬æ–‡è‡´åŠ›äºæé«˜è§†è§‰è¡¨ç°ï¼Œé€šè¿‡ç”Ÿæˆå­—å¹•æ¥ç¼©å°è§†é¢‘è¡¨è¾¾ä¸æ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„è·ç¦»ï¼Œä»è€Œä¿ƒè¿›è§†é¢‘æ–‡æœ¬åŒ¹é…ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆmLLMï¼‰å·²æ˜¾ç¤ºå‡ºå°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºæ–‡æœ¬çš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†ç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯¹äºç¡®ä¿ç”Ÿæˆå­—å¹•çš„åˆç†æ€§å’Œå®Œæ•´æ€§è‡³å…³é‡è¦ã€‚</p>
<p>å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨å­—å¹•å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘å­¦ä¹ æé«˜è¡¨è¾¾è´¨é‡ï¼Œå‡è½»å¢å¼ºå­—å¹•ä¸­çš„ç»éªŒä¸»ä¹‰ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡å’Œå¼•å…¥äº†ä¸€ç§ä¸“ä¸šå­—å¹•é€‰æ‹©æœºåˆ¶ï¼Œä¸ºæ¯æ®µè§†é¢‘å®šåˆ¶å¢å¼ºå­—å¹•ï¼Œè¿›ä¸€æ­¥æ¢ç´¢å­—å¹•å¢å¼ºçš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02885v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘æ–‡æœ¬æ£€ç´¢å› è§†é¢‘æè¿°ä¿¡æ¯çš„ä¸ªæ€§åŒ–å’Œä¸è¶³è€Œå¯¼è‡´ä¿¡æ¯ä¸åŒ¹é…é—®é¢˜ã€‚æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ä¹‹é—´çš„å·¨å¤§é¸¿æ²Ÿé˜»ç¢äº†è·¨æ¨¡æ€è¡¨ç¤ºçš„æœ‰æ•ˆå¯¹é½ï¼Œå¯¼è‡´æ£€ç´¢ç»“æœæ¨¡ç³Šã€‚å°½ç®¡æå‡ºäº†æ–‡æœ¬é‡å†™æ–¹æ³•æ¥æ‰©å¤§æ–‡æœ¬è¡¨è¾¾ï¼Œä½†ç”±äºæ–‡æœ¬è¡¨ç¤ºç©ºé—´çš„ä¸è¶³å’Œè¯­ä¹‰è´«ä¹ï¼Œæ¨¡æ€é—´çš„å·®è·ä»ç„¶æ˜¾è‘—ã€‚æœ¬æ–‡è½¬å‘æé«˜è§†è§‰è¡¨è¾¾ï¼Œé€šè¿‡ç”Ÿæˆå­—å¹•å°†è§†é¢‘è¡¨è¾¾æ‹‰è¿‘æ–‡æœ¬è¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›è§†é¢‘æ–‡æœ¬åŒ¹é…ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆmLLMï¼‰å·²æ˜¾ç¤ºå‡ºå°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºæ–‡æœ¬çš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†ç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯¹äºç¡®ä¿ç”Ÿæˆå­—å¹•çš„åˆç†æ€§å’Œå®Œæ•´æ€§è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨å­—å¹•å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘å­¦ä¹ æé«˜è¡¨è¾¾è´¨é‡ï¼Œå‡å°‘å¢å¼ºå­—å¹•ä¸­çš„ç»éªŒä¸»ä¹‰ã€‚æ­¤å¤–ï¼Œè®¾è®¡å¹¶å¼•å…¥äº†ä¸€ç§ä¸“ä¸šå­—å¹•é€‰æ‹©æœºåˆ¶ï¼Œä¸ºæ¯æ®µè§†é¢‘å®šåˆ¶å¢å¼ºå­—å¹•ï¼Œè¿›ä¸€æ­¥æ¢ç´¢å­—å¹•å¢å¼ºçš„åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®Œå…¨åŸºäºæ•°æ®é©±åŠ¨ï¼Œä¸ä»…å‡è½»äº†æ•°æ®æ”¶é›†å’Œè®¡ç®—çš„å·¥ä½œé‡ï¼Œè€Œä¸”é€šè¿‡é¿å…å¯¹è¯å…¸çš„ä¾èµ–å¹¶å¼•å…¥ä¸ªæ€§åŒ–åŒ¹é…æ¥æé«˜è‡ªé€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨MSR-VTTã€MSVDå’ŒDiDeMoä¸Šåˆ†åˆ«å®ç°äº†Top-1å¬å›ç‡68.5%ã€68.1%å’Œ62.0%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨å…¬å¼€ç½‘ç«™ä¸Šè·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/CaryXiang/ECA4VTR">https://github.com/CaryXiang/ECA4VTR</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘æ–‡æœ¬æ£€ç´¢é¢ä¸´ä¸ªæ€§åŒ–æè¿°ä¸è¶³å¯¼è‡´çš„ä¿¡æ¯ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>æ–‡æœ¬ä¸è§†è§‰ä¿¡æ¯ä¹‹é—´çš„å·¨å¤§é¸¿æ²Ÿå¯¼è‡´è·¨æ¨¡æ€è¡¨ç¤ºå¯¹é½å›°éš¾ï¼Œæ£€ç´¢ç»“æœæ¨¡ç³Šã€‚</li>
<li>æå‡ºé€šè¿‡å¢å¼ºè§†è§‰è¡¨è¾¾ï¼ˆç”Ÿæˆå­—å¹•ï¼‰æ¥ä¿ƒè¿›è§†é¢‘æ–‡æœ¬åŒ¹é…çš„æ–¹æ³•ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆmLLMï¼‰åœ¨è§†é¢‘å†…å®¹è½¬æ¢ä¸ºæ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>ç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯¹ç¡®ä¿ç”Ÿæˆå­—å¹•çš„è´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºè‡ªåŠ¨å­—å¹•å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘å­¦ä¹ æé«˜å­—å¹•è´¨é‡å¹¶å‡å°‘ç»éªŒä¸»ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9bf67cae6273ecab80d63095af99ab5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319b8bc77f98faa837a2b6533937b1b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2448aa2b382a430c9754e6e21f533db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db03f7b1267ad1d7f80d684b9a7db639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-587881193f2147930991b4f802c9b4a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be5c426d5d845d15e27c78e95185bea4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness"><a href="#Double-Visual-Defense-Adversarial-Pre-training-and-Instruction-Tuning-for-Improving-Vision-Language-Model-Robustness" class="headerlink" title="Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness"></a>Double Visual Defense: Adversarial Pre-training and Instruction Tuning   for Improving Vision-Language Model Robustness</h2><p><strong>Authors:Zeyu Wang, Cihang Xie, Brian Bartoldson, Bhavya Kailkhura</strong></p>
<p>This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel &#96;&#96;double visual defenseâ€ to enhance this robustness. Unlike previous approaches that resort to lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform large-scale adversarial vision-language pre-training from scratch using web-scale data. We then strengthen the defense by incorporating adversarial visual instruction tuning. The resulting models from each stage, $\Delta$CLIP and $\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a new state-of-the-art in adversarial defense for vision-language models. For example, the adversarial robustness of $\Delta$CLIP surpasses that of the previous best models on ImageNet-1k by ~20%. %For example, $\Delta$CLIP surpasses the previous best models on ImageNet-1k by ~20% in terms of adversarial robustness. Similarly, compared to prior art, $\Delta^2$LLaVA brings a ~30% robustness improvement to image captioning task and a ~20% robustness improvement to visual question answering task. Furthermore, our models exhibit stronger zero-shot recognition capability, fewer hallucinations, and superior reasoning performance compared to baselines. Our project page is <a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/">https://doublevisualdefense.github.io/</a>. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—å¯¹æŠ—æ€§è§†è§‰æ‰°åŠ¨çš„é²æ£’æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€æœºåˆ¶æ¥æé«˜è¿™ç§é²æ£’æ€§ã€‚ä¸åŒäºä¹‹å‰çš„æ–¹æ³•ï¼Œä¾èµ–äºå¯¹é¢„è®­ç»ƒçš„CLIPæ¨¡å‹çš„è½»é‡çº§å¯¹æŠ—å¾®è°ƒï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹ä½¿ç”¨å¤§è§„æ¨¡ç½‘ç»œæ•°æ®è¿›è¡Œäº†å¤§è§„æ¨¡çš„å¯¹æŠ—è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚ç„¶åï¼Œé€šè¿‡ç»“åˆå¯¹æŠ—è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥åŠ å¼ºé˜²å¾¡ã€‚ä»æ¯ä¸ªé˜¶æ®µå¾—å‡ºçš„æ¨¡å‹ï¼Œå³$\Delta$CLIPå’Œ$\Delta^2$LLaVAï¼Œæ˜¾ç¤ºå‡ºå¤§å¹…å¢å¼ºçš„é›¶æ ·æœ¬é²æ£’æ€§ï¼Œå¹¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é˜²å¾¡æ–¹é¢åˆ›é€ äº†æ–°çš„æŠ€æœ¯çºªå½•ã€‚ä¾‹å¦‚ï¼Œ$\Delta$CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—é²æ£’æ€§è¶…è¿‡äº†ä»¥å‰æœ€ä½³æ¨¡å‹çš„æ€§èƒ½çº¦20%ã€‚åŒæ ·åœ°ï¼Œä¸å…ˆå‰æŠ€æœ¯ç›¸æ¯”ï¼Œ$\Delta^2$LLaVAåœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸Šæé«˜äº†çº¦30%çš„é²æ£’æ€§ï¼Œåœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæé«˜äº†çº¦20%çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜è¡¨ç°å‡ºæ›´å¼ºçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ã€æ›´å°‘çš„å¹»è§‰å’Œä¼˜äºåŸºå‡†çº¿çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š[<a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/]">https://doublevisualdefense.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09446v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—è§†è§‰æ‰°åŠ¨æ”»å‡»çš„é²æ£’æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€ç­–ç•¥æ¥æå‡å…¶é²æ£’æ€§ã€‚ä¸åŒäºå…ˆå‰åªå¯¹é¢„è®­ç»ƒCLIPæ¨¡å‹è¿›è¡Œè½»é‡çº§çš„å¯¹æŠ—å¾®è°ƒçš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹è¿›è¡Œå¤§è§„æ¨¡çš„å¯¹æŠ—è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œå¹¶åˆ©ç”¨ç½‘ç»œè§„æ¨¡æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ç»“åˆå¯¹æŠ—è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥åŠ å¼ºé˜²å¾¡ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹ï¼Œå¦‚Î”CLIPå’ŒÎ”Â²LLaVAï¼Œå¤§å¹…å¢å¼ºäº†é›¶æ ·æœ¬é²æ£’æ€§ï¼Œå¹¶åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é˜²å¾¡æ–¹é¢åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒÎ”CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—é²æ£’æ€§è¶…è¿‡äº†ä»¥å‰çš„æœ€ä½³æ¨¡å‹çº¦20%ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io./">https://doublevisualdefense.github.io/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ç ”ç©¶è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—è§†è§‰æ‰°åŠ¨æ”»å‡»çš„é²æ£’æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„â€œåŒé‡è§†è§‰é˜²å¾¡â€ç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡å¯¹æŠ—è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œåˆ©ç”¨ç½‘ç»œè§„æ¨¡æ•°æ®ã€‚</li>
<li>ç»“åˆå¯¹æŠ—è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥è¿›ä¸€æ­¥å¼ºåŒ–æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ã€‚</li>
<li>Î”CLIPå’ŒÎ”Â²LLaVAæ¨¡å‹å¤§å¹…å¢å¼ºäº†é›¶æ ·æœ¬é²æ£’æ€§ï¼Œå¹¶åœ¨å¯¹æŠ—é˜²å¾¡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>Î”CLIPåœ¨ImageNet-1kä¸Šçš„å¯¹æŠ—é²æ£’æ€§è¾ƒä¹‹å‰æœ€ä½³æ¨¡å‹æé«˜äº†çº¦20%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06e359ce86298d9604dc1e221b6cf5cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-767b185b59c3c4eed70baa9c5a9e8322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c166aae33ad0dab36200cbe9f4001ca.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="KnowCoder-X-Boosting-Multilingual-Information-Extraction-via-Code"><a href="#KnowCoder-X-Boosting-Multilingual-Information-Extraction-via-Code" class="headerlink" title="KnowCoder-X: Boosting Multilingual Information Extraction via Code"></a>KnowCoder-X: Boosting Multilingual Information Extraction via Code</h2><p><strong>Authors:Yuxin Zuo, Wenxuan Jiang, Wenxuan Liu, Zixuan Li, Long Bai, Hanbin Wang, Yutao Zeng, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng</strong></p>
<p>Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in IE, a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal information extraction. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we enhance the modelâ€™s cross-lingual transferability through IE cross-lingual alignment instruction tuning on a translated instance prediction task we proposed. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by $30.17%$ and SoTA by $20.03%$, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: <a target="_blank" rel="noopener" href="https://github.com/ICT-GoKnow/KnowCoder">https://github.com/ICT-GoKnow/KnowCoder</a> </p>
<blockquote>
<p>å®è¯è¯æ®è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºè‡ªå‘çš„è·¨è¯­è¨€å¯¹é½ã€‚ç„¶è€Œï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯æå–æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„è·¨è¯­è¨€å¯¹é½ï¼Œä½†ä¸åŒè¯­è¨€ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—çš„ä¸å¹³è¡¡ï¼Œè¿™å‡¸æ˜¾äº†æ½œåœ¨çš„ç¼ºé™·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†KnowCoder-Xï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å…ˆè¿›çš„è·¨è¯­è¨€å’Œå¤šå…ƒæ–‡åŒ–èƒ½åŠ›ï¼Œç”¨äºé€šç”¨ä¿¡æ¯æå–ã€‚é¦–å…ˆï¼Œå®ƒä½¿ç”¨Pythonç±»æ ‡å‡†åŒ–äº†å¤šè¯­è¨€æ¨¡å¼çš„è¡¨ç¤ºï¼Œç¡®ä¿äº†ä¸åŒè¯­è¨€ä¹‹é—´çš„ä¸€è‡´æ€§æœ¬ä½“ã€‚ç„¶åï¼Œå°†ä¸åŒè¯­è¨€çš„ä¿¡æ¯æå–å…¬å¼åŒ–ä¸ºç»Ÿä¸€çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºçš„ä¿¡æ¯æå–è·¨è¯­è¨€å¯¹é½æŒ‡ä»¤è°ƒæ•´ç¿»è¯‘å®ä¾‹é¢„æµ‹ä»»åŠ¡ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æˆ‘ä»¬æå‡ºçš„ç¨³å¥ä¸‰é˜¶æ®µç®¡é“åˆæˆäº†ä¸€ä¸ªé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åŒè¯­ä¿¡æ¯æå–å¹³è¡Œæ•°æ®é›†ParallelNERï¼ŒåŒ…å«25.7ä¸‡ä¸ªæ ·æœ¬ï¼Œå¹¶è¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šä»¥ç¡®ä¿è´¨é‡ã€‚æ— éœ€åœ¨29ç§æœªè§è¿‡çš„è¯­è¨€ä¸­è¿›è¡Œè®­ç»ƒï¼ŒKnowCoder-Xè¶…è¶Šäº†ChatGPTçš„30.17%ï¼Œå¹¶è¶…è¶Šäº†å½“å‰æœ€ä½³æ°´å¹³çš„20.03%ï¼Œä»è€Œè¯æ˜äº†å…¶å“è¶Šçš„è·¨è¯­è¨€ä¿¡æ¯æå–èƒ½åŠ›ã€‚åœ¨ä¸­æ–‡å’Œè‹±æ–‡çš„64ä¸ªä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸‹çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒKnowCoder-Xé€šè¿‡æé«˜ä¿¡æ¯æå–çš„å¯¹é½æ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†è·¨è¯­è¨€ä¿¡æ¯æå–çš„è¿ç§»èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ICT-GoKnow/KnowCoder%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ICT-GoKnow/KnowCoderä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04794v2">PDF</a> 26 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>LLMså±•ç°å‡ºè·¨è¯­è¨€è‡ªå‘çš„å¯¹é½èƒ½åŠ›ï¼Œä½†åœ¨ä¿¡æ¯æŠ½å–ï¼ˆIEï¼‰é¢†åŸŸä»å­˜åœ¨è¯­è¨€é—´çš„ä¸å¹³è¡¡ã€‚ä¸ºæ­¤ï¼Œæå‡ºKnowCoder-Xæ¨¡å‹ï¼Œå…·å¤‡å…ˆè¿›çš„è·¨è¯­è¨€å’Œå¤šè¯­è¨€èƒ½åŠ›ï¼Œç”¨äºé€šç”¨ä¿¡æ¯æŠ½å–ã€‚å®ƒé€šè¿‡æ ‡å‡†åŒ–å¤šè¯­è¨€æ¨¡å¼å’Œä½¿ç”¨Pythonç±»ç¡®ä¿è·¨è¯­è¨€çš„ä¸€è‡´æ€§æœ¬ä½“ï¼Œå°†è·¨è¯­è¨€çš„ä¿¡æ¯æŠ½å–åˆ¶å®šä¸ºç»Ÿä¸€çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡IEè·¨è¯­è¨€å¯¹é½æŒ‡ä»¤è°ƒæ•´å’Œç¿»è¯‘å®ä¾‹é¢„æµ‹ä»»åŠ¡ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚ä½¿ç”¨é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åŒè¯­IEå¹³è¡Œæ•°æ®é›†ParallelNERè¿›è¡Œè®­ç»ƒï¼Œæœªç»29ç§æœªè§è¿‡çš„è¯­è¨€è®­ç»ƒï¼ŒKnowCoder-Xåœ¨ChatGPTä¸Šæé«˜äº†30.17%ï¼Œå¹¶åœ¨å…¶ä»–æŠ€æœ¯å¤„äºé¢†å…ˆåœ°ä½çš„æ¨¡å‹ä¸Šæé«˜äº†20.03%ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„è·¨è¯­è¨€IEèƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨ä¸åŒè®¾ç½®çš„ä¸­æ–‡å’Œè‹±æ–‡çš„64ä¸ªIEåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†è·¨è¯­è¨€IEè¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºè·¨è¯­è¨€è‡ªå‘çš„å¯¹é½èƒ½åŠ›ï¼Œä½†åœ¨ä¿¡æ¯æŠ½å–ï¼ˆIEï¼‰ä¸­å­˜åœ¨è¯­è¨€é—´çš„ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>KnowCoder-Xæ˜¯ä¸€ä¸ªå…·å¤‡å…ˆè¿›è·¨è¯­è¨€å’Œå¤šè¯­è¨€èƒ½åŠ›é’ˆå¯¹ä¿¡æ¯æŠ½å–è®¾è®¡çš„LLMã€‚</li>
<li>KnowCoder-Xé€šè¿‡æ ‡å‡†åŒ–å¤šè¯­è¨€æ¨¡å¼å¹¶ä½¿ç”¨Pythonç±»ç¡®ä¿è·¨è¯­è¨€çš„ä¸€è‡´æ€§æœ¬ä½“ã€‚</li>
<li>KnowCoder-Xå°†è·¨è¯­è¨€çš„ä¿¡æ¯æŠ½å–åˆ¶å®šä¸ºç»Ÿä¸€çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>KnowCoder-Xé€šè¿‡IEè·¨è¯­è¨€å¯¹é½æŒ‡ä»¤è°ƒæ•´å’Œç¿»è¯‘å®ä¾‹é¢„æµ‹ä»»åŠ¡å¢å¼ºæ¨¡å‹çš„è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚</li>
<li>KnowCoder-Xä½¿ç”¨é«˜è´¨é‡çš„åŒè¯­IEå¹³è¡Œæ•°æ®é›†ParallelNERè¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc18c146ff467ee994e7a5a2ff83f48d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91ff048186011bf8fab4f366bab0d557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbc53a6306ec373b8b25105bf755b656.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-527451acd9900a488a751ecb13eb5787.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  TxGemma Efficient and Agentic LLMs for Therapeutics
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-06c787a864f6e3acdfc3f2adff5bc6fc.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  FEABench Evaluating Language Models on Multiphysics Reasoning Ability
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15821.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
