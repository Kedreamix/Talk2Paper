<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Memory-Modular Classification Learning to Generalize with Memory   Replacement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-54d86a5a3dee98004f4c15ff24c6626e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    31 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-10-æ›´æ–°"><a href="#2025-04-10-æ›´æ–°" class="headerlink" title="2025-04-10 æ›´æ–°"></a>2025-04-10 æ›´æ–°</h1><h2 id="Memory-Modular-Classification-Learning-to-Generalize-with-Memory-Replacement"><a href="#Memory-Modular-Classification-Learning-to-Generalize-with-Memory-Replacement" class="headerlink" title="Memory-Modular Classification: Learning to Generalize with Memory   Replacement"></a>Memory-Modular Classification: Learning to Generalize with Memory   Replacement</h2><p><strong>Authors:Dahyun Kang, Ahmet Iscen, Eunchan Jo, Sua Choi, Minsu Cho, Cordelia Schmid</strong></p>
<p>We propose a novel memory-modular learner for image classification that separates knowledge memorization from reasoning. Our model enables effective generalization to new classes by simply replacing the memory contents, without the need for model retraining. Unlike traditional models that encode both world knowledge and task-specific skills into their weights during training, our model stores knowledge in the external memory of web-crawled image and text data. At inference time, the model dynamically selects relevant content from the memory based on the input image, allowing it to adapt to arbitrary classes by simply replacing the memory contents. The key differentiator that our learner meta-learns to perform classification tasks with noisy web data from unseen classes, resulting in robust performance across various classification scenarios. Experimental results demonstrate the promising performance and versatility of our approach in handling diverse classification tasks, including zero-shot&#x2F;few-shot classification of unseen classes, fine-grained classification, and class-incremental classification. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒåˆ†ç±»çš„æ–°å‹è®°å¿†æ¨¡å—åŒ–å­¦ä¹ è€…ï¼Œå®ƒå°†çŸ¥è¯†è®°å¿†ä¸æ¨ç†ç›¸åˆ†ç¦»ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ç®€å•åœ°æ›¿æ¢è®°å¿†å†…å®¹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå°±èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ–°çš„ç±»åˆ«ã€‚ä¸åŒäºä¼ ç»Ÿæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†ä¸–ç•ŒçŸ¥è¯†å’Œç‰¹å®šä»»åŠ¡æŠ€èƒ½ç¼–ç åˆ°æƒé‡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†çŸ¥è¯†å­˜å‚¨åœ¨ä»ç½‘ç»œçˆ¬å–çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„å¤–éƒ¨è®°å¿†ä¸­ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥å›¾åƒåŠ¨æ€é€‰æ‹©è®°å¿†ä¸­çš„ç›¸å…³å†…å®¹ï¼Œé€šè¿‡ç®€å•åœ°æ›¿æ¢è®°å¿†å†…å®¹ï¼Œå°±èƒ½å¤Ÿé€‚åº”ä»»æ„ç±»åˆ«ã€‚æˆ‘ä»¬çš„å­¦ä¹ è€…çš„å…³é”®åŒºåˆ«åœ¨äºï¼Œå®ƒä¼šè¿›è¡Œå…ƒå­¦ä¹ ï¼Œä½¿ç”¨æ¥è‡ªæœªè§ç±»åˆ«çš„å¸¦æœ‰å™ªå£°çš„ç½‘é¡µæ•°æ®è¿›è¡Œåˆ†ç±»ä»»åŠ¡ï¼Œä»è€Œåœ¨å„ç§åˆ†ç±»åœºæ™¯ä¸­å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤„ç†å„ç§åˆ†ç±»ä»»åŠ¡æ—¶å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬æœªè§ç±»åˆ«åˆ†ç±»ã€ç»†ç²’åº¦åˆ†ç±»å’Œç±»å¢é‡åˆ†ç±»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06021v1">PDF</a> Accepted to TMLR. Code available: <a target="_blank" rel="noopener" href="https://github.com/dahyun-kang/mml">https://github.com/dahyun-kang/mml</a></p>
<p><strong>Summary</strong><br>æœ¬æ¨¡å‹æå‡ºäº†ä¸€ç§æ–°çš„è®°å¿†æ¨¡å—åŒ–å­¦ä¹ è€…ç”¨äºå›¾åƒåˆ†ç±»ï¼Œå®ƒå°†çŸ¥è¯†è®°å¿†ä¸æ¨ç†åˆ†ç¦»ã€‚é€šè¿‡ä»…æ›¿æ¢è®°å¿†å†…å®¹ï¼Œå³å¯å®ç°å¯¹æ–°ç±»çš„æœ‰æ•ˆæ³›åŒ–ï¼Œæ— éœ€è¿›è¡Œæ¨¡å‹å†è®­ç»ƒã€‚è¯¥æ¨¡å‹å°†çŸ¥è¯†å­˜å‚¨åœ¨ä»ç½‘ç»œçˆ¬å–çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„å¤–éƒ¨è®°å¿†ä¸­ã€‚åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥å›¾åƒåŠ¨æ€é€‰æ‹©ç›¸å…³å†…å­˜å†…å®¹ï¼Œé€šè¿‡ç®€å•æ›¿æ¢å†…å­˜å†…å®¹å³å¯é€‚åº”ä»»æ„ç±»åˆ«ã€‚è¯¥æ¨¡å‹çš„å…³é”®åŒºåˆ«åœ¨äºï¼Œå®ƒèƒ½å¤Ÿä½¿ç”¨æ¥è‡ªæœªè§ç±»åˆ«çš„ç½‘ç»œæ•°æ®çš„å™ªå£°è¿›è¡Œå…ƒå­¦ä¹ åˆ†ç±»ä»»åŠ¡ï¼Œä»è€Œåœ¨å„ç§åˆ†ç±»åœºæ™¯ä¸­å®ç°ç¨³å¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹å®ç°äº†è®°å¿†å’Œæ¨ç†çš„åˆ†ç¦»ï¼Œæœ‰åŠ©äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ›¿æ¢è®°å¿†å†…å®¹ï¼Œæ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°çš„ç±»åˆ«ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>æ¨¡å‹å°†çŸ¥è¯†å­˜å‚¨åœ¨å¤–éƒ¨è®°å¿†ä¸­ï¼ŒåŒ…æ‹¬ä»ç½‘ç»œçˆ¬å–çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥å›¾åƒåŠ¨æ€é€‰æ‹©ç›¸å…³å†…å­˜å†…å®¹ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨æœªè§ç±»åˆ«çš„æƒ…å†µä¸‹è¿›è¡Œå…ƒå­¦ä¹ åˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨å¤„ç†å¤šç§åˆ†ç±»ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚æ€§èƒ½å’Œé€šç”¨æ€§ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬åˆ†ç±»ã€ç»†ç²’åº¦åˆ†ç±»å’Œç±»åˆ«å¢é‡åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1ff45c0893cfff1ed5b4a0060a9ff71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d00c86f0198b7698ac99a514cdf36ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da0cb1a9c4eda7342b72be44911d7a37.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Alignment-Free-Video-Matching-for-Few-shot-Action-Recognition"><a href="#Temporal-Alignment-Free-Video-Matching-for-Few-shot-Action-Recognition" class="headerlink" title="Temporal Alignment-Free Video Matching for Few-shot Action Recognition"></a>Temporal Alignment-Free Video Matching for Few-shot Action Recognition</h2><p><strong>Authors:SuBeen Lee, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo</strong></p>
<p>Few-Shot Action Recognition (FSAR) aims to train a model with only a few labeled video instances. A key challenge in FSAR is handling divergent narrative trajectories for precise video matching. While the frame- and tuple-level alignment approaches have been promising, their methods heavily rely on pre-defined and length-dependent alignment units (e.g., frames or tuples), which limits flexibility for actions of varying lengths and speeds. In this work, we introduce a novel TEmporal Alignment-free Matching (TEAM) approach, which eliminates the need for temporal units in action representation and brute-force alignment during matching. Specifically, TEAM represents each video with a fixed set of pattern tokens that capture globally discriminative clues within the video instance regardless of action length or speed, ensuring its flexibility. Furthermore, TEAM is inherently efficient, using token-wise comparisons to measure similarity between videos, unlike existing methods that rely on pairwise comparisons for temporal alignment. Additionally, we propose an adaptation process that identifies and removes common information across classes, establishing clear boundaries even between novel categories. Extensive experiments demonstrate the effectiveness of TEAM. Codes are available at github.com&#x2F;leesb7426&#x2F;TEAM. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰æ—¨åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨è§†é¢‘å®ä¾‹æ¥è®­ç»ƒæ¨¡å‹ã€‚FSARçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å¤„ç†ä¸åŒçš„å™äº‹è½¨è¿¹ä»¥å®ç°ç²¾ç¡®çš„è§†é¢‘åŒ¹é…ã€‚å°½ç®¡å¸§çº§å’Œå…ƒç»„çº§å¯¹é½æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºé¢„å®šä¹‰å’Œé•¿åº¦ä¾èµ–çš„å¯¹é½å•ä½ï¼ˆä¾‹å¦‚å¸§æˆ–å…ƒç»„ï¼‰ï¼Œè¿™é™åˆ¶äº†å¯¹äºä¸åŒé•¿åº¦å’Œé€Ÿåº¦åŠ¨ä½œçš„çµæ´»æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— éœ€æ—¶é—´å¯¹é½åŒ¹é…ï¼ˆTEAMï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¶ˆé™¤äº†åŠ¨ä½œè¡¨ç¤ºä¸­æ—¶é—´å•ä½çš„éœ€æ±‚ä»¥åŠåœ¨åŒ¹é…è¿‡ç¨‹ä¸­çš„å¼ºåˆ¶å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼ŒTEAMä½¿ç”¨ä¸€ç»„å›ºå®šçš„æ¨¡å¼ä»¤ç‰Œæ¥è¡¨ç¤ºæ¯ä¸ªè§†é¢‘ï¼Œè¿™äº›ä»¤ç‰Œå¯ä»¥æ•è·è§†é¢‘å®ä¾‹ä¸­çš„å…¨å±€é‰´åˆ«çº¿ç´¢ï¼Œè€Œä¸è€ƒè™‘åŠ¨ä½œçš„é•¿åº¦æˆ–é€Ÿåº¦ï¼Œä»è€Œç¡®ä¿å…¶çµæ´»æ€§ã€‚æ­¤å¤–ï¼ŒTEAMæœ¬è´¨ä¸Šæ˜¯é«˜æ•ˆçš„ï¼Œä½¿ç”¨ä»¤ç‰Œçº§çš„æ¯”è¾ƒæ¥æµ‹é‡è§†é¢‘ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºæˆå¯¹æ¯”è¾ƒè¿›è¡Œæ—¶é—´å¯¹é½ã€‚å¦å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€‚åº”è¿‡ç¨‹ï¼Œå¯ä»¥è¯†åˆ«å’Œåˆ é™¤è·¨ç±»åˆ«çš„é€šç”¨ä¿¡æ¯ï¼Œå³ä½¿åœ¨æ–°å‹ç±»åˆ«ä¹‹é—´ä¹Ÿå»ºç«‹æ¸…æ™°çš„è¾¹ç•Œã€‚å¤§é‡å®éªŒè¯æ˜äº†TEAMçš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨github.com&#x2F;leesb7426&#x2F;TEAMæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05956v1">PDF</a> 10 pages, 7 figures, 6 tables, Accepted to CVPR 2025 as Oral   Presentation</p>
<p><strong>Summary</strong><br>å°‘é‡æ ‡æ³¨è§†é¢‘æ ·æœ¬çš„åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰é¢ä¸´å¤„ç†ä¸åŒå™äº‹è½¨è¿¹çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰å’Œé•¿åº¦ä¾èµ–çš„å¯¹é½å•å…ƒï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒé•¿åº¦å’Œé€Ÿåº¦åŠ¨ä½œä¸Šçš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— éœ€æ—¶é—´å¯¹é½åŒ¹é…ï¼ˆTEAMï¼‰æ–¹æ³•ï¼Œé€šè¿‡å›ºå®šæ¨¡å¼çš„ä»¤ç‰Œè¡¨ç¤ºè§†é¢‘ï¼Œæ•æ‰å…¨å±€é‰´åˆ«çº¿ç´¢ï¼Œç¡®ä¿çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨ä»¤ç‰Œæ¯”è¾ƒæµ‹é‡è§†é¢‘ç›¸ä¼¼æ€§ï¼Œå…·æœ‰é«˜æ•ˆæ€§ã€‚åŒæ—¶ï¼Œæå‡ºé€‚åº”è¿‡ç¨‹ï¼Œæ˜ç¡®ç±»åˆ«é—´çš„ç•Œé™ã€‚å®éªŒè¯æ˜TEAMçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Action Recognition (FSAR) æ—¨åœ¨ç”¨å°‘é‡æ ‡æ³¨è§†é¢‘å®ä¾‹è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>å¤„ç†ä¸åŒå™äº‹è½¨è¿¹æ˜¯FSARçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰å’Œé•¿åº¦ä¾èµ–çš„å¯¹é½å•å…ƒï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒé•¿åº¦å’Œé€Ÿåº¦åŠ¨ä½œçš„çµæ´»æ€§ã€‚</li>
<li>TEAMæ–¹æ³•é€šè¿‡å›ºå®šæ¨¡å¼çš„ä»¤ç‰Œè¡¨ç¤ºè§†é¢‘ï¼Œæ— éœ€æ—¶é—´å¯¹é½ã€‚</li>
<li>TEAMæ–¹æ³•æ•æ‰å…¨å±€é‰´åˆ«çº¿ç´¢ï¼Œç¡®ä¿çµæ´»æ€§å¹¶æµ‹é‡è§†é¢‘ç›¸ä¼¼æ€§ã€‚</li>
<li>TEAMé‡‡ç”¨ä»¤ç‰Œæ¯”è¾ƒï¼Œå…·æœ‰é«˜æ•ˆæ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60aefaf404d75d7e9374151f31553bf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-025dab5eeacc9b929642efa02cf0086c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53f0caad65b46c482c921631eadb9dde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd43c84c0be3692d26a9dd333080fb27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03fcc6107c16411e4341187f64467ba0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Å vÃ¡benskÃ½, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educatorsâ€™ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿå¼€æ”¾æ€§åæ€å’Œé¢„æµ‹å­¦ä¸šè¡¨ç°æ–¹é¢çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„åæ€è¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”åœ¨æ•™è‚²ç¯å¢ƒä¸­å¯èƒ½æ— æ³•æœ‰æ•ˆæ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨LLMï¼Œä½¿ç”¨ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ï¼‰ï¼Œå°†å­¦ç”Ÿåæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨åŒ…å«æ¥è‡ª377åå­¦ç”Ÿåœ¨ä¸‰ä¸ªå­¦æœ¯å­¦æœŸå†…çš„5,278ç¯‡åæ€çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨å°‘æ ·æœ¬ç­–ç•¥çš„å•æ™ºèƒ½ä½“å–å¾—äº†æœ€é«˜çš„ä¸äººç±»è¯„ä»·çš„åŒ¹é…ç‡ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨é£é™©å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°è‡ªåŠ¨è¿›è¡Œåæ€è¯„ä¼°ï¼Œå‡å°‘æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œå¹¶ä¸ºå¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶çš„æ”¯æŒã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†å°†å…ˆè¿›çš„ç”Ÿæˆæ€§AIæŠ€æœ¯èå…¥æ•™è‚²å®è·µä¸­çš„æ½œåŠ›ï¼Œä»¥æé«˜å­¦ç”Ÿçš„å‚ä¸åº¦å’Œå­¦ä¸šæˆåŠŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v1">PDF</a> To be published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿå¼€æ”¾æ€§åæ€å’Œé¢„æµ‹å­¦ä¸šè¡¨ç°æ–¹é¢çš„åº”ç”¨æ¢ç©¶ã€‚ä¼ ç»Ÿè¯„ä¼°æ–¹å¼è€—æ—¶ä¸”éš¾ä»¥åœ¨æ•™è‚²ç¯å¢ƒä¸­æœ‰æ•ˆæ‰©å±•ã€‚æœ¬ç ”ç©¶é‡‡ç”¨LLMsï¼Œé€šè¿‡ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•ä¸»ä½“å’Œå¤šä¸»ä½“ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ï¼‰ï¼Œå°†å­¦ç”Ÿåæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œé‡‡ç”¨å°‘æ ·æœ¬ç­–ç•¥çš„å•ä¸»ä½“æ¨¡å‹ä¸äººç±»è¯„ä¼°åŒ¹é…åº¦æœ€é«˜ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨è¯†åˆ«å­¦ä¸šé£é™©å­¦ç”Ÿå’Œé¢„æµ‹æˆç»©ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿ã€‚è¿™è¡¨æ˜LLMsèƒ½æœ‰æ•ˆè‡ªåŠ¨åŒ–åæ€è¯„ä¼°ï¼Œå‡è½»æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œå¹¶ä¸ºå¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶æ”¯æŒã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å°†å…ˆè¿›çš„ç”Ÿæˆå¼AIæŠ€æœ¯èå…¥æ•™è‚²å®è·µï¼Œä»¥æé«˜å­¦ç”Ÿå­¦ä¹ å‚ä¸åº¦å’Œå­¦ä¸šæˆåŠŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿçš„å¼€æ”¾æ€§åæ€ï¼Œé¢„æµ‹å­¦ä¸šè¡¨ç°ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æ–¹å¼å­˜åœ¨è€—æ—¶ä¸”éš¾ä»¥åœ¨æ•™è‚²ç¯å¢ƒä¸­æœ‰æ•ˆæ‰©å±•çš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨LLMsçš„è¯„ä¼°æ–¹å¼åŒ…æ‹¬ä¸¤ç§ç­–ç•¥ï¼šå•ä¸»ä½“è¯„ä¼°å’Œå¤šä¸»ä½“è¯„ä¼°ï¼Œä»¥åŠä¸¤ç§æç¤ºæŠ€æœ¯ï¼šé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨å°‘æ ·æœ¬ç­–ç•¥çš„å•ä¸»ä½“æ¨¡å‹ä¸äººç±»è¯„ä¼°åŒ¹é…åº¦æœ€é«˜ã€‚</li>
<li>LLMsåœ¨è¯†åˆ«å­¦ä¸šé£é™©å­¦ç”Ÿå’Œé¢„æµ‹æˆç»©ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>LLMsçš„åº”ç”¨èƒ½æœ‰æ•ˆè‡ªåŠ¨åŒ–åæ€è¯„ä¼°ï¼Œå‡è½»æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5366bb7082c7ca612d81c6680389fc41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Smarter-Hiring-Are-Zero-Shot-and-Few-Shot-Pre-trained-LLMs-Ready-for-HR-Spoken-Interview-Transcript-Analysis"><a href="#Towards-Smarter-Hiring-Are-Zero-Shot-and-Few-Shot-Pre-trained-LLMs-Ready-for-HR-Spoken-Interview-Transcript-Analysis" class="headerlink" title="Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs   Ready for HR Spoken Interview Transcript Analysis?"></a>Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs   Ready for HR Spoken Interview Transcript Analysis?</h2><p><strong>Authors:Subhankar Maity, Aniket Deroy, Sudeshna Sarkar</strong></p>
<p>This research paper presents a comprehensive analysis of the performance of prominent pre-trained large language models (LLMs), including GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001, text-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in comparison to expert human evaluators in providing scores, identifying errors, and offering feedback and improvement suggestions to candidates during mock HR (Human Resources) interviews. We introduce a dataset called HURIT (Human Resource Interview Transcripts), which comprises 3,890 HR interview transcripts sourced from real-world HR interview scenarios. Our findings reveal that pre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit commendable performance and are capable of producing evaluations comparable to those of expert human evaluators. Although these LLMs demonstrate proficiency in providing scores comparable to human experts in terms of human evaluation metrics, they frequently fail to identify errors and offer specific actionable advice for candidate performance improvement in HR interviews. Our research suggests that the current state-of-the-art pre-trained LLMs are not fully conducive for automatic deployment in an HR interview assessment. Instead, our findings advocate for a human-in-the-loop approach, to incorporate manual checks for inconsistencies and provisions for improving feedback quality as a more suitable strategy. </p>
<blockquote>
<p>æœ¬æ–‡å…¨é¢åˆ†æäº†ä¸»æµé¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨¡æ‹ŸäººåŠ›èµ„æºï¼ˆHRï¼‰é¢è¯•ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬GPT-4 Turboã€GPT-3.5 Turboã€text-davinci-003ã€text-babbage-001ã€text-curie-001ã€text-ada-001ã€llama-2-7b-chatã€llama-2-13b-chatå’Œllama-2-70b-chatç­‰æ¨¡å‹ä¸ä¸“å®¶è¯„å§”ç›¸æ¯”åœ¨è¯„åˆ†ã€è¯†åˆ«é”™è¯¯å’Œå¯¹å€™é€‰äººæä¾›åé¦ˆå’Œæ”¹è¿›å»ºè®®æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºHURITï¼ˆäººåŠ›èµ„æºé¢è¯•è®°å½•ï¼‰çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ªç°å®ä¸–ç•ŒäººåŠ›èµ„æºé¢è¯•åœºæ™¯çš„3890ä»½é¢è¯•è®°å½•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œé¢„è®­ç»ƒLLMï¼Œå°¤å…¶æ˜¯GPT-4 Turboå’ŒGPT-3.5 Turboï¼Œè¡¨ç°å‡ºå€¼å¾—ç§°èµçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿäº§ç”Ÿä¸ä¸“å®¶è¯„å§”ç›¸å½“çš„è¯„ä»·ã€‚å°½ç®¡è¿™äº›LLMåœ¨æŒ‰ç…§äººç±»è¯„ä¼°æŒ‡æ ‡æä¾›å¯æ¯”åˆ†æ•°çš„è¯„åˆ†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•è¯†åˆ«é”™è¯¯å¹¶ä¸ºå€™é€‰äººåœ¨äººåŠ›èµ„æºé¢è¯•ä¸­çš„è¡¨ç°æ”¹è¿›æä¾›å…·ä½“çš„å¯æ“ä½œçš„å»ºè®®ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç›®å‰æœ€å…ˆè¿›çš„é¢„è®­ç»ƒLLMå¹¶ä¸é€‚åˆå®Œå…¨è‡ªåŠ¨éƒ¨ç½²åœ¨äººåŠ›èµ„æºé¢è¯•è¯„ä¼°ä¸­ã€‚ç›¸åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœæå€¡é‡‡ç”¨äººç±»å‚ä¸å¾ªç¯çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰‹åŠ¨æ£€æŸ¥ä¸ä¸€è‡´å¹¶æä¾›æ”¹è¿›åé¦ˆè´¨é‡çš„ç­–ç•¥ï¼Œä½œä¸ºæ›´åˆé€‚çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05683v1">PDF</a> 32 pages, 24 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šç§ä¸»æµé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹ŸäººåŠ›èµ„æºï¼ˆHRï¼‰é¢è¯•ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¼•å…¥HURITæ•°æ®é›†ï¼Œå¯¹LLMsåœ¨è¯„åˆ†ã€è¯†åˆ«é”™è¯¯ä»¥åŠæä¾›æ”¹è¿›å»ºè®®æ–¹é¢çš„èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨è¯„åˆ†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸äººç±»ä¸“å®¶ç›¸æ¯”ï¼Œå®ƒä»¬åœ¨è¯†åˆ«é”™è¯¯å’Œæä¾›å…·ä½“å¯æ“ä½œçš„æ”¹è¿›å»ºè®®æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚å› æ­¤ï¼Œç›®å‰LLMså¹¶ä¸é€‚åˆå®Œå…¨è‡ªåŠ¨éƒ¨ç½²åœ¨HRé¢è¯•è¯„ä¼°ä¸­ã€‚å»ºè®®é‡‡ç”¨äººæœºç»“åˆçš„æ–¹å¼ï¼Œä»¥æé«˜åé¦ˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹ŸäººåŠ›èµ„æºï¼ˆHRï¼‰é¢è¯•ä¸­çš„è¯„åˆ†è¡¨ç°ä¸äººç±»ä¸“å®¶ç›¸å½“ã€‚</li>
<li>GPT-4 Turboå’ŒGPT-3.5 Turboåœ¨HRé¢è¯•è¯„ä¼°ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>LLMsåœ¨è¯†åˆ«é¢è¯•ä¸­çš„é”™è¯¯å’Œæä¾›å…·ä½“å¯æ“ä½œçš„æ”¹è¿›å»ºè®®æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ç›®å‰LLMså¹¶ä¸é€‚åˆå®Œå…¨è‡ªåŠ¨éƒ¨ç½²åœ¨HRé¢è¯•è¯„ä¼°ä¸­ã€‚</li>
<li>å»ºè®®é‡‡ç”¨äººæœºç»“åˆçš„æ–¹å¼ï¼Œé€šè¿‡æ‰‹åŠ¨æ£€æŸ¥ä¸ä¸€è‡´æ€§æ¥æé«˜åé¦ˆè´¨é‡ã€‚</li>
<li>HURITæ•°æ®é›†ä¸ºè¯„ä¼°LLMsåœ¨HRé¢è¯•ä¸­çš„è¡¨ç°æä¾›äº†é‡è¦èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61dbb4d41c8a2afcf9b289065780b2ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d564a67d0331a5feba653f5c68f7b27b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-168535b36d9b4a7d8d4055e831478827.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea1de9e9f8fbc7fecff916e639fa63a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db95ae9c62a8e7ce0d8a3580038c2c82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dafffc76e2288858b53b457ea629383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931d5ea9d4e849c15ed4d38d549588db.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-shot-Personalized-Scanpath-Prediction"><a href="#Few-shot-Personalized-Scanpath-Prediction" class="headerlink" title="Few-shot Personalized Scanpath Prediction"></a>Few-shot Personalized Scanpath Prediction</h2><p><strong>Authors:Ruoyu Xue, Jingyi Xu, Sounak Mondal, Hieu Le, Gregory Zelinsky, Minh Hoai, Dimitris Samaras</strong></p>
<p>A personalized model for scanpath prediction provides insights into the visual preferences and attention patterns of individual subjects. However, existing methods for training scanpath prediction models are data-intensive and cannot be effectively personalized to new individuals with only a few available examples. In this paper, we propose few-shot personalized scanpath prediction task (FS-PSP) and a novel method to address it, which aims to predict scanpaths for an unseen subject using minimal support data of that subjectâ€™s scanpath behavior. The key to our methodâ€™s adaptability is the Subject-Embedding Network (SE-Net), specifically designed to capture unique, individualized representations for each subjectâ€™s scanpaths. SE-Net generates subject embeddings that effectively distinguish between subjects while minimizing variability among scanpaths from the same individual. The personalized scanpath prediction model is then conditioned on these subject embeddings to produce accurate, personalized results. Experiments on multiple eye-tracking datasets demonstrate that our method excels in FS-PSP settings and does not require any fine-tuning steps at test time. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/few-shot-scanpath">https://github.com/cvlab-stonybrook/few-shot-scanpath</a> </p>
<blockquote>
<p>ä¸ªæ€§åŒ–æ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹ä¸ºæˆ‘ä»¬æä¾›äº†å…³äºä¸ªä½“è§†è§‰åå¥½å’Œæ³¨æ„åŠ›æ¨¡å¼çš„è§è§£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è®­ç»ƒæ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹çš„æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œå¹¶ä¸”ä¸èƒ½ä»…é€šè¿‡å‡ ä¸ªå¯ç”¨çš„ç¤ºä¾‹æœ‰æ•ˆåœ°å¯¹æ–°çš„ä¸ªä½“è¿›è¡Œä¸ªæ€§åŒ–è®¾ç½®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å°æ ·ä¾‹ä¸ªæ€§åŒ–æ‰«æè·¯å¾„é¢„æµ‹ä»»åŠ¡ï¼ˆFS-PSPï¼‰å’Œä¸€ç§è§£å†³è¯¥ä»»åŠ¡çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨ç›®æ ‡å¯¹è±¡çš„æœ€å°æ”¯æŒæ•°æ®æ¥é¢„æµ‹å…¶æœªè§çš„æ‰«æè·¯å¾„ã€‚æˆ‘ä»¬æ–¹æ³•é€‚åº”æ€§çš„å…³é”®æ˜¯ä¸»ä½“åµŒå…¥ç½‘ç»œï¼ˆSE-Netï¼‰ï¼Œå®ƒä¸“é—¨è®¾è®¡ç”¨äºæ•è·æ¯ä¸ªä¸»ä½“æ‰«æè·¯å¾„çš„ç‹¬ç‰¹ä¸ªæ€§åŒ–è¡¨ç¤ºã€‚SE-Netç”Ÿæˆä¸»ä½“åµŒå…¥ï¼Œæœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒä¸»ä½“ï¼ŒåŒæ—¶æœ€å°åŒ–åŒä¸€ä¸»ä½“æ‰«æè·¯å¾„ä¹‹é—´çš„å˜åŒ–ã€‚ä¸ªæ€§åŒ–æ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹ç„¶ååŸºäºè¿™äº›ä¸»ä½“åµŒå…¥æ¥äº§ç”Ÿå‡†ç¡®ã€ä¸ªæ€§åŒ–çš„ç»“æœã€‚åœ¨å¤šä¸ªçœ¼åŠ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FS-PSPç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨æµ‹è¯•æ—¶ä¸éœ€è¦ä»»ä½•å¾®è°ƒæ­¥éª¤ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/few-shot-scanpath">https://github.com/cvlab-stonybrook/few-shot-scanpath</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05499v1">PDF</a> Accepted by CVPR 2025,20 pages, 10 figures</p>
<p><strong>Summary</strong><br>æ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹èƒ½æ´å¯Ÿä¸ªä½“è§†è§‰åå¥½å’Œæ³¨æ„åŠ›æ¨¡å¼ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒï¼Œéš¾ä»¥å¯¹æ–°ä¸ªä½“è¿›è¡Œä¸ªæ€§åŒ–é¢„æµ‹ï¼Œä»…ä½¿ç”¨å°‘é‡æ ·æœ¬åˆ™æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡æå‡ºå°æ ·æœ¬å‘¨ä¸ªæ€§åŒ–æ‰«æè·¯å¾„é¢„æµ‹ä»»åŠ¡ï¼ˆFS-PSPï¼‰åŠå…¶è§£å†³æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨å°‘é‡æ”¯æŒæ•°æ®é¢„æµ‹æœªè§ä¸ªä½“çš„æ‰«æè·¯å¾„ã€‚æ–¹æ³•çš„å…³é”®åœ¨äºè®¾è®¡ä¸»ä½“åµŒå…¥ç½‘ç»œï¼ˆSE-Netï¼‰ï¼Œä¸ºæ¯ä¸ªä¸ªä½“çš„æ‰«æè·¯å¾„ç”Ÿæˆç‹¬ç‰¹çš„ä¸ªæ€§åŒ–è¡¨ç¤ºï¼Œæœ‰æ•ˆåŒºåˆ†ä¸ªä½“å¹¶æœ€å°åŒ–åŒä¸€äººçš„æ‰«æè·¯å¾„å˜åŒ–ã€‚åŸºäºè¿™äº›ä¸»ä½“åµŒå…¥ï¼Œä¸ªæ€§åŒ–æ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹äº§ç”Ÿå‡†ç¡®ç»“æœã€‚åœ¨å¤šä¸ªçœ¼åŠ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨FS-PSPç¯å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œæµ‹è¯•æ—¶æ— éœ€å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªæ€§åŒ–æ¨¡å‹èƒ½æ­ç¤ºè§†è§‰åå¥½å’Œæ³¨æ„åŠ›æ¨¡å¼ã€‚</li>
<li>ç°æœ‰æ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®ï¼Œéš¾ä»¥è¿›è¡Œä¸ªæ€§åŒ–é¢„æµ‹ã€‚</li>
<li>æå‡ºå°æ ·æœ¬å‘¨ä¸ªæ€§åŒ–æ‰«æè·¯å¾„é¢„æµ‹ä»»åŠ¡ï¼ˆFS-PSPï¼‰ã€‚</li>
<li>å¼•å…¥ä¸»ä½“åµŒå…¥ç½‘ç»œï¼ˆSE-Netï¼‰ç”Ÿæˆä¸ªä½“æ‰«æè·¯å¾„çš„ç‹¬ç‰¹è¡¨ç¤ºã€‚</li>
<li>SE-Netèƒ½æœ‰æ•ˆåŒºåˆ†ä¸åŒä¸ªä½“ï¼Œå¹¶æœ€å°åŒ–åŒä¸€äººçš„æ‰«æè·¯å¾„å˜åŒ–ã€‚</li>
<li>åŸºäºä¸»ä½“åµŒå…¥çš„ä¸ªæ€§åŒ–æ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹äº§ç”Ÿå‡†ç¡®ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-93d8c13c464b94cc85346de8df466402.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cf71d65d6943906cc43c7dda370a9a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5502f591a3561cf510c8a2adf4a8db94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ef9bdd4a09b7b2f2e4ee0bb78ff12a4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Intermediate-Layer-Classifiers-for-OOD-generalization"><a href="#Intermediate-Layer-Classifiers-for-OOD-generalization" class="headerlink" title="Intermediate Layer Classifiers for OOD generalization"></a>Intermediate Layer Classifiers for OOD generalization</h2><p><strong>Authors:Arnas Uselis, Seong Joon Oh</strong></p>
<p>Deep classifiers are known to be sensitive to data distribution shifts, primarily due to their reliance on spurious correlations in training data. It has been suggested that these classifiers can still find useful features in the networkâ€™s last layer that hold up under such shifts. In this work, we question the use of last-layer representations for out-of-distribution (OOD) generalisation and explore the utility of intermediate layers. To this end, we introduce \textit{Intermediate Layer Classifiers} (ILCs). We discover that intermediate layer representations frequently offer substantially better generalisation than those from the penultimate layer. In many cases, zero-shot OOD generalisation using earlier-layer representations approaches the few-shot performance of retraining on penultimate layer representations. This is confirmed across multiple datasets, architectures, and types of distribution shifts. Our analysis suggests that intermediate layers are less sensitive to distribution shifts compared to the penultimate layer. These findings highlight the importance of understanding how information is distributed across network layers and its role in OOD generalisation, while also pointing to the limits of penultimate layer representation utility. Code is available at <a target="_blank" rel="noopener" href="https://github.com/oshapio/intermediate-layer-generalization">https://github.com/oshapio/intermediate-layer-generalization</a> </p>
<blockquote>
<p>æ·±åº¦åˆ†ç±»å™¨å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–æ•æ„Ÿï¼Œè¿™ä¸»è¦å½’å› äºå®ƒä»¬å¯¹è®­ç»ƒæ•°æ®ä¸­å¶ç„¶å…³è”æ€§çš„ä¾èµ–ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå·²æœ‰ç ”ç©¶å»ºè®®è¿™äº›åˆ†ç±»å™¨ä»å¯åœ¨ç½‘ç»œçš„æœ€åä¸€å±‚æ‰¾åˆ°åœ¨æ­¤ç±»å˜åŒ–ä¸­ä¾ç„¶æœ‰æ•ˆçš„ç‰¹å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è´¨ç–‘ä½¿ç”¨æœ€åä¸€å±‚çš„è¡¨ç¤ºæ¥è¿›è¡Œè¶…å‡ºåˆ†å¸ƒèŒƒå›´ï¼ˆOODï¼‰æ³›åŒ–çš„åšæ³•ï¼Œå¹¶æ¢ç´¢ä¸­é—´å±‚çš„å®ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸­é—´å±‚åˆ†ç±»å™¨ï¼ˆILCsï¼‰ã€‚æˆ‘ä»¬å‘ç°ä¸­é—´å±‚çš„è¡¨ç¤ºç»å¸¸æ¯”å€’æ•°ç¬¬äºŒå±‚çš„è¡¨ç¤ºæä¾›æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä½¿ç”¨æ—©æœŸå±‚è¡¨ç¤ºçš„é›¶æ ·æœ¬OODæ³›åŒ–æ¥è¿‘åœ¨å€’æ•°ç¬¬äºŒå±‚è¡¨ç¤ºä¸Šè¿›è¡Œå¾®è°ƒåçš„å°‘æ•°æ ·æœ¬æ€§èƒ½ã€‚è¿™åœ¨å¤šä¸ªæ•°æ®é›†ã€æ¶æ„å’Œåˆ†å¸ƒå˜åŒ–ç±»å‹ä¸Šå¾—åˆ°äº†è¯å®ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸­é—´å±‚ä¸å€’æ•°ç¬¬äºŒå±‚ç›¸æ¯”ï¼Œå¯¹åˆ†å¸ƒå˜åŒ–çš„æ•æ„Ÿæ€§è¾ƒä½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç†è§£ä¿¡æ¯åœ¨ç½‘ç»œå„å±‚ä¸­çš„åˆ†å¸ƒåŠå…¶åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´æ³›åŒ–ä¸­çš„ä½œç”¨çš„é‡è¦æ€§ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†å€’æ•°ç¬¬äºŒå±‚è¡¨ç¤ºå®ç”¨æ€§çš„å±€é™æ€§ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/oshapio/intermediate-layer-generalization%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/oshapio/intermediate-layer-generalizationè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05461v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦åˆ†ç±»å™¨å¯¹äºæ•°æ®åˆ†å¸ƒå˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œå¹¶æå‡ºäº†ä¸­é—´å±‚åˆ†ç±»å™¨ï¼ˆILCsï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œä¸­é—´å±‚çš„è¡¨ç¤ºåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æä¾›äº†æ¯”æœ€ç»ˆå±‚æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œç”šè‡³åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹æ¥è¿‘é‡æ–°è®­ç»ƒæœ€ç»ˆå±‚è¡¨ç¤ºçš„å°‘é‡æ ·æœ¬æ€§èƒ½ã€‚è¿™äº›å‘ç°å¯¹äºç†è§£ä¿¡æ¯åœ¨ç½‘ç»œå„å±‚çš„åˆ†å¸ƒåŠå…¶åœ¨é¢å‘æœªçŸ¥åˆ†å¸ƒæ•°æ®çš„æ³›åŒ–ä¸­çš„è§’è‰²è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦åˆ†ç±»å™¨å¯¹è®­ç»ƒæ•°æ®çš„å¶ç„¶ç›¸å…³æ€§éå¸¸æ•æ„Ÿï¼Œè¿™å¯¼è‡´äº†å…¶åœ¨é¢å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–æ—¶çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ä¸­é—´å±‚åˆ†ç±»å™¨ï¼ˆILCsï¼‰è¢«å¼•å…¥ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒä»¬åˆ©ç”¨ç½‘ç»œä¸­é—´å±‚çš„è¡¨ç¤ºè¿›è¡Œé¢„æµ‹ã€‚</li>
<li>ä¸­é—´å±‚çš„è¡¨ç¤ºé€šå¸¸æä¾›äº†æ¯”æœ€ç»ˆå±‚æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ—©æœŸå±‚çš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½æ¥è¿‘äºåœ¨æœ€ç»ˆå±‚é‡æ–°è®­ç»ƒæ‰€éœ€çš„å°‘é‡æ ·æœ¬ã€‚</li>
<li>å¤šç§æ•°æ®é›†ã€æ¶æ„å’Œåˆ†å¸ƒå˜åŒ–ç±»å‹ä¸Šçš„å®éªŒè¯å®äº†è¿™äº›å‘ç°ã€‚</li>
<li>ä¸­é—´å±‚ç›¸å¯¹äºæœ€ç»ˆå±‚å¯¹åˆ†å¸ƒå˜åŒ–çš„æ•æ„Ÿæ€§è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05461">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35cdcbec9c876fb148b37ee89daff16f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55308da7bd597d4714bfee144fda0a93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7af17e34503d745724e271662ade3045.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0516c7c39b616b5c22404d42695b62ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07a0eeca3e5895a748009e4ced152bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3841c898dcf5e3efdda4e79ad4960a7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TULIP-Towards-Unified-Language-Image-Pretraining"><a href="#TULIP-Towards-Unified-Language-Image-Pretraining" class="headerlink" title="TULIP: Towards Unified Language-Image Pretraining"></a>TULIP: Towards Unified Language-Image Pretraining</h2><p><strong>Authors:Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, David M. Chan</strong></p>
<p>Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image&#x2F;text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a $2\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over $3\times$ higher scores than SigLIP on MMVP. Our code&#x2F;checkpoints are available at <a target="_blank" rel="noopener" href="https://tulip-berkeley.github.io/">https://tulip-berkeley.github.io</a> </p>
<blockquote>
<p>å°½ç®¡CLIPå’ŒSigLIPç­‰å›¾æ–‡å¯¹æ¯”æ¨¡å‹è¿‘æœŸå–å¾—äº†æˆåŠŸï¼Œä½†è¿™äº›æ¨¡å‹åœ¨è¿›è¡Œéœ€è¦é«˜ä¿çœŸå›¾åƒç†è§£çš„ä»»åŠ¡æ—¶ï¼Œå¦‚è®¡æ•°ã€æ·±åº¦ä¼°è®¡å’Œç²¾ç»†ç›®æ ‡è¯†åˆ«ç­‰æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹é€šè¿‡æ‰§è¡Œè¯­è¨€å¯¹é½ï¼Œå€¾å‘äºä¼˜å…ˆå¤„ç†é«˜çº§è¯­ä¹‰è€Œéè§†è§‰ç†è§£ï¼Œä»è€Œå‰Šå¼±äº†å®ƒä»¬çš„å›¾åƒç†è§£èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨¡å‹åœ¨å¤„ç†è§†è§‰ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£è¯­è¨€æ–¹é¢å´é‡åˆ°å›°éš¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è¯­è¨€é©±åŠ¨ä»»åŠ¡ä¸­çš„çµæ´»æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†TULIPï¼Œå®ƒæ˜¯ç°æœ‰CLIPç±»ä¼¼æ¨¡å‹çš„å³æ’å³ç”¨æ›¿ä»£å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç”Ÿæˆæ€§æ•°æ®å¢å¼ºã€å¢å¼ºçš„å›¾åƒå›¾åƒå’Œæ–‡æœ¬æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ä»¥åŠå›¾åƒ&#x2F;æ–‡æœ¬é‡å»ºæ­£åˆ™åŒ–ï¼Œå­¦ä¹ ç²¾ç»†çš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™å…¨å±€è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•åˆ°è¶…è¿‡10äº¿å‚æ•°ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨ImageNet-1Kä¸Šå»ºç«‹äº†æ–°çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œåœ¨RxRx1ä¸Šå¯¹SigLIPè¿›è¡Œçº¿æ€§æ¢æµ‹æ—¶çš„å‡†ç¡®ç‡æé«˜äº†ä¸¤å€è¿›è¡Œå°æ ·æœ¬åˆ†ç±»ï¼›åŒæ—¶æ”¹è¿›äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨MMVPä¸Šè¾¾åˆ°äº†è¶…è¿‡SigLIPä¸‰å€çš„æˆç»©ã€‚æˆ‘ä»¬çš„ä»£ç &#x2F;æ£€æŸ¥ç‚¹å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š[<a target="_blank" rel="noopener" href="https://tulip-berkeley.github.io/]">https://tulip-berkeley.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15485v2">PDF</a> (v2) Clarified fine-tuning process, updated appendix</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TULIPæ¨¡å‹ï¼Œå®ƒæ˜¯CLIPç±»æ¨¡å‹çš„å¼€æºæ›¿ä»£æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³å›¾åƒç†è§£çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨ç”Ÿæˆå¼æ•°æ®å¢å¼ºã€å¢å¼ºçš„å›¾åƒ-å›¾åƒå’Œæ–‡æœ¬-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ä»¥åŠå›¾åƒ&#x2F;æ–‡æœ¬é‡å»ºæ­£åˆ™åŒ–ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™å…¨å±€è¯­ä¹‰å¯¹é½çš„åŒæ—¶å­¦ä¹ ç²¾ç»†çš„è§†è§‰ç‰¹å¾ã€‚æ­¤æ–¹æ³•è§„æ¨¡è¶…è¿‡1Bå‚æ•°ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰æ¨¡å‹çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TULIPæ¨¡å‹æ˜¯è§£å†³å›¾åƒç†è§£é—®é¢˜çš„å¼€æ”¾æºä»£ç è§£å†³æ–¹æ¡ˆï¼Œé’ˆå¯¹CLIPç­‰æ¨¡å‹è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>TULIPå¼ºè°ƒåœ¨ä¿ç•™å…¨å±€è¯­ä¹‰å¯¹é½çš„åŒæ—¶å­¦ä¹ ç²¾ç»†çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>TULIPåˆ©ç”¨ç”Ÿæˆå¼æ•°æ®å¢å¼ºè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¢å¼ºçš„å›¾åƒåˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°æ–‡æœ¬çš„å¯¹æ¯”å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>TULIPé€šè¿‡å›¾åƒ&#x2F;æ–‡æœ¬é‡å»ºæ­£åˆ™åŒ–è¿›è¡Œè®­ç»ƒã€‚</li>
<li>TULIPæ¨¡å‹è§„æ¨¡è¶…è¿‡1Bå‚æ•°ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰SOTAæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db91077c92830c60429a5f183c497fda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04064be4960f653e424732faaa637482.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d38f489f16deb4ab037ead38cf77ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad7431fc2668c90d23d1d96333119174.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CALF-Aligning-LLMs-for-Time-Series-Forecasting-via-Cross-modal-Fine-Tuning"><a href="#CALF-Aligning-LLMs-for-Time-Series-Forecasting-via-Cross-modal-Fine-Tuning" class="headerlink" title="CALF: Aligning LLMs for Time Series Forecasting via Cross-modal   Fine-Tuning"></a>CALF: Aligning LLMs for Time Series Forecasting via Cross-modal   Fine-Tuning</h2><p><strong>Authors:Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia</strong></p>
<p>Deep learning (e.g., Transformer) has been widely and successfully used in multivariate time series forecasting (MTSF). Unlike existing methods that focus on training models from a single modal of time series input, large language models (LLMs) based MTSF methods with cross-modal text and time series input have recently shown great superiority, especially with limited temporal data. However, current LLM-based MTSF methods usually focus on adapting and fine-tuning LLMs, while neglecting the distribution discrepancy between textual and temporal input tokens, thus leading to sub-optimal performance. To address this issue, we propose a novel Cross-Modal LLM Fine-Tuning (CALF) framework for MTSF by reducing the distribution discrepancy between textual and temporal data, which mainly consists of the temporal target branch with temporal input and the textual source branch with aligned textual input. To reduce the distribution discrepancy, we develop the cross-modal match module to first align cross-modal input distributions. Additionally, to minimize the modality distribution gap in both feature and output spaces, feature regularization loss is developed to align the intermediate features between the two branches for better weight updates, while output consistency loss is introduced to allow the output representations of both branches to correspond effectively. Thanks to the modality alignment, CALF establishes state-of-the-art performance for both long-term and short-term forecasting tasks with low computational complexity, and exhibiting favorable few-shot and zero-shot abilities similar to that in LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hank0626/LLaTA">https://github.com/Hank0626/LLaTA</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆä¾‹å¦‚Transformerï¼‰å·²å¹¿æ³›åº”ç”¨äºå¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆMTSFï¼‰ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ä»å•ä¸€æ¨¡æ€çš„æ—¶é—´åºåˆ—è¾“å…¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„MTSFæ–¹æ³•é‡‡ç”¨è·¨æ¨¡æ€æ–‡æœ¬å’Œæ—¶é—´åºåˆ—è¾“å…¥ï¼Œå·²æ˜¾ç¤ºå‡ºå·¨å¤§çš„ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™çš„æ—¶é—´æ•°æ®æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„LLM-based MTSFæ–¹æ³•é€šå¸¸ä¾§é‡äºé€‚åº”å’Œå¾®è°ƒLLMï¼Œè€Œå¿½ç•¥äº†æ–‡æœ¬å’Œæ—¶é—´è¾“å…¥æ ‡è®°ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹çš„è·¨æ¨¡æ€LLMå¾®è°ƒï¼ˆCALFï¼‰æ¡†æ¶ï¼Œé€šè¿‡å‡å°‘æ–‡æœ¬å’Œæ—¶é—´æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œè¯¥æ¡†æ¶ä¸»è¦ç”±å¸¦æœ‰æ—¶é—´è¾“å…¥çš„ç›®æ ‡åˆ†æ”¯å’Œå¸¦æœ‰å¯¹é½æ–‡æœ¬è¾“å…¥çš„æ–‡æœ¬æºåˆ†æ”¯ç»„æˆã€‚ä¸ºäº†å‡å°‘åˆ†å¸ƒå·®å¼‚ï¼Œæˆ‘ä»¬å¼€å‘äº†è·¨æ¨¡æ€åŒ¹é…æ¨¡å—æ¥é¦–å…ˆå¯¹é½è·¨æ¨¡æ€è¾“å…¥åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ€å°åŒ–ç‰¹å¾å’Œè¾“å‡ºç©ºé—´ä¸­çš„æ¨¡æ€åˆ†å¸ƒå·®è·ï¼Œæˆ‘ä»¬å¼€å‘äº†ç‰¹å¾æ­£åˆ™åŒ–æŸå¤±æ¥å¯¹é½ä¸¤ä¸ªåˆ†æ”¯ä¹‹é—´çš„ä¸­é—´ç‰¹å¾ä»¥å®ç°æ›´å¥½çš„æƒé‡æ›´æ–°ï¼ŒåŒæ—¶å¼•å…¥äº†è¾“å‡ºä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä½¿ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºè¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹åº”ã€‚å¾—ç›Šäºæ¨¡æ€å¯¹é½ï¼ŒCALFåœ¨ä¸­é•¿æœŸé¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå…·æœ‰è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶å±•ç°å‡ºä¸LLMç›¸ä¼¼çš„ä¼˜ç§€çš„å°æ ·æœ¬å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚ä»£ç å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/Hank0626/LLaTA">https://github.com/Hank0626/LLaTA</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07300v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€LLMå¾®è°ƒï¼ˆCALFï¼‰æ¡†æ¶ï¼Œç”¨äºå¤„ç†å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ã€‚é€šè¿‡å‡å°‘æ–‡æœ¬å’Œæ—¶é—´æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼ŒCALFæ¡†æ¶åŒ…æ‹¬æ—¶é—´ç›®æ ‡åˆ†æ”¯å’Œæ–‡æœ¬æºåˆ†æ”¯ï¼Œå¼€å‘è·¨æ¨¡æ€åŒ¹é…æ¨¡å—å’Œç‰¹å¾æ­£åˆ™åŒ–æŸå¤±æ¥å¯¹é½ç‰¹å¾ç©ºé—´ä¸­çš„æ¨¡æ€åˆ†å¸ƒå·®è·ï¼ŒåŒæ—¶å¼•å…¥è¾“å‡ºä¸€è‡´æ€§æŸå¤±ä»¥ç¡®ä¿ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºè¡¨ç¤ºæœ‰æ•ˆå¯¹åº”ã€‚è¯¥æ¡†æ¶å®ç°äº†ä½è®¡ç®—å¤æ‚åº¦å’Œä¼˜è¶Šçš„é¢„æµ‹æ€§èƒ½ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬å’Œæ— æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ï¼ˆå¦‚Transformerï¼‰åœ¨å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆMTSFï¼‰ä¸­å¹¿æ³›åº”ç”¨ä¸”æˆåŠŸã€‚</li>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŸºäºçš„MTSFæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¤„ç†æœ‰é™çš„æ—¶é—´åºåˆ—æ•°æ®æ—¶æ˜¾ç¤ºå‡ºå·¨å¤§ä¼˜åŠ¿ã€‚</li>
<li>å½“å‰LLM-basedçš„MTSFæ–¹æ³•ä¸»è¦å…³æ³¨LLMsçš„é€‚åº”å’Œå¾®è°ƒï¼Œå¿½ç•¥äº†æ–‡æœ¬å’Œæ—¶é—´è¾“å…¥æ ‡è®°çš„åˆ†å¸ƒå·®å¼‚ã€‚</li>
<li>CALFæ¡†æ¶æ—¨åœ¨å‡å°‘æ–‡æœ¬å’Œæ—¶é—´æ•°æ®çš„åˆ†å¸ƒå·®å¼‚ï¼ŒåŒ…æ‹¬æ—¶é—´ç›®æ ‡åˆ†æ”¯å’Œæ–‡æœ¬æºåˆ†æ”¯ã€‚</li>
<li>CALFé€šè¿‡è·¨æ¨¡æ€åŒ¹é…æ¨¡å—å’Œç‰¹å¾æ­£åˆ™åŒ–æŸå¤±æ¥å¯¹é½ä¸åŒæ¨¡æ€çš„ç‰¹å¾ç©ºé—´åˆ†å¸ƒã€‚</li>
<li>è¾“å‡ºä¸€è‡´æ€§æŸå¤±ç¡®ä¿ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºè¡¨ç¤ºæœ‰æ•ˆå¯¹åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.07300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a1c926e66a6db6f333274dc01e2b0d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bedf514f09da9b71db22f8d5ebe847f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b1862b2225e9af82fb8fc35a70af053.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54d86a5a3dee98004f4c15ff24c6626e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e303aa4a56ae1c0bc05a06ef6e2bc2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4fe26cb16296d8b1515a196e56585d6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-07dd3402ba8be4b8214932c81b0918b1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Flash Sculptor Modular 3D Worlds from Objects
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-527451acd9900a488a751ecb13eb5787.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  TxGemma Efficient and Agentic LLMs for Therapeutics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
