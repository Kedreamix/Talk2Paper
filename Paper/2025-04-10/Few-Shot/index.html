<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-10  Memory-Modular Classification Learning to Generalize with Memory   Replacement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-54d86a5a3dee98004f4c15ff24c6626e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-10-更新"><a href="#2025-04-10-更新" class="headerlink" title="2025-04-10 更新"></a>2025-04-10 更新</h1><h2 id="Memory-Modular-Classification-Learning-to-Generalize-with-Memory-Replacement"><a href="#Memory-Modular-Classification-Learning-to-Generalize-with-Memory-Replacement" class="headerlink" title="Memory-Modular Classification: Learning to Generalize with Memory   Replacement"></a>Memory-Modular Classification: Learning to Generalize with Memory   Replacement</h2><p><strong>Authors:Dahyun Kang, Ahmet Iscen, Eunchan Jo, Sua Choi, Minsu Cho, Cordelia Schmid</strong></p>
<p>We propose a novel memory-modular learner for image classification that separates knowledge memorization from reasoning. Our model enables effective generalization to new classes by simply replacing the memory contents, without the need for model retraining. Unlike traditional models that encode both world knowledge and task-specific skills into their weights during training, our model stores knowledge in the external memory of web-crawled image and text data. At inference time, the model dynamically selects relevant content from the memory based on the input image, allowing it to adapt to arbitrary classes by simply replacing the memory contents. The key differentiator that our learner meta-learns to perform classification tasks with noisy web data from unseen classes, resulting in robust performance across various classification scenarios. Experimental results demonstrate the promising performance and versatility of our approach in handling diverse classification tasks, including zero-shot&#x2F;few-shot classification of unseen classes, fine-grained classification, and class-incremental classification. </p>
<blockquote>
<p>我们提出了一种用于图像分类的新型记忆模块化学习者，它将知识记忆与推理相分离。我们的模型通过简单地替换记忆内容，而无需重新训练模型，就能够有效地推广到新的类别。不同于传统模型在训练过程中将世界知识和特定任务技能编码到权重中，我们的模型将知识存储在从网络爬取的图像和文本数据的外部记忆中。在推理过程中，模型根据输入图像动态选择记忆中的相关内容，通过简单地替换记忆内容，就能够适应任意类别。我们的学习者的关键区别在于，它会进行元学习，使用来自未见类别的带有噪声的网页数据进行分类任务，从而在各种分类场景中实现稳健的性能。实验结果表明，我们的方法在处理各种分类任务时具有广阔的应用前景，包括零样本&#x2F;少样本未见类别分类、细粒度分类和类增量分类。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06021v1">PDF</a> Accepted to TMLR. Code available: <a target="_blank" rel="noopener" href="https://github.com/dahyun-kang/mml">https://github.com/dahyun-kang/mml</a></p>
<p><strong>Summary</strong><br>本模型提出了一种新的记忆模块化学习者用于图像分类，它将知识记忆与推理分离。通过仅替换记忆内容，即可实现对新类的有效泛化，无需进行模型再训练。该模型将知识存储在从网络爬取的图像和文本数据的外部记忆中。在推理时，模型根据输入图像动态选择相关内存内容，通过简单替换内存内容即可适应任意类别。该模型的关键区别在于，它能够使用来自未见类别的网络数据的噪声进行元学习分类任务，从而在各种分类场景中实现稳健性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型实现了记忆和推理的分离，有助于提高泛化能力。</li>
<li>通过替换记忆内容，模型能够适应新的类别，无需重新训练。</li>
<li>模型将知识存储在外部记忆中，包括从网络爬取的图像和文本数据。</li>
<li>模型能够根据输入图像动态选择相关内存内容。</li>
<li>模型能够在未见类别的情况下进行元学习分类任务。</li>
<li>实验结果证明了该模型在处理多种分类任务上的优异性能和通用性，包括零样本&#x2F;少样本分类、细粒度分类和类别增量分类。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06021">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a1ff45c0893cfff1ed5b4a0060a9ff71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d00c86f0198b7698ac99a514cdf36ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da0cb1a9c4eda7342b72be44911d7a37.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Alignment-Free-Video-Matching-for-Few-shot-Action-Recognition"><a href="#Temporal-Alignment-Free-Video-Matching-for-Few-shot-Action-Recognition" class="headerlink" title="Temporal Alignment-Free Video Matching for Few-shot Action Recognition"></a>Temporal Alignment-Free Video Matching for Few-shot Action Recognition</h2><p><strong>Authors:SuBeen Lee, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo</strong></p>
<p>Few-Shot Action Recognition (FSAR) aims to train a model with only a few labeled video instances. A key challenge in FSAR is handling divergent narrative trajectories for precise video matching. While the frame- and tuple-level alignment approaches have been promising, their methods heavily rely on pre-defined and length-dependent alignment units (e.g., frames or tuples), which limits flexibility for actions of varying lengths and speeds. In this work, we introduce a novel TEmporal Alignment-free Matching (TEAM) approach, which eliminates the need for temporal units in action representation and brute-force alignment during matching. Specifically, TEAM represents each video with a fixed set of pattern tokens that capture globally discriminative clues within the video instance regardless of action length or speed, ensuring its flexibility. Furthermore, TEAM is inherently efficient, using token-wise comparisons to measure similarity between videos, unlike existing methods that rely on pairwise comparisons for temporal alignment. Additionally, we propose an adaptation process that identifies and removes common information across classes, establishing clear boundaries even between novel categories. Extensive experiments demonstrate the effectiveness of TEAM. Codes are available at github.com&#x2F;leesb7426&#x2F;TEAM. </p>
<blockquote>
<p>少量样本动作识别（FSAR）旨在仅使用少量标注视频实例来训练模型。FSAR的一个关键挑战是处理不同的叙事轨迹以实现精确的视频匹配。尽管帧级和元组级对齐方法已经显示出希望，但它们的方法严重依赖于预定义和长度依赖的对齐单位（例如帧或元组），这限制了对于不同长度和速度动作的灵活性。在这项工作中，我们引入了一种新型的无需时间对齐匹配（TEAM）方法，该方法消除了动作表示中时间单位的需求以及在匹配过程中的强制对齐。具体来说，TEAM使用一组固定的模式令牌来表示每个视频，这些令牌可以捕获视频实例中的全局鉴别线索，而不考虑动作的长度或速度，从而确保其灵活性。此外，TEAM本质上是高效的，使用令牌级的比较来测量视频之间的相似性，与现有方法不同，现有方法依赖于成对比较进行时间对齐。另外，我们提出了一个适应过程，可以识别和删除跨类别的通用信息，即使在新型类别之间也建立清晰的边界。大量实验证明了TEAM的有效性。代码可在github.com&#x2F;leesb7426&#x2F;TEAM找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05956v1">PDF</a> 10 pages, 7 figures, 6 tables, Accepted to CVPR 2025 as Oral   Presentation</p>
<p><strong>Summary</strong><br>少量标注视频样本的动作识别（FSAR）面临处理不同叙事轨迹的挑战。现有方法依赖于预定义和长度依赖的对齐单元，限制了其在不同长度和速度动作上的灵活性。本文提出了一种新型的无需时间对齐匹配（TEAM）方法，通过固定模式的令牌表示视频，捕捉全局鉴别线索，确保灵活性。此外，它采用令牌比较测量视频相似性，具有高效性。同时，提出适应过程，明确类别间的界限。实验证明TEAM的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Action Recognition (FSAR) 旨在用少量标注视频实例训练模型。</li>
<li>处理不同叙事轨迹是FSAR的一个关键挑战。</li>
<li>现有方法依赖于预定义和长度依赖的对齐单元，限制了其在不同长度和速度动作的灵活性。</li>
<li>TEAM方法通过固定模式的令牌表示视频，无需时间对齐。</li>
<li>TEAM方法捕捉全局鉴别线索，确保灵活性并测量视频相似性。</li>
<li>TEAM采用令牌比较，具有高效性，与现有方法不同。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-60aefaf404d75d7e9374151f31553bf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-025dab5eeacc9b929642efa02cf0086c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53f0caad65b46c482c921631eadb9dde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd43c84c0be3692d26a9dd333080fb27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03fcc6107c16411e4341187f64467ba0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Švábenský, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators’ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>我们探讨了大型语言模型（LLM）在自动评估学生开放性反思和预测学业表现方面的应用。传统的反思评估方法耗时且在教育环境中可能无法有效扩展。在这项工作中，我们采用LLM，使用两种评估策略（单智能体和多智能体）和两种提示技术（零样本和少样本），将学生反思转化为量化分数。我们在包含来自377名学生在三个学术学期内的5,278篇反思的数据集上进行的实验表明，采用少样本策略的单智能体取得了最高的与人类评价的匹配率。此外，利用LLM评估的反思分数的模型在风险学生识别和成绩预测任务中的表现均优于基线。这些结果表明，LLM可以有效地自动进行反思评估，减少教育工作者的工作量，并为可能需要额外帮助的学生提供及时的支持。我们的工作强调了将先进的生成性AI技术融入教育实践中的潜力，以提高学生的参与度和学业成功。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v1">PDF</a> To be published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025)</p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLMs）在自动评估学生开放性反思和预测学业表现方面的应用探究。传统评估方式耗时且难以在教育环境中有效扩展。本研究采用LLMs，通过两种评估策略（单主体和多主体）和两种提示技术（零样本和少样本），将学生反思转化为量化分数。实验数据显示，采用少样本策略的单主体模型与人类评估匹配度最高。此外，利用LLM评估的反思分数的模型在识别学业风险学生和预测成绩任务中的表现均优于基线。这表明LLMs能有效自动化反思评估，减轻教育工作者的工作量，并为可能需要额外帮助的学生提供及时支持。本研究强调了将先进的生成式AI技术融入教育实践，以提高学生学习参与度和学业成功的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究使用大规模语言模型（LLMs）自动评估学生的开放性反思，预测学业表现。</li>
<li>传统评估方式存在耗时且难以在教育环境中有效扩展的问题。</li>
<li>采用LLMs的评估方式包括两种策略：单主体评估和多主体评估，以及两种提示技术：零样本和少样本。</li>
<li>实验结果显示，采用少样本策略的单主体模型与人类评估匹配度最高。</li>
<li>LLMs在识别学业风险学生和预测成绩任务中的表现优于基线方法。</li>
<li>LLMs的应用能有效自动化反思评估，减轻教育工作者的工作量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5366bb7082c7ca612d81c6680389fc41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Smarter-Hiring-Are-Zero-Shot-and-Few-Shot-Pre-trained-LLMs-Ready-for-HR-Spoken-Interview-Transcript-Analysis"><a href="#Towards-Smarter-Hiring-Are-Zero-Shot-and-Few-Shot-Pre-trained-LLMs-Ready-for-HR-Spoken-Interview-Transcript-Analysis" class="headerlink" title="Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs   Ready for HR Spoken Interview Transcript Analysis?"></a>Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs   Ready for HR Spoken Interview Transcript Analysis?</h2><p><strong>Authors:Subhankar Maity, Aniket Deroy, Sudeshna Sarkar</strong></p>
<p>This research paper presents a comprehensive analysis of the performance of prominent pre-trained large language models (LLMs), including GPT-4 Turbo, GPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001, text-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in comparison to expert human evaluators in providing scores, identifying errors, and offering feedback and improvement suggestions to candidates during mock HR (Human Resources) interviews. We introduce a dataset called HURIT (Human Resource Interview Transcripts), which comprises 3,890 HR interview transcripts sourced from real-world HR interview scenarios. Our findings reveal that pre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit commendable performance and are capable of producing evaluations comparable to those of expert human evaluators. Although these LLMs demonstrate proficiency in providing scores comparable to human experts in terms of human evaluation metrics, they frequently fail to identify errors and offer specific actionable advice for candidate performance improvement in HR interviews. Our research suggests that the current state-of-the-art pre-trained LLMs are not fully conducive for automatic deployment in an HR interview assessment. Instead, our findings advocate for a human-in-the-loop approach, to incorporate manual checks for inconsistencies and provisions for improving feedback quality as a more suitable strategy. </p>
<blockquote>
<p>本文全面分析了主流预训练大型语言模型（LLM）在模拟人力资源（HR）面试中的表现，包括GPT-4 Turbo、GPT-3.5 Turbo、text-davinci-003、text-babbage-001、text-curie-001、text-ada-001、llama-2-7b-chat、llama-2-13b-chat和llama-2-70b-chat等模型与专家评委相比在评分、识别错误和对候选人提供反馈和改进建议方面的表现。我们引入了一个名为HURIT（人力资源面试记录）的数据集，其中包含来自现实世界人力资源面试场景的3890份面试记录。我们的研究发现，预训练LLM，尤其是GPT-4 Turbo和GPT-3.5 Turbo，表现出值得称赞的性能，能够产生与专家评委相当的评价。尽管这些LLM在按照人类评估指标提供可比分数的评分方面表现出色，但它们往往无法识别错误并为候选人在人力资源面试中的表现改进提供具体的可操作的建议。我们的研究表明，目前最先进的预训练LLM并不适合完全自动部署在人力资源面试评估中。相反，我们的研究结果提倡采用人类参与循环的方法，通过手动检查不一致并提供改进反馈质量的策略，作为更合适的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05683v1">PDF</a> 32 pages, 24 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了多种主流预训练大语言模型（LLMs）在模拟人力资源（HR）面试中的表现。通过引入HURIT数据集，对LLMs在评分、识别错误以及提供改进建议方面的能力进行了评估。研究发现，尽管LLMs在评分方面表现出色，但与人类专家相比，它们在识别错误和提供具体可操作的改进建议方面存在不足。因此，目前LLMs并不适合完全自动部署在HR面试评估中。建议采用人机结合的方式，以提高反馈质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练大语言模型（LLMs）在模拟人力资源（HR）面试中的评分表现与人类专家相当。</li>
<li>GPT-4 Turbo和GPT-3.5 Turbo在HR面试评估中展现出卓越性能。</li>
<li>LLMs在识别面试中的错误和提供具体可操作的改进建议方面存在不足。</li>
<li>目前LLMs并不适合完全自动部署在HR面试评估中。</li>
<li>建议采用人机结合的方式，通过手动检查不一致性来提高反馈质量。</li>
<li>HURIT数据集为评估LLMs在HR面试中的表现提供了重要资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05683">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-61dbb4d41c8a2afcf9b289065780b2ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d564a67d0331a5feba653f5c68f7b27b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-168535b36d9b4a7d8d4055e831478827.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea1de9e9f8fbc7fecff916e639fa63a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db95ae9c62a8e7ce0d8a3580038c2c82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dafffc76e2288858b53b457ea629383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-931d5ea9d4e849c15ed4d38d549588db.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-shot-Personalized-Scanpath-Prediction"><a href="#Few-shot-Personalized-Scanpath-Prediction" class="headerlink" title="Few-shot Personalized Scanpath Prediction"></a>Few-shot Personalized Scanpath Prediction</h2><p><strong>Authors:Ruoyu Xue, Jingyi Xu, Sounak Mondal, Hieu Le, Gregory Zelinsky, Minh Hoai, Dimitris Samaras</strong></p>
<p>A personalized model for scanpath prediction provides insights into the visual preferences and attention patterns of individual subjects. However, existing methods for training scanpath prediction models are data-intensive and cannot be effectively personalized to new individuals with only a few available examples. In this paper, we propose few-shot personalized scanpath prediction task (FS-PSP) and a novel method to address it, which aims to predict scanpaths for an unseen subject using minimal support data of that subject’s scanpath behavior. The key to our method’s adaptability is the Subject-Embedding Network (SE-Net), specifically designed to capture unique, individualized representations for each subject’s scanpaths. SE-Net generates subject embeddings that effectively distinguish between subjects while minimizing variability among scanpaths from the same individual. The personalized scanpath prediction model is then conditioned on these subject embeddings to produce accurate, personalized results. Experiments on multiple eye-tracking datasets demonstrate that our method excels in FS-PSP settings and does not require any fine-tuning steps at test time. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/few-shot-scanpath">https://github.com/cvlab-stonybrook/few-shot-scanpath</a> </p>
<blockquote>
<p>个性化扫描路径预测模型为我们提供了关于个体视觉偏好和注意力模式的见解。然而，现有的训练扫描路径预测模型的方法需要大量的数据，并且不能仅通过几个可用的示例有效地对新的个体进行个性化设置。在本文中，我们提出了小样例个性化扫描路径预测任务（FS-PSP）和一种解决该任务的新方法，旨在使用目标对象的最小支持数据来预测其未见的扫描路径。我们方法适应性的关键是主体嵌入网络（SE-Net），它专门设计用于捕获每个主体扫描路径的独特个性化表示。SE-Net生成主体嵌入，有效地区分不同主体，同时最小化同一主体扫描路径之间的变化。个性化扫描路径预测模型然后基于这些主体嵌入来产生准确、个性化的结果。在多个眼动数据集上的实验表明，我们的方法在FS-PSP环境中表现出色，并且在测试时不需要任何微调步骤。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/few-shot-scanpath">https://github.com/cvlab-stonybrook/few-shot-scanpath</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05499v1">PDF</a> Accepted by CVPR 2025,20 pages, 10 figures</p>
<p><strong>Summary</strong><br>扫描路径预测模型能洞察个体视觉偏好和注意力模式。现有方法需要大量数据训练，难以对新个体进行个性化预测，仅使用少量样本则效果不佳。本文提出小样本周个性化扫描路径预测任务（FS-PSP）及其解决方法，旨在使用少量支持数据预测未见个体的扫描路径。方法的关键在于设计主体嵌入网络（SE-Net），为每个个体的扫描路径生成独特的个性化表示，有效区分个体并最小化同一人的扫描路径变化。基于这些主体嵌入，个性化扫描路径预测模型产生准确结果。在多个眼动数据集上的实验表明，该方法在FS-PSP环境下表现优异，测试时无需微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>个性化模型能揭示视觉偏好和注意力模式。</li>
<li>现有扫描路径预测模型需要大量数据，难以进行个性化预测。</li>
<li>提出小样本周个性化扫描路径预测任务（FS-PSP）。</li>
<li>引入主体嵌入网络（SE-Net）生成个体扫描路径的独特表示。</li>
<li>SE-Net能有效区分不同个体，并最小化同一人的扫描路径变化。</li>
<li>基于主体嵌入的个性化扫描路径预测模型产生准确结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-93d8c13c464b94cc85346de8df466402.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cf71d65d6943906cc43c7dda370a9a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5502f591a3561cf510c8a2adf4a8db94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ef9bdd4a09b7b2f2e4ee0bb78ff12a4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Intermediate-Layer-Classifiers-for-OOD-generalization"><a href="#Intermediate-Layer-Classifiers-for-OOD-generalization" class="headerlink" title="Intermediate Layer Classifiers for OOD generalization"></a>Intermediate Layer Classifiers for OOD generalization</h2><p><strong>Authors:Arnas Uselis, Seong Joon Oh</strong></p>
<p>Deep classifiers are known to be sensitive to data distribution shifts, primarily due to their reliance on spurious correlations in training data. It has been suggested that these classifiers can still find useful features in the network’s last layer that hold up under such shifts. In this work, we question the use of last-layer representations for out-of-distribution (OOD) generalisation and explore the utility of intermediate layers. To this end, we introduce \textit{Intermediate Layer Classifiers} (ILCs). We discover that intermediate layer representations frequently offer substantially better generalisation than those from the penultimate layer. In many cases, zero-shot OOD generalisation using earlier-layer representations approaches the few-shot performance of retraining on penultimate layer representations. This is confirmed across multiple datasets, architectures, and types of distribution shifts. Our analysis suggests that intermediate layers are less sensitive to distribution shifts compared to the penultimate layer. These findings highlight the importance of understanding how information is distributed across network layers and its role in OOD generalisation, while also pointing to the limits of penultimate layer representation utility. Code is available at <a target="_blank" rel="noopener" href="https://github.com/oshapio/intermediate-layer-generalization">https://github.com/oshapio/intermediate-layer-generalization</a> </p>
<blockquote>
<p>深度分类器对数据分布变化敏感，这主要归因于它们对训练数据中偶然关联性的依赖。尽管如此，已有研究建议这些分类器仍可在网络的最后一层找到在此类变化中依然有效的特征。在这项工作中，我们质疑使用最后一层的表示来进行超出分布范围（OOD）泛化的做法，并探索中间层的实用性。为此，我们引入了中间层分类器（ILCs）。我们发现中间层的表示经常比倒数第二层的表示提供更好的泛化能力。在许多情况下，使用早期层表示的零样本OOD泛化接近在倒数第二层表示上进行微调后的少数样本性能。这在多个数据集、架构和分布变化类型上得到了证实。我们的分析表明，中间层与倒数第二层相比，对分布变化的敏感性较低。这些发现强调了理解信息在网络各层中的分布及其在超出分布范围泛化中的作用的重要性，同时也指出了倒数第二层表示实用性的局限性。相关代码可访问<a target="_blank" rel="noopener" href="https://github.com/oshapio/intermediate-layer-generalization%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/oshapio/intermediate-layer-generalization获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05461v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了深度分类器对于数据分布变化的敏感性，并提出了中间层分类器（ILCs）来解决这一问题。研究发现，中间层的表示在大多数情况下提供了比最终层更好的泛化能力，甚至在零样本情况下接近重新训练最终层表示的少量样本性能。这些发现对于理解信息在网络各层的分布及其在面向未知分布数据的泛化中的角色至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度分类器对训练数据的偶然相关性非常敏感，这导致了其在面对数据分布变化时的性能下降。</li>
<li>中间层分类器（ILCs）被引入以解决这一问题，它们利用网络中间层的表示进行预测。</li>
<li>中间层的表示通常提供了比最终层更好的泛化能力。</li>
<li>在某些情况下，使用早期层的零样本泛化性能接近于在最终层重新训练所需的少量样本。</li>
<li>多种数据集、架构和分布变化类型上的实验证实了这些发现。</li>
<li>中间层相对于最终层对分布变化的敏感性较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05461">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-35cdcbec9c876fb148b37ee89daff16f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55308da7bd597d4714bfee144fda0a93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7af17e34503d745724e271662ade3045.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0516c7c39b616b5c22404d42695b62ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07a0eeca3e5895a748009e4ced152bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3841c898dcf5e3efdda4e79ad4960a7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TULIP-Towards-Unified-Language-Image-Pretraining"><a href="#TULIP-Towards-Unified-Language-Image-Pretraining" class="headerlink" title="TULIP: Towards Unified Language-Image Pretraining"></a>TULIP: Towards Unified Language-Image Pretraining</h2><p><strong>Authors:Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, David M. Chan</strong></p>
<p>Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image&#x2F;text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a $2\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over $3\times$ higher scores than SigLIP on MMVP. Our code&#x2F;checkpoints are available at <a target="_blank" rel="noopener" href="https://tulip-berkeley.github.io/">https://tulip-berkeley.github.io</a> </p>
<blockquote>
<p>尽管CLIP和SigLIP等图文对比模型近期取得了成功，但这些模型在进行需要高保真图像理解的任务时，如计数、深度估计和精细目标识别等方面常常面临挑战。这些模型通过执行语言对齐，倾向于优先处理高级语义而非视觉理解，从而削弱了它们的图像理解能力。另一方面，以视觉为中心的模型在处理视觉信息方面表现出色，但在理解语言方面却遇到困难，这限制了它们在语言驱动任务中的灵活性。在这项工作中，我们引入了TULIP，它是现有CLIP类似模型的即插即用替代品。我们的方法利用生成性数据增强、增强的图像图像和文本文本对比学习以及图像&#x2F;文本重建正则化，学习精细的视觉特征，同时保留全局语义对齐。我们的方法扩展到超过10亿参数，在多个基准测试中超越了现有最先进的模型，在ImageNet-1K上建立了新的零样本性能，在RxRx1上对SigLIP进行线性探测时的准确率提高了两倍进行小样本分类；同时改进了视觉语言模型，在MMVP上达到了超过SigLIP三倍的成绩。我们的代码&#x2F;检查点可通过以下链接获取：[<a target="_blank" rel="noopener" href="https://tulip-berkeley.github.io/]">https://tulip-berkeley.github.io/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15485v2">PDF</a> (v2) Clarified fine-tuning process, updated appendix</p>
<p><strong>Summary</strong></p>
<p>本文介绍了TULIP模型，它是CLIP类模型的开源替代方案，旨在解决图像理解的问题。该模型通过利用生成式数据增强、增强的图像-图像和文本-文本对比学习以及图像&#x2F;文本重建正则化，能够在保留全局语义对齐的同时学习精细的视觉特征。此方法规模超过1B参数，并在多个基准测试中表现出超越现有最新技术（SOTA）模型的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TULIP模型是解决图像理解问题的开放源代码解决方案，针对CLIP等模型进行改进。</li>
<li>TULIP强调在保留全局语义对齐的同时学习精细的视觉特征。</li>
<li>TULIP利用生成式数据增强进行模型训练。</li>
<li>该模型采用增强的图像到图像和文本到文本的对比学习技术。</li>
<li>TULIP通过图像&#x2F;文本重建正则化进行训练。</li>
<li>TULIP模型规模超过1B参数，性能优于现有SOTA模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15485">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db91077c92830c60429a5f183c497fda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04064be4960f653e424732faaa637482.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d38f489f16deb4ab037ead38cf77ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad7431fc2668c90d23d1d96333119174.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CALF-Aligning-LLMs-for-Time-Series-Forecasting-via-Cross-modal-Fine-Tuning"><a href="#CALF-Aligning-LLMs-for-Time-Series-Forecasting-via-Cross-modal-Fine-Tuning" class="headerlink" title="CALF: Aligning LLMs for Time Series Forecasting via Cross-modal   Fine-Tuning"></a>CALF: Aligning LLMs for Time Series Forecasting via Cross-modal   Fine-Tuning</h2><p><strong>Authors:Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia</strong></p>
<p>Deep learning (e.g., Transformer) has been widely and successfully used in multivariate time series forecasting (MTSF). Unlike existing methods that focus on training models from a single modal of time series input, large language models (LLMs) based MTSF methods with cross-modal text and time series input have recently shown great superiority, especially with limited temporal data. However, current LLM-based MTSF methods usually focus on adapting and fine-tuning LLMs, while neglecting the distribution discrepancy between textual and temporal input tokens, thus leading to sub-optimal performance. To address this issue, we propose a novel Cross-Modal LLM Fine-Tuning (CALF) framework for MTSF by reducing the distribution discrepancy between textual and temporal data, which mainly consists of the temporal target branch with temporal input and the textual source branch with aligned textual input. To reduce the distribution discrepancy, we develop the cross-modal match module to first align cross-modal input distributions. Additionally, to minimize the modality distribution gap in both feature and output spaces, feature regularization loss is developed to align the intermediate features between the two branches for better weight updates, while output consistency loss is introduced to allow the output representations of both branches to correspond effectively. Thanks to the modality alignment, CALF establishes state-of-the-art performance for both long-term and short-term forecasting tasks with low computational complexity, and exhibiting favorable few-shot and zero-shot abilities similar to that in LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hank0626/LLaTA">https://github.com/Hank0626/LLaTA</a>. </p>
<blockquote>
<p>深度学习（例如Transformer）已广泛应用于多元时间序列预测（MTSF）。不同于现有方法从单一模态的时间序列输入进行模型训练，基于大型语言模型（LLM）的MTSF方法采用跨模态文本和时间序列输入，已显示出巨大的优越性，特别是在有限的时间数据情况下。然而，当前的LLM-based MTSF方法通常侧重于适应和微调LLM，而忽略了文本和时间输入标记之间的分布差异，从而导致性能不佳。为了解决这一问题，我们提出了新型的跨模态LLM微调（CALF）框架，通过减少文本和时间数据之间的分布差异，该框架主要由带有时间输入的目标分支和带有对齐文本输入的文本源分支组成。为了减少分布差异，我们开发了跨模态匹配模块来首先对齐跨模态输入分布。此外，为了最小化特征和输出空间中的模态分布差距，我们开发了特征正则化损失来对齐两个分支之间的中间特征以实现更好的权重更新，同时引入了输出一致性损失，以使两个分支的输出表示能够有效地对应。得益于模态对齐，CALF在中长期预测任务上取得了最先进的性能表现，具有较低的计算复杂度，并展现出与LLM相似的优秀的小样本和零样本能力。代码可通过 <a target="_blank" rel="noopener" href="https://github.com/Hank0626/LLaTA">https://github.com/Hank0626/LLaTA</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07300v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新颖的跨模态LLM微调（CALF）框架，用于处理多元时间序列预测问题。通过减少文本和时间数据分布差异，CALF框架包括时间目标分支和文本源分支，开发跨模态匹配模块和特征正则化损失来对齐特征空间中的模态分布差距，同时引入输出一致性损失以确保两个分支的输出表示有效对应。该框架实现了低计算复杂度和优越的预测性能，展现出强大的少样本和无样本能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习（如Transformer）在多元时间序列预测（MTSF）中广泛应用且成功。</li>
<li>大规模语言模型（LLMs）基于的MTSF方法，特别是处理有限的时间序列数据时显示出巨大优势。</li>
<li>当前LLM-based的MTSF方法主要关注LLMs的适应和微调，忽略了文本和时间输入标记的分布差异。</li>
<li>CALF框架旨在减少文本和时间数据的分布差异，包括时间目标分支和文本源分支。</li>
<li>CALF通过跨模态匹配模块和特征正则化损失来对齐不同模态的特征空间分布。</li>
<li>输出一致性损失确保两个分支的输出表示有效对应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.07300">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a1c926e66a6db6f333274dc01e2b0d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bedf514f09da9b71db22f8d5ebe847f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b1862b2225e9af82fb8fc35a70af053.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54d86a5a3dee98004f4c15ff24c6626e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e303aa4a56ae1c0bc05a06ef6e2bc2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4fe26cb16296d8b1515a196e56585d6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-07dd3402ba8be4b8214932c81b0918b1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-10  Flash Sculptor Modular 3D Worlds from Objects
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-527451acd9900a488a751ecb13eb5787.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-10  TxGemma Efficient and Agentic LLMs for Therapeutics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
