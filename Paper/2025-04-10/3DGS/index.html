<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-04-10  HiMoR Monocular Deformable Gaussian Reconstruction with Hierarchical   Motion Representation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-198e09549a4bfbad01f1cc115d977b04.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-10-更新"><a href="#2025-04-10-更新" class="headerlink" title="2025-04-10 更新"></a>2025-04-10 更新</h1><h2 id="HiMoR-Monocular-Deformable-Gaussian-Reconstruction-with-Hierarchical-Motion-Representation"><a href="#HiMoR-Monocular-Deformable-Gaussian-Reconstruction-with-Hierarchical-Motion-Representation" class="headerlink" title="HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical   Motion Representation"></a>HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical   Motion Representation</h2><p><strong>Authors:Yiming Liang, Tianhan Xu, Yuta Kikuchi</strong></p>
<p>We present Hierarchical Motion Representation (HiMoR), a novel deformation representation for 3D Gaussian primitives capable of achieving high-quality monocular dynamic 3D reconstruction. The insight behind HiMoR is that motions in everyday scenes can be decomposed into coarser motions that serve as the foundation for finer details. Using a tree structure, HiMoR’s nodes represent different levels of motion detail, with shallower nodes modeling coarse motion for temporal smoothness and deeper nodes capturing finer motion. Additionally, our model uses a few shared motion bases to represent motions of different sets of nodes, aligning with the assumption that motion tends to be smooth and simple. This motion representation design provides Gaussians with a more structured deformation, maximizing the use of temporal relationships to tackle the challenging task of monocular dynamic 3D reconstruction. We also propose using a more reliable perceptual metric as an alternative, given that pixel-level metrics for evaluating monocular dynamic 3D reconstruction can sometimes fail to accurately reflect the true quality of reconstruction. Extensive experiments demonstrate our method’s efficacy in achieving superior novel view synthesis from challenging monocular videos with complex motions. </p>
<blockquote>
<p>我们提出了分层运动表示（HiMoR），这是一种针对3D高斯原始数据的新型变形表示，能够实现高质量的单目动态3D重建。HiMoR的见解是，日常场景中的运动可以分解为较粗略的运动，这些运动作为精细细节的基础。使用树形结构，HiMoR的节点代表不同级别的运动细节，较浅的节点模拟粗糙运动以实现时间平滑，而较深的节点捕捉精细运动。此外，我们的模型使用少量共享运动基来表示不同节点集的运动，这与运动的假设相符，即运动往往是平滑和简单的。这种运动表示设计使高斯数据具有更结构化的变形，最大限度地利用时间关系来解决单目动态3D重建这一具有挑战性的任务。此外，鉴于用于评估单目动态3D重建的像素级指标有时无法准确反映重建的真实质量，我们还提出了使用更可靠的感知指标作为替代方案。大量实验表明，我们的方法在从具有复杂运动的单目视频中实现优质新颖视图合成方面非常有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06210v1">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://pfnet-research.github.io/himor">https://pfnet-research.github.io/himor</a></p>
<p><strong>Summary</strong></p>
<p>该文提出了基于分层运动表示（HiMoR）技术的三维动态重建方法，用于实现对单个视点三维模型的高质量重建。其创新点在于可将场景中的运动分解成不同级别的层次性细节运动。这种方法采用了树状结构来表示不同层次的运动细节，使用更少的共享运动基础表示不同的节点运动集合，从而实现了更为流畅和简洁的运动表现。此外，该研究还提出了一种更为可靠的感知度量方法，以替代像素级别的度量方法，更准确地对重建质量进行评估。实验结果证明了该方法在复杂运动场景下的有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种名为分层运动表示（HiMoR）的新技术，用于处理三维动态重建中的复杂运动。</li>
<li>利用树状结构表示不同层次的细节运动，通过共享运动基础实现简洁流畅的运动表现。</li>
<li>通过采用感知度量方法，更准确地评估重建质量。</li>
<li>实现优质的单视点动态三维重建技术。</li>
<li>技术能够处理复杂运动场景下的高质量重建。</li>
<li>利用时序关系进行结构化变形处理。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06210">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8f56e0c7ead2ee4354f6191f160b1541.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b52b28ed24ffc05e6ce052362c6a41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c67f9b774634331cd74bf19641efcc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="econSG-Efficient-and-Multi-view-Consistent-Open-Vocabulary-3D-Semantic-Gaussians"><a href="#econSG-Efficient-and-Multi-view-Consistent-Open-Vocabulary-3D-Semantic-Gaussians" class="headerlink" title="econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic   Gaussians"></a>econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic   Gaussians</h2><p><strong>Authors:Can Zhang, Gim Hee Lee</strong></p>
<p>The primary focus of most recent works on open-vocabulary neural fields is extracting precise semantic features from the VLMs and then consolidating them efficiently into a multi-view consistent 3D neural fields representation. However, most existing works over-trusted SAM to regularize image-level CLIP without any further refinement. Moreover, several existing works improved efficiency by dimensionality reduction of semantic features from 2D VLMs before fusing with 3DGS semantic fields, which inevitably leads to multi-view inconsistency. In this work, we propose econSG for open-vocabulary semantic segmentation with 3DGS. Our econSG consists of: 1) A Confidence-region Guided Regularization (CRR) that mutually refines SAM and CLIP to get the best of both worlds for precise semantic features with complete and precise boundaries. 2) A low dimensional contextual space to enforce 3D multi-view consistency while improving computational efficiency by fusing backprojected multi-view 2D features and follow by dimensional reduction directly on the fused 3D features instead of operating on each 2D view separately. Our econSG shows state-of-the-art performance on four benchmark datasets compared to the existing methods. Furthermore, we are also the most efficient training among all the methods. </p>
<blockquote>
<p>最近关于开放词汇神经场的主要工作集中从VLMs中提取精确语义特征，然后有效地将它们整合成多视角一致的3D神经场表示。然而，大多数现有工作过于依赖SAM来规范图像级别的CLIP，而没有进行任何进一步细化。此外，一些现有工作通过在融合到三维语义场之前对二维VLMs中的语义特征进行降维来提高效率，这不可避免地会导致多视角的不一致性。在本工作中，我们提出用于具有三维语义分割的开放词汇经济的经济SG（econSG）。我们的econSG包括：1）置信区域引导正则化（CRR），它相互优化SAM和CLIP，以获得精确语义特征的最好结果，具有完整和精确的边界。2）低维上下文空间强制实施三维多视角一致性，同时通过融合后投影的多视角二维特征来提高计算效率，然后在融合的3D特征上进行降维，而不是分别对每个二维视图进行操作。与现有方法相比，我们的econSG在四个基准数据集上表现出卓越的性能。此外，我们也是所有方法中训练效率最高的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06003v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注开放词汇神经场的研究，旨在从VLMs中提取精确语义特征，并高效地整合到多视角一致的3D神经场表示中。针对现有工作的不足，提出econSG方法，包括置信区域引导正则化（CRR）和低维上下文空间，以提高语义分割的精度和多视角一致性，同时提高计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期研究重点：开放词汇神经场主要关注从VLMs提取精确语义特征，并整合到多视角一致的3D神经场表示中。</li>
<li>现有工作问题：过度依赖SAM正则化图像级别的CLIP，缺乏进一步细化，且通过降低2D VLMs的语义特征维度来提高效率，导致多视角不一致性。</li>
<li>econSG方法提出：包括置信区域引导正则化（CRR）和低维上下文空间。</li>
<li>CRR作用：相互优化SAM和CLIP，获得更精确的语义特征，实现完整和精确的边界。</li>
<li>低维上下文空间的作用：强化3D多视角一致性，提高计算效率，通过直接在融合后的3D特征上进行降维操作，而不是分别对每个2D视图进行操作。</li>
<li>econSG性能：在四个基准数据集上相比现有方法表现出最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-198e09549a4bfbad01f1cc115d977b04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a869564fba82df82e73e7266d61da0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SE4Lip-Speech-Lip-Encoder-for-Talking-Head-Synthesis-to-Solve-Phoneme-Viseme-Alignment-Ambiguity"><a href="#SE4Lip-Speech-Lip-Encoder-for-Talking-Head-Synthesis-to-Solve-Phoneme-Viseme-Alignment-Ambiguity" class="headerlink" title="SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve   Phoneme-Viseme Alignment Ambiguity"></a>SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve   Phoneme-Viseme Alignment Ambiguity</h2><p><strong>Authors:Yihuan Huang, Jiajun Liu, Yanzhen Ren, Wuyang Liu, Juhua Tang</strong></p>
<p>Speech-driven talking head synthesis tasks commonly use general acoustic features (such as HuBERT and DeepSpeech) as guided speech features. However, we discovered that these features suffer from phoneme-viseme alignment ambiguity, which refers to the uncertainty and imprecision in matching phonemes (speech) with visemes (lip). To address this issue, we propose the Speech Encoder for Lip (SE4Lip) to encode lip features from speech directly, aligning speech and lip features in the joint embedding space by a cross-modal alignment framework. The STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve the fine-grained speech features. Experimental results show that SE4Lip achieves state-of-the-art performance in both NeRF and 3DGS rendering models. Its lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline and produces results close to the ground truth videos. </p>
<blockquote>
<p>语音驱动的人头合成任务通常使用一般的声学特征（如HuBERT和DeepSpeech）作为引导语音特征。然而，我们发现这些特征存在音素-嘴部动作对齐模糊的问题，这指的是音素（语音）与嘴部动作（嘴唇）匹配的不确定性和不精确性。为了解决这一问题，我们提出了基于语音的嘴唇编码（SE4Lip），直接从语音中编码嘴唇特征，通过跨模态对齐框架在联合嵌入空间中对齐语音和嘴唇特征。SE4Lip设计中采用了基于GRU模型的STFT频谱图，以保留精细的语音特征。实验结果表明，SE4Lip在NeRF和3DGS渲染模型中均达到了最先进的性能。其唇同步精度相较于最佳基线提高了13.7%和14.2%，并且生成的结果接近真实视频。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05803v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音驱动的头部合成任务中，使用一般声学特征作为引导语音特征时存在的问题。发现存在语音与唇动（viseme）对齐的模糊性，即语音与唇部动作匹配的不确定性和不精确性。为解决这一问题，提出了基于唇特征的Speech Encoder for Lip（SE4Lip）模型。该模型通过跨模态对齐框架，直接在语音中编码唇特征，对齐语音和唇特征在联合嵌入空间。采用基于GRU模型的STFT频谱图设计，旨在保留精细的语音特征。实验结果表明，SE4Lip在NeRF和3DGS渲染模型中达到了最新技术水平，唇同步精度分别提高了13.7%和14.2%，生成结果接近真实视频。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音驱动头部合成任务中，一般声学特征存在语音与唇部动作对齐的模糊性问题。</li>
<li>SE4Lip模型旨在解决上述问题，通过直接编码语音中的唇特征来实现更精确的对齐。</li>
<li>SE4Lip采用跨模态对齐框架，确保语音和唇特征在联合嵌入空间中的对齐。</li>
<li>模型使用基于GRU的STFT频谱图设计，以保留精细的语音特征。</li>
<li>实验结果显示SE4Lip在NeRF和3DGS渲染模型中表现优异，达到最新技术水平。</li>
<li>SE4Lip模型提高了唇同步精度，相较于最佳基线提高了13.7%和14.2%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5c3431cbbb946aa4575e671c958c7d1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f807bc1f41105ffb3275c54bb89fe57.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87997f8d772ac63cb77f55fe9ae676b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c82958e438491ba1887ff6acfafa280b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d763b980211d4de561689d8554325759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff0e35942e8a9f5380a98e60e3877841.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-229e0e83f061d033e7e9eaee3f85f86f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="L3GS-Layered-3D-Gaussian-Splats-for-Efficient-3D-Scene-Delivery"><a href="#L3GS-Layered-3D-Gaussian-Splats-for-Efficient-3D-Scene-Delivery" class="headerlink" title="L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery"></a>L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery</h2><p><strong>Authors:Yi-Zhen Tsai, Xuechen Zhang, Zheng Li, Jiasi Chen</strong></p>
<p>Traditional 3D content representations include dense point clouds that consume large amounts of data and hence network bandwidth, while newer representations such as neural radiance fields suffer from poor frame rates due to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS) can be seen as a generalization of point clouds that meet the best of both worlds, with high visual quality and efficient rendering for real-time frame rates. However, delivering 3DGS scenes from a hosting server to client devices is still challenging due to high network data consumption (e.g., 1.5 GB for a single scene). The goal of this work is to create an efficient 3D content delivery framework that allows users to view high quality 3D scenes with 3DGS as the underlying data representation. The main contributions of the paper are: (1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling algorithms to choose what splats to download at what time, and (3) Trace-driven experiments from users wearing virtual reality headsets to evaluate the visual quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS demonstrates high visual quality, achieving 16.9% higher average SSIM compared to baselines, and also works with other compressed 3DGS representations. </p>
<blockquote>
<p>传统3D内容表示方法包括消耗大量数据和网络带宽的密集点云，而较新的表示方法，如神经辐射场，由于其非标准体积渲染流程，存在帧率较低的问题。3D高斯splat（3DGS）可以被视为点云的概括，兼具高视觉质量和实时帧率的高效渲染。然而，由于网络数据消耗较高（例如单个场景高达1.5GB），从托管服务器向客户端设备传输3DGS场景仍然具有挑战性。本研究的目标是创建一个高效的3D内容传输框架，允许用户使用3DGS作为基础数据表示来查看高质量的3D场景。论文的主要贡献包括：（1）创建用于高效传输的新分层3DGS场景，（2）调度算法来选择何时下载哪些splat，（3）佩戴虚拟现实头盔的用户进行的跟踪驱动实验，以评估视觉质量和延迟。我们的分层3D高斯Splats传输系统L3GS展现了高视觉质量，与基线相比实现了平均高出16.9%的SSIM指数，并且与其他压缩的3DGS表示方法也能很好地协同工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05517v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文主要探讨如何在3D内容表示中实现高质量与高实时渲染效率的问题。针对传统密集点云需要大量数据和带宽的问题，以及新兴神经网络辐射场因非标准体积渲染管道而导致帧率较低的问题，提出使用3D高斯Splats（3DGS）作为解决方案。其兼具点云的高视觉质量和高效渲染能力。然而，从服务器向客户端设备传输3DGS场景仍面临高网络数据消耗的挑战。本研究的目标是创建一个高效的3D内容交付框架，使用户能够浏览高质量采用3DGS作为底层数据表示的3D场景。主要贡献包括：创建用于高效传输的新分层3DGS场景、设计调度算法来选择何时下载哪些Splats以及使用用户佩戴虚拟现实头盔进行的跟踪驱动实验来评估视觉质量和延迟。所提出的分层3D高斯Splats交付系统（L3GS）实现了高视觉质量，平均结构相似性指数（SSIM）比基线高出16.9%，并且与其他压缩的3DGS表示形式兼容。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>介绍了传统与新兴三维内容表示方法的挑战与不足，包括数据消耗量大和渲染效率问题。</li>
<li>提出了使用3D高斯Splats（3DGS）作为解决这些挑战的方法，结合了点云的高视觉质量和高效渲染能力。</li>
<li>针对网络数据消耗问题，提出了创建分层3DGS场景的方法以提高传输效率。</li>
<li>设计了调度算法来选择何时下载哪些Splats，以优化数据传输和用户体验。</li>
<li>通过用户佩戴虚拟现实头盔进行的跟踪驱动实验验证了系统的有效性，包括高视觉质量和低延迟方面的表现。</li>
<li>L3GS系统与其他压缩的3DGS表示形式兼容，具有良好的通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05517">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f5d53e87b96516a1de492ed2f90925f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e450bcad64be4e0580ae4e48939c98c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1613367c142cdc6c904c2784a254627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547c57d17ecb5ea1897e0791715d2b70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be6ca3870af238e58860b9eaa74dfa0b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Optimizing-4D-Gaussians-for-Dynamic-Scene-Video-from-Single-Landscape-Images"><a href="#Optimizing-4D-Gaussians-for-Dynamic-Scene-Video-from-Single-Landscape-Images" class="headerlink" title="Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape   Images"></a>Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape   Images</h2><p><strong>Authors:In-Hwan Jin, Haesoo Choo, Seong-Hun Jeong, Heemoon Park, Junghwan Kim, Oh-joon Kwon, Kyeongbo Kong</strong></p>
<p>To achieve realistic immersion in landscape images, fluids such as water and clouds need to move within the image while revealing new scenes from various camera perspectives. Recently, a field called dynamic scene video has emerged, which combines single image animation with 3D photography. These methods use pseudo 3D space, implicitly represented with Layered Depth Images (LDIs). LDIs separate a single image into depth-based layers, which enables elements like water and clouds to move within the image while revealing new scenes from different camera perspectives. However, as landscapes typically consist of continuous elements, including fluids, the representation of a 3D space separates a landscape image into discrete layers, and it can lead to diminished depth perception and potential distortions depending on camera movement. Furthermore, due to its implicit modeling of 3D space, the output may be limited to videos in the 2D domain, potentially reducing their versatility. In this paper, we propose representing a complete 3D space for dynamic scene video by modeling explicit representations, specifically 4D Gaussians, from a single image. The framework is focused on optimizing 3D Gaussians by generating multi-view images from a single image and creating 3D motion to optimize 4D Gaussians. The most important part of proposed framework is consistent 3D motion estimation, which estimates common motion among multi-view images to bring the motion in 3D space closer to actual motions. As far as we know, this is the first attempt that considers animation while representing a complete 3D space from a single landscape image. Our model demonstrates the ability to provide realistic immersion in various landscape images through diverse experiments and metrics. Extensive experimental results are <a target="_blank" rel="noopener" href="https://cvsp-lab.github.io/ICLR2025_3D-MOM/">https://cvsp-lab.github.io/ICLR2025_3D-MOM/</a>. </p>
<blockquote>
<p>为了实现景观图像的逼真沉浸感，水、云等流体需要在图像内移动，同时从不同相机视角揭示新场景。最近，出现了一个名为动态场景视频的领域，它将单图像动画与3D摄影相结合。这些方法使用伪3D空间，通过分层深度图像（LDIs）进行隐式表示。LDIs将单幅图像分离成基于深度的图层，这使得水、云等元素可以在图像内移动，同时从不同相机视角揭示新场景。然而，由于景观通常包含流体等连续元素，将3D空间表示为分层深度图像会导致景观图像分离为离散层，并根据相机移动而导致深度感知减弱和潜在失真。此外，由于其隐式建模的3D空间，输出可能仅限于二维域的视频，从而可能降低了其通用性。在本文中，我们提出了一种通过单幅图像建立显式表示的完整3D空间动态场景视频的方法。该方法使用特定的四维高斯模型进行建模。该框架侧重于通过生成多视角图像和创建三维运动来优化三维高斯模型。该框架最重要的部分是一致的三维运动估计，它估计多视角图像之间的通用运动，使三维空间中的运动更接近实际运动。据我们所知，这是首次尝试在表示单幅景观图像的完整三维空间时考虑动画制作。我们的模型通过各种实验和度量标准证明了在多种景观图像中提供逼真沉浸感的能力。详细的实验结果可通过链接<a target="_blank" rel="noopener" href="https://cvsp-lab.github.io/ICLR2025_3D-MOM%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%E3%80%82">https://cvsp-lab.github.io/ICLR2025_3D-MOM进行查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05458v1">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了通过单幅图像构建动态场景视频的新技术。该技术使用隐式表示的分层深度图像（LDIs）创建伪3D空间，实现水、云等流体在图像中的动态效果。然而，LDIs在表示连续元素如流体时存在深度感知降低和潜在失真问题。本文提出通过单幅图像构建完整的3D空间，并采用优化的显式表示法——特别是使用4D高斯模型，为动态场景视频提供更真实的沉浸感。最重要的是提出了一致的3D运动估计框架，以估计多视角图像之间的共同运动，使模拟的3D空间更接近实际运动。该模型通过广泛的实验和指标证明了其有效性。详细信息可访问：<a target="_blank" rel="noopener" href="https://cvsp-lab.github.io/ICLR2025_3D-MOM/">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>动态场景视频结合了单图像动画与3D摄影技术，旨在实现更真实的沉浸感。</li>
<li>分层深度图像（LDIs）用于创建伪3D空间，使得图像中的流体可以呈现动态效果。</li>
<li>LDIs在表示连续元素（如流体）时可能导致深度感知降低和潜在失真问题。</li>
<li>提出使用显式表示的4D高斯模型构建完整的单幅图像3D空间的方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9e6831fe068941f6e9c1b375bdde76e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-681bd0279f8959171a42252520619c77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-807f7e2c410b5a3e1a2a3256df44fac2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ActiveGS-Active-Scene-Reconstruction-Using-Gaussian-Splatting"><a href="#ActiveGS-Active-Scene-Reconstruction-Using-Gaussian-Splatting" class="headerlink" title="ActiveGS: Active Scene Reconstruction Using Gaussian Splatting"></a>ActiveGS: Active Scene Reconstruction Using Gaussian Splatting</h2><p><strong>Authors:Liren Jin, Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss, Marija Popović</strong></p>
<p>Robotics applications often rely on scene reconstructions to enable downstream tasks. In this work, we tackle the challenge of actively building an accurate map of an unknown scene using an RGB-D camera on a mobile platform. We propose a hybrid map representation that combines a Gaussian splatting map with a coarse voxel map, leveraging the strengths of both representations: the high-fidelity scene reconstruction capabilities of Gaussian splatting and the spatial modelling strengths of the voxel map. At the core of our framework is an effective confidence modelling technique for the Gaussian splatting map to identify under-reconstructed areas, while utilising spatial information from the voxel map to target unexplored areas and assist in collision-free path planning. By actively collecting scene information in under-reconstructed and unexplored areas for map updates, our approach achieves superior Gaussian splatting reconstruction results compared to state-of-the-art approaches. Additionally, we demonstrate the real-world applicability of our framework using an unmanned aerial vehicle. </p>
<blockquote>
<p>机器人应用通常依赖于场景重建来实现后续任务。在这项工作中，我们解决了在移动平台上使用RGB-D相机主动构建一个未知场景的精确地图的挑战。我们提出了一种混合地图表示方法，结合了高斯溅射地图和粗略体素地图，利用两者的优势：高斯溅射的高保真场景重建能力和体素地图的空间建模优势。我们框架的核心是对高斯溅射地图进行置信建模的有效技术，以识别重建不足的区域，同时利用体素地图的空间信息进行针对未探索区域的定位和碰撞自由路径规划。通过主动收集重建不足和未探索区域的场景信息进行地图更新，我们的方法与最新方法相比实现了优越的高斯溅射重建结果。此外，我们还使用无人机展示了该框架在现实世界中的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17769v2">PDF</a> Accepted to IEEE Robotics and Automation Letters</p>
<p><strong>Summary</strong></p>
<p>在机器人应用中，构建未知场景的精确地图是一大挑战。本研究利用移动平台上的RGB-D相机，提出了一种结合高斯溅射地图和粗略体素地图的混合地图表示方法。该方法利用高斯溅射地图的高保真场景重建能力和体素地图的空间建模优势。我们的框架核心是一种有效的信心建模技术，用于识别重建不足的区域，并利用体素地图的空间信息来定位未探索的区域，帮助实现无碰撞的路径规划。通过主动收集重建不足和未探索区域的场景信息进行地图更新，我们的方法与现有技术相比，实现了卓越的高斯溅射重建结果。同时，我们还通过无人机验证了框架的实际应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究解决了在移动平台上使用RGB-D相机构建未知场景精确地图的挑战。</li>
<li>提出了一种混合地图表示方法，结合了高斯溅射地图和体素地图的优点。</li>
<li>框架中采用了有效的信心建模技术，用于识别重建不足的区域。</li>
<li>利用体素地图的空间信息来定位未探索区域，实现无碰撞路径规划。</li>
<li>通过主动收集信息更新地图，在重建不足和未探索区域取得了卓越的高斯溅射重建结果。</li>
<li>框架的实际应用通过无人机进行了验证。</li>
<li>该方法提高了机器人应用中的场景重建能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e75560e8a4ec3b89a94a076c6c9ca5a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2623191df9f0eb73a87889d1c3d77ec6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b43085dec4fd2636bcd60e599e29f667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd2d0ae48dd430da504fb536dc7f7af.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PBR-NeRF-Inverse-Rendering-with-Physics-Based-Neural-Fields"><a href="#PBR-NeRF-Inverse-Rendering-with-Physics-Based-Neural-Fields" class="headerlink" title="PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields"></a>PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields</h2><p><strong>Authors:Sean Wu, Shamik Basu, Tim Broedermann, Luc Van Gool, Christos Sakaridis</strong></p>
<p>We tackle the ill-posed inverse rendering problem in 3D reconstruction with a Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR) theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and 3D Gaussian Splatting approaches: they estimate view-dependent appearance without modeling scene materials and illumination. To address this limitation, we present an inverse rendering (IR) model capable of jointly estimating scene geometry, materials, and illumination. Our model builds upon recent NeRF-based IR approaches, but crucially introduces two novel physics-based priors that better constrain the IR estimation. Our priors are rigorously formulated as intuitive loss terms and achieve state-of-the-art material estimation without compromising novel view synthesis quality. Our method is easily adaptable to other inverse rendering and 3D reconstruction frameworks that require material estimation. We demonstrate the importance of extending current neural rendering approaches to fully model scene properties beyond geometry and view-dependent appearance. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/s3anwu/pbrnerf">https://github.com/s3anwu/pbrnerf</a> </p>
<blockquote>
<p>我们采用基于物理渲染（PBR）理论指导的神经辐射场（NeRF）方法，解决了3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯摊铺方法的关键局限性：它们在估计视相关外观时没有对场景材质和照明进行建模。为了解决这一局限性，我们提出了一种能够联合估计场景几何、材质和照明的逆渲染（IR）模型。我们的模型建立在最近的NeRF基IR方法之上，但关键地引入了两种新型基于物理的先验知识，更好地约束了IR估计。我们的先验知识被严谨地制定为直观的损失项，在不损害新型视图合成质量的情况下实现了最先进的材料估计。我们的方法可以轻松地适应其他需要材料估计的逆渲染和3D重建框架。我们证明了将当前的神经渲染方法扩展到除几何和视相关外观以外的场景属性建模的重要性。代码公开可用：<a target="_blank" rel="noopener" href="https://github.com/s3anwu/pbrnerf">https://github.com/s3anwu/pbrnerf</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09680v2">PDF</a> CVPR 2025. 16 pages, 7 figures. Code is publicly available at   <a target="_blank" rel="noopener" href="https://github.com/s3anwu/pbrnerf">https://github.com/s3anwu/pbrnerf</a></p>
<p><strong>Summary</strong><br>     利用基于物理渲染（PBR）理论的神经辐射场（NeRF）方法，解决3D重建中的逆向渲染问题，提出名为PBR-NeRF的方法。该方法解决了大多数NeRF和3D高斯拼贴方法的核心局限：它们估计视图相关的外观，但没有对场景材质和照明进行建模。为了解决这个问题，我们提出了一种能够联合估计场景几何、材质和照明的逆向渲染（IR）模型。该模型建立在最近的NeRF基IR方法之上，但关键地引入了两个新的基于物理的先验知识，更好地约束了IR估计。我们的先验知识被严谨地制定为直观损失项，实现了材料估计的业界最佳水平，且不影响新视角合成质量。我们的方法很容易适应其他需要材料估计的逆向渲染和3D重建框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用NeRF方法结合PBR理论解决3D重建中的逆向渲染问题。</li>
<li>提出了PBR-NeRF方法，解决了大多数NeRF和3D高斯拼贴方法在估计视图相关外观时未考虑场景材质和照明的问题。</li>
<li>通过引入两个新的基于物理的先验知识，更好地约束了逆向渲染（IR）模型的估计。</li>
<li>先验知识被严谨地制定为直观损失项，实现了材料估计的业界最佳水平。</li>
<li>该方法在保证新视角合成质量的同时，实现了材料估计的改进。</li>
<li>PBR-NeRF方法容易适应其他需要材料估计的逆向渲染和3D重建框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09680">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9df76b33c50801ad0ca20f78ccd60547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f72886b7742b6dfd01f9a28bc370405d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad104be6db1292cacc292f796e66fa55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46fa2d9897308a018ca9cf332b37e807.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d763b980211d4de561689d8554325759.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-10  Meta-Continual Learning of Neural Fields
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f91c77ec3d5c4828683cc17007e6a195.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-04-10  MobilePortrait Real-Time One-Shot Neural Head Avatars on Mobile Devices
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16065k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
