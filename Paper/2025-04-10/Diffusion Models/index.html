<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Transfer between Modalities with MetaQueries">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5ed045cc4a6f90da5808658bd4410315.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-10-æ›´æ–°"><a href="#2025-04-10-æ›´æ–°" class="headerlink" title="2025-04-10 æ›´æ–°"></a>2025-04-10 æ›´æ–°</h1><h2 id="Transfer-between-Modalities-with-MetaQueries"><a href="#Transfer-between-Modalities-with-MetaQueries" class="headerlink" title="Transfer between Modalities with MetaQueries"></a>Transfer between Modalities with MetaQueries</h2><p><strong>Authors:Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, Saining Xie</strong></p>
<p>Unified multimodal models aim to integrate understanding (text output) and generation (pixel output), but aligning these different modalities within a single architecture often demands complex training recipes and careful data balancing. We introduce MetaQueries, a set of learnable queries that act as an efficient interface between autoregressive multimodal LLMs (MLLMs) and diffusion models. MetaQueries connects the MLLMâ€™s latents to the diffusion decoder, enabling knowledge-augmented image generation by leveraging the MLLMâ€™s deep understanding and reasoning capabilities. Our method simplifies training, requiring only paired image-caption data and standard diffusion objectives. Notably, this transfer is effective even when the MLLM backbone remains frozen, thereby preserving its state-of-the-art multimodal understanding capabilities while achieving strong generative performance. Additionally, our method is flexible and can be easily instruction-tuned for advanced applications such as image editing and subject-driven generation. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æ—¨åœ¨èåˆç†è§£å’Œç”Ÿæˆï¼ˆåˆ†åˆ«ä¸ºæ–‡æœ¬è¾“å‡ºå’Œåƒç´ è¾“å‡ºï¼‰ï¼Œä½†åœ¨å•ä¸€æ¶æ„å†…å¯¹é½è¿™äº›ä¸åŒæ¨¡æ€é€šå¸¸éœ€è¦å¤æ‚çš„è®­ç»ƒé…æ–¹å’Œç»†è‡´çš„æ•°æ®å¹³è¡¡ã€‚æˆ‘ä»¬å¼•å…¥äº†MetaQueriesï¼Œè¿™æ˜¯ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢ï¼Œä½œä¸ºè‡ªå›å½’å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆæ¥å£ã€‚MetaQuerieså°†MLLMçš„æ½œåœ¨ç©ºé—´è¿æ¥åˆ°æ‰©æ•£è§£ç å™¨ï¼Œé€šè¿‡åˆ©ç”¨MLLMçš„æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ç°çŸ¥è¯†å¢å¼ºçš„å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒï¼Œåªéœ€é…å¯¹å›¾åƒå­—å¹•æ•°æ®å’Œæ ‡å‡†æ‰©æ•£ç›®æ ‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨MLLMéª¨å¹²è¢«å†»ç»“çš„æƒ…å†µä¸‹ï¼Œè¿™ç§è¿ç§»ä¹Ÿæ˜¯æœ‰æ•ˆçš„ï¼Œä»è€Œä¿ç•™äº†å…¶æœ€å…ˆè¿›çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶å®ç°äº†å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çµæ´»ï¼Œå¯è½»æ¾è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œç”¨äºé«˜çº§åº”ç”¨ï¼Œå¦‚å›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06256v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://xichenpan.com/metaquery">https://xichenpan.com/metaquery</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºMetaQueriesçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢åœ¨è‡ªå›å½’å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´å»ºç«‹äº†æœ‰æ•ˆçš„æ¥å£ã€‚MetaQuerieså°†MLLMçš„æ½œåœ¨ç©ºé—´ä¸æ‰©æ•£è§£ç å™¨è¿æ¥èµ·æ¥ï¼Œå®ç°äº†çŸ¥è¯†å¢å¼ºå›¾åƒç”Ÿæˆï¼Œå……åˆ†åˆ©ç”¨äº†MLLMçš„æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œåªéœ€é…å¯¹å›¾åƒå’Œå­—å¹•æ•°æ®ä»¥åŠæ ‡å‡†æ‰©æ•£ç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çµæ´»æ€§å¼ºï¼Œæ˜“äºè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œé€‚ç”¨äºå›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆç­‰é«˜çº§åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaQueriesä½œä¸ºè‡ªå›å½’å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ¥å£ï¼Œæ—¨åœ¨æ•´åˆç†è§£å’Œç”Ÿæˆä¸¤ä¸ªæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>MetaQuerieså°†MLLMçš„æ½œåœ¨ç©ºé—´ä¸æ‰©æ•£è§£ç å™¨è¿æ¥èµ·æ¥ï¼Œå®ç°çŸ¥è¯†å¢å¼ºå›¾åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒæµç¨‹ï¼Œä»…éœ€è¦é…å¯¹å›¾åƒå’Œå­—å¹•æ•°æ®ä»¥åŠæ ‡å‡†æ‰©æ•£ç›®æ ‡ã€‚</li>
<li>MetaQueriesåœ¨ä¿æŒMLLMçš„å…ˆè¿›å¤šæ¨¡æ€ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•çµæ´»æ€§é«˜ï¼Œæ˜“äºè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆç­‰é«˜çº§åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34c0bdb455721760acdffaf8afa68fd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bd7083f5108116326a8d0d4ddb5ce14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56008693de8fb99b52c62bf6f3e4a3f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a06893fb108d01abee6e848089fed5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dc37d9fdb8b36c2945169b15509e894.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CamContextI2V-Context-aware-Controllable-Video-Generation"><a href="#CamContextI2V-Context-aware-Controllable-Video-Generation" class="headerlink" title="CamContextI2V: Context-aware Controllable Video Generation"></a>CamContextI2V: Context-aware Controllable Video Generation</h2><p><strong>Authors:Luis Denninger, Sina Mokhtarzadeh Azar, Juergen Gall</strong></p>
<p>Recently, image-to-video (I2V) diffusion models have demonstrated impressive scene understanding and generative quality, incorporating image conditions to guide generation. However, these models primarily animate static images without extending beyond their provided context. Introducing additional constraints, such as camera trajectories, can enhance diversity but often degrades visual quality, limiting their applicability for tasks requiring faithful scene representation. We propose CamContextI2V, an I2V model that integrates multiple image conditions with 3D constraints alongside camera control to enrich both global semantics and fine-grained visual details. This enables more coherent and context-aware video generation. Moreover, we motivate the necessity of temporal awareness for an effective context representation. Our comprehensive study on the RealEstate10K dataset demonstrates improvements in visual quality and camera controllability. We make our code and models publicly available at: <a target="_blank" rel="noopener" href="https://github.com/LDenninger/CamContextI2V">https://github.com/LDenninger/CamContextI2V</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰çš„æ‰©æ•£æ¨¡å‹å·²ç»è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„åœºæ™¯ç†è§£å’Œç”Ÿæˆè´¨é‡ï¼Œç»“åˆäº†å›¾åƒæ¡ä»¶æ¥æŒ‡å¯¼ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦ä½¿é™æ€å›¾åƒåŠ¨ç”»åŒ–ï¼Œè€Œæ²¡æœ‰è¶…è¶Šå…¶æä¾›çš„ä¸Šä¸‹æ–‡ã€‚å¼•å…¥é¢å¤–çš„çº¦æŸï¼Œå¦‚ç›¸æœºè½¨è¿¹ï¼Œå¯ä»¥å¢å¼ºå¤šæ ·æ€§ï¼Œä½†å¾€å¾€ä¼šé™ä½è§†è§‰è´¨é‡ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦å¿ å®åœºæ™¯è¡¨ç¤ºçš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†CamContextI2Vï¼Œè¿™æ˜¯ä¸€ç§I2Væ¨¡å‹ï¼Œå®ƒç»“åˆäº†å¤šç§å›¾åƒæ¡ä»¶ã€3Dçº¦æŸå’Œç›¸æœºæ§åˆ¶ï¼Œä»¥ä¸°å¯Œå…¨å±€è¯­ä¹‰å’Œç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚è¿™èƒ½å¤Ÿå®ç°æ›´è¿è´¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†é¢‘ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æ—¶é—´æ„ŸçŸ¥å¯¹äºæœ‰æ•ˆä¸Šä¸‹æ–‡è¡¨ç¤ºçš„å¿…è¦æ€§ã€‚æˆ‘ä»¬åœ¨RealEstate10Kæ•°æ®é›†ä¸Šçš„ç»¼åˆç ”ç©¶è¯æ˜äº†åœ¨è§†è§‰è´¨é‡å’Œç›¸æœºå¯æ§æ€§æ–¹é¢çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/LDenninger/CamContextI2V%E3%80%82">https://github.com/LDenninger/CamContextI2Vã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06022v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¨¡å‹â€”â€”CamContextI2Vã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¤šç§å›¾åƒæ¡ä»¶ã€3Dçº¦æŸä»¥åŠç›¸æœºæ§åˆ¶ï¼Œæ—¨åœ¨ä¸°å¯Œå…¨å±€è¯­ä¹‰å’Œç²¾ç»†è§†è§‰ç»†èŠ‚ï¼Œä»è€Œå®ç°æ›´è¿è´¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†é¢‘ç”Ÿæˆã€‚åŒæ—¶å¼ºè°ƒäº†æ—¶é—´æ„ŸçŸ¥å¯¹äºæœ‰æ•ˆä¸Šä¸‹æ–‡è¡¨ç¤ºçš„é‡è¦æ€§ï¼Œå¹¶åœ¨RealEstate10Kæ•°æ®é›†ä¸Šè¿›è¡Œäº†ç»¼åˆç ”ç©¶ï¼Œè¯æ˜äº†å…¶åœ¨è§†è§‰è´¨é‡å’Œç›¸æœºå¯æ§æ€§æ–¹é¢çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¨¡å‹å·²å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„åœºæ™¯ç†è§£å’Œç”Ÿæˆè´¨é‡ï¼Œèƒ½é€šè¿‡ç»“åˆå›¾åƒæ¡ä»¶æ¥æŒ‡å¯¼ç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸»è¦å¯¹é™æ€å›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œç¼ºä¹ä¸Šä¸‹æ–‡æ‰©å±•ã€‚</li>
<li>å¼•å…¥é¢å¤–çš„çº¦æŸï¼ˆå¦‚ç›¸æœºè½¨è¿¹ï¼‰å¯ä»¥å¢å¼ºå¤šæ ·æ€§ï¼Œä½†å¾€å¾€ä¼šé™ä½è§†è§‰è´¨é‡ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦å¿ å®åœºæ™¯è¡¨ç¤ºçš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºçš„CamContextI2Væ¨¡å‹ç»“åˆäº†å¤šç§å›¾åƒæ¡ä»¶ã€3Dçº¦æŸå’Œç›¸æœºæ§åˆ¶ï¼Œæ—¨åœ¨ä¸°å¯Œå…¨å±€è¯­ä¹‰å’Œç²¾ç»†è§†è§‰ç»†èŠ‚ã€‚</li>
<li>CamContextI2Væ¨¡å‹å®ç°äº†æ›´è¿è´¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†é¢‘ç”Ÿæˆã€‚</li>
<li>å¼ºè°ƒäº†æ—¶é—´æ„ŸçŸ¥å¯¹äºæœ‰æ•ˆä¸Šä¸‹æ–‡è¡¨ç¤ºçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7db237d7b1dd15e21fbd06f96291bbed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30b0fe290cede459b842232cbb17737d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-722eea116f16cd01fa8e4cde062aa1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34cc1d7790a531c8773e54b52357c612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7ce04bdc97c8085a4e9f9b5cb98a334.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities"><a href="#An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities" class="headerlink" title="An Empirical Study of GPT-4o Image Generation Capabilities"></a>An Empirical Study of GPT-4o Image Generation Capabilities</h2><p><strong>Authors:Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi</strong></p>
<p>The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4oâ€™s image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆé¢†åŸŸç»å†äº†è¿…é€Ÿçš„å‘å±•ï¼Œä»æ—©æœŸçš„åŸºäºGANçš„æ–¹æ³•å‘å±•åˆ°æ‰©æ•£æ¨¡å‹ï¼Œå†åˆ°æœ€æ–°çš„å¯»æ±‚ç†è§£å’Œç”Ÿæˆä»»åŠ¡ç»Ÿä¸€çš„ç”Ÿæˆæ¶æ„ã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå°¤å…¶æ˜¯GPT-4oï¼Œå·²ç»è¯æ˜äº†é«˜ä¿çœŸåº¦å¤šæ¨¡æ€ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä½†å…¶æ¶æ„è®¾è®¡ä»ç„¶ç¥ç§˜ä¸”æœªå…¬å¼€ã€‚è¿™å¼•å‘äº†äººä»¬å…³äºå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ˜¯å¦å·²æˆåŠŸé›†æˆåˆ°è¿™äº›æ–¹æ³•çš„ç»Ÿä¸€æ¡†æ¶ä¸­çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå¹¶å°†å…¶ä¸é¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†å››ä¸ªä¸»è¦ç±»åˆ«ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆï¼Œæ¶‰åŠè¶…è¿‡20é¡¹ä»»åŠ¡ã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†GPT-4oåœ¨å„ç§è®¾ç½®ä¸‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶å°†å…¶å®šä½åœ¨æ›´å¹¿æ³›çš„ç”Ÿæˆæ¨¡å‹æ¼”å˜ä¸­ã€‚é€šè¿‡è¿™é¡¹è°ƒæŸ¥ï¼Œæˆ‘ä»¬ä¸ºæœªæ¥çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¹¶å¼ºè°ƒäº†æ¶æ„è®¾è®¡å’Œæ•°æ®è§„æ¨¡æ‰©å¤§çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05979v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    å›¾åƒç”Ÿæˆé¢†åŸŸå·²ä»æ—©æœŸçš„GANæ–¹æ³•è¿…é€Ÿå‘å±•åˆ°æ‰©æ•£æ¨¡å‹ï¼Œæœ€è¿‘æ›´æ˜¯å‡ºç°äº†ç»Ÿä¸€ç”Ÿæˆæ¶æ„ï¼Œæ—¨åœ¨å¼¥åˆç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„é¸¿æ²Ÿã€‚GPT-4oç­‰æœ€æ–°è¿›å±•è¯æ˜äº†é«˜ä¿çœŸå¤šæ¨¡æ€ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä½†å…¶æ¶æ„è®¾è®¡ä»ç¥ç§˜æœªå…¬å¼€ã€‚æœ¬ç ”ç©¶å¯¹GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›è¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œå°†å…¶ä¸é¢†å…ˆçš„å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ¯”è¾ƒã€‚è¯„ä¼°æ¶µç›–æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒåˆ°3Då’Œå›¾åƒåˆ°Xç”Ÿæˆç­‰å››ä¸ªä¸»è¦ç±»åˆ«ï¼Œè¶…è¿‡20é¡¹ä»»åŠ¡ã€‚åˆ†æçªå‡ºäº†GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹çš„ä¼˜åŠ¿å’Œå±€é™ï¼Œå¹¶å°†å…¶ç½®äºç”Ÿæˆæ¨¡å‹æ›´å¹¿æ³›çš„æ¼”è¿›èƒŒæ™¯ä¸­ã€‚æœ¬æ¬¡è°ƒæŸ¥ä¸ºæœªæ¥çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹æŒ‡æ˜äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒäº†æ¶æ„è®¾è®¡å’Œæ•°æ®è§„æ¨¡çš„ä½œç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆé¢†åŸŸç»å†ä»GANåˆ°æ‰©æ•£æ¨¡å‹ï¼Œå†è‡³ç»Ÿä¸€ç”Ÿæˆæ¶æ„çš„æ¼”å˜ã€‚</li>
<li>GPT-4oç­‰æœ€æ–°æŠ€æœ¯å±•ç¤ºäº†é«˜ä¿çœŸå¤šæ¨¡æ€ç”Ÿæˆçš„å¯è¡Œæ€§ã€‚</li>
<li>GPT-4oçš„å›¾åƒç”Ÿæˆèƒ½åŠ›é€šè¿‡å®è¯ç ”ç©¶å¾—åˆ°äº†è¯„ä¼°ï¼Œæ¶µç›–å¤šä¸ªç±»åˆ«å’Œä»»åŠ¡ã€‚</li>
<li>GPT-4oåœ¨ä¸åŒè®¾ç½®ä¸‹å±•ç°å‡ºä¼˜åŠ¿å’Œå±€é™ã€‚</li>
<li>æ¶æ„è®¾è®¡å¯¹äºç”Ÿæˆæ¨¡å‹çš„å‘å±•è‡³å…³é‡è¦ï¼Œä½†GPT-4oçš„æ¶æ„è®¾è®¡å°šæœªå…¬å¼€ã€‚</li>
<li>ç»Ÿä¸€ç”Ÿæˆæ¶æ„çš„å‘å±•å‰æ™¯å¹¿é˜”ï¼Œå°¤å…¶æ˜¯åœ¨æ¶æ„è®¾è®¡å’Œæ•°æ®è§„æ¨¡æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ada19c845658d6086bd8f06224ef8f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d8b70545b43ecb6ffa3d082185a785b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653e211f893e469876b0aff041185c8b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Ambiguous-Image-Segmentation"><a href="#Diffusion-Based-Ambiguous-Image-Segmentation" class="headerlink" title="Diffusion Based Ambiguous Image Segmentation"></a>Diffusion Based Ambiguous Image Segmentation</h2><p><strong>Authors:Jakob LÃ¸nborg Christensen, Morten Rieger Hannemose, Anders Bjorholm Dahl, Vedrana Andersen Dahl</strong></p>
<p>Medical image segmentation often involves inherent uncertainty due to variations in expert annotations. Capturing this uncertainty is an important goal and previous works have used various generative image models for the purpose of representing the full distribution of plausible expert ground truths. In this work, we explore the design space of diffusion models for generative segmentation, investigating the impact of noise schedules, prediction types, and loss weightings. Notably, we find that making the noise schedule harder with input scaling significantly improves performance. We conclude that x- and v-prediction outperform epsilon-prediction, likely because the diffusion process is in the discrete segmentation domain. Many loss weightings achieve similar performance as long as they give enough weight to the end of the diffusion process. We base our experiments on the LIDC-IDRI lung lesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we introduce a randomly cropped variant of the LIDC-IDRI dataset that is better suited for uncertainty in image segmentation. Our model also achieves SOTA in this harder setting. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¸¸å¸¸ç”±äºä¸“å®¶æ ‡æ³¨çš„å˜åŒ–è€Œæ¶‰åŠå›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚æ•æ‰è¿™ç§ä¸ç¡®å®šæ€§æ˜¯ä¸€ä¸ªé‡è¦ç›®æ ‡ï¼Œä¹‹å‰çš„å·¥ä½œå·²ç»ä½¿ç”¨å„ç§ç”Ÿæˆå›¾åƒæ¨¡å‹æ¥ä»£è¡¨ä¸“å®¶çœŸå®å€¼çš„å®Œæ•´åˆ†å¸ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç”Ÿæˆåˆ†å‰²æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼Œç ”ç©¶äº†å™ªå£°æ—¶é—´è¡¨ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡çš„å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡è¾“å…¥ç¼©æ”¾ä½¿å™ªå£°æ—¶é—´è¡¨æ›´åŠ å›°éš¾å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œxé¢„æµ‹å’Œvé¢„æµ‹ä¼˜äºÎµé¢„æµ‹ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæ‰©æ•£è¿‡ç¨‹å¤„äºç¦»æ•£åˆ†å‰²é¢†åŸŸã€‚è®¸å¤šæŸå¤±æƒé‡åªè¦å¯¹æ‰©æ•£è¿‡ç¨‹çš„ç»“æŸç»™äºˆè¶³å¤Ÿçš„é‡è§†ï¼Œå°±èƒ½è¾¾åˆ°ç±»ä¼¼çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„å®éªŒåŸºäºLIDC-IDRIè‚ºç—…å˜æ•°æ®é›†ï¼Œå¹¶è·å¾—äº†æœ€æ–°ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†LIDC-IDRIæ•°æ®é›†çš„éšæœºè£å‰ªç‰ˆæœ¬ï¼Œæ›´é€‚åˆäºå›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™ä¸ªæ›´å›°éš¾çš„è®¾ç½®ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05977v1">PDF</a> Accepted at SCIA25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œç”Ÿæˆæ€§å›¾åƒåˆ†å‰²çš„è®¾è®¡ç©ºé—´ï¼Œç ”ç©¶äº†å™ªå£°è°ƒåº¦ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è¾“å…¥ç¼©æ”¾ä½¿å™ªå£°è°ƒåº¦æ›´åŠ å›°éš¾å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚x-é¢„æµ‹å’Œv-é¢„æµ‹ä¼˜äºepsiloné¢„æµ‹ï¼Œå› ä¸ºæ‰©æ•£è¿‡ç¨‹å¤„äºç¦»æ•£åˆ†å‰²é¢†åŸŸã€‚åŸºäºLIDC-IDRIè‚ºç—…ç¶æ•°æ®é›†çš„å®éªŒè·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§éšæœºè£å‰ªçš„LIDC-IDRIæ•°æ®é›†å˜ä½“ï¼Œæ›´é€‚åˆäºå›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§ã€‚æ¨¡å‹åœ¨æ­¤æ›´å›°éš¾çš„ç¯å¢ƒä¸­åŒæ ·è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ€§å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œç ”ç©¶äº†å™ªå£°è°ƒåº¦ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>é€šè¿‡è¾“å…¥ç¼©æ”¾ä½¿å™ªå£°è°ƒåº¦æ›´åŠ å›°éš¾ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>x-é¢„æµ‹å’Œv-é¢„æµ‹åœ¨ç¦»æ•£åˆ†å‰²é¢†åŸŸçš„æ‰©æ•£è¿‡ç¨‹ä¸­è¡¨ç°ä¼˜äºepsiloné¢„æµ‹ã€‚</li>
<li>å¤šç§æŸå¤±æƒé‡åœ¨ç»™äºˆè¶³å¤Ÿé‡è§†äºæ‰©æ•£è¿‡ç¨‹ç»“æŸæ—¶éƒ½èƒ½è·å¾—ç›¸ä¼¼æ€§èƒ½ã€‚</li>
<li>å®éªŒåŸºäºLIDC-IDRIè‚ºç—…ç¶æ•°æ®é›†ï¼Œè·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é€‚åˆä¸ç¡®å®šæ€§ç ”ç©¶çš„LIDC-IDRIæ•°æ®é›†å˜ä½“ï¼Œé€šè¿‡éšæœºè£å‰ªå®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6170fc41063aa1b1104c59a8d656712b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6de7ffaa87d20e553c841ce75a2cb731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4512d984a41c79504eec109e680c6028.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Physics-aware-generative-models-for-turbulent-fluid-flows-through-energy-consistent-stochastic-interpolants"><a href="#Physics-aware-generative-models-for-turbulent-fluid-flows-through-energy-consistent-stochastic-interpolants" class="headerlink" title="Physics-aware generative models for turbulent fluid flows through   energy-consistent stochastic interpolants"></a>Physics-aware generative models for turbulent fluid flows through   energy-consistent stochastic interpolants</h2><p><strong>Authors:Nikolaj T. MÃ¼cke, Benjamin Sanderse</strong></p>
<p>Generative models have demonstrated remarkable success in domains such as text, image, and video synthesis. In this work, we explore the application of generative models to fluid dynamics, specifically for turbulence simulation, where classical numerical solvers are computationally expensive. We propose a novel stochastic generative model based on stochastic interpolants, which enables probabilistic forecasting while incorporating physical constraints such as energy stability and divergence-freeness. Unlike conventional stochastic generative models, which are often agnostic to underlying physical laws, our approach embeds energy consistency by making the parameters of the stochastic interpolant learnable coefficients. We evaluate our method on a benchmark turbulence problem - Kolmogorov flow - demonstrating superior accuracy and stability over state-of-the-art alternatives such as autoregressive conditional diffusion models (ACDMs) and PDE-Refiner. Furthermore, we achieve stable results for significantly longer roll-outs than standard stochastic interpolants. Our results highlight the potential of physics-aware generative models in accelerating and enhancing turbulence simulations while preserving fundamental conservation properties. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘åˆæˆç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°†ç”Ÿæˆæ¨¡å‹åº”ç”¨äºæµä½“åŠ¨åŠ›å­¦ï¼Œç‰¹åˆ«æ˜¯æ¹æµæ¨¡æ‹Ÿçš„åº”ç”¨ï¼Œå› ä¸ºä¼ ç»Ÿçš„æ•°å€¼æ±‚è§£å™¨åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºæ’å€¼çš„æ–°å‹éšæœºç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ¦‚ç‡é¢„æµ‹ï¼ŒåŒæ—¶ç»“åˆäº†èƒ½é‡ç¨³å®šæ€§å’Œæ— æ•£åº¦ç­‰ç‰©ç†çº¦æŸã€‚ä¸é€šå¸¸å¿½è§†åŸºæœ¬ç‰©ç†å®šå¾‹çš„ä¼ ç»Ÿéšæœºç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä½¿éšæœºæ’å€¼çš„å‚æ•°æˆä¸ºå¯å­¦ä¹ çš„ç³»æ•°æ¥åµŒå…¥èƒ½é‡ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æ¹æµé—®é¢˜â€”â€”æŸ¯å°”è«å“¥æ´›å¤«æµä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸æœ€å…ˆè¿›çš„æ›¿ä»£å“ï¼ˆå¦‚è‡ªå›å½’æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆACDMï¼‰å’ŒPDEç²¾ç‚¼å™¨ï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå¯¹äºæ¯”æ ‡å‡†éšæœºæ’å€¼æ›´é•¿çš„æ»šåŠ¨é¢„æµ‹ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†ç¨³å®šçš„ç»“æœã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹åœ¨åŠ é€Ÿå’Œæ”¹è¿›æ¹æµæ¨¡æ‹Ÿæ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶åœ¨ä¿ç•™åŸºæœ¬å®ˆæ’æ€§è´¨æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹åœ¨æµä½“åŠ¨åŠ›å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ç”¨äºæ˜‚è´µçš„ç»å…¸æ•°å€¼æ±‚è§£å™¨çš„æ¹æµæ¨¡æ‹Ÿã€‚æå‡ºä¸€ç§åŸºäºéšæœºæ’å€¼çš„æ–°å‹éšæœºç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ¦‚ç‡é¢„æµ‹çš„åŒæ—¶èå…¥èƒ½é‡ç¨³å®šæ€§å’Œæ— æ•£åº¦çš„ç‰©ç†çº¦æŸã€‚ä¸ä¼ ç»Ÿå¿½ç•¥ç‰©ç†å®šå¾‹çš„éšæœºç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡ä½¿éšæœºæ’å€¼çš„å‚æ•°æˆä¸ºå¯å­¦ä¹ ç³»æ•°æ¥åµŒå…¥èƒ½é‡ä¸€è‡´æ€§ã€‚åœ¨Kolmogorovæµç­‰åŸºå‡†æ¹æµé—®é¢˜ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜å…¶åœ¨ç²¾åº¦å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºè‡ªå›å½’æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆACDMï¼‰å’ŒPDEç²¾ç‚¼å™¨ç­‰å½“å‰ä¸»æµæ–¹æ³•ï¼Œä¸”å®ç°äº†æ›´é•¿çš„æ»šåŠ¨é¢„æµ‹ç»“æœã€‚ç»“æœçªæ˜¾äº†ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹åœ¨åŠ é€Ÿå’Œæ”¹è¿›æ¹æµæ¨¡æ‹Ÿæ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¿æŒåŸºæœ¬çš„å®ˆæ’å±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨æµä½“åŠ¨åŠ›å­¦é¢†åŸŸçš„åº”ç”¨è¢«æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¹æµæ¨¡æ‹Ÿæ–¹é¢ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹éšæœºç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäºéšæœºæ’å€¼è¿›è¡Œæ¦‚ç‡é¢„æµ‹ï¼Œå¹¶èå…¥ç‰©ç†çº¦æŸã€‚</li>
<li>ä¸ä¼ ç»Ÿéšæœºç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•åµŒå…¥èƒ½é‡ä¸€è‡´æ€§ï¼Œé€šè¿‡ä½¿éšæœºæ’å€¼çš„å‚æ•°æˆä¸ºå¯å­¦ä¹ ç³»æ•°ã€‚</li>
<li>åœ¨åŸºå‡†æ¹æµé—®é¢˜ä¸Šè¯„ä¼°ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨ç²¾åº¦å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚</li>
<li>å®ç°æ›´é•¿çš„æ»šåŠ¨é¢„æµ‹ç»“æœã€‚</li>
<li>ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹åœ¨åŠ é€Ÿå’Œæ”¹è¿›æ¹æµæ¨¡æ‹Ÿæ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04e0933bce5f668bf4cc562fc6599a35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39483f606c9785ca5536bd9ba0bf2065.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mind-the-Trojan-Horse-Image-Prompt-Adapter-Enabling-Scalable-and-Deceptive-Jailbreaking"><a href="#Mind-the-Trojan-Horse-Image-Prompt-Adapter-Enabling-Scalable-and-Deceptive-Jailbreaking" class="headerlink" title="Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and   Deceptive Jailbreaking"></a>Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and   Deceptive Jailbreaking</h2><p><strong>Authors:Junxi Chen, Junhao Dong, Xiaohua Xie</strong></p>
<p>Recently, the Image Prompt Adapter (IP-Adapter) has been increasingly integrated into text-to-image diffusion models (T2I-DMs) to improve controllability. However, in this paper, we reveal that T2I-DMs equipped with the IP-Adapter (T2I-IP-DMs) enable a new jailbreak attack named the hijacking attack. We demonstrate that, by uploading imperceptible image-space adversarial examples (AEs), the adversary can hijack massive benign users to jailbreak an Image Generation Service (IGS) driven by T2I-IP-DMs and mislead the public to discredit the service provider. Worse still, the IP-Adapterâ€™s dependency on open-source image encoders reduces the knowledge required to craft AEs. Extensive experiments verify the technical feasibility of the hijacking attack. In light of the revealed threat, we investigate several existing defenses and explore combining the IP-Adapter with adversarially trained models to overcome existing defensesâ€™ limitations. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fhdnskfbeuv/attackIPA">https://github.com/fhdnskfbeuv/attackIPA</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒImage Prompt Adapterï¼ˆIP-Adapterï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«é›†æˆåˆ°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆT2I-DMsï¼‰ä¸­ï¼Œä»¥æé«˜å¯æ§æ€§ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ­ç¤ºäº†é…å¤‡IP-Adapterçš„T2I-DMsï¼ˆT2I-IP-DMsï¼‰èƒ½å¤Ÿå®æ–½ä¸€ç§åä¸ºåŠ«æŒæ”»å‡»çš„æ–°å‹è¶Šç‹±æ”»å‡»ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡ä¸Šä¼ å‡ ä¹å¯Ÿè§‰ä¸åˆ°çš„å›¾åƒç©ºé—´å¯¹æŠ—æ ·æœ¬ï¼ˆAEsï¼‰ï¼Œæ”»å‡»è€…å¯ä»¥åŠ«æŒå¤§é‡è‰¯æ€§ç”¨æˆ·æ¥è¶Šç‹±ç”±T2I-IP-DMsé©±åŠ¨çš„å›¾åƒç”ŸæˆæœåŠ¡ï¼ˆIGSï¼‰ï¼Œå¹¶è¯¯å¯¼å…¬ä¼—è´¨ç–‘æœåŠ¡æä¾›å•†ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼ŒIP-Adapterå¯¹å¼€æºå›¾åƒç¼–ç å™¨çš„ä¾èµ–é™ä½äº†åˆ¶ä½œAEsæ‰€éœ€çš„çŸ¥è¯†ã€‚å¤§é‡å®éªŒéªŒè¯äº†åŠ«æŒæ”»å‡»çš„æŠ€æœ¯å¯è¡Œæ€§ã€‚é‰´äºæ‰€æ­ç¤ºçš„å¨èƒï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å‡ ç§ç°æœ‰çš„é˜²å¾¡æªæ–½ï¼Œå¹¶æ¢ç´¢å°†IP-Adapterä¸å¯¹æŠ—è®­ç»ƒæ¨¡å‹ç›¸ç»“åˆï¼Œä»¥å…‹æœç°æœ‰é˜²å¾¡æªæ–½çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fhdnskfbeuv/attackIPA">https://github.com/fhdnskfbeuv/attackIPA</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05838v1">PDF</a> Accepted by CVPR2025 as Highlight</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†ç ”ç©¶è€…æ­ç¤ºäº†é…å¤‡å›¾åƒæç¤ºé€‚é…å™¨ï¼ˆIP-Adapterï¼‰çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆT2I-DMsï¼‰å­˜åœ¨ä¸€ç§åä¸ºåŠ«æŒæ”»å‡»çš„æ–°æ¼æ´ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡ä¸Šä¼ å‡ ä¹æ— æ³•å¯Ÿè§‰çš„å›¾åƒç©ºé—´å¯¹æŠ—æ ·æœ¬ï¼ˆAEsï¼‰æ¥åŠ«æŒå¤§é‡è‰¯æ€§ç”¨æˆ·ï¼Œä½¿å›¾åƒç”ŸæˆæœåŠ¡ï¼ˆIGSï¼‰å—åˆ°ç ´åå¹¶è¯¯å¯¼å…¬ä¼—å¯¹æœåŠ¡æä¾›å•†çš„ä¿¡ä»»ã€‚æ­¤å¤–ï¼ŒIP-Adapterå¯¹å¼€æºå›¾åƒç¼–ç å™¨çš„ä¾èµ–é™ä½äº†åˆ¶ä½œAEsæ‰€éœ€çš„çŸ¥è¯†ã€‚æœ¬æ–‡åŒæ—¶æ¢è®¨äº†ç°æœ‰é˜²å¾¡æªæ–½çš„ç»“åˆä»¥åŠå¯¹æŠ—è®­ç»ƒæ¨¡å‹åœ¨å…‹æœç°æœ‰é˜²å¾¡å±€é™æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IP-Adapteråœ¨T2I-DMsä¸­çš„é›†æˆæé«˜äº†å¯æ§æ€§ï¼Œä½†ä¹Ÿå¼•å…¥äº†æ–°çš„å®‰å…¨éšæ‚£ã€‚</li>
<li>å­˜åœ¨ä¸€ç§åä¸ºåŠ«æŒæ”»å‡»çš„æ–°æ¼æ´ï¼Œæ”»å‡»è€…å¯åˆ©ç”¨IP-Adapterçš„ç‰¹ç‚¹æ¥ç ´åIGSå¹¶è¯¯å¯¼å…¬ä¼—ã€‚</li>
<li>æ”»å‡»è€…é€šè¿‡ä¸Šä¼ å‡ ä¹æ— æ³•å¯Ÿè§‰çš„AEsæ¥å®ç°åŠ«æŒæ”»å‡»ï¼Œä¸”IP-Adapterå¯¹å¼€æºå›¾åƒç¼–ç å™¨çš„ä¾èµ–ç®€åŒ–äº†è¿™ä¸€è¿‡ç¨‹ã€‚</li>
<li>å¹¿æ³›å®éªŒéªŒè¯äº†åŠ«æŒæ”»å‡»çš„æŠ€æœ¯å¯è¡Œæ€§ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æªæ–½å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ¢ç´¢æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç»“åˆIP-Adapterä¸å¯¹æŠ—è®­ç»ƒæ¨¡å‹å¯å…‹æœç°æœ‰é˜²å¾¡çš„å±€é™ï¼Œæé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e990254a2444b3bf2ab85006f6fabb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6ecc795417304d9f89c0cd04a87072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d50ec19a1af94c2477c1399baa28048.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4782ad5feb5783ea12c1da87ee1eae9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-940040f32e0c3a3eaec47e27175f56b4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Parasite-A-Steganography-based-Backdoor-Attack-Framework-for-Diffusion-Models"><a href="#Parasite-A-Steganography-based-Backdoor-Attack-Framework-for-Diffusion-Models" class="headerlink" title="Parasite: A Steganography-based Backdoor Attack Framework for Diffusion   Models"></a>Parasite: A Steganography-based Backdoor Attack Framework for Diffusion   Models</h2><p><strong>Authors:Jiahao Chen, Yu Pan, Yi Du, Chunkai Wu, Lin Wang</strong></p>
<p>Recently, the diffusion model has gained significant attention as one of the most successful image generation models, which can generate high-quality images by iteratively sampling noise. However, recent studies have shown that diffusion models are vulnerable to backdoor attacks, allowing attackers to enter input data containing triggers to activate the backdoor and generate their desired output. Existing backdoor attack methods primarily focused on target noise-to-image and text-to-image tasks, with limited work on backdoor attacks in image-to-image tasks. Furthermore, traditional backdoor attacks often rely on a single, conspicuous trigger to generate a fixed target image, lacking concealability and flexibility. To address these limitations, we propose a novel backdoor attack method called â€œParasiteâ€ for image-to-image tasks in diffusion models, which not only is the first to leverage steganography for triggers hiding, but also allows attackers to embed the target content as a backdoor trigger to achieve a more flexible attack. â€œParasiteâ€ as a novel attack method effectively bypasses existing detection frameworks to execute backdoor attacks. In our experiments, â€œParasiteâ€ achieved a 0 percent backdoor detection rate against the mainstream defense frameworks. In addition, in the ablation study, we discuss the influence of different hiding coefficients on the attack results. You can find our code at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Parasite-1715/">https://anonymous.4open.science/r/Parasite-1715/</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºæœ€æˆåŠŸçš„å›¾åƒç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ï¼Œå› å…¶èƒ½å¤Ÿé€šè¿‡è¿­ä»£é‡‡æ ·å™ªå£°ç”Ÿæˆé«˜è´¨é‡å›¾åƒè€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡è¾“å…¥åŒ…å«è§¦å‘å™¨çš„æ•°æ®æ¥æ¿€æ´»åé—¨ï¼Œå¹¶ç”Ÿæˆä»–ä»¬æƒ³è¦çš„è¾“å‡ºã€‚ç°æœ‰çš„åé—¨æ”»å‡»æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç›®æ ‡å™ªå£°åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸Šï¼Œè€Œå¯¹äºå›¾åƒåˆ°å›¾åƒä»»åŠ¡çš„åé—¨æ”»å‡»ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„åé—¨æ”»å‡»é€šå¸¸ä¾èµ–äºä¸€ä¸ªå•ä¸€ã€æ˜¾çœ¼çš„è§¦å‘å™¨æ¥ç”Ÿæˆå›ºå®šçš„ç›®æ ‡å›¾åƒï¼Œç¼ºä¹éšè”½æ€§å’Œçµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬é’ˆå¯¹æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒåˆ°å›¾åƒä»»åŠ¡æå‡ºäº†ä¸€ç§æ–°çš„åé—¨æ”»å‡»æ–¹æ³•ï¼Œåä¸ºâ€œå¯„ç”Ÿè™«â€ã€‚å®ƒä¸ä»…é¦–æ¬¡åˆ©ç”¨éšå†™æœ¯æ¥éšè—è§¦å‘å™¨ï¼Œè¿˜å…è®¸æ”»å‡»è€…å°†ç›®æ ‡å†…å®¹ä½œä¸ºåé—¨è§¦å‘å™¨ï¼Œä»¥å®ç°æ›´çµæ´»çš„æ”»å‡»ã€‚â€œå¯„ç”Ÿè™«â€ä½œä¸ºä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ç»•è¿‡äº†ç°æœ‰çš„æ£€æµ‹æ¡†æ¶æ¥æ‰§è¡Œåé—¨æ”»å‡»ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œâ€œå¯„ç”Ÿè™«â€é’ˆå¯¹ä¸»æµé˜²å¾¡æ¡†æ¶å®ç°äº†0%çš„åé—¨æ£€æµ‹ç‡ã€‚å¦å¤–ï¼Œåœ¨æ¶ˆèç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸åŒçš„éšè—ç³»æ•°å¯¹æ”»å‡»ç»“æœçš„å½±å“ã€‚ä½ å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Parasite-1715/%E6%89%BE%E5%88%B0%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://anonymous.4open.science/r/Parasite-1715/æ‰¾åˆ°æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05815v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹æœ€è¿‘æˆä¸ºæœ€æˆåŠŸçš„å›¾åƒç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ï¼Œä½†å…¶æ˜“å—åé—¨æ”»å‡»å½±å“ã€‚é’ˆå¯¹å›¾åƒåˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œå¯„ç”Ÿè™«â€çš„æ–°å‹åé—¨æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸ä»…åˆ©ç”¨éšå†™æœ¯éšè—è§¦å‘å™¨ï¼Œè¿˜å…è®¸æ”»å‡»è€…åµŒå…¥ç›®æ ‡å†…å®¹ä½œä¸ºåé—¨è§¦å‘å™¨ï¼Œä»¥å®ç°æ›´çµæ´»çš„æ”»å‡»ã€‚è¯¥æ”»å‡»æ–¹æ³•æˆåŠŸç»•è¿‡ç°æœ‰æ£€æµ‹æ¡†æ¶ï¼Œå¯¹ä¸»æµé˜²å¾¡æ¡†æ¶çš„åé—¨æ£€æµ‹ç‡ä¸ºé›¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ˜¯å½“ä¸‹æœ€æˆåŠŸçš„å›¾åƒç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ï¼Œä½†å­˜åœ¨åé—¨æ”»å‡»é£é™©ã€‚</li>
<li>ç°æœ‰åé—¨æ”»å‡»ä¸»è¦é›†ä¸­åœ¨å™ªå£°åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸Šï¼Œé’ˆå¯¹å›¾åƒåˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„æ”»å‡»ç ”ç©¶æœ‰é™ã€‚</li>
<li>æå‡ºçš„â€œå¯„ç”Ÿè™«â€æ”»å‡»æ–¹æ³•ç”¨äºå›¾åƒåˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä»»åŠ¡ï¼Œåˆ©ç”¨éšå†™æœ¯éšè—è§¦å‘å™¨ã€‚</li>
<li>â€œå¯„ç”Ÿè™«â€æ”»å‡»æ–¹æ³•å…è®¸æ”»å‡»è€…åµŒå…¥ç›®æ ‡å†…å®¹ä½œä¸ºåé—¨è§¦å‘å™¨ï¼Œå®ç°æ›´çµæ´»æ”»å‡»ã€‚</li>
<li>â€œå¯„ç”Ÿè™«â€æ”»å‡»æˆåŠŸç»•è¿‡ç°æœ‰æ£€æµ‹æ¡†æ¶ï¼Œå¯¹ä¸»æµé˜²å¾¡æ¡†æ¶çš„åé—¨æ£€æµ‹ç‡ä¸ºé›¶ã€‚</li>
<li>å®éªŒä¸­å‘ç°ä¸åŒéšè—ç³»æ•°å¯¹æ”»å‡»ç»“æœæœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2eb101844ea46edfa2438a7ca747351c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ae637df4304771c9504f2919de877e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6891beaea15d66ac824b47f05fab8a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4de6843efb139ec72c9873507f763d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-985f77d7fec686862fe6ea05cb52ef99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bbeb808d561c36154332b982ea2503e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Storybooth-Training-free-Multi-Subject-Consistency-for-Improved-Visual-Storytelling"><a href="#Storybooth-Training-free-Multi-Subject-Consistency-for-Improved-Visual-Storytelling" class="headerlink" title="Storybooth: Training-free Multi-Subject Consistency for Improved Visual   Storytelling"></a>Storybooth: Training-free Multi-Subject Consistency for Improved Visual   Storytelling</h2><p><strong>Authors:Jaskirat Singh, Junshen Kevin Chen, Jonas Kohler, Michael Cohen</strong></p>
<p>Training-free consistent text-to-image generation depicting the same subjects across different images is a topic of widespread recent interest. Existing works in this direction predominantly rely on cross-frame self-attention; which improves subject-consistency by allowing tokens in each frame to pay attention to tokens in other frames during self-attention computation. While useful for single subjects, we find that it struggles when scaling to multiple characters. In this work, we first analyze the reason for these limitations. Our exploration reveals that the primary-issue stems from self-attention-leakage, which is exacerbated when trying to ensure consistency across multiple-characters. This happens when tokens from one subject pay attention to other characters, causing them to appear like each other (e.g., a dog appearing like a duck). Motivated by these findings, we propose StoryBooth: a training-free approach for improving multi-character consistency. In particular, we first leverage multi-modal chain-of-thought reasoning and region-based generation to apriori localize the different subjects across the desired story outputs. The final outputs are then generated using a modified diffusion model which consists of two novel layers: 1) a bounded cross-frame self-attention layer for reducing inter-character attention leakage, and 2) token-merging layer for improving consistency of fine-grain subject details. Through both qualitative and quantitative results we find that the proposed approach surpasses prior state-of-the-art, exhibiting improved consistency across both multiple-characters and fine-grain subject details. </p>
<blockquote>
<p>æ— è®­ç»ƒä¸€è‡´æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ˜¯è¿‘æœŸå¹¿æ³›å…³æ³¨çš„ä¸»é¢˜ï¼Œè¯¥ç”Ÿæˆæ–¹æ³•æ—¨åœ¨æç»˜ä¸åŒå›¾åƒä¸­çš„ç›¸åŒä¸»é¢˜ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºè·¨å¸§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å…è®¸æ¯ä¸ªå¸§ä¸­çš„æ ‡è®°å…³æ³¨å…¶ä»–å¸§ä¸­çš„æ ‡è®°ï¼Œä»è€Œæé«˜ä¸»é¢˜ä¸€è‡´æ€§ã€‚è™½ç„¶è¿™åœ¨å•ä¸ªä¸»é¢˜ä¸Šå¾ˆæœ‰ç”¨ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒåœ¨æ‰©å±•åˆ°å¤šä¸ªè§’è‰²æ—¶é‡åˆ°äº†å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æäº†è¿™äº›é™åˆ¶çš„åŸå› ã€‚æˆ‘ä»¬çš„æ¢ç´¢è¡¨æ˜ï¼Œä¸»è¦é—®é¢˜æºäºè‡ªæ³¨æ„åŠ›æ³„æ¼ï¼Œåœ¨å°è¯•ç¡®ä¿è·¨å¤šä¸ªè§’è‰²çš„ä¸€è‡´æ€§æ—¶ï¼Œè¿™ä¸€é—®é¢˜ä¼šåŠ å‰§ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨æ¥è‡ªä¸€ä¸ªä¸»é¢˜çš„æ ‡è®°å…³æ³¨å…¶ä»–è§’è‰²æ—¶ï¼Œå¯¼è‡´ä»–ä»¬çœ‹èµ·æ¥å½¼æ­¤ç›¸ä¼¼ï¼ˆä¾‹å¦‚ï¼Œä¸€åªç‹—çœ‹èµ·æ¥åƒé¸­å­ï¼‰ã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†StoryBoothï¼šä¸€ç§æ— éœ€è®­ç»ƒå³å¯æé«˜å¤šè§’è‰²ä¸€è‡´æ€§çš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€é“¾å¼æ€ç»´æ¨ç†å’ŒåŸºäºåŒºåŸŸçš„ç”Ÿæˆæ¥å…ˆéªŒå®šä½æ‰€éœ€æ•…äº‹è¾“å‡ºä¸­çš„ä¸åŒä¸»é¢˜ã€‚æœ€ç»ˆè¾“å‡ºæ˜¯ä½¿ç”¨ç»è¿‡ä¿®æ”¹çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ï¼Œè¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªæ–°å±‚ï¼š1ï¼‰æœ‰ç•Œè·¨å¸§è‡ªæ³¨æ„åŠ›å±‚ï¼Œç”¨äºå‡å°‘è§’è‰²é—´æ³¨æ„åŠ›æ³„æ¼ï¼›ä»¥åŠ2ï¼‰æ ‡è®°åˆå¹¶å±‚ï¼Œç”¨äºæé«˜ç²¾ç»†è§’è‰²ç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡ç»“æœï¼Œæˆ‘ä»¬å‘ç°æ‰€æå‡ºçš„æ–¹æ³•è¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œåœ¨å¤šä¸ªè§’è‰²å’Œç²¾ç»†è§’è‰²ç»†èŠ‚ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ— è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æç»˜ä¸åŒå›¾åƒä¸­çš„åŒä¸€ä¸»é¢˜æ—¶ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è·¨å¸§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†åœ¨æ‰©å±•åˆ°å¤šè§’è‰²æ—¶å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡åˆ†æäº†è¿™ä¸€é—®é¢˜äº§ç”Ÿçš„åŸå› ï¼Œå¹¶æå‡ºäº†StoryBoothæ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€é“¾å¼æ€ç»´æ¨ç†å’ŒåŸºäºåŒºåŸŸçš„ç”Ÿæˆæ¥å…ˆå®šä½ä¸åŒä¸»é¢˜ï¼Œç„¶åä½¿ç”¨æ”¹è¿›åçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ€ç»ˆè¾“å‡ºï¼ŒåŒ…æ‹¬å‡å°‘è§’è‰²é—´æ³¨æ„åŠ›æ³„æ¼çš„è·¨å¸§è‡ªæ³¨æ„åŠ›å±‚å’Œç”¨äºæ”¹è¿›ç²¾ç»†ä¸»é¢˜ç»†èŠ‚ä¸€è‡´æ€§çš„ä»¤ç‰Œåˆå¹¶å±‚ã€‚æ–°æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡ç»“æœä¸Šéƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†å¤šè§’è‰²å’Œç²¾ç»†ä¸»é¢˜ç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯åœ¨æç»˜ä¸åŒå›¾åƒä¸­çš„åŒä¸€ä¸»é¢˜æ—¶å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è·¨å¸§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†åœ¨æ‰©å±•åˆ°å¤šè§’è‰²æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è·¨å¸§è‡ªæ³¨æ„åŠ›æ³„æ¼æ˜¯é™åˆ¶å¤šè§’è‰²ä¸€è‡´æ€§çš„ä¸»è¦åŸå› ã€‚</li>
<li>StoryBoothæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€é“¾å¼æ€ç»´æ¨ç†å’ŒåŸºäºåŒºåŸŸçš„ç”Ÿæˆæ¥å®šä½ä¸åŒä¸»é¢˜ã€‚</li>
<li>æ”¹è¿›åçš„æ‰©æ•£æ¨¡å‹åŒ…æ‹¬å‡å°‘è§’è‰²é—´æ³¨æ„åŠ›æ³„æ¼çš„è·¨å¸§è‡ªæ³¨æ„åŠ›å±‚å’Œç”¨äºæ”¹è¿›ä¸€è‡´æ€§ä»¤ç‰Œåˆå¹¶å±‚ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡ç»“æœä¸Šéƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-743e137a2491381a7a06185e6f185d02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98f2d83fb08afeda8eb46dcd6041e6e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ed045cc4a6f90da5808658bd4410315.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1734fe71214eb036cd7d827f46120cba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-966f8a7b64a5e299eea2ea7f46ccc73a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12fab70e153710d25e77c3ec03cb12bc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reconstruction-Free-Anomaly-Detection-with-Diffusion-Models-via-Direct-Latent-Likelihood-Evaluation"><a href="#Reconstruction-Free-Anomaly-Detection-with-Diffusion-Models-via-Direct-Latent-Likelihood-Evaluation" class="headerlink" title="Reconstruction-Free Anomaly Detection with Diffusion Models via Direct   Latent Likelihood Evaluation"></a>Reconstruction-Free Anomaly Detection with Diffusion Models via Direct   Latent Likelihood Evaluation</h2><p><strong>Authors:Shunsuke Sakai, Tatsuhito Hasegawa</strong></p>
<p>Diffusion models, with their robust distribution approximation capabilities, have demonstrated excellent performance in anomaly detection. However, conventional reconstruction-based approaches rely on computing the reconstruction error between the original and denoised images, which requires careful noise-strength tuning and over ten network evaluations per input-leading to significantly slower detection speeds. To address these limitations, we propose a novel diffusion-based anomaly detection method that circumvents the need for resource-intensive reconstruction. Instead of reconstructing the input image, we directly infer its corresponding latent variables and measure their density under the Gaussian prior distribution. Remarkably, the prior density proves effective as an anomaly score even when using a short partial diffusion process of only 2-5 steps. We evaluate our method on the MVTecAD dataset, achieving an AUC of 0.991 at 15 FPS, thereby setting a new state-of-the-art speed-AUC anomaly detection trade-off. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å‡­å€Ÿå…¶å¼ºå¤§çš„åˆ†å¸ƒè¿‘ä¼¼èƒ½åŠ›ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºé‡å»ºçš„æ–¹æ³•ä¾èµ–äºè®¡ç®—åŸå§‹å›¾åƒå’Œå»å™ªå›¾åƒä¹‹é—´çš„é‡å»ºè¯¯å·®ï¼Œè¿™éœ€è¦è¿›è¡Œä»”ç»†çš„å™ªå£°å¼ºåº¦è°ƒæ•´ï¼Œå¹¶ä¸”æ¯ä¸ªè¾“å…¥éœ€è¦è¶…è¿‡åæ¬¡çš„ç½‘ç»œè¯„ä¼°ï¼Œä»è€Œå¯¼è‡´æ£€æµ‹é€Ÿåº¦å¤§å¤§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€è¿›è¡Œèµ„æºå¯†é›†å‹çš„é‡å»ºã€‚æˆ‘ä»¬ä¸éœ€è¦é‡å»ºè¾“å…¥å›¾åƒï¼Œè€Œæ˜¯ç›´æ¥æ¨æ–­å…¶å¯¹åº”çš„æ½œåœ¨å˜é‡ï¼Œå¹¶æµ‹é‡å®ƒä»¬åœ¨é«˜æ–¯å…ˆéªŒåˆ†å¸ƒä¸‹çš„å¯†åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åªä½¿ç”¨çŸ­æ—¶é—´çš„éƒ¨åˆ†æ‰©æ•£è¿‡ç¨‹ï¼ˆä»…2-5æ­¥ï¼‰ï¼Œå…ˆéªŒå¯†åº¦ä¹Ÿè¢«è¯æ˜å¯ä»¥ä½œä¸ºæœ‰æ•ˆçš„å¼‚å¸¸åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨MVTecADæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»¥æ¯ç§’15å¸§çš„é€Ÿç‡å®ç°äº†AUCä¸º0.991ï¼Œä»è€Œå»ºç«‹äº†æœ€æ–°çš„é€Ÿåº¦-AUCå¼‚å¸¸æ£€æµ‹æƒè¡¡æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05662v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/SkyShunsuke/InversionAD">https://github.com/SkyShunsuke/InversionAD</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³•ï¼Œæ— éœ€è¿›è¡Œèµ„æºå¯†é›†å‹çš„é‡å»ºè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ç›´æ¥æ¨æ–­è¾“å…¥å›¾åƒçš„å¯¹åº”æ½œåœ¨å˜é‡ï¼Œå¹¶æµ‹é‡å…¶åœ¨é«˜æ–¯å…ˆéªŒåˆ†å¸ƒä¸‹çš„å¯†åº¦ï¼Œä»è€Œæœ‰æ•ˆå®ç°å¼‚å¸¸æ£€æµ‹ã€‚è¯¥æ–¹æ³•åœ¨MVTecADæ•°æ®é›†ä¸Šå–å¾—äº†AUCä¸º0.991çš„ä¼˜å¼‚è¡¨ç°ï¼ŒåŒæ—¶å®ç°äº†æ¯ç§’å¤„ç†15å¸§çš„é€Ÿåº¦ï¼Œè¾¾åˆ°äº†é€Ÿåº¦å’ŒAUCå¼‚å¸¸æ£€æµ‹ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰å¼ºå¤§çš„åˆ†å¸ƒè¿‘ä¼¼èƒ½åŠ›ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿé‡å»ºæ–¹æ³•éœ€è¦ä»”ç»†è°ƒæ•´å™ªå£°å¼ºåº¦ï¼Œå¹¶å¯¹æ¯ä¸ªè¾“å…¥è¿›è¡Œå¤šæ¬¡ç½‘ç»œè¯„ä¼°ï¼Œå¯¼è‡´æ£€æµ‹é€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>æ–°æ–¹æ³•ç»•è¿‡äº†èµ„æºå¯†é›†å‹çš„é‡å»ºè¿‡ç¨‹ï¼Œç›´æ¥æ¨æ–­è¾“å…¥å›¾åƒçš„æ½œåœ¨å˜é‡ã€‚</li>
<li>é€šè¿‡æµ‹é‡æ½œåœ¨å˜é‡åœ¨é«˜æ–¯å…ˆéªŒåˆ†å¸ƒä¸‹çš„å¯†åº¦ï¼Œå®ç°äº†æœ‰æ•ˆçš„å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>ä»…ä½¿ç”¨çŸ­çš„éƒ¨åˆ†æ‰©æ•£è¿‡ç¨‹ï¼ˆ2-5æ­¥ï¼‰å³å¯è·å¾—è‰¯å¥½çš„å¼‚å¸¸æ£€æµ‹ç»“æœã€‚</li>
<li>åœ¨MVTecADæ•°æ®é›†ä¸Šå–å¾—äº†å¾ˆé«˜çš„AUCå€¼ï¼ˆ0.991ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8c20038ea417de3219b56342991c899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4320a80cf562531eb7010ac48d5cae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bde19aa6a4e91667bf1a6b9484207982.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5069dc8b61d5c66e92b0229e8ab8cd3e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Studying-Image-Diffusion-Features-for-Zero-Shot-Video-Object-Segmentation"><a href="#Studying-Image-Diffusion-Features-for-Zero-Shot-Video-Object-Segmentation" class="headerlink" title="Studying Image Diffusion Features for Zero-Shot Video Object   Segmentation"></a>Studying Image Diffusion Features for Zero-Shot Video Object   Segmentation</h2><p><strong>Authors:Thanos Delatolas, Vicky Kalogeiton, Dim P. Papadopoulos</strong></p>
<p>This paper investigates the use of large-scale diffusion models for Zero-Shot Video Object Segmentation (ZS-VOS) without fine-tuning on video data or training on any image segmentation data. While diffusion models have demonstrated strong visual representations across various tasks, their direct application to ZS-VOS remains underexplored. Our goal is to find the optimal feature extraction process for ZS-VOS by identifying the most suitable time step and layer from which to extract features. We further analyze the affinity of these features and observe a strong correlation with point correspondences. Through extensive experiments on DAVIS-17 and MOSE, we find that diffusion models trained on ImageNet outperform those trained on larger, more diverse datasets for ZS-VOS. Additionally, we highlight the importance of point correspondences in achieving high segmentation accuracy, and we yield state-of-the-art results in ZS-VOS. Finally, our approach performs on par with models trained on expensive image segmentation datasets. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆZS-VOSï¼‰ä¸­çš„åº”ç”¨ï¼Œè€Œæ— éœ€åœ¨è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒæˆ–åœ¨ä»»ä½•å›¾åƒåˆ†å‰²æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ZS-VOSçš„ç›´æ¥åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ZS-VOSçš„æœ€ä½³ç‰¹å¾æå–è¿‡ç¨‹ï¼Œé€šè¿‡ç¡®å®šæœ€åˆé€‚çš„æ—¶é—´æ­¥é•¿å’Œå±‚æ¥æå–ç‰¹å¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†è¿™äº›ç‰¹å¾çš„äº²å’ŒåŠ›ï¼Œå¹¶è§‚å¯Ÿåˆ°å®ƒä»¬ä¸ç‚¹å¯¹åº”çš„å¼ºçƒˆç›¸å…³æ€§ã€‚åœ¨DAVIS-17å’ŒMOSEä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ImageNetä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨ZS-VOSæ–¹é¢çš„æ€§èƒ½ä¼˜äºåœ¨æ›´å¤§ã€æ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å®ç°é«˜åˆ†å‰²å‡†ç¡®åº¦æ—¶ç‚¹å¯¹åº”çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬åœ¨ZS-VOSä¸­è·å¾—äº†æœ€æ–°ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸åœ¨æ˜‚è´µçš„å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05468v1">PDF</a> Accepted to CVPRW2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆZS-VOSï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œæ— éœ€é’ˆå¯¹è§†é¢‘æ•°æ®è¿›è¡Œå¾®è°ƒæˆ–ä»»ä½•å›¾åƒåˆ†å‰²æ•°æ®çš„è®­ç»ƒã€‚æœ¬æ–‡é€šè¿‡è¯†åˆ«æœ€é€‚åˆçš„æ—¶é—´æ­¥é•¿å’Œå±‚æ¥æå–ç‰¹å¾ï¼Œå®ç°äº†å¯¹ZS-VOSçš„æœ€ä¼˜ç‰¹å¾æå–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼ºè°ƒäº†ç‚¹å¯¹åº”çš„é‡è¦æ€§ï¼Œå¹¶åœ¨DAVIS-17å’ŒMOSEç­‰æ•°æ®é›†ä¸Šå–å¾—äº†å…ˆè¿›çš„åˆ†å‰²ç²¾åº¦ã€‚æ‰©æ•£æ¨¡å‹åœ¨ZS-VOSä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåœ¨æ›´å¤§ã€æ›´å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆZS-VOSï¼‰ä»»åŠ¡ä¸­çš„åº”ç”¨è¢«ç ”ç©¶å¾—è¾ƒå°‘ã€‚</li>
<li>é€šè¿‡è¯†åˆ«æœ€é€‚åˆçš„æ—¶é—´æ­¥é•¿å’Œå±‚æ¥æå–ç‰¹å¾ï¼Œå¯ä»¥è¾¾åˆ°å¯¹ZS-VOSçš„æœ€ä¼˜ç‰¹å¾æå–æ•ˆæœã€‚</li>
<li>ç‚¹å¯¹åº”åœ¨æå‡åˆ†å‰²ç²¾åº¦æ–¹é¢æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚</li>
<li>åœ¨DAVIS-17å’ŒMOSEæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ZS-VOSä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåœ¨æ›´å¤§ã€æ›´å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>åœ¨ImageNetä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨ZS-VOSä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•è¾¾åˆ°äº†ä¸åœ¨æ˜‚è´µçš„å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d016d82726907510a3ef651b318f3db2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2e2578f0da1ebfdcbf9dc4510c57a91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7866760aa12e58057dcc885e7691de4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36fcb105bb87e0fab00abc0e7d99eaad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1bb14e4e9837e168f6f1b48ee00ec6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="REWIND-Real-Time-Egocentric-Whole-Body-Motion-Diffusion-with-Exemplar-Based-Identity-Conditioning"><a href="#REWIND-Real-Time-Egocentric-Whole-Body-Motion-Diffusion-with-Exemplar-Based-Identity-Conditioning" class="headerlink" title="REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with   Exemplar-Based Identity Conditioning"></a>REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with   Exemplar-Based Identity Conditioning</h2><p><strong>Authors:Jihyun Lee, Weipeng Xu, Alexander Richard, Shih-En Wei, Shunsuke Saito, Shaojie Bai, Te-Li Wang, Minhyuk Sung, Tae-Kyun Kim, Jason Saragih</strong></p>
<p>We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a one-step diffusion model for real-time, high-fidelity human motion estimation from egocentric image inputs. While an existing method for egocentric whole-body (i.e., body and hands) motion estimation is non-real-time and acausal due to diffusion-based iterative motion refinement to capture correlations between body and hand poses, REWIND operates in a fully causal and real-time manner. To enable real-time inference, we introduce (1) cascaded body-hand denoising diffusion, which effectively models the correlation between egocentric body and hand motions in a fast, feed-forward manner, and (2) diffusion distillation, which enables high-quality motion estimation with a single denoising step. Our denoising diffusion model is based on a modified Transformer architecture, designed to causally model output motions while enhancing generalizability to unseen motion lengths. Additionally, REWIND optionally supports identity-conditioned motion estimation when identity prior is available. To this end, we propose a novel identity conditioning method based on a small set of pose exemplars of the target identity, which further enhances motion estimation quality. Through extensive experiments, we demonstrate that REWIND significantly outperforms the existing baselines both with and without exemplar-based identity conditioning. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº† REWINDï¼ˆå®æ—¶ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å…¨èº«åŠ¨ä½œæ‰©æ•£ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»è‡ªæˆ‘ä¸­å¿ƒçš„å›¾åƒè¾“å…¥ä¸­è¿›è¡Œå®æ—¶ã€é«˜ä¿çœŸçš„äººç±»åŠ¨ä½œä¼°è®¡ã€‚ç°æœ‰çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å…¨èº«ï¼ˆå³èº«ä½“å’Œæ‰‹ï¼‰åŠ¨ä½œä¼°è®¡æ–¹æ³•ç”±äºåŸºäºæ‰©æ•£çš„è¿­ä»£åŠ¨ä½œç»†åŒ–æ¥æ•æ‰èº«ä½“å’Œæ‰‹éƒ¨å§¿åŠ¿ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå› æ­¤æ˜¯éå®æ—¶çš„å’Œéå› æœçš„ï¼Œè€Œ REWIND åˆ™ä»¥å®Œå…¨å› æœå’Œå®æ—¶çš„æ–¹å¼è¿›è¡Œæ“ä½œã€‚ä¸ºäº†å®ç°å®æ—¶æ¨ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ï¼ˆ1ï¼‰çº§è”çš„èº«ä½“æ‰‹éƒ¨å»å™ªæ‰©æ•£ï¼Œå®ƒä»¥å¿«é€Ÿå‰é¦ˆçš„æ–¹å¼æœ‰æ•ˆåœ°å¯¹è‡ªæˆ‘ä¸­å¿ƒçš„èº«ä½“å’Œæ‰‹éƒ¨åŠ¨ä½œè¿›è¡Œå»ºæ¨¡ï¼›ï¼ˆ2ï¼‰æ‰©æ•£è’¸é¦ï¼Œå®ƒèƒ½åœ¨ä¸€æ­¥å»å™ªè¿‡ç¨‹ä¸­å®ç°é«˜è´¨é‡çš„åŠ¨ä½œä¼°è®¡ã€‚æˆ‘ä»¬çš„å»å™ªæ‰©æ•£æ¨¡å‹åŸºäºæ”¹è¿›çš„ Transformer æ¶æ„ï¼Œæ—¨åœ¨å› æœåœ°æ¨¡æ‹Ÿè¾“å‡ºåŠ¨ä½œï¼ŒåŒæ—¶æé«˜å¯¹ä¸åŒé•¿åº¦åŠ¨ä½œçš„æ³›åŒ–èƒ½åŠ›ã€‚å¦å¤–ï¼Œå½“æä¾›èº«ä»½å…ˆéªŒä¿¡æ¯æ—¶ï¼ŒREWIND è¿˜æ”¯æŒæœ‰æ¡ä»¶èº«ä»½çš„åŠ¨ä½œä¼°è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç›®æ ‡èº«ä»½çš„å°è§„æ¨¡å§¿æ€èŒƒä¾‹é›†çš„æ–°å‹èº«ä»½æ¡ä»¶æ–¹æ³•ï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†åŠ¨ä½œä¼°è®¡çš„è´¨é‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ REWIND åœ¨æœ‰æ— èŒƒä¾‹é›†èº«ä»½æ¡ä»¶ä¸‹éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04956v2">PDF</a> Accepted to CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://jyunlee.github.io/projects/rewind/">https://jyunlee.github.io/projects/rewind/</a></p>
<p><strong>Summary</strong></p>
<p>REWINDæ˜¯ä¸€ä¸ªä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»ç¬¬ä¸€äººç§°å›¾åƒè¾“å…¥ä¸­è¿›è¡Œå®æ—¶ã€é«˜ä¿çœŸçš„äººä½“è¿åŠ¨ä¼°è®¡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå®ç°äº†å…¨èº«ï¼ˆèº«ä½“å’Œæ‰‹éƒ¨ï¼‰è¿åŠ¨çš„å®æ—¶å› æœæ¨æ–­ã€‚é€šè¿‡å¼•å…¥çº§è”èº«ä½“æ‰‹éƒ¨å»å™ªæ‰©æ•£å’Œæ‰©æ•£è’¸é¦æŠ€æœ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿåœ°å‰å‘å»ºæ¨¡èº«ä½“å’Œæ‰‹éƒ¨çš„è¿åŠ¨å…³è”ï¼Œå¹¶ç”¨å•ä¸ªå»å™ªæ­¥éª¤å®ç°é«˜è´¨é‡çš„è¿åŠ¨ä¼°è®¡ã€‚æ­¤å¤–ï¼ŒREWINDè¿˜æ”¯æŒåœ¨æœ‰èº«ä»½å…ˆéªŒçš„æƒ…å†µä¸‹è¿›è¡Œèº«ä»½æ¡ä»¶è¿åŠ¨ä¼°è®¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºç›®æ ‡èº«ä»½å§¿åŠ¿èŒƒä¾‹çš„æ–°å‹èº«ä»½æ¡ä»¶æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜è¿åŠ¨ä¼°è®¡è´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒREWINDåœ¨æ— å’Œæœ‰æ¡ä»¶ä¾‹ç¤ºèŒƒä¾‹çš„æƒ…å†µä¸‹éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REWINDæ˜¯ä¸€ä¸ªç”¨äºå®æ—¶é«˜ä¿çœŸäººä½“è¿åŠ¨ä¼°è®¡çš„ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»ç¬¬ä¸€äººç§°å›¾åƒè¾“å…¥ä¸­è¿›è¡Œæ¨æ–­ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒREWINDå®ç°äº†å…¨èº«è¿åŠ¨çš„å®æ—¶å› æœæ¨æ–­ã€‚</li>
<li>é€šè¿‡å¼•å…¥çº§è”èº«ä½“æ‰‹éƒ¨å»å™ªæ‰©æ•£ï¼ŒREWINDèƒ½å¤Ÿå¿«é€Ÿåœ°å‰å‘å»ºæ¨¡èº«ä½“å’Œæ‰‹éƒ¨çš„è¿åŠ¨å…³è”ã€‚</li>
<li>æ‰©æ•£è’¸é¦æŠ€æœ¯ä½¿REWINDèƒ½å¤Ÿåœ¨å•ä¸ªå»å™ªæ­¥éª¤ä¸­å®ç°é«˜è´¨é‡çš„è¿åŠ¨ä¼°è®¡ã€‚</li>
<li>REWINDæ”¯æŒèº«ä»½æ¡ä»¶è¿åŠ¨ä¼°è®¡ï¼Œè¿›ä¸€æ­¥æé«˜è¿åŠ¨ä¼°è®¡è´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç›®æ ‡èº«ä»½å§¿åŠ¿èŒƒä¾‹çš„æ–°å‹èº«ä»½æ¡ä»¶æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb449c522916a0fee2f4a8b27a8dd0b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e81f98747d86474e3f1cbd299db9509.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Privacy-Attacks-on-Image-AutoRegressive-Models"><a href="#Privacy-Attacks-on-Image-AutoRegressive-Models" class="headerlink" title="Privacy Attacks on Image AutoRegressive Models"></a>Privacy Attacks on Image AutoRegressive Models</h2><p><strong>Authors:Antoni Kowalczuk, Jan DubiÅ„ski, Franziska Boenisch, Adam Dziedzic</strong></p>
<p>Image autoregressive generation has emerged as a powerful new paradigm, with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns about their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to those of DMs as a reference point. Specifically, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images, with a True Positive Rate at False Positive Rate &#x3D; 1% (TPR@FPR&#x3D;1%) of 86.38%, compared to just 6.38% for DMs using comparable attacks. We leverage our novel MIA to perform dataset inference (DI) for IARs and show that it requires as few as 6 samples to detect dataset membership, compared to 200 samples for DI in DMs. This confirms a higher level of information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-d30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are empirically significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. This trend suggests that incorporating techniques from DMs into IARs, such as modeling the per-token probability distribution using a diffusion procedure, could help mitigate IARsâ€™ vulnerability to privacy attacks. We make our code available at: <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a> </p>
<blockquote>
<p>å›¾åƒè‡ªå›å½’ç”Ÿæˆå·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„æ–°èŒƒå¼ï¼Œå›¾åƒè‡ªå›å½’æ¨¡å‹ï¼ˆIARsï¼‰åœ¨å›¾åƒè´¨é‡æ–¹é¢ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç›¸åŒ¹é…ï¼ˆFIDï¼š1.48å¯¹1.58ï¼‰ï¼ŒåŒæ—¶å…è®¸æ›´é«˜çš„ç”Ÿæˆé€Ÿåº¦ã€‚ç„¶è€Œï¼Œä¸IARsç›¸å…³çš„éšç§é£é™©å°šæœªå¾—åˆ°æ¢ç´¢ï¼Œè¿™å¼•å‘äº†å¯¹å…¶è´Ÿè´£ä»»éƒ¨ç½²çš„æ‹…å¿§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹IARsè¿›è¡Œäº†å…¨é¢çš„éšç§åˆ†æï¼Œå¹¶å°†å…¶ä¸DMsçš„éšç§é£é™©è¿›è¡Œæ¯”è¾ƒï¼Œä½œä¸ºå‚è€ƒç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œè¯¥æ”»å‡»åœ¨æ£€æµ‹è®­ç»ƒå›¾åƒæ–¹é¢å®ç°äº†éå¸¸é«˜çš„æˆåŠŸç‡ï¼Œåœ¨å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ä¸º1%çš„æƒ…å†µä¸‹ï¼ŒçœŸé˜³æ€§ç‡ï¼ˆTPRï¼‰è¾¾åˆ°86.38%ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œä½¿ç”¨ç±»ä¼¼æ”»å‡»çš„DMsä»…ä¸º6.38%ã€‚æˆ‘ä»¬åˆ©ç”¨æ–°å‹MIAå¯¹IARsè¿›è¡Œæ•°æ®é›†æ¨ç†ï¼ˆDIï¼‰ï¼Œå¹¶è¡¨æ˜ä»…éœ€6ä¸ªæ ·æœ¬å³å¯æ£€æµ‹æ•°æ®é›†æˆå‘˜èº«ä»½ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒDMsè¿›è¡ŒDIéœ€è¦200ä¸ªæ ·æœ¬ã€‚è¿™è¯å®äº†IARsä¸­ä¿¡æ¯æ³„éœ²ç¨‹åº¦è¾ƒé«˜ã€‚æœ€åï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»IARä¸­æå–æ•°ç™¾ä¸ªè®­ç»ƒæ•°æ®ç‚¹ï¼ˆä¾‹å¦‚ï¼Œä»VAR-d30ä¸­æå–698ä¸ªï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜å­˜åœ¨åŸºæœ¬çš„éšç§æ•ˆç”¨æƒè¡¡ï¼šå°½ç®¡IARåœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸å®ç°ç±»ä¼¼æ€§èƒ½çš„DMsç›¸æ¯”ï¼Œå®ƒä»¬åœ¨å®é™…ä¸­æ›´å®¹æ˜“å—åˆ°éšç§æ”»å‡»ã€‚è¿™ä¸€è¶‹åŠ¿è¡¨æ˜ï¼Œå°†DMsçš„æŠ€æœ¯èå…¥IARsä¸­ï¼Œä¾‹å¦‚ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹å¯¹æ¯ä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¯èƒ½æœ‰åŠ©äºå‡è½»IARså¯¹éšç§æ”»å‡»çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/sprintml/privacy_attacks_against_iarsè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02514v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å›¾åƒè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼ˆIARsï¼‰çš„éšç§é£é™©é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸è¾ƒäºæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼ŒIARsåœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨æ›´é«˜çš„éšç§æ³„éœ²é£é™©ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œè¯¥æ”»å‡»åœ¨æ£€æµ‹è®­ç»ƒå›¾åƒæ–¹é¢è¡¨ç°å‡ºæé«˜çš„æˆåŠŸç‡ï¼Œå¹¶è¯å®äº†IARsåœ¨æ•°æ®é›†æ¨ç†ï¼ˆDIï¼‰æ–¹é¢çš„ä¿¡æ¯æ³„éœ²æ›´ä¸ºä¸¥é‡ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶IARså…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ï¼Œä½†åœ¨éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå¹¶æå‡ºå°†DMsçš„æŠ€æœ¯èå…¥IARsä¸­ï¼Œä»¥ç¼“è§£å…¶éšç§æ”»å‡»æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼ˆIARsï¼‰ä¸æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç›¸æ¯”ï¼Œåœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>IARså­˜åœ¨è¾ƒé«˜çš„éšç§æ³„éœ²é£é™©ï¼Œå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œåœ¨æ£€æµ‹IARsçš„è®­ç»ƒå›¾åƒæ–¹é¢è¡¨ç°å‡ºæé«˜æˆåŠŸç‡ã€‚</li>
<li>IARsåœ¨æ•°æ®é›†æ¨ç†ï¼ˆDIï¼‰æ–¹é¢çš„ä¿¡æ¯æ³„éœ²æ›´ä¸ºä¸¥é‡ï¼Œä»…éœ€å°‘é‡æ ·æœ¬å³å¯æ£€æµ‹æ•°æ®é›†æˆå‘˜ã€‚</li>
<li>IARsåœ¨éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œç›¸è¾ƒDMsæ›´æ˜“å—åˆ°éšç§æ”»å‡»ã€‚</li>
<li>ç›¸æ¯”DMsï¼ŒIARsçš„éšç§æ³„éœ²é—®é¢˜å‡¸æ˜¾äº†éšç§ä¸æ•ˆç”¨çš„æƒè¡¡ã€‚</li>
<li>å°†DMsçš„æŠ€æœ¯èå…¥IARsä¸­ï¼Œæœ‰åŠ©äºç¼“è§£IARsçš„éšç§æ”»å‡»æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3bf8cb90fd82ad9b91a3f087acb0698b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b62fdde08f1ecb2ec199d326c14d19.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Fast-constrained-sampling-in-pre-trained-diffusion-models"><a href="#Fast-constrained-sampling-in-pre-trained-diffusion-models" class="headerlink" title="Fast constrained sampling in pre-trained diffusion models"></a>Fast constrained sampling in pre-trained diffusion models</h2><p><strong>Authors:Alexandros Graikos, Nebojsa Jojic, Dimitris Samaras</strong></p>
<p>Large denoising diffusion models, such as Stable Diffusion, have been trained on billions of image-caption pairs to perform text-conditioned image generation. As a byproduct of this training, these models have acquired general knowledge about image statistics, which can be useful for other inference tasks. However, when confronted with sampling an image under new constraints, e.g. generating the missing parts of an image, using large pre-trained text-to-image diffusion models is inefficient and often unreliable. Previous approaches either utilize backpropagation, making them significantly slower and more memory-demanding than text-to-image inference, or only enforce the constraint locally, failing to capture critical long-range correlations. In this work, we propose an algorithm that enables fast and high-quality generation under arbitrary constraints. We observe that, during inference, we can interchange between gradient updates computed on the noisy image and updates computed on the final, clean image. This allows us to employ a numerical approximation to expensive gradient computations, incurring significant speed-ups in inference. Our approach produces results that rival or surpass the state-of-the-art training-free inference approaches while requiring a fraction of the time. We demonstrate the effectiveness of our algorithm under both linear and non-linear constraints. An implementation is provided at <a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling">https://github.com/cvlab-stonybrook/fast-constrained-sampling</a>. </p>
<blockquote>
<p>å¤§å‹å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰å·²ç»é€šè¿‡å¯¹æ•°åäº¿å›¾åƒæ ‡é¢˜å¯¹è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆã€‚ä½œä¸ºè¿™ç§è®­ç»ƒçš„ä¸€ä¸ªå‰¯äº§å“ï¼Œè¿™äº›æ¨¡å‹å·²ç»è·å¾—äº†å…³äºå›¾åƒç»Ÿè®¡çš„é€šç”¨çŸ¥è¯†ï¼Œè¿™å¯¹äºå…¶ä»–æ¨ç†ä»»åŠ¡å¯èƒ½æ˜¯æœ‰ç”¨çš„ã€‚ç„¶è€Œï¼Œå½“é¢ä¸´åœ¨æ–°çš„çº¦æŸä¸‹é‡‡æ ·å›¾åƒæ—¶ï¼Œä¾‹å¦‚ç”Ÿæˆå›¾åƒçš„ç¼ºå¤±éƒ¨åˆ†ï¼Œä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯ä½æ•ˆä¸”ç»å¸¸ä¸å¯é çš„ã€‚ä»¥å‰çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨åå‘ä¼ æ’­ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ¯”æ–‡æœ¬åˆ°å›¾åƒçš„æ¨ç†è¿‡ç¨‹æ›´æ…¢ä¸”æ›´å å†…å­˜ï¼Œè¦ä¹ˆåªå±€éƒ¨å®æ–½çº¦æŸï¼Œæ— æ³•æ•æ‰é‡è¦çš„é•¿ç¨‹ç›¸å…³æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§èƒ½å¤Ÿåœ¨ä»»æ„çº¦æŸä¸‹å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„ç®—æ³•ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å˜ˆæ‚å›¾åƒä¸Šè®¡ç®—çš„æ¢¯åº¦æ›´æ–°å’Œæœ€ç»ˆå¹²å‡€å›¾åƒä¸Šè®¡ç®—çš„æ›´æ–°ä¹‹é—´è¿›è¡Œäº’æ¢ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡‡ç”¨æ˜‚è´µçš„æ¢¯åº¦è®¡ç®—çš„æ•°å€¼è¿‘ä¼¼ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„ç»“æœå¯ä¸æˆ–è¶…è¶Šæœ€æ–°çš„æ— è®­ç»ƒæ¨ç†æ–¹æ³•ç›¸åª²ç¾ï¼ŒåŒæ—¶æ‰€éœ€æ—¶é—´å¤§å¤§å‡å°‘ã€‚æˆ‘ä»¬åœ¨çº¿æ€§çº¦æŸå’Œéçº¿æ€§çº¦æŸä¸‹éƒ½è¯æ˜äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸€ä¸ªå®ç°ç¤ºä¾‹å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/cvlab-stonybrook/fast-constrained-samplingæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18804v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰ç»è¿‡æ•°åäº¿å›¾åƒæ ‡é¢˜å¯¹è®­ç»ƒï¼Œå¯è¿›è¡Œæ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆã€‚æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—äº†æœ‰å…³å›¾åƒç»Ÿè®¡çš„é€šç”¨çŸ¥è¯†ï¼Œè¿™å¯¹å…¶æ¨ç†ä»»åŠ¡æœ‰ç›Šã€‚ç„¶è€Œï¼Œå¯¹äºåœ¨æ–°çš„çº¦æŸæ¡ä»¶ä¸‹é‡‡æ ·å›¾åƒï¼ˆä¾‹å¦‚ç”Ÿæˆå›¾åƒçš„ç¼ºå¤±éƒ¨åˆ†ï¼‰ï¼Œä½¿ç”¨å¤§å‹é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ•ˆç‡è¾ƒä½ä¸”å¸¸ä¸å¯é ã€‚å…ˆå‰çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨åå‘ä¼ æ’­ï¼Œä½¿å…¶æ¯”æ–‡æœ¬åˆ°å›¾åƒæ¨ç†æ›´æ…¢ä¸”æ›´å å†…å­˜ï¼Œè¦ä¹ˆä»…å±€éƒ¨å®æ–½çº¦æŸï¼Œæ— æ³•æ•æ‰å…³é”®çš„é•¿ç¨‹å…³è”ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œå¯åœ¨ä»»æ„çº¦æŸä¸‹å®ç°å¿«é€Ÿã€é«˜è´¨é‡çš„ç”Ÿæˆã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥åœ¨å˜ˆæ‚å›¾åƒä¸Šè®¡ç®—çš„æ¢¯åº¦æ›´æ–°å’Œæœ€ç»ˆæ¸…æ´å›¾åƒä¸Šè®¡ç®—çš„æ›´æ–°ä¹‹é—´è¿›è¡Œäº¤æ¢ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡‡ç”¨æ˜‚è´µçš„æ¢¯åº¦è®¡ç®—çš„æ•°å€¼é€¼è¿‘ï¼Œåœ¨æ¨ç†ä¸­å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨ä¸è®­ç»ƒæ— å…³çš„æ¨ç†æ–¹æ³•ä¸­å®ç°äº†è‰¯å¥½çš„æ•ˆæœã€‚å®ç°åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling%E3%80%82">https://github.com/cvlab-stonybrook/fast-constrained-samplingã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å»å™ªæ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionå·²è®­ç»ƒäºæ•°åäº¿å›¾åƒæ ‡é¢˜å¯¹ï¼Œæ”¯æŒæ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒä¸­è·å–äº†å›¾åƒç»Ÿè®¡çš„é€šç”¨çŸ¥è¯†ï¼Œæœ‰ç›Šäºå…¶ä»–æ¨ç†ä»»åŠ¡ã€‚</li>
<li>é¢å¯¹æ–°çº¦æŸæ¡ä»¶é‡‡æ ·å›¾åƒæ—¶ï¼Œç°æœ‰æ–¹æ³•æ•ˆç‡ä¸é«˜ä¸”å¸¸ä¸å¯é ã€‚</li>
<li>å…ˆå‰æ–¹æ³•ä½¿ç”¨åå‘ä¼ æ’­å¯¼è‡´é€Ÿåº¦æ…¢ã€å†…å­˜éœ€æ±‚å¤§ï¼Œæˆ–ä»…å±€éƒ¨å®æ–½çº¦æŸè€Œæ— æ³•æ•æ‰é•¿ç¨‹å…³è”ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§ç®—æ³•ï¼Œèƒ½åœ¨ä»»æ„çº¦æŸä¸‹å®ç°å¿«é€Ÿé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>æ¨ç†è¿‡ç¨‹ä¸­å¯äº¤æ¢å˜ˆæ‚å›¾åƒå’Œæ¸…æ´å›¾åƒä¸Šçš„æ¢¯åº¦æ›´æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e4d7161358901f069e7f70de7da3b51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f19e3248130b36a126b7e7769b8f8858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4095c92f962c497caff5e369bf33283a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe35ca1e67106364e4c2eefa799b5660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2264b9c8e2d87f6263ecc12d85842a74.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Continuous-Diffusion-for-Mixed-Type-Tabular-Data"><a href="#Continuous-Diffusion-for-Mixed-Type-Tabular-Data" class="headerlink" title="Continuous Diffusion for Mixed-Type Tabular Data"></a>Continuous Diffusion for Mixed-Type Tabular Data</h2><p><strong>Authors:Markus Mueller, Kathrin Gruber, Dennis Fok</strong></p>
<p>Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular Data. CDTD is based on a novel combination of score matching and score interpolation to enforce a unified continuous noise distribution for both continuous and categorical features. We explicitly acknowledge the necessity of homogenizing distinct data types by relying on model-specific loss calibration and initialization schemes.To further address the high heterogeneity in mixed-type tabular data, we introduce adaptive feature- or type-specific noise schedules. These ensure balanced generative performance across features and optimize the allocation of model capacity across features and diffusion time. Our experimental results show that CDTD consistently outperforms state-of-the-art benchmark models, captures feature correlations exceptionally well, and that heterogeneity in the noise schedule design boosts sample quality. Replication code is available at <a target="_blank" rel="noopener" href="https://github.com/muellermarkus/cdtd">https://github.com/muellermarkus/cdtd</a>. </p>
<blockquote>
<p>åŸºäºåˆ†æ•°ç”Ÿæˆæ¨¡å‹ï¼Œé€šå¸¸è¢«ç§°ä¸ºæ‰©æ•£æ¨¡å‹ï¼Œå·²ç»æˆåŠŸåº”ç”¨äºæ–‡æœ¬å’Œå›¾åƒæ•°æ®çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CDTDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„è¿ç»­æ‰©æ•£æ¨¡å‹ã€‚CDTDåŸºäºåˆ†æ•°åŒ¹é…å’Œåˆ†æ•°æ’å€¼çš„ç»„åˆï¼Œä»¥å¼ºåˆ¶æ‰§è¡Œç»Ÿä¸€è¿ç»­å™ªå£°åˆ†å¸ƒï¼Œé€‚ç”¨äºè¿ç»­å’Œåˆ†ç±»ç‰¹å¾ã€‚æˆ‘ä»¬æ˜ç¡®æ‰¿è®¤é€šè¿‡ä¾èµ–æ¨¡å‹ç‰¹å®šçš„æŸå¤±æ ¡å‡†å’Œåˆå§‹åŒ–æ–¹æ¡ˆæ¥ç»Ÿä¸€ä¸åŒç±»å‹æ•°æ®çš„å¿…è¦æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„é«˜å¼‚è´¨æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”ç‰¹å¾æˆ–ç±»å‹ç‰¹å®šçš„å™ªå£°æ—¶é—´è¡¨ã€‚è¿™äº›ç¡®ä¿äº†è·¨ç‰¹å¾çš„å¹³è¡¡ç”Ÿæˆæ€§èƒ½ï¼Œå¹¶ä¼˜åŒ–äº†æ¨¡å‹å®¹é‡åœ¨ç‰¹å¾å’Œæ‰©æ•£æ—¶é—´ä¸Šçš„åˆ†é…ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCDTDæŒç»­ä¼˜äºæœ€æ–°åŸºå‡†æ¨¡å‹ï¼Œå¼‚å¸¸æ•æ‰ç‰¹å¾ç›¸å…³æ€§ï¼Œå¹¶ä¸”å™ªå£°æ—¶é—´è¡¨è®¾è®¡ä¸­çš„å¼‚è´¨æ€§æé«˜äº†æ ·æœ¬è´¨é‡ã€‚å¤ç°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/muellermarkus/cdtd%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/muellermarkus/cdtdæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10431v5">PDF</a> published at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨æ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„åº”ç”¨ä¸Šä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºCDTDï¼Œä¸€ç§ç”¨äºæ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„è¿ç»­æ‰©æ•£æ¨¡å‹ã€‚CDTDé€šè¿‡ç»“åˆåˆ†æ•°åŒ¹é…å’Œåˆ†æ•°æ’å€¼æ¥å¼ºåˆ¶æ‰§è¡Œç»Ÿä¸€è¿ç»­å™ªå£°åˆ†å¸ƒï¼Œé€‚ç”¨äºè¿ç»­å’Œåˆ†ç±»ç‰¹å¾ã€‚ä¸ºåº”å¯¹æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„é«˜å¼‚è´¨æ€§ï¼Œå¼•å…¥è‡ªé€‚åº”ç‰¹å¾æˆ–ç±»å‹ç‰¹å®šå™ªå£°æ—¶é—´è¡¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDTDæŒç»­ä¼˜äºæœ€æ–°åŸºå‡†æ¨¡å‹ï¼Œç‰¹å¾å…³è”æ•æ‰å‡ºè‰²ï¼Œä¸”å™ªå£°æ—¶é—´è¡¨è®¾è®¡çš„å¼‚è´¨æ€§æå‡äº†æ ·æœ¬è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ··åˆç±»å‹è¡¨æ ¼æ•°æ®æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>CDTDæ˜¯ä¸€ç§ç”¨äºæ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„è¿ç»­æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œç»Ÿä¸€è¿ç»­å™ªå£°åˆ†å¸ƒæ¥å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®ç‰¹å¾ã€‚</li>
<li>CDTDç»“åˆäº†åˆ†æ•°åŒ¹é…å’Œåˆ†æ•°æ’å€¼æŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸ºåº”å¯¹æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„é«˜å¼‚è´¨æ€§ï¼ŒCDTDå¼•å…¥äº†è‡ªé€‚åº”ç‰¹å¾æˆ–ç±»å‹ç‰¹å®šçš„å™ªå£°æ—¶é—´è¡¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCDTDåœ¨ç”Ÿæˆæ··åˆç±»å‹è¡¨æ ¼æ•°æ®æ–¹é¢ä¼˜äºå…¶ä»–æœ€æ–°æ¨¡å‹ã€‚</li>
<li>CDTDèƒ½å¤Ÿå¾ˆå¥½åœ°æ•æ‰ç‰¹å¾ä¹‹é—´çš„å…³è”ã€‚</li>
<li>å™ªå£°æ—¶é—´è¡¨è®¾è®¡çš„å¼‚è´¨æ€§å¯¹æ ·æœ¬è´¨é‡æœ‰ç§¯æå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.10431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a7df2ba9e1eb8e1b9ff21d4eb0d6f95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c39f935500cd684bfb632aca667e8863.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e92d19d47564f9348c29ab736468571.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7b9cbee8aede2cf6c6cba8321a8a4016.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  HRMedSeg Unlocking High-resolution Medical Image segmentation via   Memory-efficient Attention Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d763b980211d4de561689d8554325759.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Meta-Continual Learning of Neural Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">22963.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
