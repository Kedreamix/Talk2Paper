<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-10  Transfer between Modalities with MetaQueries">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5ed045cc4a6f90da5808658bd4410315.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    52 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-10-更新"><a href="#2025-04-10-更新" class="headerlink" title="2025-04-10 更新"></a>2025-04-10 更新</h1><h2 id="Transfer-between-Modalities-with-MetaQueries"><a href="#Transfer-between-Modalities-with-MetaQueries" class="headerlink" title="Transfer between Modalities with MetaQueries"></a>Transfer between Modalities with MetaQueries</h2><p><strong>Authors:Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, Saining Xie</strong></p>
<p>Unified multimodal models aim to integrate understanding (text output) and generation (pixel output), but aligning these different modalities within a single architecture often demands complex training recipes and careful data balancing. We introduce MetaQueries, a set of learnable queries that act as an efficient interface between autoregressive multimodal LLMs (MLLMs) and diffusion models. MetaQueries connects the MLLM’s latents to the diffusion decoder, enabling knowledge-augmented image generation by leveraging the MLLM’s deep understanding and reasoning capabilities. Our method simplifies training, requiring only paired image-caption data and standard diffusion objectives. Notably, this transfer is effective even when the MLLM backbone remains frozen, thereby preserving its state-of-the-art multimodal understanding capabilities while achieving strong generative performance. Additionally, our method is flexible and can be easily instruction-tuned for advanced applications such as image editing and subject-driven generation. </p>
<blockquote>
<p>统一多模态模型旨在融合理解和生成（分别为文本输出和像素输出），但在单一架构内对齐这些不同模态通常需要复杂的训练配方和细致的数据平衡。我们引入了MetaQueries，这是一组可学习的查询，作为自回归多模态大型语言模型（MLLMs）和扩散模型之间的有效接口。MetaQueries将MLLM的潜在空间连接到扩散解码器，通过利用MLLM的深度理解和推理能力，实现知识增强的图像生成。我们的方法简化了训练，只需配对图像字幕数据和标准扩散目标。值得注意的是，即使在MLLM骨干被冻结的情况下，这种迁移也是有效的，从而保留了其最先进的多模态理解能力，同时实现了强大的生成性能。此外，我们的方法灵活，可轻松进行指令调整，用于高级应用，如图像编辑和主题驱动生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06256v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://xichenpan.com/metaquery">https://xichenpan.com/metaquery</a></p>
<p><strong>Summary</strong></p>
<p>文本介绍了一种名为MetaQueries的新方法，该方法通过一组可学习的查询在自回归多模态大型语言模型（MLLMs）和扩散模型之间建立了有效的接口。MetaQueries将MLLM的潜在空间与扩散解码器连接起来，实现了知识增强图像生成，充分利用了MLLM的深度理解和推理能力。该方法简化了训练过程，只需配对图像和字幕数据以及标准扩散目标。此外，该方法灵活性强，易于进行指令微调，适用于图像编辑和主题驱动生成等高级应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaQueries作为自回归多模态大型语言模型（MLLMs）和扩散模型之间的接口，旨在整合理解和生成两个方面的能力。</li>
<li>MetaQueries将MLLM的潜在空间与扩散解码器连接起来，实现知识增强图像生成。</li>
<li>该方法简化了训练流程，仅需要配对图像和字幕数据以及标准扩散目标。</li>
<li>MetaQueries在保持MLLM的先进多模态理解能力的同时，实现了强大的生成性能。</li>
<li>该方法灵活性高，易于进行指令微调以适应不同的应用场景。</li>
<li>该方法可以应用于图像编辑和主题驱动生成等高级应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-34c0bdb455721760acdffaf8afa68fd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bd7083f5108116326a8d0d4ddb5ce14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56008693de8fb99b52c62bf6f3e4a3f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a06893fb108d01abee6e848089fed5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dc37d9fdb8b36c2945169b15509e894.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CamContextI2V-Context-aware-Controllable-Video-Generation"><a href="#CamContextI2V-Context-aware-Controllable-Video-Generation" class="headerlink" title="CamContextI2V: Context-aware Controllable Video Generation"></a>CamContextI2V: Context-aware Controllable Video Generation</h2><p><strong>Authors:Luis Denninger, Sina Mokhtarzadeh Azar, Juergen Gall</strong></p>
<p>Recently, image-to-video (I2V) diffusion models have demonstrated impressive scene understanding and generative quality, incorporating image conditions to guide generation. However, these models primarily animate static images without extending beyond their provided context. Introducing additional constraints, such as camera trajectories, can enhance diversity but often degrades visual quality, limiting their applicability for tasks requiring faithful scene representation. We propose CamContextI2V, an I2V model that integrates multiple image conditions with 3D constraints alongside camera control to enrich both global semantics and fine-grained visual details. This enables more coherent and context-aware video generation. Moreover, we motivate the necessity of temporal awareness for an effective context representation. Our comprehensive study on the RealEstate10K dataset demonstrates improvements in visual quality and camera controllability. We make our code and models publicly available at: <a target="_blank" rel="noopener" href="https://github.com/LDenninger/CamContextI2V">https://github.com/LDenninger/CamContextI2V</a>. </p>
<blockquote>
<p>最近，图像到视频（I2V）的扩散模型已经表现出了令人印象深刻的场景理解和生成质量，结合了图像条件来指导生成。然而，这些模型主要使静态图像动画化，而没有超越其提供的上下文。引入额外的约束，如相机轨迹，可以增强多样性，但往往会降低视觉质量，限制了它们在需要忠实场景表示的任务中的应用。我们提出了CamContextI2V，这是一种I2V模型，它结合了多种图像条件、3D约束和相机控制，以丰富全局语义和精细的视觉细节。这能够实现更连贯和上下文感知的视频生成。此外，我们强调了时间感知对于有效上下文表示的必要性。我们在RealEstate10K数据集上的综合研究证明了在视觉质量和相机可控性方面的改进。我们的代码和模型可在以下网址公开获取：<a target="_blank" rel="noopener" href="https://github.com/LDenninger/CamContextI2V%E3%80%82">https://github.com/LDenninger/CamContextI2V。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06022v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的图像到视频（I2V）扩散模型——CamContextI2V。该模型结合了多种图像条件、3D约束以及相机控制，旨在丰富全局语义和精细视觉细节，从而实现更连贯和上下文感知的视频生成。同时强调了时间感知对于有效上下文表示的重要性，并在RealEstate10K数据集上进行了综合研究，证明了其在视觉质量和相机可控性方面的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像到视频（I2V）扩散模型已展现出令人印象深刻的场景理解和生成质量，能通过结合图像条件来指导生成。</li>
<li>现有模型主要对静态图像进行动画处理，缺乏上下文扩展。</li>
<li>引入额外的约束（如相机轨迹）可以增强多样性，但往往会降低视觉质量，限制了其在需要忠实场景表示的任务中的应用。</li>
<li>提出的CamContextI2V模型结合了多种图像条件、3D约束和相机控制，旨在丰富全局语义和精细视觉细节。</li>
<li>CamContextI2V模型实现了更连贯和上下文感知的视频生成。</li>
<li>强调了时间感知对于有效上下文表示的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06022">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7db237d7b1dd15e21fbd06f96291bbed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30b0fe290cede459b842232cbb17737d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-722eea116f16cd01fa8e4cde062aa1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34cc1d7790a531c8773e54b52357c612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7ce04bdc97c8085a4e9f9b5cb98a334.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities"><a href="#An-Empirical-Study-of-GPT-4o-Image-Generation-Capabilities" class="headerlink" title="An Empirical Study of GPT-4o Image Generation Capabilities"></a>An Empirical Study of GPT-4o Image Generation Capabilities</h2><p><strong>Authors:Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi</strong></p>
<p>The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o’s image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling. </p>
<blockquote>
<p>图像生成领域经历了迅速的发展，从早期的基于GAN的方法发展到扩散模型，再到最新的寻求理解和生成任务统一的生成架构。最近的进展，尤其是GPT-4o，已经证明了高保真度多模态生成的可行性，但其架构设计仍然神秘且未公开。这引发了人们关于图像和文本生成是否已成功集成到这些方法的统一框架中的问题。在这项工作中，我们对GPT-4o的图像生成能力进行了实证研究，并将其与领先的开源和商业模型进行了比较。我们的评估涵盖了四个主要类别，包括文本到图像、图像到图像、图像到3D和图像到X生成，涉及超过20项任务。我们的分析突出了GPT-4o在各种设置下的优势和局限性，并将其定位在更广泛的生成模型演变中。通过这项调查，我们为未来的统一生成模型指明了有前景的方向，并强调了架构设计和数据规模扩大的作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05979v1">PDF</a> </p>
<p><strong>摘要</strong><br>    图像生成领域已从早期的GAN方法迅速发展到扩散模型，最近更是出现了统一生成架构，旨在弥合理解和生成任务之间的鸿沟。GPT-4o等最新进展证明了高保真多模态生成的可行性，但其架构设计仍神秘未公开。本研究对GPT-4o的图像生成能力进行了实证研究，将其与领先的开源和商业模型进行基准测试比较。评估涵盖文本到图像、图像到图像、图像到3D和图像到X生成等四个主要类别，超过20项任务。分析突出了GPT-4o在不同设置下的优势和局限，并将其置于生成模型更广泛的演进背景中。本次调查为未来的统一生成模型指明了有前景的研究方向，强调了架构设计和数据规模的作用。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>图像生成领域经历从GAN到扩散模型，再至统一生成架构的演变。</li>
<li>GPT-4o等最新技术展示了高保真多模态生成的可行性。</li>
<li>GPT-4o的图像生成能力通过实证研究得到了评估，涵盖多个类别和任务。</li>
<li>GPT-4o在不同设置下展现出优势和局限。</li>
<li>架构设计对于生成模型的发展至关重要，但GPT-4o的架构设计尚未公开。</li>
<li>统一生成架构的发展前景广阔，尤其是在架构设计和数据规模方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1ada19c845658d6086bd8f06224ef8f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d8b70545b43ecb6ffa3d082185a785b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-653e211f893e469876b0aff041185c8b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Ambiguous-Image-Segmentation"><a href="#Diffusion-Based-Ambiguous-Image-Segmentation" class="headerlink" title="Diffusion Based Ambiguous Image Segmentation"></a>Diffusion Based Ambiguous Image Segmentation</h2><p><strong>Authors:Jakob Lønborg Christensen, Morten Rieger Hannemose, Anders Bjorholm Dahl, Vedrana Andersen Dahl</strong></p>
<p>Medical image segmentation often involves inherent uncertainty due to variations in expert annotations. Capturing this uncertainty is an important goal and previous works have used various generative image models for the purpose of representing the full distribution of plausible expert ground truths. In this work, we explore the design space of diffusion models for generative segmentation, investigating the impact of noise schedules, prediction types, and loss weightings. Notably, we find that making the noise schedule harder with input scaling significantly improves performance. We conclude that x- and v-prediction outperform epsilon-prediction, likely because the diffusion process is in the discrete segmentation domain. Many loss weightings achieve similar performance as long as they give enough weight to the end of the diffusion process. We base our experiments on the LIDC-IDRI lung lesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we introduce a randomly cropped variant of the LIDC-IDRI dataset that is better suited for uncertainty in image segmentation. Our model also achieves SOTA in this harder setting. </p>
<blockquote>
<p>医学图像分割常常由于专家标注的变化而涉及固有的不确定性。捕捉这种不确定性是一个重要目标，之前的工作已经使用各种生成图像模型来代表专家真实值的完整分布。在这项工作中，我们探索了生成分割扩散模型的设计空间，研究了噪声时间表、预测类型和损失权重的影响。值得注意的是，我们发现通过输入缩放使噪声时间表更加困难可以显著提高性能。我们得出结论，x预测和v预测优于ε预测，这可能是因为扩散过程处于离散分割领域。许多损失权重只要对扩散过程的结束给予足够的重视，就能达到类似的表现。我们的实验基于LIDC-IDRI肺病变数据集，并获得了最新（SOTA）性能。此外，我们还引入了LIDC-IDRI数据集的随机裁剪版本，更适合于图像分割的不确定性。我们的模型在这个更困难的设置中也达到了最新水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05977v1">PDF</a> Accepted at SCIA25</p>
<p><strong>Summary</strong></p>
<p>本文探讨了使用扩散模型进行生成性图像分割的设计空间，研究了噪声调度、预测类型和损失权重的影响。研究发现，通过输入缩放使噪声调度更加困难可以显著提高性能。x-预测和v-预测优于epsilon预测，因为扩散过程处于离散分割领域。基于LIDC-IDRI肺病灶数据集的实验获得了最先进的性能，并引入了一种随机裁剪的LIDC-IDRI数据集变体，更适合于图像分割的不确定性。模型在此更困难的环境中同样达到先进水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在生成性图像分割中的应用，研究了噪声调度、预测类型和损失权重对性能的影响。</li>
<li>通过输入缩放使噪声调度更加困难，可以显著提高性能。</li>
<li>x-预测和v-预测在离散分割领域的扩散过程中表现优于epsilon预测。</li>
<li>多种损失权重在给予足够重视于扩散过程结束时都能获得相似性能。</li>
<li>实验基于LIDC-IDRI肺病灶数据集，获得了最先进的性能。</li>
<li>引入了一种适合不确定性研究的LIDC-IDRI数据集变体，通过随机裁剪实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6170fc41063aa1b1104c59a8d656712b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6de7ffaa87d20e553c841ce75a2cb731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4512d984a41c79504eec109e680c6028.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Physics-aware-generative-models-for-turbulent-fluid-flows-through-energy-consistent-stochastic-interpolants"><a href="#Physics-aware-generative-models-for-turbulent-fluid-flows-through-energy-consistent-stochastic-interpolants" class="headerlink" title="Physics-aware generative models for turbulent fluid flows through   energy-consistent stochastic interpolants"></a>Physics-aware generative models for turbulent fluid flows through   energy-consistent stochastic interpolants</h2><p><strong>Authors:Nikolaj T. Mücke, Benjamin Sanderse</strong></p>
<p>Generative models have demonstrated remarkable success in domains such as text, image, and video synthesis. In this work, we explore the application of generative models to fluid dynamics, specifically for turbulence simulation, where classical numerical solvers are computationally expensive. We propose a novel stochastic generative model based on stochastic interpolants, which enables probabilistic forecasting while incorporating physical constraints such as energy stability and divergence-freeness. Unlike conventional stochastic generative models, which are often agnostic to underlying physical laws, our approach embeds energy consistency by making the parameters of the stochastic interpolant learnable coefficients. We evaluate our method on a benchmark turbulence problem - Kolmogorov flow - demonstrating superior accuracy and stability over state-of-the-art alternatives such as autoregressive conditional diffusion models (ACDMs) and PDE-Refiner. Furthermore, we achieve stable results for significantly longer roll-outs than standard stochastic interpolants. Our results highlight the potential of physics-aware generative models in accelerating and enhancing turbulence simulations while preserving fundamental conservation properties. </p>
<blockquote>
<p>生成模型在文本、图像和视频合成等领域取得了显著的成功。在这项工作中，我们探索了将生成模型应用于流体动力学，特别是湍流模拟的应用，因为传统的数值求解器在计算上非常昂贵。我们提出了一种基于随机插值的新型随机生成模型，该模型能够进行概率预测，同时结合了能量稳定性和无散度等物理约束。与通常忽视基本物理定律的传统随机生成模型不同，我们的方法通过使随机插值的参数成为可学习的系数来嵌入能量一致性。我们在基准湍流问题——柯尔莫哥洛夫流上评估了我们的方法，与最先进的替代品（如自回归条件扩散模型（ACDM）和PDE精炼器）相比，我们的方法在准确性和稳定性方面表现出优势。此外，对于比标准随机插值更长的滚动预测，我们还实现了稳定的结果。我们的结果突出了物理感知生成模型在加速和改进湍流模拟方面的潜力，同时在保留基本守恒性质方面发挥重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了生成模型在流体动力学领域的应用，特别是用于昂贵的经典数值求解器的湍流模拟。提出一种基于随机插值的新型随机生成模型，能够在概率预测的同时融入能量稳定性和无散度的物理约束。与传统忽略物理定律的随机生成模型不同，该方法通过使随机插值的参数成为可学习系数来嵌入能量一致性。在Kolmogorov流等基准湍流问题上进行了评估，证明其在精度和稳定性方面优于自回归条件扩散模型（ACDM）和PDE精炼器等当前主流方法，且实现了更长的滚动预测结果。结果突显了物理感知生成模型在加速和改进湍流模拟方面的潜力，同时保持基本的守恒属性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型在流体动力学领域的应用被探索，特别是在湍流模拟方面。</li>
<li>提出一种新型随机生成模型，基于随机插值进行概率预测，并融入物理约束。</li>
<li>与传统随机生成模型不同，该方法嵌入能量一致性，通过使随机插值的参数成为可学习系数。</li>
<li>在基准湍流问题上评估，证明该方法在精度和稳定性方面优于当前主流方法。</li>
<li>实现更长的滚动预测结果。</li>
<li>物理感知生成模型在加速和改进湍流模拟方面具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-04e0933bce5f668bf4cc562fc6599a35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39483f606c9785ca5536bd9ba0bf2065.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mind-the-Trojan-Horse-Image-Prompt-Adapter-Enabling-Scalable-and-Deceptive-Jailbreaking"><a href="#Mind-the-Trojan-Horse-Image-Prompt-Adapter-Enabling-Scalable-and-Deceptive-Jailbreaking" class="headerlink" title="Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and   Deceptive Jailbreaking"></a>Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and   Deceptive Jailbreaking</h2><p><strong>Authors:Junxi Chen, Junhao Dong, Xiaohua Xie</strong></p>
<p>Recently, the Image Prompt Adapter (IP-Adapter) has been increasingly integrated into text-to-image diffusion models (T2I-DMs) to improve controllability. However, in this paper, we reveal that T2I-DMs equipped with the IP-Adapter (T2I-IP-DMs) enable a new jailbreak attack named the hijacking attack. We demonstrate that, by uploading imperceptible image-space adversarial examples (AEs), the adversary can hijack massive benign users to jailbreak an Image Generation Service (IGS) driven by T2I-IP-DMs and mislead the public to discredit the service provider. Worse still, the IP-Adapter’s dependency on open-source image encoders reduces the knowledge required to craft AEs. Extensive experiments verify the technical feasibility of the hijacking attack. In light of the revealed threat, we investigate several existing defenses and explore combining the IP-Adapter with adversarially trained models to overcome existing defenses’ limitations. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fhdnskfbeuv/attackIPA">https://github.com/fhdnskfbeuv/attackIPA</a>. </p>
<blockquote>
<p>最近，Image Prompt Adapter（IP-Adapter）越来越多地被集成到文本到图像扩散模型（T2I-DMs）中，以提高可控性。然而，本文揭示了配备IP-Adapter的T2I-DMs（T2I-IP-DMs）能够实施一种名为劫持攻击的新型越狱攻击。我们证明，通过上传几乎察觉不到的图像空间对抗样本（AEs），攻击者可以劫持大量良性用户来越狱由T2I-IP-DMs驱动的图像生成服务（IGS），并误导公众质疑服务提供商。更糟糕的是，IP-Adapter对开源图像编码器的依赖降低了制作AEs所需的知识。大量实验验证了劫持攻击的技术可行性。鉴于所揭示的威胁，我们调查了几种现有的防御措施，并探索将IP-Adapter与对抗训练模型相结合，以克服现有防御措施的局限性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/fhdnskfbeuv/attackIPA">https://github.com/fhdnskfbeuv/attackIPA</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05838v1">PDF</a> Accepted by CVPR2025 as Highlight</p>
<p><strong>Summary</strong></p>
<p>文本描述了研究者揭示了配备图像提示适配器（IP-Adapter）的文本到图像扩散模型（T2I-DMs）存在一种名为劫持攻击的新漏洞。攻击者可以通过上传几乎无法察觉的图像空间对抗样本（AEs）来劫持大量良性用户，使图像生成服务（IGS）受到破坏并误导公众对服务提供商的信任。此外，IP-Adapter对开源图像编码器的依赖降低了制作AEs所需的知识。本文同时探讨了现有防御措施的结合以及对抗训练模型在克服现有防御局限方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IP-Adapter在T2I-DMs中的集成提高了可控性，但也引入了新的安全隐患。</li>
<li>存在一种名为劫持攻击的新漏洞，攻击者可利用IP-Adapter的特点来破坏IGS并误导公众。</li>
<li>攻击者通过上传几乎无法察觉的AEs来实现劫持攻击，且IP-Adapter对开源图像编码器的依赖简化了这一过程。</li>
<li>广泛实验验证了劫持攻击的技术可行性。</li>
<li>现有防御措施存在局限性，需要探索新的解决方案。</li>
<li>结合IP-Adapter与对抗训练模型可克服现有防御的局限，提高系统的安全性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05838">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e990254a2444b3bf2ab85006f6fabb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6ecc795417304d9f89c0cd04a87072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d50ec19a1af94c2477c1399baa28048.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4782ad5feb5783ea12c1da87ee1eae9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-940040f32e0c3a3eaec47e27175f56b4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Parasite-A-Steganography-based-Backdoor-Attack-Framework-for-Diffusion-Models"><a href="#Parasite-A-Steganography-based-Backdoor-Attack-Framework-for-Diffusion-Models" class="headerlink" title="Parasite: A Steganography-based Backdoor Attack Framework for Diffusion   Models"></a>Parasite: A Steganography-based Backdoor Attack Framework for Diffusion   Models</h2><p><strong>Authors:Jiahao Chen, Yu Pan, Yi Du, Chunkai Wu, Lin Wang</strong></p>
<p>Recently, the diffusion model has gained significant attention as one of the most successful image generation models, which can generate high-quality images by iteratively sampling noise. However, recent studies have shown that diffusion models are vulnerable to backdoor attacks, allowing attackers to enter input data containing triggers to activate the backdoor and generate their desired output. Existing backdoor attack methods primarily focused on target noise-to-image and text-to-image tasks, with limited work on backdoor attacks in image-to-image tasks. Furthermore, traditional backdoor attacks often rely on a single, conspicuous trigger to generate a fixed target image, lacking concealability and flexibility. To address these limitations, we propose a novel backdoor attack method called “Parasite” for image-to-image tasks in diffusion models, which not only is the first to leverage steganography for triggers hiding, but also allows attackers to embed the target content as a backdoor trigger to achieve a more flexible attack. “Parasite” as a novel attack method effectively bypasses existing detection frameworks to execute backdoor attacks. In our experiments, “Parasite” achieved a 0 percent backdoor detection rate against the mainstream defense frameworks. In addition, in the ablation study, we discuss the influence of different hiding coefficients on the attack results. You can find our code at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Parasite-1715/">https://anonymous.4open.science/r/Parasite-1715/</a>. </p>
<blockquote>
<p>最近，扩散模型作为最成功的图像生成模型之一，因其能够通过迭代采样噪声生成高质量图像而备受关注。然而，最近的研究表明，扩散模型容易受到后门攻击的影响，攻击者可以通过输入包含触发器的数据来激活后门，并生成他们想要的输出。现有的后门攻击方法主要集中在目标噪声到图像和文本到图像的任务上，而对于图像到图像任务的后门攻击研究相对较少。此外，传统的后门攻击通常依赖于一个单一、显眼的触发器来生成固定的目标图像，缺乏隐蔽性和灵活性。为了解决这些限制，我们针对扩散模型中的图像到图像任务提出了一种新的后门攻击方法，名为“寄生虫”。它不仅首次利用隐写术来隐藏触发器，还允许攻击者将目标内容作为后门触发器，以实现更灵活的攻击。“寄生虫”作为一种新的攻击方法，有效地绕过了现有的检测框架来执行后门攻击。在我们的实验中，“寄生虫”针对主流防御框架实现了0%的后门检测率。另外，在消融研究中，我们讨论了不同的隐藏系数对攻击结果的影响。你可以在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Parasite-1715/%E6%89%BE%E5%88%B0%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://anonymous.4open.science/r/Parasite-1715/找到我们的代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05815v1">PDF</a> </p>
<p><strong>Summary</strong><br>     扩散模型最近成为最成功的图像生成模型之一，但其易受后门攻击影响。针对图像到图像的扩散模型任务，我们提出了一种名为“寄生虫”的新型后门攻击方法，该方法不仅利用隐写术隐藏触发器，还允许攻击者嵌入目标内容作为后门触发器，以实现更灵活的攻击。该攻击方法成功绕过现有检测框架，对主流防御框架的后门检测率为零。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型是当下最成功的图像生成模型之一，但存在后门攻击风险。</li>
<li>现有后门攻击主要集中在噪声到图像和文本到图像的任务上，针对图像到图像的扩散模型的攻击研究有限。</li>
<li>提出的“寄生虫”攻击方法用于图像到图像的扩散模型任务，利用隐写术隐藏触发器。</li>
<li>“寄生虫”攻击方法允许攻击者嵌入目标内容作为后门触发器，实现更灵活攻击。</li>
<li>“寄生虫”攻击成功绕过现有检测框架，对主流防御框架的后门检测率为零。</li>
<li>实验中发现不同隐藏系数对攻击结果有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2eb101844ea46edfa2438a7ca747351c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ae637df4304771c9504f2919de877e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6891beaea15d66ac824b47f05fab8a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4de6843efb139ec72c9873507f763d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-985f77d7fec686862fe6ea05cb52ef99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bbeb808d561c36154332b982ea2503e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Storybooth-Training-free-Multi-Subject-Consistency-for-Improved-Visual-Storytelling"><a href="#Storybooth-Training-free-Multi-Subject-Consistency-for-Improved-Visual-Storytelling" class="headerlink" title="Storybooth: Training-free Multi-Subject Consistency for Improved Visual   Storytelling"></a>Storybooth: Training-free Multi-Subject Consistency for Improved Visual   Storytelling</h2><p><strong>Authors:Jaskirat Singh, Junshen Kevin Chen, Jonas Kohler, Michael Cohen</strong></p>
<p>Training-free consistent text-to-image generation depicting the same subjects across different images is a topic of widespread recent interest. Existing works in this direction predominantly rely on cross-frame self-attention; which improves subject-consistency by allowing tokens in each frame to pay attention to tokens in other frames during self-attention computation. While useful for single subjects, we find that it struggles when scaling to multiple characters. In this work, we first analyze the reason for these limitations. Our exploration reveals that the primary-issue stems from self-attention-leakage, which is exacerbated when trying to ensure consistency across multiple-characters. This happens when tokens from one subject pay attention to other characters, causing them to appear like each other (e.g., a dog appearing like a duck). Motivated by these findings, we propose StoryBooth: a training-free approach for improving multi-character consistency. In particular, we first leverage multi-modal chain-of-thought reasoning and region-based generation to apriori localize the different subjects across the desired story outputs. The final outputs are then generated using a modified diffusion model which consists of two novel layers: 1) a bounded cross-frame self-attention layer for reducing inter-character attention leakage, and 2) token-merging layer for improving consistency of fine-grain subject details. Through both qualitative and quantitative results we find that the proposed approach surpasses prior state-of-the-art, exhibiting improved consistency across both multiple-characters and fine-grain subject details. </p>
<blockquote>
<p>无训练一致文本到图像生成是近期广泛关注的主题，该生成方法旨在描绘不同图像中的相同主题。目前的研究主要依赖于跨帧自注意力机制，通过允许每个帧中的标记关注其他帧中的标记，从而提高主题一致性。虽然这在单个主题上很有用，但我们发现它在扩展到多个角色时遇到了困难。在这项工作中，我们首先分析了这些限制的原因。我们的探索表明，主要问题源于自注意力泄漏，在尝试确保跨多个角色的一致性时，这一问题会加剧。这种情况发生在来自一个主题的标记关注其他角色时，导致他们看起来彼此相似（例如，一只狗看起来像鸭子）。受这些发现的启发，我们提出了StoryBooth：一种无需训练即可提高多角色一致性的方法。具体而言，我们首先利用多模态链式思维推理和基于区域的生成来先验定位所需故事输出中的不同主题。最终输出是使用经过修改的扩散模型生成的，该模型包含两个新层：1）有界跨帧自注意力层，用于减少角色间注意力泄漏；以及2）标记合并层，用于提高精细角色细节的一致性。通过定性和定量结果，我们发现所提出的方法超越了先前最先进的水平，在多个角色和精细角色细节上表现出更高的一致性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了无训练文本到图像生成的问题，特别是在描绘不同图像中的同一主题时。现有方法主要依赖跨帧自注意力机制，但在扩展到多角色时存在局限性。本文分析了这一问题产生的原因，并提出了StoryBooth方法，通过多模态链式思维推理和基于区域的生成来先定位不同主题，然后使用改进后的扩散模型生成最终输出，包括减少角色间注意力泄漏的跨帧自注意力层和用于改进精细主题细节一致性的令牌合并层。新方法在定性和定量结果上都优于现有技术，提高了多角色和精细主题细节的一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无训练文本到图像生成技术在描绘不同图像中的同一主题时受到广泛关注。</li>
<li>现有方法主要依赖跨帧自注意力机制，但在扩展到多角色时存在局限性。</li>
<li>跨帧自注意力泄漏是限制多角色一致性的主要原因。</li>
<li>StoryBooth方法通过多模态链式思维推理和基于区域的生成来定位不同主题。</li>
<li>改进后的扩散模型包括减少角色间注意力泄漏的跨帧自注意力层和用于改进一致性令牌合并层。</li>
<li>新方法在定性和定量结果上都优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-743e137a2491381a7a06185e6f185d02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98f2d83fb08afeda8eb46dcd6041e6e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ed045cc4a6f90da5808658bd4410315.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1734fe71214eb036cd7d827f46120cba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-966f8a7b64a5e299eea2ea7f46ccc73a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12fab70e153710d25e77c3ec03cb12bc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reconstruction-Free-Anomaly-Detection-with-Diffusion-Models-via-Direct-Latent-Likelihood-Evaluation"><a href="#Reconstruction-Free-Anomaly-Detection-with-Diffusion-Models-via-Direct-Latent-Likelihood-Evaluation" class="headerlink" title="Reconstruction-Free Anomaly Detection with Diffusion Models via Direct   Latent Likelihood Evaluation"></a>Reconstruction-Free Anomaly Detection with Diffusion Models via Direct   Latent Likelihood Evaluation</h2><p><strong>Authors:Shunsuke Sakai, Tatsuhito Hasegawa</strong></p>
<p>Diffusion models, with their robust distribution approximation capabilities, have demonstrated excellent performance in anomaly detection. However, conventional reconstruction-based approaches rely on computing the reconstruction error between the original and denoised images, which requires careful noise-strength tuning and over ten network evaluations per input-leading to significantly slower detection speeds. To address these limitations, we propose a novel diffusion-based anomaly detection method that circumvents the need for resource-intensive reconstruction. Instead of reconstructing the input image, we directly infer its corresponding latent variables and measure their density under the Gaussian prior distribution. Remarkably, the prior density proves effective as an anomaly score even when using a short partial diffusion process of only 2-5 steps. We evaluate our method on the MVTecAD dataset, achieving an AUC of 0.991 at 15 FPS, thereby setting a new state-of-the-art speed-AUC anomaly detection trade-off. </p>
<blockquote>
<p>扩散模型凭借其强大的分布近似能力，在异常检测方面表现出卓越的性能。然而，传统的基于重建的方法依赖于计算原始图像和去噪图像之间的重建误差，这需要进行仔细的噪声强度调整，并且每个输入需要超过十次的网络评估，从而导致检测速度大大降低。为了解决这些局限性，我们提出了一种新型的基于扩散的异常检测方法，该方法无需进行资源密集型的重建。我们不需要重建输入图像，而是直接推断其对应的潜在变量，并测量它们在高斯先验分布下的密度。值得注意的是，即使只使用短时间的部分扩散过程（仅2-5步），先验密度也被证明可以作为有效的异常分数。我们在MVTecAD数据集上评估了我们的方法，以每秒15帧的速率实现了AUC为0.991，从而建立了最新的速度-AUC异常检测权衡标准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05662v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/SkyShunsuke/InversionAD">https://github.com/SkyShunsuke/InversionAD</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散模型的异常检测新方法，无需进行资源密集型的重建过程。该方法直接推断输入图像的对应潜在变量，并测量其在高斯先验分布下的密度，从而有效实现异常检测。该方法在MVTecAD数据集上取得了AUC为0.991的优异表现，同时实现了每秒处理15帧的速度，达到了速度和AUC异常检测之间的最佳平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型具有强大的分布近似能力，在异常检测中表现出卓越性能。</li>
<li>传统重建方法需要仔细调整噪声强度，并对每个输入进行多次网络评估，导致检测速度较慢。</li>
<li>新方法绕过了资源密集型的重建过程，直接推断输入图像的潜在变量。</li>
<li>通过测量潜在变量在高斯先验分布下的密度，实现了有效的异常检测。</li>
<li>仅使用短的部分扩散过程（2-5步）即可获得良好的异常检测结果。</li>
<li>在MVTecAD数据集上取得了很高的AUC值（0.991）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05662">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a8c20038ea417de3219b56342991c899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4320a80cf562531eb7010ac48d5cae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bde19aa6a4e91667bf1a6b9484207982.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5069dc8b61d5c66e92b0229e8ab8cd3e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Studying-Image-Diffusion-Features-for-Zero-Shot-Video-Object-Segmentation"><a href="#Studying-Image-Diffusion-Features-for-Zero-Shot-Video-Object-Segmentation" class="headerlink" title="Studying Image Diffusion Features for Zero-Shot Video Object   Segmentation"></a>Studying Image Diffusion Features for Zero-Shot Video Object   Segmentation</h2><p><strong>Authors:Thanos Delatolas, Vicky Kalogeiton, Dim P. Papadopoulos</strong></p>
<p>This paper investigates the use of large-scale diffusion models for Zero-Shot Video Object Segmentation (ZS-VOS) without fine-tuning on video data or training on any image segmentation data. While diffusion models have demonstrated strong visual representations across various tasks, their direct application to ZS-VOS remains underexplored. Our goal is to find the optimal feature extraction process for ZS-VOS by identifying the most suitable time step and layer from which to extract features. We further analyze the affinity of these features and observe a strong correlation with point correspondences. Through extensive experiments on DAVIS-17 and MOSE, we find that diffusion models trained on ImageNet outperform those trained on larger, more diverse datasets for ZS-VOS. Additionally, we highlight the importance of point correspondences in achieving high segmentation accuracy, and we yield state-of-the-art results in ZS-VOS. Finally, our approach performs on par with models trained on expensive image segmentation datasets. </p>
<blockquote>
<p>本文探讨了大规模扩散模型在零样本视频目标分割（ZS-VOS）中的应用，而无需在视频数据上进行微调或在任何图像分割数据上进行训练。虽然扩散模型在各种任务中表现出了强大的视觉表示能力，但它们在ZS-VOS的直接应用仍然未被充分探索。我们的目标是找到ZS-VOS的最佳特征提取过程，通过确定最合适的时间步长和层来提取特征。我们进一步分析了这些特征的亲和力，并观察到它们与点对应的强烈相关性。在DAVIS-17和MOSE上的大量实验表明，在ImageNet上训练的扩散模型在ZS-VOS方面的性能优于在更大、更多样化的数据集上训练的模型。此外，我们强调了实现高分割准确度时点对应的重要性，我们在ZS-VOS中获得了最新结果。最后，我们的方法与在昂贵的图像分割数据集上训练的模型表现相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05468v1">PDF</a> Accepted to CVPRW2025</p>
<p><strong>Summary</strong><br>扩散模型在零样本视频目标分割（ZS-VOS）任务中展现出强大的潜力，无需针对视频数据进行微调或任何图像分割数据的训练。本文通过识别最适合的时间步长和层来提取特征，实现了对ZS-VOS的最优特征提取过程。此外，本文强调了点对应的重要性，并在DAVIS-17和MOSE等数据集上取得了先进的分割精度。扩散模型在ZS-VOS任务上的表现优于在更大、更多样化数据集上训练的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在零样本视频目标分割（ZS-VOS）任务中的应用被研究得较少。</li>
<li>通过识别最适合的时间步长和层来提取特征，可以达到对ZS-VOS的最优特征提取效果。</li>
<li>点对应在提升分割精度方面扮演着重要角色。</li>
<li>在DAVIS-17和MOSE数据集上进行的实验表明，扩散模型在ZS-VOS任务上的表现优于在更大、更多样化数据集上训练的模型。</li>
<li>在ImageNet上训练的扩散模型在ZS-VOS任务上的表现优于其他数据集训练的模型。</li>
<li>该方法达到了与在昂贵的图像分割数据集上训练的模型相当的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d016d82726907510a3ef651b318f3db2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2e2578f0da1ebfdcbf9dc4510c57a91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7866760aa12e58057dcc885e7691de4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36fcb105bb87e0fab00abc0e7d99eaad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1bb14e4e9837e168f6f1b48ee00ec6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="REWIND-Real-Time-Egocentric-Whole-Body-Motion-Diffusion-with-Exemplar-Based-Identity-Conditioning"><a href="#REWIND-Real-Time-Egocentric-Whole-Body-Motion-Diffusion-with-Exemplar-Based-Identity-Conditioning" class="headerlink" title="REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with   Exemplar-Based Identity Conditioning"></a>REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with   Exemplar-Based Identity Conditioning</h2><p><strong>Authors:Jihyun Lee, Weipeng Xu, Alexander Richard, Shih-En Wei, Shunsuke Saito, Shaojie Bai, Te-Li Wang, Minhyuk Sung, Tae-Kyun Kim, Jason Saragih</strong></p>
<p>We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a one-step diffusion model for real-time, high-fidelity human motion estimation from egocentric image inputs. While an existing method for egocentric whole-body (i.e., body and hands) motion estimation is non-real-time and acausal due to diffusion-based iterative motion refinement to capture correlations between body and hand poses, REWIND operates in a fully causal and real-time manner. To enable real-time inference, we introduce (1) cascaded body-hand denoising diffusion, which effectively models the correlation between egocentric body and hand motions in a fast, feed-forward manner, and (2) diffusion distillation, which enables high-quality motion estimation with a single denoising step. Our denoising diffusion model is based on a modified Transformer architecture, designed to causally model output motions while enhancing generalizability to unseen motion lengths. Additionally, REWIND optionally supports identity-conditioned motion estimation when identity prior is available. To this end, we propose a novel identity conditioning method based on a small set of pose exemplars of the target identity, which further enhances motion estimation quality. Through extensive experiments, we demonstrate that REWIND significantly outperforms the existing baselines both with and without exemplar-based identity conditioning. </p>
<blockquote>
<p>我们提出了 REWIND（实时以自我为中心的全身动作扩散），这是一种一步扩散模型，用于从自我中心的图像输入中进行实时、高保真的人类动作估计。现有的以自我为中心的全身（即身体和手）动作估计方法由于基于扩散的迭代动作细化来捕捉身体和手部姿势之间的相关性，因此是非实时的和非因果的，而 REWIND 则以完全因果和实时的方式进行操作。为了实现实时推理，我们引入了（1）级联的身体手部去噪扩散，它以快速前馈的方式有效地对自我中心的身体和手部动作进行建模；（2）扩散蒸馏，它能在一步去噪过程中实现高质量的动作估计。我们的去噪扩散模型基于改进的 Transformer 架构，旨在因果地模拟输出动作，同时提高对不同长度动作的泛化能力。另外，当提供身份先验信息时，REWIND 还支持有条件身份的动作估计。为此，我们提出了一种基于目标身份的小规模姿态范例集的新型身份条件方法，这进一步提高了动作估计的质量。通过广泛的实验，我们证明 REWIND 在有无范例集身份条件下都显著优于现有基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04956v2">PDF</a> Accepted to CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://jyunlee.github.io/projects/rewind/">https://jyunlee.github.io/projects/rewind/</a></p>
<p><strong>Summary</strong></p>
<p>REWIND是一个一步扩散模型，可从第一人称图像输入中进行实时、高保真的人体运动估计。与现有方法相比，它实现了全身（身体和手部）运动的实时因果推断。通过引入级联身体手部去噪扩散和扩散蒸馏技术，模型能够快速地前向建模身体和手部的运动关联，并用单个去噪步骤实现高质量的运动估计。此外，REWIND还支持在有身份先验的情况下进行身份条件运动估计，并提出了一种基于目标身份姿势范例的新型身份条件方法，进一步提高运动估计质量。实验证明，REWIND在无和有条件例示范例的情况下都显著优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REWIND是一个用于实时高保真人体运动估计的一步扩散模型，可从第一人称图像输入中进行推断。</li>
<li>与现有方法相比，REWIND实现了全身运动的实时因果推断。</li>
<li>通过引入级联身体手部去噪扩散，REWIND能够快速地前向建模身体和手部的运动关联。</li>
<li>扩散蒸馏技术使REWIND能够在单个去噪步骤中实现高质量的运动估计。</li>
<li>REWIND支持身份条件运动估计，进一步提高运动估计质量。</li>
<li>提出了一种基于目标身份姿势范例的新型身份条件方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb449c522916a0fee2f4a8b27a8dd0b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e81f98747d86474e3f1cbd299db9509.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Privacy-Attacks-on-Image-AutoRegressive-Models"><a href="#Privacy-Attacks-on-Image-AutoRegressive-Models" class="headerlink" title="Privacy Attacks on Image AutoRegressive Models"></a>Privacy Attacks on Image AutoRegressive Models</h2><p><strong>Authors:Antoni Kowalczuk, Jan Dubiński, Franziska Boenisch, Adam Dziedzic</strong></p>
<p>Image autoregressive generation has emerged as a powerful new paradigm, with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns about their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to those of DMs as a reference point. Specifically, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images, with a True Positive Rate at False Positive Rate &#x3D; 1% (TPR@FPR&#x3D;1%) of 86.38%, compared to just 6.38% for DMs using comparable attacks. We leverage our novel MIA to perform dataset inference (DI) for IARs and show that it requires as few as 6 samples to detect dataset membership, compared to 200 samples for DI in DMs. This confirms a higher level of information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-d30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are empirically significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. This trend suggests that incorporating techniques from DMs into IARs, such as modeling the per-token probability distribution using a diffusion procedure, could help mitigate IARs’ vulnerability to privacy attacks. We make our code available at: <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a> </p>
<blockquote>
<p>图像自回归生成已经成为一种强大的新范式，图像自回归模型（IARs）在图像质量方面与最先进的扩散模型（DMs）相匹配（FID：1.48对1.58），同时允许更高的生成速度。然而，与IARs相关的隐私风险尚未得到探索，这引发了对其负责任部署的担忧。为了填补这一空白，我们对IARs进行了全面的隐私分析，并将其与DMs的隐私风险进行比较，作为参考点。具体来说，我们开发了一种新型的成员推理攻击（MIA），该攻击在检测训练图像方面实现了非常高的成功率，在假阳性率（FPR）为1%的情况下，真阳性率（TPR）达到86.38%，相比之下，使用类似攻击的DMs仅为6.38%。我们利用新型MIA对IARs进行数据集推理（DI），并表明仅需6个样本即可检测数据集成员身份，相比之下，DMs进行DI需要200个样本。这证实了IARs中信息泄露程度较高。最后，我们能够从IAR中提取数百个训练数据点（例如，从VAR-d30中提取698个）。我们的结果表明存在基本的隐私效用权衡：尽管IAR在图像生成质量和速度方面表现出色，但与实现类似性能的DMs相比，它们在实际中更容易受到隐私攻击。这一趋势表明，将DMs的技术融入IARs中，例如使用扩散过程对每令牌概率分布进行建模，可能有助于减轻IARs对隐私攻击的脆弱性。我们的代码可在：<a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/sprintml/privacy_attacks_against_iars访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02514v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了图像自回归生成模型（IARs）的隐私风险问题。研究发现，相较于扩散模型（DMs），IARs在图像生成质量和速度上表现优异，但同时也存在更高的隐私泄露风险。文章提出了一种新型成员推理攻击（MIA），该攻击在检测训练图像方面表现出极高的成功率，并证实了IARs在数据集推理（DI）方面的信息泄露更为严重。研究指出，虽然IARs具有优秀的性能，但在隐私保护方面存在显著缺陷，并提出将DMs的技术融入IARs中，以缓解其隐私攻击漏洞。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像自回归生成模型（IARs）与扩散模型（DMs）相比，在图像生成质量和速度上具有优势。</li>
<li>IARs存在较高的隐私泄露风险，尚未得到充分探索。</li>
<li>提出了一种新型成员推理攻击（MIA），在检测IARs的训练图像方面表现出极高成功率。</li>
<li>IARs在数据集推理（DI）方面的信息泄露更为严重，仅需少量样本即可检测数据集成员。</li>
<li>IARs在隐私保护方面存在显著缺陷，相较DMs更易受到隐私攻击。</li>
<li>相比DMs，IARs的隐私泄露问题凸显了隐私与效用的权衡。</li>
<li>将DMs的技术融入IARs中，有助于缓解IARs的隐私攻击漏洞。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3bf8cb90fd82ad9b91a3f087acb0698b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b62fdde08f1ecb2ec199d326c14d19.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Fast-constrained-sampling-in-pre-trained-diffusion-models"><a href="#Fast-constrained-sampling-in-pre-trained-diffusion-models" class="headerlink" title="Fast constrained sampling in pre-trained diffusion models"></a>Fast constrained sampling in pre-trained diffusion models</h2><p><strong>Authors:Alexandros Graikos, Nebojsa Jojic, Dimitris Samaras</strong></p>
<p>Large denoising diffusion models, such as Stable Diffusion, have been trained on billions of image-caption pairs to perform text-conditioned image generation. As a byproduct of this training, these models have acquired general knowledge about image statistics, which can be useful for other inference tasks. However, when confronted with sampling an image under new constraints, e.g. generating the missing parts of an image, using large pre-trained text-to-image diffusion models is inefficient and often unreliable. Previous approaches either utilize backpropagation, making them significantly slower and more memory-demanding than text-to-image inference, or only enforce the constraint locally, failing to capture critical long-range correlations. In this work, we propose an algorithm that enables fast and high-quality generation under arbitrary constraints. We observe that, during inference, we can interchange between gradient updates computed on the noisy image and updates computed on the final, clean image. This allows us to employ a numerical approximation to expensive gradient computations, incurring significant speed-ups in inference. Our approach produces results that rival or surpass the state-of-the-art training-free inference approaches while requiring a fraction of the time. We demonstrate the effectiveness of our algorithm under both linear and non-linear constraints. An implementation is provided at <a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling">https://github.com/cvlab-stonybrook/fast-constrained-sampling</a>. </p>
<blockquote>
<p>大型去噪扩散模型（如Stable Diffusion）已经通过对数十亿图像标题对进行训练，实现了文本条件图像生成。作为这种训练的一个副产品，这些模型已经获得了关于图像统计的通用知识，这对于其他推理任务可能是有用的。然而，当面临在新的约束下采样图像时，例如生成图像的缺失部分，使用大型预训练文本到图像扩散模型是低效且经常不可靠的。以前的方法要么使用反向传播，这使得它们比文本到图像的推理过程更慢且更占内存，要么只局部实施约束，无法捕捉重要的长程相关性。在这项工作中，我们提出了一种能够在任意约束下快速生成高质量图像的算法。我们观察到，在推理过程中，我们可以在嘈杂图像上计算的梯度更新和最终干净图像上计算的更新之间进行互换。这使我们能够采用昂贵的梯度计算的数值近似，从而在推理过程中实现显著的速度提升。我们的方法产生的结果可与或超越最新的无训练推理方法相媲美，同时所需时间大大减少。我们在线性约束和非线性约束下都证明了算法的有效性。一个实现示例可在[<a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/cvlab-stonybrook/fast-constrained-sampling找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18804v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型去噪扩散模型（如Stable Diffusion）经过数十亿图像标题对训练，可进行文本条件图像生成。模型在训练过程中获得了有关图像统计的通用知识，这对其推理任务有益。然而，对于在新的约束条件下采样图像（例如生成图像的缺失部分），使用大型预训练文本到图像扩散模型效率较低且常不可靠。先前的方法要么使用反向传播，使其比文本到图像推理更慢且更占内存，要么仅局部实施约束，无法捕捉关键的长程关联。本研究提出了一种算法，可在任意约束下实现快速、高质量的生成。我们观察到在推理过程中，可以在嘈杂图像上计算的梯度更新和最终清洁图像上计算的更新之间进行交换。这使我们能够采用昂贵的梯度计算的数值逼近，在推理中实现了显著的速度提升。我们的算法在与训练无关的推理方法中实现了良好的效果。实现地址：<a target="_blank" rel="noopener" href="https://github.com/cvlab-stonybrook/fast-constrained-sampling%E3%80%82">https://github.com/cvlab-stonybrook/fast-constrained-sampling。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型去噪扩散模型如Stable Diffusion已训练于数十亿图像标题对，支持文本条件图像生成。</li>
<li>模型在训练中获取了图像统计的通用知识，有益于其他推理任务。</li>
<li>面对新约束条件采样图像时，现有方法效率不高且常不可靠。</li>
<li>先前方法使用反向传播导致速度慢、内存需求大，或仅局部实施约束而无法捕捉长程关联。</li>
<li>本研究提出一种算法，能在任意约束下实现快速高质量的图像生成。</li>
<li>推理过程中可交换嘈杂图像和清洁图像上的梯度更新。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5e4d7161358901f069e7f70de7da3b51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f19e3248130b36a126b7e7769b8f8858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4095c92f962c497caff5e369bf33283a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe35ca1e67106364e4c2eefa799b5660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2264b9c8e2d87f6263ecc12d85842a74.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Continuous-Diffusion-for-Mixed-Type-Tabular-Data"><a href="#Continuous-Diffusion-for-Mixed-Type-Tabular-Data" class="headerlink" title="Continuous Diffusion for Mixed-Type Tabular Data"></a>Continuous Diffusion for Mixed-Type Tabular Data</h2><p><strong>Authors:Markus Mueller, Kathrin Gruber, Dennis Fok</strong></p>
<p>Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular Data. CDTD is based on a novel combination of score matching and score interpolation to enforce a unified continuous noise distribution for both continuous and categorical features. We explicitly acknowledge the necessity of homogenizing distinct data types by relying on model-specific loss calibration and initialization schemes.To further address the high heterogeneity in mixed-type tabular data, we introduce adaptive feature- or type-specific noise schedules. These ensure balanced generative performance across features and optimize the allocation of model capacity across features and diffusion time. Our experimental results show that CDTD consistently outperforms state-of-the-art benchmark models, captures feature correlations exceptionally well, and that heterogeneity in the noise schedule design boosts sample quality. Replication code is available at <a target="_blank" rel="noopener" href="https://github.com/muellermarkus/cdtd">https://github.com/muellermarkus/cdtd</a>. </p>
<blockquote>
<p>基于分数生成模型，通常被称为扩散模型，已经成功应用于文本和图像数据的生成。然而，它们在混合类型表格数据中的应用仍然未被充分探索。在这项工作中，我们提出了CDTD，这是一种用于混合类型表格数据的连续扩散模型。CDTD基于分数匹配和分数插值的组合，以强制执行统一连续噪声分布，适用于连续和分类特征。我们明确承认通过依赖模型特定的损失校准和初始化方案来统一不同类型数据的必要性。为了进一步解决混合类型表格数据中的高异质性，我们引入了自适应特征或类型特定的噪声时间表。这些确保了跨特征的平衡生成性能，并优化了模型容量在特征和扩散时间上的分配。我们的实验结果表明，CDTD持续优于最新基准模型，异常捕捉特征相关性，并且噪声时间表设计中的异质性提高了样本质量。复现代码可在<a target="_blank" rel="noopener" href="https://github.com/muellermarkus/cdtd%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/muellermarkus/cdtd找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10431v5">PDF</a> published at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>扩散模型在文本和图像数据生成方面取得了显著成功，但在混合类型表格数据的应用上仍待探索。本文提出CDTD，一种用于混合类型表格数据的连续扩散模型。CDTD通过结合分数匹配和分数插值来强制执行统一连续噪声分布，适用于连续和分类特征。为应对混合类型表格数据中的高异质性，引入自适应特征或类型特定噪声时间表。实验结果表明，CDTD持续优于最新基准模型，特征关联捕捉出色，且噪声时间表设计的异质性提升了样本质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在生成文本和图像数据方面表现出色，但在处理混合类型表格数据时仍面临挑战。</li>
<li>CDTD是一种用于混合类型表格数据的连续扩散模型，通过强制执行统一连续噪声分布来处理不同类型的数据特征。</li>
<li>CDTD结合了分数匹配和分数插值技术，以提高模型的性能。</li>
<li>为应对混合类型表格数据中的高异质性，CDTD引入了自适应特征或类型特定的噪声时间表。</li>
<li>实验结果表明，CDTD在生成混合类型表格数据方面优于其他最新模型。</li>
<li>CDTD能够很好地捕捉特征之间的关联。</li>
<li>噪声时间表设计的异质性对样本质量有积极影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.10431">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8a7df2ba9e1eb8e1b9ff21d4eb0d6f95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c39f935500cd684bfb632aca667e8863.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e92d19d47564f9348c29ab736468571.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7b9cbee8aede2cf6c6cba8321a8a4016.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-10  HRMedSeg Unlocking High-resolution Medical Image segmentation via   Memory-efficient Attention Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d763b980211d4de561689d8554325759.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-10  Meta-Continual Learning of Neural Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">22963.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
