<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  FEABench Evaluating Language Models on Multiphysics Reasoning Ability">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-06c787a864f6e3acdfc3f2adff5bc6fc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-10-æ›´æ–°"><a href="#2025-04-10-æ›´æ–°" class="headerlink" title="2025-04-10 æ›´æ–°"></a>2025-04-10 æ›´æ–°</h1><h2 id="FEABench-Evaluating-Language-Models-on-Multiphysics-Reasoning-Ability"><a href="#FEABench-Evaluating-Language-Models-on-Multiphysics-Reasoning-Ability" class="headerlink" title="FEABench: Evaluating Language Models on Multiphysics Reasoning Ability"></a>FEABench: Evaluating Language Models on Multiphysics Reasoning Ability</h2><p><strong>Authors:Nayantara Mudur, Hao Cui, Subhashini Venugopalan, Paul Raccuglia, Michael P. Brenner, Peter Norgaard</strong></p>
<p>Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMsâ€™ reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at <a target="_blank" rel="noopener" href="https://github.com/google/feabench">https://github.com/google/feabench</a> </p>
<blockquote>
<p>æ„å»ºç²¾ç¡®æ¨¡æ‹Ÿç°å®ä¸–ç•Œå¹¶è°ƒç”¨æ•°å€¼æ±‚è§£å™¨è§£å†³å®šé‡é—®é¢˜æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„æœ¬è´¨è¦æ±‚ã€‚æˆ‘ä»¬æ¨å‡ºäº†FEABenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†ä½¿ç”¨æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰æ¨¡æ‹Ÿå’Œè§£å†³ç‰©ç†ã€æ•°å­¦å’Œå·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„æ¨ç†æ¥è°ƒæŸ¥LLMè§£å†³è¿™äº›é—®é¢˜çš„èƒ½åŠ›ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°è¿›è¡Œæ¨ç†å¹¶æ“ä½œCOMSOL MultiphysicsÂ®è¿™ä¸€æœ‰é™å…ƒåˆ†æè½¯ä»¶æ¥è®¡ç®—ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ä»£ç†ï¼Œè¯¥ä»£ç†å…·å¤‡é€šè¿‡è½¯ä»¶çš„åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰è¿›è¡Œäº¤äº’çš„èƒ½åŠ›ï¼Œæ£€æŸ¥å…¶è¾“å‡ºå¹¶ä½¿ç”¨å·¥å…·åœ¨å¤šæ¬¡è¿­ä»£ä¸­æ”¹è¿›å…¶è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„ç­–ç•¥ç”Ÿæˆå¯æ‰§è¡ŒAPIè°ƒç”¨çš„æ¬¡æ•°è¾¾åˆ°88%ã€‚èƒ½å¤ŸæˆåŠŸä¸æœ‰é™å…ƒåˆ†æè½¯ä»¶äº¤äº’å¹¶æ“ä½œä»¥è§£å†³è¯¸å¦‚æˆ‘ä»¬åŸºå‡†æµ‹è¯•ä¸­é—®é¢˜ç­‰çŠ¶å†µçš„LLMï¼Œå°†æ¨åŠ¨å·¥ç¨‹è‡ªåŠ¨åŒ–é¢†åŸŸçš„è¾¹ç•Œã€‚è·å¾—è¿™ç§èƒ½åŠ›å°†ä½¿LLMçš„æ¨ç†æŠ€èƒ½ä¸æ•°å€¼æ±‚è§£å™¨çš„ç²¾ç¡®åº¦ç›¸ç»“åˆï¼Œæ¨åŠ¨å¼€å‘èƒ½å¤Ÿåº”å¯¹ç°å®ä¸–ç•Œå¤æ‚é—®é¢˜çš„è‡ªä¸»ç³»ç»Ÿçš„å‘å±•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/google/feabench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/google/feabenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06260v1">PDF</a> 39 pages. Accepted at the NeurIPS 2024 Workshops on Mathematical   Reasoning and AI and Open-World Agents</p>
<p><strong>Summary</strong></p>
<p>FEABenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŠLLMä»£ç†æ¨¡æ‹Ÿå’Œè§£å†³ç‰©ç†ã€æ•°å­¦åŠå·¥ç¨‹é—®é¢˜èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒè¿ç”¨æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰åŠCOMSOL Multiphysicsè½¯ä»¶æ¥è§£ç­”é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥ç ”ç©¶LLMsç«¯åˆ°ç«¯è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œé€šè¿‡æ¨ç†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å¹¶æ“ä½œFEAè½¯ä»¶è®¡ç®—ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ä»£ç†ï¼Œå¯é€è¿‡è½¯ä»¶çš„åº”ç”¨ç¨‹åºæ¥å£ï¼ˆAPIï¼‰äº’åŠ¨ã€æ£€è§†è¾“å‡ºå¹¶ä½¿ç”¨å·¥å…·æ”¹å–„å¤šæ¬¡è¿­ä»£çš„è§£å†³æ–¹æ¡ˆã€‚æœ€ä½³ç­–ç•¥ç”Ÿæˆçš„å¯æ‰§è¡ŒAPIè°ƒç”¨å‡†ç¡®ç‡è¾¾åˆ°äº†88%ã€‚æ­¤èƒ½åŠ›å°†æ¨åŠ¨å·¥ç¨‹è‡ªåŠ¨åŒ–å‰æ²¿ï¼Œæå‡LLMsä¸æ•°å€¼è§£ç®—å™¨çš„ç»“åˆæ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨é’ˆå¯¹å¤æ‚ç°å®é—®é¢˜çš„è‡ªä¸»ç³»ç»Ÿå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FEABenchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨¡æ‹Ÿä¸è§£å†³é—®é¢˜èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯åœ¨å·¥ç¨‹å’Œç§‘å­¦é¢†åŸŸçš„å®šé‡é—®é¢˜ä¸Šã€‚</li>
<li>è¯¥æµ‹è¯•åˆ©ç”¨æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰å’ŒCOMSOL Multiphysicsè½¯ä»¶æ¥è®¾è®¡å’Œè§£å†³ç‰©ç†ã€æ•°å­¦å’Œå·¥ç¨‹é—®é¢˜ã€‚</li>
<li>LLMséœ€é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°è¿›è¡Œæ¨ç†ï¼Œå¹¶æ“ä½œFEAè½¯ä»¶ä»¥è®¡ç®—ç­”æ¡ˆï¼Œä½“ç°å…¶ç«¯åˆ°ç«¯çš„è§£å†³é—®é¢˜èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œè¯¥ä»£ç†èƒ½é€šè¿‡è½¯ä»¶APIè¿›è¡Œäº’åŠ¨ï¼Œå¹¶èƒ½æ”¹å–„è§£å†³æ–¹æ¡ˆçš„å¤šæ¬¡è¿­ä»£ã€‚</li>
<li>æœ€ä½³ç­–ç•¥ç”Ÿæˆçš„APIè°ƒç”¨å‡†ç¡®ç‡è¾¾åˆ°äº†88%ï¼Œæ˜¾ç¤ºå‡ºè¾ƒé«˜çš„è‡ªåŠ¨åŒ–æ½œåŠ›ã€‚</li>
<li>LLMsç»“åˆæ•°å€¼è§£ç®—å™¨çš„èƒ½åŠ›å°†æ¨åŠ¨å·¥ç¨‹è‡ªåŠ¨åŒ–çš„å‘å±•ï¼Œå¹¶æå‡è‡ªä¸»ç³»ç»Ÿè§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0e6eb3a08e2c216f7adb37ea59031ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e009ee066f8bace188cbd33c6115987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fffaff2da821dec3273000a5a18c2ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f6f59f186d32a73947d11cb7fb7a983.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-869c11750ebea92b48b2b043ad5fdd18.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Transfer-between-Modalities-with-MetaQueries"><a href="#Transfer-between-Modalities-with-MetaQueries" class="headerlink" title="Transfer between Modalities with MetaQueries"></a>Transfer between Modalities with MetaQueries</h2><p><strong>Authors:Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, Saining Xie</strong></p>
<p>Unified multimodal models aim to integrate understanding (text output) and generation (pixel output), but aligning these different modalities within a single architecture often demands complex training recipes and careful data balancing. We introduce MetaQueries, a set of learnable queries that act as an efficient interface between autoregressive multimodal LLMs (MLLMs) and diffusion models. MetaQueries connects the MLLMâ€™s latents to the diffusion decoder, enabling knowledge-augmented image generation by leveraging the MLLMâ€™s deep understanding and reasoning capabilities. Our method simplifies training, requiring only paired image-caption data and standard diffusion objectives. Notably, this transfer is effective even when the MLLM backbone remains frozen, thereby preserving its state-of-the-art multimodal understanding capabilities while achieving strong generative performance. Additionally, our method is flexible and can be easily instruction-tuned for advanced applications such as image editing and subject-driven generation. </p>
<blockquote>
<p>ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹æ—¨åœ¨èåˆç†è§£å’Œç”Ÿæˆï¼ˆåˆ†åˆ«ä¸ºæ–‡æœ¬è¾“å‡ºå’Œåƒç´ è¾“å‡ºï¼‰ï¼Œä½†åœ¨å•ä¸€æ¶æ„å†…å¯¹é½è¿™äº›ä¸åŒçš„æ¨¡æ€é€šå¸¸éœ€è¦å¤æ‚çš„è®­ç»ƒé…æ–¹å’Œç»†è‡´çš„æ•°æ®å¹³è¡¡ã€‚æˆ‘ä»¬å¼•å…¥äº†MetaQueriesï¼Œè¿™æ˜¯ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢ï¼Œä½œä¸ºè‡ªå›å½’å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆæ¥å£ã€‚MetaQuerieså°†MLLMçš„æ½œåœ¨ç©ºé—´è¿æ¥åˆ°æ‰©æ•£è§£ç å™¨ï¼Œé€šè¿‡åˆ©ç”¨MLLMçš„æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ç°çŸ¥è¯†å¢å¼ºå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒï¼Œåªéœ€è¦é…å¯¹å›¾åƒå’Œå­—å¹•æ•°æ®ä»¥åŠæ ‡å‡†æ‰©æ•£ç›®æ ‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿MLLMçš„éª¨å¹²ä¿æŒå†»ç»“çŠ¶æ€ï¼Œè¿™ç§è½¬ç§»ä»ç„¶æœ‰æ•ˆï¼Œä»è€Œä¿ç•™å…¶æœ€å…ˆè¿›çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶å®ç°å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çµæ´»ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°é€šè¿‡æŒ‡ä»¤å¾®è°ƒç”¨äºé«˜çº§åº”ç”¨ï¼Œå¦‚å›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06256v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://xichenpan.com/metaquery">https://xichenpan.com/metaquery</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†MetaQueriesï¼Œè¿™æ˜¯ä¸€ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢ï¼Œä½œä¸ºè‡ªå›å½’å¤šæ¨¡æ€LLMï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆæ¥å£ã€‚MetaQueriesèƒ½å¤Ÿå°†MLLMçš„æ½œå˜é‡ä¸æ‰©æ•£è§£ç å™¨è¿æ¥èµ·æ¥ï¼Œåˆ©ç”¨MLLMçš„æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›å®ç°çŸ¥è¯†å¢å¼ºçš„å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œåªéœ€é…å¯¹å›¾åƒå’Œå­—å¹•æ•°æ®ä»¥åŠæ ‡å‡†æ‰©æ•£ç›®æ ‡å³å¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çµæ´»æ€§å¼ºï¼Œæ˜“äºè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œé€‚ç”¨äºå›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆç­‰é«˜çº§åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaQueriesæ˜¯ä¸€ç§å¯å­¦ä¹ çš„æŸ¥è¯¢ï¼Œç”¨äºè¿æ¥è‡ªå›å½’å¤šæ¨¡æ€LLMsï¼ˆMLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œå®ç°ä¸åŒæ¨¡æ€ä¹‹é—´çš„æœ‰æ•ˆæ•´åˆã€‚</li>
<li>MetaQuerieså°†MLLMçš„æ½œå˜é‡ä¸æ‰©æ•£è§£ç å™¨è¿æ¥ï¼Œåˆ©ç”¨MLLMçš„æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›è¿›è¡ŒçŸ¥è¯†å¢å¼ºçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œåªéœ€ä½¿ç”¨é…å¯¹å›¾åƒå’Œå­—å¹•æ•°æ®ä»¥åŠæ ‡å‡†æ‰©æ•£ç›®æ ‡ã€‚</li>
<li>MLLMçš„éª¨å¹²ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥ä¿æŒå†»ç»“çŠ¶æ€ï¼Œä»è€Œä¿ç•™å…¶æœ€æ–°çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›å’Œå¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰çµæ´»æ€§ï¼Œèƒ½å¤Ÿè½»æ¾é€‚åº”æŒ‡ä»¤å¾®è°ƒï¼Œé€‚ç”¨äºå›¾åƒç¼–è¾‘å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆç­‰é«˜çº§åº”ç”¨ã€‚</li>
<li>é€šè¿‡MetaQueriesï¼Œæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨LLMçš„ç†è§£èƒ½åŠ›æ¥å¢å¼ºå›¾åƒçš„ç”Ÿæˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-34c0bdb455721760acdffaf8afa68fd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bd7083f5108116326a8d0d4ddb5ce14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56008693de8fb99b52c62bf6f3e4a3f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a06893fb108d01abee6e848089fed5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dc37d9fdb8b36c2945169b15509e894.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LExT-Towards-Evaluating-Trustworthiness-of-Natural-Language-Explanations"><a href="#LExT-Towards-Evaluating-Trustworthiness-of-Natural-Language-Explanations" class="headerlink" title="LExT: Towards Evaluating Trustworthiness of Natural Language   Explanations"></a>LExT: Towards Evaluating Trustworthiness of Natural Language   Explanations</h2><p><strong>Authors:Krithi Shailya, Shreya Rajpal, Gokul S Krishnan, Balaraman Ravindran</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT) (The code and set up to reproduce our experiments are publicly available at <a target="_blank" rel="noopener" href="https://github.com/cerai-iitm/LExT">https://github.com/cerai-iitm/LExT</a>). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸçš„é›†æˆåº¦ä¸æ–­æé«˜ï¼Œå·²ç»æå‡ºäº†å‡ ç§ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šçš„æ–¹æ³•ã€‚è¿™äº›è§£é‡Šå¯¹äºæé«˜æ¨¡å‹çš„è§£é‡Šæ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ç­‰é€æ˜åº¦å’Œå¯é æ€§è‡³å…³é‡è¦çš„æ•æ„Ÿé¢†åŸŸã€‚é‰´äºLLMç”Ÿæˆçš„è§£é‡ŠåŠå…¶å·²çŸ¥çš„å…³æ³¨ç‚¹ï¼Œå¯¹è¯„ä¼°æ¨¡å‹ç”Ÿæˆè§£é‡Šçš„ç¨³å¥è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡ï¼Œå¦‚BLEUå’ŒROUGEï¼Œå¯ä»¥æ•æ‰è¯­æ³•å’Œè¯­ä¹‰å‡†ç¡®æ€§ï¼Œä½†å¿½ç•¥äº†äº‹å®å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œå¿ è¯šæ€§ç­‰å…¶ä»–æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé‡åŒ–è‡ªç„¶è¯­è¨€è§£é‡Šå¯ä¿¡åº¦çš„é€šç”¨æ¡†æ¶ï¼Œå¹³è¡¡äº†å¯ä¿¡åº¦å’Œå¿ è¯šåº¦ï¼Œä»¥å¾—å‡ºå…¨é¢çš„è¯­è¨€è§£é‡Šå¯ä¿¡åº¦åˆ†æ•°ï¼ˆLExTï¼‰ï¼ˆæˆ‘ä»¬çš„å®éªŒä»£ç å’Œè®¾ç½®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cerai-iitm/LExT%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%EF%BC%89%E3%80%82%E5%88%A9%E7%94%A8%E6%88%91%E4%BB%AC%E7%9A%84%E9%A2%86%E5%9F%9F%E9%80%9A%E7%94%A8%E6%A1%86%E6%9E%B6%E5%BA%94%E7%94%A8%E4%BA%8E%E5%8C%BB%E7%96%97%E9%A2%86%E5%9F%9F%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%85%AC%E5%85%B1%E5%8C%BB%E5%AD%A6%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%84%E4%BC%B0%E4%BA%86%E5%85%AD%E4%B8%AA%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8C%85%E6%8B%AC%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E7%BB%93%E6%9E%9C%E8%A1%A8%E6%98%8E%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E5%9C%A8%E7%94%9F%E6%88%90%E5%8F%AF%E4%BF%A1%E8%A7%A3%E9%87%8A%E6%96%B9%E9%9D%A2%E5%AD%98%E5%9C%A8%E6%98%BE%E8%91%97%E5%B7%AE%E5%BC%82%E3%80%82%E9%80%9A%E8%BF%87%E6%AF%94%E8%BE%83%E8%BF%99%E4%BA%9B%E8%A7%A3%E9%87%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E8%A7%82%E5%AF%9F%E5%88%B0%E4%BA%86%E4%B8%80%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E7%8E%B0%E8%B1%A1%EF%BC%8C%E5%A6%82%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BF%A0%E8%AF%9A%E5%BA%A6%E4%B8%8D%E4%B8%80%E8%87%B4%E4%BB%A5%E5%8F%8A%E5%AE%83%E4%BB%AC%E5%80%BE%E5%90%91%E4%BA%8E%E4%BC%98%E4%BA%8E%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E3%80%82%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%BC%BA%E8%B0%83%E4%BA%86%E4%BD%BF%E7%94%A8%E9%87%8F%E8%BA%AB%E5%AE%9A%E5%88%B6%E7%9A%84%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6%E6%9D%A5%E8%AF%84%E4%BC%B0%E6%95%8F%E6%84%9F%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%A7%A3%E9%87%8A%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%8C%E4%B8%BA%E6%94%B9%E5%96%84%E5%8C%BB%E7%96%97%E4%BF%9D%E5%81%A5%E5%92%8C%E8%B6%85%E8%B6%8A%E9%A2%86%E5%9F%9F%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%92%8C%E9%80%8F%E6%98%8E%E5%BA%A6%E6%8F%90%E4%BE%9B%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/cerai-iitm/LExTä¸Šå…¬å¼€è·å–ï¼‰ã€‚åˆ©ç”¨æˆ‘ä»¬çš„é¢†åŸŸé€šç”¨æ¡†æ¶åº”ç”¨äºåŒ»ç–—é¢†åŸŸï¼Œä½¿ç”¨å…¬å…±åŒ»å­¦æ•°æ®é›†è¯„ä¼°äº†å…­ä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹å’Œé€šç”¨æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå¯ä¿¡è§£é‡Šæ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›è§£é‡Šï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€äº›æœ‰è¶£çš„ç°è±¡ï¼Œå¦‚é€šç”¨æ¨¡å‹çš„å¿ è¯šåº¦ä¸ä¸€è‡´ä»¥åŠå®ƒä»¬å€¾å‘äºä¼˜äºç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œè¿›ä¸€æ­¥å¼ºè°ƒäº†ä½¿ç”¨é‡èº«å®šåˆ¶çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æ•æ„Ÿé¢†åŸŸä¸­çš„è‡ªç„¶è¯­è¨€è§£é‡Šçš„é‡è¦æ€§ï¼Œä¸ºæ”¹å–„åŒ»ç–—ä¿å¥å’Œè¶…è¶Šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹çš„å¯é æ€§å’Œé€æ˜åº¦æä¾›äº†åŸºç¡€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06227v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•æ„Ÿé¢†åŸŸå¦‚åŒ»ç–—ä¸­çš„è‡ªç„¶è¯­è¨€è§£é‡Šç”Ÿæˆçš„é‡è¦æ€§åŠå…¶è¯„ä»·éœ€æ±‚ã€‚æå‡ºäº†ä¸€ç§è¡¡é‡è¯­è¨€è§£é‡Šä¿¡ä»»åº¦çš„é€šç”¨æ¡†æ¶ï¼Œç»¼åˆè€ƒè™‘äº†å¯é¢„æµ‹æ€§å’Œå¿ å®æ€§ã€‚é€šè¿‡ä½¿ç”¨å…¬å¼€æ•°æ®é›†å¯¹åŒ»ç–—é¢†åŸŸçš„å…­ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨ç”Ÿæˆå¯ä¿¡è§£é‡Šæ–¹é¢çš„æ˜¾è‘—å·®å¼‚ã€‚å¼ºè°ƒä½¿ç”¨å®šåˆ¶è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æ•æ„Ÿé¢†åŸŸçš„è‡ªç„¶è¯­è¨€è§£é‡Šçš„é‡è¦æ€§ï¼Œä¸ºæé«˜è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ç­‰é¢†åŸŸçš„å¯ä¿¡åº¦å’Œé€æ˜åº¦å¥ å®šåŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•æ„Ÿé¢†åŸŸå¦‚åŒ»ç–—ä¸­ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šçš„é‡è¦æ€§ã€‚</li>
<li>å½“å‰è¯„ä¼°è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€è§£é‡Šçš„éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¡¡é‡è¯­è¨€è§£é‡Šä¿¡ä»»åº¦çš„é€šç”¨æ¡†æ¶ï¼ŒåŒ…æ‹¬å¯é¢„æµ‹æ€§å’Œå¿ å®æ€§ä¸¤ä¸ªæ–¹é¢ã€‚</li>
<li>ä½¿ç”¨å…¬å¼€æ•°æ®é›†å¯¹åŒ»ç–—é¢†åŸŸçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨ç”Ÿæˆå¯ä¿¡è§£é‡Šæ–¹é¢çš„å·®å¼‚ã€‚</li>
<li>ä¸€èˆ¬ç›®çš„æ¨¡å‹åœ¨å¿ å®æ€§æ–¹é¢å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œæœ‰æ—¶åœ¨ç”Ÿæˆå¯ä¿¡è§£é‡Šæ–¹é¢è¡¨ç°ä¼˜äºç‰¹å®šé¢†åŸŸçš„ç²¾ç»†è°ƒæ•´æ¨¡å‹ã€‚</li>
<li>å¼ºè°ƒä½¿ç”¨å®šåˆ¶çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æ•æ„Ÿé¢†åŸŸçš„è‡ªç„¶è¯­è¨€è§£é‡Šçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5a8a94723b19a72470377a529f2a7e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-318bdf7596a6042e19f6a6b6445501eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f7f39042b589f7036d3e92b4e968e55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70c6738828c4233999d66b028fd071ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f65a8a5982c05341d9b7282f0ea1d5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74f71ff73ba8895a8e3785a5a3403e8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-128K-to-4M-Efficient-Training-of-Ultra-Long-Context-Large-Language-Models"><a href="#From-128K-to-4M-Efficient-Training-of-Ultra-Long-Context-Large-Language-Models" class="headerlink" title="From 128K to 4M: Efficient Training of Ultra-Long Context Large Language   Models"></a>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language   Models</h2><p><strong>Authors:Chejian Xu, Wei Ping, Peng Xu, Zihan Liu, Boxin Wang, Mohammad Shoeybi, Bo Li, Bryan Catanzaro</strong></p>
<p>Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and multimodal data. In this work, we introduce a efficient training recipe for building ultra-long context LLMs from aligned instruct model, pushing the boundaries of context lengths from 128K to 1M, 2M, and 4M tokens. Our approach leverages efficient continued pretraining strategies to extend the context window and employs effective instruction tuning to maintain the instruction-following and reasoning abilities. Our UltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves state-of-the-art performance across a diverse set of long-context benchmarks. Importantly, models trained with our approach maintain competitive performance on standard benchmarks, demonstrating balanced improvements for both long and short context tasks. We further provide an in-depth analysis of key design choices, highlighting the impacts of scaling strategies and data composition. Our findings establish a robust framework for efficiently scaling context lengths while preserving general model capabilities. We release all model weights at: <a target="_blank" rel="noopener" href="https://ultralong.github.io/">https://ultralong.github.io/</a>. </p>
<blockquote>
<p>é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›å¯¹äºå¹¿æ³›çš„åº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬æ–‡æ¡£å’Œè§†é¢‘ç†è§£ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ¨ç†æ—¶é—´ç¼©æ”¾ç­‰ï¼Œæ‰€æœ‰è¿™äº›åº”ç”¨éƒ½éœ€è¦æ¨¡å‹å¤„ç†å’Œæ¨ç†é•¿æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒé…æ–¹ï¼Œç”¨äºä»å¯¹é½çš„æŒ‡ä»¤æ¨¡å‹ä¸­æ„å»ºè¶…é•¿ä¸Šä¸‹æ–‡LLMï¼Œå°†ä¸Šä¸‹æ–‡é•¿åº¦çš„è¾¹ç•Œä»128Kæ¨è‡³1Mã€2Må’Œ4Mä»¤ç‰Œã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é«˜æ•ˆçš„æŒç»­é¢„è®­ç»ƒç­–ç•¥æ¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶é‡‡ç”¨æœ‰æ•ˆçš„æŒ‡ä»¤è°ƒæ•´æ¥ä¿æŒæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„UltraLong-8Bï¼ŒåŸºäºLlama3.1-Instructä¸æˆ‘ä»¬é…æ–¹æ„å»ºï¼Œåœ¨å¤šç§é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œé‡‡ç”¨æˆ‘ä»¬æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šä¿æŒç«äº‰åŠ›ï¼Œè¯æ˜åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡å’ŒçŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šéƒ½æœ‰å¹³è¡¡æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜å¯¹å…³é”®è®¾è®¡é€‰æ‹©è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œçªå‡ºäº†æ‰©å±•ç­–ç•¥å’Œæ•°æ®ç»„åˆçš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªç¨³å¥çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦åŒæ—¶ä¿ç•™é€šç”¨æ¨¡å‹èƒ½åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰æ¨¡å‹æƒé‡ï¼š<a target="_blank" rel="noopener" href="https://ultralong.github.io/">https://ultralong.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06214v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹äºè¶…é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›åœ¨å„ç§åº”ç”¨ä¸­çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬æ–‡æ¡£å’Œè§†é¢‘ç†è§£ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ¨ç†æ—¶é—´ç¼©æ”¾ç­‰ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒé…æ–¹ï¼Œç”¨äºæ„å»ºè¶…é•¿ä¸Šä¸‹æ–‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¯¹é½æŒ‡ä»¤æ¨¡å‹å®ç°ä¸Šä¸‹æ–‡é•¿åº¦çš„æ‰©å±•ï¼Œä»128Kæ¨å‘äº†1Mã€2Må’Œ4Mçš„ä»¤ç‰Œé•¿åº¦ã€‚è¯¥ç ”ç©¶é‡‡ç”¨äº†é«˜æ•ˆçš„æŒç»­é¢„è®­ç»ƒç­–ç•¥æ¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶é‡‡ç”¨æœ‰æ•ˆçš„æŒ‡ä»¤å¾®è°ƒæ¥ä¿æŒæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚UltraLong-8Bæ¨¡å‹åœ¨å¤šæ ·åŒ–çš„é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ã€‚è¯¥ç ”ç©¶è¡¨æ˜è®­ç»ƒçš„æ–¹æ³•ä¸ä»…åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­æœ‰æ‰€æ”¹è¿›ï¼Œåœ¨çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä¹Ÿæœ‰æ˜¾è‘—çš„å¹³è¡¡æ”¹è¿›ã€‚æ–‡ç« è¿˜å¯¹å…³é”®è®¾è®¡é€‰æ‹©è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¼ºè°ƒäº†æ‰©å±•ç­–ç•¥å’Œæ•°æ®å¤„ç†çš„å½±å“ã€‚è¯¥ç ”ç©¶ä¸ºæœ‰æ•ˆåœ°æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦åŒæ—¶ä¿ç•™ä¸€èˆ¬æ¨¡å‹èƒ½åŠ›æä¾›äº†ç¨³å¥çš„æ¡†æ¶ã€‚æ‰€æœ‰æ¨¡å‹æƒé‡å‡å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://ultralong.github.io/%E3%80%82">https://ultralong.github.io/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºè°ƒé•¿ä¸Šä¸‹æ–‡èƒ½åŠ›åœ¨å„ç§åº”ç”¨ä¸­çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬æ–‡æ¡£å’Œè§†é¢‘ç†è§£ç­‰ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒé…æ–¹ï¼Œç”¨äºæ„å»ºè¶…é•¿ä¸Šä¸‹æ–‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°ä¸Šä¸‹æ–‡é•¿åº¦çš„æ˜¾è‘—æ‰©å±•ã€‚</li>
<li>é€šè¿‡æŒç»­é¢„è®­ç»ƒç­–ç•¥å’ŒæŒ‡ä»¤å¾®è°ƒæ¥ä¿æŒæ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>UltraLong-8Bæ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ã€‚</li>
<li>è®­ç»ƒæ–¹æ³•ä¸ä»…é€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œè¿˜èƒ½åœ¨çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å®ç°å¹³è¡¡æ”¹è¿›ã€‚</li>
<li>æ·±å…¥åˆ†æäº†å…³é”®è®¾è®¡é€‰æ‹©ï¼ŒåŒ…æ‹¬æ‰©å±•ç­–ç•¥å’Œæ•°æ®å¤„ç†çš„å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c706e19b84420da79d4af4550adb987b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76c189547299e244abb977c9f558d73c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44ecea6b249f4be7c4acfbb2c543af6f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="V-MAGE-A-Game-Evaluation-Framework-for-Assessing-Visual-Centric-Capabilities-in-Multimodal-Large-Language-Models"><a href="#V-MAGE-A-Game-Evaluation-Framework-for-Assessing-Visual-Centric-Capabilities-in-Multimodal-Large-Language-Models" class="headerlink" title="V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric   Capabilities in Multimodal Large Language Models"></a>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric   Capabilities in Multimodal Large Language Models</h2><p><strong>Authors:Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, Lijuan Wang</strong></p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CSU-JPG/V-MAGE">https://github.com/CSU-JPG/V-MAGE</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥å·²ç»åœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œéšç€è¯„ä¼°ä»é™æ€æ•°æ®é›†è½¬å‘å¼€æ”¾ä¸–ç•ŒåŠ¨æ€ç¯å¢ƒï¼Œå½“å‰çš„æ¸¸æˆåŸºå‡†æµ‹è¯•ä»ç„¶ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ— æ³•è¯„ä¼°ç°å®ä¸–ç•Œå†³ç­–æ‰€éœ€çš„å„ç§æ¨ç†æŠ€èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šèƒ½åŠ›æ¸¸æˆè¯„ä¼°ï¼ˆV-MAGEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¸¸æˆçš„è®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚V-MAGEåŒ…å«äº”ä¸ªä¸åŒçš„æ¸¸æˆå’Œè¶…è¿‡ä¸‰åä¸ªæ‰‹å·¥åˆ¶ä½œçš„å…³å¡ï¼Œæµ‹è¯•æ¨¡å‹åœ¨æ ¸å¿ƒè§†è§‰æŠ€èƒ½æ–¹é¢çš„å®šä½ã€è½¨è¿¹è·Ÿè¸ªã€æ—¶é—´æŠŠæ¡å’Œè§†è§‰è®°å¿†èƒ½åŠ›ï¼Œä»¥åŠé«˜çº§æ¨ç†èƒ½åŠ›å¦‚é•¿æœŸè§„åˆ’å’Œæ·±æ€ç†Ÿè™‘ã€‚æˆ‘ä»¬ä½¿ç”¨V-MAGEè¯„ä¼°é¢†å…ˆçš„MLLMsï¼Œæ­ç¤ºå®ƒä»¬åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æ‰€æœ‰æ¸¸æˆç¯å¢ƒä¸­ï¼Œæ ¹æ®åŸƒæ´›è¯„çº§æ¯”è¾ƒï¼Œè¡¨ç°æœ€ä½³çš„MLLMä¸äººç±»ç›¸æ¯”å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å…³é”®çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹çŠ¯çš„å„ç§ç±»å‹çš„æ„ŸçŸ¥é”™è¯¯ï¼Œå¹¶ä»ä»£ç†ä¸­å¿ƒè§†è§’æå‡ºäº†æ½œåœ¨çš„æ”¹è¿›æ–¹å‘ï¼Œå¦‚æ”¹è¿›ä»£ç†ç­–ç•¥å’Œè§£å†³æ„ŸçŸ¥ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/CSU-JPG/V-MAGE%E4%B8%8A%E3%80%82">https://github.com/CSU-JPG/V-MAGEä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06148v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œéšç€è¯„ä¼°ä»é™æ€æ•°æ®é›†è½¬å‘å¼€æ”¾ä¸–ç•ŒåŠ¨æ€ç¯å¢ƒï¼Œç°æœ‰çš„åŸºäºæ¸¸æˆçš„åŸºå‡†æµ‹è¯•æ˜¾å¾—æ‰è¥Ÿè§è‚˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹è§†è§‰ä¸­å¿ƒä»»åŠ¡å¹¶æ— æ³•è¯„ä¼°ç°å®ä¸–ç•Œå†³ç­–æ‰€éœ€çš„å¤šæ ·åŒ–æ¨ç†æŠ€èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†è§†è§‰ä¸­å¿ƒå¤šèƒ½åŠ›æ¸¸æˆè¯„ä¼°ï¼ˆV-MAGEï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¸¸æˆçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚V-MAGEåŒ…å«äº”ä¸ªä¸åŒçš„æ¸¸æˆå’Œ30å¤šä¸ªæ‰‹å·¥å®šåˆ¶çº§åˆ«ï¼Œæµ‹è¯•æ¨¡å‹åœ¨æ ¸å¿ƒè§†è§‰æŠ€èƒ½ä»¥åŠé•¿æœŸè§„åˆ’å’Œè¾©è®ºä¹‹ç±»çš„é«˜çº§æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨V-MAGEè¯„ä¼°é¢†å…ˆçš„MLLMsï¼Œæ­ç¤ºå…¶åœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æ‰€æœ‰æ¸¸æˆç¯å¢ƒä¸­ï¼Œæ ¹æ®åŸƒæ´›è¯„çº§æ¯”è¾ƒï¼Œè¡¨ç°æœ€ä½³çš„MLLMä¸äººç±»ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†å…³é”®çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹çŠ¯çš„å„ç§ç±»å‹çš„æ„ŸçŸ¥é”™è¯¯ï¼Œå¹¶ä»ä»£ç†ä¸­å¿ƒçš„è§’åº¦æå‡ºäº†æ½œåœ¨çš„æ”¹è¿›æ–¹å‘ï¼Œå¦‚æ”¹è¿›ä»£ç†ç­–ç•¥å’Œè§£å†³æ„ŸçŸ¥é”™è¯¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å½“å‰åŸºäºæ¸¸æˆçš„è¯„ä¼°ç¼ºä¹è§†è§‰ä¸­å¿ƒä»»åŠ¡ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥V-MAGEæ¡†æ¶ï¼ŒåŒ…å«äº”ä¸ªæ¸¸æˆå’Œå¤šä¸ªæ‰‹å·¥å®šåˆ¶çº§åˆ«ï¼Œç”¨äºè¯„ä¼°MLLMsçš„æ ¸å¿ƒè§†è§‰æŠ€èƒ½å’Œé«˜çº§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>V-MAGEæ­ç¤ºäº†MLLMåœ¨è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>è¡¨ç°æœ€ä½³çš„MLLMä¸äººç±»ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
<li>æ¨¡å‹å­˜åœ¨å¤šç§æ„ŸçŸ¥é”™è¯¯ï¼Œéœ€è¦æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-353ca58a8b6465aa1188389faf91d3ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a2147ac2368a45a888631a5db1acade.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5601e6c5597ef4da775d2caaae60cf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d77b9acc422b80dbac14e9393f9eafe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb6c151107fff219b5a6c4568ee96243.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Leanabell-Prover-Posttraining-Scaling-in-Formal-Reasoning"><a href="#Leanabell-Prover-Posttraining-Scaling-in-Formal-Reasoning" class="headerlink" title="Leanabell-Prover: Posttraining Scaling in Formal Reasoning"></a>Leanabell-Prover: Posttraining Scaling in Formal Reasoning</h2><p><strong>Authors:Jingyuan Zhang, Qi Wang, Xingguang Ji, Yahui Liu, Yang Yue, Fuzheng Zhang, Di Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1&#x2F;O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•çªæ˜¾äº†ä½¿ç”¨Lean 4ä»£ç è¿›è¡Œå½¢å¼æ¨ç†çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ­£å¦‚Open AI O1&#x2F;O3å’ŒDeepseek R1æ‰€å±•ç¤ºçš„ï¼Œæœ€è¿‘çš„è®­ç»ƒåæ‰©å±•å¹¶æœªå½»åº•æ”¹å˜ATPã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ATPçš„æ•´ä¸ªè®­ç»ƒåè¿‡ç¨‹ï¼Œæ—¨åœ¨å°†å…¶ä¸è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹çš„çªç ´æˆæœç›¸ç»“åˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ç”±ä¼—å¤šè¯­å¥è¯æ˜å¯¹å’Œæ—¨åœ¨èå…¥æ¨¡æ‹Ÿäººç±»æ¨ç†å’Œå‡è®¾æ”¹è¿›çš„è®¤çŸ¥è¡Œä¸ºçš„å…¶ä»–æ•°æ®ç»„æˆçš„æ··åˆæ•°æ®é›†ï¼Œå¯¹å½“å‰ATPæ¨¡å‹è¿›è¡ŒæŒç»­è®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨Lean 4ç¼–è¯‘å™¨æä¾›çš„æˆæœå¥–åŠ±æ¥å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡æˆ‘ä»¬è®¾è®¡çš„æŒç»­è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œæˆ‘ä»¬æˆåŠŸæ”¹è¿›äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨ï¼ŒåŒ…æ‹¬DeepSeek-Prover-v1.5å’ŒGoedel-Proverï¼Œå¹¶åœ¨æ•´ä¸ªè¯æ˜ç”Ÿæˆé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨MiniF2Fä¸Šï¼Œæˆ‘ä»¬è¾¾åˆ°äº†59.8%ï¼ˆpass@32ï¼‰çš„é€šè¿‡ç‡ã€‚è¿™æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œä¸­çš„é¡¹ç›®ï¼Œæˆ‘ä»¬å°†é€æ­¥æ›´æ–°æˆ‘ä»¬çš„å‘ç°ï¼Œå¹¶å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®å’Œè®­ç»ƒç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è¯‘æ–‡ç®€åŒ–ä¸è¯´æ˜</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06122v1">PDF</a> 23 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰æ–¹é¢çš„è¿›å±•å‡¸æ˜¾äº†ä¸Lean 4ä»£ç è¿›è¡Œå½¢å¼æ¨ç†çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒATPå°šæœªå—åˆ°Open AI O1&#x2F;O3å’ŒDeepseek R1æ‰€å±•ç¤ºçš„è¿‘æœŸåè®­ç»ƒæ‰©å±•çš„é©å‘½æ€§å½±å“ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥ATPçš„åè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥ä½¿å…¶ä¸è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹çš„çªç ´è¿›å±•ä¿æŒä¸€è‡´ã€‚é€šè¿‡æŒç»­è®­ç»ƒå½“å‰ATPæ¨¡å‹ä¸æ··åˆæ•°æ®é›†ï¼Œä»¥åŠåˆ©ç”¨Lean 4ç¼–è¯‘å™¨è¿”å›çš„ç»“æœå¥–åŠ±æ¥æ¢ç´¢å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬æˆåŠŸæ”¹è¿›äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨ï¼ŒåŒ…æ‹¬DeepSeek-Prover-v1.5å’ŒGoedel-Proverï¼Œå¹¶åœ¨æ•´ä¸ªè¯æ˜ç”Ÿæˆé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨MiniF2Fä¸Šè¾¾åˆ°äº†59.8%çš„é€šè¿‡ç‡ï¼ˆpass@32ï¼‰ã€‚æ­¤é¡¹ç›®æ­£åœ¨è¿›è¡Œä¸­ï¼Œæˆ‘ä»¬å°†ä¸æ–­æ›´æ–°æˆ‘ä»¬çš„å‘ç°ï¼Œå¹¶å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®å’Œè®­ç»ƒç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ATPä¸­çš„åº”ç”¨å±•ç¤ºäº†å½¢å¼æ¨ç†çš„æ½œåŠ›ã€‚</li>
<li>ATPå°šæœªè¢«è¿‘æœŸåè®­ç»ƒæ‰©å±•æ˜¾è‘—å½±å“ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯è°ƒæŸ¥å¹¶æ”¹è¿›ATPçš„åè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥ä¸è‡ªç„¶è¯­è¨€æ¨ç†æ¨¡å‹çš„è¿›å±•åŒæ­¥ã€‚</li>
<li>é€šè¿‡æŒç»­è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ æ”¹è¿›äº†ç°æœ‰çš„å½¢å¼è¯æ˜å™¨ã€‚</li>
<li>åœ¨MiniF2Fä¸Šå–å¾—äº†è¾ƒé«˜çš„é€šè¿‡ç‡ã€‚</li>
<li>æ­¤é¡¹ç›®æ­£åœ¨è¿›è¡Œä¸­ï¼Œå°†æŒç»­æ›´æ–°æ•°æ®å’Œè®­ç»ƒç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3b626ca31962b6045da5c38976d4adfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-488ecde7aa50f655a60b9deeefcb47bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-736ba68fa87823813c71a8b46b444929.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MDK12-Bench-A-Multi-Discipline-Benchmark-for-Evaluating-Reasoning-in-Multimodal-Large-Language-Models"><a href="#MDK12-Bench-A-Multi-Discipline-Benchmark-for-Evaluating-Reasoning-in-Multimodal-Large-Language-Models" class="headerlink" title="MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in   Multimodal Large Language Models"></a>MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in   Multimodal Large Language Models</h2><p><strong>Authors:Pengfei Zhou, Fanrui Zhang, Xiaopeng Peng, Zhaopan Xu, Jiaxin Ai, Yansheng Qiu, Chuanhao Li, Zhen Li, Ming Li, Yukang Feng, Jianwen Sun, Haoquan Zhang, Zizhen Li, Xiaofeng Mao, Wangbo Zhao, Kai Wang, Xiaojun Chang, Wenqi Shao, Yang You, Kaipeng Zhang</strong></p>
<p>Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LanceZPF/MDK12">https://github.com/LanceZPF/MDK12</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨ç†æ˜¯å°†è¯­è¨€å’Œè§†è§‰çº¿ç´¢æ•´åˆåˆ°é—®é¢˜è§£å†³å’Œå†³ç­–åˆ¶å®šä¸­ï¼Œæ˜¯äººç±»æ™ºèƒ½çš„åŸºæœ¬è¦ç´ ï¼Œä¹Ÿæ˜¯å®ç°äººå·¥é€šç”¨æ™ºèƒ½çš„å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å¯¹å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚ç°æœ‰å¤§å¤šæ•°æ¨ç†åŸºå‡†æµ‹è¯•éƒ½å—é™äºæ•°æ®é‡æœ‰é™ã€é¢†åŸŸè¦†ç›–èŒƒå›´ç‹­çª„ä»¥åŠçŸ¥è¯†åˆ†å¸ƒä¸å¤Ÿç»“æ„åŒ–ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MDK12-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ç°å®ä¸–ç•Œä¸­çš„K-12è€ƒè¯•è¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€åœ°ç†å’Œä¿¡æ¯ç§‘å­¦å…­ä¸ªå­¦ç§‘ï¼ŒåŒ…å«ä»å°å­¦åˆ°12å¹´çº§ä¸åŒéš¾åº¦çº§åˆ«çš„14ä¸‡ä¸ªæ¨ç†å®ä¾‹ã€‚å®ƒåŸºäºç»„ç»‡è‰¯å¥½çš„çŸ¥è¯†ç»“æ„ã€è¯¦ç»†çš„ç­”æ¡ˆè§£é‡Šã€éš¾åº¦æ ‡ç­¾å’Œè·¨å¹´çº§åˆ†åŒºï¼Œæä¾›äº†6827ä¸ªå®ä¾‹çº§çŸ¥è¯†ç‚¹æ³¨é‡Šï¼Œä¸ºå…¨é¢è¯„ä¼°æä¾›äº†ä¸€ä¸ªç¨³å¥çš„å¹³å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è¯„ä¼°è¿‡ç¨‹ä¸­çš„é—®é¢˜å½¢å¼ã€é—®é¢˜ç±»å‹å’Œå›¾åƒé£æ ¼çš„è‡ªåŠ©å¼•å¯¼ï¼Œæ¥ç¼“è§£æ•°æ®æ±¡æŸ“é—®é¢˜ã€‚åœ¨MDK12-Benchä¸Šçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†å½“å‰MLLMsåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„é‡å¤§å±€é™æ€§ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å‘ç°å¯¹ä¸‹ä¸€ä»£æ¨¡å‹çš„å‘å±•æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LanceZPF/MDK12%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LanceZPF/MDK12ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05782v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€æ¨ç†æ˜¯é›†æˆè¯­è¨€å’Œè§†è§‰çº¿ç´¢åˆ°é—®é¢˜è§£å†³å’Œå†³ç­–åˆ¶å®šä¸­çš„åŸºæœ¬äººç±»æ™ºèƒ½è¡¨ç°ï¼Œä¹Ÿæ˜¯æœç€äººå·¥æ™ºèƒ½é€šç”¨å‘å±•è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MDK12-Benchè·¨å­¦ç§‘åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡K-12ç°å®è€ƒè¯•è¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†å…­ä¸ªå­¦ç§‘ï¼ŒåŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„14ä¸‡ä¸ªæ¨ç†å®ä¾‹ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ–°å‹åŠ¨æ€è¯„ä¼°æ¡†æ¶æ¥ç¼“è§£æ•°æ®æ±¡æŸ“é—®é¢˜ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰MLLMåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚æœ¬æ–‡çš„å‘ç°å¯¹äºä¸‹ä¸€ä»£æ¨¡å‹çš„å‘å±•æä¾›äº†æ·±åˆ»çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚</li>
<li>å½“å‰å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›è¯„ä¼°å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦è·¨å­¦ç§‘åŸºå‡†æµ‹è¯•æ¥å…¨é¢è¯„ä¼°å…¶æ€§èƒ½ã€‚</li>
<li>MDK12-BenchåŸºå‡†æµ‹è¯•æ¶µç›–äº†å¤šä¸ªå­¦ç§‘ï¼ŒåŒ…æ‹¬æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€åœ°ç†å’Œä¿¡æ¯ç§‘å­¦ï¼Œæä¾›äº†å¤§è§„æ¨¡çš„æ¨ç†å®ä¾‹åº“ã€‚</li>
<li>MDK12-BenchåŸºå‡†æµ‹è¯•é‡‡ç”¨äº†è¯¦ç»†çš„ç­”æ¡ˆè§£é‡Šã€éš¾åº¦æ ‡ç­¾å’Œè·¨å¹´çº§åˆ†åŒºï¼Œä¸ºè¯„ä¼°æä¾›äº†ç¨³å¥çš„å¹³å°ã€‚</li>
<li>æ–°å‹åŠ¨æ€è¯„ä¼°æ¡†æ¶è¢«ç”¨äºç¼“è§£æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œé€šè¿‡è°ƒæ•´é—®é¢˜å½¢å¼ã€é—®é¢˜ç±»å‹å’Œå›¾åƒé£æ ¼æ¥è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰MLLMåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œè¿™ä¸ºä¸‹ä¸€ä»£æ¨¡å‹çš„å‘å±•æä¾›äº†æŒ‘æˆ˜å’Œæœºé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-407bd4915cbb4e2085228879b45015a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-880dcdd060cb2121d2582b72145748d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6a0b0562758b87455d194c182ed41db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05ee6d88cf3fe88cc80762e8c200bb9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c2c4cfeda0666af04269fdb42f02971.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86301f336d4e0afe56013b538325ca57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-693302fda3d13ca96d56e1eff4d83897.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="iEBAKER-Improved-Remote-Sensing-Image-Text-Retrieval-Framework-via-Eliminate-Before-Align-and-Keyword-Explicit-Reasoning"><a href="#iEBAKER-Improved-Remote-Sensing-Image-Text-Retrieval-Framework-via-Eliminate-Before-Align-and-Keyword-Explicit-Reasoning" class="headerlink" title="iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via   Eliminate Before Align and Keyword Explicit Reasoning"></a>iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via   Eliminate Before Align and Keyword Explicit Reasoning</h2><p><strong>Authors:Yan Zhang, Zhong Ji, Changxu Meng, Yanwei Pang, Jungong Han</strong></p>
<p>Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR), which aims at searching for the corresponding targets based on the given query. Among these efforts, the application of Foundation Models (FMs), such as CLIP, to the domain of remote sensing has yielded encouraging outcomes. However, existing FM based methodologies neglect the negative impact of weakly correlated sample pairs and fail to account for the key distinctions among remote sensing texts, leading to biased and superficial exploration of sample pairs. To address these challenges, we propose an approach named iEBAKER (an Improved Eliminate Before Align strategy with Keyword Explicit Reasoning framework) for RSITR. Specifically, we propose an innovative Eliminate Before Align (EBA) strategy to filter out the weakly correlated sample pairs, thereby mitigating their deviations from optimal embedding space during alignment.Further, two specific schemes are introduced from the perspective of whether local similarity and global similarity affect each other. On this basis, we introduce an alternative Sort After Reversed Retrieval (SAR) strategy, aims at optimizing the similarity matrix via reverse retrieval. Additionally, we incorporate a Keyword Explicit Reasoning (KER) module to facilitate the beneficial impact of subtle key concept distinctions. Without bells and whistles, our approach enables a direct transition from FM to RSITR task, eliminating the need for additional pretraining on remote sensing data. Extensive experiments conducted on three popular benchmark datasets demonstrate that our proposed iEBAKER method surpasses the state-of-the-art models while requiring less training data. Our source code will be released at <a target="_blank" rel="noopener" href="https://github.com/zhangy0822/iEBAKER">https://github.com/zhangy0822/iEBAKER</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨é¥æ„Ÿå›¾åƒæ–‡æœ¬æ£€ç´¢ï¼ˆRSITRï¼‰ä¸Šï¼Œå…¶ç›®æ ‡æ˜¯æ ¹æ®ç»™å®šçš„æŸ¥è¯¢æ¥æœç´¢ç›¸åº”çš„ç›®æ ‡ã€‚åœ¨è¿™äº›åŠªåŠ›ä¸­ï¼Œå°†CLIPç­‰Foundation Modelsï¼ˆFMsï¼‰åº”ç”¨äºé¥æ„Ÿé¢†åŸŸå·²ç»å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºFMçš„æ–¹æ³•å¿½ç•¥äº†å¼±ç›¸å…³æ ·æœ¬å¯¹çš„è´Ÿé¢å½±å“ï¼Œæœªèƒ½è€ƒè™‘åˆ°é¥æ„Ÿæ–‡æœ¬ä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œå¯¼è‡´å¯¹æ ·æœ¬å¯¹çš„åè§å’Œè‚¤æµ…çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºiEBAKERçš„æ–¹æ³•ï¼ˆå¸¦æœ‰å…³é”®è¯æ˜¾å¼æ¨ç†æ¡†æ¶çš„æ”¹è¿›å‰æ¶ˆé™¤å¯¹é½ç­–ç•¥ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¶ˆé™¤å‰å¯¹é½ï¼ˆEBAï¼‰ç­–ç•¥ï¼Œä»¥è¿‡æ»¤æ‰å¼±ç›¸å…³çš„æ ·æœ¬å¯¹ï¼Œä»è€Œå‡è½»å®ƒä»¬åœ¨å¯¹é½è¿‡ç¨‹ä¸­åç¦»æœ€ä½³åµŒå…¥ç©ºé—´çš„æƒ…å†µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»å±€éƒ¨ç›¸ä¼¼æ€§å’Œå…¨å±€ç›¸ä¼¼æ€§æ˜¯å¦ç›¸äº’å½±å“çš„è§’åº¦ä»‹ç»äº†ä¸¤ç§å…·ä½“æ–¹æ¡ˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ›¿ä»£çš„SARï¼ˆSort After Reversed Retrievalï¼‰ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡åå‘æ£€ç´¢ä¼˜åŒ–ç›¸ä¼¼æ€§çŸ©é˜µã€‚å¦å¤–ï¼Œæˆ‘ä»¬åŠ å…¥äº†å…³é”®è¯æ˜¾å¼æ¨ç†ï¼ˆKERï¼‰æ¨¡å—ï¼Œä»¥ä¿ƒè¿›å¾®å¦™å…³é”®æ¦‚å¿µåŒºåˆ«çš„ç§¯æå½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€é¢å¤–çš„è£…é¥°å’Œé…ä»¶ï¼Œå°±èƒ½ç›´æ¥ä»FMè¿‡æ¸¡åˆ°RSITRä»»åŠ¡ï¼Œæ— éœ€åœ¨é¥æ„Ÿæ•°æ®ä¸Šè¿›è¡Œé¢å¤–çš„é¢„è®­ç»ƒã€‚åœ¨ä¸‰ä¸ªæµè¡ŒåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„iEBAKERæ–¹æ³•è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhangy0822/iEBAKER%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/zhangy0822/iEBAKERä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05644v1">PDF</a> </p>
<p><strong>Summary</strong><br>    è¿‘æœŸç ”ç©¶å…³æ³¨é¥æ„Ÿå›¾åƒæ–‡æœ¬æ£€ç´¢ï¼ˆRSITRï¼‰ï¼Œåº”ç”¨Foundation Modelsï¼ˆFMsï¼‰å¦‚CLIPå–å¾—é¼“èˆæ€§æˆæœã€‚ä½†ç°æœ‰æ–¹æ³•å¿½ç•¥å¼±ç›¸å…³æ ·æœ¬å¯¹çš„å½±å“ï¼Œæœªèƒ½å…³é”®åŒºåˆ†é¥æ„Ÿæ–‡æœ¬ï¼Œå¯¼è‡´æ ·æœ¬å¯¹æ¢ç´¢æœ‰åè§å’Œè¡¨é¢åŒ–ã€‚ä¸ºæ­¤ï¼Œæå‡ºiEBAKERæ–¹æ³•ï¼Œé‡‡ç”¨Eliminate Before Alignç­–ç•¥è¿‡æ»¤å¼±ç›¸å…³æ ·æœ¬å¯¹ï¼Œä»å±€éƒ¨å’Œå…¨å±€ç›¸ä¼¼æ€§çš„è§’åº¦å¼•å…¥SARç­–ç•¥ä¼˜åŒ–ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå¹¶èå…¥Keyword Explicit Reasoningæ¨¡å—åŠ©åŠ›å…³é”®æ¦‚å¿µåŒºåˆ†ã€‚è¯¥æ–¹æ³•ç›´æ¥ä»FMè½¬å‘RSITRä»»åŠ¡ï¼Œæ— éœ€åœ¨é¥æ„Ÿæ•°æ®ä¸Šé¢å¤–é¢„è®­ç»ƒï¼Œåœ¨ä¸‰ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒiEBAKERè¶…è¶Šç°æœ‰æœ€ä½³æ¨¡å‹ï¼Œä¸”éœ€è¦æ›´å°‘è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Remote Sensing Image-Text Retrieval (RSITR) æ—¨åœ¨æ ¹æ®ç»™å®šæŸ¥è¯¢æœç´¢ç›¸åº”ç›®æ ‡ã€‚</li>
<li>Foundation Modelsï¼ˆFMsï¼‰åœ¨é¥æ„Ÿé¢†åŸŸçš„åº”ç”¨å·²äº§ç”Ÿé¼“èˆäººå¿ƒçš„ç»“æœã€‚</li>
<li>ç°æœ‰FMæ–¹æ³•å¿½ç•¥å¼±ç›¸å…³æ ·æœ¬å¯¹çš„è´Ÿé¢å½±å“ã€‚</li>
<li>iEBAKERæ–¹æ³•é€šè¿‡Eliminate Before Alignç­–ç•¥è¿‡æ»¤å¼±ç›¸å…³æ ·æœ¬å¯¹ã€‚</li>
<li>SARç­–ç•¥ç”¨äºä¼˜åŒ–ç›¸ä¼¼åº¦çŸ©é˜µï¼Œè€ƒè™‘å±€éƒ¨å’Œå…¨å±€ç›¸ä¼¼æ€§çš„ç›¸äº’å½±å“ã€‚</li>
<li>Keyword Explicit Reasoningæ¨¡å—åŠ©åŠ›å…³é”®æ¦‚å¿µçš„åŒºåˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-deb6fde8bc6d52bf9b6dd82fde885074.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9aa340d81694e5426b86a937bb3ccf59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f15c025d81a8d894173ccc225dc1c541.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0547316512d7779ffb8b136bbd2f38.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BoolE-Exact-Symbolic-Reasoning-via-Boolean-Equality-Saturation"><a href="#BoolE-Exact-Symbolic-Reasoning-via-Boolean-Equality-Saturation" class="headerlink" title="BoolE: Exact Symbolic Reasoning via Boolean Equality Saturation"></a>BoolE: Exact Symbolic Reasoning via Boolean Equality Saturation</h2><p><strong>Authors:Jiaqi Yin, Zhan Song, Chen Chen, Qihao Hu, Cunxi Yu</strong></p>
<p>Boolean symbolic reasoning for gate-level netlists is a critical step in verification, logic and datapath synthesis, and hardware security. Specifically, reasoning datapath and adder tree in bit-blasted Boolean networks is particularly crucial for verification and synthesis, and challenging. Conventional approaches either fail to accurately (exactly) identify the function blocks of the designs in gate-level netlist with structural hashing and symbolic propagation, or their reasoning performance is highly sensitive to structure modifications caused by technology mapping or logic optimization. This paper introduces BoolE, an exact symbolic reasoning framework for Boolean netlists using equality saturation. BoolE optimizes scalability and performance by integrating domain-specific Boolean ruleset for term rewriting. We incorporate a novel extraction algorithm into BoolE to enhance its structural insight and computational efficiency, which adeptly identifies and captures multi-input, multi-output high-level structures (e.g., full adder) in the reconstructed e-graph.   Our experiments show that BoolE surpasses state-of-the-art symbolic reasoning baselines, including the conventional functional approach (ABC) and machine learning-based method (Gamora). Specifically, we evaluated its performance on various multiplier architecture with different configurations. Our results show that BoolE identifies $3.53\times$ and $3.01\times$ more exact full adders than ABC in carry-save array and Booth-encoded multipliers, respectively. Additionally, we integrated BoolE into multiplier formal verification tasks, where it significantly accelerates the performance of traditional formal verification tools using computer algebra, demonstrated over four orders of magnitude runtime improvements. </p>
<blockquote>
<p>é’ˆå¯¹é—¨çº§ç½‘è¡¨çš„å¸ƒå°”ç¬¦å·æ¨ç†æ˜¯éªŒè¯ã€é€»è¾‘å’Œæ•°æ®è·¯å¾„åˆæˆä»¥åŠç¡¬ä»¶å®‰å…¨ä¸­çš„å…³é”®æ­¥éª¤ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨æ¯”ç‰¹çˆ†ç‚¸çš„å¸ƒå°”ç½‘ç»œä¸­æ¨ç†æ•°æ®è·¯å¾„å’ŒåŠ æ³•æ ‘å¯¹äºéªŒè¯å’Œåˆæˆè‡³å…³é‡è¦ï¼Œä¸”éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿçš„æ–¹æ³•è¦ä¹ˆæ— æ³•å‡†ç¡®åœ°ï¼ˆç¡®åˆ‡åœ°ï¼‰è¯†åˆ«é—¨çº§ç½‘è¡¨ä¸­è®¾è®¡çš„åŠŸèƒ½å—ï¼Œä½¿ç”¨ç»“æ„æ•£åˆ—å’Œç¬¦å·ä¼ æ’­ï¼Œè¦ä¹ˆå®ƒä»¬çš„æ¨ç†æ€§èƒ½å¯¹æŠ€æœ¯æ˜ å°„æˆ–é€»è¾‘ä¼˜åŒ–å¼•èµ·çš„ç»“æ„ä¿®æ”¹éå¸¸æ•æ„Ÿã€‚æœ¬æ–‡ä»‹ç»äº†BoolEï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ç­‰å¼é¥±å’Œçš„å¸ƒå°”ç½‘è¡¨ç²¾ç¡®ç¬¦å·æ¨ç†æ¡†æ¶ã€‚BoolEé€šè¿‡é›†æˆç‰¹å®šé¢†åŸŸçš„å¸ƒå°”è§„åˆ™é›†è¿›è¡Œæœ¯è¯­é‡å†™æ¥ä¼˜åŒ–å¯æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬å°†ä¸€ç§æ–°å‹æå–ç®—æ³•çº³å…¥BoolEï¼Œä»¥æé«˜å…¶ç»“æ„æ´å¯ŸåŠ›å’Œè®¡ç®—æ•ˆç‡ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå·§å¦™åœ°åœ¨é‡æ„çš„eå›¾ä¸­è¯†åˆ«å’Œæ•è·å¤šè¾“å…¥ã€å¤šè¾“å‡ºçš„é«˜çº§ç»“æ„ï¼ˆä¾‹å¦‚å…¨åŠ å™¨ï¼‰ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒBoolEè¶…è¶Šäº†æœ€æ–°çš„ç¬¦å·æ¨ç†åŸºçº¿ï¼ŒåŒ…æ‹¬ä¼ ç»ŸåŠŸèƒ½æ–¹æ³•ï¼ˆABCï¼‰å’ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼ˆGamoraï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å„ç§é…ç½®çš„å¤šè·¯æ¶æ„ä¸Šè¯„ä¼°äº†å…¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æºå¸¦ä¿å­˜é˜µåˆ—å’ŒBoothç¼–ç çš„å¤šè·¯ä¸­ï¼ŒBoolEè¯†åˆ«å‡ºçš„å…¨åŠ å™¨æ¯”ABCå¤š3.53å€å’Œ3.01å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†BoolEé›†æˆåˆ°å¤šè·¯æ­£å¼éªŒè¯ä»»åŠ¡ä¸­ï¼Œå®ƒæ˜¾è‘—åŠ é€Ÿäº†ä½¿ç”¨è®¡ç®—æœºä»£æ•°çš„ä¼ ç»Ÿå½¢å¼éªŒè¯å·¥å…·çš„æ€§èƒ½ï¼Œåœ¨å››ä¸ªæ•°é‡çº§çš„è¿è¡Œæ—¶é—´ä¸Šå–å¾—äº†æ˜æ˜¾çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05577v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¸ƒå°”ç¬¦å·æ¨ç†ä¸­ï¼Œé’ˆå¯¹é—¨çº§ç½‘è¡¨çš„æ¨ç†æ˜¯éªŒè¯ã€é€»è¾‘åˆæˆã€ç¡¬ä»¶å®‰å…¨ä¸­çš„å…³é”®æ­¥éª¤ã€‚å¯¹äºä½çˆ†ç ´å¸ƒå°”ç½‘ç»œä¸­çš„æ¨ç†è·¯å¾„å’ŒåŠ æ³•å™¨æ ‘å°¤å…¶é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰æ–¹æ³•æœªèƒ½å‡†ç¡®åœ°è¯†åˆ«è®¾è®¡ä¸­çš„åŠŸèƒ½å—æˆ–å¯¹ç»“æ„å˜åŠ¨ï¼ˆæŠ€æœ¯æ˜ å°„æˆ–é€»è¾‘ä¼˜åŒ–å¯¼è‡´ï¼‰æ•æ„Ÿã€‚æœ¬æ–‡ä»‹ç»BoolEï¼Œä¸€ä¸ªåŸºäºç­‰å¼é¥±å’Œçš„å¸ƒå°”ç½‘è¡¨ç²¾ç¡®ç¬¦å·æ¨ç†æ¡†æ¶ã€‚BoolEé€šè¿‡é›†æˆé¢†åŸŸç‰¹å®šçš„å¸ƒå°”è§„åˆ™é›†è¿›è¡Œæœ¯è¯­é‡å†™æ¥ä¼˜åŒ–å¯æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨æ–°é¢–æå–ç®—æ³•å¢å¼ºBoolEçš„ç»“æ„æ´å¯ŸåŠ›å’Œè®¡ç®—æ•ˆç‡ï¼Œå·§å¦™è¯†åˆ«å¹¶æ•è·å¤šè¾“å…¥å¤šè¾“å‡ºçš„é«˜çº§ç»“æ„ï¼ˆå¦‚å…¨åŠ å™¨ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒBoolEè¶…è¶Šå…ˆè¿›çš„ç¬¦å·æ¨ç†åŸºçº¿ï¼ŒåŒ…æ‹¬ä¼ ç»ŸåŠŸèƒ½æ–¹æ³•ï¼ˆABCï¼‰å’Œæœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆGamoraï¼‰ã€‚å¯¹äºä¸åŒé…ç½®çš„å¤šè·¯å™¨æ¶æ„æ€§èƒ½è¯„ä¼°æ˜¾ç¤ºï¼ŒBoolEè¯†åˆ«çš„å…¨åŠ å™¨æ¯”ABCå¤š3.53å€å’Œ3.01å€ã€‚æ­¤å¤–ï¼Œå°†BoolEé›†æˆåˆ°å¤šè·¯å™¨å½¢å¼éªŒè¯ä»»åŠ¡ä¸­ï¼Œä¸ä¼ ç»Ÿå½¢å¼éªŒè¯å·¥å…·ç›¸æ¯”ï¼Œä½¿ç”¨è®¡ç®—æœºä»£æ•°æ˜¾è‘—åŠ é€Ÿæ€§èƒ½ï¼Œå®ç°è¶…è¿‡å››ä¸ªæ•°é‡çº§çš„è¿è¡Œæ—¶æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Boolean symbolic reasoning for gate-level netlists is crucial in verification, logic synthesis, and hardware security.</li>
<li>Existing methods have difficulties accurately identifying function blocks in gate-level netlists or are sensitive to structural changes.</li>
<li>The paper introduces BoolE, an exact symbolic reasoning framework for Boolean netlists using equality saturation.</li>
<li>BoolE optimizes scalability and performance through term rewriting with a domain-specific Boolean ruleset.</li>
<li>A novel extraction algorithm enhances BoolEâ€™s structural insight and computational efficiency.</li>
<li>BoolE surpasses state-of-the-art symbolic reasoning baselines, identifying more exact full adders than conventional methods.</li>
<li>Integrating BoolE into multiplier formal verification tasks significantly accelerates traditional formal verification tools.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c59d1b0a6941fc965f0d2dc0739cb343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06c787a864f6e3acdfc3f2adff5bc6fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fa9b4297c9b483933e8572ceab184b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-462ccd4651d5b0977e1457af3ed96073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95f0fcf0d873446465c0a167ee1bb510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf60eda244eb963a68d6346526996a23.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Generalization-Capabilities-of-Large-Language-Models-on-Code-Reasoning"><a href="#Evaluating-the-Generalization-Capabilities-of-Large-Language-Models-on-Code-Reasoning" class="headerlink" title="Evaluating the Generalization Capabilities of Large Language Models on   Code Reasoning"></a>Evaluating the Generalization Capabilities of Large Language Models on   Code Reasoning</h2><p><strong>Authors:Rem Yang, Julian Dai, Nikos Vasilakis, Martin Rinard</strong></p>
<p>We assess how the code reasoning abilities of large language models (LLMs) generalize to different kinds of programs. We present techniques for obtaining in- and out-of-distribution programs with different characteristics: code sampled from a domain-specific language, code automatically generated by an LLM, code collected from competitive programming contests, and mutated versions of these programs. We also present an experimental methodology for evaluating LLM generalization by comparing their performance on these programs. We perform an extensive evaluation across 10 state-of-the-art models from the past year, obtaining insights into their generalization capabilities over time and across different classes of programs. Our results highlight that while earlier models exhibit behavior consistent with pattern matching, the latest models exhibit strong generalization abilities on code reasoning. </p>
<blockquote>
<p>æˆ‘ä»¬è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç æ¨ç†èƒ½åŠ›å¦‚ä½•æ¨å¹¿åˆ°ä¸åŒç±»å‹çš„ç¨‹åºã€‚æˆ‘ä»¬å±•ç¤ºäº†è·å¾—å…·æœ‰ä¸åŒç‰¹æ€§çš„åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ç¨‹åºçš„æŠ€æœ¯ï¼šä»ç‰¹å®šé¢†åŸŸè¯­è¨€é‡‡æ ·å¾—åˆ°çš„ä»£ç ã€ç”±LLMè‡ªåŠ¨ç”Ÿæˆçš„ä»£ç ã€ä»ç«èµ›ç¼–ç¨‹æ¯”èµ›ä¸­æ”¶é›†çš„ä»£ç ä»¥åŠè¿™äº›ç¨‹åºçš„å˜ä½“ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ¯”è¾ƒè¿™äº›ç¨‹åºä¸Šçš„è¡¨ç°ï¼Œæå‡ºäº†ä¸€ç§è¯„ä¼°LLMæ³›åŒ–æ€§çš„å®éªŒæ–¹æ³•ã€‚æˆ‘ä»¬å¯¹è¿‡å»ä¸€å¹´ä¸­çš„10ä¸ªæœ€æ–°å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œäº†è§£å®ƒä»¬åœ¨æ—¶é—´å’Œä¸åŒç±»åˆ«çš„ç¨‹åºä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ—©æœŸæ¨¡å‹çš„è¡Œä¸ºä¸æ¨¡å¼åŒ¹é…ä¸€è‡´ï¼Œä½†æœ€æ–°æ¨¡å‹åœ¨ä»£ç æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05518v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç æ¨ç†èƒ½åŠ›åœ¨ä¸åŒç±»å‹ç¨‹åºä¸Šçš„æ³›åŒ–æƒ…å†µã€‚æ–‡ç« ä»‹ç»äº†è·å–ä¸åŒç‰¹æ€§ç¨‹åºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰¹å®šé¢†åŸŸè¯­è¨€ä¸­çš„ä»£ç æ ·æœ¬ã€LLMè‡ªåŠ¨ç”Ÿæˆçš„ä»£ç ã€æ¥è‡ªç¼–ç¨‹ç«èµ›çš„ä»£ç ä»¥åŠè¿™äº›ç¨‹åºçš„å˜ç§ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§è¯„ä¼°LLMæ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒè¿™äº›ç¨‹åºä¸Šçš„è¡¨ç°æ¥è¿›è¡Œè¯„ä»·ã€‚æ–‡ç« å¯¹è¿‡å»ä¸€å¹´ä¸­çš„10ä¸ªæœ€æ–°æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶æ·±å…¥äº†è§£äº†å®ƒä»¬åœ¨æ—¶é—´æ¨ç§»å’Œä¸åŒç±»åˆ«ç¨‹åºä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œæ—©æœŸæ¨¡å‹çš„è¡Œä¸ºä¸æ¨¡å¼åŒ¹é…ä¸€è‡´ï¼Œè€Œæœ€æ–°æ¨¡å‹åœ¨ä»£ç æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡ç« è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç æ¨ç†èƒ½åŠ›åœ¨ä¸åŒç±»å‹ç¨‹åºä¸Šçš„æ³›åŒ–æƒ…å†µã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½æ¥è¯„ä¼°LLMçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†è·å–å…·æœ‰ä¸åŒç‰¹æ€§çš„ç¨‹åºå’Œè¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰¹å®šé¢†åŸŸè¯­è¨€ä¸­çš„ä»£ç æ ·æœ¬ã€è‡ªåŠ¨ç”Ÿæˆçš„ä»£ç ã€æ¥è‡ªç¼–ç¨‹ç«èµ›çš„ä»£ç ä»¥åŠç¨‹åºçš„å˜ç§ã€‚</li>
<li>å¯¹è¿‡å»ä¸€å¹´çš„10ä¸ªæœ€æ–°æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚</li>
<li>æ—©æœŸæ¨¡å‹çš„è¡Œä¸ºä¸æ¨¡å¼åŒ¹é…ä¸€è‡´ã€‚</li>
<li>æœ€æ–°æ¨¡å‹åœ¨ä»£ç æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-adbacbc7e9744f0e9d3fdd5592fe44de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa0b2f91e3ce7e6e83e21793e2b1cbec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6f9070f809832867ce2541f97ea941.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cec44bd95737d7c1e7ff48fe0670aca.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GraphRAFT-Retrieval-Augmented-Fine-Tuning-for-Knowledge-Graphs-on-Graph-Databases"><a href="#GraphRAFT-Retrieval-Augmented-Fine-Tuning-for-Knowledge-Graphs-on-Graph-Databases" class="headerlink" title="GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph   Databases"></a>GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph   Databases</h2><p><strong>Authors:Alfred Clemedtson, Borun Shi</strong></p>
<p>Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLMâ€™s context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q&amp;As on large text-attributed KGs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¯­è¨€å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨æ¶‰åŠç§äººæ•°æ®æ—¶å®¹æ˜“å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ£€ç´¢ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ç›¸å…³ä¸”åŒ¹é…çš„æ•°æ®ï¼Œå¹¶æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ç»™å‡ºç­”æ¡ˆã€‚GraphRAGå°†æ­¤æ–¹æ³•æ‰©å±•åˆ°ç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰å’Œå…³äºå¤šä¸ªè·³è·ƒç‚¹çš„å®ä½“çš„æŸ¥è¯¢ã€‚æœ€è¿‘çš„GraphRAGæ–¹æ³•å¤§å¤šå¿½ç•¥äº†æ£€ç´¢æ­¥éª¤ï¼Œæˆ–è€…å…·æœ‰æŠ½è±¡æˆ–ä½æ•ˆçš„ä¸´æ—¶æ£€ç´¢æµç¨‹ã€‚è¿™é˜»æ­¢äº†å®ƒä»¬åœ¨çŸ¥è¯†å›¾è°±å­˜å‚¨åœ¨æ”¯æŒå›¾å½¢æŸ¥è¯¢è¯­è¨€çš„å›¾å½¢æ•°æ®åº“ä¸­çš„åœºæ™¯ä¸­ä½¿ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GraphRAFTï¼Œè¿™æ˜¯ä¸€ä¸ªæ£€ç´¢å’Œæ¨ç†æ¡†æ¶ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆç»è¿‡éªŒè¯çš„æ­£ç¡®CypheræŸ¥è¯¢ï¼Œä»¥æ£€ç´¢é«˜è´¨é‡å­å›¾ä¸Šä¸‹æ–‡å¹¶äº§ç”Ÿå‡†ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥åœ¨å­˜å‚¨åœ¨æœ¬åœ°å›¾å½¢æ•°æ®åº“çš„çŸ¥è¯†å›¾è°±ä¸Šä½¿ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ ·æœ¬é«˜æ•ˆçš„ï¼Œå¹¶éšç€è®­ç»ƒæ•°æ®çš„å¯ç”¨æ€§è€Œæ‰©å±•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å‹å¸¦æ–‡æœ¬å±æ€§çŸ¥è¯†å›¾è°±ä¸Šçš„ä¸¤ä¸ªæŒ‘æˆ˜æ€§é—®ç­”æµ‹è¯•çš„æ‰€æœ‰å››ä¸ªæ ‡å‡†æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºæ‰€æœ‰æœ€æ–°æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Large Language Modelsåœ¨å¤„ç†ä¸ç§äººæ•°æ®ç›¸å…³çš„æŸ¥è¯¢æ—¶æ˜“å‡ºç°å¹»æƒ³çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†GraphRAFTè¿™ä¸€åŸºäºæ£€ç´¢å’Œæ¨ç†çš„æ¡†æ¶ã€‚GraphRAFTå¯ä»¥å¾®è°ƒLLMsï¼Œç”Ÿæˆå¯é çš„CypheræŸ¥è¯¢ï¼Œä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢é«˜è´¨é‡å­å›¾ä¸Šä¸‹æ–‡å¹¶ç»™å‡ºå‡†ç¡®ç­”æ¡ˆã€‚æ­¤æ¡†æ¶é€‚ç”¨äºå­˜å‚¨åœ¨åŸç”Ÿå›¾å½¢æ•°æ®åº“ä¸­çš„çŸ¥è¯†å›¾è°±ï¼Œå…·æœ‰æ ·æœ¬æ•ˆç‡é«˜ã€éšè®­ç»ƒæ•°æ®å¯ç”¨æ€§è€Œæ‰©å±•çš„ä¼˜ç‚¹ã€‚åœ¨å¤§å‹æ–‡æœ¬å±æ€§çŸ¥è¯†å›¾è°±çš„Q&amp;Aä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªæ ‡å‡†æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Language Modelsåœ¨å¤„ç†è¯­è¨€å’Œæ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¶‰åŠç§äººæ•°æ®çš„æŸ¥è¯¢æ—¶å®¹æ˜“å‡ºé”™ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡æ£€ç´¢ä¸LLMä¸Šä¸‹æ–‡çª—å£ç›¸å…³çš„æ•°æ®å¹¶æç¤ºLLMæ¥å›ç­”é—®é¢˜ã€‚</li>
<li>GraphRAGå°†è¿™ç§æ–¹æ³•æ‰©å±•åˆ°ç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ï¼Œå¹¶å¤„ç†å…³äºå®ä½“å¤šè·³çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰GraphRAGæ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œå¿½ç•¥æ£€ç´¢æ­¥éª¤æˆ–æ‹¥æœ‰æŠ½è±¡ã€ä½æ•ˆçš„æ£€ç´¢è¿‡ç¨‹ï¼Œæ— æ³•é€‚åº”äºçŸ¥è¯†å›¾è°±å­˜å‚¨åœ¨å›¾å½¢æ•°æ®åº“çš„æƒ…å†µã€‚</li>
<li>GraphRAFTæ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢å’Œæ¨ç†çš„æ¡†æ¶ï¼Œå¯ä»¥å¾®è°ƒLLMsä»¥ç”Ÿæˆå‡†ç¡®çš„CypheræŸ¥è¯¢ï¼Œä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢é«˜è´¨é‡ä¿¡æ¯å¹¶ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚</li>
<li>GraphRAFTæ˜¯é¦–ä¸ªå¯ä»¥åœ¨å­˜å‚¨åœ¨åŸç”Ÿå›¾å½¢æ•°æ®åº“ä¸­çš„çŸ¥è¯†å›¾è°±ä¸Šä½¿ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ce2e7291a36e3b4c56719672307b6da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10a54d06d17561919431f59a8ff71ef6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc4135b2f1b7c74750ba796432f56add.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ZeroED-Hybrid-Zero-shot-Error-Detection-through-Large-Language-Model-Reasoning"><a href="#ZeroED-Hybrid-Zero-shot-Error-Detection-through-Large-Language-Model-Reasoning" class="headerlink" title="ZeroED: Hybrid Zero-shot Error Detection through Large Language Model   Reasoning"></a>ZeroED: Hybrid Zero-shot Error Detection through Large Language Model   Reasoning</h2><p><strong>Authors:Wei Ni, Kaihang Zhang, Xiaoye Miao, Xiangyu Zhao, Yangyang Wu, Yaoshu Wang, Jianwei Yin</strong></p>
<p>Error detection (ED) in tabular data is crucial yet challenging due to diverse error types and the need for contextual understanding. Traditional ED methods often rely heavily on manual criteria and labels, making them labor-intensive. Large language models (LLM) can minimize human effort but struggle with errors requiring a comprehensive understanding of data context. In this paper, we propose ZeroED, a novel hybrid zero-shot error detection framework, which combines LLM reasoning ability with the manual label-based ED pipeline. ZeroED operates in four steps, i.e., feature representation, error labeling, training data construction, and detector training. Initially, to enhance error distinction, ZeroED generates rich data representations using error reason-aware binary features, pre-trained embeddings, and statistical features. Then, ZeroED employs LLM to label errors holistically through in-context learning, guided by a two-step reasoning process for detailed error detection guidelines. To reduce token costs, LLMs are applied only to representative data selected via clustering-based sampling. High-quality training data is constructed through in-cluster label propagation and LLM augmentation with verification. Finally, a classifier is trained to detect all errors. Extensive experiments on seven public datasets demonstrate that, ZeroED substantially outperforms state-of-the-art methods by a maximum 30% improvement in F1 score and up to 90% token cost reduction. </p>
<blockquote>
<p>é”™è¯¯æ£€æµ‹ï¼ˆEDï¼‰åœ¨è¡¨æ ¼æ•°æ®ä¸­éå¸¸é‡è¦ï¼Œä½†åŒæ—¶ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨å¤šç§é”™è¯¯ç±»å‹ï¼Œä¸”éœ€è¦ç†è§£ä¸Šä¸‹æ–‡ã€‚ä¼ ç»Ÿçš„EDæ–¹æ³•å¾€å¾€ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æ ‡å‡†å’Œæ ‡ç­¾ï¼Œä½¿å…¶éœ€è¦å¤§é‡äººå·¥ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥æœ€å¤§é™åº¦åœ°å‡å°‘äººå·¥åŠªåŠ›ï¼Œä½†å¯¹äºéœ€è¦å…¨é¢ç†è§£æ•°æ®ä¸Šä¸‹æ–‡çš„é”™è¯¯å´å¾€å¾€æ— èƒ½ä¸ºåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ZeroEDï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ··åˆé›¶æ ·æœ¬é”™è¯¯æ£€æµ‹æ¡†æ¶ï¼Œå®ƒå°†LLMçš„æ¨ç†èƒ½åŠ›ä¸åŸºäºæ‰‹åŠ¨æ ‡ç­¾çš„EDç®¡é“ç›¸ç»“åˆã€‚ZeroEDæœ‰å››ä¸ªæ­¥éª¤ï¼šç‰¹å¾è¡¨ç¤ºã€é”™è¯¯æ ‡ç­¾ã€è®­ç»ƒæ•°æ®æ„å»ºå’Œæ£€æµ‹å™¨è®­ç»ƒã€‚é¦–å…ˆï¼Œä¸ºäº†å¢å¼ºé”™è¯¯åŒºåˆ†åº¦ï¼ŒZeroEDä½¿ç”¨é”™è¯¯åŸå› æ„ŸçŸ¥çš„äºŒè¿›åˆ¶ç‰¹å¾ã€é¢„è®­ç»ƒåµŒå…¥å’Œç»Ÿè®¡ç‰¹å¾æ¥ç”Ÿæˆä¸°å¯Œçš„æ•°æ®è¡¨ç¤ºã€‚ç„¶åï¼ŒZeroEDé‡‡ç”¨LLMé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å…¨é¢æ ‡æ³¨é”™è¯¯ï¼Œå¹¶é€šè¿‡ä¸¤æ­¥æ¨ç†è¿‡ç¨‹ä¸ºè¯¦ç»†çš„é”™è¯¯æ£€æµ‹æä¾›æŒ‡å¯¼ã€‚ä¸ºäº†å‡å°‘ä»¤ç‰Œæˆæœ¬ï¼Œä»…å¯¹é€šè¿‡èšç±»é‡‡æ ·é€‰æ‹©çš„æœ‰ä»£è¡¨æ€§çš„æ•°æ®åº”ç”¨LLMã€‚é€šè¿‡é›†ç¾¤å†…æ ‡ç­¾ä¼ æ’­å’ŒLLMå¢å¼ºéªŒè¯æ¥æ„å»ºé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚æœ€åï¼Œè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨æ¥æ£€æµ‹æ‰€æœ‰é”™è¯¯ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒZeroEDæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒF1å¾—åˆ†æœ€é«˜å¯æé«˜30%ï¼Œä»¤ç‰Œæˆæœ¬æœ€å¤šå¯é™ä½90%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05345v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong><br>æ•°æ®è¡¨é”™è¯¯æ£€æµ‹ï¼ˆEDï¼‰éå¸¸é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºé”™è¯¯ç±»å‹å¤šæ ·ä¸”éœ€è¦ç†è§£ä¸Šä¸‹æ–‡ã€‚ä¼ ç»ŸEDæ–¹æ³•ä¾èµ–æ‰‹åŠ¨æ ‡å‡†å’Œæ ‡ç­¾ï¼Œå·¥ä½œé‡å¤§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥å‡å°‘äººåŠ›æŠ•å…¥ï¼Œä½†åœ¨éœ€è¦å…¨é¢ç†è§£æ•°æ®ä¸Šä¸‹æ–‡æ—¶ä¼šå‡ºç°å›°éš¾ã€‚æœ¬æ–‡æå‡ºZeroEDï¼Œä¸€ç§æ–°å‹é›¶æ ·æœ¬é”™è¯¯æ£€æµ‹æ¡†æ¶ï¼Œç»“åˆLLMæ¨ç†èƒ½åŠ›ä¸åŸºäºæ‰‹åŠ¨æ ‡ç­¾çš„EDç®¡é“ã€‚ZeroEDé€šè¿‡å››ä¸ªæ­¥éª¤æ“ä½œï¼šç‰¹å¾è¡¨ç¤ºã€é”™è¯¯æ ‡ç­¾ã€è®­ç»ƒæ•°æ®æ„å»ºå’Œæ£€æµ‹å™¨è®­ç»ƒã€‚é¦–å…ˆï¼Œä½¿ç”¨é”™è¯¯åŸå› æ„ŸçŸ¥äºŒå…ƒç‰¹å¾ã€é¢„è®­ç»ƒåµŒå…¥å’Œç»Ÿè®¡ç‰¹å¾æ¥å¢å¼ºé”™è¯¯åŒºåˆ†åº¦ã€‚ç„¶åï¼Œå€ŸåŠ©LLMé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å¯¹é”™è¯¯è¿›è¡Œæ•´ä½“æ ‡ç­¾ï¼Œå¹¶é€šè¿‡ä¸¤æ­¥æ¨ç†è¿‡ç¨‹æä¾›è¯¦ç»†çš„é”™è¯¯æ£€æµ‹æŒ‡å—ã€‚ä¸ºé™ä½ä»¤ç‰Œæˆæœ¬ï¼Œä»…å¯¹é€šè¿‡èšç±»é‡‡æ ·é€‰æ‹©çš„æœ‰ä»£è¡¨æ€§çš„æ•°æ®åº”ç”¨LLMã€‚é€šè¿‡é›†ç¾¤å†…æ ‡ç­¾ä¼ æ’­å’ŒLLMéªŒè¯æ„å»ºé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚æœ€åï¼Œè®­ç»ƒåˆ†ç±»å™¨ä»¥æ£€æµ‹æ‰€æœ‰é”™è¯¯ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒZeroEDåœ¨F1åˆ†æ•°ä¸Šæœ€å¤šé«˜å‡º30%ï¼Œä»¤ç‰Œæˆæœ¬æœ€å¤šå‡å°‘90%ï¼Œæ˜æ˜¾ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é”™è¯¯æ£€æµ‹åœ¨è¡¨æ ¼æ•°æ®ä¸­éå¸¸é‡è¦ä¸”å…·æŒ‘æˆ˜æ€§ï¼Œå› é”™è¯¯ç±»å‹å¤šæ ·ä¸”éœ€ç†è§£ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä¼ ç»Ÿé”™è¯¯æ£€æµ‹æ–¹æ³•ä¾èµ–å¤§é‡æ‰‹åŠ¨æ ‡ç­¾ï¼Œå·¥ä½œé‡å¤§ä¸”æ•ˆç‡ä¸é«˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¯å‡å°‘äººåŠ›æŠ•å…¥ï¼Œä½†åœ¨å…¨é¢ç†è§£æ•°æ®ä¸Šä¸‹æ–‡æ–¹é¢å¯èƒ½é‡åˆ°å›°éš¾ã€‚</li>
<li>ZeroEDæ˜¯ä¸€ç§æ–°å‹é›¶æ ·æœ¬é”™è¯¯æ£€æµ‹æ¡†æ¶ï¼Œç»“åˆäº†LLMæ¨ç†èƒ½åŠ›ä¸åŸºäºæ‰‹åŠ¨æ ‡ç­¾çš„EDç®¡é“ã€‚</li>
<li>ZeroEDé€šè¿‡å››ä¸ªæ­¥éª¤æ“ä½œï¼šç‰¹å¾è¡¨ç¤ºä»¥å¢å¼ºé”™è¯¯åŒºåˆ†åº¦ã€é”™è¯¯æ ‡ç­¾ã€è®­ç»ƒæ•°æ®æ„å»ºå’Œæ£€æµ‹å™¨è®­ç»ƒã€‚</li>
<li>ZeroEDä½¿ç”¨LLMè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ä»¥æä¾›å…¨é¢çš„é”™è¯¯æ ‡ç­¾å’Œè¯¦ç»†çš„é”™è¯¯æ£€æµ‹æŒ‡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31389924da1490886c03564aea5b8d6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d1da0c0eb71005e3c558df8d141c7dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6987fab82de7db6ce18041b2ac82fd4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29346fd76459f195b2708a3dc24e6892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebc307c8ff988231db9228dfc8a8db4d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks"><a href="#VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks" class="headerlink" title="VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks"></a>VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks</h2><p><strong>Authors:Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, Lin Yan</strong></p>
<p>We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of $\mathbf{60.4}$. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†VAPOï¼ŒåŸºäºä»·å€¼çš„å¢å¼ºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼ˆValue-based Augmented Proximal Policy Optimization frameworkï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŸºäºä»·å€¼çš„èŒƒå¼ä¸­çš„æ¨ç†æ¨¡å‹é‡èº«å®šåˆ¶çš„æ–°å‹æ¡†æ¶ã€‚ä»¥AIME 2024æ•°æ®é›†ä¸ºåŸºå‡†ï¼ŒåŸºäºQwen 32Bé¢„è®­ç»ƒæ¨¡å‹çš„VAPOè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¾—åˆ†60.4åˆ†ã€‚åœ¨ç›¸åŒçš„å®éªŒè®¾ç½®ä¸‹ç›´æ¥æ¯”è¾ƒï¼ŒVAPOçš„æ€§èƒ½è¶…è¿‡äº†ä¹‹å‰æŠ¥å‘Šçš„DeepSeek-R1-Zero-Qwen-32Bå’ŒDAPOçš„ç»“æœè¶…è¿‡10åˆ†ã€‚VAPOçš„è®­ç»ƒè¿‡ç¨‹ä»¥å…¶ç¨³å®šæ€§å’Œé«˜æ•ˆæ€§è€Œè„±é¢–è€Œå‡ºã€‚å®ƒä»…åœ¨5000æ­¥å†…å°±è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨å¤šæ¬¡ç‹¬ç«‹è¿è¡Œä¸­ï¼Œæ²¡æœ‰å‡ºç°è®­ç»ƒå´©æºƒçš„æƒ…å†µï¼Œè¯æ˜äº†å…¶å¯é æ€§ã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†åŸºäºä»·å€¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶çš„é•¿é“¾æ€ç»´ï¼ˆlong-CoTï¼‰æ¨ç†ã€‚æˆ‘ä»¬ç¡®å®šäº†å›°æ‰°åŸºäºä»·å€¼æ–¹æ³•çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä»·å€¼æ¨¡å‹åè§ã€å­˜åœ¨å¼‚è´¨åºåˆ—é•¿åº¦ä»¥åŠå¥–åŠ±ä¿¡å·ç¨€ç–ã€‚é€šè¿‡ç³»ç»Ÿè®¾è®¡ï¼ŒVAPOæä¾›äº†ä¸€ä¸ªç»¼åˆè§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†è¿™äº›æŒ‘æˆ˜ï¼Œä»è€Œåœ¨é•¿é“¾æ€ç»´æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†å¢å¼ºçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05118v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VAPOï¼Œä¸€ç§é’ˆå¯¹ä»·å€¼åŸºç¡€èŒƒå¼ä¸­çš„æ¨ç†æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œè¢«æå‡ºå¹¶ç”¨äºAIME 2024æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚åŸºäºQwen 32Bé¢„è®­ç»ƒæ¨¡å‹çš„VAPOè¾¾åˆ°äº†60.4çš„åˆ›çºªå½•åˆ†æ•°ã€‚åœ¨ç›¸åŒçš„å®éªŒè®¾ç½®ä¸‹ï¼ŒVAPOè¾ƒä¹‹å‰çš„DeepSeek-R1-Zero-Qwen-32Bå’ŒDAPOçš„ç»“æœé«˜å‡ºè¶…è¿‡10åˆ†ã€‚å…¶è®­ç»ƒè¿‡ç¨‹ç¨³å®šé«˜æ•ˆï¼Œèƒ½åœ¨çŸ­çŸ­5000æ­¥å†…è¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚æ­¤å¤–ï¼ŒVAPOèƒ½å¤Ÿè§£å†³ä»·å€¼åŸºç¡€æ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä»·å€¼æ¨¡å‹åè§ã€ä¸åŒåºåˆ—é•¿åº¦çš„å­˜åœ¨å’Œå¥–åŠ±ä¿¡å·çš„ç¨€ç–æ€§ã€‚é€šè¿‡ç³»ç»Ÿè®¾è®¡ï¼ŒVAPOæä¾›äº†ä¸€ä¸ªç»¼åˆè§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†è¿™äº›æŒ‘æˆ˜ï¼Œä»è€Œåœ¨é•¿é“¾æ€ç»´æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VAPOæ˜¯ä¸€ä¸ªé’ˆå¯¹ä»·å€¼åŸºç¡€èŒƒå¼ä¸­çš„æ¨ç†æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿é“¾æ€ç»´æ¨ç†ä»»åŠ¡ã€‚</li>
<li>åœ¨AIME 2024æ•°æ®é›†ä¸Šï¼ŒVAPOè¾¾åˆ°äº†åˆ›çºªå½•çš„60.4åˆ†ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>VAPOè®­ç»ƒè¿‡ç¨‹ç¨³å®šé«˜æ•ˆï¼Œèƒ½åœ¨çŸ­æ—¶é—´å†…è¾¾åˆ°æœ€ä½³æ€§èƒ½çŠ¶æ€ã€‚</li>
<li>VAPOè§£å†³äº†ä»·å€¼åŸºç¡€æ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä»·å€¼æ¨¡å‹åè§ã€ä¸åŒåºåˆ—é•¿åº¦çš„å­˜åœ¨å’Œå¥–åŠ±ä¿¡å·çš„ç¨€ç–æ€§ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿè®¾è®¡ï¼ŒVAPOæä¾›äº†å¯¹ä»¥ä¸ŠæŒ‘æˆ˜çš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>VAPOå…·æœ‰é«˜åº¦çš„å¯é æ€§ï¼Œå¤šæ¬¡ç‹¬ç«‹è¿è¡Œä¸­æœªå‡ºç°è®­ç»ƒå´©æºƒçš„æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa7e4202422f19ccb14aa9c289f38572.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning"><a href="#Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning" class="headerlink" title="Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning"></a>Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning</h2><p><strong>Authors:Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºç¬¦åˆäººç±»åå¥½çš„å…³é”®æŠ€æœ¯ã€‚ä¸ºäº†å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œå¤§å¤šæ•°ç°æœ‰çš„RLHFç®—æ³•ä½¿ç”¨Bradley-Terryæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¾èµ–äºå¯èƒ½æ— æ³•åæ˜ ç°å®ä¸–ç•Œåˆ¤æ–­å¤æ‚æ€§å’Œå¤šå˜æ€§çš„äººç±»åå¥½å‡è®¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„ç®—æ³•ï¼Œä»¥æé«˜åœ¨è¿™ç§å¥–åŠ±æ¨¡å‹è¯¯åˆ¤ä¸‹çš„ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬çš„ç®—æ³•é™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°ç®—å™¨çš„æ–¹å·®ï¼Œä»è€Œæé«˜äº†åæ‚”ç•Œã€‚åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç®—æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šï¼Œæœ‰7 ç»“æˆä¸ºå­¦ç•Œå¹¿æ³›å…³æ³¨çš„çƒ­ç‚¹ã€‚å› æ­¤æœ¬æ–‡æå‡ºä¸€ç§ç¨³å¥çš„ç®—æ³•ï¼Œæ—¨åœ¨æé«˜åœ¨å¥–åŠ±æ¨¡å‹ä¸å‡†ç¡®æ—¶çš„ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚è¯¥ç®—æ³•èƒ½æœ‰æ•ˆå‡å°‘å¥–åŠ±ä¸ç­–ç•¥ä¼°ç®—å™¨çš„å˜å¼‚æ€§ï¼Œä»è€Œæé«˜é™åˆ¶åæ‚”çš„ç†è®ºè¾¹ç•Œã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ç®—æ³•è¡¨ç°å“è¶Šï¼Œåœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šçš„å“åº”ç‡é«˜è¾¾77%-81%ï¼Œè¶…è¶ŠåŸºçº¿æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03784v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½çš„å…³é”®æŠ€æœ¯ã€‚ç°æœ‰å¤§å¤šæ•°RLHFç®—æ³•ä½¿ç”¨Bradley-Terryæ¨¡å‹æ¥å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œè¿™ä¾èµ–äºå¯èƒ½æ— æ³•åæ˜ ç°å®ä¸–ç•Œåˆ¤æ–­å¤æ‚æ€§å’Œå¯å˜æ€§çš„å‡è®¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å¥çš„ç®—æ³•ï¼Œä»¥æé«˜åœ¨å¥–åŠ±æ¨¡å‹è¯¯æŒ‡å®šæƒ…å†µä¸‹çš„ç°æœ‰æ–¹æ³•æ€§èƒ½ã€‚ç†è®ºä¸Šï¼Œè¯¥ç®—æ³•é™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°è®¡é‡çš„æ–¹å·®ï¼Œæé«˜äº†åæ‚”ç•Œã€‚åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç®—æ³•å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šï¼Œ77-81%çš„å“åº”ä¼˜äºåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯ç”¨äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½ã€‚</li>
<li>ç°æœ‰RLHFç®—æ³•ä¸»è¦ä½¿ç”¨Bradley-Terryæ¨¡å‹ï¼Œå­˜åœ¨å¯¹ç°å®ä¸–ç•Œåˆ¤æ–­çš„å¤æ‚æ€§å’Œå¯å˜æ€§çš„å‡è®¾ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å¥çš„ç®—æ³•ï¼Œæ—¨åœ¨æé«˜åœ¨å¥–åŠ±æ¨¡å‹è¯¯æŒ‡å®šæƒ…å†µä¸‹çš„ç°æœ‰æ–¹æ³•æ€§èƒ½ã€‚</li>
<li>è¯¥ç®—æ³•ç†è®ºä¸Šé™ä½äº†å¥–åŠ±å’Œæ”¿ç­–ä¼°è®¡é‡çš„æ–¹å·®ï¼Œæé«˜äº†åæ‚”ç•Œã€‚</li>
<li>åœ¨LLMåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œæ–°ç®—æ³•æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨Anthropicæœ‰ç›Šå’Œæ— å®³æ•°æ®é›†ä¸Šï¼Œæ–°ç®—æ³•çš„å“åº”ä¼˜äºåŸºå‡†æµ‹è¯•çš„æ¯”ä¾‹è¾¾åˆ°77-81%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d4e4f21d8505ad2aa777c73d77a33ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e11698f7eb32422462ad842f284e0178.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f710b8c6511a6ab1436f675dc3c4aa43.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CrowdVLM-R1-Expanding-R1-Ability-to-Vision-Language-Model-for-Crowd-Counting-using-Fuzzy-Group-Relative-Policy-Reward"><a href="#CrowdVLM-R1-Expanding-R1-Ability-to-Vision-Language-Model-for-Crowd-Counting-using-Fuzzy-Group-Relative-Policy-Reward" class="headerlink" title="CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd   Counting using Fuzzy Group Relative Policy Reward"></a>CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd   Counting using Fuzzy Group Relative Policy Reward</h2><p><strong>Authors:Zhiqiang Wang, Pengbin Feng, Yanbin Lin, Shuzhang Cai, Zongao Bian, Jinghua Yan, Xingquan Zhu</strong></p>
<p>We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that integrates Group Relative Policy Optimization (GRPO) with a fuzzy reward function to enhance learning efficiency. Unlike the conventional binary 0&#x2F;1 accuracy reward, our fuzzy reward model provides nuanced incentives, encouraging more precise outputs. Experimental results demonstrate that GRPO with a standard 0&#x2F;1 accuracy reward underperforms compared to supervised fine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B), surpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across five in-domain datasets. On an out-of-domain dataset, FGRPR achieves performance comparable to SFT but excels when target values are larger, as its fuzzy reward function assigns higher rewards to closer approximations. This approach is broadly applicable to tasks where the precision of the answer is critical. Code and data: <a target="_blank" rel="noopener" href="https://github.com/yeyimilk/CrowdVLM-R1">https://github.com/yeyimilk/CrowdVLM-R1</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†æ¨¡ç³Šç»„ç›¸å¯¹ç­–ç•¥å¥–åŠ±ï¼ˆFGRPRï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸æ¨¡ç³Šå¥–åŠ±å‡½æ•°ç›¸ç»“åˆï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„äºŒå…ƒ0&#x2F;1å‡†ç¡®ç‡å¥–åŠ±ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡ç³Šå¥–åŠ±æ¨¡å‹æä¾›äº†å¾®å¦™çš„æ¿€åŠ±ï¼Œé¼“åŠ±æ›´ç²¾ç¡®çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ ‡å‡†0&#x2F;1å‡†ç¡®ç‡å¥–åŠ±çš„GRPOåœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é¢å‰è¡¨ç°ä¸ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåº”ç”¨äºQwen2.5-VLï¼ˆ3Bå’Œ7Bï¼‰çš„FGRPRåœ¨æ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT4oã€LLaMA2ï¼ˆ90Bï¼‰å’ŒSFTï¼‰ä¸Šè¡¨ç°å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè·¨äº”ä¸ªé¢†åŸŸå†…çš„æ•°æ®é›†ã€‚åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šï¼ŒFGRPRçš„æ€§èƒ½ä¸SFTç›¸å½“ï¼Œä½†å½“ç›®æ ‡å€¼è¾ƒå¤§æ—¶è¡¨ç°æ›´å‡ºè‰²ï¼Œå› ä¸ºå…¶æ¨¡ç³Šå¥–åŠ±å‡½æ•°ä¼šå‘æ¥è¿‘çš„è¿‘ä¼¼å€¼åˆ†é…æ›´é«˜çš„å¥–åŠ±ã€‚è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºç­”æ¡ˆç²¾ç¡®åº¦è‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/yeyimilk/CrowdVLM-R1">https://github.com/yeyimilk/CrowdVLM-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03724v1">PDF</a> 11 pages, 6 figures and 4 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¨¡ç³Šå¥–åŠ±å‡½æ•°çš„æ¨¡ç³Šç»„ç›¸å¯¹ç­–ç•¥å¥–åŠ±ï¼ˆFGRPRï¼‰æ¡†æ¶ç»“åˆäº†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„äºŒå…ƒ0&#x2F;1ç²¾åº¦å¥–åŠ±ï¼Œæ¨¡ç³Šå¥–åŠ±æ¨¡å‹æä¾›æ›´ç²¾ç»†çš„æ¿€åŠ±ï¼Œä¿ƒè¿›æ›´ç²¾ç¡®çš„è¾“å‡ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æ ‡å‡†0&#x2F;1ç²¾åº¦å¥–åŠ±çš„GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸å¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚è€Œåº”ç”¨äºQwen2.5-VL(åŒ…æ‹¬è§„æ¨¡3Bå’Œ7Bçš„æ¨¡å‹æµ‹è¯•åœºæ™¯ä¸­ï¼Œç»“åˆäº†æ¨¡ç³Šå¥–åŠ±å‡½æ•°çš„FGRPRæ¡†æ¶åœ¨äº”ä¸ªåŒé¢†åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æ‰€æœ‰åŸºå‡†æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT4oå’ŒLLaMAç³»åˆ—ã€‚å¯¹äºç›®æ ‡å€¼è¾ƒå¤§çš„æƒ…å†µï¼ŒFGRPRè¡¨ç°å¾—å°¤ä¸ºå‡ºè‰²ã€‚è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºç­”æ¡ˆç²¾åº¦è‡³å…³é‡è¦çš„ä»»åŠ¡ä¸­ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FGRPRç»“åˆäº†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸æ¨¡ç³Šå¥–åŠ±å‡½æ•°æ¥æå‡å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>ä¼ ç»ŸäºŒå…ƒç²¾åº¦å¥–åŠ±åœ¨æŸäº›åœºæ™¯ä¸‹æ•ˆæœæœ‰é™ï¼Œè€Œæ¨¡ç³Šå¥–åŠ±æ¨¡å‹èƒ½æä¾›æ›´ä¸ºç²¾ç»†çš„æ¿€åŠ±ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œç»“åˆæ¨¡ç³Šå¥–åŠ±å‡½æ•°çš„FGRPRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–åŸºå‡†æ¨¡å‹ã€‚</li>
<li>FGRPRåœ¨ç›®æ ‡å€¼è¾ƒå¤§çš„æƒ…å†µä¸‹è¡¨ç°å¾—å°¤å…¶å‡ºè‰²ã€‚ç”±äºå…¶è¾ƒé«˜çš„ç²¾ç¡®åº¦å’Œå¯¹ç»†èŠ‚çš„å…³æ³¨ç¨‹åº¦æé«˜ä½¿å…¶åœ¨ç²¾ç¡®æ§åˆ¶ç±»çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db95411299b350032cc534ca43c06349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b15a0a3e84f95d690db1df587c4b051b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d68acb4208e39e11c9d8177cecaa4e36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c77a6cb5aa82b7f5dadc10098df52beb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-034b77393ac7b2c9e9b318fd5b412fb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-657772080b45137057d4d9ee672ae243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5311f337d2940eced9fdf2fa4496289.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments"><a href="#DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments" class="headerlink" title="DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments"></a>DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments</h2><p><strong>Authors:Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu</strong></p>
<p>Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher">https://github.com/GAIR-NLP/DeepResearcher</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é…å¤‡äº†ç½‘ç»œæœç´¢èƒ½åŠ›ï¼Œå·²æ˜¾ç¤ºå‡ºç”¨äºæ·±åº¦ç ”ç©¶ä»»åŠ¡çš„æƒŠäººæ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„æç¤ºï¼ˆåŸºäºæç¤ºçš„å·¥ç¨‹ï¼‰è¡¨ç°ä¸ç¨³å®šï¼Œæˆ–è€…åœ¨å—æ§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç¯å¢ƒä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆåŸºäºRAGçš„æ–¹æ³•ï¼‰ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œäº’åŠ¨çš„å¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DeepResearcherï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ç«¯åˆ°ç«¯è®­ç»ƒåŸºäºLLMçš„æ·±åº¦ç ”ç©¶ä»£ç†çš„ç»¼åˆæ¡†æ¶ï¼Œé€šè¿‡ä¸çœŸå®çš„ç½‘ç»œæœç´¢äº’åŠ¨å®ç°è§„æ¨¡åŒ–ã€‚ä¸åŒäºå‡è®¾æ‰€æœ‰å¿…è¦ä¿¡æ¯éƒ½å­˜åœ¨äºå›ºå®šè¯­æ–™åº“ä¸­çš„åŸºäºRAGçš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒä»£ç†ä»¥åº”å¯¹å¼€æ”¾ç½‘ç»œçš„å˜ˆæ‚ã€éç»“æ„åŒ–å’ŒåŠ¨æ€çš„ç‰¹æ€§ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ç§ä¸“ç”¨å¤šä»£ç†æ¶æ„ï¼Œæµè§ˆä»£ç†ä»å„ç§ç½‘é¡µç»“æ„ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å…‹æœé‡å¤§æŠ€æœ¯æŒ‘æˆ˜ã€‚åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeepResearcherè¾ƒåŸºäºæç¤ºçš„å·¥ç¨‹åŸºçº¿å®ç°äº†é«˜è¾¾28.9ç‚¹çš„å®è´¨æ€§æ”¹è¿›ï¼Œè¾ƒåŸºäºRAGçš„RLä»£ç†ä¹Ÿå®ç°äº†é«˜è¾¾7.2ç‚¹çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å®šæ€§åˆ†ææ­ç¤ºäº†æ¥è‡ªç«¯åˆ°ç«¯RLè®­ç»ƒçš„çªå‘è®¤çŸ¥è¡Œä¸ºï¼ŒåŒ…æ‹¬åˆ¶å®šè®¡åˆ’çš„èƒ½åŠ›ã€ä»å¤šä¸ªæ¥æºè¿›è¡Œäº¤å‰éªŒè¯ä¿¡æ¯çš„èƒ½åŠ›ã€å‚ä¸è‡ªæˆ‘åæ€ä»¥é‡æ–°å®šå‘ç ”ç©¶çš„èƒ½åŠ›ï¼Œä»¥åŠåœ¨æ— æ³•æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œåœ¨çœŸå®ä¸–ç•Œç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä¸ä»…æ˜¯å®ç°ç»†èŠ‚ï¼Œè€Œä¸”æ˜¯å¼€å‘ä¸ç°å®ä¸–ç•Œåº”ç”¨å¯¹é½çš„ç¨³å¥ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher%E5%8F%91%E5%B8%83%E4%BA%86DeepResearcher%E3%80%82">https://github.com/GAIR-NLP/DeepResearcherå‘å¸ƒäº†DeepResearcherã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03160v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆç½‘ç»œæœç´¢èƒ½åŠ›åœ¨æ·±ç ”ç©¶ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹åŠ¨å·¥ç¨‹æç¤ºæˆ–å¼ºåŒ–å­¦ä¹ åœ¨å—æ§æ£€ç´¢å¢å¼ºç”Ÿæˆç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œæ€§èƒ½ä¸ç¨³å®šä¸”æ— æ³•æ•æ‰çœŸå®ä¸–ç•Œäº’åŠ¨çš„å¤æ‚æ€§ã€‚æœ¬æ–‡ä»‹ç»DeepResearcherï¼Œå®ƒæ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„é¦–ä¸ªLLMæ·±ç ”ç©¶ä»£ç†ç»¼åˆæ¡†æ¶ï¼Œèƒ½å¤Ÿåº”å¯¹å¼€æ”¾ç½‘ç»œçš„å™ªå£°ã€éç»“æ„åŒ–å’ŒåŠ¨æ€ç‰¹æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDeepResearcheråœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šè¾ƒåŸºäºæç¤ºçš„å·¥ç¨‹æ–¹æ³•å’ŒåŸºäºRAGçš„RLä»£ç†æœ‰æ˜¾è‘—æ”¹å–„ã€‚æœ¬æ–‡æ­ç¤ºäº†ä»ç«¯åˆ°ç«¯RLè®­ç»ƒä¸­æ¶Œç°å‡ºçš„è®¤çŸ¥è¡Œä¸ºï¼ŒåŒ…æ‹¬åˆ¶å®šè®¡åˆ’ã€è·¨æºéªŒè¯ä¿¡æ¯ã€è‡ªæˆ‘åæ€ä»¥è°ƒæ•´ç ”ç©¶æ–¹å‘ä»¥åŠæ— æ³•æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®ä¸–ç•Œç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä¸ä»…æ˜¯å®ç°ç»†èŠ‚ï¼Œæ›´æ˜¯å¼€å‘ç¬¦åˆå®é™…éœ€æ±‚éœ€æ±‚çš„ç¨³å¥ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆç½‘ç»œæœç´¢èƒ½åŠ›ç”¨äºæ·±ç ”ç©¶ä»»åŠ¡è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨å·¥ç¨‹æç¤ºæˆ–å—æ§ç¯å¢ƒä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ€§èƒ½æœ‰é™ã€‚</li>
<li>DeepResearcheræ˜¯é¦–ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„LLMæ·±ç ”ç©¶ä»£ç†ç»¼åˆæ¡†æ¶ã€‚</li>
<li>DeepResearcherèƒ½åœ¨çœŸå®ä¸–ç•Œçš„å¼€æ”¾ã€å™ªå£°ã€éç»“æ„åŒ–ç½‘ç»œä¸­è¿›è¡Œä¿¡æ¯æ£€ç´¢å’Œå¯¼èˆªã€‚</li>
<li>ä¸åŸºäºæç¤ºçš„å·¥ç¨‹æ–¹æ³•å’ŒåŸºäºRAGçš„RLä»£ç†ç›¸æ¯”ï¼ŒDeepResearcheråœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>ç«¯åˆ°ç«¯RLè®­ç»ƒä¸­å‡ºç°è®¤çŸ¥è¡Œä¸ºï¼Œå¦‚åˆ¶å®šè®¡åˆ’ã€è·¨æºéªŒè¯ã€è‡ªæˆ‘åæ€ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d13bba323910f2d99a9e6b9cea40bf9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fca4243f7e11b0d587e275a3743f5a51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be48e5abcc893510cef1f35226280b92.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Envisioning-Beyond-the-Pixels-Benchmarking-Reasoning-Informed-Visual-Editing"><a href="#Envisioning-Beyond-the-Pixels-Benchmarking-Reasoning-Informed-Visual-Editing" class="headerlink" title="Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual   Editing"></a>Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual   Editing</h2><p><strong>Authors:Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, Haodong Duan</strong></p>
<p>Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/RISEBench">https://github.com/PhoenixZ810/RISEBench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†RISEBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨ç†æŒ‡å¯¼çš„è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„åŸºå‡†æµ‹è¯•ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªç±»åˆ«ç²¾å¿ƒç­–åˆ’äº†é«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡äººç±»è¯„å§”å’ŒLMM-as-a-judgeæ–¹æ³•è¯„ä¼°æŒ‡ä»¤æ¨ç†ã€å¤–è§‚ä¸€è‡´æ€§å’Œè§†è§‰å¯è¡Œæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGPT-4o-Nativeæ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œä½†å³ä½¿æ˜¯æœ€å…ˆè¿›çš„ç³»ç»Ÿä¹Ÿåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šé¢ä¸´å›°éš¾ï¼Œè¿™å‡¸æ˜¾äº†ä¸€ä¸ªä»è¢«å¿½è§†çš„é¢†åŸŸã€‚ä½œä¸ºåˆæ­¥å°è¯•ï¼ŒRISEBenchæ—¨åœ¨æä¾›å…³äºæ¨ç†æ„ŸçŸ¥è§†è§‰ç¼–è¾‘çš„åŸºç¡€è§è§£ï¼Œå¹¶å‚¬åŒ–æœªæ¥çš„ç ”ç©¶ã€‚å°½ç®¡ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œä½†æˆ‘ä»¬è‡´åŠ›äºä¸æ–­æ‰©å±•å’Œç²¾ç‚¼è¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä»¥æ”¯æŒå¯¹ä¸‹ä¸€ä»£å¤šæ¨¡æ€ç³»ç»Ÿæ›´å…¨é¢ã€å¯é å’Œå¯æ‰©å±•çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/RISEBench%E4%B8%8A%E5%B8%A%E4%BA%A4%E7%82%B9%E6%B6%AF%E3%80%82~">https://github.com/PhoenixZ810/RISEBenchä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02826v2">PDF</a> 27 pages, 23 figures, 1 table. Technical Report</p>
<p><strong>Summary</strong><br>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RISEBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ¨ç†ä¿¡æ¯è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„åŸºå‡†æµ‹è¯•ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªç±»åˆ«ç²¾å¿ƒç­–åˆ’äº†é«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡äººç±»è¯„å§”å’ŒLMM-as-a-judgeçš„æ–¹æ³•è¯„ä¼°æŒ‡ä»¤æ¨ç†ã€å¤–è§‚ä¸€è‡´æ€§å’Œè§†è§‰å¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGPT-4o-Nativeåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚ä½œä¸ºåˆæ­¥å°è¯•ï¼ŒRISEBenchæ—¨åœ¨ä¸ºæ¨ç†æ„ŸçŸ¥è§†è§‰ç¼–è¾‘æä¾›åŸºç¡€è§è§£ï¼Œå¹¶å‚¬åŒ–æœªæ¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/RISEBench">https://github.com/PhoenixZ810/RISEBench</a>å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘ä»å­˜æŒ‘æˆ˜ã€‚</li>
<li>RISEBenchæ˜¯é¦–ä¸ªé’ˆå¯¹æ¨ç†ä¿¡æ¯è§†è§‰ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ï¼Œèšç„¦å››ç§å…³é”®æ¨ç†ç±»å‹ã€‚</li>
<li>RISEBenchè¯„ä¼°æŒ‡ä»¤æ¨ç†ã€å¤–è§‚ä¸€è‡´æ€§å’Œè§†è§‰å¯ä¿¡åº¦ã€‚</li>
<li>GPT-4o-Nativeåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>RISEBenchæ—¨åœ¨ä¸ºæ¨ç†æ„ŸçŸ¥è§†è§‰ç¼–è¾‘æä¾›åŸºç¡€è§è§£ï¼Œå‚¬åŒ–ç›¸å…³ç ”ç©¶ã€‚</li>
<li>RISEBenchå°†ä¸æ–­æ‰©å±•å’Œå®Œå–„ï¼Œä»¥æ”¯æŒå¯¹ä¸‹ä¸€ä»£å¤šæ¨¡æ€ç³»ç»Ÿæ›´å…¨é¢ã€å¯é å’Œå¯ä¼¸ç¼©çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-378b83ccb5285b2e37e4b7e969baa078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b224c6dc21a4b101242b8066d25ced8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67227a73fb8c3bb609df8d641074f313.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models"><a href="#Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models" class="headerlink" title="Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models"></a>Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models</h2><p><strong>Authors:Zhaochen Wang, Bryan Hooi, Yiwei Wang, Ming-Hsuan Yang, Zi Huang, Yujun Cai</strong></p>
<p>Vision-language models (VLMs) have advanced rapidly in processing multimodal information, but their ability to reconcile conflicting signals across modalities remains underexplored. This work investigates how VLMs process ASCII art, a unique medium where textual elements collectively form visual patterns, potentially creating semantic-visual conflicts. We introduce a novel evaluation framework that systematically challenges five state-of-the-art models (including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where character-level semantics deliberately contradict global visual patterns. Our experiments reveal a strong text-priority bias: VLMs consistently prioritize textual information over visual patterns, with visual recognition ability declining dramatically as semantic complexity increases. Various mitigation attempts through visual parameter tuning and prompt engineering yielded only modest improvements, suggesting that this limitation requires architectural-level solutions. These findings uncover fundamental flaws in how current VLMs integrate multimodal information, providing important guidance for future model development while highlighting significant implications for content moderation systems vulnerable to adversarial examples. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å…¶åœ¨è°ƒå’Œè·¨æ¨¡æ€å†²çªä¿¡å·æ–¹é¢çš„èƒ½åŠ›ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†VLMså¦‚ä½•å¤„ç†ASCIIè‰ºæœ¯è¿™ä¸€ç‹¬ç‰¹åª’ä»‹ï¼Œå…¶ä¸­æ–‡æœ¬å…ƒç´ å…±åŒå½¢æˆè§†è§‰æ¨¡å¼ï¼Œå¯èƒ½äº§ç”Ÿè¯­ä¹‰è§†è§‰å†²çªã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å¯¹æŠ—æ€§ASCIIè‰ºæœ¯ç³»ç»Ÿåœ°æŒ‘æˆ˜äº†äº”ç§æœ€æ–°æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Claudeå’ŒGeminiï¼‰ï¼Œå…¶ä¸­å­—ç¬¦çº§è¯­ä¹‰æ•…æ„ä¸å…¨å±€è§†è§‰æ¨¡å¼ç›¸çŸ›ç›¾ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä¸€ä¸ªå¼ºçƒˆçš„æ–‡æœ¬ä¼˜å…ˆåè§ï¼šVLMså§‹ç»ˆä¼˜å…ˆå¤„ç†æ–‡æœ¬ä¿¡æ¯è€Œéè§†è§‰æ¨¡å¼ï¼Œéšç€è¯­ä¹‰å¤æ‚æ€§çš„å¢åŠ ï¼Œå…¶è§†è§‰è¯†åˆ«èƒ½åŠ›æ€¥å‰§ä¸‹é™ã€‚é€šè¿‡è§†è§‰å‚æ•°è°ƒæ•´å’Œæç¤ºå·¥ç¨‹è¿›è¡Œçš„å„ç§ç¼“è§£å°è¯•ä»…äº§ç”Ÿäº†å¾®å°çš„æ”¹å–„ï¼Œè¿™è¡¨æ˜è¿™ä¸€å±€é™æ€§éœ€è¦æ¶æ„çº§çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰VLMså¦‚ä½•æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯çš„åŸºæœ¬ç¼ºé™·ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦æŒ‡å¯¼ï¼ŒåŒæ—¶å¼ºè°ƒäº†å¯¹äºæ˜“å—å¯¹æŠ—æ€§æ ·æœ¬å½±å“çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿæ‰€å­˜åœ¨çš„é‡å¤§å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01589v2">PDF</a> Under review at COLM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†ASCIIè‰ºæœ¯æ—¶çš„è¡¨ç°ï¼Œè¿™æ˜¯ä¸€ç§æ–‡æœ¬å…ƒç´ å…±åŒå½¢æˆè§†è§‰å›¾æ¡ˆçš„ç‰¹æ®Šåª’ä»‹ï¼Œå¯èƒ½ä¼šäº§ç”Ÿè¯­ä¹‰è§†è§‰å†²çªã€‚ç ”ç©¶é€šè¿‡å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°æŒ‘æˆ˜äº†äº”ç§å…ˆè¿›çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Claudeå’ŒGeminiï¼‰ï¼Œä½¿ç”¨å¯¹æŠ—æ€§ASCIIè‰ºæœ¯è¿›è¡Œæµ‹è¯•ï¼Œå…¶ä¸­å­—ç¬¦çº§åˆ«çš„è¯­ä¹‰ä¸å…¨å±€è§†è§‰æ¨¡å¼å­˜åœ¨å†²çªã€‚å®éªŒè¡¨æ˜ï¼ŒVLMså­˜åœ¨å¼ºçƒˆçš„æ–‡æœ¬ä¼˜å…ˆåè§ï¼Œéšç€è¯­ä¹‰å¤æ‚æ€§çš„å¢åŠ ï¼Œå…¶å¯¹è§†è§‰æ¨¡å¼çš„è¯†åˆ«èƒ½åŠ›æ€¥å‰§ä¸‹é™ã€‚å°è¯•é€šè¿‡è§†è§‰å‚æ•°è°ƒæ•´å’Œæç¤ºå·¥ç¨‹è¿›è¡Œç¼“è§£ï¼Œä½†æ•ˆæœæœ‰é™ï¼Œè¡¨æ˜è¿™ä¸€å±€é™æ€§éœ€è¦æ¶æ„çº§åˆ«çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰VLMsåœ¨æ•´åˆå¤šæ¨¡å¼ä¿¡æ¯æ—¶çš„æ ¹æœ¬ç¼ºé™·ï¼Œä¸ºæœªæ¥æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå¹¶å¼ºè°ƒäº†å¯¹æŠ—æ€§ç¤ºä¾‹å¯¹å†…å®¹å®¡æ ¸ç³»ç»Ÿçš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤„ç†ASCIIè‰ºæœ¯æ—¶é¢ä¸´è¯­ä¹‰è§†è§‰å†²çªçš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶ä»¥æµ‹è¯•VLMsåœ¨ASCIIè‰ºæœ¯å¤„ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>VLMsè¡¨ç°å‡ºå¼ºçƒˆçš„æ–‡æœ¬ä¼˜å…ˆåè§ã€‚</li>
<li>éšç€è¯­ä¹‰å¤æ‚æ€§çš„å¢åŠ ï¼ŒVLMsçš„è§†è§‰è¯†åˆ«èƒ½åŠ›æ€¥å‰§ä¸‹é™ã€‚</li>
<li>ç›®å‰çš„ç¼“è§£æ–¹æ³•ï¼ˆå¦‚è§†è§‰å‚æ•°è°ƒæ•´å’Œæç¤ºå·¥ç¨‹ï¼‰æ•ˆæœæœ‰é™ã€‚</li>
<li>éœ€è¦æ¶æ„çº§åˆ«çš„è§£å†³æ–¹æ¡ˆæ¥æ”¹å–„VLMsçš„å¤šæ¨¡å¼ä¿¡æ¯æ•´åˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fba226ac1c24d88730e225457b144dec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e81cbb8a73316ed933e2d19a7e996c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca9d360c92ab8688ac92701da6266211.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c94b39072d6453daedc922c65ae233f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e4a1a374783db52a0f60de721c3e442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9e8a907e9fccd8ab3c9bca813807f41.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks"><a href="#FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks" class="headerlink" title="FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks"></a>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks</h2><p><strong>Authors:Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</strong></p>
<p>The prevailing method for neural speech enhancement predominantly utilizes fully-supervised deep learning with simulated pairs of far-field noisy-reverberant speech and clean speech. Nonetheless, these models frequently demonstrate restricted generalizability to mixtures recorded in real-world conditions. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a novel framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce a novel evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods. </p>
<blockquote>
<p>å½“å‰ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºçš„ä¸»æµæ–¹æ³•ä¸»è¦æ˜¯åˆ©ç”¨æ¨¡æ‹Ÿçš„è¿œåœºå¸¦å™ªå£°å’Œæ··å“çš„è¯­éŸ³ä¸æ¸…æ´è¯­éŸ³é…å¯¹è¿›è¡Œå…¨ç›‘ç£æ·±åº¦å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¯¹äºçœŸå®ç¯å¢ƒä¸‹å½•åˆ¶çš„æ··åˆè¯­éŸ³å¾€å¾€è¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ç›´æ¥å¯¹çœŸå®æ··åˆè¯­éŸ³è¿›è¡Œå¢å¼ºæ¨¡å‹çš„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†å•é€šé“è¿œåœºåˆ°è¿‘åœºè¯­éŸ³å¢å¼ºï¼ˆFNSEï¼‰ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨ä»¥ä½ä¿¡å™ªæ¯”ã€é«˜æ··å“ä»¥åŠä¸­é«˜é¢‘è¡°å‡ä¸ºç‰¹å¾çš„çœŸå®ä¸–ç•Œæ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†FNSE-SBGANè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†åŸºäºSchrodinger Bridgeï¼ˆSBï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œä¸»è§‚è¯„ä¼°ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸è¿œåœºä¿¡å·ç›¸æ¯”ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰é™ä½äº†é«˜è¾¾14.58%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFNSE-SBGANä¿æŒäº†å‡ºè‰²çš„ä¸»è§‚è´¨é‡ï¼Œä¸ºçœŸå®ä¸–ç•Œè¿œåœºè¯­éŸ³å¢å¼ºå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨æ—¶é¢‘åŸŸçš„çŸ©é˜µç§©åˆ†æï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†ç³»ç»Ÿçš„è§è§£ï¼Œæ­ç¤ºäº†ä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12936v2">PDF</a> 13 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºçœŸå®æ··åˆæ•°æ®çš„å•é€šé“è¿œåœºåˆ°è¿‘åœºè¯­éŸ³å¢å¼ºï¼ˆFNSEï¼‰ä»»åŠ¡ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œä¸­ä½ä¿¡å™ªæ¯”ã€é«˜å›å£°å’Œä¸­é«˜é¢‘è¡°å‡çš„ç‰¹æ€§ï¼Œæå‡ºä¸€ç§æ–°å‹çš„FNSE-SBGANæ¡†æ¶ï¼Œç»“åˆSchrodinger Bridgeæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚è¯¥æ¡†æ¶å®ç°äº†å„é¡¹æŒ‡æ ‡çš„å“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿè¿œåœºä¿¡å·ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰é™ä½äº†é«˜è¾¾14.58%ã€‚åŒæ—¶ï¼ŒFNSE-SBGANä¿ç•™äº†ä¸»è§‚è´¨é‡ï¼Œä¸ºçœŸå®ä¸–ç•Œè¿œåœºè¯­éŸ³å¢å¼ºè®¾ç«‹äº†æ–°åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡ç‚¹ä¸ºå•é€šé“è¿œåœºåˆ°è¿‘åœºè¯­éŸ³å¢å¼ºï¼ˆFNSEï¼‰åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸‹çš„åº”ç”¨ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨çœŸå®æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>æå‡ºFNSE-SBGANæ¡†æ¶ï¼Œç»“åˆSchrodinger Bridgeæ‰©æ•£æ¨¡å‹å’ŒGANsã€‚</li>
<li>FNSE-SBGANå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶ä¿ç•™äº†è¯­éŸ³çš„ä¸»è§‚è´¨é‡ã€‚</li>
<li>å¼•å…¥åŸºäºçŸ©é˜µç§©åˆ†æçš„æ—¶é—´-é¢‘ç‡åŸŸè¯„ä»·æ¡†æ¶ï¼Œæä¾›æ¨¡å‹æ€§èƒ½çš„ç³»ç»Ÿæ€§è§è§£ã€‚</li>
<li>æ­¤è¯„ä»·æ¡†æ¶æ­ç¤ºäº†ä¸åŒç”Ÿæˆæ–¹æ³•çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ad9a35fab71d60f025fcf34c20a1aa88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8853a8c3a6fd9c1ce121200b015bddfd.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning"><a href="#Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning" class="headerlink" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning"></a>Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, Jiawei Han</strong></p>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆï¼Œå…³é”®åœ¨äºé«˜æ•ˆåœ°è·å–å¤–éƒ¨çŸ¥è¯†å’Œæœ€æ–°ä¿¡æ¯ã€‚è™½ç„¶å¯ä»¥é€šè¿‡æç¤ºæ‹¥æœ‰æ¨ç†èƒ½åŠ›çš„å…ˆè¿›LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æœç´¢å¼•æ“ï¼Œä½†ç”±äºLLMå¯èƒ½ä¸å®Œå…¨å…·å¤‡ä¸æœç´¢å¼•æ“è¿›è¡Œæœ€ä½³äº¤äº’çš„èƒ½åŠ›ï¼Œå› æ­¤è¿™ç§æ–¹æ³•çš„æ€§èƒ½å¾€å¾€ä¸ä½³ã€‚æœ¬æ–‡ä»‹ç»äº†Search-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ¨ç†æ¡†æ¶çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ‰©å±•ï¼ŒLLMåœ¨å…¶ä¸­å­¦ä¹ åœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢ï¼Œå¹¶è¿›è¡Œå®æ—¶æ£€ç´¢ã€‚Search-R1é€šè¿‡å¤šè½®æœç´¢äº¤äº’ä¼˜åŒ–LLMçš„æ¨ç†è½¨è¿¹ï¼Œåˆ©ç”¨æ£€ç´¢ä»¤ç‰Œå±è”½è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒå’Œä¸€ä¸ªç®€å•çš„åŸºäºç»“æœå¥–åŠ±å‡½æ•°ã€‚åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼ŒSearch-R1åœ¨å„ç§RAGåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æé«˜äº†41%ï¼ˆQwen2.5-7Bï¼‰å’Œ20%ï¼ˆQwen2.5-3Bï¼‰ã€‚æœ¬æ–‡è¿˜è¿›ä¸€æ­¥æä¾›äº†å…³äºRLä¼˜åŒ–æ–¹æ³•ã€LLMé€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨æ€çš„å®è¯è§è§£ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä½äº<a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09516v3">PDF</a> 31 pages</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é«˜æ•ˆè·å–å¤–éƒ¨çŸ¥è¯†å’Œæœ€æ–°ä¿¡æ¯å¯¹äºè¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆè‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†Search-R1ï¼Œä¸€ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ‰©å±•ï¼Œç”¨äºä¼˜åŒ–LLMçš„æ¨ç†è½¨è¿¹ã€‚Search-R1ä½¿LLMèƒ½å¤Ÿåœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»ç”Ÿæˆï¼ˆå¤šä¸ªï¼‰æœç´¢æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢åˆ°çš„æ ‡è®°å±è”½æŠ€æœ¯è¿›è¡Œç¨³å®šçš„RLè®­ç»ƒï¼Œé€šè¿‡ç®€å•çš„åŸºäºç»“æœå¥–åŠ±å‡½æ•°è¿›è¡Œå®æ—¶æ£€ç´¢äº¤äº’ã€‚åœ¨ä¸ƒä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–éšæœºè¾…åŠ©ç”Ÿæˆï¼ˆRAGï¼‰åŸºçº¿æ¨¡å‹ï¼ŒSearch-R1åœ¨åŒä¸€è®¾ç½®ä¸‹æ€§èƒ½æé«˜äº†çº¦ç™¾åˆ†ä¹‹å››åï¼ˆQwen2.5-7Bæ•°æ®é›†ï¼‰å’Œç™¾åˆ†ä¹‹äºŒåï¼ˆQwen2.5-3Bæ•°æ®é›†ï¼‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä¸ºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ã€è¯­è¨€æ¨¡å‹çš„é€‰æ‹©ä»¥åŠå“åº”é•¿åº¦åŠ¨æ€ç­‰æä¾›äº†ç»éªŒè§è§£ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨PeterGriffinJinçš„GitHubä»“åº“ä¸­ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹æ‘˜è¦ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜æ•ˆæ¨ç†ä¾èµ–äºå¯¹å¤–éƒ¨çŸ¥è¯†å’Œæœ€æ–°ä¿¡æ¯çš„å¿«é€Ÿè·å–ã€‚</li>
<li>å¼•å…¥Search-R1ï¼šç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ‰©å±•ï¼Œç”¨äºä¼˜åŒ–LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¡Œä¸ºã€‚</li>
<li>LLMé€šè¿‡Search-R1å¯è‡ªä¸»ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œå®ç°å¤šè½®æœç´¢äº¤äº’ã€‚</li>
<li>åˆ©ç”¨æ£€ç´¢åˆ°çš„æ ‡è®°å±è”½æŠ€æœ¯ç¨³å®šRLè®­ç»ƒï¼Œé€šè¿‡ç®€å•çš„åŸºäºç»“æœå¥–åŠ±å‡½æ•°å¢å¼ºå®æ—¶æ£€ç´¢äº¤äº’æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼ŒSearch-R1æ˜¾è‘—æé«˜äº†é—®ç­”æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥è®ºæ–‡æä¾›äº†å…³äºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ã€è¯­è¨€æ¨¡å‹é€‰æ‹©å’Œå“åº”é•¿åº¦åŠ¨æ€çš„å®è´µç»éªŒè§è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2d557fd71b904b6d8a465233c77d0be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af27f02acfaa2b3e1c7056aedffd6780.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Hogwild! Inference Parallel LLM Generation via Concurrent Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ea9bf2804d6b87cc4cc40478f031b843.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-09  DanceMosaic High-Fidelity Dance Generation with Multimodal Editability
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
