<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-10  FEABench Evaluating Language Models on Multiphysics Reasoning Ability">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-06c787a864f6e3acdfc3f2adff5bc6fc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-10-更新"><a href="#2025-04-10-更新" class="headerlink" title="2025-04-10 更新"></a>2025-04-10 更新</h1><h2 id="FEABench-Evaluating-Language-Models-on-Multiphysics-Reasoning-Ability"><a href="#FEABench-Evaluating-Language-Models-on-Multiphysics-Reasoning-Ability" class="headerlink" title="FEABench: Evaluating Language Models on Multiphysics Reasoning Ability"></a>FEABench: Evaluating Language Models on Multiphysics Reasoning Ability</h2><p><strong>Authors:Nayantara Mudur, Hao Cui, Subhashini Venugopalan, Paul Raccuglia, Michael P. Brenner, Peter Norgaard</strong></p>
<p>Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs’ reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at <a target="_blank" rel="noopener" href="https://github.com/google/feabench">https://github.com/google/feabench</a> </p>
<blockquote>
<p>构建精确模拟现实世界并调用数值求解器解决定量问题是工程和科学的本质要求。我们推出了FEABench，这是一个基准测试，旨在评估大型语言模型（LLM）和LLM代理使用有限元分析（FEA）模拟和解决物理、数学和工程问题的能力。我们引入了一个全面的评估方案，通过端到端的推理来调查LLM解决这些问题的能力，通过自然语言问题描述进行推理并操作COMSOL Multiphysics®这一有限元分析软件来计算答案。此外，我们还设计了一个语言模型代理，该代理具备通过软件的应用程序编程接口（API）进行交互的能力，检查其输出并使用工具在多次迭代中改进其解决方案。我们表现最佳的策略生成可执行API调用的次数达到88%。能够成功与有限元分析软件交互并操作以解决诸如我们基准测试中问题等状况的LLM，将推动工程自动化领域的边界。获得这种能力将使LLM的推理技能与数值求解器的精确度相结合，推动开发能够应对现实世界复杂问题的自主系统的发展。代码可在<a target="_blank" rel="noopener" href="https://github.com/google/feabench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/google/feabench找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06260v1">PDF</a> 39 pages. Accepted at the NeurIPS 2024 Workshops on Mathematical   Reasoning and AI and Open-World Agents</p>
<p><strong>Summary</strong></p>
<p>FEABench是一个用于评估大型语言模型（LLMs）及LLM代理模拟和解决物理、数学及工程问题能力的基准测试。它运用有限元分析（FEA）及COMSOL Multiphysics软件来解答问题。本文介绍了一个全面的评估方案，以研究LLMs端到端解决问题的能力，通过推理自然语言问题描述并操作FEA软件计算答案。此外，设计了一个语言模型代理，可透过软件的应用程序接口（API）互动、检视输出并使用工具改善多次迭代的解决方案。最佳策略生成的可执行API调用准确率达到了88%。此能力将推动工程自动化前沿，提升LLMs与数值解算器的结合推理能力，推动针对复杂现实问题的自主系统发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FEABench是一个评估大型语言模型（LLMs）模拟与解决问题能力的基准测试，特别是在工程和科学领域的定量问题上。</li>
<li>该测试利用有限元分析（FEA）和COMSOL Multiphysics软件来设计和解决物理、数学和工程问题。</li>
<li>LLMs需通过自然语言描述进行推理，并操作FEA软件以计算答案，体现其端到端的解决问题能力。</li>
<li>引入了一种语言模型代理，该代理能通过软件API进行互动，并能改善解决方案的多次迭代。</li>
<li>最佳策略生成的API调用准确率达到了88%，显示出较高的自动化潜力。</li>
<li>LLMs结合数值解算器的能力将推动工程自动化的发展，并提升自主系统解决复杂问题的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06260">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d0e6eb3a08e2c216f7adb37ea59031ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e009ee066f8bace188cbd33c6115987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fffaff2da821dec3273000a5a18c2ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f6f59f186d32a73947d11cb7fb7a983.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-869c11750ebea92b48b2b043ad5fdd18.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Transfer-between-Modalities-with-MetaQueries"><a href="#Transfer-between-Modalities-with-MetaQueries" class="headerlink" title="Transfer between Modalities with MetaQueries"></a>Transfer between Modalities with MetaQueries</h2><p><strong>Authors:Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, Saining Xie</strong></p>
<p>Unified multimodal models aim to integrate understanding (text output) and generation (pixel output), but aligning these different modalities within a single architecture often demands complex training recipes and careful data balancing. We introduce MetaQueries, a set of learnable queries that act as an efficient interface between autoregressive multimodal LLMs (MLLMs) and diffusion models. MetaQueries connects the MLLM’s latents to the diffusion decoder, enabling knowledge-augmented image generation by leveraging the MLLM’s deep understanding and reasoning capabilities. Our method simplifies training, requiring only paired image-caption data and standard diffusion objectives. Notably, this transfer is effective even when the MLLM backbone remains frozen, thereby preserving its state-of-the-art multimodal understanding capabilities while achieving strong generative performance. Additionally, our method is flexible and can be easily instruction-tuned for advanced applications such as image editing and subject-driven generation. </p>
<blockquote>
<p>统一的多模态模型旨在融合理解和生成（分别为文本输出和像素输出），但在单一架构内对齐这些不同的模态通常需要复杂的训练配方和细致的数据平衡。我们引入了MetaQueries，这是一组可学习的查询，作为自回归多模态大型语言模型（MLLMs）和扩散模型之间的有效接口。MetaQueries将MLLM的潜在空间连接到扩散解码器，通过利用MLLM的深度理解和推理能力，实现知识增强图像生成。我们的方法简化了训练，只需要配对图像和字幕数据以及标准扩散目标。值得注意的是，即使MLLM的骨干保持冻结状态，这种转移仍然有效，从而保留其最先进的多模态理解能力，同时实现强大的生成性能。此外，我们的方法灵活，可以很容易地通过指令微调用于高级应用，如图像编辑和主题驱动生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06256v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://xichenpan.com/metaquery">https://xichenpan.com/metaquery</a></p>
<p><strong>Summary</strong></p>
<p>文本介绍了MetaQueries，这是一组可学习的查询，作为自回归多模态LLM（MLLMs）和扩散模型之间的有效接口。MetaQueries能够将MLLM的潜变量与扩散解码器连接起来，利用MLLM的深度理解和推理能力实现知识增强的图像生成。该方法简化了训练过程，只需配对图像和字幕数据以及标准扩散目标即可。此外，该方法灵活性强，易于进行指令微调，适用于图像编辑和主题驱动生成等高级应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaQueries是一种可学习的查询，用于连接自回归多模态LLMs（MLLMs）和扩散模型，实现不同模态之间的有效整合。</li>
<li>MetaQueries将MLLM的潜变量与扩散解码器连接，利用MLLM的深度理解和推理能力进行知识增强的图像生成。</li>
<li>该方法简化了训练过程，只需使用配对图像和字幕数据以及标准扩散目标。</li>
<li>MLLM的骨干网络在训练过程中可以保持冻结状态，从而保留其最新的多模态理解能力和强大的生成性能。</li>
<li>该方法具有灵活性，能够轻松适应指令微调，适用于图像编辑和主题驱动生成等高级应用。</li>
<li>通过MetaQueries，模型能够利用LLM的理解能力来增强图像的生成质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-34c0bdb455721760acdffaf8afa68fd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bd7083f5108116326a8d0d4ddb5ce14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56008693de8fb99b52c62bf6f3e4a3f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a06893fb108d01abee6e848089fed5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dc37d9fdb8b36c2945169b15509e894.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LExT-Towards-Evaluating-Trustworthiness-of-Natural-Language-Explanations"><a href="#LExT-Towards-Evaluating-Trustworthiness-of-Natural-Language-Explanations" class="headerlink" title="LExT: Towards Evaluating Trustworthiness of Natural Language   Explanations"></a>LExT: Towards Evaluating Trustworthiness of Natural Language   Explanations</h2><p><strong>Authors:Krithi Shailya, Shreya Rajpal, Gokul S Krishnan, Balaraman Ravindran</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT) (The code and set up to reproduce our experiments are publicly available at <a target="_blank" rel="noopener" href="https://github.com/cerai-iitm/LExT">https://github.com/cerai-iitm/LExT</a>). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond. </p>
<blockquote>
<p>随着大型语言模型（LLM）在高风险领域的集成度不断提高，已经提出了几种生成自然语言解释的方法。这些解释对于提高模型的解释性至关重要，特别是在医疗等透明度和可靠性至关重要的敏感领域。鉴于LLM生成的解释及其已知的关注点，对评估模型生成解释的稳健评估框架的需求日益增加。自然语言生成指标，如BLEU和ROUGE，可以捕捉语法和语义准确性，但忽略了事实准确性、一致性和忠诚性等其他方面。为了解决这一空白，我们提出了一个量化自然语言解释可信度的通用框架，平衡了可信度和忠诚度，以得出全面的语言解释可信度分数（LExT）（我们的实验代码和设置可在<a target="_blank" rel="noopener" href="https://github.com/cerai-iitm/LExT%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%EF%BC%89%E3%80%82%E5%88%A9%E7%94%A8%E6%88%91%E4%BB%AC%E7%9A%84%E9%A2%86%E5%9F%9F%E9%80%9A%E7%94%A8%E6%A1%86%E6%9E%B6%E5%BA%94%E7%94%A8%E4%BA%8E%E5%8C%BB%E7%96%97%E9%A2%86%E5%9F%9F%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%85%AC%E5%85%B1%E5%8C%BB%E5%AD%A6%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%84%E4%BC%B0%E4%BA%86%E5%85%AD%E4%B8%AA%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8C%85%E6%8B%AC%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E7%BB%93%E6%9E%9C%E8%A1%A8%E6%98%8E%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E5%9C%A8%E7%94%9F%E6%88%90%E5%8F%AF%E4%BF%A1%E8%A7%A3%E9%87%8A%E6%96%B9%E9%9D%A2%E5%AD%98%E5%9C%A8%E6%98%BE%E8%91%97%E5%B7%AE%E5%BC%82%E3%80%82%E9%80%9A%E8%BF%87%E6%AF%94%E8%BE%83%E8%BF%99%E4%BA%9B%E8%A7%A3%E9%87%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E8%A7%82%E5%AF%9F%E5%88%B0%E4%BA%86%E4%B8%80%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E7%8E%B0%E8%B1%A1%EF%BC%8C%E5%A6%82%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BF%A0%E8%AF%9A%E5%BA%A6%E4%B8%8D%E4%B8%80%E8%87%B4%E4%BB%A5%E5%8F%8A%E5%AE%83%E4%BB%AC%E5%80%BE%E5%90%91%E4%BA%8E%E4%BC%98%E4%BA%8E%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E3%80%82%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%BC%BA%E8%B0%83%E4%BA%86%E4%BD%BF%E7%94%A8%E9%87%8F%E8%BA%AB%E5%AE%9A%E5%88%B6%E7%9A%84%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6%E6%9D%A5%E8%AF%84%E4%BC%B0%E6%95%8F%E6%84%9F%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E8%A7%A3%E9%87%8A%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%8C%E4%B8%BA%E6%94%B9%E5%96%84%E5%8C%BB%E7%96%97%E4%BF%9D%E5%81%A5%E5%92%8C%E8%B6%85%E8%B6%8A%E9%A2%86%E5%9F%9F%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%92%8C%E9%80%8F%E6%98%8E%E5%BA%A6%E6%8F%90%E4%BE%9B%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/cerai-iitm/LExT上公开获取）。利用我们的领域通用框架应用于医疗领域，使用公共医学数据集评估了六个模型，包括特定领域的模型和通用模型。我们的研究结果表明，这些模型在生成可信解释方面存在显著差异。通过比较这些解释，我们观察到了一些有趣的现象，如通用模型的忠诚度不一致以及它们倾向于优于特定领域的微调模型。这项工作进一步强调了使用量身定制的评估框架来评估敏感领域中的自然语言解释的重要性，为改善医疗保健和超越领域的语言模型的可靠性和透明度提供了基础。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06227v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型语言模型（LLMs）在敏感领域如医疗中的自然语言解释生成的重要性及其评价需求。提出了一种衡量语言解释信任度的通用框架，综合考虑了可预测性和忠实性。通过使用公开数据集对医疗领域的六个模型进行评估，发现不同模型在生成可信解释方面的显著差异。强调使用定制评估框架来评估敏感领域的自然语言解释的重要性，为提高语言模型在医疗等领域的可信度和透明度奠定基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在敏感领域如医疗中生成自然语言解释的重要性。</li>
<li>当前评估语言模型生成的自然语言解释的需求。</li>
<li>提出了一种衡量语言解释信任度的通用框架，包括可预测性和忠实性两个方面。</li>
<li>使用公开数据集对医疗领域的模型进行评估，发现不同模型在生成可信解释方面的差异。</li>
<li>一般目的模型在忠实性方面存在不一致性，有时在生成可信解释方面表现优于特定领域的精细调整模型。</li>
<li>强调使用定制的评估框架来评估敏感领域的自然语言解释的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06227">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e5a8a94723b19a72470377a529f2a7e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-318bdf7596a6042e19f6a6b6445501eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f7f39042b589f7036d3e92b4e968e55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70c6738828c4233999d66b028fd071ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f65a8a5982c05341d9b7282f0ea1d5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74f71ff73ba8895a8e3785a5a3403e8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-128K-to-4M-Efficient-Training-of-Ultra-Long-Context-Large-Language-Models"><a href="#From-128K-to-4M-Efficient-Training-of-Ultra-Long-Context-Large-Language-Models" class="headerlink" title="From 128K to 4M: Efficient Training of Ultra-Long Context Large Language   Models"></a>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language   Models</h2><p><strong>Authors:Chejian Xu, Wei Ping, Peng Xu, Zihan Liu, Boxin Wang, Mohammad Shoeybi, Bo Li, Bryan Catanzaro</strong></p>
<p>Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and multimodal data. In this work, we introduce a efficient training recipe for building ultra-long context LLMs from aligned instruct model, pushing the boundaries of context lengths from 128K to 1M, 2M, and 4M tokens. Our approach leverages efficient continued pretraining strategies to extend the context window and employs effective instruction tuning to maintain the instruction-following and reasoning abilities. Our UltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves state-of-the-art performance across a diverse set of long-context benchmarks. Importantly, models trained with our approach maintain competitive performance on standard benchmarks, demonstrating balanced improvements for both long and short context tasks. We further provide an in-depth analysis of key design choices, highlighting the impacts of scaling strategies and data composition. Our findings establish a robust framework for efficiently scaling context lengths while preserving general model capabilities. We release all model weights at: <a target="_blank" rel="noopener" href="https://ultralong.github.io/">https://ultralong.github.io/</a>. </p>
<blockquote>
<p>长上下文能力对于广泛的应用至关重要，包括文档和视频理解、上下文学习和推理时间缩放等，所有这些应用都需要模型处理和推理长文本和多模态数据。在这项工作中，我们介绍了一种有效的训练配方，用于从对齐的指令模型中构建超长上下文LLM，将上下文长度的边界从128K推至1M、2M和4M令牌。我们的方法利用高效的持续预训练策略来扩展上下文窗口，并采用有效的指令调整来保持指令遵循和推理能力。我们的UltraLong-8B，基于Llama3.1-Instruct与我们配方构建，在多种长上下文基准测试中达到最新性能。重要的是，采用我们方法训练的模型在标准基准测试上保持竞争力，证明在长上下文任务和短上下文任务上都有平衡改进。我们还对关键设计选择进行了深入分析，突出了扩展策略和数据组合的影响。我们的研究建立了一个稳健的框架，能够高效扩展上下文长度同时保留通用模型能力。我们发布所有模型权重：<a target="_blank" rel="noopener" href="https://ultralong.github.io/">https://ultralong.github.io/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06214v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了对于超长上下文能力在各种应用中的重要性，包括文档和视频理解、上下文学习和推理时间缩放等。文章提出了一种高效的训练配方，用于构建超长上下文的大型语言模型，通过对齐指令模型实现上下文长度的扩展，从128K推向了1M、2M和4M的令牌长度。该研究采用了高效的持续预训练策略来扩展上下文窗口，并采用有效的指令微调来保持指令遵循和推理能力。UltraLong-8B模型在多样化的长上下文基准测试中实现了最佳性能，并且在标准基准测试中保持竞争力。该研究表明训练的方法不仅在长上下文任务中有所改进，在短上下文任务中也有显著的平衡改进。文章还对关键设计选择进行了深入分析，强调了扩展策略和数据处理的影响。该研究为有效地扩展上下文长度同时保留一般模型能力提供了稳健的框架。所有模型权重均已发布在<a target="_blank" rel="noopener" href="https://ultralong.github.io/%E3%80%82">https://ultralong.github.io/。</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>强调长上下文能力在各种应用中的重要性，包括文档和视频理解等。</li>
<li>介绍了一种高效的训练配方，用于构建超长上下文的大型语言模型，实现上下文长度的显著扩展。</li>
<li>通过持续预训练策略和指令微调来保持模型的指令遵循和推理能力。</li>
<li>UltraLong-8B模型在长上下文基准测试中表现最佳，同时在标准基准测试中保持竞争力。</li>
<li>训练方法不仅适用于长上下文任务，还能在短上下文任务中实现平衡改进。</li>
<li>深入分析了关键设计选择，包括扩展策略和数据处理的影响。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c706e19b84420da79d4af4550adb987b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76c189547299e244abb977c9f558d73c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44ecea6b249f4be7c4acfbb2c543af6f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="V-MAGE-A-Game-Evaluation-Framework-for-Assessing-Visual-Centric-Capabilities-in-Multimodal-Large-Language-Models"><a href="#V-MAGE-A-Game-Evaluation-Framework-for-Assessing-Visual-Centric-Capabilities-in-Multimodal-Large-Language-Models" class="headerlink" title="V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric   Capabilities in Multimodal Large Language Models"></a>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric   Capabilities in Multimodal Large Language Models</h2><p><strong>Authors:Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, Lijuan Wang</strong></p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CSU-JPG/V-MAGE">https://github.com/CSU-JPG/V-MAGE</a>. </p>
<blockquote>
<p>近期多模态大语言模型（MLLMs）的进步已经在各种多模态基准测试中取得了显著的改进。然而，随着评估从静态数据集转向开放世界动态环境，当前的游戏基准测试仍然不足，因为它们缺乏以视觉为中心的任务，并且无法评估现实世界决策所需的各种推理技能。为了解决这一问题，我们引入了以视觉为中心的多能力游戏评估（V-MAGE），这是一个基于游戏的设计框架，旨在评估MLLMs的视觉推理能力。V-MAGE包含五个不同的游戏和超过三十个手工制作的关卡，测试模型在核心视觉技能方面的定位、轨迹跟踪、时间把握和视觉记忆能力，以及高级推理能力如长期规划和深思熟虑。我们使用V-MAGE评估领先的MLLMs，揭示它们在视觉感知和推理方面的重大挑战。在所有游戏环境中，根据埃洛评级比较，表现最佳的MLLM与人类相比存在明显的性能差距。我们的研究结果突出了关键的局限性，包括模型犯的各种类型的感知错误，并从代理中心视角提出了潜在的改进方向，如改进代理策略和解决感知不准确的问题。相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/CSU-JPG/V-MAGE%E4%B8%8A%E3%80%82">https://github.com/CSU-JPG/V-MAGE上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06148v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期多模态大语言模型（MLLMs）的进步在多模态基准测试中取得了显著的提升。然而，随着评估从静态数据集转向开放世界动态环境，现有的基于游戏的基准测试显得捉襟见肘，因为它们缺乏视觉中心任务并无法评估现实世界决策所需的多样化推理技能。为解决这一问题，我们推出了视觉中心多能力游戏评估（V-MAGE）框架，这是一个基于游戏的评估框架，旨在评估MLLMs的视觉推理能力。V-MAGE包含五个不同的游戏和30多个手工定制级别，测试模型在核心视觉技能以及长期规划和辩论之类的高级推理方面的能力。我们使用V-MAGE评估领先的MLLMs，揭示其在视觉感知和推理方面的重大挑战。在所有游戏环境中，根据埃洛评级比较，表现最佳的MLLM与人类之间存在显著的性能差距。我们的研究发现了关键的局限性，包括模型犯的各种类型的感知错误，并从代理中心的角度提出了潜在的改进方向，如改进代理策略和解决感知错误。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大语言模型（MLLMs）在多个基准测试中表现优异，但在动态环境中存在不足。</li>
<li>当前基于游戏的评估缺乏视觉中心任务，无法全面评估模型的视觉推理能力。</li>
<li>引入V-MAGE框架，包含五个游戏和多个手工定制级别，用于评估MLLMs的核心视觉技能和高级推理能力。</li>
<li>V-MAGE揭示了MLLM在视觉感知和推理方面的挑战。</li>
<li>表现最佳的MLLM与人类之间存在显著性能差距。</li>
<li>模型存在多种感知错误，需要改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06148">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-353ca58a8b6465aa1188389faf91d3ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a2147ac2368a45a888631a5db1acade.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5601e6c5597ef4da775d2caaae60cf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d77b9acc422b80dbac14e9393f9eafe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb6c151107fff219b5a6c4568ee96243.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Leanabell-Prover-Posttraining-Scaling-in-Formal-Reasoning"><a href="#Leanabell-Prover-Posttraining-Scaling-in-Formal-Reasoning" class="headerlink" title="Leanabell-Prover: Posttraining Scaling in Formal Reasoning"></a>Leanabell-Prover: Posttraining Scaling in Formal Reasoning</h2><p><strong>Authors:Jingyuan Zhang, Qi Wang, Xingguang Ji, Yahui Liu, Yang Yue, Fuzheng Zhang, Di Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1&#x2F;O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details. </p>
<blockquote>
<p>近年来，通过大型语言模型（LLMs）在自动定理证明（ATP）方面的最新进展突显了使用Lean 4代码进行形式推理的潜力。然而，正如Open AI O1&#x2F;O3和Deepseek R1所展示的，最近的训练后扩展并未彻底改变ATP。在这项工作中，我们研究了ATP的整个训练后过程，旨在将其与自然语言推理模型的突破成果相结合。首先，我们使用由众多语句证明对和旨在融入模拟人类推理和假设改进的认知行为的其他数据组成的混合数据集，对当前ATP模型进行持续训练。接下来，我们探索使用Lean 4编译器提供的成果奖励来强化学习。通过我们设计的持续训练和强化学习过程，我们成功改进了现有的形式证明器，包括DeepSeek-Prover-v1.5和Goedel-Prover，并在整个证明生成领域实现了最先进的性能。例如，在MiniF2F上，我们达到了59.8%（pass@32）的通过率。这是一个正在进行中的项目，我们将逐步更新我们的发现，并发布我们的数据和训练细节。</p>
</blockquote>
<p><strong>译文简化与说明</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06122v1">PDF</a> 23 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>近期，利用大型语言模型（LLMs）在自动定理证明（ATP）方面的进展凸显了与Lean 4代码进行形式推理的潜力。然而，ATP尚未受到Open AI O1&#x2F;O3和Deepseek R1所展示的近期后训练扩展的革命性影响。本研究旨在调查ATP的后训练过程，以使其与自然语言推理模型的突破进展保持一致。通过持续训练当前ATP模型与混合数据集，以及利用Lean 4编译器返回的结果奖励来探索强化学习，我们成功改进了现有的形式证明器，包括DeepSeek-Prover-v1.5和Goedel-Prover，并在整个证明生成领域实现了最先进的性能。例如，在MiniF2F上达到了59.8%的通过率（pass@32）。此项目正在进行中，我们将不断更新我们的发现，并发布我们的数据和训练细节。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在ATP中的应用展示了形式推理的潜力。</li>
<li>ATP尚未被近期后训练扩展显著影响。</li>
<li>研究目标是调查并改进ATP的后训练过程，以与自然语言推理模型的进展同步。</li>
<li>通过持续训练和强化学习改进了现有的形式证明器。</li>
<li>在MiniF2F上取得了较高的通过率。</li>
<li>此项目正在进行中，将持续更新数据和训练细节。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3b626ca31962b6045da5c38976d4adfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-488ecde7aa50f655a60b9deeefcb47bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-736ba68fa87823813c71a8b46b444929.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MDK12-Bench-A-Multi-Discipline-Benchmark-for-Evaluating-Reasoning-in-Multimodal-Large-Language-Models"><a href="#MDK12-Bench-A-Multi-Discipline-Benchmark-for-Evaluating-Reasoning-in-Multimodal-Large-Language-Models" class="headerlink" title="MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in   Multimodal Large Language Models"></a>MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in   Multimodal Large Language Models</h2><p><strong>Authors:Pengfei Zhou, Fanrui Zhang, Xiaopeng Peng, Zhaopan Xu, Jiaxin Ai, Yansheng Qiu, Chuanhao Li, Zhen Li, Ming Li, Yukang Feng, Jianwen Sun, Haoquan Zhang, Zizhen Li, Xiaofeng Mao, Wangbo Zhao, Kai Wang, Xiaojun Chang, Wenqi Shao, Yang You, Kaipeng Zhang</strong></p>
<p>Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LanceZPF/MDK12">https://github.com/LanceZPF/MDK12</a>. </p>
<blockquote>
<p>多模态推理是将语言和视觉线索整合到问题解决和决策制定中，是人类智能的基本要素，也是实现人工通用智能的关键步骤。然而，在多模态大型语言模型（MLLMs）中对多模态推理能力的评估仍然不足。现有大多数推理基准测试都受限于数据量有限、领域覆盖范围狭窄以及知识分布不够结构化。为了弥补这些差距，我们引入了MDK12-Bench，这是一个跨学科基准测试，通过现实世界中的K-12考试评估MLLMs的推理能力。我们的基准测试涵盖了数学、物理、化学、生物、地理和信息科学六个学科，包含从小学到12年级不同难度级别的14万个推理实例。它基于组织良好的知识结构、详细的答案解释、难度标签和跨年级分区，提供了6827个实例级知识点注释，为全面评估提供了一个稳健的平台。此外，我们还提出了一种新的动态评估框架，通过评估过程中的问题形式、问题类型和图像风格的自助引导，来缓解数据污染问题。在MDK12-Bench上的广泛实验揭示了当前MLLMs在多模态推理方面的重大局限性。我们在基准测试上的发现对下一代模型的发展提供了见解。我们的数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/LanceZPF/MDK12%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LanceZPF/MDK12上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05782v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong><br>     多模态推理是集成语言和视觉线索到问题解决和决策制定中的基本人类智能表现，也是朝着人工智能通用发展迈出的重要一步。然而，多模态大型语言模型（MLLMs）的多模态推理能力评估仍然不足。为了解决这个问题，本文提出了MDK12-Bench跨学科基准测试，通过K-12现实考试评估MLLMs的推理能力。该基准测试涵盖了六个学科，包含不同难度级别的14万个推理实例，并提供了一个新型动态评估框架来缓解数据污染问题。实验结果揭示了当前MLLM在多模态推理方面的显著局限性。本文的发现对于下一代模型的发展提供了深刻的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态推理是人类智能的重要组成部分，也是人工智能领域的重要研究方向。</li>
<li>当前对多模态大型语言模型（MLLMs）的多模态推理能力评估存在不足，需要跨学科基准测试来全面评估其性能。</li>
<li>MDK12-Bench基准测试涵盖了多个学科，包括数学、物理、化学、生物、地理和信息科学，提供了大规模的推理实例库。</li>
<li>MDK12-Bench基准测试采用了详细的答案解释、难度标签和跨年级分区，为评估提供了稳健的平台。</li>
<li>新型动态评估框架被用于缓解数据污染问题，通过调整问题形式、问题类型和图像风格来进行评估。</li>
<li>实验结果表明，当前MLLM在多模态推理方面存在显著局限性，这为下一代模型的发展提供了挑战和机遇。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-407bd4915cbb4e2085228879b45015a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-880dcdd060cb2121d2582b72145748d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6a0b0562758b87455d194c182ed41db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05ee6d88cf3fe88cc80762e8c200bb9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c2c4cfeda0666af04269fdb42f02971.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86301f336d4e0afe56013b538325ca57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-693302fda3d13ca96d56e1eff4d83897.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="iEBAKER-Improved-Remote-Sensing-Image-Text-Retrieval-Framework-via-Eliminate-Before-Align-and-Keyword-Explicit-Reasoning"><a href="#iEBAKER-Improved-Remote-Sensing-Image-Text-Retrieval-Framework-via-Eliminate-Before-Align-and-Keyword-Explicit-Reasoning" class="headerlink" title="iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via   Eliminate Before Align and Keyword Explicit Reasoning"></a>iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via   Eliminate Before Align and Keyword Explicit Reasoning</h2><p><strong>Authors:Yan Zhang, Zhong Ji, Changxu Meng, Yanwei Pang, Jungong Han</strong></p>
<p>Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR), which aims at searching for the corresponding targets based on the given query. Among these efforts, the application of Foundation Models (FMs), such as CLIP, to the domain of remote sensing has yielded encouraging outcomes. However, existing FM based methodologies neglect the negative impact of weakly correlated sample pairs and fail to account for the key distinctions among remote sensing texts, leading to biased and superficial exploration of sample pairs. To address these challenges, we propose an approach named iEBAKER (an Improved Eliminate Before Align strategy with Keyword Explicit Reasoning framework) for RSITR. Specifically, we propose an innovative Eliminate Before Align (EBA) strategy to filter out the weakly correlated sample pairs, thereby mitigating their deviations from optimal embedding space during alignment.Further, two specific schemes are introduced from the perspective of whether local similarity and global similarity affect each other. On this basis, we introduce an alternative Sort After Reversed Retrieval (SAR) strategy, aims at optimizing the similarity matrix via reverse retrieval. Additionally, we incorporate a Keyword Explicit Reasoning (KER) module to facilitate the beneficial impact of subtle key concept distinctions. Without bells and whistles, our approach enables a direct transition from FM to RSITR task, eliminating the need for additional pretraining on remote sensing data. Extensive experiments conducted on three popular benchmark datasets demonstrate that our proposed iEBAKER method surpasses the state-of-the-art models while requiring less training data. Our source code will be released at <a target="_blank" rel="noopener" href="https://github.com/zhangy0822/iEBAKER">https://github.com/zhangy0822/iEBAKER</a>. </p>
<blockquote>
<p>最近的研究集中在遥感图像文本检索（RSITR）上，其目标是根据给定的查询来搜索相应的目标。在这些努力中，将CLIP等Foundation Models（FMs）应用于遥感领域已经取得了令人鼓舞的结果。然而，现有的基于FM的方法忽略了弱相关样本对的负面影响，未能考虑到遥感文本之间的关键区别，导致对样本对的偏见和肤浅的探索。为了解决这些挑战，我们提出了一种名为iEBAKER的方法（带有关键词显式推理框架的改进前消除对齐策略）。具体来说，我们提出了一种创新的消除前对齐（EBA）策略，以过滤掉弱相关的样本对，从而减轻它们在对齐过程中偏离最佳嵌入空间的情况。此外，我们从局部相似性和全局相似性是否相互影响的角度介绍了两种具体方案。在此基础上，我们引入了一种替代的SAR（Sort After Reversed Retrieval）策略，旨在通过反向检索优化相似性矩阵。另外，我们加入了关键词显式推理（KER）模块，以促进微妙关键概念区别的积极影响。我们的方法无需额外的装饰和配件，就能直接从FM过渡到RSITR任务，无需在遥感数据上进行额外的预训练。在三个流行基准数据集上进行的广泛实验表明，我们提出的iEBAKER方法超越了最先进的模型，同时需要较少的训练数据。我们的源代码将在<a target="_blank" rel="noopener" href="https://github.com/zhangy0822/iEBAKER%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/zhangy0822/iEBAKER上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05644v1">PDF</a> </p>
<p><strong>Summary</strong><br>    近期研究关注遥感图像文本检索（RSITR），应用Foundation Models（FMs）如CLIP取得鼓舞性成果。但现有方法忽略弱相关样本对的影响，未能关键区分遥感文本，导致样本对探索有偏见和表面化。为此，提出iEBAKER方法，采用Eliminate Before Align策略过滤弱相关样本对，从局部和全局相似性的角度引入SAR策略优化相似度矩阵，并融入Keyword Explicit Reasoning模块助力关键概念区分。该方法直接从FM转向RSITR任务，无需在遥感数据上额外预训练，在三个标准数据集上的实验显示，iEBAKER超越现有最佳模型，且需要更少训练数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Remote Sensing Image-Text Retrieval (RSITR) 旨在根据给定查询搜索相应目标。</li>
<li>Foundation Models（FMs）在遥感领域的应用已产生鼓舞人心的结果。</li>
<li>现有FM方法忽略弱相关样本对的负面影响。</li>
<li>iEBAKER方法通过Eliminate Before Align策略过滤弱相关样本对。</li>
<li>SAR策略用于优化相似度矩阵，考虑局部和全局相似性的相互影响。</li>
<li>Keyword Explicit Reasoning模块助力关键概念的区分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-deb6fde8bc6d52bf9b6dd82fde885074.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9aa340d81694e5426b86a937bb3ccf59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f15c025d81a8d894173ccc225dc1c541.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0547316512d7779ffb8b136bbd2f38.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BoolE-Exact-Symbolic-Reasoning-via-Boolean-Equality-Saturation"><a href="#BoolE-Exact-Symbolic-Reasoning-via-Boolean-Equality-Saturation" class="headerlink" title="BoolE: Exact Symbolic Reasoning via Boolean Equality Saturation"></a>BoolE: Exact Symbolic Reasoning via Boolean Equality Saturation</h2><p><strong>Authors:Jiaqi Yin, Zhan Song, Chen Chen, Qihao Hu, Cunxi Yu</strong></p>
<p>Boolean symbolic reasoning for gate-level netlists is a critical step in verification, logic and datapath synthesis, and hardware security. Specifically, reasoning datapath and adder tree in bit-blasted Boolean networks is particularly crucial for verification and synthesis, and challenging. Conventional approaches either fail to accurately (exactly) identify the function blocks of the designs in gate-level netlist with structural hashing and symbolic propagation, or their reasoning performance is highly sensitive to structure modifications caused by technology mapping or logic optimization. This paper introduces BoolE, an exact symbolic reasoning framework for Boolean netlists using equality saturation. BoolE optimizes scalability and performance by integrating domain-specific Boolean ruleset for term rewriting. We incorporate a novel extraction algorithm into BoolE to enhance its structural insight and computational efficiency, which adeptly identifies and captures multi-input, multi-output high-level structures (e.g., full adder) in the reconstructed e-graph.   Our experiments show that BoolE surpasses state-of-the-art symbolic reasoning baselines, including the conventional functional approach (ABC) and machine learning-based method (Gamora). Specifically, we evaluated its performance on various multiplier architecture with different configurations. Our results show that BoolE identifies $3.53\times$ and $3.01\times$ more exact full adders than ABC in carry-save array and Booth-encoded multipliers, respectively. Additionally, we integrated BoolE into multiplier formal verification tasks, where it significantly accelerates the performance of traditional formal verification tools using computer algebra, demonstrated over four orders of magnitude runtime improvements. </p>
<blockquote>
<p>针对门级网表的布尔符号推理是验证、逻辑和数据路径合成以及硬件安全中的关键步骤。特别是，在比特爆炸的布尔网络中推理数据路径和加法树对于验证和合成至关重要，且非常具有挑战性。传统的方法要么无法准确地（确切地）识别门级网表中设计的功能块，使用结构散列和符号传播，要么它们的推理性能对技术映射或逻辑优化引起的结构修改非常敏感。本文介绍了BoolE，这是一个使用等式饱和的布尔网表精确符号推理框架。BoolE通过集成特定领域的布尔规则集进行术语重写来优化可扩展性和性能。我们将一种新型提取算法纳入BoolE，以提高其结构洞察力和计算效率，该算法能够巧妙地在重构的e图中识别和捕获多输入、多输出的高级结构（例如全加器）。我们的实验表明，BoolE超越了最新的符号推理基线，包括传统功能方法（ABC）和基于机器学习的方法（Gamora）。具体来说，我们在各种配置的多路架构上评估了其性能。我们的结果表明，在携带保存阵列和Booth编码的多路中，BoolE识别出的全加器比ABC多3.53倍和3.01倍。此外，我们将BoolE集成到多路正式验证任务中，它显著加速了使用计算机代数的传统形式验证工具的性能，在四个数量级的运行时间上取得了明显的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05577v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在布尔符号推理中，针对门级网表的推理是验证、逻辑合成、硬件安全中的关键步骤。对于位爆破布尔网络中的推理路径和加法器树尤其重要且具有挑战性。当前方法未能准确地识别设计中的功能块或对结构变动（技术映射或逻辑优化导致）敏感。本文介绍BoolE，一个基于等式饱和的布尔网表精确符号推理框架。BoolE通过集成领域特定的布尔规则集进行术语重写来优化可扩展性和性能。我们采用新颖提取算法增强BoolE的结构洞察力和计算效率，巧妙识别并捕获多输入多输出的高级结构（如全加器）。实验表明，BoolE超越先进的符号推理基线，包括传统功能方法（ABC）和机器学习方法（Gamora）。对于不同配置的多路器架构性能评估显示，BoolE识别的全加器比ABC多3.53倍和3.01倍。此外，将BoolE集成到多路器形式验证任务中，与传统形式验证工具相比，使用计算机代数显著加速性能，实现超过四个数量级的运行时改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Boolean symbolic reasoning for gate-level netlists is crucial in verification, logic synthesis, and hardware security.</li>
<li>Existing methods have difficulties accurately identifying function blocks in gate-level netlists or are sensitive to structural changes.</li>
<li>The paper introduces BoolE, an exact symbolic reasoning framework for Boolean netlists using equality saturation.</li>
<li>BoolE optimizes scalability and performance through term rewriting with a domain-specific Boolean ruleset.</li>
<li>A novel extraction algorithm enhances BoolE’s structural insight and computational efficiency.</li>
<li>BoolE surpasses state-of-the-art symbolic reasoning baselines, identifying more exact full adders than conventional methods.</li>
<li>Integrating BoolE into multiplier formal verification tasks significantly accelerates traditional formal verification tools.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c59d1b0a6941fc965f0d2dc0739cb343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06c787a864f6e3acdfc3f2adff5bc6fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fa9b4297c9b483933e8572ceab184b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-462ccd4651d5b0977e1457af3ed96073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95f0fcf0d873446465c0a167ee1bb510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf60eda244eb963a68d6346526996a23.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Generalization-Capabilities-of-Large-Language-Models-on-Code-Reasoning"><a href="#Evaluating-the-Generalization-Capabilities-of-Large-Language-Models-on-Code-Reasoning" class="headerlink" title="Evaluating the Generalization Capabilities of Large Language Models on   Code Reasoning"></a>Evaluating the Generalization Capabilities of Large Language Models on   Code Reasoning</h2><p><strong>Authors:Rem Yang, Julian Dai, Nikos Vasilakis, Martin Rinard</strong></p>
<p>We assess how the code reasoning abilities of large language models (LLMs) generalize to different kinds of programs. We present techniques for obtaining in- and out-of-distribution programs with different characteristics: code sampled from a domain-specific language, code automatically generated by an LLM, code collected from competitive programming contests, and mutated versions of these programs. We also present an experimental methodology for evaluating LLM generalization by comparing their performance on these programs. We perform an extensive evaluation across 10 state-of-the-art models from the past year, obtaining insights into their generalization capabilities over time and across different classes of programs. Our results highlight that while earlier models exhibit behavior consistent with pattern matching, the latest models exhibit strong generalization abilities on code reasoning. </p>
<blockquote>
<p>我们评估大型语言模型（LLM）的代码推理能力如何推广到不同类型的程序。我们展示了获得具有不同特性的分布内和分布外程序的技术：从特定领域语言采样得到的代码、由LLM自动生成的代码、从竞赛编程比赛中收集的代码以及这些程序的变体。我们还通过比较这些程序上的表现，提出了一种评估LLM泛化性的实验方法。我们对过去一年中的10个最新先进模型进行了广泛评估，了解它们在时间和不同类别的程序上的泛化能力。我们的结果表明，虽然早期模型的行为与模式匹配一致，但最新模型在代码推理方面表现出强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05518v1">PDF</a> </p>
<p><strong>Summary</strong>：本文评估大型语言模型（LLM）的代码推理能力在不同类型程序上的泛化情况。文章介绍了获取不同特性程序的方法，包括特定领域语言中的代码样本、LLM自动生成的代码、来自编程竞赛的代码以及这些程序的变种。同时，文章还提出了一种评估LLM泛化能力的方法，通过比较这些程序上的表现来进行评价。文章对过去一年中的10个最新模型进行了广泛评估，并深入了解了它们在时间推移和不同类别程序上的泛化能力。结果表明，早期模型的行为与模式匹配一致，而最新模型在代码推理方面表现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>文章评估了大型语言模型（LLM）的代码推理能力在不同类型程序上的泛化情况。</li>
<li>通过比较不同模型的性能来评估LLM的泛化能力。</li>
<li>文章介绍了获取具有不同特性的程序和评估方法，包括特定领域语言中的代码样本、自动生成的代码、来自编程竞赛的代码以及程序的变种。</li>
<li>对过去一年的10个最新模型进行了广泛评估。</li>
<li>早期模型的行为与模式匹配一致。</li>
<li>最新模型在代码推理方面表现出强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-adbacbc7e9744f0e9d3fdd5592fe44de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa0b2f91e3ce7e6e83e21793e2b1cbec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6f9070f809832867ce2541f97ea941.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cec44bd95737d7c1e7ff48fe0670aca.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GraphRAFT-Retrieval-Augmented-Fine-Tuning-for-Knowledge-Graphs-on-Graph-Databases"><a href="#GraphRAFT-Retrieval-Augmented-Fine-Tuning-for-Knowledge-Graphs-on-Graph-Databases" class="headerlink" title="GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph   Databases"></a>GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph   Databases</h2><p><strong>Authors:Alfred Clemedtson, Borun Shi</strong></p>
<p>Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM’s context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q&amp;As on large text-attributed KGs. </p>
<blockquote>
<p>大型语言模型在处理语言和推理方面表现出卓越的能力，但在涉及私人数据时容易出现幻觉。检索增强生成（RAG）检索与大型语言模型上下文窗口相关且匹配的数据，并提示大型语言模型给出答案。GraphRAG将此方法扩展到结构化的知识图谱（KGs）和关于多个跳跃点的实体的查询。最近的GraphRAG方法大多忽略了检索步骤，或者具有抽象或低效的临时检索流程。这阻止了它们在知识图谱存储在支持图形查询语言的图形数据库中的场景中使用。在这项工作中，我们提出了GraphRAFT，这是一个检索和推理框架，对大型语言模型进行微调，以生成经过验证的正确Cypher查询，以检索高质量子图上下文并产生准确答案。我们的方法是第一个可以在存储在本地图形数据库的知识图谱上使用的解决方案。基准测试表明，我们的方法是样本高效的，并随着训练数据的可用性而扩展。我们的方法在大型带文本属性知识图谱上的两个挑战性问答测试的所有四个标准指标上都显著优于所有最新模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Large Language Models在处理与私人数据相关的查询时易出现幻想的问题。为解决这个问题，文章提出了GraphRAFT这一基于检索和推理的框架。GraphRAFT可以微调LLMs，生成可靠的Cypher查询，从知识图谱中检索高质量子图上下文并给出准确答案。此框架适用于存储在原生图形数据库中的知识图谱，具有样本效率高、随训练数据可用性而扩展的优点。在大型文本属性知识图谱的Q&amp;A任务上，该方法在四个标准指标上的表现均优于现有模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Language Models在处理语言和推理任务时表现出色，但在涉及私人数据的查询时容易出错。</li>
<li>检索增强生成（RAG）方法通过检索与LLM上下文窗口相关的数据并提示LLM来回答问题。</li>
<li>GraphRAG将这种方法扩展到结构化的知识图谱（KGs），并处理关于实体多跳的问题。</li>
<li>现有GraphRAG方法存在缺陷，忽略检索步骤或拥有抽象、低效的检索过程，无法适应于知识图谱存储在图形数据库的情况。</li>
<li>GraphRAFT是一个基于检索和推理的框架，可以微调LLMs以生成准确的Cypher查询，从知识图谱中检索高质量信息并给出正确答案。</li>
<li>GraphRAFT是首个可以在存储在原生图形数据库中的知识图谱上使用的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ce2e7291a36e3b4c56719672307b6da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10a54d06d17561919431f59a8ff71ef6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc4135b2f1b7c74750ba796432f56add.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ZeroED-Hybrid-Zero-shot-Error-Detection-through-Large-Language-Model-Reasoning"><a href="#ZeroED-Hybrid-Zero-shot-Error-Detection-through-Large-Language-Model-Reasoning" class="headerlink" title="ZeroED: Hybrid Zero-shot Error Detection through Large Language Model   Reasoning"></a>ZeroED: Hybrid Zero-shot Error Detection through Large Language Model   Reasoning</h2><p><strong>Authors:Wei Ni, Kaihang Zhang, Xiaoye Miao, Xiangyu Zhao, Yangyang Wu, Yaoshu Wang, Jianwei Yin</strong></p>
<p>Error detection (ED) in tabular data is crucial yet challenging due to diverse error types and the need for contextual understanding. Traditional ED methods often rely heavily on manual criteria and labels, making them labor-intensive. Large language models (LLM) can minimize human effort but struggle with errors requiring a comprehensive understanding of data context. In this paper, we propose ZeroED, a novel hybrid zero-shot error detection framework, which combines LLM reasoning ability with the manual label-based ED pipeline. ZeroED operates in four steps, i.e., feature representation, error labeling, training data construction, and detector training. Initially, to enhance error distinction, ZeroED generates rich data representations using error reason-aware binary features, pre-trained embeddings, and statistical features. Then, ZeroED employs LLM to label errors holistically through in-context learning, guided by a two-step reasoning process for detailed error detection guidelines. To reduce token costs, LLMs are applied only to representative data selected via clustering-based sampling. High-quality training data is constructed through in-cluster label propagation and LLM augmentation with verification. Finally, a classifier is trained to detect all errors. Extensive experiments on seven public datasets demonstrate that, ZeroED substantially outperforms state-of-the-art methods by a maximum 30% improvement in F1 score and up to 90% token cost reduction. </p>
<blockquote>
<p>错误检测（ED）在表格数据中非常重要，但同时也具有挑战性，因为存在多种错误类型，且需要理解上下文。传统的ED方法往往严重依赖于手动标准和标签，使其需要大量人工。大型语言模型（LLM）可以最大限度地减少人工努力，但对于需要全面理解数据上下文的错误却往往无能为力。在本文中，我们提出了ZeroED，这是一种新型混合零样本错误检测框架，它将LLM的推理能力与基于手动标签的ED管道相结合。ZeroED有四个步骤：特征表示、错误标签、训练数据构建和检测器训练。首先，为了增强错误区分度，ZeroED使用错误原因感知的二进制特征、预训练嵌入和统计特征来生成丰富的数据表示。然后，ZeroED采用LLM通过上下文学习来全面标注错误，并通过两步推理过程为详细的错误检测提供指导。为了减少令牌成本，仅对通过聚类采样选择的有代表性的数据应用LLM。通过集群内标签传播和LLM增强验证来构建高质量的训练数据。最后，训练一个分类器来检测所有错误。在七个公共数据集上的广泛实验表明，ZeroED显著优于最新方法，F1得分最高可提高30%，令牌成本最多可降低90%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05345v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong><br>数据表错误检测（ED）非常重要但具有挑战性，因为错误类型多样且需要理解上下文。传统ED方法依赖手动标准和标签，工作量大。大型语言模型（LLM）可以减少人力投入，但在需要全面理解数据上下文时会出现困难。本文提出ZeroED，一种新型零样本错误检测框架，结合LLM推理能力与基于手动标签的ED管道。ZeroED通过四个步骤操作：特征表示、错误标签、训练数据构建和检测器训练。首先，使用错误原因感知二元特征、预训练嵌入和统计特征来增强错误区分度。然后，借助LLM通过上下文学习对错误进行整体标签，并通过两步推理过程提供详细的错误检测指南。为降低令牌成本，仅对通过聚类采样选择的有代表性的数据应用LLM。通过集群内标签传播和LLM验证构建高质量训练数据。最后，训练分类器以检测所有错误。在七个公共数据集上的广泛实验表明，ZeroED在F1分数上最多高出30%，令牌成本最多减少90%，明显优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>错误检测在表格数据中非常重要且具挑战性，因错误类型多样且需理解上下文。</li>
<li>传统错误检测方法依赖大量手动标签，工作量大且效率不高。</li>
<li>大型语言模型可减少人力投入，但在全面理解数据上下文方面可能遇到困难。</li>
<li>ZeroED是一种新型零样本错误检测框架，结合了LLM推理能力与基于手动标签的ED管道。</li>
<li>ZeroED通过四个步骤操作：特征表示以增强错误区分度、错误标签、训练数据构建和检测器训练。</li>
<li>ZeroED使用LLM进行上下文学习以提供全面的错误标签和详细的错误检测指南。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05345">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-31389924da1490886c03564aea5b8d6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d1da0c0eb71005e3c558df8d141c7dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6987fab82de7db6ce18041b2ac82fd4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29346fd76459f195b2708a3dc24e6892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebc307c8ff988231db9228dfc8a8db4d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks"><a href="#VAPO-Efficient-and-Reliable-Reinforcement-Learning-for-Advanced-Reasoning-Tasks" class="headerlink" title="VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks"></a>VAPO: Efficient and Reliable Reinforcement Learning for Advanced   Reasoning Tasks</h2><p><strong>Authors:Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, Lin Yan</strong></p>
<p>We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of $\mathbf{60.4}$. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks. </p>
<blockquote>
<p>我们提出了VAPO，基于价值的增强近端策略优化框架（Value-based Augmented Proximal Policy Optimization framework），这是一个针对基于价值的范式中的推理模型量身定制的新型框架。以AIME 2024数据集为基准，基于Qwen 32B预训练模型的VAPO达到了最先进的得分60.4分。在相同的实验设置下直接比较，VAPO的性能超过了之前报告的DeepSeek-R1-Zero-Qwen-32B和DAPO的结果超过10分。VAPO的训练过程以其稳定性和高效性而脱颖而出。它仅在5000步内就达到了最先进的性能。此外，在多次独立运行中，没有出现训练崩溃的情况，证明了其可靠性。本研究深入探讨了基于价值强化学习框架的长链思维（long-CoT）推理。我们确定了困扰基于价值方法的三个关键挑战：价值模型偏见、存在异质序列长度以及奖励信号稀疏。通过系统设计，VAPO提供了一个综合解决方案，有效地缓解了这些挑战，从而在长链思维推理任务中实现了增强的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05118v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VAPO，一种针对价值基础范式中的推理模型的新型框架，被提出并用于AIME 2024数据集进行基准测试。基于Qwen 32B预训练模型的VAPO达到了60.4的创纪录分数。在相同的实验设置下，VAPO较之前的DeepSeek-R1-Zero-Qwen-32B和DAPO的结果高出超过10分。其训练过程稳定高效，能在短短5000步内达到最佳状态。此外，VAPO能够解决价值基础方法中的三个关键挑战：价值模型偏见、不同序列长度的存在和奖励信号的稀疏性。通过系统设计，VAPO提供了一个综合解决方案，有效地缓解了这些挑战，从而在长链思维推理任务中实现了卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VAPO是一个针对价值基础范式中的推理模型的新型框架，旨在解决长链思维推理任务。</li>
<li>在AIME 2024数据集上，VAPO达到了创纪录的60.4分，显著优于其他模型。</li>
<li>VAPO训练过程稳定高效，能在短时间内达到最佳性能状态。</li>
<li>VAPO解决了价值基础方法中的三个关键挑战：价值模型偏见、不同序列长度的存在和奖励信号的稀疏性。</li>
<li>通过系统设计，VAPO提供了对以上挑战的综合解决方案。</li>
<li>VAPO具有高度的可靠性，多次独立运行中未出现训练崩溃的情况。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aa7e4202422f19ccb14aa9c289f38572.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning"><a href="#Robust-Reinforcement-Learning-from-Human-Feedback-for-Large-Language-Models-Fine-Tuning" class="headerlink" title="Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning"></a>Robust Reinforcement Learning from Human Feedback for Large Language   Models Fine-Tuning</h2><p><strong>Authors:Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）已成为使大型语言模型（LLM）的输出符合人类偏好的关键技术。为了学习奖励函数，大多数现有的RLHF算法使用Bradley-Terry模型，该模型依赖于可能无法反映现实世界判断复杂性和多变性的人类偏好假设。在本文中，我们提出了一种稳健的算法，以提高在这种奖励模型误判下的现有方法的性能。从理论上讲，我们的算法降低了奖励和政策估算器的方差，从而提高了后悔界。在LLM基准数据集上的经验评估表明，该算法始终优于现有方法，在Anthropic有益和无害数据集上，有7 结成为学界广泛关注的热点。因此本文提出一种稳健的算法，旨在提高在奖励模型不准确时的现有方法的性能。该算法能有效减少奖励与策略估算器的变异性，从而提高限制后悔的理论边界。在大型语言模型基准测试数据集上的实证评估表明，本文提出的算法表现卓越，在Anthropic有益和无害数据集上的响应率高达77%-81%，超越基线水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03784v1">PDF</a> </p>
<p><strong>Summary</strong>：强化学习从人类反馈（RLHF）已成为将大型语言模型（LLM）的输出与人类偏好对齐的关键技术。现有大多数RLHF算法使用Bradley-Terry模型来学习奖励函数，这依赖于可能无法反映现实世界判断复杂性和可变性的假设。本文提出了一种稳健的算法，以提高在奖励模型误指定情况下的现有方法性能。理论上，该算法降低了奖励和政策估计量的方差，提高了后悔界。在LLM基准数据集上的经验评估表明，该算法始终优于现有方法，在Anthropic有益和无害数据集上，77-81%的响应优于基准测试。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>强化学习从人类反馈（RLHF）技术用于对齐大型语言模型输出与人类偏好。</li>
<li>现有RLHF算法主要使用Bradley-Terry模型，存在对现实世界判断的复杂性和可变性的假设不足的问题。</li>
<li>本文提出了一种稳健的算法，旨在提高在奖励模型误指定情况下的现有方法性能。</li>
<li>该算法理论上降低了奖励和政策估计量的方差，提高了后悔界。</li>
<li>在LLM基准数据集上的经验评估显示，新算法性能优于现有方法。</li>
<li>在Anthropic有益和无害数据集上，新算法的响应优于基准测试的比例达到77-81%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4d4e4f21d8505ad2aa777c73d77a33ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e11698f7eb32422462ad842f284e0178.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f710b8c6511a6ab1436f675dc3c4aa43.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CrowdVLM-R1-Expanding-R1-Ability-to-Vision-Language-Model-for-Crowd-Counting-using-Fuzzy-Group-Relative-Policy-Reward"><a href="#CrowdVLM-R1-Expanding-R1-Ability-to-Vision-Language-Model-for-Crowd-Counting-using-Fuzzy-Group-Relative-Policy-Reward" class="headerlink" title="CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd   Counting using Fuzzy Group Relative Policy Reward"></a>CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd   Counting using Fuzzy Group Relative Policy Reward</h2><p><strong>Authors:Zhiqiang Wang, Pengbin Feng, Yanbin Lin, Shuzhang Cai, Zongao Bian, Jinghua Yan, Xingquan Zhu</strong></p>
<p>We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that integrates Group Relative Policy Optimization (GRPO) with a fuzzy reward function to enhance learning efficiency. Unlike the conventional binary 0&#x2F;1 accuracy reward, our fuzzy reward model provides nuanced incentives, encouraging more precise outputs. Experimental results demonstrate that GRPO with a standard 0&#x2F;1 accuracy reward underperforms compared to supervised fine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B), surpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across five in-domain datasets. On an out-of-domain dataset, FGRPR achieves performance comparable to SFT but excels when target values are larger, as its fuzzy reward function assigns higher rewards to closer approximations. This approach is broadly applicable to tasks where the precision of the answer is critical. Code and data: <a target="_blank" rel="noopener" href="https://github.com/yeyimilk/CrowdVLM-R1">https://github.com/yeyimilk/CrowdVLM-R1</a> </p>
<blockquote>
<p>我们提出了模糊组相对策略奖励（FGRPR）这一新型框架，它将组相对策略优化（GRPO）与模糊奖励函数相结合，以提高学习效率。与传统的二元0&#x2F;1准确率奖励不同，我们的模糊奖励模型提供了微妙的激励，鼓励更精确的输出。实验结果表明，使用标准0&#x2F;1准确率奖励的GRPO在监督微调（SFT）面前表现不佳。相比之下，应用于Qwen2.5-VL（3B和7B）的FGRPR在所有基线模型（包括GPT4o、LLaMA2（90B）和SFT）上表现均优于基线模型，跨五个领域内的数据集。在跨域数据集上，FGRPR的性能与SFT相当，但当目标值较大时表现更出色，因为其模糊奖励函数会向接近的近似值分配更高的奖励。该方法可广泛应用于答案精确度至关重要的任务。代码和数据集可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/yeyimilk/CrowdVLM-R1">https://github.com/yeyimilk/CrowdVLM-R1</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03724v1">PDF</a> 11 pages, 6 figures and 4 tables</p>
<p><strong>Summary</strong></p>
<p>基于模糊奖励函数的模糊组相对策略奖励（FGRPR）框架结合了组相对策略优化（GRPO），提高了学习效率。相较于传统的二元0&#x2F;1精度奖励，模糊奖励模型提供更精细的激励，促进更精确的输出。实验结果显示，使用标准0&#x2F;1精度奖励的GRPO在多个基准测试中表现不如监督微调（SFT）。而应用于Qwen2.5-VL(包括规模3B和7B的模型测试场景中，结合了模糊奖励函数的FGRPR框架在五个同领域数据集上的表现超过了所有基准模型，包括GPT4o和LLaMA系列。对于目标值较大的情况，FGRPR表现得尤为出色。该方法可广泛应用于答案精度至关重要的任务中。相关代码和数据可通过链接访问。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FGRPR结合了组相对策略优化（GRPO）与模糊奖励函数来提升学习效率。</li>
<li>传统二元精度奖励在某些场景下效果有限，而模糊奖励模型能提供更为精细的激励。</li>
<li>实验显示，结合模糊奖励函数的FGRPR在多个数据集上的表现优于其他基准模型。</li>
<li>FGRPR在目标值较大的情况下表现得尤其出色。由于其较高的精确度和对细节的关注程度极高使其在精确控制类的任务中表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03724">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db95411299b350032cc534ca43c06349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b15a0a3e84f95d690db1df587c4b051b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d68acb4208e39e11c9d8177cecaa4e36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c77a6cb5aa82b7f5dadc10098df52beb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-034b77393ac7b2c9e9b318fd5b412fb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-657772080b45137057d4d9ee672ae243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5311f337d2940eced9fdf2fa4496289.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments"><a href="#DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments" class="headerlink" title="DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments"></a>DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments</h2><p><strong>Authors:Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu</strong></p>
<p>Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher">https://github.com/GAIR-NLP/DeepResearcher</a>. </p>
<blockquote>
<p>大型语言模型（LLM）配备了网络搜索能力，已显示出用于深度研究任务的惊人潜力。然而，当前的方法主要依赖于手动设计的提示（基于提示的工程）表现不稳定，或者在受控的检索增强生成（RAG）环境中使用强化学习（基于RAG的方法），无法捕捉真实世界互动的复杂性。在本文中，我们介绍了DeepResearcher，这是第一个通过强化学习（RL）在真实世界环境中端到端训练基于LLM的深度研究代理的综合框架，通过与真实的网络搜索互动实现规模化。不同于假设所有必要信息都存在于固定语料库中的基于RAG的方法，我们的方法训练代理以应对开放网络的嘈杂、非结构化和动态的特性。我们实现了一种专用多代理架构，浏览代理从各种网页结构中提取相关信息，并克服重大技术挑战。在开放域研究任务上的大量实验表明，DeepResearcher较基于提示的工程基线实现了高达28.9点的实质性改进，较基于RAG的RL代理也实现了高达7.2点的改进。我们的定性分析揭示了来自端到端RL训练的突发认知行为，包括制定计划的能力、从多个来源进行交叉验证信息的能力、参与自我反思以重新定向研究的能力，以及在无法找到明确答案时保持诚实。我们的结果强调，在真实世界网络环境中进行端到端训练不仅是实现细节，而且是开发与现实世界应用对齐的稳健研究能力的根本要求。我们在<a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher%E5%8F%91%E5%B8%83%E4%BA%86DeepResearcher%E3%80%82">https://github.com/GAIR-NLP/DeepResearcher发布了DeepResearcher。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03160v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）结合网络搜索能力在深研究任务中展现出巨大潜力。然而，当前方法主要依赖于手动工程提示或强化学习在受控检索增强生成环境中进行训练，性能不稳定且无法捕捉真实世界互动的复杂性。本文介绍DeepResearcher，它是通过强化学习在真实世界环境中进行端到端训练的首个LLM深研究代理综合框架，能够应对开放网络的噪声、非结构化和动态特性。实验证明，DeepResearcher在开放域研究任务上较基于提示的工程方法和基于RAG的RL代理有显著改善。本文揭示了从端到端RL训练中涌现出的认知行为，包括制定计划、跨源验证信息、自我反思以调整研究方向以及无法找到明确答案时保持诚实。结果表明，在真实世界网络环境中进行端到端训练不仅是实现细节，更是开发符合实际需求需求的稳健研究能力的根本要求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）结合网络搜索能力用于深研究任务表现出显著潜力。</li>
<li>当前方法主要依赖手动工程提示或受控环境中的强化学习，性能有限。</li>
<li>DeepResearcher是首个通过强化学习在真实世界环境进行端到端训练的LLM深研究代理综合框架。</li>
<li>DeepResearcher能在真实世界的开放、噪声、非结构化网络中进行信息检索和导航。</li>
<li>与基于提示的工程方法和基于RAG的RL代理相比，DeepResearcher在开放域研究任务上有显著改善。</li>
<li>端到端RL训练中出现认知行为，如制定计划、跨源验证、自我反思等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d13bba323910f2d99a9e6b9cea40bf9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fca4243f7e11b0d587e275a3743f5a51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be48e5abcc893510cef1f35226280b92.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Envisioning-Beyond-the-Pixels-Benchmarking-Reasoning-Informed-Visual-Editing"><a href="#Envisioning-Beyond-the-Pixels-Benchmarking-Reasoning-Informed-Visual-Editing" class="headerlink" title="Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual   Editing"></a>Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual   Editing</h2><p><strong>Authors:Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, Haodong Duan</strong></p>
<p>Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/RISEBench">https://github.com/PhoenixZ810/RISEBench</a>. </p>
<blockquote>
<p>多模态大型模型（LMMs）在视觉理解和生成方面取得了显著进展，但在通用视觉编辑方面仍面临挑战，特别是在遵循复杂指令、保持外观一致性和支持灵活输入格式方面。为了解决这一差距，我们引入了RISEBench，这是第一个用于评估推理指导的视觉编辑（RISE）的基准测试。RISEBench专注于四种关键推理类型：时间推理、因果推理、空间推理和逻辑推理。我们为每个类别精心策划了高质量的测试用例，并提出了一个评估框架，该框架通过人类评委和LMM-as-a-judge方法评估指令推理、外观一致性和视觉可行性。我们的实验表明，GPT-4o-Native显著优于其他开源和专有模型，但即使是最先进的系统也在逻辑推理任务上面临困难，这凸显了一个仍被忽视的领域。作为初步尝试，RISEBench旨在提供关于推理感知视觉编辑的基础见解，并催化未来的研究。尽管仍处于早期阶段，但我们致力于不断扩展和精炼这个基准测试，以支持对下一代多模态系统更全面、可靠和可扩展的评估。我们的代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/RISEBench%E4%B8%8A%E5%B8%A%E4%BA%A4%E7%82%B9%E6%B6%AF%E3%80%82~">https://github.com/PhoenixZ810/RISEBench上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02826v2">PDF</a> 27 pages, 23 figures, 1 table. Technical Report</p>
<p><strong>Summary</strong><br>大型多模态模型（LMMs）在视觉理解和生成方面取得了显著进展，但在通用视觉编辑方面仍面临挑战，特别是遵循复杂指令、保持外观一致性和支持灵活输入格式方面。为解决这一差距，我们推出了RISEBench，这是首个针对推理信息视觉编辑（RISE）的基准测试。RISEBench专注于四种关键推理类型：时间推理、因果推理、空间推理和逻辑推理。我们为每个类别精心策划了高质量的测试用例，并提出了一个评估框架，该框架通过人类评委和LMM-as-a-judge的方法评估指令推理、外观一致性和视觉可信度。我们的实验表明，GPT-4o-Native在逻辑推理任务上表现突出，但仍面临一些挑战。作为初步尝试，RISEBench旨在为推理感知视觉编辑提供基础见解，并催化未来研究。我们的代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/RISEBench">https://github.com/PhoenixZ810/RISEBench</a>发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型在视觉理解和生成方面取得显著进展，但在通用视觉编辑仍存挑战。</li>
<li>RISEBench是首个针对推理信息视觉编辑的基准测试，聚焦四种关键推理类型。</li>
<li>RISEBench评估指令推理、外观一致性和视觉可信度。</li>
<li>GPT-4o-Native在逻辑推理任务上表现突出，但仍有提升空间。</li>
<li>RISEBench旨在为推理感知视觉编辑提供基础见解，催化相关研究。</li>
<li>RISEBench将不断扩展和完善，以支持对下一代多模态系统更全面、可靠和可伸缩的评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02826">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-378b83ccb5285b2e37e4b7e969baa078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b224c6dc21a4b101242b8066d25ced8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67227a73fb8c3bb609df8d641074f313.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models"><a href="#Text-Speaks-Louder-than-Vision-ASCII-Art-Reveals-Textual-Biases-in-Vision-Language-Models" class="headerlink" title="Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models"></a>Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in   Vision-Language Models</h2><p><strong>Authors:Zhaochen Wang, Bryan Hooi, Yiwei Wang, Ming-Hsuan Yang, Zi Huang, Yujun Cai</strong></p>
<p>Vision-language models (VLMs) have advanced rapidly in processing multimodal information, but their ability to reconcile conflicting signals across modalities remains underexplored. This work investigates how VLMs process ASCII art, a unique medium where textual elements collectively form visual patterns, potentially creating semantic-visual conflicts. We introduce a novel evaluation framework that systematically challenges five state-of-the-art models (including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where character-level semantics deliberately contradict global visual patterns. Our experiments reveal a strong text-priority bias: VLMs consistently prioritize textual information over visual patterns, with visual recognition ability declining dramatically as semantic complexity increases. Various mitigation attempts through visual parameter tuning and prompt engineering yielded only modest improvements, suggesting that this limitation requires architectural-level solutions. These findings uncover fundamental flaws in how current VLMs integrate multimodal information, providing important guidance for future model development while highlighting significant implications for content moderation systems vulnerable to adversarial examples. </p>
<blockquote>
<p>视觉语言模型（VLMs）在处理多模态信息方面取得了快速进展，但其在调和跨模态冲突信号方面的能力仍被忽视。本研究探讨了VLMs如何处理ASCII艺术这一独特媒介，其中文本元素共同形成视觉模式，可能产生语义视觉冲突。我们引入了一个新型评估框架，该框架使用对抗性ASCII艺术系统地挑战了五种最新模型（包括GPT-4o、Claude和Gemini），其中字符级语义故意与全局视觉模式相矛盾。我们的实验揭示了一个强烈的文本优先偏见：VLMs始终优先处理文本信息而非视觉模式，随着语义复杂性的增加，其视觉识别能力急剧下降。通过视觉参数调整和提示工程进行的各种缓解尝试仅产生了微小的改善，这表明这一局限性需要架构级的解决方案。这些发现揭示了当前VLMs如何整合多模态信息的基本缺陷，为未来的模型开发提供了重要指导，同时强调了对于易受对抗性样本影响的内容审核系统所存在的重大影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01589v2">PDF</a> Under review at COLM 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了视觉语言模型（VLMs）在处理ASCII艺术时的表现，这是一种文本元素共同形成视觉图案的特殊媒介，可能会产生语义视觉冲突。研究通过引入新的评估框架，系统性地挑战了五种先进的模型（包括GPT-4o、Claude和Gemini），使用对抗性ASCII艺术进行测试，其中字符级别的语义与全局视觉模式存在冲突。实验表明，VLMs存在强烈的文本优先偏见，随着语义复杂性的增加，其对视觉模式的识别能力急剧下降。尝试通过视觉参数调整和提示工程进行缓解，但效果有限，表明这一局限性需要架构级别的解决方案。这些发现揭示了当前VLMs在整合多模式信息时的根本缺陷，为未来模型开发提供了重要指导，并强调了对抗性示例对内容审核系统的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在处理ASCII艺术时面临语义视觉冲突的挑战。</li>
<li>引入新的评估框架以测试VLMs在ASCII艺术处理方面的性能。</li>
<li>VLMs表现出强烈的文本优先偏见。</li>
<li>随着语义复杂性的增加，VLMs的视觉识别能力急剧下降。</li>
<li>目前的缓解方法（如视觉参数调整和提示工程）效果有限。</li>
<li>需要架构级别的解决方案来改善VLMs的多模式信息整合能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fba226ac1c24d88730e225457b144dec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e81cbb8a73316ed933e2d19a7e996c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca9d360c92ab8688ac92701da6266211.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c94b39072d6453daedc922c65ae233f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e4a1a374783db52a0f60de721c3e442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9e8a907e9fccd8ab3c9bca813807f41.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks"><a href="#FNSE-SBGAN-Far-field-Speech-Enhancement-with-Schrodinger-Bridge-and-Generative-Adversarial-Networks" class="headerlink" title="FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks"></a>FNSE-SBGAN: Far-field Speech Enhancement with Schrodinger Bridge and   Generative Adversarial Networks</h2><p><strong>Authors:Tong Lei, Qinwen Hu, Ziyao Lin, Andong Li, Rilin Chen, Meng Yu, Dong Yu, Jing Lu</strong></p>
<p>The prevailing method for neural speech enhancement predominantly utilizes fully-supervised deep learning with simulated pairs of far-field noisy-reverberant speech and clean speech. Nonetheless, these models frequently demonstrate restricted generalizability to mixtures recorded in real-world conditions. To address this issue, this study investigates training enhancement models directly on real mixtures. Specifically, we revisit the single-channel far-field to near-field speech enhancement (FNSE) task, focusing on real-world data characterized by low signal-to-noise ratio (SNR), high reverberation, and mid-to-high frequency attenuation. We propose FNSE-SBGAN, a novel framework that integrates a Schrodinger Bridge (SB)-based diffusion model with generative adversarial networks (GANs). Our approach achieves state-of-the-art performance across various metrics and subjective evaluations, significantly reducing the character error rate (CER) by up to 14.58% compared to far-field signals. Experimental results demonstrate that FNSE-SBGAN preserves superior subjective quality and establishes a new benchmark for real-world far-field speech enhancement. Additionally, we introduce a novel evaluation framework leveraging matrix rank analysis in the time-frequency domain, providing systematic insights into model performance and revealing the strengths and weaknesses of different generative methods. </p>
<blockquote>
<p>当前神经网络语音增强的主流方法主要是利用模拟的远场带噪声和混响的语音与清洁语音配对进行全监督深度学习。然而，这些模型对于真实环境下录制的混合语音往往表现出有限的泛化能力。为解决这一问题，本研究旨在直接对真实混合语音进行增强模型的训练。具体来说，我们重新审视单通道远场到近场语音增强（FNSE）任务，重点关注以低信噪比、高混响以及中高频衰减为特征的真实世界数据。我们提出了FNSE-SBGAN这一新型框架，它结合了基于Schrodinger Bridge（SB）的扩散模型与生成对抗网络（GANs）。我们的方法在各种指标和主观评估上均达到了最先进的性能，与远场信号相比，字符错误率（CER）降低了高达14.58%。实验结果表明，FNSE-SBGAN保持了出色的主观质量，为真实世界远场语音增强建立了新的基准。此外，我们还引入了一种新的评估框架，利用时频域的矩阵秩分析，为模型性能提供了系统的见解，揭示了不同生成方法的优缺点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12936v2">PDF</a> 13 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于真实混合数据的单通道远场到近场语音增强（FNSE）任务。针对现实世界中低信噪比、高回声和中高频衰减的特性，提出一种新型的FNSE-SBGAN框架，结合Schrodinger Bridge扩散模型和生成对抗网络（GANs）。该框架实现了各项指标的卓越性能，相较于传统远场信号，字符错误率（CER）降低了高达14.58%。同时，FNSE-SBGAN保留了主观质量，为真实世界远场语音增强设立了新基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究重点为单通道远场到近场语音增强（FNSE）在真实世界数据下的应用。</li>
<li>现有模型在真实条件下的泛化能力受限。</li>
<li>提出FNSE-SBGAN框架，结合Schrodinger Bridge扩散模型和GANs。</li>
<li>FNSE-SBGAN实现了卓越的性能，显著降低字符错误率（CER）。</li>
<li>该框架保留了语音的主观质量。</li>
<li>引入基于矩阵秩分析的时间-频率域评价框架，提供模型性能的系统性见解。</li>
<li>此评价框架揭示了不同生成方法的优点和缺点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ad9a35fab71d60f025fcf34c20a1aa88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8853a8c3a6fd9c1ce121200b015bddfd.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning"><a href="#Search-R1-Training-LLMs-to-Reason-and-Leverage-Search-Engines-with-Reinforcement-Learning" class="headerlink" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning"></a>Search-R1: Training LLMs to Reason and Leverage Search Engines with   Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, Jiawei Han</strong></p>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>. </p>
<blockquote>
<p>在大规模语言模型（LLM）中进行有效的推理和文本生成，关键在于高效地获取外部知识和最新信息。虽然可以通过提示拥有推理能力的先进LLM在推理过程中使用搜索引擎，但由于LLM可能不完全具备与搜索引擎进行最佳交互的能力，因此这种方法的性能往往不佳。本文介绍了Search-R1，这是一个针对推理框架的强化学习（RL）扩展，LLM在其中学习在逐步推理过程中自主生成（多个）搜索查询，并进行实时检索。Search-R1通过多轮搜索交互优化LLM的推理轨迹，利用检索令牌屏蔽进行稳定的RL训练和一个简单的基于结果奖励函数。在七个问答数据集上的实验表明，在相同设置下，Search-R1在各种RAG基准测试上的性能提高了41%（Qwen2.5-7B）和20%（Qwen2.5-3B）。本文还进一步提供了关于RL优化方法、LLM选择和响应长度动态的实证见解。代码和模型检查点位于<a target="_blank" rel="noopener" href="https://github.com/PeterGriffinJin/Search-R1">https://github.com/PeterGriffinJin/Search-R1</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09516v3">PDF</a> 31 pages</p>
<p><strong>Summary</strong><br>大语言模型（LLM）高效获取外部知识和最新信息对于进行有效的推理和文本生成至关重要。本文介绍了Search-R1，一种强化学习（RL）的扩展，用于优化LLM的推理轨迹。Search-R1使LLM能够在逐步推理过程中自主生成（多个）搜索查询，并利用检索到的标记屏蔽技术进行稳定的RL训练，通过简单的基于结果奖励函数进行实时检索交互。在七个问答数据集上的实验表明，相较于其他随机辅助生成（RAG）基线模型，Search-R1在同一设置下性能提高了约百分之四十（Qwen2.5-7B数据集）和百分之二十（Qwen2.5-3B数据集）。此外，本文还为强化学习的优化方法、语言模型的选择以及响应长度动态等提供了经验见解。代码和模型检查点已发布在PeterGriffinJin的GitHub仓库中。 </p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键要点摘要：</p>
<ul>
<li>大型语言模型（LLM）的高效推理依赖于对外部知识和最新信息的快速获取。</li>
<li>引入Search-R1：结合强化学习（RL）的扩展，用于优化LLM在推理过程中的行为。</li>
<li>LLM通过Search-R1可自主生成搜索查询，实现多轮搜索交互。</li>
<li>利用检索到的标记屏蔽技术稳定RL训练，通过简单的基于结果奖励函数增强实时检索交互效果。</li>
<li>实验结果表明，相较于基线模型，Search-R1显著提高了问答数据集上的性能。</li>
<li>该论文提供了关于强化学习优化方法、语言模型选择和响应长度动态的宝贵经验见解。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09516">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e2d557fd71b904b6d8a465233c77d0be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af27f02acfaa2b3e1c7056aedffd6780.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-10  Hogwild! Inference Parallel LLM Generation via Concurrent Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-09/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ea9bf2804d6b87cc4cc40478f031b843.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-04-09  DanceMosaic High-Fidelity Dance Generation with Multimodal Editability
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
