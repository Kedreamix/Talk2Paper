<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  TxGemma Efficient and Agentic LLMs for Therapeutics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-527451acd9900a488a751ecb13eb5787.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-10-æ›´æ–°"><a href="#2025-04-10-æ›´æ–°" class="headerlink" title="2025-04-10 æ›´æ–°"></a>2025-04-10 æ›´æ–°</h1><h2 id="TxGemma-Efficient-and-Agentic-LLMs-for-Therapeutics"><a href="#TxGemma-Efficient-and-Agentic-LLMs-for-Therapeutics" class="headerlink" title="TxGemma: Efficient and Agentic LLMs for Therapeutics"></a>TxGemma: Efficient and Agentic LLMs for Therapeutics</h2><p><strong>Authors:Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi</strong></p>
<p>Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanityâ€™s Last Exam benchmark (Chemistry &amp; Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high). </p>
<blockquote>
<p>æ²»ç–—æ€§å‘å±•æ˜¯ä¸€é¡¹å……æ»¡æˆæœ¬å’Œé«˜é£é™©çš„åŠªåŠ›ï¼Œç»å¸¸é¢ä¸´é«˜å¤±è´¥ç‡çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TxGemmaï¼Œè¿™æ˜¯ä¸€å¥—é«˜æ•ˆã€é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½å¤Ÿè¿›è¡Œè¯ç‰©æ€§è´¨é¢„æµ‹ä»¥åŠæ¨ç†å’Œå¯è§£é‡Šæ€§çš„äº¤äº’ã€‚ä¸åŒäºç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼ŒTxGemmaèƒ½å¤Ÿç»¼åˆæ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œå¹¶å¯åœ¨æ•´ä¸ªæ²»ç–—å‘å±•æµç¨‹ä¸­å¹¿æ³›åº”ç”¨ã€‚è¯¥å¥—ä»¶åŒ…æ‹¬åŸºäºä»åŒ…å«å°åˆ†å­ã€è›‹ç™½è´¨ã€æ ¸é…¸ã€ç–¾ç—…å’Œç»†èƒç³»çš„å…¨é¢æ•°æ®é›†ä¸Šè®­ç»ƒçš„Gemma-2å¾®è°ƒå¾—åˆ°çš„å‚æ•°ä¸º2Bã€9Bå’Œ27Bçš„æ¨¡å‹ã€‚åœ¨66ä¸ªæ²»ç–—æ€§å¼€å‘ä»»åŠ¡ä¸­ï¼ŒTxGemmaåœ¨é€šç”¨æ¨¡å‹æ–¹é¢å®ç°äº†å“è¶Šæˆ–ç›¸å½“çš„æ€§èƒ½ï¼ˆå…¶ä¸­å“è¶Šè¡¨ç°ä¸ºåœ¨å…¶ä¸­çš„45ä¸ªä»»åŠ¡ä¸Šï¼‰ï¼Œåœ¨é’ˆå¯¹ä¸“ä¸šæ¨¡å‹çš„å¯¹æŠ—ä¸­ä¹Ÿå®ç°äº†å“è¶Šè¡¨ç°ï¼ˆå…¶ä¸­åœ¨å…¶ä¸­çš„26ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼‰ã€‚ç›¸å¯¹äºåœ¨æ²»ç–—ä¸‹æ¸¸ä»»åŠ¡å¦‚ä¸´åºŠè¯•éªŒä¸è‰¯ååº”é¢„æµ‹ä¸­å¯¹TxGemmaæ¨¡å‹è¿›è¡Œå¾®è°ƒç›¸å¯¹äºå¯¹åŸºç¡€LLMè¿›è¡Œå¾®è°ƒæ‰€éœ€æ›´å°‘çš„è®­ç»ƒæ•°æ®ï¼Œä½¿å¾—TxGemmaåœ¨æ•°æ®å—é™çš„åº”ç”¨åœºæ™¯ä¸­é€‚ç”¨ã€‚é™¤äº†è¿™äº›é¢„æµ‹åŠŸèƒ½å¤–ï¼ŒTxGemmaè¿˜å…·æœ‰å¯¹è¯æ¨¡å‹ï¼Œå¯ä»¥å¼¥åˆé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“ç”¨å±æ€§é¢„æµ‹å™¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¿™å…è®¸ç§‘å­¦å®¶ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’ï¼ŒåŸºäºåˆ†å­ç»“æ„æä¾›æœºæ¢°æ¨ç†ä»¥åšå‡ºé¢„æµ‹ï¼Œå¹¶å‚ä¸ç§‘å­¦è®¨è®ºã€‚åœ¨æ­¤ä¹‹ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†ç”±Gemini 2.5é©±åŠ¨çš„é€šç”¨æ²»ç–—å‰‚ç³»ç»ŸAgentic-Txï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œæ¨ç†ã€è¡ŒåŠ¨ã€ç®¡ç†å¤šæ ·åŒ–å·¥ä½œæµç¨‹å¹¶è·å–å¤–éƒ¨é¢†åŸŸçŸ¥è¯†ã€‚Agentic-Txåœ¨äººç±»æœ€åè€ƒè¯•åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¹‹å‰çš„é¢†å…ˆæ¨¡å‹ï¼Œç›¸è¾ƒäºo3-mini (high)ï¼Œåœ¨Chemistry &amp; Biologyä¸Šç›¸å¯¹æå‡äº†52.3%ï¼Œåœ¨GPQA (Chemistry)ä¸Šæå‡äº†26.7%ï¼Œå¹¶åœ¨ChemBench-Preferenceå’ŒChemBench-Miniä¸Šåˆ†åˆ«æé«˜äº†6.3%å’Œæé«˜äº†äº†2.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06196v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ²»ç–—å¼€å‘è¿‡ç¨‹ä¸­é¢ä¸´çš„é«˜æˆæœ¬å’Œé«˜é£é™©é—®é¢˜ï¼Œå¹¶ä¸ºæ­¤å¼•å…¥äº†TxGemmaï¼Œä¸€å¥—èƒ½å¤Ÿé¢„æµ‹æ²»ç–—ç‰¹æ€§å¹¶è¿›è¡Œæ¨ç†å’Œè§£é‡Šçš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚TxGemmaèƒ½å¤Ÿä»å„ç§æ¥æºç»¼åˆä¿¡æ¯ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºæ²»ç–—å¼€å‘ç®¡é“ã€‚ç»è¿‡åœ¨åŒ…å«å°åˆ†å­ã€è›‹ç™½è´¨ã€æ ¸é…¸ã€ç–¾ç—…å’Œç»†èƒç³»çš„ç»¼åˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒTxGemmaåœ¨66é¡¹æ²»ç–—å¼€å‘ä»»åŠ¡ä¸­å–å¾—äº†å“è¶Šæˆ–ç›¸å½“çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·æœ‰å¯¹è¯æ¨¡å‹ï¼Œå¯ä»¥å¼¥è¡¥é€šç”¨LLMå’Œä¸“é—¨å±æ€§é¢„æµ‹å™¨ä¹‹é—´çš„å·®è·ï¼Œå…è®¸ç§‘å­¦å®¶ä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤æµï¼Œå¹¶æä¾›åŸºäºåˆ†å­ç»“æ„çš„é¢„æµ‹æœºåˆ¶ç†ç”±å’Œç§‘å­¦è®¨è®ºã€‚åŸºäºè¿™äº›åŠŸèƒ½ï¼Œè¿›ä¸€æ­¥å¼•å…¥äº†Agentic-Txç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç”±Gemini 2.5é©±åŠ¨ï¼Œèƒ½å¤Ÿè¿›è¡Œæ¨ç†ã€è¡ŒåŠ¨ã€ç®¡ç†å„ç§å·¥ä½œæµç¨‹å’Œè·å–å¤–éƒ¨é¢†åŸŸçŸ¥è¯†ã€‚Agentic-Txåœ¨Humanityâ€™s Last ExamåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†å…ˆå‰çš„é¢†å…ˆæ¨¡å‹ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯ä¸€é¡¹å…³äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨åŠ¨æ²»ç–—å¼€å‘è¿›æ­¥çš„é‡è¦ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TxGemmaæ˜¯é«˜æ•ˆã€é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¥—ä»¶ï¼Œèƒ½å¤Ÿé¢„æµ‹æ²»ç–—ç‰¹æ€§å¹¶å…·æœ‰æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚</li>
<li>å®ƒèƒ½å¤Ÿç»¼åˆæ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Œå¹¿æ³›åº”ç”¨äºæ²»ç–—å¼€å‘ç®¡é“ã€‚</li>
<li>TxGemmaåœ¨ä¸åŒæ²»ç–—å¼€å‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸å¯¹äºå½“å‰é€šç”¨å’Œä¸“å®¶æ¨¡å‹éƒ½æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>TxGemmaçš„å¯¹è¯æ¨¡å‹å…è®¸ç§‘å­¦å®¶ä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤æµï¼Œå¹¶æä¾›åŸºäºåˆ†å­ç»“æ„çš„é¢„æµ‹æœºåˆ¶ç†ç”±å’Œç§‘å­¦è®¨è®ºã€‚</li>
<li>Agentic-Txç³»ç»Ÿç”±Gemini 2.5é©±åŠ¨ï¼Œå…·æœ‰æ¨ç†ã€è¡ŒåŠ¨ã€ç®¡ç†å¤šæ ·å·¥ä½œæµç¨‹å’Œè·å–å¤–éƒ¨é¢†åŸŸçŸ¥è¯†çš„èƒ½åŠ›ã€‚</li>
<li>Agentic-Txåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†å…ˆå‰çš„é¢†å…ˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-112b46d89b5a6ab9ace41fc3b3a027c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-009f0e0b9eb524cf328a00c7c326d849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0636d1d182202dd898a1391b966d3dc0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment"><a href="#Single-Agent-vs-Multi-Agent-LLM-Strategies-for-Automated-Student-Reflection-Assessment" class="headerlink" title="Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment"></a>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student   Reflection Assessment</h2><p><strong>Authors:Gen Li, Li Chen, Cheng Tang, Valdemar Å vÃ¡benskÃ½, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada</strong></p>
<p>We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educatorsâ€™ workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿå¼€æ”¾æ€§åæ€å’Œé¢„æµ‹å­¦ä¸šè¡¨ç°æ–¹é¢çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„åæ€è¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”å¯èƒ½æ— æ³•åœ¨æ•™è‚²ç¯å¢ƒä¸­æœ‰æ•ˆåœ°æ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨LLMï¼Œä½¿ç”¨ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼ˆå•ä»£ç†å’Œå¤šä»£ç†ï¼‰å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼ˆé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ï¼‰ï¼Œå°†å­¦ç”Ÿåæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨æ¥è‡ª377åå­¦ç”Ÿã€ä¸ºæœŸä¸‰ä¸ªå­¦æœŸçš„5,278ä»½åæ€æ•°æ®ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å°‘æ ·æœ¬ç­–ç•¥çš„å•ä»£ç†æ–¹æ³•ä¸äººè¯„åˆ†çš„åŒ¹é…åº¦æœ€é«˜ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨å¤„äºé£é™©çš„å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°éƒ½ä¼˜äºåŸºçº¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°è‡ªåŠ¨è¿›è¡Œåæ€è¯„ä¼°ï¼Œå‡å°‘æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œå¹¶ä¸ºå¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶çš„æ”¯æŒã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†å°†å…ˆè¿›çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸æ•™è‚²å®è·µç›¸ç»“åˆï¼Œä»¥æé«˜å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¸šæˆåŠŸçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05716v1">PDF</a> To be published in Proceedings of the 29th Pacific-Asia Conference on   Knowledge Discovery and Data Mining (PAKDD 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºè‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿçš„å¼€æ”¾æ–‡æœ¬åæ€å’Œé¢„æµ‹å­¦ä¸šè¡¨ç°ã€‚ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥åœ¨æ•™è‚²ç¯å¢ƒä¸­æœ‰æ•ˆæ‰©å±•ã€‚æœ¬ç ”ç©¶åˆ©ç”¨LLMså°†å­¦ç”Ÿçš„åæ€è½¬åŒ–ä¸ºé‡åŒ–åˆ†æ•°ï¼Œé‡‡ç”¨ä¸¤ç§è¯„ä¼°ç­–ç•¥å’Œä¸¤ç§æç¤ºæŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¸¦æœ‰å°‘æ•°é•œå¤´ç­–ç•¥çš„å•ä¸€æ™ºèƒ½ä½“è·å¾—äº†æœ€é«˜çš„äººç±»è¯„ä¼°åŒ¹é…ç‡ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨é£é™©å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿ã€‚è¿™è¡¨æ˜LLMså¯ä»¥æœ‰æ•ˆè‡ªåŠ¨åŒ–åæ€è¯„ä¼°ï¼Œå‡è½»æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œå¹¶ä¸ºå¯èƒ½éœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶çš„æ”¯æŒã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å°†é«˜çº§ç”Ÿæˆå¼AIæŠ€æœ¯èå…¥æ•™è‚²å®è·µä»¥æé«˜å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¸šæˆåŠŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºè‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿçš„å¼€æ”¾æ–‡æœ¬åæ€ã€‚</li>
<li>ä¼ ç»Ÿçš„å­¦ç”Ÿåæ€è¯„ä¼°æ–¹æ³•è€—æ—¶ä¸”éš¾ä»¥æ‰©å±•ã€‚</li>
<li>é‡‡ç”¨ä¸¤ç§è¯„ä¼°ç­–ç•¥å’Œä¸¤ç§æç¤ºæŠ€æœ¯ï¼Œå³å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“ä»¥åŠé›¶é•œå¤´å’Œå°‘æ•°é•œå¤´ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¸¦æœ‰å°‘æ•°é•œå¤´ç­–ç•¥çš„å•æ™ºèƒ½ä½“è¯„ä¼°ç­–ç•¥è·å¾—äº†æœ€é«˜çš„äººç±»è¯„ä¼°åŒ¹é…ç‡ã€‚</li>
<li>ä½¿ç”¨LLMè¯„ä¼°çš„åæ€åˆ†æ•°çš„æ¨¡å‹åœ¨é£é™©å­¦ç”Ÿè¯†åˆ«å’Œæˆç»©é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>LLMsçš„è‡ªåŠ¨åŒ–è¯„ä¼°å¯å‡è½»æ•™è‚²å·¥ä½œè€…çš„å·¥ä½œé‡ï¼Œä¸ºéœ€è¦é¢å¤–å¸®åŠ©çš„å­¦ç”Ÿæä¾›åŠæ—¶æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5366bb7082c7ca612d81c6680389fc41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f36f8bc2ee799fda8b05d31568d376e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e58fb5bda2006bfef3be73859d505f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ff0799879c85a715d451fa0ee35b3a8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EduPlanner-LLM-Based-Multi-Agent-Systems-for-Customized-and-Intelligent-Instructional-Design"><a href="#EduPlanner-LLM-Based-Multi-Agent-Systems-for-Customized-and-Intelligent-Instructional-Design" class="headerlink" title="EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent   Instructional Design"></a>EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent   Instructional Design</h2><p><strong>Authors:Xueqiao Zhang, Chao Zhang, Jianwen Sun, Jun Xiao, Yi Yang, Yawei Luo</strong></p>
<p>Large Language Models (LLMs) have significantly advanced smart education in the Artificial General Intelligence (AGI) era. A promising application lies in the automatic generalization of instructional design for curriculum and learning activities, focusing on two key aspects: (1) Customized Generation: generating niche-targeted teaching content based on studentsâ€™ varying learning abilities and states, and (2) Intelligent Optimization: iteratively optimizing content based on feedback from learning effectiveness or test scores. Currently, a single large LLM cannot effectively manage the entire process, posing a challenge for designing intelligent teaching plans. To address these issues, we developed EduPlanner, an LLM-based multi-agent system comprising an evaluator agent, an optimizer agent, and a question analyst, working in adversarial collaboration to generate customized and intelligent instructional design for curriculum and learning activities. Taking mathematics lessons as our example, EduPlanner employs a novel Skill-Tree structure to accurately model the background mathematics knowledge of student groups, personalizing instructional design for curriculum and learning activities according to studentsâ€™ knowledge levels and learning abilities. Additionally, we introduce the CIDDP, an LLM-based five-dimensional evaluation module encompassing clarity, Integrity, Depth, Practicality, and Pertinence, to comprehensively assess mathematics lesson plan quality and bootstrap intelligent optimization. Experiments conducted on the GSM8K and Algebra datasets demonstrate that EduPlanner excels in evaluating and optimizing instructional design for curriculum and learning activities. Ablation studies further validate the significance and effectiveness of each component within the framework. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Zc0812/Edu_Planner">https://github.com/Zc0812/Edu_Planner</a> </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ—¶ä»£ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½æ•™è‚²æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¸€ä¸ªå……æ»¡å¸Œæœ›çš„åº”ç”¨é¢†åŸŸåœ¨äºè‡ªåŠ¨æ¦‚æ‹¬æ•™å­¦è®¾è®¡å’Œå­¦ä¹ æ´»åŠ¨ï¼Œå®ƒä¸»è¦å…³æ³¨ä¸¤ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆ1ï¼‰å®šåˆ¶åŒ–ç”Ÿæˆï¼šæ ¹æ®å­¦ç”Ÿçš„ä¸åŒå­¦ä¹ èƒ½åŠ›å’ŒçŠ¶æ€ï¼Œç”Ÿæˆé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ•™å­¦å†…å®¹ï¼›ï¼ˆ2ï¼‰æ™ºèƒ½ä¼˜åŒ–ï¼šæ ¹æ®å­¦ä¹ æ•ˆæœçš„åé¦ˆæˆ–è€ƒè¯•æˆç»©ï¼Œå¯¹å†…å®¹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ç›®å‰ï¼Œå•ä¸€çš„å¤§å‹LLMæ— æ³•æœ‰æ•ˆåœ°ç®¡ç†æ•´ä¸ªè¿‡ç¨‹ï¼Œä¸ºè®¾è®¡æ™ºèƒ½æ•™å­¦è®¡åˆ’å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†EduPlannerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬è¯„ä¼°ä»£ç†ã€ä¼˜åŒ–ä»£ç†å’Œé—®é¢˜åˆ†æå¸ˆï¼Œä»–ä»¬é€šè¿‡å¯¹æŠ—åä½œçš„æ–¹å¼ï¼Œä¸ºè¯¾ç¨‹å’Œå­¦ä¹ æ´»åŠ¨ç”Ÿæˆå®šåˆ¶å’Œæ™ºèƒ½çš„æ•™å­¦è®¾è®¡ã€‚ä»¥æ•°å­¦è¯¾ç¨‹ä¸ºä¾‹ï¼ŒEduPlanneré‡‡ç”¨æ–°é¢–çš„Skill-Treeç»“æ„ï¼Œå‡†ç¡®å»ºæ¨¡å­¦ç”Ÿç¾¤ä½“çš„èƒŒæ™¯æ•°å­¦çŸ¥è¯†ï¼Œæ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ æ°´å¹³å’Œèƒ½åŠ›ä¸ªæ€§åŒ–æ•™å­¦è®¾è®¡å’Œå­¦ä¹ æ´»åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†CIDDPï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„äº”ç»´è¯„ä¼°æ¨¡å—ï¼ŒåŒ…æ‹¬æ¸…æ™°åº¦ã€å®Œæ•´æ€§ã€æ·±åº¦ã€å®ç”¨æ€§å’Œè´´åˆ‡æ€§ï¼Œå…¨é¢è¯„ä¼°æ•°å­¦è¯¾ç¨‹è®¡åˆ’çš„è´¨é‡ï¼Œå¹¶å¯åŠ¨æ™ºèƒ½ä¼˜åŒ–ã€‚åœ¨GSM8Kå’Œä»£æ•°æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒEduPlanneråœ¨è¯„ä¼°å’Œä¼˜åŒ–è¯¾ç¨‹å’Œæ•™å­¦æ´»åŠ¨çš„æ•™å­¦è®¾è®¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¡†æ¶å†…å„ç»„ä»¶çš„é‡è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zc0812/Edu_Planner">https://github.com/Zc0812/Edu_Planner</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05370v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ—¶ä»£å¯¹æ™ºèƒ½æ•™è‚²äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œå°¤å…¶åœ¨è‡ªåŠ¨é€šç”¨æ•™å­¦è®¾è®¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚EduPlanneræ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿï¼Œèƒ½è§£å†³æ™ºèƒ½æ•™å­¦è®¾è®¡é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿç”±è¯„ä¼°ä»£ç†ã€ä¼˜åŒ–ä»£ç†å’Œé—®é¢˜åˆ†æå¸ˆç»„æˆï¼Œé€šè¿‡ååŒå·¥ä½œï¼Œä¸ºè¯¾ç¨‹å’Œå­¦ä¹ æ´»åŠ¨ç”Ÿæˆå®šåˆ¶å’Œæ™ºèƒ½åŒ–çš„æ•™å­¦è®¾è®¡ã€‚ä»¥æ•°å­¦è¯¾ä¸ºä¾‹ï¼ŒEduPlanneré‡‡ç”¨æŠ€èƒ½æ ‘ç»“æ„å‡†ç¡®å»ºæ¨¡å­¦ç”Ÿç¾¤ä½“çš„èƒŒæ™¯æ•°å­¦çŸ¥è¯†ï¼Œå¹¶æ ¹æ®å­¦ç”Ÿçš„çŸ¥è¯†æ°´å¹³å’Œå­¦ä¹ èƒ½åŠ›ä¸ªæ€§åŒ–æ•™å­¦è®¾è®¡ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†CIDDPè¯„ä»·æ¨¡å—ï¼Œå…¨é¢è¯„ä¼°æ•°å­¦è¯¾ç¨‹è®¡åˆ’è´¨é‡å¹¶å¯åŠ¨æ™ºèƒ½ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒEduPlanneråœ¨è¯„ä¼°å’Œä¼˜åŒ–æ•™å­¦è®¾è®¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ™ºèƒ½æ•™è‚²é¢†åŸŸå…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>EduPlanneræ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šä»£ç†ç³»ç»Ÿï¼Œç”¨äºæ™ºèƒ½æ•™å­¦è®¾è®¡ã€‚</li>
<li>EduPlanneråŒ…æ‹¬è¯„ä¼°ä»£ç†ã€ä¼˜åŒ–ä»£ç†å’Œé—®é¢˜åˆ†æå¸ˆï¼ŒååŒå·¥ä½œã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨æŠ€èƒ½æ ‘ç»“æ„å‡†ç¡®å»ºæ¨¡å­¦ç”Ÿç¾¤ä½“çš„èƒŒæ™¯çŸ¥è¯†ã€‚</li>
<li>CIDDPè¯„ä»·æ¨¡å—ç”¨äºå…¨é¢è¯„ä¼°æ•°å­¦è¯¾ç¨‹è®¡åˆ’è´¨é‡ã€‚</li>
<li>åœ¨GSM8Kå’ŒAlgebraæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜EduPlanneræ•ˆæœæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5b86d49d9b289b40e12a592b8f42b5f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-893920879d09bd852c7443ff50295d64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82400c3ba14614ff8b7a37cd8431b04c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd162d33615a0101eeb4cab51e4d811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8efb5a549a89a34e09a4b9a669fdcb7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Attention-Augmented-Inverse-Reinforcement-Learning-with-Graph-Convolutions-for-Multi-Agent-Task-Allocation"><a href="#Attention-Augmented-Inverse-Reinforcement-Learning-with-Graph-Convolutions-for-Multi-Agent-Task-Allocation" class="headerlink" title="Attention-Augmented Inverse Reinforcement Learning with Graph   Convolutions for Multi-Agent Task Allocation"></a>Attention-Augmented Inverse Reinforcement Learning with Graph   Convolutions for Multi-Agent Task Allocation</h2><p><strong>Authors:Huilin Yin, Zhikun Yang, Daniel Watzenig</strong></p>
<p>Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ä»»åŠ¡åˆ†é…ï¼ˆMATAï¼‰åœ¨åˆä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå¯¹ç‰©æµã€æœç´¢å’Œæ•‘æ´ä»¥åŠæœºå™¨äººåè°ƒç­‰åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚è™½ç„¶ä¼ ç»Ÿçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—åˆ°ä¾èµ–æ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°å’ŒåŠ¨æ€ç¯å¢ƒä¸­æ•ˆç‡ä½ä¸‹çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºé€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰çš„æ¡†æ¶ï¼Œå…¶ä¸­èå…¥äº†å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰å’Œå›¾æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜å¥–åŠ±å‡½æ•°çš„å­¦ä¹ å’Œä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚åˆ©ç”¨ä¸“å®¶æ¼”ç¤ºæ¥æ¨æ–­æœ€ä¼˜å¥–åŠ±å¯†åº¦ï¼Œå‡å°‘å¯¹æ‰‹å·¥è®¾è®¡çš„ä¾èµ–ï¼Œæé«˜é€‚åº”æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨ç´¯ç§¯å¥–åŠ±å’Œä»»åŠ¡æ‰§è¡Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05045v2">PDF</a> Added a clarification on the source of expert trajectories in Section   V</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ™ºèƒ½ä½“ä»»åŠ¡åˆ†é…ï¼ˆMATAï¼‰åœ¨åˆä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é‡è¦ä½œç”¨ï¼Œä»¥åŠå¯¹ç‰©æµã€æœç´¢å’Œæ•‘æ´ã€æœºå™¨äººåè°ƒç­‰åº”ç”¨çš„é‡è¦å½±å“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰çš„æ¡†æ¶ï¼Œç»“åˆäº†å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰å’Œå›¾æ³¨æ„åŠ›æœºåˆ¶æ¥æé«˜å¥–åŠ±å‡½æ•°çš„å­¦ä¹ å’Œä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚é€šè¿‡ä¸“å®¶æ¼”ç¤ºæ¥æ¨æ–­æœ€ä¼˜å¥–åŠ±å¯†åº¦ï¼Œå‡å°‘äº†å¯¹æ‰‹å·¥è®¾è®¡çš„ä¾èµ–ï¼Œæé«˜äº†é€‚åº”æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç´¯ç§¯å¥–åŠ±å’Œä»»åŠ¡æ‰§è¡Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MATAåœ¨åˆä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å…·æœ‰å…³é”®ä½œç”¨ï¼Œå°¤å…¶åœ¨ç‰©æµã€æœç´¢å’Œæ•‘æ´ä»¥åŠæœºå™¨äººåè°ƒç­‰é¢†åŸŸã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•å—é™äºæ‰‹åŠ¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŸºäºé€†å¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œèå…¥MHSAå’Œå›¾æ³¨æ„åŠ›æœºåˆ¶æ¥æå‡å¥–åŠ±å‡½æ•°å­¦ä¹ ä¸ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚</li>
<li>é€šè¿‡ä¸“å®¶æ¼”ç¤ºæ¥æ¨æ–­æœ€ä¼˜å¥–åŠ±å¯†åº¦ï¼Œé™ä½äº†å¯¹æ‰‹å·¥è®¾è®¡çš„ä¾èµ–ï¼Œæé«˜äº†é€‚åº”æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œåœ¨ç´¯ç§¯å¥–åŠ±å’Œä»»åŠ¡æ‰§è¡Œæ•ˆç‡æ–¹é¢è¶…è¶Šäº†å¹¿æ³›ä½¿ç”¨çš„MARLç®—æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä»»åŠ¡åˆ†é…æä¾›äº†æ–°çš„è§†è§’å’Œè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f702a32b843918556b6b16f41a46cad1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7da76283cccbc7fe6566c92993bd78ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-299cca77406be28adbddc4b06a5d7694.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca7525f9b77e52545650c04458c2cc6c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="APIGen-MT-Agentic-Pipeline-for-Multi-Turn-Data-Generation-via-Simulated-Agent-Human-Interplay"><a href="#APIGen-MT-Agentic-Pipeline-for-Multi-Turn-Data-Generation-via-Simulated-Agent-Human-Interplay" class="headerlink" title="APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated   Agent-Human Interplay"></a>APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated   Agent-Human Interplay</h2><p><strong>Authors:Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, Caiming Xiong</strong></p>
<p>Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models â€“ the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at <a target="_blank" rel="noopener" href="https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4">https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4</a> and project website is <a target="_blank" rel="noopener" href="https://apigen-mt.github.io/">https://apigen-mt.github.io</a> </p>
<blockquote>
<p>è®­ç»ƒé’ˆå¯¹å¤šè½®äº¤äº’çš„æœ‰æ•ˆAIä»£ç†éœ€è¦é«˜è´¨é‡çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®éœ€è¦æ•æ‰çœŸå®çš„äººæœºäº¤äº’åŠ¨æ€ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ•°æ®ç¨€ç¼ºä¸”éš¾ä»¥æ‰‹åŠ¨æ”¶é›†ã€‚æˆ‘ä»¬æ¨å‡ºäº†APIGen-MTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¯éªŒè¯å’Œå¤šæ ·åŒ–çš„å¤šè½®ä»£ç†æ•°æ®ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬çš„ä»£ç†ç®¡é“ä¼šç”Ÿæˆå…·æœ‰çœŸå®åŠ¨ä½œçš„ä»»åŠ¡è“å›¾ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„å®¡å°ç»„å’Œè¿­ä»£åé¦ˆå¾ªç¯ã€‚è¿™äº›è“å›¾éšåé€šè¿‡æ¨¡æ‹Ÿçš„äººæœºäº¤äº’è½¬åŒ–ä¸ºå®Œæ•´çš„äº¤äº’è½¨è¿¹ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ç³»åˆ—æ¨¡å‹ï¼Œå³xLAM-2-fc-rç³»åˆ—ï¼Œå‚æ•°èŒƒå›´ä»1Båˆ°70Bã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨Ï„åŸºå‡†æµ‹è¯•å’ŒBFCLåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å‰æ²¿æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒClaude 3.5ã€‚è¾ƒå°çš„æ¨¡å‹åœ¨å¤šæ¬¡è¯•éªŒä¸­è¶…è¶Šäº†è¾ƒå¤§çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè½®è®¾ç½®ä¸­ï¼ŒåŒæ—¶ä¿æŒäº†å“è¶Šçš„ç¨³å®šæ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»éªŒè¯è“å›¾åˆ°ç»†èŠ‚çš„æ–¹æ³•äº§ç”Ÿäº†é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿå¼€å‘å‡ºæ›´å¯é ã€æ›´é«˜æ•ˆã€èƒ½åŠ›æ›´å¼ºçš„ä»£ç†ã€‚æˆ‘ä»¬å…¬å¼€äº†æ”¶é›†çš„åˆæˆæ•°æ®å’Œè®­ç»ƒçš„xLAM-2-fc-ræ¨¡å‹ï¼Œä»¥ä¿ƒè¿›AIä»£ç†ç ”ç©¶çš„å‘å±•ã€‚æ¨¡å‹å¯åœ¨HuggingFaceä¸Šè·å¾—ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4">é“¾æ¥</a>ï¼Œé¡¹ç›®ç½‘ç«™ä¸ºï¼š<a target="_blank" rel="noopener" href="https://apigen-mt.github.io/">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03601v2">PDF</a> 12 pages plus references and appendices</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAPIGen-MTçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¯éªŒè¯å’Œå¤šæ ·åŒ–çš„å¤šè½®äº¤äº’ä»£ç†æ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»£ç†ç®¡é“ç”Ÿæˆè¯¦ç»†çš„ä»»åŠ¡è“å›¾ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„å®¡å§”å‘˜ä¼šå’Œè¿­ä»£åé¦ˆå¾ªç¯è¿›è¡ŒéªŒè¯ã€‚è¿™äº›è“å›¾é€šè¿‡æ¨¡æ‹Ÿäººæœºäº’åŠ¨è½¬åŒ–ä¸ºå®Œæ•´çš„äº¤äº’è½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„æ•°æ®è´¨é‡é«˜ï¼Œè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šè½®äº¤äº’ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯é æ€§å¼ºã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®å¼€æºäº†åˆæˆæ•°æ®å’Œè®­ç»ƒçš„æ¨¡å‹ï¼Œä»¥æ¨åŠ¨äººå·¥æ™ºèƒ½ä»£ç†ç ”ç©¶çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>APIGen-MTæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¤šè½®äº¤äº’ä»£ç†æ•°æ®çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä»£ç†ç®¡é“ç”Ÿæˆè¯¦ç»†çš„ä»»åŠ¡è“å›¾ï¼Œå¹¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¯„å®¡å’Œè¿­ä»£åé¦ˆå¾ªç¯è¿›è¡ŒéªŒè¯ã€‚</li>
<li>APIGen-MTèƒ½å¤Ÿæ¨¡æ‹Ÿäººæœºäº’åŠ¨ï¼Œå°†è“å›¾è½¬åŒ–ä¸ºå®Œæ•´çš„äº¤äº’è½¨è¿¹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºAPIGen-MTç”Ÿæˆçš„è®­ç»ƒæ•°æ®è´¨é‡é«˜ï¼Œè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šè½®äº¤äº’ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹è¡¨ç°å‡ºé«˜å¯é æ€§ã€é«˜æ•ˆç‡ã€‚</li>
<li>é¡¹ç›®å¼€æºäº†åˆæˆæ•°æ®å’Œè®­ç»ƒçš„æ¨¡å‹ï¼Œä»¥ä¿ƒè¿›äººå·¥æ™ºèƒ½ä»£ç†ç ”ç©¶çš„å‘å±•ã€‚</li>
<li>æ¨¡å‹å¯åœ¨HuggingFaceä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4">é“¾æ¥</a>ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5c56e5682a0277009f5248e43abddea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58d4d162d876665e765e23ec56b6df3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e13ee1d589589a4b8d518be7eca2364e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a78cf6006e8cb73de1dd6fba565439.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Thinking-Longer-Not-Larger-Enhancing-Software-Engineering-Agents-via-Scaling-Test-Time-Compute"><a href="#Thinking-Longer-Not-Larger-Enhancing-Software-Engineering-Agents-via-Scaling-Test-Time-Compute" class="headerlink" title="Thinking Longer, Not Larger: Enhancing Software Engineering Agents via   Scaling Test-Time Compute"></a>Thinking Longer, Not Larger: Enhancing Software Engineering Agents via   Scaling Test-Time Compute</h2><p><strong>Authors:Yingwei Ma, Yongbin Li, Yihong Dong, Xue Jiang, Rongyu Cao, Jue Chen, Fei Huang, Binhua Li</strong></p>
<p>Recent advancements in software engineering agents have demonstrated promising capabilities in automating program improvements. However, their reliance on closed-source or resource-intensive models introduces significant deployment challenges in private environments, prompting a critical question: \textit{How can personally deployable open-source LLMs achieve comparable code reasoning performance?}   To this end, we propose a unified Test-Time Compute scaling framework that leverages increased inference-time computation instead of larger models. Our framework incorporates two complementary strategies: internal TTC and external TTC. Internally, we introduce a \textit{development-contextualized trajectory synthesis} method leveraging real-world software repositories to bootstrap multi-stage reasoning processes, such as fault localization and patch generation. We further enhance trajectory quality through rejection sampling, rigorously evaluating trajectories along accuracy and complexity. Externally, we propose a novel \textit{development-process-based search} strategy guided by reward models and execution verification. This approach enables targeted computational allocation at critical development decision points, overcoming limitations of existing â€œend-point onlyâ€ verification methods.   Evaluations on SWE-bench Verified demonstrate our \textbf{32B model achieves a 46% issue resolution rate}, surpassing significantly larger models such as DeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical validation of the test-time scaling phenomenon within SWE agents, revealing that \textbf{models dynamically allocate more tokens to increasingly challenging problems}, effectively enhancing reasoning capabilities. We publicly release all training data, models, and code to facilitate future research. <a target="_blank" rel="noopener" href="https://github.com/yingweima2022/SWE-Reasoner">https://github.com/yingweima2022/SWE-Reasoner</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè½¯ä»¶å·¥ç¨‹ä»£ç†æ–¹é¢çš„æœ€æ–°è¿›å±•åœ¨ç¨‹åºè‡ªåŠ¨åŒ–æ”¹è¿›æ–¹é¢å±•ç°å‡ºä»¤äººé¼“èˆçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å°é—­æºä»£ç æˆ–èµ„æºå¯†é›†å‹æ¨¡å‹çš„ä¾èµ–ï¼Œç»™ç§æœ‰ç¯å¢ƒéƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç”±æ­¤å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸ªäººå¯éƒ¨ç½²çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°ç›¸å½“çš„è½¯ä»¶ä»£ç æ¨ç†æ€§èƒ½ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¢åŠ æ¨ç†æ—¶é—´è®¡ç®—è€Œä¸æ˜¯ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸¤ç§äº’è¡¥çš„ç­–ç•¥ï¼šå†…éƒ¨TTCå’Œå¤–éƒ¨TTCã€‚åœ¨å†…éƒ¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ©ç”¨ç°å®ä¸–ç•Œè½¯ä»¶ä»“åº“è¿›è¡Œâ€œå¼€å‘è¯­å¢ƒåŒ–è½¨è¿¹åˆæˆâ€çš„æ–¹æ³•ï¼Œä»¥å¯åŠ¨å¤šé˜¶æ®µæ¨ç†è¿‡ç¨‹ï¼Œå¦‚æ•…éšœå®šä½å’Œè¡¥ä¸ç”Ÿæˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æ‹’ç»é‡‡æ ·æé«˜è½¨è¿¹è´¨é‡ï¼Œä¸¥æ ¼è¯„ä¼°è½¨è¿¹çš„å‡†ç¡®æ€§å’Œå¤æ‚æ€§ã€‚åœ¨å¤–éƒ¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå¼€å‘è¿‡ç¨‹çš„æœç´¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»¥å¥–åŠ±æ¨¡å‹å’Œæ‰§è¡ŒåŠ›éªŒè¯ä¸ºæŒ‡å¯¼ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨å…³é”®çš„å¼€å‘å†³ç­–ç‚¹å®ç°æœ‰é’ˆå¯¹æ€§çš„è®¡ç®—åˆ†é…ï¼Œå…‹æœäº†ç°æœ‰â€œä»…ç»ˆç‚¹â€éªŒè¯æ–¹æ³•çš„å±€é™æ€§ã€‚åœ¨SWE-bench Verifiedä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„32Bæ¨¡å‹è¾¾åˆ°äº†46%çš„é—®é¢˜è§£å†³ç‡ï¼Œè¶…è¿‡äº†æ˜¾è‘—æ›´å¤§çš„æ¨¡å‹ï¼Œå¦‚DeepSeek R1 671Bå’ŒOpenAI o1ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®è¯éªŒè¯äº†SWEä»£ç†ä¸­çš„æµ‹è¯•æ—¶é—´æ‰©å±•ç°è±¡ï¼Œè¡¨æ˜æ¨¡å‹ä¼šâ€œåŠ¨æ€åœ°ä¸ºè¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜åˆ†é…æ›´å¤šçš„ä»¤ç‰Œâ€ï¼Œä»è€Œæœ‰æ•ˆæé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ‰€æœ‰è®­ç»ƒæ•°æ®ã€æ¨¡å‹å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚<a target="_blank" rel="noopener" href="https://github.com/yingweima2022/SWE-Reasoner">https://github.com/yingweima2022/SWE-Reasoner</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23803v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè½¯ä»¶å·¥ç¨‹ä»£ç†é¢†åŸŸçš„è¿›å±•åœ¨è‡ªåŠ¨åŒ–ç¨‹åºæ”¹è¿›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶åœ¨ç§æœ‰ç¯å¢ƒä¸­çš„éƒ¨ç½²æŒ‘æˆ˜ä»ç„¶æ˜¾è‘—ã€‚é’ˆå¯¹æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç»Ÿä¸€æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åœ¨ä¸ä¾èµ–æ›´å¤§æ¨¡å‹çš„æƒ…å†µä¸‹æå‡ä»£ç æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬é‡‡å–ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šå†…éƒ¨ä¸å¤–éƒ¨æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•ã€‚å†…éƒ¨é‡‡ç”¨å¼€å‘ä¸Šä¸‹æ–‡è½¨è¿¹åˆæˆæ–¹æ³•ï¼Œç»“åˆç°å®ä¸–ç•Œè½¯ä»¶ä»“åº“è¿›è¡Œå¤šé˜¶æ®µæ¨ç†è¿‡ç¨‹ï¼Œå¦‚æ•…éšœå®šä½å’Œè¡¥ä¸ç”Ÿæˆã€‚å¤–éƒ¨åˆ™æå‡ºåŸºäºå¼€å‘è¿‡ç¨‹çš„æœç´¢ç­–ç•¥ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹å’Œæ‰§è¡ŒéªŒè¯å®ç°è®¡ç®—èµ„æºçš„ç²¾å‡†åˆ†é…ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é—®é¢˜è§£å†³ç‡ä¸Šè¶…è¶Šå¤§å‹æ¨¡å‹ï¼Œä¸”åŠ¨æ€åˆ†é…æ›´å¤šä»¤ç‰Œåº”å¯¹å¤æ‚é—®é¢˜ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å·²å…¬å¼€å…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è½¯ä»¶å·¥ç¨‹ä»£ç†åœ¨è‡ªåŠ¨åŒ–ç¨‹åºæ”¹è¿›æ–¹é¢å±•ç°æ½œåŠ›ï¼Œä½†åœ¨ç§æœ‰ç¯å¢ƒä¸­éƒ¨ç½²å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºç»Ÿä¸€æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•æ¡†æ¶ï¼Œä»¥æé«˜ä»£ç æ¨ç†æ€§èƒ½ï¼Œæ— éœ€ä¾èµ–æ›´å¤§çš„æ¨¡å‹ã€‚</li>
<li>å†…éƒ¨é‡‡ç”¨å¼€å‘ä¸Šä¸‹æ–‡è½¨è¿¹åˆæˆæ–¹æ³•ï¼Œç»“åˆè½¯ä»¶ä»“åº“è¿›è¡Œæ•…éšœå®šä½å’Œè¡¥ä¸ç”Ÿæˆç­‰å¤šé˜¶æ®µæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡æ‹’ç»é‡‡æ ·æé«˜è½¨è¿¹è´¨é‡ï¼Œä¸¥æ ¼æŒ‰ç…§å‡†ç¡®æ€§å’Œå¤æ‚æ€§è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å¤–éƒ¨é‡‡ç”¨åŸºäºå¼€å‘è¿‡ç¨‹çš„æœç´¢ç­–ç•¥ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹å’Œæ‰§è¡ŒéªŒè¯å®ç°è®¡ç®—èµ„æºçš„ç²¾å‡†åˆ†é…ã€‚</li>
<li>æ¨¡å‹è¡¨ç°å‡ºè¾ƒé«˜çš„é—®é¢˜è§£å†³ç‡ï¼Œå¹¶åŠ¨æ€åˆ†é…æ›´å¤šèµ„æºåº”å¯¹å¤æ‚é—®é¢˜ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d43a0454a3e25a6b26dcfd16bd6b654.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0be1ef87ee770b7c676a2aa2c8950506.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dad49df68602b8d1528816dd91e25232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d744cd0f86b1300cf3e0ec37b71e5e79.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="How-Well-Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments"><a href="#How-Well-Can-Modern-LLMs-Act-as-Agent-Cores-in-Radiology-Environments" class="headerlink" title="How Well Can Modern LLMs Act as Agent Cores in Radiology Environments?"></a>How Well Can Modern LLMs Act as Agent Cores in Radiology Environments?</h2><p><strong>Authors:Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>We introduce RadA-BenchPlat, an evaluation platform that benchmarks the performance of large language models (LLMs) act as agent cores in radiology environments using 2,200 radiologist-verified synthetic patient records covering six anatomical regions, five imaging modalities, and 2,200 disease scenarios, resulting in 24,200 question-answer pairs that simulate diverse clinical situations. The platform also defines ten categories of tools for agent-driven task solving and evaluates seven leading LLMs, revealing that while models like Claude-3.7-Sonnet can achieve a 67.1% task completion rate in routine settings, they still struggle with complex task understanding and tool coordination, limiting their capacity to serve as the central core of automated radiology systems. By incorporating four advanced prompt engineering strategiesâ€“where prompt-backpropagation and multi-agent collaboration contributed 16.8% and 30.7% improvements, respectivelyâ€“the performance for complex tasks was enhanced by 48.2% overall. Furthermore, automated tool building was explored to improve robustness, achieving a 65.4% success rate, thereby offering promising insights for the future integration of fully automated radiology applications into clinical practice. All of our code and data are openly available at <a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench">https://github.com/MAGIC-AI4Med/RadABench</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†RadA-BenchPlatè¯„ä¼°å¹³å°ï¼Œè¯¥å¹³å°é€šè¿‡ä½¿ç”¨2200ä»½ç»è¿‡æ”¾å°„ç§‘åŒ»ç”Ÿæ ¸å®çš„äººå·¥åˆæˆæ‚£è€…è®°å½•ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­çš„ä»£ç†æ ¸å¿ƒæ€§èƒ½è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚è¿™äº›è®°å½•æ¶µç›–äº†å…­ä¸ªè§£å‰–åŒºåŸŸã€äº”ç§æˆåƒæ¨¡å¼å’Œ2200ç§ç–¾ç—…æƒ…æ™¯ï¼Œäº§ç”Ÿäº†æ¨¡æ‹Ÿå„ç§ä¸´åºŠæƒ…å¢ƒçš„24200ä¸ªé—®ç­”å¯¹ã€‚å¹³å°è¿˜å®šä¹‰äº†åå¤§ç±»å·¥å…·ç”¨äºä»£ç†é©±åŠ¨çš„ä»»åŠ¡è§£å†³ï¼Œå¹¶è¯„ä¼°äº†ä¸ƒæ¬¾é¢†å…ˆçš„LLMã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶åœ¨å¸¸è§„ç¯å¢ƒä¸­ï¼ŒåƒClaude-3.7-Sonnetè¿™æ ·çš„æ¨¡å‹å¯ä»¥è¾¾åˆ°67.1%çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç†è§£å’Œå¤„ç†å¤æ‚çš„ä»»åŠ¡å¹¶åè°ƒå·¥å…·ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ä½œä¸ºè‡ªåŠ¨åŒ–æ”¾å°„ç³»ç»Ÿæ ¸å¿ƒçš„èƒ½åŠ›ã€‚é€šè¿‡é‡‡ç”¨å››ç§å…ˆè¿›çš„æç¤ºå·¥ç¨‹ç­–ç•¥â€”â€”å…¶ä¸­æç¤ºåå‘ä¼ æ’­å’Œå¤šä»£ç†åä½œåˆ†åˆ«è´¡çŒ®äº†16.8%å’Œ30.7%çš„æ”¹è¿›â€”â€”å¤æ‚ä»»åŠ¡çš„æ€§èƒ½æ€»ä½“æé«˜äº†48.2%ã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†è‡ªåŠ¨å·¥å…·æ„å»ºä»¥æé«˜ç¨³å¥æ€§ï¼Œè¾¾åˆ°äº†65.4%çš„æˆåŠŸç‡ï¼Œä»è€Œä¸ºæœªæ¥å…¨è‡ªåŠ¨æ”¾å°„åº”ç”¨ç¨‹åºèå…¥ä¸´åºŠå®è·µæä¾›äº†æœ‰å‰æ™¯çš„è§è§£ã€‚æˆ‘ä»¬çš„æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½å…¬å¼€å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MAGIC-AI4Med/RadABench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MAGIC-AI4Med/RadABenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09529v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>RadA-BenchPlatå¹³å°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­çš„æ€§èƒ½ï¼Œåˆ©ç”¨2200ä»½ç»è¿‡æ”¾å°„ç§‘åŒ»ç”ŸéªŒè¯çš„åˆæˆæ‚£è€…è®°å½•æ¨¡æ‹Ÿå¤šç§ä¸´åºŠæƒ…å¢ƒã€‚è¯¥å¹³å°å®šä¹‰äº†åç±»å·¥å…·ç”¨äºä»£ç†é©±åŠ¨çš„ä»»åŠ¡è§£å†³ï¼Œå¹¶è¯„ä¼°äº†ä¸ƒæ¬¾é¢†å…ˆçš„LLMæ¨¡å‹ã€‚è™½ç„¶ä¸€äº›æ¨¡å‹å¦‚Claude-3.7-Sonnetåœ¨æ—¥å¸¸è®¾ç½®ä¸­çš„ä»»åŠ¡å®Œæˆç‡å¯è¾¾67.1%ï¼Œä½†å®ƒä»¬å¯¹äºå¤æ‚ä»»åŠ¡ç†è§£å’Œå·¥å…·åè°ƒä»å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥ä½œä¸ºè‡ªåŠ¨åŒ–æ”¾å°„ç³»ç»Ÿçš„æ ¸å¿ƒã€‚é€šè¿‡é‡‡ç”¨å››ç§å…ˆè¿›çš„æç¤ºå·¥ç¨‹ç­–ç•¥å’Œå¤šä»£ç†åä½œç­‰æŠ€æœ¯ï¼Œå¤æ‚ä»»åŠ¡çš„æ€§èƒ½æ€»ä½“æé«˜äº†48.2%ã€‚åŒæ—¶ï¼Œæ¢ç´¢äº†è‡ªåŠ¨åŒ–å·¥å…·å»ºè®¾ä»¥æé«˜ç¨³å¥æ€§ï¼Œè¾¾åˆ°65.4%çš„æˆåŠŸç‡ï¼Œä¸ºæœªæ¥å…¨è‡ªåŠ¨æ”¾å°„å­¦åº”ç”¨çš„ä¸´åºŠå®è·µæä¾›äº†æœ‰å‰æ™¯çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RadA-BenchPlatæ˜¯ä¸€ä¸ªè¯„ä¼°å¹³å°ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>å¹³å°ä½¿ç”¨åˆæˆæ‚£è€…è®°å½•æ¨¡æ‹Ÿä¸´åºŠæƒ…å¢ƒï¼Œæ¶µç›–å…­ä¸ªè§£å‰–åŒºåŸŸã€äº”ç§æˆåƒæ¨¡å¼å’Œ2200ç§ç–¾ç—…åœºæ™¯ã€‚</li>
<li>å¹³å°å®šä¹‰äº†åç±»å·¥å…·ç”¨äºä»£ç†é©±åŠ¨çš„ä»»åŠ¡è§£å†³ï¼Œè¯„ä¼°äº†ä¸ƒæ¬¾LLMæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸€äº›æ¨¡å‹åœ¨æ—¥å¸¸ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡ç†è§£å’Œå·¥å…·åè°ƒæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹ç­–ç•¥ï¼Œå¦‚æç¤ºåå‘ä¼ æ’­å’Œå¤šä»£ç†åä½œï¼Œå¯ä»¥æé«˜å¤æ‚ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>è‡ªåŠ¨åŒ–å·¥å…·å»ºè®¾å¯ä»¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œè¾¾åˆ°65.4%çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e10b5eeeedf6eb51e69c4c8d338cbe49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b70ad4a7ff799314230f6243d7532a84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0835bfb7adb77f937b9d67027191d29.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Augmenting-the-action-space-with-conventions-to-improve-multi-agent-cooperation-in-Hanabi"><a href="#Augmenting-the-action-space-with-conventions-to-improve-multi-agent-cooperation-in-Hanabi" class="headerlink" title="Augmenting the action space with conventions to improve multi-agent   cooperation in Hanabi"></a>Augmenting the action space with conventions to improve multi-agent   cooperation in Hanabi</h2><p><strong>Authors:F. Bredell, H. A. Engelbrecht, J. C. Schoeman</strong></p>
<p>The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, hidden information, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for a various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of &#96;&#96;rulesâ€™â€™. Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting the action space using conventions, which act as special cooperative actions that span over multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play across a various number of cooperators within Hanabi. </p>
<blockquote>
<p>æ±‰è¯ºå¡”ç‰Œæ¸¸æˆè¢«è®¤ä¸ºæ˜¯æµ‹è¯•å’Œå‘å±•å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æœ‰åŠ›å·¥å…·ï¼Œå…¶åŸå› åœ¨äºå…¶åˆä½œæ€§è´¨ã€éšè—ä¿¡æ¯ã€æœ‰é™çš„äº¤æµå’Œå¤æ‚çš„ç‰¹æ€§ã€‚ä¹‹å‰çš„ç ”ç©¶åŠªåŠ›å·²ç»æ¢ç´¢äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨æ±‰è¯ºå¡”æ¸¸æˆä¸­çš„èƒ½åŠ›ï¼Œä¸»è¦é›†ä¸­åœ¨é«˜çº§æ¶æ„è®¾è®¡ä»¥åŠç®—æ³•æ“ä½œï¼Œä»¥å®ç°å„ç§åˆä½œäººæ•°çš„æœ€æ–°æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´è§£å†³æ–¹æ¡ˆå¤æ‚ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚äººç±»æœ‰æ•ˆåœ°è§£å†³æ±‰è¯ºå¡”æ¸¸æˆéœ€è¦è¿ç”¨è§„åˆ™ï¼Œè§„åˆ™å…è®¸åŸºäºé¢„å…ˆå®šä¹‰å’Œå…±åŒè®¤å¯çš„ä¸€å¥—è§„åˆ™æ¥éšå«åœ°ä¼ è¾¾æ€æƒ³æˆ–çŸ¥è¯†ã€‚åŒ…å«éƒ¨åˆ†å¯è§‚å¯Ÿæ€§çš„å¤šæ™ºèƒ½ä½“é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“å­˜åœ¨æœ‰é™çš„äº¤æµæ—¶ï¼Œå¯ä»¥å¤§å¤§å—ç›Šäºéšæ€§çŸ¥è¯†å…±äº«çš„ä½¿ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è§„åˆ™æ¥å¢åŠ åŠ¨ä½œç©ºé—´çš„æ–°æ–¹æ³•ï¼Œè¿™äº›è§„åˆ™ä½œä¸ºç‰¹æ®Šçš„åˆä½œåŠ¨ä½œï¼Œè·¨è¶Šå¤šä¸ªæ—¶é—´æ­¥éª¤å’Œå¤šä¸ªæ™ºèƒ½ä½“ï¼Œéœ€è¦æ™ºèƒ½ä½“ç§¯æå‚ä¸ä»¥è¾¾åˆ°å®ç°ã€‚è¿™äº›è§„åˆ™åŸºäºç°æœ‰çš„äººç±»è§„åˆ™ï¼Œå¯¼è‡´æ±‰è¯ºå¡”æ¸¸æˆä¸­å„ç§åˆä½œäººæ•°çš„è‡ªç©å’Œè·¨ç©çš„ç°æœ‰æŠ€æœ¯æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06333v2">PDF</a> This paper is under review at the journal of autonomous agents and   multi-agent systems (JAAMAS). The updated manuscript is the revised version   after the first round of peer revision</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨æ±‰è¯ºå¡”æ¸¸æˆä¸­çš„æµ‹è¯•ä¸å‘å±•ã€‚ç”±äºæ±‰è¯ºå¡”æ¸¸æˆçš„åˆä½œæ€§ã€éšè—ä¿¡æ¯ã€æœ‰é™æ²Ÿé€šä»¥åŠé«˜åº¦å¤æ‚æ€§ï¼Œä½¿å…¶æˆä¸ºæµ‹è¯•å’Œå‘å±•å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æœ‰åŠ›å¹³å°ã€‚ç ”ç©¶äººå‘˜å·²æ¢ç´¢äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨æ±‰è¯ºå¡”æ¸¸æˆä¸­çš„èƒ½åŠ›ï¼Œå¹¶è‡´åŠ›äºè®¾è®¡é«˜çº§æ¶æ„å’Œç®—æ³•æ“ä½œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™å¸¸å¸¸å¯¼è‡´å¤æ‚çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æƒ¯ä¾‹æ¥æ‰©å±•è¡ŒåŠ¨ç©ºé—´çš„æ–°æ–¹æ³•ï¼Œè¿™äº›æƒ¯ä¾‹ä½œä¸ºç‰¹æ®Šçš„åˆä½œè¡ŒåŠ¨ï¼Œè·¨è¶Šå¤šä¸ªæ—¶é—´æ­¥éª¤å’Œå¤šä¸ªæ™ºèƒ½ä½“ï¼Œè¦æ±‚æ™ºèƒ½ä½“ç§¯æå‚ä¸ä»¥è¾¾åˆ°å®ç°ã€‚è¿™äº›æƒ¯ä¾‹åŸºäºç°æœ‰çš„äººç±»æƒ¯ä¾‹ï¼Œå¹¶åœ¨æ±‰è¯ºå¡”æ¸¸æˆä¸­æ˜¾è‘—æé«˜äº†ç°æœ‰æŠ€æœ¯çš„è‡ªæˆ‘ç©å’Œè·¨ç©å®¶åˆä½œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ±‰è¯ºå¡”æ¸¸æˆæ˜¯å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„é‡è¦æµ‹è¯•å’Œå‘å±•å¹³å°ï¼Œå› å…¶åˆä½œæ€§ã€éšè—ä¿¡æ¯ã€æœ‰é™æ²Ÿé€šå’Œå¤æ‚æ€§ã€‚</li>
<li>æ­¤å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è®¾è®¡é«˜çº§æ¶æ„å’Œç®—æ³•æ“ä½œä¸Šï¼Œä»¥å®ç°æ±‰è¯ºå¡”æ¸¸æˆä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¿™äº›æ–¹æ³•å¸¸å¸¸å¯¼è‡´å¤æ‚çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>åˆ©ç”¨äººç±»æƒ¯ä¾‹ï¼Œå¯ä»¥é€šè¿‡æ‰©å±•è¡ŒåŠ¨ç©ºé—´æ¥æé«˜å¤šæ™ºèƒ½ä½“æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ç‰¹æ®Šçš„åˆä½œè¡ŒåŠ¨â€”â€”æƒ¯ä¾‹ï¼Œè¿™äº›è¡ŒåŠ¨è·¨è¶Šå¤šä¸ªæ—¶é—´æ­¥éª¤å’Œå¤šä¸ªæ™ºèƒ½ä½“ã€‚</li>
<li>æ™ºèƒ½ä½“éœ€è¦ç§¯æå‚ä¸æƒ¯ä¾‹ä»¥è¾¾åˆ°å®ç°ï¼Œè¿™æ˜¾è‘—æé«˜äº†æ±‰è¯ºå¡”æ¸¸æˆä¸­ç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a711b8b041dfec9576c11165d2e17885.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Inverse-Attention-Agents-for-Multi-Agent-Systems"><a href="#Inverse-Attention-Agents-for-Multi-Agent-Systems" class="headerlink" title="Inverse Attention Agents for Multi-Agent Systems"></a>Inverse Attention Agents for Multi-Agent Systems</h2><p><strong>Authors:Qian Long, Ruoyan Li, Minglu Zhao, Tao Gao, Demetri Terzopoulos</strong></p>
<p>A major challenge for Multi-Agent Systems is enabling agents to adapt dynamically to diverse environments in which opponents and teammates may continually change. Agents trained using conventional methods tend to excel only within the confines of their training cohorts; their performance drops significantly when confronting unfamiliar agents. To address this shortcoming, we introduce Inverse Attention Agents that adopt concepts from the Theory of Mind (ToM) implemented algorithmically using an attention mechanism trained in an end-to-end manner. Crucial to determining the final actions of these agents, the weights in their attention model explicitly represent attention to different goals. We furthermore propose an inverse attention network that deduces the ToM of agents based on observations and prior actions. The network infers the attentional states of other agents, thereby refining the attention weights to adjust the agentâ€™s final action. We conduct experiments in a continuous environment, tackling demanding tasks encompassing cooperation, competition, and a blend of both. They demonstrate that the inverse attention network successfully infers the attention of other agents, and that this information improves agent performance. Additional human experiments show that, compared to baseline agent models, our inverse attention agents exhibit superior cooperation with humans and better emulate human behaviors. </p>
<blockquote>
<p>å¯¹äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè€Œè¨€ï¼Œé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€‚åº”ä¸æ–­å˜åŒ–çš„å¤šç§ç¯å¢ƒï¼Œè¿™äº›ç¯å¢ƒä¸­çš„å¯¹æ‰‹å’Œé˜Ÿå‹å¯èƒ½ä¼šä¸æ–­å˜åŒ–ã€‚ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è®­ç»ƒçš„æ™ºèƒ½ä½“å¾€å¾€åªåœ¨å…¶è®­ç»ƒç¾¤ä½“èŒƒå›´å†…è¡¨ç°å‡ºè‰²ï¼›å½“é¢å¯¹ä¸ç†Ÿæ‚‰çš„æ™ºèƒ½ä½“æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€†å‘æ³¨æ„åŠ›æ™ºèƒ½ä½“ï¼Œå®ƒé‡‡ç”¨äº†å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰çš„æ¦‚å¿µï¼Œé€šè¿‡ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ç®—æ³•å®ç°ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚è¿™äº›æ™ºèƒ½ä½“çš„æœ€ç»ˆè¡ŒåŠ¨å–å†³äºå…¶æ³¨æ„åŠ›æ¨¡å‹çš„æƒé‡ï¼Œè¯¥æƒé‡æ˜ç¡®è¡¨ç¤ºäº†å¯¹ä¸åŒç›®æ ‡çš„å…³æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºè§‚å¯Ÿå’Œå…ˆå‰è¡ŒåŠ¨æ¥æ¨æ–­æ™ºèƒ½ä½“å¿ƒæ™ºç†è®ºçš„é€†å‘æ³¨æ„åŠ›ç½‘ç»œã€‚è¯¥ç½‘ç»œæ¨æ–­å…¶ä»–æ™ºèƒ½ä½“çš„æ³¨æ„åŠ›çŠ¶æ€ï¼Œä»è€Œè°ƒæ•´æ³¨æ„åŠ›æƒé‡ä»¥è°ƒæ•´æ™ºèƒ½ä½“çš„æœ€ç»ˆè¡ŒåŠ¨ã€‚æˆ‘ä»¬åœ¨è¿ç»­ç¯å¢ƒä¸­è¿›è¡Œå®éªŒï¼Œè§£å†³åŒ…å«åˆä½œã€ç«äº‰å’Œä¸¤è€…æ··åˆçš„è‰°å·¨ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œé€†å‘æ³¨æ„åŠ›ç½‘ç»œæˆåŠŸæ¨æ–­å‡ºå…¶ä»–æ™ºèƒ½ä½“çš„æ³¨æ„åŠ›ï¼Œå¹¶ä¸”è¿™ä¸€ä¿¡æ¯æé«˜äº†æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚é¢å¤–çš„äººç±»å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ™ºèƒ½ä½“æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„é€†å‘æ³¨æ„åŠ›æ™ºèƒ½ä½“ä¸äººç±»è¡¨ç°å‡ºæ›´å¥½çš„åˆä½œï¼Œæ›´èƒ½æ¨¡ä»¿äººç±»è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21794v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¤šå˜ç¯å¢ƒä¸­ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿé¢ä¸´ä¸€å¤§æŒ‘æˆ˜ï¼Œå³å¦‚ä½•ä½¿æ™ºèƒ½ä½“é€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä¸­çš„å¯¹æ‰‹å’Œé˜Ÿå‹ã€‚é‡‡ç”¨å¸¸è§„è®­ç»ƒæ–¹æ³•çš„æ™ºèƒ½ä½“é€šå¸¸ä»…åœ¨è®­ç»ƒç¾¤ä½“å†…éƒ¨è¡¨ç°å‡ºè‰²ï¼Œè€Œåœ¨é¢å¯¹é™Œç”Ÿæ™ºèƒ½ä½“æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€†å‘æ³¨æ„åŠ›æ™ºèƒ½ä½“ï¼Œå®ƒé‡‡ç”¨å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰çš„æ¦‚å¿µï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä»¥ç«¯å¯¹ç«¯æ–¹å¼å®ç°ç®—æ³•å®ç°ã€‚å†³å®šè¿™äº›æ™ºèƒ½ä½“æœ€ç»ˆè¡ŒåŠ¨çš„å…³é”®æ˜¯å®ƒä»¬æ³¨æ„åŠ›æ¨¡å‹çš„æƒé‡ï¼Œè¿™äº›æƒé‡æ˜ç¡®ä½“ç°äº†å¯¹ä¸åŒç›®æ ‡çš„å…³æ³¨ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºè§‚å¯Ÿå’Œå…ˆå‰è¡ŒåŠ¨æ¥æ¨æ–­æ™ºèƒ½ä½“å¿ƒæ™ºç†è®ºçš„é€†å‘æ³¨æ„åŠ›ç½‘ç»œã€‚è¯¥ç½‘ç»œä¼šæ¨æ–­å…¶ä»–æ™ºèƒ½ä½“çš„æ³¨æ„åŠ›çŠ¶æ€ï¼Œä»è€Œè°ƒæ•´æ³¨æ„åŠ›æƒé‡ä»¥è°ƒæ•´æ™ºèƒ½ä½“çš„æœ€ç»ˆè¡ŒåŠ¨ã€‚åœ¨è¿ç»­ç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼Œé€†å‘æ³¨æ„åŠ›ç½‘ç»œæˆåŠŸæ¨æ–­å‡ºå…¶ä»–æ™ºèƒ½ä½“çš„æ³¨æ„åŠ›ï¼Œå¹¶ä¸”è¿™ä¸€ä¿¡æ¯æé«˜äº†æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚é¢å¤–çš„äººç±»å®éªŒæ˜¾ç¤ºï¼Œä¸åŸºçº¿æ™ºèƒ½ä½“æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„é€†å‘æ³¨æ„åŠ›æ™ºèƒ½ä½“ä¸äººç±»åˆä½œæ›´ä¸ºå‡ºè‰²ï¼Œæ›´èƒ½æ¨¡ä»¿äººç±»è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ™ºèƒ½ä½“èƒ½é€‚åº”ä¸æ–­å˜åŒ–çš„å¯¹æ‰‹å’Œé˜Ÿå‹ã€‚</li>
<li>ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•çš„æ™ºèƒ½ä½“åœ¨é¢ä¸´é™Œç”Ÿæ™ºèƒ½ä½“æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¼•å…¥é€†å‘æ³¨æ„åŠ›æ™ºèƒ½ä½“ï¼Œç»“åˆå¿ƒæ™ºç†è®ºï¼ˆToMï¼‰çš„æ¦‚å¿µï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç®—æ³•å®ç°ã€‚</li>
<li>æ³¨æ„åŠ›æ¨¡å‹çš„æƒé‡å¯¹æ™ºèƒ½ä½“çš„æœ€ç»ˆè¡ŒåŠ¨èµ·åˆ°å…³é”®ä½œç”¨ï¼Œåæ˜ äº†å¯¹ä¸åŒç›®æ ‡çš„å…³æ³¨ã€‚</li>
<li>é€†å‘æ³¨æ„åŠ›ç½‘ç»œèƒ½åŸºäºè§‚å¯Ÿå’Œå…ˆå‰è¡ŒåŠ¨æ¨æ–­å…¶ä»–æ™ºèƒ½ä½“çš„å¿ƒæ™ºç†è®ºã€‚</li>
<li>é€†å‘æ³¨æ„åŠ›ç½‘ç»œæˆåŠŸæ¨æ–­å‡ºå…¶ä»–æ™ºèƒ½ä½“çš„æ³¨æ„åŠ›ï¼Œæé«˜äº†æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-caa49dd0ce1c1888bdef567a9b02e46e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f14b38287870d91fccbed24c59288f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e402e8dbe84fe8e929b91ad82f831748.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GRAPPA-Generalizing-and-Adapting-Robot-Policies-via-Online-Agentic-Guidance"><a href="#GRAPPA-Generalizing-and-Adapting-Robot-Policies-via-Online-Agentic-Guidance" class="headerlink" title="GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic   Guidance"></a>GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic   Guidance</h2><p><strong>Authors:Arthur Bucker, Pablo Ortega-Kral, Jonathan Francis, Jean Oh</strong></p>
<p>Robot learning approaches such as behavior cloning and reinforcement learning have shown great promise in synthesizing robot skills from human demonstrations in specific environments. However, these approaches often require task-specific demonstrations or designing complex simulation environments, which limits the development of generalizable and robust policies for unseen real-world settings. Recent advances in the use of foundation models for robotics (e.g., LLMs, VLMs) have shown great potential in enabling systems to understand the semantics in the world from large-scale internet data. However, it remains an open challenge to use this knowledge to enable robotic systems to understand the underlying dynamics of the world, to generalize policies across different tasks, and to adapt policies to new environments. To alleviate these limitations, we propose an agentic framework for robot self-guidance and self-improvement, which consists of a set of role-specialized conversational agents, such as a high-level advisor, a grounding agent, a monitoring agent, and a robotic agent. Our framework iteratively grounds a base robot policy to relevant objects in the environment and uses visuomotor cues to shift the action distribution of the policy to more desirable states, online, while remaining agnostic to the subjective configuration of a given robot hardware platform. We demonstrate that our approach can effectively guide manipulation policies to achieve significantly higher success rates, both in simulation and in real-world experiments, without the need for additional human demonstrations or extensive exploration. Code and videos available at: <a target="_blank" rel="noopener" href="https://agenticrobots.github.io/">https://agenticrobots.github.io</a> </p>
<blockquote>
<p>æœºå™¨äººå­¦ä¹ çš„æ–¹æ³•ï¼Œå¦‚è¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ ï¼Œåœ¨åˆæˆç‰¹å®šç¯å¢ƒä¸­äººç±»æ¼”ç¤ºçš„æœºå™¨äººæŠ€èƒ½æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦ç‰¹å®šçš„ä»»åŠ¡æ¼”ç¤ºæˆ–è®¾è®¡å¤æ‚çš„ä»¿çœŸç¯å¢ƒï¼Œè¿™é™åˆ¶äº†é’ˆå¯¹æœªè§è¿‡çš„çœŸå®ä¸–ç•Œç¯å¢ƒåˆ¶å®šå¯æ¨å¹¿å’Œç¨³å¥çš„ç­–ç•¥ã€‚æœ€è¿‘ï¼Œåœ¨æœºå™¨äººæŠ€æœ¯ä¸­ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚LLMå’ŒVLMï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿä½¿å¾—ç³»ç»Ÿä»å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®ä¸­ç†è§£ä¸–ç•Œè¯­ä¹‰ã€‚ç„¶è€Œï¼Œä½¿ç”¨è¿™äº›çŸ¥è¯†ä½¿æœºå™¨äººç³»ç»Ÿç†è§£ä¸–ç•Œçš„åŸºæœ¬åŠ¨æ€ã€åœ¨ä¸åŒä»»åŠ¡ä¸­æ¨å¹¿ç­–ç•¥ä»¥åŠé€‚åº”æ–°ç¯å¢ƒï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºæœºå™¨äººè‡ªæˆ‘æŒ‡å¯¼å’Œè‡ªæˆ‘æ”¹è¿›çš„ä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ç³»åˆ—ä¸“ä¸šè§’è‰²çš„å¯¹è¯ä»£ç†ï¼Œå¦‚é«˜çº§é¡¾é—®ã€æ¥åœ°ä»£ç†ã€ç›‘æ§ä»£ç†å’Œæœºå™¨äººä»£ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†åŸºç¡€æœºå™¨äººç­–ç•¥è¿­ä»£åœ°å…³è”åˆ°ç¯å¢ƒä¸­çš„ç›¸å…³å¯¹è±¡ï¼Œå¹¶ä½¿ç”¨è§†è§‰è¿åŠ¨çº¿ç´¢åœ¨çº¿å°†ç­–ç•¥çš„è¡ŒåŠ¨åˆ†å¸ƒè½¬ç§»åˆ°æ›´ç†æƒ³çš„çŠ¶æ€ï¼ŒåŒæ—¶ä¿æŒå¯¹ç»™å®šæœºå™¨äººç¡¬ä»¶å¹³å°çš„ä¸»è§‚é…ç½®çš„ç‹¬ç«‹æ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼æ“ä½œç­–ç•¥ï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­å®ç°æ›´é«˜çš„æˆåŠŸç‡ï¼Œæ— éœ€é¢å¤–çš„äººç±»æ¼”ç¤ºæˆ–å¤§é‡æ¢ç´¢ã€‚ä»£ç å’Œè§†é¢‘å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://agenticrobots.github.ioæŸ¥çœ‹./">https://agenticrobots.github.ioæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06473v3">PDF</a> 21 pages, 12 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ ç­‰æœºå™¨äººå­¦ä¹ æ–¹æ³•çš„æ½œåŠ›ï¼Œç»“åˆå¤§å‹äº’è”ç½‘æ•°æ®ä¸‹çš„æœºå™¨äººå¯¹ä¸–ç•Œçš„è¯­ä¹‰ç†è§£ï¼Œæå‡ºäº†ä¸€ç§æœºå™¨äººè‡ªæˆ‘æŒ‡å¯¼å’Œè‡ªæˆ‘æ”¹è¿›çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬é«˜çº§é¡¾é—®ã€æ¥åœ°æ™ºèƒ½ä½“ã€ç›‘æ§æ™ºèƒ½ä½“å’Œæœºå™¨äººæ™ºèƒ½ä½“ç­‰è§’è‰²ä¸“ä¸šåŒ–å¯¹è¯æ™ºèƒ½ä½“ã€‚æ­¤æ¡†æ¶æ— éœ€é¢å¤–çš„ä¸»è§‚æœºå™¨äººç¡¬ä»¶é…ç½®æˆ–å¤§é‡çš„æ¼”ç¤ºæ¢ç´¢ï¼Œä¾¿å¯ä»¥æŒ‡å¯¼æ“çºµç­–ç•¥è¾¾åˆ°è¾ƒé«˜çš„æˆåŠŸç‡ã€‚æ›´å¤šè¯¦æƒ…å‚è§ç›¸å…³ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººå­¦ä¹ æŠ€æœ¯å¦‚è¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ ç­‰åœ¨ç‰¹å®šç¯å¢ƒä¸­åˆæˆæœºå™¨äººæŠ€èƒ½å·²æœ‰è‰¯å¥½è¡¨ç°ï¼Œä½†ä»éœ€ä»»åŠ¡ç‰¹å®šçš„æ¼”ç¤ºæˆ–å¤æ‚çš„ä»¿çœŸç¯å¢ƒè®¾è®¡ï¼Œé™åˆ¶äº†å…¶åœ¨æœªçŸ¥çœŸå®ç¯å¢ƒä¸­çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ç­–ç•¥çš„å‘å±•ã€‚</li>
<li>åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚LLMså’ŒVLMsï¼‰ä¸ºæœºå™¨äººè¿›è¡Œè¯­ä¹‰ç†è§£å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¦‚ä½•åˆ©ç”¨è¿™äº›çŸ¥è¯†ä½¿æœºå™¨äººç†è§£ä¸–ç•Œåº•å±‚åŠ¨æ€ã€åœ¨ä¸åŒä»»åŠ¡é—´é€šç”¨ç­–ç•¥ä»¥åŠå¦‚ä½•é€‚åº”æ–°ç¯å¢ƒä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ç”¨äºæœºå™¨äººè‡ªæˆ‘æŒ‡å¯¼å’Œè‡ªæˆ‘æ”¹è¿›ï¼ŒåŒ…æ‹¬å¤šç§è§’è‰²ä¸“ä¸šå¯¹è¯æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹åŸºç¡€æœºå™¨äººç­–ç•¥è¿›è¡Œè¿­ä»£è°ƒæ•´ä»¥ä½¿å…¶é€‚åº”ç¯å¢ƒå¹¶ä¼˜åŒ–å…¶è¡ŒåŠ¨åˆ†å¸ƒæ¥å®ç°å¼•å¯¼ç­–ç•¥ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šæœºå™¨äººçš„ç¡¬ä»¶å¹³å°é…ç½®ã€‚</li>
<li>é€šè¿‡ä»¿çœŸå’ŒçœŸå®å®éªŒéªŒè¯äº†æ­¤æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å®ƒèƒ½æœ‰æ•ˆåœ°æé«˜ç­–ç•¥çš„æˆåŠŸç‡ã€‚ä¸”è¿™ä¸€è¿‡ç¨‹æ— éœ€é¢å¤–çš„äººåŠ›æ¼”ç¤ºæˆ–å¤§è§„æ¨¡çš„å‹˜æ¢æ¢ç´¢è¿‡ç¨‹ã€‚ä»£ç å’Œè§†é¢‘ææ–™å·²åœ¨æŒ‡å®šç½‘ç«™å…¬å¼€ä¾›å¤§ä¼—æŸ¥çœ‹ä¸‹è½½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7c04dd542a3de9d72f64d198acfbf5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69f989cde08b3bc4359983cb7cd84530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee90c195ebdaaf85f1efab12fd48fe13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4770b180db56e11dd0838cdadb2e8ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1379277849d010159ea3c140923f9f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-527451acd9900a488a751ecb13eb5787.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StateAct-Enhancing-LLM-Base-Agents-via-Self-prompting-and-State-tracking"><a href="#StateAct-Enhancing-LLM-Base-Agents-via-Self-prompting-and-State-tracking" class="headerlink" title="StateAct: Enhancing LLM Base Agents via Self-prompting and   State-tracking"></a>StateAct: Enhancing LLM Base Agents via Self-prompting and   State-tracking</h2><p><strong>Authors:Nikolai Rozanov, Marek Rei</strong></p>
<p>Large language models (LLMs) are increasingly used as autonomous agents, tackling tasks from robotics to web navigation. Their performance depends on the underlying base agent. Existing methods, however, struggle with long-context reasoning and goal adherence. We introduce StateAct, a novel and efficient base agent that enhances decision-making through (1) self-prompting, which reinforces task goals at every step, and (2) chain-of-states, an extension of chain-of-thought that tracks state information over time. StateAct outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30% on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also demonstrate that StateAct can be used as a drop-in replacement for ReAct with advanced LLM agent methods such as test-time scaling, yielding an additional 12% gain on Textcraft. By improving efficiency and long-range reasoning without requiring additional training or retrieval, StateAct provides a scalable foundation for LLM agents. We open source our code to support further research at <a target="_blank" rel="noopener" href="https://github.com/ai-nikolai/stateact">https://github.com/ai-nikolai/stateact</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œè‡ªä¸»ä»£ç†ï¼Œå¤„ç†ä»æœºå™¨äººæŠ€æœ¯åˆ°ç½‘é¡µå¯¼èˆªçš„ä»»åŠ¡ã€‚å®ƒä»¬çš„æ€§èƒ½å–å†³äºåŸºç¡€ä¸»ä½“ä»£ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡æ¨ç†å’Œç›®æ ‡ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†StateActï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é«˜æ•ˆçš„ä¸»ä½“ä»£ç†ï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢å¢å¼ºå†³ç­–åˆ¶å®šï¼šï¼ˆ1ï¼‰è‡ªæˆ‘æç¤ºï¼Œæ¯ä¸€æ­¥éƒ½å¼ºåŒ–ä»»åŠ¡ç›®æ ‡ï¼›ï¼ˆ2ï¼‰çŠ¶æ€é“¾ï¼Œæ˜¯å¯¹æ€ç»´é“¾çš„æ‰©å±•ï¼Œéšæ—¶é—´è·Ÿè¸ªçŠ¶æ€ä¿¡æ¯ã€‚StateActåœ¨å¤šä¸ªå‰æ²¿LLMä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨Alfworldä¸Šçš„æ€§èƒ½è¶…è¿‡ä¹‹å‰çš„æœ€ä½³åŸºç¡€ä»£ç†ReActè¶…è¿‡10%ï¼Œåœ¨Textcraftä¸Šè¶…è¿‡30%ï¼Œåœ¨Webshopä¸Šè¶…è¿‡7%ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼ŒStateActå¯ä»¥ä½œä¸ºé«˜çº§LLMä»£ç†æ–¹æ³•ï¼ˆå¦‚æµ‹è¯•æ—¶ç¼©æ”¾ï¼‰çš„å³æ’å³ç”¨æ›¿ä»£å“ï¼Œåœ¨Textcraftä¸Šäº§ç”Ÿé¢å¤–çš„12%æ”¶ç›Šã€‚é€šè¿‡æé«˜æ•ˆç‡å’Œé•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æ£€ç´¢ï¼ŒStateActä¸ºLLMä»£ç†æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai-nikolai/stateact%E4%B8%8A%E5%85%AC%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/ai-nikolai/stateactä¸Šå…¬å¼€æºä»£ç ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02810v3">PDF</a> 9 pages, 5 pages appendix, 7 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªä¸»ä»£ç†ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨äººåˆ°ç½‘é¡µå¯¼èˆªç­‰ä»»åŠ¡ã€‚å…¶æ€§èƒ½å–å†³äºåŸºç¡€ä»£ç†çš„è¡¨ç°ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´é•¿è¯­å¢ƒæ¨ç†å’Œç›®æ ‡ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼•å…¥StateActï¼Œä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„åŸºç¡€ä»£ç†ï¼Œé€šè¿‡ï¼ˆ1ï¼‰è‡ªæˆ‘æç¤ºï¼Œæ¯ä¸€æ­¥éƒ½å¼ºåŒ–ä»»åŠ¡ç›®æ ‡ï¼›ï¼ˆ2ï¼‰çŠ¶æ€é“¾ï¼Œæ‰©å±•äº†æ€è€ƒé“¾ï¼Œè·Ÿè¸ªéšæ—¶é—´å˜åŒ–çš„çŠ¶æ€ä¿¡æ¯ï¼Œä»è€Œæé«˜å†³ç­–èƒ½åŠ›ã€‚StateActåœ¨Alfworldä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€ä½³åŸºç¡€ä»£ç†ReActè¶…è¿‡10%ï¼Œåœ¨Textcraftä¸Šè¶…è¿‡30%ï¼Œåœ¨Webshopä¸Šè¶…è¿‡7%ï¼Œä¸”å¯åº”ç”¨äºå¤šä¸ªå‰æ²¿LLMã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†StateActå¯ä»¥ç”¨ä½œæ›¿æ¢ReActçš„é«˜çº§LLMä»£ç†æ–¹æ³•ï¼Œå¦‚æµ‹è¯•æ—¶ç¼©æ”¾ï¼Œåœ¨Textcraftä¸Šäº§ç”Ÿé¢å¤–çš„12%æ”¶ç›Šã€‚é€šè¿‡æé«˜æ•ˆç‡å’Œé•¿ç¨‹æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æ£€ç´¢ï¼ŒStateActä¸ºLLMä»£ç†æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚æˆ‘ä»¬å…¬å¼€äº†ä»£ç ä»¥æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«å¹¿æ³›åº”ç”¨äºå¤šç§ä»»åŠ¡ï¼Œå…¶æ€§èƒ½å–å†³äºåŸºç¡€ä»£ç†çš„è¡¨ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨é•¿è¯­å¢ƒæ¨ç†å’Œç›®æ ‡ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>StateActæ˜¯ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„åŸºç¡€ä»£ç†ï¼Œé€šè¿‡è‡ªæˆ‘æç¤ºå’ŒçŠ¶æ€é“¾æŠ€æœ¯æé«˜å†³ç­–èƒ½åŠ›ã€‚</li>
<li>StateActåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€ä½³åŸºç¡€ä»£ç†ReActã€‚</li>
<li>StateActå¯ä¸é«˜çº§LLMä»£ç†æ–¹æ³•ç»“åˆä½¿ç”¨ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>StateActæé«˜äº†æ•ˆç‡å’Œé•¿ç¨‹æ¨ç†èƒ½åŠ›ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æ£€ç´¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80b505988311d9294bf1b3d88ed04b64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f70f26e2212cce1e45352cae92b1621.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dbe07be26022ddccfae816fda3e73cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76e97984f02085cfee7a2b1bdab15093.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MAPF-GPT-Imitation-Learning-for-Multi-Agent-Pathfinding-at-Scale"><a href="#MAPF-GPT-Imitation-Learning-for-Multi-Agent-Pathfinding-at-Scale" class="headerlink" title="MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale"></a>MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale</h2><p><strong>Authors:Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik</strong></p>
<p>Multi-agent pathfinding (MAPF) is a problem that generally requires finding collision-free paths for multiple agents in a shared environment. Solving MAPF optimally, even under restrictive assumptions, is NP-hard, yet efficient solutions for this problem are critical for numerous applications, such as automated warehouses and transportation systems. Recently, learning-based approaches to MAPF have gained attention, particularly those leveraging deep reinforcement learning. Typically, such learning-based MAPF solvers are augmented with additional components like single-agent planning or communication. Orthogonally, in this work we rely solely on imitation learning that leverages a large dataset of expert MAPF solutions and transformer-based neural network to create a foundation model for MAPF called MAPF-GPT. The latter is capable of generating actions without additional heuristics or communication. MAPF-GPT demonstrates zero-shot learning abilities when solving the MAPF problems that are not present in the training dataset. We show that MAPF-GPT notably outperforms the current best-performing learnable MAPF solvers on a diverse range of problem instances and is computationally efficient during inference. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ï¼ˆMAPFï¼‰æ˜¯ä¸€ä¸ªé€šå¸¸éœ€è¦åœ¨å…±äº«ç¯å¢ƒä¸­ä¸ºå¤šä¸ªæ™ºèƒ½ä½“æ‰¾åˆ°æ— ç¢°æ’è·¯å¾„çš„é—®é¢˜ã€‚è§£å†³MAPFé—®é¢˜ï¼Œå³ä½¿åœ¨é™åˆ¶æ€§å‡è®¾ä¸‹ï¼Œä¹Ÿæ˜¯NPéš¾çš„ã€‚ç„¶è€Œï¼Œå¯¹äºè®¸å¤šåº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨åŒ–ä»“åº“å’Œè¿è¾“ç³»ç»Ÿï¼‰æ¥è¯´ï¼Œé«˜æ•ˆè§£å†³æ­¤é—®é¢˜çš„è§£å†³æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼ŒåŸºäºå­¦ä¹ çš„MAPFæ–¹æ³•å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚é€šå¸¸ï¼Œè¿™æ ·çš„åŸºäºå­¦ä¹ çš„MAPFæ±‚è§£å™¨ä¼šè¾…ä»¥å•æ™ºèƒ½ä½“è§„åˆ’æˆ–é€šä¿¡ç­‰é™„åŠ ç»„ä»¶ã€‚ä¸æ­¤ä¸åŒï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»…ä¾é åˆ©ç”¨å¤§é‡ä¸“å®¶MAPFè§£å†³æ–¹æ¡ˆæ•°æ®é›†å’ŒåŸºäºå˜å‹å™¨çš„ç¥ç»ç½‘ç»œè¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼Œä»è€Œåˆ›å»ºäº†ä¸€ä¸ªç§°ä¸ºMAPF-GPTçš„MAPFåŸºç¡€æ¨¡å‹ã€‚åè€…èƒ½å¤Ÿåœ¨ä¸é™„åŠ å¯å‘å¼ä¿¡æ¯æˆ–é€šä¿¡çš„æƒ…å†µä¸‹ç”ŸæˆåŠ¨ä½œã€‚MAPF-GPTåœ¨è§£å†³è®­ç»ƒæ•°æ®é›†ä¸­ä¸å­˜åœ¨çš„MAPFé—®é¢˜æ—¶ï¼Œè¡¨ç°å‡ºäº†é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜äº†MAPF-GPTåœ¨å¤šç§é—®é¢˜å®ä¾‹ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå½“å‰æ€§èƒ½æœ€ä½³çš„å¯å­¦ä¹ MAPFæ±‚è§£å™¨ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¡ç®—æ•ˆç‡é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00134v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“è·¯å¾„æŸ¥æ‰¾ï¼ˆMAPFï¼‰é—®é¢˜è¦æ±‚ä¸ºå…±äº«ç¯å¢ƒä¸­çš„å¤šä¸ªæ™ºèƒ½ä½“æ‰¾åˆ°æ— ç¢°æ’çš„è·¯å¾„ã€‚å°½ç®¡åœ¨é™åˆ¶æ€§å‡è®¾ä¸‹è§£å†³MAPFé—®é¢˜æ˜¯NPéš¾é¢˜ï¼Œä½†å¯¹å…¶é«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„éœ€æ±‚å¯¹äºè‡ªåŠ¨åŒ–ä»“åº“å’Œè¿è¾“ç³»ç»Ÿç­‰ä¼—å¤šåº”ç”¨è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼ŒåŸºäºå­¦ä¹ çš„MAPFæ–¹æ³•å¼•èµ·äº†å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚é€šå¸¸ï¼Œè¿™äº›åŸºäºå­¦ä¹ çš„MAPFæ±‚è§£å™¨ä¼šè¾…ä»¥å•æ™ºèƒ½ä½“è§„åˆ’æˆ–é€šä¿¡ç­‰ç»„ä»¶ã€‚ä¸æ­¤ä¸åŒï¼Œæœ¬ç ”ç©¶ä»…ä¾èµ–æ¨¡ä»¿å­¦ä¹ ï¼Œåˆ©ç”¨å¤§é‡ä¸“å®¶MAPFè§£å†³æ–¹æ¡ˆæ•°æ®é›†å’ŒåŸºäºtransformerçš„ç¥ç»ç½‘ç»œï¼Œä¸ºMAPFåˆ›å»ºä¸€ä¸ªåŸºç¡€æ¨¡å‹â€”â€”MAPF-GPTã€‚åè€…èƒ½å¤Ÿåœ¨ä¸æ·»åŠ å¯å‘å¼æˆ–é€šä¿¡çš„æƒ…å†µä¸‹ç”ŸæˆåŠ¨ä½œã€‚MAPF-GPTè¡¨ç°å‡ºè§£å†³æœªåœ¨è®­ç»ƒæ•°æ®é›†ä¸­å‡ºç°çš„MAPFé—®é¢˜çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMAPF-GPTåœ¨å¤šç§é—®é¢˜å®ä¾‹ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€ä½³çš„å¯å­¦ä¹ MAPFæ±‚è§£å™¨ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¡ç®—æ•ˆç‡é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“è·¯å¾„æŸ¥æ‰¾ï¼ˆMAPFï¼‰æ˜¯å¯»æ‰¾å¤šä¸ªæ™ºèƒ½ä½“åœ¨å…±äº«ç¯å¢ƒä¸­çš„æ— ç¢°æ’è·¯å¾„çš„é—®é¢˜ã€‚</li>
<li>MAPFé—®é¢˜åœ¨é™åˆ¶æ€§å‡è®¾ä¸‹æ˜¯NPéš¾é¢˜ï¼Œä½†å¯¹é«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„éœ€æ±‚åœ¨è®¸å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºå­¦ä¹ çš„MAPFæ–¹æ³•å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>MAPF-GPTæ˜¯ä¸€ä¸ªåŸºäºæ¨¡ä»¿å­¦ä¹ å’Œtransformerç¥ç»ç½‘ç»œçš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿç”ŸæˆåŠ¨ä½œè€Œæ— éœ€é¢å¤–çš„å¯å‘å¼æˆ–é€šä¿¡ã€‚</li>
<li>MAPF-GPTè¡¨ç°å‡ºè§£å†³æœªåœ¨è®­ç»ƒæ•°æ®é›†ä¸­å‡ºç°çš„MAPFé—®é¢˜çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>MAPF-GPTåœ¨å¤šç§é—®é¢˜å®ä¾‹ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æœ€ä½³çš„å¯å­¦ä¹ MAPFæ±‚è§£å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-91ac004874a7a065a62224e4d4e61d30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5d16ce540effcb7de6b02e3a3c08636.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb28c1a23b65b7b68fce635bc94e7389.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf62d7f42ddb39809f1aadf3ae813a68.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DrugAgent-Multi-Agent-Large-Language-Model-Based-Reasoning-for-Drug-Target-Interaction-Prediction"><a href="#DrugAgent-Multi-Agent-Large-Language-Model-Based-Reasoning-for-Drug-Target-Interaction-Prediction" class="headerlink" title="DrugAgent: Multi-Agent Large Language Model-Based Reasoning for   Drug-Target Interaction Prediction"></a>DrugAgent: Multi-Agent Large Language Model-Based Reasoning for   Drug-Target Interaction Prediction</h2><p><strong>Authors:Yoshitaka Inoue, Tianci Song, Xinling Wang, Augustin Luna, Tianfan Fu</strong></p>
<p>Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces. Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. While drug-target interaction (DTI) prediction is important for drug discovery, existing approaches face challenges due to complex biological systems and the lack of interpretability needed for clinical applications. DrugAgent is a multi-agent LLM system for DTI prediction that combines multiple specialized perspectives with transparent reasoning. Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning. We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we demonstrated the contributions of each agent, with the AI agent being the most impactful, followed by the KG agent and search agent. Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance. Code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DrugAgent-B2EA">https://anonymous.4open.science/r/DrugAgent-B2EA</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œå®ƒä»¬èƒ½å¤Ÿé€šè¿‡ç±»ä¼¼äººç±»çš„æ¥å£è§£å†³å„ç§å„æ ·çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå…¶è®­ç»ƒçš„å±€é™æ€§ä½¿å¾—å®ƒä»¬æ— æ³•åœ¨å¤šè§†è§’å—ç›Šçš„åœºæ™¯ä¸­å‡†ç¡®å›ç­”é—®é¢˜ã€‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå…è®¸å¯¹é—®é¢˜ç­”æ¡ˆè¿›è¡Œä¼˜åŒ–ä»¥å¢å¼ºç»“æœçš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚è™½ç„¶è¯ç‰©é¶ç‚¹ç›¸äº’ä½œç”¨ï¼ˆDTIï¼‰é¢„æµ‹å¯¹äºè¯ç‰©å‘ç°å¾ˆé‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•ç”±äºå¤æ‚çš„ç”Ÿç‰©ç³»ç»Ÿå’Œç¼ºä¹ä¸´åºŠåº”ç”¨ä¸­æ‰€éœ€çš„è§£é‡Šæ€§è€Œé¢ä¸´æŒ‘æˆ˜ã€‚DrugAgentæ˜¯ä¸€ä¸ªç”¨äºDTIé¢„æµ‹çš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œå®ƒç»“åˆäº†å¤šä¸ªä¸“ä¸šè§†è§’å’Œé€æ˜çš„æ¨ç†ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡ä»¥ä¸‹æ–¹å¼é€‚åº”å¹¶æ‰©å±•äº†ç°æœ‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼šï¼ˆ1ï¼‰å°†åŸºäºåè°ƒå™¨çš„æ¶æ„åº”ç”¨äºDTIé¢†åŸŸï¼›ï¼ˆ2ï¼‰æ•´åˆç‰¹å®šé¢†åŸŸçš„æ•°æ®æºï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ é¢„æµ‹ã€çŸ¥è¯†å›¾è°±å’Œæ–‡çŒ®è¯æ®ï¼›ï¼ˆ3ï¼‰èå…¥Chain-of-Thoughtï¼ˆCoTï¼‰å’ŒReActï¼ˆReason+Actï¼‰æ¡†æ¶ä»¥å®ç°é€æ˜çš„DTIæ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨æ¿€é…¶æŠ‘åˆ¶å‰‚æ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬çš„å¤šæ™ºèƒ½ä½“LLMæ–¹æ³•åœ¨éæ¨ç†å¤šæ™ºèƒ½ä½“æ¨¡å‹ï¼ˆGPT-4o miniï¼‰çš„F1åˆ†æ•°ä¸Šæé«˜äº†45%ï¼ˆ0.514å¯¹æ¯”0.355ï¼‰ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æ¯ä¸ªæ™ºèƒ½ä½“çš„è´¡çŒ®ï¼Œå…¶ä¸­AIæ™ºèƒ½ä½“å½±å“æœ€å¤§ï¼Œå…¶æ¬¡æ˜¯KGæ™ºèƒ½ä½“å’Œæœç´¢æ™ºèƒ½ä½“ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ¥è‡ªå¤šä¸ªæ¥æºçš„è¯æ®ï¼Œä¸ºæ¯æ¬¡é¢„æµ‹æä¾›äº†è¯¦ç»†ã€äººç±»å¯è§£é‡Šçš„ç†ç”±â€”â€”è¿™å¯¹äºç”Ÿç‰©åŒ»å­¦åº”ç”¨è‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨é¢„æµ‹èƒŒåçš„ç†ç”±æ˜¯ä¸´åºŠå†³ç­–å’Œæ³•è§„åˆè§„çš„å…³é”®å› ç´ ã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DrugAgent-B2EA">https://anonymous.4open.science/r/DrugAgent-B2EA</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13378v4">PDF</a> 15 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡äººç±»æ¥å£å¤„ç†å¤šæ ·åŒ–çš„é—®é¢˜ï¼Œä½†å…¶è®­ç»ƒé™åˆ¶å¯¼è‡´åœ¨æŸäº›åœºæ™¯ä¸­ç¼ºä¹å‡†ç¡®æ€§ã€‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¢å¼ºç»“æœçš„ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚é’ˆå¯¹è¯ç‰©å‘ç°ä¸­çš„è¯ç‰©é¶ç‚¹ç›¸äº’ä½œç”¨ï¼ˆDTIï¼‰é¢„æµ‹ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚DrugAgentæ˜¯ä¸€ä¸ªç”¨äºDTIé¢„æµ‹çš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œå®ƒç»“åˆäº†å¤šä¸ªä¸“ä¸šè§†è§’å’Œé€æ˜æ¨ç†ã€‚è¯¥ç³»ç»Ÿé€‚åº”å¹¶æ‰©å±•äº†ç°æœ‰å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒåŒ…æ‹¬åŸºäºåè°ƒæ¶æ„çš„DTIé¢†åŸŸåº”ç”¨ã€é›†æˆç‰¹å®šé¢†åŸŸæ•°æ®æºå’Œèå…¥Chain-of-ThoughtåŠReActæ¡†æ¶è¿›è¡Œé€æ˜DTIæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œä¸GPT-4o miniç›¸æ¯”ï¼ŒDrugAgentåœ¨æ¿€é…¶æŠ‘åˆ¶å‰‚æ•°æ®é›†ä¸Šçš„F1å¾—åˆ†é«˜å‡º45%ï¼ˆä»åŸæ¥çš„ä½å€¼æå‡ä¸ºä¸­é—´åé«˜çš„ä¼˜ç§€æˆç»©ï¼‰ã€‚æ¯ä¸ªæ™ºèƒ½ä½“çš„è´¡çŒ®éƒ½è¢«æ­ç¤ºå‡ºæ¥ï¼Œå…¶ä¸­AIæ™ºèƒ½ä½“çš„å½±å“æœ€å¤§ï¼Œå…¶æ¬¡æ˜¯çŸ¥è¯†å›¾è°±æ™ºèƒ½ä½“å’Œæœç´¢æ™ºèƒ½ä½“ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œé€šè¿‡ç»“åˆå¤šç§æ¥æºçš„è¯æ®ï¼Œè¯¥æ–¹æ³•ä¸ºæ¯ä¸ªé¢„æµ‹æä¾›äº†è¯¦ç»†çš„å¯è§£é‡Šçš„æ¨ç†ï¼Œè¿™å¯¹ç†è§£é¢„æµ‹èƒŒåçš„åŸå› å’Œç›‘ç®¡æœºæ„å®¡æ ¸æ¥è¯´æä¸ºå…³é”®ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§[ç½‘ç«™é“¾æ¥]ã€‚å…·ä½“è¯¦æƒ…è¯·é˜…è¯»ä¸‹æ–‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡äººç±»æ¥å£å›ç­”å¤šæ ·åŒ–é—®é¢˜ï¼Œä½†åœ¨ç‰¹å®šåœºæ™¯ä¸­å­˜åœ¨å±€é™æ€§ã€‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¯æé«˜ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>DrugAgentæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œç”¨äºè¯ç‰©é¶ç‚¹ç›¸äº’ä½œç”¨ï¼ˆDTIï¼‰é¢„æµ‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„æŒ‘æˆ˜ã€‚</li>
<li>DrugAgentç»“åˆäº†åŸºäºåè°ƒæ¶æ„çš„DTIé¢†åŸŸåº”ç”¨ã€ç‰¹å®šé¢†åŸŸæ•°æ®æºå’Œé€æ˜æ¨ç†æ¡†æ¶ã€‚å®éªŒè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f4571053655e266e436f5ae7f7be693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de48f29afb881d5575f489536eb16c49.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="POGEMA-A-Benchmark-Platform-for-Cooperative-Multi-Agent-Pathfinding"><a href="#POGEMA-A-Benchmark-Platform-for-Cooperative-Multi-Agent-Pathfinding" class="headerlink" title="POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding"></a>POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding</h2><p><strong>Authors:Alexey Skrynnik, Anton Andreychuk, Anatolii Borzilov, Alexander Chernyavskiy, Konstantin Yakovlev, Aleksandr Panov</strong></p>
<p>Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments, typically involving a small number of agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot pathfinding, which have traditionally been approached with classical non-learnable methods (e.g., heuristic search), are now being suggested for solution using learning-based or hybrid methods. However, in this domain, it remains difficult, if not impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To address this, we introduce POGEMA, a comprehensive set of tools that includes a fast environment for learning, a problem instance generator, a collection of predefined problem instances, a visualization toolkit, and a benchmarking tool for automated evaluation. We also introduce and define an evaluation protocol that specifies a range of domain-related metrics, computed based on primary evaluation indicators (such as success rate and path length), enabling a fair multi-fold comparison. The results of this comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æœ€è¿‘åœ¨è§£å†³å„ç§ç¯å¢ƒä¸­çš„æŒ‘æˆ˜æ€§åˆä½œå’Œç«äº‰å¤šæ™ºèƒ½ä½“é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œé€šå¸¸æ¶‰åŠå°‘é‡æ™ºèƒ½ä½“å’Œå®Œå…¨å¯è§‚å¯Ÿæ€§ã€‚æ­¤å¤–ï¼Œä¸€ç³»åˆ—å…³é”®çš„æœºå™¨äººç›¸å…³ä»»åŠ¡ï¼Œå¦‚å¤šæœºå™¨äººè·¯å¾„è§„åˆ’ï¼Œä¼ ç»Ÿä¸Šé‡‡ç”¨ç»å…¸çš„éå­¦ä¹ æ–¹æ³•ï¼ˆä¾‹å¦‚å¯å‘å¼æœç´¢ï¼‰æ¥è§£å†³ï¼Œç°åœ¨å»ºè®®ä½¿ç”¨åŸºäºå­¦ä¹ æˆ–æ··åˆæ–¹æ³•æ¥è§£å†³ã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸ªé¢†åŸŸï¼Œç”±äºç¼ºä¹ä¸€ä¸ªæ—¢æ”¯æŒå­¦ä¹ å’Œè¯„ä¼°çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¿›è¡Œç»å…¸æ–¹æ³•ã€åŸºäºå­¦ä¹ çš„æ–¹æ³•å’Œæ··åˆæ–¹æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒä»ç„¶å›°éš¾ç”šè‡³ä¸å¯èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†POGEMAï¼Œè¿™æ˜¯ä¸€å¥—ç»¼åˆå·¥å…·ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¿«é€Ÿå­¦ä¹ ç¯å¢ƒã€é—®é¢˜å®ä¾‹ç”Ÿæˆå™¨ã€é¢„å®šä¹‰é—®é¢˜å®ä¾‹é›†åˆã€å¯è§†åŒ–å·¥å…·åŒ…å’Œç”¨äºè‡ªåŠ¨åŒ–è¯„ä¼°çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚æˆ‘ä»¬è¿˜ä»‹ç»å¹¶å®šä¹‰äº†ä¸€ä¸ªè¯„ä¼°åè®®ï¼Œè¯¥åè®®è§„å®šäº†åŸºäºä¸»è¦è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚æˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦ï¼‰çš„ä¸€ç³»åˆ—ä¸é¢†åŸŸç›¸å…³çš„åº¦é‡æ ‡å‡†ï¼Œä»¥å®ç°å…¬å¹³çš„å¤šæ–¹é¢æ¯”è¾ƒã€‚æœ¬æ¬¡æ¯”è¾ƒæ¶‰åŠå¤šç§å…ˆè¿›çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€åŸºäºæœç´¢å’Œæ··åˆæ–¹æ³•ï¼Œå¹¶å‘ˆç°äº†æ¯”è¾ƒç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.14931v3">PDF</a> Published as a conference paper at The International Conference on   Learning Representations 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³å„ç§ç¯å¢ƒä¸­çš„åˆä½œä¸ç«äº‰å¤šæ™ºèƒ½ä½“é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ¶‰åŠå°‘é‡æ™ºèƒ½ä½“å’Œå®Œå…¨å¯è§‚å¯Ÿæ€§çš„æƒ…å†µä¸‹ã€‚ä¼ ç»Ÿçš„æœºå™¨äººç›¸å…³ä»»åŠ¡å¦‚å¤šæœºå™¨äººè·¯å¾„è§„åˆ’å¤šé‡‡ç”¨ç»å…¸çš„éå­¦ä¹ æ–¹æ³•ï¼ˆå¦‚å¯å‘å¼æœç´¢ï¼‰ï¼Œç°åœ¨å»ºè®®ä½¿ç”¨åŸºäºå­¦ä¹ æˆ–æ··åˆæ–¹æ³•æ¥è§£å†³ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ”¯æŒå­¦ä¹ å’Œè¯„ä¼°çš„ç»Ÿä¸€æ¡†æ¶ï¼Œéš¾ä»¥åœ¨ç»å…¸æ–¹æ³•ã€åŸºäºå­¦ä¹ çš„æ–¹æ³•å’Œæ··åˆæ–¹æ³•ä¹‹é—´è¿›è¡Œå…¬å¹³æ¯”è¾ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†POGEMAå·¥å…·é›†ï¼ŒåŒ…æ‹¬å­¦ä¹ å¿«é€Ÿç¯å¢ƒã€é—®é¢˜å®ä¾‹ç”Ÿæˆå™¨ã€é¢„è®¾é—®é¢˜å®ä¾‹é›†åˆã€å¯è§†åŒ–å·¥å…·å’Œè‡ªåŠ¨åŒ–è¯„ä¼°çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚æˆ‘ä»¬è¿˜å¼•å…¥å¹¶å®šä¹‰äº†ä¸€ä¸ªè¯„ä¼°åè®®ï¼Œè¯¥åè®®åŸºäºä¸»è¦è¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚æˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦ï¼‰è®¡ç®—ä¸€ç³»åˆ—ä¸é¢†åŸŸç›¸å…³çš„æŒ‡æ ‡ï¼Œä»¥å®ç°å…¬å¹³çš„å¤šé‡æ¯”è¾ƒã€‚ç»™å‡ºäº†æ¶‰åŠå¤šç§å…ˆè¿›MARLã€åŸºäºæœç´¢å’Œæ··åˆæ–¹æ³•çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³åˆä½œä¸ç«äº‰å¤šæ™ºèƒ½ä½“é—®é¢˜ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨äººä»»åŠ¡å¦‚å¤šæœºå™¨äººè·¯å¾„è§„åˆ’å¤šé‡‡ç”¨ç»å…¸éå­¦ä¹ æ–¹æ³•ã€‚</li>
<li>åŸºäºå­¦ä¹ å’Œæ··åˆæ–¹æ³•æ­£åœ¨è¢«æè®®ç”¨äºè§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ç¼ºä¹ç»Ÿä¸€æ¡†æ¶è¿›è¡Œå…¬å¹³æ¯”è¾ƒå„ç§æ–¹æ³•ï¼ˆç»å…¸ã€åŸºäºå­¦ä¹ ã€æ··åˆï¼‰ã€‚</li>
<li>å¼•å…¥POGEMAå·¥å…·é›†ï¼ŒåŒ…æ‹¬å­¦ä¹ å¿«é€Ÿç¯å¢ƒã€é—®é¢˜ç”Ÿæˆç­‰ï¼Œä»¥æ”¯æŒè¯„ä¼°ã€‚</li>
<li>å®šä¹‰è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬ä¸»è¦è¯„ä»·æŒ‡æ ‡ï¼ˆæˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦ç­‰ï¼‰ï¼Œä»¥å®ç°å…¬å¹³æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.14931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49f53f7f2295866cb996ff63b494c2ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f1ed62ad0f994971cfadb8ff37e3fa6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GenoTEX-An-LLM-Agent-Benchmark-for-Automated-Gene-Expression-Data-Analysis"><a href="#GenoTEX-An-LLM-Agent-Benchmark-for-Automated-Gene-Expression-Data-Analysis" class="headerlink" title="GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data   Analysis"></a>GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data   Analysis</h2><p><strong>Authors:Haoyang Liu, Shuyu Chen, Ye Zhang, Haohan Wang</strong></p>
<p>Recent advancements in machine learning have significantly improved the identification of disease-associated genes from gene expression datasets. However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To support the evaluation and development of such methods, we introduce GenoTEX, a benchmark dataset for the automated analysis of gene expression data. GenoTEX provides analysis code and results for solving a wide range of gene-trait association problems, encompassing dataset selection, preprocessing, and statistical analysis, in a pipeline that follows computational genomics standards. The benchmark includes expert-curated annotations from bioinformaticians to ensure accuracy and reliability. To provide baselines for these tasks, we present GenoAgent, a team of LLM-based agents that adopt a multi-step programming workflow with flexible self-correction, to collaboratively analyze gene expression datasets. Our experiments demonstrate the potential of LLM-based methods in analyzing genomic data, while error analysis highlights the challenges and areas for future improvement. We propose GenoTEX as a promising resource for benchmarking and enhancing automated methods for gene expression data analysis. The benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/Liu-Hy/GenoTEX">https://github.com/Liu-Hy/GenoTEX</a>. </p>
<blockquote>
<p>è¿‘æœŸæœºå™¨å­¦ä¹ çš„å‘å±•æå¤§åœ°æé«˜äº†ä»åŸºå› è¡¨è¾¾æ•°æ®é›†ä¸­è¯†åˆ«ä¸ç–¾ç—…ç›¸å…³åŸºå› çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›è¿‡ç¨‹é€šå¸¸éœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨æ“ä½œï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†ç”±äºå®ƒä»¬æ—¥ç›Šå¢å¼ºçš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œåœ¨è‡ªåŠ¨åŒ–è¿™äº›ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ä¸ºäº†æ”¯æŒæ­¤ç±»æ–¹æ³•çš„è¯„ä¼°å’Œå‘å±•ï¼Œæˆ‘ä»¬å¼•å…¥äº†GenoTEXï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŸºå› è¡¨è¾¾æ•°æ®è‡ªåŠ¨åŒ–åˆ†æçš„åŸºå‡†æ•°æ®é›†ã€‚GenoTEXæä¾›åˆ†æä»£ç å’Œç»“æœï¼Œä»¥è§£å†³å¹¿æ³›çš„åŸºå› ç‰¹å¾å…³è”é—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€é¢„å¤„ç†å’Œç»Ÿè®¡åˆ†æï¼Œéµå¾ªè®¡ç®—åŸºå› ç»„å­¦æ ‡å‡†çš„ç®¡é“ã€‚åŸºå‡†æµ‹è¯•åŒ…æ‹¬ç”Ÿç‰©ä¿¡æ¯å­¦å®¶çš„ä¸“å®¶ç²¾é€‰æ³¨é‡Šï¼Œä»¥ç¡®ä¿å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†ä¸ºè¿™äº›ä»»åŠ¡æä¾›åŸºå‡†çº¿ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GenoAgentï¼Œè¿™æ˜¯ä¸€ç»„åŸºäºLLMçš„ä»£ç†ï¼Œé‡‡ç”¨å…·æœ‰çµæ´»è‡ªæˆ‘æ ¡æ­£çš„å¤šæ­¥ç¼–ç¨‹å·¥ä½œæµç¨‹ï¼Œä»¥ååŒåˆ†æåŸºå› è¡¨è¾¾æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†åŸºäºLLMçš„æ–¹æ³•åœ¨åˆ†æåŸºå› ç»„æ•°æ®æ–¹é¢çš„æ½œåŠ›ï¼Œè€Œè¯¯å·®åˆ†æåˆ™çªå‡ºäº†æœªæ¥çš„æŒ‘æˆ˜å’Œæ”¹è¿›é¢†åŸŸã€‚æˆ‘ä»¬æè®®GenoTEXä½œä¸ºä¸€ä¸ªæœ‰å¸Œæœ›çš„èµ„æºï¼Œç”¨äºè¯„ä¼°å’Œå¢å¼ºåŸºå› è¡¨è¾¾æ•°æ®åˆ†æçš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚è¯¥åŸºå‡†æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Liu-Hy/GenoTEX%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Liu-Hy/GenoTEXä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15341v3">PDF</a> 31 pages, 4 figures</p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†ä»åŸºå› è¡¨è¾¾æ•°æ®é›†ä¸­è¯†åˆ«ä¸ç–¾ç—…ç›¸å…³åŸºå› çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›è¿‡ç¨‹é€šå¸¸éœ€è¦å¤§é‡çš„ä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨æ“ä½œï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æ˜¾ç¤ºå‡ºè‡ªåŠ¨åŒ–è¿™äº›ä»»åŠ¡çš„æ½œåŠ›ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰æ—¥ç›Šå¢å¼ºçš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒå’Œå¼€å‘è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GenoTEXï¼Œä¸€ä¸ªç”¨äºåŸºå› è¡¨è¾¾æ•°æ®è‡ªåŠ¨åŒ–åˆ†æçš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚GenoTEXæä¾›äº†ä¸€ä¸ªåˆ†æä»£ç å’Œç»“æœï¼Œç”¨äºè§£å†³å¹¿æ³›çš„åŸºå› ç‰¹å¾å…³è”é—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€é¢„å¤„ç†å’Œç»Ÿè®¡åˆ†æï¼Œéµå¾ªè®¡ç®—åŸºå› ç»„å­¦æ ‡å‡†çš„ç®¡é“ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬ç”Ÿç‰©ä¿¡æ¯å­¦å®¶çš„ä¸“å®¶æ³¨é‡Šï¼Œä»¥ç¡®ä¿å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†ä¸ºè¿™äº›ä»»åŠ¡æä¾›åŸºå‡†ï¼Œæˆ‘ä»¬æå‡ºäº†GenoAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„ä»£ç†å›¢é˜Ÿï¼Œé‡‡ç”¨å¤šæ­¥ç¼–ç¨‹å·¥ä½œæµç¨‹å’Œçµæ´»çš„è‡ªæ ¡æ­£åŠŸèƒ½ï¼Œä»¥ååŒåˆ†æåŸºå› è¡¨è¾¾æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†åŸºäºLLMçš„æ–¹æ³•åœ¨åˆ†æåŸºå› ç»„æ•°æ®ä¸­çš„æ½œåŠ›ï¼Œè€Œè¯¯å·®åˆ†æåˆ™çªå‡ºäº†æŒ‘æˆ˜å’Œæœªæ¥æ”¹è¿›çš„é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºGenoTEXæ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„èµ„æºï¼Œç”¨äºè¯„ä¼°å’Œå¢å¼ºåŸºå› è¡¨è¾¾æ•°æ®åˆ†æçš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•æé«˜äº†ä»åŸºå› è¡¨è¾¾æ•°æ®é›†ä¸­è¯†åˆ«ç–¾ç—…ç›¸å…³åŸºå› çš„èƒ½åŠ›ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†åœ¨è‡ªåŠ¨åŒ–åŸºå› è¡¨è¾¾æ•°æ®åˆ†æä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>GenoTEXæ˜¯ä¸€ä¸ªç”¨äºåŸºå› è¡¨è¾¾æ•°æ®è‡ªåŠ¨åŒ–åˆ†æçš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«åˆ†æä»£ç å’Œä¸“å®¶æ³¨é‡Šã€‚</li>
<li>GenoAgentæ˜¯åŸºäºLLMçš„ä»£ç†å›¢é˜Ÿï¼Œé‡‡ç”¨å¤šæ­¥ç¼–ç¨‹å·¥ä½œæµç¨‹å’Œè‡ªæ ¡æ­£åŠŸèƒ½æ¥ååŒåˆ†æåŸºå› è¡¨è¾¾æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¯æ˜äº†åŸºäºLLMçš„æ–¹æ³•åœ¨åˆ†æåŸºå› ç»„æ•°æ®ä¸­çš„æ½œåŠ›ã€‚</li>
<li>è¯¯å·®åˆ†æè¡¨æ˜éœ€è¦æ”¹è¿›çš„é¢†åŸŸå’Œæœªæ¥æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46e64007628a054b474c51c41a0c8ec3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f1385b512f9a2224087860bebb7e837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d06f865b2954fc437288343bf60ae3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bbba5c80c2742397bd5a6dbd1869521.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-10/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-54d86a5a3dee98004f4c15ff24c6626e.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Memory-Modular Classification Learning to Generalize with Memory   Replacement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0e207f82e9069e79daf28733a42be928.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-10  Hogwild! Inference Parallel LLM Generation via Concurrent Attention
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
