<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-08-15  RAGulating Compliance A Multi-Agent Knowledge Graph for Regulatory QA">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-84bd85b3a4fa71e30770edb169a9c624.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    59 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-15-更新"><a href="#2025-08-15-更新" class="headerlink" title="2025-08-15 更新"></a>2025-08-15 更新</h1><h2 id="RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA"><a href="#RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA" class="headerlink" title="RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA"></a>RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA</h2><p><strong>Authors:Bhavik Agarwal, Hemant Sunil Jomraj, Simone Kaplunov, Jack Krolick, Viktoria Rojkova</strong></p>
<p>Regulatory compliance question answering (QA) requires precise, verifiable information, and domain-specific expertise, posing challenges for Large Language Models (LLMs). In this work, we present a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with Retrieval-Augmented Generation (RAG) to address these demands. First, agents build and maintain an ontology-free KG by extracting subject–predicate–object (SPO) triplets from regulatory documents and systematically cleaning, normalizing, deduplicating, and updating them. Second, these triplets are embedded and stored along with their corresponding textual sections and metadata in a single enriched vector database, allowing for both graph-based reasoning and efficient information retrieval. Third, an orchestrated agent pipeline leverages triplet-level retrieval for question answering, ensuring high semantic alignment between user queries and the factual “who-did-what-to-whom” core captured by the graph. Our hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader audit-focused applications. </p>
<blockquote>
<p>监管合规问答（QA）需要精确、可验证的信息和特定领域的专业知识，这对大型语言模型（LLM）提出了挑战。在这项工作中，我们提出了一种新型的多代理框架，该框架将监管三元组的知识图谱与检索增强生成（RAG）相结合，以满足这些需求。首先，代理通过从监管文档中提取主体-谓语-对象（SPO）三元组来构建和维护无本体论知识图谱，并系统地对其进行清理、归一化、去重和更新。其次，这些三元组嵌入并与其相应的文本段落和元数据一起存储在单个丰富的向量数据库中，允许基于图的推理和高效的信息检索。第三，协同的代理管道利用三元组级别的检索来进行问答，确保用户查询与图中捕获的“谁做了哪些行为对谁”的核心内容之间存在高度语义对齐。我们的混合系统在复杂的监管查询中优于传统方法，通过嵌入的三元组确保事实正确性，通过统一的向量数据库实现可追溯性，并通过子图可视化增强理解，为合规驱动和更广泛的审计重点应用提供了稳健的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09893v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究提出了一种新型多智能体框架，通过整合监管知识图谱和检索增强生成技术来解决监管合规问答的挑战。智能体构建并维护一个本体自由的KG，通过从监管文件中提取SPO三元组并进行清洗、归一化、去重和更新。这些三元组嵌入并存储在丰富的向量数据库中，支持基于图的推理和高效的信息检索。通过利用三元组级别的检索进行问答，确保用户查询与图形捕获的“谁对谁做了何事”核心之间的高语义一致性。该混合系统在复杂监管查询方面的表现优于传统方法，确保事实正确性、通过统一的向量数据库进行可追溯性，并通过子图可视化增强理解，为合规驱动和更广泛的审计重点应用提供了稳健的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究提出了一个新型的多智能体框架来解决监管合规问答的挑战。</li>
<li>智能体通过构建并维护一个本体自由的KG来提取SPO三元组信息。</li>
<li>KG中的三元组被嵌入并存储在丰富的向量数据库中，支持基于图的推理和高效的信息检索。</li>
<li>该框架利用三元组级别的检索进行问答，提高了用户查询与图形核心之间的语义一致性。</li>
<li>混合系统在复杂监管查询方面的表现优于传统方法，确保了事实的正确性和可追溯性。</li>
<li>子图可视化增强了系统对监管信息的理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09893">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-568dfe93864ae0b9949c976db49d73ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee8f4947836bfc7c1842e4d8c98b21c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4e2cb7f0ea4b8fa4560d4eb0f7b235a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0604d1794ea3edc8354f76c872b60e7e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving"><a href="#AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving" class="headerlink" title="AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust   GAIA Problem Solving"></a>AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust   GAIA Problem Solving</h2><p><strong>Authors:Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, Jinjie Gu</strong></p>
<p>The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, as agents increasingly depend on multiple tools, they encounter new challenges: extended contexts from disparate sources and noisy or irrelevant tool outputs can undermine system reliability and accuracy. These challenges underscore the necessity for enhanced stability in agent-based systems. To address this, we introduce dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. In our approach, the Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process, effectively reducing errors arising from noise and bolstering problem-solving robustness. Extensive experiments on the GAIA test dataset reveal that our dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. As a result, our dynamic MAS system achieved first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight the practical value of collaborative agent roles in developing more reliable and trustworthy intelligent systems. </p>
<blockquote>
<p>随着大型语言模型（LLM）的快速发展，智能代理能够利用多种外部工具解决复杂的现实世界问题。然而，随着代理越来越依赖于多种工具，它们面临着新的挑战：来自不同来源的扩展上下文和嘈杂或无关的工具输出可能会破坏系统可靠性和准确性。这些挑战强调了基于代理的系统中增强稳定性的必要性。为了解决这个问题，我们引入了动态监督和操作机制，在AWorld框架内构建了一个稳健且动态的多代理系统（MAS）架构。在我们的方法中，执行代理会在关键步骤调用守卫代理来验证和纠正推理过程，有效地减少由噪声引起的错误并增强解决问题的稳健性。在GAIA测试数据集上的广泛实验表明，我们的动态操作机制显著提高了解决方案的有效性和稳定性，优于单代理系统（SAS）和标准工具增强系统。因此，我们的动态MAS系统在著名的GAIA排行榜上获得了开源项目第一名。这些发现凸显了协作代理角色在开发更可靠、更值得信赖的智能系统中的实际价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09889v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的快速发展使得智能代理能够利用多种外部工具解决复杂的现实世界问题。然而，随着代理越来越依赖于多种工具，他们面临新的挑战：来自不同来源的扩展上下文和嘈杂或无关的工具输出可能会破坏系统可靠性和准确性。为解决这些问题，我们引入了动态监督和操作机制，在AWorld框架内构建了一个稳健且动态的多代理系统（MAS）架构。执行代理在关键步骤中调用守卫代理进行验证和校正推理过程，有效降低噪声引起的错误并提升问题解决的稳健性。在GAIA测试数据集上的大量实验表明，我们的动态操作机制显著提高了解方案的有效性和稳定性，优于单代理系统（SAS）和标准工具增强系统。因此，我们的动态MAS系统在著名的GAIA排行榜上获得开源项目第一名。这些发现凸显了协作代理角色在开发更可靠、更值得信赖的智能系统中的实际价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的快速发展促进了智能代理利用外部工具解决复杂问题的能力。</li>
<li>依赖多个工具带来挑战，如上下文分散、工具输出嘈杂或无关。</li>
<li>需要增强代理系统的稳定性。</li>
<li>引入动态监督和操作机制，构建稳健的多代理系统（MAS）架构。</li>
<li>执行代理调用守卫代理进行验证和校正，降低噪声错误，提高问题解决的稳健性。</li>
<li>在GAIA测试数据集上的实验表明，动态操作机制提高解方案的有效性和稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09889">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-80c73558791f10bca819428af1a25ef0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deddb3ea5996422094aeaf80bb4e909c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4971518c8d91fbacbc933a3bb909cef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdacbd56ed7c3536c5c998d69ed7b66e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4afcabdb16ee890dd11da85902e3501b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics"><a href="#HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics" class="headerlink" title="HumanGenesis: Agent-Based Geometric and Generative Modeling for   Synthetic Human Dynamics"></a>HumanGenesis: Agent-Based Geometric and Generative Modeling for   Synthetic Human Dynamics</h2><p><strong>Authors:Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</strong></p>
<p>\textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \emph{geometric inconsistency} and \emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \emph{motion generalization limitations} and \emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration. </p>
<blockquote>
<p><strong>合成人类动力学</strong>旨在生成表达情感、受意图驱动动作的人的真实视频。然而，当前的方法面临两大挑战：(1)由于有限的3D建模和细节保留导致的<strong>几何不一致性</strong>和<strong>粗糙重建</strong>；(2)由于生成能力较弱导致的<strong>运动泛化限制</strong>和<strong>场景不和谐</strong>。为了解决这些问题，我们提出了<strong>HumanGenesis</strong>，这是一个通过四个协作代理整合几何和生成建模的框架：(1)<strong>重建器</strong>使用单目视频通过三维高斯摊铺和变形分解构建一致的三维人体场景表示；(2)<strong>批评代理</strong>通过基于多轮MLLM的反射来识别和细化不良区域，提高重建的保真度；(3)<strong>姿态引导者</strong>通过使用时间感知参数编码器生成表达性姿态序列，实现运动泛化；(4)<strong>视频协调器</strong>通过一个混合渲染管道与扩散合成现实且连贯的视频，并通过反馈循环改进重建器。HumanGenesis在文本引导合成、视频重放和新颖姿态泛化等任务上实现了最先进的性能，在表达性、几何保真和场景集成方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09858v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了合成人类动力学（Synthetic human dynamics）的目标和挑战，并阐述了HumanGenesis框架通过整合几何和生成建模来解决这些问题的方法。该框架包括四个协作代理：Reconstructor、Critique Agent、Pose Guider和Video Harmonizer。HumanGenesis在文本引导合成、视频重演绎和新颖姿态推广等任务上取得了最先进的性能，显著提高了表现力、几何保真度和场景集成能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><strong>Synthetic human dynamics的目标</strong>：生成表现人类表达意图的视频。</li>
<li><strong>主要挑战</strong>：几何不一致性和粗糙重建、运动推广的局限性以及场景不和谐。</li>
<li><strong>HumanGenesis框架的功能</strong>：通过四个协作代理解决上述问题，包括重建、评估、姿态引导和视频和谐化。</li>
<li><strong>Reconstructor的作用</strong>：利用3D高斯喷射和变形分解从单目视频中构建一致的3D人类场景表示。</li>
<li><strong>Critique Agent的功能</strong>：通过多轮MLLM反馈增强重建的忠实度。</li>
<li><strong>Pose Guider的重要性</strong>：通过时间感知参数编码器生成表达性姿态序列，实现运动推广。</li>
<li><strong>Video Harmonizer的作用</strong>：通过混合渲染管道与扩散技术，合成逼真的连贯视频，并通过Back-to-4D反馈循环优化重建过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09858">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bb06d1e0e3fb6ddcc349d6f69ff864b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8dc2ff97445f89f928b14db25ffcef7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5a313c233c7a7298c0fca946898426.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0acb34337ca1635ffc17bbc026b456d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e24333e3a1d25bab6870e3e5485f23c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory"><a href="#Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory" class="headerlink" title="Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with   Long-Term Memory"></a>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with   Long-Term Memory</h2><p><strong>Authors:Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</strong></p>
<p>We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot’s perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at <a target="_blank" rel="noopener" href="https://github.com/bytedance-seed/m3-agent">https://github.com/bytedance-seed/m3-agent</a> </p>
<blockquote>
<p>我们介绍了M3-Agent，这是一种配备长期记忆的新型多模态代理框架。与人类相似，M3-Agent能够处理实时的视觉和听觉输入，以构建和更新其长期记忆。除了情景记忆之外，它还发展出语义记忆，使其能够随着时间的推移积累世界知识。它的记忆以实体为中心、多模态的方式组织，允许对环境的更深层次和更一致的理解。接受指令后，M3-Agent能够自主进行多轮迭代推理，从记忆中检索相关信息以完成任务。为了评估多模态代理中的记忆效果和基于记忆推理的能力，我们开发了M3-Bench，这是一个新的长视频问答基准测试。M3-Bench包括从机器人视角捕获的100个新录制现实世界视频（M3-Bench-robot）和涵盖不同场景的929个网络视频（M3-Bench-web）。我们注释了问题答案对，旨在测试代理应用程序所需的关键能力，例如人类理解、一般知识提取和跨模态推理。实验结果表明，通过强化学习训练的M3-Agent超越了最强基线——使用Gemini-1.5-pro和GPT-4o的提示代理，在M3-Bench-robot、M3-Bench-web和VideoMME-long上的准确率分别提高了6.7%、7.7%和5.3%。我们的工作使多模态代理朝着更类似于人类的长时记忆方向发展，并为其实用设计提供了见解。模型、代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/bytedance-seed/m3-agent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bytedance-seed/m3-agent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09736v1">PDF</a> </p>
<p><strong>Summary</strong><br>     引入了一种配备长期记忆的新型多模态代理框架M3-Agent。该框架可以处理实时的视觉和听觉输入来构建和更新其长期记忆，具备积累世界知识的能力。其记忆以实体为中心、多模态的方式组织，从而实现对环境的更深和更一致的理解。开发新的视频问答基准测试M3-Bench，以评估多模态代理的长期记忆效果和基于记忆推理的能力。M3-Agent在测试中表现优异，超越了最强基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新型的多模态代理框架M3-Agent，具备处理实时视觉和听觉输入的能力。</li>
<li>M3-Agent拥有长期记忆，可以积累世界知识。</li>
<li>M3-Agent的记忆以实体为中心、多模态的方式组织。</li>
<li>开发了一个新的视频问答基准测试M3-Bench，用于评估多模态代理的记忆和推理能力。</li>
<li>M3-Agent在M3-Bench测试中表现优于最强的基线模型。</li>
<li>M3-Agent通过强化学习进行训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09736">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-08ce0560c62f133bdd3256f0e3479d8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e3d57b7970b35415912f02b69d3f5b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d402162a75b9869bddcc4fc2fa75e1a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61cb5764a43a536769aa76a58f6c351b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ReqInOne-A-Large-Language-Model-Based-Agent-for-Software-Requirements-Specification-Generation"><a href="#ReqInOne-A-Large-Language-Model-Based-Agent-for-Software-Requirements-Specification-Generation" class="headerlink" title="ReqInOne: A Large Language Model-Based Agent for Software Requirements   Specification Generation"></a>ReqInOne: A Large Language Model-Based Agent for Software Requirements   Specification Generation</h2><p><strong>Authors:Taohong Zhu, Lucas C. Cordeiro, Youcheng Sun</strong></p>
<p>Software Requirements Specification (SRS) is one of the most important documents in software projects, but writing it manually is time-consuming and often leads to ambiguity. Existing automated methods rely heavily on manual analysis, while recent Large Language Model (LLM)-based approaches suffer from hallucinations and limited controllability. In this paper, we propose ReqInOne, an LLM-based agent that follows the common steps taken by human requirements engineers when writing an SRS to convert natural language into a structured SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into three tasks: summary, requirement extraction, and requirement classification, each supported by tailored prompt templates to improve the quality and consistency of LLM outputs.   We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the generated SRSs against those produced by the holistic GPT-4-based method from prior work as well as by entry-level requirements engineers. Expert evaluations show that ReqInOne produces more accurate and well-structured SRS documents. The performance advantage of ReqInOne benefits from its modular design, and experimental results further demonstrate that its requirement classification component achieves comparable or even better results than the state-of-the-art requirement classification model. </p>
<blockquote>
<p>软件需求规格说明（SRS）是软件项目中最重要的文档之一，但手动编写既耗时又容易导致歧义。现有的自动化方法很大程度上依赖于手动分析，而最近基于大型语言模型（LLM）的方法则存在虚构和可控性有限的问题。在本文中，我们提出了ReqInOne，这是一个基于LLM的代理，它遵循人类需求工程师在编写SRS时采取的通用步骤，将自然语言转换为结构化的SRS。ReqInOne采用模块化架构，将SRS生成分解为三个任务：摘要、需求提取和需求分类，每个任务都由专门的提示模板支持，以提高LLM输出的质量和一致性。我们使用GPT-4o、LLaMA 3和DeepSeek-R1对ReqInOne进行了评估，并将生成的SRS与之前工作中基于整体GPT-4的方法以及入门级需求工程师产生的SRS进行了比较。专家评估表明，ReqInOne产生的SRS文档更准确、结构更好。ReqInOne的性能优势得益于其模块化设计，而且实验结果表明，其在需求分类组件方面达到了与最新需求分类模型相当甚至更好的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了ReqInOne，一个基于大型语言模型（LLM）的自动化工具，用于将自然语言转化为结构化的软件需求规格（SRS）。ReqInOne采用模块化架构，将SRS生成分解为摘要、需求提取和需求分类三个任务，并通过定制提示模板来提高LLM输出的质量和一致性。实验结果表明，ReqInOne生成的SRS文档更准确、结构更好，其性能优势得益于模块化设计，并且其需求分类组件的性能与最先进的模型相当或更好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReqInOne是一个基于LLM的自动化工具，用于生成软件需求规格（SRS）。</li>
<li>ReqInOne采用模块化设计，将SRS生成分为摘要、需求提取和需求分类三个任务。</li>
<li>ReqInOne使用定制提示模板来提高大型语言模型（LLM）输出的质量和一致性。</li>
<li>ReqInOne生成的SRS文档更准确、结构更好。</li>
<li>ReqInOne的性能优势源于其模块化设计。</li>
<li>ReqInOne的需求分类组件性能与最先进的模型相当或更好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09648">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c30e9d7e4f84824442bfdce2b3c6a810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcafb024df32cc69f22ada623c0e69d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f165529b325f01d23c3220a55ecb46a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85067b77688d13174cdbfa941cf45216.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-530b4e2530f79dfe1e855c5648b21ade.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Preacher-Paper-to-Video-Agentic-System"><a href="#Preacher-Paper-to-Video-Agentic-System" class="headerlink" title="Preacher: Paper-to-Video Agentic System"></a>Preacher: Paper-to-Video Agentic System</h2><p><strong>Authors:Jingwei Liu, Ling Yang, Hao Luo, Fan Wang, Hongyan Li, Mengdi Wang</strong></p>
<p>The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a topdown approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: <a target="_blank" rel="noopener" href="https://github.com/GenVerse/Paper2Video">https://github.com/GenVerse/Paper2Video</a> </p>
<blockquote>
<p>这篇论文将研究论文转化为结构化的视频摘要，提炼关键概念、方法和结论，使其易于理解并以良好的组织方式呈现。尽管最先进的视频生成模型显示出潜力，但它们受限于有限的上下文窗口、严格的视频持续时间限制、有限的风格多样性和无法表示特定领域知识。为了解决这些局限性，我们引入了Preacher，这是第一个论文到视频的智能系统。Preacher采用自上而下的方法分解、总结和重构论文，然后进行自下而上的视频生成，将多样化的视频片段合成一个连贯的摘要。为了对齐跨模态表示，我们定义了关键场景并引入了渐进式思维链（P-CoT）进行精细的迭代规划。Preacher成功地在五个研究领域生成了高质量的视频摘要，显示出超越当前视频生成模型的专长。代码将在以下网址发布：<a target="_blank" rel="noopener" href="https://github.com/GenVerse/Paper2Video">https://github.com/GenVerse/Paper2Video</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09632v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了一种将研究论文转化为结构化视频摘要的任务，其中存在视频生成模型的局限性。为此，引入Preacher作为首个论文转视频的系统，通过自上而下分解、总结和重构论文内容，并结合自下而上的视频生成，将多样化的视频片段合成一个连贯的摘要。系统采用渐进式思维链进行精细迭代规划，成功生成高质量的视频摘要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文转视频摘要任务旨在将研究论文的关键概念、方法和结论转化为易于理解、结构化的视频格式。</li>
<li>当前视频生成模型存在局限性，如上下文窗口有限、视频时长约束、风格单一以及无法表达领域特定知识。</li>
<li>Preacher是首个论文转视频的智能化系统，采用自上而下与自下而上的方法，实现对论文的分解、总结和重构，以及视频生成。</li>
<li>Preacher通过定义关键场景和引入渐进式思维链（P-CoT）进行精细迭代规划，实现跨模态表示的对齐。</li>
<li>Preacher成功生成高质量的视频摘要，并展示在五个研究领域的专业能力。</li>
<li>系统将在GitHub上发布代码以供公众使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09632">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-50999c62775d2fc4a1c41938baa61249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c35f78f977b7b562e643467f781ae79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aabd58efacb3021bc328b855ffb6879f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-796ef1f12ffd673831080a00b4094a7f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation"><a href="#Distilling-LLM-Prior-to-Flow-Model-for-Generalizable-Agent’s-Imagination-in-Object-Goal-Navigation" class="headerlink" title="Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination   in Object Goal Navigation"></a>Distilling LLM Prior to Flow Model for Generalizable Agent’s Imagination   in Object Goal Navigation</h2><p><strong>Authors:Badi Li, Ren-jie Lu, Yu Zhou, Jingke Meng, Wei-shi Zheng</strong></p>
<p>The Object Goal Navigation (ObjectNav) task challenges agents to locate a specified object in an unseen environment by imagining unobserved regions of the scene. Prior approaches rely on deterministic and discriminative models to complete semantic maps, overlooking the inherent uncertainty in indoor layouts and limiting their ability to generalize to unseen environments. In this work, we propose GOAL, a generative flow-based framework that models the semantic distribution of indoor environments by bridging observed regions with LLM-enriched full-scene semantic maps. During training, spatial priors inferred from large language models (LLMs) are encoded as two-dimensional Gaussian fields and injected into target maps, distilling rich contextual knowledge into the flow model and enabling more generalizable completions. Extensive experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D and Gibson, and shows strong generalization in transfer settings to HM3D. Codes and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/Badi-Li/GOAL">https://github.com/Badi-Li/GOAL</a>. </p>
<blockquote>
<p>对象目标导航（ObjectNav）任务挑战智能体通过在场景中想象未观察到的区域，在未知环境中定位指定对象。之前的方法依赖于确定性和判别模型来完成语义地图，忽略了室内布局的内在不确定性，并限制了它们在未知环境中的泛化能力。在这项工作中，我们提出了GOAL，这是一个基于生成流的框架，通过桥接观察区域和丰富的语言模型（LLM）的全场景语义地图，对室内环境的语义分布进行建模。在训练过程中，从大型语言模型推断出的空间先验被编码为二维高斯场并注入目标地图，将丰富的上下文知识蒸馏到流模型中，使模型能够更通用地完成。大量实验表明，GOAL在MP3D和Gibson上达到了最先进的性能水平，并在转移到HM3D时显示出强大的泛化能力。代码和预先训练的模型可在<a target="_blank" rel="noopener" href="https://github.com/Badi-Li/GOAL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Badi-Li/GOAL找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09423v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于目标导航（ObjectNav）任务挑战代理在未见过的环境中寻找指定对象，通过想象场景的未观测区域来完成。先前的方法依赖于确定性和判别模型来完成语义地图，忽略了室内布局的固有不确定性，限制了它们在未见环境中的泛化能力。本文提出了GOAL，这是一个基于生成流的框架，通过桥接观测区域和LLM丰富的全场景语义地图，对室内环境的语义分布进行建模。在训练过程中，从大语言模型推断的空间先验被编码为二维高斯场并注入目标地图，将丰富的上下文知识蒸馏到流模型中，使完成任务更具泛化性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ObjectGoalNav任务挑战代理在未见过的环境中寻找指定对象，需要想象未观测的区域来完成任务。</li>
<li>现有方法主要依赖于确定性和判别模型完成语义地图，存在局限性。</li>
<li>GOAL框架采用生成流的方式对室内环境的语义分布进行建模，结合观测区域和LLM丰富的全场景语义地图。</li>
<li>空间先验被编码为二维高斯场并注入目标地图，丰富上下文知识。</li>
<li>GOAL在MP3D和Gibson数据集上达到最佳性能。</li>
<li>GOAL在HM3D的转移设置中也表现出强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09423">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a62d07f6d68e576315ad0cee128ac637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b58aa2d7978847db9c79f95ad94e31b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f991e46a007b6daf921fd00a2aee8083.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Waymo-3DSkelMo-A-Multi-Agent-3D-Skeletal-Motion-Dataset-for-Pedestrian-Interaction-Modeling-in-Autonomous-Driving"><a href="#Waymo-3DSkelMo-A-Multi-Agent-3D-Skeletal-Motion-Dataset-for-Pedestrian-Interaction-Modeling-in-Autonomous-Driving" class="headerlink" title="Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian   Interaction Modeling in Autonomous Driving"></a>Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian   Interaction Modeling in Autonomous Driving</h2><p><strong>Authors:Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho</strong></p>
<p>Large-scale high-quality 3D motion datasets with multi-person interactions are crucial for data-driven models in autonomous driving to achieve fine-grained pedestrian interaction understanding in dynamic urban environments. However, existing datasets mostly rely on estimating 3D poses from monocular RGB video frames, which suffer from occlusion and lack of temporal continuity, thus resulting in unrealistic and low-quality human motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics, derived from the Waymo Perception dataset. Our key insight is to utilize 3D human body shape and motion priors to enhance the quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The dataset covers over 14,000 seconds across more than 800 real driving scenarios, including rich interactions among an average of 27 agents per scene (with up to 250 agents in the largest scene). Furthermore, we establish 3D pose forecasting benchmarks under varying pedestrian densities, and the results demonstrate its value as a foundational resource for future research on fine-grained human behavior understanding in complex urban environments. The dataset and code will be available at <a target="_blank" rel="noopener" href="https://github.com/GuangxunZhu/Waymo-3DSkelMo">https://github.com/GuangxunZhu/Waymo-3DSkelMo</a> </p>
<blockquote>
<p>大规模高质量的三维动作数据集对于自动驾驶中的数据驱动模型至关重要，旨在实现在动态环境中对行人交互的精细理解。然而，现有的数据集大多依赖于从单目RGB视频帧中估计三维姿态，这些视频存在遮挡问题并且缺乏时间连续性，因此产生了不真实和低质量的人类动作。本文介绍了Waymo-3DSkelMo数据集，它是第一个大规模数据集，提供高质量、时间连贯的三维骨骼运动以及明确的交互语义，来源于Waymo感知数据集。我们的关键见解是利用三维人体形状和运动先验知识来提高从原始激光雷达点云中提取的三维姿态序列的质量。该数据集覆盖了超过14,000秒的时间，涵盖超过800个真实驾驶场景，包括平均每场景中有高达27个智能体之间的丰富交互（最大场景中最多可达250个智能体）。此外，我们在不同的行人密度下建立了三维姿态预测基准测试，结果表明它对于未来在复杂城市环境中进行精细人类行为理解研究的基础资源具有重要意义。数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/GuangxunZhu/Waymo-3DSkelMo%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/GuangxunZhu/Waymo-3DSkelMo上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09404v1">PDF</a> ACM Multimedia 2025 (Dataset Track) Paper</p>
<p><strong>Summary</strong></p>
<p>Waymo-3DSkelMo数据集对于数据驱动模型在动态城市环境中实现精细的行人交互理解至关重要。它提供了高质量、时间连贯的3D骨骼运动，具有明确的交互语义，并利用3D人体形状和运动先验知识提高从LiDAR点云提取的3D姿势序列的质量。数据集包含超过14,000秒的驾驶场景，涵盖丰富的行人交互。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Waymo-3DSkelMo是首个提供高质量、时间连贯的3D骨骼运动的大型数据集，具有明确的交互语义。</li>
<li>数据集利用3D人体形状和运动先验知识，提高从LiDAR点云提取的3D姿势序列的质量。</li>
<li>数据集包含超过14,000秒的驾驶场景，覆盖丰富的行人交互，场景中包含平均27个行人（最大场景可达250个行人）。</li>
<li>Waymo-3DSkelMo数据集对于数据驱动模型在动态城市环境中实现精细的行人交互理解至关重要。</li>
<li>数据集建立了在不同行人密度下的3D姿态预测基准测试，证明了其在未来复杂城市环境中精细人类行为理解研究中的价值。</li>
<li>Waymo-3DSkelMo数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/GuangxunZhu/Waymo-3DSkelMo%E4%B8%8A%E6%8F%90%E4%BE%9B%EF%BC%8C%E6%96%B9%E4%BE%BF%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/GuangxunZhu/Waymo-3DSkelMo上提供，方便未来研究使用。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09404">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7ac833fd19922b5383741f940d3967f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43499667c3c51b6cb8f0981e9c0d10e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb0910bc69628b4da6287002da717f64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5e74a981f0ef639f48b301c3ee6f63e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL"><a href="#Beyond-Ten-Turns-Unlocking-Long-Horizon-Agentic-Search-with-Large-Scale-Asynchronous-RL" class="headerlink" title="Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale   Asynchronous RL"></a>Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale   Asynchronous RL</h2><p><strong>Authors:Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu</strong></p>
<p>Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. &lt;&#x3D;10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/ASearcher">https://github.com/inclusionAI/ASearcher</a>. </p>
<blockquote>
<p>最近基于大型语言模型（LLM）的代理人的进展，通过整合外部工具，在处理复杂、知识密集型任务方面展现了显著的能力。在多种工具选择中，搜索工具在访问大量外部知识方面发挥着至关重要的作用。然而，开源代理人仍然缺乏实现专家级的搜索智能，即解决模糊查询、进行精确搜索、分析结果以及进行全面探索的能力。现有方法在可扩展性、效率和数据质量方面存在不足。例如，现有在线强化学习（RL）方法的小回合限制（例如&lt;&#x3D;10），限制了复杂策略的学习。本文介绍了ASearcher，一个用于大规模RL训练的搜索引擎代理人的开源项目。我们的主要贡献包括：（1）可扩展的完全异步RL训练，能够在维持高效训练的同时实现长期搜索。（2）基于提示的大型语言模型代理人能够自主合成高质量、具有挑战性的问答，创建大规模问答数据集。通过强化学习训练，我们的基于提示的QwQ-32B代理人实现了显著改进，在xBench和GAIA上分别实现了46.7%和20.8%的Avg@4增益。值得注意的是，我们的代理人展示了极端的长期搜索，在训练过程中的工具调用超过40回合，输出令牌超过15万。通过简单的代理人设计和不使用外部大型语言模型，ASearcher-Web-QwQ在xBench上实现了42.1的Avg@4分数，在GAIA上实现了52.8的分数，超越了现有的开源32B代理人。我们在<a target="_blank" rel="noopener" href="https://github.com/inclusionAI/ASearcher%E5%BC%80%E6%BA%90%E6%88%91%E4%BB%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%81%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/inclusionAI/ASearcher开源我们的模型、训练数据和代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07976v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-based代理的新进展通过整合外部工具展现了处理复杂、知识密集型任务的能力。搜索工具在访问大量外部知识中发挥着关键作用。然而，开源代理仍未能实现专家级的搜索智能，存在解决模糊查询、生成精确搜索、分析结果和全面探索的能力不足的问题。现有方法存在可扩展性、效率和数据质量方面的不足。本文介绍了ASearcher，一个用于大规模RL训练的开源项目。主要贡献包括：可伸缩的完全异步RL训练，能够在维持高效训练的同时进行长期搜索；基于提示的LLM代理能够自主合成高质量、具挑战性的问答，创建大规模QA数据集。通过RL训练，我们的基于提示的QwQ-32B代理在xBench和GAIA上分别实现了46.7%和20.8%的Avg@4增益。值得注意的是，我们的代理展现了极端长期搜索，训练过程中的工具调用超过40轮，输出令牌超过150k。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based代理在整合外部工具方面展现出色，尤其在处理复杂、知识密集型任务时表现突出。</li>
<li>搜索工具在访问外部知识中起关键作用，但现有开源代理在搜索智能方面仍有所欠缺。</li>
<li>当前方法存在可扩展性、效率和数据质量方面的挑战。</li>
<li>ASearcher项目通过引入可伸缩的完全异步RL训练，提高了长期搜索的能力并保持高效。</li>
<li>基于提示的LLM代理能自主合成高质量QA数据集，是ASearcher的一大贡献。</li>
<li>QwQ-32B代理通过RL训练在xBench和GAIA上取得了显著成绩，展现了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b095ee9ac1a8fb1afe7903d337f2916.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c836bbede69c8863e7b4fbacbf3545ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-760778ade6cb934839ce86ea2ff26424.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ContestTrade-A-Multi-Agent-Trading-System-Based-on-Internal-Contest-Mechanism"><a href="#ContestTrade-A-Multi-Agent-Trading-System-Based-on-Internal-Contest-Mechanism" class="headerlink" title="ContestTrade: A Multi-Agent Trading System Based on Internal Contest   Mechanism"></a>ContestTrade: A Multi-Agent Trading System Based on Internal Contest   Mechanism</h2><p><strong>Authors:Li Zhao, Rui Sun, Zuoyou Jiang, Bo Yang, Yuxiao Bai, Mengting Chen, Xinyang Wang, Jing Li, Zuo Bai</strong></p>
<p>In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model’s constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent’s performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multi-agent systems and traditional quantitative investment methods across diverse evaluation metrics. ContestTrade is open-sourced on GitHub at <a target="_blank" rel="noopener" href="https://github.com/FinStep-AI/ContestTrade">https://github.com/FinStep-AI/ContestTrade</a>. </p>
<blockquote>
<p>在金融交易领域，基于大型语言模型（LLM）的代理展现出巨大的潜力。然而，对市场噪声的高度敏感性削弱了LLM交易系统的性能。为了解决这一局限性，我们提出了一种新型的多代理系统，该系统以现代企业管理结构为灵感，具备内部竞争机制。该系统由两个专业团队组成：（1）数据团队——负责处理和压缩大量市场数据，将其转化为多样化的文本因素，确保它们符合模型的约束语境。（2）研究团队——任务是基于深度研究方法做出并行多路径交易决策。核心创新在于在每个团队内部实施实时评估和排名机制，以真实的市场反馈为驱动。每个代理的表现都会进行持续打分和排名，只有表现最佳的代理的输出才会被采用。这种设计使系统能够自适应地调整动态环境，增强对市场噪声的稳健性，并最终实现优越的交易性能。实验结果表明，我们提出的系统在多种评估指标上显著优于现有的多代理系统和传统的量化投资方法。ContestTrade已在GitHub上开源，网址为：<a target="_blank" rel="noopener" href="https://github.com/FinStep-AI/ContestTrade%E3%80%82">https://github.com/FinStep-AI/ContestTrade。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00554v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的金融交易代理展现出巨大潜力，但易受市场噪音影响。为此，我们提出一种新型多代理系统，借鉴现代企业管理制度中的竞争机制。系统分为数据团队和研究团队，分别负责数据处理与深度研究决策。创新之处在于引入实时评价和排名机制，通过市场反馈来评估每个代理的表现，并只采用表现最佳的代理输出。该系统能适应环境变化，增强对噪音的稳健性，最终提供出色的交易性能。实验结果显示，该系统在多个评估指标上显著优于现有多代理系统和传统量化投资方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在金融交易领域展现出巨大潜力。</li>
<li>市场噪音对基于大型语言模型的交易系统性能产生负面影响。</li>
<li>提出一种新型多代理系统，借鉴现代企业管理制度中的竞争机制以提升性能。</li>
<li>系统包含数据团队和研究团队，分别负责数据处理和深度研究决策。</li>
<li>实时评价和排名机制用于评估每个代理的表现，并仅采用最佳表现代理的输出。</li>
<li>系统能够适应环境变化，增强稳健性，对抗市场噪音。</li>
<li>实验结果显示，该系统在多个评估指标上优于现有方法和传统量化投资手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae0c6d44f2f14285a15e41b75bcf0f11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63a2a4554b46987fe5bbee5dd238d79e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-845f97c4ca49462c05ce2a5ee8a567bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9d6f4cdc1d825c2c7632a2518b983be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0c12f5c6cfba9d27a3d774b509d09a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f25eb3b5bb11a003a53aa98f905ffe3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="WebArXiv-Evaluating-Multimodal-Agents-on-Time-Invariant-arXiv-Tasks"><a href="#WebArXiv-Evaluating-Multimodal-Agents-on-Time-Invariant-arXiv-Tasks" class="headerlink" title="WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks"></a>WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks</h2><p><strong>Authors:Zihao Sun, Ling Chen</strong></p>
<p>Recent progress in large language models (LLMs) has enabled the development of autonomous web agents capable of navigating and interacting with real websites. However, evaluating such agents remains challenging due to the instability and inconsistency of existing benchmarks, which often rely on dynamic content or oversimplified simulations. In this work, we introduce WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks grounded in the arXiv platform. WebArXiv ensures reproducible and reliable evaluation by anchoring tasks in fixed web snapshots with deterministic ground truths and standardized action trajectories. Through behavioral analysis, we identify a common failure mode, Rigid History Reflection, where agents over-rely on fixed interaction histories. To address this, we propose a lightweight dynamic reflection mechanism that allows agents to selectively retrieve relevant past steps during decision-making. We evaluate ten state-of-the-art web agents on WebArXiv. Results demonstrate clear performance differences across agents and validate the effectiveness of our proposed reflection strategy. </p>
<blockquote>
<p>最近，大型语言模型（LLM）的进展为能够导航和与现实网站进行交互的自主网络代理的发展提供了可能。然而，由于现有基准测试的不稳定性和不一致性，评估此类代理仍然具有挑战性，这些基准测试通常依赖于动态内容或过于简化的模拟。在这项工作中，我们介绍了WebArXiv，这是一个由基于arXiv平台的275个网络任务组成的静态且时间不变的基准测试。WebArXiv通过锚定任务在固定的网络快照中，确保具有可重复性和可靠性的评估，并具有确定性的真实依据和标准化的行动轨迹。通过行为分析，我们发现了一种常见的失败模式，即刚性历史反射，代理过度依赖于固定的交互历史记录。为了解决这一问题，我们提出了一种轻量级的动态反射机制，允许代理在决策过程中有选择地检索相关的过去步骤。我们在WebArXiv上评估了十个最先进的网络代理。结果表明，各代理之间的性能差异显著，并验证了我们所提出的反射策略的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00938v2">PDF</a> 10 pages, 9 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的最新进展使得能够开发出可以浏览和与现实网站交互的自主网络代理。然而，由于现有基准测试的不稳定性和不一致性，评估这些代理仍然具有挑战性。这项工作介绍了WebArXiv，一个由基于arXiv平台的网络任务组成的静态且时间不变的基准测试。WebArXiv通过将任务锚定在具有确定性地面真相和标准行动轨迹的固定网页快照上，确保可重复和可靠的评估。通过行为分析，我们确定了常见的失败模式——刚性历史反射，即代理过度依赖固定的交互历史。为解决这一问题，我们提出了一种轻量级的动态反射机制，允许代理在决策过程中有选择地检索相关的过去步骤。我们对十个最先进的网络代理进行了WebArXiv评估。结果表明各代理之间的性能差异显著，并验证了我们所提出的反射策略的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的进步促进了自主网络代理的发展，能够与现实网站交互。</li>
<li>现有代理评估基准测试存在不稳定性和不一致性问题。</li>
<li>介绍了WebArXiv，一个静态且时间不变的基准测试，由基于arXiv平台的网络任务组成。</li>
<li>WebArXiv通过固定网页快照确保可重复和可靠的评估，具有确定性地面真相和标准行动轨迹。</li>
<li>发现了代理常见的失败模式——刚性历史反射，即过度依赖固定交互历史。</li>
<li>为解决这一问题，提出了轻量级的动态反射机制，允许代理在决策时选择性检索相关历史信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9955233532663f175fdee354a528fa5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b82a1ef1172789c0a65cb29ab3c629f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e9df62d1c8d45d5cf792d50faae6ea1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MetaCipher-A-Time-Persistent-and-Universal-Multi-Agent-Framework-for-Cipher-Based-Jailbreak-Attacks-for-LLMs"><a href="#MetaCipher-A-Time-Persistent-and-Universal-Multi-Agent-Framework-for-Cipher-Based-Jailbreak-Attacks-for-LLMs" class="headerlink" title="MetaCipher: A Time-Persistent and Universal Multi-Agent Framework for   Cipher-Based Jailbreak Attacks for LLMs"></a>MetaCipher: A Time-Persistent and Universal Multi-Agent Framework for   Cipher-Based Jailbreak Attacks for LLMs</h2><p><strong>Authors:Boyuan Chen, Minghao Shao, Abdul Basit, Siddharth Garg, Muhammad Shafique</strong></p>
<p>As large language models (LLMs) grow more capable, they face growing vulnerability to sophisticated jailbreak attacks. While developers invest heavily in alignment finetuning and safety guardrails, researchers continue publishing novel attacks, driving progress through adversarial iteration. This dynamic mirrors a strategic game of continual evolution. However, two major challenges hinder jailbreak development: the high cost of querying top-tier LLMs and the short lifespan of effective attacks due to frequent safety updates. These factors limit cost-efficiency and practical impact of research in jailbreak attacks. To address this, we propose MetaCipher, a low-cost, multi-agent jailbreak framework that generalizes across LLMs with varying safety measures. Using reinforcement learning, MetaCipher is modular and adaptive, supporting extensibility to future strategies. Within as few as 10 queries, MetaCipher achieves state-of-the-art attack success rates on recent malicious prompt benchmarks, outperforming prior jailbreak methods. We conduct a large-scale empirical evaluation across diverse victim models and benchmarks, demonstrating its robustness and adaptability. Warning: This paper contains model outputs that may be offensive or harmful, shown solely to demonstrate jailbreak efficacy. </p>
<blockquote>
<p>随着大型语言模型（LLM）的能力越来越强，它们面临越来越复杂的越狱攻击（jailbreak attacks）的威胁。虽然开发者在微调对齐和安全防护方面投入了大量精力，但研究者们仍在不断发布新型攻击方法，通过对抗迭代推动技术进步。这种动态反映了一种持续演化的战略博弈。然而，存在两大挑战阻碍越狱攻击的发展：一是查询顶尖LLM的成本高昂，二是由于频繁的安全更新，有效攻击的寿命短暂。这些因素限制了越狱攻击研究中成本和实际影响的效率。为了解决这一问题，我们提出了MetaCipher，一个低成本、多代理的越狱框架，可跨具有不同安全措施的LLM进行通用化。使用强化学习，MetaCipher具有模块化和自适应的特点，支持未来策略的扩展性。在仅10次查询内，MetaCipher达到了最新恶意提示基准测试的攻击成功率之巅，超越了先前的越狱方法。我们对各种不同的受害者模型和基准测试进行了大规模的经验评估，证明了其稳健性和适应性。警告：本论文包含可能具有攻击性或有害性的模型输出，仅用于展示越狱效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22557v2">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLMs）随着功能不断增强，面临着日益增长的复杂越狱攻击风险。尽管开发人员在安全更新方面投入了大量努力，研究者仍不断推出新型攻击方法，通过对抗性迭代推动技术进步。为此，我们提出了MetaCipher，一个低成本、多智能体的越狱框架，能够应对不同安全措施的LLMs。MetaCipher使用强化学习，具有模块化和自适应特点，未来策略具有可扩展性。它在少量查询内达到了针对最新恶意提示基准测试的最先进攻击成功率，超越了之前的越狱方法。大规模实证研究证明了其稳健性和适应性。请注意，本文中包含的模型输出可能具有攻击性或有害性，仅用于展示越狱效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）随着功能增强，面临更复杂越狱攻击风险。</li>
<li>开发者在安全防护方面投入大量努力，但研究者仍推出新型攻击方法。</li>
<li>MetaCipher是一个低成本、多智能体的越狱框架，能够应对不同安全措施的LLMs。</li>
<li>MetaCipher使用强化学习，具有模块化和自适应特点。</li>
<li>MetaCipher在少量查询内达到了针对最新恶意提示基准测试的最先进攻击成功率。</li>
<li>MetaCipher经过大规模实证研究证明其稳健性和适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9e2682585be26b15438b2303770c43ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-411d6c92d715a181d2bd8f075b0242e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04df01b9a711fc44d67a8d103e3b5d39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e157b7208b1a53ace023057ac3841a4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b0bb06a341eacd361ff525de4c4447.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AgentOrchestra-A-Hierarchical-Multi-Agent-Framework-for-General-Purpose-Task-Solving"><a href="#AgentOrchestra-A-Hierarchical-Multi-Agent-Framework-for-General-Purpose-Task-Solving" class="headerlink" title="AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose   Task Solving"></a>AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose   Task Solving</h2><p><strong>Authors:Wentao Zhang, Liang Zeng, Yuzhen Xiao, Yongcong Li, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, Bo An</strong></p>
<p>Recent advances in agent systems have demonstrated remarkable capabilities in solving both general-purpose and highly complex tasks. However, most current models lack mechanisms for coordinating specialized agents and have limited ability to generalize to new or diverse domains. To this end, we introduce AgentOrchestra, a hierarchical multi-agent framework for general-purpose task solving that integrates high-level planning with modular agent collaboration. Drawing inspiration from a conductor orchestrating a symphony, and grounded in the principles of extensibility, multimodality, modularity, and coordination, it features a central planning agent that decomposes complex objectives and delegates sub-tasks to a team of specialized agents. Each sub-agent is equipped with general programming tools, as well as abilities to tackle a wide range of real-world specific tasks, including data analysis, file operations, web navigation, and interactive reasoning in dynamic multimodal environments. Notably, AgentOrchestra introduces an MCP Manager Agent that enables intelligent evolution through dynamic tool creation, retrieval, and reuse mechanisms, significantly enhancing the system’s adaptability and scalability. AgentOrchestra supports flexible orchestration through explicit sub-goal formulation, inter-agent communication, and adaptive role allocation. We evaluate the framework on three widely used benchmarks for assessing LLM-based agent systems. Experimental results show that AgentOrchestra consistently outperforms flat-agent and monolithic baselines in terms of task success rate and adaptability. On the GAIA benchmark testing dataset, AgentOrchestra achieves an average score of 83.39%, ranking among the top general-purpose agents. These results highlight the effectiveness of hierarchical organization and role specialization in building scalable and general-purpose LLM-based agent systems. </p>
<blockquote>
<p>近期代理系统领域的进展在解决通用和高复杂度任务方面表现出了显著的能力。然而，当前大多数模型缺乏协调专业代理的机制，且在新领域或多样领域的泛化能力有限。为此，我们引入了AgentOrchestra，这是一个用于通用任务解决的分层多代理框架，它将高级规划与模块化代理协作集成在一起。它借鉴了指挥协调交响乐团的灵感，并基于可扩展性、多模态性、模块化和协调的原则，拥有一个中央规划代理，可以分解复杂目标并将子任务委派给一组专业代理。每个子代理都配备了通用编程工具，以及处理各种现实世界特定任务的能力，包括数据分析、文件操作、网络导航以及在动态多模式环境中的交互推理。值得一提的是，AgentOrchestra引入了一个MCP管理代理，通过动态的工具创建、检索和再利用机制，实现了智能进化，显著增强了系统的适应性和可扩展性。AgentOrchestra通过明确的子目标制定、代理间通信和自适应角色分配来支持灵活编排。我们在三个广泛使用的评估LLM基于代理系统的基准测试上对框架进行了评估。实验结果表明，在任务成功率和适应性方面，AgentOrchestra持续优于平面代理和单一基准测试。在GAIA基准测试数据集上，AgentOrchestra的平均得分为8_百分之三十三点三（约合百分之八十三），位居通用代理之首。这些结果突出了分层组织和角色专业化在构建可扩展和通用的LLM基于代理系统方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12508v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AgentOrchestra是一个用于通用任务解决的多层次多智能体框架，它融合了高级规划与模块化智能体协作。该框架借鉴了指挥交响乐的思维，展现出扩展性、多模态性、模块化和协调的原则。通过中央规划智能体分解复杂目标并委派子任务给专业智能体团队，实现了智能协同。每个子智能体具备通用编程工具以及应对多种现实特定任务的能力。此外，AgentOrchestra引入了MCP管理智能体，通过动态创建、检索和重用工具实现智能进化，增强了系统的适应性和可扩展性。评估结果显示，AgentOrchestra在任务成功率和适应性方面超越扁平智能体和单一基准测试，且在GAIA基准测试数据集中平均得分达83.39%，成为顶尖通用智能体之一。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AgentOrchestra是一个多层次多智能体框架，用于解决通用任务。</li>
<li>融合了高级规划与模块化智能体协作，借鉴指挥交响乐的思维。</li>
<li>展现出扩展性、多模态性、模块化和协调的原则。</li>
<li>中央规划智能体负责分解复杂目标并委派子任务给专业智能体团队。</li>
<li>每个子智能体具备通用编程工具以及应对多种现实特定任务的能力。</li>
<li>AgentOrchestra引入了MCP管理智能体，增强了系统的适应性和可扩展性。</li>
<li>AgentOrchestra在任务成功率和适应性方面表现出色，且在GAIA基准测试中得分高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-20226e53d3c52c4aef04294f8755b5e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6089fe933ce6f60df2807dba1c4447f9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments"><a href="#DefenderBench-A-Toolkit-for-Evaluating-Language-Agents-in-Cybersecurity-Environments" class="headerlink" title="DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments"></a>DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity   Environments</h2><p><strong>Authors:Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed</strong></p>
<p>Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench’s modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench">https://github.com/microsoft/DefenderBench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）代理在人类语言理解和推理方面展示了令人印象深刻的能力，然而它们在网络安全方面的潜力尚未被充分探索。我们推出了DefenderBench，这是一个实用的开源工具包，可以在进攻、防御和基于网络安全知识的任务中评估语言代理。DefenderBench包括网络入侵、恶意内容检测、代码漏洞分析和网络安全知识评估的环境。它特意为研究者们设计，旨在提供公平严格的评估，同时保持经济实惠和易于访问。我们使用标准化的代理框架，对若干最新技术和流行的LLMs进行基准测试，包括开放和封闭权重模型。我们的结果表明，Claude-3.7-sonnet表现最佳，DefenderBench得分为81.65，其次是Claude-3.7-sonnet-think，得分为78.40，而最好的开放权重模型Llama 3.3 70B紧随其后，DefenderBench得分为71.81。DefenderBench的模块化设计允许无缝集成自定义LLMs和任务，促进结果可重复性和公平比较。DefenderBench的匿名版本可在<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/microsoft/DefenderBench上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00739v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在理解和推理人类语言方面表现出强大的能力，但在网络安全领域的应用潜力尚未得到充分探索。本文介绍了DefenderBench，这是一个用于评估语言模型在攻击、防御和网络安全知识任务上的实用开源工具包。DefenderBench包含网络入侵、恶意内容检测、代码漏洞分析和网络安全知识评估的环境。其旨在成为面向研究人员的负担得起的易于访问的工具，提供公平严谨的评价方法。文章通过标准化的框架评估了一些先进的大型语言模型，发现Claude-3.7-sonnet表现最佳，得分81.65，其次是Claude-3.7-sonnet-think得分78.40，而表现最好的开源模型Llama 3.3 70B得分紧随其后为71.81。DefenderBench的模块化设计允许无缝集成自定义的大型语言模型和任务，促进了研究的可重复性和公平比较。匿名版本的DefenderBench可在<a target="_blank" rel="noopener" href="https://github.com/microsoft/DefenderBench%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/microsoft/DefenderBench获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在网络安全领域的应用潜力尚未充分探索。</li>
<li>DefenderBench是一个用于评估语言模型在网络安全领域的实用开源工具包。</li>
<li>DefenderBench包含网络入侵、恶意内容检测、代码漏洞分析和网络安全知识评估的环境。</li>
<li>DefenderBench旨在成为面向研究人员的负担得起的易于访问的工具，提供公平严谨的评价方法。</li>
<li>Claude-3.7-sonnet在DefenderBench的评估中表现最佳，得分81.65。</li>
<li>DefenderBench允许无缝集成自定义的大型语言模型和任务，促进研究的可重复性和公平比较。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a67556e6a11465719036bb51ab278f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bab7f4d11665b5e3feed5f17ce028c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce477798b819c94cd5e4fe2f624ddee.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CO-Bench-Benchmarking-Language-Model-Agents-in-Algorithm-Search-for-Combinatorial-Optimization"><a href="#CO-Bench-Benchmarking-Language-Model-Agents-in-Algorithm-Search-for-Combinatorial-Optimization" class="headerlink" title="CO-Bench: Benchmarking Language Model Agents in Algorithm Search for   Combinatorial Optimization"></a>CO-Bench: Benchmarking Language Model Agents in Algorithm Search for   Combinatorial Optimization</h2><p><strong>Authors:Weiwei Sun, Shengyu Feng, Shanda Li, Yiming Yang</strong></p>
<p>Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems – a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agentic frameworks against established human-designed algorithms, revealing the strengths and limitations of existing LLM agents and identifying promising directions for future research. CO-Bench is publicly available at <a target="_blank" rel="noopener" href="https://github.com/sunnweiwei/CO-Bench">https://github.com/sunnweiwei/CO-Bench</a>. </p>
<blockquote>
<p>尽管基于大型语言模型（LLM）的代理在软件工程和机器学习研究等领域引起了广泛关注，但它们在推进组合优化（CO）方面的作用仍然相对未被充分探索。这一差距凸显了深入理解它们解决结构化、约束密集型问题的潜力的重要性，然而目前这一追求受限于缺乏全面的基准测试来进行系统研究。为了解决这一问题，我们引入了CO-Bench，这是一个包含36个来自广泛领域和复杂度级别的实际CO问题的基准测试套件。CO-Bench包括结构化的问题表述和经过筛选的数据，以支持对LLM代理的严格调查。我们评估了多个代理框架与已建立的人工设计算法，揭示了现有LLM代理的优势和局限性，并确定了未来研究的有前途的方向。CO-Bench可在<a target="_blank" rel="noopener" href="https://github.com/sunnweiwei/CO-Bench%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/sunnweiwei/CO-Bench公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04310v2">PDF</a> </p>
<p><strong>Summary</strong><br>LLM在软件工程和机器学习等领域备受关注，但在组合优化领域的应用相对缺乏研究。为此，本文提出CO-Bench，包含从各种领域和复杂度级别中抽取的36个真实世界组合优化问题，以支持对LLM的全面系统研究。文章通过评估多个代理框架与已知人类设计算法，揭示了现有LLM代理的优势和局限性，并指出了未来研究的有前途的方向。CO-Bench已在GitHub上公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM在组合优化领域的应用受到关注但相对缺乏研究。</li>
<li>CO-Bench是一个新的基准测试套件，包含真实世界的组合优化问题，旨在支持对LLM的全面系统研究。</li>
<li>CO-Bench包含从各种领域和复杂度级别中抽取的问题。</li>
<li>文章评估了多个代理框架和人类设计算法，揭示了LLM代理的优缺点。</li>
<li>CO-Bench为未来的研究指出了有前途的方向。</li>
<li>CO-Bench已在GitHub上公开可用，便于研究人员使用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04310">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ecdf4ff80a18f417ab3d81a4bd29bea1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3e9dbbf6346259ecd59657681d74f48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea9651dd027f05dd2d7eaf013ba2538.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84bd85b3a4fa71e30770edb169a9c624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-109ab5cdd59876c3dcc03a8f7b1f46ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f97bf9cec1ac1890b290cb95f182e5f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3b468e9cc9767f15282c89dc41195453.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-15  Translation of Text Embedding via Delta Vector to Suppress Strongly   Entangled Content in Text-to-Image Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0604d1794ea3edc8354f76c872b60e7e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-15  Noise Hypernetworks Amortizing Test-Time Compute in Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
