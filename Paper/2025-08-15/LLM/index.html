<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  Noise Hypernetworks Amortizing Test-Time Compute in Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0604d1794ea3edc8354f76c872b60e7e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-15-æ›´æ–°"><a href="#2025-08-15-æ›´æ–°" class="headerlink" title="2025-08-15 æ›´æ–°"></a>2025-08-15 æ›´æ–°</h1><h2 id="Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models"><a href="#Noise-Hypernetworks-Amortizing-Test-Time-Compute-in-Diffusion-Models" class="headerlink" title="Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models"></a>Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</h2><p><strong>Authors:Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, Zeynep Akata</strong></p>
<p>The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/HyperNoise">https://github.com/ExplainableML/HyperNoise</a> </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾çš„æ–°èŒƒå¼åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚æ¨ç†æ¨¡å‹ï¼‰å’Œç”Ÿæˆè§†è§‰æ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„çªç ´ï¼Œå…è®¸æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ†é…é¢å¤–çš„è®¡ç®—æ¥æœ‰æ•ˆåº”å¯¹æ—¥ç›Šå¤æ‚çš„é—®é¢˜ã€‚å°½ç®¡è¿™ç§æ–¹æ³•æœ‰æ‰€æ”¹è¿›ï¼Œä½†å‡ºç°äº†ä¸€ä¸ªé‡è¦å±€é™æ€§ï¼šè®¡ç®—æ—¶é—´çš„æ˜¾è‘—å¢åŠ ä½¿å¾—è¯¥è¿‡ç¨‹å¯¹äºè®¸å¤šåº”ç”¨è€Œè¨€å˜å¾—ç¼“æ…¢ä¸”ä¸åˆ‡å®é™…ã€‚è€ƒè™‘åˆ°è¿™ç§èŒƒå¼çš„æˆåŠŸåŠå…¶æ—¥ç›Šå¢é•¿çš„ç”¨é€”ï¼Œæˆ‘ä»¬å¯»æ±‚åœ¨é¿å…æ¨ç†å¼€é”€çš„åŒæ—¶ä¿ç•™å…¶å¥½å¤„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ¨¡å‹åè®­ç»ƒæœŸé—´å°†æµ‹è¯•æ—¶ç¼©æ”¾çŸ¥è¯†é›†æˆåˆ°æ¨¡å‹ä¸­çš„å…³é”®é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨å™ªå£°è¶…ç½‘ç»œå–ä»£äº†æ‰©æ•£æ¨¡å‹ä¸­çš„å¥–åŠ±å¼•å¯¼æµ‹è¯•æ—¶é—´å™ªå£°ä¼˜åŒ–ï¼Œè¯¥è¶…ç½‘ç»œè°ƒåˆ¶åˆå§‹è¾“å…¥å™ªå£°ã€‚æˆ‘ä»¬ä¸ºè’¸é¦ç”Ÿæˆå™¨å­¦ä¹ è¿™ç§å¥–åŠ±å€¾å‘åˆ†å¸ƒæå‡ºäº†ä¸€ä¸ªç†è®ºä¸Šçš„æ¡†æ¶ï¼Œé€šè¿‡ä¸€ä¸ªå¯è¡Œçš„å™ªå£°ç©ºé—´ç›®æ ‡æ¥å®ç°ï¼Œè¯¥ç›®æ ‡åœ¨ä¿æŒå¯¹åŸºç¡€æ¨¡å‹å¿ å®æ€§çš„åŒæ—¶ä¼˜åŒ–æ‰€éœ€ç‰¹æ€§ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾ƒå°‘çš„è®¡ç®—æˆæœ¬ä¸‹æ¢å¤äº†å¤§é‡æ¥è‡ªæ˜¾å¼æµ‹è¯•æ—¶é—´ä¼˜åŒ–çš„è´¨é‡æ”¶ç›Šã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/HyperNoise%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ExplainableML/HyperNoiseæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09968v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://noisehypernetworks.github.io/">https://noisehypernetworks.github.io/</a></p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£æµ‹è¯•æ—¶é—´æ‰©å±•æ¨¡å¼åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆè§†è§‰æ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œå…è®¸æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºä»¥åº”å¯¹æ—¥ç›Šå¤æ‚çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œè®¡ç®—æ—¶é—´çš„å¢åŠ ä½¿å¾—å®é™…åº”ç”¨å˜å¾—ç¼“æ…¢ä¸”ä¸åˆ‡å®é™…ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨æ¨¡å‹åè®­ç»ƒé˜¶æ®µé›†æˆæµ‹è¯•æ—¶é—´æ‰©å±•çŸ¥è¯†çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªå™ªå£°è¶…ç½‘ç»œå–ä»£äº†æ‰©æ•£æ¨¡å‹ä¸­çš„å¥–åŠ±å¼•å¯¼æµ‹è¯•æ—¶é—´å™ªå£°ä¼˜åŒ–ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç†è®ºæ‰å®çš„å­¦ä¹ å¥–åŠ±å€¾æ–œåˆ†å¸ƒçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”¨äºè’¸é¦ç”Ÿæˆå™¨ï¼Œé€šè¿‡ä¸€ä¸ªå¯æ“ä½œçš„å™ªå£°ç©ºé—´ç›®æ ‡ï¼Œåœ¨ä¿æŒå¯¹åŸºç¡€æ¨¡å‹ä¿çœŸåº¦çš„åŒæ—¶ä¼˜åŒ–æ‰€éœ€ç‰¹æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨è¾ƒä½çš„è®¡ç®—æˆæœ¬ä¸‹æ¢å¤å¤§éƒ¨åˆ†æµ‹è¯•æ—¶é—´ä¼˜åŒ–çš„è´¨é‡æ”¶ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´æ‰©å±•æ¨¡å¼åœ¨LLMå’Œç”Ÿæˆè§†è§‰æ¨¡å‹ä¸­å–å¾—æ˜¾è‘—çªç ´ï¼Œå…è®¸æ¨¡å‹åº”å¯¹å¤æ‚é—®é¢˜ã€‚</li>
<li>è¿™ç§æ¨¡å¼çš„ä¸€ä¸ªå…³é”®é™åˆ¶æ˜¯è®¡ç®—æ—¶é—´æ˜¾è‘—å¢åŠ ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å˜å¾—ä¸åˆ‡å®é™…ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå³é€šè¿‡å™ªå£°è¶…ç½‘ç»œé›†æˆæµ‹è¯•æ—¶é—´æ‰©å±•çŸ¥è¯†ã€‚</li>
<li>è¯¥æ–¹æ³•å–ä»£äº†æ‰©æ•£æ¨¡å‹ä¸­çš„å¥–åŠ±å¼•å¯¼æµ‹è¯•æ—¶é—´å™ªå£°ä¼˜åŒ–ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç†è®ºæ‰å®çš„å­¦ä¹ å¥–åŠ±å€¾æ–œåˆ†å¸ƒçš„æ¡†æ¶ç”¨äºè’¸é¦ç”Ÿæˆå™¨ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å™ªå£°ç©ºé—´ç›®æ ‡æ¥ä¼˜åŒ–æ‰€éœ€ç‰¹æ€§å¹¶ä¿æŒå¯¹åŸºç¡€æ¨¡å‹çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-482f290ddafe4065d81bc0da510850c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcb2ae846f083eeba5506a328d8d6aa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3af97d21df124aac8d78d6e0934c5edc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks"><a href="#Neural-Bandit-Based-Optimal-LLM-Selection-for-a-Pipeline-of-Tasks" class="headerlink" title="Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks"></a>Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks</h2><p><strong>Authors:Baran Atalar, Eddie Zhang, Carlee Joe-Wong</strong></p>
<p>With the increasing popularity of large language models (LLMs) for a variety of tasks, there has been a growing interest in strategies that can predict which out of a set of LLMs will yield a successful answer at low cost. This problem promises to become more and more relevant as providers like Microsoft allow users to easily create custom LLM â€œassistantsâ€ specialized to particular types of queries. However, some tasks (i.e., queries) may be too specialized and difficult for a single LLM to handle alone. These applications often benefit from breaking down the task into smaller subtasks, each of which can then be executed by a LLM expected to perform well on that specific subtask. For example, in extracting a diagnosis from medical records, one can first select an LLM to summarize the record, select another to validate the summary, and then select another, possibly different, LLM to extract the diagnosis from the summarized record. Unlike existing LLM selection or routing algorithms, this setting requires that we select a sequence of LLMs, with the output of each LLM feeding into the next and potentially influencing its success. Thus, unlike single LLM selection, the quality of each subtaskâ€™s output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned and accounted for during selection. We propose a neural contextual bandit-based algorithm that trains neural networks that model LLM success on each subtask in an online manner, thus learning to guide the LLM selections for the different subtasks, even in the absence of historical LLM performance data. Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ™®åŠåº¦è¶Šæ¥è¶Šé«˜ï¼Œäººä»¬å¯¹äºèƒ½å¤Ÿé¢„æµ‹å“ªäº›LLMèƒ½å¤Ÿåœ¨ä½æˆæœ¬ä¸‹äº§ç”ŸæˆåŠŸç­”æ¡ˆçš„ç­–ç•¥çš„å…´è¶£ä¹Ÿåœ¨å¢é•¿ã€‚éšç€åƒå¾®è½¯è¿™æ ·çš„æä¾›å•†è®©ç”¨æˆ·èƒ½å¤Ÿè½»æ¾åˆ›å»ºé’ˆå¯¹ç‰¹å®šç±»å‹æŸ¥è¯¢çš„å®šåˆ¶LLMâ€œåŠ©ç†â€ï¼Œè¿™ä¸ªé—®é¢˜å°†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œä¸€äº›ä»»åŠ¡ï¼ˆå³æŸ¥è¯¢ï¼‰å¯èƒ½è¿‡äºä¸“ä¸šåŒ–å’Œå¤æ‚ï¼Œå•ä¸€LLMå¯èƒ½éš¾ä»¥å•ç‹¬å¤„ç†ã€‚è¿™äº›åº”ç”¨ç¨‹åºé€šå¸¸å—ç›Šäºå°†ä»»åŠ¡åˆ†è§£æˆè¾ƒå°çš„å­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡ç„¶åå¯ä»¥ç”±ä¸€ä¸ªåœ¨ç‰¹å®šå­ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„LLMæ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œåœ¨ä»åŒ»ç–—è®°å½•ä¸­æå–è¯Šæ–­ä¿¡æ¯æ—¶ï¼Œå¯ä»¥å…ˆé€‰æ‹©ä¸€ä¸ªLLMæ¥æ€»ç»“è®°å½•ï¼Œå†é€‰æ‹©å¦ä¸€ä¸ªæ¥éªŒè¯æ‘˜è¦ï¼Œç„¶åé€‰æ‹©å¦ä¸€ä¸ªå¯èƒ½æ˜¯ä¸åŒçš„LLMä»æ‘˜è¦çš„è®°å½•ä¸­æå–è¯Šæ–­ä¿¡æ¯ã€‚ä¸ç°æœ‰çš„LLMé€‰æ‹©æˆ–è·¯ç”±ç®—æ³•ä¸åŒï¼Œæ­¤è®¾ç½®è¦æ±‚æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªLLMåºåˆ—ï¼Œæ¯ä¸ªLLMçš„è¾“å‡ºéƒ½ä¼šè¾“å…¥åˆ°ä¸‹ä¸€ä¸ªLLMä¸­å¹¶å¯èƒ½å½±å“å…¶æˆåŠŸã€‚å› æ­¤ï¼Œä¸å•ä¸€LLMé€‰æ‹©ä¸åŒï¼Œæ¯ä¸ªå­ä»»åŠ¡çš„è¾“å‡ºç›´æ¥å½±å“è¾“å…¥ï¼Œå› æ­¤ç›´æ¥å½±å“ä¸‹æ¸¸LLMçš„æˆæœ¬å’ŒæˆåŠŸç‡ï¼Œäº§ç”Ÿå¤æ‚çš„æ€§èƒ½ä¾èµ–å…³ç³»ï¼Œå¿…é¡»åœ¨é€‰æ‹©è¿‡ç¨‹ä¸­å­¦ä¹ å’Œè€ƒè™‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œä¸Šä¸‹æ–‡å¼ºç›—ç®—æ³•çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨çº¿è®­ç»ƒç¥ç»ç½‘ç»œå¯¹æ¯é¡¹ä»»åŠ¡çš„LLMæˆåŠŸç‡è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå­¦ä¹ ä¸ºä¸åŒçš„å­ä»»åŠ¡é€‰æ‹©LLMï¼Œå³ä½¿åœ¨æ²¡æœ‰å†å²LLMæ€§èƒ½æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨ç”µä¿¡é—®ç­”å’ŒåŒ»ç–—è¯Šæ–­é¢„æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å…¶ä»–LLMé€‰æ‹©ç®—æ³•ç›¸æ¯”å…·æœ‰æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09958v1">PDF</a> Submitted to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ™®åŠå¼•å‘äº†å¯¹å…¶é¢„æµ‹ç­–ç•¥çš„å…´è¶£ï¼Œè¿™äº›ç­–ç•¥æ—¨åœ¨ä»¥ä½æˆæœ¬é¢„æµ‹å“ªäº›LLMèƒ½å¤ŸæˆåŠŸå®Œæˆä»»åŠ¡ã€‚éšç€åƒå¾®è½¯è¿™æ ·çš„æä¾›å•†å…è®¸ç”¨æˆ·è½»æ¾åˆ›å»ºé’ˆå¯¹ç‰¹å®šç±»å‹æŸ¥è¯¢çš„è‡ªå®šä¹‰LLMâ€œåŠ©æ‰‹â€ï¼Œè¿™ä¸ªé—®é¢˜å°†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é’ˆå¯¹æŸäº›å¤æ‚çš„ä»»åŠ¡ï¼Œå•ç‹¬ä¸€ä¸ªLLMå¯èƒ½éš¾ä»¥å¤„ç†ã€‚é€šè¿‡å°†è¿™äº›ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡å¹¶ç”±é€‚åˆçš„LLMæ‰§è¡Œï¼Œå¯ä»¥æé«˜æ•ˆç‡ã€‚æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ä¸Šä¸‹æ–‡å¼ºç›—ç®—æ³•çš„LLMé€‰æ‹©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨çº¿å»ºæ¨¡æ¯ä¸ªå­ä»»åŠ¡ä¸ŠLLMçš„æˆåŠŸç‡ï¼Œä»è€ŒæŒ‡å¯¼ä¸åŒå­ä»»åŠ¡çš„LLMé€‰æ‹©ï¼Œå³ä½¿åœ¨æ²¡æœ‰å†å²LLMæ€§èƒ½æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆå·¥ä½œã€‚å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç”µä¿¡é—®ç­”å’ŒåŒ»ç–—è¯Šæ–­é¢„æµ‹æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ­£åœ¨æˆä¸ºå¤šç§ä»»åŠ¡çš„çƒ­é—¨é€‰æ‹©ï¼Œå› æ­¤å¯¹é¢„æµ‹å…¶æˆåŠŸç‡çš„ç­–ç•¥äº§ç”Ÿäº†å…´è¶£ã€‚</li>
<li>éšç€LLMåŠ©æ‰‹çš„æ™®åŠï¼Œé¢„æµ‹å“ªäº›LLMèƒ½æˆåŠŸå®Œæˆä»»åŠ¡çš„é—®é¢˜å˜å¾—æ›´ä¸ºé‡è¦ã€‚</li>
<li>é’ˆå¯¹æŸäº›å¤æ‚çš„ä»»åŠ¡ï¼Œå•ç‹¬ä¸€ä¸ªLLMå¯èƒ½éš¾ä»¥å¤„ç†ï¼Œå°†å…¶åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡å¯æé«˜æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ä¸Šä¸‹æ–‡å¼ºç›—ç®—æ³•çš„LLMé€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥åœ¨çº¿å»ºæ¨¡æ¯ä¸ªå­ä»»åŠ¡ä¸ŠLLMçš„æˆåŠŸç‡ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æŒ‡å¯¼LLMé€‰æ‹©æ¥æé«˜ä»»åŠ¡æˆåŠŸç‡å¹¶é™ä½æˆæœ¬ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14eaafd68935b323ff0d0cfe259f0073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6863fa1393091ef17a5e7a1937435b48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fbaf17a7244941988f0e5e35ee31a37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-742d00b757f228f8d188d93a5609934d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Performance-of-GPT-5-Frontier-Models-in-Ophthalmology-Question-Answering"><a href="#Performance-of-GPT-5-Frontier-Models-in-Ophthalmology-Question-Answering" class="headerlink" title="Performance of GPT-5 Frontier Models in Ophthalmology Question Answering"></a>Performance of GPT-5 Frontier Models in Ophthalmology Question Answering</h2><p><strong>Authors:Fares Antaki, David Mikhail, Daniel Milad, Danny A Mammo, Sumit Sharma, Sunil K Srivastava, Bing Yu Chen, Samir Touma, Mertcan Sevgi, Jonathan El-Khoury, Pearse A Keane, Qingyu Chen, Yih Chung Tham, Renaud Duval</strong></p>
<p>Large language models (LLMs) such as GPT-5 integrate advanced reasoning capabilities that may improve performance on complex medical question-answering tasks. For this latest generation of reasoning models, the configurations that maximize both accuracy and cost-efficiency have yet to be established. We evaluated 12 configurations of OpenAIâ€™s GPT-5 series (three model tiers across four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using 260 closed-access multiple-choice questions from the American Academy of Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome was multiple-choice accuracy; secondary outcomes included head-to-head ranking via a Bradley-Terry model, rationale quality assessment using a reference-anchored, pairwise LLM-as-a-judge framework, and analysis of accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano variants (P &lt; .001), o1-high (P &#x3D; .04), and GPT-4o (P &lt; .001), but not o3-high (0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x stronger than o3-high) and rationale quality (1.11x stronger than o3-high). Cost-accuracy analysis identified several GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering the most favorable low-cost, high-performance balance. These results benchmark GPT-5 on a high-quality ophthalmology dataset, demonstrate the influence of reasoning effort on accuracy, and introduce an autograder framework for scalable evaluation of LLM-generated answers against reference standards in ophthalmology. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-5é›†æˆäº†å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œå¯èƒ½ä¼šæé«˜åœ¨å¤æ‚åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å¯¹äºè¿™ä¸€æœ€æ–°ä¸€ä»£çš„æ¨ç†æ¨¡å‹ï¼Œå°šæœªç¡®å®šèƒ½å¤Ÿæœ€å¤§åŒ–å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç›Šçš„é…ç½®ã€‚æˆ‘ä»¬è¯„ä¼°äº†OpenAIçš„GPT-5ç³»åˆ—ï¼ˆå››ä¸ªæ¨ç†åŠªåŠ›è®¾ç½®ä¸­çš„ä¸‰ä¸ªæ¨¡å‹å±‚æ¬¡ï¼‰ä»¥åŠo1-highã€o3-highå’ŒGPT-4oï¼Œä½¿ç”¨ç¾å›½çœ¼ç§‘ç§‘å­¦é™¢åŸºç¡€ä¸´åºŠç§‘å­¦è¯¾ç¨‹ï¼ˆBCSCï¼‰æ•°æ®é›†çš„260é“å°é—­å¼å¤šé€‰é¢˜ã€‚ä¸»è¦ç»“æœæ˜¯å¤šé€‰é¢˜çš„å‡†ç¡®æ€§ï¼›æ¬¡è¦ç»“æœåŒ…æ‹¬é€šè¿‡Bradley-Terryæ¨¡å‹è¿›è¡Œçš„å¤´å¯¹å¤´æ’åã€ä½¿ç”¨å‚è€ƒé”šå®šã€æˆå¯¹çš„LLMä½œä¸ºæ³•å®˜æ¡†æ¶çš„åˆç†æ€§è´¨é‡è¯„ä¼°ï¼Œä»¥åŠä½¿ç”¨åŸºäºä»¤ç‰Œçš„æˆæœ¬ä¼°ç®—è¿›è¡Œçš„å‡†ç¡®æ€§æˆæœ¬æƒè¡¡åˆ†æã€‚GPT-5-highçš„å‡†ç¡®ç‡æœ€é«˜ï¼ˆ0.965ï¼›95%ç½®ä¿¡åŒºé—´ï¼Œ0.942-0.985ï¼‰ï¼Œä¼˜äºæ‰€æœ‰GPT-5-nanoå˜ä½“ï¼ˆP &lt; .001ï¼‰ã€o1-highï¼ˆP &#x3D; .04ï¼‰å’ŒGPT-4oï¼ˆP &lt; .001ï¼‰ï¼Œä½†ä¸o3-highï¼ˆ0.958ï¼›95%ç½®ä¿¡åŒºé—´ï¼Œ0.931-0.981ï¼‰æ— æ˜¾è‘—å·®å¼‚ã€‚GPT-5-highåœ¨å‡†ç¡®æ€§å’Œåˆç†æ€§æ–¹é¢å‡æ’åç¬¬ä¸€ï¼ˆæ¯”o3-highé«˜å‡º1.66å€å’Œ1.11å€ï¼‰ã€‚æˆæœ¬å‡†ç¡®æ€§åˆ†æç¡®å®šäº†å¸•ç´¯æ‰˜å‰æ²¿çš„å‡ ä¸ªGPT-5é…ç½®ï¼Œå…¶ä¸­GPT-5-mini-lowæä¾›äº†æœ€å…·æˆæœ¬æ•ˆç›Šã€é«˜æ€§èƒ½å¹³è¡¡çš„æ–¹æ¡ˆã€‚è¿™äº›ç»“æœåœ¨çœ¼ç§‘æ•°æ®é›†ä¸Šå¯¹GPT-5è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†æ¨ç†åŠªåŠ›å¯¹å‡†ç¡®æ€§çš„å½±å“ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„åˆ†æ¡†æ¶ï¼Œå¯è§„æ¨¡åŒ–åœ°è¯„ä¼°LLMç”Ÿæˆçš„ç­”æ¡ˆä¸çœ¼ç§‘é¢†åŸŸçš„å‚è€ƒæ ‡å‡†ä¹‹é—´çš„å¯¹æ¯”æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09956v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-5é›†æˆäº†å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œå¯èƒ½æé«˜å¤æ‚åŒ»å­¦é—®ç­”ä»»åŠ¡çš„æ€§èƒ½ã€‚å¯¹äºè¿™ä¸€ä»£æ¨ç†æ¨¡å‹ï¼Œå°šæœªå»ºç«‹æ—¢èƒ½æœ€å¤§åŒ–å‡†ç¡®æ€§åˆå®ç°æˆæœ¬æ•ˆç›Šçš„é…ç½®ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹OpenAIçš„GPT-5ç³»åˆ—ï¼ˆä¸‰ä¸ªæ¨¡å‹å±‚æ¬¡ï¼Œå››ç§æ¨ç†éš¾åº¦è®¾ç½®ï¼‰ä»¥åŠo1-highã€o3-highå’ŒGPT-4oè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨äº†ç¾å›½çœ¼ç§‘ç§‘å­¦é™¢åŸºç¡€ä¸´åºŠç§‘å­¦è¯¾ç¨‹ï¼ˆBCSCï¼‰æ•°æ®é›†çš„260é“å°é—­å¤šé€‰é¢˜ã€‚ä¸»è¦ç»“æœæ˜¯å¤šé¡¹é€‰æ‹©é¢˜å‡†ç¡®ç‡ï¼›æ¬¡è¦ç»“æœåŒ…æ‹¬åŸºäºBradley-Terryæ¨¡å‹çš„æ’åå¯¹æ¯”ã€åˆ©ç”¨å‚è€ƒé”šå®šçš„æˆå¯¹LLMè¯„åˆ¤æ¡†æ¶çš„åˆç†æ€§è´¨é‡è¯„ä¼°ä»¥åŠåŸºäºä»¤ç‰Œæˆæœ¬çš„å‡†ç¡®åº¦æˆæœ¬æƒè¡¡åˆ†æã€‚GPT-5-highçš„å‡†ç¡®ç‡æœ€é«˜ï¼ˆ0.965ï¼›95%ç½®ä¿¡åŒºé—´ï¼Œ0.942-0.985ï¼‰ï¼Œä¼˜äºæ‰€æœ‰GPT-5-nanoå˜ç§ï¼ˆP &lt; .001ï¼‰ã€o1-highï¼ˆP &#x3D; .04ï¼‰å’ŒGPT-4oï¼ˆP &lt; .001ï¼‰ï¼Œä½†ä¸o3-highï¼ˆ0.958ï¼›95%ç½®ä¿¡åŒºé—´ï¼Œ0.931-0.981ï¼‰ç›¸å½“ã€‚GPT-5-highåœ¨å‡†ç¡®ç‡å’Œåˆç†æ€§è´¨é‡æ–¹é¢å‡æ’åç¬¬ä¸€ï¼ˆåˆ†åˆ«æ˜¯o3çš„ä¸¤å€å’Œä¸€ç‚¹ä¸€ä¸€å€ï¼‰ã€‚æˆæœ¬å‡†ç¡®æ€§åˆ†æç¡®å®šäº†è‹¥å¹²GPT-5é…ç½®ä½äºå¸•ç´¯æ‰˜å‰æ²¿ï¼Œå…¶ä¸­GPT-5-mini-lowæä¾›äº†ä½æˆæœ¬å’Œé«˜æ€§èƒ½ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚è¯¥ç ”ç©¶å¯¹GPT-5åœ¨çœ¼ç§‘é«˜è´¨é‡æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº†æ¨ç†éš¾åº¦å¯¹å‡†ç¡®åº¦çš„å½±å“ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„åˆ†æ¡†æ¶ï¼Œå¯é’ˆå¯¹çœ¼ç§‘é¢†åŸŸçš„å‚è€ƒæ ‡å‡†å¯¹LLMç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œå¯è§„æ¨¡åŒ–è¯„ä¼°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GPT-5åœ¨å¤æ‚åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ä¸ä¹‹å‰çš„æ¨¡å‹ç›¸æ¯”ï¼Œå¦‚GPT-4oå’Œo3ç³»åˆ—æ¨¡å‹ã€‚</li>
<li>GPT-5ç³»åˆ—çš„ä¸åŒé…ç½®åœ¨å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå…¶ä¸­GPT-5-highè¡¨ç°æœ€ä½³ã€‚</li>
<li>é™¤äº†å‡†ç¡®æ€§å¤–ï¼ŒGPT-5ç³»åˆ—æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œç­”æ¡ˆåˆç†æ€§æ–¹é¢ä¹Ÿè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>æˆæœ¬æ•ˆç›Šåˆ†æè¡¨æ˜ï¼ŒGPT-5ç³»åˆ—ä¸­å­˜åœ¨ä¸€äº›å…·æœ‰é«˜æ€§ä»·æ¯”çš„é…ç½®ã€‚</li>
<li>GPT-5åœ¨çœ¼ç§‘æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¯„ä¼°ä¸ºè¯¥é¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚</li>
<li>æ¨ç†éš¾åº¦å¯¹LLMçš„æ€§èƒ½æœ‰å½±å“ï¼Œè¿™æ„å‘³ç€é’ˆå¯¹ä¸åŒä»»åŠ¡å¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹é…ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae9530e0d0fc73c20a3da619c27214f2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning"><a href="#Stable-Diffusion-Models-are-Secretly-Good-at-Visual-In-Context-Learning" class="headerlink" title="Stable Diffusion Models are Secretly Good at Visual In-Context Learning"></a>Stable Diffusion Models are Secretly Good at Visual In-Context Learning</h2><p><strong>Authors:Trevine Oorloff, Vishwanath Sindagi, Wele Gedara Chaminda Bandara, Ali Shafahi, Amin Ghiasi, Charan Prakash, Reza Ardekani</strong></p>
<p>Large language models (LLM) in natural language processing (NLP) have demonstrated great potential for in-context learning (ICL) â€“ the ability to leverage a few sets of example prompts to adapt to various tasks without having to explicitly update the model weights. ICL has recently been explored for computer vision tasks with promising early outcomes. These approaches involve specialized training and&#x2F;or additional data that complicate the process and limit its generalizability. In this work, we show that off-the-shelf Stable Diffusion models can be repurposed for visual in-context learning (V-ICL). Specifically, we formulate an in-place attention re-computation within the self-attention layers of the Stable Diffusion architecture that explicitly incorporates context between the query and example prompts. Without any additional fine-tuning, we show that this repurposed Stable Diffusion model is able to adapt to six different tasks: foreground segmentation, single object detection, semantic segmentation, keypoint detection, edge detection, and colorization. For example, the proposed approach improves the mean intersection over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by 8.9% and 3.2% over recent methods such as Visual Prompting and IMProv, respectively. Additionally, we show that the proposed method is able to effectively leverage multiple prompts through ensembling to infer the task better and further improve the performance. </p>
<blockquote>
<p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯æŒ‡åˆ©ç”¨å°‘é‡ç¤ºä¾‹æç¤ºæ¥é€‚åº”å„ç§ä»»åŠ¡ï¼Œè€Œæ— éœ€æ˜ç¡®æ›´æ–°æ¨¡å‹æƒé‡ã€‚è™½ç„¶æœ€è¿‘å·²ç»å¼€å§‹æ¢ç´¢è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ä¸Šä¸‹æ–‡å­¦ä¹ å¹¶è·å¾—äº†æœ‰å¸Œæœ›çš„æ—©æœŸæˆæœï¼Œä½†è¿™äº›æ–¹æ³•æ¶‰åŠä¸“é—¨åŸ¹è®­å’Œ&#x2F;æˆ–é¢å¤–æ•°æ®ï¼Œè¿™å¢åŠ äº†å¤æ‚æ€§å¹¶é™åˆ¶äº†å…¶æ™®éæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æˆçš„Stable Diffusionæ¨¡å‹å¯ä»¥ç”¨äºè§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆV-ICLï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨Stable Diffusionæ¶æ„çš„è‡ªæ³¨æ„åŠ›å±‚å†…åˆ¶å®šäº†å³æ—¶æ³¨æ„åŠ›é‡æ–°è®¡ç®—ï¼Œè¯¥è®¡ç®—æ˜¾å¼åœ°ç»“åˆäº†æŸ¥è¯¢å’Œç¤ºä¾‹æç¤ºä¹‹é—´çš„ä¸Šä¸‹æ–‡ã€‚æ— éœ€ä»»ä½•é¢å¤–çš„å¾®è°ƒï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§é‡æ–°è®¾è®¡çš„Stable Diffusionæ¨¡å‹èƒ½å¤Ÿé€‚åº”å…­ç§ä¸åŒçš„ä»»åŠ¡ï¼šå‰æ™¯åˆ†å‰²ã€å•ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å…³é”®ç‚¹æ£€æµ‹ã€è¾¹ç¼˜æ£€æµ‹å’Œå½©è‰²åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨Pascal-5iæ•°æ®é›†ä¸Šï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‰æ™¯åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰æé«˜äº†8.9%ï¼Œç›¸è¾ƒäºæœ€è¿‘çš„Visual Promptingå’ŒIMProvæ–¹æ³•åˆ†åˆ«æé«˜äº†3.2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿé€šè¿‡é›†æˆå¤šä¸ªæç¤ºæ¥æ›´å¥½åœ°æ¨æ–­ä»»åŠ¡å¹¶è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09949v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå³åˆ©ç”¨å°‘é‡ç¤ºä¾‹æç¤ºé€‚åº”å„ç§ä»»åŠ¡è€Œæ— éœ€æ˜¾å¼æ›´æ–°æ¨¡å‹æƒé‡ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ç°æˆçš„Stable Diffusionæ¨¡å‹å¯ç”¨äºè§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆV-ICLï¼‰ã€‚é€šè¿‡è‡ªæˆ‘æ³¨æ„å±‚å†…çš„å³æ—¶æ³¨æ„åŠ›é‡æ–°è®¡ç®—ï¼Œæ˜ç¡®ç»“åˆäº†æŸ¥è¯¢å’Œç¤ºä¾‹æç¤ºä¹‹é—´çš„ä¸Šä¸‹æ–‡ã€‚æ— éœ€ä»»ä½•ç²¾ç»†è°ƒæ•´ï¼Œè¯¥æ¨¡å‹å°±èƒ½é€‚åº”å…­ç§ä¸åŒä»»åŠ¡ï¼Œå¹¶åœ¨Pascal-5iæ•°æ®é›†ä¸Šçš„å‰æ™¯åˆ†å‰²ä»»åŠ¡æé«˜äº†å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰8.9%å’Œ3.2%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡é›†æˆå¤šä¸ªæç¤ºæ¥æ›´å¥½åœ°æ¨æ–­ä»»åŠ¡å¹¶è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>Stable Diffusionæ¨¡å‹å¯ç”¨äºè§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆV-ICLï¼‰ã€‚</li>
<li>é€šè¿‡åœ¨è‡ªæˆ‘æ³¨æ„å±‚å†…è¿›è¡Œå³æ—¶æ³¨æ„åŠ›é‡æ–°è®¡ç®—ï¼ŒStable Diffusionæ¨¡å‹èƒ½æ˜ç¡®ç»“åˆæŸ¥è¯¢å’Œç¤ºä¾‹æç¤ºä¹‹é—´çš„ä¸Šä¸‹æ–‡ã€‚</li>
<li>æ— éœ€ç²¾ç»†è°ƒæ•´ï¼Œè¯¥æ¨¡å‹å°±èƒ½é€‚åº”å¤šç§ä»»åŠ¡ï¼Œå¦‚å‰æ™¯åˆ†å‰²ã€å•ç›®æ ‡æ£€æµ‹ç­‰ã€‚</li>
<li>åœ¨Pascal-5iæ•°æ®é›†ä¸Šçš„å‰æ™¯åˆ†å‰²ä»»åŠ¡ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†æœ€è¿‘çš„æ–¹æ³•ï¼Œå¦‚Visual Promptingå’ŒIMProvã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡é›†æˆå¤šä¸ªæç¤ºæ¥è¿›ä¸€æ­¥æ”¹å–„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6b9f52a116a3e8ad38c21899aeae99f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a5b6121f8eaf0c9cd32acdb230f9cc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05de99994e6d70901e5a906e6ecc4aa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daff12a80d96d514912762f6540f22bc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models"><a href="#VisCodex-Unified-Multimodal-Code-Generation-via-Merging-Vision-and-Coding-Models" class="headerlink" title="VisCodex: Unified Multimodal Code Generation via Merging Vision and   Coding Models"></a>VisCodex: Unified Multimodal Code Generation via Merging Vision and   Coding Models</h2><p><strong>Authors:Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei</strong></p>
<p>Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨è§†è§‰å’Œæ–‡æœ¬ç†è§£çš„èåˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆä»£ç çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisCodexï¼Œè¿™æ˜¯ä¸€ä¸ªæ— ç¼èåˆè§†è§‰å’Œç¼–ç è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä½¿MLLMå…·å¤‡å¼ºå¤§çš„å¤šæ¨¡æ€ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºä»»åŠ¡å‘é‡çš„æ¨¡å‹èåˆæŠ€æœ¯ï¼Œå°†æœ€å…ˆè¿›çš„ç¼–ç LLMé›†æˆåˆ°å¼ºå¤§çš„è§†è§‰è¯­è¨€ä¸»å¹²ä¸­ï¼ŒåŒæ—¶ä¿ç•™è§†è§‰ç†è§£å’Œé«˜çº§ç¼–ç æŠ€èƒ½ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°è®­ç»ƒï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¤šæ¨¡æ€ç¼–ç æ•°æ®é›†ï¼ˆMCDï¼‰ï¼Œè¿™æ˜¯ä¸€å¤§è§„æ¨¡ä¸”å¤šæ ·åŒ–çš„æ ·æœ¬é›†åˆï¼ŒåŒ…å«59.8ä¸‡ä¸ªæ ·æœ¬ï¼ŒåŒ…æ‹¬é«˜è´¨é‡çš„HTMLä»£ç ã€å›¾è¡¨å›¾åƒä»£ç å¯¹ã€å›¾åƒå¢å¼ºçš„StackOverflowé—®ç­”å’Œç®—æ³•é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†InfiBench-Vè¿™ä¸€æ–°å‹ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å¤„ç†è§†è§‰ä¸°å¯Œã€ç°å®ä¸–ç•Œç¼–ç¨‹é—®é¢˜ä¸Šçš„èƒ½åŠ›ï¼Œè¿™äº›é—®é¢˜è¦æ±‚æ·±åˆ»ç†è§£å’Œè¿ç”¨æ–‡æœ¬å’Œè§†è§‰ä¸Šä¸‹æ–‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVisCodexåœ¨å¼€æºMLLMä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ¥è¿‘ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oçš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ¨¡å‹èåˆç­–ç•¥å’Œæ–°æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09945v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMsé€šè¿‡VisCodexæ¡†æ¶å®ç°äº†è§†è§‰ä¸ç¼–ç¨‹è¯­è¨€æ¨¡å‹çš„èåˆï¼Œæå‡äº†å¤šæ¨¡æ€ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä»»åŠ¡å‘é‡æ¨¡å‹èåˆæŠ€æœ¯ï¼Œå°†å…ˆè¿›çš„ç¼–ç¨‹LLMèå…¥å¼ºå¤§çš„è§†è§‰è¯­è¨€ä¸»å¹²ç½‘ï¼ŒåŒæ—¶ä¿ç•™è§†è§‰ç†è§£å’Œé«˜çº§ç¼–ç¨‹æŠ€èƒ½ã€‚ä¸ºæ”¯æŒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå¼•å…¥äº†å¤šæ¨¡æ€ç¼–ç æ•°æ®é›†ï¼ˆMCDï¼‰ï¼Œå¹¶è®¾ç«‹äº†InfiBench-VåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰ä¸°å¯Œçš„å®é™…ç¼–ç¨‹é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒVisCodexåœ¨å¼€æºMLLMsä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæ¥è¿‘GPT-4oç­‰ä¸“æœ‰æ¨¡å‹ï¼ŒéªŒè¯äº†æ¨¡å‹èåˆç­–ç•¥å’Œæ–°å‹æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsé€šè¿‡VisCodexæ¡†æ¶å®ç°äº†è§†è§‰ä¸ç¼–ç¨‹è¯­è¨€çš„èåˆã€‚</li>
<li>VisCodexé‡‡ç”¨ä»»åŠ¡å‘é‡æ¨¡å‹èåˆæŠ€æœ¯ï¼Œæ•´åˆäº†å…ˆè¿›çš„ç¼–ç¨‹LLMå’Œè§†è§‰è¯­è¨€ä¸»å¹²ç½‘ã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€ç¼–ç æ•°æ®é›†ï¼ˆMCDï¼‰ï¼Œæ”¯æŒVisCodexçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>MCDæ•°æ®é›†åŒ…å«é«˜è´¨é‡HTMLä»£ç ã€å›¾è¡¨å›¾åƒä»£ç å¯¹ã€å›¾åƒå¢å¼ºå‹StackOverflowé—®ç­”å’Œç®—æ³•é—®é¢˜ã€‚</li>
<li>è®¾ç«‹äº†InfiBench-VåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰ä¸°å¯Œçš„ç¼–ç¨‹é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>VisCodexåœ¨å¼€æºMLLMsä¸­è¡¨ç°æœ€ä½³ï¼Œæ¥è¿‘ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00776cbb416a67db4ed26fa95d1c86d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83e5f5e2b4ad9dc577e8be7d958f9063.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Evaluation-framework-of-Alignment-Techniques-for-LLMs"><a href="#A-Comprehensive-Evaluation-framework-of-Alignment-Techniques-for-LLMs" class="headerlink" title="A Comprehensive Evaluation framework of Alignment Techniques for LLMs"></a>A Comprehensive Evaluation framework of Alignment Techniques for LLMs</h2><p><strong>Authors:Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œç¡®ä¿å®ƒä»¬çš„è¾“å‡ºä¸äººç±»ä»·å€¼è§‚å’Œå®‰å…¨æ ‡å‡†ç›¸ä¸€è‡´å·²æˆä¸ºå…³é”®ã€‚è¯¥é¢†åŸŸå·²ç»å¼€å‘äº†å¤šç§å¯¹é½æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼ˆRLHFã€æŒ‡ä»¤è°ƒæ•´ï¼‰ã€äº‹åæ ¡æ­£ç³»ç»Ÿå’Œæ¨ç†æ—¶é—´å¹²é¢„ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿å¾—ç³»ç»Ÿåœ°æ¯”è¾ƒè¿™äº›èŒƒå¼å¹¶æŒ‡å¯¼éƒ¨ç½²å†³ç­–å˜å¾—å›°éš¾ã€‚æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯çš„å¤šç»´åº¦è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ºæ‰€æœ‰ä¸»è¦çš„å¯¹é½èŒƒå¼æä¾›äº†ç³»ç»Ÿçš„æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»å››ä¸ªå…³é”®ç»´åº¦è¯„ä¼°æ–¹æ³•ï¼šå¯¹é½æ£€æµ‹ã€å¯¹é½è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œç¨³å¥æ€§ã€‚é€šè¿‡å¯¹ä¸åŒåŸºç¡€æ¨¡å‹å’Œå¯¹é½ç­–ç•¥çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ¡†æ¶åœ¨è¯†åˆ«å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§æ–¹é¢çš„å®ç”¨æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09937v1">PDF</a> In submission</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é›†æˆåº¦ä¸æ–­æé«˜ï¼Œç¡®ä¿å…¶è¾“å‡ºç¬¦åˆäººç±»ä»·å€¼è§‚å’Œå…¬å…±å®‰å…¨æ ‡å‡†è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¯¹LLMå¯¹é½æŠ€æœ¯çš„å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªè·¨æ‰€æœ‰ä¸»è¦å¯¹é½èŒƒå¼çš„ç³»ç»Ÿæ¯”è¾ƒã€‚è¯„ä¼°æ–¹æ³•åŒ…æ‹¬å››ä¸ªå…³é”®ç»´åº¦ï¼šå¯¹é½æ£€æµ‹ã€å¯¹é½è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œç¨³å¥æ€§ã€‚é€šè¿‡å®éªŒï¼Œè¯¥æ¡†æ¶è¯æ˜äº†å…¶è¯†åˆ«å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹å¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®åº”ç”¨ä¸­çš„é›†æˆåº¦æé«˜ï¼Œéœ€è¦ç¡®ä¿è¾“å‡ºç¬¦åˆäººç±»ä»·å€¼è§‚å’Œå…¬å…±å®‰å…¨æ ‡å‡†ã€‚</li>
<li>ç›®å‰å­˜åœ¨å¤šç§LLMå¯¹é½æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ã€äº‹åæ ¡æ­£ç³»ç»Ÿå’Œæ¨ç†æ—¶é—´å¹²é¢„ç­‰ï¼Œå„æœ‰ä¼˜åŠ¿å’Œå±€é™ã€‚</li>
<li>ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œéš¾ä»¥ç³»ç»Ÿåœ°æ¯”è¾ƒè¿™äº›èŒƒå¼å¹¶åšå‡ºéƒ¨ç½²å†³ç­–ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†ä¸€ä¸ªå¤šç»´åº¦è¯„ä¼°LLMå¯¹é½æŠ€æœ¯çš„æ¡†æ¶ï¼Œæ¶µç›–å¯¹é½æ£€æµ‹ã€å¯¹é½è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œç¨³å¥æ€§å››ä¸ªå…³é”®ç»´åº¦ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºæœªæ¥çš„LLMå¯¹é½ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9c209fde65d61636c34b1d23a5f90a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbd1db1bada69c28f2ca2c9b9e390de8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da1b8b0f56150902ea6617ead343ddc1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Finetuning-Large-Language-Model-as-an-Effective-Symbolic-Regressor"><a href="#Finetuning-Large-Language-Model-as-an-Effective-Symbolic-Regressor" class="headerlink" title="Finetuning Large Language Model as an Effective Symbolic Regressor"></a>Finetuning Large Language Model as an Effective Symbolic Regressor</h2><p><strong>Authors:Yingfan Hua, Ruikun Li, Jun Yao, Guohang Zhuang, Shixiang Tang, Bin Liu, Wanli Ouyang, Yan Lu</strong></p>
<p>Deriving governing equations from observational data, known as Symbolic Regression (SR), is a cornerstone of scientific discovery. Large Language Models (LLMs) have shown promise in this task by leveraging their vast cross-disciplinary scientific knowledge. However, existing LLM-based methods primarily rely on direct inference or prompt engineering, often requiring excessive inference iterations to converge on correct formulas or failing to treating complex equation targets. These limitations in effectiveness and generalization stem from an inherent tension between pre-trained LLMsâ€™ proficiency in approximate reasoning and the high-precision demands of SR tasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR capability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning remains a critical barrier. We thus introduce SymbArena, specifically engineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse equations formulated as corpora of 1.83 billion tokens for LLM utilization, enabling effective training and inference. Further, SymbArena proposes a heuristics metric to precisely quantify form-level consistency, going beyond existing SR numerical-oriented evaluation strategies. With this benchmark, we explore mainstream LLM fine-tuning techniques for SR tasks and establish SymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental results validate SymbolicChat as the first LLM to exceed traditional numerical methods in both numerical precision and symbolic form accuracy, outperforming the second-best LLM baseline with improvements of 2-fold gains in R2 score and 8.37% in form-level consistency score. </p>
<blockquote>
<p>ä»è§‚æµ‹æ•°æ®ä¸­æ¨å¯¼å‡ºæ§åˆ¶æ–¹ç¨‹ï¼Œè¢«ç§°ä¸ºç¬¦å·å›å½’ï¼ˆSRï¼‰ï¼Œæ˜¯ç§‘å­¦å‘ç°çš„æ ¸å¿ƒã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œå®ƒä»¬èƒ½å¤Ÿåˆ©ç”¨è·¨å­¦ç§‘çš„ä¸°å¯Œç§‘å­¦çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç›´æ¥æ¨ç†æˆ–æç¤ºå·¥ç¨‹ï¼Œé€šå¸¸éœ€è¦è¿‡å¤šçš„æ¨ç†è¿­ä»£æ‰èƒ½æ‰¾åˆ°æ­£ç¡®çš„å…¬å¼ï¼Œæˆ–è€…æ— æ³•å¤„ç†å¤æ‚çš„æ–¹ç¨‹ç›®æ ‡ã€‚è¿™äº›æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›çš„å±€é™æ€§æºäºé¢„è®­ç»ƒLLMçš„è¿‘ä¼¼æ¨ç†èƒ½åŠ›ä¸SRä»»åŠ¡çš„é«˜ç²¾åº¦è¦æ±‚ä¹‹é—´çš„å†…åœ¨çŸ›ç›¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºå¯¹LLMè¿›è¡Œå¾®è°ƒä»¥å¢å¼ºå…¶SRèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¼ºä¹ç”¨äºSRå®šå‘å¾®è°ƒçš„ä¸“ç”¨æ•°æ®é›†ä»æ˜¯å…³é”®éšœç¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SymbArenaï¼Œå®ƒæ˜¯ä¸“é—¨ä¸ºä¼˜åŒ–LLMè¿›è¡ŒSRè€Œè®¾è®¡çš„ã€‚æ­¤åŸºå‡†æµ‹è¯•åŒ…å«148,102ä¸ªå¤šæ ·åŒ–çš„æ–¹ç¨‹ï¼Œä½œä¸ºLLMåˆ©ç”¨çš„è¯­æ–™åº“ï¼ŒåŒ…å«1.83äº¿ä¸ªä»¤ç‰Œï¼Œå¯å®ç°æœ‰æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ã€‚æ­¤å¤–ï¼ŒSymbArenaæå‡ºäº†ä¸€ä¸ªå¯å‘å¼åº¦é‡æ ‡å‡†ï¼Œå¯ä»¥ç²¾ç¡®åœ°é‡åŒ–å½¢å¼å±‚é¢çš„ä¸€è‡´æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„SRæ•°å€¼å¯¼å‘è¯„ä¼°ç­–ç•¥ã€‚å€ŸåŠ©æ­¤åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é’ˆå¯¹SRä»»åŠ¡çš„ä¸»æµLLMå¾®è°ƒæŠ€æœ¯ï¼Œå¹¶å»ºç«‹äº†åŸºäºLLMçš„ç¬¦å·èŠå¤©ç®€å•æœ‰æ•ˆçš„å¼ºåŸºçº¿ã€‚å®éªŒç»“æœéªŒè¯äº†ç¬¦å·èŠå¤©ä½œä¸ºç¬¬ä¸€ä¸ªè¶…è¶Šä¼ ç»Ÿæ•°å€¼æ–¹æ³•çš„LLMï¼Œåœ¨æ•°å€¼ç²¾åº¦å’Œç¬¦å·å½¢å¼å‡†ç¡®æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç¬¬äºŒåLLMåŸºå‡†çº¿ï¼Œå…¶åœ¨R2å¾—åˆ†ä¸Šæé«˜äº†ä¸¤å€çš„å¾—åˆ†ï¼Œåœ¨å½¢å¼å±‚é¢çš„ä¸€è‡´æ€§å¾—åˆ†ä¸Šæé«˜äº†8.37%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09897v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºè§‚æµ‹æ•°æ®æ¨å¯¼å‡ºçš„æ§åˆ¶æ–¹ç¨‹ï¼Œå³ç¬¦å·å›å½’ï¼ˆSRï¼‰ï¼Œæ˜¯ç§‘å­¦å‘ç°çš„æ ¸å¿ƒã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿™ä¸€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œå®ƒä»¬å¯ä»¥è¿ç”¨ä¸°å¯Œçš„è·¨å­¦ç§‘ç§‘å­¦çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ–¹æ³•ä¸»è¦ä¾èµ–äºç›´æ¥æ¨ç†æˆ–æç¤ºå·¥ç¨‹ï¼Œè¿™é€šå¸¸éœ€è¦è¿‡å¤šçš„æ¨ç†è¿­ä»£æ¥æ”¶æ•›åˆ°æ­£ç¡®çš„å…¬å¼ï¼Œæˆ–è€…æ— æ³•å¤„ç†å¤æ‚çš„æ–¹ç¨‹ç›®æ ‡ã€‚è¿™äº›åœ¨æ•ˆæœå’Œæ³›åŒ–ä¸Šçš„å±€é™æ€§æºäºé¢„è®­ç»ƒLLMçš„è¿‘ä¼¼æ¨ç†èƒ½åŠ›ä¸SRä»»åŠ¡çš„é«˜ç²¾åº¦è¦æ±‚ä¹‹é—´çš„å†…åœ¨çŸ›ç›¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹LLMè¿›è¡Œå¾®è°ƒä»¥å¢å¼ºå…¶SRèƒ½åŠ›çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç¼ºä¹é¢å‘SRçš„ä¸“ç”¨æ•°æ®é›†ä»æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„é‡å¤§éšœç¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SymbArenaï¼Œå®ƒæ˜¯ä¸“é—¨ä¸ºä¼˜åŒ–LLMè¿›è¡ŒSRè€Œè®¾è®¡çš„ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«148,102ä¸ªå¤šæ ·åŒ–çš„æ–¹ç¨‹ï¼Œä½œä¸ºLLMåˆ©ç”¨çš„è¯­æ–™åº“ï¼ŒåŒ…å«çº¦è¾¾åˆ°äº†åºå¤§çš„1.83äº¿ä¸ªæ ‡è®°ç¬¦å·ã€‚æ­¤å¤–ï¼ŒSymbArenaæå‡ºäº†ä¸€ä¸ªå¯å‘å¼åº¦é‡æ ‡å‡†æ¥ç²¾ç¡®é‡åŒ–å½¢å¼çº§åˆ«çš„ä¸€è‡´æ€§ï¼Œçªç ´äº†ç°æœ‰çš„SRæ•°å€¼è¯„ä¼°ç­–ç•¥çš„é™åˆ¶ã€‚æœ‰äº†è¿™ä¸ªåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é’ˆå¯¹SRä»»åŠ¡çš„ä¸»æµLLMå¾®è°ƒæŠ€æœ¯å¹¶å»ºç«‹äº†SymbolicChatæ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„åŸºäºLLMçš„SRåŸºçº¿æ¨¡å‹ã€‚å®éªŒç»“æœéªŒè¯äº†SymbolicChatä½œä¸ºé¦–ä¸ªè¶…è¶Šä¼ ç»Ÿæ•°å€¼æ–¹æ³•çš„LLMæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ•°å€¼ç²¾åº¦å’Œç¬¦å·å½¢å¼å‡†ç¡®æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºç¬¬äºŒå¥½çš„LLMåŸºçº¿æ¨¡å‹åœ¨RÂ²å¾—åˆ†ä¸Šæé«˜äº†ä¸¤å€å¹¶æé«˜äº†å½¢å¼ä¸€è‡´æ€§å¾—åˆ†è¾¾8.37%ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸ªåŸºå‡†æ¨¡å‹å’Œå¹³å°å°†ä¿ƒè¿›SRä»»åŠ¡çš„è¿›ä¸€æ­¥å‘å±•å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„ç ”ç©¶è¿›æ­¥ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¬¦å·å›å½’ï¼ˆSRï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¾—ç›Šäºå…¶ä¸°å¯Œçš„è·¨å­¦ç§‘ç§‘å­¦çŸ¥è¯†ã€‚</li>
<li>å½“å‰LLMæ–¹æ³•åœ¨SRä»»åŠ¡ä¸­ä¸»è¦é¢ä¸´è¿‡åº¦è¿­ä»£ã€å¤„ç†å¤æ‚æ–¹ç¨‹å›°éš¾ç­‰é—®é¢˜ã€‚</li>
<li>LLMçš„è¿‘ä¼¼æ¨ç†èƒ½åŠ›ä¸SRä»»åŠ¡çš„é«˜ç²¾åº¦éœ€æ±‚ä¹‹é—´å­˜åœ¨çŸ›ç›¾ï¼Œéœ€è¦é€šè¿‡å¾®è°ƒå¢å¼ºLLMçš„SRèƒ½åŠ›ã€‚</li>
<li>ç¼ºä¹é¢å‘SRçš„ä¸“ç”¨æ•°æ®é›†æ˜¯é™åˆ¶LLMåœ¨SRä»»åŠ¡ä¸Šè¡¨ç°çš„å…³é”®éšœç¢ä¹‹ä¸€ã€‚</li>
<li>å¼•å…¥SymbArenaåŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¸ºä¼˜åŒ–LLMè¿›è¡ŒSRæä¾›è§£å†³æ–¹æ¡ˆã€‚è¯¥å¹³å°åŒ…å«å¤§é‡å¤šæ ·åŒ–çš„æ–¹ç¨‹ä½œä¸ºè¯­æ–™åº“ï¼Œå¹¶æä¾›äº†å¯å‘å¼åº¦é‡æ ‡å‡†æ¥ç²¾ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åŸºäºSymbArenaå¹³å°å»ºç«‹çš„SymbolicChatæ¨¡å‹åœ¨SRä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ•°å€¼æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-007f8573cb5b3c218f59a4be27d2c67e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a04b63bbb55e80b47bfa818705687141.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a59adfac396e2189d52685d0104c242e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA"><a href="#RAGulating-Compliance-A-Multi-Agent-Knowledge-Graph-for-Regulatory-QA" class="headerlink" title="RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA"></a>RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA</h2><p><strong>Authors:Bhavik Agarwal, Hemant Sunil Jomraj, Simone Kaplunov, Jack Krolick, Viktoria Rojkova</strong></p>
<p>Regulatory compliance question answering (QA) requires precise, verifiable information, and domain-specific expertise, posing challenges for Large Language Models (LLMs). In this work, we present a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with Retrieval-Augmented Generation (RAG) to address these demands. First, agents build and maintain an ontology-free KG by extracting subjectâ€“predicateâ€“object (SPO) triplets from regulatory documents and systematically cleaning, normalizing, deduplicating, and updating them. Second, these triplets are embedded and stored along with their corresponding textual sections and metadata in a single enriched vector database, allowing for both graph-based reasoning and efficient information retrieval. Third, an orchestrated agent pipeline leverages triplet-level retrieval for question answering, ensuring high semantic alignment between user queries and the factual â€œwho-did-what-to-whomâ€ core captured by the graph. Our hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader audit-focused applications. </p>
<blockquote>
<p>ç›‘ç®¡åˆè§„é—®ç­”ï¼ˆQAï¼‰éœ€è¦å‡†ç¡®ã€å¯éªŒè¯çš„ä¿¡æ¯å’Œç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œè¿™å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºäº†æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç›‘ç®¡ä¸‰å…ƒç»„çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œä»¥æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚é¦–å…ˆï¼Œä»£ç†é€šè¿‡ä»ç›‘ç®¡æ–‡æ¡£ä¸­æå–ä¸»ä½“-è°“è¯­-å®¾è¯­ï¼ˆSPOï¼‰ä¸‰å…ƒç»„æ¥æ„å»ºå’Œç»´æŠ¤æ— æœ¬ä½“çŸ¥è¯†å›¾è°±ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç³»ç»Ÿæ¸…ç†ã€è§„èŒƒåŒ–ã€å»é‡å’Œæ›´æ–°ã€‚å…¶æ¬¡ï¼Œè¿™äº›ä¸‰å…ƒç»„è¢«åµŒå…¥å¹¶ä¸å…¶ç›¸åº”çš„æ–‡æœ¬æ®µè½å’Œå…ƒæ•°æ®ä¸€èµ·å­˜å‚¨åœ¨ä¸€ä¸ªå•ä¸€çš„ä¸°å¯Œå‘é‡æ•°æ®åº“ä¸­ï¼Œå…è®¸åŸºäºå›¾æ¨ç†å’Œé«˜æ•ˆçš„ä¿¡æ¯æ£€ç´¢ã€‚ç¬¬ä¸‰ï¼ŒååŒä»£ç†ç®¡é“åˆ©ç”¨ä¸‰å…ƒç»„çº§åˆ«çš„æ£€ç´¢æ¥è¿›è¡Œé—®ç­”ï¼Œç¡®ä¿ç”¨æˆ·æŸ¥è¯¢ä¸å›¾ä¸­æ•è·çš„â€œè°åšäº†å“ªäº›è¡Œä¸ºå¹¶å½±å“äº†è°â€çš„æ ¸å¿ƒäº‹å®ä¹‹é—´å­˜åœ¨é«˜åº¦è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬çš„æ··åˆç³»ç»Ÿåœ¨å¤æ‚çš„ç›‘ç®¡æŸ¥è¯¢ä¸­ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥çš„ä¸‰å…ƒç»„ç¡®ä¿äº‹å®æ­£ç¡®æ€§ï¼Œé€šè¿‡ç»Ÿä¸€çš„å‘é‡æ•°æ®åº“å®ç°å¯è¿½æº¯æ€§ï¼Œå¹¶é€šè¿‡å­å›¾å¯è§†åŒ–å¢å¼ºç†è§£ï¼Œä¸ºåˆè§„é©±åŠ¨å’Œæ›´å¹¿æ³›çš„å®¡è®¡é‡ç‚¹åº”ç”¨æä¾›äº†ç¨³å¥çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09893v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åº”å¯¹ç›‘ç®¡åˆè§„é—®é¢˜å›ç­”ï¼ˆQAï¼‰æ—¶é¢ä¸´ç²¾ç¡®ä¿¡æ¯å’Œä¸“ä¸šçŸ¥è¯†çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç›‘ç®¡ä¸‰å…ƒç»„çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æ™ºèƒ½ä½“é€šè¿‡ä»ç›‘ç®¡æ–‡æ¡£ä¸­æå–ä¸»ä½“-è°“è¯­-å¯¹è±¡ï¼ˆSPOï¼‰ä¸‰å…ƒç»„æ„å»ºå’Œç»´æŠ¤æ— æœ¬ä½“çŸ¥è¯†å›¾è°±ï¼Œå¹¶è¿›è¡Œæ¸…æ´—ã€å½’ä¸€åŒ–ã€å»é‡å’Œæ›´æ–°ã€‚è¿™äº›ä¸‰å…ƒç»„åµŒå…¥å¹¶ä¸å…¶ç›¸åº”çš„æ–‡æœ¬æ®µè½å’Œå…ƒæ•°æ®ä¸€èµ·å­˜å‚¨åœ¨å•ä¸€çš„ä¸°å¯Œå‘é‡æ•°æ®åº“ä¸­ï¼Œæ”¯æŒåŸºäºå›¾çš„æ¨ç†å’Œé«˜æ•ˆçš„ä¿¡æ¯æ£€ç´¢ã€‚æœ€åï¼Œåˆ©ç”¨ä¸‰å…ƒç»„çº§åˆ«çš„æ£€ç´¢æœºåˆ¶æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œç¡®ä¿ç”¨æˆ·æŸ¥è¯¢ä¸å›¾è°±ä¸­æ•è·çš„â€œè°å¯¹è°åšäº†ä»€ä¹ˆâ€æ ¸å¿ƒäº‹å®ä¹‹é—´çš„é«˜åº¦è¯­ä¹‰å¯¹é½ã€‚è¯¥æ··åˆç³»ç»Ÿä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨å¤æ‚çš„ç›‘ç®¡æŸ¥è¯¢ä¸­ç¡®ä¿äº‹å®æ­£ç¡®æ€§ã€é€šè¿‡ç»Ÿä¸€çš„å‘é‡æ•°æ®åº“å®ç°å¯è¿½æº¯æ€§ï¼Œå¹¶é€šè¿‡å­å›¾å¯è§†åŒ–å¢å¼ºç†è§£ï¼Œä¸ºåˆè§„é©±åŠ¨å’Œæ›´å¹¿æ³›çš„å®¡è®¡é‡ç‚¹åº”ç”¨æä¾›äº†ç¨³å¥çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå¤„ç†ç›‘ç®¡åˆè§„é—®é¢˜å›ç­”çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡SPOä¸‰å…ƒç»„æ„å»ºå’Œç»´æŠ¤çŸ¥è¯†å›¾è°±ï¼Œä»¥é€‚åº”ç›‘ç®¡åˆè§„é¢†åŸŸçš„éœ€æ±‚ã€‚</li>
<li>ä¸°å¯Œçš„å‘é‡æ•°æ®åº“èåˆäº†çŸ¥è¯†å›¾è°±å’Œæ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒåŸºäºå›¾çš„æ¨ç†å’Œä¿¡æ¯æ£€ç´¢ã€‚</li>
<li>åˆ©ç”¨ä¸‰å…ƒç»„çº§åˆ«çš„æ£€ç´¢æœºåˆ¶æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¢å¼ºäº†è¯­ä¹‰å¯¹é½å’Œäº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡å­å›¾å¯è§†åŒ–å¢å¼ºäº†ç†è§£ï¼Œæé«˜äº†å¤æ‚æŸ¥è¯¢çš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>ä¿è¯äº†äº‹å®çš„æ­£ç¡®æ€§å’Œå¯è¿½æº¯æ€§ï¼Œä¼˜äºä¼ ç»Ÿçš„å¤„ç†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-568dfe93864ae0b9949c976db49d73ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8f4947836bfc7c1842e4d8c98b21c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4e2cb7f0ea4b8fa4560d4eb0f7b235a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0604d1794ea3edc8354f76c872b60e7e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving"><a href="#AWorld-Dynamic-Multi-Agent-System-with-Stable-Maneuvering-for-Robust-GAIA-Problem-Solving" class="headerlink" title="AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust   GAIA Problem Solving"></a>AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust   GAIA Problem Solving</h2><p><strong>Authors:Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, Jinjie Gu</strong></p>
<p>The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, as agents increasingly depend on multiple tools, they encounter new challenges: extended contexts from disparate sources and noisy or irrelevant tool outputs can undermine system reliability and accuracy. These challenges underscore the necessity for enhanced stability in agent-based systems. To address this, we introduce dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. In our approach, the Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process, effectively reducing errors arising from noise and bolstering problem-solving robustness. Extensive experiments on the GAIA test dataset reveal that our dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. As a result, our dynamic MAS system achieved first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight the practical value of collaborative agent roles in developing more reliable and trustworthy intelligent systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä½¿å¾—æ™ºèƒ½ä»£ç†èƒ½å¤Ÿåˆ©ç”¨å¤šç§å¤–éƒ¨å·¥å…·æ¥è§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œé—®é¢˜ã€‚ç„¶è€Œï¼Œéšç€ä»£ç†è¶Šæ¥è¶Šä¾èµ–å¤šç§å·¥å…·ï¼Œå®ƒä»¬é¢ä¸´ç€æ–°çš„æŒ‘æˆ˜ï¼šæ¥è‡ªä¸åŒæ¥æºçš„æ‰©å±•ä¸Šä¸‹æ–‡å’Œå˜ˆæ‚æˆ–æ— å…³çš„å·¥å…·è¾“å‡ºå¯èƒ½ä¼šç ´åç³»ç»Ÿå¯é æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™äº›æŒ‘æˆ˜å¼ºè°ƒäº†æé«˜åŸºäºä»£ç†çš„ç³»ç»Ÿçš„ç¨³å®šæ€§çš„å¿…è¦æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€ç›‘ç£å’Œæ“ä½œæœºåˆ¶ï¼Œåœ¨AWorldæ¡†æ¶å†…æ„å»ºäº†ä¸€ä¸ªç¨³å¥ä¸”åŠ¨æ€çš„å¤šä»£ç†ç³»ç»Ÿï¼ˆMASï¼‰æ¶æ„ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæ‰§è¡Œä»£ç†ä¼šåœ¨å…³é”®æ­¥éª¤è°ƒç”¨å®ˆå«ä»£ç†æ¥éªŒè¯å’Œçº æ­£æ¨ç†è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†ç”±å™ªå£°å¼•èµ·çš„é”™è¯¯ï¼Œå¹¶å¢å¼ºäº†è§£å†³é—®é¢˜çš„ç¨³å¥æ€§ã€‚åœ¨GAIAæµ‹è¯•æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŠ¨æ€æ“ä½œæœºåˆ¶æ˜¾è‘—æé«˜äº†è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ï¼Œä¼˜äºå•ä»£ç†ç³»ç»Ÿï¼ˆSASï¼‰å’Œæ ‡å‡†å·¥å…·å¢å¼ºç³»ç»Ÿã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„åŠ¨æ€MASç³»ç»Ÿåœ¨è‘—åçš„GAIAæ’è¡Œæ¦œä¸Šè·å¾—äº†å¼€æºé¡¹ç›®ç¬¬ä¸€åã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åä½œä»£ç†è§’è‰²åœ¨å¼€å‘æ›´å¯é å’Œå¯ä¿¡çš„æ™ºèƒ½ç³»ç»Ÿä¸­çš„å®é™…ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09889v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä½¿å¾—æ™ºèƒ½ä»£ç†èƒ½å¤Ÿåˆ©ç”¨å¤šç§å¤–éƒ¨å·¥å…·è§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œé—®é¢˜ã€‚ç„¶è€Œï¼Œéšç€ä»£ç†è¶Šæ¥è¶Šå¤šåœ°ä¾èµ–å¤šç§å·¥å…·ï¼Œä»–ä»¬é¢ä¸´æ–°çš„æŒ‘æˆ˜ï¼šæ¥è‡ªä¸åŒæ¥æºçš„æ‰©å±•ä¸Šä¸‹æ–‡å’Œå˜ˆæ‚æˆ–æ— å…³çš„å·¥å…·è¾“å‡ºä¼šç ´åç³»ç»Ÿå¯é æ€§å’Œå‡†ç¡®æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€ç›‘ç£æœºåˆ¶å’Œæ“æ§æœºåˆ¶ï¼Œåœ¨AWorldæ¡†æ¶å†…æ„å»ºäº†ä¸€ä¸ªç¨³å¥ä¸”åŠ¨æ€çš„å¤šä»£ç†ç³»ç»Ÿï¼ˆMASï¼‰æ¶æ„ã€‚é€šè¿‡æ‰§è¡Œä»£ç†åœ¨å…³é”®æ­¥éª¤ä¸­è°ƒç”¨é˜²æŠ¤ä»£ç†è¿›è¡ŒéªŒè¯å’Œæ ¡æ­£æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å‡å°‘äº†ç”±å™ªå£°å¼•èµ·çš„é”™è¯¯ï¼Œå¢å¼ºäº†è§£å†³é—®é¢˜çš„ç¨³å¥æ€§ã€‚åœ¨GAIAæµ‹è¯•æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŠ¨æ€æ“æ§æœºåˆ¶æ˜¾è‘—æé«˜äº†è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ï¼Œä¼˜äºå•ä»£ç†ç³»ç»Ÿï¼ˆSASï¼‰å’Œæ ‡å‡†å·¥å…·å¢å¼ºç³»ç»Ÿã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„åŠ¨æ€MASç³»ç»Ÿåœ¨è‘—åçš„GAIAæ’è¡Œæ¦œä¸Šè·å¾—äº†å¼€æºé¡¹ç›®ç¬¬ä¸€åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½æ™ºèƒ½ä»£ç†åˆ©ç”¨å¤šæ ·å¤–éƒ¨å·¥å…·è§£å†³å¤æ‚é—®é¢˜ã€‚</li>
<li>æ™ºèƒ½ä»£ç†åœ¨ä¾èµ–å¤šå·¥å…·æ—¶é¢ä¸´ä¸Šä¸‹æ–‡æ‰©å±•å’Œå·¥å…·è¾“å‡ºå™ªå£°çš„æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦å¢å¼ºä»£ç†ç³»ç»Ÿçš„ç¨³å®šæ€§ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥åŠ¨æ€ç›‘ç£æœºåˆ¶å’Œæ“æ§æœºåˆ¶ï¼Œæ„å»ºç¨³å¥å’ŒåŠ¨æ€çš„å¤šä»£ç†ç³»ç»Ÿï¼ˆMASï¼‰æ¶æ„ã€‚</li>
<li>æ‰§è¡Œä»£ç†é€šè¿‡è°ƒç”¨é˜²æŠ¤ä»£ç†éªŒè¯å’Œæ ¡æ­£æ¨ç†è¿‡ç¨‹ï¼Œæé«˜ç³»ç»Ÿç¨³å¥æ€§ã€‚</li>
<li>åœ¨GAIAæµ‹è¯•æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŠ¨æ€æ“æ§æœºåˆ¶æé«˜è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-80c73558791f10bca819428af1a25ef0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-deddb3ea5996422094aeaf80bb4e909c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4971518c8d91fbacbc933a3bb909cef4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdacbd56ed7c3536c5c998d69ed7b66e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4afcabdb16ee890dd11da85902e3501b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning"><a href="#Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning" class="headerlink" title="Beyond Scaling Law: A Data-Efficient Distillation Framework for   Reasoning"></a>Beyond Scaling Law: A Data-Efficient Distillation Framework for   Reasoning</h2><p><strong>Authors:Xiaojun Wu, Xiaoguang Jiang, Huiyang Li, Jucai Zhai, Dengfeng Liu, Qiaobo Hao, Huang Liu, Zhiguo Yang, Ji Xie, Ninglun Gu, Jin Yang, Kailai Zhang, Yelun Bao, Jun Wang</strong></p>
<p>Large language models (LLMs) demonstrate remarkable reasoning capabilities in tasks such as algorithmic coding and mathematical problem-solving. Recent methods have improved reasoning through expanded corpus and multistage training combining reinforcement learning and supervised fine-tuning. Although some methods suggest that small but targeted dataset can incentivize reasoning via only distillation, a reasoning scaling laws is still taking shape, increasing computational costs. To address this, we propose a data-efficient distillation framework (DED) that optimizes the Pareto frontier of reasoning distillation. Inspired by the on-policy learning and diverse roll-out strategies of reinforcement learning, the key idea of our approach is threefold: (1) We identify that benchmark scores alone do not determine an effective teacher model. Through comprehensive comparisons of leading reasoning LLMs, we develop a method to select an optimal teacher model. (2) While scaling distillation can enhance reasoning, it often degrades out-of-domain performance. A carefully curated, smaller corpus achieves a balanced trade-off between in-domain and out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the student model to develop robust reasoning skills. We validate our method through evaluations on mathematical reasoning (AIME 2024&#x2F;2025, MATH-500) and code generation (LiveCodeBench), achieving state-of-the-art results with only 0.8k carefully curated examples, bypassing the need for extensive scaling. Our systematic analysis demonstrates that DED outperforms existing methods by considering factors beyond superficial hardness, token length, or teacher model capability. This work offers a practical and efficient pathway to advanced reasoning while preserving general capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æ³•ç¼–ç å’Œæ•°å­¦é—®é¢˜è§£å†³ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡æ‰©å¤§è¯­æ–™åº“å’Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å¾®è°ƒçš„å¤šé˜¶æ®µè®­ç»ƒæ¥æ”¹å–„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡ä¸€äº›æ–¹æ³•è®¤ä¸ºï¼Œå°è§„æ¨¡ä½†æœ‰é’ˆå¯¹æ€§çš„æ•°æ®é›†å¯ä»¥é€šè¿‡ä»…é€šè¿‡è’¸é¦æ¥æ¿€åŠ±æ¨ç†ï¼Œä½†æ¨ç†è§„æ¨¡æ³•åˆ™ä»åœ¨å½¢æˆä¸­ï¼Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ•°æ®é«˜æ•ˆè’¸é¦æ¡†æ¶ï¼ˆDEDï¼‰ï¼Œè¯¥æ¡†æ¶ä¼˜åŒ–äº†æ¨ç†è’¸é¦çš„å¸•ç´¯æ‰˜è¾¹ç•Œã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°å¼ºåŒ–å­¦ä¹ çš„åŸºäºç­–ç•¥çš„å­¦ä¹ å’Œå¤šæ ·åŒ–æ»šåŠ¨ç­–ç•¥å¯å‘ï¼Œå…¶å…³é”®æ€æƒ³åŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬å‘ç°ä»…é€šè¿‡åŸºå‡†åˆ†æ•°å¹¶ä¸èƒ½ç¡®å®šæœ‰æ•ˆçš„æ•™å¸ˆæ¨¡å‹ã€‚é€šè¿‡å¯¹é¢†å…ˆçš„æ¨ç†å‹LLMçš„å…¨é¢æ¯”è¾ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‰æ‹©æœ€ä½³æ•™å¸ˆæ¨¡å‹çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰è™½ç„¶æ‰©å¤§è’¸é¦è§„æ¨¡å¯ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒå¾€å¾€ä¼šé™ä½åŸŸå¤–æ€§èƒ½ã€‚ä¸€ä¸ªç²¾å¿ƒæŒ‘é€‰çš„å°å‹è¯­æ–™åº“èƒ½å¤Ÿåœ¨åŸŸå†…å’ŒåŸŸå¤–èƒ½åŠ›ä¹‹é—´å®ç°å¹³è¡¡ã€‚ï¼ˆ3ï¼‰å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹é¼“åŠ±å­¦ç”Ÿæ¨¡å‹å‘å±•ç¨³å¥çš„æ¨ç†æŠ€èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ•°å­¦æ¨ç†ï¼ˆAIME 2024&#x2F;2025ï¼ŒMATH-500ï¼‰å’Œä»£ç ç”Ÿæˆï¼ˆLiveCodeBenchï¼‰æ–¹é¢çš„è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨0.8kä¸ªç²¾å¿ƒæŒ‘é€‰çš„ä¾‹å­å³å¯å®ç°æœ€æ–°æŠ€æœ¯æˆæœï¼Œæ— éœ€å¤§è§„æ¨¡æ‰©å±•ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼ŒDEDåœ¨è€ƒè™‘åˆ°è¶…è¶Šè¡¨é¢éš¾åº¦ã€ä»¤ç‰Œé•¿åº¦æˆ–æ•™å¸ˆæ¨¡å‹èƒ½åŠ›ç­‰å› ç´ æ—¶ï¼Œå…¶è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”é«˜æ•ˆçš„é€”å¾„æ¥å®ç°é«˜çº§æ¨ç†ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09883v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æ³•ç¼–ç å’Œæ•°å­¦é—®é¢˜è§£å†³ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ‰©å¤§è¯­æ–™åº“å’Œå¤šé˜¶æ®µè®­ç»ƒç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å¾®è°ƒçš„æ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³æ¨ç†è§„æ¨¡æ³•åˆ™å¸¦æ¥çš„è®¡ç®—æˆæœ¬å¢åŠ é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆè’¸é¦æ¡†æ¶ï¼ˆDEDï¼‰ï¼Œä¼˜åŒ–äº†æ¨ç†è’¸é¦çš„å¸•ç´¯æ‰˜è¾¹ç•Œã€‚è¯¥æ¡†æ¶å—åˆ°å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥å¯å‘ï¼Œé€šè¿‡é€‰æ‹©æœ€ä½³æ•™å¸ˆæ¨¡å‹ã€å¹³è¡¡è¯­æ–™åº“å¤§å°ä»¥åŠå¤šæ ·åŒ–æ¨ç†è½¨è¿¹ï¼Œå®ç°äº†åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„æœ€æ–°æˆæœï¼Œä»…ä½¿ç”¨ç²¾å¿ƒæŒ‘é€‰çš„0.8kç¤ºä¾‹ï¼Œæ— éœ€å¤§è§„æ¨¡æ‰©å±•ã€‚ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼ŒDEDåœ¨è¶…è¶Šè¡¨é¢éš¾åº¦ã€ä»¤ç‰Œé•¿åº¦æˆ–æ•™å¸ˆæ¨¡å‹èƒ½åŠ›ç­‰å› ç´ çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”é«˜æ•ˆçš„å®ç°é«˜çº§æ¨ç†çš„é€”å¾„ï¼ŒåŒæ—¶ä¿ç•™äº†é€šç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç®—æ³•ç¼–ç å’Œæ•°å­¦é—®é¢˜è§£å†³æ–¹é¢ã€‚</li>
<li>æ‰©å¤§è¯­æ–™åº“å’Œå¤šé˜¶æ®µè®­ç»ƒç»“åˆå¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†æ•°æ®é«˜æ•ˆè’¸é¦æ¡†æ¶ï¼ˆDEDï¼‰æ¥è§£å†³è®¡ç®—æˆæœ¬å¢åŠ çš„é—®é¢˜ï¼Œä¼˜åŒ–æ¨ç†è’¸é¦çš„å¸•ç´¯æ‰˜è¾¹ç•Œã€‚</li>
<li>DEDæ¡†æ¶é€šè¿‡é€‰æ‹©æœ€ä½³æ•™å¸ˆæ¨¡å‹ã€å¹³è¡¡è¯­æ–™åº“å¤§å°å’Œå¤šæ ·åŒ–æ¨ç†è½¨è¿¹æ¥å®ç°ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œä»…ä½¿ç”¨å°‘é‡çš„ç²¾å¿ƒæŒ‘é€‰çš„ç¤ºä¾‹ã€‚</li>
<li>ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼ŒDEDåœ¨å¤šä¸ªå› ç´ ä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-be18b3dc6fb6b4fca9e2223bf16ce82f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d8ca2a1b41507080eb5ca77008fa438.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-930dd47922487cb735156a8b5826bc5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c32012235e6713faace476ff75284799.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1871aee9fc4780d35d7aaa5b3d40dd10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05a1dd9641c1ca0faf22d4206b90c69c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc8a753ca2a511f7f1c81bc236fb5c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6165896e9a1be66acea3260720220765.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification"><a href="#Exploring-the-Potential-of-Large-Language-Models-in-Fine-Grained-Review-Comment-Classification" class="headerlink" title="Exploring the Potential of Large Language Models in Fine-Grained Review   Comment Classification"></a>Exploring the Potential of Large Language Models in Fine-Grained Review   Comment Classification</h2><p><strong>Authors:Linh Nguyen, Chunhua Liu, Hong Yi Lin, Patanamon Thongtanunam</strong></p>
<p>Code review is a crucial practice in software development. As code review nowadays is lightweight, various issues can be identified, and sometimes, they can be trivial. Research has investigated automated approaches to classify review comments to gauge the effectiveness of code reviews. However, previous studies have primarily relied on supervised machine learning, which requires extensive manual annotation to train the models effectively. To address this limitation, we explore the potential of using Large Language Models (LLMs) to classify code review comments. We assess the performance of LLMs to classify 17 categories of code review comments. Our results show that LLMs can classify code review comments, outperforming the state-of-the-art approach using a trained deep learning model. In particular, LLMs achieve better accuracy in classifying the five most useful categories, which the state-of-the-art approach struggles with due to low training examples. Rather than relying solely on a specific small training data distribution, our results show that LLMs provide balanced performance across high- and low-frequency categories. These results suggest that the LLMs could offer a scalable solution for code review analytics to improve the effectiveness of the code review process. </p>
<blockquote>
<p>ä»£ç å®¡æŸ¥æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„ä¸€é¡¹é‡è¦å®è·µã€‚ç”±äºç°åœ¨çš„ä»£ç å®¡æŸ¥è¾ƒä¸ºè½»ä¾¿ï¼Œå¯ä»¥è¯†åˆ«å‡ºå„ç§é—®é¢˜ï¼Œæœ‰æ—¶è¿™äº›é—®é¢˜å¯èƒ½å¾®ä¸è¶³é“ã€‚å·²æœ‰ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨åˆ†ç±»å®¡æŸ¥è¯„è®ºçš„æ–¹æ³•ï¼Œä»¥è¡¡é‡ä»£ç å®¡æŸ¥çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºæœ‰ç›‘ç£çš„æœºå™¨å­¦ä¹ ï¼Œè¿™éœ€è¦å¤§é‡çš„æ‰‹åŠ¨æ³¨é‡Šæ¥æœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä»£ç å®¡æŸ¥è¯„è®ºè¿›è¡Œåˆ†ç±»çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†LLMå¯¹17ç±»ä»£ç å®¡æŸ¥è¯„è®ºè¿›è¡Œåˆ†ç±»çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLMèƒ½å¤Ÿåˆ†ç±»ä»£ç å®¡æŸ¥è¯„è®ºï¼Œå¹¶ä¸”ä¼˜äºä½¿ç”¨è®­ç»ƒå¥½çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒLLMåœ¨åˆ†ç±»äº”ç§æœ€æœ‰ç›Šçš„ç±»åˆ«æ—¶è¾¾åˆ°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œç”±äºè®­ç»ƒå®ä¾‹è¾ƒå°‘ï¼Œæœ€å…ˆè¿›çš„æ–¹æ³•åœ¨è¿™äº›ç±»åˆ«ä¸Šé‡åˆ°äº†å›°éš¾ã€‚ä¸ä»…ä¾èµ–äºç‰¹å®šçš„å°‘é‡è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸åŒï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨é«˜é¢‘å’Œä½é¢‘ç±»åˆ«ä¹‹é—´æä¾›äº†å¹³è¡¡çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMå¯èƒ½ä¸ºä»£ç å®¡æŸ¥åˆ†ææä¾›å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æé«˜ä»£ç å®¡æŸ¥è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09832v1">PDF</a> Accepted at 2025 IEEE International Conference on Source Code   Analysis &amp; Manipulation (SCAM)</p>
<p><strong>Summary</strong><br>ä»£ç å®¡æŸ¥æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦ç¯èŠ‚ã€‚å½“å‰ï¼Œå°½ç®¡ä»£ç å®¡æŸ¥å˜å¾—æ›´ä¸ºè½»é‡çº§ï¼Œä»èƒ½é€šè¿‡å®¡æŸ¥è¯†åˆ«å¤šç§é—®é¢˜ï¼Œç”šè‡³æ˜¯ä¸€äº›å°é—®é¢˜ã€‚ä¹‹å‰çš„ç ”ç©¶å·²å°è¯•ç”¨è‡ªåŠ¨åŒ–æ–¹å¼å¯¹ä»£ç å®¡æŸ¥è¯„è®ºè¿›è¡Œåˆ†ç±»ï¼Œä»¥è¡¡é‡ä»£ç å®¡æŸ¥çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºç›‘ç£æœºå™¨å­¦ä¹ ï¼Œéœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä»£ç å®¡æŸ¥è¯„è®ºè¿›è¡Œåˆ†ç±»çš„æ½œåŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒLLMèƒ½å¤Ÿå¾ˆå¥½åœ°å¯¹ä»£ç å®¡æŸ¥è¯„è®ºè¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼ŒLLMåœ¨äº”ä¸ªæœ€æœ‰ç”¨çš„åˆ†ç±»ç±»åˆ«ä¸­è¡¨ç°æ›´ç²¾ç¡®ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒLLMä¸ä¾èµ–ç‰¹å®šçš„æœ‰é™è®­ç»ƒæ•°æ®åˆ†å¸ƒï¼Œåœ¨é«˜é¢‘å’Œä½é¢‘ç±»åˆ«ä¹‹é—´æä¾›äº†å‡è¡¡çš„æ€§èƒ½è¡¨ç°ã€‚è¿™è¡¨æ˜LLMå¯èƒ½ä¸ºä»£ç å®¡æŸ¥åˆ†ææä¾›å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æé«˜ä»£ç å®¡æŸ¥è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç å®¡æŸ¥æ˜¯è½¯ä»¶å¼€å‘çš„é‡è¦ç¯èŠ‚ï¼Œå¯ä»¥è¯†åˆ«å„ç§é—®é¢˜ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶å·²ç»å°è¯•é€šè¿‡è‡ªåŠ¨åŒ–åˆ†ç±»è¯„ä¼°ä»£ç å®¡æŸ¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç›‘ç£æœºå™¨å­¦ä¹ éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>LLMåœ¨ä»£ç å®¡æŸ¥è¯„è®ºåˆ†ç±»æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>LLMèƒ½å¤Ÿå¾ˆå¥½åœ°å¯¹ä»£ç å®¡æŸ¥è¯„è®ºè¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>LLMåœ¨ç‰¹å®šåˆ†ç±»ç±»åˆ«ä¸­è¡¨ç°æ›´ç²¾ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2d800f951518f7ad27a51ef730dc0623.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb64b9b4e6e73a0c01f2a254a2fdb04c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-097512c3599508ffbbacd34a68e4d77a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80ec2f6fb7421d6cc685a7cb3b86782b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4a54d71825be2b4ca23a7e482b45617.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video"><a href="#ViMoNet-A-Multimodal-Vision-Language-Framework-for-Human-Behavior-Understanding-from-Motion-and-Video" class="headerlink" title="ViMoNet: A Multimodal Vision-Language Framework for Human Behavior   Understanding from Motion and Video"></a>ViMoNet: A Multimodal Vision-Language Framework for Human Behavior   Understanding from Motion and Video</h2><p><strong>Authors:Rajan Das Gupta, Md Yeasin Rahat, Nafiz Fahad, Abir Ahmed, Liew Tze Hui</strong></p>
<p>This study investigates how large language models (LLMs) can be used to understand human behavior using motion and video data. We think that mixing both types is essential to completely capture the nuanced movements and meanings of human actions, in contrast to recent models that simply concentrate on motion data or films. To address this, we provide ViMoNet, a straightforward yet effective framework for comprehending, characterizing, and deducing human action. ViMoNet employs a joint training strategy that leverages the advantages of two data types: detailed motion-text data, which is more exact, and generic video-text data, which is more comprehensive but less detailed. This aids in the modelâ€™s acquisition of rich data regarding time and space in human behavior. Additionally, we provide a brand new dataset named VIMOS that contains a variety of films, motion sequences, instructions, and subtitles. We developed ViMoNet-Bench, a standardized benchmark with carefully labeled samples, to evaluate how well models understand human behavior. Our tests show that ViMoNet outperforms existing methods in caption generation, motion understanding, and behavior interpretation. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åŠ¨ä½œå’Œè§†é¢‘æ•°æ®æ¥ç†è§£äººç±»è¡Œä¸ºã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæ··åˆä¸¤ç§ç±»å‹çš„æ•°æ®å¯¹äºå®Œå…¨æ•æ‰äººç±»è¡Œä¸ºçš„ç»†å¾®åŠ¨ä½œå’Œå«ä¹‰è‡³å…³é‡è¦ï¼Œè¿™ä¸æœ€è¿‘ä»…ä¸“æ³¨äºåŠ¨ä½œæ•°æ®æˆ–ç”µå½±çš„æ¨¡å‹å½¢æˆå¯¹æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†ViMoNetï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºç†è§£ã€è¡¨å¾å’Œæ¨æ–­äººç±»è¡Œä¸ºã€‚ViMoNeté‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ä¸¤ç§æ•°æ®ç±»å‹çš„ä¼˜åŠ¿ï¼šè¯¦ç»†çš„åŠ¨ä½œæ–‡æœ¬æ•°æ®ï¼Œæ›´å‡†ç¡®ï¼›é€šç”¨çš„è§†é¢‘æ–‡æœ¬æ•°æ®ï¼Œæ›´å…¨é¢ä½†ä¸å¤ªè¯¦ç»†ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹è·å¾—æœ‰å…³äººç±»è¡Œä¸ºçš„æ—¶é—´å’Œç©ºé—´çš„ä¸°å¯Œæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¨æ–°çš„æ•°æ®é›†VIMOSï¼Œå…¶ä¸­åŒ…å«å„ç§ç”µå½±ã€è¿åŠ¨åºåˆ—ã€æŒ‡ä»¤å’Œå­—å¹•ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹å¯¹äººç±»è¡Œä¸ºçš„äº†è§£ç¨‹åº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ViMoNet-Benchï¼Œå…¶ä¸­åŒ…å«ç²¾å¿ƒæ ‡è®°çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„æµ‹è¯•è¡¨æ˜ï¼Œåœ¨ç”Ÿæˆæè¿°ã€åŠ¨ä½œç†è§£å’Œè¡Œä¸ºè§£é‡Šæ–¹é¢ï¼ŒViMoNetçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09818v1">PDF</a> Accepted in ICCVDM â€˜25</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åŠ¨ä½œå’Œè§†é¢‘æ•°æ®ç†è§£äººç±»è¡Œä¸ºã€‚æœ¬ç ”ç©¶è®¤ä¸ºæ··åˆä¸¤ç§ç±»å‹çš„æ•°æ®å¯¹äºå®Œå…¨æ•æ‰äººç±»åŠ¨ä½œçš„ç»†å¾®åŠ¨ä½œå’Œå«ä¹‰è‡³å…³é‡è¦ï¼Œä¸æœ€è¿‘ä»…ä¸“æ³¨äºåŠ¨ä½œæ•°æ®æˆ–ç”µå½±çš„æ¨¡å‹å½¢æˆå¯¹æ¯”ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†ViMoNetï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºç†è§£ã€è¡¨å¾å’Œæ¨æ–­äººç±»è¡Œä¸ºã€‚ViMoNeté‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ä¸¤ç§æ•°æ®ç±»å‹çš„ä¼˜åŠ¿ï¼šè¯¦ç»†çš„è¿åŠ¨æ–‡æœ¬æ•°æ®ï¼Œæ›´ç²¾ç¡®ï¼›é€šç”¨çš„è§†é¢‘æ–‡æœ¬æ•°æ®ï¼Œæ›´å…¨é¢ä½†ä¸å¤ªè¯¦ç»†ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹è·å–æœ‰å…³äººç±»è¡Œä¸ºçš„æ—¶é—´å’Œç©ºé—´çš„ä¸°å¯Œæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†åä¸ºVIMOSçš„æ–°æ•°æ®é›†ï¼ŒåŒ…å«å„ç§ç”µå½±ã€è¿åŠ¨åºåˆ—ã€æŒ‡ä»¤å’Œå­—å¹•ã€‚æˆ‘ä»¬å¼€å‘äº†ViMoNet-Benchæ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œå¸¦æœ‰ç²¾å¿ƒæ ‡æ³¨çš„æ ·æœ¬ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹äººç±»è¡Œä¸ºçš„äº†è§£ç¨‹åº¦ã€‚æµ‹è¯•è¡¨æ˜ï¼ŒViMoNetåœ¨ç”Ÿæˆå­—å¹•ã€ç†è§£è¿åŠ¨å’Œè§£é‡Šè¡Œä¸ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£äººç±»è¡Œä¸ºæ–¹é¢çš„åº”ç”¨ï¼Œé€šè¿‡åŠ¨ä½œå’Œè§†é¢‘æ•°æ®çš„ç»“åˆæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ··åˆåŠ¨ä½œå’Œè§†é¢‘æ•°æ®å¯¹äºå®Œå…¨æ•æ‰äººç±»åŠ¨ä½œçš„ç»†å¾®åŠ¨ä½œå’Œå«ä¹‰è‡³å…³é‡è¦ã€‚</li>
<li>æ¨å‡ºæ–°çš„æ¡†æ¶ViMoNetï¼Œç”¨äºç†è§£ã€è¡¨å¾å’Œæ¨æ–­äººç±»è¡Œä¸ºï¼Œé‡‡ç”¨è”åˆè®­ç»ƒç­–ç•¥ç»“åˆä¸¤ç§æ•°æ®ç±»å‹ã€‚</li>
<li>ViMoNetåœ¨å­—å¹•ç”Ÿæˆã€è¿åŠ¨ç†è§£å’Œè¡Œä¸ºè§£é‡Šæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ–°çš„æ•°æ®é›†VIMOSï¼ŒåŒ…å«ç”µå½±ã€è¿åŠ¨åºåˆ—ã€æŒ‡ä»¤å’Œå­—å¹•ç­‰å¤šç§æ•°æ®ç±»å‹ã€‚</li>
<li>å¼€å‘æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ViMoNet-Benchï¼Œç”¨äºè¯„ä¼°æ¨¡å‹å¯¹äººç±»è¡Œä¸ºçš„äº†è§£ç¨‹åº¦ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºä½¿ç”¨LLMç†è§£äººç±»è¡Œä¸ºæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef2031bb315f2ff13227251ffcbd2394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7112567789877d5c7aaf095c2e652ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c18b5a70f67b355199f206bb4ea7b0da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf57b29e3940243e80ed61984af856e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0c3202912cdb0ea90a93eb5cfa976ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0d12fb07b9d76801a41f615f60bdc8d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Signer-Invariant-Conformer-and-Multi-Scale-Fusion-Transformer-for-Continuous-Sign-Language-Recognition"><a href="#A-Signer-Invariant-Conformer-and-Multi-Scale-Fusion-Transformer-for-Continuous-Sign-Language-Recognition" class="headerlink" title="A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for   Continuous Sign Language Recognition"></a>A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for   Continuous Sign Language Recognition</h2><p><strong>Authors:Md Rezwanul Haque, Md. Milon Islam, S M Taslim Uddin Raju, Fakhri Karray</strong></p>
<p>Continuous Sign Language Recognition (CSLR) faces multiple challenges, including significant inter-signer variability and poor generalization to novel sentence structures. Traditional solutions frequently fail to handle these issues efficiently. For overcoming these constraints, we propose a dual-architecture framework. For the Signer-Independent (SI) challenge, we propose a Signer-Invariant Conformer that combines convolutions with multi-head self-attention to learn robust, signer-agnostic representations from pose-based skeletal keypoints. For the Unseen-Sentences (US) task, we designed a Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that captures both fine-grained posture dynamics, enabling the modelâ€™s ability to comprehend novel grammatical compositions. Experiments on the challenging Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US task, the transformer model scores a WER of 47.78%, surpassing previous work. In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th in the SI task, demonstrating the performance of these models. The findings validate our key hypothesis: that developing task-specific networks designed for the particular challenges of CSLR leads to considerable performance improvements and establishes a new baseline for further research. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah">https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah</a>. </p>
<blockquote>
<p>è¿ç»­æ‰‹è¯­è¯†åˆ«ï¼ˆCSLRï¼‰é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ‰‹è¯­è€…ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ä»¥åŠå¯¹æ‰‹è¯­æ–°å¥å­ç»“æ„çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®çš„é—®é¢˜ã€‚ä¼ ç»Ÿè§£å†³æ–¹æ¡ˆå¾€å¾€æ— æ³•æœ‰æ•ˆåœ°å¤„ç†è¿™äº›é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæ¶æ„æ¡†æ¶ã€‚é’ˆå¯¹æ‰‹è¯­è€…ç‹¬ç«‹ï¼ˆSIï¼‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‰‹è¯­ä¸å˜å·ç§¯ï¼ˆSigner-Invariant Conformerï¼‰ï¼Œå®ƒå°†å·ç§¯ä¸å¤šå¤´è‡ªæ³¨æ„åŠ›ç›¸ç»“åˆï¼Œä»åŸºäºå§¿æ€çš„éª¨éª¼å…³é”®ç‚¹å­¦ä¹ ç¨³å¥çš„ã€å¯¹æ‰‹è¯­è€…æ— å…³çš„è¡¨ç¤ºã€‚å¯¹äºæœªè§å¥å­ï¼ˆUSï¼‰ä»»åŠ¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šå°ºåº¦èåˆå˜æ¢å™¨ï¼Œå…·æœ‰æ–°é¢–çš„åŒé€šé“æ—¶åºç¼–ç å™¨ï¼Œèƒ½å¤Ÿæ•æ‰ç²¾ç»†çš„å§¿æ€åŠ¨æ€å˜åŒ–ï¼Œä»è€Œä½¿æ¨¡å‹å…·å¤‡ç†è§£æ–°æ‰‹è¯­è¯­æ³•ç»„åˆçš„èƒ½åŠ›ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„Isharah-1000æ•°æ®é›†ä¸Šçš„å®éªŒä¸ºCSLRåŸºå‡†æµ‹è¯•å»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚æ‰€æå‡ºçš„æ‰‹è¯­ä¸å˜å·ç§¯æ¶æ„åœ¨æ‰‹è¯­è€…ç‹¬ç«‹æŒ‘æˆ˜ä¸Šçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è¾¾åˆ°äº†13.07%ï¼Œæ¯”ç°æœ‰æŠ€æœ¯é™ä½äº†13.53%ã€‚åœ¨æœªè§å¥å­ä»»åŠ¡ä¸­ï¼Œå˜æ¢æ¨¡å‹çš„WERä¸º47.78%ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„å·¥ä½œã€‚åœ¨SignEval 2025 CSLRæŒ‘æˆ˜ä¸­ï¼Œæˆ‘ä»¬çš„å›¢é˜Ÿåœ¨æœªè§å¥å­ä»»åŠ¡ä¸­æ’åç¬¬2ï¼Œåœ¨æ‰‹è¯­è€…ç‹¬ç«‹ä»»åŠ¡ä¸­æ’åç¬¬4ï¼Œè¯æ˜äº†è¿™äº›æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ ¸å¿ƒå‡è®¾ï¼šé’ˆå¯¹CSLRçš„ç‰¹å®šæŒ‘æˆ˜å¼€å‘çš„ä»»åŠ¡ç‰¹å®šç½‘ç»œä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶æä¾›äº†æ–°çš„åŸºå‡†ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharahè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09372v1">PDF</a> Accepted for the IEEE&#x2F;CVF International Conference on Computer Vision   (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¿ç»­æ‰‹è¯­è¯†åˆ«ï¼ˆCSLRï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸åŒç­¾åè€…ä¹‹é—´çš„å·®å¼‚ä»¥åŠå¯¹æ‰‹è¯­æ–°å¥å­ç»“æ„çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„è§£å†³æ–¹æ¡ˆå¾€å¾€æ— æ³•æœ‰æ•ˆåœ°å¤„ç†è¿™äº›é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ¶æ„æ¡†æ¶ã€‚é’ˆå¯¹ç­¾åè€…ç‹¬ç«‹ï¼ˆSIï¼‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ç­¾åè€…æ— å…³å·ç§¯æ¨¡å—ï¼Œç»“åˆäº†å·ç§¯å’Œå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»åŸºäºå§¿æ€çš„éª¨éª¼å…³é”®ç‚¹å­¦ä¹ ç¨³å¥çš„ç­¾åè€…æ— å…³è¡¨ç¤ºã€‚å¯¹äºæœªè§å¥å­ï¼ˆUSï¼‰ä»»åŠ¡ï¼Œè®¾è®¡äº†ä¸€ç§å¤šå°ºåº¦èåˆè½¬æ¢å™¨ï¼Œå…·æœ‰æ–°å‹åŒè·¯å¾„ä¸´æ—¶ç¼–ç å™¨ï¼Œèƒ½å¤Ÿæ•æ‰ç²¾ç»†å§¿æ€åŠ¨æ€ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£æ–°çš„è¯­æ³•ç»„åˆã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„Isharah-1000æ•°æ®é›†ä¸Šçš„å®éªŒä¸ºCSLRåŸºå‡†æµ‹è¯•å»ºç«‹äº†æ–°æ ‡å‡†ã€‚æ‹Ÿè®®çš„å·ç§¯æ¨¡å‹åœ¨SIæŒ‘æˆ˜ä¸Šçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º13.07%ï¼Œæ¯”ç°æœ‰æŠ€æœ¯é™ä½äº†13.53%ã€‚åœ¨USä»»åŠ¡ä¸Šï¼Œtransformeræ¨¡å‹çš„WERå¾—åˆ†ä¸º47.78%ï¼Œè¶…è¿‡äº†ä»¥å‰çš„å·¥ä½œã€‚åœ¨SignEval 2025 CSLRæŒ‘æˆ˜ä¸­ï¼Œæˆ‘ä»¬çš„å›¢é˜Ÿåœ¨USä»»åŠ¡ä¸­è·å¾—äº†ç¬¬äºŒåï¼Œåœ¨SIä»»åŠ¡ä¸­è·å¾—äº†ç¬¬å››åï¼Œè¯æ˜äº†è¿™äº›æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„å…³é”®å‡è®¾ï¼šé’ˆå¯¹CSLRçš„ç‰¹å®šæŒ‘æˆ˜å¼€å‘çš„ä»»åŠ¡ç‰¹å®šç½‘ç»œä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ç ”ç©¶å¥ å®šäº†æ–°åŸºå‡†ã€‚æºä»£ç å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah%E3%80%82">https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharahã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä»‹ç»äº†è¿ç»­æ‰‹è¯­è¯†åˆ«ï¼ˆCSLRï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç­¾åè€…ä¹‹é—´çš„å·®å¼‚å’Œå¯¹æ‰‹è¯­æ–°å¥å­ç»“æ„çš„æ³›åŒ–é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŒæ¶æ„æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬é’ˆå¯¹ç­¾åè€…ç‹¬ç«‹çš„å·ç§¯æ¨¡å—å’Œé’ˆå¯¹æœªè§å¥å­çš„å¤šå°ºåº¦èåˆè½¬æ¢å™¨ã€‚</li>
<li>åœ¨Isharah-1000æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°æå‡ºçš„æ¶æ„åœ¨å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>åœ¨SignEval 2025 CSLRæŒ‘æˆ˜ä¸­å–å¾—äº†è‰¯å¥½æˆç»©ï¼ŒéªŒè¯äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœæ”¯æŒå¼€å‘ä»»åŠ¡ç‰¹å®šç½‘ç»œä»¥æé«˜CSLRæ€§èƒ½çš„è§‚ç‚¹ã€‚</li>
<li>æä¾›äº†æºä»£ç é“¾æ¥ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œå‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30c1a95297ff64bf1c4633f673536eaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dec083949bdebe8440960d1658212073.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e8c163893ebccff4a676ec9a9751f70.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models"><a href="#A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models" class="headerlink" title="A Survey on Parallel Text Generation: From Parallel Decoding to   Diffusion Language Models"></a>A Survey on Parallel Text Generation: From Parallel Decoding to   Diffusion Language Models</h2><p><strong>Authors:Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu</strong></p>
<p>As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at <a target="_blank" rel="noopener" href="https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation">https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation</a>. </p>
<blockquote>
<p>éšç€æ–‡æœ¬ç”Ÿæˆå·²æˆä¸ºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå®ƒä¸ºä¸€ç³»åˆ—ä¸‹æ¸¸åº”ç”¨æä¾›äº†æ”¯æŒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„LLMä¾èµ–äºè‡ªå›å½’ï¼ˆARï¼‰ç”Ÿæˆï¼Œå³åŸºäºå…ˆå‰ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°ç”Ÿæˆä»¤ç‰Œï¼Œè¿™å¯¼è‡´ç”Ÿæˆé€Ÿåº¦å› è¿‡ç¨‹çš„å›ºæœ‰é¡ºåºæ€§è€Œå—åˆ°é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯â€”â€”ä¸€ç±»æ—¨åœ¨æ‰“ç ´é€ä¸ªä»¤ç‰Œç”Ÿæˆçš„ç“¶é¢ˆå¹¶æé«˜æ¨ç†æ•ˆç‡çš„æŠ€æœ¯ã€‚å°½ç®¡å…´è¶£æ—¥ç›Šå¢é•¿ï¼Œä½†å¯¹äºæ„æˆå¹¶è¡Œæ–‡æœ¬ç”Ÿæˆçš„å…·ä½“æŠ€æœ¯ä»¥åŠå¦‚ä½•æé«˜æ¨ç†æ€§èƒ½çš„é—®é¢˜ï¼Œä»ç¼ºä¹å…¨é¢çš„åˆ†æã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿè°ƒæŸ¥ã€‚æˆ‘ä»¬å°†ç°æœ‰æ–¹æ³•åˆ†ç±»ä¸ºåŸºäºARå’ŒéARçš„æ–¹æ³•ï¼Œå¹¶è¯¦ç»†åˆ†æäº†æ¯ä¸€ç±»åˆ«ä¸­çš„æ ¸å¿ƒæŠ€æœ¯ã€‚æ ¹æ®è¿™ç§åˆ†ç±»æ³•ï¼Œæˆ‘ä»¬ä»é€Ÿåº¦ã€è´¨é‡å’Œæ•ˆç‡æ–¹é¢è¯„ä¼°äº†å®ƒä»¬çš„ç†è®ºæƒè¡¡ï¼Œå¹¶è€ƒå¯Ÿäº†å®ƒä»¬ä¸å…¶ä»–åŠ é€Ÿç­–ç•¥çš„æ½œåœ¨ç»„åˆå’Œæ¯”è¾ƒã€‚æœ€åï¼ŒåŸºäºæˆ‘ä»¬çš„ç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æœ€è¿‘çš„è¿›å±•ï¼Œç¡®å®šäº†å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆç ”ç©¶çš„å¸Œæœ›æ–¹å‘ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªGitHubä»“åº“æ¥ç´¢å¼•ç›¸å…³è®ºæ–‡å’Œå¼€æ”¾èµ„æºï¼š<a target="_blank" rel="noopener" href="https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation">https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08712v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯æ–‡æœ¬ç”Ÿæˆï¼Œè¿™æ”¯æŒäº†è®¸å¤šä¸‹æ¸¸åº”ç”¨ã€‚ä½†å¤§å¤šæ•°LLMä¾èµ–äºè‡ªå›å½’ï¼ˆARï¼‰ç”Ÿæˆï¼Œè¿™å¯¼è‡´ç”Ÿæˆé€Ÿåº¦æœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è€…å¼€å§‹æ¢ç´¢å¹¶è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œå°†å…¶åˆ†ä¸ºARå’ŒéARä¸¤ç±»ï¼Œå¹¶è¯¦ç»†åˆ†æäº†æ¯ç§æ–¹æ³•çš„æ ¸å¿ƒæŠ€æœ¯ã€‚è¯„ä¼°äº†å®ƒä»¬åœ¨é€Ÿåº¦ã€è´¨é‡å’Œæ•ˆç‡æ–¹é¢çš„ç†è®ºæƒè¡¡ï¼Œå¹¶æ¢è®¨äº†ä¸å…¶ä»–åŠ é€Ÿç­–ç•¥çš„æ½œåœ¨ç»„åˆå’Œæ¯”è¾ƒã€‚åŸºäºç ”ç©¶ç»“æœï¼Œæœ¬æ–‡å¼ºè°ƒäº†æœ€æ–°è¿›å±•ã€è¯†åˆ«äº†å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆçš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³è®ºæ–‡å’Œå¼€æ”¾èµ„æºå¯è®¿é—®GitHubä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/zhanglingzhe0%2020/Awesome-Parallel-Text-Generation%E3%80%82">https://github.com/zhanglingzhe0<br>  020&#x2F;Awesome-Parallel-Text-Generationã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°ä»£LLMçš„æ ¸å¿ƒèƒ½åŠ›æ˜¯æ–‡æœ¬ç”Ÿæˆï¼Œå¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸åº”ç”¨ã€‚</li>
<li>å¤§å¤šæ•°LLMé‡‡ç”¨è‡ªå›å½’ï¼ˆARï¼‰ç”Ÿæˆï¼Œå­˜åœ¨ç”Ÿæˆé€Ÿåº¦é™åˆ¶ã€‚</li>
<li>å¹³è¡Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯æ—¨åœ¨æ‰“ç ´é€ä¸ªä»¤ç‰Œçš„ç”Ÿæˆç“¶é¢ˆï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>æœ¬æ–‡å¯¹å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿè°ƒæŸ¥ï¼ŒåŒ…æ‹¬ARå’ŒéARä¸¤ç§ç±»å‹çš„æŠ€æœ¯åˆ†æã€‚</li>
<li>åœ¨é€Ÿåº¦ã€è´¨é‡å’Œæ•ˆç‡æ–¹é¢è¿›è¡Œäº†ç†è®ºæƒè¡¡ï¼Œå¹¶æ¢è®¨äº†ä¸å…¶ä»–åŠ é€Ÿç­–ç•¥çš„æ¯”è¾ƒå’Œç»„åˆæ½œåŠ›ã€‚</li>
<li>æ€»ç»“äº†æœ€æ–°è¿›å±•ï¼ŒæŒ‡å‡ºäº†å¼€æ”¾æŒ‘æˆ˜ï¼Œæ¦‚è¿°äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-10f97cd76cf07e33b126d6bc692b969b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-828c5662a399a6138b507110262f35fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4b0a53fbb2e8da780bfd1e5496385ba.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Block-Balancing-Load-in-LLM-Serving-with-Context-Knowledge-and-Predictive-Scheduling"><a href="#Block-Balancing-Load-in-LLM-Serving-with-Context-Knowledge-and-Predictive-Scheduling" class="headerlink" title="Block: Balancing Load in LLM Serving with Context, Knowledge and   Predictive Scheduling"></a>Block: Balancing Load in LLM Serving with Context, Knowledge and   Predictive Scheduling</h2><p><strong>Authors:Wei Da, Evangelia Kalyvianaki</strong></p>
<p>This paper presents Block, a distributed scheduling framework designed to optimize load balancing and auto-provisioning across instances in large language model serving frameworks by leveraging contextual information from incoming requests. Unlike popular model serving systems that rely on monolithic and heuristic task schedulers, Block operates as a fully distributed, stateless, and predictive scheduling system to achieve low overhead, reliability, and scalability. It leverages the deterministic and predictable characteristics of LLM inferences, such as host configurations, response lengths, and hardware performance, to make scheduling decisions based on accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block significantly outperforms heuristic schedulers, boosting serving capacity by up to 16.7% and reducing P99 tail latency by up to 49.5%. These performance gains remain consistent across diverse models, workloads and configurations. Code and data are open-sourced. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Blockï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è°ƒåº¦æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æ¥è‡ªä¼ å…¥è¯·æ±‚çš„ä¸Šæ–‡ä¿¡æ¯ï¼Œåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æœåŠ¡æ¡†æ¶çš„å®ä¾‹ä¹‹é—´ä¼˜åŒ–è´Ÿè½½å‡è¡¡å’Œè‡ªåŠ¨é…ç½®ã€‚ä¸ä¾èµ–å•ä¸€ä¸”å¯å‘å¼ä»»åŠ¡è°ƒåº¦å™¨çš„æµè¡Œæ¨¡å‹æœåŠ¡ç³»ç»Ÿä¸åŒï¼ŒBlockä½œä¸ºä¸€ä¸ªå®Œå…¨åˆ†å¸ƒå¼ã€æ— çŠ¶æ€å’Œé¢„æµ‹æ€§çš„è°ƒåº¦ç³»ç»Ÿï¼Œå®ç°äº†ä½å¼€é”€ã€å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®ƒåˆ©ç”¨LLMæ¨æ–­çš„ç¡®å®šæ€§é¢„æµ‹ç‰¹æ€§ï¼Œå¦‚ä¸»æœºé…ç½®ã€å“åº”é•¿åº¦å’Œç¡¬ä»¶æ€§èƒ½ï¼ŒåŸºäºå‡†ç¡®é¢„æµ‹çš„æŒ‡æ ‡åšå‡ºè°ƒåº¦å†³ç­–ã€‚åœ¨12 GPUé›†ç¾¤ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒBlockæ˜¾è‘—ä¼˜äºå¯å‘å¼è°ƒåº¦å™¨ï¼ŒæœåŠ¡å®¹é‡æå‡é«˜è¾¾16.7%ï¼ŒP99å°¾éƒ¨å»¶è¿Ÿé™ä½é«˜è¾¾49.5%ã€‚è¿™äº›æ€§èƒ½æå‡åœ¨ä¸åŒçš„æ¨¡å‹ã€å·¥ä½œè´Ÿè½½å’Œé…ç½®ä¸‹éƒ½ä¿æŒäº†ä¸€è‡´æ€§ã€‚ä»£ç å’Œæ•°æ®éƒ½å·²å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03611v2">PDF</a> 12 pages, 8 figures excluding appendix. V1: Fix some typos and   grammar issue</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†Blockï¼Œä¸€ä¸ªåˆ†å¸ƒå¼è°ƒåº¦æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ¥è‡ªä¼ å…¥è¯·æ±‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡æ¡†æ¶ä¸­çš„å®ä¾‹è¿›è¡Œè´Ÿè½½å¹³è¡¡å’Œè‡ªåŠ¨é…ç½®ä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–å•ä¸€å¯å‘å¼ä»»åŠ¡è°ƒåº¦å™¨çš„æ¨¡å‹æœåŠ¡ç³»ç»Ÿä¸åŒï¼ŒBlockæ˜¯ä¸€ä¸ªå®Œå…¨åˆ†å¸ƒå¼ã€æ— çŠ¶æ€å’Œé¢„æµ‹æ€§çš„è°ƒåº¦ç³»ç»Ÿï¼Œå¯å®ç°ä½å¼€é”€ã€å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡åˆ©ç”¨LLMæ¨æ–­çš„ç¡®å®šæ€§ï¼Œå¦‚ä¸»æœºé…ç½®ã€å“åº”é•¿åº¦å’Œç¡¬ä»¶æ€§èƒ½ç­‰å‡†ç¡®é¢„æµ‹æŒ‡æ ‡æ¥è¿›è¡Œè°ƒåº¦å†³ç­–ã€‚åœ¨åŒ…å«å¤§å‹è¯­è¨€æ¨¡å‹çš„åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBlockæ˜¾è‘—ä¼˜äºå¯å‘å¼è°ƒåº¦å™¨ï¼Œæœ€å¤šå¯æé«˜æœåŠ¡å®¹é‡è¾¾16.7%ï¼Œå¹¶å°†P99å»¶è¿Ÿé™ä½è‡³æœ€é«˜è¾¾49.5%ã€‚æ— è®ºæ¨¡å‹ã€å·¥ä½œè´Ÿè½½å’Œé…ç½®å¦‚ä½•å˜åŒ–ï¼Œæ€§èƒ½æå‡å‡ä¿æŒä¸€è‡´ã€‚ä»£ç å’Œæ•°æ®å‡å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Blockæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†å¸ƒå¼è°ƒåº¦æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–è´Ÿè½½å¹³è¡¡å’Œè‡ªåŠ¨é…ç½®ã€‚</li>
<li>å®ƒé‡‡ç”¨é¢„æµ‹æ€§è°ƒåº¦å†³ç­–ï¼Œåˆ©ç”¨ä¼ å…¥è¯·æ±‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>Blockç›¸è¾ƒäºä¼ ç»Ÿå¯å‘å¼ä»»åŠ¡è°ƒåº¦å™¨æ›´åŠ åˆ†å¸ƒå¼ã€æ— çŠ¶æ€å’Œé¢„æµ‹æ€§ã€‚</li>
<li>Blockæ¡†æ¶èƒ½å¤Ÿå®ç°ä½å¼€é”€ã€é«˜å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>Blockæ¡†æ¶åˆ©ç”¨LLMæ¨æ–­çš„ç¡®å®šæ€§å› ç´ è¿›è¡Œè°ƒåº¦å†³ç­–ï¼Œå¦‚ä¸»æœºé…ç½®ã€å“åº”é•¿åº¦å’Œç¡¬ä»¶æ€§èƒ½ç­‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒBlockæ˜¾è‘—æé«˜äº†æœåŠ¡å®¹é‡å’Œé™ä½äº†å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5aa07955df0d06a60b3d3d3440babe96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-711ea3a8256e4f2138705e74ba79f537.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4a6dca24055a0b1352c4b776b9c6e69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41e8f637a53f094ce2548a45a1a5b541.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-934303af0fd3db8ea94b50cfabf248e8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SpaCE-10-A-Comprehensive-Benchmark-for-Multimodal-Large-Language-Models-in-Compositional-Spatial-Intelligence"><a href="#SpaCE-10-A-Comprehensive-Benchmark-for-Multimodal-Large-Language-Models-in-Compositional-Spatial-Intelligence" class="headerlink" title="SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models   in Compositional Spatial Intelligence"></a>SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models   in Compositional Spatial Intelligence</h2><p><strong>Authors:Ziyang Gong, Wenhao Li, Oliver Ma, Songyuan Li, Jiayi Ji, Xue Yang, Gen Luo, Junchi Yan, Rongrong Ji</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved remarkable progress in various multimodal tasks. To pursue higher intelligence in space, MLLMs require integrating multiple atomic spatial capabilities to handle complex and dynamic tasks. However, existing benchmarks struggle to comprehensively evaluate the spatial intelligence of common MLLMs from the atomic level to the compositional level. To fill this gap, we present SpaCE-10, a comprehensive benchmark for compositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial capabilities, which are combined to form 8 compositional capabilities. Based on these definitions, we propose a novel hierarchical annotation pipeline to generate high-quality and diverse question-answer (QA) pairs. With over 150+ hours of human expert effort, we obtain over 5k QA pairs for 811 real indoor scenes in SpaCE-10, which covers various evaluation settings like point cloud input and multi-choice QA. We conduct an extensive evaluation of common MLLMs on SpaCE-10 and find that even the most advanced MLLM still lags behind humans by large margins. Through our careful study, we also draw several significant findings that benefit the MLLM community. For example, we reveal that the shortcoming of counting capability greatly limits the compositional spatial capabilities of existing MLLMs. The evaluation code and benchmark datasets are available at <a target="_blank" rel="noopener" href="https://github.com/Cuzyoung/SpaCE-10">https://github.com/Cuzyoung/SpaCE-10</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ä¸ºäº†è¿½æ±‚æ›´é«˜çš„ç©ºé—´æ™ºèƒ½ï¼ŒMLLMséœ€è¦æ•´åˆå¤šç§åŸå­ç©ºé—´èƒ½åŠ›æ¥å¤„ç†å¤æ‚å’ŒåŠ¨æ€çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å¸¸è§MLLMsçš„ç©ºé—´æ™ºèƒ½æ–¹é¢ï¼Œä»åŸå­çº§åˆ«åˆ°ç»„åˆçº§åˆ«éƒ½å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SpaCE-10ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»„åˆç©ºé—´è¯„ä¼°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚åœ¨SpaCE-10ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†10ç§åŸå­ç©ºé—´èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›ç›¸ç»“åˆå½¢æˆäº†8ç§ç»„åˆèƒ½åŠ›ã€‚åŸºäºè¿™äº›å®šä¹‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å±‚æ¬¡æ³¨é‡Šç®¡é“ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„é—®ç­”å¯¹ã€‚ç»è¿‡è¶…è¿‡150å°æ—¶çš„äººåŠ›ä¸“å®¶åŠªåŠ›ï¼Œæˆ‘ä»¬åœ¨SpaCE-10ä¸­è·å¾—äº†è¶…è¿‡5000ä¸ªé—®ç­”å¯¹ï¼Œæ¶‰åŠ811ä¸ªçœŸå®çš„å®¤å†…åœºæ™¯ï¼Œæ¶µç›–äº†å„ç§è¯„ä¼°è®¾ç½®ï¼Œå¦‚ç‚¹äº‘è¾“å…¥å’Œå¤šé¡¹é€‰æ‹©é—®ç­”ã€‚æˆ‘ä»¬å¯¹å¸¸è§çš„MLLMsåœ¨SpaCE-10ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„MLLMä»ç„¶è¿œè¿œè½åäºäººç±»ã€‚é€šè¿‡æˆ‘ä»¬çš„æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬è¿˜å¾—å‡ºäº†ä¸€äº›å¯¹MLLMç¤¾åŒºæœ‰ç›Šçš„é‡å¤§å‘ç°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ­ç¤ºå‡ºè®¡æ•°èƒ½åŠ›çš„ä¸è¶³æå¤§åœ°é™åˆ¶äº†ç°æœ‰MLLMsçš„ç»„åˆç©ºé—´èƒ½åŠ›ã€‚è¯„ä¼°ä»£ç å’ŒåŸºå‡†æµ‹è¯•æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cuzyoung/SpaCE-10">https://github.com/Cuzyoung/SpaCE-10</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07966v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹å…¨é¢çš„ç©ºé—´æ™ºèƒ½åŸºå‡†æµ‹è¯•æ¥è¡¡é‡å…¶åœ¨åŸå­å’Œç»„åˆå±‚é¢ä¸Šçš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æ¨å‡ºäº†SpaCE-10åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…æ‹¬å®šä¹‰çš„ç©ºé—´åŸå­èƒ½åŠ›å’Œç»„åˆèƒ½åŠ›ï¼Œä»¥åŠåŸºäºè¿™äº›å®šä¹‰çš„æ–°å‹åˆ†å±‚æ³¨é‡Šç®¡é“ç”Ÿæˆçš„é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„é—®ç­”å¯¹ã€‚é€šè¿‡å¯¹ç°æœ‰MLLMçš„è¯„ä¼°å‘ç°ï¼Œæœ€å…ˆè¿›çš„æŠ€æœ¯ä»å­˜åœ¨è¾ƒå¤§çš„ç¼ºé™·ï¼Œç ”ç©¶æå‡ºäº†ä¸€äº›æœ‰ç›ŠäºMLLMå‘å±•çš„å…³é”®è§è§£ã€‚å¯¹æ­¤è¯„ä¼°ä»£ç å’ŒåŸºå‡†æ•°æ®é›†å¯è®¿é—®ç‰¹å®šç½‘ç«™è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—è¿›æ­¥ï¼Œä½†ä»éœ€æé«˜ç©ºé—´æ™ºèƒ½èƒ½åŠ›ã€‚</li>
<li>å½“å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢è¡¡é‡MLLMä»åŸå­åˆ°ç»„åˆå±‚é¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>SpaCE-10æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ç»„åˆç©ºé—´æ™ºèƒ½çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚</li>
<li>SpaCE-10å®šä¹‰äº†ç©ºé—´åŸå­èƒ½åŠ›å’Œç»„åˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ–°å‹åˆ†å±‚æ³¨é‡Šç®¡é“ç”Ÿæˆé—®ç­”å¯¹ã€‚</li>
<li>å¯¹ç°æœ‰MLLMåœ¨SpaCE-10ä¸Šçš„è¯„ä¼°å‘ç°å…¶ä»è½åäºäººç±»èƒ½åŠ›æ°´å¹³è¾ƒå¤§ã€‚</li>
<li>è®¡æ•°èƒ½åŠ›çš„ä¸è¶³ä¸¥é‡é™åˆ¶äº†ç°æœ‰MLLMçš„ç»„åˆç©ºé—´èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fee1789155154281550b299f484842cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74d975c9c90ad066f48160b83cb6d7ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03f18f4b61a208a028a00c703b775c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420496678cfa5dc1e1a2622563a04a74.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Efficient-Inference-for-Large-Reasoning-Models-A-Survey"><a href="#Efficient-Inference-for-Large-Reasoning-Models-A-Survey" class="headerlink" title="Efficient Inference for Large Reasoning Models: A Survey"></a>Efficient Inference for Large Reasoning Models: A Survey</h2><p><strong>Authors:Yue Liu, Jiaying Wu, Yufei He, Ruihan Gong, Jun Xia, Liang Li, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi, Stan Z. Li, Keqin Li</strong></p>
<p>Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in solving complex tasks. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. The overview structure of this paper is shown in Figure~\ref{fig:paper_structure}. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from reasoning scenarios, object functions, and performance &amp; efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring the safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMsâ€™ inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field. A collection of efficient reasoning methods for LRMs (papers and codes) is provided at this link: <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs">https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å­¦ä¹ æ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶å®¡æ…çš„æ¨ç†è¿‡ç¨‹å¯¼è‡´äº†ä»¤ç‰Œä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€å†…å­˜æ¶ˆè€—å’Œæ¨ç†æ—¶é—´å»¶é•¿ã€‚å› æ­¤ï¼Œè¿™ç¯‡ç»¼è¿°ä¸“é—¨é’ˆå¯¹LRMsè®¾è®¡çš„é«˜æ•ˆæ¨ç†æ–¹æ³•è¿›è¡Œå›é¡¾ï¼Œé‡ç‚¹å…³æ³¨åœ¨ä¿æŒæ¨ç†è´¨é‡çš„åŒæ—¶ç¼“è§£ä»¤ç‰Œæ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚æœ¬æ–‡çš„æ¦‚è¿°ç»“æ„å¦‚å›¾~\ref{fig:paper_structure}æ‰€ç¤ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªåˆ†ç±»æ³•ï¼Œå°†æœ€è¿‘çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªä¸»è¦ç±»åˆ«ï¼šï¼ˆaï¼‰æ˜ç¡®çš„ç´§å‡‘æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œåœ¨ä¿ç•™æ˜ç¡®æ¨ç†ç»“æ„çš„åŒæ—¶å‡å°‘ä»¤ç‰Œï¼›ï¼ˆbï¼‰éšæ€§çš„æ½œåœ¨CoTï¼Œå°†æ¨ç†æ­¥éª¤ç¼–ç åœ¨éšè—è¡¨ç¤ºä¸­è€Œä¸æ˜¯æ˜ç¡®çš„ä»¤ç‰Œä¸­ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»æ¨ç†åœºæ™¯ã€ç›®æ ‡å‡½æ•°ã€æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†å®è¯åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†è¯¥é¢†åŸŸçš„å¼€æ”¾æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»¥äººä¸ºä¸­å¿ƒçš„å¯æ§æ¨ç†ã€è§£é‡Šæ€§ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€ç¡®ä¿é«˜æ•ˆæ¨ç†çš„å®‰å…¨æ€§å’Œé«˜æ•ˆæ¨ç†çš„å¹¿æ³›åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†é€šè¿‡æ¨¡å‹åˆå¹¶ã€æ–°æ¶æ„å’Œä»£ç†è·¯ç”±å™¨ç­‰æŠ€æœ¯æé«˜LRMsæ¨ç†æ•ˆç‡çš„å…³é”®è§è§£ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä½œä¸ºæœ‰ä»·å€¼çš„æŒ‡å—ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å…‹æœè¿™ä¸€æ´»è·ƒé¢†åŸŸçš„æŒ‘æˆ˜ã€‚æœ‰å…³LRMsçš„é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼ˆè®ºæ–‡å’Œä»£ç ï¼‰çš„é›†åˆå¯åœ¨æ­¤é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs%E3%80%82">https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23077v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ¨ç†å­¦ä¹ æ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶æ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹å¯¼è‡´äº†æ ‡è®°ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€å†…å­˜æ¶ˆè€—å’Œæ¨ç†æ—¶é—´å»¶é•¿ã€‚æœ¬æ–‡å›é¡¾äº†é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è®¾è®¡çš„æœ‰æ•ˆæ¨ç†æ–¹æ³•ï¼Œé‡ç‚¹æ˜¯åœ¨å‡å°‘æ ‡è®°ä½æ•ˆçš„åŒæ—¶ä¿æŒæ¨ç†è´¨é‡ã€‚æ–‡ç« ä»‹ç»äº†åˆ†ç±»æ–¹æ³•ï¼Œå°†æœ€è¿‘çš„æ–¹æ³•åˆ†ä¸ºä¸¤å¤§ç±»ï¼šæ˜¾å¼ç´§å‡‘çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œéšå¼æ½œåœ¨æ€ç»´é“¾ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»æ¨ç†åœºæ™¯ã€ç›®æ ‡å‡½æ•°å’Œæ€§èƒ½æ•ˆç‡æ–¹é¢è¿›è¡Œäº†å®è¯åˆ†æï¼Œå¹¶æå‡ºäº†è¯¥é¢†åŸŸçš„å¼€æ”¾æŒ‘æˆ˜å’Œå…³é”®è§è§£ï¼ŒåŒ…æ‹¬äººç±»ä¸ºä¸­å¿ƒçš„å¯æ§æ¨ç†ã€è§£é‡Šæ€§å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ç­‰ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜åœ¨è¿™ä¸€æ´»è·ƒé¢†åŸŸå…‹æœæŒ‘æˆ˜æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LRMsé€šè¿‡æ¨ç†å­¦ä¹ æé«˜LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè§£å†³å¤æ‚ä»»åŠ¡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>LRMsçš„æ¨ç†è¿‡ç¨‹å­˜åœ¨æ ‡è®°ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€å†…å­˜æ¶ˆè€—å’Œæ¨ç†æ—¶é—´å»¶é•¿çš„é—®é¢˜ã€‚</li>
<li>è®ºæ–‡å¯¹é’ˆå¯¹LRMsçš„æœ‰æ•ˆæ¨ç†æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»å’Œæ¦‚è¿°ï¼Œåˆ†ä¸ºæ˜¾å¼ç´§å‡‘CoTå’Œéšå¼æ½œåœ¨CoTä¸¤å¤§ç±»ã€‚</li>
<li>æ–‡ç« ä»å¤šä¸ªè§’åº¦å¯¹ç°æœ‰çš„æ¨ç†æ–¹æ³•è¿›è¡Œäº†å®è¯åˆ†æï¼ŒåŒ…æ‹¬æ¨ç†åœºæ™¯ã€ç›®æ ‡å‡½æ•°ä»¥åŠæ€§èƒ½æ•ˆç‡ç­‰æ–¹é¢ã€‚</li>
<li>æ–‡ç« æå‡ºäº†å¤šä¸ªé¢†åŸŸçš„å¼€æ”¾æŒ‘æˆ˜å’Œå…³é”®è§è§£ï¼ŒåŒ…æ‹¬å¯æ§æ€§ã€è§£é‡Šæ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5342dcb90be6cf2a3da17fe9cfe5a28b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbc94529a237bd0148b97e7caffe7d8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4cea3dc19f4ef2f2f97cadb1f003b5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b99d08cf6acfb5dd71bdbbecdc3dbdf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ce8e585f018c8b26546f67c59e6592a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a48062935141c31ba4dee4f0ca91e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a8b7006a4baf58fa994485c7c6abb97.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="One-shot-Optimized-Steering-Vectors-Mediate-Safety-relevant-Behaviors-in-LLMs"><a href="#One-shot-Optimized-Steering-Vectors-Mediate-Safety-relevant-Behaviors-in-LLMs" class="headerlink" title="One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in   LLMs"></a>One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in   LLMs</h2><p><strong>Authors:Jacob Dunefsky, Arman Cohan</strong></p>
<p>Steering vectors (SVs) have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing SVs through gradient descent on a single training example, and systematically investigate how these SVs generalize. We consider several SV optimization techniques and find that the resulting SVs effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot SVs that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized SVs can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, we extend work on â€œemergent misalignmentâ€ and show that SVs optimized to induce a model to write vulnerable code cause the model to respond harmfully on unrelated open-ended prompts. Finally, we use one-shot SV optimization to investigate how an instruction-tuned LLM recovers from outputting false information, and find that this ability is independent of the modelâ€™s explicit verbalization that the information was false. Overall, our findings suggest that optimizing SVs on a single example can mediate a wide array of misaligned behaviors in LLMs. Code can be found at <a target="_blank" rel="noopener" href="https://github.com/jacobdunefsky/one-shot-steering-repro">https://github.com/jacobdunefsky/one-shot-steering-repro</a> and <a target="_blank" rel="noopener" href="https://github.com/jacobdunefsky/one-shot-steering-misalignment">https://github.com/jacobdunefsky/one-shot-steering-misalignment</a>. </p>
<blockquote>
<p>è½¬å‘å‘é‡ï¼ˆSVsï¼‰ä½œä¸ºä¸€ç§è§£é‡Šå’Œæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•å·²ç»å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å½“å‰çš„æ–¹æ³•é€šå¸¸éœ€è¦æ„å»ºå¤§é‡çš„å¯¹æ¯”æ•°æ®é›†ï¼Œè¿™é€šå¸¸æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œå¹¶ä¸”å¯èƒ½ä¼šæ•è·é”™è¯¯çš„å…³è”ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡æ¢¯åº¦ä¸‹é™å¯¹ä¸€ä¸ªå•ä¸€è®­ç»ƒç¤ºä¾‹ç›´æ¥ä¼˜åŒ–SVsï¼Œå¹¶ç³»ç»Ÿåœ°ç ”ç©¶è¿™äº›SVsçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è€ƒè™‘äº†å‡ ç§SVä¼˜åŒ–æŠ€æœ¯ï¼Œå¹¶å‘ç°æ‰€å¾—çš„SVså¯ä»¥æœ‰æ•ˆåœ°åœ¨å¤šä¸ªæ¨¡å‹ä¸­è°ƒèŠ‚ä¸å®‰å…¨ç›¸å…³çš„è¡Œä¸ºã€‚å®é™…ä¸Šï¼Œåœ¨å¯¹é½ä¼ªé€ æ¨¡å‹çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¼˜åŒ–ä¸€æ¬¡æ€§çš„SVsï¼Œåœ¨è‰¯æ€§ç¤ºä¾‹ä¸Šå¼•å‘æœ‰å®³è¡Œä¸ºï¼Œè€Œå…¶å¦å®šå½¢å¼åˆ™åœ¨æ¶æ€§ç¤ºä¾‹ä¸ŠæŠ‘åˆ¶æœ‰å®³è¡Œä¸ºã€‚åœ¨æ‹’ç»æŠ‘åˆ¶çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€æ¬¡æ€§ä¼˜åŒ–çš„SVså¯ä»¥åœ¨è¾“å…¥ä¹‹é—´è½¬ç§»ï¼Œå¯¼è‡´Harmbenchæ”»å‡»æˆåŠŸç‡è¾¾åˆ°96.9%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ‰©å±•äº†å…³äºâ€œæ–°å…´é”™ä½â€çš„å·¥ä½œï¼Œå¹¶è¡¨æ˜ä¼˜åŒ–SVsä»¥å¼•å¯¼æ¨¡å‹ç¼–å†™è„†å¼±ä»£ç ä¼šå¯¼è‡´æ¨¡å‹åœ¨æ— å…³çš„å¼€æ”¾å¼æç¤ºä¸Šäº§ç”Ÿæœ‰å®³ååº”ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€æ¬¡æ€§SVä¼˜åŒ–æ¥ç ”ç©¶æŒ‡ä»¤è°ƒæ•´å‹LLMå¦‚ä½•ä»è¾“å‡ºé”™è¯¯ä¿¡æ¯çš„æƒ…å¢ƒä¸­æ¢å¤ï¼Œå¹¶å‘ç°è¿™ç§èƒ½åŠ›ä¸æ¨¡å‹æ˜ç¡®è¡¨è¿°ä¿¡æ¯æ˜¯å‡çš„æ— å…³ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹å•ä¸ªç¤ºä¾‹è¿›è¡Œä¼˜åŒ–å¯ä»¥è°ƒæ•´LLMsä¸­ä¸€ç³»åˆ—é”™ä½çš„è¡Œä¸ºã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jacobdunefsky/one-shot-steering-repro%E5%92%8Chttps://github.com/jacobdunefsky/one-shot-steering-misalignment%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jacobdunefsky/one-shot-steering-reproå’Œhttps://github.com/jacobdunefsky/one-shot-steering-misalignmentä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18862v2">PDF</a> Published at COLM 2025. 30 pages, 7 figures. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/jacobdunefsky/one-shot-steering-repro">https://github.com/jacobdunefsky/one-shot-steering-repro</a> and   <a target="_blank" rel="noopener" href="https://github.com/jacobdunefsky/one-shot-steering-misalignment">https://github.com/jacobdunefsky/one-shot-steering-misalignment</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†æ–°å…´çš„æ–¹å‘å‘é‡ï¼ˆSVsï¼‰åœ¨è§£é‡Šå’Œæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢çš„æ½œåŠ›ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡å¯¹æ¯”æ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå•ä¸ªè®­ç»ƒæ ·æœ¬é€šè¿‡æ¢¯åº¦ä¸‹é™ç›´æ¥ä¼˜åŒ–SVsçš„æ–¹æ³•ï¼Œå¹¶ç³»ç»Ÿç ”ç©¶äº†SVsçš„æ³›åŒ–æ€§èƒ½ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œä¼˜åŒ–çš„SVså¯ä»¥æœ‰æ•ˆåœ°è°ƒæ•´å¤šä¸ªæ¨¡å‹çš„å®‰å…¨ç›¸å…³è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œåœ¨æ¨¡æ‹Ÿå¯¹é½æ¬ºéª—æ¨¡å‹çš„å®éªŒä¸­ï¼Œä¼˜åŒ–çš„SVså¯ä»¥åœ¨è‰¯æ€§æ ·æœ¬ä¸Šè¯±å¯¼æœ‰å®³è¡Œä¸ºï¼Œè€Œå…¶å¦å®šå½¢å¼åˆ™å¯åœ¨æ¶æ€§æ ·æœ¬ä¸ŠæŠ‘åˆ¶æœ‰å®³è¡Œä¸ºã€‚æ­¤å¤–ï¼Œåœ¨æ‹’ç»æŠ‘åˆ¶å®éªŒä¸­ï¼Œå•æ¬¡ä¼˜åŒ–çš„SVså¯åœ¨ä¸åŒè¾“å…¥ä¹‹é—´è¿›è¡Œè½¬ç§»ï¼Œå¯¼è‡´Harmbenchæ”»å‡»æˆåŠŸç‡è¾¾åˆ°96.9%ã€‚æœ¬ç ”ç©¶è¿˜æ¢è®¨äº†â€œçªå‘é”™ä½â€é—®é¢˜ï¼Œå‘ç°é€šè¿‡ä¼˜åŒ–SVsä»¥å¼•å¯¼æ¨¡å‹ç¼–å†™è„†å¼±ä»£ç ä¼šå¯¼è‡´æ¨¡å‹åœ¨æ— å…³çš„å¼€æ”¾å¼æç¤ºä¸Šäº§ç”Ÿæœ‰å®³ååº”ã€‚æœ€åï¼Œæœ¬ç ”ç©¶åˆ©ç”¨å•æ¬¡SVä¼˜åŒ–æ¥æ¢ç©¶æŒ‡ä»¤è°ƒæ•´å‹LLMå¦‚ä½•å¤„ç†è¾“å‡ºè™šå‡ä¿¡æ¯çš„æƒ…å†µï¼Œå‘ç°è¿™ç§èƒ½åŠ›ä¸æ¨¡å‹æ˜ç¡®è¡¨è¿°è™šå‡ä¿¡æ¯æ— å…³ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶è¡¨æ˜åœ¨å•ä¸ªæ ·æœ¬ä¸Šä¼˜åŒ–SVså¯ä»¥è°ƒæ•´LLMsçš„å„ç§é”™ä½è¡Œä¸ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºé€šè¿‡æ¢¯åº¦ä¸‹é™å¯¹å•ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡ŒSVä¼˜åŒ–ï¼Œä»¥è§£é‡Šå’Œæ§åˆ¶LLMsã€‚</li>
<li>ä¼˜åŒ–çš„SVsèƒ½å¤Ÿæœ‰æ•ˆè°ƒæ•´æ¨¡å‹çš„å®‰å…¨ç›¸å…³è¡Œä¸ºã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå¯¹é½æ¬ºéª—æ¨¡å‹çš„å®éªŒä¸­ï¼Œå•æ¬¡ä¼˜åŒ–çš„SVså¯åœ¨è‰¯æ€§åŠæ¶æ€§æ ·æœ¬ä¸Šè¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºæ•ˆæœã€‚</li>
<li>SVsçš„ä¼˜åŒ–å¯ä»¥å¯¼è‡´LLMåœ¨æ‹’ç»æŠ‘åˆ¶å®éªŒä¸­å®ç°é«˜æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>ç ”ç©¶å‘ç°â€œçªå‘é”™ä½â€é—®é¢˜ï¼Œå³ä¼˜åŒ–SVså¼•å¯¼æ¨¡å‹ç¼–å†™è„†å¼±ä»£ç å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æ— å…³æç¤ºä¸Šäº§ç”Ÿæœ‰å®³ååº”ã€‚</li>
<li>å•æ¬¡SVä¼˜åŒ–å¯ç”¨äºæ¢ç©¶LLMå¦‚ä½•å¤„ç†è¾“å‡ºè™šå‡ä¿¡æ¯çš„æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca3dc84db3d0165d618abd69d473ff52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cfff79a8d1409bbf300fb3b2265f89f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71306814e777387f553395cc66148eec.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ALFA-Aligning-LLMs-to-Ask-Good-Questions-A-Case-Study-in-Clinical-Reasoning"><a href="#ALFA-Aligning-LLMs-to-Ask-Good-Questions-A-Case-Study-in-Clinical-Reasoning" class="headerlink" title="ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical   Reasoning"></a>ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical   Reasoning</h2><p><strong>Authors:Shuyue Stella Li, Jimin Mun, Faeze Brahman, Pedram Hosseini, Bryceton G. Thomas, Jessica M. Sin, Bing Ren, Jonathan S. Ilgen, Yulia Tsvetkov, Maarten Sap</strong></p>
<p>Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decision-making. We present ALignment via Fine-grained Attributes, (ALFA) a framework that improves LLM question-asking by (i) decomposing the notion of a â€œgoodâ€ question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SoTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹æ— æ³•æå‡ºæœ‰æ•ˆé—®é¢˜ï¼Œä½¿å…¶åœ¨å†³ç­–ä¸­éœ€è¦ä¸»åŠ¨æ”¶é›†ä¿¡æ¯çš„é¢†åŸŸå˜å¾—ä¸å¯é ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºç²¾ç»†å±æ€§å¯¹é½ï¼ˆALFAï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡ï¼ˆiï¼‰å°†â€œå¥½é—®é¢˜â€çš„æ¦‚å¿µåˆ†è§£ä¸ºä¸€å¥—åŸºäºç†è®ºçš„å±æ€§ï¼ˆå¦‚æ¸…æ™°åº¦ã€ç›¸å…³æ€§ï¼‰ï¼Œï¼ˆiiï¼‰å¯æ§åœ°åˆæˆç‰¹å®šå±æ€§çš„é—®é¢˜å˜ä½“ï¼Œï¼ˆiiiï¼‰é€šè¿‡åŸºäºåå¥½çš„ä¼˜åŒ–å¯¹é½æ¨¡å‹ï¼Œä»¥æ˜ç¡®å­¦ä¹ æ²¿ç€è¿™äº›ç²¾ç»†å±æ€§æå‡ºæ›´å¥½çš„é—®é¢˜ã€‚æˆ‘ä»¬ä»¥ä¸´åºŠæ¨ç†ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œä»‹ç»äº†MediQ-AskDocsæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±åŒ…å«1.7ä¸‡ä»½ç°å®ä¸–ç•Œä¸´åºŠäº’åŠ¨çš„8ä¸‡ä»½ç‰¹å®šå±æ€§åå¥½é…å¯¹è·Ÿè¿›é—®é¢˜ç»„æˆï¼Œä»¥åŠä¸€ä¸ªç”¨äºè¯„ä¼°æé—®èƒ½åŠ›çš„æ–°å‹ä¸“å®¶æ³¨é‡Šäº¤äº’å¼åŒ»ç–—é—®ç­”ä»»åŠ¡ã€‚ä¸å½“å‰æŠ€æœ¯çŠ¶æ€ï¼ˆSoTAï¼‰çš„æŒ‡ä»¤å¾®è°ƒLLMç›¸æ¯”ï¼Œä½¿ç”¨ALFAå¯¹é½çš„æ¨¡å‹åœ¨MediQ-AskDocsä¸Šå°†è¯Šæ–­é”™è¯¯å‡å°‘äº†56.6%ï¼Œé—®é¢˜çº§åˆ«çš„èƒœç‡ä¸º64.4%ï¼Œå¹¶ä¸”å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç»“æ„åŒ–ã€ç²¾ç»†å±æ€§çš„æ˜ç¡®æŒ‡å¯¼æé—®ï¼Œä¸ºæ”¹è¿›LLMæä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸“å®¶åº”ç”¨é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14860v2">PDF</a> 29 pages, 8 figures, 12 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸ç¡®å®šæ€§åœºæ™¯ä¸‹æé—®èƒ½åŠ›æœ‰é™ï¼Œå¯¼è‡´åœ¨éœ€è¦ä¸»åŠ¨è·å–ä¿¡æ¯ä»¥è¾…åŠ©å†³ç­–çš„é¢†åŸŸå¯é æ€§ä¸è¶³ã€‚æœ¬æ–‡æå‡ºALignment via Fine-grained Attributesï¼ˆALFAï¼‰æ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£â€œå¥½é—®é¢˜â€çš„æ¦‚å¿µã€å¯æ§åœ°åˆæˆç‰¹å®šå±æ€§é—®é¢˜å˜ç§ï¼Œä»¥åŠåŸºäºåå¥½çš„æ¨¡å‹å¯¹é½æ–¹å¼ï¼Œæ˜¾å¼åœ°å­¦ä¹ æ²¿è¿™äº›ç²¾ç»†å±æ€§æå‡ºæ›´å¥½é—®é¢˜ã€‚ä»¥ä¸´åºŠæ¨ç†ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†MediQ-AskDocsæ•°æ®é›†ï¼ŒåŒ…å«1.7ä¸‡ä»½çœŸå®ä¸´åºŠäº’åŠ¨æ•°æ®ï¼Œä»¥åŠ8ä¸‡ä»½ç‰¹å®šå±æ€§åå¥½é…å¯¹é—®é¢˜ï¼Œè¿˜æœ‰å…¨æ–°ä¸“å®¶æ ‡æ³¨çš„äº’åŠ¨åŒ»ç–—é—®ç­”ä»»åŠ¡æ¥è¯„ä¼°æé—®èƒ½åŠ›ã€‚ä¸æœ€æ–°æŒ‡ä»¤å‹LLMç›¸æ¯”ï¼Œé‡‡ç”¨ALFAå¯¹é½çš„æ¨¡å‹åœ¨MediQ-AskDocsä¸Šå°†è¯Šæ–­é”™è¯¯ç‡é™ä½äº†56.6%ï¼Œé—®é¢˜çº§åˆ«èƒœç‡ä¸º64.4%ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç»“æ„åŒ–ã€ç²¾ç»†å±æ€§æ˜ç¡®æŒ‡å¯¼æé—®ï¼Œä¸ºæ”¹å–„LLMæä¾›äº†å¯è§„æ¨¡åŒ–è·¯å¾„ï¼Œå°¤å…¶åœ¨ä¸“ä¸šåº”ç”¨é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä¸ç¡®å®šæ€§ä¸‹æé—®èƒ½åŠ›æœ‰é™ï¼Œå°¤å…¶åœ¨éœ€ä¸»åŠ¨è·å–ä¿¡æ¯å†³ç­–é¢†åŸŸã€‚</li>
<li>ALFAæ¡†æ¶é€šè¿‡åˆ†è§£å¥½é—®é¢˜çš„å±æ€§ã€åˆæˆç‰¹å®šå±æ€§é—®é¢˜ã€åŸºäºåå¥½ä¼˜åŒ–æ¨¡å‹æ¥æå‡LLMçš„æé—®èƒ½åŠ›ã€‚</li>
<li>ä»¥ä¸´åºŠæ¨ç†ä¸ºä¾‹ï¼Œå¼•å…¥MediQ-AskDocsæ•°æ®é›†ï¼Œå«çœŸå®ä¸´åºŠäº’åŠ¨æ•°æ®åŠä¸“å®¶æ ‡æ³¨çš„äº’åŠ¨åŒ»ç–—é—®ç­”ä»»åŠ¡ã€‚</li>
<li>ä¸ç°æœ‰LLMç›¸æ¯”ï¼Œé‡‡ç”¨ALFAçš„æ¨¡å‹åœ¨è¯Šæ–­é”™è¯¯ç‡ä¸Šæ˜¾è‘—é™ä½ï¼Œé—®é¢˜çº§åˆ«èƒœç‡é«˜ã€‚</li>
<li>ALFAå¯¹é½æ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ã€‚</li>
<li>ç²¾ç»†å±æ€§æŒ‡å¯¼æé—®æ”¹å–„LLMæ•ˆæœå…·æœ‰å¯è§„æ¨¡åŒ–è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d497f3d8ffb5f10269bdb445ee679fe9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6986b5cae208f5a3a0fe918c24862ed2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RocketKV-Accelerating-Long-Context-LLM-Inference-via-Two-Stage-KV-Cache-Compression"><a href="#RocketKV-Accelerating-Long-Context-LLM-Inference-via-Two-Stage-KV-Cache-Compression" class="headerlink" title="RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache   Compression"></a>RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache   Compression</h2><p><strong>Authors:Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov</strong></p>
<p>Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme. The source code is available here: <a target="_blank" rel="noopener" href="https://github.com/NVlabs/RocketKV">https://github.com/NVlabs/RocketKV</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£ç é˜¶æ®µä¾èµ–äºKVç¼“å­˜æ¥æœ‰æ•ˆå¤„ç†æ‰©å±•çš„ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼ŒKVç¼“å­˜çš„å¤§å°ä¸è¾“å…¥é•¿åº¦æˆæ­£æ¯”å¢é•¿ï¼Œéšç€è§£ç çš„è¿›è¡Œï¼Œå¯¹å†…å­˜å¸¦å®½å’Œå®¹é‡éƒ½é€ æˆäº†è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RocketKVï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„KVç¼“å­˜å‹ç¼©ç­–ç•¥ï¼ŒåŒ…å«ä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå®ƒå¯¹è¾“å…¥åºåˆ—ä»¤ç‰Œæ‰§è¡Œç²—ç²’åº¦æ°¸ä¹…KVç¼“å­˜é€å‡ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå®ƒé‡‡ç”¨æ··åˆç¨€ç–æ³¨æ„åŠ›æ–¹æ³•æ¥è¿›è¡Œç²¾ç»†ç²’åº¦çš„å‰kä¸ªç¨€ç–æ³¨æ„åŠ›ï¼Œé€šè¿‡åˆ©ç”¨å¤´éƒ¨å’Œåºåˆ—ç»´åº¦ç¼©å‡æ¥è¿‘ä¼¼æ³¨æ„åŠ›å¾—åˆ†ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒRocketKVæä¾›äº†é«˜è¾¾400å€çš„å‹ç¼©æ¯”ï¼Œä¸å®Œæ•´çš„KVç¼“å­˜åŸºçº¿ç›¸æ¯”ï¼Œåœ¨NVIDIA A100 GPUä¸Šçš„è§£ç é˜¶æ®µç«¯åˆ°ç«¯é€Ÿåº¦æé«˜äº†é«˜è¾¾3.7å€ï¼Œå³°å€¼å†…å­˜å‡å°‘äº†é«˜è¾¾32.6%ï¼ŒåŒæ—¶åœ¨å„ç§é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šå®ç°äº†å¯å¿½ç•¥çš„ç²¾åº¦æŸå¤±ã€‚æˆ‘ä»¬è¿˜ä¸ºå¤šè½®åœºæ™¯æå‡ºäº†RocketKVçš„å˜ä½“ï¼Œå®ƒå§‹ç»ˆä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å‡ ä¹è¾¾åˆ°ç†æƒ³çš„å‰kä¸ªæ³¨æ„åŠ›æ–¹æ¡ˆçš„ç²¾åº¦ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NVlabs/RocketKV%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NVlabs/RocketKVä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14051v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£ç é˜¶æ®µä¾èµ–KVç¼“å­˜é«˜æ•ˆå¤„ç†æ‰©å±•ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼ŒKVç¼“å­˜å¤§å°éšè¾“å…¥é•¿åº¦æˆæ¯”ä¾‹å¢é•¿ï¼Œç»™å†…å­˜å¸¦å®½å’Œå®¹é‡å¸¦æ¥è´Ÿæ‹…ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RocketKVï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„KVç¼“å­˜å‹ç¼©ç­–ç•¥ï¼ŒåŒ…å«ä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µå¯¹è¾“å…¥åºåˆ—æ ‡è®°è¿›è¡Œç²—ç•¥çš„æ°¸ä¹…KVç¼“å­˜é©±é€ï¼Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ··åˆç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è¿›è¡Œç²¾ç»†çš„top-kç¨€ç–æ³¨æ„åŠ›ï¼Œé€šè¿‡åˆ©ç”¨å¤´éƒ¨å’Œåºåˆ—ç»´åº¦ç¼©å‡æ¥è¿‘ä¼¼æ³¨æ„åŠ›åˆ†æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒRocketKVæä¾›äº†é«˜è¾¾400å€çš„å‹ç¼©æ¯”ï¼Œç«¯åˆ°ç«¯é€Ÿåº¦æé«˜äº†3.7å€ï¼Œè§£ç é˜¶æ®µçš„å³°å€¼å†…å­˜å‡å°‘äº†32.6%ï¼ŒåŒæ—¶åœ¨å„ç§é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æŸå¤±å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬è¿˜ä¸ºRocketKVæ¨å‡ºäº†é’ˆå¯¹å¤šè½®åœºæ™¯çš„ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å§‹ç»ˆä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚æºä»£ç å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/NVlabs/RocketKV">https://github.com/NVlabs/RocketKV</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è§£ç é˜¶æ®µä¾èµ–KVç¼“å­˜ï¼Œä½†å†…å­˜éœ€æ±‚éšè¾“å…¥é•¿åº¦å¢é•¿ã€‚</li>
<li>RocketKVæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„KVç¼“å­˜å‹ç¼©ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³å†…å­˜è´Ÿæ‹…é—®é¢˜ã€‚</li>
<li>RocketKVåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µè¿›è¡Œç²—ç•¥çš„æ°¸ä¹…ç¼“å­˜é©±é€ï¼Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ··åˆç¨€ç–æ³¨æ„åŠ›è¿›è¡Œç²¾ç»†å¤„ç†ã€‚</li>
<li>RocketKVæä¾›äº†é«˜è¾¾400å€çš„å‹ç¼©æ¯”å’Œæ˜¾è‘—çš„ç«¯åˆ°ç«¯é€Ÿåº¦æå‡ã€‚</li>
<li>RocketKVèƒ½æ˜¾è‘—å‡å°‘è§£ç é˜¶æ®µçš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®ç‡ã€‚</li>
<li>RocketKVæœ‰é’ˆå¯¹å¤šè½®åœºæ™¯çš„ç‰ˆæœ¬ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14f02daf499c5de6dbf6d9eab5488995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d33a663c817a25eb3f9ca481233601f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ca26d53f8a0a8017678de3ce7840689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca7cee9db4ce3ab6ee9c860796163b84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fc227b8bc6f19f964d76d76887fbaf2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-103dac24a6da8fff5b49c7aebd263642.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-84bd85b3a4fa71e30770edb169a9c624.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  RAGulating Compliance A Multi-Agent Knowledge Graph for Regulatory QA
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2e39c3f7e03328e375be8c865853e45d.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  A Comprehensive Evaluation framework of Alignment Techniques for LLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32298.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
