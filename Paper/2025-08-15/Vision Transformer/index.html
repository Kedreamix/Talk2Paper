<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-15  SemPT Semantic Prompt Tuning for Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-99a8ecabb0b06fa71d017986b21d72f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-01
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-15-更新"><a href="#2025-08-15-更新" class="headerlink" title="2025-08-15 更新"></a>2025-08-15 更新</h1><h2 id="SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models"><a href="#SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="SemPT: Semantic Prompt Tuning for Vision-Language Models"></a>SemPT: Semantic Prompt Tuning for Vision-Language Models</h2><p><strong>Authors:Xiao Shi, Yangjun Ou, Zhenzhong Chen</strong></p>
<p>Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between preserving category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on sparse category labels or disparate LLM-generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide LLM in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning. </p>
<blockquote>
<p>视觉迁移学习对于未见类别是一个热门的研究课题，同时也是一项具有挑战性的任务，因为保持特定类别的表示和获取可迁移知识之间存在固有的冲突。预训练在大量图像文本对上的视觉语言模型（VLMs）提供了一个有前景的解决方案。然而，现有的提示调整方法依赖于稀疏的类别标签或分散的LLM生成描述，这导致知识表示碎片化并阻碍知识的迁移。为了解决这一局限性，我们引入了语义提示调整（SemPT）这一新框架，它通过利用跨类别的共享属性级知识来解决泛化挑战。具体来说，SemPT采用两步提示策略来引导LLM提取共享的视觉属性并生成属性级描述，在确保连贯结构的同时，捕捉超越标签的可迁移语义线索。然后，对属性级描述的嵌入应用视觉引导加权，以减少来自无关属性的噪声并增强文本嵌入。此外，图像嵌入与标签和属性增强文本嵌入共同对齐，平衡对可见类别的区分度和对未见类别的可迁移性。考虑到类别暴露的可用性，我们的推理会动态选择在可见类别时使用标准的标签嵌入，在未见类别时使用属性增强嵌入，以确保有效的适应。在15个基准数据集上的大量实验表明，SemPT在各种设置下均达到了最先进的性能，包括基础到新颖的泛化、跨数据集迁移、跨域迁移和少样本学习。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10645v1">PDF</a> </p>
<p><strong>Summary</strong><br>视觉迁移学习对于未见类别是一个活跃的研究课题，但也是一个具有挑战性的任务。预训练在大量图像文本对上的视觉语言模型（VLMs）为解决此问题提供了有前景的解决方案。然而，现有的提示调整方法依赖于稀疏的类别标签或分散的LLM生成描述，这破坏了知识表示并阻碍了可转移性。为了解决这一局限性，我们引入了语义提示调整（SemPT）这一新框架，它通过利用类别之间的共享属性级知识来解决泛化挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉迁移学习对于未见类别是一个挑战，因为需要平衡保持类别特定表示和获取可转移知识之间的矛盾。</li>
<li>预训练的视觉语言模型（VLMs）是解决此问题的有前途的方法。</li>
<li>现有提示调整方法的局限性在于它们依赖于稀疏的类别标签或LLM生成的描述，这阻碍了知识的转移。</li>
<li>SemPT框架通过利用跨类别的共享属性级知识来解决泛化挑战。</li>
<li>SemPT采用两步提示策略，引导LLM提取共享视觉属性和生成属性级描述，捕捉可转移语义线索。</li>
<li>通过视觉引导的加权方法，减少无关属性的噪声并增强文本嵌入。</li>
<li>SemPT在多个基准数据集上的实验表明，它在各种设置下实现了最先进的性能，包括从基础到新颖的泛化、跨数据集转移、跨域转移和少样本学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10645">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1616057c449bdf030c62fc7153ded7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a7e3f73e77e13e56ee03baff6258bc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5722113344929679e94d6ba228691cbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-137d70338d2d00a469864a7d66ec70db.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images"><a href="#GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images" class="headerlink" title="GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For   Salient Object Detection in Optical Remote Sensing Images"></a>GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For   Salient Object Detection in Optical Remote Sensing Images</h2><p><strong>Authors:Mengyu Ren, Yutong Li, Hua Li, Runmin Cong, Sam Kwong</strong></p>
<p>Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the model’s structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the decoder of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mamba’s local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority. </p>
<blockquote>
<p>在光学遥感图像（ORSIs）中，显著目标检测（SOD）面临诸多挑战，包括目标尺度的显著差异以及目标与背景之间对比度低的问题。现有的基于视觉变压器（ViTs）和卷积神经网络（CNNs）架构的方法旨在利用全局和局部特征，但有效集成这些异质特征的难度限制了它们的整体性能。为了克服这些局限性，我们提出了图增强上下文和区域感知网络（GCRPNet），它基于Mamba架构，能够同时捕捉长距离依赖关系并增强区域特征表示。具体来说，我们采用视觉状态空间（VSS）编码器提取多尺度特征。为了进一步实现这些特征的深度引导和增强，我们首先设计了一个差异相似性引导分层图注意力模块（DS-HGAM）。该模块增强了不同尺度特征之间的跨层交互能力，提高了模型的结构感知能力，使其更有效地区分前景和背景。然后，我们设计了LEVSS块作为GCRPNet的解码器。该模块结合了我们的自适应扫描策略和多粒度协同注意力增强模块（MCAEM）。它对通过多尺度卷积处理的特征图执行自适应补丁扫描，从而捕获丰富的局部区域信息并增强Mamba的局部建模能力。大量的实验结果证明了所提出模型达到了最先进的性能，验证了其有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10542v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了在光学遥感图像中进行显著性目标检测的挑战。提出的GCRPNet模型采用图增强上下文与区域感知网络结构，基于Mamba架构实现长距离依赖捕获与区域特征表示增强。利用视觉状态空间编码器提取多尺度特征，并设计差异相似性引导层次图注意力模块强化跨层交互能力，提升模型的结构感知力。此外，设计了LEVSS解码块结合自适应扫描策略和多粒度协作注意力增强模块，增强局部建模能力。实验结果显示该模型性能达到最新水平。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>GCRPNet解决了光学遥感图像显著性目标检测中目标尺度变化和背景对比度低的问题。</li>
<li>模型结合了全局和局部特征提取技术，并采用图增强上下文与区域感知网络进行优化。</li>
<li>通过视觉状态空间编码器实现多尺度特征的提取。</li>
<li>差异相似性引导层次图注意力模块强化了不同尺度特征间的跨层交互能力，提升了模型的结构感知能力。</li>
<li>LEVSS解码块结合了自适应扫描策略和多粒度协作注意力增强模块，能有效捕捉丰富的局部区域信息。</li>
<li>实验结果证明了模型的有效性和优越性，达到了最新性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10542">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-75960cd094ac015390df721288ebcf85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe4631d550a03dce9b0401ce6e952bbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef5f9b3198268884039c925e3b11eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b06639abc14d543a39388a56ff6e833e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver"><a href="#ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver" class="headerlink" title="ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot   Perceiver"></a>ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot   Perceiver</h2><p><strong>Authors:Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Haodong Yan, Yuxin Huang, Feilong Tang, Donglin Wang, Haoang Li</strong></p>
<p>Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions. Instead, visual attention is always dispersed. To guide the visual attention grounding on the correct target, we propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. Conditioned on the model’s visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects. This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation. Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the model’s generalization in visual reconstruction. Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization. Our project page is <a target="_blank" rel="noopener" href="https://zionchow.github.io/ReconVLA/">https://zionchow.github.io/ReconVLA/</a>. </p>
<blockquote>
<p>近期，视觉-语言-动作（VLA）模型的进步使得机器人能够整合多模式理解与动作执行。然而，我们的实证分析显示，当前的VLA在分配视觉注意力到目标区域时面临困难。相反，视觉注意力总是分散的。为了引导视觉注意力正确地定位到目标上，我们提出了ReconVLA，这是一种具有隐性定位范式的重建型VLA模型。基于模型的视觉输出，扩散变压器旨在重建图像中的注视区域，该区域对应于被操纵的目标对象。这一过程促使VLA模型学习精细的表征并准确地分配视觉注意力，从而有效地利用任务特定的视觉信息并进行精确操作。此外，我们创建了一个大规模预训练数据集，包含超过10万个轨迹和来自开源机器人数据集的200万个数据样本，进一步提升了模型在视觉重建中的泛化能力。仿真和现实世界的大量实验证明了我们隐性定位方法的优越性，展示了其精确操作和泛化的能力。我们的项目页面是<a target="_blank" rel="noopener" href="https://zionchow.github.io/ReconVLA/%E3%80%82">https://zionchow.github.io/ReconVLA/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10333v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在机器人视觉领域的一项新研究——ReconVLA模型，该模型解决了机器人对于目标区域的视觉注意力分配问题。研究发现当前VLA模型存在视觉注意力分散的问题，因此提出使用扩散变压器重建图像中的注视区域，引导模型学习精细的表征并准确分配视觉注意力。此外，该研究还构建了一个大规模预训练数据集，并通过仿真和真实世界的实验验证了其精确操控和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLA模型在机器人领域面临视觉注意力分配问题。</li>
<li>ReconVLA模型通过重建图像中的注视区域来解决这一问题。</li>
<li>扩散变压器在模型中的作用是依据模型的视觉输出来重建注视区域。</li>
<li>该方法使模型能够学习精细的表征并准确分配视觉注意力。</li>
<li>研究构建了一个大规模的预训练数据集，用于提升模型的泛化能力。</li>
<li>仿真和真实世界的实验验证了ReconVLA模型的精确操控能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10333">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e468733f60157090ceb5a6ad170d6859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5447ba29cd2eb84b9ea059c085ba1d05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c2556ad0b523f3f5cc53b3d2b3f8b7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfa824e1f3ccf9d0132800600f8cf9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbfaabe7eec0d5c9e50c0c87072ce448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abf06f576de55c16a28900a5e3de8ec0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2486339f613bcf180d3771061f1b0de.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RelayFormer-A-Unified-Local-Global-Attention-Framework-for-Scalable-Image-and-Video-Manipulation-Localization"><a href="#RelayFormer-A-Unified-Local-Global-Attention-Framework-for-Scalable-Image-and-Video-Manipulation-Localization" class="headerlink" title="RelayFormer: A Unified Local-Global Attention Framework for Scalable   Image and Video Manipulation Localization"></a>RelayFormer: A Unified Local-Global Attention Framework for Scalable   Image and Video Manipulation Localization</h2><p><strong>Authors:Wen Huang, Jiarui Yang, Tao Dai, Jiawei Li, Shaoxiong Zhan, Bin Wang, Shu-Tao Xia</strong></p>
<p>Visual manipulation localization (VML) – across both images and videos – is a crucial task in digital forensics that involves identifying tampered regions in visual content. However, existing methods often lack cross-modal generalization and struggle to handle high-resolution or long-duration inputs efficiently.   We propose RelayFormer, a unified and modular architecture for visual manipulation localization across images and videos. By leveraging flexible local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables scalable, resolution-agnostic processing with strong generalization. Our framework integrates seamlessly with existing Transformer-based backbones, such as ViT and SegFormer, via lightweight adaptation modules that require only minimal architectural changes, ensuring compatibility without disrupting pretrained representations.   Furthermore, we design a lightweight, query-based mask decoder that supports one-shot inference across video sequences with linear complexity. Extensive experiments across multiple benchmarks demonstrate that our approach achieves state-of-the-art localization performance, setting a new baseline for scalable and modality-agnostic VML. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/WenOOI/RelayFormer">https://github.com/WenOOI/RelayFormer</a>. </p>
<blockquote>
<p>视觉操控定位（VML）——无论是图片还是视频——是数字取证中的一项重要任务，涉及识别视觉内容中被篡改的区域。然而，现有方法往往缺乏跨模态泛化能力，难以高效处理高分辨率或长时间输入的图像。我们提出了RelayFormer，这是一种用于图片和视频视觉操控定位的统一模块化架构。它通过利用灵活的局部单元和全局-局部中继注意力（GLoRA）机制，实现了可伸缩的、与分辨率无关的处理，并具有较强的泛化能力。我们的框架通过轻量级适配模块无缝集成现有的基于Transformer的主干网络，如ViT和SegFormer，只需进行最小的架构更改，确保与预训练表示兼容而不会造成干扰。此外，我们设计了一个轻量级的、基于查询的掩膜解码器，支持在视频序列上进行一次推断，具有线性复杂度。在多个基准测试上的广泛实验表明，我们的方法达到了最先进的定位性能，为可扩展和模态无关的VML设定了新的基准。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/WenOOI/RelayFormer">https://github.com/WenOOI/RelayFormer</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09459v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RelayFormer是一种用于图像和视频篡改区域定位的统一模块化架构。它通过灵活的局部单元和全局-局部中继注意力（GLoRA）机制，实现了可伸缩的、分辨率无关的处理，并具有较强的泛化能力。该框架通过轻量级适配模块与现有的Transformer基础架构（如ViT和SegFormer）无缝集成，只需进行最小的架构更改，确保了与预训练表示的兼容性。此外，设计了一个轻量级的基于查询的掩膜解码器，支持在视频序列上进行一次推理，具有线性复杂度。在多基准的大量实验表明，该方法达到了最新的定位性能，为可伸缩和模态无关的篡改区域定位设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RelayFormer是一个用于视觉篡改定位的统一模块化架构，适用于图像和视频。</li>
<li>利用灵活的局部单元和全局-局部中继注意力（GLoRA）机制，实现分辨率无关的处理。</li>
<li>RelayFormer具有强大的泛化能力，能处理各种模态的视觉内容。</li>
<li>该框架通过轻量级适配模块与现有Transformer架构兼容。</li>
<li>设计了一个轻量级的掩膜解码器，支持一次处理整个视频序列。</li>
<li>RelayFormer达到了最新的定位性能基准。</li>
<li>代码已公开在GitHub上供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cb9d320cca697ab63237d5fcd49157f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4b0c7f5e95df3a15bcb8138ac830398.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ca99a6758bf8def559b9d67b5b27725.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c477333f2b799d76a6afaf3007714e13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2959992e16ca922fcbef420f783e9ed1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c4ae5bf07112b90b595b4f7b3f5e919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee0cd89b2329072b8ab9469a7059647.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Explainable-to-Explained-AI-Ideas-for-Falsifying-and-Quantifying-Explanations"><a href="#From-Explainable-to-Explained-AI-Ideas-for-Falsifying-and-Quantifying-Explanations" class="headerlink" title="From Explainable to Explained AI: Ideas for Falsifying and Quantifying   Explanations"></a>From Explainable to Explained AI: Ideas for Falsifying and Quantifying   Explanations</h2><p><strong>Authors:Yoni Schirris, Eric Marcus, Jonas Teuwen, Hugo Horlings, Efstratios Gavves</strong></p>
<p>Explaining deep learning models is essential for clinical integration of medical image analysis systems. A good explanation highlights if a model depends on spurious features that undermines generalization and harms a subset of patients or, conversely, may present novel biological insights. Although techniques like GradCAM can identify influential features, they are measurement tools that do not themselves form an explanation. We propose a human-machine-VLM interaction system tailored to explaining classifiers in computational pathology, including multi-instance learning for whole-slide images. Our proof of concept comprises (1) an AI-integrated slide viewer to run sliding-window experiments to test claims of an explanation, and (2) quantification of an explanation’s predictiveness using general-purpose vision-language models. The results demonstrate that this allows us to qualitatively test claims of explanations and can quantifiably distinguish competing explanations. This offers a practical path from explainable AI to explained AI in digital pathology and beyond. Code and prompts are available at <a target="_blank" rel="noopener" href="https://github.com/nki-ai/x2x">https://github.com/nki-ai/x2x</a>. </p>
<blockquote>
<p>解释深度学习模型对于医疗图像分析系统的临床整合至关重要。一个好的解释可以突出显示模型是否依赖于会破坏泛化并伤害部分患者的偶然特征，或者相反，可以呈现新的生物学见解。虽然GradCAM等技术可以识别出有影响力的特征，但它们只是测量工具，本身并不构成解释。我们提出了一种针对计算病理学中分类器解释的人类-机器-视觉语言模型（VLM）交互系统，包括用于全幻灯图像的多实例学习。我们的概念验证包括（1）一个集成了人工智能的幻灯片查看器，用于运行滑动窗口实验来验证解释的主张，以及（2）使用通用视觉语言模型量化解释的预测性。结果表明，这使我们能够定性验证解释的主张，并能够定量区分竞争性的解释。这为数字病理学等领域从可解释的AI到解释的AI提供了一条实用路径。代码和提示可在<a target="_blank" rel="noopener" href="https://github.com/nki-ai/x2x%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nki-ai/x2x上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09205v1">PDF</a> 10 pages, 2 figures, 2 tables, submitted at MICCAI IMIMIC workshop</p>
<p><strong>Summary</strong><br>在医疗图像分析系统的临床整合中，解释深度学习模型至关重要。解释有助于揭示模型是否依赖于影响泛化和损害患者群体的特征，或揭示新的生物学见解。虽然技术如GradCAM可以识别重要特征，但它们只是测量工具而非解释工具。本文提出一种针对计算病理学分类器的解释的人机交互系统，包括用于全幻灯片图像的多实例学习。我们的概念验证包括：（1）AI集成幻灯片查看器运行滑动窗口实验来测试解释主张，（2）使用通用视觉语言模型量化解释的预测性。结果表明，该方法可以定性测试解释主张并可以定量区分不同解释。这为数字病理学等领域实现从可解释的AI到解释的AI的实用路径。<strong>Key Takeaways</strong></p>
<ol>
<li>解释深度学习模型对于医疗图像分析系统的临床整合至关重要。</li>
<li>良好的模型解释能揭示模型是否依赖影响泛化和患者群体的特征。</li>
<li>虽然存在如GradCAM的技术可以识别重要特征，但它们主要是测量工具而非解释工具。</li>
<li>提出一种针对计算病理学分类器解释的人机交互系统，包括多实例学习用于全幻灯片图像。</li>
<li>该系统通过AI集成幻灯片查看器进行滑动窗口实验来测试解释主张。</li>
<li>使用通用视觉语言模型量化解释的预测性，以区分不同的解释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-754cf183e8a70f2f1315001997c82636.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56139d798e9aab0999668d0e170338c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfbc6a35d197591f462d4461fe683cb5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Calibrated-Self-supervised-Vision-Transformers-Improve-Intracranial-Arterial-Calcification-Segmentation-from-Clinical-CT-Head-Scans"><a href="#Calibrated-Self-supervised-Vision-Transformers-Improve-Intracranial-Arterial-Calcification-Segmentation-from-Clinical-CT-Head-Scans" class="headerlink" title="Calibrated Self-supervised Vision Transformers Improve Intracranial   Arterial Calcification Segmentation from Clinical CT Head Scans"></a>Calibrated Self-supervised Vision Transformers Improve Intracranial   Arterial Calcification Segmentation from Clinical CT Head Scans</h2><p><strong>Authors:Benjamin Jin, Grant Mair, Joanna M. Wardlaw, Maria del C. Valdés Hernández</strong></p>
<p>Vision Transformers (ViTs) have gained significant popularity in the natural image domain but have been less successful in 3D medical image segmentation. Nevertheless, 3D ViTs are particularly interesting for large medical imaging volumes due to their efficient self-supervised training within the masked autoencoder (MAE) framework, which enables the use of imaging data without the need for expensive manual annotations. Intracranial arterial calcification (IAC) is an imaging biomarker visible on routinely acquired CT scans linked to neurovascular diseases such as stroke and dementia, and automated IAC quantification could enable their large-scale risk assessment. We pre-train ViTs with MAE and fine-tune them for IAC segmentation for the first time. To develop our models, we use highly heterogeneous data from a large clinical trial, the third International Stroke Trial (IST-3). We evaluate key aspects of MAE pre-trained ViTs in IAC segmentation, and analyse the clinical implications. We show: 1) our calibrated self-supervised ViT beats a strong supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial for ViTs for IAC segmentation and interpolation upsampling with regular convolutions is preferable to transposed convolutions for ViT-based models, and 3) our ViTs increase robustness to higher slice thicknesses and improve risk group classification in a clinical scenario by 46%. Our code is available online. </p>
<blockquote>
<p>视觉Transformer（ViTs）在自然图像领域已经大受欢迎，但在3D医学图像分割方面的表现却不尽如人意。然而，对于大型医学图像体积而言，3D ViTs特别有趣，因为它们能在掩码自动编码器（MAE）框架内进行高效的自监督训练，从而能够使用图像数据而无需昂贵的手动注释。颅内动脉钙化（IAC）是常规CT扫描上可见的神经血管疾病（如中风和痴呆症）的成像生物标志物，自动的IAC定量评估可实现大规模风险评估。我们首次使用MAE对ViTs进行预训练，并对其进行微调以进行IAC分割。为了开发我们的模型，我们使用了来自大型临床试验的具有高度异质性的数据——第三次国际中风试验（IST-3）。我们评估了MAE预训练的ViTs在IAC分割方面的关键方面，并分析了其临床意义。我们的研究结果表明：1）我们的校准自监督ViT比受监督的nnU-Net基准测试高出3.2 Dice点；2）对于IAC分割的ViTs而言，较小的补丁尺寸至关重要，并且对于基于ViT的模型而言，使用常规卷积进行插值上采样比转置卷积更可取；3）我们的ViTs提高了对更高切片厚度的鲁棒性，并在临床场景中提高了风险组分类的准确性达46%。我们的代码可在网上获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01744v2">PDF</a> Accepted at the 3rd Data Engineering in Medical Imaging workshop @   MICCAI 2025</p>
<p><strong>Summary</strong><br>     視覺變換器（ViTs）在自然圖像领域廣受欢迎，但在3D医学影像分割上效果較差。然而，由於其内在自监督训练的MAE框架可有效利用影像数据，即使不需昂贵的手动标注，也能在大規模医学影像中實現高功效。本文首次預訓練ViTs於MAE框架上，并微调用于颅内動脉钙化（IAC）分割模型。我们运用來自大规模訓練数据的、具有显著差异的数据進行了訓練和评估，並分析了其临床影响。本文的研究結果包括：我們的校準自监督ViT比强大的监督型nnU-Net基准高出3.2 Dice点；低塊大小對ViT進行IAC分割非常關鍵；插值上采樣使用常規卷積優于轉置卷積對基於ViT的模型；我们的ViT能提高風險群組分期的健碁性達到提高近一半的預測能力。我们提供相關代码。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViTs) are effectively applied to 3D medical image segmentation, particularly for large medical imaging volumes.</li>
<li>MAE framework enables self-supervised training of ViTs, facilitating the use of imaging data without manual annotations.</li>
<li>For the first time, ViTs are pre-trained and fine-tuned for intracranial arterial calcification (IAC) segmentation.</li>
<li>Low patch sizes are crucial for ViTs in IAC segmentation, and interpolation upsampling with regular convolutions is preferred.</li>
<li>ViTs improve robustness to higher slice thicknesses and enhance risk group classification in a clinical scenario.</li>
<li>The study demonstrates a significant performance boost over a strong supervised nnU-Net baseline, with a 3.2 Dice point increase.</li>
<li>The research has practical implications in large-scale risk assessment for neurovascular diseases like stroke and dementia through automated IAC quantification.</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01744">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e726a17ee8dd91c5be7f0257702a8198.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7335c39ff98765dc1910d40d5a0d93ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1da436980b5adefc8b6a51c0cea99a9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ae879be75d1c2b8c514666adaaded1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2525aa2c1471a186ed786f33ff586c31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3561540d9bfe9c674119444e86d4574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cc980406a9efa35e389237bd0154996.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Lightweight-Transformer-with-Phase-Only-Cross-Attention-for-Illumination-Invariant-Biometric-Authentication"><a href="#A-Lightweight-Transformer-with-Phase-Only-Cross-Attention-for-Illumination-Invariant-Biometric-Authentication" class="headerlink" title="A Lightweight Transformer with Phase-Only Cross-Attention for   Illumination-Invariant Biometric Authentication"></a>A Lightweight Transformer with Phase-Only Cross-Attention for   Illumination-Invariant Biometric Authentication</h2><p><strong>Authors:Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza, Bishakh Bhattacharya</strong></p>
<p>Traditional biometric systems have encountered significant setbacks due to various unavoidable factors, for example, wearing of face masks in face recognition-based biometrics and hygiene concerns in fingerprint-based biometrics. This paper proposes a novel lightweight vision transformer with phase-only cross-attention (POC-ViT) using dual biometric traits of forehead and periocular portions of the face, capable of performing well even with face masks and without any physical touch, offering a promising alternative to traditional methods. The POC-ViT framework is designed to handle two biometric traits and to capture inter-dependencies in terms of relative structural patterns. Each channel consists of a Cross-Attention using phase-only correlation (POC) that captures both their individual and correlated structural patterns. The computation of cross-attention using POC extracts the phase correlation in the spatial features. Therefore, it is robust against variations in resolution and intensity, as well as illumination changes in the input images. The lightweight model is suitable for edge device deployment. The performance of the proposed framework was successfully demonstrated using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP) database, having 350 subjects. The POC-ViT framework outperformed state-of-the-art methods with an outstanding classification accuracy of $98.8%$ with the dual biometric traits. </p>
<blockquote>
<p>传统生物识别系统由于各种不可避免的因素而遭遇了重大挫折，例如在基于面部识别的生物识别中佩戴口罩的问题，以及在基于指纹识别的生物识别中的卫生问题。本文提出了一种新型轻量级视觉转换器，即仅相位交叉注意力（POC-ViT），它使用额头和眼部周围的面部双重生物特征，即使佩戴口罩无需任何物理接触也能表现良好，为传统方法提供了有前景的替代方案。POC-ViT框架被设计成处理两种生物特征，并捕获相对结构模式的相互依赖性。每个通道由仅使用相位相关性的交叉注意力（POC）组成，能够捕获它们各自的和相关联的结构模式。使用POC计算交叉注意力提取空间特征的相位相关性。因此，它对分辨率和强度的变化、输入图像中的照明变化具有鲁棒性。这种轻量级模型适合在边缘设备进行部署。使用额头皮下静脉模式和眼部生物特征模式（FSVP-PBP）数据库（包含350个主体）成功展示了所提出框架的性能。POC-ViT框架利用双重生物特征实现了卓越的分类准确率（98.8%），超越了最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19160v3">PDF</a> Submitted to IEEE</p>
<p><strong>Summary</strong></p>
<p>一种新型轻量级视觉转换器POC-ViT被提出，它利用额头和眼周的双生物特征进行面部识别，能够在佩戴口罩的情况下和无须任何物理接触的情况下表现良好，为传统生物识别方法提供了有前景的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统生物识别系统因不可抗因素（如戴口罩、卫生问题）而受阻。</li>
<li>POC-ViT是一种新型轻量级视觉转换器，使用额头和眼周的双生物特征进行识别。</li>
<li>POC-ViT设计用于处理两种生物特征，并捕捉其相对结构模式的相互依赖性。</li>
<li>跨注意使用POC计算提取空间特征的相位相关性，使其对分辨率和强度变化、输入图像中的照明变化具有鲁棒性。</li>
<li>该轻量级模型适合在边缘设备进行部署。</li>
<li>在使用额头皮下血管模式和眼周生物特征模式的数据库中进行测试，POC-ViT框架表现出卓越的分类准确性，达到98.8%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-98685b27ac6566c0b86f1a43e3482acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2dc33ec5d16713a530a426c0c3f3dad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99a8ecabb0b06fa71d017986b21d72f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd26f70ef56c192ee4bb33adddc3a026.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-99d1ce2fb51a4920e2ef39c53db45d83.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-08-15  Beyond conventional vision RGB-event fusion for robust object detection   in dynamic traffic scenarios
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91498fc8130f811e249fa4330b6fb585.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-08-15  Data-Efficient Learning for Generalizable Surgical Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
