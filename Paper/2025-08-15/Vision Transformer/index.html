<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  SemPT Semantic Prompt Tuning for Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-99a8ecabb0b06fa71d017986b21d72f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    29 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-15-æ›´æ–°"><a href="#2025-08-15-æ›´æ–°" class="headerlink" title="2025-08-15 æ›´æ–°"></a>2025-08-15 æ›´æ–°</h1><h2 id="SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models"><a href="#SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="SemPT: Semantic Prompt Tuning for Vision-Language Models"></a>SemPT: Semantic Prompt Tuning for Vision-Language Models</h2><p><strong>Authors:Xiao Shi, Yangjun Ou, Zhenzhong Chen</strong></p>
<p>Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between preserving category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on sparse category labels or disparate LLM-generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide LLM in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning. </p>
<blockquote>
<p>è§†è§‰è¿ç§»å­¦ä¹ å¯¹äºæœªè§ç±»åˆ«æ˜¯ä¸€ä¸ªçƒ­é—¨çš„ç ”ç©¶è¯¾é¢˜ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºä¿æŒç‰¹å®šç±»åˆ«çš„è¡¨ç¤ºå’Œè·å–å¯è¿ç§»çŸ¥è¯†ä¹‹é—´å­˜åœ¨å›ºæœ‰çš„å†²çªã€‚é¢„è®­ç»ƒåœ¨å¤§é‡å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ä¾èµ–äºç¨€ç–çš„ç±»åˆ«æ ‡ç­¾æˆ–åˆ†æ•£çš„LLMç”Ÿæˆæè¿°ï¼Œè¿™å¯¼è‡´çŸ¥è¯†è¡¨ç¤ºç¢ç‰‡åŒ–å¹¶é˜»ç¢çŸ¥è¯†çš„è¿ç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰æç¤ºè°ƒæ•´ï¼ˆSemPTï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨è·¨ç±»åˆ«çš„å…±äº«å±æ€§çº§çŸ¥è¯†æ¥è§£å†³æ³›åŒ–æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼ŒSemPTé‡‡ç”¨ä¸¤æ­¥æç¤ºç­–ç•¥æ¥å¼•å¯¼LLMæå–å…±äº«çš„è§†è§‰å±æ€§å¹¶ç”Ÿæˆå±æ€§çº§æè¿°ï¼Œåœ¨ç¡®ä¿è¿è´¯ç»“æ„çš„åŒæ—¶ï¼Œæ•æ‰è¶…è¶Šæ ‡ç­¾çš„å¯è¿ç§»è¯­ä¹‰çº¿ç´¢ã€‚ç„¶åï¼Œå¯¹å±æ€§çº§æè¿°çš„åµŒå…¥åº”ç”¨è§†è§‰å¼•å¯¼åŠ æƒï¼Œä»¥å‡å°‘æ¥è‡ªæ— å…³å±æ€§çš„å™ªå£°å¹¶å¢å¼ºæ–‡æœ¬åµŒå…¥ã€‚æ­¤å¤–ï¼Œå›¾åƒåµŒå…¥ä¸æ ‡ç­¾å’Œå±æ€§å¢å¼ºæ–‡æœ¬åµŒå…¥å…±åŒå¯¹é½ï¼Œå¹³è¡¡å¯¹å¯è§ç±»åˆ«çš„åŒºåˆ†åº¦å’Œå¯¹æœªè§ç±»åˆ«çš„å¯è¿ç§»æ€§ã€‚è€ƒè™‘åˆ°ç±»åˆ«æš´éœ²çš„å¯ç”¨æ€§ï¼Œæˆ‘ä»¬çš„æ¨ç†ä¼šåŠ¨æ€é€‰æ‹©åœ¨å¯è§ç±»åˆ«æ—¶ä½¿ç”¨æ ‡å‡†çš„æ ‡ç­¾åµŒå…¥ï¼Œåœ¨æœªè§ç±»åˆ«æ—¶ä½¿ç”¨å±æ€§å¢å¼ºåµŒå…¥ï¼Œä»¥ç¡®ä¿æœ‰æ•ˆçš„é€‚åº”ã€‚åœ¨15ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSemPTåœ¨å„ç§è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ã€è·¨åŸŸè¿ç§»å’Œå°‘æ ·æœ¬å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10645v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰è¿ç§»å­¦ä¹ å¯¹äºæœªè§ç±»åˆ«æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶è¯¾é¢˜ï¼Œä½†ä¹Ÿæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚é¢„è®­ç»ƒåœ¨å¤§é‡å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ä¾èµ–äºç¨€ç–çš„ç±»åˆ«æ ‡ç­¾æˆ–åˆ†æ•£çš„LLMç”Ÿæˆæè¿°ï¼Œè¿™ç ´åäº†çŸ¥è¯†è¡¨ç¤ºå¹¶é˜»ç¢äº†å¯è½¬ç§»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰æç¤ºè°ƒæ•´ï¼ˆSemPTï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç±»åˆ«ä¹‹é—´çš„å…±äº«å±æ€§çº§çŸ¥è¯†æ¥è§£å†³æ³›åŒ–æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¿ç§»å­¦ä¹ å¯¹äºæœªè§ç±»åˆ«æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦å¹³è¡¡ä¿æŒç±»åˆ«ç‰¹å®šè¡¨ç¤ºå’Œè·å–å¯è½¬ç§»çŸ¥è¯†ä¹‹é—´çš„çŸ›ç›¾ã€‚</li>
<li>é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ˜¯è§£å†³æ­¤é—®é¢˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æç¤ºè°ƒæ•´æ–¹æ³•çš„å±€é™æ€§åœ¨äºå®ƒä»¬ä¾èµ–äºç¨€ç–çš„ç±»åˆ«æ ‡ç­¾æˆ–LLMç”Ÿæˆçš„æè¿°ï¼Œè¿™é˜»ç¢äº†çŸ¥è¯†çš„è½¬ç§»ã€‚</li>
<li>SemPTæ¡†æ¶é€šè¿‡åˆ©ç”¨è·¨ç±»åˆ«çš„å…±äº«å±æ€§çº§çŸ¥è¯†æ¥è§£å†³æ³›åŒ–æŒ‘æˆ˜ã€‚</li>
<li>SemPTé‡‡ç”¨ä¸¤æ­¥æç¤ºç­–ç•¥ï¼Œå¼•å¯¼LLMæå–å…±äº«è§†è§‰å±æ€§å’Œç”Ÿæˆå±æ€§çº§æè¿°ï¼Œæ•æ‰å¯è½¬ç§»è¯­ä¹‰çº¿ç´¢ã€‚</li>
<li>é€šè¿‡è§†è§‰å¼•å¯¼çš„åŠ æƒæ–¹æ³•ï¼Œå‡å°‘æ— å…³å±æ€§çš„å™ªå£°å¹¶å¢å¼ºæ–‡æœ¬åµŒå…¥ã€‚</li>
<li>SemPTåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨å„ç§è®¾ç½®ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä»åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è½¬ç§»ã€è·¨åŸŸè½¬ç§»å’Œå°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1616057c449bdf030c62fc7153ded7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a7e3f73e77e13e56ee03baff6258bc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5722113344929679e94d6ba228691cbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-137d70338d2d00a469864a7d66ec70db.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images"><a href="#GCRPNet-Graph-Enhanced-Contextual-and-Regional-Perception-Network-For-Salient-Object-Detection-in-Optical-Remote-Sensing-Images" class="headerlink" title="GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For   Salient Object Detection in Optical Remote Sensing Images"></a>GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For   Salient Object Detection in Optical Remote Sensing Images</h2><p><strong>Authors:Mengyu Ren, Yutong Li, Hua Li, Runmin Cong, Sam Kwong</strong></p>
<p>Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the modelâ€™s structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the decoder of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mambaâ€™s local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority. </p>
<blockquote>
<p>åœ¨å…‰å­¦é¥æ„Ÿå›¾åƒï¼ˆORSIsï¼‰ä¸­ï¼Œæ˜¾è‘—ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç›®æ ‡å°ºåº¦çš„æ˜¾è‘—å·®å¼‚ä»¥åŠç›®æ ‡ä¸èƒŒæ™¯ä¹‹é—´å¯¹æ¯”åº¦ä½çš„é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºè§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰æ¶æ„çš„æ–¹æ³•æ—¨åœ¨åˆ©ç”¨å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œä½†æœ‰æ•ˆé›†æˆè¿™äº›å¼‚è´¨ç‰¹å¾çš„éš¾åº¦é™åˆ¶äº†å®ƒä»¬çš„æ•´ä½“æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å›¾å¢å¼ºä¸Šä¸‹æ–‡å’ŒåŒºåŸŸæ„ŸçŸ¥ç½‘ç»œï¼ˆGCRPNetï¼‰ï¼Œå®ƒåŸºäºMambaæ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶å¢å¼ºåŒºåŸŸç‰¹å¾è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨è§†è§‰çŠ¶æ€ç©ºé—´ï¼ˆVSSï¼‰ç¼–ç å™¨æå–å¤šå°ºåº¦ç‰¹å¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥å®ç°è¿™äº›ç‰¹å¾çš„æ·±åº¦å¼•å¯¼å’Œå¢å¼ºï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªå·®å¼‚ç›¸ä¼¼æ€§å¼•å¯¼åˆ†å±‚å›¾æ³¨æ„åŠ›æ¨¡å—ï¼ˆDS-HGAMï¼‰ã€‚è¯¥æ¨¡å—å¢å¼ºäº†ä¸åŒå°ºåº¦ç‰¹å¾ä¹‹é—´çš„è·¨å±‚äº¤äº’èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡å‹çš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿å…¶æ›´æœ‰æ•ˆåœ°åŒºåˆ†å‰æ™¯å’ŒèƒŒæ™¯ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†LEVSSå—ä½œä¸ºGCRPNetçš„è§£ç å™¨ã€‚è¯¥æ¨¡å—ç»“åˆäº†æˆ‘ä»¬çš„è‡ªé€‚åº”æ‰«æç­–ç•¥å’Œå¤šç²’åº¦ååŒæ³¨æ„åŠ›å¢å¼ºæ¨¡å—ï¼ˆMCAEMï¼‰ã€‚å®ƒå¯¹é€šè¿‡å¤šå°ºåº¦å·ç§¯å¤„ç†çš„ç‰¹å¾å›¾æ‰§è¡Œè‡ªé€‚åº”è¡¥ä¸æ‰«æï¼Œä»è€Œæ•è·ä¸°å¯Œçš„å±€éƒ¨åŒºåŸŸä¿¡æ¯å¹¶å¢å¼ºMambaçš„å±€éƒ¨å»ºæ¨¡èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜äº†æ‰€æå‡ºæ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10542v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å…‰å­¦é¥æ„Ÿå›¾åƒä¸­è¿›è¡Œæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹çš„æŒ‘æˆ˜ã€‚æå‡ºçš„GCRPNetæ¨¡å‹é‡‡ç”¨å›¾å¢å¼ºä¸Šä¸‹æ–‡ä¸åŒºåŸŸæ„ŸçŸ¥ç½‘ç»œç»“æ„ï¼ŒåŸºäºMambaæ¶æ„å®ç°é•¿è·ç¦»ä¾èµ–æ•è·ä¸åŒºåŸŸç‰¹å¾è¡¨ç¤ºå¢å¼ºã€‚åˆ©ç”¨è§†è§‰çŠ¶æ€ç©ºé—´ç¼–ç å™¨æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶è®¾è®¡å·®å¼‚ç›¸ä¼¼æ€§å¼•å¯¼å±‚æ¬¡å›¾æ³¨æ„åŠ›æ¨¡å—å¼ºåŒ–è·¨å±‚äº¤äº’èƒ½åŠ›ï¼Œæå‡æ¨¡å‹çš„ç»“æ„æ„ŸçŸ¥åŠ›ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†LEVSSè§£ç å—ç»“åˆè‡ªé€‚åº”æ‰«æç­–ç•¥å’Œå¤šç²’åº¦åä½œæ³¨æ„åŠ›å¢å¼ºæ¨¡å—ï¼Œå¢å¼ºå±€éƒ¨å»ºæ¨¡èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GCRPNetè§£å†³äº†å…‰å­¦é¥æ„Ÿå›¾åƒæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ä¸­ç›®æ ‡å°ºåº¦å˜åŒ–å’ŒèƒŒæ™¯å¯¹æ¯”åº¦ä½çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾æå–æŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨å›¾å¢å¼ºä¸Šä¸‹æ–‡ä¸åŒºåŸŸæ„ŸçŸ¥ç½‘ç»œè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡è§†è§‰çŠ¶æ€ç©ºé—´ç¼–ç å™¨å®ç°å¤šå°ºåº¦ç‰¹å¾çš„æå–ã€‚</li>
<li>å·®å¼‚ç›¸ä¼¼æ€§å¼•å¯¼å±‚æ¬¡å›¾æ³¨æ„åŠ›æ¨¡å—å¼ºåŒ–äº†ä¸åŒå°ºåº¦ç‰¹å¾é—´çš„è·¨å±‚äº¤äº’èƒ½åŠ›ï¼Œæå‡äº†æ¨¡å‹çš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>LEVSSè§£ç å—ç»“åˆäº†è‡ªé€‚åº”æ‰«æç­–ç•¥å’Œå¤šç²’åº¦åä½œæ³¨æ„åŠ›å¢å¼ºæ¨¡å—ï¼Œèƒ½æœ‰æ•ˆæ•æ‰ä¸°å¯Œçš„å±€éƒ¨åŒºåŸŸä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75960cd094ac015390df721288ebcf85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe4631d550a03dce9b0401ce6e952bbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef5f9b3198268884039c925e3b11eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b06639abc14d543a39388a56ff6e833e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver"><a href="#ReconVLA-Reconstructive-Vision-Language-Action-Model-as-Effective-Robot-Perceiver" class="headerlink" title="ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot   Perceiver"></a>ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot   Perceiver</h2><p><strong>Authors:Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Haodong Yan, Yuxin Huang, Feilong Tang, Donglin Wang, Haoang Li</strong></p>
<p>Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions. Instead, visual attention is always dispersed. To guide the visual attention grounding on the correct target, we propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. Conditioned on the modelâ€™s visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects. This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation. Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the modelâ€™s generalization in visual reconstruction. Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization. Our project page is <a target="_blank" rel="noopener" href="https://zionchow.github.io/ReconVLA/">https://zionchow.github.io/ReconVLA/</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿæ•´åˆå¤šæ¨¡å¼ç†è§£ä¸åŠ¨ä½œæ‰§è¡Œã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„å®è¯åˆ†ææ˜¾ç¤ºï¼Œå½“å‰çš„VLAåœ¨åˆ†é…è§†è§‰æ³¨æ„åŠ›åˆ°ç›®æ ‡åŒºåŸŸæ—¶é¢ä¸´å›°éš¾ã€‚ç›¸åï¼Œè§†è§‰æ³¨æ„åŠ›æ€»æ˜¯åˆ†æ•£çš„ã€‚ä¸ºäº†å¼•å¯¼è§†è§‰æ³¨æ„åŠ›æ­£ç¡®åœ°å®šä½åˆ°ç›®æ ‡ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ReconVLAï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰éšæ€§å®šä½èŒƒå¼çš„é‡å»ºå‹VLAæ¨¡å‹ã€‚åŸºäºæ¨¡å‹çš„è§†è§‰è¾“å‡ºï¼Œæ‰©æ•£å˜å‹å™¨æ—¨åœ¨é‡å»ºå›¾åƒä¸­çš„æ³¨è§†åŒºåŸŸï¼Œè¯¥åŒºåŸŸå¯¹åº”äºè¢«æ“çºµçš„ç›®æ ‡å¯¹è±¡ã€‚è¿™ä¸€è¿‡ç¨‹ä¿ƒä½¿VLAæ¨¡å‹å­¦ä¹ ç²¾ç»†çš„è¡¨å¾å¹¶å‡†ç¡®åœ°åˆ†é…è§†è§‰æ³¨æ„åŠ›ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„è§†è§‰ä¿¡æ¯å¹¶è¿›è¡Œç²¾ç¡®æ“ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡ä¸ªè½¨è¿¹å’Œæ¥è‡ªå¼€æºæœºå™¨äººæ•°æ®é›†çš„200ä¸‡ä¸ªæ•°æ®æ ·æœ¬ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹åœ¨è§†è§‰é‡å»ºä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»¿çœŸå’Œç°å®ä¸–ç•Œçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬éšæ€§å®šä½æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºäº†å…¶ç²¾ç¡®æ“ä½œå’Œæ³›åŒ–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://zionchow.github.io/ReconVLA/%E3%80%82">https://zionchow.github.io/ReconVLA/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10333v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æœºå™¨äººè§†è§‰é¢†åŸŸçš„ä¸€é¡¹æ–°ç ”ç©¶â€”â€”ReconVLAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è§£å†³äº†æœºå™¨äººå¯¹äºç›®æ ‡åŒºåŸŸçš„è§†è§‰æ³¨æ„åŠ›åˆ†é…é—®é¢˜ã€‚ç ”ç©¶å‘ç°å½“å‰VLAæ¨¡å‹å­˜åœ¨è§†è§‰æ³¨æ„åŠ›åˆ†æ•£çš„é—®é¢˜ï¼Œå› æ­¤æå‡ºä½¿ç”¨æ‰©æ•£å˜å‹å™¨é‡å»ºå›¾åƒä¸­çš„æ³¨è§†åŒºåŸŸï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ ç²¾ç»†çš„è¡¨å¾å¹¶å‡†ç¡®åˆ†é…è§†è§‰æ³¨æ„åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶é€šè¿‡ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒéªŒè¯äº†å…¶ç²¾ç¡®æ“æ§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸé¢ä¸´è§†è§‰æ³¨æ„åŠ›åˆ†é…é—®é¢˜ã€‚</li>
<li>ReconVLAæ¨¡å‹é€šè¿‡é‡å»ºå›¾åƒä¸­çš„æ³¨è§†åŒºåŸŸæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ‰©æ•£å˜å‹å™¨åœ¨æ¨¡å‹ä¸­çš„ä½œç”¨æ˜¯ä¾æ®æ¨¡å‹çš„è§†è§‰è¾“å‡ºæ¥é‡å»ºæ³¨è§†åŒºåŸŸã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç²¾ç»†çš„è¡¨å¾å¹¶å‡†ç¡®åˆ†é…è§†è§‰æ³¨æ„åŠ›ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒéªŒè¯äº†ReconVLAæ¨¡å‹çš„ç²¾ç¡®æ“æ§èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e468733f60157090ceb5a6ad170d6859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5447ba29cd2eb84b9ea059c085ba1d05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c2556ad0b523f3f5cc53b3d2b3f8b7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfa824e1f3ccf9d0132800600f8cf9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbfaabe7eec0d5c9e50c0c87072ce448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abf06f576de55c16a28900a5e3de8ec0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2486339f613bcf180d3771061f1b0de.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RelayFormer-A-Unified-Local-Global-Attention-Framework-for-Scalable-Image-and-Video-Manipulation-Localization"><a href="#RelayFormer-A-Unified-Local-Global-Attention-Framework-for-Scalable-Image-and-Video-Manipulation-Localization" class="headerlink" title="RelayFormer: A Unified Local-Global Attention Framework for Scalable   Image and Video Manipulation Localization"></a>RelayFormer: A Unified Local-Global Attention Framework for Scalable   Image and Video Manipulation Localization</h2><p><strong>Authors:Wen Huang, Jiarui Yang, Tao Dai, Jiawei Li, Shaoxiong Zhan, Bin Wang, Shu-Tao Xia</strong></p>
<p>Visual manipulation localization (VML) â€“ across both images and videos â€“ is a crucial task in digital forensics that involves identifying tampered regions in visual content. However, existing methods often lack cross-modal generalization and struggle to handle high-resolution or long-duration inputs efficiently.   We propose RelayFormer, a unified and modular architecture for visual manipulation localization across images and videos. By leveraging flexible local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables scalable, resolution-agnostic processing with strong generalization. Our framework integrates seamlessly with existing Transformer-based backbones, such as ViT and SegFormer, via lightweight adaptation modules that require only minimal architectural changes, ensuring compatibility without disrupting pretrained representations.   Furthermore, we design a lightweight, query-based mask decoder that supports one-shot inference across video sequences with linear complexity. Extensive experiments across multiple benchmarks demonstrate that our approach achieves state-of-the-art localization performance, setting a new baseline for scalable and modality-agnostic VML. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/WenOOI/RelayFormer">https://github.com/WenOOI/RelayFormer</a>. </p>
<blockquote>
<p>è§†è§‰æ“æ§å®šä½ï¼ˆVMLï¼‰â€”â€”æ— è®ºæ˜¯å›¾ç‰‡è¿˜æ˜¯è§†é¢‘â€”â€”æ˜¯æ•°å­—å–è¯ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œæ¶‰åŠè¯†åˆ«è§†è§‰å†…å®¹ä¸­è¢«ç¯¡æ”¹çš„åŒºåŸŸã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥é«˜æ•ˆå¤„ç†é«˜åˆ†è¾¨ç‡æˆ–é•¿æ—¶é—´è¾“å…¥çš„å›¾åƒã€‚æˆ‘ä»¬æå‡ºäº†RelayFormerï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾ç‰‡å’Œè§†é¢‘è§†è§‰æ“æ§å®šä½çš„ç»Ÿä¸€æ¨¡å—åŒ–æ¶æ„ã€‚å®ƒé€šè¿‡åˆ©ç”¨çµæ´»çš„å±€éƒ¨å•å…ƒå’Œå…¨å±€-å±€éƒ¨ä¸­ç»§æ³¨æ„åŠ›ï¼ˆGLoRAï¼‰æœºåˆ¶ï¼Œå®ç°äº†å¯ä¼¸ç¼©çš„ã€ä¸åˆ†è¾¨ç‡æ— å…³çš„å¤„ç†ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡è½»é‡çº§é€‚é…æ¨¡å—æ— ç¼é›†æˆç°æœ‰çš„åŸºäºTransformerçš„ä¸»å¹²ç½‘ç»œï¼Œå¦‚ViTå’ŒSegFormerï¼Œåªéœ€è¿›è¡Œæœ€å°çš„æ¶æ„æ›´æ”¹ï¼Œç¡®ä¿ä¸é¢„è®­ç»ƒè¡¨ç¤ºå…¼å®¹è€Œä¸ä¼šé€ æˆå¹²æ‰°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„ã€åŸºäºæŸ¥è¯¢çš„æ©è†œè§£ç å™¨ï¼Œæ”¯æŒåœ¨è§†é¢‘åºåˆ—ä¸Šè¿›è¡Œä¸€æ¬¡æ¨æ–­ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®šä½æ€§èƒ½ï¼Œä¸ºå¯æ‰©å±•å’Œæ¨¡æ€æ— å…³çš„VMLè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/WenOOI/RelayFormer">https://github.com/WenOOI/RelayFormer</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09459v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RelayFormeræ˜¯ä¸€ç§ç”¨äºå›¾åƒå’Œè§†é¢‘ç¯¡æ”¹åŒºåŸŸå®šä½çš„ç»Ÿä¸€æ¨¡å—åŒ–æ¶æ„ã€‚å®ƒé€šè¿‡çµæ´»çš„å±€éƒ¨å•å…ƒå’Œå…¨å±€-å±€éƒ¨ä¸­ç»§æ³¨æ„åŠ›ï¼ˆGLoRAï¼‰æœºåˆ¶ï¼Œå®ç°äº†å¯ä¼¸ç¼©çš„ã€åˆ†è¾¨ç‡æ— å…³çš„å¤„ç†ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§é€‚é…æ¨¡å—ä¸ç°æœ‰çš„TransformeråŸºç¡€æ¶æ„ï¼ˆå¦‚ViTå’ŒSegFormerï¼‰æ— ç¼é›†æˆï¼Œåªéœ€è¿›è¡Œæœ€å°çš„æ¶æ„æ›´æ”¹ï¼Œç¡®ä¿äº†ä¸é¢„è®­ç»ƒè¡¨ç¤ºçš„å…¼å®¹æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„åŸºäºæŸ¥è¯¢çš„æ©è†œè§£ç å™¨ï¼Œæ”¯æŒåœ¨è§†é¢‘åºåˆ—ä¸Šè¿›è¡Œä¸€æ¬¡æ¨ç†ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚åœ¨å¤šåŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°çš„å®šä½æ€§èƒ½ï¼Œä¸ºå¯ä¼¸ç¼©å’Œæ¨¡æ€æ— å…³çš„ç¯¡æ”¹åŒºåŸŸå®šä½è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RelayFormeræ˜¯ä¸€ä¸ªç”¨äºè§†è§‰ç¯¡æ”¹å®šä½çš„ç»Ÿä¸€æ¨¡å—åŒ–æ¶æ„ï¼Œé€‚ç”¨äºå›¾åƒå’Œè§†é¢‘ã€‚</li>
<li>åˆ©ç”¨çµæ´»çš„å±€éƒ¨å•å…ƒå’Œå…¨å±€-å±€éƒ¨ä¸­ç»§æ³¨æ„åŠ›ï¼ˆGLoRAï¼‰æœºåˆ¶ï¼Œå®ç°åˆ†è¾¨ç‡æ— å…³çš„å¤„ç†ã€‚</li>
<li>RelayFormerå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤„ç†å„ç§æ¨¡æ€çš„è§†è§‰å†…å®¹ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§é€‚é…æ¨¡å—ä¸ç°æœ‰Transformeræ¶æ„å…¼å®¹ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„æ©è†œè§£ç å™¨ï¼Œæ”¯æŒä¸€æ¬¡å¤„ç†æ•´ä¸ªè§†é¢‘åºåˆ—ã€‚</li>
<li>RelayFormerè¾¾åˆ°äº†æœ€æ–°çš„å®šä½æ€§èƒ½åŸºå‡†ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cb9d320cca697ab63237d5fcd49157f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4b0c7f5e95df3a15bcb8138ac830398.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ca99a6758bf8def559b9d67b5b27725.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c477333f2b799d76a6afaf3007714e13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2959992e16ca922fcbef420f783e9ed1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c4ae5bf07112b90b595b4f7b3f5e919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee0cd89b2329072b8ab9469a7059647.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Explainable-to-Explained-AI-Ideas-for-Falsifying-and-Quantifying-Explanations"><a href="#From-Explainable-to-Explained-AI-Ideas-for-Falsifying-and-Quantifying-Explanations" class="headerlink" title="From Explainable to Explained AI: Ideas for Falsifying and Quantifying   Explanations"></a>From Explainable to Explained AI: Ideas for Falsifying and Quantifying   Explanations</h2><p><strong>Authors:Yoni Schirris, Eric Marcus, Jonas Teuwen, Hugo Horlings, Efstratios Gavves</strong></p>
<p>Explaining deep learning models is essential for clinical integration of medical image analysis systems. A good explanation highlights if a model depends on spurious features that undermines generalization and harms a subset of patients or, conversely, may present novel biological insights. Although techniques like GradCAM can identify influential features, they are measurement tools that do not themselves form an explanation. We propose a human-machine-VLM interaction system tailored to explaining classifiers in computational pathology, including multi-instance learning for whole-slide images. Our proof of concept comprises (1) an AI-integrated slide viewer to run sliding-window experiments to test claims of an explanation, and (2) quantification of an explanationâ€™s predictiveness using general-purpose vision-language models. The results demonstrate that this allows us to qualitatively test claims of explanations and can quantifiably distinguish competing explanations. This offers a practical path from explainable AI to explained AI in digital pathology and beyond. Code and prompts are available at <a target="_blank" rel="noopener" href="https://github.com/nki-ai/x2x">https://github.com/nki-ai/x2x</a>. </p>
<blockquote>
<p>è§£é‡Šæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹äºåŒ»ç–—å›¾åƒåˆ†æç³»ç»Ÿçš„ä¸´åºŠæ•´åˆè‡³å…³é‡è¦ã€‚ä¸€ä¸ªå¥½çš„è§£é‡Šå¯ä»¥çªå‡ºæ˜¾ç¤ºæ¨¡å‹æ˜¯å¦ä¾èµ–äºä¼šç ´åæ³›åŒ–å¹¶ä¼¤å®³éƒ¨åˆ†æ‚£è€…çš„å¶ç„¶ç‰¹å¾ï¼Œæˆ–è€…ç›¸åï¼Œå¯ä»¥å‘ˆç°æ–°çš„ç”Ÿç‰©å­¦è§è§£ã€‚è™½ç„¶GradCAMç­‰æŠ€æœ¯å¯ä»¥è¯†åˆ«å‡ºæœ‰å½±å“åŠ›çš„ç‰¹å¾ï¼Œä½†å®ƒä»¬åªæ˜¯æµ‹é‡å·¥å…·ï¼Œæœ¬èº«å¹¶ä¸æ„æˆè§£é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹è®¡ç®—ç—…ç†å­¦ä¸­åˆ†ç±»å™¨è§£é‡Šçš„äººç±»-æœºå™¨-è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰äº¤äº’ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç”¨äºå…¨å¹»ç¯å›¾åƒçš„å¤šå®ä¾‹å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¦‚å¿µéªŒè¯åŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªé›†æˆäº†äººå·¥æ™ºèƒ½çš„å¹»ç¯ç‰‡æŸ¥çœ‹å™¨ï¼Œç”¨äºè¿è¡Œæ»‘åŠ¨çª—å£å®éªŒæ¥éªŒè¯è§£é‡Šçš„ä¸»å¼ ï¼Œä»¥åŠï¼ˆ2ï¼‰ä½¿ç”¨é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹é‡åŒ–è§£é‡Šçš„é¢„æµ‹æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®šæ€§éªŒè¯è§£é‡Šçš„ä¸»å¼ ï¼Œå¹¶èƒ½å¤Ÿå®šé‡åŒºåˆ†ç«äº‰æ€§çš„è§£é‡Šã€‚è¿™ä¸ºæ•°å­—ç—…ç†å­¦ç­‰é¢†åŸŸä»å¯è§£é‡Šçš„AIåˆ°è§£é‡Šçš„AIæä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ã€‚ä»£ç å’Œæç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nki-ai/x2x%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nki-ai/x2xä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09205v1">PDF</a> 10 pages, 2 figures, 2 tables, submitted at MICCAI IMIMIC workshop</p>
<p><strong>Summary</strong><br>åœ¨åŒ»ç–—å›¾åƒåˆ†æç³»ç»Ÿçš„ä¸´åºŠæ•´åˆä¸­ï¼Œè§£é‡Šæ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚è§£é‡Šæœ‰åŠ©äºæ­ç¤ºæ¨¡å‹æ˜¯å¦ä¾èµ–äºå½±å“æ³›åŒ–å’ŒæŸå®³æ‚£è€…ç¾¤ä½“çš„ç‰¹å¾ï¼Œæˆ–æ­ç¤ºæ–°çš„ç”Ÿç‰©å­¦è§è§£ã€‚è™½ç„¶æŠ€æœ¯å¦‚GradCAMå¯ä»¥è¯†åˆ«é‡è¦ç‰¹å¾ï¼Œä½†å®ƒä»¬åªæ˜¯æµ‹é‡å·¥å…·è€Œéè§£é‡Šå·¥å…·ã€‚æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹è®¡ç®—ç—…ç†å­¦åˆ†ç±»å™¨çš„è§£é‡Šçš„äººæœºäº¤äº’ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒçš„å¤šå®ä¾‹å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¦‚å¿µéªŒè¯åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰AIé›†æˆå¹»ç¯ç‰‡æŸ¥çœ‹å™¨è¿è¡Œæ»‘åŠ¨çª—å£å®éªŒæ¥æµ‹è¯•è§£é‡Šä¸»å¼ ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹é‡åŒ–è§£é‡Šçš„é¢„æµ‹æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®šæ€§æµ‹è¯•è§£é‡Šä¸»å¼ å¹¶å¯ä»¥å®šé‡åŒºåˆ†ä¸åŒè§£é‡Šã€‚è¿™ä¸ºæ•°å­—ç—…ç†å­¦ç­‰é¢†åŸŸå®ç°ä»å¯è§£é‡Šçš„AIåˆ°è§£é‡Šçš„AIçš„å®ç”¨è·¯å¾„ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>è§£é‡Šæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹äºåŒ»ç–—å›¾åƒåˆ†æç³»ç»Ÿçš„ä¸´åºŠæ•´åˆè‡³å…³é‡è¦ã€‚</li>
<li>è‰¯å¥½çš„æ¨¡å‹è§£é‡Šèƒ½æ­ç¤ºæ¨¡å‹æ˜¯å¦ä¾èµ–å½±å“æ³›åŒ–å’Œæ‚£è€…ç¾¤ä½“çš„ç‰¹å¾ã€‚</li>
<li>è™½ç„¶å­˜åœ¨å¦‚GradCAMçš„æŠ€æœ¯å¯ä»¥è¯†åˆ«é‡è¦ç‰¹å¾ï¼Œä½†å®ƒä»¬ä¸»è¦æ˜¯æµ‹é‡å·¥å…·è€Œéè§£é‡Šå·¥å…·ã€‚</li>
<li>æå‡ºä¸€ç§é’ˆå¯¹è®¡ç®—ç—…ç†å­¦åˆ†ç±»å™¨è§£é‡Šçš„äººæœºäº¤äº’ç³»ç»Ÿï¼ŒåŒ…æ‹¬å¤šå®ä¾‹å­¦ä¹ ç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡AIé›†æˆå¹»ç¯ç‰‡æŸ¥çœ‹å™¨è¿›è¡Œæ»‘åŠ¨çª—å£å®éªŒæ¥æµ‹è¯•è§£é‡Šä¸»å¼ ã€‚</li>
<li>ä½¿ç”¨é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹é‡åŒ–è§£é‡Šçš„é¢„æµ‹æ€§ï¼Œä»¥åŒºåˆ†ä¸åŒçš„è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-754cf183e8a70f2f1315001997c82636.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56139d798e9aab0999668d0e170338c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfbc6a35d197591f462d4461fe683cb5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Calibrated-Self-supervised-Vision-Transformers-Improve-Intracranial-Arterial-Calcification-Segmentation-from-Clinical-CT-Head-Scans"><a href="#Calibrated-Self-supervised-Vision-Transformers-Improve-Intracranial-Arterial-Calcification-Segmentation-from-Clinical-CT-Head-Scans" class="headerlink" title="Calibrated Self-supervised Vision Transformers Improve Intracranial   Arterial Calcification Segmentation from Clinical CT Head Scans"></a>Calibrated Self-supervised Vision Transformers Improve Intracranial   Arterial Calcification Segmentation from Clinical CT Head Scans</h2><p><strong>Authors:Benjamin Jin, Grant Mair, Joanna M. Wardlaw, Maria del C. ValdÃ©s HernÃ¡ndez</strong></p>
<p>Vision Transformers (ViTs) have gained significant popularity in the natural image domain but have been less successful in 3D medical image segmentation. Nevertheless, 3D ViTs are particularly interesting for large medical imaging volumes due to their efficient self-supervised training within the masked autoencoder (MAE) framework, which enables the use of imaging data without the need for expensive manual annotations. Intracranial arterial calcification (IAC) is an imaging biomarker visible on routinely acquired CT scans linked to neurovascular diseases such as stroke and dementia, and automated IAC quantification could enable their large-scale risk assessment. We pre-train ViTs with MAE and fine-tune them for IAC segmentation for the first time. To develop our models, we use highly heterogeneous data from a large clinical trial, the third International Stroke Trial (IST-3). We evaluate key aspects of MAE pre-trained ViTs in IAC segmentation, and analyse the clinical implications. We show: 1) our calibrated self-supervised ViT beats a strong supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial for ViTs for IAC segmentation and interpolation upsampling with regular convolutions is preferable to transposed convolutions for ViT-based models, and 3) our ViTs increase robustness to higher slice thicknesses and improve risk group classification in a clinical scenario by 46%. Our code is available online. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸå·²ç»å¤§å—æ¬¢è¿ï¼Œä½†åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„è¡¨ç°å´ä¸å°½å¦‚äººæ„ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å‹åŒ»å­¦å›¾åƒä½“ç§¯è€Œè¨€ï¼Œ3D ViTsç‰¹åˆ«æœ‰è¶£ï¼Œå› ä¸ºå®ƒä»¬èƒ½åœ¨æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰æ¡†æ¶å†…è¿›è¡Œé«˜æ•ˆçš„è‡ªç›‘ç£è®­ç»ƒï¼Œä»è€Œèƒ½å¤Ÿä½¿ç”¨å›¾åƒæ•°æ®è€Œæ— éœ€æ˜‚è´µçš„æ‰‹åŠ¨æ³¨é‡Šã€‚é¢…å†…åŠ¨è„‰é’™åŒ–ï¼ˆIACï¼‰æ˜¯å¸¸è§„CTæ‰«æä¸Šå¯è§çš„ç¥ç»è¡€ç®¡ç–¾ç—…ï¼ˆå¦‚ä¸­é£å’Œç—´å‘†ç—‡ï¼‰çš„æˆåƒç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œè‡ªåŠ¨çš„IACå®šé‡è¯„ä¼°å¯å®ç°å¤§è§„æ¨¡é£é™©è¯„ä¼°ã€‚æˆ‘ä»¬é¦–æ¬¡ä½¿ç”¨MAEå¯¹ViTsè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥è¿›è¡ŒIACåˆ†å‰²ã€‚ä¸ºäº†å¼€å‘æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªå¤§å‹ä¸´åºŠè¯•éªŒçš„å…·æœ‰é«˜åº¦å¼‚è´¨æ€§çš„æ•°æ®â€”â€”ç¬¬ä¸‰æ¬¡å›½é™…ä¸­é£è¯•éªŒï¼ˆIST-3ï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†MAEé¢„è®­ç»ƒçš„ViTsåœ¨IACåˆ†å‰²æ–¹é¢çš„å…³é”®æ–¹é¢ï¼Œå¹¶åˆ†æäº†å…¶ä¸´åºŠæ„ä¹‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼š1ï¼‰æˆ‘ä»¬çš„æ ¡å‡†è‡ªç›‘ç£ViTæ¯”å—ç›‘ç£çš„nnU-NetåŸºå‡†æµ‹è¯•é«˜å‡º3.2 Diceç‚¹ï¼›2ï¼‰å¯¹äºIACåˆ†å‰²çš„ViTsè€Œè¨€ï¼Œè¾ƒå°çš„è¡¥ä¸å°ºå¯¸è‡³å…³é‡è¦ï¼Œå¹¶ä¸”å¯¹äºåŸºäºViTçš„æ¨¡å‹è€Œè¨€ï¼Œä½¿ç”¨å¸¸è§„å·ç§¯è¿›è¡Œæ’å€¼ä¸Šé‡‡æ ·æ¯”è½¬ç½®å·ç§¯æ›´å¯å–ï¼›3ï¼‰æˆ‘ä»¬çš„ViTsæé«˜äº†å¯¹æ›´é«˜åˆ‡ç‰‡åšåº¦çš„é²æ£’æ€§ï¼Œå¹¶åœ¨ä¸´åºŠåœºæ™¯ä¸­æé«˜äº†é£é™©ç»„åˆ†ç±»çš„å‡†ç¡®æ€§è¾¾46%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ç½‘ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01744v2">PDF</a> Accepted at the 3rd Data Engineering in Medical Imaging workshop @   MICCAI 2025</p>
<p><strong>Summary</strong><br>     è¦–è¦ºè®Šæ›å™¨ï¼ˆViTsï¼‰åœ¨è‡ªç„¶åœ–åƒé¢†åŸŸå»£å—æ¬¢è¿ï¼Œä½†åœ¨3DåŒ»å­¦å½±åƒåˆ†å‰²ä¸Šæ•ˆæœè¼ƒå·®ã€‚ç„¶è€Œï¼Œç”±æ–¼å…¶å†…åœ¨è‡ªç›‘ç£è®­ç»ƒçš„MAEæ¡†æ¶å¯æœ‰æ•ˆåˆ©ç”¨å½±åƒæ•°æ®ï¼Œå³ä½¿ä¸éœ€æ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œä¹Ÿèƒ½åœ¨å¤§è¦æ¨¡åŒ»å­¦å½±åƒä¸­å¯¦ç¾é«˜åŠŸæ•ˆã€‚æœ¬æ–‡é¦–æ¬¡é è¨“ç·´ViTsæ–¼MAEæ¡†æ¶ä¸Šï¼Œå¹¶å¾®è°ƒç”¨äºé¢…å†…å‹•è„‰é’™åŒ–ï¼ˆIACï¼‰åˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬è¿ç”¨ä¾†è‡ªå¤§è§„æ¨¡è¨“ç·´æ•°æ®çš„ã€å…·æœ‰æ˜¾è‘—å·®å¼‚çš„æ•°æ®é€²è¡Œäº†è¨“ç·´å’Œè¯„ä¼°ï¼Œä¸¦åˆ†æäº†å…¶ä¸´åºŠå½±å“ã€‚æœ¬æ–‡çš„ç ”ç©¶çµæœåŒ…æ‹¬ï¼šæˆ‘å€‘çš„æ ¡æº–è‡ªç›‘ç£ViTæ¯”å¼ºå¤§çš„ç›‘ç£å‹nnU-NetåŸºå‡†é«˜å‡º3.2 Diceç‚¹ï¼›ä½å¡Šå¤§å°å°ViTé€²è¡ŒIACåˆ†å‰²éå¸¸é—œéµï¼›æ’å€¼ä¸Šé‡‡æ¨£ä½¿ç”¨å¸¸è¦å·ç©å„ªäºè½‰ç½®å·ç©å°åŸºæ–¼ViTçš„æ¨¡å‹ï¼›æˆ‘ä»¬çš„ViTèƒ½æé«˜é¢¨éšªç¾¤çµ„åˆ†æœŸçš„å¥ç¢æ€§é”åˆ°æé«˜è¿‘ä¸€åŠçš„é æ¸¬èƒ½åŠ›ã€‚æˆ‘ä»¬æä¾›ç›¸é—œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViTs) are effectively applied to 3D medical image segmentation, particularly for large medical imaging volumes.</li>
<li>MAE framework enables self-supervised training of ViTs, facilitating the use of imaging data without manual annotations.</li>
<li>For the first time, ViTs are pre-trained and fine-tuned for intracranial arterial calcification (IAC) segmentation.</li>
<li>Low patch sizes are crucial for ViTs in IAC segmentation, and interpolation upsampling with regular convolutions is preferred.</li>
<li>ViTs improve robustness to higher slice thicknesses and enhance risk group classification in a clinical scenario.</li>
<li>The study demonstrates a significant performance boost over a strong supervised nnU-Net baseline, with a 3.2 Dice point increase.</li>
<li>The research has practical implications in large-scale risk assessment for neurovascular diseases like stroke and dementia through automated IAC quantification.</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e726a17ee8dd91c5be7f0257702a8198.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7335c39ff98765dc1910d40d5a0d93ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1da436980b5adefc8b6a51c0cea99a9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ae879be75d1c2b8c514666adaaded1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2525aa2c1471a186ed786f33ff586c31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3561540d9bfe9c674119444e86d4574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cc980406a9efa35e389237bd0154996.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Lightweight-Transformer-with-Phase-Only-Cross-Attention-for-Illumination-Invariant-Biometric-Authentication"><a href="#A-Lightweight-Transformer-with-Phase-Only-Cross-Attention-for-Illumination-Invariant-Biometric-Authentication" class="headerlink" title="A Lightweight Transformer with Phase-Only Cross-Attention for   Illumination-Invariant Biometric Authentication"></a>A Lightweight Transformer with Phase-Only Cross-Attention for   Illumination-Invariant Biometric Authentication</h2><p><strong>Authors:Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza, Bishakh Bhattacharya</strong></p>
<p>Traditional biometric systems have encountered significant setbacks due to various unavoidable factors, for example, wearing of face masks in face recognition-based biometrics and hygiene concerns in fingerprint-based biometrics. This paper proposes a novel lightweight vision transformer with phase-only cross-attention (POC-ViT) using dual biometric traits of forehead and periocular portions of the face, capable of performing well even with face masks and without any physical touch, offering a promising alternative to traditional methods. The POC-ViT framework is designed to handle two biometric traits and to capture inter-dependencies in terms of relative structural patterns. Each channel consists of a Cross-Attention using phase-only correlation (POC) that captures both their individual and correlated structural patterns. The computation of cross-attention using POC extracts the phase correlation in the spatial features. Therefore, it is robust against variations in resolution and intensity, as well as illumination changes in the input images. The lightweight model is suitable for edge device deployment. The performance of the proposed framework was successfully demonstrated using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP) database, having 350 subjects. The POC-ViT framework outperformed state-of-the-art methods with an outstanding classification accuracy of $98.8%$ with the dual biometric traits. </p>
<blockquote>
<p>ä¼ ç»Ÿç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿç”±äºå„ç§ä¸å¯é¿å…çš„å› ç´ è€Œé­é‡äº†é‡å¤§æŒ«æŠ˜ï¼Œä¾‹å¦‚åœ¨åŸºäºé¢éƒ¨è¯†åˆ«çš„ç”Ÿç‰©è¯†åˆ«ä¸­ä½©æˆ´å£ç½©çš„é—®é¢˜ï¼Œä»¥åŠåœ¨åŸºäºæŒ‡çº¹è¯†åˆ«çš„ç”Ÿç‰©è¯†åˆ«ä¸­çš„å«ç”Ÿé—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹è½»é‡çº§è§†è§‰è½¬æ¢å™¨ï¼Œå³ä»…ç›¸ä½äº¤å‰æ³¨æ„åŠ›ï¼ˆPOC-ViTï¼‰ï¼Œå®ƒä½¿ç”¨é¢å¤´å’Œçœ¼éƒ¨å‘¨å›´çš„é¢éƒ¨åŒé‡ç”Ÿç‰©ç‰¹å¾ï¼Œå³ä½¿ä½©æˆ´å£ç½©æ— éœ€ä»»ä½•ç‰©ç†æ¥è§¦ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ï¼Œä¸ºä¼ ç»Ÿæ–¹æ³•æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚POC-ViTæ¡†æ¶è¢«è®¾è®¡æˆå¤„ç†ä¸¤ç§ç”Ÿç‰©ç‰¹å¾ï¼Œå¹¶æ•è·ç›¸å¯¹ç»“æ„æ¨¡å¼çš„ç›¸äº’ä¾èµ–æ€§ã€‚æ¯ä¸ªé€šé“ç”±ä»…ä½¿ç”¨ç›¸ä½ç›¸å…³æ€§çš„äº¤å‰æ³¨æ„åŠ›ï¼ˆPOCï¼‰ç»„æˆï¼Œèƒ½å¤Ÿæ•è·å®ƒä»¬å„è‡ªçš„å’Œç›¸å…³è”çš„ç»“æ„æ¨¡å¼ã€‚ä½¿ç”¨POCè®¡ç®—äº¤å‰æ³¨æ„åŠ›æå–ç©ºé—´ç‰¹å¾çš„ç›¸ä½ç›¸å…³æ€§ã€‚å› æ­¤ï¼Œå®ƒå¯¹åˆ†è¾¨ç‡å’Œå¼ºåº¦çš„å˜åŒ–ã€è¾“å…¥å›¾åƒä¸­çš„ç…§æ˜å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚è¿™ç§è½»é‡çº§æ¨¡å‹é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ã€‚ä½¿ç”¨é¢å¤´çš®ä¸‹é™è„‰æ¨¡å¼å’Œçœ¼éƒ¨ç”Ÿç‰©ç‰¹å¾æ¨¡å¼ï¼ˆFSVP-PBPï¼‰æ•°æ®åº“ï¼ˆåŒ…å«350ä¸ªä¸»ä½“ï¼‰æˆåŠŸå±•ç¤ºäº†æ‰€æå‡ºæ¡†æ¶çš„æ€§èƒ½ã€‚POC-ViTæ¡†æ¶åˆ©ç”¨åŒé‡ç”Ÿç‰©ç‰¹å¾å®ç°äº†å“è¶Šçš„åˆ†ç±»å‡†ç¡®ç‡ï¼ˆ98.8%ï¼‰ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19160v3">PDF</a> Submitted to IEEE</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹è½»é‡çº§è§†è§‰è½¬æ¢å™¨POC-ViTè¢«æå‡ºï¼Œå®ƒåˆ©ç”¨é¢å¤´å’Œçœ¼å‘¨çš„åŒç”Ÿç‰©ç‰¹å¾è¿›è¡Œé¢éƒ¨è¯†åˆ«ï¼Œèƒ½å¤Ÿåœ¨ä½©æˆ´å£ç½©çš„æƒ…å†µä¸‹å’Œæ— é¡»ä»»ä½•ç‰©ç†æ¥è§¦çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ï¼Œä¸ºä¼ ç»Ÿç”Ÿç‰©è¯†åˆ«æ–¹æ³•æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿå› ä¸å¯æŠ—å› ç´ ï¼ˆå¦‚æˆ´å£ç½©ã€å«ç”Ÿé—®é¢˜ï¼‰è€Œå—é˜»ã€‚</li>
<li>POC-ViTæ˜¯ä¸€ç§æ–°å‹è½»é‡çº§è§†è§‰è½¬æ¢å™¨ï¼Œä½¿ç”¨é¢å¤´å’Œçœ¼å‘¨çš„åŒç”Ÿç‰©ç‰¹å¾è¿›è¡Œè¯†åˆ«ã€‚</li>
<li>POC-ViTè®¾è®¡ç”¨äºå¤„ç†ä¸¤ç§ç”Ÿç‰©ç‰¹å¾ï¼Œå¹¶æ•æ‰å…¶ç›¸å¯¹ç»“æ„æ¨¡å¼çš„ç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>è·¨æ³¨æ„ä½¿ç”¨POCè®¡ç®—æå–ç©ºé—´ç‰¹å¾çš„ç›¸ä½ç›¸å…³æ€§ï¼Œä½¿å…¶å¯¹åˆ†è¾¨ç‡å’Œå¼ºåº¦å˜åŒ–ã€è¾“å…¥å›¾åƒä¸­çš„ç…§æ˜å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚</li>
<li>è¯¥è½»é‡çº§æ¨¡å‹é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ã€‚</li>
<li>åœ¨ä½¿ç”¨é¢å¤´çš®ä¸‹è¡€ç®¡æ¨¡å¼å’Œçœ¼å‘¨ç”Ÿç‰©ç‰¹å¾æ¨¡å¼çš„æ•°æ®åº“ä¸­è¿›è¡Œæµ‹è¯•ï¼ŒPOC-ViTæ¡†æ¶è¡¨ç°å‡ºå“è¶Šçš„åˆ†ç±»å‡†ç¡®æ€§ï¼Œè¾¾åˆ°98.8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98685b27ac6566c0b86f1a43e3482acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2dc33ec5d16713a530a426c0c3f3dad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99a8ecabb0b06fa71d017986b21d72f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd26f70ef56c192ee4bb33adddc3a026.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-99d1ce2fb51a4920e2ef39c53db45d83.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  Beyond conventional vision RGB-event fusion for robust object detection   in dynamic traffic scenarios
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91498fc8130f811e249fa4330b6fb585.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  Data-Efficient Learning for Generalizable Surgical Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
