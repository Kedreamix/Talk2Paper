<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  A Comprehensive Evaluation framework of Alignment Techniques for LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2e39c3f7e03328e375be8c865853e45d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-15-æ›´æ–°"><a href="#2025-08-15-æ›´æ–°" class="headerlink" title="2025-08-15 æ›´æ–°"></a>2025-08-15 æ›´æ–°</h1><h2 id="A-Comprehensive-Evaluation-framework-of-Alignment-Techniques-for-LLMs"><a href="#A-Comprehensive-Evaluation-framework-of-Alignment-Techniques-for-LLMs" class="headerlink" title="A Comprehensive Evaluation framework of Alignment Techniques for LLMs"></a>A Comprehensive Evaluation framework of Alignment Techniques for LLMs</h2><p><strong>Authors:Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi</strong></p>
<p>As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œç¡®ä¿å…¶è¾“å‡ºç¬¦åˆäººç±»ä»·å€¼è§‚å’Œå®‰å…¨æ ‡å‡†å˜å¾—è‡³å…³é‡è¦ã€‚è¯¥é¢†åŸŸå·²ç»å¼€å‘äº†å¤šç§å¯¹é½æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼ˆRLHFã€æŒ‡ä»¤å¾®è°ƒï¼‰ã€äº‹åæ ¡æ­£ç³»ç»Ÿå’Œæ¨ç†æ—¶é—´å¹²é¢„ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚ç„¶è€Œï¼Œç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ä½¿å¾—éš¾ä»¥ç³»ç»Ÿåœ°æ¯”è¾ƒè¿™äº›èŒƒå¼å¹¶åšå‡ºéƒ¨ç½²å†³ç­–ã€‚æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯çš„å¤šç»´åº¦è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæä¾›äº†æ‰€æœ‰ä¸»è¦å¯¹é½èŒƒå¼çš„ç³»ç»Ÿæ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ¡†æ¶æ²¿ç€å››ä¸ªå…³é”®ç»´åº¦è¯„ä¼°æ–¹æ³•ï¼šå¯¹é½æ£€æµ‹ã€å¯¹é½è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œç¨³å¥æ€§ã€‚é€šè¿‡å¯¹ä¸åŒçš„åŸºç¡€æ¨¡å‹å’Œå¯¹é½ç­–ç•¥è¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ¡†æ¶åœ¨è¯†åˆ«å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§æ–¹é¢çš„å®ç”¨æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09937v1">PDF</a> In submission</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®åº”ç”¨ä¸­çš„é›†æˆæ—¥ç›Šå¢å¤šï¼Œç¡®ä¿å…¶è¾“å‡ºç¬¦åˆäººç±»ä»·å€¼è§‚å’Œå®‰å…¨æ ‡å‡†è‡³å…³é‡è¦ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯¹LLMçš„å¯¹é½æŠ€æœ¯è¿›è¡Œäº†å¤šç»´è¯„ä»·ï¼Œè¯¥æ¡†æ¶æä¾›äº†å„ä¸»é€šè¿‡å¯¹é½æ–¹æ³•æ²¿ç€å››ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼šå¯¹é½æ£€æµ‹ã€å¯¹é½è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œç¨³å¥æ€§ã€‚é€šè¿‡è·¨ä¸åŒåŸºç¡€æ¨¡å‹å’Œå¯¹é½ç­–ç•¥çš„è¯•éªŒï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®åº”ç”¨ä¸­çš„é›†æˆå¼•å‘äº†å¯¹ç¡®ä¿å…¶è¾“å‡ºç¬¦åˆäººç±»ä»·å€¼è§‚å’Œå®‰å…¨æ ‡å‡†çš„å…³æ³¨ã€‚</li>
<li>å­˜åœ¨å¤šç§LLMå¯¹é½æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚å¼ºåŒ–å­¦ä¹ ã€æŒ‡ä»¤è°ƒæ•´ï¼‰ã€äº‹åæ ¡æ­£ç³»ç»Ÿå’Œæ¨ç†æ—¶é—´å¹²é¢„ç­‰ï¼Œå„æœ‰ä¼˜åŠ¿å’Œå±€é™ã€‚</li>
<li>ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œéš¾ä»¥ç³»ç»Ÿåœ°æ¯”è¾ƒå„ç§å¯¹é½èŒƒå¼å¹¶æŒ‡å¯¼éƒ¨ç½²å†³ç­–ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦ï¼ˆå¦‚å¯¹é½æ£€æµ‹ã€å¯¹é½è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œç¨³å¥æ€§ï¼‰è¯„ä¼°LLMçš„å¯¹é½æŠ€æœ¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æœ‰åŠ©äºè¯†åˆ«å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¹è¿›ç°æœ‰æ–¹æ³•å’Œå¼€å‘æ–°çš„å¯¹é½ç­–ç•¥æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e9c209fde65d61636c34b1d23a5f90a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbd1db1bada69c28f2ca2c9b9e390de8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da1b8b0f56150902ea6617ead343ddc1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning"><a href="#Beyond-Scaling-Law-A-Data-Efficient-Distillation-Framework-for-Reasoning" class="headerlink" title="Beyond Scaling Law: A Data-Efficient Distillation Framework for   Reasoning"></a>Beyond Scaling Law: A Data-Efficient Distillation Framework for   Reasoning</h2><p><strong>Authors:Xiaojun Wu, Xiaoguang Jiang, Huiyang Li, Jucai Zhai, Dengfeng Liu, Qiaobo Hao, Huang Liu, Zhiguo Yang, Ji Xie, Ninglun Gu, Jin Yang, Kailai Zhang, Yelun Bao, Jun Wang</strong></p>
<p>Large language models (LLMs) demonstrate remarkable reasoning capabilities in tasks such as algorithmic coding and mathematical problem-solving. Recent methods have improved reasoning through expanded corpus and multistage training combining reinforcement learning and supervised fine-tuning. Although some methods suggest that small but targeted dataset can incentivize reasoning via only distillation, a reasoning scaling laws is still taking shape, increasing computational costs. To address this, we propose a data-efficient distillation framework (DED) that optimizes the Pareto frontier of reasoning distillation. Inspired by the on-policy learning and diverse roll-out strategies of reinforcement learning, the key idea of our approach is threefold: (1) We identify that benchmark scores alone do not determine an effective teacher model. Through comprehensive comparisons of leading reasoning LLMs, we develop a method to select an optimal teacher model. (2) While scaling distillation can enhance reasoning, it often degrades out-of-domain performance. A carefully curated, smaller corpus achieves a balanced trade-off between in-domain and out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the student model to develop robust reasoning skills. We validate our method through evaluations on mathematical reasoning (AIME 2024&#x2F;2025, MATH-500) and code generation (LiveCodeBench), achieving state-of-the-art results with only 0.8k carefully curated examples, bypassing the need for extensive scaling. Our systematic analysis demonstrates that DED outperforms existing methods by considering factors beyond superficial hardness, token length, or teacher model capability. This work offers a practical and efficient pathway to advanced reasoning while preserving general capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æ³•ç¼–ç å’Œæ•°å­¦é—®é¢˜è§£å†³ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡æ‰©å¤§è¯­æ–™åº“å’Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å¾®è°ƒçš„å¤šé˜¶æ®µè®­ç»ƒï¼Œæé«˜äº†æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶ä¸€äº›æ–¹æ³•è®¤ä¸ºï¼Œåªéœ€é€šè¿‡è’¸é¦å°±å¯ä»¥æ¿€åŠ±æ¨ç†èƒ½åŠ›ï¼Œä½†æ¨ç†è§„æ¨¡æ³•åˆ™ä»åœ¨å½¢æˆä¸­ï¼Œè®¡ç®—æˆæœ¬ä¸æ–­å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆè’¸é¦æ¡†æ¶ï¼ˆDEDï¼‰ï¼Œä»¥ä¼˜åŒ–æ¨ç†è’¸é¦çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°å¼ºåŒ–å­¦ä¹ ä¸­çš„åœ¨çº¿ç­–ç•¥å­¦ä¹ å’Œå¤šæ ·åŒ–æ»šåŠ¨ç­–ç•¥å¯å‘ï¼Œå…¶å…³é”®æ€æƒ³æœ‰ä¸‰ç‚¹ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬å‘ç°ä»…å‡­åŸºå‡†æµ‹è¯•åˆ†æ•°å¹¶ä¸èƒ½ç¡®å®šæœ‰æ•ˆçš„æ•™å¸ˆæ¨¡å‹ã€‚é€šè¿‡å¯¹é¢†å…ˆçš„æ¨ç†LLMè¿›è¡Œå…¨é¢æ¯”è¾ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‰æ‹©æœ€ä½³æ•™å¸ˆæ¨¡å‹çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰è™½ç„¶è§„æ¨¡è’¸é¦å¯ä»¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒå¾€å¾€ä¼šé™ä½åŸŸå¤–æ€§èƒ½ã€‚ä¸€ä¸ªç²¾å¿ƒæŒ‘é€‰çš„å°å‹è¯­æ–™åº“èƒ½å¤Ÿåœ¨åŸŸå†…å’ŒåŸŸå¤–èƒ½åŠ›ä¹‹é—´å®ç°å¹³è¡¡ã€‚ï¼ˆ3ï¼‰å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹é¼“åŠ±å­¦ç”Ÿæ¨¡å‹å‘å±•ç¨³å¥çš„æ¨ç†æŠ€èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ•°å­¦æ¨ç†ï¼ˆAIME 2024&#x2F;2025ï¼ŒMATH-500ï¼‰å’Œä»£ç ç”Ÿæˆï¼ˆLiveCodeBenchï¼‰ä¸Šçš„è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨0.8kä¸ªç²¾å¿ƒæŒ‘é€‰çš„ç¤ºä¾‹å³å¯å®ç°æœ€æ–°ç»“æœï¼Œæ— éœ€å¤§è§„æ¨¡æ‰©å±•ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼ŒDEDåœ¨è¶…è¶Šè¡¨é¢éš¾åº¦ã€ä»¤ç‰Œé•¿åº¦æˆ–æ•™å¸ˆæ¨¡å‹èƒ½åŠ›ç­‰å› ç´ çš„æƒ…å†µä¸‹ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„é€”å¾„æ¥å®ç°é«˜çº§æ¨ç†ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09883v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®—æ³•ç¼–ç å’Œæ•°å­¦é—®é¢˜è§£å†³ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººç©ç›®çš„æ¨ç†èƒ½åŠ›ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡æ‰©å¤§è¯­æ–™åº“å’Œå¤šé˜¶æ®µè®­ç»ƒç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å¾®è°ƒæ¥æ”¹è¿›æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡æœ‰äº›æ–¹æ³•è®¤ä¸ºå°è€Œç²¾å‡†çš„æ•°æ®é›†å¯ä»¥é€šè¿‡è’¸é¦æ¿€åŠ±æ¨ç†ï¼Œä½†æ¨ç†è§„æ¨¡æ³•åˆ™ä»åœ¨å½¢æˆä¸­ï¼Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆè’¸é¦æ¡†æ¶ï¼ˆDEDï¼‰ï¼Œä¼˜åŒ–äº†æ¨ç†è’¸é¦çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥å¯å‘ï¼Œå…³é”®æ€æƒ³æœ‰ä¸‰ç‚¹ï¼šä¸€æ˜¯æˆ‘ä»¬ç¡®å®šäº†åŸºå‡†æµ‹è¯•åˆ†æ•°å¹¶ä¸èƒ½å•ç‹¬å†³å®šä¸€ä¸ªæœ‰æ•ˆçš„æ•™å¸ˆæ¨¡å‹ã€‚é€šè¿‡å¯¹é¢†å…ˆçš„æ¨ç†LLMçš„å…¨é¢æ¯”è¾ƒï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‰æ‹©æœ€ä½³æ•™å¸ˆæ¨¡å‹çš„æ–¹æ³•ã€‚äºŒæ˜¯è™½ç„¶è§„æ¨¡åŒ–è’¸é¦å¯ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒå¾€å¾€ä¼šé™ä½éåŸŸå†…æ€§èƒ½ã€‚ç²¾å¿ƒæŒ‘é€‰çš„å°è¯­æ–™åº“å¯ä»¥åœ¨åŸŸå†…å’ŒåŸŸå¤–èƒ½åŠ›ä¹‹é—´å®ç°å¹³è¡¡ã€‚ä¸‰æ˜¯å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹é¼“åŠ±å­¦ç”Ÿæ¨¡å‹å‘å±•ç¨³å¥çš„æ¨ç†æŠ€èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ•°å­¦æ¨ç†ï¼ˆAIME 2024&#x2F;2025ã€MATH-500ï¼‰å’Œä»£ç ç”Ÿæˆï¼ˆLiveCodeBenchï¼‰ä¸Šçš„è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨ç²¾å¿ƒæŒ‘é€‰çš„0.8kç¤ºä¾‹å³å¯è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæ— éœ€å¤§è§„æ¨¡æ‰©å±•ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼ŒDEDåœ¨è¶…è¶Šè¡¨é¢éš¾åº¦ã€ä»¤ç‰Œé•¿åº¦æˆ–æ•™å¸ˆæ¨¡å‹èƒ½åŠ›ç­‰å› ç´ çš„æƒ…å†µä¸‹ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”é«˜æ•ˆçš„é€”å¾„æ¥å®ç°é«˜çº§æ¨ç†èƒ½åŠ›çš„åŒæ—¶ä¿æŒä¸€èˆ¬èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨ç®—æ³•å’Œæ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡æ‰©å¤§è¯­æ–™åº“å’Œå¤šé˜¶æ®µè®­ç»ƒç»“åˆå¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒæ¥æ”¹å–„æ¨ç†ã€‚</li>
<li>æå‡ºä¸€ç§æ•°æ®é«˜æ•ˆè’¸é¦æ¡†æ¶ï¼ˆDEDï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨ç†è’¸é¦çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
<li>DEDçš„å…³é”®æ€æƒ³åŒ…æ‹¬é€‰æ‹©æœ€ä½³æ•™å¸ˆæ¨¡å‹ã€å¹³è¡¡åŸŸå†…å’ŒåŸŸå¤–èƒ½åŠ›ï¼Œä»¥åŠé¼“åŠ±å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDEDåœ¨ç²¾å¿ƒæŒ‘é€‰çš„å°æ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDEDè€ƒè™‘äº†è¶…è¶Šè¡¨é¢éš¾åº¦ã€ä»¤ç‰Œé•¿åº¦å’Œæ•™å¸ˆæ¨¡å‹èƒ½åŠ›çš„å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be18b3dc6fb6b4fca9e2223bf16ce82f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d8ca2a1b41507080eb5ca77008fa438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-930dd47922487cb735156a8b5826bc5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c32012235e6713faace476ff75284799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1871aee9fc4780d35d7aaa5b3d40dd10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05a1dd9641c1ca0faf22d4206b90c69c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc8a753ca2a511f7f1c81bc236fb5c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6165896e9a1be66acea3260720220765.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts"><a href="#PRELUDE-A-Benchmark-Designed-to-Require-Global-Comprehension-and-Reasoning-over-Long-Contexts" class="headerlink" title="PRELUDE: A Benchmark Designed to Require Global Comprehension and   Reasoning over Long Contexts"></a>PRELUDE: A Benchmark Designed to Require Global Comprehension and   Reasoning over Long Contexts</h2><p><strong>Authors:Mo Yu, Tsz Ting Chung, Chulun Zhou, Tong Li, Rui Lu, Jiangnan Li, Liyan Xu, Haoshu Lu, Ning Zhang, Jing Li, Jie Zhou</strong></p>
<p>We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a characterâ€™s prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks â€“ as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by &gt;15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†PRELUDEåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜¯é€šè¿‡åˆ¤æ–­è§’è‰²å‰ä¼ æ•…äº‹æ˜¯å¦ç¬¦åˆåŸè‘—çš„è§„èŒƒå™äº‹æ¥è¯„ä¼°å¯¹é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ä»»åŠ¡å¯¹å…¨å±€ç†è§£å’Œæ·±åº¦æ¨ç†çš„è¦æ±‚é«˜äºç°æœ‰åŸºå‡†æµ‹è¯•â€”â€”ç”±äºå‰ä¼ ä¸æ˜¯åŸæ•…äº‹çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤è¯„ä¼°å…¶åˆç†æ€§é€šå¸¸éœ€è¦æœç´¢å’Œæ•´åˆé—´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼Œ88%çš„å®ä¾‹éœ€è¦æ¥è‡ªå™äº‹å¤šä¸ªéƒ¨åˆ†çš„è¯æ®ã€‚å®éªŒç»“æœçªå‡ºäº†æˆ‘ä»¬ä»»åŠ¡çš„æŒ‘æˆ˜æ€§ï¼šåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ã€RAGå’ŒåŸºäºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢†åŸŸå†…è®­ç»ƒä»¥åŠå•†ä¸šæ·±åº¦ç ”ç©¶æœåŠ¡æ–¹é¢ï¼Œä¸äººç±»ç›¸æ¯”éƒ½æœ‰è¶…è¿‡15%çš„å·®è·ã€‚è¿›ä¸€æ­¥çš„äººç±»ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹å¸¸å¸¸åœ¨æ¨ç†ä¸Šæœ‰ç¼ºé™·ï¼Œå³ä½¿ç­”æ¡ˆæ­£ç¡®ï¼Œä¸äººç±»ç›¸æ¯”æ¨ç†å‡†ç¡®æ€§ä»æœ‰è¶…è¿‡30%çš„å·®è·ã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨é•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09848v1">PDF</a> First 7 authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://gorov.github.io/prelude">https://gorov.github.io/prelude</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æå‡ºäº†PRELUDEåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°é€šè¿‡åˆ¤æ–­è§’è‰²å‰ä¼ æ•…äº‹æ˜¯å¦ä¸åŸè‘—çš„è§„èŒƒå™äº‹ä¸€è‡´æ¥è¯„ä»·å¯¹é•¿æ–‡æœ¬å†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»»åŠ¡å¯¹å…¨å±€ç†è§£å’Œæ·±åº¦æ¨ç†çš„è¦æ±‚é«˜äºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œå› ä¸ºå‰ä¼ æ•…äº‹å¹¶éåŸè‘—å†…å®¹çš„ä¸€éƒ¨åˆ†ï¼Œè¯„ä¼°å…¶åˆç†æ€§é€šå¸¸éœ€è¦æœç´¢å’Œæ•´åˆé—´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼Œæœ‰ç™¾åˆ†ä¹‹å…«åå…«çš„ä¾‹å­éœ€è¦å¼•ç”¨å¤šä¸ªå™äº‹éƒ¨åˆ†çš„è¯æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼šå³ä½¿åœ¨åˆ©ç”¨æœ€å…ˆè¿›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€ç”Ÿæˆå¢å¼ºç½‘ç»œæŠ€æœ¯å’Œæ·±åº¦é¢†åŸŸè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥åŠå•†ä¸šæ·±åº¦ç ”ç©¶æœåŠ¡çš„æƒ…å†µä¸‹ï¼Œä¸äººç±»ç›¸æ¯”ä»è½åè¶…è¿‡ç™¾åˆ†ä¹‹åäº”ã€‚è¿›ä¸€æ­¥çš„äººç±»ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹è™½ç„¶èƒ½äº§å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä½†æ¨ç†è¿‡ç¨‹å¾€å¾€å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´æ¨ç†å‡†ç¡®åº¦ä¸äººç±»ç›¸å·®è¶…è¿‡ç™¾åˆ†ä¹‹ä¸‰åã€‚è¿™äº›å‘ç°å‡¸æ˜¾å‡ºåœ¨é•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>PRELUDEæ˜¯ä¸€ä¸ªè¯„ä¼°é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡åˆ¤æ–­è§’è‰²å‰ä¼ æ•…äº‹æ˜¯å¦ä¸åŸè‘—ä¸€è‡´æ¥è¯„ä»·ç†è§£ç¨‹åº¦ã€‚</li>
<li>è¯¥ä»»åŠ¡è¦æ±‚æ›´é«˜çš„å…¨å±€ç†è§£å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œå› ä¸ºå‰ä¼ æ•…äº‹çš„åˆç†æ€§éœ€è¦æ•´åˆé—´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°ä¾‹å­éœ€è¦å¼•ç”¨å¤šä¸ªå™äº‹éƒ¨åˆ†çš„è¯æ®æ¥åˆ¤æ–­å‰ä¼ æ•…äº‹çš„åˆç†æ€§ã€‚</li>
<li>æœ€å…ˆè¿›çš„LLMså’Œæ·±åº¦ç ”ç©¶æœåŠ¡åœ¨è¿›è¡Œæ­¤ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸äººç±»è¡¨ç°ç›¸å·®è¶…è¿‡ç™¾åˆ†ä¹‹åäº”ã€‚</li>
<li>è¿›ä¸€æ­¥ç ”ç©¶æ­ç¤ºæ¨¡å‹è™½ç„¶èƒ½äº§å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä½†æ¨ç†è¿‡ç¨‹å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´æ¨ç†å‡†ç¡®åº¦è¾ƒä½ã€‚</li>
<li>æ¨¡å‹ä¸äººç±»åœ¨æ¨ç†å‡†ç¡®åº¦ä¸Šå­˜åœ¨çš„å·®è·è¶…è¿‡ç™¾åˆ†ä¹‹ä¸‰åï¼Œå‡¸æ˜¾å‡ºé•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4380c0cf2c6ac02d631cc75c6d8f7978.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e383127d2c4ecb9b4c1c55bfb19fe94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb25035232d675fb107e10134ad5393f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d7244b626cf31639f022bd86348162b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d4ec3f0a7d11d5b015890fdc88b245f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69597ff74ef4b7a3b98544173f696d56.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BigCharts-R1-Enhanced-Chart-Reasoning-with-Visual-Reinforcement-Finetuning"><a href="#BigCharts-R1-Enhanced-Chart-Reasoning-with-Visual-Reinforcement-Finetuning" class="headerlink" title="BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement   Finetuning"></a>BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement   Finetuning</h2><p><strong>Authors:Ahmed Masry, Abhay Puri, Masoud Hashemi, Juan A. Rodriguez, Megh Thakkar, Khyati Mahajan, Vikas Yadav, Sathwik Tejaswi Madhusudhan, Alexandre PichÃ©, Dzmitry Bahdanau, Christopher Pal, David Vazquez, Enamul Hoque, Perouz Taslakian, Sai Rajeswar, Spandana Gella</strong></p>
<p>Charts are essential to data analysis, transforming raw data into clear visual representations that support human decision-making. Although current vision-language models (VLMs) have made significant progress, they continue to struggle with chart comprehension due to training on datasets that lack diversity and real-world authenticity, or on automatically extracted underlying data tables of charts, which can contain numerous estimation errors. Furthermore, existing models only rely on supervised fine-tuning using these low-quality datasets, severely limiting their effectiveness. To address these issues, we first propose BigCharts, a dataset creation pipeline that generates visually diverse chart images by conditioning the rendering process on real-world charts sourced from multiple online platforms. Unlike purely synthetic datasets, BigCharts incorporates real-world data, ensuring authenticity and visual diversity, while still retaining accurate underlying data due to our proposed replotting process. Additionally, we introduce a comprehensive training framework that integrates supervised fine-tuning with Group Relative Policy Optimization (GRPO)-based reinforcement learning. By introducing novel reward signals specifically designed for chart reasoning, our approach enhances model robustness and generalization across diverse chart styles and domains, resulting in a state-of-the-art chart reasoning model, BigCharts-R1. Extensive experiments demonstrate that our models surpass existing methods on multiple chart question-answering benchmarks compared to even larger open-source and closed-source models. </p>
<blockquote>
<p>å›¾è¡¨æ˜¯æ•°æ®åˆ†æçš„æ ¸å¿ƒï¼Œå°†åŸå§‹æ•°æ®è½¬åŒ–ä¸ºæ¸…æ™°çš„è§†è§‰è¡¨ç°å½¢å¼ï¼Œä»¥æ”¯æŒäººç±»çš„å†³ç­–åˆ¶å®šã€‚å°½ç®¡å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®é›†ç¼ºä¹å¤šæ ·æ€§å’ŒçœŸå®ä¸–ç•Œçš„çœŸå®æ€§ï¼Œæˆ–è€…ä¾èµ–äºå›¾è¡¨ä¸­è‡ªåŠ¨æå–çš„åº•å±‚æ•°æ®è¡¨ï¼ˆå¯èƒ½åŒ…å«å¤§é‡ä¼°ç®—è¯¯å·®ï¼‰ï¼Œå®ƒä»¬ä»åœ¨å›¾è¡¨ç†è§£æ–¹é¢é‡åˆ°å›°éš¾ã€‚æ­¤å¤–ï¼Œç°æœ‰æ¨¡å‹ä»…ä¾èµ–ä½¿ç”¨è¿™äº›ä½è´¨é‡æ•°æ®é›†è¿›è¡Œçš„æœ‰ç›‘ç£å¾®è°ƒï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºBigChartsæ•°æ®é›†åˆ›å»ºç®¡é“ï¼Œé€šè¿‡ä»¥ä»å¤šä¸ªåœ¨çº¿å¹³å°è·å–çš„çœŸå®ä¸–ç•Œå›¾è¡¨ä¸ºæ¡ä»¶ï¼Œç”Ÿæˆè§†è§‰å¤šæ ·çš„å›¾è¡¨å›¾åƒã€‚ä¸çº¯åˆæˆæ•°æ®é›†ä¸åŒï¼ŒBigChartsèå…¥äº†çœŸå®ä¸–ç•Œæ•°æ®ï¼Œç¡®ä¿äº†çœŸå®æ€§å’Œè§†è§‰å¤šæ ·æ€§ï¼ŒåŒæ—¶ä»ä¿ç•™äº†åº•å±‚æ•°æ®çš„å‡†ç¡®æ€§ï¼Œè¿™æ˜¯ç”±äºæˆ‘ä»¬æå‡ºçš„é‡æ–°ç»˜å›¾è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„è®­ç»ƒæ¡†æ¶ï¼Œå°†ç›‘ç£å¾®è°ƒä¸åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆã€‚é€šè¿‡å¼•å…¥ä¸“é—¨é’ˆå¯¹å›¾è¡¨æ¨ç†çš„æ–°å‹å¥–åŠ±ä¿¡å·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ¨¡å‹åœ¨å„ç§å›¾è¡¨é£æ ¼å’Œé¢†åŸŸä¸­çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œäº§ç”Ÿäº†æœ€å…ˆè¿›çš„å›¾è¡¨æ¨ç†æ¨¡å‹â€”â€”BigCharts-R1ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªå›¾è¡¨é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸æ›´å¤§çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09804v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ•°æ®å¯è§†åŒ–å›¾è¡¨å¯¹æ•°æ®åˆ†æè‡³å…³é‡è¦ï¼Œç›®å‰è™½ç„¶å·²æœ‰ä¸€äº›è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½†ç”±äºç¼ºä¹å¤šæ ·æ€§å’ŒçœŸå®æ€§çš„æ•°æ®é›†ï¼Œä»¥åŠå¯¹å›¾è¡¨æ•°æ®è‡ªåŠ¨æå–çš„è¯¯å·®ç­‰é—®é¢˜ï¼Œè¿™äº›æ¨¡å‹åœ¨å›¾è¡¨ç†è§£æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†BigChartsæ•°æ®é›†åˆ›å»ºæµç¨‹ï¼Œè¯¥æµç¨‹åŸºäºçœŸå®ä¸–ç•Œå›¾è¡¨ç”Ÿæˆè§†è§‰å¤šæ ·çš„å›¾è¡¨å›¾åƒï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç»¼åˆè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒä¸åŸºäºGroup Relative Policy Optimizationçš„å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ¡†æ¶ä¸ºå›¾è¡¨æ¨ç†å¼•å…¥æ–°å‹å¥–åŠ±ä¿¡å·ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œè·¨ä¸åŒå›¾è¡¨é£æ ¼å’Œé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œå½¢æˆå½“å‰æœ€å…ˆè¿›çš„å›¾è¡¨æ¨ç†æ¨¡å‹BigCharts-R1ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å›¾è¡¨åœ¨æ•°æ®åˆ†æä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œèƒ½å¤Ÿå°†åŸå§‹æ•°æ®è½¬åŒ–ä¸ºæ¸…æ™°çš„è§†è§‰è¡¨ç°å½¢å¼ä»¥æ”¯æŒå†³ç­–åˆ¶å®šã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ•°æ®é›†ç¼ºä¹å¤šæ ·æ€§å’ŒçœŸå®æ€§ä»¥åŠå›¾è¡¨æ•°æ®è‡ªåŠ¨æå–çš„è¯¯å·®ã€‚</li>
<li>BigChartsæ•°æ®é›†åˆ›å»ºæµç¨‹é€šè¿‡ç»“åˆçœŸå®ä¸–ç•Œå›¾è¡¨ç”Ÿæˆè§†è§‰å¤šæ ·çš„å›¾è¡¨å›¾åƒï¼Œä¿è¯çœŸå®æ€§å’Œè§†è§‰å¤šæ ·æ€§çš„åŒæ—¶ï¼Œä¿ç•™äº†å‡†ç¡®çš„åº•å±‚æ•°æ®ã€‚</li>
<li>ç»¼åˆè®­ç»ƒæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å›¾è¡¨æ¨ç†æ–¹é¢çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œé€šè¿‡å¼•å…¥æ–°å‹å¥–åŠ±ä¿¡å·å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>BigCharts-R1æ¨¡å‹åœ¨å¤šä¸ªå›¾è¡¨é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œç›¸è¾ƒäºå¼€æºå’Œé—­æºæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1e3d9684a41c7dc408a94942a95a3235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7af69108c467dedde116bc69431e5424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29c5c53d7a939776208b6f7320e7f3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca0f54a1d3ba92ad33b8f19a099fa906.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory"><a href="#Seeing-Listening-Remembering-and-Reasoning-A-Multimodal-Agent-with-Long-Term-Memory" class="headerlink" title="Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with   Long-Term Memory"></a>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with   Long-Term Memory</h2><p><strong>Authors:Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</strong></p>
<p>We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robotâ€™s perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at <a target="_blank" rel="noopener" href="https://github.com/bytedance-seed/m3-agent">https://github.com/bytedance-seed/m3-agent</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†M3-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªé…å¤‡é•¿æœŸè®°å¿†çš„æ–°å‹å¤šæ¨¡æ€ä»£ç†æ¡†æ¶ã€‚åƒäººç±»ä¸€æ ·ï¼ŒM3-Agentå¯ä»¥å¤„ç†å®æ—¶è§†è§‰å’Œå¬è§‰è¾“å…¥æ¥æ„å»ºå’Œæ›´æ–°å…¶é•¿æœŸè®°å¿†ã€‚é™¤äº†æƒ…æ™¯è®°å¿†ä¹‹å¤–ï¼Œå®ƒè¿˜å‘å±•å‡ºè¯­ä¹‰è®°å¿†ï¼Œä½¿å…¶èƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»ç§¯ç´¯ä¸–ç•ŒçŸ¥è¯†ã€‚å®ƒçš„è®°å¿†ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€æ ¼å¼ç»„ç»‡ï¼Œå…è®¸å¯¹ç¯å¢ƒçš„æ›´æ·±å’Œæ›´ä¸€è‡´çš„ç†è§£ã€‚æ¥æ”¶åˆ°æŒ‡ä»¤åï¼ŒM3-Agentä¼šè‡ªä¸»è¿›è¡Œå¤šè½®è¿­ä»£æ¨ç†ï¼Œä»è®°å¿†ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ä»¥å®Œæˆä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°å¤šæ¨¡æ€ä»£ç†ä¸­çš„è®°å¿†æœ‰æ•ˆæ€§å’ŒåŸºäºè®°å¿†æ¨ç†çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†M3-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ã€‚M3-BenchåŒ…æ‹¬100ä¸ªæ–°å½•åˆ¶çš„ä¸–ç•Œè§†é¢‘ï¼ˆä»æœºå™¨äººè§†è§’æ•è·ï¼‰ï¼ˆM3-Bench-robotï¼‰å’Œ929ä¸ªæ¶µç›–ä¸åŒåœºæ™¯çš„ç½‘é¡µè§†é¢‘ï¼ˆM3-Bench-webï¼‰ã€‚æˆ‘ä»¬æ³¨é‡Šäº†é—®é¢˜å’Œç­”æ¡ˆå¯¹ï¼Œæ—¨åœ¨æµ‹è¯•ä»£ç†åº”ç”¨ç¨‹åºå¿…éœ€çš„å…³é”®èƒ½åŠ›ï¼Œä¾‹å¦‚äººç±»ç†è§£ã€ä¸€èˆ¬çŸ¥è¯†æå–å’Œè·¨æ¨¡æ€æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„M3-Agentè¶…è¶Šäº†æœ€å¼ºåŸºçº¿â€”â€”ä½¿ç”¨Gemini-1.5-proå’ŒGPT-4oçš„æç¤ºä»£ç†ï¼Œåœ¨M3-Bench-robotã€M3-Bench-webå’ŒVideoMME-longä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†6.7%ã€7.7%å’Œ5.3%ã€‚æˆ‘ä»¬çš„å·¥ä½œæ¨åŠ¨äº†å¤šæ¨¡æ€ä»£ç†æœç€æ›´ç±»ä¼¼äºäººç±»çš„é•¿æ—¶è®°å¿†æ–¹å‘å‘å±•ï¼Œå¹¶ä¸ºå…¶å®è·µè®¾è®¡æä¾›äº†è§è§£ã€‚æ¨¡å‹ã€ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bytedance-seed/m3-agent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bytedance-seed/m3-agentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09736v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>M3-Agentæ˜¯ä¸€ä¸ªé…å¤‡é•¿æœŸè®°å¿†çš„å¤šæ¨¡æ€ä»£ç†æ¡†æ¶ï¼Œèƒ½å¤„ç†å®æ—¶è§†è§‰å’Œå¬è§‰è¾“å…¥æ¥æ„å»ºå’Œæ›´æ–°å…¶é•¿æœŸè®°å¿†ã€‚é™¤äº†æƒ…æ™¯è®°å¿†å¤–ï¼Œå®ƒè¿˜èƒ½å‘å±•è¯­ä¹‰è®°å¿†ï¼Œèƒ½å¤Ÿéšæ—¶é—´ç§¯ç´¯ä¸–ç•ŒçŸ¥è¯†ã€‚å…¶è®°å¿†ä»¥å®ä½“ä¸ºä¸­å¿ƒã€å¤šæ¨¡æ€çš„æ–¹å¼ç»„ç»‡ï¼Œèƒ½æ›´æ·±å…¥åœ°ç†è§£ç¯å¢ƒã€‚ä¸ºè¯„ä¼°å¤šæ¨¡æ€ä»£ç†çš„è®°å¿†æ•ˆæœå’ŒåŸºäºè®°å¿†æ¨ç†çš„èƒ½åŠ›ï¼Œå¼€å‘äº†M3-Benché•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ã€‚M3-Agenté€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œåœ¨M3-Benchæµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºæœ€å¼ºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M3-Agentæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ä»£ç†æ¡†æ¶ï¼Œå…·å¤‡å¤„ç†å®æ—¶è§†è§‰å’Œå¬è§‰è¾“å…¥çš„èƒ½åŠ›ï¼Œä»¥æ„å»ºå’Œæ›´æ–°å…¶é•¿æœŸè®°å¿†ã€‚</li>
<li>M3-Agentä¸ä»…èƒ½å½¢æˆæƒ…æ™¯è®°å¿†ï¼Œè¿˜èƒ½å‘å±•è¯­ä¹‰è®°å¿†ï¼Œä½¿å…¶èƒ½å¤Ÿéšæ—¶é—´ç§¯ç´¯ä¸–ç•ŒçŸ¥è¯†ã€‚</li>
<li>M3-Agentçš„è®°å¿†ä»¥å®ä½“ä¸ºä¸­å¿ƒã€å¤šæ¨¡æ€çš„æ–¹å¼ç»„ç»‡ï¼Œä»¥æé«˜å¯¹ç¯å¢ƒçš„ç†è§£ã€‚</li>
<li>ä¸ºè¯„ä¼°å¤šæ¨¡æ€ä»£ç†çš„è®°å¿†æ•ˆæœå’ŒåŸºäºè®°å¿†æ¨ç†çš„èƒ½åŠ›ï¼Œå¼•å…¥äº†M3-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œå’Œç½‘é¡µæ¥æºçš„è§†é¢‘ã€‚</li>
<li>M3-Agenté€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œåœ¨M3-Benchæµ‹è¯•ä¸­å®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œä¼˜äºç°æœ‰æœ€å¼ºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>M3-Agentçš„è®¾è®¡æ¨åŠ¨äº†å¤šæ¨¡æ€ä»£ç†å‘æ›´ç±»ä¼¼äººç±»é•¿æœŸè®°å¿†çš„æ–¹å‘å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-08ce0560c62f133bdd3256f0e3479d8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e3d57b7970b35415912f02b69d3f5b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d402162a75b9869bddcc4fc2fa75e1a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61cb5764a43a536769aa76a58f6c351b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement"><a href="#MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement" class="headerlink" title="MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR   Advancement"></a>MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR   Advancement</h2><p><strong>Authors:Weitao Jia, Jinghui Lu, Haiyang Yu, Siqi Wang, Guozhi Tang, An-Lan Wang, Weijie Yin, Dingkang Yang, Yuxiang Nie, Bin Shan, Hao Feng, Irene Li, Kun Yang, Han Wang, Jingqun Tang, Teng Fu, Changhong Jin, Chao Feng, Xiaohui Lv, Can Huang</strong></p>
<p>Recent advances demonstrate that reinforcement learning with verifiable rewards (RLVR) significantly enhances the reasoning capabilities of large language models (LLMs). However, standard RLVR faces challenges with reward sparsity, where zero rewards from consistently incorrect candidate answers provide no learning signal, particularly in challenging tasks. To address this, we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative framework that utilizes diverse expert prompts as system prompts to generate a broader range of responses, substantially increasing the likelihood of identifying correct solutions. Additionally, we introduce an inter-expert mutual learning mechanism that facilitates knowledge sharing and transfer among experts, further boosting the modelâ€™s performance through RLVR. Extensive experiments across multiple reasoning benchmarks show that MEML-GRPO delivers significant improvements, achieving an average performance gain of 4.89% with Qwen and 11.33% with Llama, effectively overcoming the core limitations of traditional RLVR methods. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¿›å±•è¡¨æ˜ï¼Œä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ ‡å‡†RLVRé¢ä¸´å¥–åŠ±ç¨€ç–çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­ä¸æ­£ç¡®çš„å€™é€‰ç­”æ¡ˆæä¾›çš„é›¶å¥–åŠ±æ— æ³•æä¾›å­¦ä¹ ä¿¡å·ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šä¸“å®¶ç›¸äº’å­¦ä¹ GRPOï¼ˆMEML-GRPOï¼‰è¿™ä¸€åˆ›æ–°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸åŒçš„ä¸“å®¶æç¤ºä½œä¸ºç³»ç»Ÿæç¤ºæ¥ç”Ÿæˆæ›´å¹¿æ³›çš„å“åº”ï¼Œå¤§å¤§æé«˜äº†æ‰¾åˆ°æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„å¯èƒ½æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¸“å®¶é—´ç›¸äº’å­¦ä¹ æœºåˆ¶ï¼Œä¿ƒè¿›äº†ä¸“å®¶ä¹‹é—´çš„çŸ¥è¯†å…±äº«å’Œè½¬ç§»ï¼Œé€šè¿‡RLVRè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMEML-GRPOå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨Qwenä¸Šå¹³å‡æ€§èƒ½æå‡4.89%ï¼Œåœ¨Llamaä¸Šæå‡11.33%ï¼Œæœ‰æ•ˆåœ°å…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•çš„æ ¸å¿ƒå±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09670v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„ç»“åˆæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ ‡å‡†RLVRé¢ä¸´å¥–åŠ±ç¨€ç–çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´åœ¨å›°éš¾ä»»åŠ¡ä¸­æ— æ³•è·å¾—å­¦ä¹ ä¿¡å·ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šä¸“å®¶ç›¸äº’å­¦ä¹ GRPOï¼ˆMEML-GRPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤šç§ä¸“å®¶æç¤ºä½œä¸ºç³»ç»Ÿæç¤ºç”Ÿæˆæ›´å¹¿æ³›çš„å“åº”ï¼Œæé«˜æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„è¯†åˆ«å¯èƒ½æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸“å®¶é—´ç›¸äº’å­¦ä¹ æœºåˆ¶ï¼Œä¿ƒè¿›çŸ¥è¯†å…±äº«å’Œè½¬ç§»ï¼Œè¿›ä¸€æ­¥é€šè¿‡RLVRæå‡æ¨¡å‹æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMEML-GRPOå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨Qwenå’ŒLlamaä¸Šåˆ†åˆ«å®ç°äº†å¹³å‡æ€§èƒ½æå‡4.89%å’Œ11.33%ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»ŸRLVRæ–¹æ³•çš„æ ¸å¿ƒå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRç»“åˆæ˜¾è‘—æå‡äº†LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ ‡å‡†RLVRé¢ä¸´å¥–åŠ±ç¨€ç–çš„æŒ‘æˆ˜ã€‚</li>
<li>MEML-GRPOæ¡†æ¶åˆ©ç”¨å¤šç§ä¸“å®¶æç¤ºç”Ÿæˆæ›´å¹¿æ³›çš„å“åº”ï¼Œæé«˜æ­£ç¡®è§£å†³æ–¹æ¡ˆçš„è¯†åˆ«å¯èƒ½æ€§ã€‚</li>
<li>MEML-GRPOå¼•å…¥ä¸“å®¶é—´ç›¸äº’å­¦ä¹ æœºåˆ¶ï¼Œä¿ƒè¿›çŸ¥è¯†å…±äº«å’Œè½¬ç§»ã€‚</li>
<li>MEML-GRPOåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>åœ¨Qwenå’ŒLlamaä¸Šï¼ŒMEML-GRPOåˆ†åˆ«å®ç°äº†å¹³å‡æ€§èƒ½æå‡4.89%å’Œ11.33%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e25c46065702f147b41f62d0c3d9a825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a80f75c80a36d962a1a7895687c8be21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b00596135792d7ec9b7997685edf1cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91a62c952c9c5fb9657241036350c391.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269ca20d7a4d9cadb3b603d754deacfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a69bb7d380a23830b34b1dbea27e6f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning"><a href="#Goal-Discovery-with-Causal-Capacity-for-Efficient-Reinforcement-Learning" class="headerlink" title="Goal Discovery with Causal Capacity for Efficient Reinforcement Learning"></a>Goal Discovery with Causal Capacity for Efficient Reinforcement Learning</h2><p><strong>Authors:Yan Yu, Yaodong Yang, Zhengbo Lu, Chengdong Ma, Wengang Zhou, Houqiang Li</strong></p>
<p>Causal inference is crucial for humans to explore the world, which can be modeled to enable an agent to efficiently explore the environment in reinforcement learning. Existing research indicates that establishing the causality between action and state transition will enhance an agent to reason how a policy affects its future trajectory, thereby promoting directed exploration. However, it is challenging to measure the causality due to its intractability in the vast state-action space of complex scenarios. In this paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework for efficient environment exploration. Specifically, we first derive a measurement of causality in state space, \emph{i.e.,} causal capacity, which represents the highest influence of an agentâ€™s behavior on future trajectories. After that, we present a Monte Carlo based method to identify critical points in discrete state space and further optimize this method for continuous high-dimensional environments. Those critical points are used to uncover where the agent makes important decisions in the environment, which are then regarded as our subgoals to guide the agent to make exploration more purposefully and efficiently. Empirical results from multi-objective tasks demonstrate that states with high causal capacity align with our expected subgoals, and our GDCC achieves significant success rate improvements compared to baselines. </p>
<blockquote>
<p>å› æœæ¨ç†å¯¹äººç±»æ¢ç´¢ä¸–ç•Œè‡³å…³é‡è¦ï¼Œå¯ä»¥è¢«å»ºæ¨¡ä»¥ä½¿å¾—æ™ºèƒ½ä½“åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œå»ºç«‹åŠ¨ä½œå’ŒçŠ¶æ€è½¬ç§»ä¹‹é—´çš„å› æœå…³ç³»å°†å¢å¼ºæ™ºèƒ½ä½“æ¨ç†æ”¿ç­–å¦‚ä½•å½±å“å…¶æœªæ¥è½¨è¿¹çš„èƒ½åŠ›ï¼Œä»è€Œä¿ƒè¿›æœ‰æ–¹å‘æ€§çš„æ¢ç´¢ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚åœºæ™¯ä¸­çš„çŠ¶æ€-åŠ¨ä½œç©ºé—´åºå¤§ï¼Œæµ‹é‡å› æœå…³ç³»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGoal Discovery with Causal Capacityï¼ˆGDCCï¼‰çš„æ–°å‹é«˜æ•ˆç¯å¢ƒæ¢ç´¢æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆæ¨å¯¼å‡ºçŠ¶æ€ç©ºé—´ä¸­çš„å› æœå…³ç³»åº¦é‡ï¼Œå³å› æœå®¹é‡ï¼Œå®ƒä»£è¡¨æ™ºèƒ½ä½“è¡Œä¸ºå¯¹æœªæ¥è½¨è¿¹çš„æœ€é«˜å½±å“ã€‚ä¹‹åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè’™ç‰¹å¡æ´›çš„æ–¹æ³•æ¥ç¡®å®šç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„å…³é”®ç‚¹ï¼Œå¹¶è¿›ä¸€æ­¥ä¸ºè¿ç»­çš„é«˜ç»´ç¯å¢ƒä¼˜åŒ–æ­¤æ–¹æ³•ã€‚è¿™äº›å…³é”®ç‚¹è¢«ç”¨æ¥å‘ç°æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­åšå‡ºé‡è¦å†³ç­–çš„åœ°æ–¹ï¼Œç„¶åå°†å…¶è§†ä¸ºå­ç›®æ ‡ï¼Œå¼•å¯¼æ™ºèƒ½ä½“è¿›è¡Œæ›´æœ‰ç›®çš„å’Œé«˜æ•ˆçš„æ¢ç´¢ã€‚å¤šç›®æ ‡ä»»åŠ¡çš„å®è¯ç»“æœè¡¨æ˜ï¼Œå…·æœ‰é«˜å› æœå®¹é‡çš„çŠ¶æ€ä¸é¢„æœŸçš„å­ç›®æ ‡ç›¸ç¬¦ï¼Œæˆ‘ä»¬çš„GDCCä¸åŸºçº¿ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„æˆåŠŸç‡æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09624v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGoal Discovery with Causal Capacityï¼ˆGDCCï¼‰çš„æ¡†æ¶ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„æœ‰æ•ˆç¯å¢ƒæ¢ç´¢ã€‚è¯¥æ¡†æ¶é€šè¿‡è¡¡é‡ä»£ç†è¡Œä¸ºå¯¹æœªæ¥è½¨è¿¹çš„æœ€é«˜å½±å“ç¨‹åº¦ï¼Œå³å› æœå®¹é‡ï¼Œæ¥é‡åŒ–å› æœå…³ç³»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä½¿ç”¨åŸºäºè’™ç‰¹å¡æ´›çš„æ–¹æ³•åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­æ‰¾åˆ°å…³é”®ç‚¹ï¼Œå¹¶é’ˆå¯¹è¿ç»­é«˜ç»´ç¯å¢ƒè¿›è¡Œä¼˜åŒ–ã€‚è¿™äº›å…³é”®ç‚¹è¢«ç”¨ä½œç¯å¢ƒä¸­ä»£ç†åšå‡ºé‡è¦å†³ç­–çš„ä¾æ®ï¼Œè¢«è§†ä¸ºå­ç›®æ ‡ï¼Œä»¥æŒ‡å¯¼ä»£ç†æ›´æœ‰ç›®çš„å’Œæœ‰æ•ˆåœ°æ¢ç´¢ã€‚åœ¨å¤šç›®æ ‡ä»»åŠ¡ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼Œå…·æœ‰é«˜å› æœå®¹é‡çš„çŠ¶æ€ä¸é¢„æœŸçš„å­ç›®æ ‡ç›¸ç¬¦ï¼ŒGDCCæ¡†æ¶ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æˆåŠŸç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå› æœæ¨ç†å¯¹äºä»£ç†ï¼ˆagentï¼‰æ¢ç´¢ç¯å¢ƒè‡³å…³é‡è¦ã€‚</li>
<li>å› æœå…³ç³»å¯å»ºæ¨¡ä»¥ä¿ƒä½¿ä»£ç†è¿›è¡Œå®šå‘æ¢ç´¢ã€‚</li>
<li>ç°æœ‰ç ”ç©¶æŒ‘æˆ˜åœ¨äºåœ¨å¤æ‚åœºæ™¯çš„å¤§è§„æ¨¡çŠ¶æ€åŠ¨ä½œç©ºé—´ä¸­æµ‹é‡å› æœå…³ç³»ã€‚</li>
<li>æœ¬æ–‡æå‡ºGoal Discovery with Causal Capacityï¼ˆGDCCï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>GDCCæ¡†æ¶é€šè¿‡è¡¡é‡å› æœå®¹é‡ï¼ˆå³ä»£ç†è¡Œä¸ºå¯¹æœªæ¥è½¨è¿¹çš„æœ€é«˜å½±å“ç¨‹åº¦ï¼‰æ¥é‡åŒ–å› æœå…³ç³»ã€‚</li>
<li>ä½¿ç”¨åŸºäºè’™ç‰¹å¡æ´›çš„æ–¹æ³•åœ¨ç¦»æ•£å’Œè¿ç»­çŠ¶æ€ç©ºé—´ä¸­è¯†åˆ«å…³é”®ç‚¹ä½œä¸ºå­ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e39c3f7e03328e375be8c865853e45d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a0d62a257b7836dbcdb64569699eab1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18e494fbec07490a84cd8d001539ef81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b34f6b5ee5f0cb4987cebd8ed98d0cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9927e57d69b28e1d65d52029ba84ba87.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue"><a href="#OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue" class="headerlink" title="OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue"></a>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue</h2><p><strong>Authors:Xuelong Geng, Qijie Shao, Hongfei Xue, Shuiyuan Wang, Hanke Xie, Zhao Guo, Yi Zhao, Guojian Li, Wenjie Tian, Chengyou Wang, Zhixian Zhao, Kangxiang Xia, Ziyu Zhang, Zhennan Lin, Tianlun Zuo, Mingchen Shao, Yuang Cao, Guobin Ma, Longhao Li, Yuhang Dai, Dehui Gao, Dake Guo, Lei Xie</strong></p>
<p>Empathy is crucial in enabling natural interactions within spoken dialogue systems, allowing machines to recognize and respond appropriately to paralinguistic cues such as age, gender, and emotion. Recent advancements in end-to-end speech language models, which unify speech understanding and generation, provide promising solutions. However, several challenges persist, including an over-reliance on large-scale dialogue datasets, insufficient extraction of paralinguistic cues vital for conveying empathy, and the lack of empathy-specific datasets and evaluation frameworks. To address these issues, we introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system designed to enhance empathetic interactions, particularly in resource-limited settings. OSUM-EChat introduces two key innovations: (1) a three-stage understanding-driven spoken dialogue training strategy that extends the capabilities of a large speech understanding model to spoken dialogue tasks, and (2) a linguistic-paralinguistic dual thinking mechanism that integrates paralinguistic understanding through a chain of thought with dialogue generation, enabling the system to produce more empathetic responses. This approach reduces reliance on large-scale dialogue datasets while maintaining high-quality empathetic interactions. Additionally, we introduce the EChat-200K dataset, a rich corpus of empathetic speech-to-speech dialogues, and the EChat-eval benchmark, a comprehensive framework for evaluating the empathetic capabilities of dialogue systems. Experimental results demonstrate that OSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic responsiveness, validating its effectiveness. </p>
<blockquote>
<p>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å®ç°è‡ªç„¶äº¤äº’è‡³å…³é‡è¦ï¼Œå®ƒè®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œé€‚å½“å›åº”è¯¸å¦‚å¹´é¾„ã€æ€§åˆ«å’Œæƒ…ç»ªç­‰å‰¯è¯­è¨€çº¿ç´¢ã€‚ç«¯åˆ°ç«¯è¯­éŸ³è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œè¿™äº›æ¨¡å‹ç»Ÿä¸€äº†è¯­éŸ³ç†è§£å’Œç”Ÿæˆï¼Œæä¾›äº†å¯Œæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡åº¦ä¾èµ–å¤§è§„æ¨¡çš„å¯¹è¯æ•°æ®é›†ï¼Œæœªèƒ½å……åˆ†æå–ç”¨äºè¡¨è¾¾å…±æƒ…çš„å…³é”®å‰¯è¯­è¨€çº¿ç´¢ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OSUM-EChatï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå…±æƒ…äº¤äº’çš„å¼€æºç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚OSUM-EChatå¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä»¥ç†è§£ä¸ºä¸»å¯¼çš„å£è¯­å¯¹è¯ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ƒæ‰©å±•äº†å¤§å‹è¯­éŸ³ç†è§£æ¨¡å‹åœ¨å£è¯­å¯¹è¯ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼›äºŒæ˜¯è¯­è¨€-å‰¯è¯­è¨€åŒæ€æœºåˆ¶ï¼Œå®ƒé€šè¿‡æ€ç»´é“¾èå…¥å‰¯è¯­è¨€ç†è§£ï¼Œä¸å¯¹è¯ç”Ÿæˆç›¸ç»“åˆï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…·å…±æƒ…çš„å›åº”ã€‚è¿™ç§æ–¹æ³•é™ä½äº†å¯¹å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„å…±æƒ…äº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†EChat-200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä»½ä¸°å¯Œçš„å…±æƒ…è¯­éŸ³å¯¹è¯è¯­æ–™åº“ï¼Œä»¥åŠEChat-evalåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°å¯¹è¯ç³»ç»Ÿå…±æƒ…èƒ½åŠ›çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºç«¯åˆ°ç«¯çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09600v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡å¼ºè°ƒäº†åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­ï¼Œå…±æƒ…èƒ½åŠ›æ˜¯è¿›è¡Œè‡ªç„¶äº¤äº’çš„å…³é”®ã€‚è¿‘æœŸçš„ç«¯åˆ°ç«¯è¯­éŸ³è¯­è¨€æ¨¡å‹çš„å‘å±•ä¸ºè¯¥é¢†åŸŸå¸¦æ¥äº†å¸Œæœ›ï¼Œä½†ä»ç„¶å­˜åœ¨å¯¹å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„è¿‡åº¦ä¾èµ–ã€ç¼ºä¹æå–å¯¹è¡¨è¾¾å…±æƒ…è‡³å…³é‡è¦çš„å‰¯è¯­è¨€çº¿ç´¢ä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†OSUM-EChatè¿™ä¸€å¼€æºçš„ç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œå®ƒæ—¨åœ¨åŠ å¼ºå…±æƒ…äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚OSUM-EChatåŒ…å«ä¸¤é¡¹åˆ›æ–°æŠ€æœ¯ï¼šä¸€æ˜¯ç†è§£é©±åŠ¨çš„ä¸‰é˜¶æ®µå£è¯­å¯¹è¯è®­ç»ƒç­–ç•¥ï¼Œå®ƒå°†å¤§è§„æ¨¡çš„è¯­éŸ³ç†è§£æ¨¡å‹æ‰©å±•åˆ°å£è¯­å¯¹è¯ä»»åŠ¡ä¸­ï¼›äºŒæ˜¯è¯­è¨€-å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ï¼Œå®ƒå°†å‰¯è¯­è¨€ç†è§£ä¸å¯¹è¯ç”Ÿæˆç›¸ç»“åˆï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…·å…±æƒ…çš„å“åº”ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†EChat-200Kæ•°æ®é›†å’ŒEChat-evalåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¯¹è¯ç³»ç»Ÿçš„å…±æƒ…èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºå…¶ä»–ç«¯åˆ°ç«¯å£è¯­å¯¹è¯æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­æ˜¯å®ç°è‡ªç„¶äº¤äº’çš„å…³é”®ã€‚</li>
<li>è¿‘æœŸç«¯åˆ°ç«¯è¯­éŸ³è¯­è¨€æ¨¡å‹çš„å‘å±•ä¸ºå£è¯­å¯¹è¯ç³»ç»Ÿå¸¦æ¥å¸Œæœ›ã€‚</li>
<li>ç›®å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ï¼šå¯¹å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ã€ç¼ºä¹æå–å‰¯è¯­è¨€çº¿ç´¢çš„èƒ½åŠ›ä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>OSUM-EChatæ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå…±æƒ…äº¤äº’çš„å¼€æºç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿã€‚</li>
<li>OSUM-EChatå¼•å…¥äº†ä¸¤é¡¹åˆ›æ–°æŠ€æœ¯ï¼šç†è§£é©±åŠ¨çš„è®­ç»ƒç­–ç•¥å’Œè¯­è¨€-å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ã€‚</li>
<li>EChat-200Kæ•°æ®é›†å’ŒEChat-evalåŸºå‡†æµ‹è¯•è¢«ç”¨äºè¯„ä¼°å¯¹è¯ç³»ç»Ÿçš„å…±æƒ…èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ab6fbb1b7f7e9430ad35a30b166012a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-535849198529c9104a7671dc4a258925.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e14f2381ae5cfac036c4c4192c0a391e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fd8ca61e02317a23b841a8f6fef2d4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1532fe434229d1e26038d8d42cfc6836.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c3b03132ebd4dd671c6c43f74be212.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TFRank-Think-Free-Reasoning-Enables-Practical-Pointwise-LLM-Ranking"><a href="#TFRank-Think-Free-Reasoning-Enables-Practical-Pointwise-LLM-Ranking" class="headerlink" title="TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking"></a>TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking</h2><p><strong>Authors:Yongqi Fan, Xiaoyang Chen, Dezhi Ye, Jie Liu, Haijin Liang, Jin Ma, Ben He, Yingfei Sun, Tong Ruan</strong></p>
<p>Reasoning-intensive ranking models built on Large Language Models (LLMs) have made notable progress, but existing approaches often rely on large-scale LLMs and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational cost and latency that limit real-world use. To address this, we propose \textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale LLMs. To improve ranking performance, TFRank effectively integrates CoT data, fine-grained score supervision, and multi-task training. Furthermore, it achieves an efficient <code>\textbf&#123;T&#125;hink-\textbf&#123;F&#125;ree&quot; reasoning capability by employing a </code>think-mode switchâ€™â€™ and pointwise format constraints. Specifically, this allows the model to leverage explicit reasoning during training while delivering precise relevance scores for complex queries at inference without generating any reasoning chains. Experiments show that TFRank (e.g., 1.7B) achieves performance comparable to models with four times more parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on the BEIR benchmark. Further analysis shows that TFRank achieves an effective balance between performance and efficiency, providing a practical solution for integrating advanced reasoning into real-world systems. Our code and data are released in the repository: <a target="_blank" rel="noopener" href="https://github.com/JOHNNY-fans/TFRank">https://github.com/JOHNNY-fans/TFRank</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å¯†é›†å‹æ’åæ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§è§„æ¨¡LLMå’Œæ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜å’Œå»¶è¿Ÿï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TFRankæ–¹æ¡ˆï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå°è§„æ¨¡LLMçš„æœ‰æ•ˆç‚¹æ¨ç†æ’åå™¨ã€‚ä¸ºäº†æé«˜æ’åæ€§èƒ½ï¼ŒTFRankæœ‰æ•ˆåœ°æ•´åˆäº†æ€ç»´é“¾æ•°æ®ã€ç²¾ç»†ç²’åº¦çš„åˆ†æ•°ç›‘ç£å’Œå¤šä»»åŠ¡è®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡é‡‡ç”¨â€œæ€è€ƒæ¨¡å¼åˆ‡æ¢â€å’Œç‚¹æ ¼å¼çº¦æŸï¼Œå®ç°äº†é«˜æ•ˆçš„â€œæ— æ€è€ƒâ€æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™å…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æ˜ç¡®çš„æ¨ç†ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶é’ˆå¯¹å¤æ‚æŸ¥è¯¢æä¾›ç²¾ç¡®çš„å…³è”åˆ†æ•°ï¼Œæ— éœ€ç”Ÿæˆä»»ä½•æ¨ç†é“¾ã€‚å®éªŒè¡¨æ˜ï¼ŒTFRankï¼ˆä¾‹å¦‚ï¼Œè§„æ¨¡ä¸º1.7Bï¼‰åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸å‚æ•°å¤šå››å€æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒTFRankåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æœ‰æ•ˆçš„å¹³è¡¡ï¼Œä¸ºå°†é«˜çº§æ¨ç†é›†æˆåˆ°å®é™…ç³»ç»Ÿä¸­æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²å‘å¸ƒåœ¨å­˜å‚¨åº“ä¸­ï¼š<a target="_blank" rel="noopener" href="https://github.com/JOHNNY-fans/TFRank">https://github.com/JOHNNY-fans/TFRank</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09539v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å¯†é›†å‹æ’åæ¨¡å‹å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤§è§„æ¨¡LLMå’Œæ˜¾å¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜å’Œå»¶è¿Ÿæ—¶é—´é•¿ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºTFRankï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå°è§„æ¨¡LLMçš„é«˜æ•ˆç‚¹å¼æ¨ç†æ’åå™¨ã€‚TFRanké€šè¿‡æ•´åˆCoTæ•°æ®ã€ç²¾ç»†åˆ†æ•°ç›‘ç£å’Œå¤šä»»åŠ¡è®­ç»ƒï¼Œæé«˜æ’åæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡ä½¿ç”¨â€œæ€ç»´æ¨¡å¼å¼€å…³â€å’Œç‚¹æ ¼å¼çº¦æŸï¼Œå®ç°äº†é«˜æ•ˆçš„â€œæ— æ€è€ƒâ€æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒTFRankï¼ˆä¾‹å¦‚1.7Bå‚æ•°ï¼‰åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸å‚æ•°å¤šå››å€çš„æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒTFRankåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ï¼Œä¸ºå°†é«˜çº§æ¨ç†é›†æˆåˆ°ç°å®ç³»ç»Ÿä¸­æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ¨ç†å¯†é›†å‹æ’åæ¨¡å‹ä¾èµ–å¤§è§„æ¨¡LLMå’Œæ˜¾å¼CoTæ¨ç†ï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿã€‚</li>
<li>TFRankæ˜¯ä¸€ç§åŸºäºå°è§„æ¨¡LLMçš„é«˜æ•ˆç‚¹å¼æ¨ç†æ’åæ¨¡å‹ã€‚</li>
<li>TFRanké€šè¿‡æ•´åˆCoTæ•°æ®ã€ç²¾ç»†åˆ†æ•°ç›‘ç£å’Œå¤šä»»åŠ¡è®­ç»ƒæ¥æé«˜æ’åæ€§èƒ½ã€‚</li>
<li>TFRankå®ç°äº†â€œæ— æ€è€ƒâ€æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡â€œæ€ç»´æ¨¡å¼å¼€å…³â€å’Œç‚¹æ ¼å¼çº¦æŸä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒTFRankåœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸è¾ƒå¤§çš„æ¨¡å‹ç›¸å½“æˆ–åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­æ›´å…·ç«äº‰åŠ›ã€‚</li>
<li>TFRankåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†å¹³è¡¡ï¼Œä¸ºé›†æˆé«˜çº§æ¨ç†åˆ°ç°å®ç³»ç»Ÿä¸­æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-082c57fad96b81cb24c7a3afa5a8e0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5217f7a1e0ae9649dc71933812719e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7732d9c9700e7041a918284db49328d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b022e69210a0ee0da2e0f9e797aecfd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26ff06db91590006b488b09a36b21e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c514114e0acf4b1e16bb47a3c5e6be9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdcfe4923b6aba87bdfb2235559f6d1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bfc47593c04ec36f2b5e7a1f5fe876d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ParallelSearch-Train-your-LLMs-to-Decompose-Query-and-Search-Sub-queries-in-Parallel-with-Reinforcement-Learning"><a href="#ParallelSearch-Train-your-LLMs-to-Decompose-Query-and-Search-Sub-queries-in-Parallel-with-Reinforcement-Learning" class="headerlink" title="ParallelSearch: Train your LLMs to Decompose Query and Search   Sub-queries in Parallel with Reinforcement Learning"></a>ParallelSearch: Train your LLMs to Decompose Query and Search   Sub-queries in Parallel with Reinforcement Learning</h2><p><strong>Authors:Shu Zhao, Tan Yu, Anbang Xu, Japinder Singh, Aaditya Shukla, Rama Akkiraju</strong></p>
<p>Reasoning-augmented search agents such as Search-R1, trained via reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable capabilities in multi-step information retrieval from external knowledge sources. These agents address the limitations of their parametric memory by dynamically gathering relevant facts to address complex reasoning tasks. However, existing approaches suffer from a fundamental architectural limitation: they process search queries strictly sequentially, even when handling inherently parallelizable and logically independent comparisons. This sequential bottleneck significantly constrains computational efficiency, particularly for queries that require multiple entity comparisons. To address this critical limitation, we propose ParallelSearch, a novel reinforcement learning framework that empowers large language models (LLMs) to recognize parallelizable query structures and execute multiple search operations concurrently. Our approach introduces dedicated reward functions that incentivize the identification of independent query components while preserving answer accuracy through jointly considering correctness, query decomposition quality, and parallel execution benefits. Comprehensive experiments demonstrate that ParallelSearch outperforms state-of-the-art baselines by an average performance gain of 2.9% across seven question-answering benchmarks. Notably, on parallelizable questions, our method achieves a 12.7% performance improvement while requiring only 69.6% of the LLM calls compared to sequential approaches. </p>
<blockquote>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ å¹¶ä½¿ç”¨å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒçš„æ¨ç†å¢å¼ºæœç´¢ä»£ç†ï¼Œå¦‚Search-R1ï¼Œåœ¨å¤šæ­¥ä¿¡æ¯æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºä»å¤–éƒ¨çŸ¥è¯†æºè·å–æ˜¾è‘—çš„èƒ½åŠ›ã€‚è¿™äº›ä»£ç†é€šè¿‡åŠ¨æ€æ”¶é›†ç›¸å…³äº‹å®æ¥è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œä»è€Œå…‹æœäº†å…¶å‚æ•°å†…å­˜çš„å±€é™æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨åŸºæœ¬æ¶æ„ä¸Šçš„å±€é™æ€§ï¼šå®ƒä»¬ä¸¥æ ¼åœ°æŒ‰é¡ºåºå¤„ç†æœç´¢æŸ¥è¯¢ï¼Œå³ä½¿å¤„ç†æœ¬è´¨ä¸Šå¯å¹¶è¡ŒåŒ–å’Œé€»è¾‘ä¸Šç‹¬ç«‹çš„æ¯”è¾ƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§é¡ºåºç“¶é¢ˆæ˜¾è‘—åœ°é™åˆ¶äº†è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦å¤šä¸ªå®ä½“æ¯”è¾ƒæŸ¥è¯¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ParallelSearchï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè¯†åˆ«å¯å¹¶è¡ŒåŒ–çš„æŸ¥è¯¢ç»“æ„å¹¶åŒæ—¶æ‰§è¡Œå¤šä¸ªæœç´¢æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸“ç”¨å¥–åŠ±åŠŸèƒ½ï¼Œä»¥æ¿€åŠ±ç‹¬ç«‹æŸ¥è¯¢ç»„ä»¶çš„è¯†åˆ«ï¼ŒåŒæ—¶è”åˆè€ƒè™‘æ­£ç¡®æ€§ã€æŸ¥è¯¢åˆ†è§£è´¨é‡å’Œå¹¶è¡Œæ‰§è¡Œæ•ˆç›Šæ¥ä¿æŒç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒParallelSearchåœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯åŸºçº¿2.9%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºå¯å¹¶è¡ŒåŒ–çš„é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæé«˜äº†12.7%ï¼ŒåŒæ—¶ä¸é¡ºåºæ–¹æ³•ç›¸æ¯”ï¼Œåªéœ€è¦LLMè°ƒç”¨çš„69.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09303v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒçš„Search-R1ç­‰æ¨ç†å¢å¼ºæœç´¢ä»£ç†åœ¨å¤šæ­¥ä¿¡æ¯æ£€ç´¢ä¸­å…·æœ‰å‡ºè‰²çš„èƒ½åŠ›ï¼Œèƒ½ä»å¤–éƒ¨çŸ¥è¯†æºè·å–ç›¸å…³äº‹å®ä»¥è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ ¹æœ¬çš„æ¶æ„é™åˆ¶ï¼šå®ƒä»¬ä¸¥æ ¼åœ°æŒ‰é¡ºåºå¤„ç†æœç´¢æŸ¥è¯¢ï¼Œå³ä½¿å¤„ç†æœ¬è´¨ä¸Šå¯å¹¶è¡ŒåŒ–å’Œé€»è¾‘ç‹¬ç«‹çš„æ¯”è¾ƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºè§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ParallelSearchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯†åˆ«å¯å¹¶è¡ŒæŸ¥è¯¢ç»“æ„å¹¶åŒæ—¶æ‰§è¡Œå¤šä¸ªæœç´¢æ“ä½œã€‚é€šè¿‡å¼•å…¥ä¸“ç”¨å¥–åŠ±å‡½æ•°æ¥æ¿€åŠ±ç‹¬ç«‹æŸ¥è¯¢ç»„ä»¶çš„è¯†åˆ«ï¼ŒåŒæ—¶è”åˆè€ƒè™‘æ­£ç¡®æ€§ã€æŸ¥è¯¢åˆ†è§£è´¨é‡å’Œå¹¶è¡Œæ‰§è¡Œæ•ˆç›Šæ¥ä¿æŒç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒParallelSearchåœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½æå‡2.9%ï¼Œåœ¨å¯å¹¶è¡ŒåŒ–é—®é¢˜ä¸Šæ€§èƒ½æå‡è¾¾12.7%ï¼Œä¸”ç›¸è¾ƒäºé¡ºåºæ–¹æ³•ä»…éœ€è¦69.6%çš„LLMè°ƒç”¨æ¬¡æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†å¢å¼ºæœç´¢ä»£ç†å¦‚Search-R1åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿä»å¤–éƒ¨çŸ¥è¯†æºåŠ¨æ€è·å–ç›¸å…³äº‹å®ã€‚</li>
<li>ç°æœ‰æœç´¢æ–¹æ³•å­˜åœ¨é¡ºåºå¤„ç†æŸ¥è¯¢çš„ç“¶é¢ˆï¼Œé™åˆ¶äº†è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šå®ä½“æ¯”è¾ƒçš„é—®é¢˜ä¸­ã€‚</li>
<li>ParallelSearchæ¡†æ¶è¢«æå‡ºä»¥è§£å†³æ­¤é—®é¢˜ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¹¶è¡ŒæŸ¥è¯¢ç»“æ„å¹¶æ‰§è¡Œå¹¶è¡Œæœç´¢æ“ä½œã€‚</li>
<li>ParallelSearché€šè¿‡ä¸“ç”¨å¥–åŠ±å‡½æ•°æ¥å¹³è¡¡æŸ¥è¯¢çš„ç‹¬ç«‹ç»„ä»¶è¯†åˆ«å’Œç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒParallelSearchåœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½ä¼˜è¶Šï¼Œå¹³å‡æå‡2.9%ã€‚</li>
<li>åœ¨å¯å¹¶è¡ŒåŒ–é—®é¢˜ä¸Šï¼ŒParallelSearchçš„æ€§èƒ½æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œè¾¾åˆ°12.7%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-702ba7e68686f81f7c38cd97d8271dd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c3de876f21f2b1437a7cddb4dc78245.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf9e006accfe43b456c84f8d30974409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e0e8874514afb39a4f01114639a637e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FineState-Bench-A-Comprehensive-Benchmark-for-Fine-Grained-State-Control-in-GUI-Agents"><a href="#FineState-Bench-A-Comprehensive-Benchmark-for-Fine-Grained-State-Control-in-GUI-Agents" class="headerlink" title="FineState-Bench: A Comprehensive Benchmark for Fine-Grained State   Control in GUI Agents"></a>FineState-Bench: A Comprehensive Benchmark for Fine-Grained State   Control in GUI Agents</h2><p><strong>Authors:Fengxian Ji, Jingpu Yang, Zirui Song, Yuanxi Wang, Zhexuan Cui, Yuke Li, Qian Jiang, Miao Fang, Xiuying Chen</strong></p>
<p>With the rapid advancement of generative artificial intelligence technology, Graphical User Interface (GUI) agents have demonstrated tremendous potential for autonomously managing daily tasks through natural language instructions. However, current evaluation frameworks for GUI agents suffer from fundamental flaws: existing benchmarks overly focus on coarse-grained task completion while neglecting fine-grained control capabilities crucial for real-world applications. To address this, we introduce FineState-Bench, the first evaluation and diagnostic standard for fine-grained GUI proxy operations, designed to quantify fine-grained control. This multi-platform (desktop, Web, mobile) framework includes 2257 task benchmarks in four components and uses a four-phase indicator for comprehensive perception-to-control assessment. To analyze perception and positioning for refined operations, we developed the plug-and-play Visual Diagnostic Assistant (VDA), enabling the first quantitative decoupling analysis of these capabilities. Experimental results on our benchmark show that the most advanced models achieve only 32.8% fine-grained interaction accuracy. Using our VDA in controlled experiments, quantifying the impact of visual capabilities, we showed that ideal visual localization boosts Gemini-2.5-Flashâ€™s success rate by 14.9%. Our diagnostic framework confirms for the first time that the primary bottleneck for current GUI proxies is basic visual positioning capability.All resources are fully open-source. github: <a target="_blank" rel="noopener" href="https://github.com/AnonymousThewarehouse/FineState-Bench">https://github.com/AnonymousThewarehouse/FineState-Bench</a> huggingface: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Willtime2006/Static-FineBench">https://huggingface.co/datasets/Willtime2006/Static-FineBench</a> </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è‡ªä¸»ç®¡ç†æ—¥å¸¸ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„GUIä»£ç†è¯„ä¼°æ¡†æ¶å­˜åœ¨æ ¹æœ¬ç¼ºé™·ï¼šç°æœ‰åŸºå‡†æµ‹è¯•è¿‡äºå…³æ³¨ç²—ç²’åº¦çš„ä»»åŠ¡å®Œæˆï¼Œè€Œå¿½è§†å¯¹äºç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦çš„ç²¾ç»†ç²’åº¦æ§åˆ¶åŠŸèƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FineState-Benchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç²¾ç»†ç²’åº¦GUIä»£ç†æ“ä½œçš„è¯„ä¼°ä¸è¯Šæ–­æ ‡å‡†ï¼Œæ—¨åœ¨é‡åŒ–ç²¾ç»†ç²’åº¦æ§åˆ¶ã€‚è¿™ä¸€å¤šå¹³å°ï¼ˆæ¡Œé¢ã€ç½‘é¡µã€ç§»åŠ¨ï¼‰æ¡†æ¶åŒ…æ‹¬å››ä¸ªç»„ä»¶ä¸­çš„2257é¡¹ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä½¿ç”¨å››é˜¶æ®µæŒ‡æ ‡è¿›è¡Œå…¨æ–¹ä½çš„æ„ŸçŸ¥åˆ°æ§åˆ¶çš„è¯„ä¼°ã€‚ä¸ºäº†å¯¹ç²¾ç»†æ“ä½œçš„æ„ŸçŸ¥å’Œå®šä½è¿›è¡Œåˆ†æï¼Œæˆ‘ä»¬å¼€å‘äº†å³æ’å³ç”¨çš„è§†è§‰è¯Šæ–­åŠ©æ‰‹ï¼ˆVDAï¼‰ï¼Œå®ç°å¯¹è¿™äº›èƒ½åŠ›çš„é¦–æ¬¡å®šé‡è§£è€¦åˆ†æã€‚åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä»…å®ç°äº†32.8%çš„ç²¾ç»†äº¤äº’å‡†ç¡®ç‡ã€‚åœ¨å—æ§å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨VDAé‡åŒ–è§†è§‰åŠŸèƒ½çš„å½±å“ï¼Œå¹¶æ˜¾ç¤ºç†æƒ³çš„è§†è§‰å®šä½å¯ä»¥å°†Gemini-2.5-Flashçš„æˆåŠŸç‡æé«˜14.9%ã€‚æˆ‘ä»¬çš„è¯Šæ–­æ¡†æ¶é¦–æ¬¡è¯å®ï¼Œå½“å‰GUIä»£ç†çš„ä¸»è¦ç“¶é¢ˆåœ¨äºåŸºæœ¬çš„è§†è§‰å®šä½èƒ½åŠ›ã€‚æ‰€æœ‰èµ„æºå‡å®Œå…¨å¼€æºã€‚GitHubåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/AnonymousThewarehouse/FineState-Bench%EF%BC%9BHuggingface%E5%9C%B0%E5%9D%80%EF%BC%9Ahttps://huggingface.co/datasets/Willtime2006/Static-FineBench%E3%80%82">https://github.com/AnonymousThewarehouse/FineState-Benchï¼›Huggingfaceåœ°å€ï¼šhttps://huggingface.co/datasets/Willtime2006/Static-FineBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09241v1">PDF</a> submit&#x2F;6682470 (Fengxian Ji)</p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è‡ªä¸»ç®¡ç†æ—¥å¸¸ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„GUIä»£ç†è¯„ä¼°æ¡†æ¶å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ï¼Œè¿‡äºå…³æ³¨ç²—ç²’åº¦ä»»åŠ¡å®Œæˆï¼Œå¿½è§†äº†ç°å®ä¸–ç•Œåº”ç”¨ä¸­è‡³å…³é‡è¦çš„ç²¾ç»†æ§åˆ¶åŠŸèƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FineState-Benchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç²¾ç»†ç²’åº¦GUIä»£ç†æ“ä½œçš„è¯„ä¼°ä¸è¯Šæ–­æ ‡å‡†ï¼Œæ—¨åœ¨é‡åŒ–ç²¾ç»†æ§åˆ¶ã€‚æ­¤è·¨å¹³å°ï¼ˆæ¡Œé¢ã€ç½‘ç»œã€ç§»åŠ¨ï¼‰æ¡†æ¶åŒ…æ‹¬å››ç§æˆåˆ†çš„2257é¡¹ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä½¿ç”¨å››é˜¶æ®µæŒ‡æ ‡è¿›è¡Œå…¨é¢æ„ŸçŸ¥åˆ°æ§åˆ¶çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†å³æ’å³ç”¨è§†è§‰è¯Šæ–­åŠ©æ‰‹ï¼ˆVDAï¼‰ï¼Œç”¨äºåˆ†æç²¾ç»†æ“ä½œçš„æ„ŸçŸ¥å’Œå®šä½èƒ½åŠ›ï¼Œå®ç°äº†è¿™äº›èƒ½åŠ›çš„é¦–æ¬¡å®šé‡è§£è€¦åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ç²¾ç»†äº¤äº’å‡†ç¡®ç‡ä»…ä¸º32.8%ã€‚é€šè¿‡æˆ‘ä»¬çš„VDAè¿›è¡Œçš„å—æ§å®éªŒè¡¨æ˜ï¼Œç†æƒ³çš„è§†è§‰å®šä½åŠŸèƒ½å¯ä»¥æå‡Gemini-2.5-Flashçš„æˆåŠŸç‡14.9%ã€‚æˆ‘ä»¬çš„è¯Šæ–­æ¡†æ¶é¦–æ¬¡è¯å®ï¼Œå½“å‰GUIä»£ç†çš„ä¸»è¦ç“¶é¢ˆåœ¨äºåŸºæœ¬çš„è§†è§‰å®šä½èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIä»£ç†åœ¨æ—¥å¸¸ä»»åŠ¡ç®¡ç†ä¸­å±•ç°æ½œåŠ›ã€‚</li>
<li>å½“å‰è¯„ä¼°æ¡†æ¶å¿½ç•¥ç²¾ç»†æ§åˆ¶åŠŸèƒ½çš„é‡è¦æ€§ã€‚</li>
<li>æ¨å‡ºFineState-Benchæ¡†æ¶ï¼Œç”¨äºç²¾ç»†ç²’åº¦GUIæ“ä½œçš„è¯„ä¼°ä¸è¯Šæ–­ã€‚</li>
<li>æ¡†æ¶åŒ…å«2257é¡¹ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§å¹³å°ã€‚</li>
<li>ä½¿ç”¨å››é˜¶æ®µæŒ‡æ ‡å…¨é¢è¯„ä¼°æ„ŸçŸ¥åˆ°æ§åˆ¶çš„è¿‡ç¨‹ã€‚</li>
<li>å¼€å‘VDAè¿›è¡Œæ„ŸçŸ¥å’Œå®šä½èƒ½åŠ›çš„å®šé‡è§£è€¦åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd2d4c8141977f1c81db38f629c5ed3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f65cc478d52e0660823381dd919bdf3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f7aac90ac0c232bc34d9fa32aaad250.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da35e98c7a89064bbaadd7781cc3588.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ba4af1b090ab53ccf936e03572f4a1b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Aryabhata-An-exam-focused-language-model-for-JEE-Math"><a href="#Aryabhata-An-exam-focused-language-model-for-JEE-Math" class="headerlink" title="Aryabhata: An exam-focused language model for JEE Math"></a>Aryabhata: An exam-focused language model for JEE Math</h2><p><strong>Authors:Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma</strong></p>
<p>We present Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-$n$ rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation along with novel exploration strategies such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models. This marks our first open release for community feedback (<a target="_blank" rel="noopener" href="https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0">https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0</a>); PW is actively training future models to further improve learning outcomes for students. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Aryabhata 1.0ï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹å°åº¦å­¦æœ¯è€ƒè¯•â€”â€”è”åˆå…¥å­¦è€ƒè¯•ï¼ˆJEEï¼‰ä¼˜åŒ–çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰ç´§å‡‘çš„7äº¿å‚æ•°ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•è¿…é€Ÿï¼Œä½†å½“å‰æ¨¡å‹åœ¨å­¦æœ¯åº”ç”¨ä¸Šä»æœ‰è¯¸å¤šä¸è¶³ã€‚Aryabhata 1.0é€šè¿‡åˆå¹¶å¼ºå¤§çš„å¼€æ”¾æƒé‡æ¨ç†æ¨¡å‹ï¼Œéšåé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸è¯¾ç¨‹å­¦ä¹ çš„æ–¹å¼ï¼Œå¯¹é€šè¿‡æœ€ä½³næ‹’ç»é‡‡æ ·ç­›é€‰çš„éªŒè¯æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œæˆ‘ä»¬é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œä½¿ç”¨A2Cç›®æ ‡è¿›è¡Œç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿è¯„ä¼°ï¼Œå¹¶è¾…ä»¥æ–°å‹ç­–ç•¥ï¼Œå¦‚è‡ªé€‚åº”å°ç»„è°ƒæ•´å’Œæ¸©åº¦ç¼©æ”¾ã€‚æˆ‘ä»¬åœ¨å†…éƒ¨æ•°æ®é›†ï¼ˆJEE Main 2025ï¼‰å’Œå¤–éƒ¨æ•°æ®é›†ï¼ˆMATHã€GSM8Kï¼‰ä¸Šè¯„ä¼°Aryabhataçš„è¡¨ç°ï¼Œå…¶åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶æä¾›å¾ªåºæ¸è¿›çš„æ¨ç†æ­¥éª¤ï¼Œæœ‰åŠ©äºæ•™å­¦ã€‚æˆ‘ä»¬å‘å¸ƒAryabhataä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œæ¨åŠ¨ä»¥è€ƒè¯•ä¸ºä¸­å¿ƒçš„å°å‹å¼€æºè¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚è¿™æ˜¯æˆ‘ä»¬é¦–æ¬¡å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¶é›†ç¤¾åŒºåé¦ˆï¼›PhysicsWallahæ­£åœ¨ç§¯æè®­ç»ƒæœªæ¥æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥æ”¹å–„å­¦ç”Ÿçš„å­¦ä¹ æˆæœã€‚[<a target="_blank" rel="noopener" href="https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0]">https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08665v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>Aryabhata 1.0æ˜¯ä¸€æ¬¾é’ˆå¯¹å°åº¦è”è€ƒï¼ˆJEEï¼‰ä¼˜åŒ–çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰7äº¿å‚æ•°ã€‚å®ƒé€šè¿‡åˆå¹¶å¼ºå¤§çš„å¼€æ”¾æƒé‡æ¨ç†æ¨¡å‹ï¼Œç„¶åè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè¯¾ç¨‹å­¦ä¹ æ¥å¢å¼ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å’Œè‡ªé€‚åº”åˆ†ç»„è°ƒæ•´ç­‰æŠ€æœ¯æ¥æé«˜æ€§èƒ½ã€‚Aryabhataåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶æä¾›äº†é€æ­¥æ¨ç†çš„æ•™å¯¼åŠŸèƒ½ã€‚ä½œä¸ºè€ƒè¯•ä¸­å¿ƒçš„å°å‹è¯­è¨€æ¨¡å‹çš„åŸºçŸ³æ¨¡å‹å‘å¸ƒï¼Œç”¨äºæ”¶é›†ç¤¾åŒºåé¦ˆä»¥æ”¹è¿›æœªæ¥çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Aryabhata 1.0æ˜¯ä¸€æ¬¾é’ˆå¯¹å°åº¦è”è€ƒï¼ˆJEEï¼‰çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹èåˆäº†å¤šç§æŠ€æœ¯ä¼˜åŒ–æ‰‹æ®µï¼šåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€è¯¾ç¨‹å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ç­‰ã€‚<br>3.Aryabhataé‡‡ç”¨äº†ç»è¿‡éªŒè¯çš„æ€ç»´è½¨è¿¹ï¼Œé‡‡ç”¨æœ€ä½³æ‹’ç»æŠ½æ ·æ¥æé«˜æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨è‡ªé€‚åº”åˆ†ç»„è°ƒæ•´å’Œæ¸©åº¦ç¼©æ”¾ç­‰æ–°é¢–ç­–ç•¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚ </li>
<li>åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAryabhataè¡¨ç°å‡ºä¼˜å¼‚çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚<br>6.Aryabhataæä¾›äº†é€æ­¥æ¨ç†çš„æ•™å¯¼åŠŸèƒ½ï¼Œæœ‰åŠ©äºå­¦ç”Ÿç†è§£å’Œå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-be0b5c26de4c1b33b8107a9e135e0927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d8fbcc6f0655a6c4f6c0b63adbe2af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c04b16e74b504f1ac68318931ba86e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-656bf66b8fb335cabc2f735889d6b53a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36418397c22e384dd9e80ec778d5c8e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-758e9b9cb81577b9e4930122aae8d6b3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>Capabilities of GPT-5 on Multimodal Medical Reasoning</h2><p><strong>Authors:Shansong Wang, Mingzhe Hu, Qiang Li, Mojtaba Safari, Xiaofeng Yang</strong></p>
<p>Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.26% and +26.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5â€™s ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä½¿å¾—é€šç”¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œè¶Šæ¥è¶Šå¤æ‚çš„ç‰¹å®šé¢†åŸŸæ¨ç†ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œå†³ç­–åˆ¶å®šé€šå¸¸éœ€è¦æ•´åˆå¤šç§å¼‚æ„ä¿¡æ¯æºï¼ŒåŒ…æ‹¬æ‚£è€…å™è¿°ã€ç»“æ„åŒ–æ•°æ®å’ŒåŒ»ç–—å›¾åƒã€‚æœ¬ç ”ç©¶å°†GPT-5å®šä½ä¸ºåŒ»ç–—å†³ç­–æ”¯æŒé¢†åŸŸçš„é€šç”¨å¤šæ¨¡æ€æ¨ç†å™¨ï¼Œå¹¶åœ¨ç»Ÿä¸€åè®®ä¸‹ç³»ç»Ÿåœ°è¯„ä¼°å…¶åœ¨åŸºäºæ–‡æœ¬çš„é—®é¢˜å›ç­”å’Œè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ä¸Šçš„é›¶é•œå¤´æ€ç»´é“¾æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬ç”¨MedQAã€MedXpertQAï¼ˆæ–‡æœ¬å’Œå¤šæ¨¡æ€ï¼‰ã€MMLUåŒ»ç–—å­é›†ã€USMLEè‡ªæˆ‘è¯„ä¼°è€ƒè¯•å’ŒVQA-RADçš„æ ‡å‡†åˆ†å‰²æ•°æ®æ¥è¯„ä¼°GPT-5ã€GPT-5-miniã€GPT-5-nanoä»¥åŠGPT-4o-2024-11-20çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-5æŒç»­è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œåœ¨æ‰€æœ‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—å®è´¨æ€§è¿›å±•ã€‚åœ¨MedXpertQA MMä¸Šï¼ŒGPT-5çš„æ¨ç†å’Œç†è§£åˆ†æ•°åˆ†åˆ«æ¯”GPT-4oé«˜å‡º+29.26%å’Œ+26.18%ï¼Œå¹¶ä¸”åœ¨æ¨ç†å’Œç†è§£æ–¹é¢è¶…è¶Šé¢„æˆæƒçš„äººç±»ä¸“å®¶+24.23%å’Œ+29.40%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4oåœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šä»ä½äºäººç±»ä¸“å®¶çš„æ€§èƒ½ã€‚ä¸€ä¸ªå…¸å‹çš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†GPT-5å°†è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æ•´åˆåˆ°è¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ä¸­çš„èƒ½åŠ›ï¼Œå¹¶æ¨èé€‚å½“çš„é«˜é£é™©å¹²é¢„æªæ–½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›å—æ§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGPT-5çš„æ€§èƒ½å·²ç»ä»ä¸äººç±»ç›¸å½“æå‡åˆ°äº†è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ã€‚è¿™ä¸€è¿›æ­¥å¯èƒ½ä¼šæå¤§åœ°å½±å“æœªæ¥ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„è®¾è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08224v2">PDF</a> Corrected some typos</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—é€šç”¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œè¶Šæ¥è¶Šå¤æ‚çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚æœ¬ç ”ç©¶å°†GPT-5å®šä½ä¸ºåŒ»ç–—å†³ç­–æ”¯æŒçš„ä¸€èˆ¬æ€§å¤šæ¨¡å¼æ¨ç†å™¨ï¼Œå¹¶ç³»ç»Ÿåœ°è¯„ä¼°å…¶åœ¨ç»Ÿä¸€åè®®ä¸‹åŸºäºæ–‡æœ¬å’Œè§†è§‰çš„é—®é¢˜å›ç­”ä»»åŠ¡çš„é›¶èµ·ç‚¹æ€ç»´é“¾æ¨ç†æ€§èƒ½ã€‚GPT-5åœ¨æ‰€æœ‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šæ¨¡å¼æ¨ç†æ–¹é¢å–å¾—äº†å®è´¨æ€§çš„è¿›æ­¥ã€‚åœ¨MedXpertQA MMä¸Šï¼ŒGPT-5çš„æ¨ç†å’Œç†è§£å¾—åˆ†åˆ†åˆ«æ¯”GPT-4oé«˜å‡º+29.26%å’Œ+26.18%ï¼Œå¹¶ä¸”åœ¨æ¨ç†å’Œç†è§£æ–¹é¢åˆ†åˆ«è¶…å‡ºé¢„æˆæƒçš„äººç±»ä¸“å®¶+24.23%å’Œ+29.40%ã€‚ç›¸åï¼ŒGPT-4oåœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šä»ä½äºäººç±»ä¸“å®¶çš„æ€§èƒ½ã€‚ä¸€ä¸ªå…¸å‹çš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†GPT-5å°†è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æ•´åˆåˆ°è¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ä¸­çš„èƒ½åŠ›ï¼Œå¹¶æ¨èäº†é€‚å½“çš„é«˜é£é™©å¹²é¢„æªæ–½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›å—æ§çš„å¤šæ¨¡å¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGPT-5çš„æ€§èƒ½å·²ç»ä»äººç±»ç›¸å½“çš„æ°´å¹³æå‡åˆ°äº†è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ã€‚è¿™ä¸€è¿›æ­¥å¯ä»¥ä¸ºæœªæ¥çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›é‡è¦çš„å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥åœ¨ä¸å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œå¤æ‚çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚</li>
<li>GPT-5è¢«å®šä½ä¸ºåŒ»ç–—å†³ç­–æ”¯æŒä¸­çš„ä¸€èˆ¬æ€§å¤šæ¨¡å¼æ¨ç†å™¨ã€‚</li>
<li>GPT-5åœ¨å¤šç§åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¤šæ¨¡å¼æ¨ç†æ–¹é¢ã€‚</li>
<li>GPT-5åœ¨æ¨ç†å’Œç†è§£æ–¹é¢çš„å¾—åˆ†è¶…å‡ºé¢„æˆæƒçš„äººç±»ä¸“å®¶ã€‚</li>
<li>GPT-5èƒ½å°†è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æ•´åˆåˆ°è¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ä¸­ã€‚</li>
<li>GPT-5çš„æ€§èƒ½è¶…è¶Šäº†è®¸å¤šé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¶…è¶Šäº†äººç±»ä¸“å®¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c04a4842a6008b4346f15d4e3605e33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb998a42975d483a08aa26d2dc35a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a73b4ff65d0519f7b792e2d55a2305e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-580156970b0d96670ede817ccd990adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5ac46480a182d7a80c946aa269693a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d964ae45614c6c28f0e9f47d6f994d25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5912ddd6627ade3fc53834262caea54.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning"><a href="#Pentest-R1-Towards-Autonomous-Penetration-Testing-Reasoning-Optimized-via-Two-Stage-Reinforcement-Learning" class="headerlink" title="Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized   via Two-Stage Reinforcement Learning"></a>Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized   via Two-Stage Reinforcement Learning</h2><p><strong>Authors:He Kong, Die Hu, Jingguo Ge, Liangxiong Li, Hui Li, Tong Li</strong></p>
<p>Automating penetration testing is crucial for enhancing cybersecurity, yet current Large Language Models (LLMs) face significant limitations in this domain, including poor error handling, inefficient reasoning, and an inability to perform complex end-to-end tasks autonomously. To address these challenges, we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning capabilities for this task through a two-stage reinforcement learning pipeline. We first construct a dataset of over 500 real-world, multi-step walkthroughs, which Pentest-R1 leverages for offline reinforcement learning (RL) to instill foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in an interactive Capture The Flag (CTF) environment, where it learns directly from environmental feedback to develop robust error self-correction and adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench benchmarks demonstrate the frameworkâ€™s effectiveness. On AutoPenBench, Pentest-R1 achieves a 24.2% success rate, surpassing most state-of-the-art models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a 15.0% success rate in unguided tasks, establishing a new state-of-the-art for open-source LLMs and matching the performance of top proprietary models. Ablation studies confirm that the synergy of both training stages is critical to its success. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•å¯¹äºå¢å¼ºç½‘ç»œå®‰å…¨è‡³å…³é‡è¦ï¼Œç„¶è€Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿™ä¸€é¢†åŸŸé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é”™è¯¯å¤„ç†ä¸ä½³ã€æ¨ç†æ•ˆç‡ä½ä¸‹ä»¥åŠæ— æ³•è‡ªä¸»å®Œæˆå¤æ‚çš„ç«¯åˆ°ç«¯ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Pentest-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç®¡é“ä¼˜åŒ–LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä»¥å®Œæˆæ­¤ä»»åŠ¡ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†åŒ…å«è¶…è¿‡500ä¸ªçœŸå®ä¸–ç•Œã€å¤šæ­¥éª¤è¯¦è§£çš„æ•°æ®é›†ï¼ŒPentest-R1åˆ©ç”¨è¿™äº›æ•°æ®é›†è¿›è¡Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥çŒè¾“åŸºæœ¬çš„æ”»å‡»é€»è¾‘ã€‚éšåï¼ŒLLMåœ¨ä¸€ä¸ªäº¤äº’å¼çš„Capture The Flagï¼ˆCTFï¼‰ç¯å¢ƒä¸­é€šè¿‡åœ¨çº¿RLè¿›è¡Œå¾®è°ƒï¼Œå®ƒç›´æ¥ä»ç¯å¢ƒåé¦ˆä¸­å­¦ä¹ ï¼Œä»¥å‘å±•ç¨³å¥çš„é”™è¯¯è‡ªæˆ‘ä¿®æ­£å’Œè‡ªé€‚åº”ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨Cybenchå’ŒAutoPenBenchåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚åœ¨AutoPenBenchä¸Šï¼ŒPentest-R1çš„æˆåŠŸç‡è¾¾åˆ°24.2%ï¼Œè¶…è¶Šäº†å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»…æ¬¡äºGemini 2.5 Flashã€‚åœ¨Cybenchä¸Šï¼Œå®ƒåœ¨æ— æŒ‡å¯¼ä»»åŠ¡ä¸­çš„æˆåŠŸç‡è¾¾åˆ°15.0%ï¼Œä¸ºå¼€æºLLMåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸é¡¶çº§ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œä¸¤ä¸ªè®­ç»ƒé˜¶æ®µçš„ååŒä½œç”¨æ˜¯æˆåŠŸçš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07382v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•å¯¹æå‡ç½‘ç»œå®‰å…¨è‡³å…³é‡è¦ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿™ä¸€é¢†åŸŸå­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚é”™è¯¯å¤„ç†ä¸ä½³ã€æ¨ç†æ•ˆç‡ä½ä¸‹ä»¥åŠæ— æ³•è‡ªä¸»å®Œæˆå¤æ‚ç«¯åˆ°ç«¯ä»»åŠ¡ç­‰ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Pentest-R1æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç®¡é“ä¼˜åŒ–LLMçš„æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡500ä¸ªçœŸå®ä¸–ç•Œå¤šæ­¥éª¤æµç¨‹çš„æ•°æ®é›†ï¼Œç”¨äºç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä¸ºPentest-R1æä¾›åŸºç¡€æ”»å‡»é€»è¾‘ã€‚ç„¶åï¼Œé€šè¿‡åœ¨çº¿RLåœ¨äº¤äº’å¼Capture The Flagï¼ˆCTFï¼‰ç¯å¢ƒä¸­å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶ç›´æ¥ä»ç¯å¢ƒåé¦ˆä¸­å­¦ä¹ ï¼Œå‘å±•å‡ºå¼ºå¤§çš„é”™è¯¯è‡ªæˆ‘ä¿®æ­£å’Œè‡ªé€‚åº”ç­–ç•¥ã€‚åœ¨Cybenchå’ŒAutoPenBenchåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚Pentest-R1åœ¨AutoPenBenchä¸Šå–å¾—äº†24.2%çš„æˆåŠŸç‡ï¼Œä¼˜äºå¤§å¤šæ•°æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»…æ¬¡äºGemini 2.5 Flashã€‚åœ¨Cybenchä¸Šï¼Œå®ƒåœ¨æ— æŒ‡å¯¼ä»»åŠ¡ä¸­è¾¾åˆ°äº†15.0%çš„æˆåŠŸç‡ï¼Œä¸ºå¼€æºLLMsåˆ›é€ äº†æ–°çš„æœ€ä½³æ°´å¹³ï¼Œå¹¶åŒ¹é…äº†é¡¶çº§ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œä¸¤ä¸ªé˜¶æ®µè®­ç»ƒçš„ååŒä½œç”¨æ˜¯æˆåŠŸçš„å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•å¯¹å¢å¼ºç½‘ç»œå®‰å…¨è‡³å…³é‡è¦ï¼Œä½†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­¤é¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Pentest-R1æ¡†æ¶æ—¨åœ¨é€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ è§£å†³LLMåœ¨æ¸—é€æµ‹è¯•ä¸­çš„å±€é™æ€§ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨ç¦»çº¿RLå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ºLLMä¼ æˆåŸºç¡€æ”»å‡»é€»è¾‘ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡åœ¨çº¿RLåœ¨CTFç¯å¢ƒä¸­å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œæå‡é”™è¯¯è‡ªæˆ‘ä¿®æ­£å’Œè‡ªé€‚åº”ç­–ç•¥ã€‚</li>
<li>Pentest-R1åœ¨AutoPenBenchå’ŒCybenchåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>åœ¨AutoPenBenchä¸Šï¼ŒPentest-R1çš„æˆåŠŸç‡è¶…è¿‡å¤šæ•°ç°æœ‰æ¨¡å‹ï¼Œå¹¶æ¥è¿‘æœ€ä½³è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07382">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad5608ee21d2901e09fda1fe1adc3c58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87483e38268e99d01a3e0e9df7c29bda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8a0bc3f01c4636a87caee2ad341cab3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b124a66fb1a889d71733f77b83a966d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c13733d0902782e19b3370ce53fbf40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-691f03715815595e27f2bef031a8f02c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50c2dce5d22f0bbc7fe9de382d5f718e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding"><a href="#DocR1-Evidence-Page-Guided-GRPO-for-Multi-Page-Document-Understanding" class="headerlink" title="DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding"></a>DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</h2><p><strong>Authors:Junyu Xiong, Yonghui Wang, Weichao Zhao, Chenyu Liu, Bing Yin, Wengang Zhou, Houqiang Li</strong></p>
<p>Understanding multi-page documents poses a significant challenge for multimodal large language models (MLLMs), as it requires fine-grained visual comprehension and multi-hop reasoning across pages. While prior work has explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs, its application to multi-page document understanding remains underexplored. In this paper, we introduce DocR1, an MLLM trained with a novel RL framework, Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the model to first retrieve relevant pages before generating answers. This training paradigm enables us to build high-quality models with limited supervision. To support this, we design a two-stage annotation pipeline and a curriculum learning strategy, based on which we construct two datasets: EviBench, a high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments across a wide range of benchmarks demonstrate that DocR1 achieves state-of-the-art performance on multi-page tasks, while consistently maintaining strong results on single-page benchmarks. </p>
<blockquote>
<p>ç†è§£å¤šé¡µæ–‡æ¡£å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦ç²¾ç»†çš„è§†è§‰ç†è§£å’Œè·¨é¡µçš„å¤šæ­¥æ¨ç†ã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºMLLMsä¸­çš„é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨å¤šé¡µæ–‡æ¡£ç†è§£ä¸­çš„åº”ç”¨ä»ç„¶è¢«å¿½è§†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DocR1ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æ–°å‹RLæ¡†æ¶è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚EviGRPOæ˜¯è¯æ®é¡µé¢å¼•å¯¼çš„è¯æ®GRPOï¼ˆEvidence Page-Guided GRPOï¼‰ã€‚EviGRPOç»“åˆäº†è¯æ®æ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¿ƒè¿›ä»ç²—åˆ°ç»†çš„æ¨ç†ç­–ç•¥ï¼Œå¼•å¯¼æ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰é¦–å…ˆæ£€ç´¢ç›¸å…³é¡µé¢ã€‚è¿™ç§è®­ç»ƒæ¨¡å¼ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æœ‰é™çš„ç›‘ç£ä¸‹æ„å»ºé«˜è´¨é‡æ¨¡å‹ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µæ³¨é‡Šç®¡é“å’Œä¸€ä¸ªåŸºäºè¯¾ç¨‹çš„å­¦ä¹ ç­–ç•¥ï¼ŒåŸºäºæ­¤æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šEviBenchæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„è®­ç»ƒé›†ï¼ŒåŒ…å«4.8kç¤ºä¾‹ï¼›ArxivFullQAæ˜¯ä¸€ä¸ªåŸºäºç§‘å­¦è®ºæ–‡çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«8.6ké—®ç­”å¯¹ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDocR1åœ¨å¤šé¡µä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å•é¡µåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¿æŒè‰¯å¥½çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDocR1çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶EviGRPOè¿›è¡Œè®­ç»ƒã€‚EviGRPOç»“åˆäº†è¯æ®æ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ï¼Œå¼•å¯¼æ¨¡å‹é¦–å…ˆæ£€ç´¢ç›¸å…³é¡µé¢å†ç”Ÿæˆç­”æ¡ˆï¼Œå®ç°ä»ç²—åˆ°ç»†çš„æ¨ç†ç­–ç•¥ã€‚è¿™ç§è®­ç»ƒæ¨¡å¼åœ¨æœ‰é™ç›‘ç£ä¸‹æ„å»ºé«˜è´¨é‡æ¨¡å‹ã€‚ä¸ºæ”¯æŒæ­¤ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸¤é˜¶æ®µæ³¨é‡Šç®¡é“å’ŒåŸºäºè¯¾ç¨‹çš„å­¦ä¹ ç­–ç•¥ï¼Œå¹¶æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šé«˜è´¨é‡è®­ç»ƒé›†EviBenchå’ŒåŸºäºç§‘å­¦è®ºæ–‡çš„è¯„ä¼°åŸºå‡†ArxivFullQAã€‚å®éªŒè¡¨æ˜ï¼ŒDocR1åœ¨å¤šé¡µä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶åœ¨å•é¡µåŸºå‡†æµ‹è¯•ä¸­ä¿æŒå¼ºåŠ²è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DocR1æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(MLLM)ï¼Œèƒ½å¤Ÿå¤„ç†å¤šé¡µæ–‡æ¡£ç†è§£ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶EviGRPOï¼Œç”¨äºè®­ç»ƒDocR1æ¨¡å‹ã€‚</li>
<li>EviGRPOç»“åˆè¯æ®æ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ï¼Œé‡‡ç”¨ä»ç²—åˆ°ç»†çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šé«˜è´¨é‡è®­ç»ƒé›†EviBenchå’Œè¯„ä¼°åŸºå‡†ArxivFullQAã€‚</li>
<li>DocR1é€šè¿‡ä¸¤é˜¶æ®µæ³¨é‡Šç®¡é“å’ŒåŸºäºè¯¾ç¨‹çš„å­¦ä¹ ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>DocR1åœ¨å¤šé¡µä»»åŠ¡ä¸Šå®ç°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-099f69be37d0d4aadfaab1526e61ee82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c51a3bed06b1ba474c269bec49a065c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a20d33f64f672f7e27b0b2c218f8ab61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d30075470ad5cbc7cd9d1407268d178.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82b464d9c7a9dddd7f78d1285e88cee1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc6d5d804e50084366061239dbfe6f03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64de34f9ef7784300613611719f267f3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability"><a href="#ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability" class="headerlink" title="ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"></a>ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</h2><p><strong>Authors:Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou</strong></p>
<p>Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{<a target="_blank" rel="noopener" href="https://brightbenchmark.github.io/%7D.%7D">https://brightbenchmark.github.io/}.}</a> Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/8421BCD/ReasonRank">https://github.com/8421BCD/ReasonRank</a>. </p>
<blockquote>
<p>åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ—è¡¨æ’åºæ–¹æ³•åœ¨è®¸å¤šæ®µè½æ’åºä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚éšç€å¤§å‹æ¨ç†æ¨¡å‹çš„å‘å±•ï¼Œè®¸å¤šç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶çš„é€æ­¥æ¨ç†æœ‰åŠ©äºæé«˜åˆ—è¡¨æ’åºæ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œç°æœ‰çš„é‡æ’å™¨åœ¨è®¸å¤šå¤æ‚çš„æ’ååœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”æ¨ç†å¯†é›†å‹é‡æ’å™¨çš„æ’åèƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªå¼€å‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»å¤šä¸ªé¢†åŸŸè·å–è®­ç»ƒæŸ¥è¯¢å’Œæ®µè½ï¼Œå¹¶åº”ç”¨DeepSeek-R1ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ‡ç­¾ã€‚è®¾è®¡äº†ä¸€ç§è‡ªæˆ‘ä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚ä¸ºäº†èµ‹äºˆåˆ—è¡¨é‡æ’å™¨å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„åè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ç”¨äºæ¨ç†æ¨¡å¼å­¦ä¹ çš„å†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå’Œç”¨äºè¿›ä¸€æ­¥å¢å¼ºæ’åèƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µã€‚åœ¨RLé˜¶æ®µï¼ŒåŸºäºåˆ—è¡¨æ’åºçš„æ€§è´¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šè§†è§’æ’åå¥–åŠ±ï¼Œå®ƒæ¯”åŸºäºæ’åæŒ‡æ ‡çš„å¥–åŠ±æ›´æœ‰æ•ˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬è®­ç»ƒçš„æ¨ç†å¯†é›†å‹é‡æ’å™¨ReasonRankæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶ä¸”ä¸é€ç‚¹é‡æ’å™¨Rank1ç›¸æ¯”ï¼Œå»¶è¿Ÿæ—¶é—´æ›´ä½ã€‚é€šè¿‡è¿›ä¸€æ­¥çš„å®éªŒï¼Œæˆ‘ä»¬çš„ReasonRankåœ¨BRIGHTæ’è¡Œæ¦œä¸Šå–å¾—äº†æœ€é«˜æ€§èƒ½ï¼Œå¾—åˆ†40.6åˆ†^[<a target="_blank" rel="noopener" href="https://brightbenchmark.github.io/]%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%8F%AF%E5%9C%A8[https://github.com/8421BCD/ReasonRank%E8%8E%B7%E5%8F%96%E3%80%82](https://github.com/8421BCD/ReasonRank%E8%8E%B7%E5%8F%96%E3%80%82)">https://brightbenchmark.github.io/]ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[https://github.com/8421BCD/ReasonRankè·å–ã€‚](https://github.com/8421BCD/ReasonRank%E8%8E%B7%E5%8F%96%E3%80%82)</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07050v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong><br>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªåŠ¨åŒ–æ¨ç†å¯†é›†è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶çš„åˆ—è¡¨æ’åæ–¹æ³•ã€‚é€šè¿‡DeepSeek-R1ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ ‡ç­¾ï¼Œå¹¶è®¾è®¡è‡ªæˆ‘ä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µåè®­ç»ƒå¢å¼ºæ¨¡å‹çš„æ¨ç†æ’åèƒ½åŠ›ã€‚é€šè¿‡å…¨é¢çš„å®éªŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼Œå®ç°ä½å»¶è¿Ÿå¹¶å…·æœ‰æœ€æ–°çš„é«˜æ€§èƒ½è¡¨ç°ã€‚è¯¥æ¨¡å‹è¢«ç§°ä¸ºReasonRankï¼Œå¹¶å·²åœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šå–å¾—ä¼˜å¼‚æˆç»©ã€‚æºä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ®µè½æ’åä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æ¨ç†å¯†é›†çš„è®­ç»ƒæ•°æ®ç¨€ç¼ºå¯¼è‡´ç°æœ‰é‡æ–°æ’åå™¨çš„æ€§èƒ½ä¸è¶³ã€‚</li>
<li>æå‡ºä¸€ç§è‡ªåŠ¨åŒ–æ¨ç†å¯†é›†è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ä»¥æ”¹å–„è¿™ä¸€çŠ¶å†µã€‚</li>
<li>ä½¿ç”¨DeepSeek-R1ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ ‡ç­¾å¹¶é‡‡ç”¨è‡ªæˆ‘ä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ã€‚</li>
<li>ä¸¤é˜¶æ®µåè®­ç»ƒä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†æ’åèƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†åŸºäºå¤šè§†è§’æ’åçš„å¥–åŠ±æœºåˆ¶ä»¥æé«˜å¼ºåŒ–å­¦ä¹ é˜¶æ®µçš„æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a3f036ea55434d0a7e1e8df4b2ba05c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bf74ad19c087b0645870523525ca004.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd2e577899524bfb6181c5094398b050.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e00b06cc0960d3a7fdb209345eed01a1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance"><a href="#AMFT-Aligning-LLM-Reasoners-by-Meta-Learning-the-Optimal-Imitation-Exploration-Balance" class="headerlink" title="AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal   Imitation-Exploration Balance"></a>AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal   Imitation-Exploration Balance</h2><p><strong>Authors:Lixuan He, Jie Feng, Yong Li</strong></p>
<p>Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFTâ€™s implicit, path-level reward and RLâ€™s explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFTâ€™s stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via <a target="_blank" rel="noopener" href="https://github.com/hlxtsyj/AMFT">https://github.com/hlxtsyj/AMFT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µç®¡é“è¿›è¡Œå¾®è°ƒï¼Œä»¥å®Œæˆæ¨ç†ä»»åŠ¡ã€‚è¿™ä¸€è¿‡ç¨‹å……æ»¡äº†ç¾éš¾æ€§é—å¿˜å’Œæ¨¡ä»¿ä¸æ¢ç´¢ä¹‹é—´çš„æ¬¡ä¼˜æƒè¡¡ã€‚æœ€è¿‘çš„å•é˜¶æ®µæ–¹æ³•è¯•å›¾é€šè¿‡å¯å‘å¼æ–¹æ³•ç»Ÿä¸€SFTå’ŒRLï¼Œä½†ç¼ºä¹ä¸€ç§åŠ¨æ€å¹³è¡¡è¿™ä¸¤ç§èŒƒå¼çš„åŸåˆ™æ€§æœºåˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡éšå¼å¥–åŠ±çš„ç†è®ºè§†è§’é‡æ–°çœ‹å¾…è¿™ä¸€æŒ‘æˆ˜ï¼Œå°†SFTå’ŒRLè§†ä¸ºäº’è¡¥çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œä¸æ˜¯æˆªç„¶ä¸åŒçš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”å…ƒå¾®è°ƒï¼ˆAMFTï¼‰è¿™ä¸€æ–°é¢–çš„å•é˜¶æ®µç®—æ³•ï¼Œå®ƒå­¦ä¹ SFTçš„éšå¼è·¯å¾„çº§å¥–åŠ±å’ŒRLçš„æ˜¾å¼ç»“æœçº§å¥–åŠ±ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚AMFTçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå…ƒæ¢¯åº¦è‡ªé€‚åº”æƒé‡æ§åˆ¶å™¨ï¼Œå®ƒå°†SFT-RLçš„å¹³è¡¡è§†ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œé€šè¿‡åŠ¨æ€ä¼˜åŒ–ä»¥æœ€å¤§åŒ–é•¿æœŸä»»åŠ¡æ€§èƒ½ã€‚è¿™ç§å‰ç»æ€§çš„æ–¹æ³•é€šè¿‡æ”¿ç­–ç†µè¿›è¡Œç¨³å®šæ€§è°ƒèŠ‚ï¼Œèƒ½å¤Ÿè‡ªä¸»åœ°å‘ç°æœ‰æ•ˆçš„è®­ç»ƒè¯¾ç¨‹ã€‚æˆ‘ä»¬åœ¨æ¶µç›–æ•°å­¦æ¨ç†ã€æŠ½è±¡è§†è§‰æ¨ç†ï¼ˆé€šç”¨ç‚¹ï¼‰å’Œè§†è§‰è¯­è¨€å¯¼èˆªï¼ˆV-IRLï¼‰ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹AMFTè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚AMFTæŒç»­åˆ·æ–°äº†æœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼Œå¹¶åœ¨ç¦»åˆ†å¸ƒï¼ˆOODï¼‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„å¤–æ¨èƒ½åŠ›ã€‚æ¶ˆèç ”ç©¶å’Œè®­ç»ƒåŠ¨æ€åˆ†æè¯å®ï¼Œå…ƒå­¦ä¹ æ§åˆ¶å™¨å¯¹äºAMFTçš„ç¨³å®šæ€§ã€æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½è‡³å…³é‡è¦ï¼Œä¸ºLLMå¯¹é½æä¾›äº†æ›´å…·åŸåˆ™æ€§å’Œæœ‰æ•ˆæ€§çš„èŒƒå¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hlxtsyj/AMFT%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hlxtsyj/AMFTå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p>ä¸­æ–‡ç¿»è¯‘å¦‚ä¸‹ï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06944v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/hlxtsyj/AMFT">https://github.com/hlxtsyj/AMFT</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µç®¡é“ä¸­è¿›è¡Œå¾®è°ƒä»¥æ‰§è¡Œæ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹é¢ä¸´ç€ç¾éš¾æ€§é—å¿˜å’Œæ¨¡ä»¿ä¸æ¢ç´¢ä¹‹é—´æ¬¡ä¼˜æƒè¡¡çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºè‡ªé€‚åº”å…ƒå¾®è°ƒï¼ˆAMFTï¼‰çš„æ–°å‹å•é˜¶æ®µç®—æ³•ï¼Œå®ƒé€šè¿‡éšå¼å¥–åŠ±å’Œæ˜¾å¼å¥–åŠ±ä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚AMFTçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå…ƒæ¢¯åº¦è‡ªé€‚åº”æƒé‡æ§åˆ¶å™¨ï¼Œå®ƒå°†SFTå’ŒRLä¹‹é—´çš„å¹³è¡¡è§†ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå¹¶å¯¹å…¶è¿›è¡ŒåŠ¨æ€ä¼˜åŒ–ï¼Œä»¥æœ€å¤§åŒ–é•¿æœŸä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ¶µç›–æ•°å­¦æ¨ç†ã€æŠ½è±¡è§†è§‰æ¨ç†ï¼ˆGeneral Pointsï¼‰å’Œè§†è§‰è¯­è¨€å¯¼èˆªï¼ˆV-IRLï¼‰ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹AMFTè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚AMFTè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚æ¶ˆèç ”ç©¶å’Œè®­ç»ƒåŠ¨æ€åˆ†æè¯å®ï¼Œå…ƒå­¦ä¹ æ§åˆ¶å™¨å¯¹äºAMFTçš„ç¨³å®šæ€§ã€æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½è‡³å…³é‡è¦ï¼Œä¸ºLLMå¯¹é½æä¾›äº†æ›´ç³»ç»Ÿå’Œæœ‰æ•ˆçš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¾®è°ƒè¿›è¡Œæ¨ç†ä»»åŠ¡æ—¶é¢ä¸´ç¾éš¾æ€§é—å¿˜å’Œæ¨¡ä»¿ä¸æ¢ç´¢ä¹‹é—´æƒè¡¡çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°è¯•é€šè¿‡å¯å‘å¼æ–¹æ³•ç»Ÿä¸€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä½†ç¼ºä¹åŠ¨æ€å¹³è¡¡ä¸¤ç§æ–¹æ³•çš„æœºåˆ¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”å…ƒå¾®è°ƒï¼ˆAMFTï¼‰ç®—æ³•ï¼Œé€šè¿‡éšå¼å¥–åŠ±å’Œæ˜¾å¼å¥–åŠ±ä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>AMFTä½¿ç”¨ä¸€ä¸ªå…ƒæ¢¯åº¦è‡ªé€‚åº”æƒé‡æ§åˆ¶å™¨æ¥åŠ¨æ€ä¼˜åŒ–SFTå’ŒRLä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>AMFTåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¶ˆèç ”ç©¶å’Œè®­ç»ƒåŠ¨æ€åˆ†æè¯å®ï¼Œå…ƒå­¦ä¹ æ§åˆ¶å™¨å¯¹AMFTçš„æ€§èƒ½èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f85a4685a5507aa4e36ef6fc15d5d4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3447eaa172948c46a61f22ddbe59b33e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37969e86dcae21a50f34ab430e2afc13.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning"><a href="#AR-GRPO-Training-Autoregressive-Image-Generation-Models-via-Reinforcement-Learning" class="headerlink" title="AR-GRPO: Training Autoregressive Image Generation Models via   Reinforcement Learning"></a>AR-GRPO: Training Autoregressive Image Generation Models via   Reinforcement Learning</h2><p><strong>Authors:Shihao Yuan, Yahui Liu, Yang Yue, Jingyuan Zhang, Wangmeng Zuo, Qi Wang, Fuzheng Zhang, Guorui Zhou</strong></p>
<p>Inspired by the success of reinforcement learning (RL) in refining large language models (LLMs), we propose AR-GRPO, an approach to integrate online RL training into autoregressive (AR) image generation models. We adapt the Group Relative Policy Optimization (GRPO) algorithm to refine the vanilla autoregressive modelsâ€™ outputs by carefully designed reward functions that evaluate generated images across multiple quality dimensions, including perceptual quality, realism, and semantic fidelity. We conduct comprehensive experiments on both class-conditional (i.e., class-to-image) and text-conditional (i.e., text-to-image) image generation tasks, demonstrating that our RL-enhanced framework significantly improves both the image quality and human preference of generated images compared to the standard AR baselines. Our results show consistent improvements across various evaluation metrics, establishing the viability of RL-based optimization for AR image generation and opening new avenues for controllable and high-quality image synthesis. The source codes and models are available at: <a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/AR-GRPO">https://github.com/Kwai-Klear/AR-GRPO</a>. </p>
<blockquote>
<p>å—å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å–å¾—æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†AR-GRPOæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°†åœ¨çº¿RLè®­ç»ƒé›†æˆåˆ°è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„æ–¹æ³•ã€‚æˆ‘ä»¬é‡‡ç”¨Group Relative Policy Optimization (GRPO)ç®—æ³•ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°æ¥ä¼˜åŒ–åŸºæœ¬çš„è‡ªå›å½’æ¨¡å‹çš„è¾“å‡ºï¼Œè¿™äº›å¥–åŠ±å‡½æ•°ä»å¤šä¸ªè´¨é‡ç»´åº¦è¯„ä¼°ç”Ÿæˆçš„å›¾åƒï¼ŒåŒ…æ‹¬æ„ŸçŸ¥è´¨é‡ã€é€¼çœŸåº¦å’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚æˆ‘ä»¬å¯¹ç±»åˆ«æ¡ä»¶ï¼ˆå³ç±»åˆ°å›¾åƒï¼‰å’Œæ–‡æœ¬æ¡ä»¶ï¼ˆå³æ–‡æœ¬åˆ°å›¾åƒï¼‰çš„å›¾åƒç”Ÿæˆä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¢å¼ºå‹RLæ¡†æ¶ä¸æ ‡å‡†çš„è‡ªå›å½’åŸºçº¿ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„å›¾åƒçš„è´¨é‡å’Œäººç±»åå¥½ã€‚æˆ‘ä»¬çš„ç»“æœåœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æé«˜ï¼Œè¯æ˜äº†åŸºäºRLçš„ä¼˜åŒ–åœ¨ARå›¾åƒç”Ÿæˆä¸­çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸ºå¯æ§å’Œé«˜è´¨é‡çš„å›¾åƒåˆæˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚æºä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/AR-GRPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Kwai-Klear/AR-GRPOè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06924v1">PDF</a> 27 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é›†æˆåˆ°è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„æ–¹æ³•AR-GRPOã€‚è¯¥æ–¹æ³•é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°æ¥è¯„ä¼°ç”Ÿæˆçš„å›¾åƒåœ¨å¤šä¸ªè´¨é‡ç»´åº¦ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥è´¨é‡ã€é€¼çœŸåº¦å’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†ARåŸºçº¿ç›¸æ¯”ï¼ŒRLå¢å¼ºçš„æ¡†æ¶æ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„å›¾åƒçš„è´¨é‡å’Œäººç±»åå¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AR-GRPOæ–¹æ³•æˆåŠŸå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼ŒAR-GRPOè¯„ä¼°äº†ç”Ÿæˆå›¾åƒåœ¨æ„ŸçŸ¥è´¨é‡ã€é€¼çœŸåº¦å’Œè¯­ä¹‰ä¿çœŸåº¦ç­‰å¤šä¸ªç»´åº¦ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒAR-GRPOåœ¨ç±»æ¡ä»¶ï¼ˆå¦‚ç±»åˆ«åˆ°å›¾åƒï¼‰å’Œæ–‡æœ¬æ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬åˆ°å›¾åƒï¼‰çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸æ ‡å‡†ARåŸºçº¿ç›¸æ¯”ï¼ŒAR-GRPOç”Ÿæˆçš„å›¾åƒè´¨é‡å’Œäººç±»åå¥½å‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>AR-GRPOæ–¹æ³•åœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¸€è‡´ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>AR-GRPOçš„æºä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/AR-GRPO%E4%B8%8A%E8%8E%B7%E3%80%82">https://github.com/Kwai-Klear/AR-GRPOä¸Šè·å–ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5fabc5d6e11c097b8a4171ddb4eaaac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38533be3bae1d203b71557c01d8508aa.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language"><a href="#Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language" class="headerlink" title="Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"></a>Technical Report: Full-Stack Fine-Tuning for the Q Programming Language</h2><p><strong>Authors:Brendan R. Hogan, Will Brown, Adel Boyarsky, Anderson Schneider, Yuriy Nevmyvaka</strong></p>
<p>Even though large language models are becoming increasingly capable, it is still unreasonable to expect them to excel at tasks that are under-represented on the Internet. Leveraging LLMs for specialized applications, particularly in niche programming languages and private domains, remains challenging and largely unsolved. In this work, we address this gap by presenting a comprehensive, open-source approach for adapting LLMs to the Q programming language, a popular tool in quantitative finance that is much less present on the Internet compared to Python, C, Java, and other &#96;&#96;mainstreamâ€ languages and is therefore not a strong suit of general-purpose AI models. We introduce a new Leetcode style evaluation dataset for Q, benchmark major frontier models on the dataset, then do pretraining, supervised fine tuning, and reinforcement learning to train a suite of reasoning and non-reasoning models based on the Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our best model achieves a pass@1 accuracy of 59 percent on our Q benchmark, surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent. Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task. In addition to releasing models, code, and data, we provide a detailed blueprint for dataset construction, model pretraining, supervised fine-tuning, and reinforcement learning. Our methodology is broadly applicable, and we discuss how these techniques can be extended to other tasks, including those where evaluation may rely on soft or subjective signals. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è¶Šæ¥è¶Šå¼ºï¼Œä½†æœŸå¾…å®ƒä»¬åœ¨äº’è”ç½‘ä¸Šä»£è¡¨æ€§ä¸è¶³çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ä»ç„¶ä¸åˆç†ã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºä¸“ä¸šåº”ç”¨ç¨‹åºï¼Œç‰¹åˆ«æ˜¯åœ¨å°ä¼—ç¼–ç¨‹è¯­è¨€å’Œç§æœ‰é¢†åŸŸï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ä¸”å¤§éƒ¨åˆ†å°šæœªè§£å†³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§é€‚åº”Qç¼–ç¨‹è¯­è¨€çš„å…¨é¢å¼€æºæ–¹æ³•æ¥è§£å†³è¿™ä¸€å·®è·ã€‚Qæ˜¯é‡åŒ–é‡‘èä¸­æµè¡Œçš„å·¥å…·ï¼Œä¸Pythonã€Cã€Javaç­‰å…¶ä»–â€œä¸»æµâ€è¯­è¨€ç›¸æ¯”ï¼Œå…¶åœ¨äº’è”ç½‘ä¸Šçš„å­˜åœ¨æ„Ÿè¦ä½å¾—å¤šï¼Œå› æ­¤å¹¶ä¸æ˜¯é€šç”¨AIæ¨¡å‹çš„å¼ºé¡¹ã€‚æˆ‘ä»¬ä¸ºQè¯­è¨€å¼•å…¥äº†æ–°çš„Leetcodeé£æ ¼è¯„ä¼°æ•°æ®é›†ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸»è¦çš„å‰æ²¿æ¨¡å‹ï¼Œç„¶åè¿›è¡Œé¢„è®­ç»ƒã€æœ‰ç›‘ç£çš„å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ï¼ŒåŸºäºQwen-2.5ç³»åˆ—è®­ç»ƒäº†ä¸€å¥—æ¨ç†å’Œéæ¨ç†æ¨¡å‹ï¼Œæ¶µç›–äº”ç§å‚æ•°å¤§å°ï¼ˆ1.5Bã€3Bã€7Bã€14Bã€32Bï¼‰ã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹åœ¨æˆ‘ä»¬çš„QåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†59%çš„pass@1å‡†ç¡®ç‡ï¼Œæ¯”æœ€ä½³æ€§èƒ½çš„å‰æ²¿æ¨¡å‹Claude Opus-4é«˜å‡º29.5%ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹ï¼Œå³ä½¿æ˜¯æˆ‘ä»¬æœ€å°çš„1.5Bæ¨¡å‹ï¼Œåœ¨æ­¤ä»»åŠ¡ä¸Šä¹Ÿä¼˜äºGPT-4.1ã€‚é™¤äº†å‘å¸ƒæ¨¡å‹ã€ä»£ç å’Œæ•°æ®å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å…³äºæ•°æ®é›†æ„å»ºã€æ¨¡å‹é¢„è®­ç»ƒã€æœ‰ç›‘ç£çš„å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ çš„è¯¦ç»†è“å›¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¹¿æ³›é€‚ç”¨çš„ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡ï¼ŒåŒ…æ‹¬é‚£äº›å¯èƒ½ä¾èµ–äºè½¯æŒ‡æ ‡æˆ–ä¸»è§‚ä¿¡å·çš„è¯„ä¼°ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06813v2">PDF</a> 40 pages</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº’è”ç½‘è¾ƒå°‘ä»£è¡¨é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€‚åº”äºQç¼–ç¨‹è¯­è¨€ï¼ˆé‡‘èé‡åŒ–ä¸­æµè¡Œå·¥å…·ï¼‰çš„ç»¼åˆã€å¼€æºæ–¹æ³•ã€‚ç ”ç©¶å‘å¸ƒäº†é’ˆå¯¹Qç¼–ç¨‹è¯­è¨€çš„Leetcodeé£æ ¼è¯„ä¼°æ•°æ®é›†ï¼Œå¯¹å‰æ²¿æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•è®­ç»ƒæ¨¡å‹ã€‚æœ€ä½³æ¨¡å‹åœ¨Qè¯„ä¼°ä¸Šè¾¾åˆ°59%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šå…¶ä»–å‰æ²¿æ¨¡å‹å¹¶ä¼˜äºGPT-4.1ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›æ•°æ®é›†æ„å»ºã€æ¨¡å‹é¢„è®­ç»ƒç­‰è¯¦ç»†è“å›¾ã€‚æ­¤æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£è¡¨æ€§ä¸è¶³çš„ä»»åŠ¡é¢†åŸŸï¼ˆå¦‚é‡‘èé‡åŒ–ç¼–ç¨‹è¯­è¨€Qï¼‰è¡¨ç°ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>é’ˆå¯¹Qç¼–ç¨‹è¯­è¨€ï¼Œæå‡ºäº†ä¸€ç§ç»¼åˆã€å¼€æºçš„é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>å‘å¸ƒäº†é’ˆå¯¹Qç¼–ç¨‹è¯­è¨€çš„Leetcodeé£æ ¼è¯„ä¼°æ•°æ®é›†ï¼Œç”¨äºæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯è®­ç»ƒæ¨¡å‹ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨Qè¯„ä¼°ä¸Šå®ç°è¾ƒé«˜å‡†ç¡®ç‡ï¼Œä¼˜äºå½“å‰å‰æ²¿æ¨¡å‹å’ŒGPT-4.1ã€‚</li>
<li>ç ”ç©¶æä¾›äº†è¯¦ç»†çš„æ•°æ®é›†æ„å»ºã€æ¨¡å‹é¢„è®­ç»ƒç­‰è“å›¾ï¼Œä¸ºå…¶ä»–ç±»ä¼¼ä»»åŠ¡æä¾›å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92b614bf599590fa3389377d8548d29f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35f51bd7dc885f5fb6c792d0e3d899a3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ContextGuard-LVLM-Enhancing-News-Veracity-through-Fine-grained-Cross-modal-Contextual-Consistency-Verification"><a href="#ContextGuard-LVLM-Enhancing-News-Veracity-through-Fine-grained-Cross-modal-Contextual-Consistency-Verification" class="headerlink" title="ContextGuard-LVLM: Enhancing News Veracity through Fine-grained   Cross-modal Contextual Consistency Verification"></a>ContextGuard-LVLM: Enhancing News Veracity through Fine-grained   Cross-modal Contextual Consistency Verification</h2><p><strong>Authors:Sihan Ma, Qiming Wu, Ruotong Jiang, Frank Burns</strong></p>
<p>The proliferation of digital news media necessitates robust methods for verifying content veracity, particularly regarding the consistency between visual and textual information. Traditional approaches often fall short in addressing the fine-grained cross-modal contextual consistency (FCCC) problem, which encompasses deeper alignment of visual narrative, emotional tone, and background information with text, beyond mere entity matching. To address this, we propose ContextGuard-LVLM, a novel framework built upon advanced Vision-Language Large Models (LVLMs) and integrating a multi-stage contextual reasoning mechanism. Our model is uniquely enhanced through reinforced or adversarial learning paradigms, enabling it to detect subtle contextual misalignments that evade zero-shot baselines. We extend and augment three established datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new fine-grained contextual annotations, including â€œcontextual sentiment,â€ â€œvisual narrative theme,â€ and â€œscene-event logical coherence,â€ and introduce a comprehensive CTXT (Contextual Coherence) entity type. Extensive experiments demonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art zero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all fine-grained consistency tasks, showing significant improvements in complex logical reasoning and nuanced contextual understanding. Furthermore, our model exhibits superior robustness to subtle perturbations and a higher agreement rate with human expert judgments on challenging samples, affirming its efficacy in discerning sophisticated forms of context detachment. </p>
<blockquote>
<p>æ•°å­—æ–°é—»åª’ä½“çš„æ™®åŠè¿«åˆ‡éœ€è¦éªŒè¯å†…å®¹çœŸå®æ€§çš„ç¨³å¥æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ä¹‹é—´çš„ä¸€è‡´æ€§æ–¹é¢ã€‚ä¼ ç»Ÿçš„æ–¹æ³•å¾€å¾€éš¾ä»¥è§£å†³ç²¾ç»†çš„è·¨æ¨¡æ€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼ˆFCCCï¼‰é—®é¢˜ï¼Œè¯¥é—®é¢˜æ¶‰åŠè§†è§‰å™äº‹ã€æƒ…æ„ŸåŸºè°ƒã€èƒŒæ™¯ä¿¡æ¯ä¸æ–‡æœ¬ä¹‹é—´çš„æ›´æ·±å±‚æ¬¡å¯¹é½ï¼Œè€Œä¸ä»…ä»…æ˜¯å®ä½“åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ContextGuard-LVLMæ¡†æ¶ï¼Œå®ƒæ˜¯åŸºäºå…ˆè¿›çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆLVLMsï¼‰æ„å»ºçš„ï¼Œå¹¶èåˆäº†å¤šé˜¶æ®µä¸Šä¸‹æ–‡æ¨ç†æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¼ºåŒ–æˆ–å¯¹æŠ—æ€§å­¦ä¹ èŒƒå¼è¿›è¡Œäº†ç‹¬ç‰¹å¢å¼ºï¼Œä½¿å…¶èƒ½å¤Ÿæ£€æµ‹é›¶æ ·æœ¬åŸºçº¿æ‰€å¿½ç•¥çš„ç»†å¾®ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ã€‚æˆ‘ä»¬å¯¹ä¸‰ä¸ªç°æœ‰çš„æ•°æ®é›†ï¼ˆTamperedNews-Entã€News400-Entã€MMG-Entï¼‰è¿›è¡Œäº†æ‰©å±•å’Œæ‰©å……ï¼ŒåŠ å…¥äº†æ–°çš„ç²¾ç»†ä¸Šä¸‹æ–‡æ³¨é‡Šï¼ŒåŒ…æ‹¬â€œä¸Šä¸‹æ–‡æƒ…æ„Ÿâ€ã€â€œè§†è§‰å™äº‹ä¸»é¢˜â€å’Œâ€œåœºæ™¯äº‹ä»¶é€»è¾‘è¿è´¯æ€§â€ï¼Œå¹¶å¼•å…¥äº†å…¨é¢çš„CTXTï¼ˆä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼‰å®ä½“ç±»å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒContextGuard-LVLMåœ¨æ‰€æœ‰ç²¾ç»†ä¸€è‡´æ€§ä»»åŠ¡ä¸Šéƒ½è¶…è¶Šäº†æœ€æ–°çš„é›¶æ ·æœ¬LVLMåŸºçº¿ï¼ˆInstructBLIPå’ŒLLaVA 1.5ï¼‰ï¼Œåœ¨å¤æ‚çš„é€»è¾‘æ¨ç†å’Œå¾®å¦™ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯¹ç»†å¾®æ‰°åŠ¨å…·æœ‰æ›´é«˜çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ä¸Šä¸äººç±»ä¸“å®¶åˆ¤æ–­æœ‰è¾ƒé«˜çš„ç¬¦åˆç‡ï¼Œè¯å®äº†å®ƒåœ¨è¾¨åˆ«å¤æ‚ä¸Šä¸‹æ–‡è„±ç¦»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå…ˆè¿›çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆLVLMsï¼‰çš„ContextGuard-LVLMæ¡†æ¶ï¼Œç”¨äºéªŒè¯æ•°å­—æ–°é—»åª’ä½“çš„å†…å®¹çœŸå®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šé˜¶æ®µä¸Šä¸‹æ–‡æ¨ç†æœºåˆ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†è·¨æ¨¡æ€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼ˆFCCCï¼‰é—®é¢˜æ—¶çš„ä¸è¶³ï¼Œæé«˜äº†æ¨¡å‹åœ¨æ£€æµ‹ä¸Šä¸‹æ–‡ç»†å¾®ä¸å¯¹é½æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒContextGuard-LVLMåœ¨å‡ ä¹æ‰€æœ‰ç²¾ç»†çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä»»åŠ¡ä¸Šéƒ½ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬LVLMåŸºçº¿ï¼Œè¡¨ç°å‡ºæ›´å‡ºè‰²çš„å¤æ‚é€»è¾‘æ¨ç†å’Œç»†å¾®ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—æ–°é—»åª’ä½“çš„æ™®åŠè¦æ±‚å¼€å‘éªŒè¯å†…å®¹çœŸå®æ€§çš„ç¨³å¥æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ä¹‹é—´çš„ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†è·¨æ¨¡æ€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼ˆFCCCï¼‰é—®é¢˜æ—¶å¸¸å¸¸ä¸è¶³ï¼Œéœ€è¦æ›´æ·±å…¥çš„å¯¹è§†è§‰å™äº‹ã€æƒ…æ„ŸåŸºè°ƒã€èƒŒæ™¯ä¿¡æ¯ä¸æ–‡æœ¬çš„åŒ¹é…ã€‚</li>
<li>ContextGuard-LVLMæ¡†æ¶åŸºäºå…ˆè¿›çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆLVLMsï¼‰ï¼Œé€šè¿‡å¤šé˜¶æ®µä¸Šä¸‹æ–‡æ¨ç†æœºåˆ¶è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>å¼ºåŒ–æˆ–å¯¹æŠ—æ€§å­¦ä¹ èŒƒå¼è¢«ç”¨æ¥å¢å¼ºContextGuard-LVLMçš„æ£€æµ‹èƒ½åŠ›ï¼Œä»¥è¯†åˆ«é€ƒé¿é›¶æ ·æœ¬åŸºçº¿çš„å¾®å¦™ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ã€‚</li>
<li>å¯¹ä¸‰ä¸ªå·²å»ºç«‹çš„æ•°æ®é›†è¿›è¡Œäº†æ‰©å±•å’Œå¢å¼ºï¼Œå¼•å…¥äº†æ–°çš„ç²¾ç»†ä¸Šä¸‹æ–‡æ³¨é‡Šï¼ŒåŒ…æ‹¬â€œä¸Šä¸‹æ–‡æƒ…æ„Ÿâ€ã€â€œè§†è§‰å™äº‹ä¸»é¢˜â€å’Œâ€œåœºæ™¯äº‹ä»¶é€»è¾‘è¿è´¯æ€§â€ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒContextGuard-LVLMåœ¨æ‰€æœ‰ç²¾ç»†çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬LVLMåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d87b745a3f1325fd2eacd99b2d7e56ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d59b48e4500a0a6f8c83293341ef32f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0604d1794ea3edc8354f76c872b60e7e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  Noise Hypernetworks Amortizing Test-Time Compute in Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-659630aaf80338dd7d2941cb20ebb92e.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  RealisMotion Decomposed Human Motion Control and Video Generation in   the World Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
