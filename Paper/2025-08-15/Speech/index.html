<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  HM-Talker Hybrid Motion Modeling for High-Fidelity Talking Head   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fabdbcee9e0a637a52589d9950c0adbc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    46 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-15-æ›´æ–°"><a href="#2025-08-15-æ›´æ–°" class="headerlink" title="2025-08-15 æ›´æ–°"></a>2025-08-15 æ›´æ–°</h1><h2 id="HM-Talker-Hybrid-Motion-Modeling-for-High-Fidelity-Talking-Head-Synthesis"><a href="#HM-Talker-Hybrid-Motion-Modeling-for-High-Fidelity-Talking-Head-Synthesis" class="headerlink" title="HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head   Synthesis"></a>HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head   Synthesis</h2><p><strong>Authors:Shiyu Liu, Kui Jiang, Xianming Liu, Hongxun Yao, Xiaocheng Feng</strong></p>
<p>Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlationsâ€“an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit&#x2F;explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit&#x2F;explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talkerâ€™s superiority over state-of-the-art methods in visual quality and lip-sync accuracy. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´è§†é¢‘ç”Ÿæˆæé«˜äº†äººæœºäº¤äº’ä¸­çš„ç”¨æˆ·å‚ä¸åº¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ç»å¸¸äº§ç”Ÿè¿åŠ¨æ¨¡ç³Šå’Œå˜´å”‡æŠ–åŠ¨çš„è§†é¢‘ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºéŸ³é¢‘é¢éƒ¨è¿åŠ¨å…³è”çš„éšå¼å»ºæ¨¡â€”â€”è¿™ç§æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„å‘éŸ³å…ˆéªŒçŸ¥è¯†ï¼ˆå³ä¸è¯­éŸ³ç›¸å…³çš„é¢éƒ¨è¿åŠ¨çš„è§£å‰–æŒ‡å¯¼ï¼‰ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†HM-Talkerï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è¯´è¯äººå¤´éƒ¨çš„å…¨æ–°æ¡†æ¶ã€‚HM-Talkeråˆ©ç”¨äº†ä¸€ç§æ··åˆè¿åŠ¨è¡¨ç¤ºæ³•ï¼Œç»“åˆäº†éšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢ã€‚æ˜¾å¼çº¿ç´¢ä½¿ç”¨è¡Œä¸ºå•å…ƒï¼ˆAUsï¼‰ï¼Œå³é¢éƒ¨è§£å‰–ä¸Šå®šä¹‰çš„è‚Œè‚‰è¿åŠ¨ï¼Œä»¥åŠéšå¼ç‰¹å¾æ¥æœ€å°åŒ–éŸ³ç´ -è¡¨æƒ…åŠ¨ä½œçš„ä¸å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„è·¨æ¨¡æ€åˆ†è§£æ¨¡å—ï¼ˆCMDMï¼‰åœ¨é¢„æµ‹ç›´æ¥ä¸éŸ³é¢‘è¾“å…¥å¯¹é½çš„è§†è§‰çº¿ç´¢çš„AUsæ—¶ï¼Œæå–äº’è¡¥çš„éšå¼&#x2F;æ˜¾å¼è¿åŠ¨ç‰¹å¾ã€‚ä¸ºäº†å‡è½»æ˜¾å¼ç‰¹å¾ä¸­çš„èº«ä»½ç›¸å…³åè§ï¼Œå¹¶å¢å¼ºè·¨ä¸»ä½“æ³›åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼ˆHMMMï¼‰ã€‚è¯¥æ¨¡å—åŠ¨æ€åˆå¹¶éšæœºé…å¯¹çš„éšå¼&#x2F;æ˜¾å¼ç‰¹å¾ï¼Œå¼ºåˆ¶å®æ–½èº«ä»½æ— å…³çš„å­¦ä¹ ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œå®ç°äº†è·¨ä¸åŒèº«ä»½çš„ç¨³å¥å˜´å”‡åŒæ­¥ï¼Œæ¨åŠ¨äº†ä¸ªæ€§åŒ–è¯´è¯å¤´éƒ¨åˆæˆçš„è¿›æ­¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå˜´å”‡åŒæ­¥å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10566v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHM-Talkerçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è¯´è¯äººå¤´åƒã€‚è¯¥æ¡†æ¶ç»“åˆäº†éšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢çš„æ··åˆè¿åŠ¨è¡¨ç¤ºï¼Œä½¿ç”¨é¢éƒ¨è‚Œè‚‰åŠ¨ä½œå•ä½ï¼ˆAUsï¼‰ç­‰æ˜¾å¼çº¿ç´¢å’Œéšå¼ç‰¹å¾ï¼Œä»¥æœ€å°åŒ–éŸ³ç´ ä¸è¡¨æƒ…ä¹‹é—´çš„ä¸åŒ¹é…ã€‚é€šè¿‡è·¨æ¨¡æ€è§£è€¦æ¨¡å—ï¼ˆCMDMï¼‰æå–äº’è¡¥çš„éšå¼&#x2F;æ˜¾å¼è¿åŠ¨ç‰¹å¾ï¼Œå¹¶ä»éŸ³é¢‘è¾“å…¥ä¸­ç›´æ¥é¢„æµ‹ä¸è§†è§‰çº¿ç´¢å¯¹é½çš„AUsã€‚åŒæ—¶ï¼Œé€šè¿‡æ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼ˆHMMMï¼‰åŠ¨æ€åˆå¹¶éšæœºé…å¯¹çš„éšå¼&#x2F;æ˜¾å¼ç‰¹å¾ï¼Œå®ç°èº«ä»½æ— å…³çš„å­¦ä¹ ï¼Œæé«˜äº†è·¨ä¸»ä½“æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒHM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå”‡åŒæ­¥å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HM-Talkeræ¡†æ¶ç»“åˆäº†éšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢ï¼Œæé«˜äº†éŸ³é¢‘é©±åŠ¨çš„å¤´åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>æ˜¾å¼è¿åŠ¨çº¿ç´¢ä½¿ç”¨é¢éƒ¨è‚Œè‚‰åŠ¨ä½œå•ä½ï¼ˆAUsï¼‰ï¼Œå‡å°‘äº†éŸ³ç´ ä¸è¡¨æƒ…ä¹‹é—´çš„ä¸åŒ¹é…ã€‚</li>
<li>Cross-Modal Disentanglement Module (CMDM)ç”¨äºæå–éšå¼&#x2F;æ˜¾å¼è¿åŠ¨ç‰¹å¾çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>Hybrid Motion Modeling Module (HMMM)é€šè¿‡åŠ¨æ€åˆå¹¶éšå¼&#x2F;æ˜¾å¼ç‰¹å¾å®ç°èº«ä»½æ— å…³çš„å­¦ä¹ ï¼Œæé«˜äº†è·¨ä¸»ä½“æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>HM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå”‡åŒæ­¥å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è¯´è¯äººå¤´åƒï¼Œå¢å¼ºäº†äººæœºäº¤äº’ä¸­çš„ç”¨æˆ·å‚ä¸åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7afac66b8df2cfdcac41d77b1d398766.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7513ff99be6f2458fa9404d6a5e54ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db42566ef66fcdea05b45432682d0613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89c0fba6949d44d082d9bd511e4d327c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f53c1fb0c3a68cf5b16a61f314253e5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbf1c3012832ec30fe55aa7728d31cd7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Alternating-Approach-Putt-Models-for-Multi-Stage-Speech-Enhancement"><a href="#Alternating-Approach-Putt-Models-for-Multi-Stage-Speech-Enhancement" class="headerlink" title="Alternating Approach-Putt Models for Multi-Stage Speech Enhancement"></a>Alternating Approach-Putt Models for Multi-Stage Speech Enhancement</h2><p><strong>Authors:Iksoon Jeong, Kyung-Joong Kim, Kang-Hun Ahn</strong></p>
<p>Speech enhancement using artificial neural networks aims to remove noise from noisy speech signals while preserving the speech content. However, speech enhancement networks often introduce distortions to the speech signal, referred to as artifacts, which can degrade audio quality. In this work, we propose a post-processing neural network designed to mitigate artifacts introduced by speech enhancement models. Inspired by the analogy of making a <code>Putt&#39; after an </code>Approachâ€™ in golf, we name our model PuttNet. We demonstrate that alternating between a speech enhancement model and the proposed Putt model leads to improved speech quality, as measured by perceptual quality scores (PESQ), objective intelligibility (STOI), and background noise intrusiveness (CBAK) scores. Furthermore, we illustrate with graphical analysis why this alternating Approach outperforms repeated application of either model alone. </p>
<blockquote>
<p>ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œè¯­éŸ³å¢å¼ºæ—¨åœ¨ä»å˜ˆæ‚çš„è¯­éŸ³ä¿¡å·ä¸­å»é™¤å™ªå£°ï¼ŒåŒæ—¶ä¿ç•™è¯­éŸ³å†…å®¹ã€‚ç„¶è€Œï¼Œè¯­éŸ³å¢å¼ºç½‘ç»œå¾€å¾€ä¼šåœ¨è¯­éŸ³ä¿¡å·ä¸­å¼•å…¥å¤±çœŸï¼Œç§°ä¸ºä¼ªå½±ï¼Œè¿™å¯èƒ½ä¼šé™ä½éŸ³é¢‘è´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åå¤„ç†ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨å‡è½»è¯­éŸ³å¢å¼ºæ¨¡å‹å¼•å…¥çš„ä¼ªå½±ã€‚å—é«˜å°”å¤«è¿åŠ¨ä¸­â€œæ¨æ†â€ä¸â€œæŒ¥æ†â€ä¹‹é—´çš„ç±»æ¯”å¯å‘ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å‘½åä¸ºPuttNetã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨è¯­éŸ³å¢å¼ºæ¨¡å‹å’Œæå‡ºçš„Puttæ¨¡å‹ä¹‹é—´è¿›è¡Œäº¤æ›¿ï¼Œå¯ä»¥æé«˜è¯­éŸ³è´¨é‡ï¼Œè¿™æ˜¯é€šè¿‡æ„ŸçŸ¥è´¨é‡å¾—åˆ†ï¼ˆPESQï¼‰ã€å®¢è§‚æ¸…æ™°åº¦ï¼ˆSTOIï¼‰å’ŒèƒŒæ™¯å™ªå£°å¹²æ‰°ï¼ˆCBAKï¼‰æ¥è¡¡é‡çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å›¾å½¢åˆ†æè¯´æ˜äº†è¿™ç§äº¤æ›¿æ–¹æ³•ä¸ºä½•ä¼˜äºå•ç‹¬é‡å¤åº”ç”¨ä»»ä¸€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10436v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼ºæŠ€æœ¯æ—¨åœ¨ä»å«å™ªè¯­éŸ³ä¿¡å·ä¸­æ¶ˆé™¤å™ªå£°ï¼ŒåŒæ—¶ä¿ç•™è¯­éŸ³å†…å®¹ã€‚ç„¶è€Œï¼Œè¯­éŸ³å¢å¼ºç½‘ç»œå¸¸å¸¸ç»™è¯­éŸ³ä¿¡å·å¼•å…¥å¤±çœŸï¼Œå³æ‰€è°“çš„ä¼ªå½±ï¼Œè¿™ä¼šé™ä½éŸ³é¢‘è´¨é‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åå¤„ç†ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨å‡è½»è¯­éŸ³å¢å¼ºæ¨¡å‹å¼•å…¥çš„ä¼ªå½±ã€‚æˆ‘ä»¬å€Ÿé‰´é«˜å°”å¤«è¿åŠ¨ä¸­å¼€çƒï¼ˆApproachï¼‰ä¹‹åçš„æ¨çƒï¼ˆPuttï¼‰çš„ç±»æ¯”ï¼Œå°†æ¨¡å‹å‘½åä¸ºPuttNetã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨è¯­éŸ³å¢å¼ºæ¨¡å‹å’Œæå‡ºçš„Puttæ¨¡å‹ä¹‹é—´äº¤æ›¿ä½¿ç”¨ï¼Œèƒ½æé«˜è¯­éŸ³è´¨é‡ï¼Œè¿™åœ¨æ„ŸçŸ¥è´¨é‡å¾—åˆ†ï¼ˆPESQï¼‰ã€å®¢è§‚å¯æ‡‚åº¦ï¼ˆSTOIï¼‰å’ŒèƒŒæ™¯å™ªå£°å¹²æ‰°æ€§ï¼ˆCBAKï¼‰å¾—åˆ†ä¸Šéƒ½æœ‰æ‰€ä½“ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œç”¨äºè¯­éŸ³å¢å¼ºï¼Œç›®çš„æ˜¯ä»å«å™ªè¯­éŸ³ä¸­æ¶ˆé™¤å™ªå£°ï¼ŒåŒæ—¶ä¿ç•™è¯­éŸ³å†…å®¹ã€‚</li>
<li>è¯­éŸ³å¢å¼ºç½‘ç»œå¯èƒ½å¼•å…¥ä¼ªå½±ï¼Œå³å¤±çœŸï¼Œå½±å“éŸ³é¢‘è´¨é‡ã€‚</li>
<li>æå‡ºä¸€ç§åå¤„ç†ç¥ç»ç½‘ç»œæ¨¡å‹PuttNetï¼Œæ—¨åœ¨å‡è½»è¯­éŸ³å¢å¼ºæ¨¡å‹å¼•å…¥çš„ä¼ªå½±ã€‚</li>
<li>PuttNetçš„å‘½åæºäºé«˜å°”å¤«è¿åŠ¨ä¸­çš„å¼€çƒï¼ˆApproachï¼‰å’Œæ¨çƒï¼ˆPuttï¼‰çš„ç±»æ¯”ã€‚</li>
<li>äº¤æ›¿ä½¿ç”¨è¯­éŸ³å¢å¼ºæ¨¡å‹å’ŒPuttNetæ¨¡å‹ï¼Œèƒ½æé«˜è¯­éŸ³è´¨é‡ï¼Œè¿™åœ¨æ„ŸçŸ¥è´¨é‡ã€å®¢è§‚å¯æ‡‚åº¦å’ŒèƒŒæ™¯å™ªå£°å¹²æ‰°æ€§ç­‰æ–¹é¢éƒ½æœ‰ä½“ç°ã€‚</li>
<li>å›¾å½¢åˆ†æè¯æ˜äº†è¿™ç§äº¤æ›¿æ–¹æ³•ä¼˜äºå•ç‹¬é‡å¤åº”ç”¨ä»»ä¸€æ¨¡å‹ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹è¿›ç¥ç»ç½‘ç»œåœ¨è¯­éŸ³å¢å¼ºä¸­çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec1271216c2c02de17c47949ff0cf4ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63ac22f148a0b666c11bf23287e74c74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2edc35c197e87cc4025bd063e1dd02cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97fe92952299b1e2afea8ccff4f225b7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Analysis-on-ASR-System-Combination-for-Attention-CTC-Factored-Hybrid-and-Transducer-Models"><a href="#A-Comparative-Analysis-on-ASR-System-Combination-for-Attention-CTC-Factored-Hybrid-and-Transducer-Models" class="headerlink" title="A Comparative Analysis on ASR System Combination for Attention, CTC,   Factored Hybrid, and Transducer Models"></a>A Comparative Analysis on ASR System Combination for Attention, CTC,   Factored Hybrid, and Transducer Models</h2><p><strong>Authors:Noureldin Bayoumi, Robin Schmitt, Tina Raissi, Albert Zeyer, Ralf SchlÃ¼ter, Hermann Ney</strong></p>
<p>Combination approaches for speech recognition (ASR) systems cover structured sentence-level or word-based merging techniques as well as combination of model scores during beam search. In this work, we compare model combination across popular ASR architectures. Our method leverages the complementary strengths of different models in exploring diverse portions of the search space. We rescore a joint hypothesis list of two model candidates. We then identify the best hypothesis through log-linear combination of these sequence-level scores. While model combination during first-pass recognition may yield improved performance, it introduces variability due to differing decoding methods, making direct comparison more challenging. Our two-pass method ensures consistent comparisons across all system combination results presented in this study. We evaluate model pair candidates with varying architectures and label topologies and units. Experimental results are provided for the Librispeech 960h task. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„ç»„åˆæ–¹æ³•æ¶µç›–äº†ç»“æ„åŒ–çš„å¥å­çº§åˆ«æˆ–åŸºäºå•è¯çš„åˆå¹¶æŠ€æœ¯ï¼Œä»¥åŠåœ¨å…‰æŸæœç´¢è¿‡ç¨‹ä¸­ç»“åˆæ¨¡å‹åˆ†æ•°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†æµè¡ŒASRæ¶æ„ä¸­çš„æ¨¡å‹ç»„åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸åŒæ¨¡å‹åœ¨æ¢ç´¢æœç´¢ç©ºé—´ä¸åŒéƒ¨åˆ†çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬å¯¹ä¸¤ä¸ªæ¨¡å‹å€™é€‰è€…çš„è”åˆå‡è®¾åˆ—è¡¨è¿›è¡Œé‡æ–°è¯„åˆ†ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è¿™äº›åºåˆ—çº§åˆ†æ•°çš„å¯¹æ•°çº¿æ€§ç»„åˆæ¥ç¡®å®šæœ€ä½³å‡è®¾ã€‚è™½ç„¶åœ¨ä¸€éè¯†åˆ«è¿‡ç¨‹ä¸­ä½¿ç”¨æ¨¡å‹ç»„åˆå¯èƒ½ä¼šæé«˜æ€§èƒ½ï¼Œä½†å®ƒç”±äºä¸åŒçš„è§£ç æ–¹æ³•è€Œå¼•å…¥äº†å˜åŒ–æ€§ï¼Œä½¿å¾—ç›´æ¥æ¯”è¾ƒæ›´å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„ä¸¤éæ–¹æ³•ç¡®ä¿äº†æœ¬ç ”ç©¶ä¸­æ‰€æœ‰ç³»ç»Ÿç»„åˆç»“æœçš„ä¸€è‡´æ¯”è¾ƒã€‚æˆ‘ä»¬ç”¨ä¸åŒæ¶æ„ã€æ ‡ç­¾æ‹“æ‰‘å’Œå•ä½çš„æ¨¡å‹å¯¹å€™é€‰å¯¹è±¡è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœä¸ºLibrispeech 960å°æ—¶ä»»åŠ¡æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09880v1">PDF</a> Accepted for presentation at IEEE Speech Communication; 16th ITG   Conference</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„ç»„åˆæ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒASRæ¶æ„ä¸­çš„æ¨¡å‹ç»„åˆï¼Œåˆ©ç”¨ä¸åŒæ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿æ¥æ¢ç´¢æœç´¢ç©ºé—´çš„ä¸åŒéƒ¨åˆ†ã€‚é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆç»“åˆä¸¤ä¸ªæ¨¡å‹å€™é€‰äººçš„å‡è®¾åˆ—è¡¨è¿›è¡Œé‡è¯„åˆ†ï¼Œç„¶åé€šè¿‡åºåˆ—çº§åˆ†æ•°çš„å¯¹æ•°çº¿æ€§ç»„åˆæ¥ç¡®å®šæœ€ä½³å‡è®¾ã€‚è™½ç„¶åœ¨ç¬¬ä¸€éè¯†åˆ«è¿‡ç¨‹ä¸­ä½¿ç”¨æ¨¡å‹ç»„åˆå¯èƒ½ä¼šæé«˜æ€§èƒ½ï¼Œä½†ç”±äºä¸åŒçš„è§£ç æ–¹æ³•ä¼šå¼•å…¥å˜åŒ–ï¼Œä½¿å¾—ç›´æ¥æ¯”è¾ƒæ›´å…·æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶é‡‡ç”¨çš„ä¸¤é˜¶æ®µæ–¹æ³•ç¡®ä¿äº†æ‰€æœ‰ç³»ç»Ÿç»„åˆç»“æœçš„ä¸€è‡´æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ç ”ç©¶äº†è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„ç»„åˆæ–¹æ³•ï¼Œæ¶‰åŠæ¨¡å‹ç»„åˆçš„å¤šç§æ–¹å¼ã€‚</li>
<li>é€šè¿‡ç»“åˆä¸åŒæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿæ¢ç´¢æœç´¢ç©ºé—´çš„ä¸åŒéƒ¨åˆ†ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•è¿›è¡Œæ¨¡å‹ç»„åˆï¼Œé¦–å…ˆé‡è¯„åˆ†å‡è®¾åˆ—è¡¨ï¼Œç„¶åç¡®å®šæœ€ä½³å‡è®¾ã€‚</li>
<li>æ¨¡å‹ç»„åˆåœ¨ç¬¬ä¸€éè¯†åˆ«ä¸­å¯èƒ½æé«˜æ€§èƒ½ï¼Œä½†ç›´æ¥æ¯”è¾ƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨çš„ä¸¤é˜¶æ®µæ–¹æ³•ç¡®ä¿äº†æ‰€æœ‰ç³»ç»Ÿç»„åˆç»“æœçš„ä¸€è‡´æ¯”è¾ƒã€‚</li>
<li>æ–‡ç« å¯¹å…·æœ‰ä¸åŒæ¶æ„å’Œæ ‡ç­¾æ‹“æ‰‘åŠå•å…ƒçš„æ¨¡å‹å€™é€‰è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-141e87a6b85c4572cff067a84d95fc2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c4e7ccb57a611c1897470c4d3d50ae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f51302330ec42073a9ea838371322d08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f48d53e588dfd0106d3253b858ec6022.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Analysis-of-Domain-Shift-across-ASR-Architectures-via-TTS-Enabled-Separation-of-Target-Domain-and-Acoustic-Conditions"><a href="#Analysis-of-Domain-Shift-across-ASR-Architectures-via-TTS-Enabled-Separation-of-Target-Domain-and-Acoustic-Conditions" class="headerlink" title="Analysis of Domain Shift across ASR Architectures via TTS-Enabled   Separation of Target Domain and Acoustic Conditions"></a>Analysis of Domain Shift across ASR Architectures via TTS-Enabled   Separation of Target Domain and Acoustic Conditions</h2><p><strong>Authors:Tina Raissi, Nick Rossenbach, Ralf SchlÃ¼ter</strong></p>
<p>We analyze automatic speech recognition (ASR) modeling choices under domain mismatch, comparing classic modular and novel sequence-to-sequence (seq2seq) architectures. Across the different ASR architectures, we examine a spectrum of modeling choices, including label units, context length, and topology. To isolate language domain effects from acoustic variation, we synthesize target domain audio using a text-to-speech system trained on LibriSpeech. We incorporate target domain n-gram and neural language models for domain adaptation without retraining the acoustic model. To our knowledge, this is the first controlled comparison of optimized ASR systems across state-of-the-art architectures under domain shift, offering insights into their generalization. The results show that, under domain shift, rather than the decoder architecture choice or the distinction between classic modular and novel seq2seq models, it is specific modeling choices that influence performance. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ†æäº†é¢†åŸŸä¸åŒ¹é…æƒ…å†µä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å»ºæ¨¡é€‰æ‹©ï¼Œå¯¹æ¯”äº†ç»å…¸çš„æ¨¡å—åŒ–æ¶æ„å’Œæ–°é¢–çš„é¡ºåºåˆ°åºåˆ—ï¼ˆseq2seqï¼‰æ¶æ„ã€‚åœ¨ä¸åŒçš„ASRæ¶æ„ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŒ…æ‹¬æ ‡ç­¾å•ä½ã€ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ‹“æ‰‘ç»“æ„ç­‰ä¸€ç³»åˆ—å»ºæ¨¡é€‰æ‹©ã€‚ä¸ºäº†å°†è¯­è¨€é¢†åŸŸçš„å½±å“ä¸å£°éŸ³å˜åŒ–åŒºåˆ†å¼€æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨åœ¨LibriSpeechä¸Šè®­ç»ƒçš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿåˆæˆç›®æ ‡é¢†åŸŸçš„éŸ³é¢‘ã€‚æˆ‘ä»¬å°†ç›®æ ‡é¢†åŸŸä¸­çš„nå…ƒç»„å’Œç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹èå…¥åˆ°æ— éœ€é‡æ–°è®­ç»ƒå£°å­¦æ¨¡å‹çš„é¢†åŸŸä¸­ï¼Œå®ç°è‡ªé€‚åº”é€‚é…ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡åœ¨ä¸åŒé¢†å…ˆæ¶æ„çš„ASRç³»ç»Ÿä¸­è¿›è¡Œå—æ§æ¯”è¾ƒï¼Œä»¥äº†è§£å…¶åœ¨é¢†åŸŸè½¬ç§»ä¸­çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é¢†åŸŸè½¬ç§»çš„æƒ…å†µä¸‹ï¼Œå½±å“æ€§èƒ½çš„ä¸æ˜¯è§£ç å™¨æ¶æ„çš„é€‰æ‹©æˆ–ç»å…¸æ¨¡å—åŒ–ä¸æ–°é¢–seq2seqæ¨¡å‹ä¹‹é—´çš„åŒºåˆ«ï¼Œè€Œæ˜¯ç‰¹å®šçš„å»ºæ¨¡é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09868v1">PDF</a> Accepted for presentation at IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ†æäº†é¢†åŸŸä¸åŒ¹é…æƒ…å†µä¸‹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„å»ºæ¨¡é€‰æ‹©ï¼Œå¯¹æ¯”äº†ç»å…¸æ¨¡å—åŒ–å’Œæ–°å‹åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰æ¶æ„ã€‚æ–‡ç« é€šè¿‡åˆæˆç›®æ ‡åŸŸéŸ³é¢‘ï¼Œæ¢è®¨äº†æ ‡ç­¾å•å…ƒã€ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ‹“æ‰‘ç»“æ„ç­‰ä¸€ç³»åˆ—å»ºæ¨¡é€‰æ‹©çš„å½±å“ã€‚ä¸ºæé«˜é¢†åŸŸé€‚åº”æ€§ï¼Œæ–‡ç« å¼•å…¥äº†ç›®æ ‡åŸŸn-gramå’Œç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒå£°å­¦æ¨¡å‹ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹ä¼˜åŒ–åçš„ASRç³»ç»Ÿåœ¨é¢†åŸŸè½¬ç§»æƒ…å†µä¸‹çš„å…ˆè¿›æ¶æ„è¿›è¡Œäº†æ§åˆ¶æ¯”è¾ƒï¼Œä¸ºå…¶æ³›åŒ–èƒ½åŠ›æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å¯¹æ¯”äº†ç»å…¸æ¨¡å—åŒ–å’Œæ–°å‹åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰æ¶æ„åœ¨é¢†åŸŸä¸åŒ¹é…æƒ…å†µä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å»ºæ¨¡é€‰æ‹©ã€‚</li>
<li>é€šè¿‡åˆæˆç›®æ ‡åŸŸéŸ³é¢‘ï¼Œæ¢è®¨äº†è¯­è¨€é¢†åŸŸæ•ˆæœä¸å£°å­¦å˜åŒ–çš„å½±å“ã€‚</li>
<li>å¼•å…¥ç›®æ ‡åŸŸn-gramå’Œç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼Œæé«˜é¢†åŸŸé€‚åº”æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå£°å­¦æ¨¡å‹ã€‚</li>
<li>æ–‡ç« é‡ç‚¹å¼ºè°ƒäº†å»ºæ¨¡é€‰æ‹©ï¼Œå¦‚æ ‡ç­¾å•å…ƒã€ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ‹“æ‰‘ç»“æ„ï¼Œå¯¹ASRæ€§èƒ½çš„å½±å“ã€‚</li>
<li>åœ¨é¢†åŸŸè½¬ç§»æƒ…å†µä¸‹ï¼Œè§£ç å™¨æ¶æ„çš„é€‰æ‹©ä»¥åŠç»å…¸æ¨¡å—åŒ–å’Œseq2seqæ¨¡å‹ä¹‹é—´çš„åŒºåˆ«å¹¶ä¸æ˜¯å½±å“æ€§èƒ½çš„ä¸»è¦å› ç´ ã€‚</li>
<li>æœ¬æ–‡æ˜¯é¦–æ¬¡å¯¹ä¼˜åŒ–åçš„ASRç³»ç»Ÿåœ¨é¢†åŸŸè½¬ç§»æƒ…å†µä¸‹çš„å…ˆè¿›æ¶æ„è¿›è¡Œæ§åˆ¶æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92b180df22d75c43a3fad1a1951ed243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf10b1d550413bd2f6076009580e698.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c4501bf22a1b756c38b3ca8708ac70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58352439b96a9846fd2054a8df47b6a2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Assessing-the-Feasibility-of-Lightweight-Whisper-Models-for-Low-Resource-Urdu-Transcription"><a href="#Assessing-the-Feasibility-of-Lightweight-Whisper-Models-for-Low-Resource-Urdu-Transcription" class="headerlink" title="Assessing the Feasibility of Lightweight Whisper Models for Low-Resource   Urdu Transcription"></a>Assessing the Feasibility of Lightweight Whisper Models for Low-Resource   Urdu Transcription</h2><p><strong>Authors:Abdul Rehman Antall, Naveed Akhtar</strong></p>
<p>This study evaluates the feasibility of lightweight Whisper models (Tiny, Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu being the 10th most spoken language globally with over 230 million speakers, its representation in automatic speech recognition (ASR) systems remains limited due to dialectal diversity, code-switching, and sparse training data. We benchmark these models on a curated Urdu dataset using word error rate (WER), without fine-tuning. Results show Whisper-Small achieves the lowest error rates (33.68% WER), outperforming Tiny (67.08% WER) and Base (53.67% WER). Qualitative analysis reveals persistent challenges in phonetic accuracy and lexical coherence, particularly for complex utterances. While Whisper-Small demonstrates promise for deployable Urdu ASR, significant gaps remain. Our findings emphasize lay the groundwork for future research into effective, low-resource ASR systems. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°è½»é‡åŒ–Whisperæ¨¡å‹ï¼ˆTinyã€Baseã€Smallï¼‰åœ¨ä½èµ„æºç¯å¢ƒä¸‹è¿›è¡Œä¹Œå°”éƒ½è¯­è¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å¯è¡Œæ€§ã€‚å°½ç®¡ä¹Œå°”éƒ½è¯­æ˜¯å…¨çƒç¬¬åå¤§è¯­è¨€ï¼Œæ‹¥æœ‰è¶…è¿‡2.3äº¿ä½¿ç”¨è€…ï¼Œä½†ç”±äºæ–¹è¨€å¤šæ ·æ€§ã€ä»£ç åˆ‡æ¢å’Œç¨€ç–çš„è®­ç»ƒæ•°æ®ï¼Œå…¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„ä»£è¡¨æ€§ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬åœ¨ç²¾é€‰çš„ä¹Œå°”éƒ½è¯­æ•°æ®é›†ä¸Šï¼Œæœªç»å¾®è°ƒï¼Œä½¿ç”¨å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å¯¹è¿™äº›æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒWhisper-Smallå–å¾—äº†æœ€ä½çš„è¯¯å·®ç‡ï¼ˆ33.68% WERï¼‰ï¼Œä¼˜äºTinyï¼ˆ67.08% WERï¼‰å’ŒBaseï¼ˆ53.67% WERï¼‰ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œåœ¨è¯­éŸ³å‡†ç¡®æ€§å’Œè¯æ±‡è¿è´¯æ€§æ–¹é¢ä»ç„¶å­˜åœ¨æŒä¹…çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„è¯è¯­ä¸­ã€‚è™½ç„¶Whisper-Smallåœ¨å¯éƒ¨ç½²çš„ä¹Œå°”éƒ½è¯­ASRæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»å­˜åœ¨é‡å¤§å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†æœªæ¥ç ”ç©¶æœ‰æ•ˆä½èµ„æºASRç³»ç»Ÿçš„åŸºç¡€å·¥ä½œçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09865v1">PDF</a> 8 pages, 3 figures, 1 table, including references and appendix</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†è½»é‡çº§Whisperæ¨¡å‹ï¼ˆTinyã€Baseã€Smallï¼‰åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ç”¨äºä¹Œå°”éƒ½è¯­è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„å¯è¡Œæ€§ã€‚è™½ç„¶ä¹Œå°”éƒ½è¯­æ˜¯å…¨çƒç¬¬åå¤§è¯­è¨€å¹¶æ‹¥æœ‰è¶…è¿‡ä¸¤äº¿ä¸‰åƒä¸‡çš„æ¯è¯­è€…ï¼Œä½†ç”±äºæ–¹è¨€å¤šæ ·æ€§ã€ä»£ç åˆ‡æ¢ä»¥åŠè®­ç»ƒæ•°æ®ç¨€ç¼ºç­‰é—®é¢˜ï¼Œå…¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„ä»£è¡¨æ€§ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬åœ¨ç²¾é€‰çš„ä¹Œå°”éƒ½è¯­æ•°æ®é›†ä¸Šå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œä¸”æœªå¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚ç»“æœè¡¨æ˜ï¼ŒWhisper-Smallå–å¾—äº†æœ€ä½çš„é”™è¯¯ç‡ï¼ˆ33.68% WERï¼‰ï¼Œä¼˜äºTinyï¼ˆ67.08% WERï¼‰å’ŒBaseï¼ˆ53.67% WERï¼‰ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œè¯­éŸ³å‡†ç¡®åº¦å’Œè¯æ±‡è¿è´¯æ€§æ–¹é¢ä»å­˜åœ¨æŒç»­çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤æ‚çš„é™ˆè¿°ã€‚è™½ç„¶Whisper-Smallå¯¹äºå¯éƒ¨ç½²çš„ä¹Œå°”éƒ½è¯­ASRè¡¨ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ¢ç´¢é«˜æ•ˆã€èµ„æºæœ‰é™çš„ASRç³»ç»Ÿæä¾›äº†é‡è¦çš„ç ”ç©¶åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶è¯„ä¼°äº†è½»é‡çº§Whisperæ¨¡å‹åœ¨èµ„æºæœ‰é™çš„åœºæ™¯ä¸‹å¯¹ä¹Œå°”éƒ½è¯­è¯­éŸ³è¯†åˆ«çš„é€‚ç”¨æ€§ã€‚</li>
<li>å°½ç®¡ä¹Œå°”éƒ½è¯­æœ‰å¤§é‡çš„æ¯è¯­è€…ï¼Œå…¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­çš„ä»£è¡¨æ€§ä»ç„¶å—é™ã€‚</li>
<li>ä¸»è¦çš„æŒ‘æˆ˜æ¥æºäºæ–¹è¨€å¤šæ ·æ€§ã€ä»£ç åˆ‡æ¢å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>Whisper-Smallæ¨¡å‹å–å¾—äº†æœ€ä½çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹è¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
<li>å®šæ€§åˆ†ææ­ç¤ºäº†è¯­éŸ³å‡†ç¡®æ€§å’Œè¯æ±‡è¿è´¯æ€§æ–¹é¢çš„æŒç»­æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚è¯­å¥æ—¶ã€‚</li>
<li>è™½ç„¶Whisper-Smallæ¨¡å‹åœ¨ä¹Œå°”éƒ½è¯­ASRæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa0ab31a0c249a24932fe1924f0b31bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7e1433570471963b0850e7154b5223c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f42cb123b16b5f95ffafe5d374d57dd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b807f50cbb3d8be8aac20e81e0eb9ca4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e4eafcebfd7ee83ef7174f35f53ac4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d7dee212c8a158ae33a18878912b839.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue"><a href="#OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue" class="headerlink" title="OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue"></a>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue</h2><p><strong>Authors:Xuelong Geng, Qijie Shao, Hongfei Xue, Shuiyuan Wang, Hanke Xie, Zhao Guo, Yi Zhao, Guojian Li, Wenjie Tian, Chengyou Wang, Zhixian Zhao, Kangxiang Xia, Ziyu Zhang, Zhennan Lin, Tianlun Zuo, Mingchen Shao, Yuang Cao, Guobin Ma, Longhao Li, Yuhang Dai, Dehui Gao, Dake Guo, Lei Xie</strong></p>
<p>Empathy is crucial in enabling natural interactions within spoken dialogue systems, allowing machines to recognize and respond appropriately to paralinguistic cues such as age, gender, and emotion. Recent advancements in end-to-end speech language models, which unify speech understanding and generation, provide promising solutions. However, several challenges persist, including an over-reliance on large-scale dialogue datasets, insufficient extraction of paralinguistic cues vital for conveying empathy, and the lack of empathy-specific datasets and evaluation frameworks. To address these issues, we introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system designed to enhance empathetic interactions, particularly in resource-limited settings. OSUM-EChat introduces two key innovations: (1) a three-stage understanding-driven spoken dialogue training strategy that extends the capabilities of a large speech understanding model to spoken dialogue tasks, and (2) a linguistic-paralinguistic dual thinking mechanism that integrates paralinguistic understanding through a chain of thought with dialogue generation, enabling the system to produce more empathetic responses. This approach reduces reliance on large-scale dialogue datasets while maintaining high-quality empathetic interactions. Additionally, we introduce the EChat-200K dataset, a rich corpus of empathetic speech-to-speech dialogues, and the EChat-eval benchmark, a comprehensive framework for evaluating the empathetic capabilities of dialogue systems. Experimental results demonstrate that OSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic responsiveness, validating its effectiveness. </p>
<blockquote>
<p>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å®ç°è‡ªç„¶äº¤äº’è‡³å…³é‡è¦ï¼Œå®ƒä½¿æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œé€‚å½“å›åº”è¯¸å¦‚å¹´é¾„ã€æ€§åˆ«å’Œæƒ…ç»ªç­‰å‰¯è¯­è¨€çº¿ç´¢ã€‚ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œè¿™äº›æ¨¡å‹ç»Ÿä¸€äº†è¯­éŸ³ç†è§£å’Œç”Ÿæˆï¼Œæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡åº¦ä¾èµ–å¤§è§„æ¨¡çš„å¯¹è¯æ•°æ®é›†ã€å¯¹ä¼ è¾¾å…±æƒ…è‡³å…³é‡è¦çš„å‰¯è¯­è¨€çº¿ç´¢æå–ä¸è¶³ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OSUM-EChatï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå…±æƒ…äº’åŠ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚OSUM-EChatå¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªä¸‰é˜¶æ®µçš„ç†è§£é©±åŠ¨å£è¯­å¯¹è¯è®­ç»ƒç­–ç•¥ï¼Œå®ƒæ‰©å±•äº†å¤§å‹è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨å£è¯­å¯¹è¯ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ä¸€ç§è¯­è¨€å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ï¼Œå®ƒå°†å‰¯è¯­è¨€ç†è§£ä¸æ€ç»´é“¾ç›¸ç»“åˆï¼Œæ¨åŠ¨å¯¹è¯ç”Ÿæˆï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…·å…±æƒ…çš„å›åº”ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„å…±æƒ…äº’åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†EChat-200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä»½ä¸°å¯Œçš„å…±æƒ…è¯­éŸ³å¯¹è¯è¯­æ–™åº“ï¼Œä»¥åŠEChat-evalåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°å¯¹è¯ç³»ç»Ÿå…±æƒ…èƒ½åŠ›çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºç«¯åˆ°ç«¯çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09600v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å®ç°è‡ªç„¶äº¤äº’æ—¶ï¼Œå…±æƒ…çš„é‡è¦æ€§ã€‚æ–‡ç« ä»‹ç»äº†å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­çš„ä½œç”¨ï¼ŒåŒ…æ‹¬è¯†åˆ«å’Œç†è§£éè¯­è¨€çº¿ç´¢å¦‚å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿçš„èƒ½åŠ›ã€‚åŒæ—¶æåˆ°äº†ç«¯åˆ°ç«¯çš„è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•å’Œé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†OSUM-EChatç³»ç»Ÿï¼Œå®ƒé€šè¿‡å¼•å…¥ç†è§£é©±åŠ¨çš„ä¸‰é˜¶æ®µå£è¯­å¯¹è¯è®­ç»ƒç­–ç•¥å’Œç»“åˆè¯­è¨€ä¸éè¯­è¨€è®¤çŸ¥çš„åŒé‡æ€è€ƒæœºåˆ¶æ¥å¢å¼ºå…±æ„Ÿæƒ…å¢ƒä¸‹çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†ä¸ºè¯„ä¼°å…±æƒ…èƒ½åŠ›å¯¹è¯ç³»ç»Ÿæ€§èƒ½è€Œå¼•å…¥çš„EChat-200Kæ•°æ®é›†å’ŒEChat-evalåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„å£è¯­å¯¹è¯æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬è®ºæ–‡çš„ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­æ˜¯å®ç°è‡ªç„¶äº¤äº’çš„å…³é”®è¦ç´ ã€‚</li>
<li>æœºå™¨éœ€è¦ç†è§£å’Œå“åº”éè¯­è¨€çº¿ç´¢å¦‚å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿã€‚</li>
<li>ç«¯åˆ°ç«¯çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ä¸ºå£è¯­å¯¹è¯æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>OSUM-EChatç³»ç»Ÿé€šè¿‡è®­ç»ƒç­–ç•¥å’Œåˆ›æ–°æœºåˆ¶å¢å¼ºäº†å…±æ„Ÿæƒ…å¢ƒä¸‹çš„äº¤äº’ã€‚</li>
<li>OSUM-EChatå¼•å…¥çš„ç†è§£é©±åŠ¨çš„ä¸‰é˜¶æ®µå£è¯­å¯¹è¯è®­ç»ƒç­–ç•¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯­è¨€ä¸éè¯­è¨€è®¤çŸ¥çš„åŒé‡æ€è€ƒæœºåˆ¶ä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…·å…±æƒ…çš„å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ab6fbb1b7f7e9430ad35a30b166012a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535849198529c9104a7671dc4a258925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e14f2381ae5cfac036c4c4192c0a391e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fd8ca61e02317a23b841a8f6fef2d4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1532fe434229d1e26038d8d42cfc6836.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28c3b03132ebd4dd671c6c43f74be212.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Objective-Soups-Multilingual-Multi-Task-Modeling-for-Speech-Processing"><a href="#Objective-Soups-Multilingual-Multi-Task-Modeling-for-Speech-Processing" class="headerlink" title="Objective Soups: Multilingual Multi-Task Modeling for Speech Processing"></a>Objective Soups: Multilingual Multi-Task Modeling for Speech Processing</h2><p><strong>Authors:A F M Saif, Lisha Chen, Xiaodong Cui, Songtao Lu, Brian Kingsbury, Tianyi Chen</strong></p>
<p>Training a single model for multilingual, multi-task speech processing (MSP) is severely hampered by conflicting objectives between tasks like speech recognition and translation. While multi-objective optimization (MOO) aims to align gradient updates, its effectiveness diminishes as the number of tasks grows, making it difficult to find a common descent direction. This raises a fundamental question: should highly conflicting objectives be optimized jointly or separated into a hierarchical structure? To address this question, this paper investigates three multi-objective MSP formulations, which we refer to as \textbf{objective soup recipes}. These formulations apply multi-objective optimization at different optimization levels to mitigate potential conflicts among all objectives. To ensure efficiency, we introduce a lightweight layer-selection mechanism that computes the conflict-avoiding gradient using only the most problematic layers, minimizing computational and memory overhead. Extensive experiments on CoVoST v2, LibriSpeech, and AISHELL-1 reveal that a bi-level recipe separating recognition and translation tasks consistently outperforms standard flat optimization. Our work demonstrates that hierarchical MOO is a more effective and scalable approach for building state-of-the-art MSP models. Our code has been released at <a target="_blank" rel="noopener" href="https://github.com/afmsaif/Objective_Soups">https://github.com/afmsaif/Objective_Soups</a>. </p>
<blockquote>
<p>å¯¹å¤šç§è¯­è¨€å¤šä»»åŠ¡è¯­éŸ³å¤„ç†ï¼ˆMSPï¼‰è¿›è¡Œå•ä¸€æ¨¡å‹è®­ç»ƒï¼Œå› ä»»åŠ¡å¦‚è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ä¹‹é—´å­˜åœ¨ç›®æ ‡å†²çªè€Œå—åˆ°ä¸¥é‡é˜»ç¢ã€‚è™½ç„¶å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMOOï¼‰æ—¨åœ¨å¯¹é½æ¢¯åº¦æ›´æ–°ï¼Œä½†éšç€ä»»åŠ¡æ•°é‡çš„å¢é•¿ï¼Œå…¶æœ‰æ•ˆæ€§é™ä½ï¼Œéš¾ä»¥æ‰¾åˆ°å…±åŒçš„ä¸‹é™æ–¹å‘ã€‚è¿™å°±æå‡ºäº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šé«˜åº¦å†²çªçš„ç›®æ ‡åº”è¯¥è”åˆä¼˜åŒ–è¿˜æ˜¯åˆ†å±‚ä¼˜åŒ–ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†ä¸‰ç§å¤šç›®æ ‡MSPå…¬å¼ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œç›®æ ‡æ±¤é…æ–¹â€ã€‚è¿™äº›å…¬å¼åœ¨ä¸åŒçš„ä¼˜åŒ–å±‚æ¬¡ä¸Šåº”ç”¨å¤šç›®æ ‡ä¼˜åŒ–ï¼Œä»¥å‡è½»æ‰€æœ‰ç›®æ ‡ä¹‹é—´çš„æ½œåœ¨å†²çªã€‚ä¸ºäº†ä¿è¯æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„å±‚é€‰æ‹©æœºåˆ¶ï¼Œä»…ä½¿ç”¨æœ€å›°éš¾çš„å±‚æ¥è®¡ç®—é¿å…å†²çªçš„æ¢¯åº¦ï¼Œä»¥æœ€å°åŒ–è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚åœ¨CoVoST v2ã€LibriSpeechå’ŒAISHELL-1ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå°†è¯†åˆ«å’Œç¿»è¯‘ä»»åŠ¡åˆ†ç¦»çš„äºŒçº§é…æ–¹å§‹ç»ˆä¼˜äºæ ‡å‡†å¹³é¢ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œåˆ†å±‚MOOæ˜¯ä¸€ç§æ›´æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œé€‚ç”¨äºæ„å»ºæœ€å…ˆè¿›çš„MSPæ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/afmsaif/Objective_Soups%E4%B8%8A%E3%80%82">https://github.com/afmsaif/Objective_Soupsä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09228v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å•ä¸€æ¨¡å‹åœ¨å¤šè¯­è¨€å¤šä»»åŠ¡è¯­éŸ³å¤„ç†ï¼ˆMSPï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä»»åŠ¡ç›®æ ‡ä¹‹é—´çš„å†²çªé—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸‰ç§å¤šç›®æ ‡è¯­éŸ³å¤„ç†æ–¹æ¡ˆï¼Œè¢«ç§°ä¸ºâ€œç›®æ ‡æ±¤é…æ–¹â€ï¼Œåœ¨ä¸åŒä¼˜åŒ–å±‚æ¬¡ä¸Šåº”ç”¨å¤šç›®æ ‡ä¼˜åŒ–æ¥ç¼“è§£å„ç›®æ ‡ä¹‹é—´çš„æ½œåœ¨å†²çªã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„å±‚é€‰æ‹©æœºåˆ¶ï¼Œä»…ä½¿ç”¨æœ€å…·æœ‰é—®é¢˜çš„å±‚æ¥è®¡ç®—é¿å…å†²çªçš„æ¢¯åº¦ï¼Œä»¥æé«˜æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨åŒå±‚é…æ–¹åˆ†ç¦»è¯†åˆ«å’Œç¿»è¯‘ä»»åŠ¡çš„æ–¹æ³•ä¼˜äºæ ‡å‡†çš„å¹³é¢ä¼˜åŒ–æ–¹æ³•ã€‚æ–‡ç« è¯æ˜åˆ†å±‚å¤šç›®æ ‡ä¼˜åŒ–æ˜¯æ„å»ºå…ˆè¿›MSPæ¨¡å‹æ›´æœ‰æ•ˆã€æ›´å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€å¤šä»»åŠ¡è¯­éŸ³å¤„ç†ï¼ˆMSPï¼‰åœ¨å•ä¸€æ¨¡å‹ä¸­é¢ä¸´ä»»åŠ¡ç›®æ ‡å†²çªçš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMOOï¼‰æ—¨åœ¨å¯¹é½æ¢¯åº¦æ›´æ–°ï¼Œä½†éšç€ä»»åŠ¡æ•°é‡çš„å¢åŠ ï¼Œå…¶æœ‰æ•ˆæ€§ä¼šé™ä½ï¼Œéš¾ä»¥æ‰¾åˆ°å…±åŒçš„ä¸‹é™æ–¹å‘ã€‚</li>
<li>æå‡ºäº†ä¸‰ç§â€œç›®æ ‡æ±¤é…æ–¹â€çš„å¤šç›®æ ‡MSPæ–¹æ¡ˆï¼Œä»¥ç¼“è§£ä¸åŒä¼˜åŒ–ç›®æ ‡ä¹‹é—´çš„æ½œåœ¨å†²çªã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„å±‚é€‰æ‹©æœºåˆ¶ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œé¿å…å†…å­˜å¼€é”€ã€‚</li>
<li>åŒå±‚é…æ–¹åˆ†ç¦»è¯†åˆ«å’Œç¿»è¯‘ä»»åŠ¡çš„æ–¹æ³•åœ¨å®éªŒä¸Šè¡¨ç°æœ€å¥½ï¼Œä¼˜äºæ ‡å‡†çš„å¹³é¢ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>åˆ†å±‚å¤šç›®æ ‡ä¼˜åŒ–æ˜¯æ„å»ºå…ˆè¿›MSPæ¨¡å‹æ›´æœ‰æ•ˆã€æ›´å¯æ‰©å±•çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-60734f27a0e5678f64cd2757269dcd40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e26e1a8bf3f45a44357dcfdeb0c9e84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fabdbcee9e0a637a52589d9950c0adbc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens"><a href="#Fairness-of-Automatic-Speech-Recognition-Looking-Through-a-Philosophical-Lens" class="headerlink" title="Fairness of Automatic Speech Recognition: Looking Through a   Philosophical Lens"></a>Fairness of Automatic Speech Recognition: Looking Through a   Philosophical Lens</h2><p><strong>Authors:Anna Seo Gyeong Choi, Hoon Choi</strong></p>
<p>Automatic Speech Recognition (ASR) systems now mediate countless human-technology interactions, yet research on their fairness implications remains surprisingly limited. This paper examines ASR bias through a philosophical lens, arguing that systematic misrecognition of certain speech varieties constitutes more than a technical limitation â€“ it represents a form of disrespect that compounds historical injustices against marginalized linguistic communities. We distinguish between morally neutral classification (discriminate1) and harmful discrimination (discriminate2), demonstrating how ASR systems can inadvertently transform the former into the latter when they consistently misrecognize non-standard dialects. We identify three unique ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns: the temporal burden placed on speakers of non-standard varieties (â€œtemporal taxationâ€), the disruption of conversational flow when systems misrecognize speech, and the fundamental connection between speech patterns and personal&#x2F;cultural identity. These factors create asymmetric power relationships that existing technical fairness metrics fail to capture. The paper analyzes the tension between linguistic standardization and pluralism in ASR development, arguing that current approaches often embed and reinforce problematic language ideologies. We conclude that addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression worthy of technological accommodation. This philosophical reframing offers new pathways for developing ASR systems that respect linguistic diversity and speaker autonomy. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¦‚ä»Šä¸­ä»‹äº†æ— æ•°çš„äººæœºäº¤äº’ï¼Œç„¶è€Œå…³äºå®ƒä»¬å¯¹å…¬å¹³æ€§çš„å½±å“çš„ç ”ç©¶ä»ç„¶å‡ºäººæ„æ–™åœ°æœ‰é™ã€‚æœ¬æ–‡é€šè¿‡å“²å­¦è§†è§’ç ”ç©¶ASRåè§ï¼Œè®¤ä¸ºå¯¹æŸäº›è¯­éŸ³ç‰¹æ€§çš„ç³»ç»Ÿæ€§è¯¯è¯†åˆ«ä¸ä»…ä»…æ˜¯æŠ€æœ¯å±€é™â€”â€”å®ƒä»£è¡¨äº†å¯¹è¾¹ç¼˜åŒ–è¯­è¨€ç¤¾ç¾¤å†å²ä¸å…¬çš„ä¸€ç§ä¸å°Šé‡ã€‚æˆ‘ä»¬åŒºåˆ†äº†é“å¾·ä¸­ç«‹çš„åˆ†ç±»ï¼ˆè¾¨åˆ«1ï¼‰å’Œæœ‰å®³çš„æ­§è§†ï¼ˆè¾¨åˆ«2ï¼‰ï¼Œå±•ç¤ºäº†å½“ASRç³»ç»ŸæŒç»­è¯¯è¯†åˆ«éæ ‡å‡†æ–¹è¨€æ—¶ï¼Œå¦‚ä½•å°†å‰è€…æ— æ„ä¸­è½¬å˜ä¸ºåè€…ã€‚æˆ‘ä»¬ç¡®å®šäº†è¯­éŸ³æŠ€æœ¯çš„ä¸‰ä¸ªç‹¬ç‰¹çš„é“å¾·ç»´åº¦ï¼Œè¿™äº›ç»´åº¦å°†ASRåè§ä¸å…¶ä»–ç®—æ³•å…¬å¹³æ€§é—®é¢˜åŒºåˆ†å¼€æ¥ï¼šå¯¹éæ ‡å‡†æ–¹è¨€å‘è¨€äººçš„æ—¶é—´è´Ÿæ‹…ï¼ˆâ€œæ—¶é—´ç¨è´Ÿâ€ï¼‰ã€ç³»ç»Ÿè¯¯è¯†åˆ«è¯­éŸ³æ—¶å¯¹ä¼šè¯æµç¨‹çš„ç ´åï¼Œä»¥åŠè¯­éŸ³æ¨¡å¼ä¸ä¸ªäºº&#x2F;æ–‡åŒ–èº«ä»½ä¹‹é—´çš„æ ¹æœ¬è”ç³»ã€‚è¿™äº›å› ç´ é€ æˆäº†ä¸å¯¹ç§°çš„æƒåŠ›å…³ç³»ï¼Œè€Œç°æœ‰çš„æŠ€æœ¯å…¬å¹³æŒ‡æ ‡å´æ— æ³•æ•æ‰åˆ°è¿™ä¸€ç‚¹ã€‚æœ¬æ–‡åˆ†æäº†ASRå‘å±•ä¸­è¯­è¨€æ ‡å‡†åŒ–å’Œå¤šå…ƒåŒ–çš„ç´§å¼ å…³ç³»ï¼Œè®¤ä¸ºå½“å‰çš„æ–¹æ³•å¾€å¾€åµŒå…¥å¹¶å¼ºåŒ–äº†æœ‰é—®é¢˜çš„è¯­è¨€ç†å¿µã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œè§£å†³ASRåè§ä¸ä»…éœ€è¦æŠ€æœ¯å¹²é¢„ï¼›å®ƒè¦æ±‚å°†å¤šæ ·çš„è¯­éŸ³å˜ä½“è§†ä¸ºå€¼å¾—æŠ€æœ¯é€‚åº”çš„åˆæ³•è¡¨è¾¾æ–¹å¼ã€‚è¿™ç§å“²å­¦é‡æ„ä¸ºå¼€å‘å°Šé‡è¯­è¨€å¤šæ ·æ€§å’Œè¯´è¯è€…è‡ªä¸»æƒçš„ASRç³»ç»Ÿæä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07143v2">PDF</a> Accepted to AIES 2025</p>
<p><strong>Summary</strong>ï¼š<br>è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¯¹å¤šç§è¯­è¨€çš„å…¬å¹³æ€§å½±å“å¤‡å—å…³æ³¨ï¼Œè¯¥æ–‡æ¢è®¨äº†å…¶èƒŒåçš„åè§é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¯¹ç‰¹å®šè¯­éŸ³ç§ç±»çš„ç³»ç»Ÿæ€§è¯¯è¯†åˆ«ä¸ä»…æ˜¯æŠ€æœ¯å±€é™ï¼Œæ›´æ˜¯å¯¹è¾¹ç¼˜è¯­è¨€ç¾¤ä½“å†å²ä¸å…¬çš„åŠ å‰§ã€‚æ–‡ç« åŒºåˆ†äº†é“å¾·ä¸­ç«‹åˆ†ç±»å’Œæœ‰å®³æ­§è§†ï¼Œå¹¶æŒ‡å‡ºè¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨è¯¯è¯†åˆ«éæ ‡å‡†æ–¹è¨€æ—¶å¦‚ä½•å°†å‰è€…è½¬åŒ–ä¸ºåè€…ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼ºè°ƒäº†è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„ä¸‰ä¸ªç‹¬ç‰¹ä¼¦ç†ç»´åº¦ï¼ŒåŒ…æ‹¬éæ ‡å‡†è¯­éŸ³è¯´è¯è€…çš„æ—¶é—´è´Ÿæ‹…ã€ç³»ç»Ÿè¯¯è¯†åˆ«å¯¼è‡´çš„å¯¹è¯æµç•…æ€§çš„ç ´åä»¥åŠè¯­éŸ³æ¨¡å¼ä¸ä¸ªäººæ–‡åŒ–èº«ä»½çš„ç´§å¯†è”ç³»ã€‚ç°æœ‰æŠ€æœ¯å…¬å¹³æŒ‡æ ‡æ— æ³•æ•æ‰è¿™äº›ä¸å¯¹ç§°çš„æƒåŠ›å’Œå…³ç³»é—®é¢˜ã€‚æ–‡ç« å‘¼ååœ¨è¯­éŸ³è¯†åˆ«å‘å±•ä¸­å¹³è¡¡è¯­è¨€æ ‡å‡†åŒ–å’Œå¤šå…ƒåŒ–çš„å¼ åŠ›ï¼Œå¹¶å¼ºè°ƒè§£å†³è¯­éŸ³è¯†åˆ«åè§ä¸ä»…éœ€è¦æŠ€æœ¯å¹²é¢„ï¼Œè¿˜éœ€è¦æ‰¿è®¤å¤šç§è¯­éŸ³å½¢å¼ä½œä¸ºå€¼å¾—æŠ€æœ¯é€‚åº”çš„åˆæ³•è¡¨è¾¾æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„åè§é—®é¢˜å—åˆ°å…³æ³¨ï¼Œå®ƒå¯¹ç‰¹å®šè¯­éŸ³ç§ç±»çš„ç³»ç»Ÿæ€§è¯¯è¯†åˆ«ä»£è¡¨äº†è¾¹ç¼˜è¯­è¨€ç¾¤ä½“çš„å†å²ä¸å…¬ã€‚</li>
<li>åŒºåˆ†äº†é“å¾·ä¸­ç«‹åˆ†ç±»å’Œæœ‰å®³æ­§è§†ï¼Œå¹¶æŒ‡å‡ºäº†äºŒè€…åœ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚</li>
<li>è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„ä¼¦ç†é—®é¢˜åŒ…æ‹¬éæ ‡å‡†è¯­éŸ³è¯´è¯è€…çš„æ—¶é—´è´Ÿæ‹…ã€å¯¹è¯æµç•…æ€§çš„ç ´åä»¥åŠè¯­éŸ³æ¨¡å¼ä¸ä¸ªäººæ–‡åŒ–èº«ä»½çš„ç´§å¯†è”ç³»ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯å…¬å¹³æŒ‡æ ‡æ— æ³•æ•æ‰è¿™äº›ä¼¦ç†é—®é¢˜ä¸­çš„ä¸å¯¹ç§°æƒåŠ›å…³ç³»ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒå¹³è¡¡è¯­è¨€æ ‡å‡†åŒ–å’Œå¤šå…ƒåŒ–çš„é‡è¦æ€§åœ¨è¯­éŸ³è¯†åˆ«å‘å±•ä¸­ã€‚</li>
<li>è§£å†³è¯­éŸ³è¯†åˆ«åè§ä¸ä»…éœ€è¦æŠ€æœ¯å¹²é¢„ï¼Œè¿˜éœ€è¦æ‰¿è®¤å¤šç§è¯­éŸ³å½¢å¼ä½œä¸ºåˆæ³•è¡¨è¾¾æ–¹å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51970bafb1e9fb977787a49f566a6cc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77ce3909ef5f1ef6c34dec44f182507b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LCS-CTC-Leveraging-Soft-Alignments-to-Enhance-Phonetic-Transcription-Robustness"><a href="#LCS-CTC-Leveraging-Soft-Alignments-to-Enhance-Phonetic-Transcription-Robustness" class="headerlink" title="LCS-CTC: Leveraging Soft Alignments to Enhance Phonetic Transcription   Robustness"></a>LCS-CTC: Leveraging Soft Alignments to Enhance Phonetic Transcription   Robustness</h2><p><strong>Authors:Zongli Ye, Jiachen Lian, Akshaj Gupta, Xuanru Zhou, Haodong Li, Krish Patel, Hwi Joo Park, Dingkun Zhou, Chenxu Guo, Shuhe Li, Sam Wang, Iris Zhou, Cheol Jun Cho, Zoe Ezzes, Jet M. J. Vonk, Brittany T. Morin, Rian Bogley, Lisa Wauters, Zachary A. Miller, Maria Luisa Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Phonetic speech transcription is crucial for fine-grained linguistic analysis and downstream speech applications. While Connectionist Temporal Classification (CTC) is a widely used approach for such tasks due to its efficiency, it often falls short in recognition performance, especially under unclear and nonfluent speech. In this work, we propose LCS-CTC, a two-stage framework for phoneme-level speech recognition that combines a similarity-aware local alignment algorithm with a constrained CTC training objective. By predicting fine-grained frame-phoneme cost matrices and applying a modified Longest Common Subsequence (LCS) algorithm, our method identifies high-confidence alignment zones which are used to constrain the CTC decoding path space, thereby reducing overfitting and improving generalization ability, which enables both robust recognition and text-free forced alignment. Experiments on both LibriSpeech and PPA demonstrate that LCS-CTC consistently outperforms vanilla CTC baselines, suggesting its potential to unify phoneme modeling across fluent and non-fluent speech. </p>
<blockquote>
<p>è¯­éŸ³å‘éŸ³è½¬å½•å¯¹äºç²¾ç»†çš„è¯­è¨€åˆ†æå’Œä¸‹æ¸¸è¯­éŸ³åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰ç”±äºå…¶æ•ˆç‡è€Œå¹¿æ³›åº”ç”¨äºæ­¤ç±»ä»»åŠ¡ï¼Œä½†åœ¨è¯†åˆ«æ€§èƒ½ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸æ¸…æ™°å’Œéæµåˆ©è¯­éŸ³çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LCS-CTCï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸ³ç´ çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œå®ƒå°†ç›¸ä¼¼æ„ŸçŸ¥å±€éƒ¨å¯¹é½ç®—æ³•ä¸å—çº¦æŸçš„CTCè®­ç»ƒç›®æ ‡ç›¸ç»“åˆã€‚é€šè¿‡é¢„æµ‹ç²¾ç»†çš„å¸§éŸ³ç´ æˆæœ¬çŸ©é˜µå¹¶åº”ç”¨ä¿®æ”¹åçš„æœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLCSï¼‰ç®—æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«é«˜ç½®ä¿¡åº¦çš„å¯¹é½åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸç”¨äºçº¦æŸCTCè§£ç è·¯å¾„ç©ºé—´ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå®ç°ç¨³å¥çš„è¯†åˆ«å’Œå…æ–‡æœ¬å¼ºåˆ¶å¯¹é½ã€‚åœ¨LibriSpeechå’ŒPPAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLCS-CTCå§‹ç»ˆä¼˜äºæ™®é€šCTCåŸºå‡†æµ‹è¯•ï¼Œè¿™è¡¨æ˜å…¶åœ¨æµåˆ©å’Œéæµåˆ©è¯­éŸ³çš„éŸ³ç´ å»ºæ¨¡ä¸Šå…·æœ‰ç»Ÿä¸€æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03937v2">PDF</a> 2025 ASRU. Correct Author List</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬æå‡ºäº†LCS-CTCï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸ³ç´ çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œç»“åˆäº†ç›¸ä¼¼åº¦æ„ŸçŸ¥çš„å±€éƒ¨å¯¹é½ç®—æ³•å’Œçº¦æŸæ€§CTCè®­ç»ƒç›®æ ‡ã€‚é€šè¿‡é¢„æµ‹ç²¾ç»†çš„å¸§-éŸ³ç´ æˆæœ¬çŸ©é˜µå¹¶åº”ç”¨æ”¹è¿›çš„æœ€é•¿å…¬å…±å­åºåˆ—ç®—æ³•ï¼ŒLCS-CTCèƒ½å¤Ÿè¯†åˆ«é«˜ç½®ä¿¡åº¦çš„å¯¹é½åŒºåŸŸï¼Œç”¨äºçº¦æŸCTCè§£ç è·¯å¾„ç©ºé—´ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä½¿å¾—LCS-CTCæ—¢å…·æœ‰ç¨³å¥çš„è¯†åˆ«èƒ½åŠ›ï¼Œåˆèƒ½å®ç°æ— æ–‡æœ¬å¼ºåˆ¶å¯¹é½ã€‚åœ¨LibriSpeechå’ŒPPAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLCS-CTCæŒç»­ä¼˜äºä¼ ç»Ÿçš„CTCåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æµåˆ©å’Œéæµåˆ©è¯­éŸ³ä¸­çš„éŸ³ç´ å»ºæ¨¡æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Phonetic speech transcriptionå¯¹äºç²¾ç»†çš„è¯­è¨€åˆ†æå’Œä¸‹æ¸¸è¯­éŸ³åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>CTCï¼ˆè¿æ¥æ—¶åºåˆ†ç±»ï¼‰å› æ•ˆç‡è€Œå¹¿æ³›åº”ç”¨äºè¯­éŸ³ä»»åŠ¡ï¼Œä½†åœ¨ä¸æ¸…æ™°å’Œéæµåˆ©çš„è¯­éŸ³ä¸­è¯†åˆ«æ€§èƒ½è¾ƒå·®ã€‚</li>
<li>LCS-CTCæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸ³ç´ çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç›¸ä¼¼åº¦æ„ŸçŸ¥çš„å±€éƒ¨å¯¹é½ç®—æ³•å’Œçº¦æŸæ€§CTCè®­ç»ƒç›®æ ‡æ¥è§£å†³CTCçš„è¯†åˆ«é—®é¢˜ã€‚</li>
<li>LCS-CTCé€šè¿‡é¢„æµ‹ç²¾ç»†çš„å¸§-éŸ³ç´ æˆæœ¬çŸ©é˜µå¹¶åº”ç”¨LCSç®—æ³•ï¼Œèƒ½è¯†åˆ«é«˜ç½®ä¿¡åº¦çš„å¯¹é½åŒºåŸŸï¼Œçº¦æŸCTCè§£ç è·¯å¾„ã€‚</li>
<li>LCS-CTCå‡å°‘äº†æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LCS-CTCå®ç°äº†ç¨³å¥çš„è¯­éŸ³è¯†åˆ«å’Œæ— æ–‡æœ¬å¼ºåˆ¶å¯¹é½ã€‚</li>
<li>åœ¨LibriSpeechå’ŒPPAçš„å®éªŒä¸­ï¼ŒLCS-CTCä¼˜äºä¼ ç»Ÿçš„CTCåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è¯­éŸ³è¯†åˆ«çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-475918b4e7712aeef9c0836153873cf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37a2aa67fe5c2aa8aadc758cb2f3afcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdb8f05def465add5d432de098037e98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3e469d4e4137241b2cb490bccca9d4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad67dea7b82008fd504606fbf8978400.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9b0ae52c0e2bed2e4aa6740f2f83888.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ea2fcfa9ce9e8ab272994d18846fad2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MEDTalk-Multimodal-Controlled-3D-Facial-Animation-with-Dynamic-Emotions-by-Disentangled-Embedding"><a href="#MEDTalk-Multimodal-Controlled-3D-Facial-Animation-with-Dynamic-Emotions-by-Disentangled-Embedding" class="headerlink" title="MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions   by Disentangled Embedding"></a>MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions   by Disentangled Embedding</h2><p><strong>Authors:Chang Liu, Ye Pan, Chenyang Ding, Susanto Rahardja, Xiaokang Yang</strong></p>
<p>Audio-driven emotional 3D facial animation aims to generate synchronized lip movements and vivid facial expressions. However, most existing approaches focus on static and predefined emotion labels, limiting their diversity and naturalness. To address these challenges, we propose MEDTalk, a novel framework for fine-grained and dynamic emotional talking head generation. Our approach first disentangles content and emotion embedding spaces from motion sequences using a carefully designed cross-reconstruction process, enabling independent control over lip movements and facial expressions. Beyond conventional audio-driven lip synchronization, we integrate audio and speech text, predicting frame-wise intensity variations and dynamically adjusting static emotion features to generate realistic emotional expressions. Furthermore, to enhance control and personalization, we incorporate multimodal inputs-including text descriptions and reference expression images-to guide the generation of user-specified facial expressions. With MetaHuman as the priority, our generated results can be conveniently integrated into the industrial production pipeline. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/SJTU-Lucy/MEDTalk">https://github.com/SJTU-Lucy/MEDTalk</a>. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„æƒ…æ„Ÿ3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”ŸæˆåŒæ­¥çš„å”‡éƒ¨è¿åŠ¨å’Œç”ŸåŠ¨çš„é¢éƒ¨è¡¨æƒ…ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾§é‡äºé™æ€å’Œé¢„å®šä¹‰çš„æƒ…æ„Ÿæ ‡ç­¾ï¼Œè¿™é™åˆ¶äº†å…¶å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MEDTalkï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç²¾ç»†ç²’åº¦å’ŒåŠ¨æ€æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨ç”Ÿæˆçš„å…¨æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„è·¨é‡å»ºè¿‡ç¨‹ï¼Œä»è¿åŠ¨åºåˆ—ä¸­åˆ†ç¦»å†…å®¹å’Œæƒ…æ„ŸåµŒå…¥ç©ºé—´ï¼Œå®ç°å¯¹å”‡éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¡¨æƒ…çš„ç‹¬ç«‹æ§åˆ¶ã€‚é™¤äº†ä¼ ç»Ÿçš„éŸ³é¢‘é©±åŠ¨å”‡åŒæ­¥å¤–ï¼Œæˆ‘ä»¬è¿˜æ•´åˆäº†éŸ³é¢‘å’Œè¯­éŸ³æ–‡æœ¬ï¼Œé¢„æµ‹å¸§å¼ºåº¦å˜åŒ–å¹¶åŠ¨æ€è°ƒæ•´é™æ€æƒ…æ„Ÿç‰¹å¾ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„æƒ…æ„Ÿè¡¨è¾¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºæ§åˆ¶å’Œä¸ªæ€§åŒ–ï¼Œæˆ‘ä»¬çº³å…¥å¤šæ¨¡å¼è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬æè¿°å’Œå‚è€ƒè¡¨æƒ…å›¾åƒï¼Œä»¥å¼•å¯¼ç”¨æˆ·æŒ‡å®šçš„é¢éƒ¨è¡¨æƒ…ç”Ÿæˆã€‚ä»¥MetaHumanä¸ºä¼˜å…ˆï¼Œæˆ‘ä»¬ç”Ÿæˆçš„ç»“æœå¯ä»¥æ–¹ä¾¿åœ°é›†æˆåˆ°å·¥ä¸šç”Ÿäº§æµç¨‹ä¸­ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/SJTU-Lucy/MEDTalk%E3%80%82">https://github.com/SJTU-Lucy/MEDTalkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06071v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMEDTalkçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç²¾ç»†ç²’åº¦å’ŒåŠ¨æ€æƒ…æ„Ÿè¯´è¯å¤´ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾å¿ƒè®¾è®¡äº¤å‰é‡å»ºè¿‡ç¨‹ï¼Œä»è¿åŠ¨åºåˆ—ä¸­åˆ†ç¦»å†…å®¹å’Œæƒ…æ„ŸåµŒå…¥ç©ºé—´ï¼Œå®ç°å¯¹å”‡åŠ¨å’Œé¢éƒ¨è¡¨æƒ…çš„ç‹¬ç«‹æ§åˆ¶ã€‚é™¤ä¼ ç»ŸéŸ³é¢‘é©±åŠ¨çš„å”‡éƒ¨åŒæ­¥å¤–ï¼Œè¿˜ç»“åˆäº†éŸ³é¢‘å’Œè¯­éŸ³æ–‡æœ¬ï¼Œé¢„æµ‹å¸§å¼ºåº¦å˜åŒ–å¹¶åŠ¨æ€è°ƒæ•´é™æ€æƒ…æ„Ÿç‰¹å¾ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„æƒ…æ„Ÿè¡¨è¾¾ã€‚åŒæ—¶ï¼Œä¸ºæé«˜æ§åˆ¶å’Œä¸ªæ€§åŒ–ï¼Œè¿˜çº³å…¥äº†åŒ…æ‹¬æ–‡æœ¬æè¿°å’Œå‚è€ƒè¡¨æƒ…å›¾åƒåœ¨å†…çš„å¤šæ¨¡å¼è¾“å…¥ï¼Œä»¥æŒ‡å¯¼ç”¨æˆ·æŒ‡å®šçš„é¢éƒ¨è¡¨æƒ…ç”Ÿæˆã€‚æœ€ç»ˆç”Ÿæˆçš„ç»“æœå¯ä»¥ä¾¿æ·åœ°èå…¥å·¥ä¸šç”Ÿäº§æµç¨‹ä¸­ï¼Œå¦‚MetaHumanç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MEDTalkæ¡†æ¶æ—¨åœ¨ç”Ÿæˆç²¾ç»†ç²’åº¦å’ŒåŠ¨æ€çš„æƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»ã€‚</li>
<li>é€šè¿‡äº¤å‰é‡å»ºè¿‡ç¨‹åˆ†ç¦»å†…å®¹å’Œæƒ…æ„ŸåµŒå…¥ç©ºé—´ï¼Œå®ç°å”‡åŠ¨å’Œé¢éƒ¨è¡¨æƒ…çš„ç‹¬ç«‹æ§åˆ¶ã€‚</li>
<li>ç»“åˆéŸ³é¢‘ã€è¯­éŸ³æ–‡æœ¬è¿›è¡Œå”‡éƒ¨åŒæ­¥å’Œé¢éƒ¨è¡¨æƒ…é¢„æµ‹ã€‚</li>
<li>é¢„æµ‹å¸§å¼ºåº¦å˜åŒ–å¹¶åŠ¨æ€è°ƒæ•´é™æ€æƒ…æ„Ÿç‰¹å¾ä»¥ç”Ÿæˆæ›´çœŸå®çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>çº³å…¥å¤šæ¨¡å¼è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬æè¿°å’Œå‚è€ƒè¡¨æƒ…å›¾åƒï¼Œä»¥å¢å¼ºæ§åˆ¶å’Œä¸ªæ€§åŒ–ã€‚</li>
<li>ç”Ÿæˆçš„ç»“æœå¯èå…¥å·¥ä¸šç”Ÿäº§æµç¨‹ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e088d7d7787d0d9e93464e1d62c2f01d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cee7a771b1f37ff3bb0f7a2a22bd9d4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a492f75a4deb6aa3fd9e98f3f387ee5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-038287a533b520c55d5b1068fd361394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-441ea5f408e91485922da2996d0384fb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Audio-3DVG-Unified-Audio-â€“-Point-Cloud-Fusion-for-3D-Visual-Grounding"><a href="#Audio-3DVG-Unified-Audio-â€“-Point-Cloud-Fusion-for-3D-Visual-Grounding" class="headerlink" title="Audio-3DVG: Unified Audio â€“ Point Cloud Fusion for 3D Visual Grounding"></a>Audio-3DVG: Unified Audio â€“ Point Cloud Fusion for 3D Visual Grounding</h2><p><strong>Authors:Duc Cao-Dinh, Khai Le-Duc, Anh Dao, Bach Phan Tat, Chris Ngo, Duy M. H. Nguyen, Nguyen X. Khanh, Thanh Nguyen-Tang</strong></p>
<p>3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce (i) Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an (ii) Audio-Guided Attention module that models the interactions between target candidates and mentioned objects, enhancing discrimination in cluttered 3D environments. To support benchmarking, we (iii) synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods, highlight the promise of integrating spoken language into 3D vision tasks. </p>
<blockquote>
<p>3Dè§†è§‰å®šä½ï¼ˆ3DVGï¼‰æ¶‰åŠåœ¨è‡ªç„¶è¯­è¨€çš„åŸºç¡€ä¸Šåœ¨3Dç‚¹äº‘ä¸­å®šä½ç›®æ ‡å¯¹è±¡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œåœ¨ä½¿ç”¨æ–‡æœ¬æè¿°æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åˆ©ç”¨å£è¯­çš„åŸºäºéŸ³é¢‘çš„3Dè§†è§‰å®šä½ä»ç„¶è¢«ä½ä¼°ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å—è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³è¡¨ç¤ºå­¦ä¹ çš„è¿›æ­¥çš„æ¨åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-3DVGï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒé›†æˆäº†éŸ³é¢‘å’Œç©ºé—´ä¿¡æ¯ä»¥å¢å¼ºå®šä½ã€‚æˆ‘ä»¬å¹¶ä¸å°†è¯­éŸ³è§†ä¸ºå•ä¸€è¾“å…¥ï¼Œè€Œæ˜¯å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªäº’è¡¥çš„ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ï¼ˆiï¼‰å¯¹è±¡æåŠæ£€æµ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œå®ƒæ˜ç¡®åœ°è¯†åˆ«äº†éŸ³é¢‘ä¸­æåˆ°çš„å“ªäº›å¯¹è±¡ï¼Œä»è€Œå®ç°äº†æ›´ç»“æ„åŒ–çš„éŸ³é¢‘åœºæ™¯æ¨ç†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ï¼ˆiiï¼‰éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯¹ç›®æ ‡å€™é€‰ç‰©å’ŒæåŠçš„å¯¹è±¡ä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œæé«˜äº†åœ¨æ‚ä¹±çš„3Dç¯å¢ƒä¸­çš„è¾¨åˆ«èƒ½åŠ›ã€‚ï¼ˆiiiï¼‰ä¸ºäº†æ”¯æŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹åŒ…æ‹¬ScanReferã€Sr3Då’ŒNr3Dåœ¨å†…çš„æ ‡å‡†3DVGæ•°æ®é›†è¿›è¡Œäº†éŸ³é¢‘æè¿°åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAudio-3DVGä¸ä»…åœ¨åŸºäºéŸ³é¢‘çš„å®šä½æ–¹é¢å–å¾—äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨ä¸åŸºäºæ–‡æœ¬çš„æ–¹æ³•ç«äº‰æ—¶ä¹Ÿæœ‰ä¼˜åŠ¿ï¼Œè¿™çªæ˜¾äº†å°†å£è¯­èå…¥3Dè§†è§‰ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00669v2">PDF</a> Preprint, 51 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†3Dè§†è§‰å®šä½ï¼ˆ3DVGï¼‰åœ¨ä¸‰ç»´ç‚¹äº‘ä¸­å®šä½ç›®æ ‡ç‰©ä½“çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€‚é’ˆå¯¹éŸ³é¢‘ä¸ºåŸºç¡€çš„3Dè§†è§‰å®šä½ï¼ˆAudio-based 3DVGï¼‰è¿™ä¸€å°šæœªå……åˆ†æ¢ç´¢ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œæå‡ºäº†Audio-3DVGæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†éŸ³é¢‘å’Œç©ºé—´ä¿¡æ¯ï¼Œå°†è¯­éŸ³åˆ†è§£ä¸ºå¯¹è±¡æåŠæ£€æµ‹å’ŒéŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›ä¸¤ä¸ªäº’è¡¥çš„ä»»åŠ¡ã€‚åŒæ—¶ï¼Œä¸ºæ”¯æŒåŸºå‡†æµ‹è¯•ï¼Œå¯¹æ ‡å‡†3DVGæ•°æ®é›†è¿›è¡Œäº†éŸ³é¢‘æè¿°åˆæˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAudio-3DVGä¸ä»…åœ¨éŸ³é¢‘å®šä½æ–¹é¢å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œè€Œä¸”åœ¨ä¸åŸºäºæ–‡æœ¬çš„3Dè§†è§‰ä»»åŠ¡æ–¹æ³•ç›¸æ¯”ä¹Ÿæœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dè§†è§‰å®šä½ï¼ˆ3DVGï¼‰æ¶‰åŠåœ¨ä¸‰ç»´ç‚¹äº‘ä¸­æ ¹æ®è‡ªç„¶è¯­è¨€å®šä½ç›®æ ‡ç‰©ä½“ã€‚</li>
<li>éŸ³é¢‘ä¸ºåŸºç¡€çš„3Dè§†è§‰å®šä½ï¼ˆAudio-based 3DVGï¼‰æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œå°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>æå‡ºçš„Audio-3DVGæ¡†æ¶ç»“åˆäº†éŸ³é¢‘å’Œç©ºé—´ä¿¡æ¯ï¼Œç”¨äºå¢å¼ºå®šä½ã€‚</li>
<li>è¯­éŸ³è¢«åˆ†è§£ä¸ºå¯¹è±¡æåŠæ£€æµ‹å’ŒéŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›ä¸¤ä¸ªäº’è¡¥çš„ä»»åŠ¡ã€‚</li>
<li>å¯¹è±¡æåŠæ£€æµ‹æ˜¯ä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œæ˜ç¡®è¯†åˆ«éŸ³é¢‘ä¸­æåˆ°çš„ç‰©ä½“ï¼Œä½¿éŸ³é¢‘åœºæ™¯æ¨ç†æ›´åŠ ç»“æ„åŒ–ã€‚</li>
<li>éŸ³é¢‘å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—æ—¨åœ¨å»ºæ¨¡ç›®æ ‡å€™é€‰ç‰©å’ŒæåŠç‰©ä½“ä¹‹é—´çš„äº¤äº’ï¼Œæé«˜åœ¨æ‚ä¹±çš„ä¸‰ç»´ç¯å¢ƒä¸­çš„è¾¨åˆ«åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dbf77665d03c6482b1ae9ce09147bf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e49fee79bc87cddf7d4a51cdfb79636.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44f43573829e205c76be8c213e901234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98be81f359df2e312ad849760c47ba8d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EmoVoice-LLM-based-Emotional-Text-To-Speech-Model-with-Freestyle-Text-Prompting"><a href="#EmoVoice-LLM-based-Emotional-Text-To-Speech-Model-with-Freestyle-Text-Prompting" class="headerlink" title="EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text   Prompting"></a>EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text   Prompting</h2><p><strong>Authors:Guanrou Yang, Chen Yang, Qian Chen, Ziyang Ma, Wenxi Chen, Wen Wang, Tianrui Wang, Yifan Yang, Zhikang Niu, Wenrui Liu, Fan Yu, Zhihao Du, Zhifu Gao, ShiLiang Zhang, Xie Chen</strong></p>
<p>Human speech goes beyond the mere transfer of information; it is a profound exchange of emotions and a connection between individuals. While Text-to-Speech (TTS) models have made huge progress, they still face challenges in controlling the emotional expression in the generated speech. In this work, we propose EmoVoice, a novel emotion-controllable TTS model that exploits large language models (LLMs) to enable fine-grained freestyle natural language emotion control, and a phoneme boost variant design that makes the model output phoneme tokens and audio tokens in parallel to enhance content consistency, inspired by chain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring expressive speech and fine-grained emotion labels with natural language descriptions. EmoVoice achieves state-of-the-art performance on the English EmoVoice-DB test set using only synthetic training data, and on the Chinese Secap test set using our in-house data. We further investigate the reliability of existing emotion evaluation metrics and their alignment with human perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and Gemini to assess emotional speech. Dataset, code, checkpoints, and demo samples are available at <a target="_blank" rel="noopener" href="https://github.com/yanghaha0908/EmoVoice">https://github.com/yanghaha0908/EmoVoice</a>. </p>
<blockquote>
<p>äººç±»è¨€è¯­ä¸ä»…ä»…æ˜¯ä¸ºäº†ä¼ é€’ä¿¡æ¯ï¼Œæ›´æ˜¯ä¸€ç§æƒ…æ„Ÿä¸Šçš„æ·±åˆ»äº¤æµå’Œä¸ªäººä¹‹é—´çš„è¿æ¥ã€‚å°½ç®¡æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å·²ç»å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œä½†åœ¨æ§åˆ¶ç”Ÿæˆè¯­éŸ³çš„æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EmoVoiceï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æƒ…æ„Ÿå¯æ§TTSæ¨¡å‹ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å®ç°ç²¾ç»†çš„è‡ªç”±å½¢å¼è‡ªç„¶è¯­è¨€æƒ…æ„Ÿæ§åˆ¶ï¼Œä»¥åŠä¸€ç§å¹¶è¡Œè¾“å‡ºéŸ³ç´ æ ‡è®°å’ŒéŸ³é¢‘æ ‡è®°çš„å˜ä½“è®¾è®¡ï¼Œä»¥å¢å¼ºå†…å®¹ä¸€è‡´æ€§ï¼Œè¿™å—åˆ°äº†æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ¨¡æ€é“¾ï¼ˆCoMï¼‰æŠ€æœ¯çš„å¯å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†EmoVoice-DBï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„40å°æ—¶è‹±è¯­æƒ…æ„Ÿæ•°æ®é›†ï¼ŒåŒ…å«è¡¨è¾¾æ€§è¯­éŸ³å’Œå…·æœ‰è‡ªç„¶è¯­è¨€æè¿°çš„ç²¾ç»†æƒ…æ„Ÿæ ‡ç­¾ã€‚EmoVoiceä»…ä½¿ç”¨åˆæˆè®­ç»ƒæ•°æ®ï¼Œåœ¨è‹±è¯­EmoVoice-DBæµ‹è¯•é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä½¿ç”¨æˆ‘ä»¬å†…éƒ¨æ•°æ®çš„ä¸­æ–‡Secapæµ‹è¯•é›†ä¸Šä¹Ÿå–å¾—äº†è‰¯å¥½è¡¨ç°ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº†ç°æœ‰æƒ…æ„Ÿè¯„ä¼°æŒ‡æ ‡çš„å¯é æ€§åŠå…¶ä¸äººç±»æ„ŸçŸ¥åå¥½çš„ä¸€è‡´æ€§ï¼Œå¹¶æ¢è®¨äº†ä½¿ç”¨æœ€å…ˆè¿›çš„å¤šæ¨¡æ€LLMs GPT-4o-audioå’ŒGeminiæ¥è¯„ä¼°æƒ…æ„Ÿè¯­éŸ³ã€‚æ•°æ®é›†ã€ä»£ç ã€æ£€æŸ¥ç‚¹å’Œæ¼”ç¤ºæ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yanghaha0908/EmoVoice%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yanghaha0908/EmoVoiceæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12867v4">PDF</a> Accepted at ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†EmoVoiceï¼Œä¸€ç§æ–°å‹æƒ…æ„Ÿå¯æ§çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°ç²¾ç»†çš„è‡ªç„¶è¯­è¨€æƒ…æ„Ÿæ§åˆ¶ï¼Œå¹¶é€šè¿‡å¹¶è¡Œè¾“å‡ºéŸ³ç´ æ ‡è®°å’ŒéŸ³é¢‘æ ‡è®°æ¥æé«˜å†…å®¹ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†EmoVoice-DBï¼Œä¸€ä¸ªé«˜è´¨é‡çš„æƒ…æ„Ÿæ•°æ®é›†ï¼ŒåŒ…å«è¡¨è¾¾æ€§è¯­éŸ³å’Œç²¾ç»†æƒ…æ„Ÿæ ‡ç­¾çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚EmoVoiceåœ¨è‹±æ–‡æ•°æ®é›†å’Œä¸­æ–‡æ•°æ®é›†ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ç°æœ‰æƒ…æ„Ÿè¯„ä¼°æŒ‡æ ‡çš„å¯é æ€§åŠå…¶ä¸äººç±»æ„ŸçŸ¥åå¥½çš„ä¸€è‡´æ€§ï¼Œå¹¶æ¢ç´¢äº†ä½¿ç”¨æœ€å…ˆè¿›çš„å¤šæ¨¡æ€LLMå¯¹æƒ…æ„Ÿè¯­éŸ³è¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmoVoiceæ˜¯ä¸€ä¸ªæ–°å‹çš„æƒ…æ„Ÿå¯æ§æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç»†çš„è‡ªç„¶è¯­è¨€æƒ…æ„Ÿæ§åˆ¶ã€‚</li>
<li>EmoVoiceåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå»ºæ¨¡ï¼Œæé«˜äº†æƒ…æ„Ÿè¡¨è¾¾çš„å‡†ç¡®æ€§ã€‚</li>
<li>EmoVoiceé€šè¿‡å¹¶è¡Œè¾“å‡ºéŸ³ç´ æ ‡è®°å’ŒéŸ³é¢‘æ ‡è®°ï¼Œå¢å¼ºäº†å†…å®¹ä¸€è‡´æ€§ã€‚</li>
<li>æ¨å‡ºäº†é«˜è´¨é‡çš„æƒ…æ„Ÿæ•°æ®é›†EmoVoice-DBï¼ŒåŒ…å«è¡¨è¾¾æ€§è¯­éŸ³å’Œç²¾ç»†æƒ…æ„Ÿæ ‡ç­¾çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚</li>
<li>EmoVoiceåœ¨è‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</li>
<li>ç ”ç©¶äººå‘˜æ¢è®¨äº†ç°æœ‰æƒ…æ„Ÿè¯„ä¼°æŒ‡æ ‡çš„å¯é æ€§åŠå…¶ä¸äººç±»æ„ŸçŸ¥åå¥½çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a14efc1c01ba8c0b01c03137cbc42ec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19424b059b00990b3a5e4b40353fd6f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f8b8f824e7680815b8618381c5c21b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c7959d78c8a7935a8a941acc9717758.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-002790b9d15bcadeba967796018f6558.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc4196e3281ed544b5120bc8970520ad.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-15/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-532ca8df40131a56bf3a128e51f61cdb.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  Hybrid Generative Fusion for Efficient and Privacy-Preserving Face   Recognition Dataset Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-15/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f91c7627b3ea7b0d1be3e61a86a8189a.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-15  T-CACE A Time-Conditioned Autoregressive Contrast Enhancement   Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and   Diagnosis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
