<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-16  Anchor Token Matching Implicit Structure Locking for Training-free AR   Image Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b65add2678bc8eb9dbbe33652fc51cf8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-16-更新"><a href="#2025-04-16-更新" class="headerlink" title="2025-04-16 更新"></a>2025-04-16 更新</h1><h2 id="Anchor-Token-Matching-Implicit-Structure-Locking-for-Training-free-AR-Image-Editing"><a href="#Anchor-Token-Matching-Implicit-Structure-Locking-for-Training-free-AR-Image-Editing" class="headerlink" title="Anchor Token Matching: Implicit Structure Locking for Training-free AR   Image Editing"></a>Anchor Token Matching: Implicit Structure Locking for Training-free AR   Image Editing</h2><p><strong>Authors:Taihang Hu, Linxuan Li, Kai Wang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</strong></p>
<p>Text-to-image generation has seen groundbreaking advancements with diffusion models, enabling high-fidelity synthesis and precise image editing through cross-attention manipulation. Recently, autoregressive (AR) models have re-emerged as powerful alternatives, leveraging next-token generation to match diffusion models. However, existing editing techniques designed for diffusion models fail to translate directly to AR models due to fundamental differences in structural control. Specifically, AR models suffer from spatial poverty of attention maps and sequential accumulation of structural errors during image editing, which disrupt object layouts and global consistency. In this work, we introduce Implicit Structure Locking (ISLock), the first training-free editing strategy for AR visual models. Rather than relying on explicit attention manipulation or fine-tuning, ISLock preserves structural blueprints by dynamically aligning self-attention patterns with reference images through the Anchor Token Matching (ATM) protocol. By implicitly enforcing structural consistency in latent space, our method ISLock enables structure-aware editing while maintaining generative autonomy. Extensive experiments demonstrate that ISLock achieves high-quality, structure-consistent edits without additional training and is superior or comparable to conventional editing techniques. Our findings pioneer the way for efficient and flexible AR-based image editing, further bridging the performance gap between diffusion and autoregressive generative models. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM">https://github.com/hutaiHang/ATM</a> </p>
<blockquote>
<p>文本到图像生成领域在扩散模型（Diffusion Models）的推动下取得了突破性进展，通过跨注意力操纵（cross-attention manipulation）实现了高保真合成和精确图像编辑。近期，自回归（AR）模型作为强大的替代方案重新崭露头角，它利用下一个标记生成（next-token generation）技术匹配扩散模型。然而，针对扩散模型设计的现有编辑技术无法直接应用于AR模型，因为它们在结构控制上存在根本差异。具体来说，AR模型在图像编辑时存在注意力图的空间贫困（spatial poverty of attention maps）和结构性错误顺序累积的问题，这会破坏对象布局和全局一致性。在这项工作中，我们介绍了隐式结构锁定（ISLock）——这是一种针对AR视觉模型的首个无需训练即可进行编辑的策略。ISLock不依赖于显式注意力操纵或微调，而是通过锚标记匹配（ATM）协议动态地将自我注意力模式与参考图像对齐，从而保留结构蓝图。通过在潜在空间中隐式强制执行结构一致性，我们的ISLock方法能够在保持生成自主性的同时进行结构感知编辑。大量实验表明，ISLock无需额外训练即可实现高质量、结构一致性的编辑，并且其性能优于或可与传统编辑技术相提并论。我们的发现开辟了基于自回归（AR）的高效灵活图像编辑的道路，进一步缩小了扩散模型和自回归生成模型之间的性能差距。代码将在<a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM">https://github.com/hutaiHang/ATM</a>公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在文本转图像生成领域取得了突破性进展，近期自回归（AR）模型作为强有力的替代方案崭露头角。然而，针对扩散模型设计的编辑技术无法直接应用于AR模型，因为两者在结构控制上存在根本差异。针对AR模型的空间注意力图贫困和图像编辑中的结构错误累积问题，本文提出了无需训练的编辑策略——隐式结构锁定（ISLock）。通过动态对齐参考图像的自我关注模式，ISLock能够保留结构蓝图。实验表明，ISLock能够在无需额外训练的情况下实现高质量、结构一致性的编辑，并优于或相当于传统编辑技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本转图像生成领域取得重大进展，自回归（AR）模型成为有力替代。</li>
<li>现有编辑技术无法直接应用于AR模型，因为结构控制上的根本差异。</li>
<li>AR模型面临空间注意力图贫困和图像编辑中的结构错误累积问题。</li>
<li>ISLock是首个针对AR视觉模型的无需训练的编辑策略。</li>
<li>ISLock通过动态对齐自我关注模式与参考图像，保留结构蓝图。</li>
<li>ISLock实现了高质量、结构一致性的编辑，无需额外训练。</li>
<li>ISLock优于或相当于传统编辑技术，为AR模型基于图像编辑开辟了新途径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10434">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-93beb0f0b0b7f8d87d7e885c2a4146d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33d316dd2b5db7bc9e3692e3c4bbc54b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f90b8b1fbe9d7fa92cf3cf4633b2541.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4105ed1ab16161f5d59433951ac8d3d6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MonoDiff9D-Monocular-Category-Level-9D-Object-Pose-Estimation-via-Diffusion-Model"><a href="#MonoDiff9D-Monocular-Category-Level-9D-Object-Pose-Estimation-via-Diffusion-Model" class="headerlink" title="MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via   Diffusion Model"></a>MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via   Diffusion Model</h2><p><strong>Authors:Jian Liu, Wei Sun, Hui Yang, Jin Zheng, Zichen Geng, Hossein Rahmani, Ajmal Mian</strong></p>
<p>Object pose estimation is a core means for robots to understand and interact with their environment. For this task, monocular category-level methods are attractive as they require only a single RGB camera. However, current methods rely on shape priors or CAD models of the intra-class known objects. We propose a diffusion-based monocular category-level 9D object pose generation method, MonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion models to alleviate the need for shape priors, CAD models, or depth sensors for intra-class unknown object pose estimation. We first estimate coarse depth via DINOv2 from the monocular image in a zero-shot manner and convert it into a point cloud. We then fuse the global features of the point cloud with the input image and use the fused features along with the encoded time step to condition MonoDiff9D. Finally, we design a transformer-based denoiser to recover the object pose from Gaussian noise. Extensive experiments on two popular benchmark datasets show that MonoDiff9D achieves state-of-the-art monocular category-level 9D object pose estimation accuracy without the need for shape priors or CAD models at any stage. Our code will be made public at <a target="_blank" rel="noopener" href="https://github.com/CNJianLiu/MonoDiff9D">https://github.com/CNJianLiu/MonoDiff9D</a>. </p>
<blockquote>
<p>对象姿态估计是机器人理解和与其环境交互的核心手段。对于此任务，单目类别级方法很有吸引力，因为它们仅需要单个RGB相机。然而，当前的方法依赖于形状先验或已知对象的类内CAD模型。我们提出了一种基于扩散的单目类别级9D对象姿态生成方法，称为MonoDiff9D。我们的动机是利用扩散模型的概率性质，减轻对形状先验、CAD模型或深度传感器的需求，用于类内未知对象姿态估计。我们首先通过DINOv2以零样本方式从单目图像估计粗略深度并将其转换为点云。然后，我们将点云的全局特征与输入图像融合，并使用融合的特征以及编码的时间步长来调节MonoDiff9D。最后，我们设计了一个基于变压器的去噪器，从高斯噪声中恢复对象姿态。在两个流行的基准数据集上的大量实验表明，MonoDiff9D在不使用形状先验或任何阶段的CAD模型的情况下，实现了最先进的单目类别级9D对象姿态估计精度。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/CNJianLiu/MonoDiff9D%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/CNJianLiu/MonoDiff9D公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10433v1">PDF</a> Accepted by ICRA’25</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散模型的单目类别级9D物体姿态生成方法，名为MonoDiff9D。该方法利用扩散模型的概率性质，无需形状先验、CAD模型或深度传感器即可实现同类未知物体的姿态估计。通过单目图像进行粗略深度估计，转换为点云，并与输入图像融合特征。设计基于变压器的去噪器，从高斯噪声中恢复物体姿态。在主流数据集上的实验表明，MonoDiff9D在无需形状先验或CAD模型的条件下，实现了最先进的单目类别级9D物体姿态估计精度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对象姿态估计是机器人理解和与其环境交互的核心手段。</li>
<li>当前方法需要形状先验或已知对象的CAD模型。</li>
<li>提出了一种基于扩散模型的MonoDiff9D方法，用于单目类别级9D对象姿态生成。</li>
<li>MonoDiff9D利用扩散模型的概率性质，无需形状先验、CAD模型或深度传感器。</li>
<li>通过单目图像估计粗略深度并转换为点云，与输入图像融合特征。</li>
<li>设计基于变压器的去噪器，从高斯噪声中恢复物体姿态。</li>
<li>在主流数据集上的实验表明，MonoDiff9D实现了最先进的估计精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10433">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-383d12eb88b458170e284b7ed64b61f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0aa45c5b62ca44aed46d63d9cd4c79f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40a95a1a32095dd9ba549eb5df5d2819.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835c471efe5317fe973cdb0583aeb1ac.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation"><a href="#OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation" class="headerlink" title="OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation"></a>OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation</h2><p><strong>Authors:Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang</strong></p>
<p>Autoregressive models have achieved remarkable success across various domains, yet their performance in 3D shape generation lags significantly behind that of diffusion models. In this paper, we introduce OctGPT, a novel multiscale autoregressive model for 3D shape generation that dramatically improves the efficiency and performance of prior 3D autoregressive approaches, while rivaling or surpassing state-of-the-art diffusion models. Our method employs a serialized octree representation to efficiently capture the hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded via octree structures, while fine-grained details are represented by binary tokens generated using a vector quantized variational autoencoder (VQVAE), transforming 3D shapes into compact \emph{multiscale binary sequences} suitable for autoregressive prediction. To address the computational challenges of handling long sequences, we incorporate octree-based transformers enhanced with 3D rotary positional encodings, scale-specific embeddings, and token-parallel generation schemes. These innovations reduce training time by 13 folds and generation time by 69 folds, enabling the efficient training of high-resolution 3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days. OctGPT showcases exceptional versatility across various tasks, including text-, sketch-, and image-conditioned generation, as well as scene-level synthesis involving multiple objects. Extensive experiments demonstrate that OctGPT accelerates convergence and improves generation quality over prior autoregressive methods, offering a new paradigm for high-quality, scalable 3D content creation. </p>
<blockquote>
<p>自回归模型已经在各个领域取得了显著的成果，但在3D形状生成方面，其性能远远落后于扩散模型。在本文中，我们介绍了OctGPT，这是一种用于3D形状生成的新型多尺度自回归模型，它显著提高了之前3D自回归方法的效率和性能，同时与最先进的扩散模型相抗衡甚至超越。我们的方法采用序列化八叉树表示法，有效地捕捉3D形状的层次结构和空间结构。粗糙几何结构通过八叉树结构进行编码，而细粒度细节则由使用向量量化变分自动编码器（VQVAE）生成的二进制令牌表示，将3D形状转换为适合自回归预测的多尺度二进制序列。为了解决处理长序列的计算挑战，我们结合了基于八叉树的变压器，并增强了3D旋转位置编码、尺度特定嵌入和令牌并行生成方案。这些创新将训练时间缩短了13倍，生成时间缩短了69倍，使得在仅四台NVIDIA 4090 GPU上在几天内就能有效地训练高分辨率的3D形状，例如$1024^3$。OctGPT在各种任务中展示了出色的通用性，包括文本、草图和图像条件下的生成，以及涉及多个对象的场景级别合成。大量实验表明，OctGPT加速了收敛，提高了生成质量，为高质量、可扩展的3D内容创建提供了新的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09975v1">PDF</a> SIGGRAPH 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了OctGPT，这是一种用于3D形状生成的新型多尺度自回归模型。它通过采用序列化八叉树表示法，有效地捕捉3D形状的层次结构和空间结构，提高了先前3D自回归方法的效率和性能，同时与或超越了最先进的扩散模型。通过采用基于八叉树的变压器、3D旋转位置编码、尺度特定嵌入和令牌并行生成方案等创新技术，减少了训练和生成时间。OctGPT在不同任务中表现出卓越的多功能性，包括文本、草图和图像条件生成，以及涉及多个对象的场景级别合成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OctGPT是一个用于3D形状生成的多尺度自回归模型，显著提高了自回归模型的效率和性能。</li>
<li>OctGPT采用了序列化八叉树表示法，有效捕捉3D形状的层次结构和空间结构。</li>
<li>OctGPT通过与扩散模型的竞争，展现了强大的性能。</li>
<li>通过采用基于八叉树的变压器和其他创新技术，OctGPT减少了训练和生成时间。</li>
<li>OctGPT具有出色的多功能性，支持文本、草图和图像条件生成，以及场景级别合成。</li>
<li>OctGPT可以在不同任务中生成高质量、可伸缩的3D内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-510495a30a76a8367e0998c6e8924c97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee13c13531eb57544bc391d2986841b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5797e9b6bff5f87db57e40e7fd37c868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5758c207f83189b9417bd59194aa3ec8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPICE-A-Synergistic-Precise-Iterative-and-Customizable-Image-Editing-Workflow"><a href="#SPICE-A-Synergistic-Precise-Iterative-and-Customizable-Image-Editing-Workflow" class="headerlink" title="SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing   Workflow"></a>SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing   Workflow</h2><p><strong>Authors:Kenan Tang, Yanhong Li, Yao Qin</strong></p>
<p>Recent prompt-based image editing models have demonstrated impressive prompt-following capability at structural editing tasks. However, existing models still fail to perform local edits, follow detailed editing prompts, or maintain global image quality beyond a single editing step. To address these challenges, we introduce SPICE, a training-free workflow that accepts arbitrary resolutions and aspect ratios, accurately follows user requirements, and improves image quality consistently during more than 100 editing steps. By synergizing the strengths of a base diffusion model and a Canny edge ControlNet model, SPICE robustly handles free-form editing instructions from the user. SPICE outperforms state-of-the-art baselines on a challenging realistic image-editing dataset consisting of semantic editing (object addition, removal, replacement, and background change), stylistic editing (texture changes), and structural editing (action change) tasks. Not only does SPICE achieve the highest quantitative performance according to standard evaluation metrics, but it is also consistently preferred by users over existing image-editing methods. We release the workflow implementation for popular diffusion model Web UIs to support further research and artistic exploration. </p>
<blockquote>
<p>近期基于提示的图像编辑模型在结构编辑任务中展现出了令人印象深刻的遵循提示能力。然而，现有模型仍然无法执行局部编辑、遵循详细的编辑提示或在单步编辑之外保持全局图像质量。为了应对这些挑战，我们引入了SPICE，这是一种无需训练的工作流程，可接受任意分辨率和长宽比，能准确地遵循用户需求，并在超过100步编辑过程中持续提高图像质量。SPICE通过结合基础扩散模型和Canny边缘ControlNet模型的优点，稳健地处理用户的自由形式编辑指令。在包含语义编辑（对象添加、删除、替换和背景更改）、风格编辑（纹理更改）和结构编辑（动作更改）任务的挑战性现实图像编辑数据集上，SPICE超越了最新基线。根据标准评估指标，SPICE不仅达到了最高的量化性能，而且相较于现有图像编辑方法，用户也始终更偏爱使用SPICE。我们为流行的扩散模型Web界面发布了工作流程实现，以支持进一步的研究和艺术探索。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09697v1">PDF</a> 24 pages, 21 figures. Figure 9(b) has been accepted by CVPR AI Art   Gallery 2025</p>
<p><strong>Summary</strong></p>
<p>现有基于prompt的图像编辑模型在处理结构编辑任务时具有良好的表现，但在进行局部编辑、遵循详细编辑提示和维持图像全局质量方面存在不足。为解决这些问题，我们提出了SPICE训练无流程模型，该模型支持任意分辨率和比例，能够准确遵循用户需求，并在超过100步的编辑过程中持续提高图像质量。通过结合基础扩散模型和Canny边缘ControlNet模型的优点，SPICE能够稳健地处理用户的自由形式编辑指令。SPICE在包含语义编辑、风格编辑和结构编辑任务的挑战性真实图像编辑数据集上优于最新基线技术。它不仅在标准评估指标上取得了最高的定量性能，而且相较于现有图像编辑方法更受用户青睐。我们为流行的扩散模型Web UI发布了工作流程实现，以支持进一步的研究和艺术探索。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPICE是一个训练无流程的模型，用于解决现有图像编辑模型在局部编辑、遵循详细编辑提示和维持全局图像质量方面的不足。</li>
<li>SPICE支持任意分辨率和比例，能够准确遵循用户的自由形式编辑指令。</li>
<li>SPICE结合了基础扩散模型和Canny边缘ControlNet模型的优点，以处理复杂的图像编辑任务。</li>
<li>SPICE在包含语义编辑、风格编辑和结构编辑任务的图像编辑数据集上表现优异。</li>
<li>SPICE不仅在标准评估指标上取得最高定量性能，还获得了用户的青睐。</li>
<li>SPICE模型对于进一步的科研和艺术探索具有很高的支持性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09697">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9d733997d4021412c71a89a52f473785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51a4a634b6034ac528e134ceee3ba6ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9692e747204189d1a500c74692cc4a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a42134c8e30e1bedf36fd77f717e482b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d269ea27a84b11a9719118ad2165a807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e8a39520aa4b36df121967c4971710.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Early-Bird-Diffusion-Investigating-and-Leveraging-Timestep-Aware-Early-Bird-Tickets-in-Diffusion-Models-for-Efficient-Training"><a href="#Early-Bird-Diffusion-Investigating-and-Leveraging-Timestep-Aware-Early-Bird-Tickets-in-Diffusion-Models-for-Efficient-Training" class="headerlink" title="Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware   Early-Bird Tickets in Diffusion Models for Efficient Training"></a>Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware   Early-Bird Tickets in Diffusion Models for Efficient Training</h2><p><strong>Authors:Lexington Whalen, Zhenbang Du, Haoran You, Chaojian Li, Sixu Li,  Yingyan,  Lin</strong></p>
<p>Training diffusion models (DMs) requires substantial computational resources due to multiple forward and backward passes across numerous timesteps, motivating research into efficient training techniques. In this paper, we propose EB-Diff-Train, a new efficient DM training approach that is orthogonal to other methods of accelerating DM training, by investigating and leveraging Early-Bird (EB) tickets – sparse subnetworks that manifest early in the training process and maintain high generation quality.   We first investigate the existence of traditional EB tickets in DMs, enabling competitive generation quality without fully training a dense model.   Then, we delve into the concept of diffusion-dedicated EB tickets, drawing on insights from varying importance of different timestep regions. These tickets adapt their sparsity levels according to the importance of corresponding timestep regions, allowing for aggressive sparsity during non-critical regions while conserving computational resources for crucial timestep regions.   Building on this, we develop an efficient DM training technique that derives timestep-aware EB tickets, trains them in parallel, and combines them during inference for image generation. Extensive experiments validate the existence of both traditional and timestep-aware EB tickets, as well as the effectiveness of our proposed EB-Diff-Train method. This approach can significantly reduce training time both spatially and temporally – achieving 2.9$\times$ to 5.8$\times$ speedups over training unpruned dense models, and up to 10.3$\times$ faster training compared to standard train-prune-finetune pipelines – without compromising generative quality.   Our code is available at <a target="_blank" rel="noopener" href="https://github.com/GATECH-EIC/Early-Bird-Diffusion">https://github.com/GATECH-EIC/Early-Bird-Diffusion</a>. </p>
<blockquote>
<p>训练扩散模型（DMs）需要大量的计算资源，因为需要在多个时间步上进行多次正向和反向传递，这促使人们研究高效的训练技术。在本文中，我们提出了EB-Diff-Train，这是一种新的高效DM训练方法，它与其他加速DM训练的方法正交，通过研究和利用早期鸟（EB）门票——稀疏子网络，这些子网络在训练过程中早期出现并保持良好的生成质量。首先，我们研究了DM中传统EB门票的存在，能够在不全面训练密集模型的情况下实现有竞争力的生成质量。然后，我们深入研究了扩散专用EB门票的概念，从不同时间步区域的重要性中汲取见解。这些门票根据相应时间步区域的重要性调整其稀疏度，在非关键区域实现大胆的稀疏性，同时保留计算资源用于关键时间步区域。在此基础上，我们开发了一种高效的DM训练技术，该技术派生出时间感知EB门票，并行训练它们，并在图像生成时进行组合推理。大量实验验证了传统和时间感知EB门票的存在以及我们提出的EB-Diff-Train方法的有效性。这种方法可以在空间和时间上显著减少训练时间——相对于未修剪的密集模型实现2.9至5.8倍的速度提升，与标准的训练-修剪-微调管道相比，训练速度最快可提高10.3倍——且不影响生成质量。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/GATECH-EIC/Early-Bird-Diffusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GATECH-EIC/Early-Bird-Diffusion找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09606v1">PDF</a> 10 pages, 5 figures. Accepted to the IEEE&#x2F;CVF Conference on Computer   Vision and Pattern Recognition 2025</p>
<p><strong>摘要</strong></p>
<p>本文提出一种高效的扩散模型（DM）训练新方法EB-Diff-Train。该方法通过探究和利用早期鸟（EB）票，即训练过程中早期出现的稀疏子网络并保持高生成质量，来提高DM的训练效率。研究内容包括：探究DM中传统EB票的存在，使在不完全训练密集模型的情况下也能实现竞争的生成质量；深入研究扩散专用的EB票，根据不同时间步区域的重要性调整其稀疏水平，实现非关键区域的激进稀疏，同时保留关键时间步区域的计算资源。在此基础上，开发了一种高效的DM训练技术，该技术产生时间步感知的EB票，并行训练，并在图像生成时进行推理组合。实验验证了传统和时间步感知的EB票的存在以及EB-Diff-Train方法的有效性。该方法可在时间和空间上显著减少训练时间，与未修剪的密集模型相比，最高可达5.8倍的速度提升，与标准的训练-修剪-微调管道相比，最高可达10.3倍的训练速度提升，同时不损害生成质量。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型（DMs）的训练需要大量计算资源，由于其在多个时间步长上的前向和反向传递。</li>
<li>EB-Diff-Train是一种新的高效DM训练方法，通过利用早期鸟（EB）票来提高训练效率。</li>
<li>研究确认了传统EB票在DM中的存在，即使在不完全训练的情况下也能实现高质量的生成。</li>
<li>引入了扩散专用的EB票概念，根据时间步区域的重要性调整稀疏性。</li>
<li>方法实现了在关键和非关键时间步区域的资源优化分配，允许在非关键区域进行更大的稀疏性。</li>
<li>EB-Diff-Train方法显著提高了DM的训练效率，与未修剪的密集模型相比，最高可提高5.8倍的速度。</li>
<li>该方法在不损害生成质量的情况下实现了快速训练，代码已公开发布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09606">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9763a500e82eb85c18ed2c1c75fa70bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-702c59cbb43456321fc1f35f8f537c9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e55393bfec27d74e081507bfda62235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2652e91f1b54c12eae4600df56502ab8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-993db11d4b3776315b2837e8e261af2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b7f5a63d9a4a4a5b5711451f332cc8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DiffuMural-Restoring-Dunhuang-Murals-with-Multi-scale-Diffusion"><a href="#DiffuMural-Restoring-Dunhuang-Murals-with-Multi-scale-Diffusion" class="headerlink" title="DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion"></a>DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion</h2><p><strong>Authors:Puyu Han, Jiaju Kang, Yuhang Pan, Erting Pan, Zeyu Zhang, Qunchao Jin, Juntao Jiang, Zhichen Liu, Luqi Gong</strong></p>
<p>Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics. </p>
<blockquote>
<p>大规模预训练扩散模型在条件图像生成领域取得了优异的结果。然而，古壁画修复作为该领域的一个重要下游任务，由于缺陷面积大、训练样本稀少，对基于扩散模型的修复方法构成了重大挑战。条件修复任务更关心修复部分是否符合壁画修复的审美标准，涉及整体风格和细节接缝等方面，而当前研究中缺乏评估启发式图像补充的指标。因此，我们提出了DiffuMural，它是一种结合多尺度收敛和协作扩散机制的方法，通过ControlNet和循环一致性损失来优化生成图像与条件控制之间的匹配。DiffuMural在壁画修复方面表现出卓越的能力，利用来自23幅大型敦煌壁画的一致视觉美学训练数据。该模型在恢复细节、实现整体外观连贯性，以及解决因缺乏事实依据而不完整的壁画所带来的独特挑战方面表现出色。我们的评估框架包含了四个关键指标来定量评估不完整的壁画：事实准确性、纹理细节、上下文语义和整体视觉连贯性。此外，我们融入了人文价值评估，以确保修复的壁画保留其文化和艺术意义。大量实验证明，我们的方法在定性和定量指标上均优于当前先进技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09513v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型预训练扩散模型的优秀表现，本研究提出针对古壁画修复的任务，构建了一种多尺度收敛与协同扩散机制的扩散模型DiffuMural。结合ControlNet和循环一致性损失优化生成图像与条件控制的匹配。在壁画修复领域展现卓越能力，利用来自敦煌的大型壁画训练数据，实现细节恢复、整体外观连贯性和对缺乏事实依据的不完整壁画的独特挑战。评价框架包括四项关键指标和人类价值评估，以确保修复的壁画保留其文化和艺术意义。本研究的方法在定性和定量指标上均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型预训练扩散模型在条件图像生成领域表现出色。</li>
<li>古壁画修复是扩散模型面临的重要下游任务，存在大面积缺陷和稀缺训练样本的挑战。</li>
<li>DiffuMural模型结合了多尺度收敛和协同扩散机制，优化生成图像与条件控制的匹配。</li>
<li>DiffuMural在壁画修复领域展现卓越能力，利用来自敦煌的大型壁画训练数据。</li>
<li>评估框架包括四项关键指标：事实准确性、纹理细节、上下文语义和整体视觉连贯性。</li>
<li>引入人文价值评估确保修复的壁画保留文化和艺术意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc6031a67bc93f8d364594b2685b576f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c4ef794574acdb1e4a0bda7c86ab2c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff47012dd38c265ee8531860c2e0fb8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a103ed427c9b92029565b4900ca7ea90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fe5211deff2f538dec76761f91523cf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="D-2-iT-Dynamic-Diffusion-Transformer-for-Accurate-Image-Generation"><a href="#D-2-iT-Dynamic-Diffusion-Transformer-for-Accurate-Image-Generation" class="headerlink" title="D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation"></a>D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation</h2><p><strong>Authors:Weinan Jia, Mengqi Huang, Nan Chen, Lei Zhang, Zhendong Mao</strong></p>
<p>Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D$^2$iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/jiawn-creator/Dynamic-DiT">https://github.com/jiawn-creator/Dynamic-DiT</a>. </p>
<blockquote>
<p>扩散模型因其生成高保真图像的能力而受到广泛认可。尽管扩散变压器（DiT）架构具有出色的性能和可扩展性，但在扩散过程中，它在不同的图像区域应用固定压缩，而忽略了这些区域中存在的天然信息密度差异。然而，大压缩会导致局部真实感有限，而小压缩会增加计算复杂性并损害全局一致性，最终影响生成的图像质量。为了解决这些局限性，我们提出了动态压缩不同图像区域的策略，通过识别不同区域的重要性，并引入了一种新的两阶段框架，旨在提高图像生成的有效性和效率：（1）第一阶段采用动态变分自编码器（DVAE），使用分层编码器以不同的下采样率编码不同的图像区域，以适应其特定的信息密度，从而为扩散过程提供更准确、更自然的潜在代码。（2）第二阶段采用动态扩散变压器（D^2iT），通过预测多粒度噪声生成图像，包括粗粒度（平滑区域中较少的潜在代码）和细粒度（详细区域中更多的潜在代码），这通过动态粒度和动态内容的变压器的组合来实现。结合噪声的粗略预测和详细区域的校正策略实现了全局一致性和局部真实感的统一。在各种生成任务上的综合实验验证了我们的方法的有效性。代码将在<a target="_blank" rel="noopener" href="https://github.com/jiawn-creator/Dynamic-DiT%E4%B8%8A%E9%87%8A%E5%B8%83%E3%80%82">https://github.com/jiawn-creator/Dynamic-DiT上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09454v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了针对Diffusion Transformer（DiT）模型的改进方案，解决了其在图像生成过程中对不同区域进行固定压缩的问题。通过动态识别图像区域的重要性，实现区域动态压缩，并引入两阶段框架，包括动态VAE（DVAE）和动态扩散变压器（D^2iT）。DVAE通过分层编码器按不同下采样率编码图像区域，D^2iT则通过预测多粒度噪声实现全局一致性和局部现实性的统一。实验验证该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion models can generate high-fidelity images but face challenges in balancing local realism and global consistency.</li>
<li>DiT模型在扩散过程中对不同图像区域应用固定压缩，忽略了自然信息密度的差异。</li>
<li>大压缩会导致局部现实感有限，小压缩则增加计算复杂性并影响全局一致性。</li>
<li>本文提出了动态压缩不同图像区域的方法，以识别各区域的重要性。</li>
<li>引入两阶段框架，包括DVAE和D^2iT，增强图像生成的有效性和效率。</li>
<li>DVAE使用分层编码器，按图像区域的信息密度进行下采样编码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09454">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-63927cca7783a51ea951e7db2efad9b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96186962ca7f3884c97dca6eb530fffd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-465429dcf7afb5f4efc867f6c82bc0a6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Structure-Accurate-Medical-Image-Translation-based-on-Dynamic-Frequency-Balance-and-Knowledge-Guidance"><a href="#Structure-Accurate-Medical-Image-Translation-based-on-Dynamic-Frequency-Balance-and-Knowledge-Guidance" class="headerlink" title="Structure-Accurate Medical Image Translation based on Dynamic Frequency   Balance and Knowledge Guidance"></a>Structure-Accurate Medical Image Translation based on Dynamic Frequency   Balance and Knowledge Guidance</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Multimodal medical images play a crucial role in the precise and comprehensive clinical diagnosis. Diffusion model is a powerful strategy to synthesize the required medical images. However, existing approaches still suffer from the problem of anatomical structure distortion due to the overfitting of high-frequency information and the weakening of low-frequency information. Thus, we propose a novel method based on dynamic frequency balance and knowledge guidance. Specifically, we first extract the low-frequency and high-frequency components by decomposing the critical features of the model using wavelet transform. Then, a dynamic frequency balance module is designed to adaptively adjust frequency for enhancing global low-frequency features and effective high-frequency details as well as suppressing high-frequency noise. To further overcome the challenges posed by the large differences between different medical modalities, we construct a knowledge-guided mechanism that fuses the prior clinical knowledge from a visual language model with visual features, to facilitate the generation of accurate anatomical structures. Experimental evaluations on multiple datasets show the proposed method achieves significant improvements in qualitative and quantitative assessments, verifying its effectiveness and superiority. </p>
<blockquote>
<p>多模态医学图像在精确和全面的临床诊断和治疗中发挥着至关重要的作用。扩散模型是合成所需医学图像的一种强大策略。然而，现有方法仍存在因高频信息过度拟合而导致解剖结构扭曲的问题，以及低频信息减弱的问题。因此，我们提出了一种基于动态频率平衡和知识指导的新型方法。具体来说，我们首先通过小波变换分解模型的关键特征，提取低频和高频成分。然后，设计了一个动态频率平衡模块，以自适应地调整频率，增强全局低频特征和有效的高频细节，同时抑制高频噪声。为了克服不同医学模态之间差异巨大所带来的挑战，我们构建了一个知识引导机制，该机制融合了视觉语言模型的先验临床知识与视觉特征，以促进准确解剖结构的生成。在多个数据集上的实验评估表明，该方法在定性和定量评估方面取得了显著的改进，验证了其有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09441v1">PDF</a> Medical image translation, Diffusion model, 16 pages</p>
<p><strong>Summary</strong></p>
<p>基于动态频率平衡和知识指导的医学图像合成方法，解决了现有扩散模型在医学图像合成中因高频信息过拟合和低频信息减弱导致的解剖结构失真问题。通过小波变换提取图像的低频和高频成分，设计动态频率平衡模块自适应调整频率，增强全局低频特征和有效高频细节，同时抑制高频噪声。此外，构建知识引导机制，融合视觉语言模型的先验临床知识，促进生成准确的解剖结构。实验评估显示，该方法在多个数据集上实现了显著的改进，验证了其有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态医学图像在临床诊断中具有重要作用，扩散模型是合成医学图像的有效策略。</li>
<li>现有方法存在解剖结构失真问题，主要由于高频信息过拟合和低频信息减弱。</li>
<li>引入动态频率平衡模块，自适应调整频率，增强低频和有效高频细节，抑制高频噪声。</li>
<li>提出知识引导机制，融合先验临床知识，促进准确解剖结构的生成。</li>
<li>方法在多个数据集上进行实验评估，实现显著改进。</li>
<li>方法的优点在于解决了现有问题，提高了医学图像合成的准确性和质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-59c78c60ea084f22d34cd5f9d3ea7a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b404e101c127f39edab2fb3f81f3efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13b2cc77bb25f602d4436a16eb69167f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f84f0c62d5592bddc9f43b73e0577f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ce23a5ca6d3d3889891fca53d0d3f5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Text-To-3D-Object-Generation-For-Scalable-Room-Assembly"><a href="#Text-To-3D-Object-Generation-For-Scalable-Room-Assembly" class="headerlink" title="Text To 3D Object Generation For Scalable Room Assembly"></a>Text To 3D Object Generation For Scalable Room Assembly</h2><p><strong>Authors:Sonia Laguna, Alberto Garcia-Garcia, Marie-Julie Rakotosaona, Stylianos Moschoglou, Leonhard Helminger, Sergio Orts-Escolano</strong></p>
<p>Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications. </p>
<blockquote>
<p>现代用于场景理解的机器学习模型，如深度估计和对象跟踪，依赖于模仿真实世界部署场景的大规模、高质量数据集。为了解决数据稀缺的问题，我们提出了一种用于生成可扩展、高质量和可定制的3D室内场景的合成数据端到端系统。该系统通过整合和适应文本到图像和多视角扩散模型，以及基于神经辐射场的网格化技术，从文本提示生成高保真3D对象资产，并使用渲染工具将其融入到预先定义的平面布局中。通过引入新颖的损失函数和培训策略到现有方法中，该系统支持按需场景生成，旨在缓解当前可用数据的稀缺性，这些现有数据通常需要艺术家手动制作。此系统推动了合成数据在解决机器学习训练限制方面的作用，为真实世界应用启用了更稳健和可推广的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09328v1">PDF</a> Published at the ICLR 2025 Workshop on Synthetic Data</p>
<p><strong>Summary</strong></p>
<p>本文提出一种端到端的合成数据生成系统，用于生成可扩展、高质量、可定制的3D室内场景。该系统结合了文本到图像和多视角扩散模型与基于神经辐射场的网格化技术，通过文本提示生成高保真度的3D对象资产，并将其融入预定义的平面布局中。引入新颖的损失函数和培训策略，该系统支持按需生成场景，旨在解决当前可用数据的稀缺问题，通常这些数据是由艺术家手工制作的。此系统推动了合成数据在解决机器学习训练限制方面的作用，为真实世界应用提供了更稳健和可推广的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种端到端的合成数据生成系统，用于生成3D室内场景。</li>
<li>系统结合了文本到图像和多视角扩散模型技术。</li>
<li>利用神经辐射场基于网格化的技术生成高保真度的3D对象资产。</li>
<li>系统支持将生成的3D对象资产融入预定义的平面布局中。</li>
<li>通过引入新颖的损失函数和培训策略，支持按需生成场景。</li>
<li>该系统旨在解决当前机器学习训练数据的稀缺问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09328">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-909b61142337e1325ad8b9399a253bc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb5ee585dd7073b29a390a5c9f94e097.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f4f660c7064558f8c84b1663a601822.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da3ed3c968d77d763fa6f1f93f7ff9d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f41f1f3cbcbd6ad8c6d0fc4948fc08a4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-Visual-Explanations-to-Counterfactual-Explanations-with-Latent-Diffusion"><a href="#From-Visual-Explanations-to-Counterfactual-Explanations-with-Latent-Diffusion" class="headerlink" title="From Visual Explanations to Counterfactual Explanations with Latent   Diffusion"></a>From Visual Explanations to Counterfactual Explanations with Latent   Diffusion</h2><p><strong>Authors:Tung Luu, Nam Le, Duc Le, Bac Le</strong></p>
<p>Visual counterfactual explanations are ideal hypothetical images that change the decision-making of the classifier with high confidence toward the desired class while remaining visually plausible and close to the initial image. In this paper, we propose a new approach to tackle two key challenges in recent prominent works: i) determining which specific counterfactual features are crucial for distinguishing the “concept” of the target class from the original class, and ii) supplying valuable explanations for the non-robust classifier without relying on the support of an adversarially robust model. Our method identifies the essential region for modification through algorithms that provide visual explanations, and then our framework generates realistic counterfactual explanations by combining adversarial attacks based on pruning the adversarial gradient of the target classifier and the latent diffusion model. The proposed method outperforms previous state-of-the-art results on various evaluation criteria on ImageNet and CelebA-HQ datasets. In general, our method can be applied to arbitrary classifiers, highlight the strong association between visual and counterfactual explanations, make semantically meaningful changes from the target classifier, and provide observers with subtle counterfactual images. </p>
<blockquote>
<p>视觉反事实解释是一种理想的假设图像，它能够以高信心改变分类器的决策，使决策转向目标类别，同时保持视觉上的合理性和接近原始图像。在本文中，我们提出了一种新方法来解决最近热门工作中的两个关键挑战：一）确定哪些特定的反事实特征对于区分目标类别的“概念”与原始类别至关重要；二）在不依赖对抗性稳健模型的支持下，为不稳健的分类器提供有价值的解释。我们的方法通过提供视觉解释的算法来确定需要修改的关键区域，然后我们的框架通过结合对抗性攻击和基于目标分类器的对抗性梯度的修剪以及潜在扩散模型，生成现实的反事实解释。该方法在ImageNet和CelebA-HQ数据集上的各种评价标准上均优于以前的最先进结果。总的来说，我们的方法可以应用于任意分类器，突出了视觉和反事实解释之间的强烈关联，使目标分类器能够进行语义上有意义的变化，并为观察者提供微妙的反事实图像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09202v1">PDF</a> 2025 IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision   (WACV)</p>
<p><strong>Summary</strong><br>     本文提出了视觉反事实解释的新方法，旨在解决两个关键问题：确定目标类“概念”与原始类之间的关键反事实特征，以及为非稳健分类器提供有价值的解释而不依赖于对抗稳健模型的支持。该方法通过提供视觉解释的算法确定修改的关键区域，然后通过结合对抗性攻击和潜在扩散模型生成逼真的反事实解释。在ImageNet和CelebA-HQ数据集上，该方法优于现有技术，可应用于任意分类器，强调视觉与反事实解释之间的强烈关联，为观察者提供微妙的反事实图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉反事实解释是通过生成理想假设图像来改变分类器的决策，使决策更有信心地朝向目标类别，同时保持视觉上的合理性和接近原始图像。</li>
<li>该方法解决了两个关键挑战：确定目标类别与原始类别之间的关键反事实特征，以及为非稳健分类器提供解释而不依赖对抗稳健模型。</li>
<li>方法通过提供视觉解释的算法来确定修改的关键区域。</li>
<li>结合对抗性攻击和潜在扩散模型生成逼真的反事实解释。</li>
<li>该方法在多个评估标准上优于现有技术，适用于各种图像数据集。</li>
<li>方法可应用于任意分类器，强调视觉与反事实解释之间的强烈关联。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a467d853ecc29cb6b20f76c982c3540d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9b896df3886040e664b71c62160d84e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0b8e3f9410881fb82a094b4496c31c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e4fff29e56740a4454d94c5bda2626f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Sculpting-Memory-Multi-Concept-Forgetting-in-Diffusion-Models-via-Dynamic-Mask-and-Concept-Aware-Optimization"><a href="#Sculpting-Memory-Multi-Concept-Forgetting-in-Diffusion-Models-via-Dynamic-Mask-and-Concept-Aware-Optimization" class="headerlink" title="Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via   Dynamic Mask and Concept-Aware Optimization"></a>Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via   Dynamic Mask and Concept-Aware Optimization</h2><p><strong>Authors:Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma</strong></p>
<p>Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose \textbf{Dynamic Mask coupled with Concept-Aware Loss}, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our \textbf{Dynamic Mask} mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our \textbf{Concept-Aware Loss} explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. The code will be released publicly. </p>
<blockquote>
<p>文本到图像（T2I）的扩散模型已经从文本提示生成高质量图像方面取得了显著的成就。然而，它们存储大量知识的能力在一些需要选择性遗忘的场景中引发了关注，例如去除版权内容、减少偏见或消除有害概念。现有的遗忘方法虽然能移除某些概念，但由于不稳定、残留知识持续存在以及生成质量下降，它们在多概念遗忘方面遇到了困难。为了应对这些挑战，我们提出了名为“动态掩码与概念感知损失”的新型遗忘框架，旨在处理扩散模型中的多概念遗忘问题。我们的“动态掩码”机制能基于当前优化状态自适应地更新梯度掩码，允许选择性地修改权重以避免干扰无关知识。此外，我们的“概念感知损失”通过超类对齐明确指导遗忘过程，同时通过基于知识蒸馏的正则化损失确保先前未学习的概念在连续遗忘过程中保持遗忘。我们进行了大量实验来评估我们的方法。结果表明，我们的方法在遗忘有效性、输出保真度和语义连贯性方面优于现有遗忘技术，特别是在多概念场景中。我们的工作为生成模型中的稳定和高保真遗忘提供了一个有原则和灵活性的框架。代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09039v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）的扩散模型在根据文本提示生成高质量图像方面取得了显著的成功。然而，其存储大量知识的能力在某些需要选择性遗忘的场景中引发了担忧，如去除版权内容、减少偏见或消除有害概念。为了应对挑战，我们提出了一个名为“动态掩膜与概念感知损失”的新型遗忘框架，用于处理扩散模型中的多概念遗忘问题。我们的动态掩膜机制能够根据当前的优化状态自适应地更新梯度掩膜，实现选择性权重修改，避免干扰无关知识。同时，我们的概念感知损失通过超类对齐方式明确指导遗忘过程，保证语义一致性。通过大量实验评估，我们的方法展现出优异的遗忘效果、输出保真度和语义连贯性，尤其在多概念场景中表现尤为突出。我们的研究为生成模型中的稳定和高保真遗忘提供了一个有原则和灵活性的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像的扩散模型在生成高质量图像方面取得了显著成功。</li>
<li>在需要选择性遗忘的场景中（如去除版权内容、减少偏见等），这些模型的挑战在于如何实现多概念遗忘。</li>
<li>现有遗忘方法在处理多概念遗忘时面临稳定性、残留知识持久性和生成质量下降的问题。</li>
<li>提出的“动态掩膜与概念感知损失”框架旨在解决这些挑战。</li>
<li>动态掩膜机制能够自适应更新梯度掩膜，实现选择性权重修改，避免干扰无关知识。</li>
<li>概念感知损失通过超类对齐明确指导遗忘过程，确保语义一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09039">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-60dca484263aab096a5b3b0efaa88f28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-711937d5ced94a074bb100564205b892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac74c88ecfc93593677a68b0b1c2a301.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1168143a7f02442c618bad9f662946ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd12f9695ff1384d6d994cc72ec83391.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models"><a href="#ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models" class="headerlink" title="ID-Booth: Identity-consistent Face Generation with Diffusion Models"></a>ID-Booth: Identity-consistent Face Generation with Diffusion Models</h2><p><strong>Authors:Darian Tomašević, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Štruc, Peter Peer</strong></p>
<p>Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at <a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>. </p>
<blockquote>
<p>近期生成建模技术的进步使得能够生成适用于多种领域的高质量合成数据，其中包括人脸识别。在这里，最先进的生成模型通常依赖于强大预训练扩散模型的条件和微调，以促进合成具有所需身份的逼真图像。然而，这些模型在训练过程中往往不考虑主体的身份，导致生成身份与预期身份之间的一致性较差。相比之下，采用基于身份的训练目标的方法往往会在身份的各个方面出现过拟合现象，进而降低了可生成图像的多样性。为了解决这些问题，我们在本文中提出了一种基于扩散的新型生成框架，称为ID-Booth。ID-Booth由一个负责数据生成的降噪网络、一个用于将图像映射到低维潜在空间及其反向映射的变分自动编码器和一个文本编码器组成，该文本编码器可实现基于提示对生成过程的控制。该框架利用了一种新型的三重身份训练目标，能够在保持预训练扩散模型的合成能力的同时，实现身份一致的图像生成。使用最先进的潜在扩散模型和多种提示进行的实验表明，我们的方法在身份内部一致性和身份间可分性方面优于其他方法，同时实现了更高的图像多样性。此外，生成的数据能够有效增强小规模数据集，以隐私保护的方式训练性能更好的识别模型。ID-Booth框架的源代码可在<a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07392v2">PDF</a> IEEE International Conference on Automatic Face and Gesture   Recognition (FG) 2025, 14 pages</p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种基于扩散的新型生成模型ID-Booth，该模型结合了去噪网络、变分自编码器和文本编码器，能够在生成图像时实现基于身份的提示控制。ID-Booth采用新颖的三重身份训练目标，能够在保留预训练扩散模型的合成能力的同时，实现身份一致的图像生成。实验证明，与其他方法相比，ID-Booth在身份内一致性和身份间可分性方面表现更佳，同时保持了较高的图像多样性。这为小型数据集的有效扩充和性能更佳的隐私保护识别模型的训练提供了可能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>先进的生成模型通常依赖于预训练的扩散模型的调整和微调，以合成具有所需身份的逼真图像。</li>
<li>当前模型在训练期间很少考虑身份因素，导致生成身份与预期身份之间的一致性较差。</li>
<li>基于身份的训练目标方法往往过分关注身份的各个方面，从而降低了可生成图像的多样性。</li>
<li>ID-Booth通过结合去噪网络、变分自编码器和文本编码器，解决了上述问题。</li>
<li>ID-Booth采用新颖的三重身份训练目标，实现了身份一致的图像生成，同时保留了预训练扩散模型的合成能力。</li>
<li>实验表明，ID-Booth在保持图像多样性的同时，提高了身份内的一致性和身份间的可分性。</li>
<li>ID-Booth产生的数据可用于有效地扩充小型数据集，并以隐私保护的方式训练性能更佳的识别模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07392">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3054bab5118e188633e9900a2369beba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a1325fe4dfc46da49d23433810504b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-814022dbf79c1a1cc8eb9da0ac5986c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising"><a href="#Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising" class="headerlink" title="Solving Inverse Problems using Diffusion with Iterative Colored   Renoising"></a>Solving Inverse Problems using Diffusion with Iterative Colored   Renoising</h2><p><strong>Authors:Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter</strong></p>
<p>Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and “renoises” the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting “DDfire” offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval. Our implementation is at <a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire">https://github.com/matt-bendel/DDfire</a> </p>
<blockquote>
<p>使用预训练的扩散模型以无监督的方式解决成像反问题，需要在扩散反向过程中近似测量条件得分函数的梯度。我们显示现有方法产生的近似值相对较差，尤其是在反向过程的早期，因此，我们提出了一种新方法，该方法在每个扩散步骤中多次重新估计和“增加噪声”。我们称这种迭代方法为快速迭代重噪声化（FIRE），它注入有色噪声，以确保预训练的扩散模型始终看到白噪声，这与它的训练方式相符。然后，我们将FIRE嵌入到DDIM反向过程中，并证明由此产生的DDfire在多个线性反问题以及相位检索方面提供了最先进的准确性和运行时性能。我们的实现位于<a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire%E3%80%82">https://github.com/matt-bendel/DDfire。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17468v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>利用预训练的扩散模型以无监督的方式解决成像逆问题，需要在扩散逆过程中近似测量条件分数函数的梯度。现有方法产生的近似值较差，特别是在逆过程的早期，因此提出了一种新的方法——快速迭代重噪声法（FIRE），该方法在每个扩散步骤中多次重新估计和“加入噪声”。我们将FIRE嵌入到DDIM逆过程中，并证明“DDfire”在多个线性逆问题和相位检索方面提供了最先进的准确性和运行时效率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散模型可以无监督地解决成像逆问题。</li>
<li>需要近似测量条件分数函数的梯度在扩散逆过程中。</li>
<li>现有方法的近似值在扩散逆过程早期表现较差。</li>
<li>提出了一种新的方法——快速迭代重噪声法（FIRE），该方法在扩散过程的每个步骤中多次重新估计和加入噪声。</li>
<li>FIRE方法通过注入有色噪声，确保预训练的扩散模型始终看到白噪声，与其训练方式一致。</li>
<li>将FIRE嵌入到DDIM逆过程中，形成了“DDfire”方法。</li>
<li>“DDfire”在多个线性逆问题和相位检索方面表现出最先进的准确性和运行时效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ff568fabae0abd2cbb6e3227c56cdb32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f5a0d543b4b18350a3d7847871ef27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-905c2f5a68de50a71c724b8331d17d2f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation"><a href="#StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation" class="headerlink" title="StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation"></a>StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation</h2><p><strong>Authors:Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang</strong></p>
<p>Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGen’s superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen">https://zju3dv.github.io/StarGen</a>. </p>
<blockquote>
<p>近期重建和生成模型方面的进展极大地推动了场景重建和新颖视角生成的技术。然而，由于计算资源的限制，这些大型模型的每次推断都局限于一个小范围，使得大范围一致的场景生成面临挑战。为了解决这一问题，我们提出了StarGen，这是一个采用预训练视频扩散模型的新型框架，以自回归的方式进行大范围场景生成。每个视频片段的生成都基于空间相邻图像的3D变形和之前生成的片段的时间重叠图像，这提高了大范围场景生成中的时空一致性，并实现了精确的姿态控制。这种时空条件与各种输入条件兼容，能够促进多种任务，包括稀疏视图插值、永久视图生成和布局控制的城市生成。定量和定性评估表明，StarGen在可扩展性、保真度和姿态准确性方面优于最先进的方法。项目页面：<a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen%E3%80%82">https://zju3dv.github.io/StarGen。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05763v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型重建和生成模型的最新进展极大地改进了场景重建和新颖视角生成。然而，由于计算限制，这些大型模型的每次推断都局限于小范围，使得长距离一致场景生成面临挑战。为解决这个问题，我们提出了StarGen框架，它采用预训练的视频扩散模型进行自回归长距离场景生成。每个视频剪辑的生成基于空间相邻图像的3D变形和先前生成的剪辑中时间上重叠的图像，提高了长距离场景生成的时空一致性，并实现了精确的姿态控制。这种时空条件适用于各种输入条件，可以促进包括稀疏视图插值、永久视图生成和布局控制城市生成在内的各种任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型重建和生成模型的最新进展推动了场景重建和新颖视角生成的进步。</li>
<li>由于计算限制，现有模型在长距离场景生成方面面临挑战。</li>
<li>StarGen框架采用预训练的视频扩散模型进行自回归长距离场景生成。</li>
<li>StarGen利用空间相邻图像的3D变形和先前生成的剪辑中的重叠图像，提高时空一致性。</li>
<li>StarGen具备精确的姿态控制能力。</li>
<li>框架支持多种任务，包括稀疏视图插值、永久视图生成和布局控制城市生成等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-22b127f4087696eff2c46cbb2d3e8e10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cf0e8dcc9abb373f6f850fcd56b32a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1a7dec6813a97f986d23fee4a718fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a973b1603e00e9a3843ad9f288cc4aa7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Navigating-Image-Restoration-with-VAR’s-Distribution-Alignment-Prior"><a href="#Navigating-Image-Restoration-with-VAR’s-Distribution-Alignment-Prior" class="headerlink" title="Navigating Image Restoration with VAR’s Distribution Alignment Prior"></a>Navigating Image Restoration with VAR’s Distribution Alignment Prior</h2><p><strong>Authors:Siyang Wang, Feng Zhao</strong></p>
<p>Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VAR’s adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks. </p>
<blockquote>
<p>生成模型经过大量高质量数据集的训练，能够有效捕捉清洁图像的结构和统计特性，使其在图像恢复中将退化特征转换为清洁特征方面具有强大的先验能力。VAR作为一种新型图像生成范式，通过应用下一尺度预测方法，在生成质量上超越了扩散模型。它通过自回归过程逐步捕捉全局结构和精细细节，这与恢复社区广泛认可的多尺度恢复原则相一致。此外，我们在使用VAR进行图像重建过程时观察到，尺度预测会自动调制输入，便于后续尺度的表示与清洁图像分布的对齐。为了利用VAR在图像恢复任务中的自适应分布对齐能力，我们将VAR内的多尺度潜在表示作为恢复先验，从而推进了我们精心设计的VarFormer框架。这些先验的战略应用使我们的VarFormer在未见过任务上实现了显著泛化，同时降低了训练计算成本。大量实验证明，我们的VarFormer在各种恢复任务上优于现有的多任务图像恢复方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21063v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大规模高质量数据集训练的生成模型能有效捕捉清晰图像的结构和统计特性，可作为图像修复中将退化特征转化为清晰特征的强大先验。VAR作为一种新型图像生成范式，通过尺度预测方法，在生成质量上超越了扩散模型。VAR遵循广泛认可的图像修复多尺度恢复原则，逐步捕捉全局结构和精细细节。在利用VAR进行图像重建时，尺度预测会自动调整输入，促进后续尺度表示与干净图像分布的对齐。我们将VAR中的多尺度潜在表示作为修复先验，构建了精心设计的VarFormer框架，利用其自适应分布对齐能力。VarFormer在未见任务上实现了卓越泛化能力，同时降低了训练计算成本。实验表明，VarFormer在多种修复任务上优于现有多任务图像修复方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型基于大规模高质量数据集可有效捕捉图像结构和统计特性。</li>
<li>VAR成为新的图像生成范式，通过尺度预测提升生成质量。</li>
<li>VAR遵循多尺度恢复原则，能逐步捕捉全局结构和精细细节。</li>
<li>在图像重建过程中，VAR的尺度预测能自动调整输入，促进后续尺度与干净图像分布对齐。</li>
<li>VarFormer利用VAR中的多尺度潜在表示作为修复先验。</li>
<li>VarFormer实现卓越泛化能力并降低训练计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21063">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-460f3bbfb724c48a3f1a79775c244a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3874c7c5a9090b0a5be87bf228b5d2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a35552c3b292b12b5879f78487d231d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34a2362ccb0b54582a79c040db28c626.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97022af2dfa48cd126f6af7ccdfc2034.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Nearly-Zero-Cost-Protection-Against-Mimicry-by-Personalized-Diffusion-Models"><a href="#Nearly-Zero-Cost-Protection-Against-Mimicry-by-Personalized-Diffusion-Models" class="headerlink" title="Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion   Models"></a>Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion   Models</h2><p><strong>Authors:Namhyuk Ahn, KiYoon Yoo, Wonhyuk Ahn, Daesik Kim, Seung-Hun Nam</strong></p>
<p>Recent advancements in diffusion models revolutionize image generation but pose risks of misuse, such as replicating artworks or generating deepfakes. Existing image protection methods, though effective, struggle to balance protection efficacy, invisibility, and latency, thus limiting practical use. We introduce perturbation pre-training to reduce latency and propose a mixture-of-perturbations approach that dynamically adapts to input images to minimize performance degradation. Our novel training strategy computes protection loss across multiple VAE feature spaces, while adaptive targeted protection at inference enhances robustness and invisibility. Experiments show comparable protection performance with improved invisibility and drastically reduced inference time. The code and demo are available at <a target="_blank" rel="noopener" href="https://webtoon.github.io/impasto">https://webtoon.github.io/impasto</a> </p>
<blockquote>
<p>最近扩散模型的进展虽然为图像生成带来了革命性的变化，但也带来了滥用风险，比如复制艺术品或生成深度伪造图像。虽然现有图像保护方法有效，但在保护效果、隐形性和延迟之间难以平衡，从而限制了实际应用。我们引入扰动预训练以减少延迟，并提出一种混合扰动方法，该方法可动态适应输入图像，以最小化性能下降。我们新颖的训练策略计算了多个VAE特征空间中的保护损失，同时推理阶段的自适应目标保护增强了稳健性和隐形性。实验表明，保护性能相当，隐形性有所提高，推理时间大大减少。代码和演示可在<a target="_blank" rel="noopener" href="https://webtoon.github.io/impasto">https://webtoon.github.io/impasto</a>查看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11423v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>近期扩散模型在图像生成方面的进步推动了艺术创新，但同时也带来了滥用风险，如复制艺术品或生成深度伪造图像。现有图像保护方法在保护效果、隐形性和延迟之间难以平衡，限制了实际应用。我们引入扰动预训练以降低延迟，并提出混合扰动方法，动态适应输入图像以最小化性能下降。我们的新训练策略计算多个VAE特征空间中的保护损失，同时在推理时进行自适应靶向保护，以增强稳健性和隐形性。实验表明，保护性能相当，隐形性提高，推理时间大幅降低。相关代码和演示可在webtoon.github.io&#x2F;impasto上查看。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的最新进展推动了图像生成的发展，但存在滥用风险。</li>
<li>现有图像保护方法在平衡保护效果、隐形性和延迟方面存在困难。</li>
<li>引入扰动预训练以降低延迟和提高性能。</li>
<li>提出混合扰动方法，动态适应输入图像。</li>
<li>新训练策略计算多个VAE特征空间中的保护损失。</li>
<li>自适应靶向保护增强稳健性和隐形性。</li>
<li>实验显示保护性能相当，隐形性提高，推理时间减少。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11423">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b22c3a20357f03d1ffb3b40f79472fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6689a16fdb4c438f40447b6ba7878adc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a25e0a3fe7d8715d2288b31f628e940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd8d6741cb53e18f02af421d0a1e8843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b65add2678bc8eb9dbbe33652fc51cf8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GAF-Gaussian-Avatar-Reconstruction-from-Monocular-Videos-via-Multi-view-Diffusion"><a href="#GAF-Gaussian-Avatar-Reconstruction-from-Monocular-Videos-via-Multi-view-Diffusion" class="headerlink" title="GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view   Diffusion"></a>GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view   Diffusion</h2><p><strong>Authors:Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner</strong></p>
<p>We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices. </p>
<blockquote>
<p>我们提出了一种从单目视频重建可动画的3D高斯化身的新方法，这些视频是由智能手机等商品设备捕捉的。从这种记录中进行逼真的3D头部化身重建是一个挑战，因为观察有限，导致未观测区域约束不足，并在新视角中产生伪影。为了解决这个问题，我们引入了一个多视角头部扩散模型，利用其先验知识来填充缺失区域并确保高斯平铺渲染中的视角一致性。为了实现精确的观点控制，我们使用基于FLAME的头部重建渲染的法线图，这提供了像素对齐的归纳偏见。我们还根据输入图像提取的VAE特征对扩散模型进行条件处理，以保留面部身份和外观细节。对于高斯化身重建，我们通过使用迭代去噪图像作为伪真实值来提炼多视角扩散先验知识，有效地减轻过饱和问题。为了提高逼真度，我们应用潜在上采样先验知识来精细化去噪潜在代码，然后将其解码为图像。我们在NeRSemble数据集上评估了我们的方法，结果表明GAF在新型视角合成方面优于先前最先进的方法。此外，我们还展示了从单目视频捕捉的商品设备上的更高保真度化身重建。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10209v2">PDF</a> Paper Video: <a target="_blank" rel="noopener" href="https://youtu.be/QuIYTljvhyg">https://youtu.be/QuIYTljvhyg</a> Project Page:   <a target="_blank" rel="noopener" href="https://tangjiapeng.github.io/projects/GAF">https://tangjiapeng.github.io/projects/GAF</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的方法，利用单目视频和商品设备（如智能手机）来重建可动画的3D高斯头像。由于有限的观察角度带来的未观测区域约束不足的问题，传统的从单目视频进行真实感的3D头像重建会有很大的挑战。为解决这一问题，我们提出了一种多视角头部扩散模型，通过其先验知识填充缺失区域并确保高斯贴图渲染中的视角一致性。为了精确控制视角，我们采用基于FLAME的头部重建渲染法向量图，提供像素对齐的诱导偏差。此外，我们将扩散模型的条件设定为输入图像的VAE特征，以保留面部身份和外观细节。对于高斯头像重建，我们通过使用去噪图像作为伪真实值来提炼多视角扩散先验知识，有效缓解过度饱和问题。为进一步提高真实感，我们应用潜在上采样先验知识来细化去噪潜在编码，然后将其解码为图像。我们在NeRSemble数据集上评估了我们的方法，结果表明GAF在新型视角合成方面优于之前的最先进方法。此外，我们还展示了从单目视频捕获的商品设备中生成更高保真度的头像重建。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种利用单目视频和商品设备（智能手机）重建可动画的3D高斯头像的新方法。</li>
<li>通过引入多视角头部扩散模型解决了有限的观察角度带来的问题，填充缺失区域并确保视角一致性。</li>
<li>利用基于FLAME的头部重建渲染法向量图实现精确视角控制。</li>
<li>使用输入图像的VAE特征作为扩散模型的条件以保留面部身份和外观细节。</li>
<li>通过使用迭代去噪图像作为伪真实值提炼多视角扩散先验知识来缓解过度饱和问题。</li>
<li>应用潜在上采样先验知识提高重建头像的真实感。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10209">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a84f7e9e47425e042b2c5497db496304.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f0fc2007b008c3ee32c93478fbd1cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6f4cbd72dfce29c3660a4214a132a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fe1ac51560723367b066f9d3c914b58.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Conceptwm-A-Diffusion-Model-Watermark-for-Concept-Protection"><a href="#Conceptwm-A-Diffusion-Model-Watermark-for-Concept-Protection" class="headerlink" title="Conceptwm: A Diffusion Model Watermark for Concept Protection"></a>Conceptwm: A Diffusion Model Watermark for Concept Protection</h2><p><strong>Authors:Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu</strong></p>
<p>The personalization techniques of diffusion models succeed in generating specific concepts but also pose threats to copyright protection and illegal use. Model Watermarking is an effective method to prevent the unauthorized use of subject-driven or style-driven image generation, safeguarding concept copyrights. However, under the goal of concept-oriented protection, current watermarking schemes typically add watermarks to all images rather than applying them in a refined manner targeted at specific concepts. Additionally, the personalization techniques of diffusion models can easily remove watermarks. Existing watermarking methods struggle to achieve fine-grained watermark embedding with a few images of specific concept and prevent removal of watermarks through personalized fine-tuning. Therefore, we introduce a novel concept-oriented watermarking framework that seamlessly embeds imperceptible watermarks into the concept of diffusion models. We introduce Fidelity-preserving Latent Watermarking (FLW) to generate latent watermarks based on image characteristics and the Adversarial Watermarking Modulation module to prevent “jailbreaking” via personalized finetuning. To enhance U-Net’s efficiency in learning watermark patterns with limited samples, we propose Efficient Concept Watermark Finetuning, which alternates optimization of model parameters for both watermark embedding and concept learning. We conduct extensive experiments and ablation studies to verify our framework. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Conceptwm-4EB3/">https://anonymous.4open.science/r/Conceptwm-4EB3/</a>. </p>
<blockquote>
<p>个性化扩散模型的生成技术虽然能够成功生成特定概念，但也对版权保护和非法使用构成了威胁。模型水印是一种防止主题驱动或风格驱动图像生成未经授权使用的有效方法，保护概念版权。然而，在面向概念保护的目标下，当前的水印方案通常对所有图像都添加水印，而不是以精细的方式针对特定概念进行应用。此外，扩散模型的个性化技术很容易去除水印。现有的水印方法难以实现针对少数具有特定概念图像进行精细粒度水印嵌入，并防止通过个性化微调去除水印。因此，我们引入了一种新型的概念导向水印框架，无缝地将不可见水印嵌入到扩散模型的概念中。我们提出了基于图像特性的保真性保持潜在水印（FLW）生成潜在水印，并引入对抗性水印调制模块，以防止通过个性化微调进行“越狱”。为了提高U-Net在有限样本中学习水印模式的效率，我们提出了高效概念水印微调方法，该方法交替优化用于水印嵌入和概念学习的模型参数。我们进行了大量实验和消融研究来验证我们的框架。我们的代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Conceptwm-4EB3/">https://anonymous.4open.science/r/Conceptwm-4EB3/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11688v2">PDF</a> </p>
<p><strong>Summary</strong><br>     扩散模型的个性化技术在生成特定概念的同时，也对版权保护和非法使用构成了威胁。模型水印是一种防止滥用主题驱动或风格驱动图像生成的有效方法，保护概念版权。然而，在概念导向的保护目标下，当前的水印方案通常对所有图像都添加水印，而非针对特定概念进行精细应用。此外，扩散模型的个性化技术可以轻松去除水印。现有的水印方法难以实现少数具有特定概念图像的水印嵌入，并防止通过个性化微调去除水印。因此，我们引入了一种新的概念导向水印框架，无缝地将不可察觉的水印嵌入扩散模型的概念中。我们提出基于图像特性的保真性保留潜在水印（FLW）和对抗性水印调制模块，以防止个性化微调导致的“越狱”。为提高U-Net在学习水印模式时的效率并减少样本量限制，我们提出了高效概念水印微调方法，该方法交替优化水印嵌入和概念学习的模型参数。我们进行了大量实验和消融研究来验证我们的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的个人化技术在生成特定概念方面具有优势，但也带来版权保护和非法使用的风险。</li>
<li>模型水印是保护概念版权的有效方法，但现有方案在针对特定概念的精细保护上存在不足。</li>
<li>当前的水印容易被扩散模型的个性化技术去除。</li>
<li>提出一种新的概念导向水印框架，能够无缝嵌入不可察觉的水印在扩散模型的概念中。</li>
<li>引入保真性保留潜在水印（FLW）和对抗性水印调制模块来增强水印的稳固性和安全性。</li>
<li>提出高效概念水印微调方法以提高U-Net在学习水印模式时的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f928c16859bd027713a3d8a7dd2f6f93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63cd55547f012af34abca0e5852371c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71124a52048f4b2f8fdd40d7fc225916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bea4e165425e68a8be8413c08bf55697.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f31758c6328f74427e6375a52a2fd68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-220d339423fe6d3d137b5f3dab41aa8a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TPC-Test-time-Procrustes-Calibration-for-Diffusion-based-Human-Image-Animation"><a href="#TPC-Test-time-Procrustes-Calibration-for-Diffusion-based-Human-Image-Animation" class="headerlink" title="TPC: Test-time Procrustes Calibration for Diffusion-based Human Image   Animation"></a>TPC: Test-time Procrustes Calibration for Diffusion-based Human Image   Animation</h2><p><strong>Authors:Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, Chang D. Yoo</strong></p>
<p>Human image animation aims to generate a human motion video from the inputs of a reference human image and a target motion video. Current diffusion-based image animation systems exhibit high precision in transferring human identity into targeted motion, yet they still exhibit irregular quality in their outputs. Their optimal precision is achieved only when the physical compositions (i.e., scale and rotation) of the human shapes in the reference image and target pose frame are aligned. In the absence of such alignment, there is a noticeable decline in fidelity and consistency. Especially, in real-world environments, this compositional misalignment commonly occurs, posing significant challenges to the practical usage of current systems. To this end, we propose Test-time Procrustes Calibration (TPC), which enhances the robustness of diffusion-based image animation systems by maintaining optimal performance even when faced with compositional misalignment, effectively addressing real-world scenarios. The TPC provides a calibrated reference image for the diffusion model, enhancing its capability to understand the correspondence between human shapes in the reference and target images. Our method is simple and can be applied to any diffusion-based image animation system in a model-agnostic manner, improving the effectiveness at test time without additional training. </p>
<blockquote>
<p>人类图像动画旨在从参考人类图像和目标运动视频输入生成人类运动视频。当前的基于扩散的图像动画系统在人像转移到目标动作时表现出高精确度，但它们的输出质量仍存在不规则现象。它们的最佳精度只有在参考图像和目标姿势框架中的人类形状的物理组成（即比例和旋转）对齐时才能实现。在缺少这种对齐的情况下，保真度和一致性会明显下降。特别是在现实环境中，这种组成上的不对齐是常见的，给当前系统的实际应用带来了重大挑战。为此，我们提出了测试时校准（TPC），它通过保持最佳性能，即使面对组成上的不对齐，也增强了基于扩散的图像动画系统的稳健性，有效地解决了现实世界场景中的问题。TPC为扩散模型提供了一个校准后的参考图像，增强了其对参考图像和目标图像之间人类形状对应关系的理解。我们的方法简单，可以应用于任何基于扩散的图像动画系统，以模型无关的方式提高测试时的有效性，而无需额外的训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.24037v2">PDF</a> 24 pages, 16 figures, NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>本文讨论的是基于扩散模型的人体图像动画技术面临的挑战。由于参考图像和目标运动视频中的身体姿态组成（如尺度和旋转）未对齐时，现有系统的精度和一致性会明显下降。为解决这一问题，本文提出了一种名为Test-time Procrustes Calibration（TPC）的方法，增强了扩散模型在人体图像动画中的稳健性，确保在面临组成不匹配时仍能保持最佳性能。TPC通过为扩散模型提供校准后的参考图像，提高了其对参考图像和目标图像之间形状对应的理解。此方法简单且可应用于任何扩散模型的人体图像动画系统，无需额外训练即可提高测试效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人体图像动画旨在从参考图像和目标运动视频生成人体运动视频。</li>
<li>当前扩散模型在人体图像动画中虽具有高精度，但在输出质量方面仍存在不规则性。</li>
<li>扩散模型在参考图像和目标姿态帧的物理组成（如尺度和旋转）对齐时才能达到最佳精度。</li>
<li>在现实世界环境中，组成不匹配是常见的，给现有系统实用带来了挑战。</li>
<li>TPC方法提高了扩散模型在人体图像动画中的稳健性，以应对组成不匹配的问题。</li>
<li>TPC通过提供校准后的参考图像，增强了扩散模型对参考图像和目标图像之间形状对应的理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.24037">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-062c35ecd588fb3c0c229cd20da00031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37973ca83c8459a2812e10e841c47d36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c183f456bda7e60e1730bb8a783a263.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-125bcc55d1d2d498796afd6615e0d313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eedc805c5dc617220269bc4384042c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef4f3d2f4126a5f164e9f50e72a4984b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="On-the-Wasserstein-Convergence-and-Straightness-of-Rectified-Flow"><a href="#On-the-Wasserstein-Convergence-and-Straightness-of-Rectified-Flow" class="headerlink" title="On the Wasserstein Convergence and Straightness of Rectified Flow"></a>On the Wasserstein Convergence and Straightness of Rectified Flow</h2><p><strong>Authors:Vansh Bansal, Saptarshi Roy, Purnamrita Sarkar, Alessandro Rinaldo</strong></p>
<p>Diffusion models have emerged as a powerful tool for image generation and denoising. Typically, generative models learn a trajectory between the starting noise distribution and the target data distribution. Recently Liu et al. (2023b) proposed Rectified Flow (RF), a generative model that aims to learn straight flow trajectories from noise to data using a sequence of convex optimization problems with close ties to optimal transport. If the trajectory is curved, one must use many Euler discretization steps or novel strategies, such as exponential integrators, to achieve a satisfactory generation quality. In contrast, RF has been shown to theoretically straighten the trajectory through successive rectifications, reducing the number of function evaluations (NFEs) while sampling. It has also been shown empirically that RF may improve the straightness in two rectifications if one can solve the underlying optimization problem within a sufficiently small error. In this paper, we make two contributions. First, we provide a theoretical analysis of the Wasserstein distance between the sampling distribution of RF and the target distribution. Our error rate is characterized by the number of discretization steps and a novel formulation of straightness stronger than that in the original work. Secondly, we present general conditions guaranteeing uniqueness and straightness of 1-RF, which is in line with previous empirical findings. As a byproduct of our analysis, we show that, in one dimension, RF started at the standard Gaussian distribution yields the Monge map. Additionally, we also present empirical results on both simulated and real datasets to validate our theoretical findings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bansal-vansh/rectified-flow">https://github.com/bansal-vansh/rectified-flow</a>. </p>
<blockquote>
<p>扩散模型已经成为图像生成和去噪的强大工具。通常，生成模型学习从初始噪声分布到目标数据分布之间的轨迹。最近，Liu等人（2023b）提出了Rectified Flow（RF）这一生成模型，旨在通过一系列与最佳传输紧密相关的凸优化问题来学习从噪声到数据的直线轨迹。如果轨迹是弯曲的，必须使用许多欧拉离散化步骤或新型策略（如指数积分器）来实现令人满意的生成质量。相比之下，RF理论上有能力通过连续校正来使轨迹直线化，从而在采样时减少函数评估次数（NFEs）。此外，经验证据表明，如果在足够小的误差范围内解决底层优化问题，RF可能在两次校正中提高直线度。本文我们做出了两项贡献。首先，我们对RF采样分布与目标分布之间的Wasserstein距离进行了理论分析。我们的误差率由离散化步骤的数量以及比原始工作中更强的直线性新型公式所决定。其次，我们提出了保证1-RF唯一性和直线性的通用条件，这与之前的经验发现相一致。作为分析的一个副产品，我们表明，在一维空间中，从标准高斯分布开始的RF会产生Monge地图。此外，我们还展示了模拟数据集和真实数据集上的实验结果，以验证我们的理论发现。代码可在<a target="_blank" rel="noopener" href="https://github.com/bansal-vansh/rectified-flow%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bansal-vansh/rectified-flow上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14949v6">PDF</a> 38 pages</p>
<p><strong>Summary</strong><br>     扩散模型已成为图像生成和去噪的强大工具。近期Liu等人提出的Rectified Flow模型旨在通过一系列凸优化问题来学习从噪声到数据的直线轨迹，与最优传输紧密相关。该模型可简化采样过程中的轨迹，提高生成质量。本文分析了Rectified Flow采样分布与目标分布之间的Wasserstein距离，并提供了独特性和直线性的通用条件保证。此外，本文还在一维条件下进行了实证分析。相关代码可在github.com&#x2F;bansal-ansh查找。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型用于图像生成和去噪。</li>
<li>Rectified Flow模型通过学习直线轨迹从噪声到数据来提高生成质量。</li>
<li>Rectified Flow通过凸优化问题和最优传输实现轨迹直线化。</li>
<li>理论分析了Rectified Flow采样分布与目标分布之间的Wasserstein距离。</li>
<li>分析了轨迹直线性的独特性和通用条件保证。</li>
<li>在一维条件下进行了实证分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14949">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d3cecd56d2c91145cd62646f64e2295b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c998c6c2c2bbe666cf899972d93cfe57.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-16  Giant and anisotropic magnetostriction in $β$-O$_{2}$ at 110 T
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8255d31c849d39aab17844f4b62711d8.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-16  LL-Gaussian Low-Light Scene Reconstruction and Enhancement via Gaussian   Splatting for Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
