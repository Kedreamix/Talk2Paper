<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Anchor Token Matching Implicit Structure Locking for Training-free AR   Image Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b65add2678bc8eb9dbbe33652fc51cf8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-16-æ›´æ–°"><a href="#2025-04-16-æ›´æ–°" class="headerlink" title="2025-04-16 æ›´æ–°"></a>2025-04-16 æ›´æ–°</h1><h2 id="Anchor-Token-Matching-Implicit-Structure-Locking-for-Training-free-AR-Image-Editing"><a href="#Anchor-Token-Matching-Implicit-Structure-Locking-for-Training-free-AR-Image-Editing" class="headerlink" title="Anchor Token Matching: Implicit Structure Locking for Training-free AR   Image Editing"></a>Anchor Token Matching: Implicit Structure Locking for Training-free AR   Image Editing</h2><p><strong>Authors:Taihang Hu, Linxuan Li, Kai Wang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</strong></p>
<p>Text-to-image generation has seen groundbreaking advancements with diffusion models, enabling high-fidelity synthesis and precise image editing through cross-attention manipulation. Recently, autoregressive (AR) models have re-emerged as powerful alternatives, leveraging next-token generation to match diffusion models. However, existing editing techniques designed for diffusion models fail to translate directly to AR models due to fundamental differences in structural control. Specifically, AR models suffer from spatial poverty of attention maps and sequential accumulation of structural errors during image editing, which disrupt object layouts and global consistency. In this work, we introduce Implicit Structure Locking (ISLock), the first training-free editing strategy for AR visual models. Rather than relying on explicit attention manipulation or fine-tuning, ISLock preserves structural blueprints by dynamically aligning self-attention patterns with reference images through the Anchor Token Matching (ATM) protocol. By implicitly enforcing structural consistency in latent space, our method ISLock enables structure-aware editing while maintaining generative autonomy. Extensive experiments demonstrate that ISLock achieves high-quality, structure-consistent edits without additional training and is superior or comparable to conventional editing techniques. Our findings pioneer the way for efficient and flexible AR-based image editing, further bridging the performance gap between diffusion and autoregressive generative models. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM">https://github.com/hutaiHang/ATM</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„æ¨åŠ¨ä¸‹å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æ“çºµï¼ˆcross-attention manipulationï¼‰å®ç°äº†é«˜ä¿çœŸåˆæˆå’Œç²¾ç¡®å›¾åƒç¼–è¾‘ã€‚è¿‘æœŸï¼Œè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„æ›¿ä»£æ–¹æ¡ˆé‡æ–°å´­éœ²å¤´è§’ï¼Œå®ƒåˆ©ç”¨ä¸‹ä¸€ä¸ªæ ‡è®°ç”Ÿæˆï¼ˆnext-token generationï¼‰æŠ€æœ¯åŒ¹é…æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ‰©æ•£æ¨¡å‹è®¾è®¡çš„ç°æœ‰ç¼–è¾‘æŠ€æœ¯æ— æ³•ç›´æ¥åº”ç”¨äºARæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬åœ¨ç»“æ„æ§åˆ¶ä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼ŒARæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ—¶å­˜åœ¨æ³¨æ„åŠ›å›¾çš„ç©ºé—´è´«å›°ï¼ˆspatial poverty of attention mapsï¼‰å’Œç»“æ„æ€§é”™è¯¯é¡ºåºç´¯ç§¯çš„é—®é¢˜ï¼Œè¿™ä¼šç ´åå¯¹è±¡å¸ƒå±€å’Œå…¨å±€ä¸€è‡´æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†éšå¼ç»“æ„é”å®šï¼ˆISLockï¼‰â€”â€”è¿™æ˜¯ä¸€ç§é’ˆå¯¹ARè§†è§‰æ¨¡å‹çš„é¦–ä¸ªæ— éœ€è®­ç»ƒå³å¯è¿›è¡Œç¼–è¾‘çš„ç­–ç•¥ã€‚ISLockä¸ä¾èµ–äºæ˜¾å¼æ³¨æ„åŠ›æ“çºµæˆ–å¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡é”šæ ‡è®°åŒ¹é…ï¼ˆATMï¼‰åè®®åŠ¨æ€åœ°å°†è‡ªæˆ‘æ³¨æ„åŠ›æ¨¡å¼ä¸å‚è€ƒå›¾åƒå¯¹é½ï¼Œä»è€Œä¿ç•™ç»“æ„è“å›¾ã€‚é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­éšå¼å¼ºåˆ¶æ‰§è¡Œç»“æ„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬çš„ISLockæ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒç”Ÿæˆè‡ªä¸»æ€§çš„åŒæ—¶è¿›è¡Œç»“æ„æ„ŸçŸ¥ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒISLockæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°é«˜è´¨é‡ã€ç»“æ„ä¸€è‡´æ€§çš„ç¼–è¾‘ï¼Œå¹¶ä¸”å…¶æ€§èƒ½ä¼˜äºæˆ–å¯ä¸ä¼ ç»Ÿç¼–è¾‘æŠ€æœ¯ç›¸æå¹¶è®ºã€‚æˆ‘ä»¬çš„å‘ç°å¼€è¾Ÿäº†åŸºäºè‡ªå›å½’ï¼ˆARï¼‰çš„é«˜æ•ˆçµæ´»å›¾åƒç¼–è¾‘çš„é“è·¯ï¼Œè¿›ä¸€æ­¥ç¼©å°äº†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM">https://github.com/hutaiHang/ATM</a>å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œè¿‘æœŸè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ä½œä¸ºå¼ºæœ‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆå´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ‰©æ•£æ¨¡å‹è®¾è®¡çš„ç¼–è¾‘æŠ€æœ¯æ— æ³•ç›´æ¥åº”ç”¨äºARæ¨¡å‹ï¼Œå› ä¸ºä¸¤è€…åœ¨ç»“æ„æ§åˆ¶ä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚é’ˆå¯¹ARæ¨¡å‹çš„ç©ºé—´æ³¨æ„åŠ›å›¾è´«å›°å’Œå›¾åƒç¼–è¾‘ä¸­çš„ç»“æ„é”™è¯¯ç´¯ç§¯é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ— éœ€è®­ç»ƒçš„ç¼–è¾‘ç­–ç•¥â€”â€”éšå¼ç»“æ„é”å®šï¼ˆISLockï¼‰ã€‚é€šè¿‡åŠ¨æ€å¯¹é½å‚è€ƒå›¾åƒçš„è‡ªæˆ‘å…³æ³¨æ¨¡å¼ï¼ŒISLockèƒ½å¤Ÿä¿ç•™ç»“æ„è“å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒISLockèƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡ã€ç»“æ„ä¸€è‡´æ€§çš„ç¼–è¾‘ï¼Œå¹¶ä¼˜äºæˆ–ç›¸å½“äºä¼ ç»Ÿç¼–è¾‘æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—é‡å¤§è¿›å±•ï¼Œè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹æˆä¸ºæœ‰åŠ›æ›¿ä»£ã€‚</li>
<li>ç°æœ‰ç¼–è¾‘æŠ€æœ¯æ— æ³•ç›´æ¥åº”ç”¨äºARæ¨¡å‹ï¼Œå› ä¸ºç»“æ„æ§åˆ¶ä¸Šçš„æ ¹æœ¬å·®å¼‚ã€‚</li>
<li>ARæ¨¡å‹é¢ä¸´ç©ºé—´æ³¨æ„åŠ›å›¾è´«å›°å’Œå›¾åƒç¼–è¾‘ä¸­çš„ç»“æ„é”™è¯¯ç´¯ç§¯é—®é¢˜ã€‚</li>
<li>ISLockæ˜¯é¦–ä¸ªé’ˆå¯¹ARè§†è§‰æ¨¡å‹çš„æ— éœ€è®­ç»ƒçš„ç¼–è¾‘ç­–ç•¥ã€‚</li>
<li>ISLocké€šè¿‡åŠ¨æ€å¯¹é½è‡ªæˆ‘å…³æ³¨æ¨¡å¼ä¸å‚è€ƒå›¾åƒï¼Œä¿ç•™ç»“æ„è“å›¾ã€‚</li>
<li>ISLockå®ç°äº†é«˜è´¨é‡ã€ç»“æ„ä¸€è‡´æ€§çš„ç¼–è¾‘ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>ISLockä¼˜äºæˆ–ç›¸å½“äºä¼ ç»Ÿç¼–è¾‘æŠ€æœ¯ï¼Œä¸ºARæ¨¡å‹åŸºäºå›¾åƒç¼–è¾‘å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-93beb0f0b0b7f8d87d7e885c2a4146d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33d316dd2b5db7bc9e3692e3c4bbc54b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f90b8b1fbe9d7fa92cf3cf4633b2541.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4105ed1ab16161f5d59433951ac8d3d6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MonoDiff9D-Monocular-Category-Level-9D-Object-Pose-Estimation-via-Diffusion-Model"><a href="#MonoDiff9D-Monocular-Category-Level-9D-Object-Pose-Estimation-via-Diffusion-Model" class="headerlink" title="MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via   Diffusion Model"></a>MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via   Diffusion Model</h2><p><strong>Authors:Jian Liu, Wei Sun, Hui Yang, Jin Zheng, Zichen Geng, Hossein Rahmani, Ajmal Mian</strong></p>
<p>Object pose estimation is a core means for robots to understand and interact with their environment. For this task, monocular category-level methods are attractive as they require only a single RGB camera. However, current methods rely on shape priors or CAD models of the intra-class known objects. We propose a diffusion-based monocular category-level 9D object pose generation method, MonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion models to alleviate the need for shape priors, CAD models, or depth sensors for intra-class unknown object pose estimation. We first estimate coarse depth via DINOv2 from the monocular image in a zero-shot manner and convert it into a point cloud. We then fuse the global features of the point cloud with the input image and use the fused features along with the encoded time step to condition MonoDiff9D. Finally, we design a transformer-based denoiser to recover the object pose from Gaussian noise. Extensive experiments on two popular benchmark datasets show that MonoDiff9D achieves state-of-the-art monocular category-level 9D object pose estimation accuracy without the need for shape priors or CAD models at any stage. Our code will be made public at <a target="_blank" rel="noopener" href="https://github.com/CNJianLiu/MonoDiff9D">https://github.com/CNJianLiu/MonoDiff9D</a>. </p>
<blockquote>
<p>å¯¹è±¡å§¿æ€ä¼°è®¡æ˜¯æœºå™¨äººç†è§£å’Œä¸å…¶ç¯å¢ƒäº¤äº’çš„æ ¸å¿ƒæ‰‹æ®µã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼Œå•ç›®ç±»åˆ«çº§æ–¹æ³•å¾ˆæœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒä»¬ä»…éœ€è¦å•ä¸ªRGBç›¸æœºã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºå½¢çŠ¶å…ˆéªŒæˆ–å·²çŸ¥å¯¹è±¡çš„ç±»å†…CADæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„å•ç›®ç±»åˆ«çº§9Då¯¹è±¡å§¿æ€ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºMonoDiff9Dã€‚æˆ‘ä»¬çš„åŠ¨æœºæ˜¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æ€§è´¨ï¼Œå‡è½»å¯¹å½¢çŠ¶å…ˆéªŒã€CADæ¨¡å‹æˆ–æ·±åº¦ä¼ æ„Ÿå™¨çš„éœ€æ±‚ï¼Œç”¨äºç±»å†…æœªçŸ¥å¯¹è±¡å§¿æ€ä¼°è®¡ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡DINOv2ä»¥é›¶æ ·æœ¬æ–¹å¼ä»å•ç›®å›¾åƒä¼°è®¡ç²—ç•¥æ·±åº¦å¹¶å°†å…¶è½¬æ¢ä¸ºç‚¹äº‘ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç‚¹äº‘çš„å…¨å±€ç‰¹å¾ä¸è¾“å…¥å›¾åƒèåˆï¼Œå¹¶ä½¿ç”¨èåˆçš„ç‰¹å¾ä»¥åŠç¼–ç çš„æ—¶é—´æ­¥é•¿æ¥è°ƒèŠ‚MonoDiff9Dã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„å»å™ªå™¨ï¼Œä»é«˜æ–¯å™ªå£°ä¸­æ¢å¤å¯¹è±¡å§¿æ€ã€‚åœ¨ä¸¤ä¸ªæµè¡Œçš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMonoDiff9Dåœ¨ä¸ä½¿ç”¨å½¢çŠ¶å…ˆéªŒæˆ–ä»»ä½•é˜¶æ®µçš„CADæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å•ç›®ç±»åˆ«çº§9Då¯¹è±¡å§¿æ€ä¼°è®¡ç²¾åº¦ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/CNJianLiu/MonoDiff9D%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/CNJianLiu/MonoDiff9Då…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10433v1">PDF</a> Accepted by ICRAâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å•ç›®ç±»åˆ«çº§9Dç‰©ä½“å§¿æ€ç”Ÿæˆæ–¹æ³•ï¼Œåä¸ºMonoDiff9Dã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æ€§è´¨ï¼Œæ— éœ€å½¢çŠ¶å…ˆéªŒã€CADæ¨¡å‹æˆ–æ·±åº¦ä¼ æ„Ÿå™¨å³å¯å®ç°åŒç±»æœªçŸ¥ç‰©ä½“çš„å§¿æ€ä¼°è®¡ã€‚é€šè¿‡å•ç›®å›¾åƒè¿›è¡Œç²—ç•¥æ·±åº¦ä¼°è®¡ï¼Œè½¬æ¢ä¸ºç‚¹äº‘ï¼Œå¹¶ä¸è¾“å…¥å›¾åƒèåˆç‰¹å¾ã€‚è®¾è®¡åŸºäºå˜å‹å™¨çš„å»å™ªå™¨ï¼Œä»é«˜æ–¯å™ªå£°ä¸­æ¢å¤ç‰©ä½“å§¿æ€ã€‚åœ¨ä¸»æµæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMonoDiff9Dåœ¨æ— éœ€å½¢çŠ¶å…ˆéªŒæˆ–CADæ¨¡å‹çš„æ¡ä»¶ä¸‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å•ç›®ç±»åˆ«çº§9Dç‰©ä½“å§¿æ€ä¼°è®¡ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è±¡å§¿æ€ä¼°è®¡æ˜¯æœºå™¨äººç†è§£å’Œä¸å…¶ç¯å¢ƒäº¤äº’çš„æ ¸å¿ƒæ‰‹æ®µã€‚</li>
<li>å½“å‰æ–¹æ³•éœ€è¦å½¢çŠ¶å…ˆéªŒæˆ–å·²çŸ¥å¯¹è±¡çš„CADæ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„MonoDiff9Dæ–¹æ³•ï¼Œç”¨äºå•ç›®ç±»åˆ«çº§9Då¯¹è±¡å§¿æ€ç”Ÿæˆã€‚</li>
<li>MonoDiff9Dåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æ€§è´¨ï¼Œæ— éœ€å½¢çŠ¶å…ˆéªŒã€CADæ¨¡å‹æˆ–æ·±åº¦ä¼ æ„Ÿå™¨ã€‚</li>
<li>é€šè¿‡å•ç›®å›¾åƒä¼°è®¡ç²—ç•¥æ·±åº¦å¹¶è½¬æ¢ä¸ºç‚¹äº‘ï¼Œä¸è¾“å…¥å›¾åƒèåˆç‰¹å¾ã€‚</li>
<li>è®¾è®¡åŸºäºå˜å‹å™¨çš„å»å™ªå™¨ï¼Œä»é«˜æ–¯å™ªå£°ä¸­æ¢å¤ç‰©ä½“å§¿æ€ã€‚</li>
<li>åœ¨ä¸»æµæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMonoDiff9Då®ç°äº†æœ€å…ˆè¿›çš„ä¼°è®¡ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-383d12eb88b458170e284b7ed64b61f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0aa45c5b62ca44aed46d63d9cd4c79f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40a95a1a32095dd9ba549eb5df5d2819.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835c471efe5317fe973cdb0583aeb1ac.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation"><a href="#OctGPT-Octree-based-Multiscale-Autoregressive-Models-for-3D-Shape-Generation" class="headerlink" title="OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation"></a>OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape   Generation</h2><p><strong>Authors:Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang</strong></p>
<p>Autoregressive models have achieved remarkable success across various domains, yet their performance in 3D shape generation lags significantly behind that of diffusion models. In this paper, we introduce OctGPT, a novel multiscale autoregressive model for 3D shape generation that dramatically improves the efficiency and performance of prior 3D autoregressive approaches, while rivaling or surpassing state-of-the-art diffusion models. Our method employs a serialized octree representation to efficiently capture the hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded via octree structures, while fine-grained details are represented by binary tokens generated using a vector quantized variational autoencoder (VQVAE), transforming 3D shapes into compact \emph{multiscale binary sequences} suitable for autoregressive prediction. To address the computational challenges of handling long sequences, we incorporate octree-based transformers enhanced with 3D rotary positional encodings, scale-specific embeddings, and token-parallel generation schemes. These innovations reduce training time by 13 folds and generation time by 69 folds, enabling the efficient training of high-resolution 3D shapes, e.g.,$1024^3$, on just four NVIDIA 4090 GPUs only within days. OctGPT showcases exceptional versatility across various tasks, including text-, sketch-, and image-conditioned generation, as well as scene-level synthesis involving multiple objects. Extensive experiments demonstrate that OctGPT accelerates convergence and improves generation quality over prior autoregressive methods, offering a new paradigm for high-quality, scalable 3D content creation. </p>
<blockquote>
<p>è‡ªå›å½’æ¨¡å‹å·²ç»åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œä½†åœ¨3Då½¢çŠ¶ç”Ÿæˆæ–¹é¢ï¼Œå…¶æ€§èƒ½è¿œè¿œè½åäºæ‰©æ•£æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OctGPTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº3Då½¢çŠ¶ç”Ÿæˆçš„æ–°å‹å¤šå°ºåº¦è‡ªå›å½’æ¨¡å‹ï¼Œå®ƒæ˜¾è‘—æé«˜äº†ä¹‹å‰3Dè‡ªå›å½’æ–¹æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ï¼ŒåŒæ—¶ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç›¸æŠ—è¡¡ç”šè‡³è¶…è¶Šã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åºåˆ—åŒ–å…«å‰æ ‘è¡¨ç¤ºæ³•ï¼Œæœ‰æ•ˆåœ°æ•æ‰3Då½¢çŠ¶çš„å±‚æ¬¡ç»“æ„å’Œç©ºé—´ç»“æ„ã€‚ç²—ç³™å‡ ä½•ç»“æ„é€šè¿‡å…«å‰æ ‘ç»“æ„è¿›è¡Œç¼–ç ï¼Œè€Œç»†ç²’åº¦ç»†èŠ‚åˆ™ç”±ä½¿ç”¨å‘é‡é‡åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVQVAEï¼‰ç”Ÿæˆçš„äºŒè¿›åˆ¶ä»¤ç‰Œè¡¨ç¤ºï¼Œå°†3Då½¢çŠ¶è½¬æ¢ä¸ºé€‚åˆè‡ªå›å½’é¢„æµ‹çš„å¤šå°ºåº¦äºŒè¿›åˆ¶åºåˆ—ã€‚ä¸ºäº†è§£å†³å¤„ç†é•¿åºåˆ—çš„è®¡ç®—æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºå…«å‰æ ‘çš„å˜å‹å™¨ï¼Œå¹¶å¢å¼ºäº†3Dæ—‹è½¬ä½ç½®ç¼–ç ã€å°ºåº¦ç‰¹å®šåµŒå…¥å’Œä»¤ç‰Œå¹¶è¡Œç”Ÿæˆæ–¹æ¡ˆã€‚è¿™äº›åˆ›æ–°å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†13å€ï¼Œç”Ÿæˆæ—¶é—´ç¼©çŸ­äº†69å€ï¼Œä½¿å¾—åœ¨ä»…å››å°NVIDIA 4090 GPUä¸Šåœ¨å‡ å¤©å†…å°±èƒ½æœ‰æ•ˆåœ°è®­ç»ƒé«˜åˆ†è¾¨ç‡çš„3Då½¢çŠ¶ï¼Œä¾‹å¦‚$1024^3$ã€‚OctGPTåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç¤ºäº†å‡ºè‰²çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾å’Œå›¾åƒæ¡ä»¶ä¸‹çš„ç”Ÿæˆï¼Œä»¥åŠæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„åœºæ™¯çº§åˆ«åˆæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOctGPTåŠ é€Ÿäº†æ”¶æ•›ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œä¸ºé«˜è´¨é‡ã€å¯æ‰©å±•çš„3Då†…å®¹åˆ›å»ºæä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09975v1">PDF</a> SIGGRAPH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OctGPTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº3Då½¢çŠ¶ç”Ÿæˆçš„æ–°å‹å¤šå°ºåº¦è‡ªå›å½’æ¨¡å‹ã€‚å®ƒé€šè¿‡é‡‡ç”¨åºåˆ—åŒ–å…«å‰æ ‘è¡¨ç¤ºæ³•ï¼Œæœ‰æ•ˆåœ°æ•æ‰3Då½¢çŠ¶çš„å±‚æ¬¡ç»“æ„å’Œç©ºé—´ç»“æ„ï¼Œæé«˜äº†å…ˆå‰3Dè‡ªå›å½’æ–¹æ³•çš„æ•ˆç‡å’Œæ€§èƒ½ï¼ŒåŒæ—¶ä¸æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨åŸºäºå…«å‰æ ‘çš„å˜å‹å™¨ã€3Dæ—‹è½¬ä½ç½®ç¼–ç ã€å°ºåº¦ç‰¹å®šåµŒå…¥å’Œä»¤ç‰Œå¹¶è¡Œç”Ÿæˆæ–¹æ¡ˆç­‰åˆ›æ–°æŠ€æœ¯ï¼Œå‡å°‘äº†è®­ç»ƒå’Œç”Ÿæˆæ—¶é—´ã€‚OctGPTåœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„å¤šåŠŸèƒ½æ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾å’Œå›¾åƒæ¡ä»¶ç”Ÿæˆï¼Œä»¥åŠæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„åœºæ™¯çº§åˆ«åˆæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OctGPTæ˜¯ä¸€ä¸ªç”¨äº3Då½¢çŠ¶ç”Ÿæˆçš„å¤šå°ºåº¦è‡ªå›å½’æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†è‡ªå›å½’æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>OctGPTé‡‡ç”¨äº†åºåˆ—åŒ–å…«å‰æ ‘è¡¨ç¤ºæ³•ï¼Œæœ‰æ•ˆæ•æ‰3Då½¢çŠ¶çš„å±‚æ¬¡ç»“æ„å’Œç©ºé—´ç»“æ„ã€‚</li>
<li>OctGPTé€šè¿‡ä¸æ‰©æ•£æ¨¡å‹çš„ç«äº‰ï¼Œå±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨åŸºäºå…«å‰æ ‘çš„å˜å‹å™¨å’Œå…¶ä»–åˆ›æ–°æŠ€æœ¯ï¼ŒOctGPTå‡å°‘äº†è®­ç»ƒå’Œç”Ÿæˆæ—¶é—´ã€‚</li>
<li>OctGPTå…·æœ‰å‡ºè‰²çš„å¤šåŠŸèƒ½æ€§ï¼Œæ”¯æŒæ–‡æœ¬ã€è‰å›¾å’Œå›¾åƒæ¡ä»¶ç”Ÿæˆï¼Œä»¥åŠåœºæ™¯çº§åˆ«åˆæˆã€‚</li>
<li>OctGPTå¯ä»¥åœ¨ä¸åŒä»»åŠ¡ä¸­ç”Ÿæˆé«˜è´¨é‡ã€å¯ä¼¸ç¼©çš„3Då†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-510495a30a76a8367e0998c6e8924c97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee13c13531eb57544bc391d2986841b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5797e9b6bff5f87db57e40e7fd37c868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5758c207f83189b9417bd59194aa3ec8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPICE-A-Synergistic-Precise-Iterative-and-Customizable-Image-Editing-Workflow"><a href="#SPICE-A-Synergistic-Precise-Iterative-and-Customizable-Image-Editing-Workflow" class="headerlink" title="SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing   Workflow"></a>SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing   Workflow</h2><p><strong>Authors:Kenan Tang, Yanhong Li, Yao Qin</strong></p>
<p>Recent prompt-based image editing models have demonstrated impressive prompt-following capability at structural editing tasks. However, existing models still fail to perform local edits, follow detailed editing prompts, or maintain global image quality beyond a single editing step. To address these challenges, we introduce SPICE, a training-free workflow that accepts arbitrary resolutions and aspect ratios, accurately follows user requirements, and improves image quality consistently during more than 100 editing steps. By synergizing the strengths of a base diffusion model and a Canny edge ControlNet model, SPICE robustly handles free-form editing instructions from the user. SPICE outperforms state-of-the-art baselines on a challenging realistic image-editing dataset consisting of semantic editing (object addition, removal, replacement, and background change), stylistic editing (texture changes), and structural editing (action change) tasks. Not only does SPICE achieve the highest quantitative performance according to standard evaluation metrics, but it is also consistently preferred by users over existing image-editing methods. We release the workflow implementation for popular diffusion model Web UIs to support further research and artistic exploration. </p>
<blockquote>
<p>è¿‘æœŸåŸºäºæç¤ºçš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨ç»“æ„ç¼–è¾‘ä»»åŠ¡ä¸­å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„éµå¾ªæç¤ºèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹ä»ç„¶æ— æ³•æ‰§è¡Œå±€éƒ¨ç¼–è¾‘ã€éµå¾ªè¯¦ç»†çš„ç¼–è¾‘æç¤ºæˆ–åœ¨å•æ­¥ç¼–è¾‘ä¹‹å¤–ä¿æŒå…¨å±€å›¾åƒè´¨é‡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SPICEï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å·¥ä½œæµç¨‹ï¼Œå¯æ¥å—ä»»æ„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”ï¼Œèƒ½å‡†ç¡®åœ°éµå¾ªç”¨æˆ·éœ€æ±‚ï¼Œå¹¶åœ¨è¶…è¿‡100æ­¥ç¼–è¾‘è¿‡ç¨‹ä¸­æŒç»­æé«˜å›¾åƒè´¨é‡ã€‚SPICEé€šè¿‡ç»“åˆåŸºç¡€æ‰©æ•£æ¨¡å‹å’ŒCannyè¾¹ç¼˜ControlNetæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œç¨³å¥åœ°å¤„ç†ç”¨æˆ·çš„è‡ªç”±å½¢å¼ç¼–è¾‘æŒ‡ä»¤ã€‚åœ¨åŒ…å«è¯­ä¹‰ç¼–è¾‘ï¼ˆå¯¹è±¡æ·»åŠ ã€åˆ é™¤ã€æ›¿æ¢å’ŒèƒŒæ™¯æ›´æ”¹ï¼‰ã€é£æ ¼ç¼–è¾‘ï¼ˆçº¹ç†æ›´æ”¹ï¼‰å’Œç»“æ„ç¼–è¾‘ï¼ˆåŠ¨ä½œæ›´æ”¹ï¼‰ä»»åŠ¡çš„æŒ‘æˆ˜æ€§ç°å®å›¾åƒç¼–è¾‘æ•°æ®é›†ä¸Šï¼ŒSPICEè¶…è¶Šäº†æœ€æ–°åŸºçº¿ã€‚æ ¹æ®æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼ŒSPICEä¸ä»…è¾¾åˆ°äº†æœ€é«˜çš„é‡åŒ–æ€§èƒ½ï¼Œè€Œä¸”ç›¸è¾ƒäºç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œç”¨æˆ·ä¹Ÿå§‹ç»ˆæ›´åçˆ±ä½¿ç”¨SPICEã€‚æˆ‘ä»¬ä¸ºæµè¡Œçš„æ‰©æ•£æ¨¡å‹Webç•Œé¢å‘å¸ƒäº†å·¥ä½œæµç¨‹å®ç°ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œè‰ºæœ¯æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09697v1">PDF</a> 24 pages, 21 figures. Figure 9(b) has been accepted by CVPR AI Art   Gallery 2025</p>
<p><strong>Summary</strong></p>
<p>ç°æœ‰åŸºäºpromptçš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨å¤„ç†ç»“æ„ç¼–è¾‘ä»»åŠ¡æ—¶å…·æœ‰è‰¯å¥½çš„è¡¨ç°ï¼Œä½†åœ¨è¿›è¡Œå±€éƒ¨ç¼–è¾‘ã€éµå¾ªè¯¦ç»†ç¼–è¾‘æç¤ºå’Œç»´æŒå›¾åƒå…¨å±€è´¨é‡æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SPICEè®­ç»ƒæ— æµç¨‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ”¯æŒä»»æ„åˆ†è¾¨ç‡å’Œæ¯”ä¾‹ï¼Œèƒ½å¤Ÿå‡†ç¡®éµå¾ªç”¨æˆ·éœ€æ±‚ï¼Œå¹¶åœ¨è¶…è¿‡100æ­¥çš„ç¼–è¾‘è¿‡ç¨‹ä¸­æŒç»­æé«˜å›¾åƒè´¨é‡ã€‚é€šè¿‡ç»“åˆåŸºç¡€æ‰©æ•£æ¨¡å‹å’ŒCannyè¾¹ç¼˜ControlNetæ¨¡å‹çš„ä¼˜ç‚¹ï¼ŒSPICEèƒ½å¤Ÿç¨³å¥åœ°å¤„ç†ç”¨æˆ·çš„è‡ªç”±å½¢å¼ç¼–è¾‘æŒ‡ä»¤ã€‚SPICEåœ¨åŒ…å«è¯­ä¹‰ç¼–è¾‘ã€é£æ ¼ç¼–è¾‘å’Œç»“æ„ç¼–è¾‘ä»»åŠ¡çš„æŒ‘æˆ˜æ€§çœŸå®å›¾åƒç¼–è¾‘æ•°æ®é›†ä¸Šä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ã€‚å®ƒä¸ä»…åœ¨æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€é«˜çš„å®šé‡æ€§èƒ½ï¼Œè€Œä¸”ç›¸è¾ƒäºç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•æ›´å—ç”¨æˆ·é’çã€‚æˆ‘ä»¬ä¸ºæµè¡Œçš„æ‰©æ•£æ¨¡å‹Web UIå‘å¸ƒäº†å·¥ä½œæµç¨‹å®ç°ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œè‰ºæœ¯æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPICEæ˜¯ä¸€ä¸ªè®­ç»ƒæ— æµç¨‹çš„æ¨¡å‹ï¼Œç”¨äºè§£å†³ç°æœ‰å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨å±€éƒ¨ç¼–è¾‘ã€éµå¾ªè¯¦ç»†ç¼–è¾‘æç¤ºå’Œç»´æŒå…¨å±€å›¾åƒè´¨é‡æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>SPICEæ”¯æŒä»»æ„åˆ†è¾¨ç‡å’Œæ¯”ä¾‹ï¼Œèƒ½å¤Ÿå‡†ç¡®éµå¾ªç”¨æˆ·çš„è‡ªç”±å½¢å¼ç¼–è¾‘æŒ‡ä»¤ã€‚</li>
<li>SPICEç»“åˆäº†åŸºç¡€æ‰©æ•£æ¨¡å‹å’ŒCannyè¾¹ç¼˜ControlNetæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä»¥å¤„ç†å¤æ‚çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>SPICEåœ¨åŒ…å«è¯­ä¹‰ç¼–è¾‘ã€é£æ ¼ç¼–è¾‘å’Œç»“æ„ç¼–è¾‘ä»»åŠ¡çš„å›¾åƒç¼–è¾‘æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SPICEä¸ä»…åœ¨æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—æœ€é«˜å®šé‡æ€§èƒ½ï¼Œè¿˜è·å¾—äº†ç”¨æˆ·çš„é’çã€‚</li>
<li>SPICEæ¨¡å‹å¯¹äºè¿›ä¸€æ­¥çš„ç§‘ç ”å’Œè‰ºæœ¯æ¢ç´¢å…·æœ‰å¾ˆé«˜çš„æ”¯æŒæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9d733997d4021412c71a89a52f473785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51a4a634b6034ac528e134ceee3ba6ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9692e747204189d1a500c74692cc4a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a42134c8e30e1bedf36fd77f717e482b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d269ea27a84b11a9719118ad2165a807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e8a39520aa4b36df121967c4971710.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Early-Bird-Diffusion-Investigating-and-Leveraging-Timestep-Aware-Early-Bird-Tickets-in-Diffusion-Models-for-Efficient-Training"><a href="#Early-Bird-Diffusion-Investigating-and-Leveraging-Timestep-Aware-Early-Bird-Tickets-in-Diffusion-Models-for-Efficient-Training" class="headerlink" title="Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware   Early-Bird Tickets in Diffusion Models for Efficient Training"></a>Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware   Early-Bird Tickets in Diffusion Models for Efficient Training</h2><p><strong>Authors:Lexington Whalen, Zhenbang Du, Haoran You, Chaojian Li, Sixu Li,  Yingyan,  Lin</strong></p>
<p>Training diffusion models (DMs) requires substantial computational resources due to multiple forward and backward passes across numerous timesteps, motivating research into efficient training techniques. In this paper, we propose EB-Diff-Train, a new efficient DM training approach that is orthogonal to other methods of accelerating DM training, by investigating and leveraging Early-Bird (EB) tickets â€“ sparse subnetworks that manifest early in the training process and maintain high generation quality.   We first investigate the existence of traditional EB tickets in DMs, enabling competitive generation quality without fully training a dense model.   Then, we delve into the concept of diffusion-dedicated EB tickets, drawing on insights from varying importance of different timestep regions. These tickets adapt their sparsity levels according to the importance of corresponding timestep regions, allowing for aggressive sparsity during non-critical regions while conserving computational resources for crucial timestep regions.   Building on this, we develop an efficient DM training technique that derives timestep-aware EB tickets, trains them in parallel, and combines them during inference for image generation. Extensive experiments validate the existence of both traditional and timestep-aware EB tickets, as well as the effectiveness of our proposed EB-Diff-Train method. This approach can significantly reduce training time both spatially and temporally â€“ achieving 2.9$\times$ to 5.8$\times$ speedups over training unpruned dense models, and up to 10.3$\times$ faster training compared to standard train-prune-finetune pipelines â€“ without compromising generative quality.   Our code is available at <a target="_blank" rel="noopener" href="https://github.com/GATECH-EIC/Early-Bird-Diffusion">https://github.com/GATECH-EIC/Early-Bird-Diffusion</a>. </p>
<blockquote>
<p>è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå› ä¸ºéœ€è¦åœ¨å¤šä¸ªæ—¶é—´æ­¥ä¸Šè¿›è¡Œå¤šæ¬¡æ­£å‘å’Œåå‘ä¼ é€’ï¼Œè¿™ä¿ƒä½¿äººä»¬ç ”ç©¶é«˜æ•ˆçš„è®­ç»ƒæŠ€æœ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EB-Diff-Trainï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é«˜æ•ˆDMè®­ç»ƒæ–¹æ³•ï¼Œå®ƒä¸å…¶ä»–åŠ é€ŸDMè®­ç»ƒçš„æ–¹æ³•æ­£äº¤ï¼Œé€šè¿‡ç ”ç©¶å’Œåˆ©ç”¨æ—©æœŸé¸Ÿï¼ˆEBï¼‰é—¨ç¥¨â€”â€”ç¨€ç–å­ç½‘ç»œï¼Œè¿™äº›å­ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ—©æœŸå‡ºç°å¹¶ä¿æŒè‰¯å¥½çš„ç”Ÿæˆè´¨é‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç ”ç©¶äº†DMä¸­ä¼ ç»ŸEBé—¨ç¥¨çš„å­˜åœ¨ï¼Œèƒ½å¤Ÿåœ¨ä¸å…¨é¢è®­ç»ƒå¯†é›†æ¨¡å‹çš„æƒ…å†µä¸‹å®ç°æœ‰ç«äº‰åŠ›çš„ç”Ÿæˆè´¨é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†æ‰©æ•£ä¸“ç”¨EBé—¨ç¥¨çš„æ¦‚å¿µï¼Œä»ä¸åŒæ—¶é—´æ­¥åŒºåŸŸçš„é‡è¦æ€§ä¸­æ±²å–è§è§£ã€‚è¿™äº›é—¨ç¥¨æ ¹æ®ç›¸åº”æ—¶é—´æ­¥åŒºåŸŸçš„é‡è¦æ€§è°ƒæ•´å…¶ç¨€ç–åº¦ï¼Œåœ¨éå…³é”®åŒºåŸŸå®ç°å¤§èƒ†çš„ç¨€ç–æ€§ï¼ŒåŒæ—¶ä¿ç•™è®¡ç®—èµ„æºç”¨äºå…³é”®æ—¶é—´æ­¥åŒºåŸŸã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„DMè®­ç»ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ´¾ç”Ÿå‡ºæ—¶é—´æ„ŸçŸ¥EBé—¨ç¥¨ï¼Œå¹¶è¡Œè®­ç»ƒå®ƒä»¬ï¼Œå¹¶åœ¨å›¾åƒç”Ÿæˆæ—¶è¿›è¡Œç»„åˆæ¨ç†ã€‚å¤§é‡å®éªŒéªŒè¯äº†ä¼ ç»Ÿå’Œæ—¶é—´æ„ŸçŸ¥EBé—¨ç¥¨çš„å­˜åœ¨ä»¥åŠæˆ‘ä»¬æå‡ºçš„EB-Diff-Trainæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´â€”â€”ç›¸å¯¹äºæœªä¿®å‰ªçš„å¯†é›†æ¨¡å‹å®ç°2.9è‡³5.8å€çš„é€Ÿåº¦æå‡ï¼Œä¸æ ‡å‡†çš„è®­ç»ƒ-ä¿®å‰ª-å¾®è°ƒç®¡é“ç›¸æ¯”ï¼Œè®­ç»ƒé€Ÿåº¦æœ€å¿«å¯æé«˜10.3å€â€”â€”ä¸”ä¸å½±å“ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GATECH-EIC/Early-Bird-Diffusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GATECH-EIC/Early-Bird-Diffusionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09606v1">PDF</a> 10 pages, 5 figures. Accepted to the IEEE&#x2F;CVF Conference on Computer   Vision and Pattern Recognition 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰è®­ç»ƒæ–°æ–¹æ³•EB-Diff-Trainã€‚è¯¥æ–¹æ³•é€šè¿‡æ¢ç©¶å’Œåˆ©ç”¨æ—©æœŸé¸Ÿï¼ˆEBï¼‰ç¥¨ï¼Œå³è®­ç»ƒè¿‡ç¨‹ä¸­æ—©æœŸå‡ºç°çš„ç¨€ç–å­ç½‘ç»œå¹¶ä¿æŒé«˜ç”Ÿæˆè´¨é‡ï¼Œæ¥æé«˜DMçš„è®­ç»ƒæ•ˆç‡ã€‚ç ”ç©¶å†…å®¹åŒ…æ‹¬ï¼šæ¢ç©¶DMä¸­ä¼ ç»ŸEBç¥¨çš„å­˜åœ¨ï¼Œä½¿åœ¨ä¸å®Œå…¨è®­ç»ƒå¯†é›†æ¨¡å‹çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç«äº‰çš„ç”Ÿæˆè´¨é‡ï¼›æ·±å…¥ç ”ç©¶æ‰©æ•£ä¸“ç”¨çš„EBç¥¨ï¼Œæ ¹æ®ä¸åŒæ—¶é—´æ­¥åŒºåŸŸçš„é‡è¦æ€§è°ƒæ•´å…¶ç¨€ç–æ°´å¹³ï¼Œå®ç°éå…³é”®åŒºåŸŸçš„æ¿€è¿›ç¨€ç–ï¼ŒåŒæ—¶ä¿ç•™å…³é”®æ—¶é—´æ­¥åŒºåŸŸçš„è®¡ç®—èµ„æºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„DMè®­ç»ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯äº§ç”Ÿæ—¶é—´æ­¥æ„ŸçŸ¥çš„EBç¥¨ï¼Œå¹¶è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å›¾åƒç”Ÿæˆæ—¶è¿›è¡Œæ¨ç†ç»„åˆã€‚å®éªŒéªŒè¯äº†ä¼ ç»Ÿå’Œæ—¶é—´æ­¥æ„ŸçŸ¥çš„EBç¥¨çš„å­˜åœ¨ä»¥åŠEB-Diff-Trainæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•å¯åœ¨æ—¶é—´å’Œç©ºé—´ä¸Šæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œä¸æœªä¿®å‰ªçš„å¯†é›†æ¨¡å‹ç›¸æ¯”ï¼Œæœ€é«˜å¯è¾¾5.8å€çš„é€Ÿåº¦æå‡ï¼Œä¸æ ‡å‡†çš„è®­ç»ƒ-ä¿®å‰ª-å¾®è°ƒç®¡é“ç›¸æ¯”ï¼Œæœ€é«˜å¯è¾¾10.3å€çš„è®­ç»ƒé€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¸æŸå®³ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„è®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œç”±äºå…¶åœ¨å¤šä¸ªæ—¶é—´æ­¥é•¿ä¸Šçš„å‰å‘å’Œåå‘ä¼ é€’ã€‚</li>
<li>EB-Diff-Trainæ˜¯ä¸€ç§æ–°çš„é«˜æ•ˆDMè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ—©æœŸé¸Ÿï¼ˆEBï¼‰ç¥¨æ¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ç ”ç©¶ç¡®è®¤äº†ä¼ ç»ŸEBç¥¨åœ¨DMä¸­çš„å­˜åœ¨ï¼Œå³ä½¿åœ¨ä¸å®Œå…¨è®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é«˜è´¨é‡çš„ç”Ÿæˆã€‚</li>
<li>å¼•å…¥äº†æ‰©æ•£ä¸“ç”¨çš„EBç¥¨æ¦‚å¿µï¼Œæ ¹æ®æ—¶é—´æ­¥åŒºåŸŸçš„é‡è¦æ€§è°ƒæ•´ç¨€ç–æ€§ã€‚</li>
<li>æ–¹æ³•å®ç°äº†åœ¨å…³é”®å’Œéå…³é”®æ—¶é—´æ­¥åŒºåŸŸçš„èµ„æºä¼˜åŒ–åˆ†é…ï¼Œå…è®¸åœ¨éå…³é”®åŒºåŸŸè¿›è¡Œæ›´å¤§çš„ç¨€ç–æ€§ã€‚</li>
<li>EB-Diff-Trainæ–¹æ³•æ˜¾è‘—æé«˜äº†DMçš„è®­ç»ƒæ•ˆç‡ï¼Œä¸æœªä¿®å‰ªçš„å¯†é›†æ¨¡å‹ç›¸æ¯”ï¼Œæœ€é«˜å¯æé«˜5.8å€çš„é€Ÿåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸æŸå®³ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹å®ç°äº†å¿«é€Ÿè®­ç»ƒï¼Œä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9763a500e82eb85c18ed2c1c75fa70bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-702c59cbb43456321fc1f35f8f537c9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e55393bfec27d74e081507bfda62235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2652e91f1b54c12eae4600df56502ab8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-993db11d4b3776315b2837e8e261af2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b7f5a63d9a4a4a5b5711451f332cc8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DiffuMural-Restoring-Dunhuang-Murals-with-Multi-scale-Diffusion"><a href="#DiffuMural-Restoring-Dunhuang-Murals-with-Multi-scale-Diffusion" class="headerlink" title="DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion"></a>DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion</h2><p><strong>Authors:Puyu Han, Jiaju Kang, Yuhang Pan, Erting Pan, Zeyu Zhang, Qunchao Jin, Juntao Jiang, Zhichen Liu, Luqi Gong</strong></p>
<p>Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨æ¡ä»¶å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚ç„¶è€Œï¼Œå¤å£ç”»ä¿®å¤ä½œä¸ºè¯¥é¢†åŸŸçš„ä¸€ä¸ªé‡è¦ä¸‹æ¸¸ä»»åŠ¡ï¼Œç”±äºç¼ºé™·é¢ç§¯å¤§ã€è®­ç»ƒæ ·æœ¬ç¨€å°‘ï¼Œå¯¹åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¿®å¤æ–¹æ³•æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æ¡ä»¶ä¿®å¤ä»»åŠ¡æ›´å…³å¿ƒä¿®å¤éƒ¨åˆ†æ˜¯å¦ç¬¦åˆå£ç”»ä¿®å¤çš„å®¡ç¾æ ‡å‡†ï¼Œæ¶‰åŠæ•´ä½“é£æ ¼å’Œç»†èŠ‚æ¥ç¼ç­‰æ–¹é¢ï¼Œè€Œå½“å‰ç ”ç©¶ä¸­ç¼ºä¹è¯„ä¼°å¯å‘å¼å›¾åƒè¡¥å……çš„æŒ‡æ ‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DiffuMuralï¼Œå®ƒæ˜¯ä¸€ç§ç»“åˆå¤šå°ºåº¦æ”¶æ•›å’Œåä½œæ‰©æ•£æœºåˆ¶çš„æ–¹æ³•ï¼Œé€šè¿‡ControlNetå’Œå¾ªç¯ä¸€è‡´æ€§æŸå¤±æ¥ä¼˜åŒ–ç”Ÿæˆå›¾åƒä¸æ¡ä»¶æ§åˆ¶ä¹‹é—´çš„åŒ¹é…ã€‚DiffuMuralåœ¨å£ç”»ä¿®å¤æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œåˆ©ç”¨æ¥è‡ª23å¹…å¤§å‹æ•¦ç…Œå£ç”»çš„ä¸€è‡´è§†è§‰ç¾å­¦è®­ç»ƒæ•°æ®ã€‚è¯¥æ¨¡å‹åœ¨æ¢å¤ç»†èŠ‚ã€å®ç°æ•´ä½“å¤–è§‚è¿è´¯æ€§ï¼Œä»¥åŠè§£å†³å› ç¼ºä¹äº‹å®ä¾æ®è€Œä¸å®Œæ•´çš„å£ç”»æ‰€å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶åŒ…å«äº†å››ä¸ªå…³é”®æŒ‡æ ‡æ¥å®šé‡è¯„ä¼°ä¸å®Œæ•´çš„å£ç”»ï¼šäº‹å®å‡†ç¡®æ€§ã€çº¹ç†ç»†èŠ‚ã€ä¸Šä¸‹æ–‡è¯­ä¹‰å’Œæ•´ä½“è§†è§‰è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èå…¥äº†äººæ–‡ä»·å€¼è¯„ä¼°ï¼Œä»¥ç¡®ä¿ä¿®å¤çš„å£ç”»ä¿ç•™å…¶æ–‡åŒ–å’Œè‰ºæœ¯æ„ä¹‰ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºå½“å‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09513v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ä¼˜ç§€è¡¨ç°ï¼Œæœ¬ç ”ç©¶æå‡ºé’ˆå¯¹å¤å£ç”»ä¿®å¤çš„ä»»åŠ¡ï¼Œæ„å»ºäº†ä¸€ç§å¤šå°ºåº¦æ”¶æ•›ä¸ååŒæ‰©æ•£æœºåˆ¶çš„æ‰©æ•£æ¨¡å‹DiffuMuralã€‚ç»“åˆControlNetå’Œå¾ªç¯ä¸€è‡´æ€§æŸå¤±ä¼˜åŒ–ç”Ÿæˆå›¾åƒä¸æ¡ä»¶æ§åˆ¶çš„åŒ¹é…ã€‚åœ¨å£ç”»ä¿®å¤é¢†åŸŸå±•ç°å“è¶Šèƒ½åŠ›ï¼Œåˆ©ç”¨æ¥è‡ªæ•¦ç…Œçš„å¤§å‹å£ç”»è®­ç»ƒæ•°æ®ï¼Œå®ç°ç»†èŠ‚æ¢å¤ã€æ•´ä½“å¤–è§‚è¿è´¯æ€§å’Œå¯¹ç¼ºä¹äº‹å®ä¾æ®çš„ä¸å®Œæ•´å£ç”»çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¯„ä»·æ¡†æ¶åŒ…æ‹¬å››é¡¹å…³é”®æŒ‡æ ‡å’Œäººç±»ä»·å€¼è¯„ä¼°ï¼Œä»¥ç¡®ä¿ä¿®å¤çš„å£ç”»ä¿ç•™å…¶æ–‡åŒ–å’Œè‰ºæœ¯æ„ä¹‰ã€‚æœ¬ç ”ç©¶çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨æ¡ä»¶å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚</li>
<li>å¤å£ç”»ä¿®å¤æ˜¯æ‰©æ•£æ¨¡å‹é¢ä¸´çš„é‡è¦ä¸‹æ¸¸ä»»åŠ¡ï¼Œå­˜åœ¨å¤§é¢ç§¯ç¼ºé™·å’Œç¨€ç¼ºè®­ç»ƒæ ·æœ¬çš„æŒ‘æˆ˜ã€‚</li>
<li>DiffuMuralæ¨¡å‹ç»“åˆäº†å¤šå°ºåº¦æ”¶æ•›å’ŒååŒæ‰©æ•£æœºåˆ¶ï¼Œä¼˜åŒ–ç”Ÿæˆå›¾åƒä¸æ¡ä»¶æ§åˆ¶çš„åŒ¹é…ã€‚</li>
<li>DiffuMuralåœ¨å£ç”»ä¿®å¤é¢†åŸŸå±•ç°å“è¶Šèƒ½åŠ›ï¼Œåˆ©ç”¨æ¥è‡ªæ•¦ç…Œçš„å¤§å‹å£ç”»è®­ç»ƒæ•°æ®ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶åŒ…æ‹¬å››é¡¹å…³é”®æŒ‡æ ‡ï¼šäº‹å®å‡†ç¡®æ€§ã€çº¹ç†ç»†èŠ‚ã€ä¸Šä¸‹æ–‡è¯­ä¹‰å’Œæ•´ä½“è§†è§‰è¿è´¯æ€§ã€‚</li>
<li>å¼•å…¥äººæ–‡ä»·å€¼è¯„ä¼°ç¡®ä¿ä¿®å¤çš„å£ç”»ä¿ç•™æ–‡åŒ–å’Œè‰ºæœ¯æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc6031a67bc93f8d364594b2685b576f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c4ef794574acdb1e4a0bda7c86ab2c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff47012dd38c265ee8531860c2e0fb8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a103ed427c9b92029565b4900ca7ea90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fe5211deff2f538dec76761f91523cf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="D-2-iT-Dynamic-Diffusion-Transformer-for-Accurate-Image-Generation"><a href="#D-2-iT-Dynamic-Diffusion-Transformer-for-Accurate-Image-Generation" class="headerlink" title="D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation"></a>D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation</h2><p><strong>Authors:Weinan Jia, Mengqi Huang, Nan Chen, Lei Zhang, Zhendong Mao</strong></p>
<p>Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D$^2$iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/jiawn-creator/Dynamic-DiT">https://github.com/jiawn-creator/Dynamic-DiT</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶ç”Ÿæˆé«˜ä¿çœŸå›¾åƒçš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›è®¤å¯ã€‚å°½ç®¡æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰æ¶æ„å…·æœ‰å‡ºè‰²çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼Œä½†åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå®ƒåœ¨ä¸åŒçš„å›¾åƒåŒºåŸŸåº”ç”¨å›ºå®šå‹ç¼©ï¼Œè€Œå¿½ç•¥äº†è¿™äº›åŒºåŸŸä¸­å­˜åœ¨çš„å¤©ç„¶ä¿¡æ¯å¯†åº¦å·®å¼‚ã€‚ç„¶è€Œï¼Œå¤§å‹ç¼©ä¼šå¯¼è‡´å±€éƒ¨çœŸå®æ„Ÿæœ‰é™ï¼Œè€Œå°å‹ç¼©ä¼šå¢åŠ è®¡ç®—å¤æ‚æ€§å¹¶æŸå®³å…¨å±€ä¸€è‡´æ€§ï¼Œæœ€ç»ˆå½±å“ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€å‹ç¼©ä¸åŒå›¾åƒåŒºåŸŸçš„ç­–ç•¥ï¼Œé€šè¿‡è¯†åˆ«ä¸åŒåŒºåŸŸçš„é‡è¦æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼šï¼ˆ1ï¼‰ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åŠ¨æ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆDVAEï¼‰ï¼Œä½¿ç”¨åˆ†å±‚ç¼–ç å™¨ä»¥ä¸åŒçš„ä¸‹é‡‡æ ·ç‡ç¼–ç ä¸åŒçš„å›¾åƒåŒºåŸŸï¼Œä»¥é€‚åº”å…¶ç‰¹å®šçš„ä¿¡æ¯å¯†åº¦ï¼Œä»è€Œä¸ºæ‰©æ•£è¿‡ç¨‹æä¾›æ›´å‡†ç¡®ã€æ›´è‡ªç„¶çš„æ½œåœ¨ä»£ç ã€‚ï¼ˆ2ï¼‰ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åŠ¨æ€æ‰©æ•£å˜å‹å™¨ï¼ˆD^2iTï¼‰ï¼Œé€šè¿‡é¢„æµ‹å¤šç²’åº¦å™ªå£°ç”Ÿæˆå›¾åƒï¼ŒåŒ…æ‹¬ç²—ç²’åº¦ï¼ˆå¹³æ»‘åŒºåŸŸä¸­è¾ƒå°‘çš„æ½œåœ¨ä»£ç ï¼‰å’Œç»†ç²’åº¦ï¼ˆè¯¦ç»†åŒºåŸŸä¸­æ›´å¤šçš„æ½œåœ¨ä»£ç ï¼‰ï¼Œè¿™é€šè¿‡åŠ¨æ€ç²’åº¦å’ŒåŠ¨æ€å†…å®¹çš„å˜å‹å™¨çš„ç»„åˆæ¥å®ç°ã€‚ç»“åˆå™ªå£°çš„ç²—ç•¥é¢„æµ‹å’Œè¯¦ç»†åŒºåŸŸçš„æ ¡æ­£ç­–ç•¥å®ç°äº†å…¨å±€ä¸€è‡´æ€§å’Œå±€éƒ¨çœŸå®æ„Ÿçš„ç»Ÿä¸€ã€‚åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiawn-creator/Dynamic-DiT%E4%B8%8A%E9%87%8A%E5%B8%83%E3%80%82">https://github.com/jiawn-creator/Dynamic-DiTä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09454v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹Diffusion Transformerï¼ˆDiTï¼‰æ¨¡å‹çš„æ”¹è¿›æ–¹æ¡ˆï¼Œè§£å†³äº†å…¶åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹ä¸åŒåŒºåŸŸè¿›è¡Œå›ºå®šå‹ç¼©çš„é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€è¯†åˆ«å›¾åƒåŒºåŸŸçš„é‡è¦æ€§ï¼Œå®ç°åŒºåŸŸåŠ¨æ€å‹ç¼©ï¼Œå¹¶å¼•å…¥ä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬åŠ¨æ€VAEï¼ˆDVAEï¼‰å’ŒåŠ¨æ€æ‰©æ•£å˜å‹å™¨ï¼ˆD^2iTï¼‰ã€‚DVAEé€šè¿‡åˆ†å±‚ç¼–ç å™¨æŒ‰ä¸åŒä¸‹é‡‡æ ·ç‡ç¼–ç å›¾åƒåŒºåŸŸï¼ŒD^2iTåˆ™é€šè¿‡é¢„æµ‹å¤šç²’åº¦å™ªå£°å®ç°å…¨å±€ä¸€è‡´æ€§å’Œå±€éƒ¨ç°å®æ€§çš„ç»Ÿä¸€ã€‚å®éªŒéªŒè¯è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion models can generate high-fidelity images but face challenges in balancing local realism and global consistency.</li>
<li>DiTæ¨¡å‹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¯¹ä¸åŒå›¾åƒåŒºåŸŸåº”ç”¨å›ºå®šå‹ç¼©ï¼Œå¿½ç•¥äº†è‡ªç„¶ä¿¡æ¯å¯†åº¦çš„å·®å¼‚ã€‚</li>
<li>å¤§å‹ç¼©ä¼šå¯¼è‡´å±€éƒ¨ç°å®æ„Ÿæœ‰é™ï¼Œå°å‹ç¼©åˆ™å¢åŠ è®¡ç®—å¤æ‚æ€§å¹¶å½±å“å…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŠ¨æ€å‹ç¼©ä¸åŒå›¾åƒåŒºåŸŸçš„æ–¹æ³•ï¼Œä»¥è¯†åˆ«å„åŒºåŸŸçš„é‡è¦æ€§ã€‚</li>
<li>å¼•å…¥ä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬DVAEå’ŒD^2iTï¼Œå¢å¼ºå›¾åƒç”Ÿæˆçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</li>
<li>DVAEä½¿ç”¨åˆ†å±‚ç¼–ç å™¨ï¼ŒæŒ‰å›¾åƒåŒºåŸŸçš„ä¿¡æ¯å¯†åº¦è¿›è¡Œä¸‹é‡‡æ ·ç¼–ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-63927cca7783a51ea951e7db2efad9b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96186962ca7f3884c97dca6eb530fffd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-465429dcf7afb5f4efc867f6c82bc0a6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Structure-Accurate-Medical-Image-Translation-based-on-Dynamic-Frequency-Balance-and-Knowledge-Guidance"><a href="#Structure-Accurate-Medical-Image-Translation-based-on-Dynamic-Frequency-Balance-and-Knowledge-Guidance" class="headerlink" title="Structure-Accurate Medical Image Translation based on Dynamic Frequency   Balance and Knowledge Guidance"></a>Structure-Accurate Medical Image Translation based on Dynamic Frequency   Balance and Knowledge Guidance</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Multimodal medical images play a crucial role in the precise and comprehensive clinical diagnosis. Diffusion model is a powerful strategy to synthesize the required medical images. However, existing approaches still suffer from the problem of anatomical structure distortion due to the overfitting of high-frequency information and the weakening of low-frequency information. Thus, we propose a novel method based on dynamic frequency balance and knowledge guidance. Specifically, we first extract the low-frequency and high-frequency components by decomposing the critical features of the model using wavelet transform. Then, a dynamic frequency balance module is designed to adaptively adjust frequency for enhancing global low-frequency features and effective high-frequency details as well as suppressing high-frequency noise. To further overcome the challenges posed by the large differences between different medical modalities, we construct a knowledge-guided mechanism that fuses the prior clinical knowledge from a visual language model with visual features, to facilitate the generation of accurate anatomical structures. Experimental evaluations on multiple datasets show the proposed method achieves significant improvements in qualitative and quantitative assessments, verifying its effectiveness and superiority. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ç²¾ç¡®å’Œå…¨é¢çš„ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ‰©æ•£æ¨¡å‹æ˜¯åˆæˆæ‰€éœ€åŒ»å­¦å›¾åƒçš„ä¸€ç§å¼ºå¤§ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»å­˜åœ¨å› é«˜é¢‘ä¿¡æ¯è¿‡åº¦æ‹Ÿåˆè€Œå¯¼è‡´è§£å‰–ç»“æ„æ‰­æ›²çš„é—®é¢˜ï¼Œä»¥åŠä½é¢‘ä¿¡æ¯å‡å¼±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†æŒ‡å¯¼çš„æ–°å‹æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹çš„å…³é”®ç‰¹å¾ï¼Œæå–ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—ï¼Œä»¥è‡ªé€‚åº”åœ°è°ƒæ•´é¢‘ç‡ï¼Œå¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆçš„é«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚ä¸ºäº†å…‹æœä¸åŒåŒ»å­¦æ¨¡æ€ä¹‹é—´å·®å¼‚å·¨å¤§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èåˆäº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ä¸è§†è§‰ç‰¹å¾ï¼Œä»¥ä¿ƒè¿›å‡†ç¡®è§£å‰–ç»“æ„çš„ç”Ÿæˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09441v1">PDF</a> Medical image translation, Diffusion model, 16 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†æŒ‡å¯¼çš„åŒ»å­¦å›¾åƒåˆæˆæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆæˆä¸­å› é«˜é¢‘ä¿¡æ¯è¿‡æ‹Ÿåˆå’Œä½é¢‘ä¿¡æ¯å‡å¼±å¯¼è‡´çš„è§£å‰–ç»“æ„å¤±çœŸé—®é¢˜ã€‚é€šè¿‡å°æ³¢å˜æ¢æå–å›¾åƒçš„ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œè®¾è®¡åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—è‡ªé€‚åº”è°ƒæ•´é¢‘ç‡ï¼Œå¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚æ­¤å¤–ï¼Œæ„å»ºçŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œèåˆè§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ï¼Œä¿ƒè¿›ç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ä¸´åºŠè¯Šæ–­ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯åˆæˆåŒ»å­¦å›¾åƒçš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨è§£å‰–ç»“æ„å¤±çœŸé—®é¢˜ï¼Œä¸»è¦ç”±äºé«˜é¢‘ä¿¡æ¯è¿‡æ‹Ÿåˆå’Œä½é¢‘ä¿¡æ¯å‡å¼±ã€‚</li>
<li>å¼•å…¥åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—ï¼Œè‡ªé€‚åº”è°ƒæ•´é¢‘ç‡ï¼Œå¢å¼ºä½é¢‘å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ï¼ŒæŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚</li>
<li>æå‡ºçŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œèåˆå…ˆéªŒä¸´åºŠçŸ¥è¯†ï¼Œä¿ƒè¿›å‡†ç¡®è§£å‰–ç»“æ„çš„ç”Ÿæˆã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒè¯„ä¼°ï¼Œå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºè§£å†³äº†ç°æœ‰é—®é¢˜ï¼Œæé«˜äº†åŒ»å­¦å›¾åƒåˆæˆçš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59c78c60ea084f22d34cd5f9d3ea7a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b404e101c127f39edab2fb3f81f3efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13b2cc77bb25f602d4436a16eb69167f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f84f0c62d5592bddc9f43b73e0577f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ce23a5ca6d3d3889891fca53d0d3f5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Text-To-3D-Object-Generation-For-Scalable-Room-Assembly"><a href="#Text-To-3D-Object-Generation-For-Scalable-Room-Assembly" class="headerlink" title="Text To 3D Object Generation For Scalable Room Assembly"></a>Text To 3D Object Generation For Scalable Room Assembly</h2><p><strong>Authors:Sonia Laguna, Alberto Garcia-Garcia, Marie-Julie Rakotosaona, Stylianos Moschoglou, Leonhard Helminger, Sergio Orts-Escolano</strong></p>
<p>Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications. </p>
<blockquote>
<p>ç°ä»£ç”¨äºåœºæ™¯ç†è§£çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¦‚æ·±åº¦ä¼°è®¡å’Œå¯¹è±¡è·Ÿè¸ªï¼Œä¾èµ–äºæ¨¡ä»¿çœŸå®ä¸–ç•Œéƒ¨ç½²åœºæ™¯çš„å¤§è§„æ¨¡ã€é«˜è´¨é‡æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç”Ÿæˆå¯æ‰©å±•ã€é«˜è´¨é‡å’Œå¯å®šåˆ¶çš„3Då®¤å†…åœºæ™¯çš„åˆæˆæ•°æ®ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆå’Œé€‚åº”æ–‡æœ¬åˆ°å›¾åƒå’Œå¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œä»¥åŠåŸºäºç¥ç»è¾å°„åœºçš„ç½‘æ ¼åŒ–æŠ€æœ¯ï¼Œä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜ä¿çœŸ3Då¯¹è±¡èµ„äº§ï¼Œå¹¶ä½¿ç”¨æ¸²æŸ“å·¥å…·å°†å…¶èå…¥åˆ°é¢„å…ˆå®šä¹‰çš„å¹³é¢å¸ƒå±€ä¸­ã€‚é€šè¿‡å¼•å…¥æ–°é¢–çš„æŸå¤±å‡½æ•°å’ŒåŸ¹è®­ç­–ç•¥åˆ°ç°æœ‰æ–¹æ³•ä¸­ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒæŒ‰éœ€åœºæ™¯ç”Ÿæˆï¼Œæ—¨åœ¨ç¼“è§£å½“å‰å¯ç”¨æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œè¿™äº›ç°æœ‰æ•°æ®é€šå¸¸éœ€è¦è‰ºæœ¯å®¶æ‰‹åŠ¨åˆ¶ä½œã€‚æ­¤ç³»ç»Ÿæ¨åŠ¨äº†åˆæˆæ•°æ®åœ¨è§£å†³æœºå™¨å­¦ä¹ è®­ç»ƒé™åˆ¶æ–¹é¢çš„ä½œç”¨ï¼Œä¸ºçœŸå®ä¸–ç•Œåº”ç”¨å¯ç”¨äº†æ›´ç¨³å¥å’Œå¯æ¨å¹¿çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09328v1">PDF</a> Published at the ICLR 2025 Workshop on Synthetic Data</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç«¯åˆ°ç«¯çš„åˆæˆæ•°æ®ç”Ÿæˆç³»ç»Ÿï¼Œç”¨äºç”Ÿæˆå¯æ‰©å±•ã€é«˜è´¨é‡ã€å¯å®šåˆ¶çš„3Då®¤å†…åœºæ™¯ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æ–‡æœ¬åˆ°å›¾åƒå’Œå¤šè§†è§’æ‰©æ•£æ¨¡å‹ä¸åŸºäºç¥ç»è¾å°„åœºçš„ç½‘æ ¼åŒ–æŠ€æœ¯ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆé«˜ä¿çœŸåº¦çš„3Då¯¹è±¡èµ„äº§ï¼Œå¹¶å°†å…¶èå…¥é¢„å®šä¹‰çš„å¹³é¢å¸ƒå±€ä¸­ã€‚å¼•å…¥æ–°é¢–çš„æŸå¤±å‡½æ•°å’ŒåŸ¹è®­ç­–ç•¥ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒæŒ‰éœ€ç”Ÿæˆåœºæ™¯ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¯ç”¨æ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œé€šå¸¸è¿™äº›æ•°æ®æ˜¯ç”±è‰ºæœ¯å®¶æ‰‹å·¥åˆ¶ä½œçš„ã€‚æ­¤ç³»ç»Ÿæ¨åŠ¨äº†åˆæˆæ•°æ®åœ¨è§£å†³æœºå™¨å­¦ä¹ è®­ç»ƒé™åˆ¶æ–¹é¢çš„ä½œç”¨ï¼Œä¸ºçœŸå®ä¸–ç•Œåº”ç”¨æä¾›äº†æ›´ç¨³å¥å’Œå¯æ¨å¹¿çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§ç«¯åˆ°ç«¯çš„åˆæˆæ•°æ®ç”Ÿæˆç³»ç»Ÿï¼Œç”¨äºç”Ÿæˆ3Då®¤å†…åœºæ™¯ã€‚</li>
<li>ç³»ç»Ÿç»“åˆäº†æ–‡æœ¬åˆ°å›¾åƒå’Œå¤šè§†è§’æ‰©æ•£æ¨¡å‹æŠ€æœ¯ã€‚</li>
<li>åˆ©ç”¨ç¥ç»è¾å°„åœºåŸºäºç½‘æ ¼åŒ–çš„æŠ€æœ¯ç”Ÿæˆé«˜ä¿çœŸåº¦çš„3Då¯¹è±¡èµ„äº§ã€‚</li>
<li>ç³»ç»Ÿæ”¯æŒå°†ç”Ÿæˆçš„3Då¯¹è±¡èµ„äº§èå…¥é¢„å®šä¹‰çš„å¹³é¢å¸ƒå±€ä¸­ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ–°é¢–çš„æŸå¤±å‡½æ•°å’ŒåŸ¹è®­ç­–ç•¥ï¼Œæ”¯æŒæŒ‰éœ€ç”Ÿæˆåœºæ™¯ã€‚</li>
<li>è¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³å½“å‰æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-909b61142337e1325ad8b9399a253bc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb5ee585dd7073b29a390a5c9f94e097.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f4f660c7064558f8c84b1663a601822.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da3ed3c968d77d763fa6f1f93f7ff9d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f41f1f3cbcbd6ad8c6d0fc4948fc08a4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-Visual-Explanations-to-Counterfactual-Explanations-with-Latent-Diffusion"><a href="#From-Visual-Explanations-to-Counterfactual-Explanations-with-Latent-Diffusion" class="headerlink" title="From Visual Explanations to Counterfactual Explanations with Latent   Diffusion"></a>From Visual Explanations to Counterfactual Explanations with Latent   Diffusion</h2><p><strong>Authors:Tung Luu, Nam Le, Duc Le, Bac Le</strong></p>
<p>Visual counterfactual explanations are ideal hypothetical images that change the decision-making of the classifier with high confidence toward the desired class while remaining visually plausible and close to the initial image. In this paper, we propose a new approach to tackle two key challenges in recent prominent works: i) determining which specific counterfactual features are crucial for distinguishing the â€œconceptâ€ of the target class from the original class, and ii) supplying valuable explanations for the non-robust classifier without relying on the support of an adversarially robust model. Our method identifies the essential region for modification through algorithms that provide visual explanations, and then our framework generates realistic counterfactual explanations by combining adversarial attacks based on pruning the adversarial gradient of the target classifier and the latent diffusion model. The proposed method outperforms previous state-of-the-art results on various evaluation criteria on ImageNet and CelebA-HQ datasets. In general, our method can be applied to arbitrary classifiers, highlight the strong association between visual and counterfactual explanations, make semantically meaningful changes from the target classifier, and provide observers with subtle counterfactual images. </p>
<blockquote>
<p>è§†è§‰åäº‹å®è§£é‡Šæ˜¯ä¸€ç§ç†æƒ³çš„å‡è®¾å›¾åƒï¼Œå®ƒèƒ½å¤Ÿä»¥é«˜ä¿¡å¿ƒæ”¹å˜åˆ†ç±»å™¨çš„å†³ç­–ï¼Œä½¿å†³ç­–è½¬å‘ç›®æ ‡ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸Šçš„åˆç†æ€§å’Œæ¥è¿‘åŸå§‹å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³æœ€è¿‘çƒ­é—¨å·¥ä½œä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€ï¼‰ç¡®å®šå“ªäº›ç‰¹å®šçš„åäº‹å®ç‰¹å¾å¯¹äºåŒºåˆ†ç›®æ ‡ç±»åˆ«çš„â€œæ¦‚å¿µâ€ä¸åŸå§‹ç±»åˆ«è‡³å…³é‡è¦ï¼›äºŒï¼‰åœ¨ä¸ä¾èµ–å¯¹æŠ—æ€§ç¨³å¥æ¨¡å‹çš„æ”¯æŒä¸‹ï¼Œä¸ºä¸ç¨³å¥çš„åˆ†ç±»å™¨æä¾›æœ‰ä»·å€¼çš„è§£é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æä¾›è§†è§‰è§£é‡Šçš„ç®—æ³•æ¥ç¡®å®šéœ€è¦ä¿®æ”¹çš„å…³é”®åŒºåŸŸï¼Œç„¶åæˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ç»“åˆå¯¹æŠ—æ€§æ”»å‡»å’ŒåŸºäºç›®æ ‡åˆ†ç±»å™¨çš„å¯¹æŠ—æ€§æ¢¯åº¦çš„ä¿®å‰ªä»¥åŠæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆç°å®çš„åäº‹å®è§£é‡Šã€‚è¯¥æ–¹æ³•åœ¨ImageNetå’ŒCelebA-HQæ•°æ®é›†ä¸Šçš„å„ç§è¯„ä»·æ ‡å‡†ä¸Šå‡ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›ç»“æœã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºä»»æ„åˆ†ç±»å™¨ï¼Œçªå‡ºäº†è§†è§‰å’Œåäº‹å®è§£é‡Šä¹‹é—´çš„å¼ºçƒˆå…³è”ï¼Œä½¿ç›®æ ‡åˆ†ç±»å™¨èƒ½å¤Ÿè¿›è¡Œè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å˜åŒ–ï¼Œå¹¶ä¸ºè§‚å¯Ÿè€…æä¾›å¾®å¦™çš„åäº‹å®å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09202v1">PDF</a> 2025 IEEE&#x2F;CVF Winter Conference on Applications of Computer Vision   (WACV)</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†è§†è§‰åäº‹å®è§£é‡Šçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç¡®å®šç›®æ ‡ç±»â€œæ¦‚å¿µâ€ä¸åŸå§‹ç±»ä¹‹é—´çš„å…³é”®åäº‹å®ç‰¹å¾ï¼Œä»¥åŠä¸ºéç¨³å¥åˆ†ç±»å™¨æä¾›æœ‰ä»·å€¼çš„è§£é‡Šè€Œä¸ä¾èµ–äºå¯¹æŠ—ç¨³å¥æ¨¡å‹çš„æ”¯æŒã€‚è¯¥æ–¹æ³•é€šè¿‡æä¾›è§†è§‰è§£é‡Šçš„ç®—æ³•ç¡®å®šä¿®æ”¹çš„å…³é”®åŒºåŸŸï¼Œç„¶åé€šè¿‡ç»“åˆå¯¹æŠ—æ€§æ”»å‡»å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé€¼çœŸçš„åäº‹å®è§£é‡Šã€‚åœ¨ImageNetå’ŒCelebA-HQæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¯åº”ç”¨äºä»»æ„åˆ†ç±»å™¨ï¼Œå¼ºè°ƒè§†è§‰ä¸åäº‹å®è§£é‡Šä¹‹é—´çš„å¼ºçƒˆå…³è”ï¼Œä¸ºè§‚å¯Ÿè€…æä¾›å¾®å¦™çš„åäº‹å®å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰åäº‹å®è§£é‡Šæ˜¯é€šè¿‡ç”Ÿæˆç†æƒ³å‡è®¾å›¾åƒæ¥æ”¹å˜åˆ†ç±»å™¨çš„å†³ç­–ï¼Œä½¿å†³ç­–æ›´æœ‰ä¿¡å¿ƒåœ°æœå‘ç›®æ ‡ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸Šçš„åˆç†æ€§å’Œæ¥è¿‘åŸå§‹å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¡®å®šç›®æ ‡ç±»åˆ«ä¸åŸå§‹ç±»åˆ«ä¹‹é—´çš„å…³é”®åäº‹å®ç‰¹å¾ï¼Œä»¥åŠä¸ºéç¨³å¥åˆ†ç±»å™¨æä¾›è§£é‡Šè€Œä¸ä¾èµ–å¯¹æŠ—ç¨³å¥æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æä¾›è§†è§‰è§£é‡Šçš„ç®—æ³•æ¥ç¡®å®šä¿®æ”¹çš„å…³é”®åŒºåŸŸã€‚</li>
<li>ç»“åˆå¯¹æŠ—æ€§æ”»å‡»å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé€¼çœŸçš„åäº‹å®è§£é‡Šã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æ ‡å‡†ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œé€‚ç”¨äºå„ç§å›¾åƒæ•°æ®é›†ã€‚</li>
<li>æ–¹æ³•å¯åº”ç”¨äºä»»æ„åˆ†ç±»å™¨ï¼Œå¼ºè°ƒè§†è§‰ä¸åäº‹å®è§£é‡Šä¹‹é—´çš„å¼ºçƒˆå…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a467d853ecc29cb6b20f76c982c3540d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9b896df3886040e664b71c62160d84e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0b8e3f9410881fb82a094b4496c31c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e4fff29e56740a4454d94c5bda2626f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Sculpting-Memory-Multi-Concept-Forgetting-in-Diffusion-Models-via-Dynamic-Mask-and-Concept-Aware-Optimization"><a href="#Sculpting-Memory-Multi-Concept-Forgetting-in-Diffusion-Models-via-Dynamic-Mask-and-Concept-Aware-Optimization" class="headerlink" title="Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via   Dynamic Mask and Concept-Aware Optimization"></a>Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via   Dynamic Mask and Concept-Aware Optimization</h2><p><strong>Authors:Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma</strong></p>
<p>Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose \textbf{Dynamic Mask coupled with Concept-Aware Loss}, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our \textbf{Dynamic Mask} mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our \textbf{Concept-Aware Loss} explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. The code will be released publicly. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹å·²ç»ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ç„¶è€Œï¼Œå®ƒä»¬å­˜å‚¨å¤§é‡çŸ¥è¯†çš„èƒ½åŠ›åœ¨ä¸€äº›éœ€è¦é€‰æ‹©æ€§é—å¿˜çš„åœºæ™¯ä¸­å¼•å‘äº†å…³æ³¨ï¼Œä¾‹å¦‚å»é™¤ç‰ˆæƒå†…å®¹ã€å‡å°‘åè§æˆ–æ¶ˆé™¤æœ‰å®³æ¦‚å¿µã€‚ç°æœ‰çš„é—å¿˜æ–¹æ³•è™½ç„¶èƒ½ç§»é™¤æŸäº›æ¦‚å¿µï¼Œä½†ç”±äºä¸ç¨³å®šã€æ®‹ç•™çŸ¥è¯†æŒç»­å­˜åœ¨ä»¥åŠç”Ÿæˆè´¨é‡ä¸‹é™ï¼Œå®ƒä»¬åœ¨å¤šæ¦‚å¿µé—å¿˜æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œåŠ¨æ€æ©ç ä¸æ¦‚å¿µæ„ŸçŸ¥æŸå¤±â€çš„æ–°å‹é—å¿˜æ¡†æ¶ï¼Œæ—¨åœ¨å¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šæ¦‚å¿µé—å¿˜é—®é¢˜ã€‚æˆ‘ä»¬çš„â€œåŠ¨æ€æ©ç â€æœºåˆ¶èƒ½åŸºäºå½“å‰ä¼˜åŒ–çŠ¶æ€è‡ªé€‚åº”åœ°æ›´æ–°æ¢¯åº¦æ©ç ï¼Œå…è®¸é€‰æ‹©æ€§åœ°ä¿®æ”¹æƒé‡ä»¥é¿å…å¹²æ‰°æ— å…³çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„â€œæ¦‚å¿µæ„ŸçŸ¥æŸå¤±â€é€šè¿‡è¶…ç±»å¯¹é½æ˜ç¡®æŒ‡å¯¼é—å¿˜è¿‡ç¨‹ï¼ŒåŒæ—¶é€šè¿‡åŸºäºçŸ¥è¯†è’¸é¦çš„æ­£åˆ™åŒ–æŸå¤±ç¡®ä¿å…ˆå‰æœªå­¦ä¹ çš„æ¦‚å¿µåœ¨è¿ç»­é—å¿˜è¿‡ç¨‹ä¸­ä¿æŒé—å¿˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒæ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜æœ‰æ•ˆæ€§ã€è¾“å‡ºä¿çœŸåº¦å’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢ä¼˜äºç°æœ‰é—å¿˜æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¦‚å¿µåœºæ™¯ä¸­ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºç”Ÿæˆæ¨¡å‹ä¸­çš„ç¨³å®šå’Œé«˜ä¿çœŸé—å¿˜æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™å’Œçµæ´»æ€§çš„æ¡†æ¶ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09039v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå…¶å­˜å‚¨å¤§é‡çŸ¥è¯†çš„èƒ½åŠ›åœ¨æŸäº›éœ€è¦é€‰æ‹©æ€§é—å¿˜çš„åœºæ™¯ä¸­å¼•å‘äº†æ‹…å¿§ï¼Œå¦‚å»é™¤ç‰ˆæƒå†…å®¹ã€å‡å°‘åè§æˆ–æ¶ˆé™¤æœ‰å®³æ¦‚å¿µã€‚ä¸ºäº†åº”å¯¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œåŠ¨æ€æ©è†œä¸æ¦‚å¿µæ„ŸçŸ¥æŸå¤±â€çš„æ–°å‹é—å¿˜æ¡†æ¶ï¼Œç”¨äºå¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šæ¦‚å¿µé—å¿˜é—®é¢˜ã€‚æˆ‘ä»¬çš„åŠ¨æ€æ©è†œæœºåˆ¶èƒ½å¤Ÿæ ¹æ®å½“å‰çš„ä¼˜åŒ–çŠ¶æ€è‡ªé€‚åº”åœ°æ›´æ–°æ¢¯åº¦æ©è†œï¼Œå®ç°é€‰æ‹©æ€§æƒé‡ä¿®æ”¹ï¼Œé¿å…å¹²æ‰°æ— å…³çŸ¥è¯†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ¦‚å¿µæ„ŸçŸ¥æŸå¤±é€šè¿‡è¶…ç±»å¯¹é½æ–¹å¼æ˜ç¡®æŒ‡å¯¼é—å¿˜è¿‡ç¨‹ï¼Œä¿è¯è¯­ä¹‰ä¸€è‡´æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç°å‡ºä¼˜å¼‚çš„é—å¿˜æ•ˆæœã€è¾“å‡ºä¿çœŸåº¦å’Œè¯­ä¹‰è¿è´¯æ€§ï¼Œå°¤å…¶åœ¨å¤šæ¦‚å¿µåœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºç”Ÿæˆæ¨¡å‹ä¸­çš„ç¨³å®šå’Œé«˜ä¿çœŸé—å¿˜æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™å’Œçµæ´»æ€§çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>åœ¨éœ€è¦é€‰æ‹©æ€§é—å¿˜çš„åœºæ™¯ä¸­ï¼ˆå¦‚å»é™¤ç‰ˆæƒå†…å®¹ã€å‡å°‘åè§ç­‰ï¼‰ï¼Œè¿™äº›æ¨¡å‹çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•å®ç°å¤šæ¦‚å¿µé—å¿˜ã€‚</li>
<li>ç°æœ‰é—å¿˜æ–¹æ³•åœ¨å¤„ç†å¤šæ¦‚å¿µé—å¿˜æ—¶é¢ä¸´ç¨³å®šæ€§ã€æ®‹ç•™çŸ¥è¯†æŒä¹…æ€§å’Œç”Ÿæˆè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„â€œåŠ¨æ€æ©è†œä¸æ¦‚å¿µæ„ŸçŸ¥æŸå¤±â€æ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>åŠ¨æ€æ©è†œæœºåˆ¶èƒ½å¤Ÿè‡ªé€‚åº”æ›´æ–°æ¢¯åº¦æ©è†œï¼Œå®ç°é€‰æ‹©æ€§æƒé‡ä¿®æ”¹ï¼Œé¿å…å¹²æ‰°æ— å…³çŸ¥è¯†ã€‚</li>
<li>æ¦‚å¿µæ„ŸçŸ¥æŸå¤±é€šè¿‡è¶…ç±»å¯¹é½æ˜ç¡®æŒ‡å¯¼é—å¿˜è¿‡ç¨‹ï¼Œç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-60dca484263aab096a5b3b0efaa88f28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-711937d5ced94a074bb100564205b892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac74c88ecfc93593677a68b0b1c2a301.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1168143a7f02442c618bad9f662946ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd12f9695ff1384d6d994cc72ec83391.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models"><a href="#ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models" class="headerlink" title="ID-Booth: Identity-consistent Face Generation with Diffusion Models"></a>ID-Booth: Identity-consistent Face Generation with Diffusion Models</h2><p><strong>Authors:Darian TomaÅ¡eviÄ‡, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Å truc, Peter Peer</strong></p>
<p>Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at <a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆå»ºæ¨¡æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆé€‚ç”¨äºå¤šç§é¢†åŸŸçš„é«˜è´¨é‡åˆæˆæ•°æ®ï¼Œå…¶ä¸­åŒ…æ‹¬äººè„¸è¯†åˆ«ã€‚åœ¨è¿™é‡Œï¼Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºå¼ºå¤§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶å’Œå¾®è°ƒï¼Œä»¥ä¿ƒè¿›åˆæˆå…·æœ‰æ‰€éœ€èº«ä»½çš„é€¼çœŸå›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾€å¾€ä¸è€ƒè™‘ä¸»ä½“çš„èº«ä»½ï¼Œå¯¼è‡´ç”Ÿæˆèº«ä»½ä¸é¢„æœŸèº«ä»½ä¹‹é—´çš„ä¸€è‡´æ€§è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡‡ç”¨åŸºäºèº«ä»½çš„è®­ç»ƒç›®æ ‡çš„æ–¹æ³•å¾€å¾€ä¼šåœ¨èº«ä»½çš„å„ä¸ªæ–¹é¢å‡ºç°è¿‡æ‹Ÿåˆç°è±¡ï¼Œè¿›è€Œé™ä½äº†å¯ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºID-Boothã€‚ID-Boothç”±ä¸€ä¸ªè´Ÿè´£æ•°æ®ç”Ÿæˆçš„é™å™ªç½‘ç»œã€ä¸€ä¸ªç”¨äºå°†å›¾åƒæ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´åŠå…¶åå‘æ˜ å°„çš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ç»„æˆï¼Œè¯¥æ–‡æœ¬ç¼–ç å™¨å¯å®ç°åŸºäºæç¤ºå¯¹ç”Ÿæˆè¿‡ç¨‹çš„æ§åˆ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†ä¸€ç§æ–°å‹çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆã€‚ä½¿ç”¨æœ€å…ˆè¿›çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¤šç§æç¤ºè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½å†…éƒ¨ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„æ•°æ®èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºå°è§„æ¨¡æ•°æ®é›†ï¼Œä»¥éšç§ä¿æŠ¤çš„æ–¹å¼è®­ç»ƒæ€§èƒ½æ›´å¥½çš„è¯†åˆ«æ¨¡å‹ã€‚ID-Boothæ¡†æ¶çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07392v2">PDF</a> IEEE International Conference on Automatic Face and Gesture   Recognition (FG) 2025, 14 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ID-Boothï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å»å™ªç½‘ç»œã€å˜åˆ†è‡ªç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå›¾åƒæ—¶å®ç°åŸºäºèº«ä»½çš„æç¤ºæ§åˆ¶ã€‚ID-Boothé‡‡ç”¨æ–°é¢–çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒID-Boothåœ¨èº«ä»½å†…ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†æ€§æ–¹é¢è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚è¿™ä¸ºå°å‹æ•°æ®é›†çš„æœ‰æ•ˆæ‰©å……å’Œæ€§èƒ½æ›´ä½³çš„éšç§ä¿æŠ¤è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒæä¾›äº†å¯èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„è°ƒæ•´å’Œå¾®è°ƒï¼Œä»¥åˆæˆå…·æœ‰æ‰€éœ€èº«ä»½çš„é€¼çœŸå›¾åƒã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´å¾ˆå°‘è€ƒè™‘èº«ä»½å› ç´ ï¼Œå¯¼è‡´ç”Ÿæˆèº«ä»½ä¸é¢„æœŸèº«ä»½ä¹‹é—´çš„ä¸€è‡´æ€§è¾ƒå·®ã€‚</li>
<li>åŸºäºèº«ä»½çš„è®­ç»ƒç›®æ ‡æ–¹æ³•å¾€å¾€è¿‡åˆ†å…³æ³¨èº«ä»½çš„å„ä¸ªæ–¹é¢ï¼Œä»è€Œé™ä½äº†å¯ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ã€‚</li>
<li>ID-Boothé€šè¿‡ç»“åˆå»å™ªç½‘ç»œã€å˜åˆ†è‡ªç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ID-Boothé‡‡ç”¨æ–°é¢–çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œå®ç°äº†èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒID-Boothåœ¨ä¿æŒå›¾åƒå¤šæ ·æ€§çš„åŒæ—¶ï¼Œæé«˜äº†èº«ä»½å†…çš„ä¸€è‡´æ€§å’Œèº«ä»½é—´çš„å¯åˆ†æ€§ã€‚</li>
<li>ID-Boothäº§ç”Ÿçš„æ•°æ®å¯ç”¨äºæœ‰æ•ˆåœ°æ‰©å……å°å‹æ•°æ®é›†ï¼Œå¹¶ä»¥éšç§ä¿æŠ¤çš„æ–¹å¼è®­ç»ƒæ€§èƒ½æ›´ä½³çš„è¯†åˆ«æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3054bab5118e188633e9900a2369beba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a1325fe4dfc46da49d23433810504b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-814022dbf79c1a1cc8eb9da0ac5986c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising"><a href="#Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising" class="headerlink" title="Solving Inverse Problems using Diffusion with Iterative Colored   Renoising"></a>Solving Inverse Problems using Diffusion with Iterative Colored   Renoising</h2><p><strong>Authors:Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter</strong></p>
<p>Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and â€œrenoisesâ€ the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting â€œDDfireâ€ offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval. Our implementation is at <a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire">https://github.com/matt-bendel/DDfire</a> </p>
<blockquote>
<p>ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»¥æ— ç›‘ç£çš„æ–¹å¼è§£å†³æˆåƒåé—®é¢˜ï¼Œéœ€è¦åœ¨æ‰©æ•£åå‘è¿‡ç¨‹ä¸­è¿‘ä¼¼æµ‹é‡æ¡ä»¶å¾—åˆ†å‡½æ•°çš„æ¢¯åº¦ã€‚æˆ‘ä»¬æ˜¾ç¤ºç°æœ‰æ–¹æ³•äº§ç”Ÿçš„è¿‘ä¼¼å€¼ç›¸å¯¹è¾ƒå·®ï¼Œå°¤å…¶æ˜¯åœ¨åå‘è¿‡ç¨‹çš„æ—©æœŸï¼Œå› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­å¤šæ¬¡é‡æ–°ä¼°è®¡å’Œâ€œå¢åŠ å™ªå£°â€ã€‚æˆ‘ä»¬ç§°è¿™ç§è¿­ä»£æ–¹æ³•ä¸ºå¿«é€Ÿè¿­ä»£é‡å™ªå£°åŒ–ï¼ˆFIREï¼‰ï¼Œå®ƒæ³¨å…¥æœ‰è‰²å™ªå£°ï¼Œä»¥ç¡®ä¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å§‹ç»ˆçœ‹åˆ°ç™½å™ªå£°ï¼Œè¿™ä¸å®ƒçš„è®­ç»ƒæ–¹å¼ç›¸ç¬¦ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†FIREåµŒå…¥åˆ°DDIMåå‘è¿‡ç¨‹ä¸­ï¼Œå¹¶è¯æ˜ç”±æ­¤äº§ç”Ÿçš„DDfireåœ¨å¤šä¸ªçº¿æ€§åé—®é¢˜ä»¥åŠç›¸ä½æ£€ç´¢æ–¹é¢æä¾›äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œè¿è¡Œæ—¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®ç°ä½äº<a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire%E3%80%82">https://github.com/matt-bendel/DDfireã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17468v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»¥æ— ç›‘ç£çš„æ–¹å¼è§£å†³æˆåƒé€†é—®é¢˜ï¼Œéœ€è¦åœ¨æ‰©æ•£é€†è¿‡ç¨‹ä¸­è¿‘ä¼¼æµ‹é‡æ¡ä»¶åˆ†æ•°å‡½æ•°çš„æ¢¯åº¦ã€‚ç°æœ‰æ–¹æ³•äº§ç”Ÿçš„è¿‘ä¼¼å€¼è¾ƒå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨é€†è¿‡ç¨‹çš„æ—©æœŸï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¿«é€Ÿè¿­ä»£é‡å™ªå£°æ³•ï¼ˆFIREï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­å¤šæ¬¡é‡æ–°ä¼°è®¡å’Œâ€œåŠ å…¥å™ªå£°â€ã€‚æˆ‘ä»¬å°†FIREåµŒå…¥åˆ°DDIMé€†è¿‡ç¨‹ä¸­ï¼Œå¹¶è¯æ˜â€œDDfireâ€åœ¨å¤šä¸ªçº¿æ€§é€†é—®é¢˜å’Œç›¸ä½æ£€ç´¢æ–¹é¢æä¾›äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œè¿è¡Œæ—¶æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯ä»¥æ— ç›‘ç£åœ°è§£å†³æˆåƒé€†é—®é¢˜ã€‚</li>
<li>éœ€è¦è¿‘ä¼¼æµ‹é‡æ¡ä»¶åˆ†æ•°å‡½æ•°çš„æ¢¯åº¦åœ¨æ‰©æ•£é€†è¿‡ç¨‹ä¸­ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„è¿‘ä¼¼å€¼åœ¨æ‰©æ•£é€†è¿‡ç¨‹æ—©æœŸè¡¨ç°è¾ƒå·®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¿«é€Ÿè¿­ä»£é‡å™ªå£°æ³•ï¼ˆFIREï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨æ‰©æ•£è¿‡ç¨‹çš„æ¯ä¸ªæ­¥éª¤ä¸­å¤šæ¬¡é‡æ–°ä¼°è®¡å’ŒåŠ å…¥å™ªå£°ã€‚</li>
<li>FIREæ–¹æ³•é€šè¿‡æ³¨å…¥æœ‰è‰²å™ªå£°ï¼Œç¡®ä¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å§‹ç»ˆçœ‹åˆ°ç™½å™ªå£°ï¼Œä¸å…¶è®­ç»ƒæ–¹å¼ä¸€è‡´ã€‚</li>
<li>å°†FIREåµŒå…¥åˆ°DDIMé€†è¿‡ç¨‹ä¸­ï¼Œå½¢æˆäº†â€œDDfireâ€æ–¹æ³•ã€‚</li>
<li>â€œDDfireâ€åœ¨å¤šä¸ªçº¿æ€§é€†é—®é¢˜å’Œç›¸ä½æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºæœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œè¿è¡Œæ—¶æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff568fabae0abd2cbb6e3227c56cdb32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f5a0d543b4b18350a3d7847871ef27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-905c2f5a68de50a71c724b8331d17d2f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation"><a href="#StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation" class="headerlink" title="StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation"></a>StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation</h2><p><strong>Authors:Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang</strong></p>
<p>Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGenâ€™s superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen">https://zju3dv.github.io/StarGen</a>. </p>
<blockquote>
<p>è¿‘æœŸé‡å»ºå’Œç”Ÿæˆæ¨¡å‹æ–¹é¢çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨æ–­éƒ½å±€é™äºä¸€ä¸ªå°èŒƒå›´ï¼Œä½¿å¾—å¤§èŒƒå›´ä¸€è‡´çš„åœºæ™¯ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StarGenï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œä»¥è‡ªå›å½’çš„æ–¹å¼è¿›è¡Œå¤§èŒƒå›´åœºæ™¯ç”Ÿæˆã€‚æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„ç”Ÿæˆéƒ½åŸºäºç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œä¹‹å‰ç”Ÿæˆçš„ç‰‡æ®µçš„æ—¶é—´é‡å å›¾åƒï¼Œè¿™æé«˜äº†å¤§èŒƒå›´åœºæ™¯ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†ç²¾ç¡®çš„å§¿æ€æ§åˆ¶ã€‚è¿™ç§æ—¶ç©ºæ¡ä»¶ä¸å„ç§è¾“å…¥æ¡ä»¶å…¼å®¹ï¼Œèƒ½å¤Ÿä¿ƒè¿›å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶çš„åŸå¸‚ç”Ÿæˆã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒStarGenåœ¨å¯æ‰©å±•æ€§ã€ä¿çœŸåº¦å’Œå§¿æ€å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen%E3%80%82">https://zju3dv.github.io/StarGenã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05763v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°æ”¹è¿›äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—é™åˆ¶ï¼Œè¿™äº›å¤§å‹æ¨¡å‹çš„æ¯æ¬¡æ¨æ–­éƒ½å±€é™äºå°èŒƒå›´ï¼Œä½¿å¾—é•¿è·ç¦»ä¸€è‡´åœºæ™¯ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StarGenæ¡†æ¶ï¼Œå®ƒé‡‡ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œè‡ªå›å½’é•¿è·ç¦»åœºæ™¯ç”Ÿæˆã€‚æ¯ä¸ªè§†é¢‘å‰ªè¾‘çš„ç”ŸæˆåŸºäºç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘ä¸­æ—¶é—´ä¸Šé‡å çš„å›¾åƒï¼Œæé«˜äº†é•¿è·ç¦»åœºæ™¯ç”Ÿæˆçš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†ç²¾ç¡®çš„å§¿æ€æ§åˆ¶ã€‚è¿™ç§æ—¶ç©ºæ¡ä»¶é€‚ç”¨äºå„ç§è¾“å…¥æ¡ä»¶ï¼Œå¯ä»¥ä¿ƒè¿›åŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆåœ¨å†…çš„å„ç§ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é‡å»ºå’Œç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’ç”Ÿæˆçš„è¿›æ­¥ã€‚</li>
<li>ç”±äºè®¡ç®—é™åˆ¶ï¼Œç°æœ‰æ¨¡å‹åœ¨é•¿è·ç¦»åœºæ™¯ç”Ÿæˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>StarGenæ¡†æ¶é‡‡ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œè‡ªå›å½’é•¿è·ç¦»åœºæ™¯ç”Ÿæˆã€‚</li>
<li>StarGenåˆ©ç”¨ç©ºé—´ç›¸é‚»å›¾åƒçš„3Då˜å½¢å’Œå…ˆå‰ç”Ÿæˆçš„å‰ªè¾‘ä¸­çš„é‡å å›¾åƒï¼Œæé«˜æ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
<li>StarGenå…·å¤‡ç²¾ç¡®çš„å§¿æ€æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¨€ç–è§†å›¾æ’å€¼ã€æ°¸ä¹…è§†å›¾ç”Ÿæˆå’Œå¸ƒå±€æ§åˆ¶åŸå¸‚ç”Ÿæˆç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22b127f4087696eff2c46cbb2d3e8e10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cf0e8dcc9abb373f6f850fcd56b32a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1a7dec6813a97f986d23fee4a718fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a973b1603e00e9a3843ad9f288cc4aa7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Navigating-Image-Restoration-with-VARâ€™s-Distribution-Alignment-Prior"><a href="#Navigating-Image-Restoration-with-VARâ€™s-Distribution-Alignment-Prior" class="headerlink" title="Navigating Image Restoration with VARâ€™s Distribution Alignment Prior"></a>Navigating Image Restoration with VARâ€™s Distribution Alignment Prior</h2><p><strong>Authors:Siyang Wang, Feng Zhao</strong></p>
<p>Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VARâ€™s adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ç»è¿‡å¤§é‡é«˜è´¨é‡æ•°æ®é›†çš„è®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ¸…æ´å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œä½¿å…¶åœ¨å›¾åƒæ¢å¤ä¸­å°†é€€åŒ–ç‰¹å¾è½¬æ¢ä¸ºæ¸…æ´ç‰¹å¾æ–¹é¢å…·æœ‰å¼ºå¤§çš„å…ˆéªŒèƒ½åŠ›ã€‚VARä½œä¸ºä¸€ç§æ–°å‹å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡åº”ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡è‡ªå›å½’è¿‡ç¨‹é€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œè¿™ä¸æ¢å¤ç¤¾åŒºå¹¿æ³›è®¤å¯çš„å¤šå°ºåº¦æ¢å¤åŸåˆ™ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨VARè¿›è¡Œå›¾åƒé‡å»ºè¿‡ç¨‹æ—¶è§‚å¯Ÿåˆ°ï¼Œå°ºåº¦é¢„æµ‹ä¼šè‡ªåŠ¨è°ƒåˆ¶è¾“å…¥ï¼Œä¾¿äºåç»­å°ºåº¦çš„è¡¨ç¤ºä¸æ¸…æ´å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚ä¸ºäº†åˆ©ç”¨VARåœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†VARå†…çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºæ¢å¤å…ˆéªŒï¼Œä»è€Œæ¨è¿›äº†æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„VarFormeræ¡†æ¶ã€‚è¿™äº›å…ˆéªŒçš„æˆ˜ç•¥åº”ç”¨ä½¿æˆ‘ä»¬çš„VarFormeråœ¨æœªè§è¿‡ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æ³›åŒ–ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„VarFormeråœ¨å„ç§æ¢å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å¤šä»»åŠ¡å›¾åƒæ¢å¤æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21063v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹èƒ½æœ‰æ•ˆæ•æ‰æ¸…æ™°å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œå¯ä½œä¸ºå›¾åƒä¿®å¤ä¸­å°†é€€åŒ–ç‰¹å¾è½¬åŒ–ä¸ºæ¸…æ™°ç‰¹å¾çš„å¼ºå¤§å…ˆéªŒã€‚VARä½œä¸ºä¸€ç§æ–°å‹å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚VARéµå¾ªå¹¿æ³›è®¤å¯çš„å›¾åƒä¿®å¤å¤šå°ºåº¦æ¢å¤åŸåˆ™ï¼Œé€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ã€‚åœ¨åˆ©ç”¨VARè¿›è¡Œå›¾åƒé‡å»ºæ—¶ï¼Œå°ºåº¦é¢„æµ‹ä¼šè‡ªåŠ¨è°ƒæ•´è¾“å…¥ï¼Œä¿ƒè¿›åç»­å°ºåº¦è¡¨ç¤ºä¸å¹²å‡€å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚æˆ‘ä»¬å°†VARä¸­çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºä¿®å¤å…ˆéªŒï¼Œæ„å»ºäº†ç²¾å¿ƒè®¾è®¡çš„VarFormeræ¡†æ¶ï¼Œåˆ©ç”¨å…¶è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ã€‚VarFormeråœ¨æœªè§ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒVarFormeråœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰å¤šä»»åŠ¡å›¾åƒä¿®å¤æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åŸºäºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å¯æœ‰æ•ˆæ•æ‰å›¾åƒç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ã€‚</li>
<li>VARæˆä¸ºæ–°çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡å°ºåº¦é¢„æµ‹æå‡ç”Ÿæˆè´¨é‡ã€‚</li>
<li>VARéµå¾ªå¤šå°ºåº¦æ¢å¤åŸåˆ™ï¼Œèƒ½é€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>åœ¨å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­ï¼ŒVARçš„å°ºåº¦é¢„æµ‹èƒ½è‡ªåŠ¨è°ƒæ•´è¾“å…¥ï¼Œä¿ƒè¿›åç»­å°ºåº¦ä¸å¹²å‡€å›¾åƒåˆ†å¸ƒå¯¹é½ã€‚</li>
<li>VarFormeråˆ©ç”¨VARä¸­çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºä¿®å¤å…ˆéªŒã€‚</li>
<li>VarFormerå®ç°å“è¶Šæ³›åŒ–èƒ½åŠ›å¹¶é™ä½è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-460f3bbfb724c48a3f1a79775c244a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3874c7c5a9090b0a5be87bf228b5d2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a35552c3b292b12b5879f78487d231d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34a2362ccb0b54582a79c040db28c626.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97022af2dfa48cd126f6af7ccdfc2034.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Nearly-Zero-Cost-Protection-Against-Mimicry-by-Personalized-Diffusion-Models"><a href="#Nearly-Zero-Cost-Protection-Against-Mimicry-by-Personalized-Diffusion-Models" class="headerlink" title="Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion   Models"></a>Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion   Models</h2><p><strong>Authors:Namhyuk Ahn, KiYoon Yoo, Wonhyuk Ahn, Daesik Kim, Seung-Hun Nam</strong></p>
<p>Recent advancements in diffusion models revolutionize image generation but pose risks of misuse, such as replicating artworks or generating deepfakes. Existing image protection methods, though effective, struggle to balance protection efficacy, invisibility, and latency, thus limiting practical use. We introduce perturbation pre-training to reduce latency and propose a mixture-of-perturbations approach that dynamically adapts to input images to minimize performance degradation. Our novel training strategy computes protection loss across multiple VAE feature spaces, while adaptive targeted protection at inference enhances robustness and invisibility. Experiments show comparable protection performance with improved invisibility and drastically reduced inference time. The code and demo are available at <a target="_blank" rel="noopener" href="https://webtoon.github.io/impasto">https://webtoon.github.io/impasto</a> </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•è™½ç„¶ä¸ºå›¾åƒç”Ÿæˆå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ»¥ç”¨é£é™©ï¼Œæ¯”å¦‚å¤åˆ¶è‰ºæœ¯å“æˆ–ç”Ÿæˆæ·±åº¦ä¼ªé€ å›¾åƒã€‚è™½ç„¶ç°æœ‰å›¾åƒä¿æŠ¤æ–¹æ³•æœ‰æ•ˆï¼Œä½†åœ¨ä¿æŠ¤æ•ˆæœã€éšå½¢æ€§å’Œå»¶è¿Ÿä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œä»è€Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥æ‰°åŠ¨é¢„è®­ç»ƒä»¥å‡å°‘å»¶è¿Ÿï¼Œå¹¶æå‡ºä¸€ç§æ··åˆæ‰°åŠ¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯åŠ¨æ€é€‚åº”è¾“å…¥å›¾åƒï¼Œä»¥æœ€å°åŒ–æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æ–°é¢–çš„è®­ç»ƒç­–ç•¥è®¡ç®—äº†å¤šä¸ªVAEç‰¹å¾ç©ºé—´ä¸­çš„ä¿æŠ¤æŸå¤±ï¼ŒåŒæ—¶æ¨ç†é˜¶æ®µçš„è‡ªé€‚åº”ç›®æ ‡ä¿æŠ¤å¢å¼ºäº†ç¨³å¥æ€§å’Œéšå½¢æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¿æŠ¤æ€§èƒ½ç›¸å½“ï¼Œéšå½¢æ€§æœ‰æ‰€æé«˜ï¼Œæ¨ç†æ—¶é—´å¤§å¤§å‡å°‘ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://webtoon.github.io/impasto">https://webtoon.github.io/impasto</a>æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11423v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„è¿›æ­¥æ¨åŠ¨äº†è‰ºæœ¯åˆ›æ–°ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ»¥ç”¨é£é™©ï¼Œå¦‚å¤åˆ¶è‰ºæœ¯å“æˆ–ç”Ÿæˆæ·±åº¦ä¼ªé€ å›¾åƒã€‚ç°æœ‰å›¾åƒä¿æŠ¤æ–¹æ³•åœ¨ä¿æŠ¤æ•ˆæœã€éšå½¢æ€§å’Œå»¶è¿Ÿä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥æ‰°åŠ¨é¢„è®­ç»ƒä»¥é™ä½å»¶è¿Ÿï¼Œå¹¶æå‡ºæ··åˆæ‰°åŠ¨æ–¹æ³•ï¼ŒåŠ¨æ€é€‚åº”è¾“å…¥å›¾åƒä»¥æœ€å°åŒ–æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬çš„æ–°è®­ç»ƒç­–ç•¥è®¡ç®—å¤šä¸ªVAEç‰¹å¾ç©ºé—´ä¸­çš„ä¿æŠ¤æŸå¤±ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶è¿›è¡Œè‡ªé€‚åº”é¶å‘ä¿æŠ¤ï¼Œä»¥å¢å¼ºç¨³å¥æ€§å’Œéšå½¢æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¿æŠ¤æ€§èƒ½ç›¸å½“ï¼Œéšå½¢æ€§æé«˜ï¼Œæ¨ç†æ—¶é—´å¤§å¹…é™ä½ã€‚ç›¸å…³ä»£ç å’Œæ¼”ç¤ºå¯åœ¨webtoon.github.io&#x2F;impastoä¸ŠæŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†å›¾åƒç”Ÿæˆçš„å‘å±•ï¼Œä½†å­˜åœ¨æ»¥ç”¨é£é™©ã€‚</li>
<li>ç°æœ‰å›¾åƒä¿æŠ¤æ–¹æ³•åœ¨å¹³è¡¡ä¿æŠ¤æ•ˆæœã€éšå½¢æ€§å’Œå»¶è¿Ÿæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>å¼•å…¥æ‰°åŠ¨é¢„è®­ç»ƒä»¥é™ä½å»¶è¿Ÿå’Œæé«˜æ€§èƒ½ã€‚</li>
<li>æå‡ºæ··åˆæ‰°åŠ¨æ–¹æ³•ï¼ŒåŠ¨æ€é€‚åº”è¾“å…¥å›¾åƒã€‚</li>
<li>æ–°è®­ç»ƒç­–ç•¥è®¡ç®—å¤šä¸ªVAEç‰¹å¾ç©ºé—´ä¸­çš„ä¿æŠ¤æŸå¤±ã€‚</li>
<li>è‡ªé€‚åº”é¶å‘ä¿æŠ¤å¢å¼ºç¨³å¥æ€§å’Œéšå½¢æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºä¿æŠ¤æ€§èƒ½ç›¸å½“ï¼Œéšå½¢æ€§æé«˜ï¼Œæ¨ç†æ—¶é—´å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b22c3a20357f03d1ffb3b40f79472fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6689a16fdb4c438f40447b6ba7878adc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a25e0a3fe7d8715d2288b31f628e940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd8d6741cb53e18f02af421d0a1e8843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b65add2678bc8eb9dbbe33652fc51cf8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GAF-Gaussian-Avatar-Reconstruction-from-Monocular-Videos-via-Multi-view-Diffusion"><a href="#GAF-Gaussian-Avatar-Reconstruction-from-Monocular-Videos-via-Multi-view-Diffusion" class="headerlink" title="GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view   Diffusion"></a>GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view   Diffusion</h2><p><strong>Authors:Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner</strong></p>
<p>We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘é‡å»ºå¯åŠ¨ç”»çš„3Dé«˜æ–¯åŒ–èº«çš„æ–°æ–¹æ³•ï¼Œè¿™äº›è§†é¢‘æ˜¯ç”±æ™ºèƒ½æ‰‹æœºç­‰å•†å“è®¾å¤‡æ•æ‰çš„ã€‚ä»è¿™ç§è®°å½•ä¸­è¿›è¡Œé€¼çœŸçš„3Då¤´éƒ¨åŒ–èº«é‡å»ºæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè§‚å¯Ÿæœ‰é™ï¼Œå¯¼è‡´æœªè§‚æµ‹åŒºåŸŸçº¦æŸä¸è¶³ï¼Œå¹¶åœ¨æ–°è§†è§’ä¸­äº§ç”Ÿä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè§†è§’å¤´éƒ¨æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨å…¶å…ˆéªŒçŸ¥è¯†æ¥å¡«å……ç¼ºå¤±åŒºåŸŸå¹¶ç¡®ä¿é«˜æ–¯å¹³é“ºæ¸²æŸ“ä¸­çš„è§†è§’ä¸€è‡´æ€§ã€‚ä¸ºäº†å®ç°ç²¾ç¡®çš„è§‚ç‚¹æ§åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºFLAMEçš„å¤´éƒ¨é‡å»ºæ¸²æŸ“çš„æ³•çº¿å›¾ï¼Œè¿™æä¾›äº†åƒç´ å¯¹é½çš„å½’çº³åè§ã€‚æˆ‘ä»¬è¿˜æ ¹æ®è¾“å…¥å›¾åƒæå–çš„VAEç‰¹å¾å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥ä¿ç•™é¢éƒ¨èº«ä»½å’Œå¤–è§‚ç»†èŠ‚ã€‚å¯¹äºé«˜æ–¯åŒ–èº«é‡å»ºï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è¿­ä»£å»å™ªå›¾åƒä½œä¸ºä¼ªçœŸå®å€¼æ¥æç‚¼å¤šè§†è§’æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œæœ‰æ•ˆåœ°å‡è½»è¿‡é¥±å’Œé—®é¢˜ã€‚ä¸ºäº†æé«˜é€¼çœŸåº¦ï¼Œæˆ‘ä»¬åº”ç”¨æ½œåœ¨ä¸Šé‡‡æ ·å…ˆéªŒçŸ¥è¯†æ¥ç²¾ç»†åŒ–å»å™ªæ½œåœ¨ä»£ç ï¼Œç„¶åå°†å…¶è§£ç ä¸ºå›¾åƒã€‚æˆ‘ä»¬åœ¨NeRSembleæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜GAFåœ¨æ–°å‹è§†è§’åˆæˆæ–¹é¢ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä»å•ç›®è§†é¢‘æ•æ‰çš„å•†å“è®¾å¤‡ä¸Šçš„æ›´é«˜ä¿çœŸåº¦åŒ–èº«é‡å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10209v2">PDF</a> Paper Video: <a target="_blank" rel="noopener" href="https://youtu.be/QuIYTljvhyg">https://youtu.be/QuIYTljvhyg</a> Project Page:   <a target="_blank" rel="noopener" href="https://tangjiapeng.github.io/projects/GAF">https://tangjiapeng.github.io/projects/GAF</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å•ç›®è§†é¢‘å’Œå•†å“è®¾å¤‡ï¼ˆå¦‚æ™ºèƒ½æ‰‹æœºï¼‰æ¥é‡å»ºå¯åŠ¨ç”»çš„3Dé«˜æ–¯å¤´åƒã€‚ç”±äºæœ‰é™çš„è§‚å¯Ÿè§’åº¦å¸¦æ¥çš„æœªè§‚æµ‹åŒºåŸŸçº¦æŸä¸è¶³çš„é—®é¢˜ï¼Œä¼ ç»Ÿçš„ä»å•ç›®è§†é¢‘è¿›è¡ŒçœŸå®æ„Ÿçš„3Då¤´åƒé‡å»ºä¼šæœ‰å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè§†è§’å¤´éƒ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å…¶å…ˆéªŒçŸ¥è¯†å¡«å……ç¼ºå¤±åŒºåŸŸå¹¶ç¡®ä¿é«˜æ–¯è´´å›¾æ¸²æŸ“ä¸­çš„è§†è§’ä¸€è‡´æ€§ã€‚ä¸ºäº†ç²¾ç¡®æ§åˆ¶è§†è§’ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºFLAMEçš„å¤´éƒ¨é‡å»ºæ¸²æŸ“æ³•å‘é‡å›¾ï¼Œæä¾›åƒç´ å¯¹é½çš„è¯±å¯¼åå·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶è®¾å®šä¸ºè¾“å…¥å›¾åƒçš„VAEç‰¹å¾ï¼Œä»¥ä¿ç•™é¢éƒ¨èº«ä»½å’Œå¤–è§‚ç»†èŠ‚ã€‚å¯¹äºé«˜æ–¯å¤´åƒé‡å»ºï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨å»å™ªå›¾åƒä½œä¸ºä¼ªçœŸå®å€¼æ¥æç‚¼å¤šè§†è§’æ‰©æ•£å…ˆéªŒçŸ¥è¯†ï¼Œæœ‰æ•ˆç¼“è§£è¿‡åº¦é¥±å’Œé—®é¢˜ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜çœŸå®æ„Ÿï¼Œæˆ‘ä»¬åº”ç”¨æ½œåœ¨ä¸Šé‡‡æ ·å…ˆéªŒçŸ¥è¯†æ¥ç»†åŒ–å»å™ªæ½œåœ¨ç¼–ç ï¼Œç„¶åå°†å…¶è§£ç ä¸ºå›¾åƒã€‚æˆ‘ä»¬åœ¨NeRSembleæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜GAFåœ¨æ–°å‹è§†è§’åˆæˆæ–¹é¢ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä»å•ç›®è§†é¢‘æ•è·çš„å•†å“è®¾å¤‡ä¸­ç”Ÿæˆæ›´é«˜ä¿çœŸåº¦çš„å¤´åƒé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨å•ç›®è§†é¢‘å’Œå•†å“è®¾å¤‡ï¼ˆæ™ºèƒ½æ‰‹æœºï¼‰é‡å»ºå¯åŠ¨ç”»çš„3Dé«˜æ–¯å¤´åƒçš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¤šè§†è§’å¤´éƒ¨æ‰©æ•£æ¨¡å‹è§£å†³äº†æœ‰é™çš„è§‚å¯Ÿè§’åº¦å¸¦æ¥çš„é—®é¢˜ï¼Œå¡«å……ç¼ºå¤±åŒºåŸŸå¹¶ç¡®ä¿è§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨åŸºäºFLAMEçš„å¤´éƒ¨é‡å»ºæ¸²æŸ“æ³•å‘é‡å›¾å®ç°ç²¾ç¡®è§†è§’æ§åˆ¶ã€‚</li>
<li>ä½¿ç”¨è¾“å…¥å›¾åƒçš„VAEç‰¹å¾ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ä»¥ä¿ç•™é¢éƒ¨èº«ä»½å’Œå¤–è§‚ç»†èŠ‚ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨è¿­ä»£å»å™ªå›¾åƒä½œä¸ºä¼ªçœŸå®å€¼æç‚¼å¤šè§†è§’æ‰©æ•£å…ˆéªŒçŸ¥è¯†æ¥ç¼“è§£è¿‡åº¦é¥±å’Œé—®é¢˜ã€‚</li>
<li>åº”ç”¨æ½œåœ¨ä¸Šé‡‡æ ·å…ˆéªŒçŸ¥è¯†æé«˜é‡å»ºå¤´åƒçš„çœŸå®æ„Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a84f7e9e47425e042b2c5497db496304.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f0fc2007b008c3ee32c93478fbd1cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6f4cbd72dfce29c3660a4214a132a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fe1ac51560723367b066f9d3c914b58.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Conceptwm-A-Diffusion-Model-Watermark-for-Concept-Protection"><a href="#Conceptwm-A-Diffusion-Model-Watermark-for-Concept-Protection" class="headerlink" title="Conceptwm: A Diffusion Model Watermark for Concept Protection"></a>Conceptwm: A Diffusion Model Watermark for Concept Protection</h2><p><strong>Authors:Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu</strong></p>
<p>The personalization techniques of diffusion models succeed in generating specific concepts but also pose threats to copyright protection and illegal use. Model Watermarking is an effective method to prevent the unauthorized use of subject-driven or style-driven image generation, safeguarding concept copyrights. However, under the goal of concept-oriented protection, current watermarking schemes typically add watermarks to all images rather than applying them in a refined manner targeted at specific concepts. Additionally, the personalization techniques of diffusion models can easily remove watermarks. Existing watermarking methods struggle to achieve fine-grained watermark embedding with a few images of specific concept and prevent removal of watermarks through personalized fine-tuning. Therefore, we introduce a novel concept-oriented watermarking framework that seamlessly embeds imperceptible watermarks into the concept of diffusion models. We introduce Fidelity-preserving Latent Watermarking (FLW) to generate latent watermarks based on image characteristics and the Adversarial Watermarking Modulation module to prevent â€œjailbreakingâ€ via personalized finetuning. To enhance U-Netâ€™s efficiency in learning watermark patterns with limited samples, we propose Efficient Concept Watermark Finetuning, which alternates optimization of model parameters for both watermark embedding and concept learning. We conduct extensive experiments and ablation studies to verify our framework. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Conceptwm-4EB3/">https://anonymous.4open.science/r/Conceptwm-4EB3/</a>. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹çš„ç”ŸæˆæŠ€æœ¯è™½ç„¶èƒ½å¤ŸæˆåŠŸç”Ÿæˆç‰¹å®šæ¦‚å¿µï¼Œä½†ä¹Ÿå¯¹ç‰ˆæƒä¿æŠ¤å’Œéæ³•ä½¿ç”¨æ„æˆäº†å¨èƒã€‚æ¨¡å‹æ°´å°æ˜¯ä¸€ç§é˜²æ­¢ä¸»é¢˜é©±åŠ¨æˆ–é£æ ¼é©±åŠ¨å›¾åƒç”Ÿæˆæœªç»æˆæƒä½¿ç”¨çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä¿æŠ¤æ¦‚å¿µç‰ˆæƒã€‚ç„¶è€Œï¼Œåœ¨é¢å‘æ¦‚å¿µä¿æŠ¤çš„ç›®æ ‡ä¸‹ï¼Œå½“å‰çš„æ°´å°æ–¹æ¡ˆé€šå¸¸å¯¹æ‰€æœ‰å›¾åƒéƒ½æ·»åŠ æ°´å°ï¼Œè€Œä¸æ˜¯ä»¥ç²¾ç»†çš„æ–¹å¼é’ˆå¯¹ç‰¹å®šæ¦‚å¿µè¿›è¡Œåº”ç”¨ã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–æŠ€æœ¯å¾ˆå®¹æ˜“å»é™¤æ°´å°ã€‚ç°æœ‰çš„æ°´å°æ–¹æ³•éš¾ä»¥å®ç°é’ˆå¯¹å°‘æ•°å…·æœ‰ç‰¹å®šæ¦‚å¿µå›¾åƒè¿›è¡Œç²¾ç»†ç²’åº¦æ°´å°åµŒå…¥ï¼Œå¹¶é˜²æ­¢é€šè¿‡ä¸ªæ€§åŒ–å¾®è°ƒå»é™¤æ°´å°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ¦‚å¿µå¯¼å‘æ°´å°æ¡†æ¶ï¼Œæ— ç¼åœ°å°†ä¸å¯è§æ°´å°åµŒå…¥åˆ°æ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µä¸­ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºå›¾åƒç‰¹æ€§çš„ä¿çœŸæ€§ä¿æŒæ½œåœ¨æ°´å°ï¼ˆFLWï¼‰ç”Ÿæˆæ½œåœ¨æ°´å°ï¼Œå¹¶å¼•å…¥å¯¹æŠ—æ€§æ°´å°è°ƒåˆ¶æ¨¡å—ï¼Œä»¥é˜²æ­¢é€šè¿‡ä¸ªæ€§åŒ–å¾®è°ƒè¿›è¡Œâ€œè¶Šç‹±â€ã€‚ä¸ºäº†æé«˜U-Netåœ¨æœ‰é™æ ·æœ¬ä¸­å­¦ä¹ æ°´å°æ¨¡å¼çš„æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆæ¦‚å¿µæ°´å°å¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•äº¤æ›¿ä¼˜åŒ–ç”¨äºæ°´å°åµŒå…¥å’Œæ¦‚å¿µå­¦ä¹ çš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶æ¥éªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Conceptwm-4EB3/">https://anonymous.4open.science/r/Conceptwm-4EB3/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11688v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–æŠ€æœ¯åœ¨ç”Ÿæˆç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ï¼Œä¹Ÿå¯¹ç‰ˆæƒä¿æŠ¤å’Œéæ³•ä½¿ç”¨æ„æˆäº†å¨èƒã€‚æ¨¡å‹æ°´å°æ˜¯ä¸€ç§é˜²æ­¢æ»¥ç”¨ä¸»é¢˜é©±åŠ¨æˆ–é£æ ¼é©±åŠ¨å›¾åƒç”Ÿæˆçš„æœ‰æ•ˆæ–¹æ³•ï¼Œä¿æŠ¤æ¦‚å¿µç‰ˆæƒã€‚ç„¶è€Œï¼Œåœ¨æ¦‚å¿µå¯¼å‘çš„ä¿æŠ¤ç›®æ ‡ä¸‹ï¼Œå½“å‰çš„æ°´å°æ–¹æ¡ˆé€šå¸¸å¯¹æ‰€æœ‰å›¾åƒéƒ½æ·»åŠ æ°´å°ï¼Œè€Œéé’ˆå¯¹ç‰¹å®šæ¦‚å¿µè¿›è¡Œç²¾ç»†åº”ç”¨ã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–æŠ€æœ¯å¯ä»¥è½»æ¾å»é™¤æ°´å°ã€‚ç°æœ‰çš„æ°´å°æ–¹æ³•éš¾ä»¥å®ç°å°‘æ•°å…·æœ‰ç‰¹å®šæ¦‚å¿µå›¾åƒçš„æ°´å°åµŒå…¥ï¼Œå¹¶é˜²æ­¢é€šè¿‡ä¸ªæ€§åŒ–å¾®è°ƒå»é™¤æ°´å°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ¦‚å¿µå¯¼å‘æ°´å°æ¡†æ¶ï¼Œæ— ç¼åœ°å°†ä¸å¯å¯Ÿè§‰çš„æ°´å°åµŒå…¥æ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µä¸­ã€‚æˆ‘ä»¬æå‡ºåŸºäºå›¾åƒç‰¹æ€§çš„ä¿çœŸæ€§ä¿ç•™æ½œåœ¨æ°´å°ï¼ˆFLWï¼‰å’Œå¯¹æŠ—æ€§æ°´å°è°ƒåˆ¶æ¨¡å—ï¼Œä»¥é˜²æ­¢ä¸ªæ€§åŒ–å¾®è°ƒå¯¼è‡´çš„â€œè¶Šç‹±â€ã€‚ä¸ºæé«˜U-Netåœ¨å­¦ä¹ æ°´å°æ¨¡å¼æ—¶çš„æ•ˆç‡å¹¶å‡å°‘æ ·æœ¬é‡é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆæ¦‚å¿µæ°´å°å¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•äº¤æ›¿ä¼˜åŒ–æ°´å°åµŒå…¥å’Œæ¦‚å¿µå­¦ä¹ çš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶æ¥éªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„ä¸ªäººåŒ–æŠ€æœ¯åœ¨ç”Ÿæˆç‰¹å®šæ¦‚å¿µæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†ä¹Ÿå¸¦æ¥ç‰ˆæƒä¿æŠ¤å’Œéæ³•ä½¿ç”¨çš„é£é™©ã€‚</li>
<li>æ¨¡å‹æ°´å°æ˜¯ä¿æŠ¤æ¦‚å¿µç‰ˆæƒçš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ¡ˆåœ¨é’ˆå¯¹ç‰¹å®šæ¦‚å¿µçš„ç²¾ç»†ä¿æŠ¤ä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>å½“å‰çš„æ°´å°å®¹æ˜“è¢«æ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–æŠ€æœ¯å»é™¤ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„æ¦‚å¿µå¯¼å‘æ°´å°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ— ç¼åµŒå…¥ä¸å¯å¯Ÿè§‰çš„æ°´å°åœ¨æ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µä¸­ã€‚</li>
<li>å¼•å…¥ä¿çœŸæ€§ä¿ç•™æ½œåœ¨æ°´å°ï¼ˆFLWï¼‰å’Œå¯¹æŠ—æ€§æ°´å°è°ƒåˆ¶æ¨¡å—æ¥å¢å¼ºæ°´å°çš„ç¨³å›ºæ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>æå‡ºé«˜æ•ˆæ¦‚å¿µæ°´å°å¾®è°ƒæ–¹æ³•ä»¥æé«˜U-Netåœ¨å­¦ä¹ æ°´å°æ¨¡å¼æ—¶çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f928c16859bd027713a3d8a7dd2f6f93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63cd55547f012af34abca0e5852371c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71124a52048f4b2f8fdd40d7fc225916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bea4e165425e68a8be8413c08bf55697.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f31758c6328f74427e6375a52a2fd68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-220d339423fe6d3d137b5f3dab41aa8a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TPC-Test-time-Procrustes-Calibration-for-Diffusion-based-Human-Image-Animation"><a href="#TPC-Test-time-Procrustes-Calibration-for-Diffusion-based-Human-Image-Animation" class="headerlink" title="TPC: Test-time Procrustes Calibration for Diffusion-based Human Image   Animation"></a>TPC: Test-time Procrustes Calibration for Diffusion-based Human Image   Animation</h2><p><strong>Authors:Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, Chang D. Yoo</strong></p>
<p>Human image animation aims to generate a human motion video from the inputs of a reference human image and a target motion video. Current diffusion-based image animation systems exhibit high precision in transferring human identity into targeted motion, yet they still exhibit irregular quality in their outputs. Their optimal precision is achieved only when the physical compositions (i.e., scale and rotation) of the human shapes in the reference image and target pose frame are aligned. In the absence of such alignment, there is a noticeable decline in fidelity and consistency. Especially, in real-world environments, this compositional misalignment commonly occurs, posing significant challenges to the practical usage of current systems. To this end, we propose Test-time Procrustes Calibration (TPC), which enhances the robustness of diffusion-based image animation systems by maintaining optimal performance even when faced with compositional misalignment, effectively addressing real-world scenarios. The TPC provides a calibrated reference image for the diffusion model, enhancing its capability to understand the correspondence between human shapes in the reference and target images. Our method is simple and can be applied to any diffusion-based image animation system in a model-agnostic manner, improving the effectiveness at test time without additional training. </p>
<blockquote>
<p>äººç±»å›¾åƒåŠ¨ç”»æ—¨åœ¨ä»å‚è€ƒäººç±»å›¾åƒå’Œç›®æ ‡è¿åŠ¨è§†é¢‘è¾“å…¥ç”Ÿæˆäººç±»è¿åŠ¨è§†é¢‘ã€‚å½“å‰çš„åŸºäºæ‰©æ•£çš„å›¾åƒåŠ¨ç”»ç³»ç»Ÿåœ¨äººåƒè½¬ç§»åˆ°ç›®æ ‡åŠ¨ä½œæ—¶è¡¨ç°å‡ºé«˜ç²¾ç¡®åº¦ï¼Œä½†å®ƒä»¬çš„è¾“å‡ºè´¨é‡ä»å­˜åœ¨ä¸è§„åˆ™ç°è±¡ã€‚å®ƒä»¬çš„æœ€ä½³ç²¾åº¦åªæœ‰åœ¨å‚è€ƒå›¾åƒå’Œç›®æ ‡å§¿åŠ¿æ¡†æ¶ä¸­çš„äººç±»å½¢çŠ¶çš„ç‰©ç†ç»„æˆï¼ˆå³æ¯”ä¾‹å’Œæ—‹è½¬ï¼‰å¯¹é½æ—¶æ‰èƒ½å®ç°ã€‚åœ¨ç¼ºå°‘è¿™ç§å¯¹é½çš„æƒ…å†µä¸‹ï¼Œä¿çœŸåº¦å’Œä¸€è‡´æ€§ä¼šæ˜æ˜¾ä¸‹é™ã€‚ç‰¹åˆ«æ˜¯åœ¨ç°å®ç¯å¢ƒä¸­ï¼Œè¿™ç§ç»„æˆä¸Šçš„ä¸å¯¹é½æ˜¯å¸¸è§çš„ï¼Œç»™å½“å‰ç³»ç»Ÿçš„å®é™…åº”ç”¨å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æµ‹è¯•æ—¶æ ¡å‡†ï¼ˆTPCï¼‰ï¼Œå®ƒé€šè¿‡ä¿æŒæœ€ä½³æ€§èƒ½ï¼Œå³ä½¿é¢å¯¹ç»„æˆä¸Šçš„ä¸å¯¹é½ï¼Œä¹Ÿå¢å¼ºäº†åŸºäºæ‰©æ•£çš„å›¾åƒåŠ¨ç”»ç³»ç»Ÿçš„ç¨³å¥æ€§ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„é—®é¢˜ã€‚TPCä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†ä¸€ä¸ªæ ¡å‡†åçš„å‚è€ƒå›¾åƒï¼Œå¢å¼ºäº†å…¶å¯¹å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´äººç±»å½¢çŠ¶å¯¹åº”å…³ç³»çš„ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€å•ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•åŸºäºæ‰©æ•£çš„å›¾åƒåŠ¨ç”»ç³»ç»Ÿï¼Œä»¥æ¨¡å‹æ— å…³çš„æ–¹å¼æé«˜æµ‹è¯•æ—¶çš„æœ‰æ•ˆæ€§ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.24037v2">PDF</a> 24 pages, 16 figures, NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºçš„æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„äººä½“å›¾åƒåŠ¨ç”»æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç”±äºå‚è€ƒå›¾åƒå’Œç›®æ ‡è¿åŠ¨è§†é¢‘ä¸­çš„èº«ä½“å§¿æ€ç»„æˆï¼ˆå¦‚å°ºåº¦å’Œæ—‹è½¬ï¼‰æœªå¯¹é½æ—¶ï¼Œç°æœ‰ç³»ç»Ÿçš„ç²¾åº¦å’Œä¸€è‡´æ€§ä¼šæ˜æ˜¾ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTest-time Procrustes Calibrationï¼ˆTPCï¼‰çš„æ–¹æ³•ï¼Œå¢å¼ºäº†æ‰©æ•£æ¨¡å‹åœ¨äººä½“å›¾åƒåŠ¨ç”»ä¸­çš„ç¨³å¥æ€§ï¼Œç¡®ä¿åœ¨é¢ä¸´ç»„æˆä¸åŒ¹é…æ—¶ä»èƒ½ä¿æŒæœ€ä½³æ€§èƒ½ã€‚TPCé€šè¿‡ä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ ¡å‡†åçš„å‚è€ƒå›¾åƒï¼Œæé«˜äº†å…¶å¯¹å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´å½¢çŠ¶å¯¹åº”çš„ç†è§£ã€‚æ­¤æ–¹æ³•ç®€å•ä¸”å¯åº”ç”¨äºä»»ä½•æ‰©æ•£æ¨¡å‹çš„äººä½“å›¾åƒåŠ¨ç”»ç³»ç»Ÿï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æé«˜æµ‹è¯•æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººä½“å›¾åƒåŠ¨ç”»æ—¨åœ¨ä»å‚è€ƒå›¾åƒå’Œç›®æ ‡è¿åŠ¨è§†é¢‘ç”Ÿæˆäººä½“è¿åŠ¨è§†é¢‘ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨äººä½“å›¾åƒåŠ¨ç”»ä¸­è™½å…·æœ‰é«˜ç²¾åº¦ï¼Œä½†åœ¨è¾“å‡ºè´¨é‡æ–¹é¢ä»å­˜åœ¨ä¸è§„åˆ™æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å‚è€ƒå›¾åƒå’Œç›®æ ‡å§¿æ€å¸§çš„ç‰©ç†ç»„æˆï¼ˆå¦‚å°ºåº¦å’Œæ—‹è½¬ï¼‰å¯¹é½æ—¶æ‰èƒ½è¾¾åˆ°æœ€ä½³ç²¾åº¦ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œç»„æˆä¸åŒ¹é…æ˜¯å¸¸è§çš„ï¼Œç»™ç°æœ‰ç³»ç»Ÿå®ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>TPCæ–¹æ³•æé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨äººä½“å›¾åƒåŠ¨ç”»ä¸­çš„ç¨³å¥æ€§ï¼Œä»¥åº”å¯¹ç»„æˆä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>TPCé€šè¿‡æä¾›æ ¡å‡†åçš„å‚è€ƒå›¾åƒï¼Œå¢å¼ºäº†æ‰©æ•£æ¨¡å‹å¯¹å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´å½¢çŠ¶å¯¹åº”çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.24037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-062c35ecd588fb3c0c229cd20da00031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37973ca83c8459a2812e10e841c47d36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c183f456bda7e60e1730bb8a783a263.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-125bcc55d1d2d498796afd6615e0d313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eedc805c5dc617220269bc4384042c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef4f3d2f4126a5f164e9f50e72a4984b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="On-the-Wasserstein-Convergence-and-Straightness-of-Rectified-Flow"><a href="#On-the-Wasserstein-Convergence-and-Straightness-of-Rectified-Flow" class="headerlink" title="On the Wasserstein Convergence and Straightness of Rectified Flow"></a>On the Wasserstein Convergence and Straightness of Rectified Flow</h2><p><strong>Authors:Vansh Bansal, Saptarshi Roy, Purnamrita Sarkar, Alessandro Rinaldo</strong></p>
<p>Diffusion models have emerged as a powerful tool for image generation and denoising. Typically, generative models learn a trajectory between the starting noise distribution and the target data distribution. Recently Liu et al. (2023b) proposed Rectified Flow (RF), a generative model that aims to learn straight flow trajectories from noise to data using a sequence of convex optimization problems with close ties to optimal transport. If the trajectory is curved, one must use many Euler discretization steps or novel strategies, such as exponential integrators, to achieve a satisfactory generation quality. In contrast, RF has been shown to theoretically straighten the trajectory through successive rectifications, reducing the number of function evaluations (NFEs) while sampling. It has also been shown empirically that RF may improve the straightness in two rectifications if one can solve the underlying optimization problem within a sufficiently small error. In this paper, we make two contributions. First, we provide a theoretical analysis of the Wasserstein distance between the sampling distribution of RF and the target distribution. Our error rate is characterized by the number of discretization steps and a novel formulation of straightness stronger than that in the original work. Secondly, we present general conditions guaranteeing uniqueness and straightness of 1-RF, which is in line with previous empirical findings. As a byproduct of our analysis, we show that, in one dimension, RF started at the standard Gaussian distribution yields the Monge map. Additionally, we also present empirical results on both simulated and real datasets to validate our theoretical findings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bansal-vansh/rectified-flow">https://github.com/bansal-vansh/rectified-flow</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºå›¾åƒç”Ÿæˆå’Œå»å™ªçš„å¼ºå¤§å·¥å…·ã€‚é€šå¸¸ï¼Œç”Ÿæˆæ¨¡å‹å­¦ä¹ ä»åˆå§‹å™ªå£°åˆ†å¸ƒåˆ°ç›®æ ‡æ•°æ®åˆ†å¸ƒä¹‹é—´çš„è½¨è¿¹ã€‚æœ€è¿‘ï¼ŒLiuç­‰äººï¼ˆ2023bï¼‰æå‡ºäº†Rectified Flowï¼ˆRFï¼‰è¿™ä¸€ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—ä¸æœ€ä½³ä¼ è¾“ç´§å¯†ç›¸å…³çš„å‡¸ä¼˜åŒ–é—®é¢˜æ¥å­¦ä¹ ä»å™ªå£°åˆ°æ•°æ®çš„ç›´çº¿è½¨è¿¹ã€‚å¦‚æœè½¨è¿¹æ˜¯å¼¯æ›²çš„ï¼Œå¿…é¡»ä½¿ç”¨è®¸å¤šæ¬§æ‹‰ç¦»æ•£åŒ–æ­¥éª¤æˆ–æ–°å‹ç­–ç•¥ï¼ˆå¦‚æŒ‡æ•°ç§¯åˆ†å™¨ï¼‰æ¥å®ç°ä»¤äººæ»¡æ„çš„ç”Ÿæˆè´¨é‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRFç†è®ºä¸Šæœ‰èƒ½åŠ›é€šè¿‡è¿ç»­æ ¡æ­£æ¥ä½¿è½¨è¿¹ç›´çº¿åŒ–ï¼Œä»è€Œåœ¨é‡‡æ ·æ—¶å‡å°‘å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰ã€‚æ­¤å¤–ï¼Œç»éªŒè¯æ®è¡¨æ˜ï¼Œå¦‚æœåœ¨è¶³å¤Ÿå°çš„è¯¯å·®èŒƒå›´å†…è§£å†³åº•å±‚ä¼˜åŒ–é—®é¢˜ï¼ŒRFå¯èƒ½åœ¨ä¸¤æ¬¡æ ¡æ­£ä¸­æé«˜ç›´çº¿åº¦ã€‚æœ¬æ–‡æˆ‘ä»¬åšå‡ºäº†ä¸¤é¡¹è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹RFé‡‡æ ·åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„Wassersteinè·ç¦»è¿›è¡Œäº†ç†è®ºåˆ†æã€‚æˆ‘ä»¬çš„è¯¯å·®ç‡ç”±ç¦»æ•£åŒ–æ­¥éª¤çš„æ•°é‡ä»¥åŠæ¯”åŸå§‹å·¥ä½œä¸­æ›´å¼ºçš„ç›´çº¿æ€§æ–°å‹å…¬å¼æ‰€å†³å®šã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿è¯1-RFå”¯ä¸€æ€§å’Œç›´çº¿æ€§çš„é€šç”¨æ¡ä»¶ï¼Œè¿™ä¸ä¹‹å‰çš„ç»éªŒå‘ç°ç›¸ä¸€è‡´ã€‚ä½œä¸ºåˆ†æçš„ä¸€ä¸ªå‰¯äº§å“ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨ä¸€ç»´ç©ºé—´ä¸­ï¼Œä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒå¼€å§‹çš„RFä¼šäº§ç”ŸMongeåœ°å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æ¨¡æ‹Ÿæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„ç†è®ºå‘ç°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bansal-vansh/rectified-flow%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bansal-vansh/rectified-flowä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14949v6">PDF</a> 38 pages</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒç”Ÿæˆå’Œå»å™ªçš„å¼ºå¤§å·¥å…·ã€‚è¿‘æœŸLiuç­‰äººæå‡ºçš„Rectified Flowæ¨¡å‹æ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—å‡¸ä¼˜åŒ–é—®é¢˜æ¥å­¦ä¹ ä»å™ªå£°åˆ°æ•°æ®çš„ç›´çº¿è½¨è¿¹ï¼Œä¸æœ€ä¼˜ä¼ è¾“ç´§å¯†ç›¸å…³ã€‚è¯¥æ¨¡å‹å¯ç®€åŒ–é‡‡æ ·è¿‡ç¨‹ä¸­çš„è½¨è¿¹ï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚æœ¬æ–‡åˆ†æäº†Rectified Flowé‡‡æ ·åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„Wassersteinè·ç¦»ï¼Œå¹¶æä¾›äº†ç‹¬ç‰¹æ€§å’Œç›´çº¿æ€§çš„é€šç”¨æ¡ä»¶ä¿è¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åœ¨ä¸€ç»´æ¡ä»¶ä¸‹è¿›è¡Œäº†å®è¯åˆ†æã€‚ç›¸å…³ä»£ç å¯åœ¨github.com&#x2F;bansal-anshæŸ¥æ‰¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒç”Ÿæˆå’Œå»å™ªã€‚</li>
<li>Rectified Flowæ¨¡å‹é€šè¿‡å­¦ä¹ ç›´çº¿è½¨è¿¹ä»å™ªå£°åˆ°æ•°æ®æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>Rectified Flowé€šè¿‡å‡¸ä¼˜åŒ–é—®é¢˜å’Œæœ€ä¼˜ä¼ è¾“å®ç°è½¨è¿¹ç›´çº¿åŒ–ã€‚</li>
<li>ç†è®ºåˆ†æäº†Rectified Flowé‡‡æ ·åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„Wassersteinè·ç¦»ã€‚</li>
<li>åˆ†æäº†è½¨è¿¹ç›´çº¿æ€§çš„ç‹¬ç‰¹æ€§å’Œé€šç”¨æ¡ä»¶ä¿è¯ã€‚</li>
<li>åœ¨ä¸€ç»´æ¡ä»¶ä¸‹è¿›è¡Œäº†å®è¯åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d3cecd56d2c91145cd62646f64e2295b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c998c6c2c2bbe666cf899972d93cfe57.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Giant and anisotropic magnetostriction in $Î²$-O$_{2}$ at 110 T
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8255d31c849d39aab17844f4b62711d8.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  LL-Gaussian Low-Light Scene Reconstruction and Enhancement via Gaussian   Splatting for Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
