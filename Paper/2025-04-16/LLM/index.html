<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  InternVL3 Exploring Advanced Training and Test-Time Recipes for   Open-Source Multimodal Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-acee592cf1878e46beb05cd5b4f5308f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-16-æ›´æ–°"><a href="#2025-04-16-æ›´æ–°" class="headerlink" title="2025-04-16 æ›´æ–°"></a>2025-04-16 æ›´æ–°</h1><h2 id="InternVL3-Exploring-Advanced-Training-and-Test-Time-Recipes-for-Open-Source-Multimodal-Models"><a href="#InternVL3-Exploring-Advanced-Training-and-Test-Time-Recipes-for-Open-Source-Multimodal-Models" class="headerlink" title="InternVL3: Exploring Advanced Training and Test-Time Recipes for   Open-Source Multimodal Models"></a>InternVL3: Exploring Advanced Training and Test-Time Recipes for   Open-Source Multimodal Models</h2><p><strong>Authors:Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Weiye Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang</strong></p>
<p>We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†InternVL3ï¼Œè¿™æ˜¯InternVLç³»åˆ—çš„ä¸€é¡¹é‡å¤§è¿›å±•ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§åŸç”Ÿå¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚InternVL3å¹¶éå°†ä»…æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¹ç¼–ä¸ºæ”¯æŒè§†è§‰è¾“å…¥çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œè€Œæ˜¯åœ¨å•ä¸ªé¢„è®­ç»ƒé˜¶æ®µä¸­ï¼Œä»å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬è¯­æ–™åº“ä¸­åŒæ—¶è·å–å¤šæ¨¡æ€å’Œè¯­è¨€èƒ½åŠ›ã€‚è¿™ç§ç»Ÿä¸€çš„è®­ç»ƒèŒƒå¼æœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»ŸMLLMäº‹åè®­ç»ƒç®¡é“ä¸­é€šå¸¸é‡åˆ°çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼ŒInternVL3å¼•å…¥äº†å¯å˜è§†è§‰ä½ç½®ç¼–ç ï¼ˆV2PEï¼‰ä»¥æ”¯æŒæ‰©å±•çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰ï¼Œå¹¶é‡‡ç”¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ä»¥åŠä¼˜åŒ–çš„è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚å¹¿æ³›çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒInternVL3åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼ŒInternVL3-78Båœ¨MMMUåŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ†72.2åˆ†ï¼Œåœ¨å¼€æºMLLMä¸­åˆ›é€ äº†æ–°çš„æŠ€æœ¯ä¹‹å·…ã€‚å…¶èƒ½åŠ›ä¸é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ä¿æŒé«˜åº¦ç«äº‰ï¼ŒåŒ…æ‹¬ChatGPT-4oã€Claude 3.5 Sonnetå’ŒGemini 2.5 Proï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„çº¯è¯­è¨€ç†Ÿç»ƒç¨‹åº¦ã€‚æœ¬ç€å¼€æ”¾ç§‘å­¦çš„åŸåˆ™ï¼Œæˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒè®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ï¼Œä»¥æ¨åŠ¨ä¸‹ä¸€ä»£MLLMçš„ç ”ç©¶å’Œå‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10479v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>InternVL3æ˜¯InternVLç³»åˆ—çš„é‡è¦è¿›å±•ï¼Œé‡‡ç”¨åŸç”Ÿå¤šæ¨¡æ€é¢„è®­ç»ƒèŒƒå¼ã€‚å®ƒè”åˆè·å–å¤šæ¨¡æ€å’Œè¯­è¨€çš„èƒ½åŠ›ï¼Œæ¥è‡ªå•ä¸€é¢„è®­ç»ƒé˜¶æ®µçš„å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬è¯­æ–™åº“ã€‚è¯¥ç»Ÿä¸€è®­ç»ƒèŒƒå¼è§£å†³äº†ä¼ ç»Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é¢ä¸´çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚InternVL3é€šè¿‡å¯å˜è§†è§‰ä½ç½®ç¼–ç ï¼ˆV2PEï¼‰æ”¯æŒæ‰©å±•çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œå¹¶é‡‡ç”¨å…ˆè¿›çš„åè®­ç»ƒæŠ€æœ¯å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒInternVL3åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼ŒInternVL3-78Båœ¨MMMUåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†72.2ï¼Œåœ¨å¼€æºMLLMä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶ä¸é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ä¿æŒç«äº‰åŠ›ã€‚éµå¾ªå…¬å¼€ç§‘å­¦åŸåˆ™ï¼Œå°†å…¬å¼€è®­ç»ƒæ•°æ®å’Œæ¨¡å‹æƒé‡ä»¥ä¿ƒè¿›ä¸‹ä¸€ä»£MLLMçš„ç ”ç©¶å’Œå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InternVL3æ˜¯InternVLç³»åˆ—ä¸­çš„ä¸€é¡¹é‡è¦æ›´æ–°ï¼Œé‡‡ç”¨åŸç”Ÿå¤šæ¨¡æ€é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>é€šè¿‡å•ä¸€é¢„è®­ç»ƒé˜¶æ®µï¼ŒInternVL3ä»å¤šæ¨¡æ€æ•°æ®å’Œçº¯æ–‡æœ¬è¯­æ–™åº“ä¸­è”åˆè·å–å¤šæ¨¡æ€å’Œè¯­è¨€çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¤æ‚æ€§å’Œå¯¹é½æŒ‘æˆ˜ã€‚</li>
<li>InternVL3é‡‡ç”¨å¯å˜è§†è§‰ä½ç½®ç¼–ç ï¼ˆV2PEï¼‰ä»¥æ”¯æŒæ‰©å±•çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ã€‚</li>
<li>å…ˆè¿›åè®­ç»ƒæŠ€æœ¯å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥æé«˜äº†InternVL3çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>InternVL3åœ¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯InternVL3-78Båœ¨MMMUåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†72.2ï¼Œè¾¾åˆ°å¼€æºMLLMçš„æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9f5b3590c6c4ce966dc926ecf706cb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1b25fb58da24b0eaf6bca0b60aec112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1adb70d0220bb7a5215393f1048cfd04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd170d85bdcc93ac61b4d30eabdedc15.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MIEB-Massive-Image-Embedding-Benchmark"><a href="#MIEB-Massive-Image-Embedding-Benchmark" class="headerlink" title="MIEB: Massive Image Embedding Benchmark"></a>MIEB: Massive Image Embedding Benchmark</h2><p><strong>Authors:Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, MÃ¡rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, Niklas Muennighoff</strong></p>
<p>Image representations are often evaluated through disjointed, task-specific protocols, leading to a fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given a piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at <a target="_blank" rel="noopener" href="https://github.com/embeddings-benchmark/mteb">https://github.com/embeddings-benchmark/mteb</a>. </p>
<blockquote>
<p>å›¾åƒçš„è¡¨ç¤ºé€šå¸¸é€šè¿‡åˆ†æ•£çš„ã€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„åè®®è¿›è¡Œè¯„ä¼°ï¼Œè¿™å¯¼è‡´å¯¹æ¨¡å‹èƒ½åŠ›çš„ç†è§£å˜å¾—ç¢ç‰‡åŒ–ã€‚ä¾‹å¦‚ï¼Œå°šä¸æ¸…æ¥šæ“…é•¿å¯¹å›¾åƒè¿›è¡Œèšç±»çš„å›¾åƒåµŒå…¥æ¨¡å‹åœ¨ç»™å®šçš„æ–‡æœ¬ä¸­æ£€ç´¢ç›¸å…³å›¾åƒæ—¶æ˜¯å¦åŒæ ·å‡ºè‰²ã€‚æˆ‘ä»¬æ¨å‡ºå¤§è§„æ¨¡å›¾åƒåµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMIEBï¼‰ï¼Œä»¥è¿„ä»Šä¸ºæ­¢æœ€å¹¿æ³›çš„èŒƒå›´è¯„ä¼°å›¾åƒå’Œå›¾åƒæ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚MIEBæ¶µç›–130ä¸ªç‹¬ç«‹ä»»åŠ¡ï¼Œæ¶‰åŠ38ç§è¯­è¨€ï¼Œæˆ‘ä»¬å°†å…¶åˆ†ä¸º8ä¸ªé«˜çº§ç±»åˆ«ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šå¯¹50ä¸ªæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æ²¡æœ‰ä¸€ç§æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ç±»åˆ«ä¸­éƒ½å ä¸»å¯¼åœ°ä½ã€‚æˆ‘ä»¬æ­ç¤ºäº†å…ˆè¿›è§†è§‰æ¨¡å‹çš„éšè—åŠŸèƒ½ï¼Œä¾‹å¦‚å®ƒä»¬å¯¹æ–‡æœ¬çš„å‡†ç¡®è§†è§‰è¡¨ç¤ºï¼Œä»¥åŠå®ƒä»¬åœ¨æ··æ‚å› ç´ å­˜åœ¨æ—¶ï¼Œå¯¹äº¤é”™ç¼–ç å’ŒåŒ¹é…å›¾åƒå’Œæ–‡æœ¬çš„èƒ½åŠ›çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒMIEBä¸Šè§†è§‰ç¼–ç å™¨çš„æ€§èƒ½ä¸å®ƒä»¬åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ€§èƒ½é«˜åº¦ç›¸å…³ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ’è¡Œæ¦œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/embeddings-benchmark/mteb%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/embeddings-benchmark/mtebå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå¤§è§„æ¨¡å›¾åƒåµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMIEBï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å›¾åƒå’Œå›¾åƒæ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨æœ€å¹¿æ³›è°±ä¸Šçš„æ€§èƒ½ã€‚MIEBæ¶µç›–38ç§è¯­è¨€ï¼ŒåŒ…æ‹¬130ä¸ªä»»åŠ¡ï¼Œåˆ†ä¸ºå…«å¤§ç±»ã€‚æœ¬æ–‡åŸºå‡†æµ‹è¯•äº†50ä¸ªæ¨¡å‹ï¼Œå‘ç°æ²¡æœ‰ä¸€ç§æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ç±»åˆ«ä¸­éƒ½å æ®ä¸»å¯¼åœ°ä½ã€‚æ­ç¤ºå…ˆè¿›è§†è§‰æ¨¡å‹çš„éšè—åŠŸèƒ½ï¼Œå¦‚æ–‡æœ¬çš„å‡†ç¡®è§†è§‰è¡¨ç¤ºï¼Œä»¥åŠå®ƒä»¬åœ¨æ··åˆç¼–ç å’ŒåŒ¹é…å›¾åƒå’Œæ–‡æœ¬æ—¶çš„æœ‰é™èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¡¨æ˜ï¼Œè§†è§‰ç¼–ç å™¨åœ¨MIEBä¸Šçš„æ€§èƒ½ä¸å®ƒä»¬åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ€§èƒ½é«˜åº¦ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å›¾åƒè¡¨ç¤ºè¯„ä¼°æ–¹æ³•å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€æ ‡å‡†ã€‚</li>
<li>å¼•å…¥å¤§è§„æ¨¡å›¾åƒåµŒå…¥åŸºå‡†æµ‹è¯•ï¼ˆMIEBï¼‰ï¼Œè¦†ç›–38ç§è¯­è¨€ï¼ŒåŒ…å«130ä¸ªä»»åŠ¡ï¼Œåˆ†ä¸ºå…«å¤§ç±»ã€‚</li>
<li>å¯¹50ä¸ªæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°æ²¡æœ‰å•ä¸€æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ç±»åˆ«ä¸­å‡å±…é¢†å…ˆåœ°ä½ã€‚</li>
<li>æ­ç¤ºå…ˆè¿›è§†è§‰æ¨¡å‹çš„éšè—åŠŸèƒ½ï¼Œä¾‹å¦‚æ–‡æœ¬çš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨æ··åˆç¼–ç å’ŒåŒ¹é…å›¾åƒä¸æ–‡æœ¬æ–¹é¢çš„èƒ½åŠ›å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è§†è§‰ç¼–ç å™¨åœ¨MIEBä¸Šçš„æ€§èƒ½ä¸å…¶åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ€§èƒ½é«˜åº¦ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-561c1dcd002fe35cb992d8d09072c0da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c65bb6cbd2ab1dd2e90b605bda3d011c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26edfc2ef8ace243b0d7888183e79838.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5f199ab26d583b92734436fc69c2bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f711f70829d68a4a7b57623cefe8da64.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Pixel-SAIL-Single-Transformer-For-Pixel-Grounded-Understanding"><a href="#Pixel-SAIL-Single-Transformer-For-Pixel-Grounded-Understanding" class="headerlink" title="Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding"></a>Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding</h2><p><strong>Authors:Tao Zhang, Xiangtai Li, Zilong Huang, Yanwei Li, Weixian Lei, Xueqing Deng, Shihao Chen, Shunping Ji, Jiashi Feng</strong></p>
<p>Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformerâ€™s fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at <a target="_blank" rel="noopener" href="https://github.com/magic-research/Sa2VA">https://github.com/magic-research/Sa2VA</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç²¾ç»†ç²’åº¦çš„åƒç´ çº§ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚ç„¶è€Œï¼Œæ‰€æœ‰çš„å·¥ä½œéƒ½ä¸¥é‡ä¾èµ–äºé¢å¤–çš„ç»„ä»¶ï¼Œå¦‚è§†è§‰ç¼–ç å™¨ï¼ˆCLIPï¼‰ã€åˆ†å‰²ä¸“å®¶ï¼Œå¯¼è‡´ç³»ç»Ÿå¤æ‚åº¦è¾ƒé«˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ‰©å±•æ€§ã€‚æœ¬æ–‡çš„ç›®æ ‡æ˜¯æ¢ç´¢ä¸€ç§é«˜åº¦ç®€åŒ–çš„MLLMï¼Œè€Œä¸å¼•å…¥é¢å¤–çš„ç»„ä»¶ã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°å•ä¸€è½¬æ¢å™¨ä½œä¸ºç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆSAILï¼‰è®¾è®¡è¿‘æœŸå·¥ä½œçš„å¯å‘ï¼Œè¿™äº›å·¥ä½œè”åˆå­¦ä¹ è§†è§‰æ ‡è®°å’Œæ–‡æœ¬æ ‡è®°åœ¨è½¬æ¢å™¨ä¸­çš„çŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†Pixel-SAILï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåƒç´ çº§MLLMä»»åŠ¡çš„å•ä¸€è½¬æ¢å™¨ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨ç®€å•çš„åŸºçº¿åŸºç¡€ä¸Šæå‡ºäº†ä¸‰é¡¹æŠ€æœ¯æ”¹è¿›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯å­¦ä¹ çš„ä¸Šé‡‡æ ·æ¨¡å—æ¥ä¼˜åŒ–è§†è§‰æ ‡è®°ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰æç¤ºæ³¨å…¥ç­–ç•¥ï¼Œä½¿å•ä¸€è½¬æ¢å™¨èƒ½å¤Ÿç†è§£è§†è§‰æç¤ºè¾“å…¥ï¼Œå¹¶ä»è§†è§‰æç¤ºåµŒå…¥å’Œè§†è§‰æ ‡è®°çš„æ—©æœŸèåˆä¸­å—ç›Šã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§†è§‰ä¸“å®¶è’¸é¦ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°æé«˜å•ä¸€è½¬æ¢å™¨çš„ç²¾ç»†ç‰¹å¾æå–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡äººå·¥æ£€æŸ¥æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„åƒç´ ç†è§£åŸºå‡†ï¼ˆPerBenchï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸‰ä¸ªä»»åŠ¡ï¼šè¯¦ç»†å¯¹è±¡æè¿°ã€åŸºäºè§†è§‰æç¤ºçš„é—®é¢˜å›ç­”ã€è§†è§‰æ–‡æœ¬å¼•ç”¨åˆ†å‰²ã€‚åœ¨å››ä¸ªå¼•ç”¨åˆ†å‰²åŸºå‡†æµ‹è¯•ã€ä¸€ä¸ªè§†è§‰æç¤ºåŸºå‡†æµ‹è¯•ä»¥åŠæˆ‘ä»¬çš„PerBenchä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Pixel-SAILåœ¨æµç¨‹æ›´åŠ ç®€å•çš„æƒ…å†µä¸‹å–å¾—äº†ç›¸å½“æˆ–æ›´å¥½çš„ç»“æœã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/magic-research/Sa2VA%E4%B8%8A%E5%B9%B3%E5%8F%B0%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/magic-research/Sa2VAä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10465v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå•æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¿›è¡Œåƒç´ çº§åˆ«çš„ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚æ­¤ç ”ç©¶ä¸­æ—¨åœ¨æ¢ç©¶æ— é¢å¤–ç»„ä»¶çš„æ›´ç®€åŒ–çš„MLLMæ¨¡å‹ã€‚ç ”ç©¶å—åˆ°å•ä¸€è½¬æ¢å™¨ä½œä¸ºç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆSAILï¼‰è®¾è®¡çš„å½±å“ï¼Œåœ¨æ­¤è®¾è®¡åŸºç¡€ä¸Šæå‡ºäº†Pixel-SAILæ¨¡å‹ã€‚å¯¹åŸºçº¿è¿›è¡Œäº†ä¸‰é¡¹æŠ€æœ¯æ”¹è¿›ï¼ŒåŒ…æ‹¬å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·æ¨¡å—æ¥ä¼˜åŒ–è§†è§‰ä»¤ç‰Œç‰¹å¾ã€æ–°å‹è§†è§‰æç¤ºæ³¨å…¥ç­–ç•¥ä½¿å•ä¸€è½¬æ¢å™¨ç†è§£è§†è§‰æç¤ºè¾“å…¥å¹¶ä»æ—©æœŸèåˆä¸­å—ç›Šï¼Œä»¥åŠé€šè¿‡è§†è§‰ä¸“å®¶è’¸é¦ç­–ç•¥æé«˜ç²¾ç»†ç‰¹å¾æå–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„åƒç´ ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆPerBenchï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPixel-SAILåœ¨å››ä¸ªå¼•ç”¨åˆ†å‰²åŸºå‡†æµ‹è¯•ã€ä¸€ä¸ªè§†è§‰æç¤ºåŸºå‡†å’ŒPerBenchä¸Šå–å¾—äº†ç›¸å½“æˆ–æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶æµç¨‹æ›´åŠ ç®€æ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åƒç´ çº§ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯æ¢ç´¢æ— é¢å¤–ç»„ä»¶çš„æ›´ç®€åŒ–çš„MLLMæ¨¡å‹ã€‚</li>
<li>Pixel-SAILæ¨¡å‹åŸºäºå•ä¸€è½¬æ¢å™¨è¿›è¡Œè®¾è®¡ï¼Œç”¨äºåƒç´ çº§çš„MLLMä»»åŠ¡ã€‚</li>
<li>æŠ€æœ¯æ”¹è¿›åŒ…æ‹¬å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·æ¨¡å—ã€è§†è§‰æç¤ºæ³¨å…¥ç­–ç•¥å’Œè§†è§‰ä¸“å®¶è’¸é¦ç­–ç•¥ã€‚</li>
<li>æ”¶é›†äº†å…¨é¢çš„åƒç´ ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆPerBenchï¼‰ã€‚</li>
<li>Pixel-SAILåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4deb65169bb3a2e333f9e97fc9453033.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b72e6510ecbc92a52424e87922ebbae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65a910f13c879c11a3cb70cb0a6b509e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Scalability-of-Simplicity-Empirical-Analysis-of-Vision-Language-Learning-with-a-Single-Transformer"><a href="#The-Scalability-of-Simplicity-Empirical-Analysis-of-Vision-Language-Learning-with-a-Single-Transformer" class="headerlink" title="The Scalability of Simplicity: Empirical Analysis of Vision-Language   Learning with a Single Transformer"></a>The Scalability of Simplicity: Empirical Analysis of Vision-Language   Learning with a Single Transformer</h2><p><strong>Authors:Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, Zilong Huang</strong></p>
<p>This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAILâ€™s properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAILâ€™s scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/bytedance/SAIL">https://github.com/bytedance/SAIL</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SAILï¼Œè¿™æ˜¯ä¸€ä¸ªå•ä¸€å˜æ¢å™¨ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ä¸€æ¶æ„ä¸­é›†æˆäº†åŸå§‹åƒç´ ç¼–ç å’Œè¯­è¨€è§£ç ã€‚ä¸åŒäºä¾èµ–é¢„è®­ç»ƒè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰çš„ç°æœ‰æ¨¡å—åŒ–MLLMï¼ŒSAILæ¶ˆé™¤äº†å¯¹å•ç‹¬è§†è§‰ç¼–ç å™¨çš„éœ€æ±‚ï¼Œå‘ˆç°å‡ºæ›´ç®€æ´çš„æ¶æ„è®¾è®¡ã€‚SAILæ²¡æœ‰å¼•å…¥æ–°å‹æ¶æ„ç»„ä»¶ï¼Œè€Œæ˜¯é€‚åº”äº†æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ¨¡æ€ä½ç½®ç¼–ç ï¼Œä»¥æ›´å¥½åœ°ç¬¦åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ç‰¹å®šç‰¹å¾ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†SAILä¸æ¨¡å—åŒ–MLLMçš„å±æ€§ï¼ŒåŒ…æ‹¬å¯æ‰©å±•æ€§ã€è·¨æ¨¡æ€ä¿¡æ¯æµæ¨¡å¼ä»¥åŠè§†è§‰è¡¨ç¤ºèƒ½åŠ›ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡ï¼ŒSAILå®ç°äº†ä¸æ¨¡å—åŒ–MLLMç›¸å½“çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå»é™¤é¢„è®­ç»ƒçš„ViTç»„ä»¶å¢å¼ºäº†SAILçš„å¯æ‰©å±•æ€§ï¼Œå¹¶äº§ç”Ÿäº†æ˜¾è‘—ä¸åŒçš„è·¨æ¨¡æ€ä¿¡æ¯æµæ¨¡å¼ã€‚æ­¤å¤–ï¼ŒSAILåœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›ï¼Œå¦‚åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†ä¸ViT-22Bç›¸å½“çš„ç»“æœã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bytedance/SAIL%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bytedance/SAILä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSAILçš„ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚ä¸åŒäºç°æœ‰çš„æ¨¡å—åŒ–MLLMï¼ŒSAILåœ¨å•ä¸€æ¶æ„å†…é›†æˆäº†åŸå§‹åƒç´ ç¼–ç å’Œè¯­è¨€è§£ç ï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒå¥½çš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ã€‚é€šè¿‡é‡‡ç”¨æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œè·¨æ¨¡æ€ä½ç½®ç¼–ç ï¼ŒSAILæ›´å¥½åœ°é€‚åº”äº†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ç‹¬ç‰¹ç‰¹å¾ã€‚åœ¨æ‰©å¤§è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡çš„åŒæ—¶ï¼ŒSAILçš„æ€§èƒ½è¾¾åˆ°äº†æ¨¡å—åŒ–MLLMçš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œå»é™¤é¢„è®­ç»ƒçš„ViTç»„ä»¶å¢å¼ºäº†SAILçš„å¯æ‰©å±•æ€§ï¼Œå¹¶å¯¼è‡´äº†ä¸åŒçš„è·¨æ¨¡æ€ä¿¡æ¯æµåŠ¨æ¨¡å¼ã€‚SAILåœ¨è§†è§‰ä»»åŠ¡å¦‚è¯­ä¹‰åˆ†å‰²ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä¸ViT-22Bç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAILæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé›†æˆäº†åŸå§‹åƒç´ ç¼–ç å’Œè¯­è¨€è§£ç åœ¨ä¸€ä¸ªæ¶æ„ä¸­ã€‚</li>
<li>ä¸æ¨¡å—åŒ–MLLMä¸åŒï¼ŒSAILæ— éœ€é¢„è®­ç»ƒçš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ã€‚</li>
<li>SAILé‡‡ç”¨æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œè·¨æ¨¡æ€ä½ç½®ç¼–ç ï¼Œä»¥æ›´å¥½åœ°é€‚åº”è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®å’Œæ¨¡å‹è§„æ¨¡ï¼ŒSAILå®ç°äº†ä¸æ¨¡å—åŒ–MLLMç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>å»é™¤é¢„è®­ç»ƒçš„ViTç»„ä»¶å¢å¼ºäº†SAILçš„å¯æ‰©å±•æ€§ï¼Œå¹¶æ”¹å˜äº†è·¨æ¨¡æ€ä¿¡æ¯æµåŠ¨æ¨¡å¼ã€‚</li>
<li>SAILåœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å¼ºå¤§ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²ï¼Œä¸ViT-22Bç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a76f03385be859471ecbafe6c3c30c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f73eda928772fb72da3cf369dfd5d8e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-207c1aa9fcd0e27f34a728e8cd1a8f8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8fb26f4d83b37f821313f772efce4c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Xiaobo Xia, Run Luo</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>ç°æœ‰æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„å·¥ä½œä¸»è¦ä¾èµ–äºåœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šè¿›è¡Œçš„ç›‘ç£ç²¾ç»†è°ƒæ•´è®­ç»ƒæ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢ä¸Šå­˜åœ¨å›°éš¾ã€‚è¿™ä¸€é—®é¢˜æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é«˜çº§ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°æœ‰å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ„å»ºä¸»è¦ä¾èµ–äºåœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šè¿›è¡Œç›‘ç£ç²¾ç»†è°ƒæ•´çš„è®­ç»ƒæ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§ç•Œé¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¯¥é—®é¢˜æ˜¾è‘—é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é«˜çº§ä»»åŠ¡ã€‚å—å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼ˆRFTï¼‰çš„å¯å‘ï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œè®¾ç½®ä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºåä¸ºâ€œ\nameâ€çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œæé«˜LVLMsåœ¨é«˜çº§ç°å®ä¸–ç•Œä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¤šä¸ªå¹³å°ï¼ˆåŒ…æ‹¬Windowsã€Linuxã€MacOSã€Androidå’ŒWebï¼‰çš„å°éƒ¨åˆ†ç²¾å¿ƒç­›é€‰çš„é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ï¼Œâ€œ\nameâ€ä»…ä½¿ç”¨0.02%çš„æ•°æ®ï¼ˆ3Kå¯¹1.3Mï¼‰ä¾¿åœ¨è·¨è¶Šä¸‰ä¸ªä¸åŒå¹³å°ï¼ˆç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µï¼‰çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¯¹OS-Atlasç­‰ç°æœ‰å…ˆè¿›æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚è¿™äº›ç»“æœè¯æ˜äº†åŸºäºç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡Œç°å®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡çš„æ½œåŠ›æ˜¯å·¨å¤§çš„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰GUIä»£ç†æ„å»ºä¸»è¦ä¾èµ–ç›‘ç£ç²¾ç»†è°ƒæ•´çš„è®­ç»ƒæ¨¡å¼ï¼Œä½†æ­¤æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€œ\nameâ€è¢«æå‡ºï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œæé«˜LVLMsåœ¨é«˜çº§ç°å®ä¸–ç•Œä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ã€‚</li>
<li>â€œ\nameâ€åˆ©ç”¨è·¨å¤šä¸ªå¹³å°çš„å°éƒ¨åˆ†é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚GRPOï¼Œæ¥æ›´æ–°å’Œå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>â€œ\nameâ€åœ¨ä»…ä½¿ç”¨0.02%çš„æ•°æ®æƒ…å†µä¸‹ï¼Œå³åœ¨3Kä¸13Mä¹‹é—´ï¼Œå®ç°äº†å¯¹ç°å…ˆè¿›æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>è¿™äº›ç»“æœåœ¨ç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µç­‰ä¸‰ä¸ªä¸åŒå¹³å°çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡Œç°å®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>â€œ\nameâ€çš„æˆåŠŸå®æ–½ä¸ºæœªæ¥çš„GUIä»£ç†å¼€å‘è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨è§£å†³ç°å®ä¸–ç•Œå¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-940a90ba274d8e742e1bd4d78a0144af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acee592cf1878e46beb05cd5b4f5308f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b8bc950229c747c33ff13ba87e28048.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="M1-Towards-Scalable-Test-Time-Compute-with-Mamba-Reasoning-Models"><a href="#M1-Towards-Scalable-Test-Time-Compute-with-Mamba-Reasoning-Models" class="headerlink" title="M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models"></a>M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models</h2><p><strong>Authors:Junxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M. Rush, Tri Dao</strong></p>
<p>Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning. </p>
<blockquote>
<p>æœ‰æ•ˆçš„æ¨ç†å¯¹äºè§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æµ‹è¯•æ—¶çš„è®¡ç®—æ‰©å±•å’Œé•¿é“¾æ¡æ€ç»´æ¨ç†ï¼Œæå‡äº†æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ç”±äºå…¶äºŒæ¬¡è®¡ç®—å¤æ‚åº¦å’Œçº¿æ€§å†…å­˜è¦æ±‚ï¼Œåœ¨æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦æ–¹é¢å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºMambaæ¶æ„çš„æ–°å‹æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹M1ï¼Œå®ƒå…è®¸è¿›è¡Œé«˜æ•ˆçš„å†…å­˜æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä»ç°æœ‰æ¨ç†æ¨¡å‹ä¸­æç‚¼çš„è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿›ä¸€æ­¥å¢å¼ºã€‚åœ¨AIMEå’ŒMATHåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒM1ä¸ä»…ä¼˜äºä»¥å‰çš„çº¿æ€§RNNæ¨¡å‹ï¼Œè€Œä¸”åœ¨ç›¸ä¼¼è§„æ¨¡ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„Deepseek R1æç‚¼æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜ä¸é«˜æ€§èƒ½é€šç”¨æ¨ç†å¼•æ“vLLMæ¯”è¾ƒäº†æˆ‘ä»¬çš„ç”Ÿæˆé€Ÿåº¦ï¼Œè§‚å¯Ÿåˆ°ä¸ç›¸åŒè§„æ¨¡çš„è½¬æ¢å™¨ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†3å€ä»¥ä¸Šã€‚é€šè¿‡æé«˜ååé‡ï¼Œæˆ‘ä»¬åœ¨å›ºå®šçš„ç”Ÿæˆæ—¶é—´é¢„ç®—å†…ï¼Œä½¿ç”¨è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨ï¼Œå®ç°äº†æ¯”DeepSeek R1è’¸é¦è½¬æ¢å™¨æ¨ç†æ¨¡å‹æ›´é«˜çš„ç²¾åº¦ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ··åˆçš„Mambaæ¨ç†æ¨¡å‹ï¼Œå¹¶æä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥æ‰©å±•æµ‹è¯•æ—¶çš„ç”Ÿæˆï¼Œä½¿ç”¨è‡ªæˆ‘ä¸€è‡´æ€§æˆ–é•¿é“¾æ¡æ€ç»´æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10449v1">PDF</a> Code is available <a target="_blank" rel="noopener" href="https://github.com/jxiw/M1">https://github.com/jxiw/M1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºMambaæ¶æ„çš„æ–°å‹æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹M1ï¼Œå…·æœ‰é«˜æ•ˆæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è’¸é¦ç°æœ‰æ¨ç†æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒM1åœ¨AIMEå’ŒMATHåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ä»…ä¼˜äºå…ˆå‰çš„çº¿æ€§RNNæ¨¡å‹ï¼Œè€Œä¸”ä¸åŒç­‰è§„æ¨¡çš„Deepseek R1è’¸é¦æ¨ç†æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼ŒM1é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å›ºå®šç”Ÿæˆæ—¶é—´é¢„ç®—ä¸‹è¶…è¿‡äº†DeepSeek R1æ¨¡å‹ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„æ··åˆæ¨ç†æ¨¡å‹M1ï¼Œå¹¶ç»™å‡ºäº†ä¸€ç§æ›´æœ‰æ•ˆçš„é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§æˆ–é•¿æ€è€ƒé“¾è¿›è¡Œæ‰©å±•æµ‹è¯•æ—¶é—´ç”Ÿæˆçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M1æ¨¡å‹åŸºäºMambaæ¶æ„ï¼Œæ˜¯ä¸€ç§æ–°å‹æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹ï¼Œå…·æœ‰é«˜æ•ˆæ¨ç†èƒ½åŠ›ã€‚</li>
<li>M1æ¨¡å‹é€šè¿‡è’¸é¦ç°æœ‰æ¨ç†æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä¼˜åŒ–äº†æ€§èƒ½ã€‚</li>
<li>åœ¨AIMEå’ŒMATHåŸºå‡†æµ‹è¯•ä¸­ï¼ŒM1æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå…ˆå‰çš„çº¿æ€§RNNæ¨¡å‹ï¼Œå¹¶è¾¾åˆ°åŒç­‰è§„æ¨¡çš„Deepseek R1æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>M1æ¨¡å‹å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œé€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨å®ç°ã€‚</li>
<li>M1æ¨¡å‹åœ¨å›ºå®šç”Ÿæˆæ—¶é—´é¢„ç®—ä¸‹è¶…è¿‡äº†DeepSeek R1æ¨¡å‹ã€‚</li>
<li>è¯¥è®ºæ–‡å±•ç¤ºäº†M1æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´æ‰©å±•ç”Ÿæˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88b43279ae375bf68dcb52262d0f6990.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eaa46c3ee5f052f5f8890bbe4fe57b87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d688c5ac3e364a4cc6d9f0ae67ff5ca9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multimodal-Long-Video-Modeling-Based-on-Temporal-Dynamic-Context"><a href="#Multimodal-Long-Video-Modeling-Based-on-Temporal-Dynamic-Context" class="headerlink" title="Multimodal Long Video Modeling Based on Temporal Dynamic Context"></a>Multimodal Long Video Modeling Based on Temporal Dynamic Context</h2><p><strong>Authors:Haoran Hao, Jiaming Han, Yiyuan Zhang, Xiangyu Yue</strong></p>
<p>Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Hoar012/TDC-Video">https://github.com/Hoar012/TDC-Video</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œç”±äºLLMçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶å’Œè§†é¢‘ä¸­çš„å¤§é‡ä¿¡æ¯ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ä»ç„¶é¢ä¸´å›°éš¾ã€‚å°½ç®¡æœ‰ä¸€äº›æœ€è¿‘çš„æ–¹æ³•æ˜¯ä¸ºé•¿è§†é¢‘ç†è§£è€Œè®¾è®¡çš„ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨ä»¤ç‰Œå‹ç¼©æ—¶ä¼šä¸¢å¤±å…³é”®ä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨å¤„ç†éŸ³é¢‘ç­‰é™„åŠ æ¨¡æ€æ—¶é‡åˆ°å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¸§ä¹‹é—´æ—¶é—´å…³ç³»çš„åŠ¨æ€é•¿è§†é¢‘ç¼–ç æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´åŠ¨æ€ä¸Šä¸‹æ–‡ï¼ˆTDCï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®å¸§é—´ç›¸ä¼¼æ€§å°†è§†é¢‘åˆ†å‰²æˆè¯­ä¹‰ä¸Šè¿è´¯çš„åœºæ™¯ï¼Œç„¶åä½¿ç”¨è§†è§‰éŸ³é¢‘ç¼–ç å™¨å°†æ¯å¸§ç¼–ç æˆä»¤ç‰Œã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ—¶é—´ä¸Šä¸‹æ–‡å‹ç¼©æœºï¼Œä»¥å‡å°‘æ¯ä¸ªæ®µå†…çš„ä»¤ç‰Œæ•°é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæŸ¥è¯¢çš„Transformeræ¥èšåˆè§†é¢‘ã€éŸ³é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬ä»¤ç‰Œï¼Œå°†å…¶å‹ç¼©æˆæœ‰é™çš„æ—¶é—´ä¸Šä¸‹æ–‡ä»¤ç‰Œé›†ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é™æ€å¸§ä»¤ç‰Œå’Œæ—¶é—´ä¸Šä¸‹æ–‡ä»¤ç‰Œè¾“å…¥LLMè¿›è¡Œè§†é¢‘ç†è§£ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¤„ç†æé•¿çš„è§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ€ç»´é“¾ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»å¤šä¸ªè§†é¢‘æ®µä¸­é€æ­¥æå–ç­”æ¡ˆã€‚è¿™äº›ä¸­é—´ç­”æ¡ˆä½œä¸ºæ¨ç†è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸ºæœ€ç»ˆç­”æ¡ˆåšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬åœ¨ä¸€èˆ¬è§†é¢‘ç†è§£å’ŒéŸ³è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hoar012/TDC-Video%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Hoar012/TDC-Videoä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10443v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¸§é—´æ—¶åºå…³ç³»çš„åŠ¨æ€é•¿è§†é¢‘ç¼–ç æ–¹æ³•ï¼Œç§°ä¸ºæ—¶åºåŠ¨æ€ä¸Šä¸‹æ–‡ï¼ˆTDCï¼‰ã€‚è¯¥æ–¹æ³•å°†è§†é¢‘åˆ†å‰²æˆè¯­ä¹‰ä¸Šä¸€è‡´çš„åœºæ™¯ï¼Œä½¿ç”¨è§†è§‰-éŸ³é¢‘ç¼–ç å™¨å¯¹æ¯å¸§è¿›è¡Œç¼–ç ï¼Œå¹¶æå‡ºä¸€ç§æ–°é¢–çš„æ—¶åºä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼Œå‡å°‘æ¯ä¸ªç‰‡æ®µä¸­çš„ä»¤ç‰Œæ•°é‡ã€‚åŒæ—¶ï¼Œç»“åˆæŸ¥è¯¢åŸºç¡€è½¬æ¢å™¨å°†è§†é¢‘ã€éŸ³é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬ä»¤ç‰Œèšé›†åˆ°æœ‰é™çš„æ—¶åºä¸Šä¸‹æ–‡ä»¤ç‰Œä¸­ã€‚ä¸ºäº†å¤„ç†æé•¿çš„è§†é¢‘ï¼Œé‡‡ç”¨æ— è®­ç»ƒçš„æ€è€ƒé“¾ç­–ç•¥ï¼Œä»å¤šä¸ªè§†é¢‘ç‰‡æ®µä¸­é€æ­¥æå–ç­”æ¡ˆã€‚è¿™äº›ä¸­é—´ç­”æ¡ˆä½œä¸ºæ¨ç†è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸ºæœ€ç»ˆç­”æ¡ˆåšå‡ºè´¡çŒ®ã€‚è¯¥æ–¹æ³•åœ¨é€šç”¨è§†é¢‘ç†è§£å’ŒéŸ³é¢‘è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åä¸ºæ—¶åºåŠ¨æ€ä¸Šä¸‹æ–‡ï¼ˆTDCï¼‰çš„ç¼–ç æ–¹æ³•ï¼Œç”¨äºå¤„ç†é•¿è§†é¢‘ç†è§£ã€‚</li>
<li>é€šè¿‡åˆ†å‰²è§†é¢‘æˆè¯­ä¹‰ä¸€è‡´çš„åœºæ™¯å¹¶ç¼–ç æ¯å¸§ï¼Œä»¥æé«˜è§†é¢‘ç†è§£æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ—¶åºä¸Šä¸‹æ–‡å‹ç¼©å™¨ï¼Œä»¥å‡å°‘ä»¤ç‰Œæ•°é‡å¹¶æé«˜å¤„ç†æ•ˆç‡ã€‚</li>
<li>ç»“åˆä½¿ç”¨æŸ¥è¯¢åŸºç¡€è½¬æ¢å™¨æ¥å¤„ç†è§†é¢‘ã€éŸ³é¢‘å’ŒæŒ‡ä»¤æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨æ— è®­ç»ƒçš„æ€è€ƒé“¾ç­–ç•¥ï¼Œé€æ­¥ä»é•¿è§†é¢‘ä¸­æå–ç­”æ¡ˆã€‚</li>
<li>æ–¹æ³•åœ¨é€šç”¨è§†é¢‘ç†è§£å’ŒéŸ³è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¼ºå¤§æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e9cf809497879e3b52c272fd6fd910c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4a19be8ddb63c4d2613922c0483f81d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16a59a8ee69b78a3e432c981c06e7924.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CliniChat-A-Multi-Source-Knowledge-Driven-Framework-for-Clinical-Interview-Dialogue-Reconstruction-and-Evaluation"><a href="#CliniChat-A-Multi-Source-Knowledge-Driven-Framework-for-Clinical-Interview-Dialogue-Reconstruction-and-Evaluation" class="headerlink" title="CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical   Interview Dialogue Reconstruction and Evaluation"></a>CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical   Interview Dialogue Reconstruction and Evaluation</h2><p><strong>Authors:Jing Chen, Zhihua Wei, Wei Zhang, Yingying Hu, Qiong Zhang</strong></p>
<p>Large language models (LLMs) hold great promise for assisting clinical interviews due to their fluent interactive capabilities and extensive medical knowledge. However, the lack of high-quality interview dialogue data and widely accepted evaluation methods has significantly impeded this process. So we propose CliniChat, a framework that integrates multi-source knowledge to enable LLMs to simulate real-world clinical interviews. It consists of two modules: Clini-Recon and Clini-Eval, each responsible for reconstructing and evaluating interview dialogues, respectively. By incorporating three sources of knowledge, Clini-Recon transforms clinical notes into systematic, professional, and empathetic interview dialogues. Clini-Eval combines a comprehensive evaluation metric system with a two-phase automatic evaluation approach, enabling LLMs to assess interview performance like experts. We contribute MedQA-Dialog, a high-quality synthetic interview dialogue dataset, and CliniChatGLM, a model specialized for clinical interviews. Experimental results demonstrate that CliniChatGLMâ€™s interview capabilities undergo a comprehensive upgrade, particularly in history-taking, achieving state-of-the-art performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶æµç•…çš„äº¤äº’èƒ½åŠ›å’Œä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†ï¼Œåœ¨è¾…åŠ©ä¸´åºŠè®¿è°ˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡è®¿è°ˆå¯¹è¯æ•°æ®çš„ç¼ºä¹ä»¥åŠå¹¿æ³›æ¥å—çš„è¯„ä¼°æ–¹æ³•çš„ç¼ºå¤±ï¼Œæå¤§åœ°é˜»ç¢äº†è¿™ä¸€è¿›ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CliniChatæ¡†æ¶ï¼Œå®ƒæ•´åˆäº†å¤šæºçŸ¥è¯†ï¼Œä½¿LLMèƒ½å¤Ÿæ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„ä¸´åºŠè®¿è°ˆã€‚CliniChatæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šClini-Reconå’ŒClini-Evalï¼Œåˆ†åˆ«è´Ÿè´£é‡å»ºå’Œè¯„ä¼°è®¿è°ˆå¯¹è¯ã€‚Clini-Reconé€šè¿‡èåˆä¸‰ç§çŸ¥è¯†æºï¼Œå°†ä¸´åºŠç¬”è®°è½¬åŒ–ä¸ºç³»ç»Ÿã€ä¸“ä¸šä¸”å¯Œæœ‰åŒæƒ…å¿ƒçš„è®¿è°ˆå¯¹è¯ã€‚Clini-Evalç»“åˆäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æŒ‡æ ‡ç³»ç»Ÿå’Œä¸€ä¸ªä¸¤é˜¶æ®µçš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿåƒä¸“å®¶ä¸€æ ·è¯„ä¼°è®¿è°ˆè¡¨ç°ã€‚æˆ‘ä»¬è´¡çŒ®äº†MedQA-Dialogè¿™ä¸€é«˜è´¨é‡åˆæˆè®¿è°ˆå¯¹è¯æ•°æ®é›†å’Œä¸“ç”¨äºä¸´åºŠè®¿è°ˆçš„CliniChatGLMæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCliniChatGLMçš„è®¿è°ˆèƒ½åŠ›å¾—åˆ°äº†å…¨é¢æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…å²é‡‡é›†æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10418v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨¡æ‹ŸçœŸå®ä¸´åºŠè®¿è°ˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç¼ºä¹é«˜è´¨é‡è®¿è°ˆå¯¹è¯æ•°æ®å’Œå¹¿æ³›æ¥å—çš„è¯„ä¼°æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºCliniChatæ¡†æ¶ï¼Œæ•´åˆå¤šæºçŸ¥è¯†ï¼Œä½¿LLMèƒ½å¤Ÿæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸´åºŠè®¿è°ˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬Clini-Reconå’ŒClini-Evalä¸¤ä¸ªæ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£é‡å»ºå’Œè¯„ä¼°è®¿è°ˆå¯¹è¯ã€‚é€šè¿‡ç»“åˆä¸‰ç§çŸ¥è¯†æºï¼ŒClini-Reconèƒ½å°†ä¸´åºŠç¬”è®°è½¬åŒ–ä¸ºç³»ç»Ÿã€ä¸“ä¸šã€å¯Œæœ‰åŒæƒ…å¿ƒçš„è®¿è°ˆå¯¹è¯ã€‚Clini-Evalåˆ™ç»“åˆå…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»å’Œä¸¤é˜¶æ®µè‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿåƒä¸“å®¶ä¸€æ ·è¯„ä¼°è®¿è°ˆè¡¨ç°ã€‚æˆ‘ä»¬è´¡çŒ®äº†MedQA-Dialogè¿™ä¸€é«˜è´¨é‡åˆæˆè®¿è°ˆå¯¹è¯æ•°æ®é›†å’Œä¸“ä¸ºä¸´åºŠè®¿è°ˆè®¾è®¡çš„CliniChatGLMæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCliniChatGLMçš„è®¿è°ˆèƒ½åŠ›å¾—åˆ°å…¨é¢æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…å²é‡‡é›†æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºåœ¨ä¸´åºŠè®¿è°ˆä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ç¼ºä¹é«˜è´¨é‡æ•°æ®å’Œè¯„ä¼°æ–¹æ³•çš„æŒ‘æˆ˜ã€‚</li>
<li>CliniChatæ¡†æ¶é€šè¿‡æ•´åˆå¤šæºçŸ¥è¯†æå‡LLMæ¨¡æ‹ŸçœŸå®ä¸´åºŠè®¿è°ˆçš„èƒ½åŠ›ã€‚</li>
<li>CliniChatåŒ…å«Clini-Reconå’ŒClini-Evalä¸¤ä¸ªæ¨¡å—ï¼Œåˆ†åˆ«è´Ÿè´£é‡å»ºå’Œè¯„ä¼°è®¿è°ˆå¯¹è¯ã€‚</li>
<li>Clini-Reconèƒ½è½¬åŒ–ä¸´åºŠç¬”è®°ä¸ºç³»ç»Ÿã€ä¸“ä¸šã€æœ‰åŒæƒ…å¿ƒçš„è®¿è°ˆå¯¹è¯ï¼Œç»“åˆä¸‰ç§çŸ¥è¯†æºã€‚</li>
<li>Clini-Evalé‡‡ç”¨å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»å’Œä¸¤é˜¶æ®µè‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œæ¨¡æ‹Ÿä¸“å®¶è¯„ä¼°è®¿è°ˆè¡¨ç°ã€‚</li>
<li>è´¡çŒ®äº†MedQA-Dialogæ•°æ®é›†ï¼Œä¸ºä¸´åºŠè®¿è°ˆå¯¹è¯æä¾›é«˜è´¨é‡åˆæˆæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fed9b64a609b48d2b663d604fdda65a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33cbdce3cffec4ceb78dae8619a70e62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c0d47a914a1405cf4a935e1fac40e0e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-SRBench-A-New-Benchmark-for-Scientific-Equation-Discovery-with-Large-Language-Models"><a href="#LLM-SRBench-A-New-Benchmark-for-Scientific-Equation-Discovery-with-Large-Language-Models" class="headerlink" title="LLM-SRBench: A New Benchmark for Scientific Equation Discovery with   Large Language Models"></a>LLM-SRBench: A New Benchmark for Scientific Equation Discovery with   Large Language Models</h2><p><strong>Authors:Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D Doan, Chandan K Reddy</strong></p>
<p>Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research. </p>
<blockquote>
<p>ç§‘å­¦æ–¹ç¨‹å‘ç°ï¼ˆScientific equation discoveryï¼‰æ˜¯ç§‘å­¦è¿›æ­¥å†å²ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ï¼Œèƒ½å¤Ÿä½¿æ¨å¯¼å‡ºè‡ªç„¶ç°è±¡çš„å®šå¾‹æˆä¸ºå¯èƒ½ã€‚è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶åœ¨å‡è®¾ç”Ÿæˆä¸­åˆ©ç”¨åµŒå…¥çš„ç§‘å­¦çŸ¥è¯†è€Œå—åˆ°å…³æ³¨ï¼Œè¿›è€Œåº”ç”¨äºè¿™ä¸€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ–¹æ³•çš„çœŸå®å‘ç°èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºå¸¸è§çš„æ–¹ç¨‹ï¼Œè¿™äº›æ–¹ç¨‹å®¹æ˜“è¢«LLMè®°å¿†ï¼Œå¯¼è‡´æ€§èƒ½æŒ‡æ ‡è†¨èƒ€ï¼Œæ— æ³•åæ˜ çœŸæ­£çš„å‘ç°èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLM-SRBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«239ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°åŸºäºLLMçš„ç§‘å­¦æ–¹ç¨‹å‘ç°æ–¹æ³•ï¼ŒåŒæ—¶é˜²æ­¢ç®€å•çš„è®°å¿†ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç±»åˆ«ï¼šLSR-Transformï¼Œå®ƒå°†å¸¸è§çš„ç‰©ç†æ¨¡å‹è½¬åŒ–ä¸ºä¸å¤ªå¸¸è§çš„æ•°å­¦è¡¨ç¤ºå½¢å¼ï¼Œä»¥æµ‹è¯•è¶…è¶Šè®°å¿†å½¢å¼çš„æ¨ç†èƒ½åŠ›ï¼›ä»¥åŠLSR-Synthï¼Œå®ƒå¼•å…¥åˆæˆé—®é¢˜ï¼Œä»¥å‘ç°é©±åŠ¨éœ€è¦æ•°æ®é©±åŠ¨æ¨ç†çš„é—®é¢˜ã€‚é€šè¿‡å¯¹å‡ ç§æœ€æ–°æ–¹æ³•è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œä½¿ç”¨å¼€æ”¾å’Œå°é—­LLMç»“åˆçš„æ–¹å¼ï¼Œæˆ‘ä»¬å‘ç°è¿„ä»Šä¸ºæ­¢è¡¨ç°æœ€å¥½çš„ç³»ç»Ÿä»…è¾¾åˆ°31.5%çš„ç¬¦å·å‡†ç¡®ç‡ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç§‘å­¦æ–¹ç¨‹å‘ç°çš„æŒ‘æˆ˜ï¼Œå°†LLM-SRBenchå®šä½ä¸ºæœªæ¥ç ”ç©¶çš„é‡è¦èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10415v1">PDF</a> Project page:   <a target="_blank" rel="noopener" href="https://github.com/deep-symbolic-mathematics/llm-srbench">https://github.com/deep-symbolic-mathematics/llm-srbench</a> , Benchmark page:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nnheui/llm-srbench">https://huggingface.co/datasets/nnheui/llm-srbench</a></p>
<p><strong>Summary</strong><br>åœ¨ç§‘å­¦è¿›æ­¥çš„å†å²ä¸­ï¼Œç§‘å­¦æ–¹ç¨‹çš„å‘ç°æ˜¯ä¸€é¡¹åŸºç¡€ä»»åŠ¡ï¼Œå®ƒæ¨åŠ¨äº†æ”¯é…è‡ªç„¶ç°è±¡çš„å®šå¾‹çš„æ¨å¯¼ã€‚è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­å› å…¶äº§ç”Ÿå‡è®¾çš„æ½œåŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ–¹æ³•çš„çœŸæ­£å‘ç°èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºLLMå®¹æ˜“è®°å¿†çš„ä¸€èˆ¬æ–¹ç¨‹ï¼Œå¯¼è‡´æ€§èƒ½æŒ‡æ ‡è†¨èƒ€ï¼Œå¹¶ä¸èƒ½åæ˜ çœŸæ­£çš„å‘ç°èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†LLM-SRBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«239ä¸ªè·¨å››ä¸ªç§‘å­¦é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°åŸºäºLLMçš„ç§‘å­¦æ–¹ç¨‹å‘ç°æ–¹æ³•ï¼ŒåŒæ—¶é˜²æ­¢äº†ç®€å•çš„è®°å¿†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç±»åˆ«ï¼šLSR-Transformï¼Œå®ƒå°†å¸¸è§çš„ç‰©ç†æ¨¡å‹è½¬åŒ–ä¸ºä¸å¤ªå¸¸è§çš„æ•°å­¦è¡¨ç¤ºå½¢å¼ï¼Œä»¥æµ‹è¯•è¶…è¶Šè®°å¿†å½¢å¼çš„æ¨ç†èƒ½åŠ›ï¼›LSR-Synthï¼Œå®ƒå¼•å…¥åˆæˆã€ä»¥å‘ç°ä¸ºå¯¼å‘çš„é—®é¢˜ï¼Œéœ€è¦æ•°æ®é©±åŠ¨æ¨ç†ã€‚é€šè¿‡å¯¹å‡ ç§æœ€æ–°æ–¹æ³•çš„å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬å¼€æ”¾å’Œå°é—­å¼çš„LLMï¼Œæˆ‘ä»¬å‘ç°ç›®å‰è¡¨ç°æœ€ä½³çš„ç³»ç»Ÿçš„ç¬¦å·å‡†ç¡®ç‡ä»…ä¸º31.5%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç§‘å­¦æ–¹ç¨‹å‘ç°çš„æŒ‘æˆ˜ï¼Œå®šä½LLM-SRBenchä¸ºæœªæ¥ç ”ç©¶çš„æœ‰ä»·å€¼çš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦æ–¹ç¨‹å‘ç°åœ¨ç§‘å­¦è¿›æ­¥ä¸­èµ·å…³é”®ä½œç”¨ï¼Œæ¨åŠ¨è‡ªç„¶ç°è±¡çš„å®šå¾‹æ¨å¯¼ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‘æœŸåœ¨ç§‘å­¦æ–¹ç¨‹å‘ç°ä»»åŠ¡ä¸­å—åˆ°å…³æ³¨ï¼Œå› å…¶äº§ç”Ÿå‡è®¾çš„æ½œåŠ›ã€‚</li>
<li>è¯„ä¼°LLMåœ¨æ–¹ç¨‹å‘ç°ä¸­çš„çœŸå®èƒ½åŠ›å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰åŸºå‡†æµ‹è¯•æ˜“äºå¯¼è‡´LLMé€šè¿‡è®°å¿†è€ŒéçœŸæ­£å‘ç°æ¥è¾¾æˆã€‚</li>
<li>LLM-SRBenchåŸºå‡†æµ‹è¯•è¢«è®¾è®¡ç”¨äºè¯„ä¼°LLMçš„ç§‘å­¦æ–¹ç¨‹å‘ç°èƒ½åŠ›ï¼ŒåŒ…å«ä¸¤ç±»é—®é¢˜ï¼šæµ‹è¯•æ¨ç†èƒ½åŠ›çš„LSR-Transformå’Œéœ€è¦æ•°æ®é©±åŠ¨æ¨ç†çš„åˆæˆé—®é¢˜LSR-Synthã€‚</li>
<li>ç›®å‰æœ€ä½³ç³»ç»Ÿçš„ç¬¦å·å‡†ç¡®ç‡ä»…ä¸º31.5%ï¼Œè¡¨æ˜ç§‘å­¦æ–¹ç¨‹å‘ç°çš„æŒ‘æˆ˜ä»ç„¶å¾ˆå¤§ã€‚</li>
<li>LLM-SRBenchå¯¹æœªæ¥ç ”ç©¶å…·æœ‰ä»·å€¼ï¼Œå¯ä½œä¸ºè¯„ä¼°å’Œæ”¹è¿›LLMåœ¨æ–¹ç¨‹å‘ç°ä¸­æ€§èƒ½çš„é‡è¦å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c2da4f8fcf2860f4ec5dd2783081069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-200acfb4db0fd8930ede00de33324858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-943aadaf76f31ac85ecfada3d019d3ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fda7d59796f5d0f7ecf139579842a347.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Performance-of-Large-Language-Models-in-Supporting-Medical-Diagnosis-and-Treatment"><a href="#Performance-of-Large-Language-Models-in-Supporting-Medical-Diagnosis-and-Treatment" class="headerlink" title="Performance of Large Language Models in Supporting Medical Diagnosis and   Treatment"></a>Performance of Large Language Models in Supporting Medical Diagnosis and   Treatment</h2><p><strong>Authors:Diogo Sousa, Guilherme Barbosa, Catarina Rocha, Dulce Oliveira</strong></p>
<p>The integration of Large Language Models (LLMs) into healthcare holds significant potential to enhance diagnostic accuracy and support medical treatment planning. These AI-driven systems can analyze vast datasets, assisting clinicians in identifying diseases, recommending treatments, and predicting patient outcomes. This study evaluates the performance of a range of contemporary LLMs, including both open-source and closed-source models, on the 2024 Portuguese National Exam for medical specialty access (PNA), a standardized medical knowledge assessment. Our results highlight considerable variation in accuracy and cost-effectiveness, with several models demonstrating performance exceeding human benchmarks for medical students on this specific task. We identify leading models based on a combined score of accuracy and cost, discuss the implications of reasoning methodologies like Chain-of-Thought, and underscore the potential for LLMs to function as valuable complementary tools aiding medical professionals in complex clinical decision-making. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°åŒ»ç–—ä¿å¥ä¸­ï¼Œå…·æœ‰æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ”¯æŒåŒ»ç–—æ²»ç–—è®¡åˆ’çš„å·¨å¤§æ½œåŠ›ã€‚è¿™äº›äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç³»ç»Ÿå¯ä»¥åˆ†æå¤§é‡æ•°æ®é›†ï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿè¯†åˆ«ç–¾ç—…ã€æ¨èæ²»ç–—æ–¹æ³•å¹¶é¢„æµ‹æ‚£è€…ç»“æœã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸€ç³»åˆ—å½“ä»£LLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹ï¼Œåœ¨2024å¹´è‘¡è„ç‰™åŒ»å­¦ä¸“ç§‘å…¥å­¦å›½å®¶è€ƒè¯•ï¼ˆPNAï¼‰ä¸Šçš„è¡¨ç°ï¼Œè¿™æ˜¯ä¸€é¡¹æ ‡å‡†åŒ–çš„åŒ»å­¦çŸ¥è¯†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†å‡†ç¡®åº¦å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢çš„å·¨å¤§å·®å¼‚ï¼Œä¸€äº›æ¨¡å‹åœ¨è¿™ä¸ªç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†åŒ»å­¦å­¦ç”Ÿçš„åŸºå‡†æ°´å¹³ã€‚æˆ‘ä»¬æ ¹æ®å‡†ç¡®åº¦å’Œæˆæœ¬çš„ç»¼åˆå¾—åˆ†ç¡®å®šäº†é¢†å…ˆçš„æ¨¡å‹ï¼Œè®¨è®ºäº†é“¾å¼æ€ç»´ç­‰æ¨ç†æ–¹æ³•çš„å½±å“ï¼Œå¹¶å¼ºè°ƒäº†LLMä½œä¸ºæœ‰ä»·å€¼çš„è¾…åŠ©å·¥å…·ï¼Œåœ¨å¸®åŠ©åŒ»å­¦ä¸“ä¸šäººå£«è¿›è¡Œå¤æ‚ä¸´åºŠå†³ç­–æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10405v1">PDF</a> 21 pages, 6 figures, 4 tables. Acknowledgements: The authors   acknowledge the support of the AITriage4SU Project (2024.07400.IACDC&#x2F;2024),   funded by the FCT (Foundation for Science and Technology), Portugal</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„é›†æˆåº”ç”¨å…·æœ‰æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ”¯æŒåŒ»ç–—æ²»ç–—è®¡åˆ’åˆ¶å®šçš„å·¨å¤§æ½œåŠ›ã€‚è¿™äº›äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç³»ç»Ÿèƒ½å¤Ÿåˆ†æåºå¤§çš„æ•°æ®é›†ï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿè¯†åˆ«ç–¾ç—…ã€æ¨èæ²»ç–—æ–¹æ¡ˆå’Œé¢„æµ‹æ‚£è€…ç»“æœã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸€ç³»åˆ—ç°ä»£LLMåœ¨2024å¹´è‘¡è„ç‰™åŒ»å­¦ä¸“ç§‘å…¥å­¦è€ƒè¯•ï¼ˆPNAï¼‰ä¸Šçš„è¡¨ç°ï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„åŒ»å­¦çŸ¥è¯†è¯„ä¼°ã€‚ç ”ç©¶ç»“æœçªå‡ºäº†å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç›Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå¤šç§æ¨¡å‹çš„è¡¨ç°åœ¨è¿™ä¸€ç‰¹å®šä»»åŠ¡ä¸Šè¶…è¿‡äº†åŒ»å­¦å­¦ç”Ÿçš„åŸºå‡†æ°´å¹³ã€‚æ ¹æ®å‡†ç¡®æ€§å’Œæˆæœ¬çš„ç»¼åˆå¾—åˆ†ç¡®å®šäº†é¢†å…ˆçš„æ¨¡å‹ï¼Œå¹¶è®¨è®ºäº†é“¾å¼æ€ç»´ç­‰æ¨ç†æ–¹æ³•çš„å½±å“ï¼Œå¼ºè°ƒäº†LLMä½œä¸ºæœ‰ä»·å€¼çš„è¾…åŠ©å·¥å…·åœ¨åŒ»ç–—ä¸“ä¸šå¤æ‚å†³ç­–åˆ¶å®šä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé›†æˆåˆ°åŒ»ç–—é¢†åŸŸå¯æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šçš„æ•ˆç‡ã€‚</li>
<li>LLMèƒ½å¤Ÿåˆ†æåºå¤§çš„åŒ»ç–—æ•°æ®é›†ï¼Œè¾…åŠ©ä¸´åºŠåŒ»ç”Ÿçš„å†³ç­–ã€‚</li>
<li>åœ¨è‘¡è„ç‰™åŒ»å­¦ä¸“ç§‘å…¥å­¦è€ƒè¯•ä¸­è¯„ä¼°äº†å¤šç§LLMçš„è¡¨ç°ã€‚</li>
<li>ä¸åŒLLMåœ¨å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç›Šä¸Šå­˜åœ¨å·®å¼‚ã€‚</li>
<li>éƒ¨åˆ†LLMçš„è¡¨ç°è¶…è¿‡äº†åŒ»å­¦å­¦ç”Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ°´å¹³ã€‚</li>
<li>é“¾å¼æ€ç»´ç­‰æ¨ç†æ–¹æ³•å¯¹LLMçš„è¡¨ç°æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-059cdd71e1157d2457c9e88a4baa2800.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ad6646cb27e0250b31bccbc971797aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a9aa0b2ddfa67f8795e0f4801e55f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43bc3a57657901b0bc3ded8b2fa6633b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f1028873544ffc421419db85dc6d814.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LLM-driven-Constrained-Copy-Generation-through-Iterative-Refinement"><a href="#LLM-driven-Constrained-Copy-Generation-through-Iterative-Refinement" class="headerlink" title="LLM-driven Constrained Copy Generation through Iterative Refinement"></a>LLM-driven Constrained Copy Generation through Iterative Refinement</h2><p><strong>Authors:Varun Vasudevan, Faezeh Akhavizadegan, Abhinav Prakash, Yokila Arora, Jason Cho, Tanya Mendiratta, Sushant Kumar, Kannan Achan</strong></p>
<p>Crafting a marketing message (copy), or copywriting is a challenging generation task, as the copy must adhere to various constraints. Copy creation is inherently iterative for humans, starting with an initial draft followed by successive refinements. However, manual copy creation is time-consuming and expensive, resulting in only a few copies for each use case. This limitation restricts our ability to personalize content to customers. Contrary to the manual approach, LLMs can generate copies quickly, but the generated content does not consistently meet all the constraints on the first attempt (similar to humans). While recent studies have shown promise in improving constrained generation through iterative refinement, they have primarily addressed tasks with only a few simple constraints. Consequently, the effectiveness of iterative refinement for tasks such as copy generation, which involves many intricate constraints, remains unclear. To address this gap, we propose an LLM-based end-to-end framework for scalable copy generation using iterative refinement. To the best of our knowledge, this is the first study to address multiple challenging constraints simultaneously in copy generation. Examples of these constraints include length, topics, keywords, preferred lexical ordering, and tone of voice. We demonstrate the performance of our framework by creating copies for e-commerce banners for three different use cases of varying complexity. Our results show that iterative refinement increases the copy success rate by $16.25-35.91$% across use cases. Furthermore, the copies generated using our approach outperformed manually created content in multiple pilot studies using a multi-armed bandit framework. The winning copy improved the click-through rate by $38.5-45.21$%. </p>
<blockquote>
<p>æ„å»ºè¥é”€ä¿¡æ¯ï¼ˆæ–‡æ¡ˆï¼‰æˆ–æ–‡æ¡ˆåˆ›ä½œæ˜¯ä¸€é¡¹å……æ»¡æŒ‘æˆ˜çš„åˆ›ä½œä»»åŠ¡ï¼Œå› ä¸ºæ–‡æ¡ˆå¿…é¡»éµå®ˆå„ç§çº¦æŸã€‚å¯¹äºäººç±»æ¥è¯´ï¼Œæ–‡æ¡ˆåˆ›ä½œæœ¬è´¨ä¸Šæ˜¯è¿­ä»£çš„ï¼Œä»åˆæ­¥è‰ç¨¿å¼€å§‹ï¼Œæ¥ç€è¿›è¡Œè¿ç»­ä¸æ–­çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨åˆ›å»ºæ–‡æ¡ˆæ—¢è€—æ—¶åˆæ˜‚è´µï¼Œå¯¼è‡´æ¯æ¬¡ç”¨ä¾‹åªæœ‰å°‘æ•°å‡ ä¸ªæ–‡æ¡ˆã€‚è¿™ç§é™åˆ¶é™åˆ¶äº†æˆ‘ä»¬å¯¹å®¢æˆ·å†…å®¹çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚ä¸æ‰‹åŠ¨æ–¹æ³•ç›¸åï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å¿«é€Ÿç”Ÿæˆæ–‡æ¡ˆï¼Œä½†ç”Ÿæˆçš„å†…å®¹å¹¶ä¸æ€»èƒ½ç¬¬ä¸€æ¬¡å°è¯•å°±æ»¡è¶³æ‰€æœ‰çº¦æŸï¼ˆä¸äººç±»ç›¸ä¼¼ï¼‰ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶é€šè¿‡è¿­ä»£æ”¹è¿›åœ¨æ”¹è¿›å—é™ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å®ƒä»¬ä¸»è¦è§£å†³äº†åªæœ‰å°‘æ•°ç®€å•çº¦æŸçš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œå¯¹äºæ¶‰åŠè®¸å¤šå¤æ‚çº¦æŸçš„ä»»åŠ¡ï¼ˆå¦‚æ–‡æ¡ˆåˆ›ä½œï¼‰çš„è¿­ä»£æ”¹è¿›çš„æœ‰æ•ˆæ€§ä»ç„¶ä¸æ¸…æ¥šã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºä½¿ç”¨è¿­ä»£æ”¹è¿›è¿›è¡Œå¯æ‰©å±•çš„æ–‡æ¡ˆç”Ÿæˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç ”ç©¶åŒæ—¶è§£å†³æ–‡æ¡ˆç”Ÿæˆä¸­å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çº¦æŸã€‚è¿™äº›çº¦æŸçš„ç¤ºä¾‹åŒ…æ‹¬é•¿åº¦ã€ä¸»é¢˜ã€å…³é”®è¯ã€é¦–é€‰è¯æ±‡é¡ºåºå’Œè¯­æ°”ã€‚æˆ‘ä»¬é€šè¿‡ä¸ºä¸‰ä¸ªä¸åŒå¤æ‚åº¦çš„ç”µå­å•†åŠ¡æ¨ªå¹…åˆ›å»ºæ–‡æ¡ˆæ¥å±•ç¤ºæˆ‘ä»¬æ¡†æ¶çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿­ä»£æ”¹è¿›æé«˜äº†æ–‡æ¡ˆæˆåŠŸç‡$16.25-35.91$%ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„æ–‡æ¡ˆåœ¨å¤šç»„è¯•ç‚¹ç ”ç©¶ä¸­è¶…è¿‡äº†æ‰‹åŠ¨åˆ›å»ºçš„å†…å®¹ï¼Œè¿™äº›ç ”ç©¶é‡‡ç”¨å¤šè‡‚è€è™æœºæ¡†æ¶ï¼Œæœ€ä½³æ–‡æ¡ˆæé«˜äº†ç‚¹å‡»ç‡$38.5-45.21$%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10391v1">PDF</a> 10 pages, 2 figures, 7 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–‡æ¡ˆç”Ÿæˆçš„é—®é¢˜ã€‚ç”±äºæ–‡æ¡ˆéœ€è¦æ»¡è¶³å¤šç§çº¦æŸï¼Œä¼ ç»Ÿçš„æ‰‹åŠ¨åˆ›ä½œæ–¹å¼æ—¢è€—æ—¶åˆæ˜‚è´µï¼Œä¸”éš¾ä»¥å®ç°ä¸ªæ€§åŒ–ã€‚è™½ç„¶LLMå¯ä»¥å¿«é€Ÿç”Ÿæˆæ–‡æ¡ˆï¼Œä½†ä¸€æ¬¡ç”Ÿæˆçš„æ–‡æ¡ˆå¹¶ä¸æ€»èƒ½å®Œå…¨æ»¡è¶³æ‰€æœ‰çº¦æŸã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºLLMçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé‡‡ç”¨è¿­ä»£ä¼˜åŒ–æ–¹å¼è¿›è¡Œæ–‡æ¡ˆç”Ÿæˆã€‚è¯¥æ¡†æ¶èƒ½åŒæ—¶å¤„ç†å¤šç§å¤æ‚çº¦æŸï¼Œå¦‚é•¿åº¦ã€ä¸»é¢˜ã€å…³é”®è¯ã€è¯æ±‡é¡ºåºå’Œè¯­è°ƒç­‰ã€‚é€šè¿‡ç”µå•†æ¨ªå¹…å¹¿å‘Šæ–‡æ¡ˆçš„ç”Ÿæˆå®ä¾‹ï¼Œè¯æ˜äº†è¿­ä»£ä¼˜åŒ–èƒ½æé«˜æ–‡æ¡ˆæˆåŠŸç‡ï¼Œä¸”ç”Ÿæˆçš„æ–‡æ¡ˆåœ¨ç‚¹å‡»ç‡æ–¹é¢è¶…è¶Šäº†æ‰‹åŠ¨åˆ›ä½œçš„æ–‡æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æ¡ˆåˆ›ä½œæ˜¯ä¸€é¡¹éœ€è¦æ»¡è¶³å¤šç§çº¦æŸçš„æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œæ‰‹åŠ¨åˆ›ä½œæ—¢è€—æ—¶åˆæ˜‚è´µï¼Œä¸”éš¾ä»¥å®ç°ä¸ªæ€§åŒ–ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¿«é€Ÿç”Ÿæˆæ–‡æ¡ˆï¼Œä½†ä¸€æ¬¡ç”Ÿæˆçš„æ–‡æ¡ˆå¹¶ä¸æ€»èƒ½æ»¡è¶³æ‰€æœ‰çº¦æŸã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºLLMçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé‡‡ç”¨è¿­ä»£ä¼˜åŒ–æ–¹å¼è¿›è¡Œæ–‡æ¡ˆç”Ÿæˆã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½åŒæ—¶å¤„ç†å¤šç§å¤æ‚çº¦æŸï¼Œå¦‚é•¿åº¦ã€ä¸»é¢˜ã€å…³é”®è¯ã€è¯æ±‡é¡ºåºå’Œè¯­è°ƒç­‰ã€‚</li>
<li>è¿­ä»£ä¼˜åŒ–èƒ½æé«˜æ–‡æ¡ˆç”Ÿæˆçš„æˆåŠŸç‡ï¼Œä¸”åœ¨ç‚¹å‡»ç‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>ç›¸è¾ƒäºæ‰‹åŠ¨åˆ›ä½œçš„æ–‡æ¡ˆï¼Œä½¿ç”¨æ­¤æ¡†æ¶ç”Ÿæˆçš„æ–‡æ¡ˆåœ¨å¤šé¡¹è¯•ç‚¹ç ”ç©¶ä¸­è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fa712b434fabc1f600f5f267e0b1faf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab4290b047a0ec462de96ec305cf1d11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0563dfa512546b63bf65b921b350ce66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bae5d20665d83246a6fe7527a3cb7855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-773cc3fd0550e833fca196596937cb03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e08383d311c667c1677435f657a21a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47d00c9efd39561ba71de6cb0a26a86f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SymRTLO-Enhancing-RTL-Code-Optimization-with-LLMs-and-Neuron-Inspired-Symbolic-Reasoning"><a href="#SymRTLO-Enhancing-RTL-Code-Optimization-with-LLMs-and-Neuron-Inspired-Symbolic-Reasoning" class="headerlink" title="SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired   Symbolic Reasoning"></a>SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired   Symbolic Reasoning</h2><p><strong>Authors:Yiting Wang, Wanghao Ye, Ping Guo, Yexiao He, Ziyao Wang, Yexiao He, Bowei Tian, Shwai He, Guoheng Sun, Zheyu Shen, Sihan Chen, Ankur Srivastava, Qingfu Zhang, Gang Qu, Ang Li</strong></p>
<p>Optimizing Register Transfer Level (RTL) code is crucial for improving the power, performance, and area (PPA) of digital circuits in the early stages of synthesis. Manual rewriting, guided by synthesis feedback, can yield high-quality results but is time-consuming and error-prone. Most existing compiler-based approaches have difficulty handling complex design constraints. Large Language Model (LLM)-based methods have emerged as a promising alternative to address these challenges. However, LLM-based approaches often face difficulties in ensuring alignment between the generated code and the provided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL optimization framework that seamlessly integrates LLM-based code rewriting with symbolic reasoning techniques. Our method incorporates a retrieval-augmented generation (RAG) system of optimization rules and Abstract Syntax Tree (AST)-based templates, enabling LLM-based rewriting that maintains syntactic correctness while minimizing undesired circuit behaviors. A symbolic module is proposed for analyzing and optimizing finite state machine (FSM) logic, allowing fine-grained state merging and partial specification handling beyond the scope of pattern-based compilers. Furthermore, a fast verification pipeline, combining formal equivalence checks with test-driven validation, further reduces the complexity of verification. Experiments on the RTL-Rewriter benchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves power, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%, respectively, compared to the state-of-the-art methods. </p>
<blockquote>
<p>å¯„å­˜å™¨ä¼ è¾“çº§åˆ«ï¼ˆRTLï¼‰ä»£ç çš„ä¼˜åŒ–å¯¹äºæé«˜æ•°å­—ç”µè·¯åœ¨åˆæˆæ—©æœŸé˜¶æ®µçš„åŠŸç‡ã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰è‡³å…³é‡è¦ã€‚æ‰‹åŠ¨é‡å†™ï¼Œè¾…ä»¥åˆæˆåé¦ˆï¼Œå¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼Œä½†è¿™ç§æ–¹æ³•è€—æ—¶ä¸”æ˜“å‡ºé”™ã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºç¼–è¯‘å™¨çš„æ–¹æ³•å¾ˆéš¾å¤„ç†å¤æ‚çš„è®¾è®¡çº¦æŸã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•å·²ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒLLMæ–¹æ³•å¾€å¾€éš¾ä»¥ç¡®ä¿ç”Ÿæˆçš„ä»£ç ä¸æä¾›çš„æç¤ºå¯¹é½ã€‚æœ¬æ–‡æå‡ºäº†SymRTLOï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»å…ƒç¬¦å·RTLä¼˜åŒ–æ¡†æ¶ï¼Œæ— ç¼é›†æˆäº†åŸºäºLLMçš„ä»£ç é‡å†™å’Œç¬¦å·æ¨ç†æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¼˜åŒ–è§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå’ŒåŸºäºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„æ¨¡æ¿ï¼Œä½¿LLMé‡å†™èƒ½å¤Ÿåœ¨ä¿æŒè¯­æ³•æ­£ç¡®çš„åŒæ—¶æœ€å°åŒ–ä¸å¿…è¦ç”µè·¯è¡Œä¸ºã€‚æå‡ºäº†ä¸€ä¸ªç¬¦å·æ¨¡å—ï¼Œç”¨äºåˆ†æå’Œä¼˜åŒ–æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰é€»è¾‘ï¼Œå®ç°ç²¾ç»†çš„çŠ¶æ€åˆå¹¶å’Œéƒ¨åˆ†è§„æ ¼å¤„ç†ï¼Œè¶…å‡ºæ¨¡å¼ç¼–è¯‘å™¨çš„èŒƒå›´ã€‚æ­¤å¤–ï¼Œç»“åˆå½¢å¼ç­‰ä»·æ£€æŸ¥å’Œæµ‹è¯•é©±åŠ¨éªŒè¯çš„å¿«é€ŸéªŒè¯ç®¡é“è¿›ä¸€æ­¥é™ä½äº†éªŒè¯çš„å¤æ‚æ€§ã€‚åœ¨Synopsys Design Compilerå’ŒYosysçš„RTL-RewriteråŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒSymRTLOåœ¨åŠŸç‡ã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰æ–¹é¢åˆ†åˆ«æé«˜äº†é«˜è¾¾43.9%ã€62.5%å’Œ51.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10369v1">PDF</a> 16 pages, 8 figures, 7 tables. Under Review</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSymRTLOçš„æ–°å‹ç¥ç»å…ƒç¬¦å·RTLä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†LLMä»£ç é‡å†™å’Œç¬¦å·æ¨ç†æŠ€æœ¯ã€‚å®ƒé‡‡ç”¨ä¼˜åŒ–è§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿå’ŒåŸºäºASTçš„æ¨¡æ¿ï¼Œç¡®ä¿LLMé‡å†™ä»£ç åœ¨ä¿æŒè¯­æ³•æ­£ç¡®æ€§çš„åŒæ—¶æœ€å°åŒ–ä¸å¸Œæœ›å‡ºç°çš„ç”µè·¯è¡Œä¸ºã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æå‡ºäº†ä¸€ç§ç¬¦å·æ¨¡å—ï¼Œç”¨äºåˆ†æå’Œä¼˜åŒ–æœ‰é™çŠ¶æ€æœºé€»è¾‘ï¼Œå®ç°äº†ç²¾ç»†çŠ¶æ€åˆå¹¶å’Œéƒ¨åˆ†è§„èŒƒå¤„ç†ï¼Œè¶…å‡ºäº†åŸºäºæ¨¡å¼çš„ç¼–è¯‘å™¨çš„èŒƒå›´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSymRTLOåœ¨Synopsys Design Compilerå’ŒYosysçš„RTL-RewriteråŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹åŠŸç‡ã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰çš„æ”¹è¿›åˆ†åˆ«é«˜è¾¾43.9%ã€62.5%å’Œ51.1%ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ä¼˜åŒ–RTLä»£ç å¯¹äºæé«˜æ•°å­—ç”µè·¯åœ¨æ—©æœŸåˆæˆé˜¶æ®µçš„åŠŸç‡ã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç¼–è¯‘å™¨æ–¹æ³•åœ¨å¤„ç†å¤æ‚è®¾è®¡çº¦æŸæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè€ŒLLMæ–¹æ³•åˆ™æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>SymRTLOæ¡†æ¶ç»“åˆäº†LLMä»£ç é‡å†™å’Œç¬¦å·æ¨ç†æŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç ä¸æä¾›çš„æç¤ºå¯¹é½ã€‚</li>
<li>ä½¿ç”¨ä¼˜åŒ–è§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿå’ŒåŸºäºASTçš„æ¨¡æ¿ï¼Œä»¥æé«˜è¯­æ³•æ­£ç¡®æ€§å’Œå‡å°‘ä¸å¸Œæœ›å‡ºç°çš„ç”µè·¯è¡Œä¸ºã€‚</li>
<li>ç¬¦å·æ¨¡å—ç”¨äºåˆ†æå’Œä¼˜åŒ–FSMé€»è¾‘ï¼Œå®ç°ç²¾ç»†çŠ¶æ€åˆå¹¶å’Œéƒ¨åˆ†è§„èŒƒå¤„ç†ã€‚</li>
<li>ç»“åˆå½¢å¼ç­‰ä»·æ£€æŸ¥å’Œæµ‹è¯•é©±åŠ¨éªŒè¯çš„å¿«é€ŸéªŒè¯æµç¨‹ï¼Œé™ä½äº†éªŒè¯çš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5cc04c272d51273956dc8021e427e104.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63aa113f831c7e0f6ba4afa8afe02cab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-709a9222dcaf8561b1a7e68141e0d43c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d9fb552602990ee898ca57635e7c11a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4e324a1b3b7517cafdaaf5f715fb69b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b89deca35911424ed634d22e4cfa879.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MultiLoKo-a-multilingual-local-knowledge-benchmark-for-LLMs-spanning-31-languages"><a href="#MultiLoKo-a-multilingual-local-knowledge-benchmark-for-LLMs-spanning-31-languages" class="headerlink" title="MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31   languages"></a>MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31   languages</h2><p><strong>Authors:Dieuwke Hupkes, Nikolay Bogoychev</strong></p>
<p>We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs covering 31 languages. MultiLoKo consists of three partitions: a main partition consisting of 500 questions per language, separately sourced to be locally relevant to the specific language, and two translated partitions, containing human-authored translations from 30 non-English languages to English and vice versa. For comparison, we also release corresponding machine-authored translations. The data is equally distributed over two splits: a dev split and a blind, out-of-distribution test split. MultiLoKo can be used to study a variety of questions regarding the multilinguality of LLMs as well as meta-questions about multilingual benchmark creation. We compute MultiLoKo scores for 11 base and chat models marketed to be multilingual and study their average performance, their performance parity across languages, how much their ability to answer questions depends on the question language, and which languages are most difficult. None of the models we studied performs well on MultiLoKo, as indicated by low average scores as well as large differences between the best and worst scoring languages. Furthermore, we find a substantial effect of the question language, indicating sub-optimal knowledge transfer between languages. Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MultiLoKoï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°LLMå¤šè¯­è¨€èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–31ç§è¯­è¨€ã€‚MultiLoKoç”±ä¸‰ä¸ªåˆ†åŒºç»„æˆï¼šä¸€ä¸ªä¸»åˆ†åŒºï¼ŒåŒ…å«é’ˆå¯¹æ¯ç§è¯­è¨€å•ç‹¬é‡‡é›†çš„500ä¸ªé—®é¢˜ï¼Œä»¥é€‚åº”å½“åœ°è¯­è¨€çš„å…·ä½“èƒŒæ™¯ï¼›ä¸¤ä¸ªç¿»è¯‘åˆ†åŒºï¼ŒåŒ…å«ä»30ç§éè‹±è¯­åˆ°è‹±è¯­å’Œä»è‹±è¯­åˆ°è¿™äº›éè‹±è¯­çš„æœºå™¨ç¿»è¯‘å’Œäººç±»ç¿»è¯‘ã€‚ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ç›¸åº”çš„æœºå™¨ç¿»è¯‘ç‰ˆæœ¬ã€‚æ•°æ®è¢«å¹³å‡åˆ†é…åˆ°ä¸¤ä¸ªåˆ†å‰²é›†ï¼šä¸€ä¸ªå¼€å‘åˆ†å‰²é›†å’Œä¸€ä¸ªç›²æ€ã€è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æµ‹è¯•åˆ†å‰²é›†ã€‚MultiLoKoå¯ç”¨äºç ”ç©¶å…³äºLLMå¤šè¯­è¨€èƒ½åŠ›çš„å„ç§é—®é¢˜ä»¥åŠå…³äºå¤šè¯­è¨€åŸºå‡†æµ‹è¯•åˆ›å»ºçš„å…ƒé—®é¢˜ã€‚æˆ‘ä»¬è®¡ç®—äº†å¸‚åœºä¸Šé”€å”®çš„11æ¬¾åŸºç¡€æ¨¡å‹å’ŒèŠå¤©æ¨¡å‹çš„MultiLoKoå¾—åˆ†ï¼Œå®ƒä»¬æ ‡æ¦œè‡ªå·±å…·å¤‡å¤šè¯­è¨€èƒ½åŠ›ï¼Œç ”ç©¶äº†å®ƒä»¬çš„å¹³å‡æ€§èƒ½ã€è·¨è¯­è¨€çš„æ€§èƒ½å‡è¡¡æ€§ã€å®ƒä»¬å›ç­”é—®é¢˜çš„èƒ½åŠ›åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå–å†³äºé—®é¢˜è¯­è¨€ï¼Œä»¥åŠå“ªäº›è¯­è¨€æ˜¯æœ€å›°éš¾çš„ã€‚æˆ‘ä»¬æ‰€ç ”ç©¶çš„æ‰€æœ‰æ¨¡å‹åœ¨MultiLoKoä¸Šçš„è¡¨ç°éƒ½ä¸ä½³ï¼Œè¿™ä½“ç°åœ¨å¹³å‡å¾—åˆ†è¾ƒä½ä»¥åŠæœ€ä½³å’Œæœ€å·®å¾—åˆ†è¯­è¨€ä¹‹é—´çš„å·®å¼‚è¾ƒå¤§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é—®é¢˜è¯­è¨€å…·æœ‰æ˜¾è‘—å½±å“ï¼Œè¡¨æ˜è¯­è¨€ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»å¹¶ä¸ç†æƒ³ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨æœ¬åœ°æ•°æ®ç›¸å¯¹äºè‹±è¯­ç¿»è¯‘æ•°æ®ä¼šå¯¼è‡´æœ€ä½³æ¨¡å‹ä¹‹é—´çš„å·®å¼‚è¶…è¿‡20åˆ†ï¼Œè¿™ä¼šæå¤§åœ°æ”¹å˜æŸäº›è¯­è¨€çš„é¢„ä¼°éš¾åº¦ã€‚å¦‚æœä½¿ç”¨æœºå™¨ç¿»è¯‘è€Œä¸æ˜¯äººå·¥ç¿»è¯‘ï¼Œæˆ‘ä»¬å‘ç°å¯¹è¯­è¨€éš¾åº¦æ’åºçš„å½±å“è¾ƒå°ï¼Œæ¨¡å‹æ’åå·®å¼‚è¾ƒå¤§ï¼Œä¸”æ‰€æœ‰æ¨¡å‹çš„é¢„ä¼°æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MultiLoKoè¿™ä¸€æ–°çš„LLMå¤šè¯­ç§æ€§èƒ½è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•æ¶µç›–31ç§è¯­è¨€ï¼Œåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼šé’ˆå¯¹ä¸åŒè¯­è¨€çš„åœ°æ–¹ç›¸å…³æ€§é—®é¢˜ã€ä»éè‹±è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ä»¥åŠåå‘ç¿»è¯‘ã€‚åŒæ—¶ï¼Œä¹Ÿæä¾›äº†æœºå™¨ç¿»è¯‘ç‰ˆæœ¬ã€‚MultiLoKoå¯ç”¨äºç ”ç©¶LLMçš„å¤šè¯­ç§æ€§èƒ½ä»¥åŠå¤šè¯­ç§åŸºå‡†æµ‹è¯•ä¸­çš„å…ƒé—®é¢˜ã€‚é€šè¿‡å¯¹11ç§åŸºç¡€åŠèŠå¤©æ¨¡å¼çš„é€šç”¨è¯­æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨MultiLoKoä¸Šçš„è¡¨ç°æ™®éä¸ä½³ï¼Œä¸åŒè¯­è¨€é—´çš„æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œä¸”ç­”é¢˜èƒ½åŠ›å—é—®é¢˜è¯­è¨€å½±å“è¾ƒå¤§ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”æœ¬åœ°æ•°æ®å’Œä½¿ç”¨è‹±è¯­ç¿»è¯‘çš„æ•°æ®ï¼Œæœ€ä½³æ¨¡å‹çš„è¡¨ç°å·®å¼‚è¶…è¿‡20åˆ†ï¼Œå¯¹è¯­è¨€éš¾åº¦çš„è¯„ä¼°äº§ç”Ÿé‡å¤§å½±å“ã€‚ä½¿ç”¨æœºå™¨ç¿»è¯‘è€Œéäººå·¥ç¿»è¯‘å¯¹è¯­è¨€éš¾åº¦çš„æ’åºå½±å“è¾ƒå°ï¼Œä½†æ¨¡å‹æ’åå·®å¼‚æ›´å¤§ï¼Œä¸”æ‰€æœ‰æ¨¡å‹çš„é¢„ä¼°æ€§èƒ½éƒ½æœ‰å¤§å¹…ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MultiLoKoæ˜¯ä¸€ä¸ªæ–°çš„LLMå¤šè¯­ç§æ€§èƒ½è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–31ç§è¯­è¨€ã€‚</li>
<li>MultiLoKoåŒ…æ‹¬é’ˆå¯¹ä¸åŒè¯­è¨€çš„åœ°æ–¹ç›¸å…³æ€§é—®é¢˜ã€äººå·¥ç¿»è¯‘å’Œæœºå™¨ç¿»è¯‘ç‰ˆæœ¬ã€‚</li>
<li>MultiLoKoå¯ç”¨äºç ”ç©¶LLMçš„å¤šè¯­ç§æ€§èƒ½ä»¥åŠåŸºå‡†æµ‹è¯•çš„å…ƒé—®é¢˜ã€‚</li>
<li>ç›®å‰å¸‚åœºä¸Šçš„é€šç”¨è¯­æ¨¡å‹åœ¨MultiLoKoä¸Šçš„è¡¨ç°æ™®éä¸ä½³ã€‚</li>
<li>ä¸åŒè¯­è¨€é—´çš„æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œç­”é¢˜èƒ½åŠ›å—é—®é¢˜è¯­è¨€å½±å“æ˜¾è‘—ã€‚</li>
<li>æ•°æ®æ¥æºï¼ˆæœ¬åœ°æ•°æ®æˆ–ç¿»è¯‘æ•°æ®ï¼‰å¯¹æ¨¡å‹è¡¨ç°æœ‰é‡å¤§å½±å“ï¼Œå°¤å…¶æ˜¯æœ€ä½³æ¨¡å‹çš„è¡¨ç°å·®å¼‚è¶…è¿‡20åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b756539db2033c0958eb8e63de997394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e7c126be9d1bc1f8d188a2dfa69f23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c10060aacc343df7a62d6bd31920ebc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge"><a href="#VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge" class="headerlink" title="VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge"></a>VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge</h2><p><strong>Authors:Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue</strong></p>
<p>Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with â€œthinkingâ€ modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•é€šå¸¸å°†æ¨ç†ä¸ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ··æ·†ï¼Œåœ¨éä¸“ä¸šç¯å¢ƒä¸­å¾ˆéš¾éš”ç¦»å’Œè¯„ä¼°ä¸€èˆ¬çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisualPuzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†è§‰æ¨ç†ä¸ºç›®æ ‡ï¼ŒåŒæ—¶åˆ»æ„å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†ä¾èµ–çš„åŸºå‡†æµ‹è¯•ã€‚VisualPuzzlesåŒ…å«äº”ä¸ªç±»åˆ«çš„é—®é¢˜ï¼šç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬çš„é—®é¢˜ä¸»è¦æ¥æºäºæ‰‹åŠ¨ç¿»è¯‘çš„ä¸­å›½å…¬åŠ¡å‘˜è€ƒè¯•ä¸­çš„é€»è¾‘æ¨ç†é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œä¸MMMUç­‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVisualPuzzleså¯¹ä¸“ä¸šçŸ¥è¯†çš„è¦æ±‚å¤§å¤§é™ä½ï¼Œä½†éœ€è¦æ›´å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°çœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„å¤šå…ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°å§‹ç»ˆè½åäºäººç±»çš„è¡¨ç°ï¼Œè€Œåœ¨çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„è‰¯å¥½è¡¨ç°å¹¶ä¸ä¸€å®šèƒ½åœ¨ä¾§é‡äºæ¨ç†å’ŒçŸ¥è¯†è¾ƒè½»çš„ä»»åŠ¡ä¸Šè·å¾—æˆåŠŸã€‚æ­¤å¤–ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›ï¼ˆå¦‚æ‰©å¤§æ¨ç†è®¡ç®—è§„æ¨¡ï¼ˆâ€œæ€è€ƒâ€æ¨¡å¼ï¼‰ï¼‰åœ¨æ¨¡å‹ç±»å‹å’Œä»»åŠ¡ç±»å‹ä¹‹é—´äº§ç”Ÿä¸ä¸€è‡´çš„æ•ˆç›Šï¼Œå¹¶ä¸”æˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹å¤§å°ä¸æ€§èƒ½ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¸æ›´ä¾§é‡äºçŸ¥è¯†çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨VisualPuzzlesä¸Šå±•ç°å‡ºä¸åŒçš„æ¨ç†å’Œç­”é¢˜æ¨¡å¼ã€‚VisualPuzzlesæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ™°çš„è§†è§’æ¥è¯„ä¼°è¶…è¶Šäº‹å®å›å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10342v1">PDF</a> 56 pages, 43 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å½“å‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ç»å¸¸æ··æ·†æ¨ç†ä¸é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisualPuzzlesåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é’ˆå¯¹è§†è§‰æ¨ç†è¿›è¡Œæµ‹è¯„ï¼Œå¹¶åˆ»æ„å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†ä¾èµ–ã€‚VisualPuzzlesåŒ…å«æ¶µç›–äº”å¤§ç±»åˆ«çš„å¤šæ ·åŒ–é—®é¢˜ï¼šç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬çš„ä¸»è¦é—®é¢˜æ¥æºäºä¸­å›½å…¬åŠ¡å‘˜è€ƒè¯•é€»è¾‘é¢˜ç›®çš„æ‰‹åŠ¨ç¿»è¯‘ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºMMMUç­‰åŸºå‡†æµ‹è¯•ï¼ŒVisualPuzzleså¯¹é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„éœ€æ±‚æ›´ä½ä¸”æ¶‰åŠæ¨ç†æ›´åŠ å¤æ‚ï¼Œæ›´èƒ½æœ‰æ•ˆè¯„ä¼°çœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤šåª’ä½“è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°å§‹ç»ˆè½åäºäººç±»è¡¨ç°ï¼Œä¸”åœ¨æ³¨é‡æ¨ç†ã€è½»çŸ¥è¯†çš„ä»»åŠ¡ä¸Šï¼ŒçŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•çš„é«˜è¡¨ç°æœªå¿…èƒ½è½¬åŒ–ä¸ºæˆåŠŸã€‚æ­¤å¤–ï¼Œå¢åŠ æ¨ç†åŠŸèƒ½å¦‚æ‰©å¤§æ¨ç†è®¡ç®—ï¼ˆâ€œæ€è€ƒâ€æ¨¡å¼ï¼‰æ‰€å¸¦æ¥çš„å¢ç›Šåœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ç±»å‹ä¹‹é—´è¡¨ç°å‡ºä¸ä¸€è‡´çš„æƒ…å†µï¼Œå¹¶ä¸”æ¨¡å‹å¤§å°ä¸è¡¨ç°ä¹‹é—´æ²¡æœ‰æ˜æ˜¾ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ç›¸è¾ƒäºæ³¨é‡çŸ¥è¯†çš„åŸºå‡†æµ‹è¯•ï¼Œæ¨¡å‹åœ¨VisualPuzzlesä¸Šå±•ç°å‡ºä¸åŒçš„æ¨ç†å’Œç­”é¢˜æ¨¡å¼ã€‚å› æ­¤ï¼ŒVisualPuzzlesæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ™°çš„è§†è§’æ¥è¯„ä¼°è¶…è¶Šäº‹å®å›å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VisualPuzzlesåŸºå‡†æµ‹è¯•æ—¨åœ¨æµ‹è¯„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå‡å°‘é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„ä¾èµ–ã€‚</li>
<li>åŒ…å«äº”å¤§ç±»åˆ«çš„é€»è¾‘é—®é¢˜ï¼Œæ¶µç›–ç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ç­‰å¤šå…ƒæ–¹é¢ã€‚é—®é¢˜æ¥æºä¸»è¦æ˜¯ä»ä¸­å›½çš„å…¬åŠ¡å‘˜è€ƒè¯•é€»è¾‘é¢˜ç›®ä¸­æ‰‹åŠ¨ç¿»è¯‘å¾—å‡ºã€‚</li>
<li>å®éªŒæ˜¾ç¤ºVisualPuzzlesèƒ½æ›´æœ‰æ•ˆåœ°è¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œå› ä¸ºå®ƒå¯¹çŸ¥è¯†çš„è¦æ±‚è¾ƒä½ä¸”æ›´æ³¨é‡æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„å¤šåª’ä½“è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°è½åäºäººç±»è¡¨ç°ã€‚çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•çš„é«˜è¡¨ç°å¹¶ä¸ç­‰åŒäºåœ¨æ³¨é‡æ¨ç†çš„ä»»åŠ¡ä¸Šçš„æˆåŠŸã€‚</li>
<li>å¢åŠ æ¨ç†è®¡ç®—ï¼ˆå¦‚å¼€å¯â€œæ€è€ƒâ€æ¨¡å¼ï¼‰æ‰€å¸¦æ¥çš„å¢ç›Šä¸ä¸€è‡´ï¼Œå¹¶ä¸”æ¨¡å‹å¤§å°ä¸å…¶è¡¨ç°å¹¶æ— æ˜ç¡®ç›¸å…³æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„ç­”é¢˜æ¨¡å¼ä¸çŸ¥è¯†å¯¼å‘çš„åŸºå‡†æµ‹è¯•ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a0db482bb6dba698457faf7640a1f79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d978ad9899040c77f89efe8c0764d40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ec584ca1e2abec1b03750f6b198f048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13f7f96ad625b34e51ee23a2c63e4a07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e74d6329666e97d1651eb85005f486e2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Forecasting-from-Clinical-Textual-Time-Series-Adaptations-of-the-Encoder-and-Decoder-Language-Model-Families"><a href="#Forecasting-from-Clinical-Textual-Time-Series-Adaptations-of-the-Encoder-and-Decoder-Language-Model-Families" class="headerlink" title="Forecasting from Clinical Textual Time Series: Adaptations of the   Encoder and Decoder Language Model Families"></a>Forecasting from Clinical Textual Time Series: Adaptations of the   Encoder and Decoder Language Model Families</h2><p><strong>Authors:Shahriar Noroozizadeh, Sayantan Kumar, Jeremy C. Weiss</strong></p>
<p>Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findingsâ€“extracted via an LLM-assisted annotation pipelineâ€“serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use. </p>
<blockquote>
<p>ä¸´åºŠç—…ä¾‹æŠ¥å‘ŠåŒ…å«äº†ä¸°å¯Œçš„ã€å…³äºç—…äººéšæ—¶é—´å˜åŒ–çš„æƒ…å†µï¼Œè€Œä¼ ç»Ÿçš„ä¾èµ–ç»“æ„åŒ–æ•°æ®çš„æœºå™¨å­¦ä¹ æ–¹æ³•å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬æ—¶é—´åºåˆ—ä¸­å¼•å…¥äº†é¢„æµ‹é—®é¢˜ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„æ³¨é‡Šç®¡é“æå–çš„æ—¶é—´æˆ³ä¸´åºŠå‘ç°ä½œä¸ºä¸»è¦è¾“å…¥æ¥è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸€ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäºå¾®è°ƒè§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºç¼–ç å™¨çš„å˜å‹å™¨æ¨¡å‹ï¼Œç”¨äºäº‹ä»¶å‘ç”Ÿçš„é¢„æµ‹ã€æ—¶é—´é¡ºåºå’Œç”Ÿå­˜åˆ†æä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºç¼–ç å™¨çš„æ¨¡å‹åœ¨çŸ­æœŸå’Œé•¿æœŸäº‹ä»¶é¢„æµ‹çš„F1å¾—åˆ†ä¸ŠæŒç»­è¡¨ç°æ›´é«˜ï¼Œå¹¶ä¸”å…·æœ‰å‡ºè‰²çš„æ—¶é—´ä¸€è‡´æ€§ã€‚è€Œç»è¿‡å¾®è°ƒåçš„æ©ç æ–¹æ³•åˆ™æé«˜äº†æ’åæ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„è§£ç å™¨æ¨¡å‹åœ¨ç”Ÿå­˜åˆ†æä¸­æ˜¾ç¤ºå‡ºç›¸å¯¹ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸé¢„åç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„æ•æ„Ÿæ€§åˆ†æè¿›ä¸€æ­¥è¯æ˜äº†æ—¶é—´é¡ºåºçš„é‡è¦æ€§ï¼Œè¿™éœ€è¦æ„å»ºä¸´åºŠæ—¶é—´åºåˆ—ï¼Œä¸æ–‡æœ¬é¡ºåºï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ç»å…¸è®­ç»ƒä¸­æ‰€ç”¨æ–‡æœ¬è¾“å…¥æ ¼å¼ï¼‰è¿›è¡Œå¯¹æ¯”ã€‚è¿™çªå‡ºäº†æœ‰åºè¯­æ–™åº“æ‰€å¸¦æ¥çš„é¢å¤–å¥½å¤„ï¼Œå¯¹äºå¹¿æ³›åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ—¶ä»£çš„æ—¶åºä»»åŠ¡å…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10340v1">PDF</a> Machine Learning for Healthcare (MLHC 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»æ–‡æœ¬æ—¶é—´åºåˆ—è¿›è¡Œé¢„æµ‹çš„é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„æ³¨é‡Šç®¡é“æå–çš„æ—¶é—´æˆ³ä¸´åºŠæ•°æ®ä½œä¸ºä¸»è¦è¾“å…¥æ¥è¿›è¡Œé¢„æµ‹ã€‚ç³»ç»Ÿè¯„ä¼°äº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬å¾®è°ƒè¿‡çš„åŸºäºè§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºç¼–ç å™¨çš„å˜æ¢å™¨ï¼Œä»»åŠ¡åŒ…æ‹¬äº‹ä»¶å‘ç”Ÿé¢„æµ‹ã€æ—¶é—´é¡ºåºå’Œç”Ÿå­˜åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºç¼–ç å™¨çš„æ¨¡å‹åœ¨çŸ­æœŸå’Œé•¿æœŸäº‹ä»¶é¢„æµ‹ä¸­æŒç»­è·å¾—æ›´é«˜çš„F1åˆ†æ•°å’Œä¼˜è¶Šçš„æ—¶é—´ä¸€è‡´æ€§ï¼Œè€Œå¾®è°ƒè¿‡çš„é®è”½æ–¹æ³•æé«˜äº†æ’åæ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒæŒ‡ä»¤è®­ç»ƒè¿‡çš„è§£ç å™¨æ¨¡å‹åœ¨ç”Ÿå­˜åˆ†æä¸­æ˜¾ç¤ºå‡ºç›¸å¯¹ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸé¢„åè®¾ç½®ä¸­ã€‚æ•æ„Ÿæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¸æ—¶é—´é¡ºåºæœ‰å…³çš„æ—¶é—´åºåˆ—æ„å»ºæ¯”æ–‡æœ¬é¡ºåºï¼ˆLLMç»å…¸è®­ç»ƒä¸­è¾“å…¥æ–‡æœ¬æ ¼å¼ï¼‰æ›´ä¸ºé‡è¦ï¼Œè¿™çªæ˜¾äº†æ—¶é—´é¡ºåºè¯­æ–™åº“å¯èƒ½å¸¦æ¥çš„é¢å¤–å¥½å¤„ï¼Œå¯¹å¹¿æ³›ä½¿ç”¨LLMçš„æ—¶ä»£çš„æ—¶é—´ä»»åŠ¡å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠæ¡ˆä¾‹æŠ¥å‘ŠåŒ…å«ä¸°å¯Œçš„æ—¶åºæ‚£è€…è½¨è¿¹ä¿¡æ¯ï¼Œä½†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨ã€‚</li>
<li>å¼•å…¥ä»æ–‡æœ¬æ—¶é—´åºåˆ—è¿›è¡Œé¢„æµ‹çš„é—®é¢˜ï¼Œä¸»è¦ä½¿ç”¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©æå–çš„æ—¶é—´æˆ³ä¸´åºŠæ•°æ®ä½œä¸ºé¢„æµ‹è¾“å…¥ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬å¾®è°ƒè¿‡çš„åŸºäºè§£ç å™¨å’ŒåŸºäºç¼–ç å™¨çš„æ¨¡å‹ã€‚</li>
<li>åŸºäºç¼–ç å™¨çš„æ¨¡å‹åœ¨äº‹ä»¶é¢„æµ‹ã€æ—¶é—´é¡ºåºå’Œç”Ÿå­˜åˆ†æä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>å¾®è°ƒè¿‡çš„é®è”½æ–¹æ³•æœ‰åŠ©äºæé«˜æ’åæ€§èƒ½ã€‚</li>
<li>æŒ‡ä»¤è®­ç»ƒè¿‡çš„è§£ç å™¨æ¨¡å‹åœ¨ç”Ÿå­˜åˆ†æä¸­ç›¸å¯¹ä¼˜åŠ¿æ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸé¢„åæƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-912aa6aeb495333ac92aed005360a4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67239e927ee377beb8f2e9f7ff182aec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e7e190e13e361146f311c6f16860043.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Probing-then-Editing-Response-Personality-of-Large-Language-Models"><a href="#Probing-then-Editing-Response-Personality-of-Large-Language-Models" class="headerlink" title="Probing then Editing Response Personality of Large Language Models"></a>Probing then Editing Response Personality of Large Language Models</h2><p><strong>Authors:Tianjie Ju, Zhenyu Shao, Bowen Wang, Yujia Chen, Zhuosheng Zhang, Hao Fei, Mong-Li Lee, Wynne Hsu, Sufeng Duan, Gongshen Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that exhibit consistent personality traits. Despite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within LLM parameters. In this paper, we introduce a layer-wise probing framework to systematically investigate the layer-wise capability of LLMs in encoding personality for responding. We conduct probing experiments on 11 open-source LLMs over the PersonalityEdit benchmark and find that LLMs predominantly encode personality for responding in their middle and upper layers, with instruction-tuned models demonstrating a slightly clearer separation of personality traits. Furthermore, by interpreting the trained probing hyperplane as a layer-wise boundary for each personality category, we propose a layer-wise perturbation method to edit the personality expressed by LLMs during inference. Our results show that even when the prompt explicitly specifies a particular personality, our method can still successfully alter the response personality of LLMs. Interestingly, the difficulty of converting between certain personality traits varies substantially, which aligns with the representational distances in our probing experiments. Finally, we conduct a comprehensive MMLU benchmark evaluation and time overhead analysis, demonstrating that our proposed personality editing method incurs only minimal degradation in general capabilities while maintaining low training costs and acceptable inference latency. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/universe-sky/probing-then-editing-personality">https://github.com/universe-sky/probing-then-editing-personality</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºç”Ÿæˆå…·æœ‰ä¸€è‡´æ€§æ ¼ç‰¹å¾å“åº”çš„æ½œåŠ›ã€‚å°½ç®¡å·²ç»åšå‡ºäº†é‡å¤§åŠªåŠ›æ¥åˆ†æé€šè¿‡è¾“å‡ºè¯„ä¼°è¡¨ç°çš„æ€§æ ¼è¡¨è¾¾ï¼Œä½†å¯¹äºæ€§æ ¼ç‰¹å¾å¦‚ä½•åœ¨LLMå‚æ•°å†…éƒ¨è¿›è¡Œç¼–ç ä»çŸ¥ä¹‹ç”šå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€å±‚æ¢æµ‹æ¡†æ¶ï¼Œä»¥ç³»ç»Ÿåœ°ç ”ç©¶LLMåœ¨å“åº”ä¸­ç¼–ç æ€§æ ¼çš„é€å±‚èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹ä¸ªæ€§ç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸Šçš„1mä¸ªå¼€æºLLMè¿›è¡Œäº†æ¢æµ‹å®éªŒï¼Œå‘ç°LLMä¸»è¦åœ¨ä¸­é—´å’Œä¸Šå±‚ç¼–ç å“åº”æ€§æ ¼ï¼ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨æ€§æ ¼ç‰¹å¾æ–¹é¢æ˜¾ç¤ºå‡ºç•¥æ¸…æ™°çš„åˆ†ç¦»ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†è®­ç»ƒå¥½çš„æ¢æµ‹è¶…å¹³é¢è§£é‡Šä¸ºæ¯ä¸ªæ€§æ ¼ç±»åˆ«çš„é€å±‚è¾¹ç•Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€å±‚æ‰°åŠ¨æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼–è¾‘LLMæ‰€è¡¨ç°å‡ºçš„æ€§æ ¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æç¤ºæ˜ç¡®æŒ‡å®šäº†ç‰¹å®šçš„æ€§æ ¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»ç„¶å¯ä»¥æˆåŠŸåœ°æ”¹å˜LLMçš„å“åº”æ€§æ ¼ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æŸäº›æ€§æ ¼ä¹‹é—´è¿›è¡Œè½¬æ¢çš„éš¾åº¦å·®å¼‚å¾ˆå¤§ï¼Œè¿™ä¸æˆ‘ä»¬çš„æ¢æµ‹å®éªŒä¸­çš„ä»£è¡¨æ€§è·ç¦»ç›¸ç¬¦ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„MMLUåŸºå‡†æµ‹è¯•è¯„ä¼°å’Œæ—¶é—´å¼€é”€åˆ†æï¼Œè¯æ˜æˆ‘ä»¬æå‡ºçš„æ€§æ ¼ç¼–è¾‘æ–¹æ³•åªä¼šå¯¹ä¸€èˆ¬èƒ½åŠ›é€ æˆæœ€å°é€€åŒ–ï¼ŒåŒæ—¶ä¿æŒä½è®­ç»ƒæˆæœ¬å’Œå¯æ¥å—çš„æ¨ç†å»¶è¿Ÿã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/universe-sky/probing-then-editing-personality%E3%80%82">https://github.com/universe-sky/probing-then-editing-personalityã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10227v1">PDF</a> Working in Progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿç”Ÿæˆå±•ç°ä¸€è‡´æ€§æ ¼ç‰¹è´¨çš„å›åº”ã€‚æœ¬æ–‡æå‡ºä¸€ç§é€å±‚æ¢æµ‹æ¡†æ¶ï¼Œç³»ç»Ÿåœ°ç ”ç©¶LLMåœ¨å›åº”ä¸­ç¼–ç æ€§æ ¼çš„é€å±‚èƒ½åŠ›ã€‚é€šè¿‡å¯¹11ä¸ªå¼€æºLLMåœ¨PersonalityEditåŸºå‡†æµ‹è¯•ä¸Šçš„æ¢æµ‹å®éªŒï¼Œå‘ç°LLMä¸»è¦åœ¨ä¸­é—´å’Œä¸Šå±‚ç¼–ç æ€§æ ¼ä»¥è¿›è¡Œå›åº”ï¼ŒæŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹åœ¨æ€§æ ¼ç‰¹è´¨ä¸Šè¡¨ç°å‡ºç¨æ¸…æ™°çš„åˆ†ç¦»ã€‚é€šè¿‡è§£é‡Šè®­ç»ƒå¥½çš„æ¢æµ‹è¶…å¹³é¢ä½œä¸ºæ¯å±‚æ€§æ ¼ç±»åˆ«çš„è¾¹ç•Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€å±‚æ‰°åŠ¨æ–¹æ³•ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼–è¾‘LLMæ‰€è¡¨ç°å‡ºçš„æ€§æ ¼ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æç¤ºæ˜ç¡®æŒ‡å®šäº†ç‰¹å®šæ€§æ ¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»ç„¶å¯ä»¥æˆåŠŸæ”¹å˜LLMçš„å›åº”æ€§æ ¼ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„MMLUåŸºå‡†æµ‹è¯•è¯„ä¼°å’Œæ—¶é•¿åˆ†æï¼Œè¯æ˜æ‰€æå‡ºçš„æ€§æ ¼ç¼–è¾‘æ–¹æ³•åªä¼šå¯¼è‡´ä¸€èˆ¬èƒ½åŠ›çš„è½»å¾®ä¸‹é™ï¼ŒåŒæ—¶ä¿æŒä½è®­ç»ƒæˆæœ¬å’Œå¯æ¥å—çš„æ¨ç†å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMèƒ½å¤Ÿç”Ÿæˆå±•ç°ä¸€è‡´æ€§æ ¼ç‰¹è´¨çš„å›åº”ã€‚</li>
<li>é€å±‚æ¢æµ‹æ¡†æ¶ç”¨äºç ”ç©¶LLMç¼–ç æ€§æ ¼çš„é€å±‚èƒ½åŠ›ã€‚</li>
<li>LLMä¸»è¦åœ¨ä¸­é—´å’Œä¸Šå±‚ç¼–ç æ€§æ ¼ä»¥è¿›è¡Œå›åº”ã€‚</li>
<li>æŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹åœ¨æ€§æ ¼ç‰¹è´¨çš„è¡¨è¾¾ä¸Šç¨æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†é€šè¿‡é€å±‚æ‰°åŠ¨æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼–è¾‘LLMçš„æ€§æ ¼ã€‚</li>
<li>å³ä½¿æç¤ºæŒ‡å®šäº†ç‰¹å®šæ€§æ ¼ï¼Œä¹Ÿèƒ½æˆåŠŸæ”¹å˜LLMçš„å›åº”æ€§æ ¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca5c9749cf7738baafe1b1aa27af95a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36d85c9ea4ad2bb595610c67d008a6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3051aeef8a7a644f0142324c39695ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36087282b363466b98a64b0a0bf180af.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization"><a href="#Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization" class="headerlink" title="Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization"></a>Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization</h2><p><strong>Authors:Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, Junxian He</strong></p>
<p>Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid">https://github.com/hkust-nlp/GUIMid</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æä¾›è·¨å¹³å°è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè‡ªåŠ¨åŒ–å¤æ‚çš„æ•°å­—ä»»åŠ¡ï¼Œå…·æœ‰æ”¹å˜ç”Ÿäº§åŠ›å·¥ä½œæµç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¡¨ç°å¾€å¾€å—åˆ°é«˜è´¨é‡è½¨è¿¹æ•°æ®ç¨€ç¼ºçš„åˆ¶çº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µï¼Œåœ¨æ•°æ®ä¸°å¯Œã€æ¨ç†å¯†é›†çš„ä»»åŠ¡ä¸Šè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç„¶åç ”ç©¶å¦‚ä½•å°†è¿™äº›ä»»åŠ¡çº³å…¥GUIè§„åˆ’åœºæ™¯ä»¥ä¿ƒè¿›æ³›åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç³»åˆ—å…·æœ‰å¯è·å–æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬GUIæ„ŸçŸ¥ã€å¤šæ¨¡æ€æ¨ç†å’Œæ–‡æœ¬æ¨ç†ã€‚é€šè¿‡å¯¹11ä¸ªä¸­é—´è®­ç»ƒä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ï¼šï¼ˆ1ï¼‰ä»»åŠ¡æ³›åŒ–è¯æ˜éå¸¸æœ‰æ•ˆï¼Œåœ¨å¤§å¤šæ•°è®¾ç½®ä¸­éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œå¤šæ¨¡æ€æ•°å­¦æ¨ç†åœ¨AndroidWorldä¸Šçš„è¡¨ç°æé«˜äº†6.3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…æ–‡æœ¬çš„æ•°å­¦æ•°æ®æ˜¾è‘—æé«˜äº†GUIç½‘ç»œä»£ç†çš„æ€§èƒ½ï¼Œåœ¨ç½‘ç»œé¢†åŸŸï¼ˆWebArenaï¼‰å’ŒAndroidWorldä¸Šåˆ†åˆ«æé«˜äº†5.6%å’Œ5.4%ï¼Œçªæ˜¾äº†ä»æ–‡æœ¬åˆ°è§†è§‰é¢†åŸŸçš„è·¨æ¨¡æ€æ³›åŒ–çš„æ˜¾è‘—æ•ˆæœï¼›ï¼ˆ2ï¼‰ä¸å…ˆå‰çš„å‡è®¾ç›¸åï¼Œä¹‹å‰è¢«è®¤ä¸ºä¸GUIä»£ç†ä»»åŠ¡ç´§å¯†ç›¸å…³å¹¶å¹¿æ³›ç”¨äºè®­ç»ƒçš„GUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ç›¸å¯¹æœ‰é™ï¼›ï¼ˆ3ï¼‰åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬ç¡®å®šäº†æœ€æœ‰æ•ˆçš„ä¸­é—´è®­ç»ƒä»»åŠ¡å¹¶ç­–åˆ’äº†ä¼˜åŒ–åçš„æ··åˆæ•°æ®é›†ï¼Œåœ¨ç½‘ç»œé¢†åŸŸï¼ˆWebArenaï¼‰å’ŒAndroidWorldä¸Šçš„ç»å¯¹æ€§èƒ½åˆ†åˆ«æé«˜äº†8.0%å’Œ12.2%ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è½¬ç§»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºè§£å†³è¿™ä¸€æ–°å…´é¢†åŸŸä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æä¾›äº†å®ç”¨æ–¹æ³•ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/hkust-nlp/GUIMidä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10127v1">PDF</a> 24 pages, 11 figures</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–æ•°å­—ä»»åŠ¡çš„è·¨å¹³å°è§£å†³æ–¹æ¡ˆä¸­æœ‰ä¸€ç§åä¸ºGUIä»£ç†ï¼ˆGraphical User Interface Agentsï¼‰çš„æŠ€æœ¯å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½å—é™äºé«˜è´¨é‡è½¨è¿¹æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision Language Modelsï¼Œç®€ç§°VLMsï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥åœ¨GUIè§„åˆ’åœºæ™¯ä¸­å®ç°çŸ¥è¯†è¿ç§»ã€‚é€šè¿‡å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»»åŠ¡æ³›åŒ–éå¸¸æœ‰æ•ˆï¼Œå¹¶åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†GUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“æœ‰é™è¿™ä¸€å‘ç°ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæœ¬æ–‡æ„å»ºäº†ä¼˜åŒ–çš„æ··åˆæ•°æ®é›†å¹¶ç¡®å®šäº†æœ€æœ‰æ•ˆçš„ä¸­é—´è®­ç»ƒä»»åŠ¡ï¼Œä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è¿ç§»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œå®ç”¨çš„è§£å†³æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIä»£ç†åœ¨è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>é«˜è´¨é‡è½¨è¿¹æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯GUIä»£ç†æ€§èƒ½æå‡çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µå¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>ä»»åŠ¡æ³›åŒ–éå¸¸æœ‰æ•ˆï¼Œåœ¨å¤§å¤šæ•°åœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä»»åŠ¡èƒ½æ˜¾è‘—æé«˜GUIä»£ç†çš„æ€§èƒ½ã€‚</li>
<li>æ–‡æœ¬å‹æ•°å­¦æ•°æ®åœ¨GUIä»£ç†çš„è¡¨ç°æå‡ä¸­å…·æœ‰è·¨æ¨¡æ€æ³›åŒ–çš„é‡è¦ä½œç”¨ã€‚ç›¸è¾ƒäºç›´è§‚çš„GUIæ„ŸçŸ¥æ•°æ®ï¼Œå…¶å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f084c3dd8cfd8ff80bc90f01d0c1aaf6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fd643e844c30b1ea9001fa8abf454a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9538ec9b281525055fe2dba4ad52577b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b06c3f274d496bdbce3525686789fc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a7e1ab5316bb2fa4676e2357902383a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Multi-Object-Grounding-via-Hierarchical-Contrastive-Siamese-Transformers"><a href="#Multi-Object-Grounding-via-Hierarchical-Contrastive-Siamese-Transformers" class="headerlink" title="Multi-Object Grounding via Hierarchical Contrastive Siamese Transformers"></a>Multi-Object Grounding via Hierarchical Contrastive Siamese Transformers</h2><p><strong>Authors:Chengyi Du, Keyan Jin</strong></p>
<p>Multi-object grounding in 3D scenes involves localizing multiple objects based on natural language input. While previous work has primarily focused on single-object grounding, real-world scenarios often demand the localization of several objects. To tackle this challenge, we propose Hierarchical Contrastive Siamese Transformers (H-COST), which employs a Hierarchical Processing strategy to progressively refine object localization, enhancing the understanding of complex language instructions. Additionally, we introduce a Contrastive Siamese Transformer framework, where two networks with the identical structure are used: one auxiliary network processes robust object relations from ground-truth labels to guide and enhance the second network, the reference network, which operates on segmented point-cloud data. This contrastive mechanism strengthens the modelâ€™ s semantic understanding and significantly enhances its ability to process complex point-cloud data. Our approach outperforms previous state-of-the-art methods by 9.5% on challenging multi-object grounding benchmarks. </p>
<blockquote>
<p>åœ¨3Dåœºæ™¯ä¸­çš„å¤šç›®æ ‡å®šä½æ˜¯æ ¹æ®è‡ªç„¶è¯­è¨€è¾“å…¥å®šä½å¤šä¸ªç›®æ ‡ã€‚è™½ç„¶ä¹‹å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å•ç›®æ ‡å®šä½ä¸Šï¼Œä½†ç°å®ä¸–ç•Œåœºæ™¯é€šå¸¸éœ€è¦å®šä½å¤šä¸ªç›®æ ‡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚å¯¹æ¯”å­ªç”Ÿå˜å‹å™¨ï¼ˆH-COSTï¼‰ï¼Œå®ƒé‡‡ç”¨åˆ†å±‚å¤„ç†ç­–ç•¥æ¥é€æ­¥ä¼˜åŒ–ç›®æ ‡å®šä½ï¼Œæé«˜å¤æ‚è¯­è¨€æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹æ¯”å­ªç”Ÿå˜å‹å™¨æ¡†æ¶ï¼Œä½¿ç”¨ä¸¤ä¸ªç»“æ„ç›¸åŒçš„ç½‘ç»œï¼šä¸€ä¸ªè¾…åŠ©ç½‘ç»œå¤„ç†æ¥è‡ªçœŸå®æ ‡ç­¾çš„ç¨³å¥ç›®æ ‡å…³ç³»ï¼Œä»¥æŒ‡å¯¼å’Œå¢å¼ºç¬¬äºŒä¸ªç½‘ç»œï¼Œå³å‚è€ƒç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨åˆ†å‰²çš„ç‚¹äº‘æ•°æ®ä¸Šè¿è¡Œã€‚è¿™ç§å¯¹æ¯”æœºåˆ¶åŠ å¼ºäº†æ¨¡å‹çš„è¯­ä¹‰ç†è§£ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å…¶å¤„ç†å¤æ‚ç‚¹äº‘æ•°æ®çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šç›®æ ‡å®šä½åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œæé«˜äº†9.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10048v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºè‡ªç„¶è¯­è¨€è¾“å…¥å®ç°ä¸‰ç»´åœºæ™¯ä¸­çš„å¤šç›®æ ‡å®šä½æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§åˆ†å±‚å¯¹æ¯”å­ªç”ŸTransformerï¼ˆH-COSTï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åˆ†å±‚å¤„ç†ç­–ç•¥ï¼Œé€æ­¥æ”¹è¿›ç›®æ ‡å®šä½ï¼Œæé«˜å¤æ‚è¯­è¨€æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”å­ªç”ŸTransformeræ¡†æ¶ï¼Œå¢å¼ºæ¨¡å‹è¯­ä¹‰ç†è§£ï¼Œæé«˜å¤„ç†å¤æ‚ç‚¹äº‘æ•°æ®çš„èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç›®æ ‡å®šä½åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>æ¨¡å‹é€šè¿‡åˆ†å±‚å¤„ç†ç­–ç•¥æ”¹è¿›ç›®æ ‡å®šä½ï¼Œé€‚åº”ä¸‰ç»´åœºæ™¯çš„å¤šç›®æ ‡å®šä½ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æ¯”å­ªç”ŸTransformeræ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªç»“æ„ç›¸åŒçš„ç½‘ç»œè¿›è¡Œäº¤äº’å­¦ä¹ ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è¾…åŠ©ç½‘ç»œå’Œå‚è€ƒç½‘ç»œçš„æ¦‚å¿µï¼Œè¾…åŠ©ç½‘ç»œåˆ©ç”¨çœŸå®æ ‡ç­¾ä¸­çš„ç¨³å¥å¯¹è±¡å…³ç³»æ¥æŒ‡å¯¼å‚è€ƒç½‘ç»œï¼Œæé«˜è¯­ä¹‰ç†è§£å’Œç‚¹äº‘æ•°æ®å¤„ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨å¤æ‚ç‚¹äº‘æ•°æ®å¤„ç†æ–¹é¢è¡¨ç°å‡ºè¾ƒå¼ºçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è¯­è¨€æŒ‡ä»¤ã€‚</li>
<li>åœ¨å¤šç›®æ ‡å®šä½åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹æ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œæå‡äº†çº¦9.5%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-70f764c6752ce92e8c192b9ad43fa254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3084f5eed556ad3281f022044ed9cd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8769a844c352a4c5d00485876c38098.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-814807afe18bb0ee27ec05d4bd01b969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fa4ae3611963f8709d3e280b171d73.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Transferable-text-data-distillation-by-trajectory-matching"><a href="#Transferable-text-data-distillation-by-trajectory-matching" class="headerlink" title="Transferable text data distillation by trajectory matching"></a>Transferable text data distillation by trajectory matching</h2><p><strong>Authors:Rong Yao, Hailin Hu, Yifei Fu, Hanting Chen, Wenyi Fang, Fanyi Du, Kai Han, Yunhe Wang</strong></p>
<p>In the realm of large language model (LLM), as the size of large models increases, it also brings higher training costs. There is a urgent need to minimize the data size in LLM training. Compared with data selection method, the data distillation method aims to synthesize a small number of data samples to achieve the training effect of the full data set and has better flexibility. Despite its successes in computer vision, the discreteness of text data has hitherto stymied its exploration in natural language processing (NLP). In this work, we proposed a method that involves learning pseudo prompt data based on trajectory matching and finding its nearest neighbor ID to achieve cross-architecture transfer. During the distillation process, we introduce a regularization loss to improve the robustness of our distilled data. To our best knowledge, this is the first data distillation work suitable for text generation tasks such as instruction tuning. Evaluations on two benchmarks, including ARC-Easy and MMLU instruction tuning datasets, established the superiority of our distillation approach over the SOTA data selection method LESS. Furthermore, our method demonstrates a good transferability over LLM structures (i.e., OPT to Llama). </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸï¼Œéšç€å¤§å‹æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå…¶è®­ç»ƒæˆæœ¬ä¹Ÿéšä¹‹æé«˜ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡è¿›è¡Œç¼©å‡ã€‚ä¸æ•°æ®é€‰æ‹©æ–¹æ³•ç›¸æ¯”ï¼Œæ•°æ®è’¸é¦æ³•çš„ç›®æ ‡æ˜¯é€šè¿‡åˆæˆå°‘é‡æ•°æ®æ ·æœ¬å®ç°å…¨æ•°æ®é›†çš„åŸ¹è®­æ•ˆæœï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„çµæ´»æ€§ã€‚å°½ç®¡å…¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†æ–‡æœ¬æ•°æ®çš„ç¦»æ•£æ€§è¿„ä»Šä¸ºæ­¢é˜»ç¢äº†å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„åº”ç”¨æ¢ç´¢ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè½¨è¿¹åŒ¹é…å­¦ä¹ ä¼ªæç¤ºæ•°æ®çš„æ–¹æ³•ï¼Œé€šè¿‡å¯»æ‰¾å…¶æœ€è¿‘é‚»å±…IDæ¥å®ç°è·¨æ¶æ„è¿ç§»ã€‚åœ¨è’¸é¦è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥æ­£åˆ™åŒ–æŸå¤±ä»¥æé«˜æˆ‘ä»¬è’¸é¦æ•°æ®çš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡é€‚ç”¨äºæŒ‡ä»¤å¾®è°ƒç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„æ•°æ®è’¸é¦å·¥ä½œã€‚åœ¨ARC-Easyå’ŒMMLUæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¯æ˜æˆ‘ä»¬çš„è’¸é¦æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç›®å‰æœ€å…ˆè¿›çš„LESSæ•°æ®é€‰æ‹©æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç»“æ„ä¹‹é—´å…·æœ‰è‰¯å¥½çš„å¯è¿ç§»æ€§ï¼ˆä¾‹å¦‚ä»OPTè¿ç§»åˆ°Llamaï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09818v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œè®­ç»ƒæˆæœ¬ä¹Ÿåœ¨ä¸Šå‡ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦å‡å°LLMè®­ç»ƒä¸­çš„æ•°æ®é‡ã€‚ç›¸æ¯”äºæ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œæ•°æ®è’¸é¦æ–¹æ³•å¯ä»¥åˆæˆå°‘é‡çš„æ•°æ®æ ·æœ¬ä»¥è¾¾åˆ°å…¨æ•°æ®é›†çš„åŸ¹è®­æ•ˆæœï¼Œå¹¶å…·å¤‡æ›´å¥½çš„çµæ´»æ€§ã€‚å°½ç®¡å®ƒåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œæ–‡æœ¬æ•°æ®çš„ç¦»æ•£æ€§é˜»ç¢äº†å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè½¨è¿¹åŒ¹é…å­¦ä¹ ä¼ªæç¤ºæ•°æ®çš„æ–¹æ³•ï¼Œå¹¶æ‰¾åˆ°å…¶æœ€è¿‘é‚»IDä»¥å®ç°è·¨æ¶æ„è¿ç§»ã€‚åœ¨è’¸é¦è¿‡ç¨‹ä¸­ï¼Œå¼•å…¥æ­£åˆ™åŒ–æŸå¤±ä»¥æé«˜è’¸é¦æ•°æ®çš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡é€‚ç”¨äºæŒ‡ä»¤è°ƒæ•´ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„æ•°æ®è’¸é¦å·¥ä½œã€‚åœ¨ARC-Easyå’ŒMMLUæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„è’¸é¦æ–¹æ³•ä¼˜äºç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•LESSã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§æ¨¡å‹ç»“æ„ä¹‹é—´è¡¨ç°å‡ºè‰¯å¥½çš„è¿ç§»æ€§ï¼ˆä¾‹å¦‚ï¼Œä»OPTåˆ°Llamaï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€å¤§å‹è¯­è¨€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œè®­ç»ƒæˆæœ¬ä¸æ–­ä¸Šå‡ï¼Œå› æ­¤éœ€è¦å‡å°‘LLMè®­ç»ƒä¸­çš„æ•°æ®é‡ã€‚</li>
<li>æ•°æ®è’¸é¦æ–¹æ³•ç›¸è¾ƒäºæ•°æ®é€‰æ‹©æ–¹æ³•å…·æœ‰æ›´å¥½çš„çµæ´»æ€§å’Œæ•ˆæœã€‚</li>
<li>æ–‡æœ¬æ•°æ®çš„ç¦»æ•£æ€§ç»™æ•°æ®è’¸é¦åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡åŸºäºè½¨è¿¹åŒ¹é…å­¦ä¹ ä¼ªæç¤ºæ•°æ®çš„æ–¹æ³•æ¥è¿›è¡Œæ•°æ®è’¸é¦ã€‚</li>
<li>åœ¨è’¸é¦è¿‡ç¨‹ä¸­å¼•å…¥æ­£åˆ™åŒ–æŸå¤±ä»¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨æŒ‡ä»¤è°ƒæ•´ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæœ¬ç ”ç©¶çš„æ•°æ®è’¸é¦æ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dae4ffd6d9d60afc439edaa4bb4f06e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8879abcb745045db55c68bbaad8aa228.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c057470a3fd30e8190c7aad13ce5e1e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-badfad053ed5e1064fc8398643380be8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Representation-Learning-for-Robust-Gene-Expression-Modeling-and-Cancer-Prognosis"><a href="#Transformer-Based-Representation-Learning-for-Robust-Gene-Expression-Modeling-and-Cancer-Prognosis" class="headerlink" title="Transformer-Based Representation Learning for Robust Gene Expression   Modeling and Cancer Prognosis"></a>Transformer-Based Representation Learning for Robust Gene Expression   Modeling and Cancer Prognosis</h2><p><strong>Authors:Shuai Jiang, Saeed Hassanpour</strong></p>
<p>Transformer-based models have achieved remarkable success in natural language and vision tasks, but their application to gene expression analysis remains limited due to data sparsity, high dimensionality, and missing values. We present GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. GexBERT learns context-aware gene embeddings by pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes. We evaluate GexBERT across three critical tasks in cancer research: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. GexBERT achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. Furthermore, its attention-based interpretability reveals biologically meaningful gene patterns across cancer types. These findings demonstrate the utility of GexBERT as a scalable and effective tool for gene expression modeling, with translational potential in settings where gene coverage is limited or incomplete. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ç”±äºæ•°æ®ç¨€ç–ã€é«˜ç»´åº¦å’Œç¼ºå¤±å€¼ï¼Œå®ƒä»¬åœ¨åŸºå› è¡¨è¾¾åˆ†æä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†GexBERTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç”¨äºåŸºå› è¡¨è¾¾æ•°æ®çš„ç¨³å¥è¡¨ç¤ºå­¦ä¹ ã€‚GexBERTé€šè¿‡åœ¨å¤§è§„æ¨¡è½¬å½•ç»„å›¾è°±ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ¥å­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŸºå› åµŒå…¥ï¼Œé€šè¿‡é®è”½å’Œæ¢å¤ç›®æ ‡æ¥æ•è·æ•°åƒä¸ªåŸºå› ä¹‹é—´çš„å…±è¡¨è¾¾å…³ç³»ã€‚æˆ‘ä»¬åœ¨ç™Œç—‡ç ”ç©¶çš„ä¸‰ä¸ªå…³é”®ä»»åŠ¡ä¸­è¯„ä¼°äº†GexBERTï¼šæ³›ç™Œåˆ†ç±»ã€ç™Œç—‡ç‰¹å¼‚æ€§ç”Ÿå­˜é¢„æµ‹å’Œç¼ºå¤±å€¼ä¼°ç®—ã€‚GexBERTåœ¨æœ‰é™çš„åŸºå› å­é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç²¾åº¦ï¼Œé€šè¿‡æ¢å¤é¢„åé”šåŸºå› çš„è¡¨è¾¾æé«˜äº†ç”Ÿå­˜é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶åœ¨é«˜ç¼ºå¤±ç‡çš„æƒ…å†µä¸‹ä¼˜äºä¼ ç»Ÿçš„ä¼°ç®—æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶åŸºäºæ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§æ­ç¤ºäº†ä¸åŒç™Œç—‡ç±»å‹ä¸­ç”Ÿç‰©æ„ä¹‰æ˜ç¡®çš„åŸºå› æ¨¡å¼ã€‚è¿™äº›å‘ç°è¯æ˜äº†GexBERTä½œä¸ºåŸºå› è¡¨è¾¾å»ºæ¨¡çš„å¯æ‰©å±•å’Œæœ‰æ•ˆå·¥å…·ï¼Œåœ¨åŸºå› è¦†ç›–æœ‰é™æˆ–ä¸å®Œæ•´çš„æƒ…å†µä¸‹å…·æœ‰ç¿»è¯‘æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09704v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åŸºå› è¡¨è¾¾åˆ†æä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†GexBERTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç”¨äºç¨³å¥åœ°è¡¨ç¤ºåŸºå› è¡¨è¾¾æ•°æ®ã€‚GexBERTé€šè¿‡åœ¨å¤§è§„æ¨¡è½¬å½•ç»„å›¾è°±ä¸Šè¿›è¡Œé¢„è®­ç»ƒå­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŸºå› åµŒå…¥ï¼Œé€šè¿‡æ©ç›–å’Œæ¢å¤ç›®æ ‡æ¥æ•è·æ•°åƒä¸ªåŸºå› ä¹‹é—´çš„å…±è¡¨è¾¾å…³ç³»ã€‚åœ¨ç™Œç—‡ç ”ç©¶çš„ä¸‰ä¸ªå…³é”®ä»»åŠ¡ä¸­ï¼ŒGexBERTå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç²¾åº¦ï¼Œæé«˜äº†ç”Ÿå­˜é¢„æµ‹çš„é¢„åé”šåŸºå› è¡¨è¾¾æ¢å¤ï¼Œå¹¶åœ¨é«˜ç¼ºå¤±ç‡çš„æƒ…å†µä¸‹ä¼˜äºä¼ ç»Ÿæ’è¡¥æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶åŸºäºæ³¨æ„åŠ›çš„è§£é‡Šæ€§æ­ç¤ºäº†ä¸åŒç™Œç—‡ç±»å‹ä¸­ç”Ÿç‰©æ„ä¹‰æ˜ç¡®çš„åŸºå› æ¨¡å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGexBERTæ˜¯ä¸€ä¸ªå¯æ‰©å±•å’Œæœ‰æ•ˆçš„åŸºå› è¡¨è¾¾å»ºæ¨¡å·¥å…·ï¼Œåœ¨åŸºå› è¦†ç›–æœ‰é™æˆ–ä¸å®Œæ•´çš„æƒ…å†µä¸‹å…·æœ‰ç¿»è¯‘æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GexBERTæ˜¯åŸºäºTransformerçš„è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç”¨äºåŸºå› è¡¨è¾¾æ•°æ®çš„ç¨³å¥è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>GexBERTé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŸºå› åµŒå…¥ï¼Œæ•æ‰åŸºå› é—´çš„å…±è¡¨è¾¾å…³ç³»ã€‚</li>
<li>GexBERTåœ¨ç™Œç—‡ç ”ç©¶çš„ä¸‰ä¸ªå…³é”®ä»»åŠ¡ä¸­å–å¾—å…ˆè¿›åˆ†ç±»ç²¾åº¦ï¼ŒåŒ…æ‹¬æ³›ç™Œåˆ†ç±»ã€ç‰¹å®šç™Œç—‡ç”Ÿå­˜é¢„æµ‹å’Œç¼ºå¤±å€¼æ’è¡¥ã€‚</li>
<li>GexBERTåœ¨æœ‰é™åŸºå› å­é›†ä¸Šå®ç°é«˜åˆ†ç±»ç²¾åº¦ï¼Œé€šè¿‡æ¢å¤é¢„åé”šåŸºå› çš„è¡¨è¾¾æé«˜ç”Ÿå­˜é¢„æµ‹ã€‚</li>
<li>GexBERTåœ¨é«˜ç¼ºå¤±ç‡æƒ…å†µä¸‹è¡¨ç°ä¼˜äºä¼ ç»Ÿæ’è¡¥æ–¹æ³•ã€‚</li>
<li>GexBERTå…·æœ‰åŸºäºæ³¨æ„åŠ›çš„è§£é‡Šæ€§ï¼Œèƒ½æ­ç¤ºç”Ÿç‰©æ„ä¹‰æ˜ç¡®çš„åŸºå› æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af26f50ffc07f68ea76c8327bdc983b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9b8fa3cb2eff6c98f0f955a6045a74f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-69ef5a21e4c999ebe06305e103031cd5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  GUI-R1  A Generalist R1-Style Vision-Language Action Model For GUI   Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0a0db482bb6dba698457faf7640a1f79.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  xVerify Efficient Answer Verifier for Reasoning Model Evaluations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
