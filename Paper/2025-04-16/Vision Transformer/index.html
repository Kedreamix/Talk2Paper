<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Masked Autoencoder Self Pre-Training for Defect Detection in   Microelectronics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-49d3d662712fa685952601d7cb182438.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    28 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-16-æ›´æ–°"><a href="#2025-04-16-æ›´æ–°" class="headerlink" title="2025-04-16 æ›´æ–°"></a>2025-04-16 æ›´æ–°</h1><h2 id="Masked-Autoencoder-Self-Pre-Training-for-Defect-Detection-in-Microelectronics"><a href="#Masked-Autoencoder-Self-Pre-Training-for-Defect-Detection-in-Microelectronics" class="headerlink" title="Masked Autoencoder Self Pre-Training for Defect Detection in   Microelectronics"></a>Masked Autoencoder Self Pre-Training for Defect Detection in   Microelectronics</h2><p><strong>Authors:Nikolai RÃ¶hrich, Alwin Hoffmann, Richard Nordsieck, Emilio Zarbali, Alireza Javanmardi</strong></p>
<p>Whereas in general computer vision, transformer-based architectures have quickly become the gold standard, microelectronics defect detection still heavily relies on convolutional neural networks (CNNs). We hypothesize that this is due to the fact that a) transformers have an increased need for data and b) labelled image generation procedures for microelectronics are costly, and labelled data is therefore sparse. Whereas in other domains, pre-training on large natural image datasets can mitigate this problem, in microelectronics transfer learning is hindered due to the dissimilarity of domain data and natural images. Therefore, we evaluate self pre-training, where models are pre-trained on the target dataset, rather than another dataset. We propose a vision transformer (ViT) pre-training framework for defect detection in microelectronics based on masked autoencoders (MAE). In MAE, a large share of image patches is masked and reconstructed by the model during pre-training. We perform pre-training and defect detection using a dataset of less than 10.000 scanning acoustic microscopy (SAM) images labelled using transient thermal analysis (TTA). Our experimental results show that our approach leads to substantial performance gains compared to a) supervised ViT, b) ViT pre-trained on natural image datasets, and c) state-of-the-art CNN-based defect detection models used in the literature. Additionally, interpretability analysis reveals that our self pre-trained models, in comparison to ViT baselines, correctly focus on defect-relevant features such as cracks in the solder material. This demonstrates that our approach yields fault-specific feature representations, making our self pre-trained models viable for real-world defect detection in microelectronics. </p>
<blockquote>
<p>åœ¨é€šç”¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼ŒåŸºäºå˜å‹å™¨çš„æ¶æ„å·²ç»è¿…é€Ÿæˆä¸ºé‡‘æ ‡å‡†ï¼Œä½†å¾®ç”µå­ç¼ºé™·æ£€æµ‹ä»ç„¶ä¸¥é‡ä¾èµ–äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚æˆ‘ä»¬å‡è®¾è¿™æ˜¯å› ä¸ºa) å˜å‹å™¨å¯¹æ•°æ®çš„éœ€æ±‚å¢åŠ ï¼Œä»¥åŠb) å¾®ç”µå­çš„æ ‡ç­¾å›¾åƒç”Ÿæˆç¨‹åºæˆæœ¬é«˜æ˜‚ï¼Œå› æ­¤æ ‡ç­¾æ•°æ®å¾ˆç¨€ç–ã€‚åœ¨å…¶ä»–é¢†åŸŸï¼Œé€šè¿‡åœ¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨å¾®ç”µå­é¢†åŸŸï¼Œç”±äºé¢†åŸŸæ•°æ®ä¸è‡ªç„¶å›¾åƒçš„å·®å¼‚æ€§ï¼Œè¿ç§»å­¦ä¹ å—åˆ°é˜»ç¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è‡ªæˆ‘é¢„è®­ç»ƒï¼Œå³æ¨¡å‹åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œä¸æ˜¯åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè¿›è¡Œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„ç”¨äºå¾®ç”µå­ç¼ºé™·æ£€æµ‹çš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰é¢„è®­ç»ƒæ¡†æ¶ã€‚åœ¨MAEä¸­ï¼Œå¤§éƒ¨åˆ†å›¾åƒè¡¥ä¸åœ¨é¢„è®­ç»ƒé˜¶æ®µè¢«æ©ç›–ï¼Œå¹¶ç”±æ¨¡å‹é‡å»ºã€‚æˆ‘ä»¬ä½¿ç”¨å°äº10,000å¼ é€šè¿‡ç¬æ€çƒ­åˆ†æï¼ˆTTAï¼‰æ ‡è®°çš„æ‰«æå£°å­¦æ˜¾å¾®é•œï¼ˆSAMï¼‰å›¾åƒè¿›è¡Œé¢„è®­ç»ƒå’Œç¼ºé™·æ£€æµ‹ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸a) ç›‘ç£ViTã€b) åœ¨è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ViTä»¥åŠc) æ–‡çŒ®ä¸­ä½¿ç”¨çš„æœ€å…ˆè¿›çš„CNNç¼ºé™·æ£€æµ‹æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¸¦æ¥äº†å·¨å¤§çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè§£é‡Šæ€§åˆ†æè¡¨æ˜ï¼Œä¸åŸºçº¿ViTç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è‡ªè®­ç»ƒæ¨¡å‹æ­£ç¡®åœ°å…³æ³¨ç¼ºé™·ç›¸å…³ç‰¹å¾ï¼Œå¦‚ç„Šæ–™ä¸­çš„è£‚ç¼ã€‚è¿™è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†ç‰¹å®šçš„æ•…éšœç‰¹å¾è¡¨ç¤ºï¼Œä½¿å¾—æˆ‘ä»¬çš„è‡ªè®­ç»ƒæ¨¡å‹åœ¨å¾®ç”µå­é¢†åŸŸçš„çœŸå®ä¸–ç•Œç¼ºé™·æ£€æµ‹ä¸­åˆ‡å®å¯è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10021v1">PDF</a> 16 pages, 5 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†åœ¨å¾®ç”µå­ç¼ºé™·æ£€æµ‹ä¸­åº”ç”¨Vision Transformerï¼ˆViTï¼‰çš„æƒ…å†µã€‚ç”±äºå¾®ç”µå­è®¾å¤‡ç¼ºé™·æ£€æµ‹çš„æ•°æ®ç¨€ç¼ºä¸”æ ‡æ³¨æˆæœ¬é«˜ï¼Œç›´æ¥åº”ç”¨é€šç”¨çš„Vision Transformeræ¨¡å‹å—åˆ°é™åˆ¶ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºMasked Autoencoderï¼ˆMAEï¼‰çš„ViTè‡ªè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºåœ¨å°‘é‡æ ‡æ³¨çš„æ‰«æå£°å­¦æ˜¾å¾®é•œï¼ˆSAMï¼‰å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒå’Œç¼ºé™·æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æ€§èƒ½æœ‰æ˜æ˜¾æå‡ï¼Œå¹¶ä¸”æ¨¡å‹èƒ½å¤Ÿå…³æ³¨åˆ°ç¼ºé™·ç›¸å…³çš„ç‰¹å¾ï¼Œå¦‚ç„Šæ–™ä¸­çš„è£‚ç¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¾®ç”µå­ç¼ºé™·æ£€æµ‹é¢†åŸŸï¼Œç”±äºæ•°æ®ç¨€ç¼ºå’Œæ ‡æ³¨æˆæœ¬é«˜ï¼Œè™½ç„¶é€šç”¨è®¡ç®—æœºè§†è§‰é¢†åŸŸTransformeræ¶æ„æˆä¸ºé‡‘æ ‡å‡†ï¼Œä½†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»å æ®ä¸»å¯¼åœ°ä½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºMasked Autoencoderï¼ˆMAEï¼‰çš„Vision Transformerï¼ˆViTï¼‰è‡ªè®­ç»ƒæ¡†æ¶ç”¨äºå¾®ç”µå­ç¼ºé™·æ£€æµ‹ã€‚</li>
<li>è‡ªè®­ç»ƒç­–ç•¥åœ¨å°‘é‡æ ‡æ³¨æ‰«æå£°å­¦æ˜¾å¾®é•œï¼ˆSAMï¼‰å›¾åƒä¸Šçš„è¡¨ç°ä¼˜äºç›‘ç£å­¦ä¹ çš„ViTã€åœ¨è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ViTä»¥åŠç°æœ‰çš„CNNç¼ºé™·æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè‡ªè®­ç»ƒæ¨¡å‹èƒ½æ­£ç¡®å…³æ³¨ç¼ºé™·ç›¸å…³ç‰¹å¾ï¼Œå¦‚ç„Šæ–™ä¸­çš„è£‚ç¼ã€‚</li>
<li>è‡ªè®­ç»ƒæ¨¡å‹å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Œå¯ç”¨äºå¾®ç”µå­é¢†åŸŸçš„çœŸå®ç¼ºé™·æ£€æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æ£€æµ‹æ€§èƒ½ï¼Œè€Œä¸”æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºç†è§£æ¨¡å‹å¦‚ä½•è¯†åˆ«å’Œå¤„ç†ç¼ºé™·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4369382c76020f99412f37d55d63d096.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e244df3a60c06ce0b98806b81246c046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc28679d237dab648a7f44fa3a057ff4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-913a6163bda37a93f72e01ddd584aa64.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Correlative-and-Discriminative-Label-Grouping-for-Multi-Label-Visual-Prompt-Tuning"><a href="#Correlative-and-Discriminative-Label-Grouping-for-Multi-Label-Visual-Prompt-Tuning" class="headerlink" title="Correlative and Discriminative Label Grouping for Multi-Label Visual   Prompt Tuning"></a>Correlative and Discriminative Label Grouping for Multi-Label Visual   Prompt Tuning</h2><p><strong>Authors:LeiLei Ma, Shuo Xu, MingKun Xie, Lei Wang, Dengdi Sun, Haifeng Zhao</strong></p>
<p>Modeling label correlations has always played a pivotal role in multi-label image classification (MLC), attracting significant attention from researchers. However, recent studies have overemphasized co-occurrence relationships among labels, which can lead to overfitting risk on this overemphasis, resulting in suboptimal models. To tackle this problem, we advocate for balancing correlative and discriminative relationships among labels to mitigate the risk of overfitting and enhance model performance. To this end, we propose the Multi-Label Visual Prompt Tuning framework, a novel and parameter-efficient method that groups classes into multiple class subsets according to label co-occurrence and mutual exclusivity relationships, and then models them respectively to balance the two relationships. In this work, since each group contains multiple classes, multiple prompt tokens are adopted within Vision Transformer (ViT) to capture the correlation or discriminative label relationship within each group, and effectively learn correlation or discriminative representations for class subsets. On the other hand, each group contains multiple group-aware visual representations that may correspond to multiple classes, and the mixture of experts (MoE) model can cleverly assign them from the group-aware to the label-aware, adaptively obtaining label-aware representation, which is more conducive to classification. Experiments on multiple benchmark datasets show that our proposed approach achieves competitive results and outperforms SOTA methods on multiple pre-trained models. </p>
<blockquote>
<p>åœ¨å¤šå…ƒæ ‡ç­¾å›¾åƒåˆ†ç±»ï¼ˆMLCï¼‰ä¸­ï¼Œå»ºæ¨¡æ ‡ç­¾ç›¸å…³æ€§å§‹ç»ˆå‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¸å¼•äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¿‡åˆ†å¼ºè°ƒäº†æ ‡ç­¾ä¹‹é—´çš„å…±ç°å…³ç³»ï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹è¿™ç§è¿‡åˆ†å¼ºè°ƒçš„è¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œäº§ç”Ÿæ¬¡ä¼˜æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸»å¼ å¹³è¡¡æ ‡ç­¾ä¹‹é—´çš„ç›¸å…³æ€§å’Œåˆ¤åˆ«æ€§å…³ç³»ï¼Œä»¥é™ä½è¿‡åº¦æ‹Ÿåˆçš„é£é™©å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ ‡ç­¾è§†è§‰æç¤ºè°ƒæ•´æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ä¸”å‚æ•°é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ ¹æ®æ ‡ç­¾çš„å…±ç°å’Œç›¸äº’æ’æ–¥å…³ç³»å°†ç±»åˆ«åˆ†ä¸ºå¤šä¸ªç±»åˆ«å­é›†ï¼Œç„¶ååˆ†åˆ«è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å¹³è¡¡è¿™ä¸¤ç§å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09990v1">PDF</a> IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR)   2025</p>
<p><strong>Summary</strong><br>å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä¸­æ ‡ç­¾å…³è”å»ºæ¨¡è‡³å…³é‡è¦ï¼Œä½†è¿‡åº¦å¼ºè°ƒæ ‡ç­¾é—´çš„å…±ç°å…³ç³»å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¹³è¡¡æ ‡ç­¾é—´çš„å…³è”æ€§å’Œåˆ¤åˆ«æ€§å…³ç³»ï¼Œæå‡ºå¤šæ ‡ç­¾è§†è§‰æç¤ºè°ƒæ•´æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ¹æ®æ ‡ç­¾å…±ç°å’Œäº’æ–¥å…³ç³»å°†ç±»åˆ«åˆ†ç»„ï¼Œå¹¶é‡‡ç”¨Vision Transformerä¸­çš„å¤šä¸ªæç¤ºæ ‡è®°æ¥æ•æ‰æ¯ä¸ªåˆ†ç»„å†…çš„æ ‡ç­¾å…³ç³»ï¼Œå­¦ä¹ ç±»åˆ«å­é›†çš„ç›¸å…³æ€§æˆ–åˆ¤åˆ«æ€§è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¨¡å‹å°†ç»„æ„ŸçŸ¥è§†è§‰è¡¨ç¤ºé€‚åº”æ€§åœ°è½¬åŒ–ä¸ºæ ‡ç­¾æ„ŸçŸ¥è¡¨ç¤ºï¼Œæœ‰åˆ©äºåˆ†ç±»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä¸­æ ‡ç­¾å…³è”å»ºæ¨¡æ˜¯å…³é”®ï¼Œä½†éœ€å¹³è¡¡å…³è”æ€§å’Œåˆ¤åˆ«æ€§å…³ç³»ï¼Œä»¥é¿å…æ¨¡å‹è¿‡æ‹Ÿåˆã€‚</li>
<li>æå‡ºå¤šæ ‡ç­¾è§†è§‰æç¤ºè°ƒæ•´æ¡†æ¶ï¼Œæ ¹æ®æ ‡ç­¾å…±ç°å’Œäº’æ–¥å…³ç³»å¯¹ç±»åˆ«è¿›è¡Œåˆ†ç»„ã€‚</li>
<li>é‡‡ç”¨Vision Transformerä¸­çš„å¤šä¸ªæç¤ºæ ‡è®°æ¥æ•æ‰æ¯ä¸ªåˆ†ç»„å†…çš„æ ‡ç­¾ç›¸å…³æ€§ã€‚</li>
<li>å­¦ä¹ ç±»åˆ«å­é›†çš„ç›¸å…³æ€§å’Œåˆ¤åˆ«æ€§è¡¨ç¤ºï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨æ··åˆä¸“å®¶æ¨¡å‹å°†ç»„æ„ŸçŸ¥è§†è§‰è¡¨ç¤ºè½¬åŒ–ä¸ºæ ‡ç­¾æ„ŸçŸ¥è¡¨ç¤ºï¼Œå¢å¼ºåˆ†ç±»æ•ˆæœã€‚</li>
<li>æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57e9aacf10b5b0bd26a9b3b25b92431a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0693d73ba0a6be8dbb64e43610e6491c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f8436716690132a6852fbfb2486b20e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GFT-Gradient-Focal-Transformer"><a href="#GFT-Gradient-Focal-Transformer" class="headerlink" title="GFT: Gradient Focal Transformer"></a>GFT: Gradient Focal Transformer</h2><p><strong>Authors:Boris Kriuk, Simranjit Kaur Gill, Shoaib Aslam, Amir Fakhrutdinov</strong></p>
<p>Fine-Grained Image Classification (FGIC) remains a complex task in computer vision, as it requires models to distinguish between categories with subtle localized visual differences. Well-studied CNN-based models, while strong in local feature extraction, often fail to capture the global context required for fine-grained recognition, while more recent ViT-backboned models address FGIC with attention-driven mechanisms but lack the ability to adaptively focus on truly discriminative regions. TransFG and other ViT-based extensions introduced part-aware token selection to enhance attention localization, yet they still struggle with computational efficiency, attention region selection flexibility, and detail-focus narrative in complex environments. This paper introduces GFT (Gradient Focal Transformer), a new ViT-derived framework created for FGIC tasks. GFT integrates the Gradient Attention Learning Alignment (GALA) mechanism to dynamically prioritize class-discriminative features by analyzing attention gradient flow. Coupled with a Progressive Patch Selection (PPS) strategy, the model progressively filters out less informative regions, reducing computational overhead while enhancing sensitivity to fine details. GFT achieves SOTA accuracy on FGVC Aircraft, Food-101, and COCO datasets with 93M parameters, outperforming ViT-based advanced FGIC models in efficiency. By bridging global context and localized detail extraction, GFT sets a new benchmark in fine-grained recognition, offering interpretable solutions for real-world deployment scenarios. </p>
<blockquote>
<p>ç»†ç²’åº¦å›¾åƒåˆ†ç±»ï¼ˆFGICï¼‰ä»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å¤æ‚ä»»åŠ¡ï¼Œå› ä¸ºå®ƒè¦æ±‚æ¨¡å‹èƒ½å¤ŸåŒºåˆ†å…·æœ‰ç»†å¾®å±€éƒ¨è§†è§‰å·®å¼‚çš„åˆ†ç±»ã€‚åŸºäºCNNçš„æ¨¡å‹è™½ç„¶åœ¨å±€éƒ¨ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¾€å¾€æ— æ³•æ•è·ç»†ç²’åº¦è¯†åˆ«æ‰€éœ€çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚è€Œæœ€æ–°çš„åŸºäºViTçš„æ¨¡å‹é‡‡ç”¨æ³¨æ„åŠ›é©±åŠ¨æœºåˆ¶æ¥è§£å†³FGICé—®é¢˜ï¼Œä½†ç¼ºä¹è‡ªé€‚åº”å…³æ³¨çœŸæ­£åˆ¤åˆ«åŒºåŸŸçš„èƒ½åŠ›ã€‚TransFGå’Œå…¶ä»–åŸºäºViTçš„æ‰©å±•é€šè¿‡å¼•å…¥éƒ¨åˆ†æ„ŸçŸ¥ä»¤ç‰Œé€‰æ‹©æ¥å¢å¼ºæ³¨æ„åŠ›å®šä½ï¼Œä½†åœ¨è®¡ç®—æ•ˆç‡ã€æ³¨æ„åŠ›åŒºåŸŸé€‰æ‹©çš„çµæ´»æ€§å’Œå¤æ‚ç¯å¢ƒä¸­çš„ç»†èŠ‚å…³æ³¨å™è¿°æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†GFTï¼ˆæ¢¯åº¦èšç„¦è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºFGICä»»åŠ¡è€Œè®¾è®¡çš„æ–°ViTè¡ç”Ÿæ¡†æ¶ã€‚GFTé›†æˆäº†æ¢¯åº¦æ³¨æ„åŠ›å­¦ä¹ å¯¹é½ï¼ˆGALAï¼‰æœºåˆ¶ï¼Œé€šè¿‡åˆ†ææ³¨æ„åŠ›æ¢¯åº¦æµæ¥åŠ¨æ€ä¼˜å…ˆå¤„ç†ç±»åˆ¤åˆ«ç‰¹å¾ã€‚ç»“åˆæ¸è¿›å¼è¡¥ä¸é€‰æ‹©ï¼ˆPPSï¼‰ç­–ç•¥ï¼Œè¯¥æ¨¡å‹é€æ­¥è¿‡æ»¤æ‰ä¿¡æ¯è¾ƒå°‘çš„åŒºåŸŸï¼Œåœ¨æé«˜è®¡ç®—æ•ˆç‡çš„åŒæ—¶æé«˜å¯¹ç»†å¾®ç»†èŠ‚çš„æ•æ„Ÿæ€§ã€‚GFTåœ¨FGVCé£æœºã€é£Ÿå“101å’ŒCOCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå…·æœ‰93Må‚æ•°ï¼Œåœ¨æ•ˆç‡ä¸Šè¶…è¶Šäº†åŸºäºViTçš„é«˜çº§FGICæ¨¡å‹ã€‚é€šè¿‡æ¡¥æ¥å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚æå–ï¼ŒGFTåœ¨ç»†ç²’åº¦è¯†åˆ«æ–¹é¢æ ‘ç«‹äº†æ–°çš„åŸºå‡†ï¼Œä¸ºç°å®ä¸–ç•Œéƒ¨ç½²åœºæ™¯æä¾›äº†å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09852v1">PDF</a> 11 pages, 3 tables, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºVision Transformerï¼ˆViTï¼‰çš„ç»†ç²’åº¦å›¾åƒåˆ†ç±»ï¼ˆFGICï¼‰æ¨¡å‹â€”â€”GFTï¼ˆGradient Focal Transformerï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡é›†æˆæ¢¯åº¦æ³¨æ„åŠ›å­¦ä¹ å¯¹é½ï¼ˆGALAï¼‰æœºåˆ¶å’Œæ¸è¿›æ€§è¡¥ä¸é€‰æ‹©ï¼ˆPPSï¼‰ç­–ç•¥ï¼Œèƒ½å¤ŸåŠ¨æ€æ•æ‰ç±»åˆ¤åˆ«ç‰¹å¾å¹¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGFTåœ¨ç»†ç²’åº¦è¯†åˆ«ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œä¸ºçœŸå®ä¸–ç•Œåº”ç”¨åœºæ™¯æä¾›äº†å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GFTæ˜¯ä¸€ç§æ–°å‹çš„ViTè¡ç”Ÿæ¡†æ¶ï¼Œä¸“ä¸ºç»†ç²’åº¦å›¾åƒåˆ†ç±»ï¼ˆFGICï¼‰ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>GFTé€šè¿‡é›†æˆGALAæœºåˆ¶å’ŒPPSç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„æ³¨æ„åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>GFTèƒ½å¤ŸåŠ¨æ€æ•æ‰ç±»åˆ¤åˆ«ç‰¹å¾ï¼Œå¹¶ä¼˜åŒ–å¯¹ç»†å¾®å·®åˆ«çš„æ•æ„Ÿæ€§ã€‚</li>
<li>GFTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬FGVC Aircraftã€Food-101å’ŒCOCOæ•°æ®é›†ã€‚</li>
<li>GFTé€šè¿‡ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚æå–ï¼Œä¸ºç»†ç²’åº¦è¯†åˆ«è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</li>
<li>GFTæä¾›äº†ä¸€ç§å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œçš„éƒ¨ç½²åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-207781ad9e1e0e6a2a074970e9cfadcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9154aa97111ac92ef4b1ceeada5356c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-243e7879f8ad9d4da56960302c7f7c54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-794bca3df3bd8a3bebdec6c42a5c9140.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RAC3-Retrieval-Augmented-Corner-Case-Comprehension-for-Autonomous-Driving-with-Vision-Language-Models"><a href="#RAC3-Retrieval-Augmented-Corner-Case-Comprehension-for-Autonomous-Driving-with-Vision-Language-Models" class="headerlink" title="RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous   Driving with Vision-Language Models"></a>RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous   Driving with Vision-Language Models</h2><p><strong>Authors:Yujin Wang, Quanfeng Liu, Jiaqi Fan, Jinlong Hong, Hongqing Chu, Mengjian Tian, Bingzhao Gao, Hong Chen</strong></p>
<p>Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-language models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, RAC3, a novel framework designed to enhance the performance of VLMs in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial fusion (FSF) image encoder, cross-modal alignment fine-tuning with hard and semi-hard negative mining, and a fast querying pipeline based on KMeans clustering and hierarchical navigable small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting strategy to guide analogical reasoning and reduce hallucinations during inference is introduced. Moreover, an update mechanism is integrated into RAC3 to ensure continual learning within the framework. Extensive experiments on the CODA and NuScenes datasets demonstrate that RAC3 significantly improves corner case comprehension across multiple downstream tasks. Compared to prior state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the CODA-LM benchmark and shows consistent performance gains when integrated with end-to-end frameworks like DriveLM. These results demonstrate the effectiveness of retrieval-augmented strategies and cross-modal alignment for safer and more interpretable autonomous driving. </p>
<blockquote>
<p>ç†è§£å’Œå¤„ç†æç«¯æƒ…å†µå¯¹äºç¡®ä¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æé«˜åœºæ™¯ç†è§£æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†å®ƒä»¬é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚å¹»è±¡å’Œç°å®ä¸–ç•ŒåŸºç¡€ä¸è¶³ï¼Œè¿™æŸå®³äº†å®ƒä»¬åœ¨å…³é”®é©¾é©¶åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†RAC3æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜VLMåœ¨æç«¯æƒ…å†µç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚RAC3é›†æˆäº†é¢‘ç‡ç©ºé—´èåˆï¼ˆFSFï¼‰å›¾åƒç¼–ç å™¨ã€å¸¦æœ‰ç¡¬å’ŒåŠç¡¬è´ŸæŒ–æ˜çš„è·¨æ¨¡æ€å¯¹é½å¾®è°ƒã€åŸºäºKMeansèšç±»å’Œåˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œï¼ˆHNSWï¼‰ç´¢å¼•çš„å¿«é€ŸæŸ¥è¯¢ç®¡é“ã€‚è¿˜å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºç­–ç•¥ï¼Œä»¥å¼•å¯¼ç±»æ¯”æ¨ç†å¹¶å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è±¡ã€‚æ­¤å¤–ï¼ŒRAC3è¿˜é›†æˆäº†æ›´æ–°æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ¡†æ¶å†…çš„æŒç»­å­¦ä¹ ã€‚åœ¨CODAå’ŒNuScenesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRAC3åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æç«¯æƒ…å†µçš„ç†è§£èƒ½åŠ›ã€‚ä¸ä¹‹å‰çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒRAC3åœ¨CODA-LMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜åˆ†74.46åˆ†ï¼Œåœ¨ä¸ç«¯åˆ°ç«¯æ¡†æ¶ï¼ˆå¦‚DriveLMï¼‰é›†æˆæ—¶è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ£€ç´¢å¢å¼ºç­–ç•¥å’Œè·¨æ¨¡æ€å¯¹é½å¯¹äºæ›´å®‰å…¨ã€æ›´å¯è§£é‡Šçš„è‡ªåŠ¨é©¾é©¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11050v2">PDF</a> 14 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºRAC3çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å…³é”®åœºæ™¯ç†è§£æ€§èƒ½ã€‚RAC3é›†æˆäº†é¢‘ç‡ç©ºé—´èåˆå›¾åƒç¼–ç å™¨ã€è·¨æ¨¡æ€å¯¹é½å¾®è°ƒä¸ç¡¬å’ŒåŠç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ç­‰æŠ€æœ¯ï¼ŒåŒæ—¶å¼•å…¥äº†åŸºäºKMeansèšç±»å’Œå±‚æ¬¡å¯å¯¼èˆªå°ä¸–ç•Œç´¢å¼•çš„å¿«é€ŸæŸ¥è¯¢ç®¡é“ã€‚æ­¤å¤–ï¼ŒRAC3è¿˜é‡‡ç”¨äº†ä¸€ç§å¤šæ¨¡æ€æ€ç»´é“¾å¼•å¯¼ç­–ç•¥ï¼Œä»¥å¼•å¯¼ç±»æ¯”æ¨ç†å¹¶å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰ã€‚åœ¨CODAå’ŒNuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAC3åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜å…³é”®åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„ç›¸æ¯”ï¼ŒRAC3åœ¨CODA-LMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜åˆ†74.46åˆ†ï¼Œå¹¶ä¸”å½“ä¸ç«¯åˆ°ç«¯æ¡†æ¶ï¼ˆå¦‚DriveLMï¼‰é›†æˆæ—¶ï¼Œæ˜¾ç¤ºå‡ºæŒç»­çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ç†è§£å’Œå¤„ç†å…³é”®åœºæ™¯è‡³å…³é‡è¦ï¼Œæ¶‰åŠå®‰å…¨æ€§å’Œå¯é æ€§ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æé«˜åœºæ™¯ç†è§£æ–¹é¢æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†é¢ä¸´æŒ‘æˆ˜å¦‚å¹»è§‰å’Œç¼ºä¹çœŸå®ä¸–ç•ŒåŸºç¡€ã€‚</li>
<li>RAC3æ¡†æ¶æ—¨åœ¨æé«˜VLMsåœ¨å…³é”®åœºæ™¯ç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>RAC3é›†æˆäº†é¢‘ç‡ç©ºé—´èåˆå›¾åƒç¼–ç å™¨ä»¥æ”¹å–„è§†è§‰å¤„ç†ã€‚</li>
<li>è·¨æ¨¡æ€å¯¹é½å¾®è°ƒä¸ç¡¬å’ŒåŠç¡¬è´Ÿæ ·æœ¬æŒ–æ˜è¢«ç”¨äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¿«é€ŸæŸ¥è¯¢ç®¡é“åŸºäºKMeansèšç±»å’ŒHNSWç´¢å¼•ï¼Œæé«˜äº†æ¨¡å‹æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-32636114725a1cf8d791c12d9824b62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-848a2facc05388cefd8f02ab59372fc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abbbd8eeb88cd0e530a27da99f92614f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-961b1040787cbe2999f90fe434eb2bf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f137217f564f38815a33c791453e6066.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ITACLIP-Boosting-Training-Free-Semantic-Segmentation-with-Image-Text-and-Architectural-Enhancements"><a href="#ITACLIP-Boosting-Training-Free-Semantic-Segmentation-with-Image-Text-and-Architectural-Enhancements" class="headerlink" title="ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,   and Architectural Enhancements"></a>ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,   and Architectural Enhancements</h2><p><strong>Authors:M. Arda AydÄ±n, Efe Mert Ã‡Ä±rpar, Elvin Abdinli, Gozde Unal, Yusuf H. Sahin</strong></p>
<p>Recent advances in foundational Vision Language Models (VLMs) have reshaped the evaluation paradigm in computer vision tasks. These foundational models, especially CLIP, have accelerated research in open-vocabulary computer vision tasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the initial results are promising, the dense prediction capabilities of VLMs still require further improvement. In this study, we enhance the semantic segmentation performance of CLIP by introducing new modules and modifications: 1) architectural changes in the last layer of ViT and the incorporation of attention maps from the middle layers with the last layer, 2) Image Engineering: applying data augmentations to enrich input image representations, and 3) using Large Language Models (LLMs) to generate definitions and synonyms for each class name to leverage CLIPâ€™s open-vocabulary capabilities. Our training-free method, ITACLIP, outperforms current state-of-the-art approaches on segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and Pascal VOC. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/m-arda-aydn/ITACLIP">https://github.com/m-arda-aydn/ITACLIP</a>. </p>
<blockquote>
<p>æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›å±•å·²ç»æ”¹å˜äº†è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„è¯„ä¼°èŒƒå¼ã€‚å°¤å…¶æ˜¯CLIPç­‰åŸºç¡€æ€§æ¨¡å‹ï¼Œå·²ç»åŠ é€Ÿäº†å¼€æ”¾è¯æ±‡è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ã€‚å°½ç®¡åˆæ­¥ç»“æœä»¤äººé¼“èˆï¼Œä½†VLMsçš„å¯†é›†é¢„æµ‹èƒ½åŠ›ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥æ–°æ¨¡å—å’Œä¿®æ”¹æ¥æé«˜CLIPçš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼š1ï¼‰å¯¹ViTæœ€åä¸€å±‚è¿›è¡Œæ¶æ„æ›´æ”¹ï¼Œå¹¶å°†ä¸­é—´å±‚çš„æ³¨æ„åŠ›å›¾ä¸æœ€åä¸€å±‚ç›¸ç»“åˆï¼›2ï¼‰å›¾åƒå·¥ç¨‹ï¼šåº”ç”¨æ•°æ®å¢å¼ºæ¥ä¸°å¯Œè¾“å…¥å›¾åƒè¡¨ç¤ºï¼›3ï¼‰ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºæ¯ä¸€ç±»åç§°ç”Ÿæˆå®šä¹‰å’ŒåŒä¹‰è¯ï¼Œä»¥åˆ©ç”¨CLIPçš„å¼€æ”¾è¯æ±‡èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•ITACLIPåœ¨COCO-Stuffã€COCO-Objectã€Pascal Contextå’ŒPascal VOCç­‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/m-arda-aydn/ITACLIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/m-arda-aydn/ITACLIPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12044v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸåŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›å±•æ”¹å˜äº†è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„è¯„ä¼°æ¨¡å¼ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼•å…¥æ–°çš„æ¨¡å—å’Œæ”¹è¿›ï¼Œæå‡äº†CLIPåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä»–ä»¬è¿›è¡Œäº†æ¶æ„å˜æ›´ï¼Œç»“åˆäº†ä¸­é—´å±‚çš„æ³¨æ„åŠ›å›¾ä¸æœ€åä¸€å±‚çš„ä¿¡æ¯ï¼›é€šè¿‡å›¾åƒå·¥ç¨‹ä¸°å¯Œè¾“å…¥å›¾åƒè¡¨å¾ï¼›å¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºæ¯ç±»ç”Ÿæˆå®šä¹‰å’ŒåŒä¹‰è¯ï¼Œä»¥åˆ©ç”¨CLIPçš„å¼€æ”¾è¯æ±‡èƒ½åŠ›ã€‚å…¶æ— è®­ç»ƒçš„æ–¹æ³•ITACLIPåœ¨COCO-Stuffã€COCO-Objectã€Pascal Contextå’ŒPascal VOCç­‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•å·²ç»æ”¹å˜äº†è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„è¯„ä¼°æ¨¡å¼ã€‚</li>
<li>CLIPç­‰æ¨¡å‹åŠ é€Ÿäº†å¼€æ”¾è¯æ±‡è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼•å…¥æ–°æ¨¡å—å’Œæ”¹è¿›æå‡äº†CLIPåœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„æ€§èƒ½ã€‚</li>
<li>æ¶æ„å˜æ›´ç»“åˆäº†ä¸­é—´å±‚çš„æ³¨æ„åŠ›å›¾ä¸æœ€åä¸€å±‚çš„ä¿¡æ¯ã€‚</li>
<li>å›¾åƒå·¥ç¨‹é€šè¿‡æ•°æ®å¢å¼ºä¸°å¯Œäº†è¾“å…¥å›¾åƒè¡¨å¾ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºæ¯ç±»ç”Ÿæˆå®šä¹‰å’ŒåŒä¹‰è¯ï¼Œä»¥å¢å¼ºCLIPçš„å¼€æ”¾è¯æ±‡èƒ½åŠ›ã€‚</li>
<li>ITACLIPæ–¹æ³•åœ¨å¤šä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-727c6668103ef4ec0fc89952c2f56d29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-721c7cbae562f9fe24caf43822a855b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7faf5a4cba96348f7ea8ea10f28410a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2f7292b74b41cbad6a4f815a3192052.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AMBER-â€“-Advanced-SegFormer-for-Multi-Band-Image-Segmentation-an-application-to-Hyperspectral-Imaging"><a href="#AMBER-â€“-Advanced-SegFormer-for-Multi-Band-Image-Segmentation-an-application-to-Hyperspectral-Imaging" class="headerlink" title="AMBER â€“ Advanced SegFormer for Multi-Band Image Segmentation: an   application to Hyperspectral Imaging"></a>AMBER â€“ Advanced SegFormer for Multi-Band Image Segmentation: an   application to Hyperspectral Imaging</h2><p><strong>Authors:Andrea Dosi, Massimo Brescia, Stefano Cavuoti, Mariarca Dâ€™Aniello, Michele Delli Veneri, Carlo Donadio, Adriano Ettari, Giuseppe Longo, Alvi Rownok, Luca Sannino, Maria Zampella</strong></p>
<p>Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex spectral and spatial features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have led to the exploration of Vision Transformers (ViTs). This paper introduces AMBER, an advanced SegFormer specifically designed for multi-band image segmentation. AMBER enhances the original SegFormer by incorporating three-dimensional convolutions, custom kernel sizes, and a Funnelizer layer. This architecture enables processing hyperspectral data directly, without requiring spectral dimensionality reduction during preprocessing. Our experiments, conducted on three benchmark datasets (Salinas, Indian Pines, and Pavia University) and on a dataset from the PRISMA satellite, show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first three datasets, and achieves state-of-the-art performance on the PRISMA dataset. These findings highlight AMBERâ€™s robustness, adaptability to both airborne and spaceborne data, and its potential as a powerful solution for remote sensing and other domains requiring advanced analysis of high-dimensional data. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹å˜äº†é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰åˆ†æé¢†åŸŸï¼Œèƒ½å¤Ÿæå–å¤æ‚çš„å…‰è°±å’Œç©ºé—´ç‰¹å¾ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²æˆä¸ºHSIåˆ†ç±»çš„æ”¯æŸ±ï¼Œä½†åœ¨æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾æ–¹é¢çš„å±€é™æ€§ä¿ƒä½¿äººä»¬æ¢ç´¢è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†AMBERï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤šæ³¢æ®µå›¾åƒåˆ†å‰²è®¾è®¡çš„å…ˆè¿›SegFormerã€‚AMBERé€šè¿‡å¼•å…¥ä¸‰ç»´å·ç§¯ã€è‡ªå®šä¹‰å†…æ ¸å¤§å°å’Œæ¼æ–—å±‚ï¼Œå¢å¼ºäº†åŸå§‹çš„SegFormerã€‚è¯¥æ¶æ„èƒ½å¤Ÿç›´æ¥å¤„ç†é«˜å…‰è°±æ•°æ®ï¼Œæ— éœ€åœ¨é¢„å¤„ç†æœŸé—´é™ä½å…‰è°±ç»´åº¦ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆSalinasã€Indian Pineså’ŒPavia Universityï¼‰ä»¥åŠPRISMAå«æ˜Ÿæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒAMBERåœ¨æ€»ä½“ç²¾åº¦ã€Kappaç³»æ•°å’Œå¹³å‡ç²¾åº¦æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„åŸºäºCNNçš„æ–¹æ³•ï¼Œåœ¨PRISMAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¿™äº›å‘ç°çªå‡ºäº†AMBERçš„ç¨³å¥æ€§ã€å¯¹æœºè½½å’Œæ˜Ÿè½½æ•°æ®çš„é€‚åº”æ€§ï¼Œä»¥åŠå…¶ä½œä¸ºé¥æ„Ÿå’Œå…¶ä»–éœ€è¦é«˜çº§é«˜ç»´æ•°æ®åˆ†æé¢†åŸŸçš„æ½œåœ¨å¼ºå¤§è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09386v2">PDF</a> submitted to Neural Computing &amp; Applications (Springer). Accepted   with minor revisions</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨å…‰è°±å›¾åƒåˆ†æé¢†åŸŸå¼•å‘é©å‘½ï¼Œå·ç§¯ç¥ç»ç½‘ç»œåœ¨å…‰è°±å›¾åƒåˆ†ç±»ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å…¶æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„å±€é™æ€§ä¿ƒä½¿äº†å¯¹è§†è§‰Transformerçš„æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¤šæ³¢æ®µå›¾åƒåˆ†å‰²çš„é«˜çº§SegFormerâ€”â€”AMBERã€‚AMBERé€šè¿‡å¼•å…¥ä¸‰ç»´å·ç§¯ã€è‡ªå®šä¹‰å†…æ ¸å¤§å°å’ŒFunnelizerå±‚ï¼Œå¢å¼ºäº†åŸå§‹SegFormerçš„æ€§èƒ½ï¼Œå¯ç›´æ¥å¤„ç†å…‰è°±æ•°æ®ï¼Œæ— éœ€é¢„å¤„ç†ä¸­çš„å…‰è°±ç»´åº¦é™ä½ã€‚å®éªŒè¡¨æ˜ï¼ŒAMBERåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºä¼ ç»ŸCNNæ–¹æ³•ï¼Œå¹¶åœ¨PRISMAæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚è¿™è¡¨æ˜AMBERå…·æœ‰ç¨³å¥æ€§å’Œå¯¹èˆªç©ºå’Œå¤ªç©ºæ•°æ®çš„é€‚åº”æ€§ï¼Œä»¥åŠåœ¨é¥æ„Ÿå’Œå…¶ä»–éœ€è¦é«˜çº§é«˜ç»´æ•°æ®åˆ†æé¢†åŸŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å…‰è°±å›¾åƒåˆ†æé¢†åŸŸå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œåœ¨HSIåˆ†ç±»ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å­˜åœ¨æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„å±€é™æ€§ã€‚</li>
<li>Vision Transformersï¼ˆViTsï¼‰çš„æ¢ç´¢æ˜¯ä¸ºäº†è§£å†³CNNçš„å±€é™æ€§ã€‚</li>
<li>AMBERæ˜¯ä¸€ç§å…ˆè¿›çš„SegFormerï¼Œä¸“ä¸ºå¤šæ³¢æ®µå›¾åƒåˆ†å‰²è®¾è®¡ã€‚</li>
<li>AMBERé€šè¿‡å¼•å…¥ä¸‰ç»´å·ç§¯ã€è‡ªå®šä¹‰å†…æ ¸å’ŒFunnelizerå±‚å¢å¼ºäº†SegFormerçš„æ€§èƒ½ã€‚</li>
<li>AMBERå¯ç›´æ¥å¤„ç†å…‰è°±æ•°æ®ï¼Œæ— éœ€é¢„å¤„ç†ä¸­çš„å…‰è°±ç»´åº¦é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49d3d662712fa685952601d7cb182438.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-541f059415dc77a1ba42eba84c447d5d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MMCLIP-Cross-modal-Attention-Masked-Modelling-for-Medical-Language-Image-Pre-Training"><a href="#MMCLIP-Cross-modal-Attention-Masked-Modelling-for-Medical-Language-Image-Pre-Training" class="headerlink" title="MMCLIP: Cross-modal Attention Masked Modelling for Medical   Language-Image Pre-Training"></a>MMCLIP: Cross-modal Attention Masked Modelling for Medical   Language-Image Pre-Training</h2><p><strong>Authors:Biao Wu, Yutong Xie, Zeyu Zhang, Minh Hieu Phan, Qi Chen, Ling Chen, Qi Wu</strong></p>
<p>Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modeling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes the MMCLIP (Masked Medical Contrastive Language-Image Pre-Training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modeling (AttMIM) and entity-driven masked language modeling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interaction, thus improving medical-enhanced features. The AttMIM module masks a portion of the image features that are highly responsive to textual features. This allows MMCLIP to improve the reconstruction of highly similar image data in medicine efficiency. Second, our MMCLIP capitalizes unpaired data to enhance multimodal learning by introducing disease-kind prompts. The experimental results show that MMCLIP achieves SOTA for zero-shot and fine-tuning classification performance on five datasets. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/White65534/MMCLIP">https://github.com/White65534/MMCLIP</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦é¢†åŸŸçš„è§†è§‰å’Œè¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰é€šè¿‡å¯¹å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œå®ç°äº†è·¨ä»»åŠ¡çš„æœ‰æ•ˆè¿ç§»ã€‚ç„¶è€Œï¼Œå½“å‰é‡‡ç”¨æ©æ¨¡å»ºæ¨¡ç­–ç•¥çš„VLPæ–¹æ³•åœ¨åº”ç”¨åˆ°åŒ»å­¦é¢†åŸŸæ—¶é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç”±äºåŒ»ç–—æ•°æ®çš„ç¨€ç¼ºï¼Œå½“å‰æ¨¡å‹éš¾ä»¥å‡†ç¡®é‡å»ºå…³é”®ç—…ç†ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œå¤§å¤šæ•°æ–¹æ³•åªé‡‡ç”¨é…å¯¹å›¾åƒæ–‡æœ¬æˆ–ä»…å›¾åƒæ•°æ®ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨é…å¯¹å’Œæœªé…å¯¹æ•°æ®çš„ç»„åˆã€‚é‰´äºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MMCLIPï¼ˆåŸºäºæ©è†œåŒ»å­¦å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰æ¡†æ¶ï¼Œé€šè¿‡æœªé…å¯¹æ•°æ®å¢å¼ºç—…ç†å­¦ä¹ å’Œç‰¹å¾å­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†æ³¨æ„åŠ›æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆAttMIMï¼‰å’Œå®ä½“é©±åŠ¨æ©è†œè¯­è¨€å»ºæ¨¡æ¨¡å—ï¼ˆEntMLMï¼‰ï¼Œé€šè¿‡å¤šæ¨¡æ€ç‰¹å¾äº¤äº’å­¦ä¹ é‡å»ºç—…ç†è§†è§‰å’Œæ–‡æœ¬æ ‡è®°ï¼Œä»è€Œæé«˜åŒ»å­¦å¢å¼ºçš„ç‰¹å¾ã€‚AttMIMæ¨¡å—ä¼šæ©å»ä¸æ–‡æœ¬ç‰¹å¾é«˜åº¦å“åº”çš„å›¾åƒç‰¹å¾çš„ä¸€éƒ¨åˆ†ã€‚è¿™ä½¿å¾—MMCLIPèƒ½å¤Ÿåœ¨åŒ»å­¦æ•ˆç‡ä¸Šæé«˜é«˜åº¦ç›¸ä¼¼å›¾åƒæ•°æ®çš„é‡å»ºã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„MMCLIPåˆ©ç”¨æœªé…å¯¹çš„æ•°æ®ï¼Œé€šè¿‡å¼•å…¥ç–¾ç—…æç¤ºæ¥å¢å¼ºå¤šæ¨¡æ€å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMCLIPåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬å’Œå¾®è°ƒåˆ†ç±»æ€§èƒ½çš„æœ€æ–°æŠ€æœ¯è¡¨ç°ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/White65534/MMCLIP%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/White65534/MMCLIPä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19546v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—é¢†åŸŸçš„è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æŠ€æœ¯ã€‚ç”±äºåŒ»ç–—æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œå½“å‰é‡‡ç”¨æ©ç›–å»ºæ¨¡ç­–ç•¥çš„VLPæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯éš¾ä»¥å‡†ç¡®é‡å»ºå…³é”®ç—…ç†ç‰¹å¾ï¼ŒäºŒæ˜¯ä»…é‡‡ç”¨é…å¯¹å›¾åƒæ–‡æœ¬æˆ–ä»…é‡‡ç”¨å›¾åƒæ•°æ®ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨é…å¯¹å’Œæœªé…å¯¹æ•°æ®çš„ç»“åˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MMCLIPï¼ˆMasked Medical Contrastive Language-Image Pre-Trainingï¼‰æ¡†æ¶ï¼Œé€šè¿‡æœªé…å¯¹æ•°æ®å¢å¼ºç—…ç†å­¦ä¹ å’Œç‰¹å¾å­¦ä¹ ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ³¨æ„åŠ›æ©ç›–å›¾åƒå»ºæ¨¡ï¼ˆAttMIMï¼‰å’Œå®ä½“é©±åŠ¨æ©ç›–è¯­è¨€å»ºæ¨¡æ¨¡å—ï¼ˆEntMLMï¼‰ï¼Œé€šè¿‡å¤šæ¨¡æ€ç‰¹å¾äº¤äº’å­¦ä¹ é‡å»ºç—…ç†è§†è§‰å’Œæ–‡æœ¬ç¬¦å·ï¼Œä»è€Œæ”¹è¿›åŒ»å­¦å¢å¼ºç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMCLIPåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬å’Œå¾®è°ƒåˆ†ç±»æ€§èƒ½çš„æœ€å¥½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—é¢†åŸŸçš„è§†è§‰ä¸è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åˆ©ç”¨å¯¹æ¯”å­¦ä¹ åœ¨å›¾åƒæ–‡æœ¬å¯¹ä¸Šå®ç°ä»»åŠ¡é—´çš„æœ‰æ•ˆè¿ç§»ã€‚</li>
<li>å½“å‰VLPæ–¹æ³•åœ¨åº”ç”¨è‡³åŒ»ç–—é¢†åŸŸæ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œæœªèƒ½å……åˆ†åˆ©ç”¨é…å¯¹ä¸æœªé…å¯¹æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>MMCLIPæ¡†æ¶é€šè¿‡å¼•å…¥AttMIMå’ŒEntMLMæ¨¡å—ï¼Œå­¦ä¹ é‡å»ºç—…ç†è§†è§‰å’Œæ–‡æœ¬ç¬¦å·ï¼Œæ”¹è¿›åŒ»å­¦å¢å¼ºç‰¹å¾ã€‚</li>
<li>MMCLIPåˆ©ç”¨æœªé…å¯¹æ•°æ®å¢å¼ºå¤šæ¨¡æ€å­¦ä¹ ï¼Œå¹¶é€šè¿‡ç–¾ç—…ç±»å‹æç¤ºæ¥æé«˜æ•ˆæœã€‚</li>
<li>MMCLIPåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬å’Œå¾®è°ƒåˆ†ç±»æ€§èƒ½çš„æœ€ä½³æ°´å¹³ã€‚</li>
<li>MMCLIPä»£ç å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/White65534/MMCLIP%E3%80%82">https://github.com/White65534/MMCLIPã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.19546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-15617b0aa1a32bd38b3f7a7e2d63d74f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb3680161535f4fad008c28a6f0dd241.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d63e4db61bdc4865469fc43cdf3c347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46e09d12164b2ad7b9b05c21085a24a2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-985e59ca4f93d44f6a58155218450430.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  FLOSS Free Lunch in Open-vocabulary Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52551950764345252d6f9ad810dd6f09.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  VideoAds for Fast-Paced Video Understanding Where Opensource Foundation   Models Beat GPT-4o & Gemini-1.5 Pro
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
