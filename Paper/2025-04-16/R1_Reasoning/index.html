<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  xVerify Efficient Answer Verifier for Reasoning Model Evaluations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0a0db482bb6dba698457faf7640a1f79.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-16-æ›´æ–°"><a href="#2025-04-16-æ›´æ–°" class="headerlink" title="2025-04-16 æ›´æ–°"></a>2025-04-16 æ›´æ–°</h1><h2 id="xVerify-Efficient-Answer-Verifier-for-Reasoning-Model-Evaluations"><a href="#xVerify-Efficient-Answer-Verifier-for-Reasoning-Model-Evaluations" class="headerlink" title="xVerify: Efficient Answer Verifier for Reasoning Model Evaluations"></a>xVerify: Efficient Answer Verifier for Reasoning Model Evaluations</h2><p><strong>Authors:Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, Zhiyu Li</strong></p>
<p>With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify. </p>
<blockquote>
<p>éšç€OpenAIæ¨å‡ºçš„o1æ¨¡å‹ï¼Œé‡‡ç”¨æ…¢é€Ÿæ€è€ƒç­–ç•¥çš„æ€è€ƒæ¨¡å‹é€æ¸å‡ºç°ã€‚ç”±äºæ­¤ç±»æ¨¡å‹äº§ç”Ÿçš„å›åº”é€šå¸¸åŒ…æ‹¬å¤æ‚æ¨ç†ã€ä¸­é—´æ­¥éª¤å’Œè‡ªçœï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¸è¶³ã€‚ä»–ä»¬å¾ˆéš¾ç¡®å®šå¤§è¯­è¨€æ¨¡å‹è¾“å‡ºæ˜¯å¦çœŸæ­£ç­‰åŒäºå‚è€ƒç­”æ¡ˆï¼Œå¹¶ä¸”åœ¨ä»å†—é•¿å¤æ‚çš„å›åº”ä¸­è¯†åˆ«å’Œæå–æœ€ç»ˆç­”æ¡ˆæ–¹é¢ä¹Ÿå­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†xVerifyï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œç”¨äºæ€è€ƒæ¨¡å‹çš„è¯„ä¼°ã€‚xVerifyåœ¨ç­‰ä»·åˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¡®å®šæ€è€ƒæ¨¡å‹äº§ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦ç­‰åŒäºå„ç§ç±»å‹å®¢è§‚é—®é¢˜çš„å‚è€ƒç­”æ¡ˆã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°xVerifyï¼Œæˆ‘ä»¬é€šè¿‡æ”¶é›†å„ç§æ•°æ®é›†ç”±å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œåˆ©ç”¨å¤šä¸ªæ€è€ƒæ¨¡å‹å’Œä¸“é—¨ç”¨äºæ€è€ƒæ¨¡å‹è¯„ä¼°çš„æŒ‘æˆ˜æ€§è¯„ä¼°é›†ï¼Œæ„å»ºäº†VARæ•°æ®é›†ã€‚é‡‡ç”¨å¤šè½®æ³¨é‡Šè¿‡ç¨‹æ¥ä¿è¯æ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚åŸºäºVARæ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†å¤šç§ä¸åŒè§„æ¨¡çš„çš„xVerifyæ¨¡å‹ã€‚åœ¨æµ‹è¯•é›†å’Œæ³›åŒ–é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œæ‰€æœ‰xVerifyæ¨¡å‹çš„æ€»ä½“F1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡è¶…è¿‡95%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ€å°çš„å˜ç§xVerify-0.5B-Iåœ¨æ‰€æœ‰è¯„ä¼°æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ï¼Œä»…æ¬¡äºGPT-4oï¼Œè€ŒxVerify-3B-Ibåœ¨æ€»ä½“æ€§èƒ½ä¸Šè¶…è¶Šäº†GPT-4oã€‚è¿™äº›ç»“æœéªŒè¯äº†xVerifyçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10481v1">PDF</a> 32 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€OpenAIå‘å¸ƒçš„o1æ¨¡å‹ï¼Œé‡‡ç”¨ç¼“æ…¢æ€è€ƒç­–ç•¥çš„åˆç†æ¨¡å‹é€æ¸å‡ºç°ã€‚è¿™äº›æ¨¡å‹äº§ç”Ÿçš„å›åº”é€šå¸¸åŒ…å«å¤æ‚çš„æ¨ç†ã€ä¸­é—´æ­¥éª¤å’Œè‡ªæˆ‘åæ€ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†xVerifyï¼Œä¸€ä¸ªé«˜æ•ˆçš„ç­”æ¡ˆéªŒè¯å™¨ç”¨äºæ¨ç†æ¨¡å‹è¯„ä¼°ã€‚xVerifyåœ¨ç­‰ä»·åˆ¤æ–­æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç¡®å®šæ¨ç†æ¨¡å‹äº§ç”Ÿçš„ç­”æ¡ˆæ˜¯å¦ä¸å„ç§ç±»å‹å®¢è§‚é—®é¢˜çš„å‚è€ƒç­”æ¡ˆç­‰ä»·ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°xVerifyï¼Œæˆ‘ä»¬æ„å»ºäº†VARæ•°æ®é›†ï¼Œé€šè¿‡æ”¶é›†å¤šä¸ªLLMç”Ÿæˆçš„é—®ç­”å¯¹ï¼Œåˆ©ç”¨å¤šä¸ªæ¨ç†æ¨¡å‹ï¼Œå¹¶è®¾è®¡ä¸“é—¨é’ˆå¯¹æ¨ç†æ¨¡å‹è¯„ä¼°çš„æŒ‘æˆ˜è¯„ä¼°é›†ã€‚é€šè¿‡å¤šè½®æ³¨é‡Šè¿‡ç¨‹æ¥ä¿è¯æ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚åŸºäºVARæ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸åŒè§„æ¨¡çš„å¤šä¸ªxVerifyæ¨¡å‹ã€‚åœ¨æµ‹è¯•é›†å’Œæ³›åŒ–é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œæ‰€æœ‰xVerifyæ¨¡å‹çš„F1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡è¶…è¿‡95%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ€å°çš„å˜ç§xVerify-0.5B-Ié™¤GPT-4oå¤–è¶…è¶Šäº†æ‰€æœ‰è¯„ä¼°æ–¹æ³•ï¼Œè€ŒxVerify-3B-Ibåœ¨æ€»ä½“æ€§èƒ½ä¸Šè¶…è¶Šäº†GPT-4oã€‚è¿™äº›ç»“æœéªŒè¯äº†xVerifyçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹é‡‡ç”¨ç¼“æ…¢æ€è€ƒç­–ç•¥é€æ¸å—åˆ°å…³æ³¨ï¼Œå…¶å›åº”åŒ…å«å¤æ‚æ¨ç†ã€ä¸­é—´æ­¥éª¤å’Œè‡ªæˆ‘åæ€ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨åˆ¤æ–­æ¨ç†æ¨¡å‹è¾“å‡ºæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ–°çš„ç­”æ¡ˆéªŒè¯å™¨ã€‚</li>
<li>æå‡ºxVerifyä½œä¸ºé«˜æ•ˆçš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œç”¨äºæ¨ç†æ¨¡å‹è¯„ä¼°ï¼Œå…·æœ‰å¼ºå¤§çš„ç­‰ä»·åˆ¤æ–­èƒ½åŠ›ã€‚</li>
<li>æ„å»ºVARæ•°æ®é›†ï¼Œé€šè¿‡æ”¶é›†å¤šä¸ªLLMç”Ÿæˆçš„é—®ç­”å¯¹ï¼Œå¹¶è®¾è®¡ä¸“é—¨é’ˆå¯¹æ¨ç†æ¨¡å‹è¯„ä¼°çš„æŒ‘æˆ˜è¯„ä¼°é›†æ¥è®­ç»ƒå’Œè¯„ä¼°xVerifyã€‚</li>
<li>å¤šè½®æ³¨é‡Šè¿‡ç¨‹ç¡®ä¿æ ‡ç­¾å‡†ç¡®æ€§ã€‚</li>
<li>è®­ç»ƒäº†ä¸åŒè§„æ¨¡çš„å¤šä¸ªxVerifyæ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†å’Œæ³›åŒ–é›†ä¸Šè¡¨ç°å‡ºé«˜F1åˆ†æ•°å’Œå‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af71bf24a3cabccd60310c9fbe48dfb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff4a12e1c8a28bcc80b0fdd402747356.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b8154edd7758709adadf1d7fd6a73ba.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Weight-Ensembling-Improves-Reasoning-in-Language-Models"><a href="#Weight-Ensembling-Improves-Reasoning-in-Language-Models" class="headerlink" title="Weight Ensembling Improves Reasoning in Language Models"></a>Weight Ensembling Improves Reasoning in Language Models</h2><p><strong>Authors:Xingyu Dang, Christina Baek, Kaiyue Wen, Zico Kolter, Aditi Raghunathan</strong></p>
<p>We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§åœ¨è®­ç»ƒæ¨ç†æ¨¡å‹è¿‡ç¨‹ä¸­å‡ºç°çš„æ•…éšœæ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸‹ç”Ÿæˆçš„å¤šæ ·æ€§å¼€å§‹å´©æºƒï¼Œå¯¼è‡´æµ‹è¯•æ—¶çš„æ‰©å±•æ€§ä¸ä½³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æœŸé—´çš„Pass@1ç‡å¯é åœ°æé«˜äº†ï¼Œä½†Pass@kå´è¿…é€Ÿæ¶åŒ–ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé€šè¿‡æ’å€¼æœ€æ–°SFTæ£€æŸ¥ç‚¹çš„æƒé‡ä¸æ—©æœŸæ£€æŸ¥ç‚¹çš„æƒé‡ï¼ˆä¹Ÿç§°ä¸ºWiSE-FTï¼‰çš„ç®€å•å¹²é¢„æªæ–½ï¼Œå‡ ä¹å¯ä»¥å®Œå…¨æ¢å¤Pass@kï¼ŒåŒæ—¶æé«˜Pass@1ã€‚WiSE-FTå˜ä½“å®ç°äº†æ›´å¥½çš„æµ‹è¯•æ—¶é—´æ‰©å±•æ€§ï¼ˆBest@kï¼Œå¤šæ•°æŠ•ç¥¨ï¼‰ï¼Œå¹¶ä¸”åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è°ƒæ•´æ—¶ï¼Œç”¨æ›´å°‘çš„æ•°æ®å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°WiSE-FTæä¾›äº†æ— æ³•é€šè¿‡è¯¸å¦‚æ¸©åº¦ç¼©æ”¾ä¹‹ç±»çš„ä»…äº§ç”Ÿå¤šæ ·æ€§çš„è§£ç ç­–ç•¥å®ç°çš„äº’è¡¥æ€§èƒ½æå‡ã€‚æˆ‘ä»¬æ­£å¼æå‡ºäº†å…³äºPass@kçš„æœŸæœ›å’Œæ–¹å·®ä¹‹é—´çš„åå·®-æ–¹å·®æƒè¡¡ï¼Œå¹¶åœ¨æµ‹è¯•åˆ†å¸ƒä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æˆ‘ä»¬å‘ç°WiSE-FTå¯ä»¥åŒæ—¶å‡å°‘åå·®å’Œæ–¹å·®ï¼Œè€Œæ¸©åº¦ç¼©æ”¾æœ¬è´¨ä¸Šæ˜¯åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ä¼šå‡ºç°ä¸€ç§å¤±è´¥æ¨¡å¼ï¼Œå³ç”Ÿæˆçš„å¤šæ ·æ€§å¼€å§‹å´©æºƒï¼Œå¯¼è‡´æµ‹è¯•æ—¶çš„æ‰©å±•æ€§ä¸ä½³ã€‚å°½ç®¡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥æé«˜Pass@1ç‡ï¼Œä½†Pass@kå´è¿…é€Ÿæ¶åŒ–ã€‚é€šè¿‡é‡‡ç”¨ä¸€ç§åä¸ºWiSE-FTçš„ç®€å•å¹²é¢„æªæ–½ï¼ˆå³å°†æœ€æ–°SFTæ£€æŸ¥ç‚¹çš„æƒé‡ä¸æ—©æœŸæ£€æŸ¥ç‚¹è¿›è¡Œæ’å€¼ï¼‰ï¼Œå‡ ä¹å¯ä»¥å®Œå…¨æ¢å¤Pass@kå¹¶æ”¹å–„Pass@1ã€‚WiSE-FTå˜ä½“åœ¨æµ‹è¯•æ—¶çš„æ‰©å±•æ€§æ›´å¥½ï¼Œå¹¶ä¸”å½“é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è°ƒæ•´æ—¶ï¼Œå¯åœ¨è¾ƒå°‘æ•°æ®çš„æƒ…å†µä¸‹è·å¾—æ›´å¥½çš„ç»“æœã€‚WiSE-FTæä¾›äº†æ— æ³•é€šè¿‡å¦‚æ¸©åº¦ç¼©æ”¾ç­‰è¯±å¯¼å¤šæ ·æ€§çš„è§£ç ç­–ç•¥å®ç°çš„æ€§èƒ½å¢ç›Šã€‚åŒæ—¶ï¼ŒWiSE-FTå¯ä»¥é™ä½Pass@kçš„åå·®å’Œæ–¹å·®ï¼Œè€Œæ¸©åº¦ç¼©æ”¾åˆ™å­˜åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‡ºç°ç”Ÿæˆå¤šæ ·æ€§å´©æºƒçš„é—®é¢˜ï¼Œå¯¼è‡´æµ‹è¯•æ—¶æ‰©å±•æ€§ä¸ä½³ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èƒ½æé«˜Pass@1ç‡ï¼Œä½†Pass@kè¡¨ç°å´æ¶åŒ–ã€‚</li>
<li>WiSE-FTå¹²é¢„æªæ–½èƒ½æœ‰æ•ˆæ¢å¤Pass@kå¹¶æ”¹å–„Pass@1ã€‚</li>
<li>WiSE-FTå˜ä½“åœ¨æµ‹è¯•æ—¶çš„æ‰©å±•æ€§æ›´å¥½ï¼Œå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è°ƒæ•´å¯æå‡æ€§èƒ½ã€‚</li>
<li>WiSE-FTæä¾›çš„æ€§èƒ½å¢ç›Šæ— æ³•é€šè¿‡å•çº¯çš„è§£ç ç­–ç•¥ï¼ˆå¦‚æ¸©åº¦ç¼©æ”¾ï¼‰å®ç°ã€‚</li>
<li>WiSE-FTèƒ½åŒæ—¶é™ä½Pass@kçš„åå·®å’Œæ–¹å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4120875d081545d7868ebe62aab4122c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35ab658160b62d3f0d616864b4337d50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c217533d16be91c9f8584fdc6592eaa7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-644c79548a4c3a9ed2164f571ce6a777.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Xiaobo Xia, Run Luo</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>ç°æœ‰æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„åŠªåŠ›å¤§å¤šä¾èµ–äºåœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šé‡‡ç”¨ç›‘ç£å¾®è°ƒè®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢æ–¹é¢ä¹Ÿå­˜åœ¨å›°éš¾ã€‚è¿™ä¸€é—®é¢˜æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é«˜çº§ä»»åŠ¡ã€‚å—å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰çš„å¯å‘ï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰ï¼Œè¯¥æŠ€æœ¯åœ¨ç°å®åœºæ™¯ä¸­æœ‰æ•ˆåœ°æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£å†³é—®é¢˜èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºåä¸ºâ€œXXXâ€çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒæ˜¯é¦–ä¸ªæ—¨åœ¨é€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œæé«˜LVLMsåœ¨ç°å®é«˜çº§ä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¤šä¸ªå¹³å°ï¼ˆåŒ…æ‹¬Windowsã€Linuxã€MacOSã€Androidå’ŒWebï¼‰çš„å°é‡ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ï¼Œâ€œXXXâ€ä»…ä½¿ç”¨0.02%çš„æ•°æ®ï¼ˆ3Kå¯¹13Mï¼‰ä¾¿åœ¨è·¨è¶Šä¸‰ä¸ªä¸åŒå¹³å°ï¼ˆç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µï¼‰çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºOS-Atlasç­‰ç°æœ‰å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¯æ˜äº†åŸºäºç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡Œç°å®GUIä»£ç†ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ–¹é¢ï¼Œç°æœ‰åŠªåŠ›å¤§å¤šä¾èµ–äºåœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šé‡‡ç”¨ç›‘ç£å¾®è°ƒè®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢ä¸Šå­˜åœ¨å›°éš¾ã€‚è¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨é«˜çº§ä»»åŠ¡ä¸­ã€‚å—å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰çš„å¯å‘ï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œåç§°â€çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œæ—¨åœ¨æé«˜LVLMsåœ¨é«˜çº§ç°å®ä¸–ç•Œä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¤šä¸ªå¹³å°ï¼ˆåŒ…æ‹¬Windowsã€Linuxã€MacOSã€Androidå’ŒWebï¼‰çš„å°é‡ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ï¼Œâ€œåç§°â€åœ¨ä»…ä½¿ç”¨0.02%çš„æ•°æ®ï¼ˆ3Kå¯¹13Mï¼‰çš„æƒ…å†µä¸‹ï¼Œåœ¨è·¨è¶Šä¸‰ä¸ªä¸åŒå¹³å°ï¼ˆç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µï¼‰çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¯¹OS-Atlasç­‰ç°æœ‰å…ˆè¿›æ–¹æ³•çš„å“è¶Šæ€§èƒ½è¡¨ç°ã€‚è¿™äº›ç»“æœè¯æ˜äº†åŸºäºç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡Œç°å®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰GUIä»£ç†æ„å»ºä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒè®­ç»ƒèŒƒå¼åœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šï¼Œå­˜åœ¨æ•°æ®éœ€æ±‚é‡å¤§å’Œç†è§£ç•Œé¢èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¢«æå‡ºæ¥æé«˜LVLMsåœ¨ç°å®ä¸–ç•ŒGUIä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œé€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨è·¨å¤šä¸ªå¹³å°çš„å°é‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä»…ä½¿ç”¨æå°æ¯”ä¾‹çš„æ•°æ®ã€‚</li>
<li>è¯¥æ–¹æ³•çªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„é™åˆ¶ï¼Œä¸ºLVLMsåœ¨GUIä»£ç†ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ”¹å–„LVLMsæ‰§è¡Œç°å®ä¸–ç•ŒGUIä»»åŠ¡æ–¹é¢çš„æ½œåŠ›å·¨å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-940a90ba274d8e742e1bd4d78a0144af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acee592cf1878e46beb05cd5b4f5308f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b8bc950229c747c33ff13ba87e28048.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="M1-Towards-Scalable-Test-Time-Compute-with-Mamba-Reasoning-Models"><a href="#M1-Towards-Scalable-Test-Time-Compute-with-Mamba-Reasoning-Models" class="headerlink" title="M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models"></a>M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models</h2><p><strong>Authors:Junxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M. Rush, Tri Dao</strong></p>
<p>Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning. </p>
<blockquote>
<p>æœ‰æ•ˆçš„æ¨ç†å¯¹äºè§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æµ‹è¯•æ—¶é—´è®¡ç®—çš„æ‰©å±•å’Œé•¿æœŸæ¨ç†é“¾çš„æ¨ç†ï¼Œæå‡äº†æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ç”±äºå…¶äºŒæ¬¡è®¡ç®—å¤æ‚åº¦å’Œçº¿æ€§å†…å­˜éœ€æ±‚ï¼Œåœ¨æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦æ–¹é¢å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºMambaæ¶æ„çš„æ–°å‹æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹M1ï¼Œå®ƒå¯ä»¥è¿›è¡Œé«˜æ•ˆçš„å†…å­˜æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä»ç°æœ‰æ¨ç†æ¨¡å‹ä¸­æç‚¼çš„è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿›è¡Œè¿›ä¸€æ­¥æ”¹è¿›ã€‚åœ¨AIMEå’ŒMATHåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒM1ä¸ä»…ä¼˜äºä»¥å‰çš„çº¿æ€§RNNæ¨¡å‹ï¼Œè€Œä¸”åœ¨ç›¸ä¼¼è§„æ¨¡ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„Deepseek R1è’¸é¦æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜ä¸é«˜æ€§èƒ½é€šç”¨æ¨ç†å¼•æ“vLLMæ¯”è¾ƒäº†æˆ‘ä»¬çš„ç”Ÿæˆé€Ÿåº¦ï¼Œå¹¶è§‚å¯Ÿåˆ°ä¸ç›¸åŒè§„æ¨¡çš„è½¬æ¢å™¨ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆé€Ÿåº¦æé«˜äº†ä¸‰å€ä»¥ä¸Šã€‚é€šè¿‡æé«˜ååé‡é€Ÿåº¦ï¼Œæˆ‘ä»¬åœ¨å›ºå®šçš„ç”Ÿæˆæ—¶é—´é¢„ç®—å†…ï¼Œä½¿ç”¨è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨ï¼Œå®ç°äº†æ¯”DeepSeek R1è’¸é¦è½¬æ¢å™¨æ¨ç†æ¨¡å‹æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ··åˆçš„Mambaæ¨ç†æ¨¡å‹ï¼Œå¹¶æä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§æˆ–é•¿æœŸæ¨ç†é“¾æ¥æ‰©å±•æµ‹è¯•æ—¶é—´ç”Ÿæˆçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10449v1">PDF</a> Code is available <a target="_blank" rel="noopener" href="https://github.com/jxiw/M1">https://github.com/jxiw/M1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°å‹æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹M1ï¼Œè¯¥æ¨¡å‹åŸºäºMambaæ¶æ„ï¼Œå¯å®ç°é«˜æ•ˆçš„å†…å­˜æ¨ç†ã€‚é€šè¿‡é‡‡ç”¨ç°æœ‰æ¨ç†æ¨¡å‹çš„è’¸é¦è¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¢å¼ºæ€§èƒ½ï¼Œå®ƒåœ¨AIMEå’ŒMATHåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†å…ˆå‰çš„çº¿æ€§RNNæ¨¡å‹ï¼Œå¹¶åŒ¹é…äº†åŒç­‰è§„æ¨¡çš„æœ€å…ˆè¿›Deepseek R1è’¸é¦æ¨ç†æ¨¡å‹çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œä¸é«˜æ€§èƒ½é€šç”¨æ¨ç†å¼•æ“vLLMç›¸æ¯”ï¼ŒM1çš„ç”Ÿæˆé€Ÿåº¦æé«˜äº†ä¸‰å€ä»¥ä¸Šã€‚é€šè¿‡æé«˜ååé‡é€Ÿåº¦ï¼Œåœ¨å›ºå®šçš„ç”Ÿæˆæ—¶é—´é¢„ç®—å†…ï¼ŒM1åˆ©ç”¨è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨å®ç°äº†æ¯”DeepSeek R1è’¸é¦å˜å‹å™¨æ¨ç†æ¨¡å‹æ›´é«˜çš„å‡†ç¡®æ€§ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆçš„Mambaæ¨ç†æ¨¡å‹ï¼Œå¹¶æä¾›äº†ä¸€ç§æ›´æœ‰æ•ˆçš„æ‰©å±•æµ‹è¯•æ—¶é—´ç”Ÿæˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶çš„è®¡ç®—é“¾æ€ç»´æ¥æå‡è§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿçš„åŸºäºå˜å‹å™¨çš„æ¨¡å‹å—é™äºä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå› ä¸ºå…¶è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚å‘ˆäºŒæ¬¡å’Œçº¿æ€§å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆçº¿æ€§RNNæ¨ç†æ¨¡å‹M1ï¼ŒåŸºäºMambaæ¶æ„ï¼Œå¯å®ç°é«˜æ•ˆçš„å†…å­˜æ¨ç†ã€‚</li>
<li>M1é€šè¿‡è’¸é¦è¿‡ç¨‹å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¢å¼ºæ€§èƒ½ï¼Œè¡¨ç°è¶…è¶Šäº†å…ˆå‰çš„çº¿æ€§RNNæ¨¡å‹ï¼Œå¹¶åŒ¹é…äº†åŒç­‰è§„æ¨¡çš„æœ€å…ˆè¿›Deepseek R1è’¸é¦æ¨ç†æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>M1çš„ç”Ÿæˆé€Ÿåº¦æ¯”é«˜æ€§èƒ½é€šç”¨æ¨ç†å¼•æ“vLLMæé«˜äº†ä¸‰å€ä»¥ä¸Šã€‚</li>
<li>åœ¨å›ºå®šçš„ç”Ÿæˆæ—¶é—´é¢„ç®—å†…ï¼ŒM1åˆ©ç”¨è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-88b43279ae375bf68dcb52262d0f6990.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaa46c3ee5f052f5f8890bbe4fe57b87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d688c5ac3e364a4cc6d9f0ae67ff5ca9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SymRTLO-Enhancing-RTL-Code-Optimization-with-LLMs-and-Neuron-Inspired-Symbolic-Reasoning"><a href="#SymRTLO-Enhancing-RTL-Code-Optimization-with-LLMs-and-Neuron-Inspired-Symbolic-Reasoning" class="headerlink" title="SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired   Symbolic Reasoning"></a>SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired   Symbolic Reasoning</h2><p><strong>Authors:Yiting Wang, Wanghao Ye, Ping Guo, Yexiao He, Ziyao Wang, Yexiao He, Bowei Tian, Shwai He, Guoheng Sun, Zheyu Shen, Sihan Chen, Ankur Srivastava, Qingfu Zhang, Gang Qu, Ang Li</strong></p>
<p>Optimizing Register Transfer Level (RTL) code is crucial for improving the power, performance, and area (PPA) of digital circuits in the early stages of synthesis. Manual rewriting, guided by synthesis feedback, can yield high-quality results but is time-consuming and error-prone. Most existing compiler-based approaches have difficulty handling complex design constraints. Large Language Model (LLM)-based methods have emerged as a promising alternative to address these challenges. However, LLM-based approaches often face difficulties in ensuring alignment between the generated code and the provided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL optimization framework that seamlessly integrates LLM-based code rewriting with symbolic reasoning techniques. Our method incorporates a retrieval-augmented generation (RAG) system of optimization rules and Abstract Syntax Tree (AST)-based templates, enabling LLM-based rewriting that maintains syntactic correctness while minimizing undesired circuit behaviors. A symbolic module is proposed for analyzing and optimizing finite state machine (FSM) logic, allowing fine-grained state merging and partial specification handling beyond the scope of pattern-based compilers. Furthermore, a fast verification pipeline, combining formal equivalence checks with test-driven validation, further reduces the complexity of verification. Experiments on the RTL-Rewriter benchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves power, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%, respectively, compared to the state-of-the-art methods. </p>
<blockquote>
<p>å¯„å­˜å™¨ä¼ è¾“çº§åˆ«ï¼ˆRTLï¼‰ä»£ç çš„ä¼˜åŒ–å¯¹äºæ”¹è¿›åˆæˆæ—©æœŸé˜¶æ®µæ•°å­—ç”µè·¯çš„ç”µæºã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰è‡³å…³é‡è¦ã€‚æ‰‹åŠ¨é‡å†™å¹¶åœ¨åˆæˆåé¦ˆçš„æŒ‡å¯¼ä¸‹è¿›è¡Œå¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼Œä½†è¿™ç§æ–¹æ³•æ—¢è€—æ—¶åˆå®¹æ˜“å‡ºé”™ã€‚ç°æœ‰çš„å¤§å¤šæ•°åŸºäºç¼–è¯‘å™¨çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„è®¾è®¡çº¦æŸæ—¶éƒ½é¢ä¸´å›°éš¾ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•å·²æˆä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒLLM-basedæ–¹æ³•é€šå¸¸éš¾ä»¥ç¡®ä¿ç”Ÿæˆçš„ä»£ç ä¸æä¾›çš„æç¤ºå¯¹é½ã€‚æœ¬æ–‡æå‡ºäº†SymRTLOï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»å…ƒç¬¦å·RTLä¼˜åŒ–æ¡†æ¶ï¼Œæ— ç¼é›†æˆäº†åŸºäºLLMçš„ä»£ç é‡å†™å’Œç¬¦å·æ¨ç†æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¼˜åŒ–è§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå’ŒåŸºäºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„æ¨¡æ¿ï¼Œä½¿åŸºäºLLMçš„é‡å†™èƒ½å¤Ÿä¿æŒè¯­æ³•æ­£ç¡®æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–ä¸éœ€è¦çš„ç”µè·¯è¡Œä¸ºã€‚æå‡ºäº†ä¸€ç§ç¬¦å·æ¨¡å—ï¼Œç”¨äºåˆ†æå’Œä¼˜åŒ–æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰é€»è¾‘ï¼Œå®ç°ç²¾ç»†ç²’åº¦çš„çŠ¶æ€åˆå¹¶å’Œéƒ¨åˆ†è§„èŒƒå¤„ç†ï¼Œè¶…å‡ºæ¨¡å¼ç¼–è¯‘å™¨çš„èŒƒå›´ã€‚æ­¤å¤–ï¼Œç»“åˆå½¢å¼ç­‰ä»·æ£€æŸ¥å’Œæµ‹è¯•é©±åŠ¨éªŒè¯çš„å¿«é€ŸéªŒè¯ç®¡é“è¿›ä¸€æ­¥é™ä½äº†éªŒè¯çš„å¤æ‚æ€§ã€‚åœ¨Synopsys Design Compilerå’ŒYosysçš„RTL-RewriteråŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒSymRTLOåˆ†åˆ«å°†ç”µæºã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰æé«˜äº†é«˜è¾¾43.9%ã€62.5%å’Œ51.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10369v1">PDF</a> 16 pages, 8 figures, 7 tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSymRTLOçš„æ–°å‹ç¥ç»å…ƒç¬¦å·RTLä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†LLMåŸºäºçš„ä»£ç é‡å†™å’Œç¬¦å·æ¨ç†æŠ€æœ¯ã€‚SymRTLOé‡‡ç”¨ä¼˜åŒ–è§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿå’ŒåŸºäºASTçš„æ¨¡æ¿ï¼Œä½¿LLMåœ¨é‡å†™ä»£ç æ—¶èƒ½ä¿æŒè¯­æ³•æ­£ç¡®æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–ä¸è‰¯ç”µè·¯è¡Œä¸ºã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æå‡ºäº†ä¸€ä¸ªç¬¦å·æ¨¡å—ï¼Œç”¨äºåˆ†æå’Œä¼˜åŒ–æœ‰é™çŠ¶æ€æœºé€»è¾‘ï¼Œå®ç°äº†ç²¾ç»†çš„çŠ¶æ€åˆå¹¶å’Œéƒ¨åˆ†è§„æ ¼å¤„ç†ï¼Œè¶…è¶Šäº†åŸºäºæ¨¡å¼çš„ç¼–è¯‘å™¨çš„èŒƒå›´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒSymRTLOåœ¨åŠŸç‡ã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰æ–¹é¢çš„æ”¹è¿›åˆ†åˆ«é«˜è¾¾43.9%ã€62.5%å’Œ51.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼˜åŒ–RTLä»£ç å¯¹äºæé«˜æ•°å­—ç”µè·¯åœ¨æ—©æœŸåˆæˆé˜¶æ®µçš„åŠŸç‡ã€æ€§èƒ½å’Œé¢ç§¯ï¼ˆPPAï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç¼–è¯‘å™¨æ–¹æ³•åœ¨å¤„ç†å¤æ‚è®¾è®¡çº¦æŸæ—¶å­˜åœ¨å›°éš¾ï¼Œè€ŒLLMæ–¹æ³•åˆ™å±•ç°å‡ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„å¸Œæœ›ã€‚</li>
<li>SymRTLOæ¡†æ¶ç»“åˆäº†LLMä»£ç é‡å†™å’Œç¬¦å·æ¨ç†æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³LLMåœ¨ç”Ÿæˆä»£ç ä¸æç¤ºå¯¹é½æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>SymRTLOé‡‡ç”¨ä¼˜åŒ–è§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿå’ŒåŸºäºASTçš„æ¨¡æ¿ï¼Œç¡®ä¿è¯­æ³•æ­£ç¡®æ€§å’Œå‡å°‘ä¸è‰¯ç”µè·¯è¡Œä¸ºã€‚</li>
<li>ç¬¦å·æ¨¡å—ç”¨äºåˆ†æå’Œä¼˜åŒ–æœ‰é™çŠ¶æ€æœºé€»è¾‘ï¼Œå®ç°ç²¾ç»†çš„çŠ¶æ€åˆå¹¶å’Œéƒ¨åˆ†è§„æ ¼å¤„ç†ï¼Œè¶…å‡ºåŸºäºæ¨¡å¼ç¼–è¯‘å™¨çš„èƒ½åŠ›èŒƒå›´ã€‚</li>
<li>SymRTLOæä¾›äº†å¿«é€ŸéªŒè¯ç®¡é“ï¼Œç»“åˆå½¢å¼ç­‰ä»·æ€§æ£€æŸ¥å’Œæµ‹è¯•é©±åŠ¨éªŒè¯ï¼Œé™ä½äº†éªŒè¯å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5cc04c272d51273956dc8021e427e104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63aa113f831c7e0f6ba4afa8afe02cab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-709a9222dcaf8561b1a7e68141e0d43c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d9fb552602990ee898ca57635e7c11a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4e324a1b3b7517cafdaaf5f715fb69b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b89deca35911424ed634d22e4cfa879.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FingER-Content-Aware-Fine-grained-Evaluation-with-Reasoning-for-AI-Generated-Videos"><a href="#FingER-Content-Aware-Fine-grained-Evaluation-with-Reasoning-for-AI-Generated-Videos" class="headerlink" title="FingER: Content Aware Fine-grained Evaluation with Reasoning for   AI-Generated Videos"></a>FingER: Content Aware Fine-grained Evaluation with Reasoning for   AI-Generated Videos</h2><p><strong>Authors:Rui Chen, Lei Sun, Jing Tang, Geng Li, Xiangxiang Chu</strong></p>
<p>Recent advances in video generation have posed great challenges in the assessment of AI-generated content, particularly with the emergence of increasingly sophisticated models. The various inconsistencies and defects observed in such videos are inherently complex, making overall scoring notoriously difficult. In this paper, we emphasize the critical importance of integrating fine-grained reasoning into video evaluation, and we propose $\textbf{F}$ing$\textbf{ER}$, a novel entity-level reasoning evaluation framework that first automatically generates $\textbf{F}$ine-grained $\textbf{E}$ntity-level questions, and then answers those questions by a $\textbf{R}$easoning model with scores, which can be subsequently weighted summed to an overall score for different applications. Specifically, we leverage LLMs to derive entity-level questions across five distinct perspectives, which (i) often focus on some specific entities of the content, thereby making answering or scoring much easier by MLLMs, and (ii) are more interpretable. Then we construct a FingER dataset, consisting of approximately 3.3k videos and corresponding 60k fine-grained QA annotations, each with detailed reasons. Based on that, we further investigate various training protocols to best incentivize the reasoning capability of MLLMs for correct answer prediction. Extensive experiments demonstrate that a reasoning model trained using Group Relative Policy Optimization (GRPO) with a cold-start strategy achieves the best performance. Notably, our model surpasses existing methods by a relative margin of $11.8%$ on GenAI-Bench and $5.5%$ on MonetBench with only 3.3k training videos, which is at most one-tenth of the training samples utilized by other methods. Our code and dataset will be released soon. </p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ç»™AIç”Ÿæˆå†…å®¹çš„è¯„ä¼°å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éšç€æ¨¡å‹è¶Šæ¥è¶Šå¤æ‚ã€‚è¿™ç±»è§†é¢‘ä¸­è§‚å¯Ÿåˆ°çš„å„ç§ä¸ä¸€è‡´å’Œç¼ºé™·æœ¬è´¨ä¸Šå¾ˆå¤æ‚ï¼Œä½¿å¾—æ•´ä½“è¯„åˆ†æä¸ºå›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å°†ç²¾ç»†æ¨ç†é›†æˆåˆ°è§†é¢‘è¯„ä¼°ä¸­çš„å…³é”®é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„å®ä½“çº§æ¨ç†è¯„ä¼°æ¡†æ¶ï¼Œåä¸ºâ€œFingERâ€ã€‚FingERé¦–å…ˆè‡ªåŠ¨ç”Ÿæˆç²¾ç»†å®ä½“çº§é—®é¢˜ï¼Œç„¶åé€šè¿‡æ¨ç†æ¨¡å‹å¯¹è¿™äº›é—®é¢˜è¿›è¡Œå›ç­”å¹¶ç»™å‡ºåˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°éšåå¯ä»¥åŠ æƒæ±‚å’Œä»¥å¾—å‡ºä¸åŒåº”ç”¨çš„æ•´ä½“åˆ†æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»äº”ä¸ªä¸åŒçš„è§’åº¦æ¨å¯¼å‡ºå®ä½“çº§é—®é¢˜ï¼Œè¿™äº›é—®é¢˜ï¼ˆiï¼‰é€šå¸¸ä¾§é‡äºå†…å®¹ä¸­çš„æŸäº›ç‰¹å®šå®ä½“ï¼Œä»è€Œä½¿MLLMsæ›´å®¹æ˜“å›ç­”æˆ–è¯„åˆ†ï¼›ï¼ˆiiï¼‰æ›´å…·å¯è§£é‡Šæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªFingERæ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦3.3kä¸ªè§†é¢‘å’Œç›¸åº”çš„6ä¸‡æ¡ç²¾ç»†é—®ç­”æ³¨é‡Šï¼Œæ¯æ¡æ³¨é‡Šéƒ½æœ‰è¯¦ç»†çš„åŸå› ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶äº†å„ç§è®­ç»ƒåè®®ï¼Œä»¥æœ€å¤§é™åº¦åœ°æ¿€å‘MLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä»¥è¿›è¡Œæ­£ç¡®çš„ç­”æ¡ˆé¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå†·å¯åŠ¨ç­–ç•¥çš„æ¨ç†æ¨¡å‹å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»…ä½¿ç”¨3.3kä¸ªè®­ç»ƒè§†é¢‘ï¼Œä¾¿åœ¨GenAI-Benchä¸Šæ¯”ç°æœ‰æ–¹æ³•é«˜å‡º11.8%çš„å‡†ç¡®ç‡ï¼Œåœ¨MonetBenchä¸Šé«˜å‡º5.5%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10358v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>     éšç€è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œè¯„ä¼°AIç”Ÿæˆå†…å®¹ï¼Œç‰¹åˆ«æ˜¯æ—¥ç›Šå…ˆè¿›çš„æ¨¡å‹ï¼Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è§†é¢‘ä¸­å„ç§ä¸ä¸€è‡´å’Œç¼ºé™·çš„å¤æ‚æ€§ä½¿å¾—æ•´ä½“è¯„åˆ†æä¸ºå›°éš¾ã€‚æœ¬æ–‡å¼ºè°ƒå°†ç²¾ç»†æ¨ç†é›†æˆåˆ°è§†é¢‘è¯„ä¼°ä¸­çš„å…³é”®é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘å®ä½“çº§æ¨ç†è¯„ä¼°æ¡†æ¶â€”â€”FingERã€‚å®ƒé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„ç²¾ç»†å®ä½“çº§é—®é¢˜ï¼Œç„¶ååˆ©ç”¨æ¨ç†æ¨¡å‹å›ç­”è¿™äº›é—®é¢˜å¹¶ç»™å‡ºåˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°å¯ä»¥åŠ æƒæ±‡æ€»ä»¥å¾—å‡ºä¸åŒåº”ç”¨çš„æ•´ä½“åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»äº”ä¸ªä¸åŒè§’åº¦è¡ç”Ÿå‡ºå®ä½“çº§é—®é¢˜ï¼Œæ„å»ºäº†ä¸€ä¸ªFingERæ•°æ®é›†ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶å„ç§è®­ç»ƒåè®®ä»¥æ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’Œå†·å¯åŠ¨ç­–ç•¥çš„æ¨ç†æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚ä»…ä½¿ç”¨ä¸‰åƒå¤šä¸ªè®­ç»ƒè§†é¢‘å°±è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨GenAI-Benchä¸Šç›¸å¯¹æé«˜äº†11.8%ï¼Œåœ¨MonetBenchä¸Šæé«˜äº†5.5%ã€‚æˆ‘ä»¬å°†å¾ˆå¿«å‘å¸ƒç›¸å…³ä»£ç å’Œæ•°æ®é›†ã€‚<br>    ï¼ˆä¸Šè¿°æ–‡æœ¬ç²¾ç®€ä¸ºä¸€å¥è¯çš„å½¢å¼æ¦‚æ‹¬ä¸»è¦å†…å®¹ï¼‰æœ¬è®ºæ–‡å…³æ³¨AIç”Ÿæˆè§†é¢‘çš„è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†ç²¾ç»†å®ä½“çº§æ¨ç†è¯„ä¼°æ¡†æ¶FingERå’Œç›¸åº”çš„æ•°æ®é›†æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œé€šè¿‡ç²¾ç»†åŒ–é—®ç­”ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹æå‡è¯„ä¼°å‡†ç¡®æ€§å¹¶è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIè§†é¢‘ç”ŸæˆæŠ€æœ¯å¿«é€Ÿå‘å±•å¸¦æ¥è¯„ä¼°æŒ‘æˆ˜ã€‚</li>
<li>è§†é¢‘ä¸­çš„ä¸ä¸€è‡´æ€§å’Œç¼ºé™·å¤æ‚åº¦é«˜ï¼Œå¯¼è‡´æ•´ä½“è¯„åˆ†å›°éš¾ã€‚</li>
<li>å¼•å…¥ç²¾ç»†æ¨ç†åœ¨è§†é¢‘è¯„ä¼°ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºæ–°é¢–çš„è§†é¢‘å®ä½“çº§æ¨ç†è¯„ä¼°æ¡†æ¶â€”â€”FingERã€‚</li>
<li>FingERè‡ªåŠ¨ç”Ÿæˆç²¾ç»†å®ä½“çº§é—®é¢˜å¹¶ä¾é æ¨ç†æ¨¡å‹å›ç­”è¯„åˆ†ã€‚</li>
<li>æ„å»ºFingERæ•°æ®é›†ä»¥æ”¯æŒç²¾ç»†åŒ–é—®ç­”çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ab35bf8bb28c711e55d5c1b4dd1ef40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1e83bca635c998d6b587ff41b0d6eab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-740049849be97d1fe90da32ca288f228.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cf42f71cc8a59ad1a3f488e9d8e1ce5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge"><a href="#VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge" class="headerlink" title="VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge"></a>VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge</h2><p><strong>Authors:Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue</strong></p>
<p>Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with â€œthinkingâ€ modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•é€šå¸¸å°†æ¨ç†ä¸ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ··æ·†ï¼Œåœ¨éä¸“ä¸šç¯å¢ƒä¸­å¾ˆéš¾éš”ç¦»å’Œè¯„ä¼°ä¸€èˆ¬çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisualPuzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†è§‰æ¨ç†ä¸ºç›®æ ‡ï¼ŒåŒæ—¶åˆ»æ„å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†ä¾èµ–çš„åŸºå‡†æµ‹è¯•ã€‚VisualPuzzlesåŒ…å«äº”ä¸ªç±»åˆ«çš„é—®é¢˜ï¼šç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬é—®é¢˜çš„ä¸»è¦æ¥æºæ˜¯æ‰‹åŠ¨ç¿»è¯‘è‡ªä¸­å›½å…¬åŠ¡å‘˜è€ƒè¯•ä¸­çš„é€»è¾‘æ¨ç†é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œä¸MMMUç­‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVisualPuzzleså¯¹ä¸“ä¸šçŸ¥è¯†çš„è¦æ±‚æ˜¾è‘—é™ä½ï¼Œä½†æ¨ç†éš¾åº¦æ›´é«˜ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°çœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°å§‹ç»ˆè½åäºäººç±»ï¼Œè€Œåœ¨çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å‡ºè‰²è¡¨ç°å¹¶ä¸ä¸€å®šèƒ½åœ¨ä»¥æ¨ç†ä¸ºé‡ç‚¹ã€è½»çŸ¥è¯†çš„ä»»åŠ¡ä¸Šå–å¾—å¥½æˆç»©ã€‚æ­¤å¤–ï¼Œå¦‚é€šè¿‡æ‰©å¤§æ¨ç†è®¡ç®—ï¼ˆä½¿ç”¨â€œæ€è€ƒâ€æ¨¡å¼ï¼‰ç­‰å¢å¼ºæ¨ç†èƒ½åŠ›çš„æªæ–½åœ¨å„ç±»æ¨¡å‹å’Œä»»åŠ¡ä¸Šäº§ç”Ÿçš„æ•ˆæœä¸ä¸€ï¼Œå¹¶ä¸”æˆ‘ä»¬æœªå‘ç°æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´çš„æ˜ç¡®å…³è”ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¸ä¾§é‡äºçŸ¥è¯†çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨VisualPuzzlesä¸Šå±•ç°å‡ºä¸åŒçš„æ¨ç†å’Œç­”é¢˜æ¨¡å¼ã€‚VisualPuzzlesæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ™°çš„é€é•œï¼Œå¯ä»¥é€šè¿‡å®ƒæ¥è¯„ä¼°è¶…è¶Šäº‹å®è®°å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10342v1">PDF</a> 56 pages, 43 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå½“å‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å¸¸æ··æ·†æ¨ç†ä¸é¢†åŸŸçŸ¥è¯†çš„ç°çŠ¶ï¼Œæœ¬æ–‡å¼•å…¥VisualPuzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é’ˆå¯¹è§†è§‰æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼Œåˆ»æ„å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†ä¾èµ–ã€‚VisualPuzzlesåŒ…å«æ¶µç›–äº”å¤§ç±»çš„é—®é¢˜ï¼šç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºå‡†æµ‹è¯•å¦‚MMMUï¼ŒVisualPuzzlesçš„é—®é¢˜éœ€è¦æ›´å°‘çš„é¢†åŸŸçŸ¥è¯†å’Œæ›´å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°è¯„ä¼°çœŸå®çš„å¤šå…ƒæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°æŒç»­è½åäºäººç±»ï¼Œå¹¶ä¸”çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„ä¼˜ç§€è¡¨ç°å¹¶ä¸ä¸€å®šèƒ½è½¬åŒ–ä¸ºå¯¹æ³¨é‡æ¨ç†ã€è½»çŸ¥è¯†çš„ä»»åŠ¡çš„æˆåŠŸã€‚æ­¤å¤–ï¼Œå¯¹äºæ¨ç†å¢å¼ºæªæ–½å¦‚æ‰©å¤§æ¨ç†è®¡ç®—è§„æ¨¡ï¼ˆæ€è€ƒæ¨¡å¼ï¼‰å’Œæ¨¡å‹å¤§å°å¯¹æ€§èƒ½çš„å½±å“å¹¶ä¸æ˜æ˜¾ã€‚è¿˜å‘ç°æ¨¡å‹åœ¨VisualPuzzlesä¸Šå±•ç°å‡ºä¸åŒäºçŸ¥è¯†é‡ç‚¹åŸºå‡†æµ‹è¯•çš„æ¨ç†å’Œå›ç­”æ¨¡å¼ã€‚VisualPuzzlesæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ™°çš„è§†è§’æ¥è¯„ä¼°è¶…è¶Šäº‹å®å›å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•æ··æ·†æ¨ç†ä¸é¢†åŸŸçŸ¥è¯†ï¼Œå¯¼è‡´éš¾ä»¥åœ¨éä¸“ä¸šç¯å¢ƒä¸­è¯„ä¼°é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥VisualPuzzlesä½œä¸ºåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸“æ³¨äºè§†è§‰æ¨ç†å¹¶å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–ã€‚</li>
<li>VisualPuzzlesåŒ…å«æ¶µç›–äº”å¤§ç±»çš„é—®é¢˜ï¼Œæ¶‰åŠç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒVisualPuzzleséœ€è¦æ›´å¤æ‚çš„æ¨ç†èƒ½åŠ›å¹¶å‡å°‘å¯¹é¢†åŸŸçŸ¥è¯†çš„ä¾èµ–ï¼Œæ›´å‡†ç¡®åœ°è¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°è½åäºäººç±»ã€‚</li>
<li>çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•çš„æˆåŠŸå¹¶ä¸ç­‰åŒäºåœ¨æ³¨é‡æ¨ç†ã€è½»çŸ¥è¯†çš„ä»»åŠ¡ä¸Šçš„æˆåŠŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a0db482bb6dba698457faf7640a1f79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d978ad9899040c77f89efe8c0764d40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ec584ca1e2abec1b03750f6b198f048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13f7f96ad625b34e51ee23a2c63e4a07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e74d6329666e97d1651eb85005f486e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Heimdall-test-time-scaling-on-the-generative-verification"><a href="#Heimdall-test-time-scaling-on-the-generative-verification" class="headerlink" title="Heimdall: test-time scaling on the generative verification"></a>Heimdall: test-time scaling on the generative verification</h2><p><strong>Authors:Wenlei Shi, Xing Jin</strong></p>
<p>An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath. </p>
<blockquote>
<p>ä¸€ä¸ªAIç³»ç»Ÿåªèƒ½åœ¨å…¶èƒ½å¤ŸéªŒè¯è‡ªèº«çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œæ‰èƒ½åˆ›é€ å’Œä¿æŒçŸ¥è¯†ã€‚å…³äºé•¿é“¾æ€ç»´æ¨ç†çš„æœ€æ–°ç ”ç©¶å·²ç»æ˜¾ç¤ºå‡ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³ç«èµ›é—®é¢˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬çš„éªŒè¯èƒ½åŠ›ä»ç„¶è¾ƒå¼±ä¸”å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†èƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§çš„é•¿é“¾éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹Heimdallã€‚é€šè¿‡çº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬åœ¨ç«äº‰æ€§æ•°å­¦é—®é¢˜ä¸Šçš„éªŒè¯å‡†ç¡®ç‡ä»62.5%æé«˜åˆ°äº†94.5%ã€‚é€šè¿‡é‡å¤é‡‡æ ·çš„æ‰©å±•ï¼Œå‡†ç¡®ç‡è¿›ä¸€æ­¥æé«˜åˆ°äº†97.5%ã€‚é€šè¿‡äººå·¥è¯„ä¼°ï¼ŒHeimdallå±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸåœ°æ£€æµ‹åˆ°äº†æŒ‘æˆ˜æ€§æ•°å­¦è¯æ˜ä¸­çš„å¤§å¤šæ•°é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶æœªåŒ…å«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ‚²è§‚éªŒè¯æ³•æ¥æ‰©å±•Heimdallçš„åŠŸèƒ½ä»¥è§£å†³æ›´å¤§è§„æ¨¡çš„é—®é¢˜ã€‚å®ƒè°ƒç”¨Heimdallæ¥åˆ¤æ–­æ±‚è§£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ ¹æ®æ‚²è§‚åŸåˆ™ï¼Œé€‰æ‹©ä¸ç¡®å®šæ€§æœ€å°çš„æœ€å¯èƒ½çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆã€‚ä»¥DeepSeek-R1-Distill-Qwen-32Bä¸ºæ±‚è§£æ¨¡å‹ï¼Œæ‚²è§‚éªŒè¯æ³•å°†AIME2025çš„è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§ä»54.2%æé«˜åˆ°è®¡ç®—é¢„ç®—æ‰©å¤§16å€æ—¶çš„70.0%ï¼Œä»¥åŠæ›´é«˜è®¡ç®—é¢„ç®—ä¸‹çš„83.3%ã€‚ä½¿ç”¨æ›´å¼ºå¤§çš„æ±‚è§£å™¨Gemini 2.5 Proï¼Œå¾—åˆ†è¾¾åˆ°äº†93.0%ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨çŸ¥è¯†å‘ç°ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰å…ƒç³»ç»Ÿï¼Œå…¶ä¸­ä¸€ä¸ªæå‡ºé—®é¢˜ï¼Œå¦ä¸€ä¸ªæä¾›è§£å†³æ–¹æ¡ˆï¼Œç¬¬ä¸‰ä¸ªéªŒè¯è§£å†³æ–¹æ¡ˆã€‚ä½¿ç”¨å‰ä¸¤ä¸ªç»„ä»¶çš„æ•°æ®åˆæˆå·¥ä½œNuminaMathï¼ŒHeimdallæœ‰æ•ˆåœ°è¯†åˆ«äº†æ•°æ®é›†ä¸­æœ‰é—®é¢˜çš„è®°å½•å¹¶æ­ç¤ºå‡ºè¿‘ä¸€åŠçš„æ•°æ®å­˜åœ¨ç¼ºé™·ï¼Œè¿™ä¸NuminaMathæœ€è¿‘çš„æ¶ˆèç ”ç©¶ç»“æœç›¸å»åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10337v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºä¸€ç§åä¸ºHeimdallçš„é•¿CoTéªŒè¯LLMç³»ç»Ÿï¼Œå¯å‡†ç¡®åˆ¤æ–­è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œå…¶åœ¨ç«èµ›æ•°å­¦é—®é¢˜çš„éªŒè¯å‡†ç¡®åº¦ä»åŸæ¥çš„62.5%æå‡è‡³94.5%ï¼Œå¹¶é€šè¿‡é‡å¤é‡‡æ ·è¿›ä¸€æ­¥å¢è‡³97.5%ã€‚Heimdallå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨æŒ‘æˆ˜æ€§çš„æ•°å­¦è¯æ˜ä¸­æ£€æµ‹å‡ºå¤§éƒ¨åˆ†é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†æ‚²è§‚éªŒè¯æ³•æ¥æ‰©å±•Heimdallçš„åŠŸèƒ½ï¼Œä»¥æé«˜é—®é¢˜è§£å†³çš„å‡†ç¡®æ€§ã€‚æœ€åï¼Œè®ºæ–‡å°è¯•æ„å»ºä¸€ä¸ªè‡ªåŠ¨çŸ¥è¯†å‘ç°ç³»ç»Ÿï¼Œå…¶ä¸­åŒ…æ‹¬æå‡ºé—®é¢˜ã€æä¾›è§£å†³æ–¹æ¡ˆå’ŒéªŒè¯è§£å†³æ–¹æ¡ˆçš„ä¸‰ä¸ªéƒ¨åˆ†ï¼Œå…¶ä¸­Heimdallèƒ½æœ‰æ•ˆè¯†åˆ«æ•°æ®é›†ä¸­çš„é—®é¢˜è®°å½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Heimdallç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­é•¿Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒHeimdallåœ¨æ•°å­¦é—®é¢˜éªŒè¯æ–¹é¢çš„å‡†ç¡®åº¦æ˜¾è‘—æé«˜ã€‚</li>
<li>Heimdallå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨å¤æ‚çš„æ•°å­¦è¯æ˜ä¸­å‘ç°å¤§éƒ¨åˆ†é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†Pessimistic Verificationï¼ˆæ‚²è§‚éªŒè¯ï¼‰æ–¹æ³•ä»¥æé«˜é—®é¢˜è§£å†³çš„å‡†ç¡®æ€§ã€‚</li>
<li>Heimdallåœ¨ä¸å…¶ä»–æ¨¡å‹ç»“åˆåï¼Œå¦‚DeepSeek-R1-Distill-Qwen-32Bå’ŒGemini 2.5 Proï¼Œè§£å†³é—®é¢˜çš„å‡†ç¡®æ€§è¿›ä¸€æ­¥æé«˜ã€‚</li>
<li>è‡ªåŠ¨çŸ¥è¯†å‘ç°ç³»ç»Ÿçš„æ„å»ºåŒ…æ‹¬æå‡ºé—®é¢˜ã€æä¾›è§£å†³æ–¹æ¡ˆå’ŒéªŒè¯è§£å†³æ–¹æ¡ˆä¸‰ä¸ªå…³é”®éƒ¨åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b420fe4b651c9a8d0c39306069f53e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59222b9d0b74eddd45246ec1bdf0ec24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4eb625ac8755be635c204c6a8b0c2c93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4363de988bd90df7f56897878c7f688f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Deep-Reasoning-Translation-via-Reinforcement-Learning"><a href="#Deep-Reasoning-Translation-via-Reinforcement-Learning" class="headerlink" title="Deep Reasoning Translation via Reinforcement Learning"></a>Deep Reasoning Translation via Reinforcement Learning</h2><p><strong>Authors:Jiaan Wang, Fandong Meng, Jie Zhou</strong></p>
<p>Recently, deep reasoning LLMs (e.g., OpenAI o1&#x2F;o3 and DeepSeek-R1) have shown promising performance in various complex tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation and taking cultural differences into account. This task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning. Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought process. Given the source sentences, the reward model teaches the deep translation model how to think and free-translate them during reinforcement learning. In this way, training DeepTrans does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3% in literature translation, and outperforms strong deep reasoning baselines as well as baselines that are fine-tuned with synthesized data. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ·±åº¦æ¨ç†LLMsï¼ˆä¾‹å¦‚OpenAI o1&#x2F;o3å’ŒDeepSeek-R1ï¼‰åœ¨å„ç§å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚åœ¨å¤šå…ƒè¯­è¨€ä¸–ç•Œä¸­ï¼Œè‡ªç”±ç¿»è¯‘æ˜¯ä¸€é¡¹é‡è¦ä¸”æœ‰è¶£çš„ä»»åŠ¡ï¼Œå®ƒè¦æ±‚è¶…è¶Šé€å­—ç¿»è¯‘ï¼Œå¹¶è€ƒè™‘æ–‡åŒ–å·®å¼‚ã€‚ç„¶è€Œï¼Œè¿™é¡¹ä»»åŠ¡åœ¨æ·±åº¦æ¨ç†LLMsä¸­ä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ·±åº¦ç¿»è¯‘æ¨¡å‹DeepTransï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè‡ªç”±ç¿»è¯‘çš„æ·±åº¦æ¨ç†ç¿»è¯‘æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ ¹æ®ç¿»è¯‘ç»“æœå’Œæ€ç»´è¿‡ç¨‹çš„é¢„å®šè¯„åˆ†æ ‡å‡†ç²¾å¿ƒæ„å»ºäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ã€‚ç»™å®šæºå¥å­ï¼Œå¥–åŠ±æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­æ•™å¯¼æ·±åº¦ç¿»è¯‘æ¨¡å‹å¦‚ä½•æ€è€ƒå¹¶è¿›è¡Œè‡ªç”±ç¿»è¯‘ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè®­ç»ƒDeepTransä¸éœ€è¦ä»»ä½•æ ‡æ³¨ç¿»è¯‘ï¼Œé¿å…äº†äººåŠ›å¯†é›†å‹çš„æ ‡æ³¨æˆ–èµ„æºå¯†é›†å‹çš„æ•°æ®åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜DeepTransçš„æœ‰æ•ˆæ€§ã€‚ä»¥Qwen2.5-7Bä¸ºéª¨å¹²ç½‘ï¼ŒDeepTransåœ¨æ–‡çŒ®ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½æé«˜äº†16.3%ï¼Œå¹¶è¶…è¶Šäº†å¼ºå¤§çš„æ·±åº¦æ¨ç†åŸºçº¿ä»¥åŠä½¿ç”¨åˆæˆæ•°æ®å¾®è°ƒè¿‡çš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ€»ç»“äº†æˆ‘ä»¬åœ¨å¼ºåŒ–å­¦ä¹ æ¢ç´¢è¿‡ç¨‹ä¸­çš„å¤±è´¥å’Œæœ‰è¶£å‘ç°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿæ¿€å‘å…¶ä»–ç ”ç©¶äººå‘˜å¯¹è‡ªç”±ç¿»è¯‘çš„ç ”ç©¶çµæ„Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10187v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±æ¨ç†LLMsåœ¨æ·±ç¿»è¯‘æ¨¡å‹æ–¹é¢çš„è¡¨ç°å¼•äººå…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ ‡æ³¨ç¿»è¯‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ·±ç¿»è¯‘æ¨¡å‹DeepTransï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°è‡ªç”±ç¿»è¯‘ã€‚è¯¥æ¨¡å‹ä½¿ç”¨é¢„å®šä¹‰çš„è¯„åˆ†æ ‡å‡†æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œä»¥æŒ‡å¯¼ç¿»è¯‘è¿‡ç¨‹å’Œç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepTransåœ¨æ–‡å­¦ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½æé«˜äº†16.3%ï¼Œå¹¶è¶…è¶Šäº†å…¶ä»–å¼ºå¤§çš„æ·±æ¨ç†æ¨¡å‹å’Œç»è¿‡åˆæˆæ•°æ®å¾®è°ƒè¿‡çš„åŸºçº¿æ¨¡å‹ã€‚æœ¬æ–‡è¿˜æ€»ç»“äº†å¼ºåŒ–å­¦ä¹ æ¢ç´¢ä¸­çš„å¤±è´¥å’Œæœ‰è¶£å‘ç°ï¼Œå¸Œæœ›èƒ½æ¿€å‘å…¶ä»–ç ”ç©¶äººå‘˜å¯¹è‡ªç”±ç¿»è¯‘é¢†åŸŸçš„å…´è¶£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±æ¨ç†LLMsåœ¨æ·±ç¿»è¯‘æ¨¡å‹é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ— æ ‡æ³¨ç¿»è¯‘æ–¹é¢ã€‚</li>
<li>DeepTransæ˜¯ä¸€ç§æ–°çš„æ·±ç¿»è¯‘æ¨¡å‹ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ å®ç°è‡ªç”±ç¿»è¯‘ã€‚</li>
<li>DeepTransä½¿ç”¨é¢„å®šä¹‰çš„è¯„åˆ†æ ‡å‡†æ„å»ºå¥–åŠ±æ¨¡å‹ï¼Œä»¥æŒ‡å¯¼ç¿»è¯‘è¿‡ç¨‹å’Œç»“æœã€‚</li>
<li>DeepTransåœ¨æ–‡å­¦ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½æé«˜äº†16.3%ã€‚</li>
<li>DeepTransè¶…è¶Šäº†å…¶ä»–å¼ºå¤§çš„æ·±æ¨ç†æ¨¡å‹å’Œç»è¿‡åˆæˆæ•°æ®å¾®è°ƒè¿‡çš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>åœ¨å¼ºåŒ–å­¦ä¹ æ¢ç´¢ä¸­ï¼Œå­˜åœ¨å¤±è´¥å’Œæœ‰è¶£å‘ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be5be0cc0b23ab235ecca842a1ddf356.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afdfe23e1e72fa68f8d025ac2b15e021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-795e79cd0fc68afc100b0e0b4044e3d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63a76b2f2027e8d2f294ae6f9d95403d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MT-R1-Zero-Advancing-LLM-based-Machine-Translation-via-R1-Zero-like-Reinforcement-Learning"><a href="#MT-R1-Zero-Advancing-LLM-based-Machine-Translation-via-R1-Zero-like-Reinforcement-Learning" class="headerlink" title="MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like   Reinforcement Learning"></a>MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like   Reinforcement Learning</h2><p><strong>Authors:Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, Zuozhu Liu</strong></p>
<p>Large-scale reinforcement learning (RL) methods have proven highly effective in enhancing the reasoning abilities of large language models (LLMs), particularly for tasks with verifiable solutions such as mathematics and coding. However, applying this idea to machine translation (MT), where outputs are flexibly formatted and difficult to automatically evaluate with explicit rules, remains underexplored. In this work, we introduce MT-R1-Zero, the first open-source adaptation of the R1-Zero RL framework for MT without supervised fine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to guide LLMs towards improved translation quality via emergent reasoning. On the WMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive performance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points. Meanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across all metrics, placing it on par with advanced proprietary models such as GPT-4o and Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves state-of-the-art scores on semantic metrics. Moreover, our work exhibits strong generalization capabilities on out-of-distribution MT tasks, robustly supporting multilingual and low-resource settings. Extensive analysis of model behavior across different initializations and reward metrics offers pioneering insight into the critical role of reward design, LLM adaptability, training dynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fzp0424/MT-R1-Zero">https://github.com/fzp0424/MT-R1-Zero</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¯éªŒè¯è§£å†³æ–¹æ¡ˆçš„ä»»åŠ¡ä¸­ï¼Œå¦‚æ•°å­¦å’Œç¼–ç ã€‚ç„¶è€Œï¼Œå°†è¿™ä¸€ç†å¿µåº”ç”¨äºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ï¼Œå…¶è¾“å‡ºæ ¼å¼çµæ´»ï¼Œéš¾ä»¥ç”¨æ˜ç¡®çš„è§„åˆ™è‡ªåŠ¨è¯„ä¼°ï¼Œè¿™æ–¹é¢çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MT-R1-Zeroï¼Œè¿™æ˜¯R1-Zero RLæ¡†æ¶åœ¨æœºå™¨ç¿»è¯‘æ–¹é¢çš„é¦–æ¬¡å¼€æºé€‚åº”ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒæˆ–å†·å¯åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§„åˆ™åº¦é‡æ··åˆå¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡æ¶Œç°æ¨ç†å¼•å¯¼LLMæé«˜ç¿»è¯‘è´¨é‡ã€‚åœ¨WMT 24è‹±æ–‡-ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„MT-R1-Zero-3B-Mixè¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œå¹³å‡è¶…è¶ŠTowerInstruct-7B-v0.2 1.26ç‚¹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„MT-R1-Zero-7B-Mixåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„å¹³å‡å¾—åˆ†é«˜è¾¾62.25ï¼Œä¸å…ˆè¿›çš„ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oå’ŒClaude-3.5-Sonnetä¸ç›¸ä¸Šä¸‹ï¼Œè€ŒMT-R1-Zero-7B-Semå˜ä½“åœ¨è¯­ä¹‰æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¯¹è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œç¨³å¥åœ°æ”¯æŒå¤šè¯­ç§å’Œä½èµ„æºç¯å¢ƒã€‚å¯¹ä¸åŒåˆå§‹åŒ–å’Œå¥–åŠ±æŒ‡æ ‡çš„æ¨¡å‹è¡Œä¸ºè¿›è¡Œå…¨é¢åˆ†æï¼Œä¸ºå¥–åŠ±è®¾è®¡ã€LLMé€‚åº”æ€§ã€è®­ç»ƒåŠ¨æ€å’Œæœºå™¨ç¿»è¯‘ä¸­R1-ZeroèŒƒå¼å†…çš„æ¶Œç°æ¨ç†æ¨¡å¼çš„å…³é”®ä½œç”¨æä¾›äº†å¼€åˆ›æ€§çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/fzp0424/MT%E2%80%9CR1%E2%80%9Zero%E3%80%82">https://github.com/fzp0424/MT-R1-Zeroã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10160v1">PDF</a> Work in progress. Our code is available at   <a target="_blank" rel="noopener" href="https://github.com/fzp0424/MT-R1-Zero">https://github.com/fzp0424/MT-R1-Zero</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¯éªŒè¯è§£å†³æ–¹æ¡ˆçš„ä»»åŠ¡å¦‚æ•°å­¦å’Œç¼–ç ä¸­ã€‚ç„¶è€Œï¼Œåœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰é¢†åŸŸï¼Œè¿™ä¸€ç†å¿µçš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ¨å‡ºMT-R1-Zeroï¼Œè¿™æ˜¯é¦–ä¸ªæ— éœ€ç›‘ç£å¾®è°ƒæˆ–å†·å¯åŠ¨çš„R1-Zeroå¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸçš„å¼€æºæ”¹é€ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§„åˆ™åº¦é‡æ··åˆå¥–åŠ±æœºåˆ¶ï¼Œä»¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹æå‡ç¿»è¯‘è´¨é‡ï¼Œå®ç°æ¶Œç°å¼æ¨ç†ã€‚åœ¨WMT 24è‹±æ–‡-ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„MT-R1-Zero-3B-Mixè¡¨ç°å‡ºå¼ºåŠ²ç«äº‰åŠ›ï¼Œå¹³å‡è¶…è¶ŠTowerInstruct-7B-v0.2è¾¾1.26åˆ†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„MT-R1-Zero-7B-Mixåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„å¹³å‡å¾—åˆ†é«˜è¾¾62.25ï¼Œä¸å…ˆè¿›çš„ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oå’ŒClaude-3.5-Sonnetä¸ç›¸ä¸Šä¸‹ï¼›è€ŒMT-R1-Zero-7B-Semå˜ä½“åœ¨è¯­ä¹‰æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†åœ¨åˆ†å¸ƒå¼å¤–çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œç¨³å¥åœ°æ”¯æŒå¤šè¯­ç§å’Œä½èµ„æºç¯å¢ƒã€‚å¯¹æ¨¡å‹è¡Œä¸ºçš„ä¸åŒåˆå§‹åŒ–å’Œå¥–åŠ±æŒ‡æ ‡çš„å…¨é¢åˆ†æï¼Œä¸ºR1-ZeroèŒƒå¼åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„å¥–åŠ±è®¾è®¡ã€å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”æ€§ã€è®­ç»ƒåŠ¨æ€å’Œæ¶Œç°å¼æ¨ç†æ¨¡å¼çš„å…³é”®ä½œç”¨æä¾›äº†å¼€åˆ›æ€§çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¢å¼ºæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œå°¤å…¶åœ¨æ•°å­¦å’Œç¼–ç ç­‰å…·æœ‰å¯éªŒè¯è§£çš„ä»»åŠ¡ä¸­ã€‚</li>
<li>åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸåº”ç”¨å¼ºåŒ–å­¦ä¹ å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚</li>
<li>æ¨å‡ºMT-R1-Zeroæ¨¡å‹ï¼Œæ˜¯é¦–ä¸ªæ— éœ€ç›‘ç£å¾®è°ƒæˆ–å†·å¯åŠ¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨æœºå™¨ç¿»è¯‘çš„åº”ç”¨ã€‚</li>
<li>æå‡ºè§„åˆ™åº¦é‡æ··åˆå¥–åŠ±æœºåˆ¶ä»¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹æé«˜ç¿»è¯‘è´¨é‡ã€‚</li>
<li>åœ¨WMTè‹±æ–‡åˆ°ä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä¸”æ³›åŒ–èƒ½åŠ›å¼ºã€‚</li>
<li>ç ”ç©¶æ·±å…¥åˆ†æäº†å¥–åŠ±è®¾è®¡ã€å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”æ€§ã€è®­ç»ƒåŠ¨æ€å’Œæ¶Œç°å¼æ¨ç†æ¨¡å¼çš„å…³é”®ä½œç”¨ã€‚</li>
<li>å¼€æºä»£ç ä¸ºç ”ç©¶è€…æä¾›äº†è¿›ä¸€æ­¥æ¢ç´¢çš„æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6731c2d9978aa541371d5264b866f1f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de316fedcf53a0a54ef10747ff16d291.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6da6cba404cdf8394fee8d1086df74f4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization"><a href="#Breaking-the-Data-Barrier-â€“-Building-GUI-Agents-Through-Task-Generalization" class="headerlink" title="Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization"></a>Breaking the Data Barrier â€“ Building GUI Agents Through Task   Generalization</h2><p><strong>Authors:Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, Junxian He</strong></p>
<p>Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid">https://github.com/hkust-nlp/GUIMid</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æä¾›è·¨å¹³å°è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè‡ªåŠ¨åŒ–å¤æ‚çš„æ•°å­—ä»»åŠ¡ï¼Œå…·æœ‰æ”¹å˜ç”Ÿäº§åŠ›å·¥ä½œæµç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¡¨ç°å¾€å¾€å—åˆ°é«˜è´¨é‡è½¨è¿¹æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ä¸“é—¨çš„ä¸­é—´è®­ç»ƒé˜¶æ®µï¼Œåœ¨æ•°æ®ä¸°å¯Œã€æ¨ç†å¯†é›†çš„ä»»åŠ¡ä¸Šè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç„¶åç ”ç©¶å¦‚ä½•å°†è¿™äº›ä»»åŠ¡çº³å…¥GUIè§„åˆ’åœºæ™¯ï¼Œä»¥ä¿ƒè¿›é€šç”¨åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç³»åˆ—å…·æœ‰å¯è·å–æŒ‡ä»¤è°ƒæ•´æ•°æ®çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬GUIæ„ŸçŸ¥ã€å¤šæ¨¡æ€æ¨ç†å’Œæ–‡æœ¬æ¨ç†ã€‚é€šè¿‡å¯¹11ä¸ªä¸­é—´è®­ç»ƒä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ï¼šï¼ˆ1ï¼‰ä»»åŠ¡é€šç”¨åŒ–è¯æ˜éå¸¸æœ‰æ•ˆï¼Œåœ¨å¤§å¤šæ•°è®¾ç½®ä¸­éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œå¤šæ¨¡æ€æ•°å­¦æ¨ç†åœ¨AndroidWorldä¸Šçš„è¡¨ç°æé«˜äº†ç»å¯¹6.3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåªæœ‰æ–‡æœ¬çš„æ•°å­¦æ•°æ®æ˜¾è‘—æé«˜äº†GUIç½‘ç»œä»£ç†çš„æ€§èƒ½ï¼Œåœ¨WebArenaä¸Šæé«˜äº†5.6%ï¼Œåœ¨AndroidWorldä¸Šæé«˜äº†5.4%ï¼Œçªæ˜¾äº†ä»æ–‡æœ¬åˆ°è§†è§‰é¢†åŸŸçš„è·¨æ¨¡æ€é€šç”¨çš„é‡è¦æ€§ï¼›ï¼ˆ2ï¼‰ä¸å…ˆå‰çš„å‡è®¾ç›¸åï¼Œä¹‹å‰è¢«è®¤ä¸ºä¸GUIä»£ç†ä»»åŠ¡ç´§å¯†ç›¸å…³å¹¶å¹¿æ³›ç”¨äºè®­ç»ƒçš„GUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“ç›¸å¯¹æœ‰é™ï¼›ï¼ˆ3ï¼‰åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬ç¡®å®šäº†æœ€æœ‰æ•ˆçš„ä¸­é—´è®­ç»ƒä»»åŠ¡å¹¶ç­–åˆ’äº†ä¼˜åŒ–çš„æ··åˆæ•°æ®é›†ï¼Œä»è€Œåœ¨WebArenaä¸Šå®ç°äº†8.0%çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œåœ¨AndroidWorldä¸Šå®ç°äº†12.2%çš„æå‡ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºGUIä»£ç†çš„è·¨åŸŸçŸ¥è¯†è½¬ç§»æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºè§£å†³è¿™ä¸€æ–°å…´é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æä¾›äº†å®ç”¨æ–¹æ³•ã€‚ç›¸å…³ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/GUIMid%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hkust-nlp/GUIMidæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10127v1">PDF</a> 24 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>GUIä»£ç†çš„è·¨å¹³å°è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆå…·æœ‰æ”¹å˜ç”Ÿäº§åŠ›å·¥ä½œæµç¨‹çš„å·¨å¤§æ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½å¸¸å¸¸å—åˆ°é«˜è´¨é‡è½¨è¿¹æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åœ¨ç‰¹å®šçš„ä¸­æœŸè®­ç»ƒé˜¶æ®µå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œä¸°å¯Œæ•°æ®ã€æ¨ç†å¯†é›†å‹ä»»åŠ¡è®­ç»ƒçš„æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å°†è¿™ç§æ–¹æ³•åº”ç”¨äºGUIè§„åˆ’åœºæ™¯ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºä»»åŠ¡æ³›åŒ–å¯æœ‰æ•ˆæé«˜æ€§èƒ½ï¼Œå…¶ä¸­å¤šæ¨¡æ€æ•°å­¦æ¨ç†å¯¹AndroidWorldä»»åŠ¡çš„æ€§èƒ½æå‡å°¤ä¸ºæ˜¾è‘—ã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†GUIæ„ŸçŸ¥æ•°æ®å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“æœ‰é™ï¼Œå¹¶åŸºäºè¿™äº›è§è§£ä¼˜åŒ–äº†ä¸­æœŸè®­ç»ƒä»»åŠ¡å’Œæ··åˆæ•°æ®é›†ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUIä»£ç†å…·æœ‰è‡ªåŠ¨åŒ–å¤æ‚æ•°å­—ä»»åŠ¡çš„è·¨å¹³å°è§£å†³æ–¹æ¡ˆæ½œåŠ›ã€‚</li>
<li>é«˜è´¨é‡è½¨è¿¹æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯GUIä»£ç†æ€§èƒ½çš„ä¸»è¦é™åˆ¶ã€‚</li>
<li>é€šè¿‡åœ¨ä¸­æœŸè®­ç»ƒé˜¶æ®µå¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œå¯ä»¥æœ‰æ•ˆæé«˜GUIä»£ç†çš„æ€§èƒ½ã€‚</li>
<li>ä»»åŠ¡æ³›åŒ–å¯¹äºæé«˜GUIä»£ç†æ€§èƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢ã€‚</li>
<li>GUIæ„ŸçŸ¥æ•°æ®å¯¹GUIä»£ç†çš„æœ€ç»ˆæ€§èƒ½å½±å“æœ‰é™ã€‚</li>
<li>ä¼˜åŒ–ä¸­æœŸè®­ç»ƒä»»åŠ¡å’Œæ··åˆæ•°æ®é›†å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f084c3dd8cfd8ff80bc90f01d0c1aaf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd643e844c30b1ea9001fa8abf454a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9538ec9b281525055fe2dba4ad52577b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b06c3f274d496bdbce3525686789fc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a7e1ab5316bb2fa4676e2357902383a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CameraBench-Benchmarking-Visual-Reasoning-in-MLLMs-via-Photography"><a href="#CameraBench-Benchmarking-Visual-Reasoning-in-MLLMs-via-Photography" class="headerlink" title="CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography"></a>CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography</h2><p><strong>Authors:I-Sheng Fang, Jun-Cheng Chen</strong></p>
<p>Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ¶‰åŠè§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„è§†è§‰æ¨ç†ä»ç„¶è¢«è¾ƒå°‘ç ”ç©¶ã€‚æœ€è¿‘çš„è¿›å±•ï¼ŒåŒ…æ‹¬èå…¥å›¾åƒè¾“å…¥çš„æ¨ç†æ¨¡å‹ï¼Œå¦‚OpenAI o1å’ŒGemini 2.0 Flash Thinkingï¼Œå¼€å¯äº†è¿™ä¸€åŠŸèƒ½ã€‚åœ¨è¿™é¡¹æŒç»­çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ‘„å½±ç›¸å…³ä»»åŠ¡ï¼Œå› ä¸ºç…§ç‰‡æ˜¯ç‰©ç†ä¸–ç•Œçš„è§†è§‰å¿«ç…§ï¼Œå…¶ä¸­åŸºç¡€ç‰©ç†å­¦ï¼ˆå³ç…§æ˜ã€æ¨¡ç³Šç¨‹åº¦ç­‰ï¼‰ä¸ç›¸æœºå‚æ•°ç›¸äº’ä½œç”¨ã€‚ä»ç…§ç‰‡çš„è§†è§‰ä¿¡æ¯ä¸­æˆåŠŸæ¨ç†å‡ºè¿™äº›æ•°å€¼ç›¸æœºè®¾ç½®ï¼Œè¦æ±‚MLLMå¯¹åŸºç¡€ç‰©ç†å­¦æœ‰æ›´æ·±çš„ç†è§£ï¼Œä»¥å®ç°ç²¾ç¡®çš„è§†è§‰ç†è§£ï¼Œè¿™æ˜¯æ‘„å½±åŠ©ç†ä»£ç†ç­‰å®é™…åº”ç”¨ä¸­å¿…ä¸å¯å°‘çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ™ºèƒ½èƒ½åŠ›çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯¹MLLMè¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°å®ƒä»¬åœ¨åŒºåˆ†ä¸æ•°å€¼ç›¸æœºè®¾ç½®ç›¸å…³çš„è§†è§‰å·®å¼‚æ–¹é¢çš„èƒ½åŠ›ï¼Œæ‰©å±•ä¹‹å‰ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆæ­¥ç»“æœè¯æ˜äº†è§†è§‰æ¨ç†åœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰å•ä¸€MLLMåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­éƒ½å§‹ç»ˆå æ®ä¸»å¯¼åœ°ä½ï¼Œè¿™å±•ç¤ºäº†åœ¨å¼€å‘å…·æœ‰æ›´å¥½è§†è§‰æ¨ç†èƒ½åŠ›çš„MLLMæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜å’Œæœºé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10090v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è§†è§‰æ¨ç†ï¼Œå³æ¶‰åŠè§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„æ¨ç†ï¼Œä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ€è¿‘ï¼Œä¸€äº›ç»“åˆå›¾åƒè¾“å…¥çš„æ¨ç†æ¨¡å‹ï¼Œå¦‚OpenAI o1å’ŒGemini 2.0 Flash Thinkingï¼Œå¼€å§‹æ‰“å¼€è¿™ä¸€é¢†åŸŸã€‚æœ¬æ–‡ä¸“æ³¨äºæ‘„å½±ç›¸å…³ä»»åŠ¡çš„ç ”ç©¶ï¼Œå› ä¸ºç…§ç‰‡æ˜¯ç‰©ç†ä¸–ç•Œçš„è§†è§‰å¿«ç…§ï¼Œå…¶ä¸­åŸºç¡€ç‰©ç†å­¦ï¼ˆå¦‚ç…§æ˜ã€æ¨¡ç³Šç¨‹åº¦ç­‰ï¼‰ä¸ç›¸æœºå‚æ•°ç›¸äº’ä½œç”¨ã€‚è¯„ä¼°MLLMsä»ç…§ç‰‡çš„è§†è§‰ä¿¡æ¯ä¸­æ¨ç†å‡ºè¿™äº›æ•°å­—ç›¸æœºè®¾ç½®çš„èƒ½åŠ›ï¼Œéœ€è¦å®ƒä»¬å¯¹åŸºç¡€ç‰©ç†å­¦æœ‰æ›´æ·±çš„ç†è§£ï¼Œä»¥å®ç°ç²¾ç¡®è§†è§‰ç†è§£ï¼Œè¿™æ˜¯æ‘„å½±åŠ©ç†ä»£ç†ç­‰å®é™…åº”ç”¨ä¸­å¿…ä¸å¯å°‘çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ™ºèƒ½çš„èƒ½åŠ›ã€‚åˆæ­¥ç»“æœè¡¨æ˜è§†è§‰æ¨ç†åœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸”æ²¡æœ‰å•ä¸€MLLMåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œè¿™æ˜¾ç¤ºäº†å¼€å‘å…·æœ‰æ›´å¥½è§†è§‰æ¨ç†èƒ½åŠ›çš„MLLMsçš„å½“å‰æŒ‘æˆ˜å’Œæœºé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>è§†è§‰æ¨ç†ï¼Œå°¤å…¶æ˜¯æ¶‰åŠæ•°å­—ç›¸æœºè®¾ç½®çš„æ¨ç†ï¼Œåœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶ï¼Œå¦‚OpenAI o1å’ŒGemini 2.0 Flash Thinkingï¼Œå¼€å§‹ç»“åˆå›¾åƒè¾“å…¥è¿›è¡Œæ¨ç†ã€‚</li>
<li>å¯¹åŸºç¡€ç‰©ç†å­¦çš„ç†è§£å¯¹äºå®ç°ç²¾ç¡®çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>è¯„ä¼°MLLMsçš„èƒ½åŠ›éœ€è¦å®ƒä»¬åœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸­ä»ç…§ç‰‡çš„è§†è§‰ä¿¡æ¯ä¸­æ¨ç†å‡ºæ•°å­—ç›¸æœºè®¾ç½®ã€‚</li>
<li>åˆæ­¥ç»“æœè¡¨æ˜è§†è§‰æ¨ç†çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºè¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ™ºèƒ½çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c76b6ee2e44f6418db43c1cc9dc549d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32128662ed1277818e37f8dfeaf5d3bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fad692f21531b4f1110ba53492ebc54a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dc30df6a97db3804833bf07d3195536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8b6a4036d6def9d5dc7bfce065e7c3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RealSafe-R1-Safety-Aligned-DeepSeek-R1-without-Compromising-Reasoning-Capability"><a href="#RealSafe-R1-Safety-Aligned-DeepSeek-R1-without-Compromising-Reasoning-Capability" class="headerlink" title="RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning   Capability"></a>RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning   Capability</h2><p><strong>Authors:Yichi Zhang, Zihao Zeng, Dongbai Li, Yao Huang, Zhijie Deng, Yinpeng Dong</strong></p>
<p>Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been rapidly progressing and achieving breakthrough performance on complex reasoning tasks such as mathematics and coding. However, the open-source R1 models have raised safety concerns in wide applications, such as the tendency to comply with malicious queries, which greatly impacts the utility of these powerful models in their applications. In this paper, we introduce RealSafe-R1 as safety-aligned versions of DeepSeek-R1 distilled models. To train these models, we construct a dataset of 15k safety-aware reasoning trajectories generated by DeepSeek-R1, under explicit instructions for expected refusal behavior. Both quantitative experiments and qualitative case studies demonstrate the modelsâ€™ improvements, which are shown in their safety guardrails against both harmful queries and jailbreak attacks. Importantly, unlike prior safety alignment efforts that often compromise reasoning performance, our method preserves the modelsâ€™ reasoning capabilities by maintaining the training data within the original distribution of generation. Model weights of RealSafe-R1 are open-source at <a target="_blank" rel="noopener" href="https://huggingface.co/RealSafe">https://huggingface.co/RealSafe</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œå¦‚OpenAI o1å’ŒDeepSeek-R1ï¼Œè¿›å±•è¿…é€Ÿï¼Œå¹¶åœ¨æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œå¼€æºçš„R1æ¨¡å‹åœ¨å¹¿æ³›åº”ç”¨ä¸­å¼•å‘äº†å®‰å…¨æ‹…å¿§ï¼Œä¾‹å¦‚å€¾å‘äºæ»¡è¶³æ¶æ„æŸ¥è¯¢ï¼Œè¿™æå¤§åœ°å½±å“äº†è¿™äº›å¼ºå¤§æ¨¡å‹åœ¨å…¶åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RealSafe-R1ä½œä¸ºåŸºäºDeepSeek-R1è’¸é¦æ¨¡å‹çš„å®‰å…¨å¯¹é½ç‰ˆæœ¬ã€‚ä¸ºäº†è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«15kä¸ªå®‰å…¨æ„ŸçŸ¥æ¨ç†è½¨è¿¹çš„æ•°æ®é›†ï¼Œè¿™äº›è½¨è¿¹ç”±DeepSeek-R1åœ¨æ˜ç¡®çš„é¢„æœŸæ‹’ç»è¡Œä¸ºæŒ‡ä»¤ä¸‹ç”Ÿæˆã€‚å®šé‡å®éªŒå’Œå®šæ€§æ¡ˆä¾‹ç ”ç©¶éƒ½è¯æ˜äº†æ¨¡å‹çš„æ”¹è¿›ï¼Œä½“ç°åœ¨å®ƒä»¬å¯¹æœ‰å®³æŸ¥è¯¢å’Œè¶Šç‹±æ”»å‡»çš„é˜²æŠ¤å®‰å…¨æ …æ ä¸Šã€‚é‡è¦çš„æ˜¯ï¼Œä¸ä»¥å¾€ç»å¸¸æŸå®³æ¨ç†æ€§èƒ½çš„å®‰å…¨å¯¹é½æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¿æŒè®­ç»ƒæ•°æ®åœ¨åŸå§‹ç”Ÿæˆæ•°æ®çš„åˆ†å¸ƒèŒƒå›´å†…ï¼Œä»è€Œä¿ç•™äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚RealSafe-R1çš„æ¨¡å‹æƒé‡å·²å¼€æºï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/RealSafe">https://huggingface.co/RealSafe</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10081v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œä½†å¼€æºæ¨¡å‹çš„å®‰å…¨é—®é¢˜ä¹Ÿæ—¥ç›Šå‡¸æ˜¾ï¼Œä¾‹å¦‚å¯èƒ½ä¼šå›åº”æ¶æ„æŸ¥è¯¢ã€‚æœ¬æ–‡ä»‹ç»äº†RealSafe-R1æ¨¡å‹ï¼Œå®ƒæ˜¯åŸºäºDeepSeek-R1çš„å®‰å…¨å¯¹é½ç‰ˆæœ¬ã€‚é€šè¿‡æ„å»ºåŒ…å«15kå®‰å…¨æ„ŸçŸ¥æ¨ç†è½¨è¿¹çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨æ‹’ç»ä¸å½“è¯·æ±‚æ—¶çš„é¢„æœŸè¡Œä¸ºæ›´åŠ æ˜ç¡®ã€‚å®éªŒå’Œæ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼ŒRealSafe-R1æ¨¡å‹åœ¨å®‰å…¨é˜²æŠ¤æ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼Œèƒ½å¤ŸæŠµå¾¡æœ‰å®³æŸ¥è¯¢å’Œè¶Šç‹±æ”»å‡»ã€‚ä¸å…¶ä»–å®‰å…¨å¯¹é½æ–¹æ³•ä¸åŒï¼ŒRealSafe-R1åœ¨ä¿æŒåŸå§‹æ•°æ®åˆ†å¸ƒç”Ÿæˆçš„åŒæ—¶ï¼Œä¿ç•™äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚RealSafe-R1æ¨¡å‹æƒé‡å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†å­˜åœ¨å®‰å…¨é—®é¢˜ï¼Œå¦‚å“åº”æ¶æ„æŸ¥è¯¢ã€‚</li>
<li>RealSafe-R1æ˜¯DeepSeek-R1çš„å®‰å…¨å¯¹é½ç‰ˆæœ¬ï¼Œæ—¨åœ¨è§£å†³å®‰å…¨é—®é¢˜ã€‚</li>
<li>RealSafe-R1é€šè¿‡æ„å»ºåŒ…å«å®‰å…¨æ„ŸçŸ¥æ¨ç†è½¨è¿¹çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«æ˜ç¡®çš„æ‹’ç»ä¸å½“è¯·æ±‚çš„é¢„æœŸè¡Œä¸ºã€‚</li>
<li>RealSafe-R1åœ¨å®‰å…¨é˜²æŠ¤æ–¹é¢æœ‰æ‰€æå‡ï¼Œèƒ½å¤ŸæŠµå¾¡æœ‰å®³æŸ¥è¯¢å’Œè¶Šç‹±æ”»å‡»ã€‚</li>
<li>ä¸å…¶ä»–å®‰å…¨å¯¹é½æ–¹æ³•ä¸åŒï¼ŒRealSafe-R1åœ¨ä¿æŒæ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®åœ¨åŸå§‹æ•°æ®åˆ†å¸ƒç”ŸæˆèŒƒå›´å†…ã€‚</li>
<li>RealSafe-R1æ¨¡å‹æƒé‡å·²å¼€æºï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-984718c5c86c420c8fbc897b7aa65120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e1a19d33bc07c0403f8bfd1fb5e70cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c49e2eadc386e40cdf2ae399c4f47d43.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GenTe-Generative-Real-world-Terrains-for-General-Legged-Robot-Locomotion-Control"><a href="#GenTe-Generative-Real-world-Terrains-for-General-Legged-Robot-Locomotion-Control" class="headerlink" title="GenTe: Generative Real-world Terrains for General Legged Robot   Locomotion Control"></a>GenTe: Generative Real-world Terrains for General Legged Robot   Locomotion Control</h2><p><strong>Authors:Hanwen Wan, Mengkang Li, Donghao Wu, Yebin Zhong, Yixuan Deng, Zhenglong Sun, Xiaoqiang Ji</strong></p>
<p>Developing bipedal robots capable of traversing diverse real-world terrains presents a fundamental robotics challenge, as existing methods using predefined height maps and static environments fail to address the complexity of unstructured landscapes. To bridge this gap, we propose GenTe, a framework for generating physically realistic and adaptable terrains to train generalizable locomotion policies. GenTe constructs an atomic terrain library that includes both geometric and physical terrains, enabling curriculum training for reinforcement learning-based locomotion policies. By leveraging function-calling techniques and reasoning capabilities of Vision-Language Models (VLMs), GenTe generates complex, contextually relevant terrains from textual and graphical inputs. The framework introduces realistic force modeling for terrain interactions, capturing effects such as soil sinkage and hydrodynamic resistance. To the best of our knowledge, GenTe is the first framework that systemically generates simulation environments for legged robot locomotion control. Additionally, we introduce a benchmark of 100 generated terrains. Experiments demonstrate improved generalization and robustness in bipedal robot locomotion. </p>
<blockquote>
<p>å¼€å‘èƒ½å¤Ÿéå†å„ç§ç°å®åœ°å½¢ç¯å¢ƒçš„åŒè¶³æœºå™¨äººæ˜¯ä¸€ä¸ªåŸºæœ¬çš„æœºå™¨äººæŠ€æœ¯æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰çš„ä½¿ç”¨é¢„å®šä¹‰é«˜åº¦å›¾å’Œé™æ€ç¯å¢ƒçš„æ–¹æ³•æ— æ³•è§£å†³éç»“æ„åŒ–åœ°å½¢çš„å¤æ‚æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†GenTeï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆç‰©ç†ç°å®ä¸”é€‚åº”æ€§å¼ºçš„åœ°å½¢ä»¥è®­ç»ƒé€šç”¨è¿åŠ¨ç­–ç•¥çš„æ¡†æ¶ã€‚GenTeæ„å»ºäº†ä¸€ä¸ªåŸå­åœ°å½¢åº“ï¼Œå…¶ä¸­åŒ…æ‹¬å‡ ä½•åœ°å½¢å’Œç‰©ç†åœ°å½¢ï¼Œä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„è¿åŠ¨ç­–ç•¥æä¾›è¯¾ç¨‹è®­ç»ƒã€‚GenTeå€ŸåŠ©å‡½æ•°è°ƒç”¨æŠ€æœ¯å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä»æ–‡æœ¬å’Œå›¾å½¢è¾“å…¥ä¸­ç”Ÿæˆå¤æ‚ã€ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„åœ°å½¢ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç°å®çš„åŠ›é‡å»ºæ¨¡æ¥è¿›è¡Œåœ°å½¢äº¤äº’ï¼Œæ•æ‰åœŸå£¤æ²‰é™å’Œæ°´åŠ¨åŠ›é˜»åŠ›ç­‰æ•ˆæœã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒGenTeæ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿåœ°ç”Ÿæˆç”¨äºè…¿å¼æœºå™¨äººè¿åŠ¨æ§åˆ¶çš„æ¨¡æ‹Ÿç¯å¢ƒçš„æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†100ä¸ªç”Ÿæˆåœ°å½¢çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼ŒåŒè¶³æœºå™¨äººçš„è¿åŠ¨å…·æœ‰æ›´å¥½çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGenTeçš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆç‰©ç†ç°å®æ€§å¼ºä¸”èƒ½é€‚åº”å¤æ‚åœ°å½¢å˜åŒ–çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œç”¨äºè®­ç»ƒå¯æ³›åŒ–çš„ä¸¤è¶³æœºå™¨äººè¿åŠ¨ç­–ç•¥ã€‚GenTeæ„å»ºäº†ä¸€ä¸ªåŒ…å«å‡ ä½•å’Œç‰©ç†åœ°å½¢çš„åŸå­åœ°å½¢åº“ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè¯¾ç¨‹è®­ç»ƒã€‚é€šè¿‡è°ƒç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„åŠŸèƒ½å’Œæ¨ç†èƒ½åŠ›ï¼ŒGenTeå¯ä»æ–‡æœ¬å’Œå›¾å½¢è¾“å…¥ç”Ÿæˆå¤æ‚ä¸”ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„åœ°å½¢ã€‚æ¡†æ¶å¼•å…¥äº†çœŸå®çš„åŠ›é‡æ¨¡å‹æ¥æ¨¡æ‹Ÿåœ°å½¢äº¤äº’çš„å„ç§æ•ˆæœï¼Œå¦‚åœŸå£¤æ²‰é™·å’Œæ°´åŠ¨åŠ›é˜»åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒGenTeæ˜¯é¦–ä¸ªç³»ç»Ÿåœ°ç”Ÿæˆæ¨¡æ‹Ÿç¯å¢ƒä»¥è®­ç»ƒè¶³å¼æœºå™¨äººè¿åŠ¨æ§åˆ¶çš„æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æé«˜ä¸¤è¶³æœºå™¨äººåœ¨åœ°å½¢å˜åŒ–ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GenTeæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆç‰©ç†ç°å®æ€§å¼ºåœ°å½¢æ¨¡æ‹Ÿç¯å¢ƒçš„æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒæœºå™¨äººè¿åŠ¨ç­–ç•¥ã€‚</li>
<li>GenTeæ„å»ºäº†ä¸€ä¸ªåŒ…å«å‡ ä½•å’Œç‰©ç†åœ°å½¢çš„åŸå­åœ°å½¢åº“ï¼Œä»¥é€‚åº”å¤æ‚åœ°å½¢å˜åŒ–ã€‚</li>
<li>GenTeåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè¯¾ç¨‹è®­ç»ƒï¼Œä»¥ä¼˜åŒ–æœºå™¨äººè¿åŠ¨ç­–ç•¥ã€‚</li>
<li>GenTeé€šè¿‡è°ƒç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„åŠŸèƒ½å’Œæ¨ç†èƒ½åŠ›ï¼Œç”Ÿæˆä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„åœ°å½¢ã€‚</li>
<li>GenTeå¼•å…¥äº†çœŸå®çš„åŠ›é‡æ¨¡å‹æ¥æ¨¡æ‹Ÿåœ°å½¢äº¤äº’çš„å„ç§æ•ˆæœï¼Œå¦‚åœŸå£¤æ²‰é™·å’Œæ°´åŠ¨åŠ›é˜»åŠ›ã€‚</li>
<li>GenTeæ˜¯é¦–ä¸ªç³»ç»Ÿåœ°ç”Ÿæˆæ¨¡æ‹Ÿç¯å¢ƒä»¥è®­ç»ƒè¶³å¼æœºå™¨äººè¿åŠ¨æ§åˆ¶çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-32d1170458c6813c8955fa1761ee2fbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6690ddb340b35d8df9fb6f4a87ec848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7be5128c23346a0a31b113c576ced923.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9373277fdfb8b26baf2575712d39b565.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7e6dc5afa8cae907e49ee73ccdbd68a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Reasoning-Models-Can-Be-Effective-Without-Thinking"><a href="#Reasoning-Models-Can-Be-Effective-Without-Thinking" class="headerlink" title="Reasoning Models Can Be Effective Without Thinking"></a>Reasoning Models Can Be Effective Without Thinking</h2><p><strong>Authors:Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, Matei Zaharia</strong></p>
<p>Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasetsâ€“including mathematical problem solving, formal theorem proving, and codingâ€“especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling. </p>
<blockquote>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡åŒ…å«æ˜ç¡®çš„ã€å†—é•¿çš„æ€è€ƒè¿‡ç¨‹ä½œä¸ºç”Ÿæˆçš„ä¸€éƒ¨åˆ†ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è´¨ç–‘è¿™ç§æ˜ç¡®çš„æ€è€ƒè¿‡ç¨‹æ˜¯å¦å¿…è¦ã€‚ä½¿ç”¨æœ€å…ˆè¿›çš„DeepSeek-R1-Distill-Qwenæ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡ç®€å•æç¤ºç»•è¿‡æ€è€ƒè¿‡ç¨‹ï¼Œç§°ä¸ºâ€œæ— æ€è€ƒâ€ï¼ˆNoThinkingï¼‰ï¼Œç«Ÿç„¶èƒ½å–å¾—æƒŠäººçš„æ•ˆæœã€‚åœ¨æ§åˆ¶ä»¤ç‰Œæ•°é‡çš„æƒ…å†µä¸‹ï¼Œæ— æ€è€ƒåœ¨ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†æ•°æ®é›†ä¸Šä¼˜äºæ€è€ƒï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬æ•°å­¦é—®é¢˜è§£å†³ã€å½¢å¼åŒ–å®šç†è¯æ˜å’Œç¼–ç ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½é¢„ç®—ç¯å¢ƒä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œä¾‹å¦‚åœ¨ACM 23ä¸Š51.3å¯¹28.9ï¼Œä½¿ç”¨700ä¸ªä»¤ç‰Œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— æ€è€ƒçš„æ€§èƒ½éšç€kçš„å¢åŠ è€Œæ›´å…·ç«äº‰åŠ›ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨æ— æ€è€ƒç‹¬ç«‹ç”ŸæˆNä¸ªè¾“å‡ºå¹¶è¿›è¡Œèšåˆçš„å¹¶è¡Œç¼©æ”¾æ–¹æ³•æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚åœ¨èšåˆæ—¶ï¼Œæˆ‘ä»¬å¯ä½¿ç”¨ç‰¹å®šçš„ä»»åŠ¡éªŒè¯å™¨ï¼Œå¦‚æœä¸å¯ç”¨ï¼Œåˆ™é‡‡ç”¨ç®€å•çš„æœ€ä½³Nç­–ç•¥ï¼Œå¦‚åŸºäºä¿¡å¿ƒçš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç±»ä¼¼å»¶è¿Ÿçš„æƒ…å†µä¸‹ä¼˜äºä¸€ç³»åˆ—åŸºçº¿ï¼Œå¹¶ä¸”ä¸æ›´é•¿å»¶è¿Ÿçš„æ€è€ƒæ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ˆé•¿è¾¾9å€ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶é‡æ–°è€ƒè™‘äº†å†—é•¿æ€è€ƒè¿‡ç¨‹çš„å¿…è¦æ€§ï¼ŒåŒæ—¶ä¸ºåœ¨ä½é¢„ç®—ç¯å¢ƒæˆ–ä½å»¶è¿Ÿæƒ…å†µä¸‹å®ç°å¼ºå¤§çš„æ¨ç†æ€§èƒ½æˆ–ä½¿ç”¨å¹¶è¡Œç¼©æ”¾æä¾›äº†ç«äº‰æ€§çš„å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09858v1">PDF</a> 33 pages, 7 main figures, 2 tables</p>
<p><strong>Summary</strong><br>     æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¼•å…¥æ˜ç¡®çš„æ€è€ƒè¿‡ç¨‹æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œä½†æœ¬æ–‡å¯¹æ­¤æå‡ºè´¨ç–‘ã€‚é€šè¿‡æœ€å…ˆè¿›DeepSeek-R1-Distill-Qwenæ¨¡å‹å‘ç°ï¼Œé€šè¿‡ç®€å•æç¤ºç»•è¿‡æ€è€ƒè¿‡ç¨‹ä¹Ÿèƒ½å–å¾—æƒŠäººæ•ˆæœã€‚åœ¨ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†æ•°æ®é›†ä¸Šï¼Œæ— æ€è€ƒæ–¹å¼åœ¨æ§åˆ¶ä»¤ç‰Œæ•°é‡çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè¶…è¶Šæ€è€ƒçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨ä½é¢„ç®—ç¯å¢ƒä¸‹ï¼Œè¡¨ç°å°¤ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ¢è®¨äº†å¹¶è¡Œæ‰©å±•æ–¹æ³•åœ¨æ— æ€è€ƒç”Ÿæˆè¾“å‡ºç‹¬ç«‹å¹¶è¡Œèšåˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ä»»åŠ¡ç‰¹å®šéªŒè¯å™¨æˆ–ç®€å•æœ€ä½³Nç­–ç•¥è¿›è¡Œèšåˆã€‚è¯¥æ–¹æ³•ä¼˜äºä½¿ç”¨æ€è€ƒçš„åŸºçº¿ï¼Œä¸”ä¸å»¶è¿Ÿæ˜¾è‘—çš„æ€è€ƒç›¸å½“ï¼ˆæœ€é•¿å¯è¾¾9å€ï¼‰ã€‚æœ¬æ–‡é‡æ–°è€ƒè™‘äº†é•¿æ—¶é—´æ€è€ƒè¿‡ç¨‹çš„å¿…è¦æ€§ï¼ŒåŒæ—¶ä¸ºä½é¢„ç®—ç¯å¢ƒæˆ–ä½å»¶è¿Ÿä¸‹å®ç°å¼ºå¤§çš„æ¨ç†æ€§èƒ½æä¾›äº†ç«äº‰å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹å¼•å…¥æ˜ç¡®æ€è€ƒè¿‡ç¨‹æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œä½†æœ¬æ–‡å¯¹æ­¤æå‡ºè´¨ç–‘ã€‚</li>
<li>é€šè¿‡ç®€å•æç¤ºç»•è¿‡æ€è€ƒè¿‡ç¨‹å¯ä»¥å–å¾—æœ‰æ•ˆç»“æœã€‚</li>
<li>åœ¨ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†æ•°æ®é›†ä¸Šï¼Œæ— æ€è€ƒæ–¹å¼åœ¨æ§åˆ¶ä»¤ç‰Œæ•°é‡çš„æƒ…å†µä¸‹è¶…è¶Šæ€è€ƒã€‚</li>
<li>æ— æ€è€ƒæ–¹å¼åœ¨ä½é¢„ç®—ç¯å¢ƒä¸‹è¡¨ç°æ›´çªå‡ºã€‚</li>
<li>å¹¶è¡Œæ‰©å±•æ–¹æ³•åœ¨æ— æ€è€ƒç”Ÿæˆè¾“å‡ºç‹¬ç«‹å¹¶è¡Œèšåˆæ–¹é¢æœ‰æ•ˆã€‚</li>
<li>ä½¿ç”¨ä»»åŠ¡ç‰¹å®šéªŒè¯å™¨æˆ–ç®€å•æœ€ä½³Nç­–ç•¥è¿›è¡Œèšåˆå¯æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-97937b55af26f90dd480c1a52f06bb80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d981f623949febea6ab57f1b625bfc8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3b7aa8d2db00956400b650c09d95c77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e4afd11f7c4758e4e104c80a8f4b63d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RAKG-Document-level-Retrieval-Augmented-Knowledge-Graph-Construction"><a href="#RAKG-Document-level-Retrieval-Augmented-Knowledge-Graph-Construction" class="headerlink" title="RAKG:Document-level Retrieval Augmented Knowledge Graph Construction"></a>RAKG:Document-level Retrieval Augmented Knowledge Graph Construction</h2><p><strong>Authors:Hairong Zhang, Jiaheng Si, Guohang Yan, Boyuan Qi, Pinlong Cai, Song Mao, Ding Wang, Botian Shi</strong></p>
<p>With the rise of knowledge graph based retrieval-augmented generation (RAG) techniques such as GraphRAG and Pike-RAG, the role of knowledge graphs in enhancing the reasoning capabilities of large language models (LLMs) has become increasingly prominent. However, traditional Knowledge Graph Construction (KGC) methods face challenges like complex entity disambiguation, rigid schema definition, and insufficient cross-document knowledge integration. This paper focuses on the task of automatic document-level knowledge graph construction. It proposes the Document-level Retrieval Augmented Knowledge Graph Construction (RAKG) framework. RAKG extracts pre-entities from text chunks and utilizes these pre-entities as queries for RAG, effectively addressing the issue of long-context forgetting in LLMs and reducing the complexity of Coreference Resolution. In contrast to conventional KGC methods, RAKG more effectively captures global information and the interconnections among disparate nodes, thereby enhancing the overall performance of the model. Additionally, we transfer the RAG evaluation framework to the KGC field and filter and evaluate the generated knowledge graphs, thereby avoiding incorrectly generated entities and relationships caused by hallucinations in LLMs. We further developed the MINE dataset by constructing standard knowledge graphs for each article and experimentally validated the performance of RAKG. The results show that RAKG achieves an accuracy of 95.91 % on the MINE dataset, a 6.2 % point improvement over the current best baseline, GraphRAG (89.71 %). The code is available at <a target="_blank" rel="noopener" href="https://github.com/LMMApplication/RAKG">https://github.com/LMMApplication/RAKG</a>. </p>
<blockquote>
<p>éšç€åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼ˆå¦‚GraphRAGå’ŒPike-RAGï¼‰çš„å…´èµ·ï¼ŒçŸ¥è¯†å›¾è°±åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä½œç”¨æ—¥ç›Šçªå‡ºã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„çŸ¥è¯†å›¾è°±æ„å»ºï¼ˆKGCï¼‰æ–¹æ³•é¢ä¸´å¤æ‚çš„å®ä½“æ¶ˆæ­§ã€ä¸¥æ ¼çš„æ¨¡å¼å®šä¹‰ä»¥åŠè·¨æ–‡æ¡£çŸ¥è¯†æ•´åˆä¸è¶³ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡ä¸“æ³¨äºè‡ªåŠ¨æ–‡æ¡£çº§çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡ï¼Œæå‡ºäº†æ–‡æ¡£çº§æ£€ç´¢å¢å¼ºçŸ¥è¯†å›¾è°±æ„å»ºï¼ˆRAKGï¼‰æ¡†æ¶ã€‚RAKGä»æ–‡æœ¬å—ä¸­æå–é¢„å®ä½“ï¼Œå¹¶åˆ©ç”¨è¿™äº›é¢„å®ä½“ä½œä¸ºRAGçš„æŸ¥è¯¢ï¼Œæœ‰æ•ˆè§£å†³LLMä¸­çš„é•¿ä¸Šä¸‹æ–‡é—å¿˜é—®é¢˜ï¼Œé™ä½æ ¸å¿ƒå‚è€ƒè§£æçš„å¤æ‚æ€§ã€‚ä¸ä¼ ç»Ÿçš„KGCæ–¹æ³•ç›¸æ¯”ï¼ŒRAKGæ›´æœ‰æ•ˆåœ°æ•è·å…¨å±€ä¿¡æ¯å’Œä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„äº’è¿ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†RAGè¯„ä¼°æ¡†æ¶è½¬ç§»åˆ°KGCé¢†åŸŸï¼Œå¯¹ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±è¿›è¡Œè¿‡æ»¤å’Œè¯„ä¼°ï¼Œä»è€Œé¿å…LLMä¸­å› å¹»è§‰è€Œäº§ç”Ÿçš„é”™è¯¯å®ä½“å’Œå…³ç³»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ä¸ºæ¯ç¯‡æ–‡ç« æ„å»ºæ ‡å‡†çŸ¥è¯†å›¾è°±æ¥å¼€å‘MINEæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†RAKGçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒRAKGåœ¨MINEæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°95.91%ï¼Œæ¯”å½“å‰æœ€ä½³åŸºçº¿GraphRAGï¼ˆ89.71%ï¼‰é«˜å‡º6.2ä¸ªç™¾åˆ†ç‚¹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LMMApplication/RAKG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LMMApplication/RAKGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09823v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æ¡£çº§åˆ«çš„çŸ¥è¯†å›¾è°±æ„å»ºæ¡†æ¶RAKGã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•ç›¸æ¯”ï¼ŒRAKGèƒ½å¤Ÿä»æ–‡æœ¬ä¸­æå–é¢„å®ä½“ï¼Œå¹¶åˆ©ç”¨è¿™äº›é¢„å®ä½“ä½œä¸ºæŸ¥è¯¢å¢å¼ºçŸ¥è¯†å›¾è°±çš„æ„å»ºï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒRAKGèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•è·å…¨å±€ä¿¡æ¯å’Œä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„äº’è¿ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†å¯¹RAKGæ€§èƒ½è¿›è¡Œè¯„ä¼°çš„æ–¹æ³•å’Œç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ä¼ ç»ŸçŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•é¢ä¸´å¤æ‚å®ä½“æ¶ˆæ­§ã€åˆšæ€§æ¨¡å¼å®šä¹‰å’Œè·¨æ–‡æ¡£çŸ¥è¯†æ•´åˆä¸è¶³ç­‰æŒ‘æˆ˜ã€‚</li>
<li>RAKGæ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨æ„å»ºæ–‡æ¡£çº§åˆ«çš„çŸ¥è¯†å›¾è°±ï¼Œæå–æ–‡æœ¬ä¸­çš„é¢„å®ä½“å¹¶ä½œä¸ºæŸ¥è¯¢å¢å¼ºçŸ¥è¯†å›¾è°±çš„æ„å»ºã€‚</li>
<li>RAKGè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é•¿ä¸Šä¸‹æ–‡é—å¿˜é—®é¢˜ï¼Œå¹¶é™ä½äº†æ ¸å¿ƒè§£æçš„å¤æ‚æ€§ã€‚</li>
<li>RAKGæ›´æœ‰æ•ˆåœ°æ•è·å…¨å±€ä¿¡æ¯å’Œä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„äº’è¿ï¼Œæé«˜äº†æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚</li>
<li>å¼€å‘äº†é’ˆå¯¹çŸ¥è¯†å›¾è°±æ„å»ºçš„RAGè¯„ä¼°æ¡†æ¶ï¼Œä»¥é¿å…å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰å¯¼è‡´çš„é”™è¯¯å®ä½“å’Œå…³ç³»ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6999b8bc9b61070fead2280ce2c22c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b2d58a404f690b81da5d5873d6f2f3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b26eadbb58b775ed22897ff7345f79e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3379dd926d64a8611ea9f43ed341605d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ee2ee818f93a460b907650a82b15577.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Training-Small-Reasoning-LLMs-with-Cognitive-Preference-Alignment"><a href="#Training-Small-Reasoning-LLMs-with-Cognitive-Preference-Alignment" class="headerlink" title="Training Small Reasoning LLMs with Cognitive Preference Alignment"></a>Training Small Reasoning LLMs with Cognitive Preference Alignment</h2><p><strong>Authors:Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang</strong></p>
<p>The reasoning capabilities of large language models (LLMs), such as OpenAIâ€™s o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚OpenAIçš„o1å’ŒDeepSeek-R1ï¼‰çš„æ¨ç†èƒ½åŠ›é€šè¿‡æ·±åº¦æ€è€ƒå–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›å¢å¼ºåŠŸèƒ½éœ€è¦å·¨å¤§çš„èµ„æºéœ€æ±‚ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æ¢ç´¢ä½¿ç”¨è¾ƒå°‘å‚æ•°è®­ç»ƒæœ‰æ•ˆæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç­–ç•¥ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºå°å‹æ¨¡å‹çš„å®¹é‡å’Œè®¤çŸ¥è½¨è¿¹ä¸å¤§å‹æ¨¡å‹ä¸åŒã€‚å› æ­¤ï¼Œç›´æ¥ä»å¤§å‹LLMè’¸é¦æ€ç»´é“¾ï¼ˆCoTï¼‰ç»“æœåˆ°å°å‹LLMæœ‰æ—¶ä¼šæ— æ•ˆï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„æ³¨é‡Šæ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºæ‰¹è¯„åæ€éªŒè¯ï¼ˆCRVï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒå°å‹ä½†åŠŸèƒ½å¼ºå¤§çš„æ¨ç†LLMã€‚æˆ‘ä»¬çš„CRVæ¡†æ¶åŒ…å«å¤šä¸ªLLMä»£ç†ï¼Œæ¯ä¸ªä»£ç†éƒ½æ“…é•¿ç‹¬ç‰¹çš„èƒ½åŠ›ï¼šï¼ˆiï¼‰æ ¹æ®å°å‹æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›å¯¹æ€ç»´é“¾è¿›è¡Œæ‰¹è¯„ï¼Œï¼ˆiiï¼‰åŸºäºæ‰¹è¯„é‡æ–°æ€è€ƒå’Œç»†åŒ–è¿™äº›æ€ç»´é“¾ï¼Œä»¥åŠï¼ˆiiiï¼‰éªŒè¯ç»†åŒ–ç»“æœçš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†è®¤çŸ¥åå¥½ä¼˜åŒ–ï¼ˆCogPOï¼‰ç®—æ³•ï¼Œé€šè¿‡è°ƒæ•´å°å‹æ¨¡å‹çš„æ€ç»´æ¥æé«˜å…¶æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶ä¸å®ƒä»¬çš„è®¤çŸ¥èƒ½åŠ›ç›¸åŒ¹é…ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å…¨é¢è¯„ä¼°è¯æ˜äº†CRVå’ŒCogPOçš„æœ‰æ•ˆæ€§ï¼Œä¸å…¶ä»–è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒå¤§ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09802v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œä½†éœ€è¦å¤§é‡çš„èµ„æºæ¥è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ã€‚ä¸ºäº†é€‚åº”å°å‹æ¨¡å‹çš„ç‹¬ç‰¹éœ€æ±‚å’Œèƒ½åŠ›å·®å¼‚ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„è®­ç»ƒæ¡†æ¶CRVï¼Œå®ƒåŒ…å«äº†æ‰¹åˆ¤ã€å†æ€è€ƒå’ŒéªŒè¯ä¸‰ä¸ªå…³é”®æ­¥éª¤ã€‚åŒæ—¶ï¼Œè¿˜æå‡ºäº†è®¤çŸ¥åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆCogPOï¼‰ï¼Œæ—¨åœ¨æé«˜å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºCRVå’ŒCogPOè®­ç»ƒæ³•çš„æ•ˆç‡æ˜æ˜¾ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†éœ€è¦å·¨å¤§çš„èµ„æºæ¥è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
<li>ç›´æ¥å°†å¤§å‹æ¨¡å‹çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç»“æœè’¸é¦åˆ°å°å‹æ¨¡å‹å¯èƒ½å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆã€‚</li>
<li>æå‡ºäº†æ–°å‹è®­ç»ƒæ¡†æ¶CRVï¼ŒåŒ…æ‹¬æ‰¹åˆ¤ã€å†æ€è€ƒå’ŒéªŒè¯ä¸‰ä¸ªæ­¥éª¤ï¼Œæ—¨åœ¨è®­ç»ƒå°å‹æ¨ç†æ¨¡å‹ã€‚</li>
<li>CRVæ¡†æ¶åŒ…å«å¤šä¸ªLLMä»£ç†ï¼Œæ¯ä¸ªä»£ç†éƒ½å…·å¤‡ç‹¬ç‰¹çš„ä¸“ä¸šèƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†è®¤çŸ¥åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆCogPOï¼‰ï¼Œç”¨äºå¢å¼ºå°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æ€ç»´ä¸æ¨¡å‹èƒ½åŠ›ç›¸åŒ¹é…ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6360f001fad463c8266190447e6e79dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-006442db0be32ea9d7667445bba60e68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9c004d7e62cc291d182cdf8e0775aed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01d1c2d50fcbec4fa7c71f1aa0b03c30.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reasoning-Court-Combining-Reasoning-Action-and-Judgment-for-Multi-Hop-Reasoning"><a href="#Reasoning-Court-Combining-Reasoning-Action-and-Judgment-for-Multi-Hop-Reasoning" class="headerlink" title="Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop   Reasoning"></a>Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop   Reasoning</h2><p><strong>Authors:Jingtian Wu, Claire Cardie</strong></p>
<p>While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods address these issues through retrieval-based techniques (grounding reasoning in external evidence), reasoning-based approaches (enhancing coherence via improved prompting), or hybrid strategies combining both elements. One prominent hybrid method, ReAct, has outperformed purely retrieval-based or reasoning-based approaches; however, it lacks internal verification of intermediate reasoning steps, allowing potential errors to propagate through complex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a novel framework that extends iterative reasoning-and-retrieval methods, such as ReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to independently evaluate multiple candidate answers and their associated reasoning generated by separate LLM agents. The judge is asked to select the answer that it considers the most factually grounded and logically coherent based on the presented reasoning and evidence, or synthesizes a new answer using available evidence and its pre-trained knowledge if all candidates are inadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA, MuSiQue) and fact-verification (FEVER) demonstrate that RC consistently outperforms state-of-the-art few-shot prompting methods without task-specific fine-tuning. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”å’Œäº‹å®éªŒè¯ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ƒä»¬ä»é¢ä¸´ç€è™šæ„å’Œæ¨ç†é”™è¯¯çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ•´åˆå¤šä¸ªä¿¡æ¯æºçš„å¤šè·³ä»»åŠ¡ä¸­ã€‚å½“å‰çš„æ–¹æ³•é€šè¿‡åŸºäºæ£€ç´¢çš„æŠ€æœ¯ï¼ˆå°†æ¨ç†åŸºäºå¤–éƒ¨è¯æ®ï¼‰ã€åŸºäºæ¨ç†çš„æ–¹æ³•ï¼ˆé€šè¿‡æ”¹è¿›æç¤ºæ¥æé«˜è¿è´¯æ€§ï¼‰æˆ–ç»“åˆè¿™ä¸¤ä¸ªè¦ç´ çš„æ··åˆç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ä¸€ç§çªå‡ºçš„æ··åˆæ–¹æ³•ReActå·²ç»è¶…è¶Šäº†çº¯ç²¹çš„åŸºäºæ£€ç´¢æˆ–åŸºäºæ¨ç†çš„æ–¹æ³•ï¼›ç„¶è€Œï¼Œå®ƒç¼ºä¹å¯¹ä¸­é—´æ¨ç†æ­¥éª¤çš„å†…éƒ¨éªŒè¯ï¼Œå…è®¸æ½œåœ¨çš„é”™è¯¯åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­ä¼ æ’­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Reasoning Courtï¼ˆRCï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒæ‰©å±•äº†å¦‚ReActä¹‹ç±»çš„è¿­ä»£æ¨ç†å’Œæ£€ç´¢æ–¹æ³•ï¼Œå¹¶é…å¤‡äº†ä¸€åä¸“èŒçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ³•å®˜ã€‚ä¸åŒäºReActï¼ŒRCé›‡ä½£è¿™ä¸ªæ³•å®˜æ¥ç‹¬ç«‹è¯„ä¼°å¤šä¸ªå€™é€‰ç­”æ¡ˆåŠå…¶ç›¸å…³çš„æ¨ç†ï¼Œè¿™äº›ç­”æ¡ˆå’Œæ¨ç†æ˜¯ç”±å•ç‹¬çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä»£ç†ç”Ÿæˆçš„ã€‚æ³•å®˜æ ¹æ®æä¾›çš„æ¨ç†å’Œè¯æ®ï¼Œé€‰æ‹©å®ƒè®¤ä¸ºæœ€åŸºäºäº‹å®å’Œé€»è¾‘è¿è´¯çš„ç­”æ¡ˆï¼Œå¦‚æœæ‰€æœ‰å€™é€‰ç­”æ¡ˆéƒ½ä¸è¶³ã€æœ‰ç¼ºé™·æˆ–æ— æ•ˆï¼Œå®ƒè¿˜ä¼šåˆ©ç”¨å¯ç”¨è¯æ®å’Œå…¶é¢„è®­ç»ƒçŸ¥è¯†åˆæˆæ–°çš„ç­”æ¡ˆã€‚åœ¨Multi-hopåŸºå‡†æµ‹è¯•ï¼ˆHotpotQAã€MuSiQueï¼‰å’Œäº‹å®éªŒè¯ï¼ˆFEVERï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒRCå§‹ç»ˆä¼˜äºæ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æœ€å…ˆè¿›çš„å°æ ·æœ¬æç¤ºæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09781v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®ç­”å’Œäº‹å®æ ¸æŸ¥ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦æ•´åˆå¤šä¸ªä¿¡æ¯æºçš„å¤šè·³ä»»åŠ¡ä¸­ä»å­˜åœ¨å¹»è§‰å’Œæ¨ç†é”™è¯¯ã€‚å½“å‰æ–¹æ³•é€šè¿‡æ£€ç´¢åŸºäºæŠ€æœ¯ã€æ¨ç†åŸºäºæ–¹æ³•å’Œæ··åˆç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”Reasoning Courtï¼ˆRCï¼‰ï¼Œå®ƒé€šè¿‡å¢åŠ ä¸€ä¸ªç‹¬ç«‹çš„LLMæ³•å®˜æ¥æ‰©å±•è¿­ä»£æ¨ç†å’Œæ£€ç´¢æ–¹æ³•ï¼Œå¦‚ReActã€‚æ³•å®˜è´Ÿè´£ç‹¬ç«‹è¯„ä¼°å¤šä¸ªå€™é€‰ç­”æ¡ˆåŠå…¶ç›¸å…³æ¨ç†ï¼Œå¹¶é€‰æ‹©å…¶è®¤ä¸ºæœ€åŸºäºäº‹å®å’Œé€»è¾‘è¿è´¯çš„ç­”æ¡ˆã€‚å¦‚æœæ‰€æœ‰å€™é€‰ç­”æ¡ˆéƒ½ä¸è¶³ã€æœ‰ç¼ºé™·æˆ–æ— æ•ˆï¼Œæ³•å®˜è¿˜ä¼šåˆ©ç”¨å¯ç”¨è¯æ®å’Œé¢„è®­ç»ƒçŸ¥è¯†åˆæˆæ–°çš„ç­”æ¡ˆã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒRCåœ¨å°‘æ ·æœ¬æç¤ºæ–¹æ³•ä¸Šä¸€è‡´åœ°ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œä¸”æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè·³ä»»åŠ¡ä¸­ä»å­˜åœ¨å¹»è§‰å’Œæ¨ç†é”™è¯¯ã€‚</li>
<li>å½“å‰è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•åŒ…æ‹¬æ£€ç´¢åŸºäºæŠ€æœ¯ã€æ¨ç†åŸºäºæ–¹æ³•å’Œæ··åˆç­–ç•¥ã€‚</li>
<li>Reasoning Courtï¼ˆRCï¼‰æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡å¢åŠ ä¸€ä¸ªç‹¬ç«‹çš„LLMæ³•å®˜æ¥æ‰©å±•è¿­ä»£æ¨ç†å’Œæ£€ç´¢æ–¹æ³•ã€‚</li>
<li>æ³•å®˜è´Ÿè´£ç‹¬ç«‹è¯„ä¼°å¤šä¸ªå€™é€‰ç­”æ¡ˆåŠå…¶ç›¸å…³æ¨ç†ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„ç­”æ¡ˆã€‚</li>
<li>å¦‚æœå€™é€‰ç­”æ¡ˆä¸è¶³ã€æœ‰ç¼ºé™·æˆ–æ— æ•ˆï¼Œæ³•å®˜ä¼šåˆ©ç”¨è¯æ®å’Œé¢„è®­ç»ƒçŸ¥è¯†åˆæˆæ–°ç­”æ¡ˆã€‚</li>
<li>RCåœ¨å°‘æ ·æœ¬æç¤ºæ–¹æ³•ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b795a39a5921ceccac85887f872f59fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db5218c13782dfaeed6dfb41a21b204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cea842cfd0f19308d4daa86e641bb599.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Two-Heads-are-Better-Than-One-Test-time-Scaling-of-Multi-agent-Collaborative-Reasoning"><a href="#Two-Heads-are-Better-Than-One-Test-time-Scaling-of-Multi-agent-Collaborative-Reasoning" class="headerlink" title="Two Heads are Better Than One: Test-time Scaling of Multi-agent   Collaborative Reasoning"></a>Two Heads are Better Than One: Test-time Scaling of Multi-agent   Collaborative Reasoning</h2><p><strong>Authors:Can Jin, Hongwu Peng, Qixin Zhang, Yujin Tang, Dimitris N. Metaxas, Tong Che</strong></p>
<p>Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have significantly improved single-agent performance on challenging reasoning tasks, how to effectively scale collaboration and reasoning in MAS remains an open question. In this work, we introduce an adaptive multi-agent framework designed to enhance collaborative reasoning through both model-level training and system-level coordination. We construct M500, a high-quality dataset containing 500 multi-agent collaborative reasoning traces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a model optimized for multi-agent collaboration. To further enable adaptive reasoning, we propose a novel CEO agent that dynamically manages the discussion process, guiding agent collaboration and adjusting reasoning depth for more effective problem-solving. Evaluated in an open-source MAS across a range of tasks-including general understanding, mathematical reasoning, and coding-our system significantly outperforms strong baselines. For instance, M1-32B achieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on MBPP-Sanitized, matching the performance of state-of-the-art models like DeepSeek-R1 on some tasks. These results highlight the importance of both learned collaboration and adaptive coordination in scaling multi-agent reasoning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jincan333/MAS-TTS">https://github.com/jincan333/MAS-TTS</a> </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸ºè§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡æä¾›äº†ä¸€æ¡å……æ»¡å¸Œæœ›çš„é“è·¯ï¼Œè¿™äº›ä»»åŠ¡é€šå¸¸æ˜¯å•ä¸€æ™ºèƒ½ä½“ç³»ç»Ÿéš¾ä»¥åº”å¯¹çš„ã€‚è™½ç„¶æœ€è¿‘åœ¨æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ–¹é¢çš„è¿›å±•å·²ç»æ˜¾è‘—æé«˜äº†å•ä¸€æ™ºèƒ½ä½“åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä½†å¦‚ä½•åœ¨MASä¸­æœ‰æ•ˆåœ°æ‰©å±•åä½œå’Œæ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªé€‚åº”å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡å‹çº§åˆ«çš„è®­ç»ƒå’Œç³»ç»Ÿçº§åˆ«çš„åè°ƒæ¥å¢å¼ºåä½œæ¨ç†ã€‚æˆ‘ä»¬æ„å»ºäº†M500ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«500ä¸ªå¤šæ™ºèƒ½ä½“åä½œæ¨ç†è½¨è¿¹çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æ­¤æ•°æ®é›†å¯¹Qwen2.5-32B-Instructè¿›è¡Œå¾®è°ƒï¼Œä»¥äº§ç”Ÿé’ˆå¯¹å¤šæ™ºèƒ½ä½“åä½œä¼˜åŒ–çš„M1-32Bæ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å®ç°è‡ªé€‚åº”æ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é¦–å¸­æ‰§è¡Œå®˜æ™ºèƒ½ä½“ï¼Œå®ƒåŠ¨æ€ç®¡ç†è®¨è®ºè¿‡ç¨‹ï¼Œå¼•å¯¼æ™ºèƒ½ä½“åä½œï¼Œå¹¶è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è§£å†³é—®é¢˜ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨åŒ…æ‹¬é€šç”¨ç†è§£ã€æ•°å­¦æ¨ç†å’Œç¼–ç ç­‰ä¸€ç³»åˆ—ä»»åŠ¡ä¸­ï¼Œåœ¨å¼€æºMASä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚ä¾‹å¦‚ï¼ŒM1-32Båœ¨GPQA-Diamondä¸Šå®ç°äº†12%çš„æ”¹è¿›ï¼Œåœ¨AIME2024ä¸Šå®ç°äº†41%çš„æ”¹è¿›ï¼Œåœ¨MBPP-Sanitizedä¸Šå®ç°äº†10%çš„æ”¹è¿›ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šä¸DeepSeek-R1ç­‰æœ€æ–°æ¨¡å‹ç›¸åŒ¹é…ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å­¦ä¹ åä½œå’Œè‡ªé€‚åº”åè°ƒåœ¨æ‰©å±•å¤šæ™ºèƒ½ä½“æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jincan333/MAS-TTS">https://github.com/jincan333/MAS-TTS</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09772v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ï¼Œæ—¨åœ¨è§£å†³å•ä¸€æ™ºèƒ½ä½“ç³»ç»Ÿéš¾ä»¥åº”å¯¹çš„å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡ã€‚æ–‡ç« é€šè¿‡æ„å»ºM500æ•°æ®é›†å’ŒM1-32Bæ¨¡å‹ï¼Œä¼˜åŒ–äº†å¤šæ™ºèƒ½ä½“çš„åä½œæ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹CEOæ™ºèƒ½ä½“ï¼Œä»¥åŠ¨æ€ç®¡ç†è®¨è®ºè¿‡ç¨‹ï¼Œè°ƒæ•´æ¨ç†æ·±åº¦ï¼Œæ›´æœ‰æ•ˆåœ°è§£å†³é—®é¢˜ã€‚åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¦‚GPQA-Diamondã€AIME2024å’ŒMBPP-Sanitizedç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡åˆ†åˆ«è¾¾åˆ°äº†12%ã€41%å’Œ10%ã€‚è¿™è¡¨æ˜å­¦ä¹ åˆ°çš„åä½œå’Œè‡ªé€‚åº”åè°ƒåœ¨æ‰©å±•å¤šæ™ºèƒ½ä½“æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå°¤å…¶å¯¹äºå•ä¸€æ™ºèƒ½ä½“éš¾ä»¥å¤„ç†çš„ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æ„å»ºM500æ•°æ®é›†å’ŒM1-32Bæ¨¡å‹ï¼Œä¼˜åŒ–äº†å¤šæ™ºèƒ½ä½“çš„åä½œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æ–°å‹CEOæ™ºèƒ½ä½“ï¼ŒåŠ¨æ€ç®¡ç†è®¨è®ºè¿‡ç¨‹ï¼Œè°ƒæ•´æ¨ç†æ·±åº¦ï¼Œæå‡é—®é¢˜è§£å†³æ•ˆç‡ã€‚</li>
<li>ç³»ç»Ÿåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸€èˆ¬ç†è§£ã€æ•°å­¦æ¨ç†å’Œç¼–ç ä»»åŠ¡ã€‚</li>
<li>åœ¨GPQA-Diamondã€AIME2024å’ŒMBPP-Sanitizedç­‰ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡åˆ†åˆ«è¾¾åˆ°äº†12%ã€41%å’Œ10%ï¼Œä¸æœ€æ–°æ¨¡å‹å¦‚DeepSeek-R1ç›¸åŒ¹é…ã€‚</li>
<li>è¯¥ç³»ç»Ÿçš„æ€§èƒ½æå‡å¾—ç›Šäºå­¦ä¹ åˆ°çš„åä½œå’Œè‡ªé€‚åº”åè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-693140b05a1e6d90c7f89506b4c037db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b724f8e0a184b134038e0238156c13ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-921f9debfca08b7e99b2ad47b42eb2bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190a3911116430d7632d3dacf135f162.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DUMP-Automated-Distribution-Level-Curriculum-Learning-for-RL-based-LLM-Post-training"><a href="#DUMP-Automated-Distribution-Level-Curriculum-Learning-for-RL-based-LLM-Post-training" class="headerlink" title="DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM   Post-training"></a>DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM   Post-training</h2><p><strong>Authors:Zhenting Wang, Guofeng Cui, Kun Wan, Wentian Zhao</strong></p>
<p>Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: <a target="_blank" rel="noopener" href="https://github.com/ZhentingWang/DUMP">https://github.com/ZhentingWang/DUMP</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åæœŸè®­ç»ƒå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥äº†å¯è§‚çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å°†è®­ç»ƒæ•°æ®è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œå¿½è§†äº†ç°ä»£LLMè®­ç»ƒé€šå¸¸æ¶‰åŠæ¥è‡ªä¸åŒåˆ†å¸ƒçš„æ··åˆæ•°æ®çš„äº‹å®ï¼Œè¿™äº›æ•°æ®çš„æ¥æºå’Œéš¾åº¦éƒ½åœ¨ä¸æ–­å˜åŒ–ã€‚è¿™ç§å¼‚è´¨æ€§å¼•å‘äº†ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå¦‚ä½•è‡ªé€‚åº”åœ°è°ƒåº¦ä¸åŒåˆ†å¸ƒçš„è®­ç»ƒä»¥ä¼˜åŒ–å­¦ä¹ æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºåˆ†å¸ƒçº§åˆ«å¯å­¦ä¹ æ€§çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè§è§£æ˜¯ï¼Œç­–ç•¥ä¼˜åŠ¿çš„å¤§å°åæ˜ äº†æ¨¡å‹åœ¨ç»™å®šåˆ†å¸ƒä¸Šè¿›ä¸€æ­¥è®­ç»ƒçš„æ½œåœ¨æ”¶ç›Šã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMåè®­ç»ƒåˆ†å¸ƒçº§è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸Šç½®ä¿¡ç•Œï¼ˆUCBï¼‰åŸåˆ™æ¥åŠ¨æ€è°ƒæ•´ä¸åŒåˆ†å¸ƒçš„é‡‡æ ·æ¦‚ç‡ã€‚è¯¥æ–¹æ³•ä¼˜å…ˆå¤„ç†å…·æœ‰é«˜å¹³å‡ä¼˜åŠ¿ï¼ˆåˆ©ç”¨ï¼‰æˆ–ä½æ ·æœ¬æ•°é‡ï¼ˆæ¢ç´¢ï¼‰çš„åˆ†å¸ƒï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªè‡ªé€‚åº”ä¸”ç†è®ºä¸Šæœ‰ä¾æ®çš„è®­ç»ƒè®¡åˆ’ã€‚æˆ‘ä»¬ä»¥GRPOä½œä¸ºåº•å±‚çš„RLç®—æ³•æ¥å®ç°æˆ‘ä»¬çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œå¹¶åœ¨å…·æœ‰ä¸åŒéš¾åº¦å’Œæ¥æºçš„é€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼Œçªæ˜¾äº†åˆ†å¸ƒæ„ŸçŸ¥è¯¾ç¨‹ç­–ç•¥åœ¨LLMåè®­ç»ƒä¸­çš„ä»·å€¼ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZhentingWang/DUMP%E3%80%82">https://github.com/ZhentingWang/DUMPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09710v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒåé˜¶æ®µçš„åº”ç”¨å·²å¸¦æ¥æ˜¾è‘—è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å°†è®­ç»ƒæ•°æ®è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œå¿½ç•¥äº†ç°ä»£LLMè®­ç»ƒæ¶‰åŠçš„å¤šç§æ•°æ®æ¥æºå’Œéš¾åº¦åˆ†å¸ƒçš„å·®å¼‚ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåˆ†å¸ƒçº§åˆ«å­¦ä¹ èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºRLé©±åŠ¨çš„LLMè®­ç»ƒåé˜¶æ®µã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç­–ç•¥ä¼˜åŠ¿å¹…åº¦æ¥è¯„ä¼°æ¨¡å‹ä»ç‰¹å®šåˆ†å¸ƒè¿›ä¸€æ­¥è®­ç»ƒä¸­çš„æ½œåœ¨æ”¶ç›Šï¼Œå¹¶åŸºäºæ­¤æå‡ºä¸€ç§åŸºäºåˆ†å¸ƒçº§åˆ«çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½ï¼Œçªæ˜¾äº†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒåé˜¶æ®µé‡‡ç”¨åˆ†å¸ƒæ„ŸçŸ¥è¯¾ç¨‹ç­–ç•¥çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒåçš„åº”ç”¨æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§å’Œåˆ†å¸ƒå·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçº§åˆ«å­¦ä¹ èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨ç­–ç•¥ä¼˜åŠ¿å¹…åº¦è¯„ä¼°æ¨¡å‹ä»ç‰¹å®šåˆ†å¸ƒè¿›ä¸€æ­¥è®­ç»ƒä¸­çš„æ½œåœ¨æ”¶ç›Šã€‚</li>
<li>é‡‡ç”¨åŸºäºUpper Confidence Bound (UCB)åŸåˆ™çš„åŠ¨æ€é‡‡æ ·æ¦‚ç‡è°ƒæ•´æ–¹æ³•ï¼Œä»¥å¹³è¡¡å¼€å‘ï¼ˆexploitationï¼‰å’Œæ¢ç´¢ï¼ˆexplorationï¼‰ã€‚</li>
<li>æ¡†æ¶åœ¨é€»è¾‘æ¨ç†è§£é¢˜é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6aa23d08ccecb9b1cb3242954288b207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b96378b455dd942cbf1d4e61398216c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-171e0e2cf366a2afdf3dacc7acf4e0ff.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-acee592cf1878e46beb05cd5b4f5308f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  InternVL3 Exploring Advanced Training and Test-Time Recipes for   Open-Source Multimodal Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e6222b5aec84d6079bbb552935ee2832.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-15  EasyGenNet An Efficient Framework for Audio-Driven Gesture Video   Generation Based on Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
