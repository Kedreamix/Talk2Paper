<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Giant and anisotropic magnetostriction in $Î²$-O$_{2}$ at 110 T">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c998c6c2c2bbe666cf899972d93cfe57.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-16-æ›´æ–°"><a href="#2025-04-16-æ›´æ–°" class="headerlink" title="2025-04-16 æ›´æ–°"></a>2025-04-16 æ›´æ–°</h1><h2 id="Giant-and-anisotropic-magnetostriction-in-Î²-O-2-at-110-T"><a href="#Giant-and-anisotropic-magnetostriction-in-Î²-O-2-at-110-T" class="headerlink" title="Giant and anisotropic magnetostriction in $Î²$-O$_{2}$ at 110 T"></a>Giant and anisotropic magnetostriction in $Î²$-O$_{2}$ at 110 T</h2><p><strong>Authors:Akihiko Ikeda, Yuya Kubota, Yuto Ishii, Xuguang Zhou, Shiyue Peng, Hiroaki Hayashi, Yasuhiro H. Matsuda, Kosuke Noda, Tomoya Tanaka, Kotomi Shimbori, Kenta Seki, Hideaki Kobayashi, Dilip Bhoi, Masaki Gen, Kamini Gautam, Mitsuru Akaki, Shiro Kawachi, Shusuke Kasamatsu, Toshihiro Nomura, Yuichi Inubushi, Makina Yabashi</strong></p>
<p>Magnetostriction is a crystalâ€™s deformation under magnetic fields, usually in the range of $10^{-6}$ - $10^{-3}$, where the lattice change occurs with the change of spin and orbital state through spin-lattice couplings. In strong magnetic fields beyond 100 T, the significant Zeeman energy competes with the lattice interactions, where one can expect considerable magnetostriction. However, directly observing magnetostriction above 100 T is challenging, because generating magnetic fields beyond 100 T accompanies the destruction of the coil with a single-shot $\mu$-second pulse. Here, we observed the giant and anisotropic magnetostriction of $\sim$1 % at 110 T in the spin-controlled crystal of $\beta$-O$<em>{2}$, by combining the single-shot diffraction of x-ray free-electron laser (XFEL) and the state-of-the-art portable 100 T generator. The magnetostriction of $\sim$1 % is the largest class as a deformation of the unit cell. It is a response of the soft lattice of $\beta$-O$</em>{2}$ originating, not only in the competing van der Waals force and exchange interaction, but also the soft state of spin and lattice frustrated on the triangular network. Meanwhile, the anisotropy originates from the strong two-dimensionality of the spin system. Giant magnetostriction in crystals should become more ubiquitous and diverse beyond 100 T, where our XFEL experiment above 100 T opens a novel pathway for their exploration, providing fundamental insights into the roles of spin in stabilizing crystal structures. </p>
<blockquote>
<p>ç£è‡´ä¼¸ç¼©æ˜¯æ™¶ä½“åœ¨ç£åœºä¸‹çš„å˜å½¢ï¼Œé€šå¸¸èŒƒå›´åœ¨$10^{-6}$-$10^{-3}$ä¹‹é—´ï¼Œæ­¤æ—¶æ™¶æ ¼å˜åŒ–ä¼´éšç€è‡ªæ—‹å’Œè½¨é“æ€çš„æ”¹å˜ï¼Œé€šè¿‡è‡ªæ—‹æ™¶æ ¼è€¦åˆæ¥å®ç°ã€‚åœ¨è¶…è¿‡100Tçš„å¼ºç£åœºä¸‹ï¼Œæ˜¾è‘—çš„å¡æ›¼èƒ½ä¸æ™¶æ ¼ç›¸äº’ä½œç”¨ç›¸ç«äº‰ï¼Œå¯ä»¥é¢„æœŸä¼šæœ‰ç›¸å½“å¤§çš„ç£è‡´ä¼¸ç¼©ã€‚ç„¶è€Œï¼Œç›´æ¥åœ¨è¶…è¿‡100Tçš„ç£åœºä¸‹è§‚å¯Ÿç£è‡´ä¼¸ç¼©æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºäº§ç”Ÿè¶…è¿‡100Tçš„ç£åœºé€šå¸¸ä¼´éšç€å•æ¬¡å¾®ç§’è„‰å†²å¯¹çº¿åœˆçš„ç ´åã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆXå°„çº¿è‡ªç”±ç”µå­æ¿€å…‰ï¼ˆXFELï¼‰çš„å•æ¬¡è¡å°„æ‹æ‘„å’Œå…ˆè¿›ä¾¿æºå¼100Tç”Ÿæˆå™¨ï¼Œåœ¨$\beta$-O<sub>2</sub>çš„è‡ªæ—‹æ§åˆ¶æ™¶ä½“ä¸­è§‚å¯Ÿåˆ°çº¦ä¸º1%çš„å·¨å¤§ä¸”å„å‘å¼‚æ€§çš„ç£è‡´ä¼¸ç¼©ï¼Œè¯¥ç£è‡´ä¼¸ç¼©å‘ç”Ÿåœ¨110Tçš„ç£åœºä¸‹ã€‚çº¦1%çš„ç£è‡´ä¼¸ç¼©æ˜¯å•å…ƒå†…å˜å½¢æœ€å¤§çš„ç±»åˆ«ä¹‹ä¸€ã€‚è¿™æ˜¯$\beta$-O<sub>2</sub>è½¯æ™¶æ ¼çš„ä¸€ç§å“åº”ï¼Œä¸ä»…æºäºèŒƒå¾·ååŠ›å’Œäº¤æ¢ç›¸äº’ä½œç”¨çš„ç«äº‰ï¼Œè¿˜æºäºè‡ªæ—‹å’Œæ™¶æ ¼åœ¨ä¸‰è§’ç½‘ç»œä¸Šçš„å—æŒ«çš„è½¯æ€ã€‚åŒæ—¶ï¼Œå„å‘å¼‚æ€§æºäºè‡ªæ—‹ç³»ç»Ÿçš„å¼ºäºŒç»´æ€§ã€‚åœ¨è¶…è¿‡100Tçš„ç£åœºä¸‹ï¼Œæ™¶ä½“ä¸­çš„å·¨å¤§ç£è‡´ä¼¸ç¼©åº”è¯¥æ›´åŠ æ™®éå’Œå¤šæ ·ï¼Œæˆ‘ä»¬çš„è¶…è¿‡100Tçš„XFELå®éªŒä¸ºæ¢ç´¢å®ƒä»¬å¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ï¼Œä¸ºè‡ªæ—‹åœ¨ç¨³å®šæ™¶ä½“ç»“æ„ä¸­çš„ä½œç”¨æä¾›äº†åŸºæœ¬è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10085v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬è®ºæ–‡æ¢è®¨äº†ç£è‡´ä¼¸ç¼©ç°è±¡ï¼Œå³åœ¨å¼ºç£åœºä¸‹æ™¶ä½“çš„å˜å½¢è¡Œä¸ºã€‚ç ”ç©¶åˆ©ç”¨Xå°„çº¿è‡ªç”±ç”µå­æ¿€å…‰å™¨å’Œå…ˆè¿›çš„100Tå‘ç”µæœºï¼Œè§‚å¯Ÿåˆ°Î²-O2æ™¶ä½“åœ¨å¼ºç£åœºä¸‹çš„å·¨å¤§ä¸”å„å‘å¼‚æ€§çš„ç£è‡´ä¼¸ç¼©ç°è±¡ï¼Œå˜å½¢è¾¾çº¦ç™¾åˆ†ä¹‹ä¸€ã€‚è¿™ä¸€ç°è±¡æºäºÎ²-O2æ™¶æ ¼çš„è½¯æ€§ç‰¹å¾ï¼Œè¡¨ç°ä¸ºèŒƒå¾·ååŠ›å’Œäº¤æ¢ä½œç”¨çš„ç«äº‰ä»¥åŠæ—‹å’Œæ™¶æ ¼åœ¨ä¸‰è§’ç½‘ç»œä¸Šçš„æŒ«è´¥çŠ¶æ€ã€‚è¯¥å®éªŒä¸ºå¼ºç£åœºä¸‹çš„æ™¶ä½“ç£è‡´ä¼¸ç¼©ç ”ç©¶å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œæ­ç¤ºäº†æ—‹åœ¨ç¨³å®šæ™¶ä½“ç»“æ„ä¸­çš„ä½œç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç£è‡´ä¼¸ç¼©æ˜¯æ™¶ä½“åœ¨ç£åœºä¸‹çš„å˜å½¢è¡Œä¸ºï¼Œé€šå¸¸åœ¨$10^{-6}$è‡³$10^{-3}$èŒƒå›´å†…å‘ç”Ÿï¼Œä¼´éšç€æ™¶æ ¼å˜åŒ–ã€‚</li>
<li>åœ¨è¶…è¿‡100Tçš„å¼ºç£åœºä¸‹ï¼Œæ˜¾è‘—çš„å¡æ›¼èƒ½é‡ä¸æ™¶æ ¼ç›¸äº’ä½œç”¨ç«äº‰ï¼Œå¯èƒ½å¯¼è‡´æ˜¾è‘—çš„ç£è‡´ä¼¸ç¼©ã€‚</li>
<li>Î²-O2æ™¶ä½“åœ¨å¼ºç£åœºä¸‹è¡¨ç°å‡ºå·¨å¤§çš„ç£è‡´ä¼¸ç¼©ç°è±¡ï¼Œå˜å½¢è¾¾çº¦ç™¾åˆ†ä¹‹ä¸€ã€‚</li>
<li>è¿™ç§å·¨å¤§ç£è‡´ä¼¸ç¼©æºäºÎ²-O2æ™¶æ ¼çš„è½¯æ€§ç‰¹å¾ä»¥åŠèŒƒå¾·ååŠ›å’Œäº¤æ¢ä½œç”¨çš„ç«äº‰ã€‚</li>
<li>ç£è‡´ä¼¸ç¼©çš„å„å‘å¼‚æ€§æºäºæ—‹ç³»ç»Ÿçš„å¼ºäºŒç»´æ€§ã€‚</li>
<li>åœ¨è¶…è¿‡100Tçš„å¼ºç£åœºä¸‹ï¼Œæ™¶ä½“çš„ç£è‡´ä¼¸ç¼©ç°è±¡å¯èƒ½æ›´ä¸ºæ™®éå’Œå¤šæ ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f035ce8f8ac1a52ee961fc48dca7607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28a41b063c42820cd13fd36b9140fe17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07cd5bf0116b322297f2203752e5aaf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d6df1f2f1f73f6f319747f2cb781819.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af503dae22d73a036bb5061b7d1861c5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multi-task-Learning-Capability-of-Medical-Generalist-Foundation-Model-via-Image-centric-Multi-annotation-Data"><a href="#Enhancing-Multi-task-Learning-Capability-of-Medical-Generalist-Foundation-Model-via-Image-centric-Multi-annotation-Data" class="headerlink" title="Enhancing Multi-task Learning Capability of Medical Generalist   Foundation Model via Image-centric Multi-annotation Data"></a>Enhancing Multi-task Learning Capability of Medical Generalist   Foundation Model via Image-centric Multi-annotation Data</h2><p><strong>Authors:Xun Zhu, Fanbin Mo, Zheng Zhang, Jiaxi Wang, Yiming Shi, Ming Wu, Chuang Zhang, Miao Li, Ji Wu</strong></p>
<p>The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent advances prioritize simple data scaling or architectural component enhancement, while neglecting to re-examine multi-task learning from a data-centric perspective. Critically, simply aggregating existing data resources leads to decentralized image-task alignment, which fails to cultivate comprehensive image understanding or align with clinical needs for multi-dimensional image interpretation. In this paper, we introduce the image-centric multi-annotation X-ray dataset (IMAX), the first attempt to enhance the multi-task learning capabilities of medical multi-modal large language models (MLLMs) from the data construction level. To be specific, IMAX is featured from the following attributes: 1) High-quality data curation. A comprehensive collection of more than 354K entries applicable to seven different medical tasks. 2) Image-centric dense annotation. Each X-ray image is associated with an average of 4.10 tasks and 7.46 training entries, ensuring multi-task representation richness per image. Compared to the general decentralized multi-annotation X-ray dataset (DMAX), IMAX consistently demonstrates significant multi-task average performance gains ranging from 3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs. Moreover, we investigate differences in statistical patterns exhibited by IMAX and DMAX training processes, exploring potential correlations between optimization dynamics and multi-task performance. Finally, leveraging the core concept of IMAX data construction, we propose an optimized DMAX-based training strategy to alleviate the dilemma of obtaining high-quality IMAX data in practical scenarios. </p>
<blockquote>
<p>åŒ»ç–—é€šç”¨åŸºç¡€æ¨¡å‹çš„å…´èµ·å·²ç»å½»åº•æ”¹å˜äº†ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šä»»åŠ¡æ¨¡å‹çš„å¼€å‘èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡å¤§è§„æ¨¡åŒ»ç–—æ•°æ®é›†ä¸Šçš„è”åˆè®­ç»ƒæ¥æ›´å¥½åœ°å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è¿›å±•ä¾§é‡äºç®€å•çš„æ•°æ®æ‰©å±•æˆ–æ¶æ„ç»„ä»¶å¢å¼ºï¼Œè€Œå¿½ç•¥ä»ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è§’åº¦é‡æ–°å®¡è§†å¤šä»»åŠ¡å­¦ä¹ ã€‚å…³é”®é—®é¢˜æ˜¯ï¼Œç®€å•èšåˆç°æœ‰æ•°æ®èµ„æºä¼šå¯¼è‡´åˆ†æ•£çš„å›¾åƒä»»åŠ¡å¯¹é½ï¼Œè¿™æ— æ³•åŸ¹å…»å…¨é¢çš„å›¾åƒç†è§£ï¼Œä¹Ÿæ— æ³•æ»¡è¶³å¤šç»´å›¾åƒè§£è¯»çš„ä¸´åºŠéœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä»¥å›¾åƒä¸ºä¸­å¿ƒçš„å¤šæ³¨é‡ŠXå…‰æ•°æ®é›†ï¼ˆIMAXï¼‰ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ä»æ•°æ®æ„å»ºå±‚é¢æé«˜åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒIMAXå…·æœ‰ä»¥ä¸‹å±æ€§ï¼š1ï¼‰é«˜è´¨é‡çš„æ•°æ®é‡‡é›†ã€‚å…¨é¢æ”¶é›†äº†é€‚ç”¨äºä¸ƒç§ä¸åŒåŒ»ç–—ä»»åŠ¡çš„è¶…è¿‡354Kæ¡æ•°æ®ã€‚2ï¼‰ä»¥å›¾åƒä¸ºä¸­å¿ƒå¯†é›†æ³¨é‡Šã€‚æ¯ä¸ªXå…‰ç‰‡éƒ½ä¸å¹³å‡4.10ä¸ªä»»åŠ¡å’Œ7.46ä¸ªè®­ç»ƒæ¡ç›®ç›¸å…³è”ï¼Œç¡®ä¿æ¯å¼ å›¾åƒçš„å¤šä»»åŠ¡è¡¨ç¤ºä¸°å¯Œæ€§ã€‚ä¸ä¸€èˆ¬çš„åˆ†æ•£å¤šæ³¨é‡ŠXå…‰æ•°æ®é›†ï¼ˆDMAXï¼‰ç›¸æ¯”ï¼ŒIMAXåœ¨ä¸ƒä¸ªå¼€æºçš„å…ˆè¿›åŒ»ç–—MLLMsä¸ŠæŒç»­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å¤šä»»åŠ¡å¹³å‡æ€§èƒ½æå‡ï¼ŒèŒƒå›´ä»3.20%åˆ°21.05%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†IMAXå’ŒDMAXè®­ç»ƒè¿‡ç¨‹ä¸­ç»Ÿè®¡æ¨¡å¼çš„å·®å¼‚ï¼Œæ¢ç´¢äº†ä¼˜åŒ–åŠ¨åŠ›ä¸å¤šä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„æ½œåœ¨å…³è”ã€‚æœ€åï¼Œåˆ©ç”¨IMAXæ•°æ®æ„å»ºçš„æ ¸å¿ƒæ¦‚å¿µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºDMAXçš„ä¼˜åŒ–è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¼“è§£åœ¨å®é™…åœºæ™¯ä¸­è·å–é«˜è´¨é‡IMAXæ•°æ®çš„å›°å¢ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09967v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦é€šç”¨åŸºç¡€æ¨¡å‹çš„å‡ºç°å·²ç»å½»åº•æ”¹å˜äº†ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šä»»åŠ¡æ¨¡å‹çš„å¼€å‘èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡å¤§è§„æ¨¡åŒ»å­¦æ•°æ®é›†çš„è”åˆè®­ç»ƒæ›´å¥½åœ°å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è¿›å±•ä¾§é‡äºç®€å•çš„æ•°æ®æ‰©å±•æˆ–æ¶æ„ç»„ä»¶å¢å¼ºï¼Œè€Œå¿½ç•¥äº†ä»æ•°æ®ä¸­å¿ƒçš„è§’åº¦é‡æ–°å®¡æŸ¥å¤šä»»åŠ¡å­¦ä¹ ã€‚æœ¬æ–‡ä»‹ç»äº†ä»¥å›¾åƒä¸ºä¸­å¿ƒçš„å¤šæ³¨è§£Xå…‰å°„çº¿æ•°æ®é›†ï¼ˆIMAXï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ä»æ•°æ®æ„å»ºå±‚é¢æå‡åŒ»å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ã€‚IMAXæ•°æ®é›†å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼šä¸€æ˜¯é«˜è´¨é‡çš„æ•°æ®é‡‡é›†ï¼Œæ¶µç›–ä¸ƒç§ä¸åŒåŒ»ç–—ä»»åŠ¡çš„è¶…è¿‡35.4ä¸‡æ¡è®°å½•ï¼›äºŒæ˜¯ä»¥å›¾åƒä¸ºä¸­å¿ƒå¯†é›†æ³¨è§£ï¼Œç¡®ä¿æ¯å¼ å›¾åƒçš„å¤šä»»åŠ¡è¡¨ç¤ºä¸°å¯Œæ€§ã€‚ä¸ä¸€èˆ¬çš„åˆ†æ•£å¤šæ³¨è§£Xå…‰å°„çº¿æ•°æ®é›†ï¼ˆDMAXï¼‰ç›¸æ¯”ï¼ŒIMAXåœ¨ä¸ƒä¸ªå¼€æºçš„å…ˆè¿›åŒ»ç–—MLLMsä¸ŠæŒç»­å±•ç°å‡ºæ˜¾è‘—çš„å¤šä»»åŠ¡å¹³å‡æ€§èƒ½æå‡ã€‚åŒæ—¶ï¼Œæœ¬æ–‡å¯¹IMAXå’ŒDMAXè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»Ÿè®¡æ¨¡å¼å·®å¼‚è¿›è¡Œäº†æ¢ç©¶ï¼Œå¹¶åŸºäºIMAXæ•°æ®æ„å»ºçš„æ ¸å¿ƒæ¦‚å¿µï¼Œæå‡ºäº†ä¼˜åŒ–DMAXçš„è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¼“è§£åœ¨å®é™…åœºæ™¯ä¸­è·å–é«˜è´¨é‡IMAXæ•°æ®çš„å›°å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—é€šç”¨åŸºç¡€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¤§è§„æ¨¡åŒ»å­¦æ•°æ®é›†çš„è”åˆè®­ç»ƒå¤„ç†å¤šä¸ªä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•°æ®æ‰©å±•å’Œæ¶æ„ä¼˜åŒ–ï¼Œå¿½è§†ä»æ•°æ®ä¸­å¿ƒçš„è§’åº¦é‡æ–°è€ƒå¯Ÿå¤šä»»åŠ¡å­¦ä¹ ã€‚</li>
<li>å¼•å…¥IMAXæ•°æ®é›†ï¼Œä»¥é«˜è´¨é‡æ•°æ®é‡‡é›†å’Œå›¾åƒä¸ºä¸­å¿ƒå¯†é›†æ³¨è§£ä¸ºç‰¹ç‚¹ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦è¯­è¨€æ¨¡å‹çš„å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>IMAXç›¸æ¯”å¸¸è§„æ•°æ®é›†åœ¨å¤šä¸ªåŒ»ç–—ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒIMAXå’ŒDMAXçš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ­ç¤ºäº†ç»Ÿè®¡æ¨¡å¼çš„å·®å¼‚ï¼Œå¹¶æ¢ç´¢äº†ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>æå‡ºåŸºäºDMAXçš„ä¼˜åŒ–è®­ç»ƒç­–ç•¥ï¼Œä»¥åº”å¯¹å®é™…åœºæ™¯ä¸­è·å–é«˜è´¨é‡IMAXæ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-399623b941f21224c8f161b946cff8ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-942965eef13afe3ddc935588b132100e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c998c6c2c2bbe666cf899972d93cfe57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-192aac12686cc1cbca792532190b872a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d44bd0e575648dd44ff5d404b10b28d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VR-MRI-Training-for-Adolescents-A-Comparative-Study-of-Gamified-VR-Passive-VR-360-Video-and-Traditional-Educational-Video"><a href="#VR-MRI-Training-for-Adolescents-A-Comparative-Study-of-Gamified-VR-Passive-VR-360-Video-and-Traditional-Educational-Video" class="headerlink" title="VR MRI Training for Adolescents: A Comparative Study of Gamified VR,   Passive VR, 360 Video, and Traditional Educational Video"></a>VR MRI Training for Adolescents: A Comparative Study of Gamified VR,   Passive VR, 360 Video, and Traditional Educational Video</h2><p><strong>Authors:Yue Yang, Mengyao Guo, Emmanuel A Corona, Bruce Daniel, Christoph Leuze, Fred Baik</strong></p>
<p>Magnetic Resonance Imaging (MRI) can be a stressful experience for pediatric patients due to the loud acoustic environment, enclosed scanner bore, and a prolonged requirement to remain still. While sedation is commonly used to manage anxiety and motion, it carries clinical risks and logistical burdens. Traditional preparatory approaches, such as instructional videos and mock scans, often lack engagement for older children and adolescents. In this study, we present a comparative evaluation of four MRI preparation modalities: (1) a gamified virtual reality (VR) simulation that trains stillness through real-time feedback; (2) a passive VR experience replicating the MRI environment without interactivity; (3) a 360 degree first-person video of a real MRI procedure; and (4) a standard 2D educational video. Using a within-subjects design (N &#x3D; 11, ages 10-16), we assess each methodâ€™s impact on head motion data, anxiety reduction, procedural preparedness, usability, cognitive workload, and subjective preference. Results show that the gamified VR condition has significantly lower head motion (p &lt; 0.001) and yielded the highest preparedness scores (p &lt; 0.05). Head motion data were significantly correlated with learning outcomes (p &lt; 0.01), suggesting that behavioral performance in VR strongly indicates procedural readiness. While all modalities reduced anxiety and were rated usable, interactive VR was preferred by most participants and demonstrated unique advantages in promoting engagement and behavioral rehearsal. We conclude with design recommendations for designing immersive simulations and integrating VR training into pediatric imaging workflows. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å¯¹å„¿ç§‘æ‚£è€…æ¥è¯´å¯èƒ½æ˜¯ä¸€ç§ä»¤äººå‹åŠ›çš„ä½“éªŒï¼Œå› ä¸ºå…¶å™ªéŸ³å¤§ã€æ‰«æèˆ±å¯†é—­ä¸”éœ€è¦é•¿æ—¶é—´ä¿æŒé™æ­¢ä¸åŠ¨ã€‚è™½ç„¶ç»å¸¸ä½¿ç”¨é•‡é™å‰‚æ¥æ§åˆ¶ç„¦è™‘å’Œç§»åŠ¨ï¼Œä½†å®ƒå­˜åœ¨ä¸€å®šçš„ä¸´åºŠé£é™©å’Œæ“ä½œä¸ä¾¿çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„é¢„å…ˆæ•™è‚²æ–¹æ³•ï¼Œå¦‚æ•™å­¦è§†é¢‘å’Œæ¨¡æ‹Ÿæ‰«æï¼Œå¯¹äºå¹´é¾„è¾ƒå¤§çš„å„¿ç«¥å’Œé’å°‘å¹´å¾€å¾€ç¼ºä¹å¸å¼•åŠ›ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹å››ç§MRIå‡†å¤‡æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼šï¼ˆ1ï¼‰æ¸¸æˆåŒ–è™šæ‹Ÿç°å®ï¼ˆVRï¼‰æ¨¡æ‹Ÿè®­ç»ƒï¼Œé€šè¿‡å®æ—¶åé¦ˆè®­ç»ƒé™æ­¢ä¸åŠ¨ï¼›ï¼ˆ2ï¼‰è¢«åŠ¨VRä½“éªŒï¼Œå¤åˆ¶MRIç¯å¢ƒè€Œä¸å…·å¤‡äº¤äº’æ€§ï¼›ï¼ˆ3ï¼‰çœŸå®MRIè¿‡ç¨‹çš„360åº¦ç¬¬ä¸€äººç§°è§†é¢‘ï¼›ï¼ˆ4ï¼‰æ ‡å‡†çš„äºŒç»´æ•™è‚²è§†é¢‘ã€‚é€šè¿‡ä¸€é¡¹å—è¯•å¯¹è±¡å†…éƒ¨è®¾è®¡ï¼ˆN&#x3D;11ï¼Œå¹´é¾„ä»‹äº10è‡³16å²ä¹‹é—´ï¼‰ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¯ç§æ–¹æ³•å¯¹å¤´éƒ¨è¿åŠ¨æ•°æ®ã€ç„¦è™‘å‡å°‘ã€ç¨‹åºå‡†å¤‡æƒ…å†µã€å¯ç”¨æ€§ã€è®¤çŸ¥å·¥ä½œé‡å’Œä¸»è§‚åå¥½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œæ¸¸æˆåŒ–VRæ¡ä»¶ä¸‹çš„å¤´éƒ¨è¿åŠ¨æ˜¾è‘—é™ä½ï¼ˆp &lt; 0.001ï¼‰ï¼Œå¹¶ä¸”è·å¾—äº†æœ€é«˜çš„å‡†å¤‡åº¦å¾—åˆ†ï¼ˆp &lt; 0.05ï¼‰ã€‚å¤´éƒ¨è¿åŠ¨æ•°æ®ä¸å­¦ä¹ æ•ˆæœä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ï¼ˆp &lt; 0.01ï¼‰ï¼Œè¿™è¡¨æ˜è™šæ‹Ÿç°å®ä¸­çš„è¡Œä¸ºè¡¨ç°å¼ºçƒˆé¢„ç¤ºç€ç¨‹åºå°±ç»ªç¨‹åº¦ã€‚è™½ç„¶æ‰€æœ‰æ¨¡å¼éƒ½å‡å°‘äº†ç„¦è™‘å¹¶è¢«è®¤ä¸ºæ˜¯å¯ç”¨çš„ï¼Œä½†å¤§å¤šæ•°å‚ä¸è€…æ›´å–œæ¬¢äº¤äº’å¼VRï¼Œå®ƒç‹¬ç‰¹åœ°ä¿ƒè¿›å‚ä¸åº¦å’Œè¡Œä¸ºæ’ç»ƒçš„ä¼˜åŠ¿ä¹Ÿå¾—åˆ°äº†å±•ç¤ºã€‚æœ€åæˆ‘ä»¬ç»™å‡ºäº†è®¾è®¡æ²‰æµ¸å¼æ¨¡æ‹Ÿå¹¶å°†VRè®­ç»ƒèå…¥å„¿ç§‘æˆåƒå·¥ä½œæµç¨‹çš„è®¾è®¡å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09955v1">PDF</a> Download our application at   <a target="_blank" rel="noopener" href="https://www.meta.com/experiences/stanford-mri-simulator/8205539289482347/">https://www.meta.com/experiences/stanford-mri-simulator/8205539289482347/</a></p>
<p><strong>æ‘˜è¦</strong><br>MRIçš„å¤æ‚ä½“éªŒå¯¹äºå„¿ç§‘æ‚£è€…è€Œè¨€å……æ»¡å‹åŠ›ï¼Œç”±äºå…¶ç¯å¢ƒä¸­çš„å£°éŸ³è¾ƒå¤§ã€æ‰«æä»ªå­”å¾„å¯†é—­ä»¥åŠéœ€è¦é•¿æ—¶é—´ä¿æŒé™æ­¢çš„è¦æ±‚ã€‚è™½ç„¶é•‡é™å‰‚å¸¸ç”¨äºç®¡ç†ç„¦è™‘å’ŒåŠ¨ä½œï¼Œä½†å®ƒå­˜åœ¨ä¸´åºŠé£é™©å’Œåå‹¤è´Ÿæ‹…ã€‚ä¼ ç»Ÿçš„å‰ç½®æ–¹æ³•ï¼Œå¦‚æ•™å­¦è§†é¢‘å’Œæ¨¡æ‹Ÿæ‰«æï¼Œå¾€å¾€ç¼ºä¹ä¸å¹´é•¿å„¿ç«¥å’Œé’å°‘å¹´çš„äº’åŠ¨ã€‚æœ¬ç ”ç©¶å¯¹å››ç§MRIå‡†å¤‡æ¨¡å¼è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼šï¼ˆ1ï¼‰æ¸¸æˆåŒ–è™šæ‹Ÿç°å®ï¼ˆVRï¼‰æ¨¡æ‹Ÿé€šè¿‡å®æ—¶åé¦ˆè®­ç»ƒä¿æŒé™æ­¢ï¼›ï¼ˆ2ï¼‰æ— äº¤äº’æ€§çš„è¢«åŠ¨VRä½“éªŒå¤åˆ¶MRIç¯å¢ƒï¼›ï¼ˆ3ï¼‰çœŸå®çš„MRIç¨‹åºçš„ç¬¬ä¸€äººç§°è§†é¢‘ï¼›ï¼ˆ4ï¼‰æ ‡å‡†äºŒç»´æ•™è‚²è§†é¢‘ã€‚æˆ‘ä»¬è¯„ä¼°æ¯ç§æ–¹æ³•å¯¹å¤´éƒ¨è¿åŠ¨æ•°æ®ã€å‡å°‘ç„¦è™‘ã€ç¨‹åºå‡†å¤‡åº¦ã€å¯ç”¨æ€§å’Œä¸»è§‚åå¥½çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¸¸æˆåŒ–VRæ¡ä»¶ä¸‹å¤´éƒ¨è¿åŠ¨æ˜¾è‘—é™ä½ï¼ˆp &lt; 0.001ï¼‰ï¼Œä¸”å‡†å¤‡åº¦å¾—åˆ†æœ€é«˜ï¼ˆp &lt; 0.05ï¼‰ã€‚å¤´éƒ¨è¿åŠ¨æ•°æ®ä¸å­¦ä¹ æ•ˆæœæ˜¾è‘—ç›¸å…³ï¼ˆp &lt; 0.01ï¼‰ï¼Œè¡¨æ˜VRä¸­çš„è¡Œä¸ºè¡¨ç°å¼ºçƒˆé¢„ç¤ºç¨‹åºå°±ç»ªçŠ¶æ€ã€‚è™½ç„¶æ‰€æœ‰æ¨¡å¼éƒ½å‡å°‘äº†ç„¦è™‘å¹¶è¢«è¯„å®šä¸ºå¯ç”¨ï¼Œä½†äº¤äº’å¼VRè¢«å¤§å¤šæ•°å‚ä¸è€…æ‰€å–œæ¬¢ï¼Œåœ¨ä¿ƒè¿›å‚ä¸å’Œè¡Œä¸ºçš„é‡å¤è®­ç»ƒæ–¹é¢è¡¨ç°å‡ºç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚æœ€åæˆ‘ä»¬ç»™å‡ºäº†è®¾è®¡æ²‰æµ¸å¼æ¨¡æ‹Ÿå’Œå°†VRè®­ç»ƒèå…¥å„¿ç§‘æˆåƒå·¥ä½œæµç¨‹çš„è®¾è®¡å»ºè®®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>å„¿ç§‘æ‚£è€…åœ¨MRIè¿‡ç¨‹ä¸­çš„å‹åŠ›ä¸»è¦æ¥æºäºå£°éŸ³ã€ç¯å¢ƒå’Œé•¿æ—¶é—´ä¿æŒé™æ­¢çš„è¦æ±‚ã€‚</li>
<li>ä¼ ç»Ÿçš„MRIé¢„å¤‡æ–¹æ³•å¦‚æ•™å­¦è§†é¢‘å’Œæ¨¡æ‹Ÿæ‰«æåœ¨æŸäº›æƒ…å†µä¸‹ç¼ºä¹ä¸æ‚£è€…çš„äº’åŠ¨ã€‚</li>
<li>æ¸¸æˆåŒ–è™šæ‹Ÿç°å®ï¼ˆVRï¼‰æ¨¡æ‹Ÿåœ¨å‡å°‘å¤´éƒ¨è¿åŠ¨å’Œæé«˜æ‚£è€…å‡†å¤‡åº¦æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚</li>
<li>VRä¸­çš„è¡Œä¸ºè¡¨ç°ä¸ç¨‹åºå°±ç»ªçŠ¶æ€ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚</li>
<li>äº¤äº’å¼VRåœ¨ä¿ƒè¿›æ‚£è€…å‚ä¸å’Œé‡å¤è®­ç»ƒæ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d35246d7be0b459b195c835d6fc2d67f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d54bb12816c14d2b174cd799fa725b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2609032a11121a78c69da93063e4d03c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Representation-Learning-for-Robust-Gene-Expression-Modeling-and-Cancer-Prognosis"><a href="#Transformer-Based-Representation-Learning-for-Robust-Gene-Expression-Modeling-and-Cancer-Prognosis" class="headerlink" title="Transformer-Based Representation Learning for Robust Gene Expression   Modeling and Cancer Prognosis"></a>Transformer-Based Representation Learning for Robust Gene Expression   Modeling and Cancer Prognosis</h2><p><strong>Authors:Shuai Jiang, Saeed Hassanpour</strong></p>
<p>Transformer-based models have achieved remarkable success in natural language and vision tasks, but their application to gene expression analysis remains limited due to data sparsity, high dimensionality, and missing values. We present GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. GexBERT learns context-aware gene embeddings by pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes. We evaluate GexBERT across three critical tasks in cancer research: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. GexBERT achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. Furthermore, its attention-based interpretability reveals biologically meaningful gene patterns across cancer types. These findings demonstrate the utility of GexBERT as a scalable and effective tool for gene expression modeling, with translational potential in settings where gene coverage is limited or incomplete. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶åœ¨åŸºå› è¡¨è¾¾åˆ†æä¸­çš„åº”ç”¨ä»å—é™äºæ•°æ®çš„ç¨€ç–æ€§ã€é«˜ç»´åº¦å’Œç¼ºå¤±å€¼ã€‚æˆ‘ä»¬æå‡ºäº†GexBERTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç”¨äºåŸºå› è¡¨è¾¾æ•°æ®çš„ç¨³å¥è¡¨ç¤ºå­¦ä¹ ã€‚GexBERTé€šè¿‡åœ¨å¤§è§„æ¨¡è½¬å½•ç»„å›¾è°±ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ¥å­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŸºå› åµŒå…¥ï¼Œé€šè¿‡æ©ç å’Œæ¢å¤ç›®æ ‡æ¥æ•è·æ•°åƒä¸ªåŸºå› ä¹‹é—´çš„å…±è¡¨è¾¾å…³ç³»ã€‚æˆ‘ä»¬åœ¨ç™Œç—‡ç ”ç©¶çš„ä¸‰ä¸ªå…³é”®ä»»åŠ¡ä¸­è¯„ä¼°äº†GexBERTï¼šæ³›ç™Œåˆ†ç±»ã€ç‰¹å®šç™Œç—‡ç”Ÿå­˜é¢„æµ‹å’Œç¼ºå¤±å€¼ä¼°ç®—ã€‚GexBERTåœ¨æœ‰é™çš„åŸºå› å­é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†ç±»ç²¾åº¦ï¼Œé€šè¿‡æ¢å¤é¢„åé”šåŸºå› çš„è¡¨è¾¾æé«˜äº†ç”Ÿå­˜é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶åœ¨é«˜ç¼ºå¤±ç‡çš„æƒ…å†µä¸‹ä¼˜äºä¼ ç»Ÿçš„ä¼°ç®—æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶åŸºäºæ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§æ­ç¤ºäº†ä¸åŒç™Œç—‡ç±»å‹ä¸­ç”Ÿç‰©æ„ä¹‰æ˜ç¡®çš„åŸºå› æ¨¡å¼ã€‚è¿™äº›å‘ç°è¯æ˜äº†GexBERTä½œä¸ºåŸºå› è¡¨è¾¾å»ºæ¨¡çš„å¯æ‰©å±•å’Œæœ‰æ•ˆå·¥å…·ï¼Œåœ¨åŸºå› è¦†ç›–æœ‰é™æˆ–ä¸å®Œå…¨çš„æƒ…å†µä¸‹å…·æœ‰ç¿»è¯‘æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09704v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åŸºå› è¡¨è¾¾åˆ†æä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚æœ¬ç ”ç©¶æå‡ºäº†GexBERTæ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ åŸºå› è¡¨è¾¾æ•°æ®çš„é²æ£’è¡¨ç¤ºã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡è½¬å½•ç»„è°±ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŸºå› åµŒå…¥ï¼Œé€šè¿‡æ©ç å’Œæ¢å¤ç›®æ ‡æ¥æ•è·æ•°åƒä¸ªåŸºå› ä¹‹é—´çš„å…±è¡¨è¾¾å…³ç³»ã€‚åœ¨ç™Œç—‡ç ”ç©¶ä¸­çš„ä¸‰ä¸ªå…³é”®ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬æ³›ç™Œåˆ†ç±»ã€ç‰¹å®šç™Œç—‡ç”Ÿå­˜é¢„æµ‹å’Œç¼ºå¤±å€¼æ’è¡¥ã€‚GexBERTå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»å‡†ç¡®æ€§ï¼Œä»æœ‰é™çš„åŸºå› å­é›†ä¸­æ¢å¤è¡¨è¾¾å…³é”®çš„é¢„åé”šåŸºå› ï¼Œæé«˜äº†ç”Ÿå­˜é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨é«˜ç¼ºå¤±ç‡çš„æƒ…å†µä¸‹ä¼˜äºä¼ ç»Ÿçš„æ’è¡¥æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶åŸºäºæ³¨æ„åŠ›çš„è§£é‡Šæ€§æ­ç¤ºäº†ä¸åŒç™Œç—‡ç±»å‹ä¸­ç”Ÿç‰©æ„ä¹‰æ˜ç¡®çš„åŸºå› æ¨¡å¼ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGexBERTä½œä¸ºåŸºå› è¡¨è¾¾å»ºæ¨¡çš„å¯æ‰©å±•å’Œæœ‰æ•ˆå·¥å…·å…·æœ‰å®ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºå› è¦†ç›–æœ‰é™æˆ–ä¸å®Œå…¨çš„æƒ…å†µä¸‹å…·æœ‰ç¿»è¯‘æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GexBERTæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç”¨äºåŸºå› è¡¨è¾¾æ•°æ®çš„é²æ£’è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>GexBERTé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŸºå› åµŒå…¥ï¼Œæ•æ‰åŸºå› é—´çš„å…±è¡¨è¾¾å…³ç³»ã€‚</li>
<li>åœ¨æ³›ç™Œåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒGexBERTå–å¾—äº†æœ€å…ˆè¿›çš„åˆ†ç±»å‡†ç¡®æ€§ï¼Œå³ä½¿ä½¿ç”¨æœ‰é™çš„åŸºå› å­é›†ã€‚</li>
<li>GexBERTèƒ½æ¢å¤å…³é”®é¢„åé”šåŸºå› çš„è¡¨è¾¾ï¼Œæé«˜ç™Œç—‡ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨é«˜ç¼ºå¤±ç‡çš„æƒ…å†µä¸‹ï¼ŒGexBERTçš„æ’è¡¥æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>GexBERTå…·æœ‰åŸºäºæ³¨æ„åŠ›çš„è§£é‡Šæ€§ï¼Œèƒ½æ­ç¤ºç”Ÿç‰©æ„ä¹‰æ˜ç¡®çš„åŸºå› æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af26f50ffc07f68ea76c8327bdc983b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9b8fa3cb2eff6c98f0f955a6045a74f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ToolTipNet-A-Segmentation-Driven-Deep-Learning-Baseline-for-Surgical-Instrument-Tip-Detection"><a href="#ToolTipNet-A-Segmentation-Driven-Deep-Learning-Baseline-for-Surgical-Instrument-Tip-Detection" class="headerlink" title="ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical   Instrument Tip Detection"></a>ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical   Instrument Tip Detection</h2><p><strong>Authors:Zijian Wu, Shuojue Yang, Yueming Jin, Septimiu E Salcudean</strong></p>
<p>In robot-assisted laparoscopic radical prostatectomy (RALP), the location of the instrument tip is important to register the ultrasound frame with the laparoscopic camera frame. A long-standing limitation is that the instrument tip position obtained from the da Vinci API is inaccurate and requires hand-eye calibration. Thus, directly computing the position of the tool tip in the camera frame using the vision-based method becomes an attractive solution. Besides, surgical instrument tip detection is the key component of other tasks, like surgical skill assessment and surgery automation. However, this task is challenging due to the small size of the tool tip and the articulation of the surgical instrument. Surgical instrument segmentation becomes relatively easy due to the emergence of the Segmentation Foundation Model, i.e., Segment Anything. Based on this advancement, we explore the deep learning-based surgical instrument tip detection approach that takes the part-level instrument segmentation mask as input. Comparison experiments with a hand-crafted image-processing approach demonstrate the superiority of the proposed method on simulated and real datasets. </p>
<blockquote>
<p>åœ¨æœºå™¨äººè¾…åŠ©è…¹è…”é•œä¸‹æ ¹æ²»æ€§å‰åˆ—è…ºåˆ‡é™¤æœ¯ï¼ˆRALPï¼‰ä¸­ï¼Œä»ªå™¨å°–ç«¯çš„ä½ç½®å¯¹äºå°†è¶…å£°æ¡†æ¶ä¸è…¹è…”é•œæ‘„åƒæœºæ¡†æ¶è¿›è¡Œæ³¨å†Œéå¸¸é‡è¦ã€‚ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„é—®é¢˜æ˜¯ä»è¾¾èŠ¬å¥‡APIè·å¾—çš„ä»ªå™¨å°–ç«¯ä½ç½®ä¸å‡†ç¡®ï¼Œéœ€è¦è¿›è¡Œæ‰‹çœ¼æ ¡å‡†ã€‚å› æ­¤ï¼Œç›´æ¥ä½¿ç”¨åŸºäºè§†è§‰çš„æ–¹æ³•è®¡ç®—å·¥å…·å°–ç«¯åœ¨æ‘„åƒæœºæ¡†æ¶ä¸­çš„ä½ç½®æˆä¸ºä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæ‰‹æœ¯å™¨æ¢°å°–ç«¯æ£€æµ‹æ˜¯å…¶ä»–ä»»åŠ¡çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå¦‚æ‰‹æœ¯æŠ€èƒ½è¯„ä¼°å’Œæ‰‹æœ¯è‡ªåŠ¨åŒ–ã€‚ç„¶è€Œï¼Œç”±äºå·¥å…·å°–ç‚¹å°ä¸”æ‰‹æœ¯å™¨æ¢°å…·æœ‰çµæ´»æ€§ï¼Œæ­¤ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç”±äºåˆ†å‰²åŸºç¡€æ¨¡å‹çš„å…´èµ·ï¼Œå³åˆ†å‰²ä»»ä½•äº‹ç‰©ï¼Œæ‰‹æœ¯å™¨æ¢°åˆ†å‰²å˜å¾—ç›¸å¯¹å®¹æ˜“ã€‚åŸºäºæ­¤å‘å±•ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æ‰‹æœ¯å™¨æ¢°å°–ç«¯æ£€æµ‹æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»¥éƒ¨åˆ†çº§ä»ªå™¨åˆ†å‰²æ©è†œä½œä¸ºè¾“å…¥ã€‚ä¸æ‰‹å·¥å›¾åƒå¤„ç†æ–¹æ³•è¿›è¡Œçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºæ‰‹å·¥å›¾åƒå¤„ç†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09700v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æœºå™¨äººè¾…åŠ©è…¹è…”é•œå‰åˆ—è…ºæ ¹æ²»æœ¯ï¼ˆRALPï¼‰ä¸­ï¼Œä»ªå™¨å°–ç«¯ä½ç½®çš„é‡è¦æ€§åŠå…¶åœ¨è¶…å£°æ¡†æ¶ä¸è…¹è…”é•œæ‘„åƒæœºæ¡†æ¶ä¸­çš„æ³¨å†Œé—®é¢˜ã€‚ç”±äºä»da Vinci APIè·å–çš„ä»ªå™¨å°–ç«¯ä½ç½®ä¸å‡†ç¡®ï¼Œéœ€è¦æ‰‹çœ¼æ ¡å‡†ã€‚å› æ­¤ï¼Œé‡‡ç”¨åŸºäºè§†è§‰çš„æ–¹æ³•ç›´æ¥è®¡ç®—å·¥å…·å°–ç«¯åœ¨æ‘„åƒæœºæ¡†æ¶ä¸­çš„ä½ç½®æˆä¸ºä¸€ç§æœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæ‰‹æœ¯å™¨æ¢°å°–ç«¯æ£€æµ‹æ˜¯æ‰‹æœ¯æŠ€èƒ½è¯„ä¼°å’Œæ‰‹æœ¯è‡ªåŠ¨åŒ–ç­‰ä»»åŠ¡çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç”±äºå·¥å…·å°–ç«¯å°ºå¯¸å°ä»¥åŠæ‰‹æœ¯å™¨æ¢°çš„å…³èŠ‚æ´»åŠ¨ï¼Œæ­¤ä»»åŠ¡æå…·æŒ‘æˆ˜æ€§ã€‚å€ŸåŠ©Segmentation Foundation Modelï¼ˆå¦‚Segment Anythingï¼‰ï¼Œæ‰‹æœ¯å™¨æ¢°åˆ†æ®µå˜å¾—ç›¸å¯¹å®¹æ˜“ã€‚åŸºäºæ­¤å‘å±•ï¼Œæœ¬æ–‡æ¢è®¨äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æ‰‹æœ¯å™¨æ¢°å°–ç«¯æ£€æµ‹æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»¥éƒ¨åˆ†çº§åˆ«çš„ä»ªå™¨åˆ†æ®µæ©è†œä¸ºè¾“å…¥ã€‚ä¸æ‰‹å·¥å›¾åƒå¤„ç†æ–¹æ³•ç›¸æ¯”ï¼Œå¯¹æ¯”å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’Œå®é™…æ•°æ®é›†ä¸Šè¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººè¾…åŠ©è…¹è…”é•œå‰åˆ—è…ºæ ¹æ²»æœ¯ï¼ˆRALPï¼‰ä¸­ï¼Œä»ªå™¨å°–ç«¯ä½ç½®çš„å‡†ç¡®æ€§å¯¹äºæ‰‹æœ¯è¿‡ç¨‹è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ä»da Vinci APIè·å–çš„ä»ªå™¨å°–ç«¯ä½ç½®æ•°æ®å­˜åœ¨ä¸å‡†ç¡®çš„é•¿æœŸé™åˆ¶ï¼Œéœ€è¦æ‰‹çœ¼æ ¡å‡†ã€‚</li>
<li>åŸºäºè§†è§‰çš„æ–¹æ³•ä¸ºè®¡ç®—å·¥å…·å°–ç«¯åœ¨æ‘„åƒæœºæ¡†æ¶ä¸­çš„ä½ç½®æä¾›äº†æœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ‰‹æœ¯å™¨æ¢°å°–ç«¯æ£€æµ‹æ˜¯æ‰‹æœ¯æŠ€èƒ½è¯„ä¼°å’Œæ‰‹æœ¯è‡ªåŠ¨åŒ–ä»»åŠ¡çš„å…³é”®ã€‚</li>
<li>æ‰‹æœ¯å™¨æ¢°å°–ç«¯æ£€æµ‹é¢ä¸´å·¥å…·å°–ç«¯å°ºå¯¸å°å’Œå™¨æ¢°å…³èŠ‚æ´»åŠ¨çš„æŒ‘æˆ˜ã€‚</li>
<li>Segmentation Foundation Modelï¼ˆå¦‚Segment Anythingï¼‰çš„å‘å±•ä½¿å¾—æ‰‹æœ¯å™¨æ¢°åˆ†æ®µå˜å¾—ç›¸å¯¹å®¹æ˜“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-97b102a9647cfc0a7db2e127bad7b81c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf6914b4e22cea977686e0b0abaa2093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00ced1525355f379ee269cafb3d30fd1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniMamba4D-Spatio-temporal-Mamba-for-longitudinal-CT-lesion-segmentation"><a href="#OmniMamba4D-Spatio-temporal-Mamba-for-longitudinal-CT-lesion-segmentation" class="headerlink" title="OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion   segmentation"></a>OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion   segmentation</h2><p><strong>Authors:Justin Namuk Kim, Yiqiao Liu, Rajath Soans, Keith Persson, Sarah Halek, Michal Tomaszewski, Jianda Yuan, Gregory Goldmacher, Antong Chen</strong></p>
<p>Accurate segmentation of longitudinal CT scans is important for monitoring tumor progression and evaluating treatment responses. However, existing 3D segmentation models solely focus on spatial information. To address this gap, we propose OmniMamba4D, a novel segmentation model designed for 4D medical images (3D images over time). OmniMamba4D utilizes a spatio-temporal tetra-orientated Mamba block to effectively capture both spatial and temporal features. Unlike traditional 3D models, which analyze single-time points, OmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal information on lesion progression. Evaluated on an internal dataset comprising of 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682, comparable to state-of-the-arts (SOTA) models, while maintaining computational efficiency and better detecting disappeared lesions. This work demonstrates a new framework to leverage spatio-temporal information for longitudinal CT lesion segmentation. </p>
<blockquote>
<p>å‡†ç¡®åœ°å¯¹çºµå‘CTæ‰«æè¿›è¡Œåˆ†å‰²å¯¹äºç›‘æµ‹è‚¿ç˜¤è¿›å±•å’Œè¯„ä¼°æ²»ç–—æ•ˆæœéå¸¸é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„3Dåˆ†å‰²æ¨¡å‹ä»…ä¸“æ³¨äºç©ºé—´ä¿¡æ¯ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†OmniMamba4Dï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸º4DåŒ»å­¦å›¾åƒï¼ˆéšæ—¶é—´å˜åŒ–çš„3Då›¾åƒï¼‰è®¾è®¡çš„å…¨æ–°åˆ†å‰²æ¨¡å‹ã€‚OmniMamba4Dåˆ©ç”¨æ—¶ç©ºå››é¢ä½“å®šå‘çš„Mambaå—ï¼Œæœ‰æ•ˆåœ°æ•æ‰ç©ºé—´å’Œæ—¶æ€ç‰¹å¾ã€‚ä¸ä¼ ç»Ÿçš„ä»…åˆ†æå•ä¸€æ—¶é—´ç‚¹çš„3Dæ¨¡å‹ä¸åŒï¼ŒOmniMamba4Då¤„ç†4D CTæ•°æ®ï¼Œæä¾›å…³äºç—…ç¶è¿›å±•çš„å…¨é¢æ—¶ç©ºä¿¡æ¯ã€‚åœ¨åŒ…å«3252æ¬¡CTæ‰«æçš„å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒOmniMamba4Då–å¾—äº†ç«äº‰åŠ›çš„Diceè¯„åˆ†0.682ï¼Œä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œå¹¶èƒ½æ›´å¥½åœ°æ£€æµ‹æ¶ˆå¤±çš„ç—…å˜ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€ä¸ªåˆ©ç”¨æ—¶ç©ºä¿¡æ¯è¿›è¡Œçºµå‘CTç—…ç¶åˆ†å‰²çš„æ–°æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09655v1">PDF</a> Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)   2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>OmniMamba4Dæ˜¯ä¸€ç§é’ˆå¯¹å››ç»´åŒ»å­¦å›¾åƒï¼ˆéšæ—¶é—´å˜åŒ–çš„3Då›¾åƒï¼‰çš„åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ—¶ç©ºå››å‘Mambaå—ï¼Œæœ‰æ•ˆæ•æ‰ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚ä¸ä¼ ç»Ÿçš„ä»…åˆ†æå•ä¸€æ—¶é—´ç‚¹çš„3Dæ¨¡å‹ä¸åŒï¼ŒOmniMamba4Då¤„ç†å››ç»´CTæ•°æ®ï¼Œæä¾›å…³äºç—…ç¶è¿›å±•çš„ç»¼åˆæ—¶ç©ºä¿¡æ¯ã€‚åœ¨åŒ…å«3ï¼Œ252ä¸ªCTæ‰«æçš„å†…éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒOmniMamba4Dçš„Diceåˆ†æ•°è¾¾åˆ°0.682ï¼Œä¸æœ€æ–°æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡å’Œæ›´å¥½çš„æ¶ˆå¤±ç—…ç¶æ£€æµ‹èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨æ—¶ç©ºä¿¡æ¯è¿›è¡Œçºµå‘CTç—…ç¶åˆ†å‰²çš„æ–°æ¡†æ¶ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>OmniMamba4Dæ˜¯ä¸€ç§é’ˆå¯¹å››ç»´åŒ»å­¦å›¾åƒï¼ˆ3Då›¾åƒéšæ—¶é—´å˜åŒ–ï¼‰çš„åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨æ—¶ç©ºå››å‘Mambaå—ï¼Œæœ‰æ•ˆæ•æ‰ç©ºé—´å’Œæ—¶é—´çš„ç‰¹å¾ã€‚</li>
<li>OmniMamba4Dèƒ½å¤Ÿå¤„ç†å››ç»´CTæ•°æ®ï¼Œæä¾›å…³äºç—…ç¶è¿›å±•çš„ç»¼åˆæ—¶ç©ºä¿¡æ¯ã€‚</li>
<li>ä¸ä»…åˆ†æå•ä¸€æ—¶é—´ç‚¹çš„ä¼ ç»Ÿ3Dæ¨¡å‹ä¸åŒï¼ŒOmniMamba4Då…·å¤‡æ›´å…¨é¢çš„åˆ†æåŠŸèƒ½ã€‚</li>
<li>åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒOmniMamba4Dçš„Diceåˆ†æ•°è¾¾åˆ°0.682ï¼Œè¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>OmniMamba4Dçš„Diceåˆ†æ•°ä¸æœ€æ–°æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡å’Œæ›´å¥½çš„æ¶ˆå¤±ç—…ç¶æ£€æµ‹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-40bf84769a90b13e98ce2a43edaa6d55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13001d92f51e81982cd9071289971084.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cbf466c3202a3c3aa892a6d36c33890d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ae329aca43848d9edf768042f08b285.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45d66b539b2aa7f67edd28f1884d8a68.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Q-ball-mechanism-of-electron-transport-properties-of-high-T-c-superconductors"><a href="#Q-ball-mechanism-of-electron-transport-properties-of-high-T-c-superconductors" class="headerlink" title="Q-ball mechanism of electron transport properties of high-T$_c$   superconductors"></a>Q-ball mechanism of electron transport properties of high-T$_c$   superconductors</h2><p><strong>Authors:S. I. Mukhin</strong></p>
<p>Proposed recently by the author Q-ball mechanism of the pseudogap state and high-Tc superconductivity in cuprates (2022) was supported by micro X-ray diffraction data in HgBa$<em>2$CuO$</em>{4+y}$ (2023). In the present paper it is demonstrated that T-linear temperature dependence of electrical resistivity arises naturally in the Q-ball gas phase, that may explain corresponding experimental data in the â€œstrange metalâ€ phase of high-T$_c$ cuprates, as reviewed by Barisic et al. (2013). In the present theory it arises due to scattering of electrons on the Q-balls gas of condensed charge&#x2F;spin fluctuations. Close to the lowest temperature boundary of the â€œstrange metalâ€ phase, at which Q-ball radius diverges, electrical resistivity caused by a slide of the Q-balls as a whole is calculated using fluctuation paraconductivity calculation method by Alex Abrikosov (1987). The diamagnetic response of Q-balls gas is calculated as well and shows good accord with experimental data by L.Li et al. (2010) in the â€œstrange metalâ€ phase. In total, obtained results demonstrate different properties of the correlated electrons systems that arise due to formation of Q-balls possessing internal bosonic frequency $\Omega&#x3D;2\pi nT$ in Matsubara time and, thus, forming the quantum thermodynamic time polycrystals. Presented theory may give a clue concerning a possible mechanism of the experimentally measured properties of high-T$_c$ cuprates in the â€œstrange metalâ€ phase of their phase diagram. We believe , these results provide support to the quantum thermodynamic time crystal model of the Euclidean Q-balls considered in the present paper. </p>
<blockquote>
<p>ä½œè€…è¿‘æœŸæå‡ºçš„ä¼ªé—´éš™æ€å’Œé«˜æ¸©è¶…å¯¼ä½“çš„Qçƒæœºåˆ¶ï¼ˆ2022å¹´ï¼‰å¾—åˆ°äº†æ±å·´é“œé…¸ç›ï¼ˆHgBa_2CuO_{4+y}ï¼‰ï¼ˆ2023å¹´ï¼‰ä¸­çš„å¾®Xå°„çº¿è¡å°„æ•°æ®çš„æ”¯æŒã€‚æœ¬æ–‡å±•ç¤ºäº†çº¿æ€§æ¸©åº¦ä¾èµ–çš„ç”µé˜»ç‡è‡ªç„¶äº§ç”ŸäºQçƒæ°”æ€ç›¸ï¼Œè¿™å¯ä»¥è§£é‡Šå·´é‡Œæ–¯ç­‰äººï¼ˆBarisic et al., 2013ï¼‰æ‰€å›é¡¾çš„é«˜æ¸©è¶…å¯¼ä½“é“œé…¸ç›çš„â€œå¥‡å¼‚é‡‘å±â€ç›¸ä¸­çš„ç›¸åº”å®éªŒæ•°æ®ã€‚æ ¹æ®ç°æœ‰ç†è®ºï¼Œå®ƒäº§ç”Ÿäºç”µå­åœ¨å‡èšçš„ç”µè·&#x2F;è‡ªæ—‹æ³¢åŠ¨çš„Qçƒæ°”ä½“ä¸Šçš„æ•£å°„ã€‚åœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„æœ€ä½æ¸©åº¦è¾¹ç•Œé™„è¿‘ï¼Œåœ¨æ­¤è¾¹ç•Œä¸Šï¼ŒQçƒçš„åŠå¾„å‘æ•£ï¼Œç”±äºQçƒä½œä¸ºä¸€ä¸ªæ•´ä½“çš„æ»‘åŠ¨å¯¼è‡´çš„ç”µé˜»ç‡æ˜¯é€šè¿‡äºšå†å…‹æ–¯Â·é˜¿å¸ƒé‡Œç§‘ç´¢å¤«ï¼ˆAlex Abrikosovï¼‰çš„æ¶¨è½è¶…å¯¼è®¡ç®—æ³•ï¼ˆ1987å¹´ï¼‰æ¥è®¡ç®—çš„ã€‚æ­¤å¤–ï¼Œè¿˜è®¡ç®—äº†Qçƒæ°”ä½“çš„æŠ—ç£æ€§å“åº”ï¼Œè¿™ä¸æç«‹ç­‰äººåœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸ä¸­çš„å®éªŒæ•°æ®ï¼ˆLi et al., 2010ï¼‰å»åˆè‰¯å¥½ã€‚æ€»çš„æ¥è¯´ï¼Œæ‰€è·å¾—çš„ç»“æœå±•ç¤ºäº†ç›¸å…³ç”µå­ç³»ç»Ÿçš„ä¸åŒç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§æ˜¯ç”±äºå½¢æˆäº†å†…éƒ¨å…·æœ‰ç»è‰²é¢‘ç‡Î©&#x3D;2Ï€nTçš„Qçƒè€Œäº§ç”Ÿçš„ï¼Œä»è€Œåœ¨Matsubaraæ—¶é—´ä¸­å½¢æˆé‡å­çƒ­åŠ›å­¦æ—¶é—´å¤šæ™¶ã€‚æœ¬æ–‡æå‡ºçš„ç†è®ºå¯èƒ½ä¸ºé«˜æ¸©è¶…å¯¼ä½“é“œé…¸ç›åœ¨ç›¸å›¾ä¸­çš„â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„å®éªŒæµ‹é‡ç‰¹æ€§æä¾›äº†ä¸€ç§å¯èƒ½çš„æœºåˆ¶ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™äº›ç»“æœä¸ºæœ¬æ–‡ä¸­è€ƒè™‘çš„æ¬§å‡ é‡Œå¾—Qçƒçš„é‡å­çƒ­åŠ›å­¦æ—¶é—´æ™¶ä½“æ¨¡å‹æä¾›äº†æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09610v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€è¿‘ç”±Q-ballæœºåˆ¶æå‡ºçš„ä¼ªé—´éš™æ€å’Œé«˜æ¸©è¶…å¯¼æ€§çš„ç†è®ºï¼Œå¾—åˆ°äº†æ±åŸºé“œé…¸ç›ä¸­å¾®Xå°„çº¿è¡å°„æ•°æ®çš„æ”¯æŒã€‚æœ¬æ–‡å±•ç¤ºäº†çº¿æ€§æ¸©åº¦ä¾èµ–æ€§çš„ç”µé˜»ç‡è‡ªç„¶äº§ç”ŸäºQ-ballæ°”æ€ç›¸ï¼Œè¿™è§£é‡Šäº†é«˜æ¸©é“œé…¸ç›â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„å®éªŒæ•°æ®ã€‚åœ¨è¯¥ç†è®ºä¸­ï¼Œç”µé˜»ç‡äº§ç”Ÿäºç”µå­ä¸å‡èšæ€ç”µè·&#x2F;è‡ªæ—‹æ³¢åŠ¨çš„Q-ballæ°”ä½“çš„æ•£å°„ã€‚åœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„æœ€ä½æ¸©åº¦è¾¹ç•Œé™„è¿‘ï¼ŒQ-ballåŠå¾„å‘æ•£æ—¶ï¼Œåˆ©ç”¨Alex Abrikosovçš„æ¶¨è½è¶…å¯¼ç”µæ€§è®¡ç®—æ–¹æ³•è®¡ç®—äº†å› æ•´ä¸ªQ-ballæ»‘åŠ¨è€Œäº§ç”Ÿçš„ç”µé˜»ç‡ã€‚æ­¤å¤–ï¼Œè¿˜è®¡ç®—äº†Q-ballæ°”ä½“çš„æŠ—ç£æ€§å“åº”ï¼Œä¸å®éªŒæ•°æ®ç›¸ç¬¦è‰¯å¥½ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶æ­ç¤ºäº†å…³è”ç”µå­ç³»ç»Ÿå› å½¢æˆå†…éƒ¨å…·æœ‰ç»è‰²é¢‘ç‡çš„Q-ballè€Œäº§ç”Ÿçš„ä¸åŒç‰¹æ€§ï¼Œåœ¨Matsubaraæ—¶é—´ä¸­å½¢æˆé‡å­çƒ­åŠ›å­¦æ—¶é—´å¤šæ™¶ä½“ã€‚æœ¬ç ”ç©¶ä¸ºâ€œå¥‡å¼‚é‡‘å±â€ç›¸é«˜æ¸©é“œé…¸ç›çš„å®éªŒç‰¹æ€§æä¾›äº†å¯èƒ½çš„æœºåˆ¶çº¿ç´¢ï¼Œå¹¶ä¸ºæˆ‘ä»¬å¯¹é‡å­çƒ­åŠ›å­¦æ—¶é—´æ™¶ä½“æ¨¡å‹çš„æ¢è®¨æä¾›äº†æ”¯æŒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Q-ballæœºåˆ¶ç†è®ºå¾—åˆ°å¾®Xå°„çº¿è¡å°„æ•°æ®çš„æ”¯æŒï¼Œè¯å®ä¼ªé—´éš™æ€å’Œé«˜æ¸©è¶…å¯¼æ€§çš„å…³ç³»ã€‚</li>
<li>å±•ç¤ºäº†ç”µé˜»ç‡çš„T-çº¿æ€§æ¸©åº¦ä¾èµ–æ€§åœ¨Q-ballæ°”æ€ç›¸ä¸­çš„è‡ªç„¶äº§ç”Ÿï¼Œè¿™è§£é‡Šäº†â€œå¥‡å¼‚é‡‘å±â€ç›¸ä¸­é«˜æ¸©é“œé…¸ç›çš„å®éªŒæ•°æ®ã€‚</li>
<li>ç”µé˜»ç‡äº§ç”Ÿäºç”µå­ä¸å‡èšæ€ç”µè·&#x2F;è‡ªæ—‹æ³¢åŠ¨çš„Q-ballæ°”ä½“çš„æ•£å°„ã€‚</li>
<li>åœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„æœ€ä½æ¸©åº¦è¾¹ç•Œé™„è¿‘ï¼Œè®¡ç®—äº†å› æ•´ä¸ªQ-ballæ»‘åŠ¨è€Œäº§ç”Ÿçš„ç”µé˜»ç‡ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„ç†è®ºé¢„æµ‹æ€§ã€‚</li>
<li>Q-ballæ°”ä½“çš„æŠ—ç£æ€§å“åº”ä¸å®éªŒæ•°æ®ç›¸ç¬¦ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†ç†è®ºçš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å…³è”ç”µå­ç³»ç»Ÿå› å½¢æˆQ-ballè€Œäº§ç”Ÿçš„ç‰¹æ€§ï¼Œè¿™äº›Q-ballåœ¨Matsubaraæ—¶é—´ä¸­å½¢æˆé‡å­çƒ­åŠ›å­¦æ—¶é—´å¤šæ™¶ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-405679086928f9c472917e2e1f03f09c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Shape-Experts-MoSE-End-to-End-Shape-Dictionary-Framework-to-Prompt-SAM-for-Generalizable-Medical-Segmentation"><a href="#Mixture-of-Shape-Experts-MoSE-End-to-End-Shape-Dictionary-Framework-to-Prompt-SAM-for-Generalizable-Medical-Segmentation" class="headerlink" title="Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework   to Prompt SAM for Generalizable Medical Segmentation"></a>Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework   to Prompt SAM for Generalizable Medical Segmentation</h2><p><strong>Authors:Jia Wei, Xiaoqi Zhao, Jonghye Woo, Jinsong Ouyang, Georges El Fakhri, Qingyu Chen, Xiaofeng Liu</strong></p>
<p>Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are not readily compatible with large foundation models such as the Segment Anything Model (SAM). In this paper, we propose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly integrates the idea of mixture-of-experts (MoE) training into dictionary learning to efficiently capture diverse and robust shape priors. Our method conceptualizes each dictionary atom as a shape expert, which specializes in encoding distinct semantic shape information. A gating network dynamically fuses these shape experts into a robust shape map, with sparse activation guided by SAM encoding to prevent overfitting. We further provide this shape map as a prompt to SAM, utilizing the powerful generalization capability of SAM through bidirectional integration. All modules, including the shape dictionary, are trained in an end-to-end manner. Extensive experiments on multiple public datasets demonstrate its effectiveness. </p>
<blockquote>
<p>å•åŸŸæ³›åŒ–ï¼ˆSDGï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¿‘æœŸå¸å¼•äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å¯¹äºSDGæ¥è¯´ï¼Œä¸€ç§æœ‰å‰æ™¯çš„ç­–ç•¥æ˜¯åˆ©ç”¨ä¸åŒæˆåƒåè®®ã€æ‰«æä»ªä¾›åº”å•†å’Œä¸´åºŠç«™ç‚¹ä¹‹é—´çš„è¯­ä¹‰å½¢çŠ¶å…ˆéªŒçš„ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¼–ç å½¢çŠ¶å…ˆéªŒçš„å­—å…¸å­¦ä¹ æ–¹æ³•å¸¸å¸¸å—åˆ°ç¦»çº¿è®¡ç®—å½¢çŠ¶å…ƒç´ é›†è¾ƒå°çš„ä»£è¡¨æ€§ä¸è¶³çš„å›°æ‰°ï¼Œæˆ–è€…åœ¨å­—å…¸å¤§å°å¢åŠ æ—¶äº§ç”Ÿè¿‡æ‹Ÿåˆç°è±¡ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸å¤§å‹çš„åŸºçŸ³æ¨¡å‹å¦‚â€œAnythingåˆ†å‰²æ¨¡å‹â€ï¼ˆSAMï¼‰å¹¶ä¸å…¼å®¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆå½¢çŠ¶ä¸“å®¶ï¼ˆMoSEï¼‰æ¡†æ¶ï¼Œå®ƒå°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è®­ç»ƒçš„ç†å¿µæ— ç¼åœ°é›†æˆåˆ°å­—å…¸å­¦ä¹ ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·å¤šæ ·åŒ–å’Œç¨³å¥çš„å½¢çŠ¶å…ˆéªŒã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¯ä¸ªå­—å…¸åŸå­æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸“å®¶ï¼Œä¸“é—¨ç”¨äºç¼–ç ä¸åŒçš„è¯­ä¹‰å½¢çŠ¶ä¿¡æ¯ã€‚é—¨æ§ç½‘ç»œåŠ¨æ€åœ°èåˆè¿™äº›å½¢çŠ¶ä¸“å®¶åˆ°ä¸€ä¸ªç¨³å¥çš„å½¢çŠ¶å›¾ä¸­ï¼Œé€šè¿‡SAMç¼–ç å¼•å¯¼ç¨€ç–æ¿€æ´»ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†è¿™ä¸ªå½¢çŠ¶å›¾ä½œä¸ºSAMçš„æç¤ºï¼Œé€šè¿‡åŒå‘é›†æˆåˆ©ç”¨SAMçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æ‰€æœ‰æ¨¡å—ï¼ŒåŒ…æ‹¬å½¢çŠ¶å­—å…¸ï¼Œéƒ½æ˜¯ç«¯åˆ°ç«¯è¿›è¡Œè®­ç»ƒçš„ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09601v1">PDF</a> Accepted to CVPR 2025 workshop</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡å…³æ³¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å•åŸŸæ³›åŒ–ï¼ˆSDGï¼‰é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„Mixture-of-Shape-Expertsï¼ˆMoSEï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è®­ç»ƒçš„æ€æƒ³ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰å¤šæ ·ä¸”ç¨³å¥çš„å½¢çŠ¶å…ˆéªŒã€‚MoSEå°†æ¯ä¸ªå­—å…¸åŸå­æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸“å®¶ï¼Œä¸“é—¨ç”¨äºç¼–ç ä¸åŒçš„è¯­ä¹‰å½¢çŠ¶ä¿¡æ¯ã€‚é€šè¿‡åŠ¨æ€èåˆè¿™äº›å½¢çŠ¶ä¸“å®¶ï¼Œå½¢æˆä¸€ä¸ªç¨³å¥çš„å½¢çŠ¶å›¾ï¼Œå¹¶å€ŸåŠ©SAMç¼–ç è¿›è¡Œç¨€ç–æ¿€æ´»ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å°†å½¢çŠ¶å›¾ä½œä¸ºSAMçš„æç¤ºï¼Œåˆ©ç”¨SAMçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›å®ç°åŒå‘é›†æˆã€‚æ‰€æœ‰æ¨¡å—åŒ…æ‹¬å½¢çŠ¶å­—å…¸å‡ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« å…³æ³¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å•åŸŸæ³›åŒ–é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç¼–ç å½¢çŠ¶å…ˆéªŒçš„å­—å…¸å­¦ä¹ æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä»£è¡¨æ€§ä¸è¶³å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„Mixture-of-Shape-Expertsï¼ˆMoSEï¼‰æ¡†æ¶ï¼Œç»“åˆæ··åˆä¸“å®¶è®­ç»ƒæ€æƒ³æå‡å½¢çŠ¶å…ˆéªŒæ•æ‰æ•ˆç‡ã€‚</li>
<li>MoSEå°†æ¯ä¸ªå­—å…¸åŸå­è§†ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸“å®¶ï¼Œä¸“é—¨ç¼–ç è¯­ä¹‰å½¢çŠ¶ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡åŠ¨æ€èåˆå½¢çŠ¶ä¸“å®¶å½¢æˆç¨³å¥çš„å½¢çŠ¶å›¾ï¼Œå€ŸåŠ©SAMç¼–ç é˜²æ­¢è¿‡æ‹Ÿåˆã€‚</li>
<li>å°†å½¢çŠ¶å›¾ä½œä¸ºSAMçš„æç¤ºï¼Œåˆ©ç”¨SAMçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›å®ç°åŒå‘é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dceed0852febdf1cc21120fe86274e06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd2a4a249c823250c0afbe7e463bcbe8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-662be9b3079f1fed4847cc443f4353e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3069cbce0dd465b04d981aef7d37ea4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DualPrompt-MedCap-A-Dual-Prompt-Enhanced-Approach-for-Medical-Image-Captioning"><a href="#DualPrompt-MedCap-A-Dual-Prompt-Enhanced-Approach-for-Medical-Image-Captioning" class="headerlink" title="DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image   Captioning"></a>DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image   Captioning</h2><p><strong>Authors:Yining Zhao, Ali Braytee, Mukesh Prasad</strong></p>
<p>Medical image captioning via vision-language models has shown promising potential for clinical diagnosis assistance. However, generating contextually relevant descriptions with accurate modality recognition remains challenging. We present DualPrompt-MedCap, a novel dual-prompt enhancement framework that augments Large Vision-Language Models (LVLMs) through two specialized components: (1) a modality-aware prompt derived from a semi-supervised classification model pretrained on medical question-answer pairs, and (2) a question-guided prompt leveraging biomedical language model embeddings. To address the lack of captioning ground truth, we also propose an evaluation framework that jointly considers spatial-semantic relevance and medical narrative quality. Experiments on multiple medical datasets demonstrate that DualPrompt-MedCap outperforms the baseline BLIP-3 by achieving a 22% improvement in modality recognition accuracy while generating more comprehensive and question-aligned descriptions. Our method enables the generation of clinically accurate reports that can serve as medical expertsâ€™ prior knowledge and automatic annotations for downstream vision-language tasks. </p>
<blockquote>
<p>é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒæè¿°å·²ç»æ˜¾ç¤ºå‡ºåœ¨è¾…åŠ©ä¸´åºŠè¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”Ÿæˆå…·æœ‰å‡†ç¡®æ¨¡æ€è¯†åˆ«èƒ½åŠ›çš„ä¸Šä¸‹æ–‡ç›¸å…³æè¿°ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†DualPrompt-MedCapï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒæç¤ºå¢å¼ºæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªä¸“ä¸šç»„ä»¶å¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªåŸºäºåŠç›‘ç£åˆ†ç±»æ¨¡å‹åœ¨åŒ»å­¦é—®ç­”å¯¹ä¸Šé¢„è®­ç»ƒçš„æ¨¡æ€æ„ŸçŸ¥æç¤ºï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåˆ©ç”¨ç”Ÿç‰©åŒ»å­¦è¯­è¨€æ¨¡å‹åµŒå…¥çš„é—®é¢˜å¼•å¯¼æç¤ºã€‚ä¸ºäº†è§£å†³ç¼ºä¹æè¿°æ€§çœŸå®æ ‡ç­¾çš„é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶è€ƒè™‘ç©ºé—´è¯­ä¹‰ç›¸å…³æ€§å’ŒåŒ»å­¦å™è¿°è´¨é‡ã€‚åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDualPrompt-MedCapç›¸è¾ƒäºåŸºçº¿BLIP-3åœ¨æ¨¡æ€è¯†åˆ«å‡†ç¡®åº¦ä¸Šæé«˜äº†22%ï¼ŒåŒæ—¶ç”Ÿæˆäº†æ›´å…¨é¢ã€æ›´ç¬¦åˆé—®é¢˜æ–¹å‘çš„æè¿°ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆä¸´åºŠå‡†ç¡®çš„æŠ¥å‘Šï¼Œå¯ä½œä¸ºåŒ»å­¦ä¸“å®¶çš„å…ˆéªŒçŸ¥è¯†å’Œä¸‹æ¸¸è§†è§‰è¯­è¨€ä»»åŠ¡çš„è‡ªåŠ¨æ³¨é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09598v1">PDF</a> 11 pages, 4 figures, 2 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒåŒ»å­¦å›¾åƒæè¿°åœ¨ä¸´åºŠè¯Šæ–­è¾…åŠ©ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”Ÿæˆå…·æœ‰å‡†ç¡®æ¨¡æ€è¯†åˆ«çš„ä¸Šä¸‹æ–‡ç›¸å…³æè¿°ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºDualPrompt-MedCapï¼Œä¸€ç§æ–°å‹åŒæç¤ºå¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªä¸“ä¸šç»„ä»¶å¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ï¼šï¼ˆ1ï¼‰åŸºäºåŒ»å­¦é—®ç­”å¯¹åŠç›‘ç£åˆ†ç±»æ¨¡å‹é¢„è®­ç»ƒçš„æ¨¡æ€æ„ŸçŸ¥æç¤ºï¼›ï¼ˆ2ï¼‰åˆ©ç”¨ç”Ÿç‰©åŒ»å­¦è¯­è¨€æ¨¡å‹åµŒå…¥çš„é—®é¢˜å¼•å¯¼æç¤ºã€‚ä¸ºè§£å†³ç¼ºä¹æè¿°æ€§çœŸå®æ ‡ç­¾çš„é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆè€ƒè™‘ç©ºé—´è¯­ä¹‰ç›¸å…³æ€§å’ŒåŒ»å­¦å™äº‹è´¨é‡ã€‚åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDualPrompt-MedCapç›¸è¾ƒäºåŸºçº¿BLIP-3æ¨¡å‹åœ¨æ¨¡æ€è¯†åˆ«å‡†ç¡®ç‡ä¸Šæé«˜äº†22%ï¼ŒåŒæ—¶ç”Ÿæˆæ›´å…¨é¢ã€æ›´ç¬¦åˆé—®é¢˜å¯¼å‘çš„æè¿°ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆä¸´åºŠå‡†ç¡®çš„æŠ¥å‘Šï¼Œå¯ä½œä¸ºåŒ»å­¦ä¸“å®¶çš„å…ˆéªŒçŸ¥è¯†å’Œä¸‹æ¸¸è§†è§‰è¯­è¨€ä»»åŠ¡çš„è‡ªåŠ¨æ³¨é‡Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæè¿°åœ¨ä¸´åºŠè¯Šæ–­è¾…åŠ©ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç”Ÿæˆå…·æœ‰å‡†ç¡®æ¨¡æ€è¯†åˆ«çš„ä¸Šä¸‹æ–‡ç›¸å…³æè¿°æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>DualPrompt-MedCapæ¡†æ¶é€šè¿‡ä¸¤ä¸ªä¸“ä¸šç»„ä»¶å¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼šæ¨¡æ€æ„ŸçŸ¥æç¤ºå’Œé—®é¢˜å¼•å¯¼æç¤ºã€‚</li>
<li>æ¨¡æ€æ„ŸçŸ¥æç¤ºåŸºäºåŒ»å­¦é—®ç­”å¯¹åŠç›‘ç£åˆ†ç±»æ¨¡å‹é¢„è®­ç»ƒã€‚</li>
<li>é—®é¢˜å¼•å¯¼æç¤ºåˆ©ç”¨ç”Ÿç‰©åŒ»å­¦è¯­è¨€æ¨¡å‹åµŒå…¥ã€‚</li>
<li>æå‡ºä¸€ä¸ªè”åˆè€ƒè™‘ç©ºé—´è¯­ä¹‰ç›¸å…³æ€§å’ŒåŒ»å­¦å™äº‹è´¨é‡çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šï¼ŒDualPrompt-MedCapç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æ˜¾è‘—æé«˜æ¨¡æ€è¯†åˆ«å‡†ç¡®ç‡ï¼Œç”Ÿæˆæ›´å…¨é¢ã€é—®é¢˜å¯¼å‘çš„æè¿°ï¼Œèƒ½ç”Ÿæˆä¸´åºŠå‡†ç¡®çš„æŠ¥å‘Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9fba418cba842448f987f9103a295208.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0bbe270aed46f9db7295922568908f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed5a7482da0d7934354d3ac0c553c9fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Structure-Accurate-Medical-Image-Translation-based-on-Dynamic-Frequency-Balance-and-Knowledge-Guidance"><a href="#Structure-Accurate-Medical-Image-Translation-based-on-Dynamic-Frequency-Balance-and-Knowledge-Guidance" class="headerlink" title="Structure-Accurate Medical Image Translation based on Dynamic Frequency   Balance and Knowledge Guidance"></a>Structure-Accurate Medical Image Translation based on Dynamic Frequency   Balance and Knowledge Guidance</h2><p><strong>Authors:Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao</strong></p>
<p>Multimodal medical images play a crucial role in the precise and comprehensive clinical diagnosis. Diffusion model is a powerful strategy to synthesize the required medical images. However, existing approaches still suffer from the problem of anatomical structure distortion due to the overfitting of high-frequency information and the weakening of low-frequency information. Thus, we propose a novel method based on dynamic frequency balance and knowledge guidance. Specifically, we first extract the low-frequency and high-frequency components by decomposing the critical features of the model using wavelet transform. Then, a dynamic frequency balance module is designed to adaptively adjust frequency for enhancing global low-frequency features and effective high-frequency details as well as suppressing high-frequency noise. To further overcome the challenges posed by the large differences between different medical modalities, we construct a knowledge-guided mechanism that fuses the prior clinical knowledge from a visual language model with visual features, to facilitate the generation of accurate anatomical structures. Experimental evaluations on multiple datasets show the proposed method achieves significant improvements in qualitative and quantitative assessments, verifying its effectiveness and superiority. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ç²¾ç¡®å…¨é¢çš„ä¸´åºŠè¯Šæ–­ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ‰©æ•£æ¨¡å‹æ˜¯åˆæˆæ‰€éœ€åŒ»å­¦å›¾åƒçš„ä¸€ç§å¼ºå¤§ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶å­˜åœ¨å› é«˜é¢‘ä¿¡æ¯è¿‡æ‹Ÿåˆå’Œä½é¢‘ä¿¡æ¯å‡å¼±è€Œå¯¼è‡´çš„è§£å‰–ç»“æ„å¤±çœŸé—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†å¼•å¯¼çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹çš„å…³é”®ç‰¹å¾æ¥æå–ä½é¢‘å’Œé«˜é¢‘æˆåˆ†ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥è‡ªé€‚åº”åœ°è°ƒæ•´é¢‘ç‡ï¼Œä»¥å¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆçš„é«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚ä¸ºäº†è¿›ä¸€æ­¥å…‹æœä¸åŒåŒ»å­¦æ¨¡æ€ä¹‹é—´å·®å¼‚è¾ƒå¤§çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ä¸è§†è§‰ç‰¹å¾ç›¸ç»“åˆï¼Œæœ‰åŠ©äºç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09441v1">PDF</a> Medical image translation, Diffusion model, 16 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†å¼•å¯¼çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆä¸­æé«˜äº†è¯Šæ–­ç²¾ç¡®åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹çš„å…³é”®ç‰¹å¾ï¼Œæå–é«˜ä½é¢‘æˆåˆ†ï¼Œå¹¶è®¾è®¡åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—è‡ªé€‚åº”è°ƒæ•´é¢‘ç‡ï¼Œå¢å¼ºå…¨å±€ä½é¢‘ç‰¹å¾å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœä¸åŒåŒ»å­¦æ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œæ„å»ºäº†çŸ¥è¯†å¼•å¯¼æœºåˆ¶ï¼Œèåˆäº†æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ï¼Œæœ‰åŠ©äºç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåœ¨ä¸´åºŠè¯Šæ–­ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯åˆæˆåŒ»å­¦å›¾åƒçš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨è§£å‰–ç»“æ„æ‰­æ›²é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºé«˜é¢‘ä¿¡æ¯çš„è¿‡æ‹Ÿåˆå’Œä½é¢‘ä¿¡æ¯çš„å‡å¼±ã€‚</li>
<li>æå‡ºäº†åŸºäºåŠ¨æ€é¢‘ç‡å¹³è¡¡å’ŒçŸ¥è¯†å¼•å¯¼çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å°æ³¢å˜æ¢åˆ†è§£æ¨¡å‹çš„å…³é”®ç‰¹å¾ã€‚</li>
<li>åŠ¨æ€é¢‘ç‡å¹³è¡¡æ¨¡å—å¯è‡ªé€‚åº”è°ƒæ•´é¢‘ç‡ï¼Œå¢å¼ºä½é¢‘å’Œæœ‰æ•ˆé«˜é¢‘ç»†èŠ‚ï¼ŒæŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚</li>
<li>çŸ¥è¯†å¼•å¯¼æœºåˆ¶èåˆäº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆéªŒä¸´åºŠçŸ¥è¯†ï¼Œæœ‰åŠ©äºç”Ÿæˆå‡†ç¡®çš„è§£å‰–ç»“æ„ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å®šæ€§å’Œå®šé‡è¯„ä¼°çš„æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59c78c60ea084f22d34cd5f9d3ea7a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b404e101c127f39edab2fb3f81f3efa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13b2cc77bb25f602d4436a16eb69167f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f84f0c62d5592bddc9f43b73e0577f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ce23a5ca6d3d3889891fca53d0d3f5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AerOSeg-Harnessing-SAM-for-Open-Vocabulary-Segmentation-in-Remote-Sensing-Images"><a href="#AerOSeg-Harnessing-SAM-for-Open-Vocabulary-Segmentation-in-Remote-Sensing-Images" class="headerlink" title="AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote   Sensing Images"></a>AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote   Sensing Images</h2><p><strong>Authors:Saikat Dutta, Akhil Vasim, Siddhant Gole, Hamid Rezatofighi, Biplab Banerjee</strong></p>
<p>Image segmentation beyond predefined categories is a key challenge in remote sensing, where novel and unseen classes often emerge during inference. Open-vocabulary image Segmentation addresses these generalization issues in traditional supervised segmentation models while reducing reliance on extensive per-pixel annotations, which are both expensive and labor-intensive to obtain. Most Open-Vocabulary Segmentation (OVS) methods are designed for natural images but struggle with remote sensing data due to scale variations, orientation changes, and complex scene compositions. This necessitates the development of OVS approaches specifically tailored for remote sensing. In this context, we propose AerOSeg, a novel OVS approach for remote sensing data. First, we compute robust image-text correlation features using multiple rotated versions of the input image and domain-specific prompts. These features are then refined through spatial and class refinement blocks. Inspired by the success of the Segment Anything Model (SAM) in diverse domains, we leverage SAM features to guide the spatial refinement of correlation features. Additionally, we introduce a semantic back-projection module and loss to ensure the seamless propagation of SAMâ€™s semantic information throughout the segmentation pipeline. Finally, we enhance the refined correlation features using a multi-scale attention-aware decoder to produce the final segmentation map. We validate our SAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark remote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms state-of-the-art open-vocabulary segmentation methods, achieving an average improvement of 2.54 h-mIoU. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²è¶…è¶Šé¢„å®šä¹‰ç±»åˆ«æ˜¯é¥æ„Ÿä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå…¶ä¸­åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç»å¸¸å‡ºç°æ–°çš„å’Œæœªè§è¿‡çš„ç±»åˆ«ã€‚å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²è§£å†³äº†ä¼ ç»Ÿç›‘ç£åˆ†å‰²æ¨¡å‹çš„æ³›åŒ–é—®é¢˜ï¼ŒåŒæ—¶å‡å°‘äº†å¯¹é¢å…ƒæ³¨é‡Šçš„ä¾èµ–ï¼Œè¿™äº›æ³¨é‡Šæ—¢æ˜‚è´µåˆè€—åŠ³åŠ›æ‰èƒ½è·å¾—ã€‚å¤§å¤šæ•°å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼ˆOVSï¼‰æ–¹æ³•æ—¨åœ¨ç”¨äºè‡ªç„¶å›¾åƒï¼Œä½†ç”±äºå°ºåº¦å˜åŒ–ã€æ–¹å‘å˜åŒ–å’Œå¤æ‚çš„åœºæ™¯ç»„åˆï¼Œå®ƒä»¬åœ¨é¥æ„Ÿæ•°æ®ä¸Šè¡¨ç°æŒ£æ‰ã€‚å› æ­¤ï¼Œæœ‰å¿…è¦å¼€å‘ä¸“é—¨ç”¨äºé¥æ„Ÿçš„OVSæ–¹æ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†AerOSegï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé¥æ„Ÿæ•°æ®çš„æ–°å‹OVSæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¾“å…¥å›¾åƒçš„å¤šä¸ªæ—‹è½¬ç‰ˆæœ¬å’Œç‰¹å®šé¢†åŸŸçš„æç¤ºæ¥è®¡ç®—ç¨³å¥çš„å›¾åƒæ–‡æœ¬å…³è”ç‰¹å¾ã€‚ç„¶åé€šè¿‡ç©ºé—´å’Œç±»åˆ«ç»†åŒ–å—å¯¹è¿™äº›ç‰¹å¾è¿›è¡Œç»†åŒ–ã€‚å—åˆ°å¤šæ ·é¢†åŸŸä¸­åˆ†æ®µä»»ä½•äº‹ç‰©æ¨¡å‹ï¼ˆSAMï¼‰æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨SAMç‰¹å¾æ¥æŒ‡å¯¼å…³è”ç‰¹å¾çš„ç©ºé—´ç»†åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯­ä¹‰åå‘æŠ•å½±æ¨¡å—å’ŒæŸå¤±æ¥ç¡®ä¿SAMçš„è¯­ä¹‰ä¿¡æ¯åœ¨æ•´ä¸ªåˆ†å‰²ç®¡é“ä¸­çš„æ— ç¼ä¼ æ’­ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šå°ºåº¦æ³¨æ„åŠ›æ„ŸçŸ¥è§£ç å™¨å¢å¼ºç»†åŒ–çš„å…³è”ç‰¹å¾ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²å›¾ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†é¥æ„Ÿæ•°æ®é›†iSAIDã€DLRSDå’ŒOpenEarthMapä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„SAMå¼•å¯¼å¼€æ”¾è¯æ±‡é¥æ„Ÿåˆ†å‰²æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºæœ€å…ˆè¿›çš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†2.54 h-mIoUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09203v1">PDF</a> Accepted at EarthVision workshop, CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­è¶…è¶Šé¢„è®¾ç±»åˆ«çš„é—®é¢˜ï¼Œæå‡ºäº†AerOSegè¿™ä¸€æ–°å‹å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—å›¾åƒæ–‡æœ¬ç›¸å…³ç‰¹å¾ã€ç©ºé—´åŠç±»åˆ«ç»†åŒ–å—çš„å¤„ç†ï¼Œå¹¶ç»“åˆäº†Segment Anything Modelï¼ˆSAMï¼‰çš„ç‰¹æ€§è¿›è¡Œç©ºé—´ç»†åŒ–ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è¯­ä¹‰åå‘æŠ•å½±æ¨¡å—å’ŒæŸå¤±å‡½æ•°æ¥ç¡®ä¿SAMè¯­ä¹‰ä¿¡æ¯çš„æ— ç¼ä¼ æ’­ï¼Œå¹¶ä½¿ç”¨å¤šå°ºåº¦æ³¨æ„åŠ›æ„ŸçŸ¥è§£ç å™¨ç”Ÿæˆæœ€ç»ˆåˆ†å‰²å›¾ã€‚åœ¨ä¸‰ä¸ªé¥æ„Ÿæ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†2.54 h-mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåˆ†å‰²é¢ä¸´è¶…è¶Šé¢„è®¾ç±»åˆ«çš„é—®é¢˜ï¼Œéœ€è¦å¼€å‘æ–°çš„æ–¹æ³•ä»¥åº”å¯¹ã€‚</li>
<li>AerOSegæ˜¯ä¸€ç§æ–°å‹çš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºå¤„ç†é¥æ„Ÿå›¾åƒæ•°æ®ã€‚</li>
<li>é€šè¿‡è®¡ç®—å›¾åƒæ–‡æœ¬ç›¸å…³ç‰¹å¾ã€ç©ºé—´åŠç±»åˆ«ç»†åŒ–å—çš„å¤„ç†æ¥è§£å†³é¥æ„Ÿå›¾åƒçš„ç‰¹ç‚¹ã€‚</li>
<li>å¼•å…¥äº†Segment Anything Modelï¼ˆSAMï¼‰è¿›è¡Œç©ºé—´ç»†åŒ–ï¼Œå¹¶ç»“åˆå…¶ç‰¹æ€§ä¼˜åŒ–åˆ†å‰²æ•ˆæœã€‚</li>
<li>è¯­ä¹‰åå‘æŠ•å½±æ¨¡å—ç¡®ä¿SAMè¯­ä¹‰ä¿¡æ¯çš„æ— ç¼ä¼ æ’­ã€‚</li>
<li>ä½¿ç”¨å¤šå°ºåº¦æ³¨æ„åŠ›æ„ŸçŸ¥è§£ç å™¨ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8f3353b69728b969dff7481df442e13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78586af2abc3f2702f498e510ebde4c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6eac889d3aebed9c7a70d4561812393.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efea7f41ee4273f696bc4bf2e1d3a6ab.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="seg2med-a-segmentation-based-medical-image-generation-framework-using-denoising-diffusion-probabilistic-models"><a href="#seg2med-a-segmentation-based-medical-image-generation-framework-using-denoising-diffusion-probabilistic-models" class="headerlink" title="seg2med: a segmentation-based medical image generation framework using   denoising diffusion probabilistic models"></a>seg2med: a segmentation-based medical image generation framework using   denoising diffusion probabilistic models</h2><p><strong>Authors:Zeyu Yang, Zhilin Chen, Yipeng Sun, Anika Strittmatter, Anish Raj, Ahmad Allababidi, Johann S. Rink, Frank G. ZÃ¶llner</strong></p>
<p>In this study, we present seg2med, an advanced medical image synthesis framework that uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality synthetic medical images conditioned on anatomical masks from TotalSegmentator. The framework synthesizes CT and MR images from segmentation masks derived from real patient data and XCAT digital phantoms, achieving a Structural Similarity Index Measure (SSIM) of 0.94 +&#x2F;- 0.02 for CT and 0.89 +&#x2F;- 0.04 for MR images compared to ground-truth images of real patients. It also achieves a Feature Similarity Index Measure (FSIM) of 0.78 +&#x2F;- 0.04 for CT images from XCAT. The generative quality is further supported by a Fr&#39;echet Inception Distance (FID) of 3.62 for CT image generation.   Additionally, seg2med can generate paired CT and MR images with consistent anatomical structures and convert images between CT and MR modalities, achieving SSIM values of 0.91 +&#x2F;- 0.03 for MR-to-CT and 0.77 +&#x2F;- 0.04 for CT-to-MR conversion. Despite the limitations of incomplete anatomical details in segmentation masks, the framework shows strong performance in cross-modality synthesis and multimodal imaging.   seg2med also demonstrates high anatomical fidelity in CT synthesis, achieving a mean Dice coefficient greater than 0.90 for 11 abdominal organs and greater than 0.80 for 34 organs out of 59 in 58 test cases. The highest Dice of 0.96 +&#x2F;- 0.01 was recorded for the right scapula. Leveraging the TotalSegmentator toolkit, seg2med enables segmentation mask generation across diverse datasets, supporting applications in clinical imaging, data augmentation, multimodal synthesis, and diagnostic algorithm development. </p>
<blockquote>
<p>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†seg2medï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆæˆæ¡†æ¶ã€‚å®ƒåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆåŒ»å­¦å›¾åƒï¼Œè¿™äº›å›¾åƒæ˜¯æ ¹æ®TotalSegmentatorçš„è§£å‰–æ©è†œè¿›è¡Œæ¡ä»¶å¤„ç†çš„ã€‚è¯¥æ¡†æ¶ä»æ¥è‡ªçœŸå®æ‚£è€…æ•°æ®å’ŒXCATæ•°å­—å¹»å½±çš„åˆ†å‰²æ©è†œä¸­åˆæˆCTå’ŒMRå›¾åƒï¼Œä¸çœŸå®æ‚£è€…çš„åŸå§‹å›¾åƒç›¸æ¯”ï¼Œå…¶ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰ä¸ºCT 0.94 +&#x2F;- 0.02ï¼ŒMRå›¾åƒä¸º0.89 +&#x2F;- 0.04ã€‚å¯¹äºæ¥è‡ªXCATçš„CTå›¾åƒï¼Œå…¶ç‰¹å¾ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆFSIMï¼‰ä¸º0.78 +&#x2F;- 0.04ã€‚CTå›¾åƒç”Ÿæˆçš„ç”Ÿæˆè´¨é‡å¾—åˆ°äº†Frâ€™echet Inception Distanceï¼ˆFIDï¼‰ä¸º3.62çš„æ”¯æŒã€‚æ­¤å¤–ï¼Œseg2medå¯ä»¥ç”Ÿæˆå…·æœ‰ä¸€è‡´è§£å‰–ç»“æ„çš„é…å¯¹CTå’ŒMRå›¾åƒï¼Œå¹¶åœ¨CTå’ŒMRæ¨¡æ€ä¹‹é—´è¿›è¡Œå›¾åƒè½¬æ¢ï¼ŒMRåˆ°CTçš„SSIMå€¼ä¸º0.91 +&#x2F;- 0.03ï¼ŒCTåˆ°MRçš„è½¬æ¢å€¼ä¸º0.77 +&#x2F;- 0.04ã€‚å°½ç®¡åˆ†å‰²æ©è†œåœ¨è§£å‰–ç»†èŠ‚ä¸Šæœ‰æ‰€ä¸è¶³ï¼Œä½†è¯¥æ¡†æ¶åœ¨è·¨æ¨¡æ€åˆæˆå’Œå¤šæ¨¡æ€æˆåƒæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚seg2medåœ¨CTåˆæˆä¸­å±•ç¤ºäº†é«˜åº¦çš„è§£å‰–ä¿çœŸåº¦ï¼Œåœ¨58ä¸ªæµ‹è¯•æ¡ˆä¾‹ä¸­ï¼Œ11ä¸ªè…¹éƒ¨å™¨å®˜çš„Diceç³»æ•°å¤§äº0.90ï¼Œå¦æœ‰è¶…è¿‡34ä¸ªå™¨å®˜çš„Diceç³»æ•°è¾¾åˆ°æˆ–è¶…è¿‡äº”åä¹çš„å…«åäº”ï¼ˆå¯¹ä¸èµ·æˆ‘å¯¹â€œå¤§äºå…«åäº”çš„ä¿¡æ¯ç¼ºä¹å……åˆ†çš„ä¸Šä¸‹ç†è§£å…¶çœŸæ­£çš„æ„ä¹‰æ‰€ä»¥ä¸èƒ½ç›´æ¥ç¿»è¯‘æˆä¸­æ–‡ï¼‰åœ¨å¯¹åˆæˆå™¨çš„æœ€é«˜ç«¯è®­ç»ƒä¸­æˆä¸ºå‡†ç¡®ç‡åˆ°è¾¾ç”šè‡³åˆ°äº†æœ‰åˆ†æ•°çš„æŒ¯å¹…ç›¸å·®ä»¥åç‚¹ä¹åå…­æ­£è´Ÿé›¶ç‚¹é›¶ä¸€ã€‚å€ŸåŠ©TotalSegmentatorå·¥å…·åŒ…ï¼Œseg2medèƒ½å¤Ÿåœ¨å„ç§æ•°æ®é›†ä¸Šç”Ÿæˆåˆ†å‰²æ©è†œï¼Œæ”¯æŒä¸´åºŠæˆåƒã€æ•°æ®å¢å¼ºã€å¤šæ¨¡æ€åˆæˆå’Œè¯Šæ–­ç®—æ³•å¼€å‘çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09182v1">PDF</a> 17 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>seg2medæ¡†æ¶åˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰å®ç°äº†é«˜è´¨é‡åŒ»å­¦å›¾åƒåˆæˆï¼Œå¯æ ¹æ®TotalSegmentatorçš„è§£å‰–éƒ¨ä½æ©è†œç”ŸæˆCTå’ŒMRå›¾åƒã€‚è¯¥æ¡†æ¶å®ç°äº†ä¸çœŸå®æ‚£è€…å›¾åƒçš„é«˜ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰ä»¥åŠç‰¹å¾ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆFSIMï¼‰ï¼Œå¹¶åœ¨è·¨æ¨¡æ€åˆæˆå’Œå¤šæ¨¡æ€æˆåƒæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚seg2medè¿˜å±•ç¤ºäº†åœ¨CTåˆæˆä¸­çš„é«˜è§£å‰–ä¿çœŸåº¦ï¼Œå¹¶åœ¨å¤šç§æµ‹è¯•æ¡ˆä¾‹ä¸­å®ç°äº†é«˜çš„Diceç³»æ•°ã€‚è¯¥æ¡†æ¶æ”¯æŒä¸´åºŠåº”ç”¨ã€æ•°æ®å¢å¼ºã€å¤šæ¨¡æ€åˆæˆå’Œè¯Šæ–­ç®—æ³•å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>seg2medæ˜¯ä¸€ä¸ªåŸºäºDenoising Diffusion Probabilistic Modelsï¼ˆDDPMï¼‰çš„å…ˆè¿›åŒ»å­¦å›¾åƒåˆæˆæ¡†æ¶ã€‚</li>
<li>å®ƒèƒ½å¤Ÿä¾æ®TotalSegmentatorçš„è§£å‰–éƒ¨ä½æ©è†œç”ŸæˆCTå’ŒMRå›¾åƒã€‚</li>
<li>seg2medåˆæˆçš„å›¾åƒä¸çœŸå®æ‚£è€…å›¾åƒçš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰é«˜ï¼Œç”Ÿæˆè´¨é‡å¾—åˆ°FrÃ©chet Inception Distanceï¼ˆFIDï¼‰çš„è¿›ä¸€æ­¥æ”¯æŒã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿå®ç°é…å¯¹CTå’ŒMRå›¾åƒçš„ç”Ÿæˆï¼Œä»¥åŠCTå’ŒMRæ¨¡æ€ä¹‹é—´çš„å›¾åƒè½¬æ¢ï¼Œè¡¨ç°å‡ºè·¨æ¨¡æ€åˆæˆå’Œå¤šæ¨¡æ€æˆåƒçš„å¼ºå¤§æ€§èƒ½ã€‚</li>
<li>seg2medåœ¨CTåˆæˆä¸­å±•ç¤ºé«˜è§£å‰–ä¿çœŸåº¦ï¼Œåœ¨å¤šç§æµ‹è¯•æ¡ˆä¾‹ä¸­å®ç°äº†é«˜çš„Diceç³»æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb5b601d69903d34415ce0b6c1909245.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-762a34aa87a48fb6d900e36800614156.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-beddd93c212f69b86bda62d7afd34f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751a0ae8a41d1d0818d406388ae253e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7e5c92d65ab4db37165bfde59c4861e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Modal-Brain-Tumor-Segmentation-via-3D-Multi-Scale-Self-attention-and-Cross-attention"><a href="#Multi-Modal-Brain-Tumor-Segmentation-via-3D-Multi-Scale-Self-attention-and-Cross-attention" class="headerlink" title="Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention   and Cross-attention"></a>Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention   and Cross-attention</h2><p><strong>Authors:Yonghao Huang, Leiting Chen, Chuan Zhou</strong></p>
<p>Due to the success of CNN-based and Transformer-based models in various computer vision tasks, recent works study the applicability of CNN-Transformer hybrid architecture models in 3D multi-modality medical segmentation tasks. Introducing Transformer brings long-range dependent information modeling ability in 3D medical images to hybrid models via the self-attention mechanism. However, these models usually employ fixed receptive fields of 3D volumetric features within each self-attention layer, ignoring the multi-scale volumetric lesion features. To address this issue, we propose a CNN-Transformer hybrid 3D medical image segmentation model, named TMA-TransBTS, based on an encoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of multi-scale 3D features and modeling of long-distance dependencies by multi-scale division and aggregation of 3D tokens in a self-attention layer. Furthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to establish a link between the encoder and the decoder for extracting rich volume representations by exploiting the mutual attention mechanism of cross-attention and multi-scale aggregation of 3D tokens. Extensive experimental results on three public 3D medical segmentation datasets show that TMA-TransBTS achieves higher averaged segmentation results than previous state-of-the-art CNN-based 3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D multi-modality brain tumors. </p>
<blockquote>
<p>ç”±äºCNNå’ŒTransformeræ¨¡å‹åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ï¼Œè¿‘æœŸçš„ç ”ç©¶å¼€å§‹æ¢è®¨CNN-Transformeræ··åˆæ¶æ„æ¨¡å‹åœ¨3Då¤šæ¨¡æ€åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚å¼•å…¥Transformeré€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ºæ··åˆæ¨¡å‹å¸¦æ¥äº†åœ¨3DåŒ»å­¦å›¾åƒä¸­çš„é•¿è·ç¦»ä¾èµ–ä¿¡æ¯å»ºæ¨¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åœ¨æ¯ä¸ªè‡ªæ³¨æ„åŠ›å±‚ä½¿ç”¨å›ºå®šçš„3Dä½“ç§¯ç‰¹å¾æ¥æ”¶åœºï¼Œå¿½ç•¥äº†å¤šå°ºåº¦ä½“ç§¯ç—…ç¶ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„CNN-Transformeræ··åˆ3DåŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œåä¸ºTMA-TransBTSã€‚TMA-TransBTSé€šè¿‡è‡ªæ³¨æ„åŠ›å±‚ä¸­çš„3Dä»¤ç‰Œçš„å¤šå°ºåº¦åˆ†å‰²å’Œèšåˆï¼Œå®ç°äº†å¤šå°ºåº¦3Dç‰¹å¾çš„åŒæ—¶æå–å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»çš„å»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒTMA-TransBTSè¿˜æå‡ºäº†ä¸€ä¸ª3Då¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´å»ºç«‹è”ç³»ï¼Œé€šè¿‡åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›çš„ç›¸äº’æ³¨æ„æœºåˆ¶å’Œ3Dä»¤ç‰Œçš„å¤šå°ºåº¦èšåˆï¼Œæ¥æå–ä¸°å¯Œçš„ä½“ç§¯è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„3DåŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹äº3Då¤šæ¨¡æ€è„‘è‚¿ç˜¤çš„åˆ†å‰²ä»»åŠ¡ï¼ŒTMA-TransBTSç›¸è¾ƒäºå…ˆå‰å…ˆè¿›çš„åŸºäºCNNçš„3Dæ–¹æ³•å’ŒCNN-Transformeræ··åˆ3Dæ–¹æ³•å–å¾—äº†æ›´é«˜çš„å¹³å‡åˆ†å‰²ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09088v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼ŒCNN-Transformeræ··åˆæ¶æ„åœ¨3Då¤šæ¨¡æ€åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½è¡¨ç°ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å¿½ç•¥å¤šå°ºåº¦å®¹ç§¯ç—…å˜ç‰¹å¾çš„é—®é¢˜ï¼Œæå‡ºåä¸ºTMA-TransBTSçš„æ··åˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œé€šè¿‡å¤šå°ºåº¦åˆ’åˆ†å’Œèšåˆè‡ªæ³¨æ„åŠ›å±‚çš„3Dæ ‡è®°ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾çš„æå–å’Œè¿œç¨‹ä¾èµ–å…³ç³»çš„å»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒTMA-TransBTSå¼•å…¥3Då¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œå»ºç«‹ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„è”ç³»ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›çš„äº’æ³¨æ„æœºåˆ¶å’Œå¤šå°ºåº¦èšåˆçš„3Dæ ‡è®°ï¼Œæå–ä¸°å¯Œçš„ä½“ç§¯è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªå…¬å…±3DåŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTMA-TransBTSåœ¨åˆ†å‰²3Då¤šæ¨¡æ€è„‘è‚¿ç˜¤æ–¹é¢çš„å¹³å‡åˆ†å‰²ç»“æœä¼˜äºå…ˆå‰çš„å…ˆè¿›CNN-basedå’ŒCNN-Transformeræ··åˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNN-Transformeræ··åˆæ¶æ„åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†3Då¤šæ¨¡æ€åŒ»å­¦å›¾åƒæ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šå°ºåº¦å®¹ç§¯ç—…å˜ç‰¹å¾æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>TMA-TransBTSæ¨¡å‹åŸºäºç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œèƒ½å¤ŸåŒæ—¶æå–å¤šå°ºåº¦ç‰¹å¾å’Œå»ºæ¨¡è¿œç¨‹ä¾èµ–å…³ç³»ã€‚</li>
<li>TMA-TransBTSé€šè¿‡å¤šå°ºåº¦åˆ’åˆ†å’Œèšåˆè‡ªæ³¨æ„åŠ›å±‚çš„3Dæ ‡è®°å®ç°è¿™ä¸€åŠŸèƒ½ã€‚</li>
<li>TMA-TransBTSå¼•å…¥3Då¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œå¢å¼ºç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>è¯¥æ¨¡å—åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›çš„äº’æ³¨æ„æœºåˆ¶å’Œå¤šå°ºåº¦èšåˆçš„3Dæ ‡è®°ï¼Œæå–ä¸°å¯Œçš„ä½“ç§¯è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTMA-TransBTSçš„åˆ†å‰²æ•ˆæœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e73b962183f81615ccad2c079763e2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54744e2ecdd0aac7efa887ec1a1838be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6f8de742615a5db9a46700d10d1c67c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5210c163057b800d56bcdfc9b73b1715.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de002163d78727d04dac5d8be4a9806a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49fdc610b77b09e4a09beb398e611a71.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Robust-SAM-On-the-Adversarial-Robustness-of-Vision-Foundation-Models"><a href="#Robust-SAM-On-the-Adversarial-Robustness-of-Vision-Foundation-Models" class="headerlink" title="Robust SAM: On the Adversarial Robustness of Vision Foundation Models"></a>Robust SAM: On the Adversarial Robustness of Vision Foundation Models</h2><p><strong>Authors:Jiahuan Long, Zhengqin Xu, Tingsong Jiang, Wen Yao, Shuai Jia, Chao Ma, Xiaoqian Chen</strong></p>
<p>The Segment Anything Model (SAM) is a widely used vision foundation model with diverse applications, including image segmentation, detection, and tracking. Given SAMâ€™s wide applications, understanding its robustness against adversarial attacks is crucial for real-world deployment. However, research on SAMâ€™s robustness is still in its early stages. Existing attacks often overlook the role of prompts in evaluating SAMâ€™s robustness, and there has been insufficient exploration of defense methods to balance the robustness and accuracy. To address these gaps, this paper proposes an adversarial robustness framework designed to evaluate and enhance the robustness of SAM. Specifically, we introduce a cross-prompt attack method to enhance the attack transferability across different prompt types. Besides attacking, we propose a few-parameter adaptation strategy to defend SAM against various adversarial attacks. To balance robustness and accuracy, we use the singular value decomposition (SVD) to constrain the space of trainable parameters, where only singular values are adaptable. Experiments demonstrate that our cross-prompt attack method outperforms previous approaches in terms of attack success rate on both SAM and SAM 2. By adapting only 512 parameters, we achieve at least a 15% improvement in mean intersection over union (mIoU) against various adversarial attacks. Compared to previous defense methods, our approach enhances the robustness of SAM while maximally maintaining its original performance. </p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰æ˜¯ä¸€ä¸ªå¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†å‰²ã€æ£€æµ‹å’Œè·Ÿè¸ªç­‰å¤šä¸ªé¢†åŸŸçš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚é‰´äºSAMçš„å¹¿æ³›åº”ç”¨ï¼Œäº†è§£å…¶å¯¹å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§å¯¹äºç°å®ä¸–ç•Œçš„éƒ¨ç½²è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå…³äºSAMé²æ£’æ€§çš„ç ”ç©¶ä»å¤„äºæ—©æœŸé˜¶æ®µã€‚ç°æœ‰çš„æ”»å‡»å¾€å¾€å¿½ç•¥äº†æç¤ºåœ¨è¯„ä¼°SAMé²æ£’æ€§ä¸­çš„ä½œç”¨ï¼Œå¯¹å¹³è¡¡é²æ£’æ€§å’Œå‡†ç¡®æ€§çš„é˜²å¾¡æ–¹æ³•çš„æ¢ç´¢ä¹Ÿä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯¹æŠ—é²æ£’æ€§æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œå¢å¼ºSAMçš„é²æ£’æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è·¨æç¤ºæ”»å‡»æ–¹æ³•ï¼Œä»¥æé«˜ä¸åŒæç¤ºç±»å‹ä¹‹é—´çš„æ”»å‡»è½¬ç§»æ€§ã€‚é™¤äº†æ”»å‡»ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å°‘å‚æ•°è‡ªé€‚åº”ç­–ç•¥æ¥é˜²å¾¡SAMå¯¹æŠ—å„ç§å¯¹æŠ—æ€§æ”»å‡»ã€‚ä¸ºäº†å¹³è¡¡é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ¥çº¦æŸå¯è®­ç»ƒå‚æ•°çš„ç©ºé—´ï¼Œå…¶ä¸­åªæœ‰å¥‡å¼‚å€¼æ˜¯å¯é€‚åº”çš„ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è·¨æç¤ºæ”»å‡»æ–¹æ³•åœ¨SAMå’ŒSAM 2ä¸Šçš„æ”»å‡»æˆåŠŸç‡é«˜äºä»¥å‰çš„æ–¹æ³•ã€‚é€šè¿‡ä»…é€‚åº”512ä¸ªå‚æ•°ï¼Œæˆ‘ä»¬åœ¨å„ç§å¯¹æŠ—æ€§æ”»å‡»ä¸‹è‡³å°‘æé«˜äº†å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰15%ã€‚ä¸ä»¥å‰çš„é˜²å¾¡æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†SAMçš„é²æ£’æ€§ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°ä¿æŒäº†å…¶åŸå§‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08906v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ã€æ£€æµ‹å’Œè·Ÿè¸ªç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶å¯¹æŠ—æ€§é²æ£’æ€§ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºä¸€ç§å¯¹æŠ—æ€§é²æ£’æ€§æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡SAMçš„é²æ£’æ€§ã€‚å¼•å…¥è·¨æç¤ºæ”»å‡»æ–¹æ³•ï¼Œæå‡ä¸åŒæç¤ºç±»å‹é—´çš„æ”»å‡»å¯è½¬ç§»æ€§ï¼Œå¹¶æå‡ºå°‘å‚æ•°é€‚åº”ç­–ç•¥è¿›è¡Œé˜²å¾¡ã€‚é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å¹³è¡¡é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œä»…é€‚åº”å°‘é‡å¥‡å¼‚å€¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ”»å‡»æˆåŠŸç‡æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨é€‚åº”å°‘é‡å‚æ•°åï¼Œå¯¹å„ç§å¯¹æŠ—æ€§æ”»å‡»çš„mIoUè‡³å°‘æé«˜äº†15%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€æ£€æµ‹å’Œè·Ÿè¸ªã€‚</li>
<li>SAMæ¨¡å‹çš„å¯¹æŠ—æ€§é²æ£’æ€§å¯¹äºå…¶åœ¨å®é™…éƒ¨ç½²ä¸­çš„åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¯¹äºSAMæ¨¡å‹é²æ£’æ€§çš„ç ”ç©¶ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œç°æœ‰çš„æ”»å‡»æ–¹æ³•å¸¸å¸¸å¿½ç•¥äº†æç¤ºåœ¨è¯„ä¼°å…¶é²æ£’æ€§ä¸­çš„ä½œç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹æŠ—æ€§é²æ£’æ€§æ¡†æ¶æ¥è¯„ä¼°å’Œæå‡SAMæ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è·¨æç¤ºæ”»å‡»æ–¹æ³•ï¼Œä»¥å¢å¼ºæ”»å‡»åœ¨ä¸åŒæç¤ºç±»å‹ä¹‹é—´çš„å¯è½¬ç§»æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å°‘å‚æ•°é€‚åº”ç­–ç•¥æ¥é˜²å¾¡å„ç§å¯¹æŠ—æ€§æ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46a5e0d123ea5f063dd938792b898635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ed41c2b41e92bc694879ef21b93236d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f20d237647a3b48f894e4c8f748806e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa6163216974997d54ec247f013e6eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41a5200c0bf9a4bccdec1722e768e74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f00ac7bb8768b23a95cded91279a6f23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1afbecd6a180d78d2065116acd1cc08.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ColonScopeX-Leveraging-Explainable-Expert-Systems-with-Multimodal-Data-for-Improved-Early-Diagnosis-of-Colorectal-Cancer"><a href="#ColonScopeX-Leveraging-Explainable-Expert-Systems-with-Multimodal-Data-for-Improved-Early-Diagnosis-of-Colorectal-Cancer" class="headerlink" title="ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data   for Improved Early Diagnosis of Colorectal Cancer"></a>ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data   for Improved Early Diagnosis of Colorectal Cancer</h2><p><strong>Authors:Natalia Sikora, Robert L. Manschke, Alethea M. Tang, Peter Dunstan, Dean A. Harris, Su Yang</strong></p>
<p>Colorectal cancer (CRC) ranks as the second leading cause of cancer-related deaths and the third most prevalent malignant tumour worldwide. Early detection of CRC remains problematic due to its non-specific and often embarrassing symptoms, which patients frequently overlook or hesitate to report to clinicians. Crucially, the stage at which CRC is diagnosed significantly impacts survivability, with a survival rate of 80-95% for Stage I and a stark decline to 10% for Stage IV. Unfortunately, in the UK, only 14.4% of cases are diagnosed at the earliest stage (Stage I).   In this study, we propose ColonScopeX, a machine learning framework utilizing explainable AI (XAI) methodologies to enhance the early detection of CRC and pre-cancerous lesions. Our approach employs a multimodal model that integrates signals from blood sample measurements, processed using the Savitzky-Golay algorithm for fingerprint smoothing, alongside comprehensive patient metadata, including medication history, comorbidities, age, weight, and BMI. By leveraging XAI techniques, we aim to render the modelâ€™s decision-making process transparent and interpretable, thereby fostering greater trust and understanding in its predictions. The proposed framework could be utilised as a triage tool or a screening tool of the general population.   This research highlights the potential of combining diverse patient data sources and explainable machine learning to tackle critical challenges in medical diagnostics. </p>
<blockquote>
<p>ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰æ˜¯å…¨çƒç¬¬äºŒå¤§ç™Œç—‡è‡´æ­»åŸå› å’Œç¬¬ä¸‰ç§æœ€å¸¸è§çš„æ¶æ€§è‚¿ç˜¤ã€‚ç”±äºç»“ç›´è‚ ç™Œçš„éç‰¹å¼‚æ€§å’Œç»å¸¸ä»¤äººå°´å°¬çš„ç—‡çŠ¶ï¼Œå…¶æ—©æœŸå‘ç°ä»ç„¶æ˜¯ä¸ªé—®é¢˜ï¼Œæ‚£è€…ç»å¸¸å¿½è§†æˆ–çŠ¹è±«æ˜¯å¦è¦å‘ä¸´åºŠåŒ»ç”ŸæŠ¥å‘Šã€‚å…³é”®çš„æ˜¯ï¼Œç»“ç›´è‚ ç™Œçš„è¯Šæ–­é˜¶æ®µå¯¹å­˜æ´»ç‡æœ‰ç€æ˜¾è‘—å½±å“ï¼ŒIæœŸçš„å­˜æ´»ç‡ä¸º80-95%ï¼Œè€ŒIVæœŸçš„å­˜æ´»ç‡åˆ™æ€¥å‰§ä¸‹é™åˆ°10%ã€‚ä¸å¹¸çš„æ˜¯ï¼Œåœ¨è‹±å›½ï¼Œä»…æœ‰14.4%çš„ç—…ä¾‹è¢«è¯Šæ–­å‡ºå¤„äºæœ€æ—©é˜¶æ®µï¼ˆIæœŸï¼‰ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ColonScopeXï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•å¢å¼ºç»“ç›´è‚ ç™Œå’Œç™Œå‰ç—…å˜æ—©æœŸæ£€æµ‹çš„æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤šæ¨¡å¼æ¨¡å‹ï¼Œæ•´åˆäº†é€šè¿‡Savitzky-Golayç®—æ³•å¤„ç†è¿‡çš„è¡€æ¶²æ ·æœ¬æµ‹é‡ä¿¡å·ä»¥è¿›è¡ŒæŒ‡çº¹å¹³æ»‘åŒ–ï¼Œä»¥åŠå…¨é¢çš„æ‚£è€…å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç”¨è¯å†å²ã€å¹¶å‘ç—‡ã€å¹´é¾„ã€ä½“é‡å’ŒBMIã€‚é€šè¿‡åˆ©ç”¨XAIæŠ€æœ¯ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹é€æ˜ä¸”å¯è§£é‡Šï¼Œä»è€Œå¢åŠ å¯¹å…¶é¢„æµ‹çš„ä¿¡ä»»å’Œç†è§£ã€‚æ‰€æå‡ºçš„æ¡†æ¶å¯ä»¥ç”¨ä½œä¸€èˆ¬äººç¾¤çš„åˆæ­¥è¯„ä¼°å·¥å…·æˆ–ç­›æŸ¥å·¥å…·ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç»“åˆå¤šç§æ‚£è€…æ•°æ®æºå’Œå¯è§£é‡Šæœºå™¨å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­è§£å†³å…³é”®æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08824v1">PDF</a> Published to AAAI-25 Bridge Program</p>
<p><strong>Summary</strong>ï¼šç»“è‚ ç™Œï¼ˆCRCï¼‰æ˜¯å…¨çƒç¬¬äºŒå¤§è‡´ç™Œæ­»å› å’Œç¬¬ä¸‰å¸¸è§çš„æ¶æ€§è‚¿ç˜¤ã€‚æ—©æœŸæ£€æµ‹å› ç—‡çŠ¶éç‰¹å¼‚ä¸”å°´å°¬å¸¸è¢«å¿½è§†ï¼Œè¯Šæ–­é˜¶æ®µå¯¹ç”Ÿå­˜ç‡æœ‰å¾ˆå¤§å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºColonScopeXï¼Œä¸€ä¸ªåˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè¡€æ¶²æ ·æœ¬æµ‹é‡ä¿¡å·å’Œæ‚£è€…ç»¼åˆä¿¡æ¯ï¼ˆå¦‚ç—…å²ã€å¹´é¾„ç­‰ï¼‰ï¼Œä»¥æé«˜CRCåŠç™Œå‰ç—…å˜çš„æ—©æœŸæ£€æµ‹æ°´å¹³ã€‚ç›®æ ‡æ˜¯æé«˜æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ï¼Œä¿ƒè¿›å…¬ä¼—å¯¹å…¶é¢„æµ‹ç»“æœçš„ä¿¡ä»»å’Œç†è§£ã€‚è¯¥æ¡†æ¶å¯ä½œä¸ºä¸€ç§é€šç”¨çš„ç­›æŸ¥å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç»“è‚ ç™Œæ˜¯å…¨çƒé‡è¦çš„å¥åº·é—®é¢˜ï¼Œæ—©æœŸæ£€æµ‹å¯¹å…¶æ²»ç–—å’Œç”Ÿå­˜ç‡æœ‰é‡è¦å½±å“ã€‚</li>
<li>å½“å‰æ—©æœŸæ£€æµ‹é¢ä¸´çš„é—®é¢˜æ˜¯æ‚£è€…å¿½è§†æˆ–éš¾ä»¥è¯†åˆ«éç‰¹å¼‚ç—‡çŠ¶ã€‚</li>
<li>åœ¨è‹±å›½ï¼Œæ—©æœŸç»“è‚ ç™Œçš„è¯Šæ–­ç‡è¾ƒä½ï¼Œä»…æœ‰14.4%ã€‚</li>
<li>ColonScopeXæ˜¯ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç»“è‚ ç™ŒåŠç™Œå‰ç—…å˜çš„æ—©æœŸæ£€æµ‹æ°´å¹³ã€‚</li>
<li>è¯¥æ¡†æ¶æ•´åˆäº†è¡€æ¶²æ ·æœ¬æµ‹é‡ä¿¡å·å’Œæ‚£è€…çš„ç»¼åˆä¿¡æ¯ï¼ŒåŒ…æ‹¬ç—…å²ã€å¹´é¾„ç­‰ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨XAIæŠ€æœ¯ï¼Œæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹æ›´åŠ é€æ˜å’Œå¯è§£é‡Šï¼Œæœ‰åŠ©äºå»ºç«‹å…¬ä¼—ä¿¡ä»»å’Œç†è§£å…¶é¢„æµ‹ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-261e829315c867f7fcce0a708d90769a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87ea7d2b35c22afc104e1eb1bcd0433c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db4d7a84e9e9db9785cec5ec51b3f503.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cca0068ba1da30ab8dabebe375ca0e9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="The-population-of-tidal-disruption-events-discovered-with-eROSITA"><a href="#The-population-of-tidal-disruption-events-discovered-with-eROSITA" class="headerlink" title="The population of tidal disruption events discovered with eROSITA"></a>The population of tidal disruption events discovered with eROSITA</h2><p><strong>Authors:Iuliia Grotova, Arne Rau, Pietro Baldini, Adelle J. Goodwin, Zhu Liu, Andrea Merloni, Mara Salvato, Gemma E. Anderson, Riccardo Arcodia, Johannes Buchner, Mirko Krumpe, Adam Malyali, Megan Masterson, James C. A. Miller-Jones, Kirpal Nandra, Raphael Shirley</strong></p>
<p>This paper presents a systematic study of X-ray-selected canonical tidal disruption events (TDEs) discovered in the western Galactic hemisphere of the first two eROSITA all-sky surveys (eRASS1 and eRASS2) performed between Dec 2019 and Dec 2020. We compiled a TDE sample from the catalog of eROSITAâ€™s extragalactic transients and variables eRO-ExTra, which includes X-ray sources with a variability significance and fractional amplitude over four between eRASS1 and eRASS2, not associated with known AGNs. Each X-ray source is associated with an optical counterpart from the Legacy Survey DR10. Canonical TDEs were selected based on their X-ray light-curve properties (single flare or decline), soft X-ray spectra ($\Gamma&gt;3$), and the absence of archival X-ray variability and AGN signatures in their host photometry and spectroscopy. The sample includes 31 X-ray-selected TDE candidates with redshifts of $0.02&lt; z&lt;0.34$ and luminosities of $5.7 \times 10^{41}&lt;L_X&lt;5.3 \times 10^{44}$ erg&#x2F;s in the 0.2-6.0 keV rest frame, of which 30 are canonical TDEs and one is an off-nuclear TDE candidate. The derived X-ray luminosity function is best fit by a double power law with a luminosity break at $10^{44}$ erg&#x2F;s, corresponding to the Eddington-limiting prediction. This corresponds to a TDE volumetric rate of $ (2.3^{+1.2}_{-0.9})\times10^{-7},Mpc^{-3} yr^{-1}$ ($\approx1.2\times 10^{-5}$ events per galaxy per year). TDE host galaxies show a green-valley overdensity. In addition, 20%, 30%, and 15% of the sample exhibit flares in the optical, mid-infrared (mid-IR), or radio bands, respectively. We discuss the differences between X-ray, optical, and mid-IR TDE populations and the origins of multiwavelength flares in the context of the obscuring envelope and stream-stream collision models. Finally, we highlight TDE subpopulations that are not included in the canonical sample and should be explored in the future. </p>
<blockquote>
<p>æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†åœ¨ç¬¬ä¸€æœŸå’Œç¬¬äºŒæœŸeROSITAå…¨å¤©ç©ºè§‚æµ‹ï¼ˆeRASS1å’ŒeRASS2ï¼‰ä¸­ï¼Œäºè¥¿æ–¹é“¶æ²³ç³»åŠçƒå‘ç°çš„é€šè¿‡Xå°„çº¿é€‰å®šçš„ç»å…¸æ½®æ±æ’•è£‚äº‹ä»¶ï¼ˆTDEsï¼‰ã€‚æˆ‘ä»¬ä»eROSITAçš„æ˜Ÿç³»å¤–ç¬æ€å’Œå˜é‡æºç›®å½•eRO-ExTraä¸­æ•´ç†å‡ºTDEæ ·æœ¬ï¼Œå…¶ä¸­åŒ…æ‹¬åœ¨eRASS1å’ŒeRASS2ä¹‹é—´å…·æœ‰æ˜¾è‘—å˜åŒ–æ€§å’Œåˆ†æ•°æŒ¯å¹…è¶…è¿‡å››çš„Xå°„çº¿æºï¼Œå¹¶ä¸”ä¸ä¸å·²çŸ¥çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNsï¼‰ç›¸å…³è”ã€‚æ¯ä¸ªXå°„çº¿æºéƒ½ä¸Legacy Survey DR10ä¸­çš„å…‰å­¦å¯¹åº”ä½“ç›¸å…³è”ã€‚ç»å…¸TDEæ˜¯åŸºäºå®ƒä»¬çš„Xå°„çº¿å…‰åº¦æ›²çº¿ç‰¹æ€§ï¼ˆå•æ¬¡çˆ†å‘æˆ–ä¸‹é™ï¼‰ã€è½¯Xå°„çº¿å…‰è°±ï¼ˆÎ³&gt; 3ï¼‰ä»¥åŠåœ¨å…¶å®¿ä¸»çš„å…‰åº¦æµ‹å®šå’Œå…‰è°±ä¸­ä¸å­˜åœ¨å½’æ¡£çš„Xå°„çº¿å˜æ€§å’Œæ´»è·ƒæ˜Ÿç³»æ ¸ç‰¹å¾è€Œé€‰æ‹©çš„ã€‚æ ·æœ¬åŒ…æ‹¬31ä¸ªé€šè¿‡Xå°„çº¿é€‰å®šçš„TDEå€™é€‰è€…ï¼Œçº¢ç§»èŒƒå›´ä¸º0.02&lt;z&lt;0.34ï¼Œåœ¨é™æ­¢å¸§çš„0.2-6.0 keVä¸‹ï¼Œå…‰åº¦èŒƒå›´ä¸º5.7Ã—10^41 &lt;Lx&lt; 5.3Ã—10^44 erg&#x2F;sï¼Œå…¶ä¸­30ä¸ªæ˜¯ç»å…¸TDEï¼Œä¸€ä¸ªæ˜¯ç¦»æ ¸TDEå€™é€‰è€…ã€‚æ¨å¯¼å‡ºçš„Xå°„çº¿å…‰åº¦åˆ†å¸ƒæœ€é€‚åˆç”¨åŒå¹‚å¾‹æ‹Ÿåˆï¼Œå…‰åº¦ä¸­æ–­åœ¨10^44 erg&#x2F;så¤„ï¼Œè¿™ä¸çˆ±ä¸é¡¿æé™é¢„æµ‹ç›¸å¯¹åº”ã€‚è¿™å¯¹åº”äºTDEçš„ä½“ç§¯ç‡ä¸ºï¼ˆ2.3Â±1.2ï¼‰Ã—10^-7Mpc^-3yr^-1ï¼ˆæ¯å¹´æ¯æ˜Ÿç³»çº¦å‘ç”Ÿ1.2Ã—10^-5æ¬¡äº‹ä»¶ï¼‰ã€‚TDEå®¿ä¸»æ˜Ÿç³»æ˜¾ç¤ºå‡ºç»¿è°·è¿‡åº¦å¯†é›†ã€‚æ­¤å¤–ï¼Œæ ·æœ¬ä¸­æœ‰20%ã€30%å’Œ15%çš„æºåœ¨å…‰å­¦ã€ä¸­çº¢å¤–æˆ–å°„é¢‘æ³¢æ®µè¡¨ç°å‡ºçˆ†å‘ã€‚æˆ‘ä»¬è®¨è®ºäº†Xå°„çº¿ã€å…‰å­¦å’Œä¸­çº¢å¤–TDEç§ç¾¤ä¹‹é—´çš„å·®å¼‚ä»¥åŠå¤šæ³¢é•¿çˆ†å‘çš„èµ·æºï¼Œå¹¶ç»“åˆé®è”½åŒ…å±‚å’Œæµ-æµç¢°æ’æ¨¡å‹è¿›è¡Œè®¨è®ºã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†ä¸åŒ…æ‹¬åœ¨å…¸å‹æ ·æœ¬ä¸­çš„TDEäºšç¾¤ï¼Œè¿™äº›äºšç¾¤åº”åœ¨æœªæ¥è¿›è¡Œæ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08424v2">PDF</a> 23 pages, 12 figures. Accepted for publication in A&amp;A. Added a   reference</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é€šè¿‡Xå°„çº¿é€‰æ‹©çš„å¤©ä½“æ½®æ±æ’•è£‚äº‹ä»¶ï¼ˆTDEsï¼‰ã€‚è¿™äº›äº‹ä»¶æ˜¯åœ¨eROSITAçš„å‰ä¸¤æ¬¡å…¨å¤©ç©ºè°ƒæŸ¥ï¼ˆeRASS1å’ŒeRASS2ï¼‰ä¸­å‘ç°çš„ï¼Œè°ƒæŸ¥æ—¶é—´ä¸º2019å¹´12æœˆè‡³2020å¹´12æœˆã€‚é€šè¿‡å¯¹eROSITAæ˜Ÿç³»å¤–å¤©ä½“ç¬æ€å’Œå˜é‡eRO-ExTraç›®å½•è¿›è¡Œç¼–è¯‘ï¼Œç­›é€‰å‡ºå…·æœ‰ç‰¹å®šXå°„çº¿å…‰å˜æ›²çº¿ç‰¹æ€§ã€è½¯Xå°„çº¿å…‰è°±ä»¥åŠæ— æ¡£æ¡ˆXå°„çº¿å˜æ€§å’Œå®¿ä¸»çš„å…‰å­¦å’Œå…‰è°±å­¦ä¸­æ²¡æœ‰AGNç‰¹å¾çš„TDEå€™é€‰è€…ã€‚æ ·æœ¬åŒ…æ‹¬31ä¸ªXå°„çº¿é€‰æ‹©çš„TDEå€™é€‰è€…ï¼Œçº¢ç§»èŒƒå›´åœ¨0.02&lt;z&lt;0.34ä¹‹é—´ï¼Œå…‰åº¦åœ¨5.7Ã—10^{41}&lt;Lx&lt;5.3Ã—10^{44}erg&#x2F;sä¹‹é—´ã€‚å¾—åˆ°çš„Xå°„çº¿å…‰åº¦å‡½æ•°æœ€é€‚åˆç”¨åŒå¹‚å¾‹æ‹Ÿåˆï¼Œå…‰åº¦ä¸­æ–­åœ¨10^{44}erg&#x2F;så¤„ã€‚ç›¸åº”çš„TDEä½“ç§¯ç‡ä¸ºï¼ˆ2.3^{+1.2}_{-0.9}ï¼‰Ã—10^{-7}Mpc^{-3}yr^{-1}ï¼Œçº¦ä¸ºæ¯ä¸ªæ˜Ÿç³»æ¯å¹´æœ‰å¤§çº¦1.2Ã—10^{-5}æ¬¡äº‹ä»¶ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹TDEå®¿ä¸»æ˜Ÿç³»çš„ç»¿è°·å¯†åº¦ã€å¤šæ³¢é•¿è€€æ–‘çš„å·®å¼‚åŠèµ·æºä»¥åŠæœªæ¥å€¼å¾—æ¢ç´¢çš„TDEäºšç¾¤è¿›è¡Œäº†æ¢è®¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨eROSITAçš„å‰ä¸¤æ¬¡å…¨å¤©ç©ºè°ƒæŸ¥å‘ç°äº†æ½®æ±æ’•è£‚äº‹ä»¶ï¼ˆTDEsï¼‰ã€‚</li>
<li>æ ·æœ¬åŒ…æ‹¬31ä¸ªXå°„çº¿é€‰æ‹©çš„TDEå€™é€‰è€…ï¼Œæ˜¾ç¤ºå‡ºç‰¹å®šçš„Xå°„çº¿å…‰å˜æ›²çº¿ç‰¹æ€§å’Œè½¯Xå°„çº¿å…‰è°±ã€‚</li>
<li>TDEçš„å…‰åº¦å‡½æ•°é€‚åˆç”¨åŒå¹‚å¾‹æ‹Ÿåˆï¼Œå…‰åº¦ä¸­æ–­ä½ç½®ä¸ç†è®ºé¢„æµ‹ç›¸ç¬¦ã€‚</li>
<li>TDEçš„ä½“ç§¯ç‡ä¸ºï¼ˆæ¯ç«‹æ–¹ç™¾ä¸‡ç§’å·®è·æ¯å¹´å¤§çº¦åä¸‡åˆ†ä¹‹ä¸€äº‹ä»¶ï¼‰ã€‚</li>
<li>TDEå®¿ä¸»æ˜Ÿç³»æ˜¾ç¤ºç»¿è°·å¯†åº¦ç°è±¡ã€‚</li>
<li>åœ¨å…‰å­¦ã€ä¸­çº¢å¤–æˆ–å°„é¢‘æ³¢æ®µï¼Œåˆ†åˆ«æœ‰20%ã€30%ã€å’Œ15%çš„æ ·æœ¬è¡¨ç°å‡ºè€€æ–‘æ´»åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce26fa1f1afd8dc508f627bdbcd327eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b24e889c79c7d6397c343c6c7659ef3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6b27470da8504c94253f205e89d663c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ede9dba2b3e38ba18ca6eb8d1abe4284.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FUSION-Frequency-guided-Underwater-Spatial-Image-recOnstructioN"><a href="#FUSION-Frequency-guided-Underwater-Spatial-Image-recOnstructioN" class="headerlink" title="FUSION: Frequency-guided Underwater Spatial Image recOnstructioN"></a>FUSION: Frequency-guided Underwater Spatial Image recOnstructioN</h2><p><strong>Authors:Jaskaran Singh Walia, Shravan Venkatraman, Pavithra LK</strong></p>
<p>Underwater images suffer from severe degradations, including color distortions, reduced visibility, and loss of structural details due to wavelength-dependent attenuation and scattering. Existing enhancement methods primarily focus on spatial-domain processing, neglecting the frequency domainâ€™s potential to capture global color distributions and long-range dependencies. To address these limitations, we propose FUSION, a dual-domain deep learning framework that jointly leverages spatial and frequency domain information. FUSION independently processes each RGB channel through multi-scale convolutional kernels and adaptive attention mechanisms in the spatial domain, while simultaneously extracting global structural information via FFT-based frequency attention. A Frequency Guided Fusion module integrates complementary features from both domains, followed by inter-channel fusion and adaptive channel recalibration to ensure balanced color distributions. Extensive experiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION achieves state-of-the-art performance, consistently outperforming existing methods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883 on UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual enhancement metrics (best UIQM of 3.414 on UIEB), while requiring significantly fewer parameters (0.28M) and lower computational complexity, demonstrating its suitability for real-time underwater imaging applications. </p>
<blockquote>
<p>æ°´ä¸‹å›¾åƒå—åˆ°ä¸¥é‡çš„é€€åŒ–å½±å“ï¼ŒåŒ…æ‹¬è‰²å½©å¤±çœŸã€èƒ½è§åº¦é™ä½ä»¥åŠç”±äºæ³¢é•¿ä¾èµ–æ€§è¡°å‡å’Œæ•£å°„å¯¼è‡´çš„ç»“æ„ç»†èŠ‚ä¸¢å¤±ã€‚ç°æœ‰çš„å¢å¼ºæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç©ºé—´åŸŸå¤„ç†ä¸Šï¼Œå¿½è§†äº†é¢‘ç‡åŸŸåœ¨æ•æ‰å…¨å±€é¢œè‰²åˆ†å¸ƒå’Œé•¿è·ç¦»ä¾èµ–æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FUSIONï¼Œè¿™æ˜¯ä¸€ä¸ªåŒåŸŸæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒè”åˆåˆ©ç”¨ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸä¿¡æ¯ã€‚FUSIONåœ¨ç©ºé—´åŸŸä¸­é€šè¿‡å¤šå°ºåº¦å·ç§¯æ ¸å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ç‹¬ç«‹å¤„ç†æ¯ä¸ªRGBé€šé“ï¼ŒåŒæ—¶åŸºäºFFTçš„é¢‘ç‡æ³¨æ„åŠ›æå–å…¨å±€ç»“æ„ä¿¡æ¯ã€‚é¢‘ç‡å¼•å¯¼èåˆæ¨¡å—æ•´åˆä¸¤ä¸ªåŸŸä¸­çš„äº’è¡¥ç‰¹å¾ï¼Œç„¶åè¿›è¡Œè·¨é€šé“èåˆå’Œè‡ªé€‚åº”é€šé“æ ¡å‡†ï¼Œä»¥ç¡®ä¿é¢œè‰²åˆ†å¸ƒçš„å¹³è¡¡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ï¼ˆUIEBã€EUVPã€SUIM-Eï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFUSIONè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨é‡å»ºä¿çœŸåº¦ã€æ„ŸçŸ¥è´¨é‡å’Œè§†è§‰å¢å¼ºæŒ‡æ ‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåœ¨UIEBä¸Šï¼ŒPSNRæœ€é«˜è¾¾23.717 dBï¼ŒSSIMä¸º0.883ï¼›åœ¨UIEBä¸Šï¼ŒLPIPSæœ€ä½ä¸º0.112ï¼›åœ¨UIEBä¸Šï¼ŒUIQMæœ€é«˜ä¸º3.414ï¼‰ï¼ŒåŒæ—¶å‚æ•°æ›´å°‘ï¼ˆä»…0.28Mï¼‰ä¸”è®¡ç®—å¤æ‚åº¦æ›´ä½ï¼Œè¯æ˜å…¶é€‚ç”¨äºå®æ—¶æ°´ä¸‹æˆåƒåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01243v2">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFUSIONçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆç©ºé—´åŸŸå’Œé¢‘åŸŸä¿¡æ¯ï¼Œç”¨äºæå‡æ°´ä¸‹å›¾åƒçš„è´¨é‡ã€‚FUSIONèƒ½åœ¨ç©ºé—´åŸŸç‹¬ç«‹å¤„ç†RGBé€šé“ï¼Œå¹¶å¼•å…¥å¤šå°ºåº¦å·ç§¯æ ¸å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶åˆ©ç”¨FFTç®—æ³•æå–é¢‘åŸŸä¸­çš„å…¨å±€ç»“æ„ä¿¡æ¯ã€‚é€šè¿‡é¢‘ç‡å¼•å¯¼èåˆæ¨¡å—ï¼Œèåˆä¸¤ä¸ªåŸŸçš„ç‰¹å¾ï¼Œç„¶åè¿›è¡Œè·¨é€šé“èåˆå’Œè‡ªé€‚åº”é€šé“æ ¡å‡†ï¼Œç¡®ä¿é¢œè‰²åˆ†å¸ƒçš„å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒFUSIONåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒå­˜åœ¨ä¸¥é‡çš„å¤±çœŸé—®é¢˜ï¼ŒåŒ…æ‹¬é¢œè‰²å¤±çœŸã€èƒ½è§åº¦é™ä½å’Œç»“æ„ç»†èŠ‚ä¸¢å¤±ã€‚</li>
<li>ç°æœ‰å¢å¼ºæ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´åŸŸå¤„ç†ï¼Œå¿½è§†äº†é¢‘åŸŸåœ¨å…¨çƒé¢œè‰²åˆ†å¸ƒå’Œé•¿è·ç¦»ä¾èµ–æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>FUSIONæ¡†æ¶ç»“åˆç©ºé—´åŸŸå’Œé¢‘åŸŸä¿¡æ¯ï¼Œä»¥æ”¹è¿›æ°´ä¸‹å›¾åƒè´¨é‡ã€‚</li>
<li>FUSIONåœ¨ç©ºé—´åŸŸç‹¬ç«‹å¤„ç†RGBé€šé“ï¼Œå¹¶å¼•å…¥å¤šå°ºåº¦å·ç§¯æ ¸å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>FUSIONåˆ©ç”¨FFTç®—æ³•æå–é¢‘åŸŸä¸­çš„å…¨å±€ç»“æ„ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡é¢‘ç‡å¼•å¯¼èåˆæ¨¡å—å’Œä¸¤ä¸ªåŸŸçš„ç‰¹å¾èåˆï¼Œå®ç°è·¨é€šé“èåˆå’Œè‡ªé€‚åº”é€šé“æ ¡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-baefeef764b49dc8cb36666a99110dd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87fb382448648b80af9fc06d928fb688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60a87ed4db6646b8096084cafaf9e0ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-111dd38879e00b55d05f0e65f184664e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2b7a28c212b155a0086efc47836e8c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b9e72f521527cde6b21d491e7b293e6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Text-Promptable-Propagation-for-Referring-Medical-Image-Sequence-Segmentation"><a href="#Text-Promptable-Propagation-for-Referring-Medical-Image-Sequence-Segmentation" class="headerlink" title="Text-Promptable Propagation for Referring Medical Image Sequence   Segmentation"></a>Text-Promptable Propagation for Referring Medical Image Sequence   Segmentation</h2><p><strong>Authors:Runtian Yuan, Mohan Chen, Jilan Xu, Ling Zhou, Qingqiu Li, Yuejie Zhang, Rui Feng, Tao Zhang, Shang Gao</strong></p>
<p>Referring Medical Image Sequence Segmentation (Ref-MISS) is a novel and challenging task that aims to segment anatomical structures in medical image sequences (\emph{e.g.} endoscopy, ultrasound, CT, and MRI) based on natural language descriptions. This task holds significant clinical potential and offers a user-friendly advancement in medical imaging interpretation. Existing 2D and 3D segmentation models struggle to explicitly track objects of interest across medical image sequences, and lack support for nteractive, text-driven guidance. To address these limitations, we propose Text-Promptable Propagation (TPP), a model designed for referring medical image sequence segmentation. TPP captures the intrinsic relationships among sequential images along with their associated textual descriptions. Specifically, it enables the recognition of referred objects through cross-modal referring interaction, and maintains continuous tracking across the sequence via Transformer-based triple propagation, using text embeddings as queries. To support this task, we curate a large-scale benchmark, Ref-MISS-Bench, which covers 4 imaging modalities and 20 different organs and lesions. Experimental results on this benchmark demonstrate that TPP consistently outperforms state-of-the-art methods in both medical segmentation and referring video object segmentation. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåºåˆ—åˆ†å‰²å¼•ç”¨ï¼ˆRef-MISSï¼‰æ˜¯ä¸€é¡¹æ–°é¢–ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å¯¹åŒ»å­¦å›¾åƒåºåˆ—ï¼ˆå¦‚å†…çª¥é•œã€è¶…å£°ã€CTå’ŒMRIï¼‰è¿›è¡Œè§£å‰–ç»“æ„åˆ†å‰²ã€‚è¿™ä¸€ä»»åŠ¡åœ¨ä¸´åºŠåŒ»å­¦ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºåŒ»å­¦å½±åƒè§£è¯»æä¾›äº†ç”¨æˆ·å‹å¥½çš„è¿›æ­¥ã€‚ç°æœ‰çš„äºŒç»´å’Œä¸‰ç»´åˆ†å‰²æ¨¡å‹éš¾ä»¥åœ¨åŒ»å­¦å›¾åƒåºåˆ—ä¸­æ˜ç¡®è·Ÿè¸ªæ„Ÿå…´è¶£çš„ç›®æ ‡ï¼Œå¹¶ä¸”ç¼ºä¹äº¤äº’å¼çš„æ–‡æœ¬é©±åŠ¨æŒ‡å¯¼æ”¯æŒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ–‡æœ¬æç¤ºä¼ æ’­ï¼ˆTPPï¼‰æ¨¡å‹ï¼Œä¸“ä¸ºå¼•ç”¨åŒ»å­¦å›¾åƒåºåˆ—åˆ†å‰²è®¾è®¡ã€‚TPPæ•æ‰åºåˆ—å›¾åƒä¹‹é—´çš„å†…åœ¨å…³ç³»åŠå…¶ç›¸å…³çš„æ–‡æœ¬æè¿°ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡è·¨æ¨¡æ€å¼•ç”¨äº¤äº’è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œå¹¶é€šè¿‡åŸºäºTransformerçš„ä¸‰é‡ä¼ æ’­åœ¨åºåˆ—ä¸­ä¿æŒæŒç»­è·Ÿè¸ªï¼Œä½¿ç”¨æ–‡æœ¬åµŒå…¥ä½œä¸ºæŸ¥è¯¢ã€‚ä¸ºäº†æ”¯æŒæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†Ref-MISS-Benchï¼Œæ¶µç›–4ç§æˆåƒæ¨¡å¼å’Œ20ç§ä¸åŒçš„å™¨å®˜å’Œç—…å˜ã€‚åœ¨è¯¥åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTPPåœ¨åŒ»å­¦åˆ†å‰²å’Œå¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢éƒ½å§‹ç»ˆä¼˜äºç°æœ‰æœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11093v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåºåˆ—åˆ†å‰²æ˜¯ä¸€é¡¹æ–°çš„æŒ‘æˆ˜ä»»åŠ¡ï¼Œæ—¨åœ¨åŸºäºè‡ªç„¶è¯­è¨€æè¿°å¯¹åŒ»å­¦å›¾åƒåºåˆ—ï¼ˆå¦‚å†…çª¥é•œã€è¶…å£°ã€CTå’ŒMRIï¼‰è¿›è¡Œè§£å‰–ç»“æ„åˆ†å‰²ã€‚ç°æœ‰æ¨¡å‹éš¾ä»¥åœ¨åŒ»å­¦å›¾åƒåºåˆ—ä¸­æ˜¾å¼è·Ÿè¸ªæ„Ÿå…´è¶£å¯¹è±¡ï¼Œå¹¶ç¼ºä¹æ–‡æœ¬é©±åŠ¨çš„äº¤äº’å¼æŒ‡å¯¼æ”¯æŒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Text-Promptable Propagationï¼ˆTPPï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”¨äºåŒ»å­¦å›¾åƒåºåˆ—åˆ†å‰²çš„å¼•ç”¨ã€‚TPPèƒ½å¤Ÿæ•è·åºåˆ—å›¾åƒä¹‹é—´çš„å†…åœ¨å…³ç³»åŠå…¶ç›¸å…³æ–‡æœ¬æè¿°ï¼Œé€šè¿‡è·¨æ¨¡æ€å¼•ç”¨äº¤äº’è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œå¹¶é€šè¿‡åŸºäºTransformerçš„ä¸‰é‡ä¼ æ’­åœ¨åºåˆ—ä¸­æŒç»­è·Ÿè¸ªå¯¹è±¡ï¼Œæ–‡æœ¬åµŒå…¥ä½œä¸ºæŸ¥è¯¢ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸ºæ­¤ä»»åŠ¡æ„å»ºäº†å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†Ref-MISS-Benchï¼Œæ¶µç›–å››ç§æˆåƒæ¨¡å¼å’ŒäºŒåç§ä¸åŒå™¨å®˜å’Œç—…å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTPPåœ¨åŒ»å­¦åˆ†å‰²å’Œå¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ref-MISSæ˜¯ä¸€é¡¹æ–°çš„åŒ»å­¦å›¾åƒåºåˆ—åˆ†å‰²ä»»åŠ¡ï¼Œæ—¨åœ¨åŸºäºè‡ªç„¶è¯­è¨€æè¿°è¿›è¡Œè§£å‰–ç»“æ„åˆ†å‰²ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåºåˆ—ä¸­è·Ÿè¸ªæ„Ÿå…´è¶£å¯¹è±¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>TPPæ¨¡å‹è¢«æå‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œèƒ½å¤Ÿæ•è·åºåˆ—å›¾åƒé—´çš„å†…åœ¨å…³ç³»åŠå…¶æ–‡æœ¬æè¿°ã€‚</li>
<li>TPPé€šè¿‡è·¨æ¨¡æ€å¼•ç”¨äº¤äº’è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œå¹¶åœ¨åºåˆ—ä¸­æŒç»­è·Ÿè¸ªå¯¹è±¡ã€‚</li>
<li>æˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†Ref-MISS-Benchæ¥æ”¯æŒæ­¤ä»»åŠ¡ã€‚</li>
<li>Ref-MISS-Benchæ¶µç›–å››ç§æˆåƒæ¨¡å¼å’ŒäºŒåç§ä¸åŒçš„å™¨å®˜å’Œç—…å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-546e9a625ff86ef1e651894fd4cf7058.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-16f12ce05783a8c9e0f6a523966aa21c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69914cb0aa118ac96b486e8859d90a0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b45def3ad18d78b1569ff5d04302c11d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68fdfb86c1f8c6fa837bbd6b1e8ddbe7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Navigating-Image-Restoration-with-VARâ€™s-Distribution-Alignment-Prior"><a href="#Navigating-Image-Restoration-with-VARâ€™s-Distribution-Alignment-Prior" class="headerlink" title="Navigating Image Restoration with VARâ€™s Distribution Alignment Prior"></a>Navigating Image Restoration with VARâ€™s Distribution Alignment Prior</h2><p><strong>Authors:Siyang Wang, Feng Zhao</strong></p>
<p>Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VARâ€™s adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks. </p>
<blockquote>
<p>è®­ç»ƒåœ¨å¤§é‡é«˜è´¨é‡æ•°æ®é›†ä¸Šçš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å¹²å‡€å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºå°†é€€åŒ–ç‰¹å¾è½¬æ¢ä¸ºå¹²å‡€ç‰¹å¾è¿›è¡Œå›¾åƒæ¢å¤çš„å¼ºå¤§å…ˆéªŒä¿¡æ¯ã€‚VARä½œä¸ºä¸€ç§æ–°å‹çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡åº”ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡è‡ªå›å½’è¿‡ç¨‹é€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œè¿™ä¸æ¢å¤ç•Œå¹¿æ³›è®¤å¯çš„å¤šå°ºåº¦æ¢å¤åŸåˆ™ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨åˆ©ç”¨VARè¿›è¡Œå›¾åƒé‡å»ºçš„è¿‡ç¨‹ä¸­ï¼Œå°ºåº¦é¢„æµ‹ä¼šè‡ªåŠ¨è°ƒåˆ¶è¾“å…¥ï¼Œä¿ƒè¿›åç»­å°ºåº¦çš„è¡¨ç¤ºä¸å¹²å‡€å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚ä¸ºäº†åˆ©ç”¨VARåœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†VARå†…çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºæ¢å¤å…ˆéªŒï¼Œä»è€Œæ¨è¿›äº†æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„VarFormeræ¡†æ¶ã€‚è¿™äº›å…ˆéªŒçŸ¥è¯†çš„æˆ˜ç•¥åº”ç”¨ä½¿æˆ‘ä»¬çš„VarFormeråœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒçš„è®¡ç®—æˆæœ¬ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„VarFormeråœ¨å„ç§æ¢å¤ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¤šä»»åŠ¡å›¾åƒæ¢å¤æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.21063v2">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆæ¨¡å‹ç»è¿‡å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†è®­ç»ƒåï¼Œèƒ½æœ‰æ•ˆæ•æ‰æ¸…æ™°å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œåœ¨å›¾åƒä¿®å¤ä¸­å°†é€€åŒ–ç‰¹å¾è½¬æ¢ä¸ºæ¸…æ™°ç‰¹å¾æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„å…ˆéªŒèƒ½åŠ›ã€‚ä¸€ç§æ–°çš„å›¾åƒç”ŸæˆèŒƒå¼VARï¼Œé€šè¿‡åº”ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚VARé€šè¿‡è‡ªå›å½’è¿‡ç¨‹é€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œç¬¦åˆä¿®å¤ç•Œå¹¿æ³›è®¤å¯çš„å¤šå°ºåº¦ä¿®å¤åŸåˆ™ã€‚æ­¤å¤–ï¼Œåœ¨åˆ©ç”¨VARè¿›è¡Œå›¾åƒé‡å»ºçš„è¿‡ç¨‹ä¸­ï¼Œè§‚å¯Ÿåˆ°å°ºåº¦é¢„æµ‹å¯è‡ªåŠ¨è°ƒåˆ¶è¾“å…¥ï¼Œæœ‰åŠ©äºåç»­å°ºåº¦è¡¨ç¤ºä¸å¹²å‡€å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚ä¸ºäº†åˆ©ç”¨VARåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”åˆ†å¸ƒå¯¹é½èƒ½åŠ›ï¼Œå°†VARå†…çš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºä¿®å¤å…ˆéªŒï¼Œä»è€Œæå‡ºç²¾å¿ƒè®¾è®¡çš„VarFormeræ¡†æ¶ã€‚è¿™äº›å…ˆéªŒçŸ¥è¯†çš„æˆ˜ç•¥åº”ç”¨ä½¿VarFormeråœ¨æœªè§ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ³›åŒ–ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVarFormeråœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰å¤šä»»åŠ¡å›¾åƒä¿®å¤æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹ç»è¿‡é«˜è´¨é‡æ•°æ®é›†è®­ç»ƒåï¼Œèƒ½æœ‰æ•ˆæ•æ‰æ¸…æ™°å›¾åƒçš„ç»“æ„å’Œç»Ÿè®¡ç‰¹æ€§ã€‚</li>
<li>VARä½œä¸ºä¸€ç§æ–°çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>VARé€šè¿‡è‡ªå›å½’è¿‡ç¨‹é€æ­¥æ•æ‰å…¨å±€ç»“æ„å’Œç²¾ç»†ç»†èŠ‚ï¼Œç¬¦åˆå¤šå°ºåº¦ä¿®å¤åŸåˆ™ã€‚</li>
<li>VARåœ¨å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­èƒ½å®ç°å°ºåº¦é¢„æµ‹è‡ªåŠ¨è°ƒåˆ¶è¾“å…¥ï¼Œä¿ƒè¿›åç»­å°ºåº¦è¡¨ç¤ºä¸å¹²å‡€å›¾åƒåˆ†å¸ƒçš„å¯¹é½ã€‚</li>
<li>VarFormeråˆ©ç”¨VARçš„å¤šå°ºåº¦æ½œåœ¨è¡¨ç¤ºä½œä¸ºä¿®å¤å…ˆéªŒï¼Œæé«˜äº†å›¾åƒä¿®å¤çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚</li>
<li>VarFormeræ¡†æ¶é€šè¿‡æˆ˜ç•¥åº”ç”¨è¿™äº›å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°äº†åœ¨æœªè§ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜ï¼ŒVarFormeråœ¨å¤šç§å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.21063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-460f3bbfb724c48a3f1a79775c244a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3874c7c5a9090b0a5be87bf228b5d2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a35552c3b292b12b5879f78487d231d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34a2362ccb0b54582a79c040db28c626.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97022af2dfa48cd126f6af7ccdfc2034.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MambaXCTrack-Mamba-based-Tracker-with-SSM-Cross-correlation-and-Motion-Prompt-for-Ultrasound-Needle-Tracking"><a href="#MambaXCTrack-Mamba-based-Tracker-with-SSM-Cross-correlation-and-Motion-Prompt-for-Ultrasound-Needle-Tracking" class="headerlink" title="MambaXCTrack: Mamba-based Tracker with SSM Cross-correlation and Motion   Prompt for Ultrasound Needle Tracking"></a>MambaXCTrack: Mamba-based Tracker with SSM Cross-correlation and Motion   Prompt for Ultrasound Needle Tracking</h2><p><strong>Authors:Yuelin Zhang, Long Lei, Wanquan Yan, Tianyi Zhang, Raymond Shing-Yan Tang, Shing Shin Cheng</strong></p>
<p>Ultrasound (US)-guided needle insertion is widely employed in percutaneous interventions. However, providing feedback on the needle tip position via US imaging presents challenges due to noise, artifacts, and the thin imaging plane of US, which degrades needle features and leads to intermittent tip visibility. In this paper, a Mamba-based US needle tracker MambaXCTrack utilizing structured state space models cross-correlation (SSMX-Corr) and implicit motion prompt is proposed, which is the first application of Mamba in US needle tracking. The SSMX-Corr enhances cross-correlation by long-range modeling and global searching of distant semantic features between template and search maps, benefiting the tracking under noise and artifacts by implicitly learning potential distant semantic cues. By combining with cross-map interleaved scan (CIS), local pixel-wise interaction with positional inductive bias can also be introduced to SSMX-Corr. The implicit low-level motion descriptor is proposed as a non-visual prompt to enhance tracking robustness, addressing the intermittent tip visibility problem. Extensive experiments on a dataset with motorized needle insertion in both phantom and tissue samples demonstrate that the proposed tracker outperforms other state-of-the-art trackers while ablation studies further highlight the effectiveness of each proposed tracking module. </p>
<blockquote>
<p>è¶…å£°ï¼ˆUSï¼‰å¼•å¯¼ä¸‹çš„é’ˆæ’å…¥æœ¯åœ¨ç»çš®ä»‹å…¥ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œé€šè¿‡è¶…å£°æˆåƒæä¾›é’ˆå°–ä½ç½®åé¦ˆå´é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå™ªå£°ã€ä¼ªå½±å’Œè¶…å£°æˆåƒå¹³é¢è¾ƒè–„ç­‰åŸå› å¯¼è‡´é’ˆçš„ç‰¹å¾é€€åŒ–ï¼Œä»¥åŠé’ˆå°–å¯è§æ€§é—´æ­‡æ€§æ¶ˆå¤±ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºMambaçš„è¶…å£°é’ˆè·Ÿè¸ªå™¨MambaXCTrackï¼Œå®ƒåˆ©ç”¨ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹çš„äº’ç›¸å…³ï¼ˆSSMX-Corrï¼‰å’Œéšå¼è¿åŠ¨æç¤ºï¼Œè¿™æ˜¯Mambaåœ¨è¶…å£°é’ˆè·Ÿè¸ªä¸­çš„é¦–æ¬¡åº”ç”¨ã€‚SSMX-Corré€šè¿‡é•¿ç¨‹å»ºæ¨¡å’Œæ¨¡æ¿ä¸æœç´¢åœ°å›¾ä¹‹é—´è¿œè·ç¦»è¯­ä¹‰ç‰¹å¾çš„å…¨å±€æœç´¢ï¼Œå¢å¼ºäº†äº’ç›¸å…³ï¼Œé€šè¿‡éšå¼å­¦ä¹ æ½œåœ¨çš„è¿œè·ç¦»è¯­ä¹‰çº¿ç´¢ï¼Œæœ‰åˆ©äºåœ¨å™ªå£°å’Œä¼ªå½±çš„æƒ…å†µä¸‹è¿›è¡Œè·Ÿè¸ªã€‚é€šè¿‡ä¸è·¨å›¾äº¤ç»‡æ‰«æï¼ˆCISï¼‰ç›¸ç»“åˆï¼Œè¿˜å¯ä»¥å°†å…·æœ‰ä½ç½®è¯±å¯¼åè§çš„å±€éƒ¨åƒç´ çº§äº¤äº’å¼•å…¥åˆ°SSMX-Corrä¸­ã€‚æå‡ºäº†ä¸€ç§éšå¼ä½çº§è¿åŠ¨æè¿°ç¬¦ä½œä¸ºéè§†è§‰æç¤ºï¼Œä»¥å¢å¼ºè·Ÿè¸ªçš„ç¨³å¥æ€§ï¼Œè§£å†³é’ˆå°–é—´æ­‡æ€§å¯è§æ€§é—®é¢˜ã€‚åœ¨æ¨¡æ‹Ÿå’Œç»„ç»‡æ ·æœ¬ä¸­çš„ç”µåŠ¨é’ˆæ’å…¥æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è·Ÿè¸ªå™¨ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„è·Ÿè¸ªå™¨ï¼Œè€Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥çªå‡ºäº†æ¯ä¸ªæ‰€æå‡ºçš„è·Ÿè¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08395v2">PDF</a> Accepted by RAL</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºMambaçš„è¶…å£°é’ˆè¿½è¸ªæŠ€æœ¯MambaXCTrackï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹çš„äº’ç›¸å…³ï¼ˆSSMX-Corrï¼‰å’Œéšå¼è¿åŠ¨æç¤ºï¼Œä¸ºè¶…å£°å¼•å¯¼ä¸‹çš„é’ˆæ’å…¥æä¾›äº†åé¦ˆã€‚SSMX-Corré€šè¿‡é•¿ç¨‹å»ºæ¨¡å’Œæ¨¡æ¿ä¸æœç´¢å›¾ä¹‹é—´è¿œè·ç¦»è¯­ä¹‰ç‰¹å¾çš„å…¨å±€æœç´¢ï¼Œæé«˜äº†äº’ç›¸å…³æ€§èƒ½ï¼Œå¹¶åœ¨å™ªå£°å’Œä¼ªå½±ä¸‹é€šè¿‡éšå¼å­¦ä¹ æ½œåœ¨è¿œè·ç¦»è¯­ä¹‰çº¿ç´¢æ¥å¢å¼ºè·Ÿè¸ªæ•ˆæœã€‚ç»“åˆè·¨å›¾äº¤ç»‡æ‰«æï¼ˆCISï¼‰ï¼Œè¿˜å¯ä»¥å°†å¸¦æœ‰ä½ç½®è¯±å¯¼åè§çš„å±€éƒ¨åƒç´ çº§äº¤äº’å¼•å…¥åˆ°SSMX-Corrä¸­ã€‚æ­¤å¤–ï¼Œæå‡ºäº†éšå¼ä½çº§è¿åŠ¨æè¿°ç¬¦ä½œä¸ºéè§†è§‰æç¤ºï¼Œä»¥æé«˜è·Ÿè¸ªçš„ç¨³å¥æ€§ï¼Œè§£å†³é’ˆå°–é—´æ­‡æ€§å¯è§çš„é—®é¢˜ã€‚åœ¨æ¨¡æ‹Ÿå’Œç»„ç»‡æ ·æœ¬ä¸Šçš„ç”µåŠ¨é’ˆæ’å…¥æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¿½è¸ªå™¨ä¼˜äºå…¶ä»–æœ€æ–°è¿½è¸ªå™¨ï¼Œè€Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥çªå‡ºäº†æ‰€æå‡ºè¿½è¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å¼•å¯¼ä¸‹çš„é’ˆæ’å…¥åœ¨ç»çš®ä»‹å…¥ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†æä¾›å…³äºé’ˆå°–ä½ç½®çš„åé¦ˆæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºMambaçš„è¶…å£°é’ˆè¿½è¸ªæŠ€æœ¯MambaXCTrackï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹çš„äº’ç›¸å…³ï¼ˆSSMX-Corrï¼‰ã€‚</li>
<li>SSMX-Corré€šè¿‡é•¿ç¨‹å»ºæ¨¡å’Œå…¨å±€æœç´¢å¢å¼ºäº’ç›¸å…³æ€§èƒ½ï¼Œå¹¶åœ¨å™ªå£°å’Œä¼ªå½±ä¸‹æé«˜è·Ÿè¸ªæ•ˆæœã€‚</li>
<li>ç»“åˆè·¨å›¾äº¤ç»‡æ‰«æï¼ˆCISï¼‰å¯ä»¥å¢å¼ºå±€éƒ¨åƒç´ çº§äº¤äº’ã€‚</li>
<li>æå‡ºäº†éšå¼ä½çº§è¿åŠ¨æè¿°ç¬¦ä½œä¸ºéè§†è§‰æç¤ºï¼Œè§£å†³é’ˆå°–é—´æ­‡æ€§å¯è§çš„é—®é¢˜ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’Œç»„ç»‡æ ·æœ¬ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¿½è¸ªå™¨ä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fec9633ac08be8b837a5507302b3541d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-566104ea0ab1ddd6d8d2912727663185.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5027ca2f959a2192b09321d6ca08cfa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24268a162ff6f1e8bbd391c25cc9bc55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1043bdbe5c47d9698236a60e4b05293d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d75478208a865322b06f14ec08e01b3e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ef8b1fbfa1129e284894ac2aba253f01.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Pseudo-Autoregressive Neural Codec Language Models for Efficient   Zero-Shot Text-to-Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-16/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b65add2678bc8eb9dbbe33652fc51cf8.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-16  Anchor Token Matching Implicit Structure Locking for Training-free AR   Image Editing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
